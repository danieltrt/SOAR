file_path,api_count,code
docs/gen_site.py,0,"b'#!/usr/bin/env python\n\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# Usage ###################\n# Run ./gen_site.py to build site with Analytics Zoo docs with following commands\n# -s: add scala docs\n# -p: add python docs\n# -m [port]: --startserver\n# -h: help\n# Example\n# ./gen_site.py -s -p -m 8080\n############################\n\nimport argparse\nimport sys\nimport os\nimport subprocess\n\n\nparser = argparse.ArgumentParser(description=\'Process Analytics Zoo docs.\')\nparser.add_argument(\'-s\', \'--scaladocs\',\n                    dest=\'scaladocsflag\', action=\'store_true\',\n                    help=\'Add scala doc to site\')\nparser.add_argument(\'-p\', \'--pythondocs\',\n                    dest=\'pythondocsflag\', action=\'store_true\',\n                    help=\'Add python doc to site\')\nparser.add_argument(\'-m\', \'--startserver\',\n                    dest=\'port\', type=int,\n                    help=\'Start server at PORT after building\')\nparser.add_argument(\'-d\', \'--startmkdocserve\',\n                    dest=\'debugport\', type=int,\n                    help=argparse.SUPPRESS)\nparser.add_argument(\'-l\', \'--localdoc\',\n                    dest=\'local_doc\', action=\'store_true\',\n                    help=\'Use local zoo doc repo(if it exists) instead of downloading from remote\')\n\nargs = parser.parse_args()\n\nscaladocs = args.scaladocsflag\n\npythondocs = args.pythondocsflag\n\nlocal_doc = args.local_doc\n\nscript_path = os.path.realpath(__file__)\ndir_name = os.path.dirname(script_path)\nos.chdir(dir_name)\n\n# check if mkdoc is installed\nsubprocess.run([\'mkdocs\', \'--version\'])  # pip install mkdocs==0.16.3\n\n# refresh local docs repo\nif not (local_doc and os.path.isdir(""/tmp/zoo-doc"")):\n    subprocess.run([\'rm\', \'-rf\', \'/tmp/zoo-doc\'])  # rm doc repo\n    # git clone doc repo\n    subprocess.run([\'git\', \'clone\', \'https://github.com/analytics-zoo/analytics-zoo.github.io.git\', \'/tmp/zoo-doc\'])\n\n# refresh theme folder\nsubprocess.run([\'rm\', \'-rf\', \'{}/mkdocs_windmill\'.format(dir_name)])  # rm theme folder\nsubprocess.run([\'cp\', \'-r\', \'/tmp/zoo-doc/mkdocs_windmill\', dir_name])\n\n# refresh css file\nsubprocess.run([\'cp\', \'/tmp/zoo-doc/extra.css\', \'{}/docs\'.format(dir_name)])  # mv theme folder\n\n# mkdocs build\nsubprocess.run([\'mkdocs\', \'build\'])\n\n# replace resources folder in site\n# mv theme folder\nsubprocess.run(\' \'.join([\'cp\', \'/tmp/zoo-doc/css/*\', \'{}/site/css\'.format(dir_name)]), shell=True)\nsubprocess.run(\' \'.join([\'cp\', \'/tmp/zoo-doc/js/*\', \'{}/site/js\'.format(dir_name)]), shell=True)\nsubprocess.run(\' \'.join([\'cp\', \'/tmp/zoo-doc/fonts/*\', \'{}/site/fonts\'.format(dir_name)]), shell=True)\nsubprocess.run(\' \'.join([\'cp\', \'/tmp/zoo-doc/img/*\', \'{}/site/img\'.format(dir_name)]), shell=True)\nsubprocess.run(\' \'.join([\'cp\', \'/tmp/zoo-doc/version-list\', \'{}/site\'.format(dir_name)]), shell=True)\n\nif scaladocs:\n    print(\'build scala doc\')\n    zoo_dir = os.path.dirname(dir_name)\n    os.chdir(zoo_dir)\n    subprocess.run([\'mvn\', \'scala:doc\'])  # build scala doc\n    scaladocs_dir = zoo_dir + \'/zoo/target/site/scaladocs/\'\n    target_dir = dir_name + \'/site/APIGuide/\'\n    if not os.path.exists(target_dir):\n        subprocess.run([\'mkdir\', target_dir])  # mkdir APIGuide\n    # mv scaladocs\n    subprocess.run(\' \'.join([\'cp\', \'-r\', scaladocs_dir, target_dir + \'scaladoc/\']), shell=True)\n\nif pythondocs:\n    print(\'build python\')\n    pyspark_dir = os.path.dirname(dir_name) + \'/pyzoo/docs/\'\n    target_dir = dir_name + \'/site/APIGuide/\'\n    os.chdir(pyspark_dir)\n    subprocess.run([\'./doc-gen.sh\'])  # build python doc\n    pythondocs_dir = pyspark_dir + \'_build/html/\'\n    if not os.path.exists(target_dir):\n        subprocess.run([\'mkdir\', target_dir])  # mkdir APIGuide\n    # mv pythondocs\n    subprocess.run(\' \'.join([\'cp\', \'-r\', pythondocs_dir, target_dir + \'python-api-doc/\']), shell=True)\n\nos.chdir(dir_name)\n\nif args.debugport:\n    print(\'starting mkdoc server in debug mode\')\n    addr = \'--dev-addr=*:\'+str(args.debugport)\n    # mkdocs start serve\n    subprocess.run([\'mkdocs\', \'serve\', addr])\n\nif args.port:\n    os.chdir(dir_name + \'/site\')\n    # start http server\n    subprocess.run([\'python\', \'-m\', \'http.server\', \'{}\'.format(args.port)])\n'"
pyzoo/setup.py,0,"b'#!/usr/bin/env python\n\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport sys\nfrom shutil import copyfile, copytree, rmtree\nimport fnmatch\nfrom setuptools import setup\n\nlong_description = \'\'\'\nAnalytics Zoo: A unified Data Analytics and AI platform for distributed TensorFlow,\n Keras, PyTorch, Apache Spark/Flink and Ray.\n\nYou may want to develop your AI solutions using Analytics Zoo if:\n\n- You want to easily prototype the entire end-to-end pipeline that applies AI models\n (e.g., TensorFlow, Keras, PyTorch, BigDL, OpenVINO, etc.) to production big data.\n- You want to transparently scale your AI applications from a laptop to large clusters with ""zero""\n code changes.\n- You want to deploy your AI pipelines to existing YARN or K8S clusters *WITHOUT* any modifications\n to the clusters.\n- You want to automate the process of applying machine learning (such as feature engineering,\n hyperparameter tuning, model selection and distributed inference).\n\nFind instructions to install analytics-zoo via pip, please visit our documentation page:\n https://analytics-zoo.github.io/master/#PythonUserGuide/install/\n\nFor source code and more information, please visit our GitHub page:\n https://github.com/intel-analytics/analytics-zoo\n\'\'\'\n\n\nTEMP_PATH = ""zoo/share""\nanalytics_zoo_home = os.path.abspath(__file__ + ""/../../"")\nSCRIPTS_TARGET = os.path.join(TEMP_PATH, ""bin/cluster-serving"")\n\nexclude_patterns = [""*__pycache__*"", ""*ipynb_checkpoints*"", ""*zouwu.use-case*""]\n\n\ndef get_analytics_zoo_packages():\n    analytics_zoo_python_home = analytics_zoo_home + ""/pyzoo/zoo""\n    analytics_zoo_packages = [\'zoo.share\']\n    for dirpath, dirs, files in os.walk(analytics_zoo_python_home):\n        package = dirpath.split(analytics_zoo_home +\n                                ""/pyzoo/"")[1].replace(\'/\', \'.\')\n        if any(fnmatch.fnmatchcase(package, pat=pattern)\n                for pattern in exclude_patterns):\n            print(""excluding"", package)\n        else:\n            analytics_zoo_packages.append(package)\n            print(""including"", package)\n    return analytics_zoo_packages\n\n\npackages = get_analytics_zoo_packages()\n\ntry:\n    with open(\'zoo/__init__.py\', \'r\') as f:\n        for line in f.readlines():\n            if \'__version__\' in line:\n                VERSION = line.strip().replace(""\\"""", """").split("" "")[2]\nexcept IOError:\n    print(""Failed to load Analytics Zoo version file for packaging. \\\n      You must be in Analytics Zoo\'s pyzoo dir."")\n    sys.exit(-1)\n\nbuilding_error_msg = """"""\nIf you are packing python API from zoo source, you must build Analytics Zoo first\nand run sdist.\n    To build Analytics Zoo with maven you can run:\n      cd $ANALYTICS_ZOO_HOME\n      ./make-dist.sh\n    Building the source dist is done in the Python directory:\n      cd $ANALYTICS_ZOO_HOME/pyzoo\n      python setup.py sdist\n      pip install dist/*.tar.gz""""""\n\n\ndef build_from_source():\n    code_path = analytics_zoo_home + ""/pyzoo/zoo/common/nncontext.py""\n    print(""Checking: %s to see if build from source"" % code_path)\n    if os.path.exists(code_path):\n        return True\n    return False\n\n\ndef init_env():\n    if build_from_source():\n        print(""Start to build distributed package"")\n        print(""HOME OF ANALYTICS ZOO: "" + analytics_zoo_home)\n        dist_source = analytics_zoo_home + ""/dist""\n        if not os.path.exists(dist_source):\n            print(building_error_msg)\n            sys.exit(-1)\n        if os.path.exists(TEMP_PATH):\n            rmtree(TEMP_PATH)\n        copytree(dist_source, TEMP_PATH)\n        copyfile(\n            analytics_zoo_home +\n            ""/pyzoo/zoo/models/__init__.py"",\n            TEMP_PATH +\n            ""/__init__.py"")\n    else:\n        print(""Do nothing for release installation"")\n\n\ndef setup_package():\n    script_names = os.listdir(SCRIPTS_TARGET)\n    scripts = list(map(lambda script: os.path.join(\n        SCRIPTS_TARGET, script), script_names))\n\n    metadata = dict(\n        name=\'analytics-zoo\',\n        version=VERSION,\n        description=\'A unified Data Analytics and AI platform for distributed TensorFlow, Keras, \'\n                    \'PyTorch, Apache Spark/Flink and Ray\',\n        long_description=long_description,\n        long_description_content_type=""text/markdown"",\n        author=\'Analytics Zoo Authors\',\n        author_email=\'bigdl-user-group@googlegroups.com\',\n        license=\'Apache License, Version 2.0\',\n        url=\'https://github.com/intel-analytics/analytics-zoo\',\n        packages=packages,\n        install_requires=[\'pyspark==2.4.3\', \'bigdl==0.10.0\', \'conda-pack==0.3.1\'],\n        extras_require={\'ray\': [\'ray==0.8.4\', \'psutil\', \'aiohttp\', \'setproctitle\'],\n                        \'automl\': [\'tensorflow>=1.15.0,<2.0.0\', \'ray[tune]==0.8.4\', \'psutil\',\n                                   \'aiohttp\', \'setproctitle\', \'pandas\', \'featuretools\',\n                                   \'scikit-learn\', \'requests\', \'bayesian-optimization\']},\n        dependency_links=[\'https://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz\'],\n        include_package_data=True,\n        package_data={""zoo.share"": [\'lib/analytics-zoo*with-dependencies.jar\', \'conf/*\', \'bin/*\',\n                                    \'extra-resources/*\']},\n        scripts=scripts,\n        classifiers=[\n            \'License :: OSI Approved :: Apache Software License\',\n            \'Programming Language :: Python :: 3\',\n            \'Programming Language :: Python :: 3.5\',\n            \'Programming Language :: Python :: 3.6\',\n            \'Programming Language :: Python :: Implementation :: CPython\'],\n        platforms=[\'mac\', \'linux\']\n    )\n\n    setup(**metadata)\n\n\nif __name__ == \'__main__\':\n    try:\n        init_env()\n        setup_package()\n    except Exception as e:\n        raise e\n    finally:\n        if build_from_source() and os.path.exists(TEMP_PATH):\n            rmtree(TEMP_PATH)\n'"
apps/variational-autoencoder/utils.py,0,"b'import numpy as np\nimport imageio\nfrom PIL import Image\n\n\ndef center_crop(x, crop_h, crop_w=None, resize_w=64):\n    # crop the images to [crop_h,crop_w,3] then resize to [resize_h,resize_w,3]\n    if crop_w is None:\n        crop_w = crop_h # the width and height after cropped\n    h, w = x.shape[:2]\n    j = int(round((h - crop_h)/2.))\n    i = int(round((w - crop_w)/2.))\n    return np.array(Image.fromarray(x[j:j+crop_h, i:i+crop_w].astype(np.uint8)).resize([resize_w, resize_w]))\n\n\ndef transform(image, npx=64, is_crop=True, resize_w=64):\n    if is_crop:\n        cropped_image = center_crop(image, npx, resize_w=resize_w)\n    else:\n        cropped_image = image\n    return np.array(cropped_image)/127.5 - 1.  # change pixel value range from [0,255] to [-1,1] to feed into CNN\n\n\ndef inverse_transform(images):\n    return (images+1.)/2. # change image pixel value(outputs from tanh in range [-1,1]) back to [0,1]\n\n\ndef imread(path, is_grayscale = False):\n    if (is_grayscale):\n        return imageio.imread(path, flatten = True).astype(np.float) # [width,height] flatten RGB image to grayscale image\n    else:\n        return imageio.imread(path).astype(np.float) # [width,height,color_dim]\n\n\ndef get_image(image_path, image_size, is_crop=True, resize_w=64, is_grayscale = False):\n    return transform(imread(image_path, is_grayscale), image_size, is_crop, resize_w)\n\n'"
docker/cluster-serving/freeze_checkpoint.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom zoo.util.tf import export_tf\nfrom optparse import OptionParser\n\n\ndef ckpt_to_frozen_graph(options):\n    with tf.gfile.GFile(options.pbPath, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n        var_list_name = [node.name + "":0""\n                         for node in graph_def.node\n                         if node.op in [""Variable"", ""VariableV2"", ""VarHandleOp""]]\n\n    # now build the graph in the memory and visualize it\n    with tf.Session() as sess:\n        graph = tf.get_default_graph()\n        tf.import_graph_def(graph_def, name="""")\n\n        var_list = [graph.get_tensor_by_name(name) for name in var_list_name]\n\n        for v in var_list:\n            tf.add_to_collection(tf.GraphKeys.TRAINABLE_VARIABLES, v)\n\n        saver = tf.train.Saver(var_list)\n        saver.restore(sess, options.ckptPath)\n\n        input_names = options.inputsName.split("","")\n        output_names = options.outputsName.split("","")\n\n        input_tensors = [graph.get_tensor_by_name(name) for name in input_names]\n        output_tensors = [graph.get_tensor_by_name(name) for name in output_names]\n\n        export_tf(sess, options.outputDir, inputs=input_tensors, outputs=output_tensors)\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--pbPath"", dest=""pbPath"",\n                      help=""The path to a TensorFlow pb file"")\n    parser.add_option(""--ckptPath"", dest=""ckptPath"",\n                      help=""The path to a TensorFlow chekpoint file"")\n    parser.add_option(""--inputsName"", dest=""inputsName"",\n                      help=""A comma separated list of Tensor names as the model inputs, ""\n                           ""e.g. input_0:0,input_1:0."")\n    parser.add_option(""--outputsName"", dest=""outputsName"",\n                      help=""A comma separated list of Tensor names as the model outputs, ""\n                           ""e.g. output_0:0,output_1:0."")\n    parser.add_option(""-o"", ""--outputDir"", dest=""outputDir"", default=""."")\n    import sys\n    (options, args) = parser.parse_args(sys.argv)\n    assert options.pbPath is not None, ""--pbPath must be provided""\n    assert options.ckptPath is not None, ""--ckptPath must be provided""\n    assert options.inputsName is not None, ""--inputsName must be provided""\n    assert options.outputsName is not None, ""--outputsName must be provided""\n    ckpt_to_frozen_graph(options)\n'"
docker/cluster-serving/quick_start.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.serving.client import InputQueue, OutputQueue\nimport os\nimport cv2\nimport json\nimport time\nfrom optparse import OptionParser\n\n\ndef run(path):\n    input_api = InputQueue()\n    base_path = path\n\n    if not base_path:\n        raise EOFError(""You have to set your image path"")\n    output_api = OutputQueue()\n    output_api.dequeue()\n    path = os.listdir(base_path)\n    for p in path:\n        if not p.endswith(""jpeg""):\n            continue\n        img = cv2.imread(os.path.join(base_path, p))\n        img = cv2.resize(img, (224, 224))\n        input_api.enqueue_image(p, img)\n\n    time.sleep(10)\n\n    # get all result and dequeue\n    result = output_api.dequeue()\n    for k in result.keys():\n        output = ""image: "" + k + "", classification-result:""\n        tmp_list = json.loads(result[k])\n        for record in range(len(tmp_list)):\n            output += "" class: "" + str(tmp_list[record][0]) \\\n                      + ""\'s prob: "" + str(tmp_list[record][1])\n        print(output)\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""-i"", ""--image_path"", dest=""path"", default=""test_image"")\n    import sys\n    (options, args) = parser.parse_args(sys.argv)\n    run(options.path)\n'"
docker/hyperzoo/quick_start.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.serving.client import InputQueue, OutputQueue\nimport os\nimport cv2\nimport json\nimport time\n\n\nif __name__ == ""__main__"":\n    input_api = InputQueue()\n\n    base_path = ""test_image""\n\n    if not base_path:\n        raise EOFError(""You have to set your image path"")\n    output_api = OutputQueue()\n    output_api.dequeue()\n    path = os.listdir(base_path)\n    for p in path:\n        if not p.endswith(""jpeg""):\n            continue\n        img = cv2.imread(os.path.join(base_path, p))\n        img = cv2.resize(img, (224, 224))\n        input_api.enqueue_image(p, img)\n\n    time.sleep(5)\n\n    # get all result and dequeue\n    result = output_api.dequeue()\n    for k in result.keys():\n        output = ""image: "" + k + "", classification-result:""\n        tmp_dict = json.loads(result[k])\n        for class_idx in tmp_dict.keys():\n            output += ""class: "" + class_idx + ""\'s prob: "" + tmp_dict[class_idx]\n        print(output)\n'"
pyzoo/dev/auto_gen.py,0,"b'#!/usr/bin/python\n\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# NB: This is just a helper script and might be removed shortly.\nimport re\n\n\ndef get_class_name(src_code):\n    import re\n    match = re.search(r""object(.*)\\s*\\{"", src_code)\n    return match.group(1).strip()\n\n\ndef get_parameters(src_code):\n    match = re.search(r""def apply.*ClassTag\\]\\s*\\((.*)\\)\\s*\\(implicit"", src_code, re.DOTALL)\n    params = [p.strip() for p in match.group(1).strip().split(""\\n"") if len(p.strip()) > 0]\n    names = []\n    values = []\n    types = []\n    for p in params:\n        if ""="" in p:  # with default value\n            match = re.search(r""(.*)\\:(.*)=([^\\,]*)"", p)\n            param_value = match.group(3).strip()\n        else:\n            match = re.search(r""(.*)\\:([^\\,]*)"", p)\n            param_value = None\n        param_name = match.group(1).strip()\n        param_type = match.group(2).strip()\n        names.append(param_name)\n        values.append(param_value)\n        types.append(param_type)\n    return zip(names, values, types)\n\n\n# wRegularizer\ndef to_py_name(scala_name):\n    name_mapping = {""w_regularizer"": ""W_regularizer""}\n    result = []\n    previous_is_lower = False\n    for c in scala_name:\n        if c.isupper() and previous_is_lower:\n            result.append(""_"")\n            previous_is_lower = False\n        else:\n            previous_is_lower = True\n        result.append(c.lower())\n    tmp_result = """".join(result)\n    return name_mapping.get(tmp_result, tmp_result)\n\n\n# print(to_py_name(""wRegularizer""))\n\ndef to_py_value(scala_value):\n    name_mapping = {""true"": ""True"", ""false"": ""False"", ""null"": ""None"",\n                    ""RandomUniform"": ""\\""glorot_uniform\\""""}\n    return name_mapping.get(scala_value, scala_value)\n\n\n#       wRegularizer: Regularizer[T] = null,\ndef to_py_param(scala_param):\n    param_name, param_value, param_type = scala_param\n    if param_value:\n        return to_py_name(param_name) + ""="" + to_py_value(param_value)\n    else:\n        return to_py_name(param_name)\n\n\ndef to_py_params(scala_params):\n    result = []\n    for param in scala_params:\n        result.append(to_py_param(param))\n    return result\n\n\ndef append_semi(result_list):\n    result = []\n    for index, r in enumerate(result_list):\n        if (index < len(result_list) - 1):\n            result.append(r + "","")\n        else:\n            result.append(r)\n    return ""\\n"".join(result)\n\n\ndef format_py_params(py_params):\n    return append_semi([8 * "" "" + param for param in py_params])\n\n\ndef format_py_params_for_value(py_params):\n    mapping = {""init"": ""to_bigdl_init(init)"",\n               ""activation"": ""get_activation_by_name(activation) if activation else None"",\n               ""W_regularizer"": ""to_bigdl_reg(W_regularizer)"",\n               ""b_regularizer"": ""to_bigdl_reg(b_regularizer)"",\n               ""input_shape"": ""list(input_shape) if input_shape else None""}\n    py_param_names = [p.split(""="")[0].strip() for p in py_params]\n    return append_semi([12 * "" "" + mapping.get(name, name) for name in py_param_names])\n\n\ndef to_py_constructor(scala_src):\n    class_name = get_class_name(scala_src)\n    scala_params = get_parameters(scala_src)\n    py_params = to_py_params(scala_params)\n\n    print("""")\n    doc_test = ""  ""\n    init_content = """"""\n        super(%s, self).__init__(None, bigdl_type, \\n%s)\n    """""" % (class_name, format_py_params_for_value(py_params))\n    result = []\n    result.append(""class %s(Layer):"" % class_name)\n    result.append(4 * "" "" + ""\\\'\'\'%s\\\'\'\'"" % doc_test)\n    result.append(\n        4 * "" "" + """"""def __init__(self, \\n%s,bigdl_type=""float""):"""""" % format_py_params(py_params))\n    result.append(init_content)\n    return ""\\n"".join(result)\n\n\ndef to_java_creator(scala_src):\n    class_name = get_class_name(scala_src)\n    scala_params = get_parameters(scala_src)\n\n    def format_creator_param_list(scala_params):\n        mapping = {""inputShape"": 8 * "" "" + ""inputShape: JList[Int] = null""}\n        result = []\n        for name, value, t in scala_params:\n            if name in mapping:\n                result.append(mapping[name])\n            else:\n                result.append(8 * "" "" + """"""%s: %s%s"""""" % (name, t, "" = "" + value if value else """"))\n        return append_semi(result)\n\n    def format_init_list(scala_params):\n        mapping = {""inputShape"": """"""toScalaShape(inputShape)""""""}\n        result = []\n        for name, value, t in scala_params:\n            if name in mapping:\n                result.append(mapping[name])\n            else:\n                result.append(name)\n        return append_semi([12 * "" "" + i for i in result])\n\n    result = []\n    formated_creator_param_list = format_creator_param_list(scala_params)\n    formated_init_list = format_init_list(scala_params)\n    result.append(""""""\n    def create%s(\\n%s): %s[T] = {\n    %s(\\n%s)\n    }\n    """""" % (class_name, formated_creator_param_list, class_name, 4 * "" "" + class_name,\n           formated_init_list))\n    return ""\\n"".join(result)\n\n\nif __name__ == ""__main__"":\n    from optparse import OptionParser\n    import sys\n    import os\n\n    cur_path = os.path.dirname(os.path.realpath(__file__))\n\n    keras_dir = os.path.join(\n        cur_path,\n        ""../../zoo/src/main/scala/com/intel/analytics/zoo/pipeline/api/keras2/layers/"")\n    parser = OptionParser()\n    parser.add_option(""-l"", ""--layer"", dest=""layer"", default=""Dense"")\n    (options, args) = parser.parse_args(sys.argv)\n    src_path = os.path.join(keras_dir, options.layer + "".scala"")\n    src_code = ""\\n"".join(open(src_path).readlines())\n    # print(dense)\n    print(""--------->Python code<-----------------"")\n    print(to_py_constructor(src_code))\n    print(""--------->Java code<-----------------"")\n    print(to_java_creator(src_code))\n'"
pyzoo/dev/diff.py,0,"b'#!/usr/bin/env python\n\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport re\nfrom os.path import isfile, join\n\nscala_layers_dirs = [""./zoo/src/main/scala/com/intel/analytics/zoo/pipeline/api/keras/layers"",\n                     ""./zoo/src/main/scala/com/intel/analytics/zoo/pipeline/api/keras/models"",\n                     ""./zoo/src/main/scala/com/intel/analytics/zoo/pipeline/api/autograd""]\npython_layers_dirs = [""./pyzoo/zoo/pipeline/api/keras/layers"",\n                      ""./pyzoo/zoo/pipeline/api/keras/engine"",\n                      ""./pyzoo/zoo/pipeline/api/keras/"",\n                      ""./pyzoo/zoo/pipeline/api/""]\n\nscala_to_python = {""CustomLossWithVariable"": ""CustomLoss""}\nscala_class_to_path = {}\n\n\ndef extract_scala_class(class_path):\n    exclude_key_words = {""KerasLayerWrapper"", ""LambdaTorch"", ""CustomLossWithFunc"", ""abstract"",\n                         ""InternalRecurrent"", ""InternalTimeDistributed"", ""InternalMM"", ""Recurrent"",\n                         ""InternalLocalOptimizer"", ""InternalDistriOptimizer"",\n                         ""EmbeddingMatrixHolder"", ""InternalParameter"", ""KerasParameter"",\n                         ""InternalCAddTable"", ""InternalGetShape"",\n                         ""EmbeddingMatrixHolder"", ""Pooling2D"", ""InternalSplitTensor"",\n                         ""SplitTensor"", ""Expand"", ""InternalMax"", ""InternalConvLSTM3D"",\n                         ""InternalConvLSTM2D"", ""InternalCMulTable"", ""SoftMax"",\n                         ""KerasConstant"", ""InternalConstant"", ""InternalERF"", ""InternalSoftMax"",\n                         ""InternalLayerNorm"", ""LayerNorm""}\n    content = ""\\n"".join([line for line in open(class_path).readlines()\n                         if all([key not in line for key in exclude_key_words])])\n    match = re.findall(r""class ([\\w]+)[^{]+"", content)\n    return match\n\n\ndef get_all_scala_layers(scala_dirs):\n    results = set()\n    raw_result = []\n    for scala_dir in scala_dirs:\n        for name in os.listdir(scala_dir):\n            if isfile(join(scala_dir, name)):\n                res = extract_scala_class(join(scala_dir, name))\n                raw_result += res\n                for item in res:\n                    scala_class_to_path[item] = join(scala_dir, name)\n        results.update(set(class_name for class_name in raw_result if class_name is not None))\n    return results\n\n\ndef get_python_classes(python_dirs):\n    exclude_classes = {""InputLayer"", ""ZooKerasLayer"", ""ZooKerasCreator"",\n                       ""KerasNet"", ""Net""}\n    raw_classes = []\n    results = []\n    for python_dir in python_dirs:\n        python_files = [join(python_dir, name) for name in os.listdir(python_dir)\n                        if isfile(join(python_dir, name)) and name.endswith(\'py\')\n                        and ""__"" not in name]\n        results += python_files\n    for p in results:\n        with open(p) as f:\n            raw_classes.extend([line for line in f.readlines() if line.startswith(""class"")])\n    classes = [name.split()[1].split(""("")[0]for name in raw_classes]\n    return set([name for name in classes if name not in exclude_classes])\n\n\nscala_layers = get_all_scala_layers(scala_layers_dirs)\npython_layers = get_python_classes(python_layers_dirs)\n\n# print(""Layers in Scala: {0}, {1}"".format(len(scala_layers), scala_layers))\n# print("""")\n# print(""Layers in Python: {0}, {1}"".format(len(python_layers), python_layers))\n\nprint(""Layers in Scala but not in Python: ""),\ndiff_count = 0\nfor name in scala_layers:\n    if name not in python_layers:\n        if name not in scala_to_python or \\\n                (name in scala_to_python and scala_to_python[name] not in python_layers):\n            print(""{} : {}"".format(name, scala_class_to_path[name]))\n            diff_count += 1\n\nif diff_count > 0:\n    raise Exception(""There exist layers in Scala but not wrapped in Python"")\n'"
pyzoo/dev/pep8-1.7.0.py,0,"b'#!/usr/bin/env python\n# pep8.py - Check Python source code formatting, according to PEP 8\n# Copyright (C) 2006-2009 Johann C. Rocholl <johann@rocholl.net>\n# Copyright (C) 2009-2014 Florent Xicluna <florent.xicluna@gmail.com>\n# Copyright (C) 2014-2016 Ian Lee <ianlee1521@gmail.com>\n#\n# Permission is hereby granted, free of charge, to any person\n# obtaining a copy of this software and associated documentation files\n# (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge,\n# publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nr""""""\nCheck Python source code formatting, according to PEP 8.\n\nFor usage and a list of options, try this:\n$ python pep8.py -h\n\nThis program and its regression test suite live here:\nhttps://github.com/pycqa/pep8\n\nGroups of errors and warnings:\nE errors\nW warnings\n100 indentation\n200 whitespace\n300 blank lines\n400 imports\n500 line length\n600 deprecation\n700 statements\n900 syntax error\n""""""\nfrom __future__ import with_statement\n\nimport os\nimport sys\nimport re\nimport time\nimport inspect\nimport keyword\nimport tokenize\nfrom optparse import OptionParser\nfrom fnmatch import fnmatch\ntry:\n    from configparser import RawConfigParser\n    from io import TextIOWrapper\nexcept ImportError:\n    from ConfigParser import RawConfigParser\n\n__version__ = \'1.7.0\'\n\nDEFAULT_EXCLUDE = \'.svn,CVS,.bzr,.hg,.git,__pycache__,.tox\'\nDEFAULT_IGNORE = \'E121,E123,E126,E226,E24,E704\'\ntry:\n    if sys.platform == \'win32\':\n        USER_CONFIG = os.path.expanduser(r\'~\\.pep8\')\n    else:\n        USER_CONFIG = os.path.join(\n            os.getenv(\'XDG_CONFIG_HOME\') or os.path.expanduser(\'~/.config\'),\n            \'pep8\'\n        )\nexcept ImportError:\n    USER_CONFIG = None\n\nPROJECT_CONFIG = (\'setup.cfg\', \'tox.ini\', \'.pep8\')\nTESTSUITE_PATH = os.path.join(os.path.dirname(__file__), \'testsuite\')\nMAX_LINE_LENGTH = 100\nREPORT_FORMAT = {\n    \'default\': \'%(path)s:%(row)d:%(col)d: %(code)s %(text)s\',\n    \'pylint\': \'%(path)s:%(row)d: [%(code)s] %(text)s\',\n}\n\nPyCF_ONLY_AST = 1024\nSINGLETONS = frozenset([\'False\', \'None\', \'True\'])\nKEYWORDS = frozenset(keyword.kwlist + [\'print\']) - SINGLETONS\nUNARY_OPERATORS = frozenset([\'>>\', \'**\', \'*\', \'+\', \'-\'])\nARITHMETIC_OP = frozenset([\'**\', \'*\', \'/\', \'//\', \'+\', \'-\'])\nWS_OPTIONAL_OPERATORS = ARITHMETIC_OP.union([\'^\', \'&\', \'|\', \'<<\', \'>>\', \'%\'])\nWS_NEEDED_OPERATORS = frozenset([\n    \'**=\', \'*=\', \'/=\', \'//=\', \'+=\', \'-=\', \'!=\', \'<>\', \'<\', \'>\',\n    \'%=\', \'^=\', \'&=\', \'|=\', \'==\', \'<=\', \'>=\', \'<<=\', \'>>=\', \'=\'])\nWHITESPACE = frozenset(\' \\t\')\nNEWLINE = frozenset([tokenize.NL, tokenize.NEWLINE])\nSKIP_TOKENS = NEWLINE.union([tokenize.INDENT, tokenize.DEDENT])\n# ERRORTOKEN is triggered by backticks in Python 3\nSKIP_COMMENTS = SKIP_TOKENS.union([tokenize.COMMENT, tokenize.ERRORTOKEN])\nBENCHMARK_KEYS = [\'directories\', \'files\', \'logical lines\', \'physical lines\']\n\nINDENT_REGEX = re.compile(r\'([ \\t]*)\')\nRAISE_COMMA_REGEX = re.compile(r\'raise\\s+\\w+\\s*,\')\nRERAISE_COMMA_REGEX = re.compile(r\'raise\\s+\\w+\\s*,.*,\\s*\\w+\\s*$\')\nERRORCODE_REGEX = re.compile(r\'\\b[A-Z]\\d{3}\\b\')\nDOCSTRING_REGEX = re.compile(r\'u?r?[""\\\']\')\nEXTRANEOUS_WHITESPACE_REGEX = re.compile(r\'[[({] | []}),;:]\')\nWHITESPACE_AFTER_COMMA_REGEX = re.compile(r\'[,;:]\\s*(?:  |\\t)\')\nCOMPARE_SINGLETON_REGEX = re.compile(r\'(\\bNone|\\bFalse|\\bTrue)?\\s*([=!]=)\'\n                                     r\'\\s*(?(1)|(None|False|True))\\b\')\nCOMPARE_NEGATIVE_REGEX = re.compile(r\'\\b(not)\\s+[^][)(}{ ]+\\s+(in|is)\\s\')\nCOMPARE_TYPE_REGEX = re.compile(r\'(?:[=!]=|is(?:\\s+not)?)\\s*type(?:s.\\w+Type\'\n                                r\'|\\s*\\(\\s*([^)]*[^ )])\\s*\\))\')\nKEYWORD_REGEX = re.compile(r\'(\\s*)\\b(?:%s)\\b(\\s*)\' % r\'|\'.join(KEYWORDS))\nOPERATOR_REGEX = re.compile(r\'(?:[^,\\s])(\\s*)(?:[-+*/|!<=>%&^]+)(\\s*)\')\nLAMBDA_REGEX = re.compile(r\'\\blambda\\b\')\nHUNK_REGEX = re.compile(r\'^@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@.*$\')\n\n# Work around Python < 2.6 behaviour, which does not generate NL after\n# a comment which is on a line by itself.\nCOMMENT_WITH_NL = tokenize.generate_tokens([\'#\\n\'].pop).send(None)[1] == \'#\\n\'\n\n\n##############################################################################\n# Plugins (check functions) for physical lines\n##############################################################################\n\n\ndef tabs_or_spaces(physical_line, indent_char):\n    r""""""Never mix tabs and spaces.\n\n    The most popular way of indenting Python is with spaces only.  The\n    second-most popular way is with tabs only.  Code indented with a mixture\n    of tabs and spaces should be converted to using spaces exclusively.  When\n    invoking the Python command line interpreter with the -t option, it issues\n    warnings about code that illegally mixes tabs and spaces.  When using -tt\n    these warnings become errors.  These options are highly recommended!\n\n    Okay: if a == 0:\\n        a = 1\\n        b = 1\n    E101: if a == 0:\\n        a = 1\\n\\tb = 1\n    """"""\n    indent = INDENT_REGEX.match(physical_line).group(1)\n    for offset, char in enumerate(indent):\n        if char != indent_char:\n            return offset, ""E101 indentation contains mixed spaces and tabs""\n\n\ndef tabs_obsolete(physical_line):\n    r""""""For new projects, spaces-only are strongly recommended over tabs.\n\n    Okay: if True:\\n    return\n    W191: if True:\\n\\treturn\n    """"""\n    indent = INDENT_REGEX.match(physical_line).group(1)\n    if \'\\t\' in indent:\n        return indent.index(\'\\t\'), ""W191 indentation contains tabs""\n\n\ndef trailing_whitespace(physical_line):\n    r""""""Trailing whitespace is superfluous.\n\n    The warning returned varies on whether the line itself is blank, for easier\n    filtering for those who want to indent their blank lines.\n\n    Okay: spam(1)\\n#\n    W291: spam(1) \\n#\n    W293: class Foo(object):\\n    \\n    bang = 12\n    """"""\n    physical_line = physical_line.rstrip(\'\\n\')    # chr(10), newline\n    physical_line = physical_line.rstrip(\'\\r\')    # chr(13), carriage return\n    physical_line = physical_line.rstrip(\'\\x0c\')  # chr(12), form feed, ^L\n    stripped = physical_line.rstrip(\' \\t\\v\')\n    if physical_line != stripped:\n        if stripped:\n            return len(stripped), ""W291 trailing whitespace""\n        else:\n            return 0, ""W293 blank line contains whitespace""\n\n\ndef trailing_blank_lines(physical_line, lines, line_number, total_lines):\n    r""""""Trailing blank lines are superfluous.\n\n    Okay: spam(1)\n    W391: spam(1)\\n\n\n    However the last line should end with a new line (warning W292).\n    """"""\n    if line_number == total_lines:\n        stripped_last_line = physical_line.rstrip()\n        if not stripped_last_line:\n            return 0, ""W391 blank line at end of file""\n        if stripped_last_line == physical_line:\n            return len(physical_line), ""W292 no newline at end of file""\n\n\ndef maximum_line_length(physical_line, max_line_length, multiline):\n    r""""""Limit all lines to a maximum of 79 characters.\n\n    There are still many devices around that are limited to 80 character\n    lines; plus, limiting windows to 80 characters makes it possible to have\n    several windows side-by-side.  The default wrapping on such devices looks\n    ugly.  Therefore, please limit all lines to a maximum of 79 characters.\n    For flowing long blocks of text (docstrings or comments), limiting the\n    length to 72 characters is recommended.\n\n    Reports error E501.\n    """"""\n    line = physical_line.rstrip()\n    length = len(line)\n    if length > max_line_length and not noqa(line):\n        # Special case for long URLs in multi-line docstrings or comments,\n        # but still report the error when the 72 first chars are whitespaces.\n        chunks = line.split()\n        if ((len(chunks) == 1 and multiline) or\n            (len(chunks) == 2 and chunks[0] == \'#\')) and \\\n                len(line) - len(chunks[-1]) < max_line_length - 7:\n            return\n        if hasattr(line, \'decode\'):   # Python 2\n            # The line could contain multi-byte characters\n            try:\n                length = len(line.decode(\'utf-8\'))\n            except UnicodeError:\n                pass\n        if length > max_line_length:\n            return (max_line_length, ""E501 line too long ""\n                    ""(%d > %d characters)"" % (length, max_line_length))\n\n\n##############################################################################\n# Plugins (check functions) for logical lines\n##############################################################################\n\n\ndef blank_lines(logical_line, blank_lines, indent_level, line_number,\n                blank_before, previous_logical, previous_indent_level):\n    r""""""Separate top-level function and class definitions with two blank lines.\n\n    Method definitions inside a class are separated by a single blank line.\n\n    Extra blank lines may be used (sparingly) to separate groups of related\n    functions.  Blank lines may be omitted between a bunch of related\n    one-liners (e.g. a set of dummy implementations).\n\n    Use blank lines in functions, sparingly, to indicate logical sections.\n\n    Okay: def a():\\n    pass\\n\\n\\ndef b():\\n    pass\n    Okay: def a():\\n    pass\\n\\n\\n# Foo\\n# Bar\\n\\ndef b():\\n    pass\n\n    E301: class Foo:\\n    b = 0\\n    def bar():\\n        pass\n    E302: def a():\\n    pass\\n\\ndef b(n):\\n    pass\n    E303: def a():\\n    pass\\n\\n\\n\\ndef b(n):\\n    pass\n    E303: def a():\\n\\n\\n\\n    pass\n    E304: @decorator\\n\\ndef a():\\n    pass\n    """"""\n    if line_number < 3 and not previous_logical:\n        return  # Don\'t expect blank lines before the first line\n    if previous_logical.startswith(\'@\'):\n        if blank_lines:\n            yield 0, ""E304 blank lines found after function decorator""\n    elif blank_lines > 2 or (indent_level and blank_lines == 2):\n        yield 0, ""E303 too many blank lines (%d)"" % blank_lines\n    elif logical_line.startswith((\'def \', \'class \', \'@\')):\n        if indent_level:\n            if not (blank_before or previous_indent_level < indent_level or\n                    DOCSTRING_REGEX.match(previous_logical)):\n                yield 0, ""E301 expected 1 blank line, found 0""\n        elif blank_before != 2:\n            yield 0, ""E302 expected 2 blank lines, found %d"" % blank_before\n\n\ndef extraneous_whitespace(logical_line):\n    r""""""Avoid extraneous whitespace.\n\n    Avoid extraneous whitespace in these situations:\n    - Immediately inside parentheses, brackets or braces.\n    - Immediately before a comma, semicolon, or colon.\n\n    Okay: spam(ham[1], {eggs: 2})\n    E201: spam( ham[1], {eggs: 2})\n    E201: spam(ham[ 1], {eggs: 2})\n    E201: spam(ham[1], { eggs: 2})\n    E202: spam(ham[1], {eggs: 2} )\n    E202: spam(ham[1 ], {eggs: 2})\n    E202: spam(ham[1], {eggs: 2 })\n\n    E203: if x == 4: print x, y; x, y = y , x\n    E203: if x == 4: print x, y ; x, y = y, x\n    E203: if x == 4 : print x, y; x, y = y, x\n    """"""\n    line = logical_line\n    for match in EXTRANEOUS_WHITESPACE_REGEX.finditer(line):\n        text = match.group()\n        char = text.strip()\n        found = match.start()\n        if text == char + \' \':\n            # assert char in \'([{\'\n            yield found + 1, ""E201 whitespace after \'%s\'"" % char\n        elif line[found - 1] != \',\':\n            code = (\'E202\' if char in \'}])\' else \'E203\')  # if char in \',;:\'\n            yield found, ""%s whitespace before \'%s\'"" % (code, char)\n\n\ndef whitespace_around_keywords(logical_line):\n    r""""""Avoid extraneous whitespace around keywords.\n\n    Okay: True and False\n    E271: True and  False\n    E272: True  and False\n    E273: True and\\tFalse\n    E274: True\\tand False\n    """"""\n    for match in KEYWORD_REGEX.finditer(logical_line):\n        before, after = match.groups()\n\n        if \'\\t\' in before:\n            yield match.start(1), ""E274 tab before keyword""\n        elif len(before) > 1:\n            yield match.start(1), ""E272 multiple spaces before keyword""\n\n        if \'\\t\' in after:\n            yield match.start(2), ""E273 tab after keyword""\n        elif len(after) > 1:\n            yield match.start(2), ""E271 multiple spaces after keyword""\n\n\ndef missing_whitespace(logical_line):\n    r""""""Each comma, semicolon or colon should be followed by whitespace.\n\n    Okay: [a, b]\n    Okay: (3,)\n    Okay: a[1:4]\n    Okay: a[:4]\n    Okay: a[1:]\n    Okay: a[1:4:2]\n    E231: [\'a\',\'b\']\n    E231: foo(bar,baz)\n    E231: [{\'a\':\'b\'}]\n    """"""\n    line = logical_line\n    for index in range(len(line) - 1):\n        char = line[index]\n        if char in \',;:\' and line[index + 1] not in WHITESPACE:\n            before = line[:index]\n            if char == \':\' and before.count(\'[\') > before.count(\']\') and \\\n                    before.rfind(\'{\') < before.rfind(\'[\'):\n                continue  # Slice syntax, no space required\n            if char == \',\' and line[index + 1] == \')\':\n                continue  # Allow tuple with only one element: (3,)\n            yield index, ""E231 missing whitespace after \'%s\'"" % char\n\n\ndef indentation(logical_line, previous_logical, indent_char,\n                indent_level, previous_indent_level):\n    r""""""Use 4 spaces per indentation level.\n\n    For really old code that you don\'t want to mess up, you can continue to\n    use 8-space tabs.\n\n    Okay: a = 1\n    Okay: if a == 0:\\n    a = 1\n    E111:   a = 1\n    E114:   # a = 1\n\n    Okay: for item in items:\\n    pass\n    E112: for item in items:\\npass\n    E115: for item in items:\\n# Hi\\n    pass\n\n    Okay: a = 1\\nb = 2\n    E113: a = 1\\n    b = 2\n    E116: a = 1\\n    # b = 2\n    """"""\n    c = 0 if logical_line else 3\n    tmpl = ""E11%d %s"" if logical_line else ""E11%d %s (comment)""\n    if indent_level % 4:\n        yield 0, tmpl % (1 + c, ""indentation is not a multiple of four"")\n    indent_expect = previous_logical.endswith(\':\')\n    if indent_expect and indent_level <= previous_indent_level:\n        yield 0, tmpl % (2 + c, ""expected an indented block"")\n    elif not indent_expect and indent_level > previous_indent_level:\n        yield 0, tmpl % (3 + c, ""unexpected indentation"")\n\n\ndef continued_indentation(logical_line, tokens, indent_level, hang_closing,\n                          indent_char, noqa, verbose):\n    r""""""Continuation lines indentation.\n\n    Continuation lines should align wrapped elements either vertically\n    using Python\'s implicit line joining inside parentheses, brackets\n    and braces, or using a hanging indent.\n\n    When using a hanging indent these considerations should be applied:\n    - there should be no arguments on the first line, and\n    - further indentation should be used to clearly distinguish itself as a\n      continuation line.\n\n    Okay: a = (\\n)\n    E123: a = (\\n    )\n\n    Okay: a = (\\n    42)\n    E121: a = (\\n   42)\n    E122: a = (\\n42)\n    E123: a = (\\n    42\\n    )\n    E124: a = (24,\\n     42\\n)\n    E125: if (\\n    b):\\n    pass\n    E126: a = (\\n        42)\n    E127: a = (24,\\n      42)\n    E128: a = (24,\\n    42)\n    E129: if (a or\\n    b):\\n    pass\n    E131: a = (\\n    42\\n 24)\n    """"""\n    first_row = tokens[0][2][0]\n    nrows = 1 + tokens[-1][2][0] - first_row\n    if noqa or nrows == 1:\n        return\n\n    # indent_next tells us whether the next block is indented; assuming\n    # that it is indented by 4 spaces, then we should not allow 4-space\n    # indents on the final continuation line; in turn, some other\n    # indents are allowed to have an extra 4 spaces.\n    indent_next = logical_line.endswith(\':\')\n\n    row = depth = 0\n    valid_hangs = (4,) if indent_char != \'\\t\' else (4, 8)\n    # remember how many brackets were opened on each line\n    parens = [0] * nrows\n    # relative indents of physical lines\n    rel_indent = [0] * nrows\n    # for each depth, collect a list of opening rows\n    open_rows = [[0]]\n    # for each depth, memorize the hanging indentation\n    hangs = [None]\n    # visual indents\n    indent_chances = {}\n    last_indent = tokens[0][2]\n    visual_indent = None\n    last_token_multiline = False\n    # for each depth, memorize the visual indent column\n    indent = [last_indent[1]]\n    if verbose >= 3:\n        print("">>> "" + tokens[0][4].rstrip())\n\n    for token_type, text, start, end, line in tokens:\n\n        newline = row < start[0] - first_row\n        if newline:\n            row = start[0] - first_row\n            newline = not last_token_multiline and token_type not in NEWLINE\n\n        if newline:\n            # this is the beginning of a continuation line.\n            last_indent = start\n            if verbose >= 3:\n                print(""... "" + line.rstrip())\n\n            # record the initial indent.\n            rel_indent[row] = expand_indent(line) - indent_level\n\n            # identify closing bracket\n            close_bracket = (token_type == tokenize.OP and text in \']})\')\n\n            # is the indent relative to an opening bracket line?\n            for open_row in reversed(open_rows[depth]):\n                hang = rel_indent[row] - rel_indent[open_row]\n                hanging_indent = hang in valid_hangs\n                if hanging_indent:\n                    break\n            if hangs[depth]:\n                hanging_indent = (hang == hangs[depth])\n            # is there any chance of visual indent?\n            visual_indent = (not close_bracket and hang > 0 and\n                             indent_chances.get(start[1]))\n\n            if close_bracket and indent[depth]:\n                # closing bracket for visual indent\n                if start[1] != indent[depth]:\n                    yield (start, ""E124 closing bracket does not match ""\n                           ""visual indentation"")\n            elif close_bracket and not hang:\n                # closing bracket matches indentation of opening bracket\'s line\n                if hang_closing:\n                    yield start, ""E133 closing bracket is missing indentation""\n            elif indent[depth] and start[1] < indent[depth]:\n                if visual_indent is not True:\n                    # visual indent is broken\n                    yield (start, ""E128 continuation line ""\n                           ""under-indented for visual indent"")\n            elif hanging_indent or (indent_next and rel_indent[row] == 8):\n                # hanging indent is verified\n                if close_bracket and not hang_closing:\n                    yield (start, ""E123 closing bracket does not match ""\n                           ""indentation of opening bracket\'s line"")\n                hangs[depth] = hang\n            elif visual_indent is True:\n                # visual indent is verified\n                indent[depth] = start[1]\n            elif visual_indent in (text, str):\n                # ignore token lined up with matching one from a previous line\n                pass\n            else:\n                # indent is broken\n                if hang <= 0:\n                    error = ""E122"", ""missing indentation or outdented""\n                elif indent[depth]:\n                    error = ""E127"", ""over-indented for visual indent""\n                elif not close_bracket and hangs[depth]:\n                    error = ""E131"", ""unaligned for hanging indent""\n                else:\n                    hangs[depth] = hang\n                    if hang > 4:\n                        error = ""E126"", ""over-indented for hanging indent""\n                    else:\n                        error = ""E121"", ""under-indented for hanging indent""\n                yield start, ""%s continuation line %s"" % error\n\n        # look for visual indenting\n        if (parens[row] and\n                token_type not in (tokenize.NL, tokenize.COMMENT) and\n                not indent[depth]):\n            indent[depth] = start[1]\n            indent_chances[start[1]] = True\n            if verbose >= 4:\n                print(""bracket depth %s indent to %s"" % (depth, start[1]))\n        # deal with implicit string concatenation\n        elif (token_type in (tokenize.STRING, tokenize.COMMENT) or\n              text in (\'u\', \'ur\', \'b\', \'br\')):\n            indent_chances[start[1]] = str\n        # special case for the ""if"" statement because len(""if ("") == 4\n        elif not indent_chances and not row and not depth and text == \'if\':\n            indent_chances[end[1] + 1] = True\n        elif text == \':\' and line[end[1]:].isspace():\n            open_rows[depth].append(row)\n\n        # keep track of bracket depth\n        if token_type == tokenize.OP:\n            if text in \'([{\':\n                depth += 1\n                indent.append(0)\n                hangs.append(None)\n                if len(open_rows) == depth:\n                    open_rows.append([])\n                open_rows[depth].append(row)\n                parens[row] += 1\n                if verbose >= 4:\n                    print(""bracket depth %s seen, col %s, visual min = %s"" %\n                          (depth, start[1], indent[depth]))\n            elif text in \')]}\' and depth > 0:\n                # parent indents should not be more than this one\n                prev_indent = indent.pop() or last_indent[1]\n                hangs.pop()\n                for d in range(depth):\n                    if indent[d] > prev_indent:\n                        indent[d] = 0\n                for ind in list(indent_chances):\n                    if ind >= prev_indent:\n                        del indent_chances[ind]\n                del open_rows[depth + 1:]\n                depth -= 1\n                if depth:\n                    indent_chances[indent[depth]] = True\n                for idx in range(row, -1, -1):\n                    if parens[idx]:\n                        parens[idx] -= 1\n                        break\n            assert len(indent) == depth + 1\n            if start[1] not in indent_chances:\n                # allow to line up tokens\n                indent_chances[start[1]] = text\n\n        last_token_multiline = (start[0] != end[0])\n        if last_token_multiline:\n            rel_indent[end[0] - first_row] = rel_indent[row]\n\n    if indent_next and expand_indent(line) == indent_level + 4:\n        pos = (start[0], indent[0] + 4)\n        if visual_indent:\n            code = ""E129 visually indented line""\n        else:\n            code = ""E125 continuation line""\n        yield pos, ""%s with same indent as next logical line"" % code\n\n\ndef whitespace_before_parameters(logical_line, tokens):\n    r""""""Avoid extraneous whitespace.\n\n    Avoid extraneous whitespace in the following situations:\n    - before the open parenthesis that starts the argument list of a\n      function call.\n    - before the open parenthesis that starts an indexing or slicing.\n\n    Okay: spam(1)\n    E211: spam (1)\n\n    Okay: dict[\'key\'] = list[index]\n    E211: dict [\'key\'] = list[index]\n    E211: dict[\'key\'] = list [index]\n    """"""\n    prev_type, prev_text, __, prev_end, __ = tokens[0]\n    for index in range(1, len(tokens)):\n        token_type, text, start, end, __ = tokens[index]\n        if (token_type == tokenize.OP and\n            text in \'([\' and\n            start != prev_end and\n            (prev_type == tokenize.NAME or prev_text in \'}])\') and\n            # Syntax ""class A (B):"" is allowed, but avoid it\n            (index < 2 or tokens[index - 2][1] != \'class\') and\n                # Allow ""return (a.foo for a in range(5))""\n                not keyword.iskeyword(prev_text)):\n            yield prev_end, ""E211 whitespace before \'%s\'"" % text\n        prev_type = token_type\n        prev_text = text\n        prev_end = end\n\n\ndef whitespace_around_operator(logical_line):\n    r""""""Avoid extraneous whitespace around an operator.\n\n    Okay: a = 12 + 3\n    E221: a = 4  + 5\n    E222: a = 4 +  5\n    E223: a = 4\\t+ 5\n    E224: a = 4 +\\t5\n    """"""\n    for match in OPERATOR_REGEX.finditer(logical_line):\n        before, after = match.groups()\n\n        if \'\\t\' in before:\n            yield match.start(1), ""E223 tab before operator""\n        elif len(before) > 1:\n            yield match.start(1), ""E221 multiple spaces before operator""\n\n        if \'\\t\' in after:\n            yield match.start(2), ""E224 tab after operator""\n        elif len(after) > 1:\n            yield match.start(2), ""E222 multiple spaces after operator""\n\n\ndef missing_whitespace_around_operator(logical_line, tokens):\n    r""""""Surround operators with a single space on either side.\n\n    - Always surround these binary operators with a single space on\n      either side: assignment (=), augmented assignment (+=, -= etc.),\n      comparisons (==, <, >, !=, <=, >=, in, not in, is, is not),\n      Booleans (and, or, not).\n\n    - If operators with different priorities are used, consider adding\n      whitespace around the operators with the lowest priorities.\n\n    Okay: i = i + 1\n    Okay: submitted += 1\n    Okay: x = x * 2 - 1\n    Okay: hypot2 = x * x + y * y\n    Okay: c = (a + b) * (a - b)\n    Okay: foo(bar, key=\'word\', *args, **kwargs)\n    Okay: alpha[:-i]\n\n    E225: i=i+1\n    E225: submitted +=1\n    E225: x = x /2 - 1\n    E225: z = x **y\n    E226: c = (a+b) * (a-b)\n    E226: hypot2 = x*x + y*y\n    E227: c = a|b\n    E228: msg = fmt%(errno, errmsg)\n    """"""\n    parens = 0\n    need_space = False\n    prev_type = tokenize.OP\n    prev_text = prev_end = None\n    for token_type, text, start, end, line in tokens:\n        if token_type in SKIP_COMMENTS:\n            continue\n        if text in (\'(\', \'lambda\'):\n            parens += 1\n        elif text == \')\':\n            parens -= 1\n        if need_space:\n            if start != prev_end:\n                # Found a (probably) needed space\n                if need_space is not True and not need_space[1]:\n                    yield (need_space[0],\n                           ""E225 missing whitespace around operator"")\n                need_space = False\n            elif text == \'>\' and prev_text in (\'<\', \'-\'):\n                # Tolerate the ""<>"" operator, even if running Python 3\n                # Deal with Python 3\'s annotated return value ""->""\n                pass\n            else:\n                if need_space is True or need_space[1]:\n                    # A needed trailing space was not found\n                    yield prev_end, ""E225 missing whitespace around operator""\n                elif prev_text != \'**\':\n                    code, optype = \'E226\', \'arithmetic\'\n                    if prev_text == \'%\':\n                        code, optype = \'E228\', \'modulo\'\n                    elif prev_text not in ARITHMETIC_OP:\n                        code, optype = \'E227\', \'bitwise or shift\'\n                    yield (need_space[0], ""%s missing whitespace ""\n                           ""around %s operator"" % (code, optype))\n                need_space = False\n        elif token_type == tokenize.OP and prev_end is not None:\n            if text == \'=\' and parens:\n                # Allow keyword args or defaults: foo(bar=None).\n                pass\n            elif text in WS_NEEDED_OPERATORS:\n                need_space = True\n            elif text in UNARY_OPERATORS:\n                # Check if the operator is being used as a binary operator\n                # Allow unary operators: -123, -x, +1.\n                # Allow argument unpacking: foo(*args, **kwargs).\n                if (prev_text in \'}])\' if prev_type == tokenize.OP\n                        else prev_text not in KEYWORDS):\n                    need_space = None\n            elif text in WS_OPTIONAL_OPERATORS:\n                need_space = None\n\n            if need_space is None:\n                # Surrounding space is optional, but ensure that\n                # trailing space matches opening space\n                need_space = (prev_end, start != prev_end)\n            elif need_space and start == prev_end:\n                # A needed opening space was not found\n                yield prev_end, ""E225 missing whitespace around operator""\n                need_space = False\n        prev_type = token_type\n        prev_text = text\n        prev_end = end\n\n\ndef whitespace_around_comma(logical_line):\n    r""""""Avoid extraneous whitespace after a comma or a colon.\n\n    Note: these checks are disabled by default\n\n    Okay: a = (1, 2)\n    E241: a = (1,  2)\n    E242: a = (1,\\t2)\n    """"""\n    line = logical_line\n    for m in WHITESPACE_AFTER_COMMA_REGEX.finditer(line):\n        found = m.start() + 1\n        if \'\\t\' in m.group():\n            yield found, ""E242 tab after \'%s\'"" % m.group()[0]\n        else:\n            yield found, ""E241 multiple spaces after \'%s\'"" % m.group()[0]\n\n\ndef whitespace_around_named_parameter_equals(logical_line, tokens):\n    r""""""Don\'t use spaces around the \'=\' sign in function arguments.\n\n    Don\'t use spaces around the \'=\' sign when used to indicate a\n    keyword argument or a default parameter value.\n\n    Okay: def complex(real, imag=0.0):\n    Okay: return magic(r=real, i=imag)\n    Okay: boolean(a == b)\n    Okay: boolean(a != b)\n    Okay: boolean(a <= b)\n    Okay: boolean(a >= b)\n    Okay: def foo(arg: int = 42):\n\n    E251: def complex(real, imag = 0.0):\n    E251: return magic(r = real, i = imag)\n    """"""\n    parens = 0\n    no_space = False\n    prev_end = None\n    annotated_func_arg = False\n    in_def = logical_line.startswith(\'def\')\n    message = ""E251 unexpected spaces around keyword / parameter equals""\n    for token_type, text, start, end, line in tokens:\n        if token_type == tokenize.NL:\n            continue\n        if no_space:\n            no_space = False\n            if start != prev_end:\n                yield (prev_end, message)\n        if token_type == tokenize.OP:\n            if text == \'(\':\n                parens += 1\n            elif text == \')\':\n                parens -= 1\n            elif in_def and text == \':\' and parens == 1:\n                annotated_func_arg = True\n            elif parens and text == \',\' and parens == 1:\n                annotated_func_arg = False\n            elif parens and text == \'=\' and not annotated_func_arg:\n                no_space = True\n                if start != prev_end:\n                    yield (prev_end, message)\n            if not parens:\n                annotated_func_arg = False\n\n        prev_end = end\n\n\ndef whitespace_before_comment(logical_line, tokens):\n    r""""""Separate inline comments by at least two spaces.\n\n    An inline comment is a comment on the same line as a statement.  Inline\n    comments should be separated by at least two spaces from the statement.\n    They should start with a # and a single space.\n\n    Each line of a block comment starts with a # and a single space\n    (unless it is indented text inside the comment).\n\n    Okay: x = x + 1  # Increment x\n    Okay: x = x + 1    # Increment x\n    Okay: # Block comment\n    E261: x = x + 1 # Increment x\n    E262: x = x + 1  #Increment x\n    E262: x = x + 1  #  Increment x\n    E265: #Block comment\n    E266: ### Block comment\n    """"""\n    prev_end = (0, 0)\n    for token_type, text, start, end, line in tokens:\n        if token_type == tokenize.COMMENT:\n            inline_comment = line[:start[1]].strip()\n            if inline_comment:\n                if prev_end[0] == start[0] and start[1] < prev_end[1] + 2:\n                    yield (prev_end,\n                           ""E261 at least two spaces before inline comment"")\n            symbol, sp, comment = text.partition(\' \')\n            bad_prefix = symbol not in \'#:\' and (symbol.lstrip(\'#\')[:1] or \'#\')\n            if inline_comment:\n                if bad_prefix or comment[:1] in WHITESPACE:\n                    yield start, ""E262 inline comment should start with \'# \'""\n            elif bad_prefix and (bad_prefix != \'!\' or start[0] > 1):\n                if bad_prefix != \'#\':\n                    yield start, ""E265 block comment should start with \'# \'""\n                elif comment:\n                    yield start, ""E266 too many leading \'#\' for block comment""\n        elif token_type != tokenize.NL:\n            prev_end = end\n\n\ndef imports_on_separate_lines(logical_line):\n    r""""""Imports should usually be on separate lines.\n\n    Okay: import os\\nimport sys\n    E401: import sys, os\n\n    Okay: from subprocess import Popen, PIPE\n    Okay: from myclas import MyClass\n    Okay: from foo.bar.yourclass import YourClass\n    Okay: import myclass\n    Okay: import foo.bar.yourclass\n    """"""\n    line = logical_line\n    if line.startswith(\'import \'):\n        found = line.find(\',\')\n        if -1 < found and \';\' not in line[:found]:\n            yield found, ""E401 multiple imports on one line""\n\n\ndef module_imports_on_top_of_file(\n        logical_line, indent_level, checker_state, noqa):\n    r""""""Imports are always put at the top of the file, just after any module\n    comments and docstrings, and before module globals and constants.\n\n    Okay: import os\n    Okay: # this is a comment\\nimport os\n    Okay: \'\'\'this is a module docstring\'\'\'\\nimport os\n    Okay: r\'\'\'this is a module docstring\'\'\'\\nimport os\n    Okay: try:\\n    import x\\nexcept:\\n    pass\\nelse:\\n    pass\\nimport y\n    Okay: try:\\n    import x\\nexcept:\\n    pass\\nfinally:\\n    pass\\nimport y\n    E402: a=1\\nimport os\n    E402: \'One string\'\\n""Two string""\\nimport os\n    E402: a=1\\nfrom sys import x\n\n    Okay: if x:\\n    import os\n    """"""\n    def is_string_literal(line):\n        if line[0] in \'uUbB\':\n            line = line[1:]\n        if line and line[0] in \'rR\':\n            line = line[1:]\n        return line and (line[0] == \'""\' or line[0] == ""\'"")\n\n    allowed_try_keywords = (\'try\', \'except\', \'else\', \'finally\')\n\n    if indent_level:  # Allow imports in conditional statements or functions\n        return\n    if not logical_line:  # Allow empty lines or comments\n        return\n    if noqa:\n        return\n    line = logical_line\n    if line.startswith(\'import \') or line.startswith(\'from \'):\n        if checker_state.get(\'seen_non_imports\', False):\n            yield 0, ""E402 module level import not at top of file""\n    elif any(line.startswith(kw) for kw in allowed_try_keywords):\n        # Allow try, except, else, finally keywords intermixed with imports in\n        # order to support conditional importing\n        return\n    elif is_string_literal(line):\n        # The first literal is a docstring, allow it. Otherwise, report error.\n        if checker_state.get(\'seen_docstring\', False):\n            checker_state[\'seen_non_imports\'] = True\n        else:\n            checker_state[\'seen_docstring\'] = True\n    else:\n        checker_state[\'seen_non_imports\'] = True\n\n\ndef compound_statements(logical_line):\n    r""""""Compound statements (on the same line) are generally discouraged.\n\n    While sometimes it\'s okay to put an if/for/while with a small body\n    on the same line, never do this for multi-clause statements.\n    Also avoid folding such long lines!\n\n    Always use a def statement instead of an assignment statement that\n    binds a lambda expression directly to a name.\n\n    Okay: if foo == \'blah\':\\n    do_blah_thing()\n    Okay: do_one()\n    Okay: do_two()\n    Okay: do_three()\n\n    E701: if foo == \'blah\': do_blah_thing()\n    E701: for x in lst: total += x\n    E701: while t < 10: t = delay()\n    E701: if foo == \'blah\': do_blah_thing()\n    E701: else: do_non_blah_thing()\n    E701: try: something()\n    E701: finally: cleanup()\n    E701: if foo == \'blah\': one(); two(); three()\n    E702: do_one(); do_two(); do_three()\n    E703: do_four();  # useless semicolon\n    E704: def f(x): return 2*x\n    E731: f = lambda x: 2*x\n    """"""\n    line = logical_line\n    last_char = len(line) - 1\n    found = line.find(\':\')\n    while -1 < found < last_char:\n        before = line[:found]\n        if ((before.count(\'{\') <= before.count(\'}\') and   # {\'a\': 1} (dict)\n             before.count(\'[\') <= before.count(\']\') and   # [1:2] (slice)\n             before.count(\'(\') <= before.count(\')\'))):    # (annotation)\n            lambda_kw = LAMBDA_REGEX.search(before)\n            if lambda_kw:\n                before = line[:lambda_kw.start()].rstrip()\n                if before[-1:] == \'=\' and isidentifier(before[:-1].strip()):\n                    yield 0, (""E731 do not assign a lambda expression, use a ""\n                              ""def"")\n                break\n            if before.startswith(\'def \'):\n                yield 0, ""E704 multiple statements on one line (def)""\n            else:\n                yield found, ""E701 multiple statements on one line (colon)""\n        found = line.find(\':\', found + 1)\n    found = line.find(\';\')\n    while -1 < found:\n        if found < last_char:\n            yield found, ""E702 multiple statements on one line (semicolon)""\n        else:\n            yield found, ""E703 statement ends with a semicolon""\n        found = line.find(\';\', found + 1)\n\n\ndef explicit_line_join(logical_line, tokens):\n    r""""""Avoid explicit line join between brackets.\n\n    The preferred way of wrapping long lines is by using Python\'s implied line\n    continuation inside parentheses, brackets and braces.  Long lines can be\n    broken over multiple lines by wrapping expressions in parentheses.  These\n    should be used in preference to using a backslash for line continuation.\n\n    E502: aaa = [123, \\\\n       123]\n    E502: aaa = (""bbb "" \\\\n       ""ccc"")\n\n    Okay: aaa = [123,\\n       123]\n    Okay: aaa = (""bbb ""\\n       ""ccc"")\n    Okay: aaa = ""bbb "" \\\\n    ""ccc""\n    Okay: aaa = 123  # \\\\\n    """"""\n    prev_start = prev_end = parens = 0\n    comment = False\n    backslash = None\n    for token_type, text, start, end, line in tokens:\n        if token_type == tokenize.COMMENT:\n            comment = True\n        if start[0] != prev_start and parens and backslash and not comment:\n            yield backslash, ""E502 the backslash is redundant between brackets""\n        if end[0] != prev_end:\n            if line.rstrip(\'\\r\\n\').endswith(\'\\\\\'):\n                backslash = (end[0], len(line.splitlines()[-1]) - 1)\n            else:\n                backslash = None\n            prev_start = prev_end = end[0]\n        else:\n            prev_start = start[0]\n        if token_type == tokenize.OP:\n            if text in \'([{\':\n                parens += 1\n            elif text in \')]}\':\n                parens -= 1\n\n\ndef break_around_binary_operator(logical_line, tokens):\n    r""""""\n    Avoid breaks before binary operators.\n\n    The preferred place to break around a binary operator is after the\n    operator, not before it.\n\n    W503: (width == 0\\n + height == 0)\n    W503: (width == 0\\n and height == 0)\n\n    Okay: (width == 0 +\\n height == 0)\n    Okay: foo(\\n    -x)\n    Okay: foo(x\\n    [])\n    Okay: x = \'\'\'\\n\'\'\' + \'\'\n    Okay: foo(x,\\n    -y)\n    Okay: foo(x,  # comment\\n    -y)\n    """"""\n    def is_binary_operator(token_type, text):\n        # The % character is strictly speaking a binary operator, but the\n        # common usage seems to be to put it next to the format parameters,\n        # after a line break.\n        return ((token_type == tokenize.OP or text in [\'and\', \'or\']) and\n                text not in ""()[]{},:.;@=%"")\n\n    line_break = False\n    unary_context = True\n    for token_type, text, start, end, line in tokens:\n        if token_type == tokenize.COMMENT:\n            continue\n        if (\'\\n\' in text or \'\\r\' in text) and token_type != tokenize.STRING:\n            line_break = True\n        else:\n            if (is_binary_operator(token_type, text) and line_break and\n                    not unary_context):\n                yield start, ""W503 line break before binary operator""\n            unary_context = text in \'([{,;\'\n            line_break = False\n\n\ndef comparison_to_singleton(logical_line, noqa):\n    r""""""Comparison to singletons should use ""is"" or ""is not"".\n\n    Comparisons to singletons like None should always be done\n    with ""is"" or ""is not"", never the equality operators.\n\n    Okay: if arg is not None:\n    E711: if arg != None:\n    E711: if None == arg:\n    E712: if arg == True:\n    E712: if False == arg:\n\n    Also, beware of writing if x when you really mean if x is not None --\n    e.g. when testing whether a variable or argument that defaults to None was\n    set to some other value.  The other value might have a type (such as a\n    container) that could be false in a boolean context!\n    """"""\n    match = not noqa and COMPARE_SINGLETON_REGEX.search(logical_line)\n    if match:\n        singleton = match.group(1) or match.group(3)\n        same = (match.group(2) == \'==\')\n\n        msg = ""\'if cond is %s:\'"" % ((\'\' if same else \'not \') + singleton)\n        if singleton in (\'None\',):\n            code = \'E711\'\n        else:\n            code = \'E712\'\n            nonzero = ((singleton == \'True\' and same) or\n                       (singleton == \'False\' and not same))\n            msg += "" or \'if %scond:\'"" % (\'\' if nonzero else \'not \')\n        yield match.start(2), (""%s comparison to %s should be %s"" %\n                               (code, singleton, msg))\n\n\ndef comparison_negative(logical_line):\n    r""""""Negative comparison should be done using ""not in"" and ""is not"".\n\n    Okay: if x not in y:\\n    pass\n    Okay: assert (X in Y or X is Z)\n    Okay: if not (X in Y):\\n    pass\n    Okay: zz = x is not y\n    E713: Z = not X in Y\n    E713: if not X.B in Y:\\n    pass\n    E714: if not X is Y:\\n    pass\n    E714: Z = not X.B is Y\n    """"""\n    match = COMPARE_NEGATIVE_REGEX.search(logical_line)\n    if match:\n        pos = match.start(1)\n        if match.group(2) == \'in\':\n            yield pos, ""E713 test for membership should be \'not in\'""\n        else:\n            yield pos, ""E714 test for object identity should be \'is not\'""\n\n\ndef comparison_type(logical_line, noqa):\n    r""""""Object type comparisons should always use isinstance().\n\n    Do not compare types directly.\n\n    Okay: if isinstance(obj, int):\n    E721: if type(obj) is type(1):\n\n    When checking if an object is a string, keep in mind that it might be a\n    unicode string too! In Python 2.3, str and unicode have a common base\n    class, basestring, so you can do:\n\n    Okay: if isinstance(obj, basestring):\n    Okay: if type(a1) is type(b1):\n    """"""\n    match = COMPARE_TYPE_REGEX.search(logical_line)\n    if match and not noqa:\n        inst = match.group(1)\n        if inst and isidentifier(inst) and inst not in SINGLETONS:\n            return  # Allow comparison for types which are not obvious\n        yield match.start(), ""E721 do not compare types, use \'isinstance()\'""\n\n\ndef python_3000_has_key(logical_line, noqa):\n    r""""""The {}.has_key() method is removed in Python 3: use the \'in\' operator.\n\n    Okay: if ""alph"" in d:\\n    print d[""alph""]\n    W601: assert d.has_key(\'alph\')\n    """"""\n    pos = logical_line.find(\'.has_key(\')\n    if pos > -1 and not noqa:\n        yield pos, ""W601 .has_key() is deprecated, use \'in\'""\n\n\ndef python_3000_raise_comma(logical_line):\n    r""""""When raising an exception, use ""raise ValueError(\'message\')"".\n\n    The older form is removed in Python 3.\n\n    Okay: raise DummyError(""Message"")\n    W602: raise DummyError, ""Message""\n    """"""\n    match = RAISE_COMMA_REGEX.match(logical_line)\n    if match and not RERAISE_COMMA_REGEX.match(logical_line):\n        yield match.end() - 1, ""W602 deprecated form of raising exception""\n\n\ndef python_3000_not_equal(logical_line):\n    r""""""New code should always use != instead of <>.\n\n    The older syntax is removed in Python 3.\n\n    Okay: if a != \'no\':\n    W603: if a <> \'no\':\n    """"""\n    pos = logical_line.find(\'<>\')\n    if pos > -1:\n        yield pos, ""W603 \'<>\' is deprecated, use \'!=\'""\n\n\ndef python_3000_backticks(logical_line):\n    r""""""Backticks are removed in Python 3: use repr() instead.\n\n    Okay: val = repr(1 + 2)\n    W604: val = `1 + 2`\n    """"""\n    pos = logical_line.find(\'`\')\n    if pos > -1:\n        yield pos, ""W604 backticks are deprecated, use \'repr()\'""\n\n\n##############################################################################\n# Helper functions\n##############################################################################\n\n\nif sys.version_info < (3,):\n    # Python 2: implicit encoding.\n    def readlines(filename):\n        """"""Read the source code.""""""\n        with open(filename, \'rU\') as f:\n            return f.readlines()\n    isidentifier = re.compile(r\'[a-zA-Z_]\\w*$\').match\n    stdin_get_value = sys.stdin.read\nelse:\n    # Python 3\n    def readlines(filename):\n        """"""Read the source code.""""""\n        try:\n            with open(filename, \'rb\') as f:\n                (coding, lines) = tokenize.detect_encoding(f.readline)\n                f = TextIOWrapper(f, coding, line_buffering=True)\n                return [l.decode(coding) for l in lines] + f.readlines()\n        except (LookupError, SyntaxError, UnicodeError):\n            # Fall back if file encoding is improperly declared\n            with open(filename, encoding=\'latin-1\') as f:\n                return f.readlines()\n    isidentifier = str.isidentifier\n\n    def stdin_get_value():\n        return TextIOWrapper(sys.stdin.buffer, errors=\'ignore\').read()\nnoqa = re.compile(r\'# no(?:qa|pep8)\\b\', re.I).search\n\n\ndef expand_indent(line):\n    r""""""Return the amount of indentation.\n\n    Tabs are expanded to the next multiple of 8.\n\n    >>> expand_indent(\'    \')\n    4\n    >>> expand_indent(\'\\t\')\n    8\n    >>> expand_indent(\'       \\t\')\n    8\n    >>> expand_indent(\'        \\t\')\n    16\n    """"""\n    if \'\\t\' not in line:\n        return len(line) - len(line.lstrip())\n    result = 0\n    for char in line:\n        if char == \'\\t\':\n            result = result // 8 * 8 + 8\n        elif char == \' \':\n            result += 1\n        else:\n            break\n    return result\n\n\ndef mute_string(text):\n    """"""Replace contents with \'xxx\' to prevent syntax matching.\n\n    >>> mute_string(\'""abc""\')\n    \'""xxx""\'\n    >>> mute_string(""\'\'\'abc\'\'\'"")\n    ""\'\'\'xxx\'\'\'""\n    >>> mute_string(""r\'abc\'"")\n    ""r\'xxx\'""\n    """"""\n    # String modifiers (e.g. u or r)\n    start = text.index(text[-1]) + 1\n    end = len(text) - 1\n    # Triple quotes\n    if text[-3:] in (\'""""""\', ""\'\'\'""):\n        start += 2\n        end -= 2\n    return text[:start] + \'x\' * (end - start) + text[end:]\n\n\ndef parse_udiff(diff, patterns=None, parent=\'.\'):\n    """"""Return a dictionary of matching lines.""""""\n    # For each file of the diff, the entry key is the filename,\n    # and the value is a set of row numbers to consider.\n    rv = {}\n    path = nrows = None\n    for line in diff.splitlines():\n        if nrows:\n            if line[:1] != \'-\':\n                nrows -= 1\n            continue\n        if line[:3] == \'@@ \':\n            hunk_match = HUNK_REGEX.match(line)\n            (row, nrows) = [int(g or \'1\') for g in hunk_match.groups()]\n            rv[path].update(range(row, row + nrows))\n        elif line[:3] == \'+++\':\n            path = line[4:].split(\'\\t\', 1)[0]\n            if path[:2] == \'b/\':\n                path = path[2:]\n            rv[path] = set()\n    return dict([(os.path.join(parent, path), rows)\n                 for (path, rows) in rv.items()\n                 if rows and filename_match(path, patterns)])\n\n\ndef normalize_paths(value, parent=os.curdir):\n    """"""Parse a comma-separated list of paths.\n\n    Return a list of absolute paths.\n    """"""\n    if not value:\n        return []\n    if isinstance(value, list):\n        return value\n    paths = []\n    for path in value.split(\',\'):\n        path = path.strip()\n        if \'/\' in path:\n            path = os.path.abspath(os.path.join(parent, path))\n        paths.append(path.rstrip(\'/\'))\n    return paths\n\n\ndef filename_match(filename, patterns, default=True):\n    """"""Check if patterns contains a pattern that matches filename.\n\n    If patterns is unspecified, this always returns True.\n    """"""\n    if not patterns:\n        return default\n    return any(fnmatch(filename, pattern) for pattern in patterns)\n\n\ndef _is_eol_token(token):\n    return token[0] in NEWLINE or token[4][token[3][1]:].lstrip() == \'\\\\\\n\'\nif COMMENT_WITH_NL:\n    def _is_eol_token(token, _eol_token=_is_eol_token):\n        return _eol_token(token) or (token[0] == tokenize.COMMENT and\n                                     token[1] == token[4])\n\n##############################################################################\n# Framework to run all checks\n##############################################################################\n\n\n_checks = {\'physical_line\': {}, \'logical_line\': {}, \'tree\': {}}\n\n\ndef _get_parameters(function):\n    if sys.version_info >= (3, 3):\n        return [parameter.name\n                for parameter\n                in inspect.signature(function).parameters.values()\n                if parameter.kind == parameter.POSITIONAL_OR_KEYWORD]\n    else:\n        return inspect.getargspec(function)[0]\n\n\ndef register_check(check, codes=None):\n    """"""Register a new check object.""""""\n    def _add_check(check, kind, codes, args):\n        if check in _checks[kind]:\n            _checks[kind][check][0].extend(codes or [])\n        else:\n            _checks[kind][check] = (codes or [\'\'], args)\n    if inspect.isfunction(check):\n        args = _get_parameters(check)\n        if args and args[0] in (\'physical_line\', \'logical_line\'):\n            if codes is None:\n                codes = ERRORCODE_REGEX.findall(check.__doc__ or \'\')\n            _add_check(check, args[0], codes, args)\n    elif inspect.isclass(check):\n        if _get_parameters(check.__init__)[:2] == [\'self\', \'tree\']:\n            _add_check(check, \'tree\', codes, None)\n\n\ndef init_checks_registry():\n    """"""Register all globally visible functions.\n\n    The first argument name is either \'physical_line\' or \'logical_line\'.\n    """"""\n    mod = inspect.getmodule(register_check)\n    for (name, function) in inspect.getmembers(mod, inspect.isfunction):\n        register_check(function)\ninit_checks_registry()\n\n\nclass Checker(object):\n    """"""Load a Python source file, tokenize it, check coding style.""""""\n\n    def __init__(self, filename=None, lines=None,\n                 options=None, report=None, **kwargs):\n        if options is None:\n            options = StyleGuide(kwargs).options\n        else:\n            assert not kwargs\n        self._io_error = None\n        self._physical_checks = options.physical_checks\n        self._logical_checks = options.logical_checks\n        self._ast_checks = options.ast_checks\n        self.max_line_length = options.max_line_length\n        self.multiline = False  # in a multiline string?\n        self.hang_closing = options.hang_closing\n        self.verbose = options.verbose\n        self.filename = filename\n        # Dictionary where a checker can store its custom state.\n        self._checker_states = {}\n        if filename is None:\n            self.filename = \'stdin\'\n            self.lines = lines or []\n        elif filename == \'-\':\n            self.filename = \'stdin\'\n            self.lines = stdin_get_value().splitlines(True)\n        elif lines is None:\n            try:\n                self.lines = readlines(filename)\n            except IOError:\n                (exc_type, exc) = sys.exc_info()[:2]\n                self._io_error = \'%s: %s\' % (exc_type.__name__, exc)\n                self.lines = []\n        else:\n            self.lines = lines\n        if self.lines:\n            ord0 = ord(self.lines[0][0])\n            if ord0 in (0xef, 0xfeff):  # Strip the UTF-8 BOM\n                if ord0 == 0xfeff:\n                    self.lines[0] = self.lines[0][1:]\n                elif self.lines[0][:3] == \'\\xef\\xbb\\xbf\':\n                    self.lines[0] = self.lines[0][3:]\n        self.report = report or options.report\n        self.report_error = self.report.error\n\n    def report_invalid_syntax(self):\n        """"""Check if the syntax is valid.""""""\n        (exc_type, exc) = sys.exc_info()[:2]\n        if len(exc.args) > 1:\n            offset = exc.args[1]\n            if len(offset) > 2:\n                offset = offset[1:3]\n        else:\n            offset = (1, 0)\n        self.report_error(offset[0], offset[1] or 0,\n                          \'E901 %s: %s\' % (exc_type.__name__, exc.args[0]),\n                          self.report_invalid_syntax)\n\n    def readline(self):\n        """"""Get the next line from the input buffer.""""""\n        if self.line_number >= self.total_lines:\n            return \'\'\n        line = self.lines[self.line_number]\n        self.line_number += 1\n        if self.indent_char is None and line[:1] in WHITESPACE:\n            self.indent_char = line[0]\n        return line\n\n    def run_check(self, check, argument_names):\n        """"""Run a check plugin.""""""\n        arguments = []\n        for name in argument_names:\n            arguments.append(getattr(self, name))\n        return check(*arguments)\n\n    def init_checker_state(self, name, argument_names):\n        """""" Prepares a custom state for the specific checker plugin.""""""\n        if \'checker_state\' in argument_names:\n            self.checker_state = self._checker_states.setdefault(name, {})\n\n    def check_physical(self, line):\n        """"""Run all physical checks on a raw input line.""""""\n        self.physical_line = line\n        for name, check, argument_names in self._physical_checks:\n            self.init_checker_state(name, argument_names)\n            result = self.run_check(check, argument_names)\n            if result is not None:\n                (offset, text) = result\n                self.report_error(self.line_number, offset, text, check)\n                if text[:4] == \'E101\':\n                    self.indent_char = line[0]\n\n    def build_tokens_line(self):\n        """"""Build a logical line from tokens.""""""\n        logical = []\n        comments = []\n        length = 0\n        prev_row = prev_col = mapping = None\n        for token_type, text, start, end, line in self.tokens:\n            if token_type in SKIP_TOKENS:\n                continue\n            if not mapping:\n                mapping = [(0, start)]\n            if token_type == tokenize.COMMENT:\n                comments.append(text)\n                continue\n            if token_type == tokenize.STRING:\n                text = mute_string(text)\n            if prev_row:\n                (start_row, start_col) = start\n                if prev_row != start_row:    # different row\n                    prev_text = self.lines[prev_row - 1][prev_col - 1]\n                    if prev_text == \',\' or (prev_text not in \'{[(\' and\n                                            text not in \'}])\'):\n                        text = \' \' + text\n                elif prev_col != start_col:  # different column\n                    text = line[prev_col:start_col] + text\n            logical.append(text)\n            length += len(text)\n            mapping.append((length, end))\n            (prev_row, prev_col) = end\n        self.logical_line = \'\'.join(logical)\n        self.noqa = comments and noqa(\'\'.join(comments))\n        return mapping\n\n    def check_logical(self):\n        """"""Build a line from tokens and run all logical checks on it.""""""\n        self.report.increment_logical_line()\n        mapping = self.build_tokens_line()\n\n        if not mapping:\n            return\n\n        (start_row, start_col) = mapping[0][1]\n        start_line = self.lines[start_row - 1]\n        self.indent_level = expand_indent(start_line[:start_col])\n        if self.blank_before < self.blank_lines:\n            self.blank_before = self.blank_lines\n        if self.verbose >= 2:\n            print(self.logical_line[:80].rstrip())\n        for name, check, argument_names in self._logical_checks:\n            if self.verbose >= 4:\n                print(\'   \' + name)\n            self.init_checker_state(name, argument_names)\n            for offset, text in self.run_check(check, argument_names) or ():\n                if not isinstance(offset, tuple):\n                    for token_offset, pos in mapping:\n                        if offset <= token_offset:\n                            break\n                    offset = (pos[0], pos[1] + offset - token_offset)\n                self.report_error(offset[0], offset[1], text, check)\n        if self.logical_line:\n            self.previous_indent_level = self.indent_level\n            self.previous_logical = self.logical_line\n        self.blank_lines = 0\n        self.tokens = []\n\n    def check_ast(self):\n        """"""Build the file\'s AST and run all AST checks.""""""\n        try:\n            tree = compile(\'\'.join(self.lines), \'\', \'exec\', PyCF_ONLY_AST)\n        except (ValueError, SyntaxError, TypeError):\n            return self.report_invalid_syntax()\n        for name, cls, __ in self._ast_checks:\n            checker = cls(tree, self.filename)\n            for lineno, offset, text, check in checker.run():\n                if not self.lines or not noqa(self.lines[lineno - 1]):\n                    self.report_error(lineno, offset, text, check)\n\n    def generate_tokens(self):\n        """"""Tokenize the file, run physical line checks and yield tokens.""""""\n        if self._io_error:\n            self.report_error(1, 0, \'E902 %s\' % self._io_error, readlines)\n        tokengen = tokenize.generate_tokens(self.readline)\n        try:\n            for token in tokengen:\n                if token[2][0] > self.total_lines:\n                    return\n                self.maybe_check_physical(token)\n                yield token\n        except (SyntaxError, tokenize.TokenError):\n            self.report_invalid_syntax()\n\n    def maybe_check_physical(self, token):\n        """"""If appropriate (based on token), check current physical line(s).""""""\n        # Called after every token, but act only on end of line.\n        if _is_eol_token(token):\n            # Obviously, a newline token ends a single physical line.\n            self.check_physical(token[4])\n        elif token[0] == tokenize.STRING and \'\\n\' in token[1]:\n            # Less obviously, a string that contains newlines is a\n            # multiline string, either triple-quoted or with internal\n            # newlines backslash-escaped. Check every physical line in the\n            # string *except* for the last one: its newline is outside of\n            # the multiline string, so we consider it a regular physical\n            # line, and will check it like any other physical line.\n            #\n            # Subtleties:\n            # - we don\'t *completely* ignore the last line; if it contains\n            #   the magical ""# noqa"" comment, we disable all physical\n            #   checks for the entire multiline string\n            # - have to wind self.line_number back because initially it\n            #   points to the last line of the string, and we want\n            #   check_physical() to give accurate feedback\n            if noqa(token[4]):\n                return\n            self.multiline = True\n            self.line_number = token[2][0]\n            for line in token[1].split(\'\\n\')[:-1]:\n                self.check_physical(line + \'\\n\')\n                self.line_number += 1\n            self.multiline = False\n\n    def check_all(self, expected=None, line_offset=0):\n        """"""Run all checks on the input file.""""""\n        self.report.init_file(self.filename, self.lines, expected, line_offset)\n        self.total_lines = len(self.lines)\n        if self._ast_checks:\n            self.check_ast()\n        self.line_number = 0\n        self.indent_char = None\n        self.indent_level = self.previous_indent_level = 0\n        self.previous_logical = \'\'\n        self.tokens = []\n        self.blank_lines = self.blank_before = 0\n        parens = 0\n        for token in self.generate_tokens():\n            self.tokens.append(token)\n            token_type, text = token[0:2]\n            if self.verbose >= 3:\n                if token[2][0] == token[3][0]:\n                    pos = \'[%s:%s]\' % (token[2][1] or \'\', token[3][1])\n                else:\n                    pos = \'l.%s\' % token[3][0]\n                print(\'l.%s\\t%s\\t%s\\t%r\' %\n                      (token[2][0], pos, tokenize.tok_name[token[0]], text))\n            if token_type == tokenize.OP:\n                if text in \'([{\':\n                    parens += 1\n                elif text in \'}])\':\n                    parens -= 1\n            elif not parens:\n                if token_type in NEWLINE:\n                    if token_type == tokenize.NEWLINE:\n                        self.check_logical()\n                        self.blank_before = 0\n                    elif len(self.tokens) == 1:\n                        # The physical line contains only this token.\n                        self.blank_lines += 1\n                        del self.tokens[0]\n                    else:\n                        self.check_logical()\n                elif COMMENT_WITH_NL and token_type == tokenize.COMMENT:\n                    if len(self.tokens) == 1:\n                        # The comment also ends a physical line\n                        token = list(token)\n                        token[1] = text.rstrip(\'\\r\\n\')\n                        token[3] = (token[2][0], token[2][1] + len(token[1]))\n                        self.tokens = [tuple(token)]\n                        self.check_logical()\n        if self.tokens:\n            self.check_physical(self.lines[-1])\n            self.check_logical()\n        return self.report.get_file_results()\n\n\nclass BaseReport(object):\n    """"""Collect the results of the checks.""""""\n\n    print_filename = False\n\n    def __init__(self, options):\n        self._benchmark_keys = options.benchmark_keys\n        self._ignore_code = options.ignore_code\n        # Results\n        self.elapsed = 0\n        self.total_errors = 0\n        self.counters = dict.fromkeys(self._benchmark_keys, 0)\n        self.messages = {}\n\n    def start(self):\n        """"""Start the timer.""""""\n        self._start_time = time.time()\n\n    def stop(self):\n        """"""Stop the timer.""""""\n        self.elapsed = time.time() - self._start_time\n\n    def init_file(self, filename, lines, expected, line_offset):\n        """"""Signal a new file.""""""\n        self.filename = filename\n        self.lines = lines\n        self.expected = expected or ()\n        self.line_offset = line_offset\n        self.file_errors = 0\n        self.counters[\'files\'] += 1\n        self.counters[\'physical lines\'] += len(lines)\n\n    def increment_logical_line(self):\n        """"""Signal a new logical line.""""""\n        self.counters[\'logical lines\'] += 1\n\n    def error(self, line_number, offset, text, check):\n        """"""Report an error, according to options.""""""\n        code = text[:4]\n        if self._ignore_code(code):\n            return\n        if code in self.counters:\n            self.counters[code] += 1\n        else:\n            self.counters[code] = 1\n            self.messages[code] = text[5:]\n        # Don\'t care about expected errors or warnings\n        if code in self.expected:\n            return\n        if self.print_filename and not self.file_errors:\n            print(self.filename)\n        self.file_errors += 1\n        self.total_errors += 1\n        return code\n\n    def get_file_results(self):\n        """"""Return the count of errors and warnings for this file.""""""\n        return self.file_errors\n\n    def get_count(self, prefix=\'\'):\n        """"""Return the total count of errors and warnings.""""""\n        return sum([self.counters[key]\n                    for key in self.messages if key.startswith(prefix)])\n\n    def get_statistics(self, prefix=\'\'):\n        """"""Get statistics for message codes that start with the prefix.\n\n        prefix=\'\' matches all errors and warnings\n        prefix=\'E\' matches all errors\n        prefix=\'W\' matches all warnings\n        prefix=\'E4\' matches all errors that have to do with imports\n        """"""\n        return [\'%-7s %s %s\' % (self.counters[key], key, self.messages[key])\n                for key in sorted(self.messages) if key.startswith(prefix)]\n\n    def print_statistics(self, prefix=\'\'):\n        """"""Print overall statistics (number of errors and warnings).""""""\n        for line in self.get_statistics(prefix):\n            print(line)\n\n    def print_benchmark(self):\n        """"""Print benchmark numbers.""""""\n        print(\'%-7.2f %s\' % (self.elapsed, \'seconds elapsed\'))\n        if self.elapsed:\n            for key in self._benchmark_keys:\n                print(\'%-7d %s per second (%d total)\' %\n                      (self.counters[key] / self.elapsed, key,\n                       self.counters[key]))\n\n\nclass FileReport(BaseReport):\n    """"""Collect the results of the checks and print only the filenames.""""""\n    print_filename = True\n\n\nclass StandardReport(BaseReport):\n    """"""Collect and print the results of the checks.""""""\n\n    def __init__(self, options):\n        super(StandardReport, self).__init__(options)\n        self._fmt = REPORT_FORMAT.get(options.format.lower(),\n                                      options.format)\n        self._repeat = options.repeat\n        self._show_source = options.show_source\n        self._show_pep8 = options.show_pep8\n\n    def init_file(self, filename, lines, expected, line_offset):\n        """"""Signal a new file.""""""\n        self._deferred_print = []\n        return super(StandardReport, self).init_file(\n            filename, lines, expected, line_offset)\n\n    def error(self, line_number, offset, text, check):\n        """"""Report an error, according to options.""""""\n        code = super(StandardReport, self).error(line_number, offset,\n                                                 text, check)\n        if code and (self.counters[code] == 1 or self._repeat):\n            self._deferred_print.append(\n                (line_number, offset, code, text[5:], check.__doc__))\n        return code\n\n    def get_file_results(self):\n        """"""Print the result and return the overall count for this file.""""""\n        self._deferred_print.sort()\n        for line_number, offset, code, text, doc in self._deferred_print:\n            print(self._fmt % {\n                \'path\': self.filename,\n                \'row\': self.line_offset + line_number, \'col\': offset + 1,\n                \'code\': code, \'text\': text,\n            })\n            if self._show_source:\n                if line_number > len(self.lines):\n                    line = \'\'\n                else:\n                    line = self.lines[line_number - 1]\n                print(line.rstrip())\n                print(re.sub(r\'\\S\', \' \', line[:offset]) + \'^\')\n            if self._show_pep8 and doc:\n                print(\'    \' + doc.strip())\n\n            # stdout is block buffered when not stdout.isatty().\n            # line can be broken where buffer boundary since other processes\n            # write to same file.\n            # flush() after print() to avoid buffer boundary.\n            # Typical buffer size is 8192. line written safely when\n            # len(line) < 8192.\n            sys.stdout.flush()\n        return self.file_errors\n\n\nclass DiffReport(StandardReport):\n    """"""Collect and print the results for the changed lines only.""""""\n\n    def __init__(self, options):\n        super(DiffReport, self).__init__(options)\n        self._selected = options.selected_lines\n\n    def error(self, line_number, offset, text, check):\n        if line_number not in self._selected[self.filename]:\n            return\n        return super(DiffReport, self).error(line_number, offset, text, check)\n\n\nclass StyleGuide(object):\n    """"""Initialize a PEP-8 instance with few options.""""""\n\n    def __init__(self, *args, **kwargs):\n        # build options from the command line\n        self.checker_class = kwargs.pop(\'checker_class\', Checker)\n        parse_argv = kwargs.pop(\'parse_argv\', False)\n        config_file = kwargs.pop(\'config_file\', False)\n        parser = kwargs.pop(\'parser\', None)\n        # build options from dict\n        options_dict = dict(*args, **kwargs)\n        arglist = None if parse_argv else options_dict.get(\'paths\', None)\n        options, self.paths = process_options(\n            arglist, parse_argv, config_file, parser)\n        if options_dict:\n            options.__dict__.update(options_dict)\n            if \'paths\' in options_dict:\n                self.paths = options_dict[\'paths\']\n\n        self.runner = self.input_file\n        self.options = options\n\n        if not options.reporter:\n            options.reporter = BaseReport if options.quiet else StandardReport\n\n        options.select = tuple(options.select or ())\n        if not (options.select or options.ignore or\n                options.testsuite or options.doctest) and DEFAULT_IGNORE:\n            # The default choice: ignore controversial checks\n            options.ignore = tuple(DEFAULT_IGNORE.split(\',\'))\n        else:\n            # Ignore all checks which are not explicitly selected\n            options.ignore = (\'\',) if options.select else tuple(options.ignore)\n        options.benchmark_keys = BENCHMARK_KEYS[:]\n        options.ignore_code = self.ignore_code\n        options.physical_checks = self.get_checks(\'physical_line\')\n        options.logical_checks = self.get_checks(\'logical_line\')\n        options.ast_checks = self.get_checks(\'tree\')\n        self.init_report()\n\n    def init_report(self, reporter=None):\n        """"""Initialize the report instance.""""""\n        self.options.report = (reporter or self.options.reporter)(self.options)\n        return self.options.report\n\n    def check_files(self, paths=None):\n        """"""Run all checks on the paths.""""""\n        if paths is None:\n            paths = self.paths\n        report = self.options.report\n        runner = self.runner\n        report.start()\n        try:\n            for path in paths:\n                if os.path.isdir(path):\n                    self.input_dir(path)\n                elif not self.excluded(path):\n                    runner(path)\n        except KeyboardInterrupt:\n            print(\'... stopped\')\n        report.stop()\n        return report\n\n    def input_file(self, filename, lines=None, expected=None, line_offset=0):\n        """"""Run all checks on a Python source file.""""""\n        if self.options.verbose:\n            print(\'checking %s\' % filename)\n        fchecker = self.checker_class(\n            filename, lines=lines, options=self.options)\n        return fchecker.check_all(expected=expected, line_offset=line_offset)\n\n    def input_dir(self, dirname):\n        """"""Check all files in this directory and all subdirectories.""""""\n        dirname = dirname.rstrip(\'/\')\n        if self.excluded(dirname):\n            return 0\n        counters = self.options.report.counters\n        verbose = self.options.verbose\n        filepatterns = self.options.filename\n        runner = self.runner\n        for root, dirs, files in os.walk(dirname):\n            if verbose:\n                print(\'directory \' + root)\n            counters[\'directories\'] += 1\n            for subdir in sorted(dirs):\n                if self.excluded(subdir, root):\n                    dirs.remove(subdir)\n            for filename in sorted(files):\n                # contain a pattern that matches?\n                if ((filename_match(filename, filepatterns) and\n                     not self.excluded(filename, root))):\n                    runner(os.path.join(root, filename))\n\n    def excluded(self, filename, parent=None):\n        """"""Check if the file should be excluded.\n\n        Check if \'options.exclude\' contains a pattern that matches filename.\n        """"""\n        if not self.options.exclude:\n            return False\n        basename = os.path.basename(filename)\n        if filename_match(basename, self.options.exclude):\n            return True\n        if parent:\n            filename = os.path.join(parent, filename)\n        filename = os.path.abspath(filename)\n        return filename_match(filename, self.options.exclude)\n\n    def ignore_code(self, code):\n        """"""Check if the error code should be ignored.\n\n        If \'options.select\' contains a prefix of the error code,\n        return False.  Else, if \'options.ignore\' contains a prefix of\n        the error code, return True.\n        """"""\n        if len(code) < 4 and any(s.startswith(code)\n                                 for s in self.options.select):\n            return False\n        return (code.startswith(self.options.ignore) and\n                not code.startswith(self.options.select))\n\n    def get_checks(self, argument_name):\n        """"""Get all the checks for this category.\n\n        Find all globally visible functions where the first argument name\n        starts with argument_name and which contain selected tests.\n        """"""\n        checks = []\n        for check, attrs in _checks[argument_name].items():\n            (codes, args) = attrs\n            if any(not (code and self.ignore_code(code)) for code in codes):\n                checks.append((check.__name__, check, args))\n        return sorted(checks)\n\n\ndef get_parser(prog=\'pep8\', version=__version__):\n    parser = OptionParser(prog=prog, version=version,\n                          usage=""%prog [options] input ..."")\n    parser.config_options = [\n        \'exclude\', \'filename\', \'select\', \'ignore\', \'max-line-length\',\n        \'hang-closing\', \'count\', \'format\', \'quiet\', \'show-pep8\',\n        \'show-source\', \'statistics\', \'verbose\']\n    parser.add_option(\'-v\', \'--verbose\', default=0, action=\'count\',\n                      help=""print status messages, or debug with -vv"")\n    parser.add_option(\'-q\', \'--quiet\', default=0, action=\'count\',\n                      help=""report only file names, or nothing with -qq"")\n    parser.add_option(\'-r\', \'--repeat\', default=True, action=\'store_true\',\n                      help=""(obsolete) show all occurrences of the same error"")\n    parser.add_option(\'--first\', action=\'store_false\', dest=\'repeat\',\n                      help=""show first occurrence of each error"")\n    parser.add_option(\'--exclude\', metavar=\'patterns\', default=DEFAULT_EXCLUDE,\n                      help=""exclude files or directories which match these ""\n                           ""comma separated patterns (default: %default)"")\n    parser.add_option(\'--filename\', metavar=\'patterns\', default=\'*.py\',\n                      help=""when parsing directories, only check filenames ""\n                           ""matching these comma separated patterns ""\n                           ""(default: %default)"")\n    parser.add_option(\'--select\', metavar=\'errors\', default=\'\',\n                      help=""select errors and warnings (e.g. E,W6)"")\n    parser.add_option(\'--ignore\', metavar=\'errors\', default=\'\',\n                      help=""skip errors and warnings (e.g. E4,W) ""\n                           ""(default: %s)"" % DEFAULT_IGNORE)\n    parser.add_option(\'--show-source\', action=\'store_true\',\n                      help=""show source code for each error"")\n    parser.add_option(\'--show-pep8\', action=\'store_true\',\n                      help=""show text of PEP 8 for each error ""\n                           ""(implies --first)"")\n    parser.add_option(\'--statistics\', action=\'store_true\',\n                      help=""count errors and warnings"")\n    parser.add_option(\'--count\', action=\'store_true\',\n                      help=""print total number of errors and warnings ""\n                           ""to standard error and set exit code to 1 if ""\n                           ""total is not null"")\n    parser.add_option(\'--max-line-length\', type=\'int\', metavar=\'n\',\n                      default=MAX_LINE_LENGTH,\n                      help=""set maximum allowed line length ""\n                           ""(default: %default)"")\n    parser.add_option(\'--hang-closing\', action=\'store_true\',\n                      help=""hang closing bracket instead of matching ""\n                           ""indentation of opening bracket\'s line"")\n    parser.add_option(\'--format\', metavar=\'format\', default=\'default\',\n                      help=""set the error format [default|pylint|<custom>]"")\n    parser.add_option(\'--diff\', action=\'store_true\',\n                      help=""report changes only within line number ranges in ""\n                           ""the unified diff received on STDIN"")\n    group = parser.add_option_group(""Testing Options"")\n    if os.path.exists(TESTSUITE_PATH):\n        group.add_option(\'--testsuite\', metavar=\'dir\',\n                         help=""run regression tests from dir"")\n        group.add_option(\'--doctest\', action=\'store_true\',\n                         help=""run doctest on myself"")\n    group.add_option(\'--benchmark\', action=\'store_true\',\n                     help=""measure processing speed"")\n    return parser\n\n\ndef read_config(options, args, arglist, parser):\n    """"""Read and parse configurations\n\n    If a config file is specified on the command line with the ""--config""\n    option, then only it is used for configuration.\n\n    Otherwise, the user configuration (~/.config/pep8) and any local\n    configurations in the current directory or above will be merged together\n    (in that order) using the read method of ConfigParser.\n    """"""\n    config = RawConfigParser()\n\n    cli_conf = options.config\n\n    local_dir = os.curdir\n\n    if USER_CONFIG and os.path.isfile(USER_CONFIG):\n        if options.verbose:\n            print(\'user configuration: %s\' % USER_CONFIG)\n        config.read(USER_CONFIG)\n\n    parent = tail = args and os.path.abspath(os.path.commonprefix(args))\n    while tail:\n        if config.read(os.path.join(parent, fn) for fn in PROJECT_CONFIG):\n            local_dir = parent\n            if options.verbose:\n                print(\'local configuration: in %s\' % parent)\n            break\n        (parent, tail) = os.path.split(parent)\n\n    if cli_conf and os.path.isfile(cli_conf):\n        if options.verbose:\n            print(\'cli configuration: %s\' % cli_conf)\n        config.read(cli_conf)\n\n    pep8_section = parser.prog\n    if config.has_section(pep8_section):\n        option_list = dict([(o.dest, o.type or o.action)\n                            for o in parser.option_list])\n\n        # First, read the default values\n        (new_options, __) = parser.parse_args([])\n\n        # Second, parse the configuration\n        for opt in config.options(pep8_section):\n            if opt.replace(\'_\', \'-\') not in parser.config_options:\n                print(""  unknown option \'%s\' ignored"" % opt)\n                continue\n            if options.verbose > 1:\n                print(""  %s = %s"" % (opt, config.get(pep8_section, opt)))\n            normalized_opt = opt.replace(\'-\', \'_\')\n            opt_type = option_list[normalized_opt]\n            if opt_type in (\'int\', \'count\'):\n                value = config.getint(pep8_section, opt)\n            elif opt_type == \'string\':\n                value = config.get(pep8_section, opt)\n                if normalized_opt == \'exclude\':\n                    value = normalize_paths(value, local_dir)\n            else:\n                assert opt_type in (\'store_true\', \'store_false\')\n                value = config.getboolean(pep8_section, opt)\n            setattr(new_options, normalized_opt, value)\n\n        # Third, overwrite with the command-line options\n        (options, __) = parser.parse_args(arglist, values=new_options)\n    options.doctest = options.testsuite = False\n    return options\n\n\ndef process_options(arglist=None, parse_argv=False, config_file=None,\n                    parser=None):\n    """"""Process options passed either via arglist or via command line args.\n\n    Passing in the ``config_file`` parameter allows other tools, such as flake8\n    to specify their own options to be processed in pep8.\n    """"""\n    if not parser:\n        parser = get_parser()\n    if not parser.has_option(\'--config\'):\n        group = parser.add_option_group(""Configuration"", description=(\n            ""The project options are read from the [%s] section of the ""\n            ""tox.ini file or the setup.cfg file located in any parent folder ""\n            ""of the path(s) being processed.  Allowed options are: %s."" %\n            (parser.prog, \', \'.join(parser.config_options))))\n        group.add_option(\'--config\', metavar=\'path\', default=config_file,\n                         help=""user config file location"")\n    # Don\'t read the command line if the module is used as a library.\n    if not arglist and not parse_argv:\n        arglist = []\n    # If parse_argv is True and arglist is None, arguments are\n    # parsed from the command line (sys.argv)\n    (options, args) = parser.parse_args(arglist)\n    options.reporter = None\n\n    if options.ensure_value(\'testsuite\', False):\n        args.append(options.testsuite)\n    elif not options.ensure_value(\'doctest\', False):\n        if parse_argv and not args:\n            if options.diff or any(os.path.exists(name)\n                                   for name in PROJECT_CONFIG):\n                args = [\'.\']\n            else:\n                parser.error(\'input not specified\')\n        options = read_config(options, args, arglist, parser)\n        options.reporter = parse_argv and options.quiet == 1 and FileReport\n\n    options.filename = _parse_multi_options(options.filename)\n    options.exclude = normalize_paths(options.exclude)\n    options.select = _parse_multi_options(options.select)\n    options.ignore = _parse_multi_options(options.ignore)\n\n    if options.diff:\n        options.reporter = DiffReport\n        stdin = stdin_get_value()\n        options.selected_lines = parse_udiff(stdin, options.filename, args[0])\n        args = sorted(options.selected_lines)\n\n    return options, args\n\n\ndef _parse_multi_options(options, split_token=\',\'):\n    r""""""Split and strip and discard empties.\n\n    Turns the following:\n\n    A,\n    B,\n\n    into [""A"", ""B""]\n    """"""\n    if options:\n        return [o.strip() for o in options.split(split_token) if o.strip()]\n    else:\n        return options\n\n\ndef _main():\n    """"""Parse options and run checks on Python source.""""""\n    import signal\n\n    # Handle ""Broken pipe"" gracefully\n    try:\n        signal.signal(signal.SIGPIPE, lambda signum, frame: sys.exit(1))\n    except AttributeError:\n        pass    # not supported on Windows\n\n    pep8style = StyleGuide(parse_argv=True)\n    options = pep8style.options\n\n    if options.doctest or options.testsuite:\n        from testsuite.support import run_tests\n        report = run_tests(pep8style)\n    else:\n        report = pep8style.check_files()\n\n    if options.statistics:\n        report.print_statistics()\n\n    if options.benchmark:\n        report.print_benchmark()\n\n    if options.testsuite and not options.quiet:\n        report.print_results()\n\n    if report.total_errors:\n        if options.count:\n            sys.stderr.write(str(report.total_errors) + \'\\n\')\n        sys.exit(1)\n\n\nif __name__ == \'__main__\':\n    _main()\n'"
pyzoo/docs/analytics_zoo_pytext.py,0,"b'#!/usr/bin/env python\n\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport re\n\n\ndef _process_docstring(app, what, name, obj, options, lines):\n    liter_re = re.compile(r\'\\s*```\\s*$\')\n\n    liter_flag = False\n\n    offset = 0\n    for j in range(len(lines)):\n        i = j+offset\n        line = lines[i]\n        # first literal block line\n        if not liter_flag and liter_re.match(line):\n            liter_flag = True\n            lines.insert(i+1, \'\')\n            offset += 1\n            lines[i] = \'::\'\n        # last literal block line\n        elif liter_flag and liter_re.match(line):\n            liter_flag = False\n            lines[i] = \'\'\n        # regular line within literal block\n        elif liter_flag:\n            line = \' \' + line\n            lines[i] = line\n        # regualr line\n        else:\n            lines[i] = line.lstrip()\n\n\ndef setup(app):\n    app.connect(""autodoc-process-docstring"", _process_docstring)\n'"
pyzoo/docs/doc-web.py,0,"b'#!/usr/bin/env python\n\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport web\n\nurls = (\n    \'/(.*)\', \'router\'\n)\n\napp = web.application(urls, globals())\n\n\nclass router:\n    def GET(self, path):\n        if path == \'\':\n            path = \'index.html\'\n        f = open(\'_build/html/\'+path)\n        return f.read()\n\nif __name__ == ""__main__"":\n    app = web.application(urls, globals())\n    app.run()\n'"
pyzoo/test/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.common.nncontext import *\nfrom zoo.util.engine import prepare_env\n\nprepare_env()\ncreator_classes = JavaCreator.get_creator_class()[:]\nJavaCreator.set_creator_class([])\nJavaCreator.add_creator_class(""com.intel.analytics.zoo.tfpark.python.PythonTFPark"")\nJavaCreator.add_creator_class(""com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames"")\nJavaCreator.add_creator_class(""com.intel.analytics.zoo.feature.python.PythonImageFeature"")\nJavaCreator.add_creator_class(""com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad"")\nJavaCreator.add_creator_class(""com.intel.analytics.zoo.models.python.PythonZooModel"")\nJavaCreator.add_creator_class(""com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras2"")\nJavaCreator.add_creator_class(""com.intel.analytics.zoo.feature.python.PythonTextFeature"")\nJavaCreator.add_creator_class(""com.intel.analytics.zoo.feature.python.PythonFeatureSet"")\nJavaCreator.add_creator_class(""com.intel.analytics.zoo.pipeline.api.net.python.PythonZooNet"")\nJavaCreator.add_creator_class(""com.intel.analytics.zoo.pipeline.inference.PythonInferenceModel"")\nJavaCreator.add_creator_class(""com.intel.analytics.zoo.pipeline.estimator.python.PythonEstimator"")\nfor clz in creator_classes:\n    JavaCreator.add_creator_class(clz)\n\n__version__ = ""0.9.0.dev0""\n'"
pyzoo/test/zoo/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/automl/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/common/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .utils import *\n'"
pyzoo/zoo/common/nncontext.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.util.common import *\nimport warnings\nimport multiprocessing\nimport os\n\n\ndef init_spark_on_local(cores=2, conf=None, python_location=None, spark_log_level=""WARN"",\n                        redirect_spark_log=True):\n    """"""\n    Create a SparkContext with Zoo configuration in local machine.\n    :param cores: The default value is 2 and you can also set it to *\n     meaning all of the available cores. i.e `init_on_local(cores=""*"")`\n    :param conf: A key value dictionary appended to SparkConf.\n    :param python_location: The path to your running python executable.\n    :param spark_log_level: Log level of Spark\n    :param redirect_spark_log: Redirect the Spark log to local file or not.\n    :return:\n    """"""\n    from zoo.util.spark import SparkRunner\n    sparkrunner = SparkRunner(spark_log_level=spark_log_level,\n                              redirect_spark_log=redirect_spark_log)\n    return sparkrunner.init_spark_on_local(cores=cores, conf=conf,\n                                           python_location=python_location)\n\n\ndef init_spark_on_yarn(hadoop_conf,\n                       conda_name,\n                       num_executor,\n                       executor_cores,\n                       executor_memory=""2g"",\n                       driver_memory=""1g"",\n                       driver_cores=4,\n                       extra_executor_memory_for_ray=None,\n                       extra_python_lib=None,\n                       penv_archive=None,\n                       additional_archive=None,\n                       hadoop_user_name=""root"",\n                       spark_yarn_archive=None,\n                       spark_log_level=""WARN"",\n                       redirect_spark_log=True,\n                       jars=None,\n                       spark_conf=None):\n    """"""\n    Create a SparkContext with Zoo configuration on Yarn cluster on ""Yarn-client"" mode.\n    You should create a conda env and install the python dependencies in that env.\n    Conda env and the python dependencies only need to be installed in the driver machine.\n    It\'s not necessary create and install those on the whole yarn cluster.\n\n    :param hadoop_conf: path to the yarn configuration folder.\n    :param conda_name: Name of the conda env.\n    :param num_executor: Number of the Executors.\n    :param executor_cores: Cores for each Executor.\n    :param executor_memory: Memory for each Executor.\n    :param driver_memory: Memory for the Driver.\n    :param driver_cores: Number of cores for the Driver.\n    :param extra_executor_memory_for_ray: Memory size for the Ray services.\n    :param extra_python_lib:\n    :param penv_archive: Ideally, program would auto-pack the conda env which is specified by\n           `conda_name`, but you can also pass the path to a packed file in ""tar.gz"" format here.\n    :param additional_archive: comma seperated additional archives that you want to upload and\n            unpack on executor\n    :param hadoop_user_name: User name for running in yarn cluster. Default value is: root\n    :param spark_yarn_archive conf value for spark.yarn.archive\n    :param spark_log_level: Log level of Spark\n    :param redirect_spark_log: Direct the Spark log to local file or not.\n    :param jars: Comma-separated list of jars to include on the driver and executor classpaths.\n    :param spark_conf: You can append extra spark conf here in key value format.\n                       i.e spark_conf={""spark.executor.extraJavaOptions"": ""-XX:+PrintGCDetails""}\n    :return: SparkContext\n    """"""\n    from zoo.util.spark import SparkRunner\n    sparkrunner = SparkRunner(spark_log_level=spark_log_level,\n                              redirect_spark_log=redirect_spark_log)\n    sc = sparkrunner.init_spark_on_yarn(\n        hadoop_conf=hadoop_conf,\n        conda_name=conda_name,\n        num_executor=num_executor,\n        executor_cores=executor_cores,\n        executor_memory=executor_memory,\n        driver_memory=driver_memory,\n        driver_cores=driver_cores,\n        extra_executor_memory_for_ray=extra_executor_memory_for_ray,\n        extra_python_lib=extra_python_lib,\n        penv_archive=penv_archive,\n        additional_archive=additional_archive,\n        hadoop_user_name=hadoop_user_name,\n        spark_yarn_archive=spark_yarn_archive,\n        jars=jars,\n        spark_conf=spark_conf)\n    return sc\n\n\ndef init_nncontext(conf=None, redirect_spark_log=True):\n    """"""\n    Creates or gets a SparkContext with optimized configuration for BigDL performance.\n    The method will also initialize the BigDL engine.\n\n    Note: if you use spark-shell or Jupyter notebook, as the Spark context is created\n    before your code, you have to set Spark conf values through command line options\n    or properties file, and init BigDL engine manually.\n\n    :param conf: User defined Spark conf\n    """"""\n    if isinstance(conf, six.string_types):\n        sc = getOrCreateSparkContext(conf=None, appName=conf)\n    else:\n        sc = getOrCreateSparkContext(conf=conf)\n    check_version()\n    if redirect_spark_log:\n        redire_spark_logs()\n        show_bigdl_info_logs()\n    init_engine()\n    return sc\n\n\ndef getOrCreateSparkContext(conf=None, appName=None):\n    """"""\n    Get the current active spark context and create one if no active instance\n    :param conf: combining bigdl configs into spark conf\n    :return: SparkContext\n    """"""\n    with SparkContext._lock:\n        if SparkContext._active_spark_context is None:\n            spark_conf = init_spark_conf() if conf is None else conf\n            if appName:\n                spark_conf.setAppName(appName)\n            return SparkContext.getOrCreate(spark_conf)\n        else:\n            return SparkContext.getOrCreate()\n\n\ndef get_analytics_zoo_conf():\n    zoo_conf_file = ""spark-analytics-zoo.conf""\n    zoo_python_wrapper = ""python-api.zip""\n\n    for p in sys.path:\n        if zoo_conf_file in p and os.path.isfile(p):\n            with open(p) if sys.version_info < (3,) else open(p, encoding=\'latin-1\') as conf_file:\n                return load_conf(conf_file.read())\n        if zoo_python_wrapper in p and os.path.isfile(p):\n            import zipfile\n            with zipfile.ZipFile(p, \'r\') as zip_conf:\n                if zoo_conf_file in zip_conf.namelist():\n                    content = zip_conf.read(zoo_conf_file)\n                    if sys.version_info >= (3,):\n                        content = str(content, \'latin-1\')\n                    return load_conf(content)\n    return {}\n\n\ndef init_env(conf):\n    # Default env\n    kmp_affinity = ""granularity=fine,compact,1,0""\n    kmp_settings = ""1""\n    omp_num_threads = ""1""\n    kmp_blocktime = ""0""\n\n    # Check env and override if necessary\n    # Currently, focused on ZOO_NUM_MKLTHREADS,\n    # OMP_NUM_THREADS, KMP_BLOCKTIME, KMP_AFFINITY\n    # and KMP_SETTINGS\n    if ""KMP_AFFINITY"" in os.environ:\n        kmp_affinity = os.environ[""KMP_AFFINITY""]\n    if ""KMP_SETTINGS"" in os.environ:\n        kmp_settings = os.environ[""KMP_SETTINGS""]\n    if ""ZOO_NUM_MKLTHREADS"" in os.environ:\n        if os.environ[""ZOO_NUM_MKLTHREADS""].lower() == ""all"":\n            omp_num_threads = conf.get(\'spark.executor.cores\', str(multiprocessing.cpu_count()))\n        else:\n            omp_num_threads = os.environ[""ZOO_NUM_MKLTHREADS""]\n    elif ""OMP_NUM_THREADS"" in os.environ:\n        omp_num_threads = os.environ[""OMP_NUM_THREADS""]\n    if ""KMP_BLOCKTIME"" in os.environ:\n        kmp_blocktime = os.environ[""KMP_BLOCKTIME""]\n\n    # Set env\n    conf.set(""spark.executorEnv.KMP_AFFINITY"", kmp_affinity)\n    conf.set(""spark.executorEnv.KMP_SETTINGS"", kmp_settings)\n    conf.set(""spark.executorEnv.KMP_BLOCKTIME"", kmp_blocktime)\n    conf.set(""spark.executorEnv.OMP_NUM_THREADS"", omp_num_threads)\n    os.environ[""KMP_AFFINITY""] = kmp_affinity\n    os.environ[""KMP_SETTINGS""] = kmp_settings\n    os.environ[""OMP_NUM_THREADS""] = omp_num_threads\n    os.environ[""KMP_BLOCKTIME""] = kmp_blocktime\n\n\ndef init_spark_conf(conf=None):\n    spark_conf = SparkConf()\n    if conf:\n        spark_conf.setAll(conf.items())\n    init_env(spark_conf)\n    zoo_conf = get_analytics_zoo_conf()\n    # Set bigDL and TF conf\n\n    spark_conf.setAll(zoo_conf.items())\n    if os.environ.get(""BIGDL_JARS"", None) and not is_spark_below_2_2():\n        for jar in os.environ[""BIGDL_JARS""].split("":""):\n            extend_spark_driver_cp(spark_conf, jar)\n\n    # add content in PYSPARK_FILES in spark.submit.pyFiles\n    # This is a workaround for current Spark on k8s\n    python_lib = os.environ.get(\'PYSPARK_FILES\', None)\n    if python_lib:\n        existing_py_files = spark_conf.get(""spark.submit.pyFiles"")\n        if existing_py_files:\n            spark_conf.set(key=""spark.submit.pyFiles"",\n                           value=""%s,%s"" % (python_lib, existing_py_files))\n        else:\n            spark_conf.set(key=""spark.submit.pyFiles"", value=python_lib)\n\n    return spark_conf\n\n\ndef check_version():\n    sc = getOrCreateSparkContext()\n    conf = sc._conf\n    if conf.get(""spark.analytics.zoo.versionCheck"", ""False"").lower() == ""true"":\n        report_warn = conf.get(\n            ""spark.analytics.zoo.versionCheck.warning"", ""False"").lower() == ""true""\n        _check_spark_version(sc, report_warn)\n\n\ndef _split_full_version(version):\n    parts = version.split(""."")\n    major = parts[0]\n    feature = parts[1]\n    maintenance = parts[2]\n    return (major, feature, maintenance)\n\n\ndef _check_spark_version(sc, report_warn):\n    version_info = _get_bigdl_verion_conf()\n    (c_major, c_feature, c_maintenance) = _split_full_version(version_info[\'spark_version\'])\n    (r_major, r_feature, r_maintenance) = _split_full_version(sc.version)\n    error_message = \\\n        """"""\n        The compile time spark version is not compatible with the spark runtime version.\n        Compile time version is %s, runtime version is %s. If you want bypass this check,\n        please set spark.analytics.zoo.versionCheck to false, and if you want to only report\n        an warning message, please set spark.analytics.zoo.versionCheck.warning to true.\n        """""" % (version_info[\'spark_version\'], sc.version)\n    if c_major != r_major:\n        if not report_warn:\n            print(""***************************Usage Error*****************************"")\n            print(error_message)\n            raise RuntimeError(error_message)\n        else:\n            warnings.warn(error_message)\n    elif not (c_maintenance == r_maintenance and c_feature == r_feature):\n        warnings.warn(""The compile time spark version may not compatible with "" +\n                      ""the Spark runtime version. "" +\n                      ""Compile time version is %s, "" % version_info[\'spark_version\'] +\n                      ""runtime version is %s"" % sc.version)\n\n\ndef _get_bigdl_verion_conf():\n    bigdl_build_file = ""zoo-version-info.properties""\n    bigdl_python_wrapper = ""python-api.zip""\n\n    for p in sys.path:\n        if bigdl_build_file in p and os.path.isfile(p):\n            with open(p) if sys.version_info < (3,) else open(p, encoding=\'latin-1\') as conf_file:\n                return load_conf(conf_file.read(), ""="")\n        if bigdl_python_wrapper in p and os.path.isfile(p):\n            import zipfile\n            with zipfile.ZipFile(p, \'r\') as zip_conf:\n                if bigdl_build_file in zip_conf.namelist():\n                    content = zip_conf.read(bigdl_build_file)\n                    if sys.version_info >= (3,):\n                        content = str(content, \'latin-1\')\n                    return load_conf(content, ""="")\n    raise RuntimeError(""Error while locating file zoo-version-info.properties, "" +\n                       ""please make sure the mvn generate-resources phase"" +\n                       "" is executed and a zoo-version-info.properties file"" +\n                       "" is located in zoo/target/extra-resources"")\n\n\ndef load_conf(conf_str, split_char=None):\n    return dict(line.split(split_char) for line in conf_str.split(""\\n"") if\n                ""#"" not in line and line.strip())\n'"
pyzoo/zoo/common/utils.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom bigdl.util.common import Sample as BSample, JTensor as BJTensor,\\\n    JavaCreator, _get_gateway, _java2py, _py2java\nimport numpy as np\nimport os\nimport tempfile\nimport uuid\nimport shutil\n\nfrom urllib.parse import urlparse\n\n\ndef convert_to_safe_path(input_path, follow_symlinks=True):\n    # resolves symbolic links\n    if follow_symlinks:\n        return os.path.realpath(input_path)\n    # covert to abs path\n    return os.path.abspath(input_path)\n\n\ndef to_list_of_numpy(elements):\n    if isinstance(elements, np.ndarray):\n        return [elements]\n    elif np.isscalar(elements):\n        return [np.array(elements)]\n    elif not isinstance(elements, list):\n        raise ValueError(""Wrong type: %s"" % type(elements))\n\n    results = []\n    for element in elements:\n        if np.isscalar(element):\n            results.append(np.array(element))\n        elif isinstance(element, np.ndarray):\n            results.append(element)\n        else:\n            raise ValueError(""Wrong type: %s"" % type(element))\n\n    return results\n\n\ndef get_file_list(path, recursive=False):\n    return callZooFunc(""float"", ""listPaths"", path, recursive)\n\n\ndef is_local_path(path):\n    parse_result = urlparse(path)\n    return len(parse_result.scheme.lower()) == 0 or parse_result.scheme.lower() == ""file""\n\n\ndef append_suffix(prefix, path):\n    # append suffix\n    splits = path.split(""."")\n    if len(splits) > 0:\n        file_name = prefix + ""."" + splits[-1]\n    else:\n        file_name = prefix\n\n    return file_name\n\n\ndef save_file(save_func, path):\n\n    if is_local_path(path):\n        save_func(path)\n    else:\n        file_name = str(uuid.uuid1())\n        file_name = append_suffix(file_name, path)\n        temp_path = os.path.join(tempfile.gettempdir(), file_name)\n\n        try:\n            save_func(temp_path)\n            put_local_file_to_remote(temp_path, path)\n        finally:\n            os.remove(temp_path)\n\n\ndef load_from_file(load_func, path):\n    if is_local_path(path):\n        return load_func(path)\n    else:\n        file_name = str(uuid.uuid1())\n        file_name = append_suffix(file_name, path)\n        temp_path = os.path.join(tempfile.gettempdir(), file_name)\n        get_remote_file_to_local(path, temp_path)\n        try:\n            return load_func(temp_path)\n        finally:\n            os.remove(temp_path)\n\n\ndef get_remote_file_to_local(remote_path, local_path, over_write=False):\n    callZooFunc(""float"", ""getRemoteFileToLocal"", remote_path, local_path, over_write)\n\n\ndef put_local_file_to_remote(local_path, remote_path, over_write=False):\n    callZooFunc(""float"", ""putLocalFileToRemote"", local_path, remote_path, over_write)\n\n\ndef set_core_number(num):\n    callZooFunc(""float"", ""setCoreNumber"", num)\n\n\ndef callZooFunc(bigdl_type, name, *args):\n    """""" Call API in PythonBigDL """"""\n    gateway = _get_gateway()\n    args = [_py2java(gateway, a) for a in args]\n    error = Exception(""Cannot find function: %s"" % name)\n    for jinvoker in JavaCreator.instance(bigdl_type, gateway).value:\n        # hasattr(jinvoker, name) always return true here,\n        # so you need to invoke the method to check if it exist or not\n        try:\n            api = getattr(jinvoker, name)\n            java_result = api(*args)\n            result = _java2py(gateway, java_result)\n        except Exception as e:\n            error = e\n            if not (""does not exist"" in str(e)\n                    and ""Method {}"".format(name) in str(e)):\n                raise e\n        else:\n            return result\n    raise error\n\n\nclass JTensor(BJTensor):\n\n    def __init__(self, storage, shape, bigdl_type=""float"", indices=None):\n        super(JTensor, self).__init__(storage, shape, bigdl_type, indices)\n\n    @classmethod\n    def from_ndarray(cls, a_ndarray, bigdl_type=""float""):\n        """"""\n        Convert a ndarray to a DenseTensor which would be used in Java side.\n        """"""\n        if a_ndarray is None:\n            return None\n        assert isinstance(a_ndarray, np.ndarray), \\\n            ""input should be a np.ndarray, not %s"" % type(a_ndarray)\n        return cls(a_ndarray,\n                   a_ndarray.shape,\n                   bigdl_type)\n\n\nclass Sample(BSample):\n\n    def __init__(self, features, labels, bigdl_type=""float""):\n        super(Sample, self).__init__(features, labels, bigdl_type)\n\n    @classmethod\n    def from_ndarray(cls, features, labels, bigdl_type=""float""):\n        features = to_list_of_numpy(features)\n        labels = to_list_of_numpy(labels)\n        return cls(\n            features=[JTensor(feature, feature.shape) for feature in features],\n            labels=[JTensor(label, label.shape) for label in labels],\n            bigdl_type=bigdl_type)\n'"
pyzoo/zoo/examples/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/feature/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/feature/common.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.util.common import *\nfrom zoo.common.utils import callZooFunc\nfrom bigdl.dataset.dataset import DataSet\nfrom pyspark.serializers import CloudPickleSerializer\nimport sys\nimport math\nimport warnings\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass Relation(object):\n    """"""\n    It represents the relationship between two items.\n    """"""\n\n    def __init__(self, id1, id2, label, bigdl_type=""float""):\n        self.id1 = id1\n        self.id2 = id2\n        self.label = int(label)\n        self.bigdl_type = bigdl_type\n\n    def __reduce__(self):\n        return Relation, (self.id1, self.id2, self.label)\n\n    def __str__(self):\n        return ""Relation [id1: %s, id2: %s, label: %s]"" % (\n            self.id1, self.id2, self.label)\n\n    def to_tuple(self):\n        return self.id1, self.id2, self.label\n\n\nclass Relations(object):\n    @staticmethod\n    def read(path, sc=None, min_partitions=1, bigdl_type=""float""):\n        """"""\n        Read relations from csv or txt file.\n        Each record is supposed to contain the following three fields in order:\n        id1(string), id2(string) and label(int).\n\n        For csv file, it should be without header.\n        For txt file, each line should contain one record with fields separated by comma.\n\n        :param path: The path to the relations file, which can either be a local or disrtibuted file\n                     system (such as HDFS) path.\n        :param sc: An instance of SparkContext.\n                   If specified, return RDD of Relation.\n                   Default is None and in this case return list of Relation.\n        :param min_partitions: Int. A suggestion value of the minimal partition number for input\n                               texts. Only need to specify this when sc is not None. Default is 1.\n        """"""\n        if sc:\n            jvalue = callZooFunc(bigdl_type, ""readRelations"", path, sc, min_partitions)\n            res = jvalue.map(lambda x: Relation(str(x[0]), str(x[1]), int(x[2])))\n        else:\n            jvalue = callZooFunc(bigdl_type, ""readRelations"", path)\n            res = [Relation(str(x[0]), str(x[1]), int(x[2])) for x in jvalue]\n        return res\n\n    @staticmethod\n    def read_parquet(path, sc, bigdl_type=""float""):\n        """"""\n        Read relations from parquet file.\n        Schema should be the following:\n        ""id1""(string), ""id2""(string) and ""label""(int).\n\n        :param path: The path to the parquet file.\n        :param sc: An instance of SparkContext.\n        :return: RDD of Relation.\n        """"""\n        jvalue = callZooFunc(bigdl_type, ""readRelationsParquet"", path, sc)\n        return jvalue.map(lambda x: Relation(str(x[0]), str(x[1]), int(x[2])))\n\n\nclass Preprocessing(JavaValue):\n    """"""\n    Preprocessing defines data transform action during feature preprocessing. Python wrapper for\n    the scala Preprocessing\n    """"""\n\n    def __init__(self, bigdl_type=""float"", *args):\n        self.bigdl_type = bigdl_type\n        self.value = callZooFunc(bigdl_type, JavaValue.jvm_class_constructor(self), *args)\n\n    def __call__(self, input):\n        """"""\n        Transform ImageSet or TextSet.\n        """"""\n        # move the import here to break circular import\n        if ""zoo.feature.image.imageset.ImageSet"" not in sys.modules:\n            from zoo.feature.image import ImageSet\n        if ""zoo.feature.text.text_set.TextSet"" not in sys.modules:\n            from zoo.feature.text import TextSet\n        # if type(input) is ImageSet:\n        if isinstance(input, ImageSet):\n            jset = callZooFunc(self.bigdl_type, ""transformImageSet"", self.value, input)\n            return ImageSet(jvalue=jset)\n        elif isinstance(input, TextSet):\n            jset = callZooFunc(self.bigdl_type, ""transformTextSet"", self.value, input)\n            return TextSet(jvalue=jset)\n\n\nclass ChainedPreprocessing(Preprocessing):\n    """"""\n    chains two Preprocessing together. The output type of the first\n    Preprocessing should be the same with the input type of the second Preprocessing.\n    """"""\n\n    def __init__(self, transformers, bigdl_type=""float""):\n        for transfomer in transformers:\n            assert isinstance(transfomer, Preprocessing), \\\n                str(transfomer) + "" should be subclass of Preprocessing ""\n\n        super(ChainedPreprocessing, self).__init__(bigdl_type, transformers)\n\n\nclass ScalarToTensor(Preprocessing):\n    """"""\n    a Preprocessing that converts a number to a Tensor.\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        super(ScalarToTensor, self).__init__(bigdl_type)\n\n\nclass SeqToTensor(Preprocessing):\n    """"""\n    a Transformer that converts an Array[_] or Seq[_] to a Tensor.\n    :param size dimensions of target Tensor.\n    """"""\n\n    def __init__(self, size=[], bigdl_type=""float""):\n        super(SeqToTensor, self).__init__(bigdl_type, size)\n\n\nclass SeqToMultipleTensors(Preprocessing):\n    """"""\n    a Transformer that converts an Array[_] or Seq[_] or ML Vector to several tensors.\n    :param size, list of int list, dimensions of target Tensors, e.g. [[2],[4]]\n    """"""\n\n    def __init__(self, size=[], bigdl_type=""float""):\n        super(SeqToMultipleTensors, self).__init__(bigdl_type, size)\n\n\nclass ArrayToTensor(Preprocessing):\n    """"""\n    a Transformer that converts an Array[_] to a Tensor.\n    :param size dimensions of target Tensor.\n    """"""\n\n    def __init__(self, size, bigdl_type=""float""):\n        super(ArrayToTensor, self).__init__(bigdl_type, size)\n\n\nclass MLlibVectorToTensor(Preprocessing):\n    """"""\n    a Transformer that converts MLlib Vector to a Tensor.\n    .. note:: Deprecated in 0.4.0. NNEstimator will automatically extract Vectors now.\n    :param size dimensions of target Tensor.\n    """"""\n\n    def __init__(self, size, bigdl_type=""float""):\n        super(MLlibVectorToTensor, self).__init__(bigdl_type, size)\n\n\nclass FeatureLabelPreprocessing(Preprocessing):\n    """"""\n    construct a Transformer that convert (Feature, Label) tuple to a Sample.\n    The returned Transformer is robust for the case label = null, in which the\n    Sample is derived from Feature only.\n    :param feature_transformer transformer for feature, transform F to Tensor[T]\n    :param label_transformer transformer for label, transform L to Tensor[T]\n    """"""\n\n    def __init__(self, feature_transformer, label_transformer, bigdl_type=""float""):\n        super(FeatureLabelPreprocessing, self).__init__(bigdl_type,\n                                                        feature_transformer, label_transformer)\n\n\nclass TensorToSample(Preprocessing):\n    """"""\n     a Transformer that converts Tensor to Sample.\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        super(TensorToSample, self).__init__(bigdl_type)\n\n\nclass FeatureToTupleAdapter(Preprocessing):\n    def __init__(self, sample_transformer, bigdl_type=""float""):\n        super(FeatureToTupleAdapter, self).__init__(bigdl_type, sample_transformer)\n\n\nclass BigDLAdapter(Preprocessing):\n    def __init__(self, bigdl_transformer, bigdl_type=""float""):\n        super(BigDLAdapter, self).__init__(bigdl_type, bigdl_transformer)\n\n\nclass ToTuple(Preprocessing):\n    """"""\n     a Transformer that converts Feature to (Feature, None).\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        super(ToTuple, self).__init__(bigdl_type)\n\n\n# todo support padding param\nclass SampleToMiniBatch(Preprocessing):\n    """"""\n     a Transformer that converts Feature to (Feature, None).\n    """"""\n\n    def __init__(self,\n                 batch_size,\n                 bigdl_type=""float""):\n        super(SampleToMiniBatch, self).__init__(bigdl_type, batch_size)\n\n\nclass FeatureSet(DataSet):\n    """"""\n    A set of data which is used in the model optimization process. The FeatureSet can be accessed in\n    a random data sample sequence. In the training process, the data sequence is a looped endless\n    sequence. While in the validation process, the data sequence is a limited length sequence.\n    Different from BigDL\'s DataSet, this FeatureSet could be cached to Intel Optane DC Persistent\n    Memory, if you set memory_type to PMEM when creating FeatureSet.\n    """"""\n\n    def __init__(self, jvalue=None, bigdl_type=""float""):\n        self.bigdl_type = bigdl_type\n        if jvalue:\n            self.value = jvalue\n\n    @classmethod\n    def image_frame(cls, image_frame, memory_type=""DRAM"",\n                    sequential_order=False,\n                    shuffle=True, bigdl_type=""float""):\n        """"""\n        Create FeatureSet from ImageFrame.\n        :param image_frame: ImageFrame\n        :param memory_type: string, DRAM, PMEM or a Int number.\n                            If it\'s DRAM, will cache dataset into dynamic random-access memory\n                            If it\'s PMEM, will cache dataset into Intel Optane DC Persistent Memory\n                            If it\'s a Int number n, will cache dataset into disk, and only hold 1/n\n                              of the data into memory during the training. After going through the\n                              1/n, we will release the current cache, and load another 1/n into\n                              memory.\n        :param sequential_order: whether to iterate the elements in the feature set\n                                 in sequential order for training.\n        :param shuffle: whether to shuffle the elements in each partition before each epoch\n                        when training\n        :param bigdl_type: numeric type\n        :return: A feature set\n        """"""\n        jvalue = callZooFunc(bigdl_type, ""createFeatureSetFromImageFrame"",\n                             image_frame, memory_type, sequential_order, shuffle)\n        return cls(jvalue=jvalue)\n\n    @classmethod\n    def image_set(cls, imageset, memory_type=""DRAM"",\n                  sequential_order=False,\n                  shuffle=True, bigdl_type=""float""):\n        """"""\n        Create FeatureSet from ImageFrame.\n        :param imageset: ImageSet\n        :param memory_type: string, DRAM or PMEM\n                            If it\'s DRAM, will cache dataset into dynamic random-access memory\n                            If it\'s PMEM, will cache dataset into Intel Optane DC Persistent Memory\n                            If it\'s a Int number n, will cache dataset into disk, and only hold 1/n\n                              of the data into memory during the training. After going through the\n                              1/n, we will release the current cache, and load another 1/n into\n                              memory.\n        :param sequential_order: whether to iterate the elements in the feature set\n                                 in sequential order for training.\n        :param shuffle: whether to shuffle the elements in each partition before each epoch\n                        when training\n        :param bigdl_type: numeric type\n        :return: A feature set\n        """"""\n        jvalue = callZooFunc(bigdl_type, ""createFeatureSetFromImageFrame"",\n                             imageset.to_image_frame(), memory_type,\n                             sequential_order, shuffle)\n        return cls(jvalue=jvalue)\n\n    @classmethod\n    def sample_rdd(cls, rdd, memory_type=""DRAM"",\n                   sequential_order=False,\n                   shuffle=True, bigdl_type=""float""):\n        """"""\n        Create FeatureSet from RDD[Sample].\n        :param rdd: A RDD[Sample]\n        :param memory_type: string, DRAM or PMEM\n                            If it\'s DRAM, will cache dataset into dynamic random-access memory\n                            If it\'s PMEM, will cache dataset into Intel Optane DC Persistent Memory\n                            If it\'s a Int number n, will cache dataset into disk, and only hold 1/n\n                              of the data into memory during the training. After going through the\n                              1/n, we will release the current cache, and load another 1/n into\n                              memory.\n        :param sequential_order: whether to iterate the elements in the feature set\n                                 in sequential order when training.\n        :param shuffle: whether to shuffle the elements in each partition before each epoch\n                        when training\n        :param bigdl_type:numeric type\n        :return: A feature set\n        """"""\n        jvalue = callZooFunc(bigdl_type, ""createSampleFeatureSetFromRDD"", rdd,\n                             memory_type, sequential_order, shuffle)\n        return cls(jvalue=jvalue)\n\n    @classmethod\n    def rdd(cls, rdd, memory_type=""DRAM"", sequential_order=False,\n            shuffle=True, bigdl_type=""float""):\n        """"""\n        Create FeatureSet from RDD.\n        :param rdd: A RDD\n        :param memory_type: string, DRAM, PMEM or a Int number.\n                            If it\'s DRAM, will cache dataset into dynamic random-access memory\n                            If it\'s PMEM, will cache dataset into Intel Optane DC Persistent Memory\n                            If it\'s a Int number n, will cache dataset into disk, and only hold 1/n\n                              of the data into memory during the training. After going through the\n                              1/n, we will release the current cache, and load another 1/n into\n                              memory.\n        :param sequential_order: whether to iterate the elements in the feature set\n                                 in sequential order when training.\n        :param shuffle: whether to shuffle the elements in each partition before each epoch\n                        when training\n        :param bigdl_type:numeric type\n        :return: A feature set\n        """"""\n        jvalue = callZooFunc(bigdl_type, ""createFeatureSetFromRDD"", rdd,\n                             memory_type, sequential_order, shuffle)\n        return cls(jvalue=jvalue)\n\n    @classmethod\n    def tf_dataset(cls, func, total_size, bigdl_type=""float""):\n        """"""\n        :param func: a function return a tensorflow dataset\n        :param total_size: total size of this dataset\n        :param bigdl_type: numeric type\n        :return: A feature set\n        """"""\n        func = CloudPickleSerializer.dumps(CloudPickleSerializer, func)\n        jvalue = callZooFunc(bigdl_type, ""createFeatureSetFromTfDataset"", func, total_size)\n        return cls(jvalue=jvalue)\n\n    @classmethod\n    def pytorch_dataloader(cls, dataloader, bigdl_type=""float""):\n        """"""\n        Create FeatureSet from pytorch dataloader\n        :param dataloader: a pytorch dataloader\n        :param bigdl_type: numeric type\n        :return: A feature set\n        """"""\n        node_num, core_num = get_node_and_core_number()\n        if dataloader.batch_size % node_num != 0:\n            true_bs = math.ceil(dataloader.batch_size / node_num) * node_num\n            warning_msg = ""Detect dataloader\'s batch_size is not divisible by node number("" + \\\n                          node_num + ""), will adjust batch_size to "" + true_bs + "" automatically""\n            warnings.warn(warning_msg)\n\n        bys = CloudPickleSerializer.dumps(CloudPickleSerializer, dataloader)\n        jvalue = callZooFunc(bigdl_type, ""createFeatureSetFromPyTorch"", bys)\n        return cls(jvalue=jvalue)\n\n    def transform(self, transformer):\n        """"""\n        Helper function to transform the data type in the data set.\n        :param transformer: the transformers to transform this feature set.\n        :return: A feature set\n        """"""\n        jvalue = callZooFunc(self.bigdl_type, ""transformFeatureSet"", self.value, transformer)\n        return FeatureSet(jvalue=jvalue)\n\n    def to_dataset(self):\n        """"""\n        To BigDL compatible DataSet\n        :return:\n        """"""\n        jvalue = callZooFunc(self.bigdl_type, ""featureSetToDataSet"", self.value)\n        return FeatureSet(jvalue=jvalue)\n'"
pyzoo/zoo/models/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/orca/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/pipeline/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/ray/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .raycontext import RayContext\n'"
pyzoo/zoo/ray/process.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport subprocess\nimport signal\nimport atexit\nimport sys\nimport psutil\n\nfrom zoo.ray.utils import gen_shutdown_per_node, is_local\n\n\nclass ProcessInfo(object):\n    def __init__(self, out, err, errorcode, pgid, tag=""default"", pids=None, node_ip=None):\n        self.out = str(out.strip())\n        self.err = str(err.strip())\n        self.pgid = pgid\n        self.pids = pids\n        self.errorcode = errorcode\n        self.tag = tag\n        self.master_addr = None\n        self.node_ip = node_ip\n\n    def __str__(self):\n        return ""node_ip: {} tag: {}, pgid: {}, pids: {}, returncode: {}, \\\n                master_addr: {},  \\n {} {}"".format(self.node_ip, self.tag, self.pgid,\n                                                   self.pids,\n                                                   self.errorcode,\n                                                   self.master_addr,\n                                                   self.out,\n                                                   self.err)\n\n\ndef pids_from_gpid(gpid):\n    processes = psutil.process_iter()\n    result = []\n    for proc in processes:\n        try:\n            if os.getpgid(proc.pid) == gpid:\n                result.append(proc.pid)\n        except Exception:\n            pass\n    return result\n\n\ndef session_execute(command, env=None, tag=None, fail_fast=False, timeout=120):\n    pro = subprocess.Popen(\n        command,\n        shell=True,\n        env=env,\n        cwd=None,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        preexec_fn=os.setsid)\n    pgid = os.getpgid(pro.pid)\n    out, err = pro.communicate(timeout=timeout)\n    out = out.decode(""utf-8"")\n    err = err.decode(""utf-8"")\n    print(out)\n    print(err)\n    errorcode = pro.returncode\n    if errorcode != 0:\n        if fail_fast:\n            raise Exception(err)\n        print(err)\n    else:\n        print(out)\n    return ProcessInfo(out=out,\n                       err=err,\n                       errorcode=pro.returncode,\n                       pgid=pgid,\n                       pids=pids_from_gpid(pgid),\n                       tag=tag)\n\n\nclass ProcessMonitor:\n    def __init__(self, process_infos, sc, ray_rdd, raycontext, verbose=False):\n        self.sc = sc\n        self.raycontext = raycontext\n        self.verbose = verbose\n        self.ray_rdd = ray_rdd\n        self.master = []\n        self.slaves = []\n        self.pgids = []\n        self.node_ips = []\n        self.process_infos = process_infos\n        for process_info in process_infos:\n            self.pgids.append(process_info.pgid)\n            self.node_ips.append(process_info.node_ip)\n            if process_info.master_addr:\n                self.master.append(process_info)\n            else:\n                self.slaves.append(process_info)\n        ProcessMonitor.register_shutdown_hook(extra_close_fn=self.clean_fn)\n        assert len(self.master) == 1, \\\n            ""We should got 1 master only, but we got {}"".format(len(self.master))\n        self.master = self.master[0]\n        if not is_local(self.sc):\n            self.print_ray_remote_err_out()\n\n    def print_ray_remote_err_out(self):\n        if self.master.errorcode != 0:\n            raise Exception(str(self.master))\n        for slave in self.slaves:\n            if slave.errorcode != 0:\n                raise Exception(str(slave))\n        if self.verbose:\n            print(self.master)\n            for slave in self.slaves:\n                print(slave)\n\n    def clean_fn(self):\n        if self.raycontext.stopped:\n            return\n        import ray\n        ray.shutdown()\n        if not self.sc:\n            print(""WARNING: SparkContext has been stopped before cleaning the Ray resources"")\n        if self.sc and (not is_local(self.sc)):\n            self.ray_rdd.map(gen_shutdown_per_node(self.pgids, self.node_ips)).collect()\n        else:\n            gen_shutdown_per_node(self.pgids, self.node_ips)([])\n\n    @staticmethod\n    def register_shutdown_hook(pgid=None, extra_close_fn=None):\n        def _shutdown():\n            if pgid:\n                gen_shutdown_per_node(pgid)(0)\n            if extra_close_fn:\n                extra_close_fn()\n\n        def _signal_shutdown(_signo, _stack_frame):\n            _shutdown()\n            sys.exit(0)\n\n        atexit.register(_shutdown)\n        signal.signal(signal.SIGTERM, _signal_shutdown)\n        signal.signal(signal.SIGINT, _signal_shutdown)\n'"
pyzoo/zoo/ray/raycontext.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport os\nimport re\nimport random\nimport signal\nimport multiprocessing\n\nfrom pyspark import BarrierTaskContext\nfrom zoo.ray.process import session_execute, ProcessMonitor\nfrom zoo.ray.utils import is_local\nfrom zoo.ray.utils import resource_to_bytes\n\n\nclass JVMGuard:\n    """"""\n    The registered pids would be put into the killing list of Spark Executor.\n    """"""\n    @staticmethod\n    def register_pids(pids):\n        import traceback\n        try:\n            from zoo.common.utils import callZooFunc\n            import zoo\n            callZooFunc(""float"",\n                        ""jvmGuardRegisterPids"",\n                        pids)\n        except Exception as err:\n            print(traceback.format_exc())\n            print(""Cannot sucessfully register pid into JVMGuard"")\n            for pid in pids:\n                os.kill(pid, signal.SIGKILL)\n            raise err\n\n\nclass RayServiceFuncGenerator(object):\n    """"""\n    This should be a pickable class.\n    """"""\n    def _prepare_env(self):\n        modified_env = os.environ.copy()\n        cwd = os.getcwd()\n        modified_env[""PATH""] = ""{}/{}:{}"".format(cwd, ""/"".join(self.python_loc.split(""/"")[:-1]),\n                                                 os.environ[""PATH""])\n        modified_env.pop(""MALLOC_ARENA_MAX"", None)\n        modified_env.pop(""RAY_BACKEND_LOG_LEVEL"", None)\n        # Unset all MKL setting as Analytics Zoo would give default values when init env.\n        # Running different programs may need different configurations.\n        modified_env.pop(""intra_op_parallelism_threads"", None)\n        modified_env.pop(""inter_op_parallelism_threads"", None)\n        modified_env.pop(""OMP_NUM_THREADS"", None)\n        modified_env.pop(""KMP_BLOCKTIME"", None)\n        modified_env.pop(""KMP_AFFINITY"", None)\n        modified_env.pop(""KMP_SETTINGS"", None)\n        if self.env:  # Add in env argument if any MKL setting is needed.\n            modified_env.update(self.env)\n        if self.verbose:\n            print(""Executing with these environment setting:"")\n            for pair in modified_env.items():\n                print(pair)\n            print(""The $PATH is: {}"".format(modified_env[""PATH""]))\n        return modified_env\n\n    def __init__(self, python_loc, redis_port, ray_node_cpu_cores,\n                 password, object_store_memory, verbose=False, env=None,\n                 extra_params=None):\n        """"""object_store_memory: integer in bytes""""""\n        self.env = env\n        self.python_loc = python_loc\n        self.redis_port = redis_port\n        self.password = password\n        self.ray_node_cpu_cores = ray_node_cpu_cores\n        self.ray_exec = self._get_ray_exec()\n        self.object_store_memory = object_store_memory\n        self.extra_params = extra_params\n        self.verbose = verbose\n        # _mxnet_worker and _mxnet_server are resource tags for distributed MXNet training only\n        # in order to diff worker from server.\n        # This is useful to allocate workers and servers in the cluster.\n        # Leave some reserved custom resources free to avoid unknown crash due to resources.\n        self.labels = \\\n            """"""--resources=\'{""_mxnet_worker"": %s, ""_mxnet_server"": %s, ""_reserved"": %s}\' """""" \\\n            % (1, 1, 2)\n\n    def gen_stop(self):\n        def _stop(iter):\n            command = ""{} stop"".format(self.ray_exec)\n            print(""Start to end the ray services: {}"".format(command))\n            session_execute(command=command, fail_fast=True)\n            return iter\n\n        return _stop\n\n    @staticmethod\n    def _enrich_command(command, object_store_memory, extra_params):\n        if object_store_memory:\n            command = command + ""--object-store-memory {} "".format(str(object_store_memory))\n        if extra_params:\n            for pair in extra_params.items():\n                command = command + "" --{} {} "".format(pair[0], pair[1])\n        return command\n\n    def _gen_master_command(self):\n        command = ""{} start --head "" \\\n                  ""--include-webui true --redis-port {} "" \\\n                  ""--redis-password {} --num-cpus {} {}"". \\\n            format(self.ray_exec, self.redis_port, self.password,\n                   self.ray_node_cpu_cores, self.labels)\n        return RayServiceFuncGenerator._enrich_command(command=command,\n                                                       object_store_memory=self.object_store_memory,\n                                                       extra_params=self.extra_params)\n\n    @staticmethod\n    def _get_raylet_command(redis_address,\n                            ray_exec,\n                            password,\n                            ray_node_cpu_cores,\n                            labels="""",\n                            object_store_memory=None,\n                            extra_params=None):\n        command = ""{} start --address {} --redis-password  {} --num-cpus {} {}  "".format(\n            ray_exec, redis_address, password, ray_node_cpu_cores, labels)\n        return RayServiceFuncGenerator._enrich_command(command=command,\n                                                       object_store_memory=object_store_memory,\n                                                       extra_params=extra_params)\n\n    def _start_ray_node(self, command, tag):\n        modified_env = self._prepare_env()\n        print(""Starting {} by running: {}"".format(tag, command))\n        process_info = session_execute(command=command, env=modified_env, tag=tag)\n        JVMGuard.register_pids(process_info.pids)\n        import ray.services as rservices\n        process_info.node_ip = rservices.get_node_ip_address()\n        return process_info\n\n    def _get_ray_exec(self):\n        python_bin_dir = ""/"".join(self.python_loc.split(""/"")[:-1])\n        return ""{}/python {}/ray"".format(python_bin_dir, python_bin_dir)\n\n    def gen_ray_start(self):\n        def _start_ray_services(iter):\n            tc = BarrierTaskContext.get()\n            # The address is sorted by partitionId according to the comments\n            # Partition 0 is the Master\n            task_addrs = [taskInfo.address for taskInfo in tc.getTaskInfos()]\n            print(task_addrs)\n            master_ip = task_addrs[0].split("":"")[0]\n            print(""current address {}"".format(task_addrs[tc.partitionId()]))\n            print(""master address {}"".format(master_ip))\n            redis_address = ""{}:{}"".format(master_ip, self.redis_port)\n            process_info = None\n            if tc.partitionId() == 0:\n                print(""partition id is : {}"".format(tc.partitionId()))\n                process_info = self._start_ray_node(command=self._gen_master_command(),\n                                                    tag=""ray-master"")\n                process_info.master_addr = redis_address\n\n            tc.barrier()\n            if tc.partitionId() != 0:\n                print(""partition id is : {}"".format(tc.partitionId()))\n                process_info = self._start_ray_node(\n                    command=RayServiceFuncGenerator._get_raylet_command(\n                        redis_address=redis_address,\n                        ray_exec=self.ray_exec,\n                        password=self.password,\n                        ray_node_cpu_cores=self.ray_node_cpu_cores,\n                        labels=self.labels,\n                        object_store_memory=self.object_store_memory,\n                        extra_params=self.extra_params),\n                    tag=""raylet"")\n            yield process_info\n        return _start_ray_services\n\n\nclass RayContext(object):\n    _active_ray_context = None\n\n    def __init__(self, sc, redis_port=None, password=""123456"", object_store_memory=None,\n                 verbose=False, env=None, extra_params=None):\n        """"""\n        The RayContext would initiate a ray cluster on top of the configuration of SparkContext.\n        After creating RayContext, call the init method to set up the cluster.\n\n        - For Spark local mode: The total available cores is equal to Spark local cores.\n        - For Spark cluster mode: The number of raylets is equal to number of executors.\n        The number of available cores for each raylet is equal to executor cores.\n\n        :param sc: An instance of SparkContext.\n        :param redis_port: redis port for the ""head"" node.\n        The value would be randomly picked if not specified.\n        :param password: Password for the redis. Default to be ""123456"" if not specified.\n        :param object_store_memory: The memory size for ray object_store in string.\n        This can be specified in bytes(b), kilobytes(k), megabytes(m) or gigabytes(g).\n        For example, 50b, 100k, 250m, 30g.\n        :param verbose: True for more logs when starting ray. Default is False.\n        :param env: The environment variable dict for running ray processes. Default is None.\n        :param extra_params: The key value dict for extra options to launch ray.\n        For example, extra_params={""temp-dir"": ""/tmp/ray/""}\n        """"""\n        assert sc is not None, ""sc cannot be None, please create a SparkContext first""\n        self.sc = sc\n        self.stopped = False\n        self.is_local = is_local(sc)\n        self.verbose = verbose\n        self.redis_password = password\n        self.object_store_memory = resource_to_bytes(object_store_memory)\n        self.ray_processesMonitor = None\n        self.env = env\n        self.extra_params = extra_params\n        self._address_info = None\n        if self.is_local:\n            self.num_ray_nodes = 1\n            self.ray_node_cpu_cores = self._get_spark_local_cores()\n        # For Spark local mode, directly call ray.init() and ray.shutdown().\n        # ray.shutdown() would clear up all the ray related processes.\n        # Ray Manager is only needed for Spark cluster mode to monitor ray processes.\n        else:\n            self.num_ray_nodes = int(self.sc.getConf().get(""spark.executor.instances""))\n            self.ray_node_cpu_cores = int(self.sc.getConf().get(""spark.executor.cores""))\n            self.python_loc = os.environ[\'PYSPARK_PYTHON\']\n            self.redis_port = random.randint(10000, 65535) if not redis_port else redis_port\n            self.ray_service = RayServiceFuncGenerator(\n                python_loc=self.python_loc,\n                redis_port=self.redis_port,\n                ray_node_cpu_cores=self.ray_node_cpu_cores,\n                password=self.redis_password,\n                object_store_memory=self.object_store_memory,\n                verbose=self.verbose,\n                env=self.env,\n                extra_params=self.extra_params)\n            self._gather_cluster_ips()\n            from bigdl.util.common import init_executor_gateway\n            print(""Start to launch the JVM guarding process"")\n            init_executor_gateway(sc)\n            print(""JVM guarding process has been successfully launched"")\n        RayContext._active_ray_context = self\n\n    @classmethod\n    def get(cls):\n        if RayContext._active_ray_context:\n            return RayContext._active_ray_context\n        else:\n            raise Exception(""No active RayContext. Please create a RayContext and init it first"")\n\n    def _gather_cluster_ips(self):\n        total_cores = int(self.num_ray_nodes) * int(self.ray_node_cpu_cores)\n\n        def info_fn(iter):\n            tc = BarrierTaskContext.get()\n            task_addrs = [taskInfo.address.split("":"")[0] for taskInfo in tc.getTaskInfos()]\n            yield task_addrs\n            tc.barrier()\n\n        ips = self.sc.range(0, total_cores,\n                            numSlices=total_cores).barrier().mapPartitions(info_fn).collect()\n        return ips[0]\n\n    def stop(self):\n        if self.stopped:\n            print(""This instance has been stopped."")\n            return\n        import ray\n        ray.shutdown()\n        if not self.is_local:\n            if not self.ray_processesMonitor:\n                print(""Please start the runner first before closing it"")\n            else:\n                self.ray_processesMonitor.clean_fn()\n        self.stopped = True\n\n    def purge(self):\n        """"""\n        Invoke ray stop to clean ray processes.\n        """"""\n        if self.stopped:\n            print(""This instance has been stopped."")\n            return\n        if self.is_local:\n            import ray\n            ray.shutdown()\n        else:\n            self.sc.range(0,\n                          self.num_ray_nodes,\n                          numSlices=self.num_ray_nodes).barrier().mapPartitions(\n                self.ray_service.gen_stop()).collect()\n        self.stopped = True\n\n    def _get_spark_local_cores(self):\n        local_symbol = re.match(r""local\\[(.*)\\]"", self.sc.master).group(1)\n        if local_symbol == ""*"":\n            return multiprocessing.cpu_count()\n        else:\n            return int(local_symbol)\n\n    def init(self, driver_cores=0):\n        """"""\n        Initiate the ray cluster.\n\n        :param driver_cores: The number of cores for the raylet on driver for Spark cluster mode.\n        Default is 0 and in this case the local driver wouldn\'t have any ray workload.\n\n        :return The dictionary of address information about the ray cluster.\n        Information contains node_ip_address, redis_address, object_store_address,\n        raylet_socket_name, webui_url and session_dir.\n        """"""\n        self.stopped = False\n        if self.is_local:\n            if self.env:\n                os.environ.update(self.env)\n            import ray\n            self._address_info = ray.init(num_cpus=self.ray_node_cpu_cores,\n                                          object_store_memory=self.object_store_memory,\n                                          resources=self.extra_params)\n        else:\n            self._start_cluster()\n            self._address_info = self._start_driver(num_cores=driver_cores)\n        return self._address_info\n\n    @property\n    def address_info(self):\n        if self._address_info:\n            return self._address_info\n        else:\n            raise Exception(""Ray cluster hasn\'t been initiated yet. Please call init first"")\n\n    def _start_cluster(self):\n        print(""Start to launch ray on cluster"")\n        ray_rdd = self.sc.range(0, self.num_ray_nodes,\n                                numSlices=self.num_ray_nodes)\n        process_infos = ray_rdd.barrier().mapPartitions(\n            self.ray_service.gen_ray_start()).collect()\n\n        self.ray_processesMonitor = ProcessMonitor(process_infos, self.sc, ray_rdd, self,\n                                                   verbose=self.verbose)\n        self.redis_address = self.ray_processesMonitor.master.master_addr\n        return self\n\n    def _start_restricted_worker(self, num_cores, node_ip_address):\n\n        extra_param = {""node-ip-address"": node_ip_address}\n        if self.extra_params is not None:\n            extra_param.update(self.extra_params)\n        command = RayServiceFuncGenerator._get_raylet_command(\n            redis_address=self.redis_address,\n            ray_exec=""ray "",\n            password=self.redis_password,\n            ray_node_cpu_cores=num_cores,\n            object_store_memory=self.object_store_memory,\n            extra_params=extra_param)\n        print(""Executing command: {}"".format(command))\n        process_info = session_execute(command=command, fail_fast=True)\n        ProcessMonitor.register_shutdown_hook(pgid=process_info.pgid)\n\n    def _start_driver(self, num_cores=0):\n        print(""Start to launch ray driver on local"")\n        import ray.services\n        node_ip = ray.services.get_node_ip_address(self.redis_address)\n        self._start_restricted_worker(num_cores=num_cores,\n                                      node_ip_address=node_ip)\n        ray.shutdown()\n        return ray.init(address=self.redis_address,\n                        redis_password=self.ray_service.password,\n                        node_ip_address=node_ip)\n'"
pyzoo/zoo/ray/utils.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport re\nimport os\nimport signal\n\n\ndef to_list(input):\n    if isinstance(input, (list, tuple)):\n        return list(input)\n    else:\n        return [input]\n\n\ndef resource_to_bytes(resource_str):\n    if not resource_str:\n        return resource_str\n    matched = re.compile(""([0-9]+)([a-z]+)?"").match(resource_str.lower())\n    fraction_matched = re.compile(""([0-9]+\\\\.[0-9]+)([a-z]+)?"").match(resource_str.lower())\n    if fraction_matched:\n        raise Exception(\n            ""Fractional values are not supported. Input was: {}"".format(resource_str))\n    try:\n        value = int(matched.group(1))\n        postfix = matched.group(2)\n        if postfix == \'b\':\n            value = value\n        elif postfix == \'k\':\n            value = value * 1000\n        elif postfix == ""m"":\n            value = value * 1000 * 1000\n        elif postfix == \'g\':\n            value = value * 1000 * 1000 * 1000\n        else:\n            raise Exception(""Not supported type: {}"".format(resource_str))\n        return value\n    except Exception:\n        raise Exception(""Size must be specified as bytes(b),""\n                        ""kilobytes(k), megabytes(m), gigabytes(g). ""\n                        ""E.g. 50b, 100k, 250m, 30g"")\n\n\ndef gen_shutdown_per_node(pgids, node_ips=None):\n    import ray.services as rservices\n    pgids = to_list(pgids)\n\n    def _shutdown_per_node(iter):\n        print(""Stopping pgids: {}"".format(pgids))\n        if node_ips:\n            current_node_ip = rservices.get_node_ip_address()\n            effect_pgids = [pair[0] for pair in zip(pgids, node_ips) if pair[1] == current_node_ip]\n        else:\n            effect_pgids = pgids\n        for pgid in effect_pgids:\n            print(""Stopping by pgid {}"".format(pgid))\n            try:\n                os.killpg(pgid, signal.SIGTERM)\n            except Exception:\n                print(""WARNING: cannot kill pgid: {}"".format(pgid))\n\n    return _shutdown_per_node\n\n\ndef is_local(sc):\n    master = sc.getConf().get(""spark.master"")\n    return master == ""local"" or master.startswith(""local["")\n'"
pyzoo/zoo/serving/client.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport base64\nimport cv2\nimport yaml\nimport redis\nimport time\nimport numpy as np\nimport pyarrow as pa\n\n\nclass API:\n    """"""\n    base level of API control\n    select data pipeline here, Redis/Kafka/...\n    interface preserved for API class\n    """"""\n    def __init__(self):\n\n        try:\n            file_path = ""config.yaml""\n        except Exception:\n            raise EOFError(""config file does not exist. Please check your config""\n                           ""at analytics-zoo/docker/cluster-serving/config.yaml"")\n\n        try:\n            with open(file_path) as f:\n                config = yaml.load(f)\n                if not config[\'data\'][\'src\']:\n                    host_port = [""localhost"", ""6379""]\n                else:\n                    host_port = config[\'data\'][\'src\'].split("":"")\n                config[\'data\'][\'host\'] = host_port[0]\n                config[\'data\'][\'port\'] = host_port[1]\n        except Exception:\n            config = {}\n            config[\'data\'] = {}\n            config[\'data\'][\'host\'], config[\'data\'][\'port\'] = ""localhost"", ""6379""\n            config[\'data\'][\'image_shape\'] = None\n\n        self.db = redis.StrictRedis(host=config[\'data\'][\'host\'],\n                                    port=config[\'data\'][\'port\'], db=0)\n        # self.db = redis.StrictRedis(host=""10.239.47.210"",\n        #                             port=""16380"", db=0)\n        try:\n            self.db.xgroup_create(""image_stream"", ""serving"")\n            self.db.xgroup_create(""tensor_stream"", ""serving"")\n        except Exception:\n            print(""redis group exist, will not create new one"")\n\n        if not config[\'data\'][\'image_shape\']:\n            self.data_shape = [""3"", ""224"", ""224""]\n        else:\n            self.data_shape = config[\'data\'][\'image_shape\'].split("","")\n        for i in range(len(self.data_shape)):\n            self.data_shape[i] = int(self.data_shape[i])\n\n\nclass InputQueue(API):\n    def __init__(self):\n        super().__init__()\n        self.c, self.h, self.w = None, None, None\n        self.stream_name = ""serving_stream""\n\n        # TODO: these params can be read from config in future\n        self.input_threshold = 0.6\n        self.interval_if_error = 1\n        self.data_shape_check()\n\n    def data_shape_check(self):\n        for num in self.data_shape:\n            if num <= 0:\n                raise Exception(""Your image shape config is invalid, ""\n                                ""your config shape is"" + str(self.data_shape)\n                                + ""no negative value is allowed."")\n            if 0 < num < 5:\n                self.c = num\n                continue\n            if not self.h:\n                self.h = num\n            else:\n                self.w = num\n        return\n\n    def enqueue(self, uri, **data):\n        sink = pa.BufferOutputStream()\n        field_list = []\n        data_list = []\n        for key, value in data.items():\n\n            if isinstance(value, str):\n                # str value will be considered as image path\n                field = pa.field(key, pa.string())\n                data = self.encode_image(value)\n                # b = bytes(data, ""utf-8"")\n                data = pa.array([data])\n                # ba = pa.array(b, type=pa.binary())\n                field_list.append(field)\n                data_list.append(data)\n\n            elif isinstance(value, np.ndarray):\n                # ndarray value will be considered as tensor\n                indices_field = pa.field(""indiceData"", pa.list_(pa.int32()))\n                indices_shape_field = pa.field(""indiceShape"", pa.list_(pa.int32()))\n                data_field = pa.field(""data"", pa.list_(pa.float32()))\n                shape_field = pa.field(""shape"", pa.list_(pa.int32()))\n                tensor_type = pa.struct(\n                    [indices_field, indices_shape_field, data_field, shape_field])\n                field = pa.field(key, tensor_type)\n\n                shape = np.array(value.shape)\n                d = value.astype(""float32"").flatten()\n                # data = pa.array([{\'data\': d}, {\'shape\': shape}, {}],\n                #                 type=tensor_type)\n                data = pa.array([{\'indiceData\': []},\n                                 {\'indiceShape\': []},\n                                 {\'data\': d},\n                                 {\'shape\': shape}], type=tensor_type)\n                field_list.append(field)\n                data_list.append(data)\n\n            elif isinstance(value, list):\n                # list will be considered as sparse tensor\n                assert len(value) == 3, ""Sparse Tensor must have list of ndarray"" \\\n                    ""with length 3, which represent indices, values, shape respectively""\n                indices_field = pa.field(""indiceData"", pa.list_(pa.int32()))\n                indices_shape_field = pa.field(""indiceShape"", pa.list_(pa.int32()))\n                value_field = pa.field(""data"", pa.list_(pa.float32()))\n                shape_field = pa.field(""shape"", pa.list_(pa.int32()))\n                sparse_tensor_type = pa.struct(\n                    [indices_field, indices_shape_field,  value_field, shape_field])\n                field = pa.field(key, sparse_tensor_type)\n\n                shape = value[2]\n                values = value[1]\n                indices = value[0].astype(""float32"").flatten()\n                indices_shape = value[0].shape\n                data = pa.array([{\'indiceData\': indices},\n                                 {\'indiceShape\': indices_shape},\n                                 {\'data\': values},\n                                 {\'shape\': shape}], type=sparse_tensor_type)\n                field_list.append(field)\n                data_list.append(data)\n            else:\n                raise TypeError(""Your request does not match any schema, ""\n                                ""please check."")\n        schema = pa.schema(field_list)\n        batch = pa.RecordBatch.from_arrays(\n            data_list, schema)\n\n        writer = pa.RecordBatchStreamWriter(sink, batch.schema)\n        writer.write_batch(batch)\n        writer.close()\n        buf = sink.getvalue()\n        b = buf.to_pybytes()\n        b64str = self.base64_encode_image(b)\n        d = {""uri"": uri, ""data"": b64str}\n        self.__enqueue_data(d)\n\n    def encode_image(self, img):\n        """"""\n        :param id: String you use to identify this record\n        :param data: Data, ndarray type\n        :return:\n        """"""\n        if isinstance(img, str):\n            img = cv2.imread(img)\n            if img.size == 0:\n                print(""You have pushed an image with path: "",\n                      img, ""the path is invalid, skipped."")\n                return\n\n        # force resize here to avoid input image shape inconsistent\n        # if the shape is consistent, it would not affect the data\n        img = cv2.resize(img, (self.h, self.w))\n        data = cv2.imencode("".jpg"", img)[1]\n        img_encoded = self.base64_encode_image(data)\n        return img_encoded\n        # arrow_b64string = pa.array(img_encoded)\n        #\n        # sink = pa.BufferOutputStream()\n        # schema = pa.schema([pa.field(""data"", pa.string())])\n        # batch = pa.RecordBatch.from_arrays(\n        #     [arrow_b64string], schema)\n        # writer = pa.RecordBatchFileWriter(sink, batch.schema)\n        # writer.write_batch(batch)\n        #\n        # d = {""uri"": uri, ""data"": img_encoded}\n        #\n        # self.__enqueue_data(d)\n\n    def enqueue_tensor(self, uri, data):\n        if isinstance(data, np.ndarray):\n            # tensor\n            data = [data]\n        if not isinstance(data, list):\n            raise Exception(""Your input is invalid, only List of ndarray and ndarray are allowed."")\n\n        sink = pa.BufferOutputStream()\n        writer = None\n        for d in data:\n            shape = np.array(d.shape)\n            d = d.astype(""float32"").flatten()\n\n            data_field = pa.field(""data"", pa.list_(pa.float32()))\n            shape_field = pa.field(""shape"", pa.list_(pa.int64()))\n            tensor_type = pa.struct([data_field, shape_field])\n\n            tensor = pa.array([{\'data\': d}, {\'shape\': shape}],\n                              type=tensor_type)\n\n            tensor_field = pa.field(uri, tensor_type)\n            schema = pa.schema([tensor_field])\n\n            batch = pa.RecordBatch.from_arrays(\n                [tensor], schema)\n            if writer is None:\n                # initialize\n                writer = pa.RecordBatchFileWriter(sink, batch.schema)\n            writer.write_batch(batch)\n\n        writer.close()\n        buf = sink.getvalue()\n        b = buf.to_pybytes()\n        tensor_encoded = self.base64_encode_image(b)\n        d = {""uri"": uri, ""data"": tensor_encoded}\n        self.__enqueue_data(d)\n\n    def __enqueue_data(self, data):\n        inf = self.db.info()\n        try:\n            if inf[\'used_memory\'] >= inf[\'maxmemory\'] * self.input_threshold:\n                raise redis.exceptions.ConnectionError\n            self.db.xadd(self.stream_name, data)\n            print(""Write to Redis successful"")\n        except redis.exceptions.ConnectionError:\n            print(""Redis queue is full, please wait for inference ""\n                  ""or delete the unprocessed records."")\n            time.sleep(self.interval_if_error)\n\n        except redis.exceptions.ResponseError as e:\n            print(e, ""Redis memory is full, please dequeue or delete."")\n            time.sleep(self.interval_if_error)\n\n    @staticmethod\n    def base64_encode_image(img):\n        # base64 encode the input NumPy array\n        return base64.b64encode(img).decode(""utf-8"")\n\n\nclass OutputQueue(API):\n    def __init__(self):\n        super().__init__()\n\n    def dequeue(self):\n        res_list = self.db.keys(\'result:*\')\n        decoded = {}\n        for res in res_list:\n            res_dict = self.db.hgetall(res.decode(\'utf-8\'))\n            res_id = res.decode(\'utf-8\').split("":"")[1]\n            res_value = res_dict[b\'value\'].decode(\'utf-8\')\n            decoded[res_id] = res_value\n            self.db.delete(res)\n        return decoded\n\n    def query(self, uri):\n        res_dict = self.db.hgetall(""result:""+uri)\n\n        if not res_dict or len(res_dict) == 0:\n            return ""{}""\n        return res_dict[b\'value\'].decode(\'utf-8\')\n'"
pyzoo/zoo/serving/quick_start.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.serving.client import InputQueue, OutputQueue\nimport os\nimport cv2\nimport json\nimport time\nfrom optparse import OptionParser\n\n\ndef run(path):\n    input_api = InputQueue()\n    base_path = path\n\n    if not base_path:\n        raise EOFError(""You have to set your image path"")\n    output_api = OutputQueue()\n    output_api.dequeue()\n    path = os.listdir(base_path)\n    for p in path:\n        if not p.endswith(""jpeg""):\n            continue\n        img = cv2.imread(os.path.join(base_path, p))\n        img = cv2.resize(img, (224, 224))\n        input_api.enqueue_image(p, img)\n\n    time.sleep(10)\n\n    # get all result and dequeue\n    result = output_api.dequeue()\n    for k in result.keys():\n        output = ""image: "" + k + "", classification-result:""\n        tmp_list = json.loads(result[k])\n        for record in range(len(tmp_list)):\n            output += "" class: "" + str(tmp_list[record][0]) \\\n                      + ""\'s prob: "" + str(tmp_list[record][1])\n        print(output)\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""-i"", ""--image_path"", dest=""path"", default=""test_image"")\n    import sys\n    (options, args) = parser.parse_args(sys.argv)\n    run(options.path)\n'"
pyzoo/zoo/serving/server.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport subprocess\n\nimport shutil\nimport glob\nimport os\nimport urllib.request\nfrom zoo.util.engine import get_analytics_zoo_classpath\n\n\nclass ClusterServing:\n\n    def __init__(self):\n        self.name = \'cluster-serving\'\n        self.proc = None\n        self.conf_path = os.path.abspath(\n            __file__ + ""/../../share/bin/cluster-serving/config.yaml"")\n        self.zoo_jar = \'zoo.jar\'\n        self.bigdl_jar = \'bigdl.jar\'\n        # self.spark_redis_jar = \'spark-redis-2.4.0-jar-with-dependencies.jar\'\n\n        # self.download_spark_redis_jar()\n        self.copy_config()\n\n        self.copy_zoo_jar()\n\n        if not os.path.exists(\'model\'):\n            os.mkdir(\'model\')\n\n    def try_copy_bigdl_jar(self):\n        try:\n            from bigdl.util.engine import get_bigdl_classpath\n            shutil.copyfile(get_bigdl_classpath(), self.bigdl_jar)\n\n        except Exception:\n            print(""WARNING: if you are running Cluster Serving using pip, you have misconfig""\n                  ""with bigdl python package, otherwise, ignore this WARNING."")\n\n    def copy_zoo_jar(self):\n        jar_path = get_analytics_zoo_classpath()\n        if jar_path:\n            self.try_copy_bigdl_jar()\n        else:\n            """"""\n            not install by pip, so run prepare_env here\n            """"""\n            build_jar_paths = glob.glob(os.path.abspath(\n\n                __file__ + ""/../../../../dist/lib/*.jar""))\n            prebuilt_jar_paths = glob.glob(os.path.abspath(\n                __file__ + ""/../../../../*.jar""))\n            jar_paths = build_jar_paths + prebuilt_jar_paths\n\n            assert len(jar_paths) > 0, ""No zoo jar is found""\n            assert len(jar_paths) == 1, ""Expecting one jar: %s"" % len(jar_paths)\n            jar_path = jar_paths[0]\n        shutil.copyfile(jar_path, self.zoo_jar)\n\n    def download_spark_redis_jar(self):\n        if not os.path.exists(self.spark_redis_jar):\n            print(""Downloading spark-redis dependency..."")\n            urllib.request.urlretrieve(\'https://oss.sonatype.org/content/repositories/\'\n                                       \'public/com/redislabs/spark-redis/2.4.0/\'\n                                       + self.spark_redis_jar,\n                                       self.spark_redis_jar)\n        else:\n            print(""spark-redis jar already exist."")\n\n    def copy_config(self):\n        if os.path.exists(""config.yaml""):\n            return\n        print(""Trying to find config file in "", self.conf_path)\n        if not os.path.exists(self.conf_path):\n            print(\'WARNING: Config file does not exist in your pip directory,\'\n                  \'are you sure that you install serving by pip?\')\n            build_conf_path = glob.glob(os.path.abspath(\n                __file__ + ""/../../../../scripts/cluster-serving/config.yaml""))\n            prebuilt_conf_path = glob.glob(os.path.abspath(\n                __file__ + ""/../../../../../bin/cluster-serving/config.yaml""))\n            conf_paths = build_conf_path + prebuilt_conf_path\n\n            assert len(conf_paths) > 0, ""No config file is found""\n            self.conf_path = conf_paths[0]\n            print(""config path is found at "", self.conf_path)\n\n            if not os.path.exists(self.conf_path):\n                raise EOFError(""Can not find your config file."")\n        try:\n            shutil.copyfile(self.conf_path, \'config.yaml\')\n        except Exception as e:\n            print(e)\n            print(""WARNING: An initialized config file already exists."")\n\n        subprocess.Popen([\'chmod\', \'a+x\', self.conf_path])\n'"
pyzoo/zoo/tfpark/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .model import KerasModel\nfrom .estimator import TFEstimator\nfrom .tf_optimizer import TFOptimizer\nfrom .tf_dataset import TFDataset\nfrom .zoo_optimizer import ZooOptimizer\nfrom .tf_predictor import TFPredictor\nfrom .tfnet import TFNet\n'"
pyzoo/zoo/tfpark/estimator.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom tensorflow.python.util import function_utils\nimport tensorflow as tf\n\nfrom bigdl.optim.optimizer import MaxIteration, Loss, TreeNNAccuracy\nfrom zoo.pipeline.api.keras import metrics\nfrom zoo.tfpark.tfnet import TFNet\nfrom zoo.tfpark.tf_optimizer import TFOptimizer\nfrom zoo.tfpark.tf_dataset import TFDataset\n\nfrom zoo.util import nest\nimport six\nimport os\n\n\nclass TFEstimator(object):\n\n    def __init__(self, estimator):\n        """"""\n        :param estimator: a tf.estimator.Estimator, in the estimator\'s model_fn,\n        ZooOptimizer must be used and only ZooOptimizer should be used to derive\n        the train_op.\n        """"""\n\n        self.estimator = estimator\n        self._model_fn = self.estimator.model_fn\n        self._model_dir = self.estimator.model_dir\n        self.config = self.estimator.config\n        self.params = self.estimator.params\n        self.tf_optimizer = None\n\n    @classmethod\n    def from_model_fn(cls, model_fn, model_dir=None, config=None,\n                      params=None, warm_start_from=None):\n        """"""\n        :param model_fn: Model function. Follows the signature:\n\n            * Args:\n\n                * `features`: This is the first item returned from the `input_fn`\n                    passed to `train`, `evaluate`, and `predict`. This should be a\n                    single `tf.Tensor` or `dict` of same.\n                * `labels`: This is the second item returned from the `input_fn`\n                    passed to `train`, `evaluate`, and `predict`. This should be a\n                    single `tf.Tensor` or `dict` of same (for multi-head models).\n                    If mode is `tf.estimator.ModeKeys.PREDICT`, `labels=None` will\n                    be passed. If the `model_fn`\'s signature does not accept\n                    `mode`, the `model_fn` must still be able to handle\n                    `labels=None`.\n                * `mode`: Optional. Specifies if this training, evaluation or\n                    prediction. See `tf.estimator.ModeKeys`.\n                * `params`: Optional `dict` of hyperparameters.  Will receive what\n                    is passed to Estimator in `params` parameter. This allows\n                    to configure Estimators from hyper parameter tuning.\n                * `config`: Optional `estimator.RunConfig` object. Will receive what\n                    is passed to Estimator as its `config` parameter, or a default\n                    value. Allows setting up things in your `model_fn` based on\n                    configuration such as `num_ps_replicas`, or `model_dir`.\n\n            * Returns:\n                `tf.estimator.EstimatorSpec`\n\n            For the train_op in tf.estimator.EstimatorSpec, it derive from and only from\n            `zoo.tfpark.ZooOptimizer`\n        :param model_dir: Directory to save model parameters, graph and etc. This can\n            also be used to load checkpoints from the directory into an estimator to\n            continue training a previously saved model. If `PathLike` object, the\n            path will be resolved. If `None`, the model_dir in `config` will be used\n            if set. If both are set, they must be same. If both are `None`, a\n            temporary directory will be used.\n        :param config: `estimator.RunConfig` configuration object.\n        :param params: `dict` of hyper parameters that will be passed into `model_fn`.\n              Keys are names of parameters, values are basic python types.\n        :param warm_start_from: Optional string filepath to a checkpoint or SavedModel to\n                       warm-start from, or a `tf.estimator.WarmStartSettings`\n                       object to fully configure warm-starting.  If the string\n                       filepath is provided instead of a\n                       `tf.estimator.WarmStartSettings`, then all variables are\n                       warm-started, and it is assumed that vocabularies\n                       and `tf.Tensor` names are unchanged.\n        """"""\n        estimator = tf.estimator.Estimator(model_fn, model_dir=model_dir, config=config,\n                                           params=params, warm_start_from=warm_start_from)\n        return cls(estimator)\n\n    def _call_model_fn(self, features, labels, mode, config):\n        model_fn_args = function_utils.fn_args(self._model_fn)\n        kwargs = {}\n        if \'labels\' in model_fn_args:\n            kwargs[\'labels\'] = labels\n        if \'mode\' in model_fn_args:\n            kwargs[\'mode\'] = mode\n        if \'params\' in model_fn_args:\n            kwargs[\'params\'] = self.params\n        if \'config\' in model_fn_args:\n            kwargs[\'config\'] = config\n\n        model_fn_results = self._model_fn(features=features, **kwargs)\n\n        return model_fn_results\n\n    def train(self, input_fn, steps=None):\n        """"""Trains a model given training data `input_fn`.\n\n        :param input_fn: A function that constructs the input data for evaluation. The\n            function should construct and return one of the following:\n            * A `TFDataset` object, each elements of which is a tuple `(features, labels)`.\n            * A `tf.data.Dataset` object: Outputs of `Dataset` object must be a tuple\n            `(features, labels)` with same constraints as below.\n            * A tuple `(features, labels)`: Where `features` is a `tf.Tensor` or a dictionary\n            of string feature name to `Tensor` and `labels` is a `Tensor` or a\n            dictionary of string label name to `Tensor`. Both `features` and\n            `labels` are consumed by `model_fn`. They should satisfy the expectation\n            of `model_fn` from inputs.\n        :param steps: Number of steps for which to train the model.\n\n        Returns:\n          `self`, for chaining.\n        """"""\n\n        with tf.Graph().as_default() as g:\n            global_step_tensor = self.estimator._create_and_assert_global_step(g)\n            add_step_input = tf.placeholder(dtype=tf.int64, shape=())\n            assign_step = tf.assign_add(global_step_tensor, add_step_input)\n            result = self.estimator._call_input_fn(input_fn, tf.estimator.ModeKeys.TRAIN)\n            if isinstance(result, TFDataset):\n                if not result.has_batch:\n                    raise ValueError(""The batch_size of TFDataset must be "" +\n                                     ""specified when used for training."")\n                spec = self._call_model_fn(result.feature_tensors,\n                                           result.label_tensors,\n                                           tf.estimator.ModeKeys.TRAIN,\n                                           self.config)\n                latest_checkpoint = self.estimator.latest_checkpoint()\n\n                with tf.Session() as sess:\n                    saver = tf.train.Saver()\n                    if latest_checkpoint:\n                        saver.restore(sess, latest_checkpoint)\n                    else:\n                        sess.run(tf.global_variables_initializer())\n\n                    zoo_ckpt_path = os.path.join(self._model_dir, ""analytics-zoo"")\n\n                    opt = TFOptimizer.from_train_op(spec.train_op,\n                                                    spec.loss,\n                                                    sess=sess,\n                                                    dataset=result,\n                                                    model_dir=zoo_ckpt_path)\n\n                    opt.optimize(MaxIteration(steps))\n                    sess.run(assign_step, feed_dict={add_step_input: steps})\n                    final_step = sess.run(global_step_tensor)\n                    model_path = os.path.join(self._model_dir, ""model"")\n                    saver.save(sess, model_path, global_step=final_step)\n                    return self\n\n        return self.estimator.train(input_fn, steps=steps)\n\n    def evaluate(self, input_fn, eval_methods, steps=None, checkpoint_path=None):\n        """"""Evaluates the model given evaluation data `input_fn`.\n\n        :param input_fn: A function that constructs the input data for evaluation. The\n            function should construct and return one of the following:\n            * A `TFDataset` object, each elements of which is a tuple `(features, labels)`.\n            * A `tf.data.Dataset` object: Outputs of `Dataset` object must be a tuple\n            `(features, labels)` with same constraints as below.\n            * A tuple `(features, labels)`: Where `features` is a `tf.Tensor` or a dictionary\n            of string feature name to `Tensor` and `labels` is a `Tensor` or a\n            dictionary of string label name to `Tensor`. Both `features` and\n            `labels` are consumed by `model_fn`. They should satisfy the expectation\n            of `model_fn` from inputs.\n        :param eval_methods: a list of strings to specify the evaluation metrics to\n                            be used in this model\n        :param steps: Number of steps for which to evaluate model.\n        :param checkpoint_path: Path of a specific checkpoint to evaluate. If `None`, the\n            latest checkpoint in `model_dir` is used.  If there are no checkpoints\n            in `model_dir`, evaluation is run with newly initialized `Variables`\n            instead of ones restored from checkpoint.\n\n        Returns:\n          A dict containing the evaluation metrics specified in `model_fn` keyed by\n          name.\n        """"""\n        if not all(isinstance(metric, six.string_types) for metric in eval_methods):\n            raise ValueError(""All metrics should be string types"")\n        from tensorflow_estimator.python.estimator.canned import prediction_keys\n        with tf.Graph().as_default() as g:\n            result = self.estimator._call_input_fn(input_fn, tf.estimator.ModeKeys.EVAL)\n            if isinstance(result, TFDataset):\n                spec = self._call_model_fn(result.feature_tensors,\n                                           result.label_tensors,\n                                           tf.estimator.ModeKeys.PREDICT,\n                                           self.config)\n                latest_checkpoint = self.estimator.latest_checkpoint()\n\n                if latest_checkpoint:\n                    checkpoint_path = latest_checkpoint\n\n                with tf.Session() as sess:\n                    if checkpoint_path:\n                        saver = tf.train.Saver()\n                        saver.restore(sess, checkpoint_path)\n                    else:\n                        sess.run(tf.global_variables_initializer())\n                    inputs = nest.flatten(result._original_tensors[0])\n                    if isinstance(spec.predictions, dict):\n                        if ""mae"" in eval_methods:\n                            outputs = [\n                                spec.predictions[prediction_keys.PredictionKeys.PREDICTIONS]]\n                        else:\n                            outputs = [\n                                spec.predictions[prediction_keys.PredictionKeys.LOGITS]]\n                    else:\n                        outputs = nest.flatten(spec.predictions)\n                        if len(outputs) > 1:\n                            raise Exception(""Evaluate on more than one output is not "" +\n                                            ""supported now"")\n                    tfnet = TFNet.from_session(sess, inputs=inputs, outputs=outputs)\n\n                    if result.batch_per_thread < 0:\n                        batch_size = result.batch_size\n                    else:\n                        batch_size = result.batch_per_thread * result.get_num_partitions()\n\n                    eval_methods = [self._to_bigdl_metric(m) for m in eval_methods]\n                    results = tfnet.evaluate(result, batch_size, eval_methods)\n                    final_result = dict([(r.method, r.result) for r in results])\n                    return final_result\n\n        return self.estimator.evaluate(input_fn, steps, checkpoint_path=checkpoint_path)\n\n    def predict(self, input_fn, predict_keys=None, checkpoint_path=None):\n        """"""Outputs predictions for given features.\n\n        :param input_fn: A function that constructs the features.\n              * A `TFDataset` object, each elements of which is a tuple `(features, None)`.\n              * A `tf.data.Dataset` object: Outputs of `Dataset` object must have\n                same constraints as below.\n              * features: A `tf.Tensor` or a dictionary of string feature name to\n                `Tensor`. features are consumed by `model_fn`. They should satisfy\n                the expectation of `model_fn` from inputs.\n              * A tuple, in which case the first item is extracted as features.\n\n        :param checkpoint_path: Path of a specific checkpoint to predict. If `None`, the\n            latest checkpoint in `model_dir` is used.  If there are no checkpoints\n            in `model_dir`, prediction is run with newly initialized `Variables`\n            instead of ones restored from checkpoint.\n\n\n        Return:\n          Evaluated values of `predictions` tensors.\n\n        """"""\n        with tf.Graph().as_default() as g:\n            result = self.estimator._call_input_fn(input_fn, tf.estimator.ModeKeys.PREDICT)\n            if isinstance(result, TFDataset):\n                spec = self._call_model_fn(result.feature_tensors,\n                                           None,\n                                           tf.estimator.ModeKeys.PREDICT,\n                                           self.config)\n                latest_checkpoint = self.estimator.latest_checkpoint()\n\n                if latest_checkpoint:\n                    checkpoint_path = latest_checkpoint\n\n                with tf.Session() as sess:\n                    if checkpoint_path:\n                        saver = tf.train.Saver()\n                        saver.restore(sess, checkpoint_path)\n                    else:\n                        sess.run(tf.global_variables_initializer())\n                    inputs = nest.flatten(result._original_tensors[0])\n                    if isinstance(spec.predictions, dict) and predict_keys is not None:\n                        outputs = [spec.predictions[key] for key in predict_keys]\n                    else:\n                        outputs = nest.flatten(spec.predictions)\n                    tfnet = TFNet.from_session(sess, inputs=inputs, outputs=outputs)\n                    predictions = tfnet.predict(result.get_prediction_data(), mini_batch=True)\n\n                    # If predictions is a dict, add back the keys and results is a dict as well.\n                    if isinstance(spec.predictions, dict):\n                        # Given a list of outputs; return a dict of outputs.\n                        def zip_key(outs, keys):\n                            if isinstance(outs, list):\n                                error_msg = ""output length is "" \\\n                                    + ""{} but keys length is {}"".format(len(outs), len(keys))\n                                assert len(outs) == len(keys), error_msg\n                            else:\n                                outs = [outs]\n                            res_dict = {}\n                            for out, key in zip(outs, keys):\n                                res_dict[key] = out\n                            return res_dict\n\n                        pred_keys = sorted(spec.predictions.keys()) if not predict_keys \\\n                            else predict_keys\n                        predictions = predictions.map(lambda res: zip_key(res, pred_keys))\n                    return predictions\n\n        return list(self.estimator.predict(input_fn, checkpoint_path=checkpoint_path))\n\n    @staticmethod\n    def _to_bigdl_metric(metric):\n        metric = metric.lower()\n        if metric == ""accuracy"" or metric == ""acc"":\n            return metrics.Accuracy()\n        elif metric == ""top5accuracy"" or metric == ""top5acc"":\n            return metrics.Top5Accuracy()\n        elif metric == ""mae"":\n            from bigdl.optim.optimizer import MAE\n            return MAE()\n        elif metric == ""auc"":\n            return metrics.AUC()\n        elif metric == ""treennaccuracy"":\n            return TreeNNAccuracy()\n        else:\n            raise TypeError(""Unsupported metric: %s"" % metric)\n'"
pyzoo/zoo/tfpark/model.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\nimport tensorflow.keras.backend as K\nfrom tensorflow.python.keras import models\nfrom tensorflow.python.keras.engine import training_utils\n\nfrom bigdl.optim.optimizer import MaxEpoch\nfrom zoo.common import load_from_file\nfrom zoo.common import save_file\nfrom zoo.common.nncontext import getOrCreateSparkContext\nfrom zoo.pipeline.api.keras.utils import to_bigdl_metric\nfrom zoo.tfpark.tf_dataset import TFNdarrayDataset, TFDataset\n\nfrom zoo.tfpark.tf_optimizer import TFOptimizer\nfrom zoo.tfpark.tf_predictor import TFPredictor\nfrom zoo.tfpark.tfnet import TFNet\n\n\nclass KerasModel(object):\n\n    def __init__(self, model, model_dir=None):\n        """"""\n        :param model: a compiled keras model\n        """"""\n        self.model = model\n        self.model_dir = model_dir\n\n    @property\n    def metrics_names(self):\n        return self.model.metrics_names\n\n    def get_weights(self):\n        return self.model.get_weights()\n\n    def set_weights(self, weights):\n        self.model.set_weights(weights)\n\n    def save_weights(self, filepath, overwrite=True, save_format=None):\n\n        def save_func(file_path):\n            self.model.save_weights(file_path, overwrite, save_format)\n        save_file(save_func, filepath)\n\n    def load_weights(self, filepath, by_name=False):\n\n        def load_func(file_path):\n            self.model.load_weights(file_path, by_name)\n        load_from_file(load_func, filepath)\n\n    def save_model(self, path):\n        """"""\n        Save the model to a single HDF5 file.\n\n        :param path: String. The path to save the model.\n        """"""\n        def save_func(file_path):\n            self.model.save(file_path)\n        save_file(save_func, path)\n\n    @staticmethod\n    def load_model(path):\n        """"""\n        Load an existing keras model (with weights) from HDF5 file.\n\n        :param path: String. The path to the pre-defined model.\n        :return: KerasModel.\n        """"""\n        def load_func(file_path):\n            return models.load_model(file_path)\n        keras_model = load_from_file(load_func, path)\n        return KerasModel(keras_model)\n\n    def fit(self,\n            x=None,\n            y=None,\n            batch_size=None,\n            epochs=1,\n            validation_data=None,\n            distributed=False,\n            **kwargs\n            ):\n        """"""\n        Train the model for a fixed num of epochs\n\n        Arguments:\n        :param x: Input data. It could be:\n            - a TFDataset object\n            - A Numpy array (or array-like), or a list of arrays\n               (in case the model has multiple inputs).\n            - A dict mapping input names to the corresponding array/tensors,\n            if the model has named inputs.\n        :param y: Target data. Like the input data `x`,\n          It should be consistent with `x` (you cannot have Numpy inputs and\n          tensor targets, or inversely). If `x` is a TFDataset, `y` should\n          not be specified (since targets will be obtained from `x`).\n        :param batch_size: Integer or `None`.\n            Number of samples per gradient update.\n            If `x` is a TFDataset, you do not need to specify batch_size.\n        :param epochs: Integer. Number of epochs to train the model.\n            An epoch is an iteration over the entire `x` and `y`\n            data provided.\n        :param validation_data: Data on which to evaluate\n            the loss and any model metrics at the end of each epoch.\n            The model will not be trained on this data.\n            `validation_data` could be:\n              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n        :param distributed: Boolean. Whether to do prediction in distributed mode or local mode.\n                     Default is True. In local mode, x must be a Numpy array.\n        """"""\n        if isinstance(x, TFDataset):\n            # todo check arguments\n            assert validation_data is None, ""validation_data must be None when "" \\\n                                            ""using TFDataset as input, please "" \\\n                                            ""use set the validation data in TFDataset""\n            if not x.has_batch:\n                raise ValueError(""The batch_size of TFDataset must be "" +\n                                 ""specified when used in KerasModel fit."")\n            if isinstance(x, TFNdarrayDataset):\n                x = _standarize_feature_label_dataset(x, self.model)\n            self._fit_distributed(x, epochs, **kwargs)\n\n        elif distributed:\n            dataset = TFDataset.from_ndarrays((x, y), val_tensors=validation_data,\n                                              batch_size=batch_size)\n            self._fit_distributed(dataset, epochs, **kwargs)\n\n        else:\n            self.model.fit(x=x,\n                           y=y,\n                           batch_size=batch_size,\n                           epochs=epochs,\n                           validation_data=validation_data,\n                           **kwargs\n                           )\n\n    def _fit_distributed(self, dataset, epochs, **kwargs):\n        self.tf_optimizer = TFOptimizer.from_keras(self.model, dataset,\n                                                   model_dir=self.model_dir,\n                                                   **kwargs)\n\n        self.tf_optimizer.optimize(MaxEpoch(epochs))\n\n    def evaluate(self,\n                 x=None,\n                 y=None,\n                 batch_per_thread=None,\n                 distributed=False\n                 ):\n        """"""\n        Evaluate a model on a given dataset\n\n        :param x: Input data. It could be:\n            - a TFDataset object\n            - A Numpy array (or array-like), or a list of arrays\n               (in case the model has multiple inputs).\n            - A dict mapping input names to the corresponding array/tensors,\n            if the model has named inputs.\n        :param y: Target data. Like the input data `x`,\n          It should be consistent with `x` (you cannot have Numpy inputs and\n          tensor targets, or inversely). If `x` is a TFDataset, `y` should\n          not be specified (since targets will be obtained from `x`).\n        :param batch_per_thread:\n          The default value is 1.\n          When distributed is True,the total batch size is batch_per_thread * rdd.getNumPartitions.\n          When distributed is False the total batch size is batch_per_thread * numOfCores.\n        :param distributed: Boolean. Whether to do prediction in distributed mode or local mode.\n                     Default is True. In local mode, x must be a Numpy array.\n        """"""\n        if isinstance(x, TFDataset):\n            if not x.has_batch:\n                raise ValueError(""The batch_per_thread of TFDataset must be "" +\n                                 ""specified when used in KerasModel evaluate."")\n            if isinstance(x, TFNdarrayDataset):\n                x = _standarize_feature_label_dataset(x, self.model)\n            # todo check arguments\n            return self._evaluate_distributed(x)\n        else:\n            if distributed:\n                dataset = TFDataset.from_ndarrays((x, y),\n                                                  batch_per_thread=-1 if batch_per_thread is None\n                                                  else batch_per_thread\n                                                  )\n                return self._evaluate_distributed(dataset)\n            else:\n                return self.model.evaluate(x=x,\n                                           y=y,\n                                           batch_size=batch_per_thread)\n\n    def _evaluate_distributed(self, dataset):\n\n        tfnet = TFNet.from_session(K.get_session(),\n                                   inputs=self.model.inputs,\n                                   outputs=self.model.outputs)\n        if dataset.batch_per_thread < 0:\n            batch_size = dataset.batch_size\n        else:\n            batch_size = dataset.batch_per_thread * dataset.get_num_partitions()\n\n        eval_methods = [to_bigdl_metric(m, self.model.loss)\n                        for m in self.metrics_names]\n\n        results = tfnet.evaluate(dataset, batch_size, eval_methods)\n        final_result = [r.result for r in results]\n\n        return final_result\n\n    def predict(self,\n                x,\n                batch_per_thread=None,\n                distributed=False):\n\n        """"""\n        Use a model to do prediction.\n\n        :param x: Input data. It could be:\n            - a TFDataset object\n            - A Numpy array (or array-like), or a list of arrays\n               (in case the model has multiple inputs).\n            - A dict mapping input names to the corresponding array/tensors,\n            if the model has named inputs.\n        :param batch_per_thread:\n          The default value is 1.\n          When distributed is True,the total batch size is batch_per_thread * rdd.getNumPartitions.\n          When distributed is False the total batch size is batch_per_thread * numOfCores.\n        :param distributed: Boolean. Whether to do prediction in distributed mode or local mode.\n                     Default is True. In local mode, x must be a Numpy array.\n        """"""\n\n        if isinstance(x, TFDataset):\n            # todo check arguments\n            if not x.has_batch:\n                raise ValueError(""The batch_per_thread of TFDataset"" +\n                                 "" must be specified when used in KerasModel predict."")\n            if isinstance(x, TFNdarrayDataset):\n                x = _standarize_feature_dataset(x, self.model)\n            return self._predict_distributed(x)\n        else:\n            if distributed:\n                sc = getOrCreateSparkContext()\n                rdd, types, shapes = _create_rdd_x(x, self.model._feed_input_names, sc)\n\n                dataset = TFDataset.from_rdd(rdd,\n                                             names=self.model._feed_input_names,\n                                             types=types,\n                                             shapes=shapes,\n                                             batch_per_thread=-1 if batch_per_thread is None\n                                             else batch_per_thread)\n                results = self._predict_distributed(dataset).collect()\n                output_num = len(self.model.outputs)\n                if output_num == 1:\n                    return np.stack(results)\n                else:\n                    predictions = []\n                    for i in range(0, output_num):\n                        predictions.append(np.stack([res[i] for res in results]))\n                    return predictions\n            else:\n                return self.model.predict(x=x,\n                                          batch_size=batch_per_thread)\n\n    def _predict_distributed(self, x):\n        predictor = TFPredictor.from_keras(self.model, x)\n        return predictor.predict()\n\n    def train_on_batch(self,\n                       x,\n                       y=None,\n                       sample_weight=None,\n                       class_weight=None,\n                       reset_metrics=True):\n        return self.model.train_on_batch(x=x,\n                                         y=y,\n                                         sample_weight=sample_weight,\n                                         class_weight=class_weight,\n                                         reset_metrics=reset_metrics)\n\n    def test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True):\n        return self.model.test_on_batch(x=x,\n                                        y=y,\n                                        sample_weight=sample_weight,\n                                        reset_metrics=reset_metrics)\n\n    def predict_on_batch(self, x):\n        return self.model.predict_on_batch(x)\n\n\ndef _standarize_feature_label_dataset(dataset, model):\n    input_names = model.input_names\n    output_names = model.output_names\n\n    def _process_labels(ys):\n        if isinstance(ys, dict):\n            return {k: np.expand_dims(y, axis=-1) if y.ndim == 0 else y for k, y in ys.items()}\n        elif isinstance(ys, list):\n            return [np.expand_dims(y, axis=-1) if y.ndim == 0 else y for y in ys]\n        else:\n            return np.expand_dims(ys, axis=-1) if ys.ndim == 0 else ys\n\n    def _training_reorder(x, input_names, output_names):\n        assert isinstance(x, tuple)\n\n        return (_reorder(x[0], input_names), _reorder(x[1], output_names))\n\n    def _reorder(x, names):\n        if isinstance(x, dict):\n            return [x[name] for name in names]\n        elif isinstance(x, list):\n            return x\n        else:\n            return [x]\n\n    rdd = dataset.rdd.map(lambda x: (x[0], _process_labels(x[1])))\\\n        .map(lambda sample: _training_reorder(sample, input_names, output_names))\n    if dataset.val_rdd is not None:\n        val_rdd = dataset.val_rdd.map(lambda x: (x[0], _process_labels(x[1])))\\\n            .map(lambda sample: _training_reorder(sample, input_names, output_names))\n    else:\n        val_rdd = None\n    tensor_structure = _training_reorder(dataset.tensor_structure, input_names, output_names)\n    new_dataset = TFNdarrayDataset(rdd, tensor_structure, dataset.batch_size,\n                                   -1, dataset.hard_code_batch_size, val_rdd)\n    new_dataset.batch_per_thread = dataset.batch_per_thread\n    return new_dataset\n\n\ndef _standarize_feature_dataset(dataset, model):\n    input_names = model.input_names\n\n    def _reorder(x, names):\n        if isinstance(x, dict):\n            return [x[name] for name in names]\n        elif isinstance(x, list):\n            return x\n        elif isinstance(x, tuple):\n            return list(x)\n        return [x]\n\n    rdd = dataset.rdd.map(lambda sample: _reorder(sample, input_names))\n    feature_schema = _reorder(dataset.tensor_structure[0], input_names)\n\n    dataset = TFNdarrayDataset(rdd, feature_schema, dataset.batch_size,\n                               -1, dataset.hard_code_batch_size)\n    return dataset\n\n\ndef _create_rdd_x_y(x, y, input_names, output_names, sc):\n    x = training_utils.standardize_input_data(x, input_names,\n                                              check_batch_axis=False,\n                                              exception_prefix=\'input\')\n    y = training_utils.standardize_input_data(y, output_names,\n                                              shapes=None, check_batch_axis=False,\n                                              exception_prefix=\'target\')\n\n    num_samples = x[0].shape[0]\n    num_inputs = len(x)\n    num_targets = len(y)\n\n    input_data = []\n    for i in range(num_samples):\n        inputs = []\n        for j in range(num_inputs):\n            inputs.append(x[j][i])\n\n        targets = []\n        for j in range(num_targets):\n            if y[j][i].ndim == 0:\n                targets.append(np.expand_dims(y[j][i], axis=1))\n            else:\n                targets.append(y[j][i])\n\n        input_data.append((inputs, targets))\n\n    x_meta = dict([(input_names[i],\n                    (input_data[0][0][i].dtype, input_data[0][0][i].shape))\n                   for i in range(len(input_names))])\n\n    y_meta = dict([(output_names[i],\n                    (input_data[0][1][i].dtype, input_data[0][1][i].shape))\n                   for i in range(len(input_names))])\n\n    rdd = sc.parallelize(input_data)\n    return rdd, x_meta, y_meta\n\n\ndef _create_rdd_x(x, input_names, sc):\n    x = training_utils.standardize_input_data(x, input_names,\n                                              check_batch_axis=False,\n                                              exception_prefix=\'input\')\n\n    num_samples = x[0].shape[0]\n    num_inputs = len(x)\n\n    input_data = []\n    for i in range(num_samples):\n        sample = []\n        for j in range(num_inputs):\n            sample.append(x[j][i])\n\n        input_data.append(sample)\n\n    types = [x.dtype for x in input_data[0]]\n    shapes = [x.shape for x in input_data[0]]\n\n    rdd = sc.parallelize(input_data)\n    return rdd, types, shapes\n'"
pyzoo/zoo/tfpark/tf_dataset.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\nimport sys\n\nfrom pyspark.ml.linalg import DenseVector, SparseVector, VectorUDT\nfrom bigdl.transform.vision.image import FeatureTransformer\nfrom bigdl.util.common import get_node_and_core_number\nfrom zoo.common.utils import callZooFunc\nfrom zoo.common import Sample, JTensor\nfrom zoo.common.nncontext import getOrCreateSparkContext\nfrom zoo.feature.common import FeatureSet, SampleToMiniBatch\nfrom zoo.feature.image import ImagePreprocessing, ImageFeatureToSample\nfrom zoo.util import nest\n\nimport tensorflow as tf\nfrom tensorflow.python.data.ops import dataset_ops\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\ndef _to_tensor_structure(tensors):\n    if isinstance(tensors, tuple):\n        tensor_structure = TensorMeta(dtype=tensors[0], shape=tensors[1], name=""input0"")\n    elif isinstance(tensors, list):\n        tensor_structure = [TensorMeta(dtype=value[0], shape=value[1],\n                                       name=""list_input_"" + str(idx))\n                            for (idx, value) in enumerate(tensors)]\n    elif isinstance(tensors, dict):\n        tensor_structure = {}\n        for key, value in tensors.items():\n            tensor_structure[key] = TensorMeta(dtype=value[0], shape=value[1], name=key)\n    else:\n        raise ValueError(""In TFDataset.from_rdd, features and labels should be a tuple, ""\n                         ""a list of tuples or a dict of tuples"")\n    return tensor_structure\n\n\ndef _tensors_to_rdd(tensors, sc, splits):\n\n    if isinstance(tensors, np.ndarray):\n        tensors = (tensors,)\n\n    if isinstance(tensors, list):\n        for i in range(len(tensors)):\n            if tensors[i].dtype == np.dtype(""float64""):\n                tensors[i] = np.float32(tensors[i])\n\n        data_list = _splits(tensors)\n        rdd = sc.parallelize(data_list, splits)\n        tensor_structure = [TensorMeta(tf.as_dtype(t.dtype),\n                                       shape=t.shape[1:],\n                                       name=""input_%s"" % i)\n                            for i, t in enumerate(tensors)]\n    else:\n        flattened = nest.flatten(tensors)\n        for i in range(len(flattened)):\n            if flattened[i].dtype == np.dtype(""float64""):\n                flattened[i] = np.float32(flattened[i])\n        data_list = _splits(flattened)\n        rdd = sc.parallelize(data_list, splits)\n        rdd = rdd.map(lambda x: nest.pack_sequence_as(tensors, x))\n        tensor_structure = nest.pack_sequence_as(tensors,\n                                                 [TensorMeta(tf.as_dtype(t.dtype),\n                                                             shape=t.shape[1:],\n                                                             name=""input_%s"" % i)\n                                                  for i, t in enumerate(flattened)])\n    return rdd, tensor_structure\n\n\ndef _splits(tensors):\n    data_list = []\n    data_size = tensors[0].shape[0]\n    for i in range(data_size):\n        sample = []\n        for j in range(len(tensors)):\n            sample.append(tensors[j][i])\n        data_list.append(sample)\n    return data_list\n\n\nclass MergeFeatureLabelImagePreprocessing(ImagePreprocessing):\n    def __init__(self, bigdl_type=""float""):\n        super(MergeFeatureLabelImagePreprocessing, self).__init__(bigdl_type)\n\n\nclass MergeFeatureLabelFeatureTransformer(FeatureTransformer):\n    def __init__(self, bigdl_type=""float""):\n        super(MergeFeatureLabelFeatureTransformer, self).__init__(bigdl_type)\n\n\nclass TensorMeta(object):\n    def __init__(self, dtype, name=None, shape=None):\n        self.dtype = dtype\n        self.name = name\n        self.shape = shape\n\n\nclass TFDataset(object):\n    def __init__(self, tensor_structure, batch_size,\n                 batch_per_thread, hard_code_batch_size=False):\n        """"""\n\n        TFDataset represents a distributed collection of elements (backed by a RDD)\n        to be feed into Tensorflow graph.\n\n        :param tensor_structure: a nested structure of TensorMeta objects specifying the\n        name, shape and data type of each element in this TFDataset\n        :param batch_size: the batch size, used for training, should be a multiple of\n        total core num\n        :param batch_per_thread: the batch size for each thread, used for inference or evaluation\n        :param hard_code_batch_size: whether to hard code the batch_size into tensorflow graph,\n        if True, the static size of the first dimension of the resulting tensors is\n        batch_size/total_core_num (training) or batch_per_thread for inference; if False,\n        it is None.\n        """"""\n\n        if batch_size > 0 and batch_per_thread > 0:\n            raise ValueError(""bath_size and batch_per_thread should not be set simultaneously"")\n\n        self.has_batch = True\n        node_num, core_num = get_node_and_core_number()\n        self.total_core_num = node_num * core_num\n        self.node_num = node_num\n        self.core_num = core_num\n        if batch_size > 0:\n            if batch_size % self.total_core_num != 0:\n                raise ValueError(""batch_size should be a multiple "" +\n                                 ""of total core number, but got batch_size: "" +\n                                 ""%s where total core number is %s"" % (batch_size,\n                                                                       self.total_core_num))\n        if batch_size <= 0 and batch_per_thread <= 0:\n            batch_per_thread = 1\n            batch_size = self.total_core_num\n            self.has_batch = False\n\n        self.batch_size = batch_size\n        self.batch_per_thread = batch_per_thread\n        self.hard_code_batch_size = hard_code_batch_size\n        self.tensor_structure = tensor_structure\n\n        if not self.hard_code_batch_size:\n            self.output_shapes = nest.pack_sequence_as(\n                self.tensor_structure, [[None] + list(t.shape)\n                                        if t is not None else None\n                                        for t in nest.flatten(self.tensor_structure)])\n        else:\n            if self.batch_per_thread > 0:\n                self.output_shapes = nest.pack_sequence_as(\n                    self.tensor_structure, [[self.batch_per_thread] + t.shape\n                                            if t is not None else None\n                                            for t in nest.flatten(self.tensor_structure)])\n            else:\n                self.output_shapes = nest.pack_sequence_as(\n                    self.tensor_structure, [[self.batch_size // self.total_core_num] + t.shape\n                                            if t is not None else None\n                                            for t in nest.flatten(self.tensor_structure)])\n\n        self.input_names = nest.pack_sequence_as(\n            self.tensor_structure, [t.name\n                                    if t is not None else None\n                                    for t in nest.flatten(self.tensor_structure)])\n\n        self._tensors = None\n\n    def _create_placeholders(self):\n\n        if not self.hard_code_batch_size:\n            tensors = nest.pack_sequence_as(\n                self.tensor_structure, [tf.placeholder(name=t.name,\n                                                       dtype=t.dtype,\n                                                       shape=[None] + list(t.shape))\n                                        for t in nest.flatten(self.tensor_structure)])\n        else:\n            if self.batch_per_thread > 0:\n                tensors = nest.pack_sequence_as(\n                    self.tensor_structure,\n                    [tf.placeholder(name=t.name,\n                                    dtype=t.dtype,\n                                    shape=[self.batch_per_thread] + list(t.shape))\n                     for t in nest.flatten(self.tensor_structure)])\n            else:\n                tensors = nest.pack_sequence_as(\n                    self.tensor_structure,\n                    [tf.placeholder(name=t.name,\n                                    dtype=t.dtype,\n                                    shape=[self.batch_size // self.total_core_num] + list(t.shape))\n                     for t in nest.flatten(self.tensor_structure)])\n\n        for tensor in nest.flatten(tensors):\n            tf.get_default_graph().clear_collection(tensor.name)\n            tf.add_to_collection(tensor.name, self)\n\n        self._original_tensors = tensors\n        self._tensors = tensors\n\n        if not self.has_batch:\n            self._tensors = nest.pack_sequence_as(self.tensor_structure,\n                                                  [t[0] for t in nest.flatten(tensors)])\n\n        return tensors\n\n    @property\n    def tensors(self):\n        """"""\n        a nested structure of TensorFlow tensor object in TensorFlow graph.\n        The elements of this dataset will be fed into these tensors on each iteration.\n        :return: the nested structure of TensorFlow tensor object\n        """"""\n\n        if self._tensors is None:\n            self._create_placeholders()\n\n        return self._tensors\n\n    @property\n    def feature_tensors(self):\n\n        if self._tensors is None:\n            self._create_placeholders()\n\n        if not isinstance(self._tensors, tuple):\n            raise ValueError(""To use feature_tensors, "" +\n                             ""the element in TFDataset must be a tuple of two components. "" +\n                             ""Please use TFDataset.from_rdd(rdd, features=..., labels=...). "")\n\n        return self._tensors[0]\n\n    @property\n    def label_tensors(self):\n\n        if self._tensors is None:\n            self._create_placeholders()\n\n        if not isinstance(self._tensors, tuple):\n            raise ValueError(""To use label_tensors, "" +\n                             ""the element in TFDataset must be a tuple of two components. "" +\n                             ""Please use TFDataset.from_rdd(rdd, features=..., labels=...). "")\n\n        return self._tensors[1]\n\n    @staticmethod\n    def _to_tensor_structure(features, labels):\n        feature_structure = _to_tensor_structure(features)\n        if labels is not None:\n            label_structure = _to_tensor_structure(labels)\n            tensor_structure = (feature_structure, label_structure)\n\n        else:\n            tensor_structure = (feature_structure,)\n        return tensor_structure\n\n    def get_prediction_data(self):\n        """"""\n        :return: an object that can be used for TFNet.predict\n        e.g. an RDD of Sample or a ImageSet\n        """"""\n        raise NotImplementedError\n\n    def get_evaluation_data(self):\n        """"""\n        :return: an object that can be used for TFNet.evaluate,\n        e.g. an RDD of Sample or a ImageSet\n        """"""\n        raise NotImplementedError\n\n    def get_training_data(self):\n        """"""\n        :return: an object that can be used to create a BigDL optimizer,\n        e.g. an RDD of Sample or a DataSet\n        """"""\n        raise NotImplementedError\n\n    def get_validation_data(self):\n        """"""\n        :return: an object that can be used to set validation in a BigDL optimizer,\n        e.g. an RDD of Sample or a DataSet\n        """"""\n        raise NotImplementedError\n\n    def get_num_partitions(self):\n        """"""\n        :return: the num of partitions of the underlying RDD\n        """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def from_rdd(*args, **kwargs):\n        """"""\n        Create a TFDataset from a rdd.\n\n        For training and evaluation, both `features` and `labels` arguments should be specified.\n        The element of the rdd should be a tuple of two, (features, labels), each has the\n        same structure of numpy.ndarrays of the argument `features`, `labels`.\n\n        E.g. if `features` is [(tf.float32, [10]), (tf.float32, [20])],\n        and `labels` is {""label1"":(tf.float32, [10]), ""label2"": (tf.float32, [20])}\n        then a valid element of the rdd could be\n\n        (\n        [np.zeros(dtype=float, shape=(10,), np.zeros(dtype=float, shape=(10,)))],\n         {""label1"": np.zeros(dtype=float, shape=(10,)),\n          ""label2"":np.zeros(dtype=float, shape=(10,))))}\n        )\n\n        If `labels` is not specified,\n        then the above element should be changed to\n        [np.zeros(dtype=float, shape=(10,), np.zeros(dtype=float, shape=(10,)))]\n\n        For inference, `labels` can be not specified.\n        The element of the rdd should be some ndarrays of the same structure of the `features`\n        argument.\n\n        A note on the legacy api: if you are using `names`, `shapes`, `types` arguments,\n        each element of the rdd should be a list of numpy.ndarray.\n\n        :param rdd: a rdd containing the numpy.ndarrays to be used\n        for training/evaluation/inference\n        :param features: the structure of input features, should one the following:\n               - a tuple (dtype, shape), e.g. (tf.float32, [28, 28, 1])\n               - a list of such tuple [(dtype1, shape1), (dtype2, shape2)],\n                     e.g. [(tf.float32, [10]), (tf.float32, [20])],\n               - a dict of such tuple, mapping string names to tuple {""name"": (dtype, shape},\n                     e.g. {""input1"":(tf.float32, [10]), ""input2"": (tf.float32, [20])}\n\n        :param labels: the structure of input labels, format is the same as features\n        :param batch_size: the batch size, used for training, should be a multiple of\n        total core num\n        :param batch_per_thread: the batch size for each thread, used for inference or evaluation\n        :param hard_code_batch_size: whether to hard code the batch_size into tensorflow graph,\n        if True, the static size of the first dimension of the resulting tensors is\n        batch_size/total_core_num (training) or batch_per_thread for inference; if False,\n        it is None.\n        :param val_rdd: validation data with the same structure of rdd\n        :param sequential_order: whether to iterate the elements in the Dataset\n                                 in sequential order when training.\n        :param shuffle: whether to shuffle the elements in each partition before each epoch\n                        when training\n        :return: a TFDataset\n        """"""\n        return TFNdarrayDataset.from_rdd(*args, **kwargs)\n\n    @staticmethod\n    def from_ndarrays(*args, **kwargs):\n        """"""\n        Create a TFDataset from a nested structure of numpy ndarrays. Each element\n        in the resulting TFDataset has the same structure of the argument tensors and\n        is created by indexing on the first dimension of each ndarray in the tensors\n        argument.\n\n        This method is equivalent to sc.parallize the tensors and call TFDataset.from_rdd\n\n        :param tensors: the numpy ndarrays\n        :param batch_size: the batch size, used for training, should be a multiple of\n        total core num\n        :param batch_per_thread: the batch size for each thread, used for inference or evaluation\n        :param hard_code_batch_size: whether to hard code the batch_size into tensorflow graph,\n        if True, the static size of the first dimension of the resulting tensors is\n        batch_size/total_core_num (training) or batch_per_thread for inference; if False,\n        it is None.\n        :param val_tensors: the numpy ndarrays used for validation during training\n        :param sequential_order: whether to iterate the elements in the Dataset\n                                 in sequential order when training.\n        :param shuffle: whether to shuffle the elements in each partition before each epoch\n                        when training\n        :return: a TFDataset\n        """"""\n        return TFNdarrayDataset.from_ndarrays(*args, **kwargs)\n\n    @staticmethod\n    def from_image_set(image_set, image, label=None,\n                       batch_size=-1, batch_per_thread=-1,\n                       hard_code_batch_size=False,\n                       validation_image_set=None,\n                       sequential_order=False,\n                       shuffle=True):\n        """"""\n        Create a TFDataset from a ImagetSet. Each ImageFeature in the ImageSet should\n        already has the ""sample"" field, i.e. the result of ImageSetToSample transformer\n\n        :param image_set: the ImageSet used to create this TFDataset\n        :param image: a tuple of two, the first element is the type of image, the second element\n        is the shape of this element, i.e. (tf.float32, [224, 224, 3]))\n        :param label: a tuple of two, the first element is the type of label, the second element\n        is the shape of this element, i.e. (tf.int32, [1]))\n        :param batch_size: the batch size, used for training, should be a multiple of\n        total core num\n        :param batch_per_thread: the batch size for each thread, used for inference or evaluation\n        :param hard_code_batch_size: whether to hard code the batch_size into tensorflow graph,\n        if True, the static size of the first dimension of the resulting tensors is\n        batch_size/total_core_num (training) or batch_per_thread for inference; if False,\n        it is None.\n        :param validation_image_set: the ImageSet used for validation during training\n        :param sequential_order: whether to iterate the elements in the Dataset\n                                 in sequential order when training.\n        :param shuffle: whether to shuffle the elements in each partition before each epoch\n                        when training\n        :return: a TFDataset\n        """"""\n        tensor_structure = TFDataset._to_tensor_structure(image, label)\n        return TFImageDataset(image_set, tensor_structure, batch_size,\n                              batch_per_thread, hard_code_batch_size,\n                              validation_image_set,\n                              sequential_order=sequential_order, shuffle=shuffle)\n\n    @staticmethod\n    def from_text_set(text_set, text, label=None,\n                      batch_size=-1, batch_per_thread=-1,\n                      hard_code_batch_size=False, validation_image_set=None,\n                      sequential_order=False, shuffle=True):\n        """"""\n        Create a TFDataset from a TextSet. The TextSet must be transformed to Sample, i.e.\n        the result of TextFeatureToSample transformer.\n        :param text_set: the TextSet used to create this TFDataset\n        :param text: a tuple of two, the first element is the type of this input feature,\n        the second element is the shape of this element, i.e. (tf.float32, [10, 100, 4])).\n        text can also be nested structure of this tuple of two.\n        :param label: a tuple of two, the first element is the type of label, the second element\n        is the shape of this element, i.e. (tf.int32, [1])). label can also be nested structure of\n        this tuple of two.\n        :param batch_size: the batch size, used for training, should be a multiple of\n        total core num\n        :param batch_per_thread: the batch size for each thread, used for inference or evaluation\n        :param hard_code_batch_size: whether to hard code the batch_size into tensorflow graph,\n        if True, the static size of the first dimension of the resulting tensors is\n        batch_size/total_core_num (training) or batch_per_thread for inference; if False,\n        it is None.\n        :param validation_image_set: The TextSet used for validation during training\n        :param sequential_order: whether to iterate the elements in the Dataset\n                                 in sequential order when training.\n        :param shuffle: whether to shuffle the elements in each partition before each epoch\n                        when training\n        :return: a TFDataset\n        """"""\n        tensor_structure = TFDataset._to_tensor_structure(text, label)\n        return TFTextDataset(text_set, tensor_structure, batch_size,\n                             batch_per_thread, hard_code_batch_size,\n                             validation_image_set,\n                             sequential_order=sequential_order, shuffle=shuffle)\n\n    @staticmethod\n    def from_tfrecord_file(sc, file_path, batch_size=-1, batch_per_thread=-1,\n                           hard_code_batch_size=False, validation_file_path=None,\n                           sequential_order=False, shuffle=True):\n        """"""\n        Create a TFDataset from tfrecord files.\n        :param sc: The SparkContext\n        :param file_path: comma seperated tfrecord file(s) path\n        :param batch_size: the batch size, used for training, should be a multiple of\n        total core num\n        :param batch_per_thread: the batch size for each thread, used for inference or evaluation\n        :param hard_code_batch_size: whether to hard code the batch_size into tensorflow graph,\n        if True, the static size of the first dimension of the resulting tensors is\n        batch_size/total_core_num (training) or batch_per_thread for inference; if False,\n        it is None.\n        :param validation_file_path: The tfrecord files used for validation\n        :param sequential_order: whether to iterate the elements in the Dataset\n                                 in sequential order when training.\n        :param shuffle: whether to shuffle the elements in each partition before each epoch\n                        when training\n        :return: a TFDataset\n        """"""\n        input_format_class = ""org.tensorflow.hadoop.io.TFRecordFileInputFormat""\n        key_class = ""org.apache.hadoop.io.BytesWritable""\n        value_class = ""org.apache.hadoop.io.NullWritable""\n        bytes_rdd = sc.newAPIHadoopFile(file_path, input_format_class,\n                                        keyClass=key_class,\n                                        valueClass=value_class)\n        bytes_rdd = bytes_rdd.map(lambda record: bytearray(record[0]))\n        validation_bytes_rdd = None\n        if validation_file_path is not None:\n            validation_bytes_rdd = sc.newAPIHadoopFile(validation_file_path,\n                                                       input_format_class,\n                                                       keyClass=key_class,\n                                                       valueClass=value_class)\n            validation_bytes_rdd = validation_bytes_rdd.map(lambda record: bytearray(record[0]))\n\n        return TFBytesDataset(bytes_rdd, batch_size, batch_per_thread,\n                              hard_code_batch_size, validation_bytes_rdd,\n                              sequential_order=sequential_order, shuffle=shuffle)\n\n    @staticmethod\n    def from_feature_set(dataset, features, labels=None, batch_size=-1, batch_per_thread=-1,\n                         hard_code_batch_size=False, validation_dataset=None):\n        """"""\n        Create a TFDataset from a FeatureSet. Currently, the element in this Feature set must be a\n        Sample, i.e. the result of ImageFeatureToSample transformer\n        :param dataset: the feature set used to create this TFDataset\n        :param features: a tuple of two, the first element is the type of this input feature,\n        the second element is the shape of this element, i.e. (tf.float32, [224, 224, 3])).\n        text can also be nested structure of this tuple of two.\n        :param labels: a tuple of two, the first element is the type of label, the second element\n        is the shape of this element, i.e. (tf.int32, [1])). label can also be nested structure of\n        this tuple of two.\n        :param batch_size: the batch size, used for training, should be a multiple of\n        total core num\n        :param batch_per_thread: the batch size for each thread, used for inference or evaluation\n        :param hard_code_batch_size: whether to hard code the batch_size into tensorflow graph,\n        if True, the static size of the first dimension of the resulting tensors is\n        batch_size/total_core_num (training) or batch_per_thread for inference; if False,\n        it is None.\n        :param validation_dataset: The FeatureSet used for validation during training\n        :return: a TFDataset\n        """"""\n        tensor_structure = TFDataset._to_tensor_structure(features, labels)\n\n        return TFFeatureDataset(dataset, tensor_structure, batch_size,\n                                batch_per_thread, hard_code_batch_size,\n                                validation_dataset)\n\n    @staticmethod\n    def from_string_rdd(string_rdd, batch_size=-1, batch_per_thread=-1,\n                        hard_code_batch_size=False, validation_string_rdd=None):\n        """"""\n        Create a TFDataset from a RDD of strings. Each element is the RDD should be a single string.\n        The returning TFDataset\'s feature_tensors has only one Tensor. the type of the Tensor\n        is tf.string, and the shape is (None,). The returning don\'t have label_tensors. If the\n        dataset is used for training, the label should be encoded in the string.\n        :param string_rdd: the RDD of strings\n        :param batch_size: the batch size, used for training, should be a multiple of\n        total core num\n        :param batch_per_thread: the batch size for each thread, used for inference or evaluation\n        :param hard_code_batch_size: whether to hard code the batch_size into tensorflow graph,\n        if True, the static size of the first dimension of the resulting tensors is\n        batch_size/total_core_num (training) or batch_per_thread for inference; if False,\n        it is None.\n        :param validation_string_rdd: the RDD of strings to be used in validation\n        :return: a TFDataset\n        """"""\n        string_rdd = string_rdd.map(lambda x: bytearray(x, ""utf-8""))\n        if validation_string_rdd is not None:\n            validation_string_rdd = validation_string_rdd.map(lambda x: bytearray(x, ""utf-8""))\n        return TFBytesDataset(string_rdd, batch_size, batch_per_thread,\n                              hard_code_batch_size, validation_string_rdd)\n\n    @staticmethod\n    def from_bytes_rdd(bytes_rdd, batch_size=-1, batch_per_thread=-1,\n                       hard_code_batch_size=False, validation_bytes_rdd=None):\n        """"""\n        Create a TFDataset from a RDD of bytes. Each element is the RDD should be a bytes object.\n        The returning TFDataset\'s feature_tensors has only one Tensor. the type of the Tensor\n        is tf.string, and the shape is (None,). The returning don\'t have label_tensors. If the\n        dataset is used for training, the label should be encoded in the bytes.\n        :param bytes_rdd: the RDD of bytes\n        :param batch_size: the batch size, used for training, should be a multiple of\n        total core num\n        :param batch_per_thread: the batch size for each thread, used for inference or evaluation\n        :param hard_code_batch_size: whether to hard code the batch_size into tensorflow graph,\n        if True, the static size of the first dimension of the resulting tensors is\n        batch_size/total_core_num (training) or batch_per_thread for inference; if False,\n        it is None.\n        :param validation_bytes_rdd: the RDD of bytes to be used in validation\n        :return: a TFDataset\n        """"""\n        return TFBytesDataset(bytes_rdd, batch_size, batch_per_thread,\n                              hard_code_batch_size, validation_bytes_rdd)\n\n    @staticmethod\n    def from_tf_data_dataset(dataset, batch_size=-1,\n                             batch_per_thread=-1, hard_code_batch_size=False,\n                             validation_dataset=None,\n                             sequential_order=False, shuffle=True, remove_checking=False):\n        """"""\n        Create a TFDataset from a tf.data.Dataset.\n\n        The recommended way to create the dataset is to reading files in a shared file\n        system (e.g. HDFS) that is accessible from every executor of this Spark Application.\n\n        If the dataset is created by reading files in the local file system, then the\n        files must exist in every executor in the exact same path. The path should be\n        absolute path and relative path is not supported.\n\n        A few kinds of dataset is not supported for now:\n        1. dataset created from tf.data.Dataset.from_generators\n        2. dataset with Dataset.batch operation.\n        3. dataset with Dataset.repeat operation\n        4. dataset contains tf.py_func, tf.py_function or tf.numpy_function\n\n        :param dataset: the tf.data.Dataset\n        :param batch_size: the batch size, used for training, should be a multiple of\n        total core num\n        :param batch_per_thread: the batch size for each thread, used for inference or evaluation\n        :param hard_code_batch_size: whether to hard code the batch_size into tensorflow graph,\n        if True, the static size of the first dimension of the resulting tensors is\n        batch_size/total_core_num (training) or batch_per_thread for inference; if False,\n        it is None.\n        :param validation_dataset: the dataset used for validation\n        :return: a TFDataset\n        """"""\n        return TFDataDataset(dataset, batch_size, batch_per_thread,\n                             hard_code_batch_size, validation_dataset,\n                             sequential_order, shuffle, remove_checking)\n\n    @staticmethod\n    def from_dataframe(df, feature_cols, labels_cols=None, batch_size=-1,\n                       batch_per_thread=-1, hard_code_batch_size=False,\n                       validation_df=None,\n                       sequential_order=False, shuffle=True):\n        """"""\n        Create a TFDataset from a pyspark.sql.DataFrame.\n\n        :param df: the DataFrame for the dataset\n        :param feature_cols: a list of string, indicating which columns are used as features.\n                            Currently supported types are FloatType, DoubleType, IntegerType,\n                            LongType, ArrayType (value should be numbers), DenseVector\n                            and SparseVector. For ArrayType, DenseVector and SparseVector,\n                            the sizes are assume to the same.\n        :param labels_cols: a list of string, indicating which columns are used as labels.\n                            Currently supported types are FloatType, DoubleType, IntegerType,\n                            LongType, ArrayType (value should be numbers), DenseVector\n                            and SparseVector. For ArrayType, DenseVector and SparseVector,\n                            the sizes are assume to the same.\n        :param batch_size: the batch size, used for training, should be a multiple of\n        total core num\n        :param batch_per_thread: the batch size for each thread, used for inference or evaluation\n        :param hard_code_batch_size: whether to hard code the batch_size into tensorflow graph,\n        if True, the static size of the first dimension of the resulting tensors is\n        batch_size/total_core_num (training) or batch_per_thread for inference; if False,\n        it is None.\n        :param validation_df: the DataFrame used for validation\n        :return: a TFDataset\n        """"""\n        return DataFrameDataset(df, feature_cols, labels_cols, batch_size,\n                                batch_per_thread, hard_code_batch_size, validation_df,\n                                sequential_order, shuffle)\n\n\nclass TFDataDataset(TFDataset):\n\n    def get_num_partitions(self):\n        # only called in inference case\n        return self.total_core_num\n\n    @staticmethod\n    def _assert_not_batched(dataset):\n\n        if isinstance(dataset, dataset_ops.DatasetV1Adapter):\n            TFDataDataset._assert_not_batched(dataset._dataset)\n        elif isinstance(dataset, dataset_ops.BatchDataset):\n            raise ValueError(""Dataset should not be batched,""\n                             ""please use a dataset without the batch operation"")\n        else:\n            for dt in dataset._inputs():\n                TFDataDataset._assert_not_batched(dt)\n\n    @staticmethod\n    def check_rules(dataset, rules, is_training):\n        from tensorflow.python.data.ops import dataset_ops\n\n        if isinstance(dataset, dataset_ops.DatasetV1Adapter):\n            TFDataDataset.check_rules(dataset._dataset, rules, is_training)\n        else:\n            for rule, message in rules:\n                assert not rule(dataset, is_training), message\n            else:\n                for dt in dataset._inputs():\n                    TFDataDataset.check_rules(dt, rules, is_training)\n\n    def __init__(self, tf_data_dataset, batch_size,\n                 batch_per_thread, hard_code_batch_size=False,\n                 validation_dataset=None,\n                 sequential_order=False, shuffle=True, remove_checking=False):\n\n        # rule 1: we assume that the dataset user passed is not batched\n        rules = [(\n            lambda dataset, is_training: isinstance(dataset, dataset_ops.BatchDataset),\n            ""Dataset should not be batched, please use a dataset without the batch operation""),\n            (\n            lambda dataset, is_training: isinstance(dataset, dataset_ops.RepeatDataset),\n                ""Dataset should not be repeated, please use a dataset without the repeat operation"")\n        ]\n\n        if not remove_checking:\n            TFDataDataset.check_rules(tf_data_dataset, rules, True)\n            if validation_dataset is not None:\n                TFDataDataset.check_rules(validation_dataset, rules, False)\n\n        py_func_ops = {""PyFunc"", ""PyFuncStateless"", ""EagerPyFunc""}\n        for node in tf.get_default_graph().as_graph_def().node:\n            op_type = node.op\n            if op_type in py_func_ops:\n                raise ValueError(""tf.py_func, tf.py_function, tf.numpy_function and"" +\n                                 "" Dataset.from_generators are not supported in TFPark"")\n\n        if shuffle:\n            from tensorflow.python.keras.engine import training_utils\n            training_utils.verify_dataset_shuffled(tf_data_dataset)\n\n        flatten_shapes = nest.flatten(tf_data_dataset.output_shapes)\n        flatten_types = nest.flatten(tf_data_dataset.output_types)\n\n        flatten_tensor_structure = [TensorMeta(dtype=flatten_types[i],\n                                               shape=list(flatten_shapes[i]),\n                                               name=""zoo_input_{}"".format(i))\n                                    for i in range(len(flatten_shapes))]\n        structure = tf_data_dataset.output_types\n        if isinstance(structure, tf.DType):\n            structure = (structure, )\n        tensor_structure = nest.pack_sequence_as(structure,\n                                                 flatten_tensor_structure)\n\n        super(TFDataDataset, self).__init__(tensor_structure, batch_size,\n                                            batch_per_thread, hard_code_batch_size)\n\n        if self.batch_size > 0 and self.has_batch:\n            # training case\n            self._per_partition_batch_size = self.batch_size // self.node_num\n            self._shard_num = self.node_num\n        else:\n            # inference case\n            self._per_partition_batch_size = self.batch_per_thread\n            self._shard_num = self.total_core_num\n\n        tf_data_dataset = tf_data_dataset.batch(self._per_partition_batch_size)\n        if validation_dataset is not None:\n            validation_dataset = validation_dataset.batch(self._per_partition_batch_size)\n\n        shard_index = tf.placeholder(dtype=tf.int64, shape=())\n        tf_data_dataset = tf_data_dataset.shard(self._shard_num, shard_index)\n        if validation_dataset is not None:\n            validation_dataset = validation_dataset.shard(self._shard_num, shard_index)\n\n        self.shard_index = shard_index\n        self.train_dataset = tf_data_dataset\n        self.train_iterator = self.train_dataset.make_initializable_iterator()\n        self.train_next_ops = nest.flatten(self.train_iterator.get_next())\n        self.output_types = [t.as_datatype_enum\n                             for t in nest.flatten(self.train_dataset.output_types)]\n\n        self.validation_dataset = validation_dataset\n        self.validation_iterator = None\n        self.validation_next_ops = None\n\n        self._train_init_op_name = self.train_iterator.initializer.name\n        self._train_output_names = [op.name for op in self.train_next_ops]\n        if validation_dataset is not None:\n            self.validation_iterator = self.validation_dataset.make_initializable_iterator()\n            self.validation_next_ops = nest.flatten(self.validation_iterator.get_next())\n            self._val_init_op_name = self.validation_iterator.initializer.name\n            self._val_output_names = [op.name for op in self.validation_next_ops]\n\n        self.table_init_name = tf.tables_initializer().name\n\n        self.sequential_order = sequential_order\n        self.shuffle = shuffle\n        self.graph = self.train_next_ops[0].graph\n        self.graph_def = bytearray(self.graph.as_graph_def().SerializeToString())\n\n    def get_prediction_data(self):\n        jvalue = callZooFunc(""float"", ""createMiniBatchRDDFromTFDataset"",\n                             self.graph_def, self._train_init_op_name, self.table_init_name,\n                             self._train_output_names,\n                             self.output_types, self.shard_index.name)\n        rdd = jvalue.value().toJavaRDD()\n        return rdd\n\n    def get_evaluation_data(self):\n\n        feature_length = len(nest.flatten(self.tensor_structure[0]))\n        jvalue = callZooFunc(""float"", ""createMiniBatchRDDFromTFDatasetEval"",\n                             self.graph_def, self._train_init_op_name, self.table_init_name,\n                             self._train_output_names,\n                             self.output_types, self.shard_index.name, feature_length)\n        rdd = jvalue.value().toJavaRDD()\n        return rdd\n\n    def get_training_data(self):\n        jvalue = callZooFunc(""float"", ""createTFDataFeatureSet"",\n                             self.graph_def, self._train_init_op_name, self.table_init_name,\n                             self._train_output_names, self.output_types, self.shard_index.name)\n        return FeatureSet(jvalue=jvalue)\n\n    def get_validation_data(self):\n        if self.validation_dataset is not None:\n            jvalue = callZooFunc(""float"", ""createTFDataFeatureSet"",\n                                 self.graph_def, self._val_init_op_name, self.table_init_name,\n                                 self._val_output_names,\n                                 self.output_types, self.shard_index.name)\n            return FeatureSet(jvalue=jvalue)\n        return None\n\n\nclass TFFeatureDataset(TFDataset):\n\n    def __init__(self, dataset, tensor_structure, batch_size,\n                 batch_per_thread, hard_code_batch_size=False, validation_dataset=None):\n        super(TFFeatureDataset, self).__init__(tensor_structure, batch_size,\n                                               batch_per_thread, hard_code_batch_size)\n        self.dataset = dataset\n        self.validation_dataset = validation_dataset\n\n    def get_prediction_data(self):\n        raise Exception(""TFFeatureDataset is only supported in training"")\n\n    def get_evaluation_data(self):\n        raise Exception(""TFFeatureDataset is only supported in training"")\n\n    def get_training_data(self):\n        fs = self.dataset.transform(MergeFeatureLabelFeatureTransformer())\n        fs = fs.transform(SampleToMiniBatch(self.batch_size))\n        return fs\n\n    def get_validation_data(self):\n        if self.validation_dataset is not None:\n            fs = self.validation_dataset.transform(\n                MergeFeatureLabelFeatureTransformer())\n            fs = fs.transform(SampleToMiniBatch(self.batch_size))\n            return fs\n        return None\n\n    def get_num_partitions(self):\n        raise Exception(""TFFeatureDataset is only supported in training"")\n\n\nclass TFBytesDataset(TFDataset):\n\n    def get_num_partitions(self):\n        return self.train_rdd.getNumPartitions()\n\n    def __init__(self, string_rdd, batch_size,\n                 batch_per_thread, hard_code_batch_size=False,\n                 validation_string_rdd=None, sequential_order=False, shuffle=True):\n\n        tensor_structure = (TensorMeta(dtype=tf.string, shape=(), name=""input""),)\n\n        super(TFBytesDataset, self).__init__(tensor_structure, batch_size,\n                                             batch_per_thread, hard_code_batch_size)\n\n        self.train_rdd = string_rdd\n        self.validation_rdd = validation_string_rdd\n        self.sequential_order = sequential_order\n        self.shuffle = shuffle\n\n    def get_prediction_data(self):\n        jvalue = callZooFunc(""float"", ""createMiniBatchRDDFromStringRDD"",\n                             self.train_rdd,\n                             self.batch_per_thread)\n        rdd = jvalue.value().toJavaRDD()\n        return rdd\n\n    def get_evaluation_data(self):\n        jvalue = callZooFunc(""float"", ""createMiniBatchRDDFromStringRDD"",\n                             self.train_rdd,\n                             self.batch_per_thread)\n        rdd = jvalue.value().toJavaRDD()\n        return rdd\n\n    def get_training_data(self):\n        jvalue = callZooFunc(""float"", ""createMiniBatchFeatureSetFromStringRDD"",\n                             self.train_rdd,\n                             self.batch_size, self.sequential_order, self.shuffle)\n        fs = FeatureSet(jvalue)\n        return fs\n\n    def get_validation_data(self):\n        if self.validation_rdd is not None:\n            jvalue = callZooFunc(""float"", ""createMiniBatchFeatureSetFromStringRDD"",\n                                 self.validation_rdd,\n                                 self.batch_size, self.sequential_order, self.shuffle)\n            fs = FeatureSet(jvalue)\n            return fs\n        return None\n\n\nclass TFTextDataset(TFDataset):\n\n    def __init__(self, text_set, tensor_structure, batch_size,\n                 batch_per_thread, hard_code_batch_size=False,\n                 validation_text_set=None, sequential_order=False, shuffle=True):\n        super(TFTextDataset, self).__init__(tensor_structure, batch_size,\n                                            batch_per_thread, hard_code_batch_size)\n        self.text_set = text_set\n        self.validation_text_set = validation_text_set\n        self.sequential_order = sequential_order\n        self.shuffle = shuffle\n\n    def get_prediction_data(self):\n        rdd = self.text_set.get_samples().map(\n            lambda sample: Sample.from_jtensor(features=sample.features,\n                                               labels=JTensor.from_ndarray(np.array([0.0]))))\n        rdd_wrapper = callZooFunc(""float"", ""zooRDDSampleToMiniBatch"", rdd, self.batch_per_thread)\n        return rdd_wrapper.value().toJavaRDD()\n\n    def get_evaluation_data(self):\n        return self.text_set.get_samples()\n\n    def get_training_data(self):\n        sample_rdd = self.text_set.get_samples().map(\n            lambda sample: Sample.from_jtensor(features=sample.features + sample.labels,\n                                               labels=JTensor.from_ndarray(np.array([0.0]))))\n        fs = FeatureSet.sample_rdd(sample_rdd,\n                                   sequential_order=self.sequential_order,\n                                   shuffle=self.shuffle)\n        fs = fs.transform(SampleToMiniBatch(self.batch_size))\n        return fs\n\n    def get_validation_data(self):\n        if self.validation_text_set is not None:\n            sample_rdd = self.validation_text_set.get_samples().map(\n                lambda sample: Sample.from_jtensor(features=sample.features + sample.labels,\n                                                   labels=JTensor.from_ndarray(np.array([0.0]))))\n            fs = FeatureSet.sample_rdd(sample_rdd,\n                                       sequential_order=self.sequential_order,\n                                       shuffle=self.shuffle)\n            fs = fs.transform(SampleToMiniBatch(self.batch_size))\n            return fs\n        return None\n\n    def get_num_partitions(self):\n        return self.text_set.get_samples().getNumPartitions()\n\n\nclass TFImageDataset(TFDataset):\n    def __init__(self, image_set, tensor_structure, batch_size,\n                 batch_per_thread, hard_code_batch_size=False,\n                 validation_image_set=None,\n                 sequential_order=False, shuffle=True):\n        super(TFImageDataset, self).__init__(tensor_structure, batch_size,\n                                             batch_per_thread, hard_code_batch_size)\n        self.image_set = image_set\n        self.validation_image_set = validation_image_set\n        self.sequential_order = sequential_order\n        self.shuffle = shuffle\n\n    def get_prediction_data(self):\n        return self.image_set\n\n    def get_evaluation_data(self):\n        return self.image_set.to_image_frame()\n\n    def get_training_data(self):\n        fs = FeatureSet.image_set(self.image_set,\n                                  sequential_order=self.sequential_order,\n                                  shuffle=self.shuffle)\n        fs = fs.transform(MergeFeatureLabelImagePreprocessing())\n        fs = fs.transform(ImageFeatureToSample())\n        fs = fs.transform(SampleToMiniBatch(self.batch_size))\n\n        return fs\n\n    def get_validation_data(self):\n        if self.validation_image_set is not None:\n            fs = FeatureSet.image_set(self.validation_image_set,\n                                      sequential_order=self.sequential_order,\n                                      shuffle=self.shuffle)\n            fs = fs.transform(MergeFeatureLabelImagePreprocessing())\n            fs = fs.transform(ImageFeatureToSample())\n            fs = fs.transform(SampleToMiniBatch(self.batch_size))\n            return fs\n        return None\n\n    def get_num_partitions(self):\n        return self.image_set.get_image().getNumPartitions()\n\n\nclass TFNdarrayDataset(TFDataset):\n\n    def __init__(self, rdd, tensor_structure, batch_size,\n                 batch_per_thread, hard_code_batch_size=False,\n                 val_rdd=None, sequential_order=True, shuffle=False):\n\n        super(TFNdarrayDataset, self).__init__(tensor_structure, batch_size,\n                                               batch_per_thread, hard_code_batch_size)\n\n        self.val_rdd = val_rdd\n        self.rdd = rdd\n        self.sequential_order = sequential_order\n        self.shuffle = shuffle\n\n    def get_prediction_data(self):\n        rdd = self.rdd.map(lambda t: Sample.from_ndarray(nest.flatten(t), np.array([0.0])))\n        rdd_wrapper = callZooFunc(""float"", ""zooRDDSampleToMiniBatch"", rdd, self.batch_per_thread)\n        return rdd_wrapper.value().toJavaRDD()\n\n    def get_evaluation_data(self):\n        if isinstance(self.tensor_structure, tuple):\n            return self.rdd.map(\n                lambda t: Sample.from_ndarray(nest.flatten(t[0]), nest.flatten(t[1])))\n        return self.rdd.map(lambda t: Sample.from_ndarray(nest.flatten(t), np.array([0.0])))\n\n    def get_training_data(self):\n        sample_rdd = self.rdd.map(\n            lambda t: Sample.from_ndarray(nest.flatten(t), np.array([0.0])))\n        fs = FeatureSet.sample_rdd(sample_rdd,\n                                   sequential_order=self.sequential_order,\n                                   shuffle=self.shuffle)\n        fs = fs.transform(SampleToMiniBatch(self.batch_size))\n        return fs\n\n    def get_validation_data(self):\n        if self.val_rdd is not None:\n            sample_rdd = self.val_rdd.map(\n                lambda t: Sample.from_ndarray(nest.flatten(t), np.array([0.0])))\n            fs = FeatureSet.sample_rdd(sample_rdd,\n                                       sequential_order=self.sequential_order,\n                                       shuffle=self.shuffle)\n            fs = fs.transform(SampleToMiniBatch(self.batch_size))\n            return fs\n        return None\n\n    def get_num_partitions(self):\n        return self.rdd.getNumPartitions()\n\n    @staticmethod\n    def from_rdd(rdd, names=None, shapes=None, types=None,\n                 batch_size=-1, batch_per_thread=-1,\n                 hard_code_batch_size=False, val_rdd=None,\n                 features=None, labels=None,\n                 sequential_order=False,\n                 shuffle=True):\n\n        if features is not None:\n            feature_structure = _to_tensor_structure(features)\n            if labels is not None:\n                label_structure = _to_tensor_structure(labels)\n                tensor_structure = (feature_structure, label_structure)\n\n            else:\n                tensor_structure = (feature_structure,)\n\n            return TFNdarrayDataset(rdd, tensor_structure,\n                                    batch_size, batch_per_thread,\n                                    hard_code_batch_size, val_rdd,\n                                    sequential_order=sequential_order,\n                                    shuffle=shuffle)\n\n        if names is not None or shapes is not None or types is not None:\n            if not names:\n                names = [""features"", ""labels""]\n            if not shapes:\n                shapes = [None] * len(names)\n\n            if not types:\n                types = [tf.float32] * len(names)\n            tensor_structure = []\n            for i in range(len(names)):\n                tensor_structure.append(TensorMeta(types[i], name=names[i], shape=shapes[i]))\n        else:\n            tensor_structure = [TensorMeta(dtype=tf.float32), TensorMeta(dtype=tf.float32)]\n\n        return TFNdarrayDataset(rdd, tensor_structure,\n                                batch_size, batch_per_thread,\n                                hard_code_batch_size, val_rdd,\n                                sequential_order=sequential_order, shuffle=shuffle)\n\n    @staticmethod\n    def from_ndarrays(tensors, batch_size=-1, batch_per_thread=-1,\n                      hard_code_batch_size=False, val_tensors=None,\n                      sequential_order=False, shuffle=True):\n        sc = getOrCreateSparkContext()\n        node_num, core_num = get_node_and_core_number()\n        total_core_num = node_num * core_num\n\n        rdd, tensor_structure = _tensors_to_rdd(tensors, sc, total_core_num)\n\n        val_rdd = None\n        if val_tensors is not None:\n            val_rdd, _ = _tensors_to_rdd(val_tensors, sc, total_core_num)\n\n        return TFNdarrayDataset(rdd, tensor_structure, batch_size,\n                                batch_per_thread, hard_code_batch_size,\n                                val_rdd, sequential_order=sequential_order, shuffle=shuffle)\n\n\nclass DataFrameDataset(TFNdarrayDataset):\n\n    @staticmethod\n    def df_datatype_to_tf(dtype):\n        import pyspark.sql.types as df_types\n        if isinstance(dtype, df_types.FloatType):\n            return (tf.float32, ())\n        if isinstance(dtype, df_types.IntegerType):\n            return (tf.int32, ())\n        if isinstance(dtype, df_types.LongType):\n            return (tf.int64, ())\n        if isinstance(dtype, df_types.DoubleType):\n            return (tf.float64, ())\n        if isinstance(dtype, df_types.ArrayType):\n            return (tf.float32, (None,))\n        if isinstance(dtype, VectorUDT):\n            return (tf.float32, (None,))\n        return None\n\n    @staticmethod\n    def is_scalar_type(dtype):\n        import pyspark.sql.types as df_types\n        if isinstance(dtype, df_types.FloatType):\n            return True\n        if isinstance(dtype, df_types.IntegerType):\n            return True\n        if isinstance(dtype, df_types.LongType):\n            return True\n        if isinstance(dtype, df_types.DoubleType):\n            return True\n        return False\n\n    def __init__(self, df, feature_cols, labels_cols=None, batch_size=-1,\n                 batch_per_thread=-1, hard_code_batch_size=False,\n                 validation_df=None,\n                 sequential_order=False, shuffle=True):\n        assert isinstance(feature_cols, list), ""feature_cols should be a list""\n        if labels_cols is not None:\n            assert isinstance(labels_cols, list), ""label_cols should be a list""\n        import pyspark\n        assert isinstance(df, pyspark.sql.DataFrame)\n\n        if labels_cols is None:\n            labels_cols = []\n\n        selected_df = df.select(*(feature_cols + labels_cols))\n        schema = selected_df.schema\n        feature_meta = []\n        for feature_col in feature_cols:\n            field = schema[feature_col]\n            name = field.name\n            data_type = field.dataType\n            if DataFrameDataset.df_datatype_to_tf(data_type) is None:\n                raise ValueError(\n                    ""data type {} of col {} is not supported for now"".format(data_type, name))\n            tf_type, tf_shape = DataFrameDataset.df_datatype_to_tf(data_type)\n            feature_meta.append(TensorMeta(tf_type, name=name, shape=tf_shape))\n\n        if labels_cols:\n            label_meta = []\n            for label_col in labels_cols:\n                field = schema[label_col]\n                name = field.name\n                data_type = field.dataType\n                if DataFrameDataset.df_datatype_to_tf(data_type) is None:\n                    raise ValueError(\n                        ""data type {} of col {} is not supported for now"".format(data_type, name))\n                tf_type, tf_shape = DataFrameDataset.df_datatype_to_tf(data_type)\n                label_meta.append(TensorMeta(tf_type, name=name, shape=tf_shape))\n\n            tensor_structure = (feature_meta, label_meta)\n        else:\n            tensor_structure = (feature_meta,)\n\n        def convert(row):\n\n            def convert_for_cols(row, cols):\n                import pyspark.sql.types as df_types\n                result = []\n                for name in cols:\n                    feature_type = schema[name].dataType\n                    if DataFrameDataset.is_scalar_type(feature_type):\n                        result.append(np.array(row[name]))\n                    elif isinstance(feature_type, df_types.ArrayType):\n                        result.append(np.array(row[name]))\n                    elif isinstance(row[name], DenseVector):\n                        result.append(row[name].values)\n                    else:\n                        assert isinstance(row[name], SparseVector),\\\n                            ""unsupported field {}, data {}"".format(schema[name], row[name])\n                        result.append(row[name].toArray())\n                if len(result) == 1:\n                    return result[0]\n                return result\n            features = convert_for_cols(row, feature_cols)\n            if labels_cols:\n                labels = convert_for_cols(row, labels_cols)\n                return (features, labels)\n            else:\n                return (features,)\n\n        rdd = selected_df.rdd.map(lambda row: convert(row))\n        val_rdd = validation_df.rdd.map(lambda row: convert(row)) if validation_df else None\n\n        super(DataFrameDataset, self).__init__(rdd, tensor_structure, batch_size,\n                                               batch_per_thread, hard_code_batch_size,\n                                               val_rdd, sequential_order, shuffle)\n'"
pyzoo/zoo/tfpark/tf_optimizer.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport json\nimport logging\nimport os\nimport sys\nimport tempfile\n\nfrom bigdl.nn.criterion import Criterion\nfrom bigdl.nn.layer import Layer\nfrom bigdl.optim.optimizer import MaxEpoch, EveryEpoch\nfrom bigdl.util.common import to_list, JavaValue\n\nfrom zoo.tfpark.zoo_optimizer import FakeOptimMethod\nfrom zoo.common.utils import callZooFunc\nfrom zoo.pipeline.api.keras.engine.topology import to_bigdl_metric, Loss, OptimMethod\nfrom zoo.pipeline.api.net.utils import find_placeholders, to_bigdl_optim_method, find_tensors\nfrom zoo.pipeline.estimator import Estimator\nfrom zoo.util import nest\n\nimport tensorflow as tf\nfrom tensorflow import gfile\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass IdentityCriterion(Criterion):\n    def __init__(self):\n        super(IdentityCriterion, self).__init__(None, ""float"")\n\n\nclass TFValidationMethod(JavaValue):\n    def __init__(self, val_method, name, output_indices, label_indices):\n        JavaValue.__init__(self, None, ""float"",\n                           val_method, name, output_indices, label_indices)\n\n\nclass StatelessMetric(JavaValue):\n    def __init__(self, metric_name, idx):\n        self.name = metric_name\n        self.idx = idx\n        JavaValue.__init__(self, None, ""float"", metric_name, idx)\n\n\nclass BigDLMetric(object):\n    def __init__(self, val_method, outputs, labels):\n        self.val_method = val_method\n        self.outputs = outputs\n        self.labels = labels\n\n\nclass TFTrainingHelper(Layer):\n    def __init__(self, path, config_proto, saver, meta, sess):\n        self.saver = saver\n        self.meta = meta\n        self.export_dir = path\n        self.sess = sess\n\n        if config_proto is not None:\n            import tensorflow as tf\n            assert isinstance(config_proto, tf.ConfigProto), \\\n                ""session_config should be a tf.ConfigProto""\n            config_proto.use_per_session_threads = True\n            byte_arr = bytearray(config_proto.SerializeToString())\n        else:\n            byte_arr = None\n\n        super(TFTrainingHelper, self).__init__(None, ""float"", path, byte_arr)\n\n    def save_checkpoint(self):\n        callZooFunc(self.bigdl_type, ""saveCheckpoint"",\n                    self.value)\n\n    def get_weights_to_python(self):\n        self.save_checkpoint()\n        self.saver.restore(self.sess, os.path.join(self.export_dir, ""model""))\n\n    def load_checkpoint(self, path):\n        callZooFunc(self.bigdl_type, ""loadZooCheckpoint"", self.value, path)\n\n\ndef _to_operation_name(name):\n    return name.split("":"")[0]\n\n\ndef _to_floats(vs):\n    return [float(v) for v in vs]\n\n\nclass TFModel(object):\n    def __init__(self, training_helper_layer, criterion, val_methods):\n\n        self.training_helper_layer = training_helper_layer\n        self.criterion = criterion\n        self.val_methods = val_methods\n\n    @staticmethod\n    def _expand_inputs(inputs, tensors_with_value, loss):\n        additional_inputs = []\n        additional_values = []\n        all_required_inputs = find_placeholders([loss])\n        all_required_inputs_names = [v.name for v in all_required_inputs]\n        if tensors_with_value:\n            for t, v in tensors_with_value.items():\n                if t.name in all_required_inputs_names:\n                    additional_inputs.append(t)\n                    additional_values.append(v)\n\n        if not isinstance(inputs, list):\n            inputs = nest.flatten(inputs)\n\n        return inputs, additional_inputs, additional_values\n\n    @staticmethod\n    def _process_session_config(session_config):\n        if session_config is not None:\n\n            assert isinstance(session_config, tf.ConfigProto), \\\n                ""session_config should be a tf.ConfigProto""\n            session_config.use_per_session_threads = True\n        return session_config\n\n    @staticmethod\n    def _process_grads(graph, grads):\n\n        with graph.as_default():\n            from zoo.util.tf import process_grad\n            grads = [process_grad(grad) for grad in grads]\n        return grads\n\n    @staticmethod\n    def _process_metrics(graph, metrics):\n\n        outputs = []\n        val_methods = None\n        if metrics is not None:\n            idx = 0\n            val_methods = []\n            for metric_name in metrics:\n                metric = metrics[metric_name]\n                if tf.is_numeric_tensor(metric):\n                    outputs.append(metric)\n                    val_methods.append(StatelessMetric(metric_name, idx))\n                    idx += 1\n                else:\n                    outputs += metric.outputs\n                    with graph.as_default():\n                        val_labels = [tf.identity(v) for v in metric.labels]\n                    outputs += val_labels\n                    method = TFValidationMethod(metric.val_method,\n                                                metric_name,\n                                                list(range(idx, idx + len(metric.outputs))),\n                                                list(range(idx + len(metric.outputs),\n                                                           idx + len(metric.outputs)\n                                                           + len(val_labels))))\n                    val_methods.append(method)\n                    idx += len(metric.outputs) + len(val_labels)\n\n        outputs = [tf.to_float(output) for output in outputs]\n        return outputs, val_methods\n\n    @staticmethod\n    def _process_variables(graph, variables, updates):\n\n        all_trainable_variables = variables\n\n        name2idx = dict([(v.name, idx) for idx, v in enumerate(all_trainable_variables)])\n\n        all_variables = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n\n        update_ops = graph.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n        if updates is not None:\n            update_ops += updates\n\n        trainable_variables = [0] * len(all_trainable_variables)\n        trainable_assigns = [0] * len(all_trainable_variables)\n        trainable_variable_placeholders = [0] * len(all_trainable_variables)\n        extra_variables = []\n        extra_variable_assigns = []\n        extra_variable_assign_placeholders = []\n        for v in all_variables:\n            p = tf.placeholder(dtype=v.dtype, shape=v.shape)\n            a = tf.assign(v, p)\n\n            # special treatment for ResourceVariable\n            if v.op.type == ""VarHandleOp"":\n                v_float_value = tf.to_float(v.read_value())\n            else:\n                v_float_value = tf.to_float(v)\n\n            if v.name in name2idx:\n                trainable_variables[name2idx[v.name]] = v_float_value\n                trainable_assigns[name2idx[v.name]] = a\n                trainable_variable_placeholders[name2idx[v.name]] = p\n            else:\n                extra_variables.append(v_float_value)\n                extra_variable_assigns.append(a)\n                extra_variable_assign_placeholders.append(p)\n\n        extra_variable_assign = tf.group(*extra_variable_assigns)\n        trainable_assign = tf.group(*trainable_assigns)\n        update_op = tf.group(update_ops)\n\n        return trainable_variables, trainable_variable_placeholders, trainable_assign, \\\n            extra_variables, extra_variable_assign_placeholders, \\\n            extra_variable_assign, update_op\n\n    @staticmethod\n    def _save_to_dir(folder, sess, graph,\n                     metric_tensors,\n                     batch_size_tensor,\n                     loss_tensor, inputs, labels, predictions,\n                     trainable_variables,\n                     trainable_variable_placeholders,\n                     trainable_assign,\n                     extra_variables,\n                     extra_variable_assign_placeholders,\n                     extra_variable_assign,\n                     grads, update_op, train_op,\n                     additional_inputs,\n                     additional_values):\n        saver = tf.train.Saver()\n        if not os.path.isdir(folder):\n            os.makedirs(folder)\n        saver.save(sess, os.path.join(folder, ""model""), write_meta_graph=False)\n\n        meta = {\n            ""inputs"": [i.name for i in inputs],\n            ""input_types"": [i.dtype.as_datatype_enum for i in inputs],\n            ""additional_inputs"": [i.name for i in additional_inputs],\n            ""additional_input_types"": [i.dtype.as_datatype_enum for i in additional_inputs],\n            ""labels"": [l.name for l in labels],\n            ""label_types"": [i.dtype.as_datatype_enum for i in labels],\n            ""predictions"": [t.name for t in predictions] if predictions else [],\n            ""metric_tensors"": [t.name for t in metric_tensors],\n            ""batch_size_tensor"": batch_size_tensor.name,\n            ""loss_tensor"": loss_tensor.name,\n            ""variables"": [v.name for v in trainable_variables],\n            ""variable_types"": [v.dtype.as_datatype_enum for v in trainable_variable_placeholders],\n            ""variable_assign_placeholders"": [v.name for v in trainable_variable_placeholders],\n            ""assign_variable_op"": trainable_assign.name,\n            ""extra_variables"": [v.name for v in extra_variables],\n            ""extra_variable_types"": [v.dtype.as_datatype_enum for v\n                                     in extra_variable_assign_placeholders],\n            ""extra_variable_assign_placeholders"": [p.name for p in\n                                                   extra_variable_assign_placeholders],\n            ""assign_extra_variable_op"": extra_variable_assign.name,\n            ""grad_variables"": [g.name for g in grads],\n            ""update_op"": update_op.name,\n            ""restore_op"": saver.saver_def.restore_op_name,\n            ""restore_path_placeholder"": saver.saver_def.filename_tensor_name,\n            ""save_op"": _to_operation_name(saver.saver_def.save_tensor_name),\n            ""save_path_placeholder"": saver.saver_def.filename_tensor_name,\n            ""default_tensor_value"": [_to_floats(v) for v in additional_values],\n            ""init_op"": tf.tables_initializer().name\n        }\n\n        if train_op is not None:\n            meta[""train_op""] = train_op.name\n\n        with open(os.path.join(folder, ""training_meta.json""), ""w"") as f:\n            f.write(json.dumps(meta))\n\n        with gfile.GFile(os.path.join(folder, ""model.meta""), ""wb"") as f:\n            f.write(graph.as_graph_def().SerializeToString())\n\n        return meta, saver\n\n    @staticmethod\n    def export(model_dir, loss_tensor, sess, inputs, labels, predictions, grads, variables, graph,\n               tensors_with_value, metrics, updates, train_op=None):\n        inputs, additional_inputs, additional_values = \\\n            TFModel._expand_inputs(inputs, tensors_with_value, loss_tensor)\n        metric_tensors, val_methods = TFModel._process_metrics(graph, metrics)\n        grads = TFModel._process_grads(graph, grads)\n\n        with graph.as_default():\n            batch_size_tensor = tf.to_float(tf.shape(inputs[0])[0])\n\n        trainable_variables, trainable_variable_placeholders, trainable_assign, \\\n            extra_variables, extra_variable_assign_placeholders, \\\n            extra_variable_assign, update_op = \\\n            TFModel._process_variables(graph, variables, updates)\n\n        meta, saver = \\\n            TFModel._save_to_dir(model_dir, sess, graph,\n                                 metric_tensors,\n                                 batch_size_tensor,\n                                 loss_tensor, inputs, labels, predictions,\n                                 trainable_variables,\n                                 trainable_variable_placeholders,\n                                 trainable_assign,\n                                 extra_variables,\n                                 extra_variable_assign_placeholders,\n                                 extra_variable_assign,\n                                 grads, update_op, train_op,\n                                 additional_inputs,\n                                 additional_values)\n        return meta, saver, val_methods\n\n    @staticmethod\n    def create(loss_tensor, sess, inputs, labels, predictions, grads, variables, graph,\n               tensors_with_value, session_config, metrics, updates,\n               model_dir, train_op=None):\n\n        if model_dir is None:\n            model_dir = tempfile.mkdtemp()\n        else:\n            if not os.path.isdir(model_dir):\n                os.makedirs(model_dir)\n\n        meta, saver, val_methods = TFModel.export(model_dir, loss_tensor, sess,\n                                                  inputs, labels, predictions, grads, variables,\n                                                  graph, tensors_with_value, metrics, updates,\n                                                  train_op)\n\n        training_helper_layer = TFTrainingHelper(model_dir,\n                                                 session_config, saver, meta, sess)\n\n        criterion = IdentityCriterion()\n\n        return TFModel(training_helper_layer, criterion, val_methods)\n\n\nclass TFOptimizer:\n    def __init__(self, tf_model, optim_method,\n                 sess=None, dataset=None,\n                 clip_norm=None, clip_value=None,\n                 model_dir=None):\n        """"""\n        TFOptimizer is used for distributed training of TensorFlow\n        on Spark/BigDL.\n\n        Note that if grads and variables are not None, then they need to be sorted by name\n        if you want to use multiple optimization methods for a TensorFlow model according to\n        variable names.\n\n        :param loss: The loss tensor of the TensorFlow model, should be a scalar\n        :param optim_method: the optimization method to be used, such as bigdl.optim.optimizer.Adam\n        :param sess: the current TensorFlow Session, if you want to used a pre-trained model, you\n        should use the Session to load the pre-trained variables and pass it to TFOptimizer.\n        """"""\n\n        self.optim_method = optim_method\n        self.sess = sess\n        self.dataset = dataset\n\n        self.clip_norm = clip_norm\n        if clip_value is not None and not isinstance(clip_value, tuple):\n            raise ValueError(""The clip_value argument should be a tuple (min_value, max_value)"")\n        self.clip_constant = clip_value\n\n        if self.dataset.batch_size <= 0:\n            raise ValueError(""You should set batch_size instead of batch_per_thread for training"")\n\n        self.model_dir = model_dir\n\n        self.tf_model = tf_model\n\n        batch_size = self.dataset.batch_size\n\n        self.train_data = self.dataset.get_training_data()\n        self.val_data = self.dataset.get_validation_data()\n\n        self.batch_size = batch_size\n\n        self.estimator = Estimator(self.tf_model.training_helper_layer,\n                                   self.optim_method,\n                                   self.model_dir)\n\n        if self.clip_norm:\n            self.estimator.set_l2_norm_gradient_clipping(self.clip_norm)\n        if self.clip_constant:\n            min_value, max_value = self.clip_constant\n            self.estimator.set_constant_gradient_clipping(min_value, max_value)\n\n    def load_checkpoint(self, path, version):\n        # todo make version optional\n        model_path = os.path.join(path, ""model.{}"".format(version))\n        optim_method_path = os.path.join(path, ""optimMethod-TFParkTraining.{}"".format(version))\n        self.tf_model.training_helper_layer.load_checkpoint(model_path)\n        self.optim_method = OptimMethod.load(optim_method_path)\n        self.estimator = Estimator(self.tf_model.training_helper_layer,\n                                   self.optim_method,\n                                   self.model_dir)\n        if self.clip_norm:\n            self.estimator.set_l2_norm_gradient_clipping(self.clip_norm)\n        if self.clip_constant:\n            min_value, max_value = self.clip_constant\n            self.estimator.set_constant_gradient_clipping(min_value, max_value)\n\n    @staticmethod\n    def _get_or_create_session(session):\n        if session is None:\n            sess = tf.Session()\n            sess.run(tf.global_variables_initializer())\n        else:\n            sess = session\n        return sess\n\n    @staticmethod\n    def _get_dataset_from_loss(loss):\n        all_required_inputs = find_placeholders([loss])\n        dataset = tf.get_collection(all_required_inputs[0].name)[0]\n        return dataset\n\n    @staticmethod\n    def _get_vars_grads(loss):\n\n        grads_vars = tf.train.GradientDescentOptimizer(0).compute_gradients(loss)\n        grads_vars.sort(key=lambda grad_var: grad_var[1].name)\n        variables = []\n        grads = []\n        for (grad, var) in grads_vars:\n            if grad is not None:\n                variables.append(var)\n                grads.append(grad)\n        return grads, variables\n\n    @staticmethod\n    def _get_vars_grads_from_train_op(train_op):\n        def predicate(t):\n            return t.name.split(""/"")[-1].startswith(""zoo_identity_op_for_grad"")\n\n        grads = find_tensors([train_op], predicate)\n        grad_ops = [grad.op for grad in grads]\n        variables = []\n        for grad in grad_ops:\n            var = list(grad.control_inputs)[0]\n            if var.name == ""VarHandleOp"":\n                variables.append(var)\n            else:\n                variables.append(list(var.outputs)[0])\n        # variables = [grad.op.control_inputs[0].outputs[0] for grad in grads]\n        return grads, variables\n\n    @classmethod\n    def from_train_op(cls, train_op, loss, *, inputs=None, labels=None, metrics=None, updates=None,\n                      sess=None, dataset=None, tensor_with_value=None, session_config=None,\n                      model_dir=None):\n        sess = TFOptimizer._get_or_create_session(sess)\n        grads, variables = TFOptimizer._get_vars_grads_from_train_op(train_op)\n        if dataset is None:\n            dataset = TFOptimizer._get_dataset_from_loss(loss)\n        _ = dataset.tensors  # trigger create tensors if not available\n        dataset_inputs = dataset._original_tensors\n        if isinstance(dataset_inputs, tuple) and len(dataset_inputs) == 2:\n            if inputs is None:\n                inputs = dataset_inputs[0]\n\n            if labels is None:\n                labels = dataset_inputs[1]\n        else:\n            if inputs is None:\n                inputs = dataset_inputs\n\n            if labels is None:\n                labels = []\n\n        inputs = nest.flatten(inputs)\n        labels = nest.flatten(labels)\n        return TFOptimizer._from_grads(loss=loss, sess=sess, inputs=inputs, labels=labels,\n                                       grads=grads,\n                                       variables=variables, dataset=dataset, metrics=metrics,\n                                       tensor_with_value=tensor_with_value,\n                                       optim_method=FakeOptimMethod(),\n                                       session_config=session_config, updates=updates,\n                                       model_dir=model_dir, train_op=train_op)\n\n    @classmethod\n    def _from_grads(cls, loss, sess, inputs, labels, grads, variables, dataset, optim_method=None,\n                    clip_norm=None, clip_value=None,\n                    metrics=None, tensor_with_value=None, session_config=None,\n                    model_dir=None, updates=None, train_op=None):\n        graph = loss.graph\n        if metrics is None:\n            metrics = {}\n\n        tf_model = TFModel.create(loss, sess, inputs, labels, [], grads, variables, graph,\n                                  tensor_with_value, session_config, metrics,\n                                  updates, model_dir=None, train_op=train_op)\n        return cls(tf_model, optim_method, sess=sess, dataset=dataset,\n                   clip_norm=clip_norm, clip_value=clip_value, model_dir=model_dir)\n\n    @classmethod\n    def from_loss(cls, loss, optim_method, session=None, inputs=None, dataset=None,\n                  val_outputs=None, val_labels=None, val_method=None,\n                  clip_norm=None, clip_value=None, metrics=None,\n                  tensor_with_value=None, session_config=None, model_dir=None, updates=None):\n        """"""\n        Create a TFOptimizer from a TensorFlow loss tensor.\n        The loss tensor must come from a TensorFlow graph that only takes TFDataset.tensors and\n        the tensors in `tensor_with_value` as inputs.\n        :param loss: The loss tensor of the TensorFlow model, should be a scalar\n        :param optim_method: the optimization method to be used, such as bigdl.optim.optimizer.Adam\n        :param session: the current TensorFlow Session, if you want to used a pre-trained model,\n        you should use the Session to load the pre-trained variables and pass it to TFOptimizer.\n        :param val_outputs: the validation output TensorFlow tensor to be used by val_methods\n        :param val_labels: the validation label TensorFlow tensor to be used by val_methods\n        :param val_method: the BigDL val_method(s) to be used.\n        :param clip_norm: float >= 0. Gradients will be clipped when their L2 norm exceeds\n        this value.\n        :param clip_value: float >= 0. Gradients will be clipped when their absolute value\n        exceeds this value.\n        :param metrics: a dictionary. The key should be a string representing the metric\'s name\n        and the value should be the corresponding TensorFlow tensor, which should be a scalar.\n        :param tensor_with_value: a dictionary. The key is TensorFlow tensor, usually a\n        placeholder, the value of the dictionary is a tuple of two elements. The first one of\n        the tuple is the value to feed to the tensor in training phase and the second one\n        is the value to feed to the tensor in validation phase.\n        :return: a TFOptimizer\n        """"""\n        sess = TFOptimizer._get_or_create_session(session)\n        grads, variables = TFOptimizer._get_vars_grads(loss)\n\n        if dataset is None and inputs is None:\n            dataset = TFOptimizer._get_dataset_from_loss(loss)\n            inputs = dataset._original_tensors\n        else:\n            if inputs is None:\n                raise ValueError(""please specify inputs"")\n            _ = dataset.tensors  # trigger creating placeholders\n\n        if isinstance(inputs, tuple) and len(inputs) == 2:\n            inputs, labels = inputs\n        else:\n            labels = []\n\n        inputs = nest.flatten(inputs)\n        labels = nest.flatten(labels)\n\n        if clip_value is not None:\n            if isinstance(clip_value, float) or isinstance(clip_value, int):\n                if clip_value <= 0:\n                    ValueError(""The clip_value argument should be positive number"")\n                clip_value = (-float(clip_value), float(clip_value))\n\n            if not isinstance(clip_value, tuple):\n                raise ValueError(""The clip_value argument should be"" +\n                                 "" a positive float/int which clips to"" +\n                                 "" (-clip_value, clip_value); "" +\n                                 ""or a tuple which clips to (min_value, max_value)"")\n\n        if val_method is not None:\n            val_methods = to_list(val_method)\n            if metrics is None:\n                metrics = {}\n\n            for i, method in enumerate(val_methods):\n                metrics[\'bigdl_metirc_\' + str(i)] = BigDLMetric(method, val_outputs, val_labels)\n\n        return TFOptimizer._from_grads(loss, sess, inputs, labels, grads, variables, dataset,\n                                       optim_method, clip_norm, clip_value,\n                                       metrics, tensor_with_value, session_config,\n                                       model_dir, updates)\n\n    @staticmethod\n    def export_training_model(export_dir, loss, sess, inputs, labels=None, predictions=None,\n                              metrics=None, tensor_with_value=None, updates=None):\n\n        grads, variables = TFOptimizer._get_vars_grads(loss)\n\n        TFModel.export(export_dir, loss, sess, inputs, labels, predictions, grads, variables,\n                       loss.graph, tensor_with_value, metrics, updates)\n        logging.info(""Exported TensorFlow model in {} for training"".format(export_dir))\n\n    @staticmethod\n    def _shape_match(model_shape, dataset_shape):\n\n        for i in range(len(dataset_shape)):\n            if dataset_shape[i].value is None:\n                return model_shape[i].value is None\n            else:\n                return dataset_shape[i].value == model_shape[i].value or \\\n                    model_shape[i].value is None\n\n    @classmethod\n    def from_keras(cls, keras_model, dataset, optim_method=None,\n                   session_config=None, model_dir=None):\n        """"""\n        Create a TFOptimizer from a tensorflow.keras model. The model must be compiled.\n        :param keras_model: the tensorflow.keras model, which must be compiled.\n        :param dataset: a TFDataset\n        :param optim_method: the optimization method to be used, such as bigdl.optim.optimizer.Adam\n        validation data.\n        :return:\n        """"""\n        import tensorflow.keras.backend as K\n\n        model_inputs = keras_model.inputs\n        if hasattr(keras_model, ""targets""):\n            model_targets = keras_model.targets\n        else:\n            model_targets = keras_model._targets\n\n        flatten_inputs = nest.flatten(dataset.feature_tensors)\n        assert len(model_inputs) == len(flatten_inputs), \\\n            (""the keras model and TFDataset should have the same number of tensors"" +\n             "" keras model has {} inputs "" +\n             ""while TFDataset has {} inputs"").format(len(model_inputs),\n                                                     len(flatten_inputs))\n        for i in range(len(flatten_inputs)):\n            if not TFOptimizer._shape_match(model_inputs[i].shape, flatten_inputs[i].shape):\n                raise ValueError((""The {}th input in keras model {}""\n                                  "" does not match the TFDataset""\n                                  ""input {}"").format(i,\n                                                     model_inputs[i],\n                                                     flatten_inputs[i]))\n\n        flatten_targets = nest.flatten(dataset.label_tensors)\n        assert len(model_targets) == len(flatten_targets), \\\n            (""the keras model and TFDataset should have the same number of tensors"" +\n             "" keras model has {} targets "" +\n             ""while TFDataset has {} labels"").format(len(model_targets),\n                                                     len(flatten_inputs))\n        # todo check targets shape, currently checking target shape will\n        # cause too much false alarm.\n\n        loss = keras_model.total_loss\n        variables = keras_model._collected_trainable_weights\n        variables.sort(key=lambda variable: variable.name)\n        keras_optimizer = keras_model.optimizer\n\n        grads = K.gradients(loss, variables)\n        if None in grads:\n            raise ValueError(\'An operation has `None` for gradient. \'\n                             \'Please make sure that all of your ops have a \'\n                             \'gradient defined (i.e. are differentiable). \'\n                             \'Common ops without gradient: \'\n                             \'K.argmax, K.round, K.eval.\')\n        clip_norm = None\n        clip_value = None\n        if hasattr(keras_optimizer, \'clipnorm\'):\n            clip_norm = keras_optimizer.clipnorm\n        if hasattr(keras_optimizer, \'clipvalue\'):\n            clip_value = (-keras_optimizer.clipvalue, keras_optimizer.clipvalue)\n\n        sess = K.get_session()\n        if optim_method is None:\n            optim_method = keras_optimizer\n        optim_method = to_bigdl_optim_method(optim_method)\n\n        if keras_model.metrics and (dataset.get_validation_data() is not None):\n            if isinstance(keras_model.metrics, dict):\n                raise ValueError(\n                    ""different metrics for different outputs are not supported right now"")\n\n            if dataset.get_validation_data() is None:\n                raise ValueError(""Validation data is not specified. Please set "" +\n                                 ""val_rdd in TFDataset"")\n\n            if len(keras_model.outputs) > 1:\n                if not all([name.endswith(""loss"") for name in keras_model.metrics_names]):\n                    raise ValueError(""metrics (except loss) for multi-head model is not supported"")\n                else:\n                    bigdl_val_methods = [Loss()]\n                    val_outputs = keras_model.outputs\n                    val_labels = model_targets\n            else:\n                bigdl_val_methods = \\\n                    [to_bigdl_metric(m, keras_model.loss) for m in keras_model.metrics_names]\n                val_outputs = keras_model.outputs\n                val_labels = model_targets\n        else:\n            val_outputs = None\n            val_labels = None\n            bigdl_val_methods = None\n\n        tensor_with_value = {\n            K.learning_phase(): [True, False]\n        }\n\n        updates = keras_model.updates\n\n        metrics = None\n\n        if bigdl_val_methods is not None:\n            val_methods = to_list(bigdl_val_methods)\n            metrics = {}\n            for i, method in enumerate(val_methods):\n                metrics[\'bigdl_metirc_\' + str(i)] = BigDLMetric(method, val_outputs, val_labels)\n\n        tf_model = TFModel.create(loss, sess, model_inputs, model_targets, keras_model.outputs,\n                                  grads, variables, loss.graph,\n                                  tensor_with_value, session_config, metrics,\n                                  updates, model_dir=None)\n\n        return cls(tf_model, optim_method, sess=sess, dataset=dataset,\n                   clip_norm=clip_norm, clip_value=clip_value, model_dir=model_dir)\n\n    def set_constant_gradient_clipping(self, min_value, max_value):\n        """"""\n        Configure constant clipping settings.\n\n        :param min_value: the minimum value to clip by\n        :param max_value: the maxmimum value to clip by\n        """"""\n        self.estimator.set_constant_gradient_clipping(min_value, max_value)\n\n    def set_gradient_clipping_by_l2_norm(self, clip_norm):\n        """"""\n        Configure L2 norm clipping settings.\n        :param clip_norm: gradient L2-Norm threshold\n        """"""\n        self.estimator.set_l2_norm_gradient_clipping(clip_norm)\n\n    def optimize(self, end_trigger=None, checkpoint_trigger=None):\n        """"""\n        Run the training loop of the this optimizer\n        :param end_trigger: BigDL\'s Trigger to indicate when to stop the training.\n        :param checkpoint_trigger: When to save a checkpoint and evaluate model.\n        """"""\n        if end_trigger is None:\n            end_trigger = MaxEpoch(1)\n\n        if checkpoint_trigger is None:\n            checkpoint_trigger = EveryEpoch()\n\n        if self.tf_model.val_methods and self.val_data is not None:\n            self.estimator.train_minibatch(train_set=self.train_data,\n                                           criterion=self.tf_model.criterion,\n                                           end_trigger=end_trigger,\n                                           checkpoint_trigger=checkpoint_trigger,\n                                           validation_set=self.val_data,\n                                           validation_method=self.tf_model.val_methods)\n        else:\n            self.estimator.train_minibatch(train_set=self.train_data,\n                                           criterion=self.tf_model.criterion,\n                                           end_trigger=end_trigger)\n\n        self.tf_model.training_helper_layer.get_weights_to_python()\n'"
pyzoo/zoo/tfpark/tf_predictor.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom zoo.pipeline.api.net.utils import find_placeholders, _check_the_same\nfrom zoo.tfpark.tfnet import TFNet\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass TFPredictor:\n    def __init__(self, sess, outputs, inputs=None, dataset=None):\n        \'\'\'\n        TFPredictor takes a list of TensorFlow tensors as the model outputs and\n        feed all the elements in TFDatasets to produce those outputs and returns\n        a Spark RDD with each of its elements representing the model prediction\n        for the corresponding input elements.\n\n        :param sess: the current TensorFlow Session, you should first use this session\n        to load the trained variables then pass into TFPredictor\n        :param outputs: the output tensors of the TensorFlow model\n        \'\'\'\n        if inputs is None:\n            dataset, inputs = TFPredictor._get_datasets_and_inputs(outputs)\n\n        self.sess = sess\n        self.dataset = dataset\n        self.inputs = inputs\n        self.tfnet = TFNet.from_session(sess, self.inputs, outputs)\n        if self.dataset.batch_per_thread <= 0:\n            raise ValueError(""You should set batch_per_thread on TFDataset "" +\n                             ""instead of batch_size for prediction"")\n\n    @staticmethod\n    def _get_datasets_and_inputs(outputs):\n        all_required_inputs = find_placeholders(outputs)\n        dataset = tf.get_collection(all_required_inputs[0].name)[0]\n        inputs = dataset.tensors\n        _check_the_same(all_required_inputs, inputs)\n        return dataset, inputs\n\n    @classmethod\n    def from_outputs(cls, sess, outputs):\n        dataset, inputs = TFPredictor._get_datasets_and_inputs(outputs)\n        return cls(sess, outputs, inputs, dataset)\n\n    @classmethod\n    def from_keras(cls, keras_model, dataset):\n\n        sess = K.get_session()\n\n        outputs = keras_model.outputs\n        inputs = keras_model.inputs\n        return cls(sess, outputs, inputs, dataset)\n\n    def predict(self):\n\n        return self.tfnet.predict(self.dataset.get_prediction_data(), mini_batch=True)\n'"
pyzoo/zoo/tfpark/tfnet.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport sys\nimport tempfile\n\nimport numpy as np\nimport six\nfrom pyspark import RDD\n\nfrom bigdl.nn.layer import Layer\nfrom zoo.common import JTensor, Sample\nfrom zoo.common.nncontext import getOrCreateSparkContext\nfrom zoo.common.utils import callZooFunc\nfrom zoo.feature.image import ImageSet\nfrom zoo.tfpark.tf_dataset import TFImageDataset, TFDataset, TFDataDataset\nimport logging\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\ndef to_sample_rdd(x, y, sc, num_slices=None):\n    """"""\n    Conver x and y into RDD[Sample]\n    :param sc: SparkContext\n    :param x: ndarray and the first dimension should be batch\n    :param y: ndarray and the first dimension should be batch\n    :param numSlices:\n    :return:\n    """"""\n    x_rdd = sc.parallelize(x, num_slices)\n    y_rdd = sc.parallelize(y, num_slices)\n    return x_rdd.zip(y_rdd).map(lambda item: Sample.from_ndarray(item[0], item[1]))\n\n\nclass TFNet(Layer):\n    def __init__(self, path, input_names=None, output_names=None,\n                 tf_session_config=None, jvalue=None, bigdl_type=""float""):\n\n        if jvalue is not None:\n            super(TFNet, self).__init__(jvalue, bigdl_type)\n            return\n\n        config_bytes = None\n        if tf_session_config is not None:\n            import tensorflow as tf\n            assert isinstance(tf_session_config, tf.ConfigProto)\n            tf_session_config.use_per_session_threads = True\n            config_bytes = bytearray(tf_session_config.SerializeToString())\n        if input_names is None and output_names is None:\n            if tf_session_config is None:\n                super(TFNet, self).__init__(None, bigdl_type,\n                                            path)\n            else:\n                super(TFNet, self).__init__(None, bigdl_type,\n                                            path, config_bytes)\n\n        else:\n            if isinstance(input_names, six.string_types):\n                input_names = [input_names]\n            if isinstance(output_names, six.string_types):\n                output_names = [output_names]\n            if tf_session_config is None:\n                super(TFNet, self).__init__(None, bigdl_type,\n                                            path,\n                                            input_names,\n                                            output_names)\n            else:\n                super(TFNet, self).__init__(None, bigdl_type,\n                                            path,\n                                            input_names,\n                                            output_names, config_bytes)\n\n    @staticmethod\n    def check_input(input):\n        """"""\n        :param input: ndarray or list of ndarray or JTensor or list of JTensor.\n        :return: (list of JTensor, isTable)\n        """"""\n\n        def to_jtensor(i):\n            if isinstance(i, np.ndarray):\n                return JTensor.from_ndarray(i)\n            elif isinstance(i, JTensor):\n                return i\n            else:\n                raise Exception(""Error unknown input type %s"" % type(i))\n\n        if type(input) is list:\n            if len(input) == 0:\n                raise Exception(\'Error when checking: empty input\')\n            return list(map(lambda i: to_jtensor(i), input)), True\n        else:\n            return [to_jtensor(input)], False\n\n    def predict(self, x, batch_per_thread=1, distributed=True, mini_batch=False):\n        """"""\n        Use a model to do prediction.\n        """"""\n        if isinstance(x, ImageSet):\n            results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                  self.value,\n                                  x,\n                                  batch_per_thread)\n            return ImageSet(results)\n\n        if isinstance(x, TFImageDataset):\n            results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                  self.value,\n                                  x.get_prediction_data(),\n                                  x.batch_per_thread)\n            return ImageSet(results)\n\n        if isinstance(x, TFDataset):\n            results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                  self.value,\n                                  x.get_prediction_data())\n            return results.map(lambda result: Layer.convert_output(result))\n\n        if mini_batch:\n            results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                  self.value,\n                                  x)\n            return results.map(lambda result: Layer.convert_output(result))\n\n        if distributed:\n            if isinstance(x, np.ndarray):\n                data_rdd = to_sample_rdd(x, np.zeros([x.shape[0]]), getOrCreateSparkContext())\n            elif isinstance(x, RDD):\n                data_rdd = x\n            else:\n                raise TypeError(""Unsupported prediction data type: %s"" % type(x))\n            results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                  self.value,\n                                  data_rdd,\n                                  batch_per_thread)\n            return results.map(lambda result: Layer.convert_output(result))\n        else:\n            start_idx = 0\n            results = []\n            while start_idx < len(x):\n                end_idx = min(start_idx + batch_per_thread, len(x))\n                results.append(self.forward(x[start_idx:end_idx]))\n                start_idx += batch_per_thread\n\n            return np.concatenate(results, axis=0)\n\n    def evaluate(self, dataset, batch_size, val_methods):\n\n        if isinstance(dataset, ImageSet):\n            return callZooFunc(self.bigdl_type,\n                               ""modelEvaluateImageFrame"",\n                               self.value,\n                               dataset.to_image_frame(),\n                               batch_size, val_methods)\n\n        if isinstance(dataset, TFImageDataset):\n            return callZooFunc(self.bigdl_type,\n                               ""modelEvaluateImageFrame"",\n                               self.value,\n                               dataset.get_evaluation_data(),\n                               batch_size, val_methods)\n        if isinstance(dataset, TFDataDataset):\n            return callZooFunc(self.bigdl_type,\n                               ""tfnetEvaluate"",\n                               self.value,\n                               dataset.get_evaluation_data(),\n                               val_methods)\n        if isinstance(dataset, TFDataset):\n            return callZooFunc(self.bigdl_type,\n                               ""modelEvaluate"",\n                               self.value,\n                               dataset.get_evaluation_data(),\n                               batch_size,\n                               val_methods)\n        else:\n            return callZooFunc(self.bigdl_type,\n                               ""modelEvaluate"",\n                               self.value,\n                               dataset, batch_size, val_methods)\n\n    @staticmethod\n    def from_export_folder(folder, tf_session_config=None):\n        """"""\n        Create a TFNet from an exported folder produced by `export_tf`\n        :param folder: the folder the TensorFlow model exported to\n        :param tf_session_config: an optional tf.ConfigProto object to\n                       set the session config in java side.\n                       This config does not necessarily be the same with your current session.\n                       E.g. sess_config = tf.ConfigProto(inter_op_parallelism_threads=1,\n                                                         intra_op_parallelism_threads=1)\n                            net = TFNet.from_session(sess, inputs, outputs, sess_config)\n        :return: a TFNet\n        """"""\n        if not os.path.isdir(folder):\n            raise ValueError(folder + "" does not exist"")\n        return TFNet(folder, tf_session_config=tf_session_config)\n\n    @staticmethod\n    def from_saved_model(model_path, tag=None, signature=None,\n                         inputs=None, outputs=None, tf_session_config=None, init_op=None):\n        """"""\n        Create a TFNet from an TensorFlow saved model\n        :param model_path: the path to the SavedModel path\n        :param tag: the tag to load in the saved model, default to ""serve""\n        :param signature: The signature of the SignatureDef that defines inputs\n                          and outputs of the graph. TFNet assumes inputs is sorted\n                          by their corresponding key in SignatureDef.\n        :param inputs: a list input tensor names of this model, you may want to use TensorFlow\'s\n                      command line tool to inspect the saved model to find the input tensor\n                      names e.g. `saved_model_cli show --dir {saved_model_path} --all`\n        :param outputs: a list output tensor names of this model, you may want to use TensorFlow\'s\n                      command line tool to inspect the saved model to find the output tensor\n                      names e.g. `saved_model_cli show --dir {saved_model_path} --all`\n        :param tf_session_config: an optional tf.ConfigProto object to\n                       set the session config in java side.\n                       This config does not necessarily be the same with your current session.\n                       E.g. sess_config = tf.ConfigProto(inter_op_parallelism_threads=1,\n                                                         intra_op_parallelism_threads=1)\n                            net = TFNet.from_session(sess, inputs, outputs, sess_config)\n        :return: a TFNet\n        """"""\n        config_bytes = None\n        if tf_session_config is not None:\n            import tensorflow as tf\n            assert isinstance(tf_session_config, tf.ConfigProto)\n            tf_session_config.use_per_session_threads = True\n            config_bytes = bytearray(tf_session_config.SerializeToString())\n\n        if inputs is None or outputs is None:\n            jvalue = callZooFunc(""float"", ""createTFNetFromSavedModel"",\n                                 model_path, tag, signature, config_bytes)\n        else:\n\n            jvalue = callZooFunc(""float"", ""createTFNetFromSavedModel"",\n                                 model_path, tag, inputs, outputs, config_bytes, init_op)\n        return TFNet(path=None, jvalue=jvalue)\n\n    @staticmethod\n    def from_session(sess, inputs, outputs,\n                     generate_backward=False,\n                     allow_non_differentiable_input=True,\n                     tf_session_config=None):\n        """"""\n        Create a TFNet from an a session and the inputs and outpus endpoints\n        of the TensorFlow graph.\n        :param sess: the TensorFlow session contain all the variables\n        :param inputs: a list of TensorFlow Tensor represents the input endpoints\n        of the TensorFlow graph\n        :param outputs: a list of TensorFlow Tensor represents the output endpoints\n        of the TensorFlow graph\n        :param tf_session_config: an optional tf.ConfigProto object to\n                       set the session config in java side.\n                       This config does not necessarily be the same with your current session.\n                       E.g. sess_config = tf.ConfigProto(inter_op_parallelism_threads=1,\n                                                         intra_op_parallelism_threads=1)\n                            net = TFNet.from_session(sess, inputs, outputs, sess_config)\n        :return a TFNet\n        """"""\n        from zoo.util.tf import export_tf\n        temp = tempfile.mkdtemp()\n        try:\n            if generate_backward:\n                logging.warning(""generate_backward option is deprecated, and will be removed in""\n                                + ""in future releases, please use TFPark ""\n                                + ""(https://analytics-zoo.github.io/master/""\n                                + ""#ProgrammingGuide/TFPark/tensorflow/) for TensorFlow training"")\n                export_tf(sess, temp, inputs, outputs,\n                          generate_backward, allow_non_differentiable_input)\n                net = TFNet.from_export_folder(temp, tf_session_config)\n            else:\n                import tensorflow as tf\n                init_op = tf.tables_initializer().name\n                input_dict = dict([(t.name, t) for t in inputs])\n                # work around feed and fetch the same tensor\n                outputs = [tf.identity(out) for out in outputs]\n                output_dict = dict([(t.name, t) for t in outputs])\n                tf.saved_model.simple_save(sess, temp, inputs=input_dict, outputs=output_dict)\n                net = TFNet.from_saved_model(temp, inputs=[t.name for t in inputs],\n                                             outputs=[t.name for t in outputs],\n                                             tf_session_config=tf_session_config, init_op=init_op)\n        finally:\n            import shutil\n            shutil.rmtree(temp)\n\n        return net\n'"
pyzoo/zoo/tfpark/zoo_optimizer.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport tensorflow as tf\nfrom bigdl.optim.optimizer import OptimMethod\nfrom zoo.util.tf import process_grad\n\n\nclass FakeOptimMethod(OptimMethod):\n\n    def __init__(self):\n        super(FakeOptimMethod, self).__init__(None, ""float"")\n\n\nclass ZooOptimizer(tf.train.Optimizer):\n    """"""An optimizer that wraps another tf.Optimizer, using an allreduce to\n    combine gradient values before applying gradients to model weights.""""""\n\n    def __init__(self, optimizer, name=None):\n        if name is None:\n            name = ""Zoo{}"".format(type(optimizer).__name__)\n        super(ZooOptimizer, self).__init__(name=name, use_locking=False)\n\n        self._optimizer = optimizer\n\n    def compute_gradients(self, *args, **kwargs):\n        """"""Compute gradients of all trainable variables.\n        See Optimizer.compute_gradients() for more info.\n        In DistributedOptimizer, compute_gradients() is overriden to also\n        allreduce the gradients before returning them.\n        """"""\n        gradients = self._optimizer.compute_gradients(*args, **kwargs)\n        results = []\n        for grad_var in gradients:\n            grad = grad_var[0]\n            var = grad_var[1]\n            grad = process_grad(grad)\n            with tf.control_dependencies([var]):\n                grad_i = tf.identity(grad, name=""zoo_identity_op_for_grad"")\n            results.append((grad_i, var))\n        return results\n\n    def apply_gradients(self, *args, **kwargs):\n        """"""Calls this same method on the underlying optimizer.""""""\n        return self._optimizer.apply_gradients(*args, **kwargs)\n\n    def get_slot(self, *args, **kwargs):\n        """"""Calls this same method on the underlying optimizer.""""""\n        return self._optimizer.get_slot(*args, **kwargs)\n\n    def get_slot_names(self, *args, **kwargs):\n        """"""Calls this same method on the underlying optimizer.""""""\n        return self._optimizer.get_slot_names(*args, **kwargs)\n\n    def variables(self, *args, **kwargs):\n        """"""Calls this same method on the underlying optimizer.""""""\n        return self._optimizer.variables(*args, **kwargs)\n\n    def _resource_apply_sparse(self, *args, **kwargs):\n        self._optimizer._resource_apply_sparse(*args, **kwargs)\n\n    def _resource_apply_dense(self, *args, **kwargs):\n        self._optimizer._resource_apply_sparse(*args, **kwargs)\n\n    def _apply_sparse(self, *args, **kwargs):\n        self._optimizer._apply_sparse(*args, **kwargs)\n\n    def _apply_dense(self, *args, **kwargs):\n        self._optimizer._apply_dense(*args, **kwargs)\n'"
pyzoo/zoo/util/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/util/engine.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nimport os\nimport glob\nimport warnings\n\n\ndef exist_pyspark():\n    # check whether pyspark package exists\n    try:\n        import pyspark\n        return True\n    except ImportError:\n        return False\n\n\ndef check_spark_source_conflict(spark_home, pyspark_path):\n    # Check if both $SPARK_HOME and pyspark package exist.\n    # Trigger a warning if two spark sources don\'t match.\n    if spark_home and not pyspark_path.startswith(spark_home):\n        warning_msg = ""Find both SPARK_HOME and pyspark. You may need to check whether they "" + \\\n                      ""match with each other. SPARK_HOME environment variable "" + \\\n                      ""is set to: "" + spark_home + \\\n                      "", and pyspark is found in: "" + pyspark_path + "". If they are unmatched, "" + \\\n                      ""you are recommended to use one source only to avoid conflict. "" + \\\n                      ""For example, you can unset SPARK_HOME and use pyspark only.""\n        warnings.warn(warning_msg)\n\n\ndef __prepare_spark_env():\n    spark_home = os.environ.get(\'SPARK_HOME\', None)\n    if exist_pyspark():\n        # use pyspark as the spark source\n        import pyspark\n        check_spark_source_conflict(spark_home, pyspark.__file__)\n    else:\n        # use $SPARK_HOME as the spark source\n        if not spark_home:\n            raise ValueError(\n                """"""Could not find Spark. Please make sure SPARK_HOME env is set:\n                   export SPARK_HOME=path to your spark home directory."""""")\n        print(""Using %s"" % spark_home)\n        py4j = glob.glob(os.path.join(spark_home, \'python/lib\', \'py4j-*.zip\'))[0]\n        pyspark = glob.glob(os.path.join(spark_home, \'python/lib\', \'pyspark*.zip\'))[0]\n        if py4j not in sys.path:\n            sys.path.insert(0, py4j)\n        if pyspark not in sys.path:\n            sys.path.insert(0, pyspark)\n\n\ndef __prepare_analytics_zoo_env():\n    jar_dir = os.path.abspath(__file__ + ""/../../"")\n    conf_paths = glob.glob(os.path.join(jar_dir, ""share/conf/*.conf""))\n    extra_resources_paths = glob.glob(os.path.join(jar_dir, ""share/extra-resources/*""))\n    analytics_zoo_classpath = get_analytics_zoo_classpath()\n\n    def append_path(env_var_name, path):\n        try:\n            if path not in os.environ[env_var_name]:\n                print(""Adding %s to %s"" % (path, env_var_name))\n                os.environ[env_var_name] = path + "":"" + os.environ[env_var_name]  # noqa\n        except KeyError:\n            os.environ[env_var_name] = path\n\n    if analytics_zoo_classpath:\n        append_path(""BIGDL_JARS"", analytics_zoo_classpath)\n\n    if conf_paths:\n        assert len(conf_paths) == 1, ""Expecting one conf, but got: %s"" % len(conf_paths)\n        if conf_paths[0] not in sys.path:\n            print(""Prepending %s to sys.path"" % conf_paths[0])\n            sys.path.insert(0, conf_paths[0])\n\n    if extra_resources_paths:\n        for resource in extra_resources_paths:\n            if resource not in extra_resources_paths:\n                print(""Prepending %s to sys.path"" % resource)\n                sys.path.insert(0, resource)\n\n    if os.environ.get(""BIGDL_JARS"", None) and is_spark_below_2_2():\n        for jar in os.environ[""BIGDL_JARS""].split("":""):\n            append_path(""SPARK_CLASSPATH"", jar)\n\n    if os.environ.get(""BIGDL_PACKAGES"", None):\n        for package in os.environ[""BIGDL_PACKAGES""].split("":""):\n            if package not in sys.path:\n                sys.path.insert(0, package)\n\n\ndef get_analytics_zoo_classpath():\n    """"""\n    Get and return the jar path for analytics-zoo if exists.\n    """"""\n    if os.getenv(""BIGDL_CLASSPATH""):\n        for path in os.getenv(""BIGDL_CLASSPATH"").split("":""):\n            if not os.path.exists(path):\n                raise ValueError(""Path {} specified BIGDL_CLASSPATH does not exist."".format(path))\n        return os.environ[""BIGDL_CLASSPATH""]\n    jar_dir = os.path.abspath(__file__ + ""/../../"")\n    jar_paths = glob.glob(os.path.join(jar_dir, ""share/lib/*.jar""))\n    if jar_paths:\n        assert len(jar_paths) == 1, ""Expecting one jar: %s"" % len(jar_paths)\n        return jar_paths[0]\n    return """"\n\n\ndef is_spark_below_2_2():\n    """"""\n    Check if spark version is below 2.2.\n    """"""\n    import pyspark\n    if hasattr(pyspark, ""version""):\n        full_version = pyspark.version.__version__\n        # We only need the general spark version (eg, 1.6, 2.2).\n        parts = full_version.split(""."")\n        spark_version = parts[0] + ""."" + parts[1]\n        if compare_version(spark_version, ""2.2"") >= 0:\n            return False\n    return True\n\n\ndef compare_version(version1, version2):\n    """"""\n    Compare version strings.\n    Return 1 if version1 is after version2;\n          -1 if version1 is before version2;\n           0 if two versions are the same.\n    """"""\n    v1_arr = version1.split(""."")\n    v2_arr = version2.split(""."")\n    len1 = len(v1_arr)\n    len2 = len(v2_arr)\n    len_max = max(len1, len2)\n    for x in range(len_max):\n        v1_token = 0\n        if x < len1:\n            v1_token = int(v1_arr[x])\n        v2_token = 0\n        if x < len2:\n            v2_token = int(v2_arr[x])\n        if v1_token < v2_token:\n            return -1\n        if v1_token > v2_token:\n            return 1\n    return 0\n\n\ndef prepare_env():\n    __prepare_spark_env()\n    __prepare_analytics_zoo_env()\n'"
pyzoo/zoo/util/nest.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport six\n\n\ndef flatten(seq):\n    if isinstance(seq, list):\n        results = []\n        for item in seq:\n            results.extend(flatten(item))\n        return results\n\n    if isinstance(seq, tuple):\n        seq = list(seq)\n        results = []\n        for item in seq:\n            results.extend(flatten(item))\n        return results\n\n    if isinstance(seq, dict):\n        sorted_keys = sorted(seq.keys())\n        result = []\n        for key in sorted_keys:\n            result.extend(flatten(seq[key]))\n        return result\n\n    return [seq]\n\n\ndef ptensor_to_numpy(seq):\n    return [t.numpy() for t in flatten(seq)]\n\n\ndef pack_sequence_as(structure, flat_sequence):\n    _, packed = _packed_nest_with_indices(structure, flat_sequence, 0)\n    return _sequence_like(structure, packed)\n\n\ndef _yield_value(iterable):\n    if isinstance(iterable, dict):\n        for key in _sorted(iterable):\n            yield iterable[key]\n    else:\n        for value in iterable:\n            yield value\n\n\ndef _sequence_like(instance, args):\n    if isinstance(instance, dict):\n        result = dict(zip(_sorted(instance), args))\n        return type(instance)((key, result[key]) for key in six.iterkeys(instance))\n    else:\n        # Not a namedtuple\n        return type(instance)(args)\n\n\ndef _packed_nest_with_indices(structure, flat, index):\n    packed = []\n    for s in _yield_value(structure):\n        if is_sequence(s):\n            new_index, child = _packed_nest_with_indices(s, flat, index)\n            packed.append(_sequence_like(s, child))\n            index = new_index\n        else:\n            packed.append(flat[index])\n            index += 1\n    return index, packed\n\n\ndef _get_attrs_values(obj):\n    attrs = getattr(obj.__class__, ""__attrs_attrs__"")\n    return [getattr(obj, a.name) for a in attrs]\n\n\ndef _sorted(dict_):\n    try:\n        return sorted(six.iterkeys(dict_))\n    except TypeError:\n        raise TypeError(""nest only supports dicts with sortable keys."")\n\n\ndef is_sequence(s):\n    return isinstance(s, dict) or isinstance(s, list) or isinstance(s, tuple)\n'"
pyzoo/zoo/util/spark.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport glob\n\nfrom pyspark import SparkContext\nfrom zoo.common.nncontext import init_spark_conf\nfrom zoo import init_nncontext\n\n\nclass SparkRunner:\n    def __init__(self,\n                 spark_log_level=""WARN"",\n                 redirect_spark_log=True):\n        self.spark_log_level = spark_log_level\n        self.redirect_spark_log = redirect_spark_log\n        self.PYTHON_ENV = ""python_env""\n        with SparkContext._lock:\n            if SparkContext._active_spark_context:\n                raise Exception(""There\'s existing SparkContext. Please close it first."")\n        import pyspark\n        print(""Current pyspark location is : {}"".format(pyspark.__file__))\n\n    # This is adopted from conda-pack.\n    def _pack_conda_main(self, args):\n        import sys\n        import traceback\n        from conda_pack.cli import fail, PARSER, context\n        import conda_pack\n        from conda_pack import pack, CondaPackException\n        args = PARSER.parse_args(args=args)\n        # Manually handle version printing to output to stdout in python < 3.4\n        if args.version:\n            print(\'conda-pack %s\' % conda_pack.__version__)\n            sys.exit(0)\n\n        try:\n            with context.set_cli():\n                pack(name=args.name,\n                     prefix=args.prefix,\n                     output=args.output,\n                     format=args.format,\n                     force=args.force,\n                     compress_level=args.compress_level,\n                     n_threads=args.n_threads,\n                     zip_symlinks=args.zip_symlinks,\n                     zip_64=not args.no_zip_64,\n                     arcroot=args.arcroot,\n                     dest_prefix=args.dest_prefix,\n                     verbose=not args.quiet,\n                     filters=args.filters)\n        except CondaPackException as e:\n            fail(""CondaPackError: %s"" % e)\n        except KeyboardInterrupt:\n            fail(""Interrupted"")\n        except Exception:\n            fail(traceback.format_exc())\n\n    def pack_penv(self, conda_name):\n        import tempfile\n        tmp_dir = tempfile.mkdtemp()\n        tmp_path = ""{}/{}.tar.gz"".format(tmp_dir, self.PYTHON_ENV)\n        print(""Start to pack current python env"")\n        self._pack_conda_main([""--output"", tmp_path, ""--n-threads"", ""8"", ""--name"", conda_name])\n        print(""Packing has been completed: {}"".format(tmp_path))\n        return tmp_path\n\n    def _create_sc(self, submit_args, conf):\n        from pyspark.sql import SparkSession\n        print(""pyspark_submit_args is: {}"".format(submit_args))\n        os.environ[\'PYSPARK_SUBMIT_ARGS\'] = submit_args\n        zoo_conf = init_spark_conf(conf)\n        sc = init_nncontext(conf=zoo_conf, redirect_spark_log=self.redirect_spark_log)\n        sc.setLogLevel(self.spark_log_level)\n\n        return sc\n\n    def _detect_python_location(self):\n        import subprocess\n        pro = subprocess.Popen(\n            ""command -v python"",\n            shell=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE)\n        out, err = pro.communicate()\n        out = out.decode(""utf-8"")\n        err = err.decode(""utf-8"")\n        errorcode = pro.returncode\n        if 0 != errorcode:\n            raise Exception(err +\n                            ""Cannot detect current python location.""\n                            ""Please set it manually by python_location"")\n        return out.strip()\n\n    def _get_bigdl_classpath_jar_name_on_driver(self):\n        from bigdl.util.engine import get_bigdl_classpath\n        bigdl_classpath = get_bigdl_classpath()\n        assert bigdl_classpath, ""Cannot find bigdl classpath""\n        return bigdl_classpath, bigdl_classpath.split(""/"")[-1]\n\n    def _get_zoo_classpath_jar_name_on_driver(self):\n        from zoo.util.engine import get_analytics_zoo_classpath\n        zoo_classpath = get_analytics_zoo_classpath()\n        assert zoo_classpath, ""Cannot find Analytics-Zoo classpath""\n        return zoo_classpath, zoo_classpath.split(""/"")[-1]\n\n    def _assemble_zoo_classpath_for_executor(self):\n        conda_env_path = ""/"".join(self._detect_python_location().split(""/"")[:-2])\n        python_interpreters = glob.glob(""{}/lib/python*"".format(conda_env_path))\n        assert len(python_interpreters) == 1, \\\n            ""Conda env should contain a single python, but got: {}:"".format(python_interpreters)\n        python_interpreter_name = python_interpreters[0].split(""/"")[-1]\n        prefix = ""{}/lib/{}/site-packages/"".format(self.PYTHON_ENV, python_interpreter_name)\n        return [""{}/zoo/share/lib/{}"".format(prefix,\n                                             self._get_zoo_classpath_jar_name_on_driver()[1]),\n                ""{}/bigdl/share/lib/{}"".format(prefix,\n                                               self._get_bigdl_classpath_jar_name_on_driver()[1])\n                ]\n\n    def init_spark_on_local(self, cores, conf=None, python_location=None):\n        print(""Start to getOrCreate SparkContext"")\n        if \'PYSPARK_PYTHON\' not in os.environ:\n            os.environ[\'PYSPARK_PYTHON\'] = \\\n                python_location if python_location else self._detect_python_location()\n        master = ""local[{}]"".format(cores)\n        zoo_conf = init_spark_conf(conf).setMaster(master)\n        sc = init_nncontext(conf=zoo_conf, redirect_spark_log=self.redirect_spark_log)\n        sc.setLogLevel(self.spark_log_level)\n        print(""Successfully got a SparkContext"")\n        return sc\n\n    def init_spark_on_yarn(self,\n                           hadoop_conf,\n                           conda_name,\n                           num_executor,\n                           executor_cores,\n                           executor_memory=""10g"",\n                           driver_memory=""1g"",\n                           driver_cores=4,\n                           extra_executor_memory_for_ray=None,\n                           extra_python_lib=None,\n                           penv_archive=None,\n                           additional_archive=None,\n                           hadoop_user_name=""root"",\n                           spark_yarn_archive=None,\n                           spark_conf=None,\n                           jars=None):\n        os.environ[""HADOOP_CONF_DIR""] = hadoop_conf\n        os.environ[\'HADOOP_USER_NAME\'] = hadoop_user_name\n        os.environ[\'PYSPARK_PYTHON\'] = ""{}/bin/python"".format(self.PYTHON_ENV)\n\n        def _yarn_opt(jars):\n\n            archive = ""{}#{}"".format(penv_archive, self.PYTHON_ENV)\n            if additional_archive:\n                archive = archive + "","" + additional_archive\n            command = "" --archives {} --num-executors {} "" \\\n                      "" --executor-cores {} --executor-memory {}"". \\\n                format(archive, num_executor, executor_cores, executor_memory)\n\n            if extra_python_lib:\n                command = command + "" --py-files {} "".format(extra_python_lib)\n            if jars:\n                command = command + "" --jars {}"".format(jars)\n            return command + "" --driver-class-path {}:{}"".\\\n                format(self._get_zoo_classpath_jar_name_on_driver()[0],\n                       self. _get_bigdl_classpath_jar_name_on_driver()[0])\n\n        def _submit_opt():\n            conf = {\n                ""spark.driver.memory"": driver_memory,\n                ""spark.driver.cores"": driver_cores,\n                ""spark.executor.cores"": executor_cores,\n                ""spark.executor.memory"": executor_memory,\n                ""spark.scheduler.minRegisterreResourcesRatio"": ""1.0""}\n            if extra_executor_memory_for_ray:\n                conf[""spark.executor.memoryOverhead""] = extra_executor_memory_for_ray\n            if spark_yarn_archive:\n                conf[""spark.yarn.archive""] = spark_yarn_archive\n            return "" --master yarn --deploy-mode client"" + _yarn_opt(jars) + \' pyspark-shell \', conf\n\n        pack_env = False\n        assert penv_archive or conda_name, \\\n            ""You should either specify penv_archive or conda_name explicitly""\n        try:\n            if not penv_archive:\n                penv_archive = self.pack_penv(conda_name)\n                pack_env = True\n\n            submit_args, conf = _submit_opt()\n\n            if not spark_conf:\n                spark_conf = {}\n            zoo_bigdl_path_on_executor = "":"".join(self._assemble_zoo_classpath_for_executor())\n\n            if ""spark.executor.extraClassPath"" in spark_conf:\n                spark_conf[""spark.executor.extraClassPath""] = ""{}:{}"".format(\n                    zoo_bigdl_path_on_executor, spark_conf[""spark.executor.extraClassPath""])\n            else:\n                spark_conf[""spark.executor.extraClassPath""] = zoo_bigdl_path_on_executor\n\n            spark_conf[""spark.executorEnv.PYTHONHOME""] = self.PYTHON_ENV\n\n            for item in spark_conf.items():\n                conf[str(item[0])] = str(item[1])\n            sc = self._create_sc(submit_args, conf)\n        finally:\n            if conda_name and penv_archive and pack_env:\n                os.remove(penv_archive)\n        return sc\n'"
pyzoo/zoo/util/tf.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom tensorflow.core.framework import attr_value_pb2\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.platform import gfile\nimport tensorflow as tf\nimport os\nimport json\nimport copy\n\nimport zoo.util.tf_graph_util as graph_util\n\n\ndef process_grad(grad):\n    if grad is not None:\n        grad = ops.convert_to_tensor_or_indexed_slices(grad)\n        if isinstance(grad, ops.IndexedSlices):\n            # In IndexedSlices is not supported in java api, we have to convert it to\n            # a dense tensor. This operation is potentially expensive, but there seems\n            # no work around\n            grad = tf.unsorted_segment_sum(grad.values, grad.indices,\n                                           grad.dense_shape[0])\n    return grad\n\n\ndef _to_operation_name(name):\n    return name.split("":"")[0]\n\n\ndef _to_floats(vs):\n    return [float(v) for v in vs]\n\n\ndef export_tf(sess, folder, inputs, outputs,\n              generate_backward=False, allow_non_differentiable_input=True):\n    """"""\n    Export the frozen tensorflow graph as well as the inputs/outputs information\n    to the folder for inference.\n\n    This function will\n    1. freeze the graph (replace all variables with constants)\n    2. strip all unused node as specified by inputs and outputs\n    3. add placeholder nodes as needed\n    4. write the frozen graph and inputs/outputs names to the folder\n\n    Note: There should not be any queuing operation between inputs and outputs\n\n    :param sess: tensorflow session holding the variables to be saved\n    :param folder: the folder where graph file and inputs/outputs information are saved\n    :param inputs: a list of tensorflow tensors that will be fed during inference\n    :param outputs: a list of tensorflow tensors that will be fetched during inference\n    :return:\n    """"""\n\n    output_node_names = list({t.op.name for t in outputs})\n\n    graph_def = sess.graph_def\n    graph = sess.graph\n\n    # clear device specifications\n    for node in graph_def.node:\n        node.device = """"\n\n    non_placeholder_input_names = []\n    type_enums = []\n    for input_tensor in inputs:\n        if input_tensor.op.type not in [""Placeholder"", ""PlaceholderWithDefault""]:\n            non_placeholder_input_names.append(input_tensor.name)\n            type_enums.append(input_tensor.dtype.as_datatype_enum)\n\n    output_names = [o.name for o in outputs]\n\n    all_variables = graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n\n    # freeze graph\n    frozen_graph_def = graph_util.convert_variables_to_constants(\n        sess,\n        graph_def,\n        output_node_names\n    )\n\n    optimized_graph_def, old_names2new = strip_unused(frozen_graph_def,\n                                                      non_placeholder_input_names,\n                                                      output_names,\n                                                      type_enums)\n\n    nodes_of_graph = []\n    for node in optimized_graph_def.node:\n        nodes_of_graph.append(node.name + "":0"")\n    nodes_of_graph_set = set(nodes_of_graph)\n\n    new_input_names = []\n    error_input_nodes = []\n    for t in inputs:\n        if t.name in old_names2new:\n            if old_names2new[t.name] not in nodes_of_graph_set:\n                error_input_nodes.append(""\\"""" + (t.name)[0:-2] + ""\\"""")\n            new_input_names.append(old_names2new[t.name])\n        else:\n            if t.name not in nodes_of_graph_set:\n                error_input_nodes.append(""\\"""" + (t.name)[0:-2] + ""\\"""")\n            new_input_names.append(t.name)\n\n    if error_input_nodes:\n        error_nodes_name = "" and "".join(error_input_nodes)\n        raise ValueError(""Node %s doesn\'t exist in the graph"" % str(error_nodes_name))\n\n    # check all placeholder in the graph are listed in the new_input_names:\n    new_input_nodes = {name.split("":"")[0] for name in new_input_names}\n    for node in optimized_graph_def.node:\n        if node.op == ""Placeholder"" and node.name not in new_input_nodes:\n            raise ValueError(\n                ""Node %s is a Placeholder but not listed in inputs, inputs are %s""\n                % (node.name, inputs))\n\n    temp_tensors = None\n    used_variables = []\n    grad_variables = []\n    grad_inputs = []\n    if generate_backward:\n        nodes = set([n.name for n in optimized_graph_def.node])\n        for v in all_variables:\n            if v.op.name in nodes:\n                used_variables.append(v.name)\n\n        with tf.Graph().as_default() as g:\n            tf.import_graph_def(optimized_graph_def, name=\'\')\n            output_tensors = [g.get_tensor_by_name(x) for x in output_names]\n            grad_output_placeholders = [tf.placeholder(dtype=x.dtype,\n                                                       name=x.name.split("":"")[0] + ""_grad"",\n                                                       shape=x.shape) for x in output_tensors]\n\n            variables = [g.get_tensor_by_name(x) for x in used_variables]\n\n            inputs = [g.get_tensor_by_name(x) for x in new_input_names]\n            grads = tf.gradients(output_tensors, variables + inputs,\n                                 grad_ys=grad_output_placeholders)\n\n            grads = [process_grad(grad) for grad in grads]\n\n            temp_tensors = _find_temp_tensors(grads, nodes)\n\n            grad_variables = [x.name for x in grads[0:len(variables)]]\n\n            grad_inputs = []\n            for i in range(len(variables), len(grads)):\n                grad = grads[i]\n                if grad is not None:\n                    grad_inputs.append(grad.name)\n                else:\n                    # if input is not differentiable, we just return zero\n                    input_tensor = inputs[i - len(variables)]\n                    if allow_non_differentiable_input:\n                        zero_grad = tf.zeros(shape=tf.shape(input_tensor))\n                        grad_inputs.append(zero_grad.name)\n                    else:\n                        raise ValueError(\n                            ""input tensor: %s is not differentiable"" % input_tensor.name)\n\n            optimized_graph_def = g.as_graph_def()\n\n    if not os.path.isdir(folder):\n        os.makedirs(folder)\n\n    with gfile.GFile(os.path.join(folder, ""frozen_inference_graph.pb""), ""wb"") as f:\n        f.write(optimized_graph_def.SerializeToString())\n\n    meta = {\n        ""input_names"": new_input_names,\n        ""output_names"": output_names\n    }\n\n    if generate_backward:\n        meta[""temp_tensors""] = list(temp_tensors)\n        meta[""variables""] = used_variables\n        meta[""grad_variables""] = grad_variables\n        meta[""grad_inputs""] = grad_inputs\n\n    with open(os.path.join(folder, ""graph_meta.json""), ""w"") as f:\n        f.write(json.dumps(meta))\n\n\ndef _find_temp_tensors(grads, forward_ops):\n    \'\'\'\n    find all the tensors that are used for computing grads and has been\n    computed during forward\n    :param grads:\n    :param forward_ops:\n    :return:\n    \'\'\'\n    import sys\n    is_py2 = sys.version[0] == \'2\'\n    if is_py2:\n        import Queue as queue\n    else:\n        import queue as queue\n    queue = queue.Queue()\n    for grad in grads:\n        queue.put(grad)\n\n    temp_tensors = set()\n    visited = set()\n    while not queue.empty():\n        tensor = queue.get()\n        # this is necessary, because input may not be differentiable\n        if tensor is None:\n            continue\n        else:\n            visited.add(tensor.name)\n            if tensor.op.type == ""Placeholder"":\n                continue\n            if tensor.op.name in forward_ops:\n                temp_tensors.add(tensor.name)\n                continue\n            for input_tensor in tensor.op.inputs:\n                # this is necessary because there may be a cycle in the graph such as tf.while_loop\n                if input_tensor.name not in visited:\n                    queue.put(input_tensor)\n    return temp_tensors\n\n\ndef strip_unused(input_graph_def, input_tensor_names, output_tensor_names,\n                 placeholder_type_enum):\n    """"""Removes unused nodes from a GraphDef.\n\n  Args:\n    input_graph_def: A graph with nodes we want to prune.\n    input_tensor_names: A list of the nodes we use as inputs.\n    output_tensor_names: A list of the output nodes.\n    placeholder_type_enum: The AttrValue enum for the placeholder data type, or\n        a list that specifies one value per input node name.\n\n  Returns:\n    A `GraphDef` with all unnecessary ops removed. and a map containing the old input\n    names to the new input names\n\n  Raises:\n    ValueError: If any element in `input_node_names` refers to a tensor instead\n      of an operation.\n    KeyError: If any element in `input_node_names` is not found in the graph.\n  """"""\n    for name in input_tensor_names:\n        if "":"" not in name:\n            raise ValueError(""Input \'%s\' appears to refer to a Operation, ""\n                             ""not a Tensor."" % name)\n\n    old2new = {}\n\n    # Here we replace the nodes we\'re going to override as inputs with\n    # placeholders so that any unused nodes that are inputs to them are\n    # automatically stripped out by extract_sub_graph().\n    not_found = {name for name in input_tensor_names}\n    input_node_names = {name.split("":"")[0] for name in input_tensor_names}\n    output_node_names = list({name.split("":"")[0] for name in output_tensor_names})\n    inputs_replaced_graph_def = graph_pb2.GraphDef()\n    for node in input_graph_def.node:\n        if node.name not in input_node_names:\n            for i in range(len(node.input)):\n                if _append_port(node.input[i]) in input_tensor_names:\n                    old_name = _append_port(node.input[i])\n                    not_found.remove(old_name)\n                    new_input_name = node.input[i].replace("":"", ""_"")\n                    placeholder_node = node_def_pb2.NodeDef()\n                    placeholder_node.op = ""Placeholder""\n                    placeholder_node.name = new_input_name\n                    if isinstance(placeholder_type_enum, list):\n                        input_node_index = input_tensor_names.index(old_name)\n                        placeholder_node.attr[""dtype""].CopyFrom(\n                            attr_value_pb2.AttrValue(type=placeholder_type_enum[\n                                input_node_index]))\n                    else:\n                        placeholder_node.attr[""dtype""].CopyFrom(\n                            attr_value_pb2.AttrValue(type=placeholder_type_enum))\n                    if ""_output_shapes"" in node.attr:\n                        placeholder_node.attr[""_output_shapes""].CopyFrom(\n                            node.attr[""_output_shapes""])\n                    node.input[i] = new_input_name\n                    old2new[old_name] = new_input_name + "":0""\n                    inputs_replaced_graph_def.node.extend([placeholder_node])\n            inputs_replaced_graph_def.node.extend([copy.deepcopy(node)])\n\n    if not_found:\n        raise KeyError(""The following input nodes were not found: %s\\n"" % not_found)\n\n    output_graph_def = graph_util.extract_sub_graph(inputs_replaced_graph_def,\n                                                    output_node_names)\n    return output_graph_def, old2new\n\n\ndef _append_port(input_name):\n    if input_name.find("":"") == -1:\n        return input_name + "":0""\n    else:\n        return input_name\n'"
pyzoo/zoo/util/tf_graph_util.py,0,"b'# This file is adapted from https://github.com/tensorflow/tensorflow/blob/master\n# /tensorflow/python/framework/graph_util_impl.py\n#\n# Copyright 2015 The TensorFlow Authors, 2019 Analytics Zoo Authors.\n# All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Helpers to manipulate a tensor graph in python.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport copy\nimport re\nimport six\n\nfrom tensorflow.core.framework import attr_value_pb2\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import deprecation\nfrom tensorflow.python.util.tf_export import tf_export\n\n_VARIABLE_OPS = {\n    ""Assign"",\n    ""AssignAdd"",\n    ""AssignSub"",\n    ""Queue"",\n    ""ScatterAdd"",\n    ""ScatterSub"",\n    ""ScatterUpdate"",\n    ""TruncatedNormal"",\n    ""Variable"",\n    ""VariableV2"",\n}\n\n\ndef _is_variable_op(op):\n    """"""Returns true if \'op\' refers to a Variable node.""""""\n    return op in _VARIABLE_OPS\n\n\n@deprecation.deprecated(\n    date=None,\n    instructions=""Use `tf.compat.v1.graph_util.must_run_on_cpu`"")\n@tf_export(v1=[""graph_util.must_run_on_cpu""])\ndef must_run_on_cpu(node, pin_variables_on_cpu=False):\n    """"""Returns True if the given node_def must run on CPU, otherwise False.\n    Args:\n      node: The node to be assigned to a device. Could be either an ops.Operation\n        or NodeDef.\n      pin_variables_on_cpu: If True, this function will return False if node_def\n        represents a variable-related op.\n    Returns:\n      True if the given node must run on CPU, otherwise False.\n    """"""\n\n    if isinstance(node, ops.Operation):\n        node_def = node.node_def\n    else:\n        assert isinstance(node, node_def_pb2.NodeDef)\n        node_def = node\n\n    # If the op is a variable-related op, should we pin it on CPU?\n    if pin_variables_on_cpu and _is_variable_op(node_def.op):\n        return True\n\n    # Constant operations producing a string or int32 must run on CPU.\n    if node_def.op == ""Const"":\n        # Get the value of the \'dtype\' attr\n        dtype = node_def.attr[""dtype""].type\n        if dtype == dtypes.string or dtype == dtypes.int32:\n            return True\n\n    if node_def.op in [""DynamicStitch"", ""ParallelDynamicStitch""]:\n        dtype = node_def.attr[""T""].type\n        if dtype == dtypes.int32:\n            # DynamicStitch on GPU only works for int32 values.\n            return True\n\n    if node_def.op in [""Cast""]:\n        dtype = node_def.attr[""SrcT""].type\n        if dtype == dtypes.int32:\n            # Cast on GPU does not works for int32 values.\n            return True\n    return False\n\n\n################################################################################\n#\n# device functions for use in with g.device(...)\n#\n################################################################################\n\n\ndef _node_name(n):\n    if n.startswith(""^""):\n        return n[1:]\n    else:\n        return n.split("":"")[0]\n\n\ndef _extract_graph_summary(graph_def):\n    """"""Extracts useful information from the graph and returns them.""""""\n    name_to_input_name = {}  # Keyed by the dest node name.\n    name_to_node = {}  # Keyed by node name.\n\n    # Keeps track of node sequences. It is important to still output the\n    # operations in the original order.\n    name_to_seq_num = {}  # Keyed by node name.\n    seq = 0\n    for node in graph_def.node:\n        n = _node_name(node.name)\n        name_to_node[n] = node\n        name_to_input_name[n] = [_node_name(x) for x in node.input]\n        if ""_class"" in node.attr:\n            # Prevent colocated nodes being lost\n            for v in node.attr[""_class""].list.s:\n                v_str = v.decode(""utf-8"")\n                if v_str.startswith(""loc:@""):\n                    colocated_node = v_str[5:]\n                    name_to_input_name[n].append(colocated_node)\n        name_to_seq_num[n] = seq\n        seq += 1\n    return name_to_input_name, name_to_node, name_to_seq_num\n\n\ndef _assert_nodes_are_present(name_to_node, nodes):\n    """"""Assert that nodes are present in the graph.""""""\n    for d in nodes:\n        assert d in name_to_node, ""%s is not in graph"" % d\n\n\ndef _bfs_for_reachable_nodes(target_nodes, name_to_input_name):\n    """"""Breadth first search for reachable nodes from target nodes.""""""\n    nodes_to_keep = set()\n    # Breadth first search to find all the nodes that we should keep.\n    next_to_visit = target_nodes[:]\n    while next_to_visit:\n        node = next_to_visit[0]\n        del next_to_visit[0]\n        if node in nodes_to_keep:\n            # Already visited this node.\n            continue\n        nodes_to_keep.add(node)\n        if node in name_to_input_name:\n            next_to_visit += name_to_input_name[node]\n    return nodes_to_keep\n\n\n@deprecation.deprecated(\n    date=None,\n    instructions=""Use `tf.compat.v1.graph_util.extract_sub_graph`"")\n@tf_export(v1=[""graph_util.extract_sub_graph""])\ndef extract_sub_graph(graph_def, dest_nodes):\n    """"""Extract the subgraph that can reach any of the nodes in \'dest_nodes\'.\n    Args:\n      graph_def: A graph_pb2.GraphDef proto.\n      dest_nodes: A list of strings specifying the destination node names.\n    Returns:\n      The GraphDef of the sub-graph.\n    Raises:\n      TypeError: If \'graph_def\' is not a graph_pb2.GraphDef proto.\n    """"""\n\n    if not isinstance(graph_def, graph_pb2.GraphDef):\n        raise TypeError(""graph_def must be a graph_pb2.GraphDef proto."")\n\n    if isinstance(dest_nodes, six.string_types):\n        raise TypeError(""dest_nodes must be a list."")\n\n    name_to_input_name, name_to_node, name_to_seq_num = _extract_graph_summary(\n        graph_def)\n    _assert_nodes_are_present(name_to_node, dest_nodes)\n\n    nodes_to_keep = _bfs_for_reachable_nodes(dest_nodes, name_to_input_name)\n\n    nodes_to_keep_list = sorted(\n        list(nodes_to_keep), key=lambda n: name_to_seq_num[n])\n    # Now construct the output GraphDef\n    out = graph_pb2.GraphDef()\n    for n in nodes_to_keep_list:\n        out.node.extend([copy.deepcopy(name_to_node[n])])\n    out.library.CopyFrom(graph_def.library)\n    out.versions.CopyFrom(graph_def.versions)\n\n    return out\n\n\n@deprecation.deprecated(\n    date=None,\n    instructions=""Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`""\n)\n@tf_export(v1=[""graph_util.tensor_shape_from_node_def_name""])\ndef tensor_shape_from_node_def_name(graph, input_name):\n    """"""Convenience function to get a shape from a NodeDef\'s input string.""""""\n    # To get a tensor, the name must be in the form <input>:<port>, for example\n    # \'Mul:0\'. The GraphDef input strings don\'t always have the port specified\n    # though, so if there isn\'t a colon we need to add a default \':0\' to the end.\n    if "":"" not in input_name:\n        canonical_name = input_name + "":0""\n    else:\n        canonical_name = input_name\n    tensor = graph.get_tensor_by_name(canonical_name)\n    shape = tensor.get_shape()\n    return shape\n\n\n@deprecation.deprecated(\n    date=None,\n    instructions=""Use `tf.compat.v1.graph_util.convert_variables_to_constants`"")\n@tf_export(v1=[""graph_util.convert_variables_to_constants""])\ndef convert_variables_to_constants(sess,\n                                   input_graph_def,\n                                   output_node_names,\n                                   variable_names_whitelist=None,\n                                   variable_names_blacklist=None):\n    """"""Replaces all the variables in a graph with constants of the same values.\n    If you have a trained graph containing Variable ops, it can be convenient to\n    convert them all to Const ops holding the same values. This makes it possible\n    to describe the network fully with a single GraphDef file, and allows the\n    removal of a lot of ops related to loading and saving the variables.\n    Args:\n      sess: Active TensorFlow session containing the variables.\n      input_graph_def: GraphDef object holding the network.\n      output_node_names: List of name strings for the result nodes of the graph.\n      variable_names_whitelist: The set of variable names to convert (by default,\n                                all variables are converted).\n      variable_names_blacklist: The set of variable names to omit converting\n                                to constants.\n    Returns:\n      GraphDef containing a simplified version of the original.\n    """"""\n\n    def trace_back_find_variable(origin_name, name_to_nodes):\n\n        nodes_in_path = set()\n        control_ops = [""Enter"", ""Exit"", ""NextIteration"", ""Switch""]\n\n        current_name = origin_name\n        while name_to_nodes[current_name].op != ""VarHandleOp"":\n            nodes_in_path.add(current_name)\n            current_node = name_to_nodes[current_name]\n            op_name = current_node.op\n            if op_name in control_ops or op_name == ""Identity"":\n                curr_input_name = _node_name(current_node.input[0])\n            else:\n                raise ValueError(""Op type %s should not be in the path "" +\n                                 ""between ReadVariableOp and VarHandleOp"" % current_node.op)\n            current_name = curr_input_name\n\n        return current_name, nodes_in_path\n\n    def create_const_op(node_name, dtype, data, data_shape=None):\n        """"""Creates a Const op.""""""\n        output_node = node_def_pb2.NodeDef()\n        output_node.op = ""Const""\n        output_node.name = node_name\n        output_node.attr[""dtype""].CopyFrom(dtype)\n        output_node.attr[""value""].CopyFrom(\n            attr_value_pb2.AttrValue(\n                tensor=tensor_util.make_tensor_proto(\n                    data, dtype=dtype.type, shape=data_shape)))\n        return output_node\n\n    # This graph only includes the nodes needed to evaluate the output nodes, and\n    # removes unneeded nodes like those involved in saving and assignment.\n    inference_graph = extract_sub_graph(input_graph_def, output_node_names)\n\n    # Identify the ops in the graph.\n    map_name_to_node = {\n        node.name: node for node in inference_graph.node\n    }\n\n    # Get list of variables.\n    variable_names = []\n    variable_dict_names = []\n    resource_identity_types = {}\n    read_variable_op_types = {}\n    for node in inference_graph.node:\n        if node.op in [""Variable"", ""VariableV2"", ""VarHandleOp""]:\n            variable_name = node.name\n            if ((variable_names_whitelist is not None\n                 and variable_name not in variable_names_whitelist)\n                or (variable_names_blacklist is not None\n                    and variable_name in variable_names_blacklist)):\n                continue\n            variable_dict_names.append(variable_name)\n            if node.op == ""VarHandleOp"":\n                variable_names.append(variable_name + ""/Read/ReadVariableOp:0"")\n            else:\n                variable_names.append(variable_name + "":0"")\n        elif node.op in [""ReadVariableOp"", ""ResourceGather"", ""VariableShape""]:\n            # There can be one or more Identity or control flow ops in between the ReadVariableOp\n            # and VarHandleOp.  Store them with the associated dtypes.\n            source_op_name, nodes_in_path = trace_back_find_variable(_node_name(node.input[0]),\n                                                                     map_name_to_node)\n            dtype = map_name_to_node[source_op_name].attr[""dtype""]\n            for node_name in nodes_in_path:\n                resource_identity_types[node_name] = dtype\n            read_variable_op_types[node.name] = dtype\n\n    # Gets map of variables and the associated data.\n    if variable_names:\n        returned_variables = sess.run(variable_names)\n    else:\n        returned_variables = []\n    variables_data_map = dict(zip(variable_dict_names, returned_variables))\n    logging.info(""Froze %d variables."", len(returned_variables))\n\n    # Reconstruct the graph with constants in place of variables.\n    output_graph_def = graph_pb2.GraphDef()\n    how_many_converted = 0\n    for input_node in inference_graph.node:\n        output_node = node_def_pb2.NodeDef()\n        if input_node.name in variables_data_map:\n            data = variables_data_map[input_node.name]\n            output_node = create_const_op(input_node.name, input_node.attr[""dtype""],\n                                          data, data.shape)\n            how_many_converted += 1\n        elif input_node.name in resource_identity_types:\n            # Converts the Identities of type RESOURCE_DT to the appropriate type\n            # based on the input they are referencing.\n            output_node.CopyFrom(input_node)\n            output_node.attr[""T""].CopyFrom(resource_identity_types[input_node.name])\n        elif input_node.op == ""ReadVariableOp"":\n            # The first branch converts all VarHandleOps of ResourceVariables to\n            # constants, so we need to convert the associated ReadVariableOps to\n            # Identity ops.\n            output_node.op = ""Identity""\n            output_node.name = input_node.name\n            output_node.input.extend([input_node.input[0]])\n            output_node.attr[""T""].CopyFrom(input_node.attr[""dtype""])\n            if ""_class"" in input_node.attr:\n                output_node.attr[""_class""].CopyFrom(input_node.attr[""_class""])\n        elif input_node.op == ""ResourceGather"":\n            # The first branch converts all VarHandleOps of ResourceGather to\n            # constants, so we need to convert the associated ResourceGather to Gather\n            # ops with a Const axis feeding into it.\n            if input_node.attr[""batch_dims""].i != 0:\n                raise ValueError(""batch_dims != 0 is not supported by freeze_graph."")\n            axis_data = input_node.attr[""batch_dims""].i\n            axis_node_name = input_node.name + ""/axis""\n            axis_dtype = input_node.attr[""Tindices""]\n            output_axis_node = create_const_op(axis_node_name, axis_dtype, axis_data)\n            output_graph_def.node.extend([output_axis_node])\n\n            output_node.op = ""GatherV2""\n            output_node.name = input_node.name\n            output_node.input.extend(\n                [input_node.input[0], input_node.input[1], axis_node_name])\n            output_node.attr[""Tparams""].CopyFrom(input_node.attr[""dtype""])\n            output_node.attr[""Tindices""].CopyFrom(input_node.attr[""Tindices""])\n            output_node.attr[""Taxis""].CopyFrom(axis_dtype)\n            if ""_class"" in input_node.attr:\n                output_node.attr[""_class""].CopyFrom(input_node.attr[""_class""])\n        elif input_node.op == ""VariableShape"":\n            output_node.op = ""Shape""\n            output_node.name = input_node.name\n            output_node.input.extend([input_node.input[0]])\n            output_node.attr[""T""].CopyFrom(read_variable_op_types[input_node.name])\n            output_node.attr[""out_type""].CopyFrom(input_node.attr[""out_type""])\n        else:\n            output_node.CopyFrom(input_node)\n        output_graph_def.node.extend([output_node])\n\n    output_graph_def.library.CopyFrom(inference_graph.library)\n    logging.info(""Converted %d variables to const ops."", how_many_converted)\n    return output_graph_def\n\n\n@deprecation.deprecated(\n    date=None,\n    instructions=""Use `tf.compat.v1.graph_util.remove_training_nodes`"")\n@tf_export(v1=[""graph_util.remove_training_nodes""])\ndef remove_training_nodes(input_graph, protected_nodes=None):\n    """"""Prunes out nodes that aren\'t needed for inference.\n    There are nodes like Identity and CheckNumerics that are only useful\n    during training, and can be removed in graphs that will be used for\n    nothing but inference. Here we identify and remove them, returning an\n    equivalent graph. To be specific, CheckNumerics nodes are always removed, and\n    Identity nodes that aren\'t involved in control edges are spliced out so that\n    their input and outputs are directly connected.\n    Args:\n      input_graph: Model to analyze and prune.\n      protected_nodes: An optional list of names of nodes to be kept\n        unconditionally. This is for example useful to preserve Identity output\n        nodes.\n    Returns:\n      A list of nodes with the unnecessary ones removed.\n    """"""\n    if not protected_nodes:\n        protected_nodes = []\n\n    types_to_remove = {""CheckNumerics"": True}\n\n    input_nodes = input_graph.node\n    names_to_remove = {}\n    for node in input_nodes:\n        if node.op in types_to_remove and node.name not in protected_nodes:\n            names_to_remove[node.name] = True\n\n    nodes_after_removal = []\n    for node in input_nodes:\n        if node.name in names_to_remove:\n            continue\n        new_node = node_def_pb2.NodeDef()\n        new_node.CopyFrom(node)\n        input_before_removal = node.input\n        del new_node.input[:]\n        for full_input_name in input_before_removal:\n            input_name = re.sub(r""^\\^"", """", full_input_name)\n            if input_name in names_to_remove:\n                continue\n            new_node.input.append(full_input_name)\n        nodes_after_removal.append(new_node)\n\n    types_to_splice = {""Identity"": True}\n    control_input_names = set()\n    node_names_with_control_input = set()\n    for node in nodes_after_removal:\n        for node_input in node.input:\n            if ""^"" in node_input:\n                control_input_names.add(node_input.replace(""^"", """"))\n                node_names_with_control_input.add(node.name)\n\n    names_to_splice = {}\n    for node in nodes_after_removal:\n        if node.op in types_to_splice and node.name not in protected_nodes:\n            # We don\'t want to remove nodes that have control edge inputs, because\n            # they might be involved in subtle dependency issues that removing them\n            # will jeopardize.\n            if node.name not in node_names_with_control_input:\n                names_to_splice[node.name] = node.input[0]\n\n    # We also don\'t want to remove nodes which are used as control edge inputs.\n    names_to_splice = {name: value for name, value in names_to_splice.items()\n                       if name not in control_input_names}\n\n    nodes_after_splicing = []\n    for node in nodes_after_removal:\n        if node.name in names_to_splice:\n            continue\n        new_node = node_def_pb2.NodeDef()\n        new_node.CopyFrom(node)\n        input_before_removal = node.input\n        del new_node.input[:]\n        for full_input_name in input_before_removal:\n            input_name = re.sub(r""^\\^"", """", full_input_name)\n            while input_name in names_to_splice:\n                full_input_name = names_to_splice[input_name]\n                input_name = re.sub(r""^\\^"", """", full_input_name)\n            new_node.input.append(full_input_name)\n        nodes_after_splicing.append(new_node)\n\n    output_graph = graph_pb2.GraphDef()\n    output_graph.node.extend(nodes_after_splicing)\n    return output_graph\n'"
pyzoo/zoo/zouwu/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/automl/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/automl/conftest.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pytest\nsc = None\nray_ctx = None\n\n\n@pytest.fixture(autouse=True, scope=\'package\')\ndef automl_fixture():\n    from zoo import init_spark_on_local\n    from zoo.ray import RayContext\n    sc = init_spark_on_local(cores=4, spark_log_level=""INFO"")\n    ray_ctx = RayContext(sc=sc, object_store_memory=""1g"")\n    ray_ctx.init()\n    yield\n    ray_ctx.stop()\n    sc.stop()\n\n\n# @pytest.fixture()\n# def setUpModule():\n#     sc = init_spark_on_local(cores=4, spark_log_level=""INFO"")\n#     ray_ctx = RayContext(sc=sc)\n#     ray_ctx.init()\n#\n#\n# def tearDownModule():\n#     ray_ctx.stop()\n#     sc.stop()\n'"
pyzoo/test/zoo/common/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/common/test_util.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nfrom bigdl.util.common import get_node_and_core_number\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.common import set_core_number\n\n\nclass TestUtil(ZooTestCase):\n\n    def test_set_core_num(self):\n        _, core_num = get_node_and_core_number()\n\n        set_core_number(core_num + 1)\n\n        _, new_core_num = get_node_and_core_number()\n\n        assert new_core_num == core_num + 1, \\\n            ""set_core_num failed, set the core"" \\\n            "" number to be {} but got {}"".format(core_num + 1, new_core_num)\n\n        set_core_number(core_num)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/feature/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/feature/test_feature_common.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.optim.optimizer import *\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.feature.common import *\nfrom zoo import init_nncontext, init_spark_conf\n\n\nclass TestFeatureCommon(ZooTestCase):\n\n    def setup_method(self, method):\n        """""" setup any state tied to the execution of the given method in a\n        class.  setup_method is invoked for every test method of a class.\n        """"""\n        sparkConf = init_spark_conf().setMaster(""local[4]"").setAppName(""test feature set"")\n        self.sc = init_nncontext(sparkConf)\n\n    def test_BigDL_adapter(self):\n        new_preprocessing = BigDLAdapter(Resize(1, 1))\n        assert isinstance(new_preprocessing, Preprocessing)\n\n    def test_relations(self):\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../resources"")\n        path = os.path.join(resource_path, ""qa"")\n        relations = Relations.read(path + ""/relations.txt"")\n        assert isinstance(relations, list)\n        relations2 = Relations.read(path + ""/relations.csv"", self.sc, 2)\n        assert isinstance(relations2, RDD)\n        relations3 = Relations.read_parquet(path + ""/relations.parquet"", self.sc)\n        assert isinstance(relations3, RDD)\n\n    def test_train_FeatureSet(self):\n\n        batch_size = 8\n        epoch_num = 5\n        images = []\n        labels = []\n        for i in range(0, 8):\n            features = np.random.uniform(0, 1, (200, 200, 3))\n            label = np.array([2])\n            images.append(features)\n            labels.append(label)\n\n        image_frame = DistributedImageFrame(self.sc.parallelize(images),\n                                            self.sc.parallelize(labels))\n\n        transformer = Pipeline([BytesToMat(), Resize(256, 256), CenterCrop(224, 224),\n                                ChannelNormalize(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n                                MatToTensor(), ImageFrameToSample(target_keys=[\'label\'])])\n        data_set = FeatureSet.image_frame(image_frame).transform(transformer).to_dataset()\n\n        model = Sequential()\n        model.add(SpatialConvolution(3, 1, 5, 5))\n        model.add(View([1 * 220 * 220]))\n        model.add(Linear(1 * 220 * 220, 20))\n        model.add(LogSoftMax())\n        optim_method = SGD(learningrate=0.01)\n        optimizer = Optimizer.create(\n            model=model,\n            training_set=data_set,\n            criterion=ClassNLLCriterion(),\n            optim_method=optim_method,\n            end_trigger=MaxEpoch(epoch_num),\n            batch_size=batch_size)\n        optimizer.set_validation(\n            batch_size=batch_size,\n            val_rdd=data_set,\n            trigger=EveryEpoch(),\n            val_method=[Top1Accuracy()]\n        )\n\n        trained_model = optimizer.optimize()\n\n        predict_result = trained_model.predict_image(image_frame.transform(transformer))\n        assert(predict_result.get_predict().count(), 8)\n\n    def create_feature_set_from_rdd(self):\n        dim = 2\n        data_len = 100\n\n        def gen_rand_sample():\n            features = np.random.uniform(0, 1, dim)\n            label = np.array((2 * features).sum() + 0.4)\n            return Sample.from_ndarray(features, label)\n\n        FeatureSet.rdd(self.sc.parallelize(range(0, data_len)).map(\n            lambda i: gen_rand_sample())).to_dataset()\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/orca/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/pipeline/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/ray/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/ray/test_ray_on_local.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom unittest import TestCase\n\nimport pytest\nimport ray\n\nfrom zoo import init_spark_on_local\nfrom zoo.ray import RayContext\n\n\nclass TestRayLocal(TestCase):\n\n    def test_local(self):\n        @ray.remote\n        class TestRay:\n            def hostname(self):\n                import socket\n                return socket.gethostname()\n\n        sc = init_spark_on_local(cores=4)\n        ray_ctx = RayContext(sc=sc, object_store_memory=""1g"")\n        address_info = ray_ctx.init()\n        assert ""object_store_address"" in address_info\n        actors = [TestRay.remote() for i in range(0, 4)]\n        print(ray.get([actor.hostname.remote() for actor in actors]))\n        ray_ctx.stop()\n        sc.stop()\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/ray/test_reinit_raycontext.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport time\nfrom unittest import TestCase\n\nimport numpy as np\nimport psutil\nimport pytest\nimport ray\n\nfrom zoo import init_spark_on_local\nfrom zoo.ray import RayContext\n\nnp.random.seed(1337)  # for reproducibility\n\n\n@ray.remote\nclass TestRay():\n    def hostname(self):\n        import socket\n        return socket.gethostname()\n\n\nclass TestUtil(TestCase):\n\n        def test_local(self):\n            node_num = 4\n            sc = init_spark_on_local(cores=node_num)\n            ray_ctx = RayContext(sc=sc, object_store_memory=""1g"")\n            ray_ctx.init()\n            actors = [TestRay.remote() for i in range(0, node_num)]\n            print(ray.get([actor.hostname.remote() for actor in actors]))\n            ray_ctx.stop()\n            time.sleep(3)\n            # repeat\n            print(""-------------------first repeat begin!------------------"")\n            ray_ctx = RayContext(sc=sc, object_store_memory=""1g"")\n            ray_ctx.init()\n            actors = [TestRay.remote() for i in range(0, node_num)]\n            print(ray.get([actor.hostname.remote() for actor in actors]))\n            ray_ctx.stop()\n            sc.stop()\n            time.sleep(3)\n            for process_info in ray_ctx.ray_processesMonitor.process_infos:\n                for pid in process_info.pids:\n                    assert not psutil.pid_exists(pid)\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/ray/test_util.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom unittest import TestCase\n\nimport pytest\n\nimport zoo.ray.utils as rutils\n\n\nclass TestUtil(TestCase):\n\n    def test_resource_to_bytes(self):\n        assert 10 == rutils.resource_to_bytes(""10b"")\n        assert 10000 == rutils.resource_to_bytes(""10k"")\n        assert 10000000 == rutils.resource_to_bytes(""10m"")\n        assert 10000000000 == rutils.resource_to_bytes(""10g"")\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/tfpark/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/tfpark/test_keras_model.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pytest\n\nfrom zoo.feature.image import *\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nimport tensorflow as tf\nimport numpy as np\nimport os\n\nfrom zoo.tfpark import KerasModel\n\nresource_path = os.path.join(os.path.split(__file__)[0], ""../resources"")\n\n\nclass TestTFParkModel(ZooTestCase):\n\n    def setup_method(self, method):\n        tf.keras.backend.clear_session()\n        super(TestTFParkModel, self).setup_method(method)\n\n    def create_multi_input_output_model(self):\n        data1 = tf.keras.layers.Input(shape=[10])\n        data2 = tf.keras.layers.Input(shape=[10])\n\n        x1 = tf.keras.layers.Flatten()(data1)\n        x1 = tf.keras.layers.Dense(10, activation=\'relu\')(x1)\n        pred1 = tf.keras.layers.Dense(2, activation=\'softmax\')(x1)\n\n        x2 = tf.keras.layers.Flatten()(data2)\n        x2 = tf.keras.layers.Dense(10, activation=\'relu\')(x2)\n        pred2 = tf.keras.layers.Dense(2)(x2)\n\n        model = tf.keras.models.Model(inputs=[data1, data2], outputs=[pred1, pred2])\n        model.compile(optimizer=\'rmsprop\',\n                      loss=[\'sparse_categorical_crossentropy\', \'mse\'])\n        return model\n\n    def create_training_data(self):\n        np.random.seed(20)\n        x = np.random.rand(20, 10)\n        y = np.random.randint(0, 2, (20))\n        return x, y\n\n    def test_training_with_validation_data_distributed_multi_heads(self):\n\n        keras_model = self.create_multi_input_output_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        val_x, val_y = self.create_training_data()\n\n        model.fit([x, x], [y, y], validation_data=([val_x, val_x], [val_y, val_y]),\n                  batch_size=4, distributed=True)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/tfpark/test_text_estimators.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nimport os\nimport random\nimport numpy as np\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.tfpark.text.estimator import *\n\nresource_path = os.path.join(os.path.split(__file__)[0], ""../resources"")\nbert_config_path = os.path.join(resource_path, ""bert/bert_config.json"")\n\n\nclass TestTextEstimators(ZooTestCase):\n\n    def setup_method(self, method):\n        tf.keras.backend.clear_session()\n        super(TestTextEstimators, self).setup_method(method)\n\n    def test_bert_classifier(self):\n        def gen_record(has_label=True):\n            res = dict()\n            res[""input_ids""] = np.random.randint(10000, size=2)\n            res[""input_mask""] = np.array([1] * 2)\n            res[""token_type_ids""] = np.array([0] * 1 + [1] * 1)\n            if has_label:\n                return res, np.array(random.choice([0, 1]))\n            else:\n                return res\n\n        estimator = BERTClassifier(2, bert_config_path, optimizer=tf.train.AdamOptimizer())\n        rdd = self.sc.parallelize([gen_record() for i in range(8)])\n        # Training is too slow and memory consuming for a unit test. Skip here. Tested manually.\n        # train_input_fn = bert_input_fn(rdd, 2, 4)\n        # estimator.train(train_input_fn, 2)\n        eval_input_fn = bert_input_fn(rdd, 2, 4)\n        print(estimator.evaluate(eval_input_fn, eval_methods=[""acc""]))\n        test_rdd = self.sc.parallelize([gen_record(has_label=False) for i in range(4)])\n        test_input_fn = bert_input_fn(test_rdd, 2, 4)\n        predictions = estimator.predict(test_input_fn)\n        assert predictions.count() == 4\n        assert len(predictions.first()) == 2\n\n    def test_bert_squad(self):\n        def gen_record(has_label=True):\n            res = dict()\n            res[""input_ids""] = np.random.randint(10000, size=2)\n            res[""input_mask""] = np.array([1] * 2)\n            res[""token_type_ids""] = np.array([0] * 1 + [1] * 1)\n            if has_label:\n                label = dict()\n                label[""start_position""] = np.array(0)\n                label[""end_position""] = np.array(0)\n                return res, label\n            else:\n                res[""unique_ids""] = np.array(np.random.randint(100))\n                return res\n        estimator = BERTSQuAD(bert_config_path, optimizer=tf.train.AdamOptimizer())\n        # Training is too slow and memory consuming for a unit test. Skip here. Tested manually.\n        # rdd = self.sc.parallelize([gen_record() for i in range(8)])\n        # train_input_fn = bert_input_fn(rdd, 2, 4, labels={""start_positions"", ""end_positions""})\n        # estimator.train(train_input_fn, 2)\n        test_rdd = self.sc.parallelize([gen_record(has_label=False) for i in range(4)])\n        test_input_fn = bert_input_fn(test_rdd, 2, 4, extra_features={""unique_ids"": (tf.int32, [])})\n        predictions = estimator.predict(test_input_fn)\n        assert predictions.count() == 4\n        assert isinstance(predictions.first(), dict)\n\n    def test_bert_ner(self):\n        def gen_record(has_label=True):\n            res = dict()\n            res[""input_ids""] = np.random.randint(10000, size=2)\n            res[""input_mask""] = np.array([1] * 2)\n            res[""token_type_ids""] = np.array([0] * 1 + [1] * 1)\n            if has_label:\n                return res, np.array(np.random.randint(10, size=2))\n            else:\n                return res\n        estimator = BERTNER(10, bert_config_path, optimizer=tf.train.AdamOptimizer())\n        # Training is too slow and memory consuming for a unit test. Skip here. Tested manually.\n        # rdd = self.sc.parallelize([gen_record() for i in range(8)])\n        # train_input_fn = bert_input_fn(rdd, 2, 4, label_size=2)\n        # estimator.train(train_input_fn, 2)\n        test_rdd = self.sc.parallelize([gen_record(has_label=False) for i in range(4)])\n        test_input_fn = bert_input_fn(test_rdd, 2, 4)\n        predictions = estimator.predict(test_input_fn)\n        assert predictions.count() == 4\n        assert len(predictions.first()) == 2\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/tfpark/test_text_models.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nimport numpy as np\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.tfpark.text.keras import *\n\n\nclass TestTextModels(ZooTestCase):\n\n    def setup_method(self, method):\n        import tensorflow as tf\n        tf.keras.backend.clear_session()\n        super(TestTextModels, self).setup_method(method)\n\n    def test_intent_entity(self):\n        model = IntentEntity(num_intents=8, num_entities=5, word_length=10,\n                             word_vocab_size=200, char_vocab_size=50)\n        input_data = [np.random.randint(200, size=(8, 30)), np.random.randint(50, size=(8, 30, 10))]\n        output = model.predict(input_data, distributed=True)\n        assert isinstance(output, list) and len(output) == 2\n        assert output[0].shape == (8, 8)\n        assert output[1].shape == (8, 30, 5)\n        self.assert_tfpark_model_save_load(model, input_data)\n\n    def test_ner_crf_reg_mode(self):\n        model = NER(num_entities=10, word_length=5, word_vocab_size=20, char_vocab_size=10)\n        input_data = [np.random.randint(20, size=(15, 12)), np.random.randint(10, size=(15, 12, 5))]\n        output = model.predict(input_data, distributed=True)\n        assert output.shape == (15, 12, 10)\n        self.assert_tfpark_model_save_load(model, input_data)\n\n    def test_ner_crf_pad_mode(self):\n        model = NER(num_entities=15, word_length=8, word_vocab_size=20,\n                    char_vocab_size=10, crf_mode=""pad"")\n        input_data = [np.random.randint(20, size=(4, 12)), np.random.randint(10, size=(4, 12, 8)),\n                      np.random.randint(12, size=(4, 1))]\n        output = model.predict(input_data, distributed=True)\n        assert output.shape == (4, 12, 15)\n        self.assert_tfpark_model_save_load(model, input_data)\n\n    def test_sequence_tagger_softmax(self):\n        model = SequenceTagger(num_pos_labels=5, num_chunk_labels=10, word_vocab_size=150)\n        input_data = np.random.randint(150, size=(10, 50))\n        output = model.predict(input_data, distributed=True)\n        assert isinstance(output, list) and len(output) == 2\n        assert output[0].shape == (10, 50, 5)\n        assert output[1].shape == (10, 50, 10)\n        self.assert_tfpark_model_save_load(model, input_data)\n\n    def test_sequence_tagger_crf(self):\n        model = SequenceTagger(num_pos_labels=8, num_chunk_labels=8, word_vocab_size=50,\n                               char_vocab_size=20, classifier=""crf"")\n        input_data = [np.random.randint(50, size=(10, 15)),\n                      np.random.randint(20, size=(10, 15, 12))]\n        output = model.predict(input_data, distributed=True)\n        assert isinstance(output, list) and len(output) == 2\n        assert output[0].shape == (10, 15, 8)\n        assert output[1].shape == (10, 15, 8)\n        self.assert_tfpark_model_save_load(model, input_data)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/tfpark/test_tf_dataset.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pytest\nfrom pyspark.ml.linalg import DenseVector\nfrom zoo.feature.common import ChainedPreprocessing, FeatureSet\nfrom zoo.feature.image import *\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.pipeline.api.keras.optimizers import Adam\nfrom zoo.tfpark import TFNet, TFOptimizer\nimport tensorflow as tf\nimport numpy as np\nimport os\n\nfrom zoo.tfpark import KerasModel, TFDataset\n\nresource_path = os.path.join(os.path.split(__file__)[0], ""../resources"")\n\n\ndef single_parse_fn(e):\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'raw\'),\n        \'image/class/label\': tf.FixedLenFeature(\n            [1], tf.int64, default_value=tf.zeros([1], dtype=tf.int64)),\n    }\n    items_to_handlers = {\n        \'image\': tf.contrib.slim.tfexample_decoder.Image(shape=[28, 28, 1], channels=1),\n        \'label\': tf.contrib.slim.tfexample_decoder.Tensor(\'image/class/label\', shape=[]),\n    }\n    decoder = tf.contrib.slim.tfexample_decoder.TFExampleDecoder(\n        keys_to_features, items_to_handlers)\n    results = decoder.decode(e)\n    if len(results[0].shape) > 0:\n        feature = results[0]\n        label = results[1]\n    else:\n        feature = results[1]\n        label = results[0]\n    return feature, label\n\n\ndef parse_fn(example):\n    results = tf.map_fn(single_parse_fn, example, dtype=(tf.uint8, tf.int64))\n    return tf.to_float(results[0]), results[1]\n\n\nclass TestTFDataset(ZooTestCase):\n\n    def setup_method(self, method):\n        tf.keras.backend.clear_session()\n        super(TestTFDataset, self).setup_method(method)\n\n    def create_model(self):\n        data = tf.keras.layers.Input(shape=[10])\n\n        x = tf.keras.layers.Flatten()(data)\n        x = tf.keras.layers.Dense(10, activation=\'relu\')(x)\n        predictions = tf.keras.layers.Dense(2, activation=\'softmax\')(x)\n\n        model = tf.keras.models.Model(inputs=data, outputs=predictions)\n        model.compile(optimizer=\'rmsprop\',\n                      loss=\'sparse_categorical_crossentropy\',\n                      metrics=[\'accuracy\'])\n        return model\n\n    def create_training_dataset(self):\n        np.random.seed(20)\n        x = np.random.rand(20, 10)\n        y = np.random.randint(0, 2, (20))\n\n        rdd_x = self.sc.parallelize(x)\n        rdd_y = self.sc.parallelize(y)\n\n        rdd = rdd_x.zip(rdd_y)\n\n        dataset = TFDataset.from_rdd(rdd,\n                                     features=(tf.float32, [10]),\n                                     labels=(tf.int32, []),\n                                     batch_size=4,\n                                     val_rdd=rdd\n                                     )\n        return dataset\n\n    def test_dataset_without_batch(self):\n        x = np.random.rand(20, 10)\n        y = np.random.randint(0, 2, (20))\n\n        rdd_x = self.sc.parallelize(x)\n        rdd_y = self.sc.parallelize(y)\n\n        rdd = rdd_x.zip(rdd_y)\n\n        dataset = TFDataset.from_rdd(rdd,\n                                     features=(tf.float32, [10]),\n                                     labels=(tf.int32, []),\n                                     names=[""features"", ""labels""],\n                                     val_rdd=rdd\n                                     )\n\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n        self.intercept(lambda: model.fit(dataset),\n                       ""The batch_size of TFDataset must be"" +\n                       "" specified when used in KerasModel fit."")\n\n        dataset = TFDataset.from_rdd(rdd,\n                                     features=(tf.float32, [10]),\n                                     labels=(tf.int32, []),\n                                     names=[""features"", ""labels""],\n                                     )\n        self.intercept(lambda: model.evaluate(dataset),\n                       ""The batch_per_thread of TFDataset must be "" +\n                       ""specified when used in KerasModel evaluate."")\n\n        dataset = TFDataset.from_rdd(rdd_x,\n                                     features=(tf.float32, [10]),\n                                     names=[""features"", ""labels""],\n                                     )\n        self.intercept(lambda: model.predict(dataset),\n                       ""The batch_per_thread of TFDataset must be"" +\n                       "" specified when used in KerasModel predict."")\n\n    def create_image_model(self):\n\n        data = tf.keras.layers.Input(shape=[224, 224, 3])\n        x = tf.keras.layers.Flatten()(data)\n        predictions = tf.keras.layers.Dense(10, activation=\'softmax\')(x)\n\n        model = tf.keras.models.Model(inputs=data, outputs=predictions)\n        model.compile(optimizer=\'rmsprop\',\n                      loss=\'sparse_categorical_crossentropy\',\n                      metrics=[\'accuracy\'])\n\n        return KerasModel(model)\n\n    def create_image_set(self, with_label):\n        image_set = self.get_raw_image_set(with_label)\n        transformer = ChainedPreprocessing([ImageResize(256, 256),\n                                            ImageRandomCrop(224, 224, True),\n                                            ImageMatToTensor(format=""NHWC""),\n                                            ImageSetToSample(input_keys=[""imageTensor""],\n                                                             target_keys=[""label""]\n                                                             if with_label else None)])\n        image_set = image_set.transform(transformer)\n        return image_set\n\n    def create_train_features_Set(self):\n        image_set = self.get_raw_image_set(with_label=True)\n        feature_set = FeatureSet.image_frame(image_set.to_image_frame())\n        train_transformer = ChainedPreprocessing([ImageBytesToMat(),\n                                                  ImageResize(256, 256),\n                                                  ImageRandomCrop(224, 224),\n                                                  ImageRandomPreprocessing(ImageHFlip(), 0.5),\n                                                  ImageChannelNormalize(\n                                                      0.485, 0.456, 0.406,\n                                                      0.229, 0.224, 0.225),\n                                                  ImageMatToTensor(to_RGB=True, format=""NHWC""),\n                                                  ImageSetToSample(input_keys=[""imageTensor""],\n                                                                   target_keys=[""label""])\n                                                  ])\n        feature_set = feature_set.transform(train_transformer)\n        feature_set = feature_set.transform(ImageFeatureToSample())\n        return feature_set\n\n    def test_training_for_imageset(self):\n\n        model = self.create_image_model()\n        image_set = self.create_image_set(with_label=True)\n        training_dataset = TFDataset.from_image_set(image_set,\n                                                    image=(tf.float32, [224, 224, 3]),\n                                                    label=(tf.int32, [1]),\n                                                    batch_size=4)\n        model.fit(training_dataset)\n\n    def test_training_for_feature_set(self):\n        model = self.create_image_model()\n        feature_set = self.create_train_features_Set()\n        training_dataset = TFDataset.from_feature_set(feature_set,\n                                                      features=(tf.float32, [224, 224, 3]),\n                                                      labels=(tf.int32, [1]),\n                                                      batch_size=8)\n        model.fit(training_dataset)\n\n    def test_evaluation_for_imageset(self):\n\n        model = self.create_image_model()\n        image_set = self.create_image_set(with_label=True)\n        eval_dataset = TFDataset.from_image_set(image_set,\n                                                image=(tf.float32, [224, 224, 3]),\n                                                label=(tf.int32, [1]),\n                                                batch_per_thread=1)\n\n        model.evaluate(eval_dataset)\n\n    def test_predict_for_imageset(self):\n        model = self.create_image_model()\n        image_set = self.create_image_set(with_label=False)\n\n        predict_dataset = TFDataset.from_image_set(image_set,\n                                                   image=(tf.float32, [224, 224, 3]),\n                                                   batch_per_thread=1)\n        results = model.predict(predict_dataset).get_predict().collect()\n        assert all(r[1] is not None for r in results)\n\n    def test_gradient_clipping(self):\n\n        data = tf.keras.layers.Input(shape=[10])\n\n        x = tf.keras.layers.Flatten()(data)\n        x = tf.keras.layers.Dense(10, activation=\'relu\')(x)\n        predictions = tf.keras.layers.Dense(2, activation=\'softmax\')(x)\n\n        model = tf.keras.models.Model(inputs=data, outputs=predictions)\n        model.compile(optimizer=tf.keras.optimizers.SGD(lr=1, clipvalue=1e-8),\n                      loss=\'sparse_categorical_crossentropy\',\n                      metrics=[\'accuracy\'])\n        model = KerasModel(model)\n\n        pre_weights = model.get_weights()\n\n        dataset = self.create_training_dataset()\n\n        # 5 iterations\n        model.fit(dataset)\n\n        current_weight = model.get_weights()\n\n        np.all(np.abs((current_weight[0] - pre_weights[0])) < 1e-7)\n\n    def test_tf_dataset_with_list_feature(self):\n        np.random.seed(20)\n        x = np.random.rand(20, 10)\n        y = np.random.randint(0, 2, (20))\n\n        rdd_x = self.sc.parallelize(x)\n        rdd_y = self.sc.parallelize(y)\n\n        rdd = rdd_x.zip(rdd_y)\n\n        dataset = TFDataset.from_rdd(rdd,\n                                     features=[(tf.float32, [10]), (tf.float32, [10])],\n                                     labels=(tf.int32, []),\n                                     batch_size=4,\n                                     val_rdd=rdd\n                                     )\n\n        for idx, tensor in enumerate(dataset.feature_tensors):\n            assert tensor.name == ""list_input_"" + str(idx) + "":0""\n\n    def test_tfdataset_with_string_rdd(self):\n        string_rdd = self.sc.parallelize([""123"", ""456""], 1)\n        ds = TFDataset.from_string_rdd(string_rdd, batch_per_thread=1)\n        input_tensor = tf.placeholder(dtype=tf.string, shape=(None,))\n        output_tensor = tf.string_to_number(input_tensor)\n        with tf.Session() as sess:\n            tfnet = TFNet.from_session(sess, inputs=[input_tensor], outputs=[output_tensor])\n        result = tfnet.predict(ds).collect()\n        assert result[0] == 123\n        assert result[1] == 456\n\n    def test_tfdataset_with_tfrecord(self):\n        train_path = os.path.join(resource_path, ""tfrecord/mnist_train.tfrecord"")\n        test_path = os.path.join(resource_path, ""tfrecord/mnist_test.tfrecord"")\n        dataset = TFDataset.from_tfrecord_file(self.sc, train_path,\n                                               batch_size=16,\n                                               validation_file_path=test_path)\n        raw_bytes = dataset.tensors[0]\n        images, labels = parse_fn(raw_bytes)\n        flat = tf.layers.flatten(images)\n        logits = tf.layers.dense(flat, 10)\n        loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits,\n                                                                     labels=labels))\n        opt = TFOptimizer.from_loss(loss, Adam())\n        opt.optimize()\n\n    def test_tfdataset_with_tf_data_dataset_which_requires_table(self):\n\n        keys = [1, 0, -1]\n        dataset = tf.data.Dataset.from_tensor_slices([1, 2, -1, 5] * 40)\n        table = tf.contrib.lookup.HashTable(\n            initializer=tf.contrib.lookup.KeyValueTensorInitializer(\n                keys=keys, values=list(reversed(keys))),\n            default_value=100)\n        dataset = dataset.map(table.lookup)\n\n        def transform(x):\n            float_x = tf.to_float(x)\n            return float_x, 1\n        dataset = dataset.map(transform)\n        dataset = TFDataset.from_tf_data_dataset(dataset, batch_size=16)\n        seq = tf.keras.Sequential(\n            [tf.keras.layers.Flatten(input_shape=()),\n             tf.keras.layers.Dense(10, activation=""softmax"")])\n        seq.compile(optimizer=tf.keras.optimizers.RMSprop(),\n                    loss=\'sparse_categorical_crossentropy\',\n                    metrics=[\'accuracy\'])\n        model = KerasModel(seq)\n        model.fit(dataset)\n\n    def test_tfdataset_with_tf_data_dataset_which_contains_bool(self):\n        dataset = tf.data.Dataset.from_tensor_slices((np.random.randn(100, 28, 28, 1),\n                                                      np.random.randint(0, 10, size=(100,)),\n                                                      np.ones(shape=(100, 28, 28, 1),\n                                                              dtype=np.bool)))\n        dataset = TFDataset.from_tf_data_dataset(dataset, batch_size=16)\n\n        feature, labels, mask = dataset.tensors\n\n        float_mask = tf.to_float(mask)\n        masked_feature = tf.to_float(feature) * float_mask\n        flatten = tf.layers.flatten(masked_feature)\n        logits = tf.layers.dense(flatten, 10)\n        loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits,\n                                                                     labels=labels))\n        opt = TFOptimizer.from_loss(loss, Adam())\n        opt.optimize()\n\n    def test_tfdataset_with_tf_data_dataset(self):\n        dataset = tf.data.Dataset.from_tensor_slices((np.random.randn(100, 28, 28, 1),\n                                                      np.random.randint(0, 10, size=(100,))))\n        dataset = dataset.map(lambda feature, label: (tf.to_float(feature), label))\n        dataset = TFDataset.from_tf_data_dataset(dataset, batch_size=16)\n        seq = tf.keras.Sequential(\n            [tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n             tf.keras.layers.Dense(10, activation=""softmax"")])\n\n        seq.compile(optimizer=tf.keras.optimizers.RMSprop(),\n                    loss=\'sparse_categorical_crossentropy\',\n                    metrics=[\'accuracy\'])\n        model = KerasModel(seq)\n        model.fit(dataset)\n        dataset = tf.data.Dataset.from_tensor_slices((np.random.randn(100, 28, 28, 1),\n                                                      np.random.randint(0, 10, size=(100,))))\n        dataset = dataset.map(lambda feature, label: (tf.to_float(feature),\n                                                      label))\n        dataset = TFDataset.from_tf_data_dataset(dataset, batch_per_thread=16)\n        model.evaluate(dataset)\n        dataset = tf.data.Dataset.from_tensor_slices(np.random.randn(100, 28, 28, 1))\n        dataset = dataset.map(lambda data: tf.to_float(data))\n        dataset = TFDataset.from_tf_data_dataset(dataset, batch_per_thread=16)\n        model.predict(dataset).collect()\n\n    def check_dataset(self, create_ds):\n\n        seq = tf.keras.Sequential(\n            [tf.keras.layers.Flatten(input_shape=(20,)),\n             tf.keras.layers.Dense(10, activation=""softmax"")])\n\n        seq.compile(optimizer=tf.keras.optimizers.RMSprop(),\n                    loss=\'sparse_categorical_crossentropy\',\n                    metrics=[\'accuracy\'])\n        model = KerasModel(seq)\n\n        model.fit(create_ds(""train""))\n        model.predict(create_ds(""predict"")).collect()\n        model.evaluate(create_ds(""evaluate""))\n\n    def make_create_ds_fn(self, train_df, val_df):\n        def create_ds(mode):\n            if mode == ""train"":\n                dataset = TFDataset.from_dataframe(train_df,\n                                                   feature_cols=[""feature""],\n                                                   labels_cols=[""label""],\n                                                   batch_size=32,\n                                                   validation_df=val_df)\n            elif mode == ""predict"":\n                dataset = TFDataset.from_dataframe(val_df,\n                                                   feature_cols=[""feature""],\n                                                   batch_per_thread=32)\n            elif mode == ""evaluate"":\n                dataset = TFDataset.from_dataframe(val_df,\n                                                   feature_cols=[""feature""],\n                                                   labels_cols=[""label""],\n                                                   batch_per_thread=32)\n            else:\n                raise ValueError(""unrecognized mode: {}"".format(mode))\n\n            return dataset\n        return create_ds\n\n    def test_tfdataset_with_dataframe(self):\n\n        rdd = self.sc.range(0, 1000)\n        df = rdd.map(lambda x: (DenseVector(np.random.rand(20).astype(np.float)),\n                                x % 10)).toDF([""feature"", ""label""])\n        train_df, val_df = df.randomSplit([0.7, 0.3])\n\n        create_ds = self.make_create_ds_fn(train_df, val_df)\n\n        self.check_dataset(create_ds)\n\n    def test_tfdataset_with_dataframe_arraytype(self):\n        rdd = self.sc.range(0, 1000)\n        df = rdd.map(lambda x: ([0.0] * 20, x % 10)).toDF([""feature"", ""label""])\n        train_df, val_df = df.randomSplit([0.7, 0.3])\n        create_ds = self.make_create_ds_fn(train_df, val_df)\n        self.check_dataset(create_ds)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/tfpark/test_tfnet.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.tfpark import TFNet, TFDataset\nfrom bigdl.util.common import *\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestTF(ZooTestCase):\n\n    resource_path = os.path.join(os.path.split(__file__)[0], ""../resources"")\n\n    def test_init_tf_net(self):\n        tfnet_path = os.path.join(TestTF.resource_path, ""tfnet"")\n        net = TFNet.from_export_folder(tfnet_path)\n        output = net.forward(np.random.rand(2, 4))\n        assert output.shape == (2, 2)\n\n    def test_for_scalar(self):\n        import tensorflow as tf\n        with tf.Graph().as_default():\n            input1 = tf.placeholder(dtype=tf.float32, shape=())\n            output = input1 + 1\n            sess = tf.Session()\n            net = TFNet.from_session(sess, [input1], [output])\n            sess.close()\n        out_value = net.forward(np.array(1.0))\n        assert len(out_value.shape) == 0\n\n        # the following test would fail on bigdl 0.6.0 due to a bug in bigdl,\n        # comment it out for now\n\n        # out_value = net.predict(np.array([1.0])).first()\n        # assert len(out_value.shape) == 0\n\n    def test_init_tfnet_from_session(self):\n        import tensorflow as tf\n        with tf.Graph().as_default():\n            input1 = tf.placeholder(dtype=tf.float32, shape=(None, 2))\n            label1 = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n            hidden = tf.layers.dense(input1, 4)\n            output = tf.layers.dense(hidden, 1)\n            loss = tf.reduce_mean(tf.square(output - label1))\n            grad_inputs = tf.gradients(loss, input1)\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                data = np.random.rand(2, 2)\n                output_value_ref = sess.run(output, feed_dict={input1: data})\n                label_value = output_value_ref - 1.0\n                grad_input_value_ref = sess.run(grad_inputs[0],\n                                                feed_dict={input1: data,\n                                                           label1: label_value})\n                net = TFNet.from_session(sess, [input1], [output], generate_backward=True)\n\n        output_value = net.forward(data)\n\n        grad_input_value = net.backward(data, np.ones(shape=(2, 1)))\n\n        self.assert_allclose(output_value, output_value_ref)\n        self.assert_allclose(grad_input_value, grad_input_value_ref)\n\n    def test_init_tfnet_from_saved_model(self):\n        model_path = os.path.join(TestTF.resource_path, ""saved-model-resource"")\n        tfnet = TFNet.from_saved_model(model_path, inputs=[""flatten_input:0""],\n                                       outputs=[""dense_2/Softmax:0""])\n        result = tfnet.predict(np.ones(dtype=np.float32, shape=(20, 28, 28, 1)))\n        result.collect()\n\n    def test_tf_net_predict(self):\n        tfnet_path = os.path.join(TestTF.resource_path, ""tfnet"")\n        import tensorflow as tf\n        tf_session_config = tf.ConfigProto(inter_op_parallelism_threads=1,\n                                           intra_op_parallelism_threads=1)\n        net = TFNet.from_export_folder(tfnet_path, tf_session_config=tf_session_config)\n        output = net.predict(np.random.rand(16, 4), batch_per_thread=5, distributed=False)\n        assert output.shape == (16, 2)\n\n    def test_tf_net_predict_dataset(self):\n        tfnet_path = os.path.join(TestTF.resource_path, ""tfnet"")\n        net = TFNet.from_export_folder(tfnet_path)\n        dataset = TFDataset.from_ndarrays((np.random.rand(16, 4),))\n        output = net.predict(dataset)\n        output = np.stack(output.collect())\n        assert output.shape == (16, 2)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/tfpark/test_tfpark_estimator.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nimport tensorflow as tf\n\nfrom zoo.feature.common import ChainedPreprocessing, FeatureSet\nfrom zoo.feature.image import *\nfrom zoo.tfpark import TFDataset, TFEstimator, ZooOptimizer\n\n\nclass TestTFParkEstimator(ZooTestCase):\n\n    def setup_method(self, method):\n        tf.keras.backend.clear_session()\n        super(TestTFParkEstimator, self).setup_method(method)\n\n    def create_model_fn(self):\n        def model_fn(features, labels, mode):\n            features = tf.layers.flatten(features)\n            h1 = tf.layers.dense(features, 64, activation=tf.nn.relu)\n            h2 = tf.layers.dense(h1, 64, activation=tf.nn.relu)\n            logits = tf.layers.dense(h2, 10)\n\n            if mode == tf.estimator.ModeKeys.EVAL or mode == tf.estimator.ModeKeys.TRAIN:\n                loss = tf.reduce_mean(\n                    tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels))\n                train_op = ZooOptimizer(tf.train.AdamOptimizer()).minimize(loss)\n                return tf.estimator.EstimatorSpec(mode, train_op=train_op,\n                                                  predictions=logits, loss=loss)\n            else:\n                return tf.estimator.EstimatorSpec(mode, predictions=logits)\n        return model_fn\n\n    def create_input_fn(self):\n\n        def input_fn(mode):\n            np.random.seed(20)\n            x = np.random.rand(20, 10)\n            y = np.random.randint(0, 10, (20))\n\n            rdd_x = self.sc.parallelize(x)\n            rdd_y = self.sc.parallelize(y)\n\n            rdd = rdd_x.zip(rdd_y)\n            if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n                dataset = TFDataset.from_rdd(rdd,\n                                             features=(tf.float32, [10]),\n                                             labels=(tf.int32, []),\n                                             batch_size=4)\n            elif mode == tf.estimator.ModeKeys.EVAL:\n                dataset = TFDataset.from_rdd(rdd,\n                                             features=(tf.float32, [10]),\n                                             labels=(tf.int32, []),\n                                             batch_per_thread=4)\n            else:\n                dataset = TFDataset.from_rdd(rdd_x,\n                                             features=(tf.float32, [10]),\n                                             batch_per_thread=4)\n            return dataset\n\n        return input_fn\n\n    def test_init_TFDataset_from_ndarrays(self):\n\n        model_fn = self.create_model_fn()\n\n        def input_fn(mode):\n            x = np.random.rand(20, 10)\n            y = np.random.randint(0, 10, (20,))\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                return TFDataset.from_ndarrays((x, y), batch_size=8)\n            elif mode == tf.estimator.ModeKeys.EVAL:\n                return TFDataset.from_ndarrays((x, y), batch_per_thread=1)\n            else:\n                return TFDataset.from_ndarrays(x, batch_per_thread=1)\n\n        estimator = TFEstimator.from_model_fn(model_fn)\n        estimator.train(input_fn, 10)\n        estimator.evaluate(input_fn, [""acc""])\n        estimator.predict(input_fn)\n\n    def test_training(self):\n        model_fn = self.create_model_fn()\n        input_fn = self.create_input_fn()\n        estimator = TFEstimator.from_model_fn(model_fn)\n        estimator.train(input_fn, steps=60000 // 320)\n\n    def test_evaluating(self):\n        model_fn = self.create_model_fn()\n        input_fn = self.create_input_fn()\n        estimator = TFEstimator.from_model_fn(model_fn)\n        eval_results = estimator.evaluate(input_fn, [""acc""])\n        assert len(eval_results) > 0\n\n    def test_predict(self):\n        model_fn = self.create_model_fn()\n        input_fn = self.create_input_fn()\n        estimator = TFEstimator.from_model_fn(model_fn)\n        results = estimator.predict(input_fn).collect()\n\n    def test_estimator_without_batch(self):\n        def model_fn(features, labels, mode):\n\n            assert features.shape.ndims == 1\n            if labels is not None:\n                assert labels.shape.ndims == 0\n\n            features = tf.expand_dims(features, axis=0)\n\n            h1 = tf.layers.dense(features, 64, activation=tf.nn.relu)\n            h2 = tf.layers.dense(h1, 64, activation=tf.nn.relu)\n            logits = tf.layers.dense(h2, 10)\n\n            if mode == tf.estimator.ModeKeys.EVAL or mode == tf.estimator.ModeKeys.TRAIN:\n                labels = tf.expand_dims(labels, axis=0)\n                loss = tf.reduce_mean(\n                    tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels))\n                train_op = ZooOptimizer(tf.train.AdamOptimizer()).minimize(loss)\n                return tf.estimator.EstimatorSpec(mode, train_op=train_op,\n                                                  predictions=logits, loss=loss)\n            else:\n                return tf.estimator.EstimatorSpec(mode, predictions=logits)\n\n        def input_fn(mode):\n            np.random.seed(20)\n            x = np.random.rand(20, 10)\n            y = np.random.randint(0, 10, (20))\n\n            rdd_x = self.sc.parallelize(x)\n            rdd_y = self.sc.parallelize(y)\n\n            rdd = rdd_x.zip(rdd_y)\n            if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n                dataset = TFDataset.from_rdd(rdd,\n                                             features=(tf.float32, [10]),\n                                             labels=(tf.int32, []))\n            else:\n                dataset = TFDataset.from_rdd(rdd_x,\n                                             features=(tf.float32, [10]))\n            return dataset\n\n        estimator = TFEstimator.from_model_fn(model_fn)\n\n        self.intercept(lambda: estimator.train(input_fn, steps=1),\n                       ""The batch_size of TFDataset must be specified when used for training."")\n\n        estimator.evaluate(input_fn, [""acc""])\n        estimator.predict(input_fn).collect()\n\n    def create_imageset_input_fn(self):\n        def input_fn(mode):\n            import os\n            resource_path = os.path.join(os.path.split(__file__)[0], ""../resources"")\n            if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n                image_folder = os.path.join(resource_path, ""cat_dog"")\n                image_set = ImageSet.read(image_folder, with_label=True, sc=self.sc,\n                                          one_based_label=False)\n                transformer = ChainedPreprocessing([ImageResize(256, 256),\n                                                    ImageRandomCrop(224, 224, True),\n                                                    ImageMatToTensor(format=""NHWC""),\n                                                    ImageSetToSample(input_keys=[""imageTensor""],\n                                                                     target_keys=[""label""])])\n                image_set = image_set.transform(transformer)\n                dataset = TFDataset.from_image_set(image_set,\n                                                   image=(tf.float32, [224, 224, 3]),\n                                                   label=(tf.int32, [1]),\n                                                   batch_size=8)\n            else:\n                image_folder = os.path.join(resource_path, ""cat_dog/*/*"")\n                image_set = ImageSet.read(image_folder, with_label=False, sc=self.sc,\n                                          one_based_label=False)\n                transformer = ChainedPreprocessing([ImageResize(256, 256),\n                                                    ImageRandomCrop(224, 224, True),\n                                                    ImageMatToTensor(format=""NHWC""),\n                                                    ImageSetToSample(\n                                                        input_keys=[""imageTensor""])])\n                image_set = image_set.transform(transformer)\n                dataset = TFDataset.from_image_set(image_set,\n                                                   image=(tf.float32, [224, 224, 3]),\n                                                   batch_per_thread=8)\n\n            return dataset\n        return input_fn\n\n    def test_estimator_for_imageset(self):\n\n        model_fn = self.create_model_fn()\n        input_fn = self.create_imageset_input_fn()\n\n        estimator = TFEstimator.from_model_fn(model_fn)\n        estimator.train(input_fn, steps=1)\n        estimator.evaluate(input_fn, [""acc""])\n        results = estimator.predict(input_fn).get_predict().collect()\n        assert all(r[1] is not None for r in results)\n\n    def create_train_feature_set_input_fn(self):\n        def input_fn(mode):\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                image_set = self.get_raw_image_set(with_label=True)\n                feature_set = FeatureSet.image_frame(image_set.to_image_frame())\n                train_transformer = ChainedPreprocessing([ImageBytesToMat(),\n                                                          ImageResize(256, 256),\n                                                          ImageRandomCrop(224, 224),\n                                                          ImageRandomPreprocessing(\n                                                              ImageHFlip(), 0.5),\n                                                          ImageChannelNormalize(\n                                                              0.485, 0.456, 0.406,\n                                                              0.229, 0.224, 0.225),\n                                                          ImageMatToTensor(\n                                                              to_RGB=True, format=""NHWC""),\n                                                          ImageSetToSample(\n                                                              input_keys=[""imageTensor""],\n                                                              target_keys=[""label""])\n                                                          ])\n                feature_set = feature_set.transform(train_transformer)\n                feature_set = feature_set.transform(ImageFeatureToSample())\n                training_dataset = TFDataset.from_feature_set(feature_set,\n                                                              features=(tf.float32, [224, 224, 3]),\n                                                              labels=(tf.int32, [1]),\n                                                              batch_size=8)\n                return training_dataset\n            else:\n                raise NotImplementedError\n        return input_fn\n\n    def test_estimator_for_feature_set(self):\n        model_fn = self.create_model_fn()\n        input_fn = self.create_train_feature_set_input_fn()\n\n        estimator = TFEstimator.from_model_fn(model_fn)\n        estimator.train(input_fn, steps=1)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/tfpark/test_tfpark_model.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nimport tensorflow as tf\nimport numpy as np\nimport os\n\nfrom zoo.tfpark import KerasModel, TFDataset, TFOptimizer\n\nresource_path = os.path.join(os.path.split(__file__)[0], ""../resources"")\n\n\nclass TestTFParkModel(ZooTestCase):\n\n    def setup_method(self, method):\n        tf.keras.backend.clear_session()\n        super(TestTFParkModel, self).setup_method(method)\n\n    def create_model(self):\n        data = tf.keras.layers.Input(shape=[10])\n\n        x = tf.keras.layers.Flatten()(data)\n        x = tf.keras.layers.Dense(10, activation=\'relu\')(x)\n        predictions = tf.keras.layers.Dense(2, activation=\'softmax\')(x)\n\n        model = tf.keras.models.Model(inputs=data, outputs=predictions)\n        model.compile(optimizer=\'rmsprop\',\n                      loss=\'sparse_categorical_crossentropy\',\n                      metrics=[\'accuracy\'])\n        return model\n\n    def create_multi_input_output_model(self):\n        data1 = tf.keras.layers.Input(shape=[10])\n        data2 = tf.keras.layers.Input(shape=[10])\n\n        x1 = tf.keras.layers.Flatten()(data1)\n        x1 = tf.keras.layers.Dense(10, activation=\'relu\')(x1)\n        pred1 = tf.keras.layers.Dense(2, activation=\'softmax\')(x1)\n\n        x2 = tf.keras.layers.Flatten()(data2)\n        x2 = tf.keras.layers.Dense(10, activation=\'relu\')(x2)\n        pred2 = tf.keras.layers.Dense(2)(x2)\n\n        model = tf.keras.models.Model(inputs=[data1, data2], outputs=[pred1, pred2])\n        model.compile(optimizer=\'rmsprop\',\n                      loss=[\'sparse_categorical_crossentropy\', \'mse\'])\n        return model\n\n    def create_training_data(self):\n        np.random.seed(20)\n        x = np.random.rand(20, 10)\n        y = np.random.randint(0, 2, (20))\n        return x, y\n\n    def create_training_dataset(self):\n        np.random.seed(20)\n        x = np.random.rand(20, 10)\n        y = np.random.randint(0, 2, (20))\n\n        rdd_x = self.sc.parallelize(x)\n        rdd_y = self.sc.parallelize(y)\n\n        rdd = rdd_x.zip(rdd_y)\n\n        dataset = TFDataset.from_rdd(rdd,\n                                     features=(tf.float32, [10]),\n                                     labels=(tf.int32, []),\n                                     batch_size=4,\n                                     val_rdd=rdd\n                                     )\n        return dataset\n\n    def create_evaluation_dataset(self):\n        np.random.seed(20)\n        x = np.random.rand(20, 10)\n        y = np.random.randint(0, 2, (20))\n\n        rdd_x = self.sc.parallelize(x)\n        rdd_y = self.sc.parallelize(y)\n\n        rdd = rdd_x.zip(rdd_y)\n\n        dataset = TFDataset.from_rdd(rdd,\n                                     features=(tf.float32, [10]),\n                                     labels=(tf.int32, []),\n                                     batch_per_thread=1\n                                     )\n        return dataset\n\n    def create_predict_dataset(self):\n        np.random.seed(20)\n        x = np.random.rand(20, 10)\n\n        rdd = self.sc.parallelize(x)\n\n        rdd = rdd.map(lambda x: [x])\n\n        dataset = TFDataset.from_rdd(rdd,\n                                     features=(tf.float32, [10]),\n                                     batch_per_thread=1\n                                     )\n        return dataset\n\n    def test_training_with_ndarray(self):\n\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        model.fit(x, y, batch_size=2)\n\n    def test_training_with_ndarry_distributed(self):\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        model.fit(x, y, batch_size=4, distributed=True)\n\n    def test_training_with_ndarry_distributed_twice(self):\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        model.fit(x, y, batch_size=4, distributed=True)\n        model.fit(x, y, batch_size=4, distributed=True)\n\n    def test_training_with_validation_data(self):\n\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        val_x, val_y = self.create_training_data()\n\n        model.fit(x, y, validation_data=(val_x, val_y), batch_size=4)\n\n    def test_training_with_validation_data_distributed(self):\n\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        val_x, val_y = self.create_training_data()\n\n        model.fit(x, y, validation_data=(val_x, val_y), batch_size=4, distributed=True)\n\n    def test_training_and_validation_with_dataset(self):\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        dataset = self.create_training_dataset()\n\n        model.fit(dataset)\n\n    def test_evaluate_with_ndarray(self):\n\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        results_pre = model.evaluate(x, y)\n\n        model.fit(x, y, batch_size=4, epochs=10)\n\n        results_after = model.evaluate(x, y)\n\n        assert results_pre[0] > results_after[0]\n\n    def test_evaluate_with_ndarray_distributed(self):\n\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        results_pre = model.evaluate(x, y)\n\n        model.fit(x, y, batch_size=4, epochs=10)\n\n        results_after = model.evaluate(x, y, distributed=True)\n\n        assert results_pre[0] > results_after[0]\n\n    def test_evaluate_and_distributed_evaluate(self):\n\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        results_pre = model.evaluate(x, y)\n\n        results_after = model.evaluate(x, y, distributed=True)\n\n        assert np.square(results_pre[0] - results_after[0]) < 0.000001\n        assert np.square(results_pre[1] - results_after[1]) < 0.000001\n\n    def test_evaluate_with_dataset(self):\n\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        results_pre = model.evaluate(x, y)\n\n        dataset = self.create_evaluation_dataset()\n\n        results_after = model.evaluate(dataset)\n\n        assert np.square(results_pre[0] - results_after[0]) < 0.000001\n        assert np.square(results_pre[1] - results_after[1]) < 0.000001\n\n    def test_predict_with_ndarray(self):\n\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        results_pre = model.evaluate(x, y)\n\n        pred_y = np.argmax(model.predict(x), axis=1)\n\n        acc = np.average((pred_y == y))\n\n        assert np.square(acc - results_pre[1]) < 0.000001\n\n    def test_predict_with_ndarray_distributed(self):\n\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        results_pre = model.evaluate(x, y)\n\n        pred_y = np.argmax(model.predict(x, distributed=True), axis=1)\n\n        acc = np.average((pred_y == y))\n\n        assert np.square(acc - results_pre[1]) < 0.000001\n\n    def test_predict_with_dataset(self):\n\n        keras_model = self.create_model()\n        model = KerasModel(keras_model)\n\n        x, y = self.create_training_data()\n\n        results_pre = model.evaluate(x, y)\n\n        pred_y = np.argmax(np.array(model.predict(\n            self.create_predict_dataset()).collect()), axis=1)\n\n        acc = np.average((pred_y == y))\n\n        assert np.square(acc - results_pre[1]) < 0.000001\n\n    # move the test here to avoid keras session to be closed (not sure about why)\n    def test_tf_optimizer_with_sparse_gradient_using_keras(self):\n        import tensorflow as tf\n\n        ids = np.random.randint(0, 10, size=[40])\n        labels = np.random.randint(0, 5, size=[40])\n        id_rdd = self.sc.parallelize(ids)\n        label_rdd = self.sc.parallelize(labels)\n        training_rdd = id_rdd.zip(label_rdd).map(lambda x: [x[0], x[1]])\n\n        dataset = TFDataset.from_rdd(training_rdd,\n                                     features=(tf.int32, []),\n                                     labels=(tf.int32, []),\n                                     batch_size=8)\n        words_input = tf.keras.layers.Input(shape=(), name=\'words_input\')\n        embedding_layer = tf.keras.layers.Embedding(input_dim=10,\n                                                    output_dim=5, name=\'word_embedding\')\n        word_embeddings = embedding_layer(words_input)\n        embedding = tf.keras.layers.Flatten()(word_embeddings)\n        output = tf.keras.layers.Dense(5, activation=""softmax"")(embedding)\n        model = tf.keras.models.Model(inputs=[words_input], outputs=[output])\n        model.compile(optimizer=""sgd"", loss=""sparse_categorical_crossentropy"")\n\n        optimizer = TFOptimizer.from_keras(model, dataset)\n        optimizer.optimize()\n\n    def test_tensorflow_optimizer(self):\n        data = tf.keras.layers.Input(shape=[10])\n\n        x = tf.keras.layers.Flatten()(data)\n        x = tf.keras.layers.Dense(10, activation=\'relu\')(x)\n        predictions = tf.keras.layers.Dense(2, activation=\'softmax\')(x)\n\n        model = tf.keras.models.Model(inputs=data, outputs=predictions)\n        model.compile(optimizer=tf.train.AdamOptimizer(),\n                      loss=\'sparse_categorical_crossentropy\',\n                      metrics=[\'accuracy\'])\n\n        keras_model = KerasModel(model)\n\n        x, y = self.create_training_data()\n\n        keras_model.fit(x, y, batch_size=4, distributed=True)\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/tfpark/test_tfpark_tfoptimizer.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pytest\n\nfrom bigdl.optim.optimizer import Adam, SGD, MaxEpoch\nfrom zoo.pipeline.api.keras.metrics import Accuracy\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nimport tensorflow as tf\nimport numpy as np\nimport tempfile\nimport os\n\nfrom zoo.tfpark import TFDataset, TFOptimizer\n\n\nclass TestTFParkTFOptimizer(ZooTestCase):\n\n    def setup_method(self, method):\n        tf.keras.backend.clear_session()\n        super(TestTFParkTFOptimizer, self).setup_method(method)\n\n    def test_tf_optimizer_with_sparse_gradient(self):\n        ids = np.random.randint(0, 10, size=[40])\n        labels = np.random.randint(0, 5, size=[40])\n        id_rdd = self.sc.parallelize(ids)\n        label_rdd = self.sc.parallelize(labels)\n        training_rdd = id_rdd.zip(label_rdd).map(lambda x: [x[0], x[1]])\n        with tf.Graph().as_default():\n            dataset = TFDataset.from_rdd(training_rdd,\n                                         names=[""ids"", ""labels""],\n                                         shapes=[[], []],\n                                         types=[tf.int32, tf.int32],\n                                         batch_size=8)\n            id_tensor, label_tensor = dataset.tensors\n            embedding_table = tf.get_variable(\n                name=""word_embedding"",\n                shape=[10, 5])\n\n            embedding = tf.nn.embedding_lookup(embedding_table, id_tensor)\n            loss = tf.reduce_mean(tf.losses.\n                                  sparse_softmax_cross_entropy(logits=embedding,\n                                                               labels=label_tensor))\n            optimizer = TFOptimizer.from_loss(loss, Adam(1e-3))\n            optimizer.optimize(end_trigger=MaxEpoch(1))\n            optimizer.sess.close()\n\n    def test_tf_optimizer_metrics(self):\n\n        features = np.random.randn(20, 10)\n        labels = np.random.randint(0, 10, size=[20])\n        with tf.Graph().as_default():\n            dataset = TFDataset.from_ndarrays((features, labels),\n                                              batch_size=4,\n                                              val_tensors=(features, labels))\n            feature_tensor, label_tensor = dataset.tensors\n            features = tf.layers.dense(feature_tensor, 8)\n            output = tf.layers.dense(features, 10)\n            loss = tf.reduce_mean(tf.losses.\n                                  sparse_softmax_cross_entropy(logits=output,\n                                                               labels=label_tensor))\n            optimizer = TFOptimizer.from_loss(loss, {""dense/"": Adam(1e-3), ""dense_1/"": SGD(0.0)},\n                                              val_outputs=[output],\n                                              val_labels=[label_tensor],\n                                              val_method=Accuracy(), metrics={""loss"": loss})\n            initial_weights = optimizer.tf_model.training_helper_layer.get_weights()\n            optimizer.optimize(end_trigger=MaxEpoch(1))\n            updated_weights = optimizer.tf_model.training_helper_layer.get_weights()\n            for i in [0, 1]:  # weights and bias combined with ""dense/"" should be updated\n                assert not np.allclose(initial_weights[i], updated_weights[i])\n            for i in [2, 3]:  # weights and bias combined with ""dense_1"" should be unchanged\n                assert np.allclose(initial_weights[i], updated_weights[i])\n            optimizer.sess.close()\n\n    def test_control_inputs(self):\n\n        features = np.random.randn(20, 10)\n        labels = np.random.randint(0, 10, size=[20])\n        with tf.Graph().as_default():\n            dataset = TFDataset.from_ndarrays((features, labels),\n                                              batch_size=4,\n                                              val_tensors=(features, labels))\n            is_training = tf.placeholder(dtype=tf.bool, shape=())\n            feature_tensor, label_tensor = dataset.tensors\n            features = tf.layers.dense(feature_tensor, 8)\n            features = tf.layers.dropout(features, training=is_training)\n            output = tf.layers.dense(features, 10)\n            loss = tf.reduce_mean(tf.losses.\n                                  sparse_softmax_cross_entropy(logits=output,\n                                                               labels=label_tensor))\n            optimizer = TFOptimizer.from_loss(loss, Adam(),\n                                              val_outputs=[output],\n                                              val_labels=[label_tensor],\n                                              val_method=Accuracy(),\n                                              tensor_with_value={is_training: (True, False)},\n                                              metrics={""loss"": loss})\n            optimizer.optimize(end_trigger=MaxEpoch(1))\n            optimizer.sess.close()\n\n    def test_checkpoint(self):\n\n        features = np.random.randn(20, 10)\n        labels = np.random.randint(0, 10, size=[20])\n        with tf.Graph().as_default():\n            dataset = TFDataset.from_ndarrays((features, labels),\n                                              batch_size=4,\n                                              val_tensors=(features, labels))\n            feature_tensor, label_tensor = dataset.tensors\n            features = tf.layers.dense(feature_tensor, 8)\n            output = tf.layers.dense(features, 10)\n            loss = tf.reduce_mean(tf.losses.\n                                  sparse_softmax_cross_entropy(logits=output,\n                                                               labels=label_tensor))\n            model_dir = tempfile.mkdtemp()\n            try:\n                optimizer = TFOptimizer.from_loss(loss, Adam(),\n                                                  val_outputs=[output],\n                                                  val_labels=[label_tensor],\n                                                  val_method=Accuracy(),\n                                                  metrics={""loss"": loss}, model_dir=model_dir)\n                optimizer.optimize(end_trigger=MaxEpoch(1))\n\n                import re\n                ckpt_path = None\n                versions = []\n                for (root, dirs, files) in os.walk(model_dir, topdown=True):\n                    temp_versions = []\n                    for file_name in files:\n                        if re.match(""^optimMethod-TFParkTraining\\.[0-9]+$"", file_name) is not None:\n                            version = int(file_name.split(""."")[1])\n                            temp_versions.append(version)\n                    if temp_versions:\n                        ckpt_path = root\n                        versions = temp_versions\n                        break\n\n                assert ckpt_path is not None, ""Cannot fine checkpoint file""\n\n                optimizer.load_checkpoint(ckpt_path, max(versions))\n                optimizer.optimize(end_trigger=MaxEpoch(1))\n                optimizer.sess.close()\n            finally:\n                import shutil\n                shutil.rmtree(model_dir)\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/zouwu/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/zouwu/conftest.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pytest\nsc = None\nray_ctx = None\n\n\n@pytest.fixture(autouse=False, scope=\'class\')\ndef init_ray_context_fixture():\n    from zoo import init_spark_on_local\n    from zoo.ray import RayContext\n    sc = init_spark_on_local(cores=4, spark_log_level=""INFO"")\n    ray_ctx = RayContext(sc=sc, object_store_memory=""1g"")\n    ray_ctx.init()\n    yield\n    ray_ctx.stop()\n    sc.stop()\n\n\n# @pytest.fixture()\n# def setUpModule():\n#     sc = init_spark_on_local(cores=4, spark_log_level=""INFO"")\n#     ray_ctx = RayContext(sc=sc)\n#     ray_ctx.init()\n#\n#\n# def tearDownModule():\n#     ray_ctx.stop()\n#     sc.stop()\n'"
pyzoo/test/zoo/zouwu/test_auto_ts.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nimport numpy as np\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\n\nfrom zoo.automl.config.recipe import LSTMGridRandomRecipe\nfrom zoo.zouwu.autots.forecast import AutoTSTrainer\nfrom zoo.zouwu.autots.forecast import TSPipeline\n\nimport pandas as pd\n\n\n@pytest.mark.usefixtures(""init_ray_context_fixture"")\nclass TestZouwuAutoTS(ZooTestCase):\n\n    def setup_method(self, method):\n        # super(TestZouwuAutoTS, self).setup_method(method)\n        self.create_data()\n\n    def teardown_method(self, method):\n        pass\n\n    def create_data(self):\n        sample_num = np.random.randint(100, 200)\n        self.train_df = pd.DataFrame({""datetime"": pd.date_range(\n            \'1/1/2019\', periods=sample_num), ""value"": np.random.randn(sample_num)})\n        val_sample_num = np.random.randint(20, 30)\n        self.validation_df = pd.DataFrame({""datetime"": pd.date_range(\n            \'1/1/2019\', periods=val_sample_num), ""value"": np.random.randn(val_sample_num)})\n\n    def test_AutoTSTrainer_smoke(self):\n        horizon = np.random.randint(1, 6)\n        tsp = AutoTSTrainer(dt_col=""datetime"",\n                            target_col=""value"",\n                            horizon=horizon,\n                            extra_features_col=None\n                            )\n        pipeline = tsp.fit(self.train_df)\n        assert isinstance(pipeline, TSPipeline)\n        assert pipeline.internal.config is not None\n        pipeline.evaluate(self.validation_df)\n        pipeline.predict(self.validation_df)\n\n    def test_AutoTrainer_LstmRecipe(self):\n        horizon = np.random.randint(1, 6)\n        tsp = AutoTSTrainer(dt_col=""datetime"",\n                            target_col=""value"",\n                            horizon=horizon,\n                            extra_features_col=None\n                            )\n        pipeline = tsp.fit(self.train_df,\n                           self.validation_df,\n                           recipe=LSTMGridRandomRecipe(\n                               num_rand_samples=5,\n                               batch_size=[1024],\n                               lstm_2_units=[8],\n                               training_iteration=1,\n                               epochs=1\n                           ))\n        assert isinstance(pipeline, TSPipeline)\n        assert pipeline.internal.config is not None\n        pipeline.evaluate(self.validation_df)\n        pipeline.predict(self.validation_df)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/zouwu/test_model_anomaly.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nimport numpy as np\nimport pandas as pd\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\n\nfrom zoo.zouwu.model.forecast import LSTMForecaster\nfrom zoo.zouwu.model.anomaly import ThresholdDetector, ThresholdEstimator\n\n\nclass TestZouwuModelAnomaly(ZooTestCase):\n\n    def gen_data(self, feature_num=6, sample_num=100):\n        return pd.DataFrame(data=np.random.randn(sample_num, feature_num))\n\n    def train_test_split(self, df, test_num, look_back):\n        test_split_index = test_num + look_back + 1\n\n        # train_df\n        train_df = df[:-test_num]\n        test_df = df[-test_split_index:]\n        test_df = test_df.reset_index(drop=True)\n        return train_df, test_df\n\n    def roll_data(self, dataset, look_back, target_col_indexes):\n        """"""\n        Generate input samples from rolling\n        """"""\n        X, Y = [], []\n        data = dataset.to_numpy()\n        for i in range(len(dataset) - look_back - 1):\n            X.append(data[i: (i + look_back)])\n            # Y.append(dataset.iloc[i + look_back, target_col_indexes])\n            Y.append(data[i + look_back][target_col_indexes])\n        return np.array(X), np.array(Y)\n\n    def test_app(self):\n        look_back = 4\n\n        # generate dataframe\n        data = self.gen_data(feature_num=6, sample_num=100)\n        # split train and test dataframes\n        train_df, test_df = self.train_test_split(data, test_num=20, look_back=look_back)\n\n        # roll data to generate model input\n        x_train, y_train = self.roll_data(dataset=train_df, look_back=look_back,\n                                          target_col_indexes=[0])\n        x_test, y_test = self.roll_data(dataset=test_df, look_back=look_back,\n                                        target_col_indexes=[0])\n\n        # create model, train on train data and predict on test\n        lstm_config = {""lstm_1_units"": 32, ""lstm_2_units"": 32, ""lr"": 0.001}\n        forecaster = LSTMForecaster(target_dim=1, feature_dim=x_train.shape[-1], **lstm_config)\n        forecaster.fit(x=x_train, y=y_train, batch_size=1024, epochs=50, distributed=False)\n        y_predict = forecaster.predict(x_test)\n\n        # find anomaly by comparing the difference between y_predict and y_test (actual)\n        threshold = 10\n        detector = ThresholdDetector()\n        anomaly_indexes = detector.detect(y=y_test,\n                                          yhat=y_predict,\n                                          threshold=threshold)\n        assert len(anomaly_indexes) == 0\n\n        # if user don\'t have a threshold, he can choose to use estimator\n        # to find a threshold first\n        ratio = 0.1\n        threshold = ThresholdEstimator().fit(y=y_test, yhat=y_predict, ratio=ratio)\n        fitted_anomaly_indexes = detector.detect(y=y_test, yhat=y_predict, threshold=threshold)\n        assert len(fitted_anomaly_indexes) == int(ratio * y_test.shape[0])\n\n    def test_threshold_case1_multivariant(self):\n        sample_num = 10\n        feature_dim = 5\n        num_anomaly = 5\n        # predicted value\n        y_pred = np.full((sample_num, feature_dim), 0)\n        # actual value\n        y_test = np.full(sample_num * feature_dim, 0.2)\n\n        gen_rand_indexes = [0, 7, 16, 33, 45]\n        y_test[gen_rand_indexes] = 10\n        y_test = y_test.reshape((sample_num, feature_dim))\n\n        anomaly_indexes = ThresholdDetector().detect(y=y_test, yhat=y_pred, threshold=3)\n        assert len(anomaly_indexes) == num_anomaly\n\n    def test_threshold_case4(self):\n        sample_num = 10\n        feature_dim = 5\n        num_anomaly = 5\n        # actual value\n        y_test = np.zeros(sample_num * feature_dim)\n\n        gen_rand_indexes = [0, 7, 16, 33, 45]\n        y_test[gen_rand_indexes] = 10\n        y_test = y_test.reshape((sample_num, feature_dim))\n\n        # use threshold (-1, 1) for each dimension\n        threshold_min = np.ones_like(y_test) * (-1)\n        threshold_max = np.ones_like(y_test)\n        anomaly_indexes = ThresholdDetector().detect(y=y_test, yhat=None,\n                                                     threshold=(threshold_min, threshold_max))\n        assert len(anomaly_indexes) == num_anomaly\n\n    def test_threshold_gaussian(self):\n        sample_num = 500\n        # actual value\n        y_test = np.full(sample_num, 2)\n        mu, sigma, ratio = 3, 0.1, 0.01\n        s = np.random.normal(mu, sigma, sample_num)\n        y = y_test + s\n\n        threshold = ThresholdEstimator().fit(y, y_test, mode=""gaussian"", ratio=ratio)\n        from scipy.stats import norm\n        assert abs(threshold-(norm.ppf(1-ratio)*sigma+mu)) < 0.02\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/zouwu/test_model_forecast.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nimport numpy as np\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.automl.feature.time_sequence import TimeSequenceFeatureTransformer\nimport tensorflow as tf\nimport pandas as pd\n\nfrom zoo.zouwu.model.forecast import LSTMForecaster\nfrom zoo.zouwu.model.forecast import MTNetForecaster\n\n\nclass TestZouwuModelForecast(ZooTestCase):\n\n    def setup_method(self, method):\n        tf.keras.backend.clear_session()\n        # super(TestZouwuModelForecast, self).setup_method(method)\n        self.ft = TimeSequenceFeatureTransformer()\n        self.create_data()\n\n    def teardown_method(self, method):\n        pass\n\n    def create_data(self):\n        def gen_train_sample(data, past_seq_len, future_seq_len):\n            data = pd.DataFrame(data)\n            x, y = self.ft._roll_train(data,\n                                       past_seq_len=past_seq_len,\n                                       future_seq_len=future_seq_len\n                                       )\n            return x, y\n\n        def gen_test_sample(data, past_seq_len):\n            test_data = pd.DataFrame(data)\n            x = self.ft._roll_test(test_data, past_seq_len=past_seq_len)\n            return x\n\n        self.long_num = 6\n        self.time_step = 2\n        look_back = (self.long_num + 1) * self.time_step\n        look_forward = 1\n        self.x_train, self.y_train = gen_train_sample(data=np.random.randn(\n            64, 4), past_seq_len=look_back, future_seq_len=look_forward)\n        self.x_val, self.y_val = gen_train_sample(data=np.random.randn(16, 4),\n                                                  past_seq_len=look_back,\n                                                  future_seq_len=look_forward)\n        self.x_test = gen_test_sample(data=np.random.randn(16, 4),\n                                      past_seq_len=look_back)\n\n    def test_forecast_lstm(self):\n        # TODO hacking to fix a bug\n        model = LSTMForecaster(target_dim=1, feature_dim=self.x_train.shape[-1])\n        model.fit(self.x_train,\n                  self.y_train,\n                  validation_data=(self.x_val, self.y_val),\n                  batch_size=8,\n                  distributed=False)\n        model.evaluate(self.x_val, self.y_val)\n        model.predict(self.x_test)\n\n    def test_forecast_mtnet(self):\n        # TODO hacking to fix a bug\n        model = MTNetForecaster(target_dim=1,\n                                feature_dim=self.x_train.shape[-1],\n                                lb_long_steps=self.long_num,\n                                lb_long_stepsize=self.time_step\n                                )\n        x_train_long, x_train_short = model.preprocess_input(self.x_train)\n        x_val_long, x_val_short = model.preprocess_input(self.x_val)\n        x_test_long, x_test_short = model.preprocess_input(self.x_test)\n\n        model.fit([x_train_long, x_train_short],\n                  self.y_train,\n                  validation_data=([x_val_long, x_val_short], self.y_val),\n                  batch_size=32,\n                  distributed=False)\n        model.evaluate([x_val_long, x_val_short], self.y_val)\n        model.predict([x_test_long, x_test_short])\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/zoo/automl/common/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/automl/common/metrics.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import r2_score as R2\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.metrics import mean_squared_log_error as MSLE\nimport numpy as np\n\n\nEPSILON = 1e-10\n\n\ndef check_input(y_true, y_pred, multioutput):\n    if y_true is None or y_pred is None:\n        raise ValueError(""The input is None."")\n    if not isinstance(y_true, (list, tuple, np.ndarray)):\n        raise ValueError(""Expected array-like input. Only list/tuple/ndarray are supported"")\n    if isinstance(y_true, (list, tuple)):\n        y_true = np.array(y_true)\n    if isinstance(y_pred, (list, tuple)):\n        y_pred = np.array(y_pred)\n\n    if y_true.ndim == 1:\n        y_true = y_true.reshape((-1, 1))\n\n    if y_pred.ndim == 1:\n        y_pred = y_pred.reshape((-1, 1))\n\n    if y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError(""y_true and y_pred have different number of samples ""\n                         ""({0}!={1})"".format(y_true.shape[0], y_pred.shape[0]))\n    if y_true.shape[1] != y_pred.shape[1]:\n        raise ValueError(""y_true and y_pred have different number of output ""\n                         ""({0}!={1})"".format(y_true.shape[1], y_pred.shape[1]))\n    allowed_multioutput_str = (\'raw_values\', \'uniform_average\',\n                               \'variance_weighted\')\n    if isinstance(multioutput, str):\n        if multioutput not in allowed_multioutput_str:\n            raise ValueError(""Allowed \'multioutput\' string values are {}. ""\n                             ""You provided multioutput={!r}""\n                             .format(allowed_multioutput_str, multioutput))\n\n    return y_true, y_pred\n\n\ndef sMAPE(y_true, y_pred, multioutput=\'raw_values\'):\n    """"""\n    calculate Symmetric mean absolute percentage error (sMAPE).\n    <math> \\text{SMAPE} = \\frac{100\\%}{n} \\sum_{t=1}^n \\frac{|F_t-A_t|}{|A_t|+|F_t|}</math>\n    :param y_true: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    :param y_pred: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n    :param multioutput: string in [\'raw_values\', \'uniform_average\']\n    :return:float or ndarray of floats\n        A non-negative floating point value (the best value is 0.0), or an\n        array of floating point values, one for each individual target.\n    """"""\n    y_true, y_pred = check_input(y_true, y_pred, multioutput)\n    output_errors = np.mean(100 * np.abs(y_true - y_pred) /\n                            (np.abs(y_true) + np.abs(y_pred) + EPSILON), axis=0,)\n    if multioutput == \'raw_values\':\n        return output_errors\n    return np.mean(output_errors)\n\n\ndef MPE(y_true, y_pred, multioutput=\'raw_values\'):\n    """"""\n    calculate mean percentage error (MPE).\n    <math> \\text{MPE} = \\frac{100\\%}{n}\\sum_{t=1}^n \\frac{a_t-f_t}{a_t} </math>\n    :param y_true: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    :param y_pred: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n    :param multioutput: string in [\'raw_values\', \'uniform_average\']\n    :return:float or ndarray of floats\n        A non-negative floating point value (the best value is 0.0), or an\n        array of floating point values, one for each individual target.\n    """"""\n    y_true, y_pred = check_input(y_true, y_pred, multioutput)\n    output_errors = np.mean(100 * (y_true - y_pred) / (y_true + EPSILON), axis=0,)\n    if multioutput == \'raw_values\':\n        return output_errors\n    return np.mean(output_errors)\n\n\ndef MAPE(y_true, y_pred, multioutput=\'raw_values\'):\n    """"""\n    calculate mean absolute percentage error (MAPE).\n    <math>\\mbox{M} = \\frac{100\\%}{n}\\sum_{t=1}^n  \\left|\\frac{A_t-F_t}{A_t}\\right|, </math>\n    :param y_true: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    :param y_pred: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n    :param multioutput: string in [\'raw_values\', \'uniform_average\']\n    :return:float or ndarray of floats\n        A non-negative floating point value (the best value is 0.0), or an\n        array of floating point values, one for each individual target.\n    """"""\n    y_true, y_pred = check_input(y_true, y_pred, multioutput)\n    output_errors = np.mean(100 * np.abs((y_true - y_pred) / (y_true + EPSILON)), axis=0,)\n    if multioutput == \'raw_values\':\n        return output_errors\n    return np.mean(output_errors)\n\n\ndef MDAPE(y_true, y_pred, multioutput=\'raw_values\'):\n    """"""\n    calculate Median Absolute Percentage Error (MDAPE).\n    :param y_true: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    :param y_pred: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n    :param multioutput: string in [\'raw_values\', \'uniform_average\']\n    :return:float or ndarray of floats\n        A non-negative floating point value (the best value is 0.0), or an\n        array of floating point values, one for each individual target.\n    """"""\n    y_true, y_pred = check_input(y_true, y_pred, multioutput)\n    output_errors = np.median(100 * np.abs((y_true - y_pred) / (y_true + EPSILON)), axis=0,)\n    if multioutput == \'raw_values\':\n        return output_errors\n    return np.mean(output_errors)\n\n\ndef sMDAPE(y_true, y_pred, multioutput=\'raw_values\'):\n    """"""\n    calculate Symmetric Median Absolute Percentage Error (sMDAPE).\n    :param y_true: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    :param y_pred: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n    :param multioutput: string in [\'raw_values\', \'uniform_average\']\n    :return:float or ndarray of floats\n        A non-negative floating point value (the best value is 0.0), or an\n        array of floating point values, one for each individual target.\n    """"""\n    y_true, y_pred = check_input(y_true, y_pred, multioutput)\n    output_errors = np.median(100 * np.abs(y_true - y_pred) /\n                              (np.abs(y_true) + np.abs(y_pred) + EPSILON), axis=0, )\n    if multioutput == \'raw_values\':\n        return output_errors\n    return np.mean(output_errors)\n\n\ndef ME(y_true, y_pred, multioutput=\'raw_values\'):\n    """"""\n    calculate Mean Error (ME).\n    :param y_true: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    :param y_pred: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n    :param multioutput: string in [\'raw_values\', \'uniform_average\']\n    :return:float or ndarray of floats\n        A non-negative floating point value (the best value is 0.0), or an\n        array of floating point values, one for each individual target.\n    """"""\n    y_true, y_pred = check_input(y_true, y_pred, multioutput)\n    output_errors = np.mean(y_true - y_pred, axis=0,)\n    if multioutput == \'raw_values\':\n        return output_errors\n    return np.mean(output_errors)\n\n\ndef MSPE(y_true, y_pred, multioutput=\'raw_values\'):\n    """"""\n    calculate mean squared percentage error (MSPE).\n    <math>\\operatorname{MSPE}(L)=\\operatorname{E}\n    \\left[\\left( g(x_i)-\\widehat{g}(x_i)\\right)^2\\right].</math>\n    :param y_true: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    :param y_pred: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n    :param multioutput: string in [\'raw_values\', \'uniform_average\']\n    :return:float or ndarray of floats\n        A non-negative floating point value (the best value is 0.0), or an\n        array of floating point values, one for each individual target.\n    """"""\n    y_true, y_pred = check_input(y_true, y_pred, multioutput)\n    output_errors = np.mean(np.square(y_true - y_pred), axis=0,)\n    if multioutput == \'raw_values\':\n        return output_errors\n    return np.mean(output_errors)\n\n\ndef RMSE(y_true, y_pred, multioutput=\'raw_values\'):\n    """"""\n    calculate square root of the mean squared error (RMSE).\n    :param y_true: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    :param y_pred: array-like of shape = (n_samples) or (n_samples, n_outputs)\n        Estimated target values.\n    :param multioutput: string in [\'raw_values\', \'uniform_average\']\n    :return:float or ndarray of floats\n        A non-negative floating point value (the best value is 0.0), or an\n        array of floating point values, one for each individual target.\n    """"""\n    return np.sqrt(MSE(y_true, y_pred, multioutput=multioutput))\n\n\nclass Evaluator(object):\n    """"""\n    Evaluate metrics for y_true and y_pred\n    """"""\n\n    metrics_func = {\n        # Absolute\n        \'me\': ME,\n        \'mae\': MAE,\n        \'mse\': MSE,\n        \'rmse\': RMSE,\n        \'msle\': MSLE,\n        \'r2\': R2,\n        # Relative\n        \'mpe\': MPE,\n        \'mape\': MAPE,\n        \'mspe\': MSPE,\n        \'smape\': sMAPE,\n        \'mdape\': MDAPE,\n        \'smdape\': sMDAPE,\n    }\n\n    @staticmethod\n    def evaluate(metric, y_true, y_pred, multioutput=\'raw_values\'):\n        if not Evaluator.check_metric(metric):\n            raise ValueError(""metric "" + metric + "" is not supported"")\n        return Evaluator.metrics_func[metric](y_true, y_pred, multioutput=multioutput)\n\n    @staticmethod\n    def check_metric(metric):\n        return True if metric in Evaluator.metrics_func.keys() else False\n'"
pyzoo/zoo/automl/common/parameters.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# where pipeline saves file by default\nDEFAULT_PPL_DIR = ""./saved_pipeline""\nDEFAULT_CONFIG_DIR = ""./saved_configs""\n\n# reward metric in tune reporter\nREWARD_METRIC = ""reward_metric""\n'"
pyzoo/zoo/automl/common/util.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport shutil\nimport tempfile\nimport zipfile\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\n\nIDENTIFIER_LEN = 27\n\n\ndef train_val_test_split(df,\n                         val_ratio=0,\n                         test_ratio=0.1,\n                         look_back=0,\n                         horizon=1):\n    """"""\n    split input dataframe into train_df, val_df and test_df according to split ratio.\n    The dataframe is splitted in its originally order in timeline.\n    e.g. |......... train_df(80%) ........ | ... val_df(10%) ...| ...test_df(10%)...|\n    :param df: dataframe to be splitted\n    :param val_ratio: validation ratio\n    :param test_ratio: test ratio\n    :param look_back: the length to look back\n    :param horizon: num of steps to look forward\n    :return:\n    """"""\n    # suitable to nyc taxi dataset.\n\n    total_num = df.index.size\n    test_num = int(total_num * test_ratio)\n    val_num = int(total_num * val_ratio)\n\n    test_split_index = test_num + look_back + horizon - 1\n    val_split_index = test_split_index + val_num\n\n    train_df = df.iloc[:-(test_num + val_num)]\n    val_df = df.iloc[-val_split_index: -test_num]\n    test_df = df.iloc[-test_split_index:]\n\n    if not pd.api.types.is_datetime64_any_dtype(df.index.dtype):\n        val_df = val_df.reset_index(drop=True)\n        test_df = test_df.reset_index(drop=True)\n\n    return train_df, val_df, test_df\n\n\nclass NumpyEncoder(json.JSONEncoder):\n    """"""\n    convert numpy array to list for JSON serialize\n    """"""\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)\n\n\ndef save_config(file_path, config, replace=False):\n    """"""\n    :param file_path: the file path of config to be saved.\n    :param config: dict. The config to be saved\n    :param replace: whether to replace if the config file already existed.\n    :return:\n    """"""\n    if os.path.isfile(file_path) and not replace:\n        with open(file_path, ""r"") as input_file:\n            old_config = json.load(input_file)\n        old_config.update(config)\n        config = old_config.copy()\n\n    file_dirname = os.path.dirname(os.path.abspath(file_path))\n    if file_dirname and not os.path.exists(file_dirname):\n        os.makedirs(file_dirname)\n\n    with open(file_path, ""w"") as output_file:\n        json.dump(config, output_file, cls=NumpyEncoder)\n\n\ndef load_config(file_path):\n    with open(file_path, ""r"") as input_file:\n        data = json.load(input_file)\n    return data\n\n\ndef save(file_path, feature_transformers=None, model=None, config=None):\n    if not os.path.isdir(file_path):\n        os.mkdir(file_path)\n    config_path = os.path.join(file_path, ""config.json"")\n    model_path = os.path.join(file_path, ""weights_tune.h5"")\n    if feature_transformers is not None:\n        feature_transformers.save(config_path, replace=True)\n    if model is not None:\n        model.save(model_path, config_path)\n    if config is not None:\n        save_config(config_path, config)\n\n\ndef save_zip(file, feature_transformers=None, model=None, config=None):\n    file_dirname = os.path.dirname(os.path.abspath(file))\n    if file_dirname and not os.path.exists(file_dirname):\n        os.makedirs(file_dirname)\n\n    dirname = tempfile.mkdtemp(prefix=""automl_save_"")\n    try:\n        save(dirname,\n             feature_transformers=feature_transformers,\n             model=model,\n             config=config)\n        with zipfile.ZipFile(file, \'w\') as f:\n            for dirpath, dirnames, filenames in os.walk(dirname):\n                for filename in filenames:\n                    f.write(os.path.join(dirpath, filename), filename)\n        assert os.path.isfile(file)\n    finally:\n        shutil.rmtree(dirname)\n\n\ndef process(cmd):\n    import subprocess\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    # process.wait()\n    outs, errors = proc.communicate()\n    # if outs:\n    #     print(""hdfs std out:"", outs)\n    if errors:\n        print(""hdfs errors:"", errors)\n    return outs, errors\n\n\ndef get_remote_list(dir_in):\n    # dir_in = ""hdfs://172.16.0.103:9000/yushan/""\n    args = ""hdfs dfs -ls "" + dir_in + "" | awk \'{print $8}\'""\n    s_output, _ = process(args)\n\n    all_dart_dirs = s_output.split()\n    names = []\n    for filename in all_dart_dirs:\n        filename = filename.decode()\n        name_list = filename.split(\'/\')\n        names.append(name_list[-1])\n    # print(names)\n    return names\n\n\ndef upload_ppl_hdfs(upload_dir, ckpt_name):\n    # The default upload_dir is {remote_root}/ray_results/automl\n    # The name of ray checkpoint_dir is train_func_0_{config}_{time}_{tmp},\n    # with a max identifier length of 130.\n    # If there is a list([]) in config and is truncated into part of [],\n    # then the path name can\'t be identified by hadoop command.\n    # Therefore we use the last IDENTIFIER_LEN=27 of checkpoint_dir as upload_dir_name,\n    # with a format of {time}_{tmp}, in order to avoid misinterpretation.\n    log_dir = os.path.abspath(""."")\n    log_name = os.path.basename(log_dir)[-IDENTIFIER_LEN:]\n    remote_log_dir = os.path.join(upload_dir, log_name)\n    if log_name not in get_remote_list(upload_dir):\n        cmd = ""hadoop fs -mkdir {remote_log_dir};"" \\\n              "" hadoop fs -put -f {local_file} {remote_log_dir}""\\\n            .format(local_file=ckpt_name, remote_log_dir=remote_log_dir)\n    else:\n        cmd = "" hadoop fs -put -f {local_file} {remote_log_dir}"".format(\n            local_file=ckpt_name,\n            remote_log_dir=remote_log_dir)\n    # print(""upload hdfs cmd is:"", sync_cmd)\n    process(cmd)\n\n\ndef restore(file, feature_transformers=None, model=None, config=None):\n    model_path = os.path.join(file, ""weights_tune.h5"")\n    config_path = os.path.join(file, ""config.json"")\n    local_config = load_config(config_path)\n    if config is not None:\n        all_config = config.copy()\n        all_config.update(local_config)\n    else:\n        all_config = local_config\n    if model:\n        model.restore(model_path, **all_config)\n    if feature_transformers:\n        feature_transformers.restore(**all_config)\n    return all_config\n\n\ndef restore_zip(file, feature_transformers=None, model=None, config=None):\n    dirname = tempfile.mkdtemp(prefix=""automl_save_"")\n    try:\n        with zipfile.ZipFile(file) as zf:\n            zf.extractall(dirname)\n\n        all_config = restore(dirname, feature_transformers, model, config)\n    finally:\n        shutil.rmtree(dirname)\n    return all_config\n\n\ndef restore_hdfs(model_path, remote_dir, feature_transformers=None, model=None, config=None):\n    model_name = os.path.basename(model_path)\n    local_best_dirname = os.path.basename(os.path.dirname(model_path))\n    remote_model = os.path.join(remote_dir, local_best_dirname[-IDENTIFIER_LEN:], model_name)\n    tmp_dir = tempfile.mkdtemp(prefix=""automl_save_"")\n    try:\n        cmd = ""hadoop fs -get {} {}"".format(remote_model, tmp_dir)\n        # print(""get hdfs cmd is:"", cmd)\n        process(cmd)\n        with zipfile.ZipFile(os.path.join(tmp_dir, model_name)) as zf:\n            zf.extractall(tmp_dir)\n            # print(os.listdir(tmp_dir))\n\n        all_config = restore(tmp_dir, feature_transformers, model, config)\n    finally:\n        shutil.rmtree(tmp_dir)\n    return all_config\n\n\ndef convert_bayes_configs(config):\n    selected_features = []\n    new_config = {}\n    for config_name, config_value in config.items():\n        if config_name.startswith(\'bayes_feature\'):\n            # print(config_name, config_value)\n            if config_value >= 0.5:\n                feature_name = config_name.replace(\'bayes_feature_\', \'\')\n                selected_features.append(feature_name)\n        elif config_name == \'batch_size_log\':\n            batch_size = int(2 ** config_value)\n            new_config[\'batch_size\'] = batch_size\n        elif config_name.endswith(\'float\'):\n            int_config_name = config_name.replace(\'_float\', \'\')\n            int_config_value = int(config_value)\n            new_config[int_config_name] = int_config_value\n        else:\n            new_config[config_name] = config_value\n    if selected_features:\n        new_config[\'selected_features\'] = json.dumps(selected_features)\n    # print(""config after bayes conversion is "", new_config)\n    return new_config\n'"
pyzoo/zoo/automl/config/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/automl/config/recipe.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom abc import ABCMeta, abstractmethod\nfrom zoo.automl.search.abstract import *\nimport numpy as np\nfrom ray import tune\nimport json\n\n\nclass Recipe(metaclass=ABCMeta):\n    """"""\n    Recipe\n    """"""\n\n    def __init__(self):\n        # ----- runtime parameters\n        self.training_iteration = 1\n        self.num_samples = 1\n        self.reward_metric = None\n\n    @abstractmethod\n    def search_space(self, all_available_features):\n        pass\n\n    def runtime_params(self):\n        runtime_config = {\n            ""training_iteration"": self.training_iteration,\n            ""num_samples"": self.num_samples,\n        }\n        if self.reward_metric is not None:\n            runtime_config[""reward_metric""] = self.reward_metric\n        return runtime_config\n\n    def fixed_params(self):\n        return None\n\n    def search_algorithm_params(self):\n        return None\n\n    def search_algorithm(self):\n        return None\n\n    def scheduler_params(self):\n        pass\n\n\nclass SmokeRecipe(Recipe):\n    """"""\n    A very simple Recipe for smoke test that runs one epoch and one iteration\n    with only 1 random sample.\n    """"""\n\n    def __init__(self):\n        super(self.__class__, self).__init__()\n\n    def search_space(self, all_available_features):\n        return {\n            ""selected_features"": json.dumps(all_available_features),\n            ""model"": ""LSTM"",\n            ""lstm_1_units"": tune.choice([32, 64]),\n            ""dropout_1"": tune.uniform(0.2, 0.5),\n            ""lstm_2_units"": tune.choice([32, 64]),\n            ""dropout_2"": tune.uniform(0.2, 0.5),\n            ""lr"": 0.001,\n            ""batch_size"": 1024,\n            ""epochs"": 1,\n            ""past_seq_len"": 2,\n        }\n\n\nclass MTNetSmokeRecipe(Recipe):\n    """"""\n    A very simple Recipe for smoke test that runs one epoch and one iteration\n    with only 1 random sample.\n    """"""\n\n    def __init__(self):\n        super(self.__class__, self).__init__()\n\n    def search_space(self, all_available_features):\n        return {\n            ""selected_features"": json.dumps(all_available_features),\n            ""model"": ""MTNet"",\n            ""lr"": 0.001,\n            ""batch_size"": 16,\n            ""epochs"": 1,\n            ""dropout"": 0.2,\n            ""time_step"": tune.choice([3, 4]),\n            ""filter_size"": 2,\n            ""long_num"": tune.choice([3, 4]),\n            ""ar_size"": tune.choice([2, 3]),\n            ""past_seq_len"": tune.sample_from(lambda spec:\n                                             (spec.config.long_num + 1) * spec.config.time_step),\n        }\n\n\nclass PastSeqParamHandler(object):\n    """"""\n    Utility to handle PastSeq Param\n    """"""\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def get_past_seq_config(look_back):\n        """"""\n        generate pass sequence config based on look_back\n        :param look_back: look_back configuration\n        :return: search configuration for past sequence\n        """"""\n        if isinstance(\n            look_back,\n            tuple) and len(look_back) == 2 and isinstance(\n                look_back[0],\n                int) and isinstance(\n                look_back[1],\n                int):\n            if look_back[1] < 2:\n                raise ValueError(\n                    ""The max look back value should be at least 2"")\n            if look_back[0] < 2:\n                print(\n                    ""The input min look back value is smaller than 2. ""\n                    ""We sample from range (2, {}) instead."".format(\n                        look_back[1]))\n            past_seq_config = tune.randint(look_back[0], look_back[1] + 1)\n        elif isinstance(look_back, int):\n            if look_back < 2:\n                raise ValueError(\n                    ""look back value should not be smaller than 2. ""\n                    ""Current value is "", look_back)\n            past_seq_config = look_back\n        else:\n            raise ValueError(\n                ""look back is {}.\\n ""\n                ""look_back should be either a tuple with 2 int values:""\n                "" (min_len, max_len) or a single int"".format(look_back))\n        return past_seq_config\n\n\nclass GridRandomRecipe(Recipe):\n    """"""\n    A recipe involves both grid search and random search.\n       tsp = TimeSequencePredictor(...,recipe = GridRandomRecipe(1))\n    """"""\n\n    def __init__(\n            self,\n            num_rand_samples=1,\n            look_back=2,\n            epochs=5,\n            training_iteration=10):\n        """"""\n        Constructor.\n        :param num_rand_samples: number of hyper-param configurations sampled randomly\n        :param look_back: the length to look back, either a tuple with 2 int values,\n          which is in format is (min len, max len), or a single int, which is\n          a fixed length to look back.\n        :param training_iteration: no. of iterations for training (n epochs) in trials\n        :param epochs: no. of epochs to train in each iteration\n        """"""\n        super(self.__class__, self).__init__()\n        self.num_samples = num_rand_samples\n        self.training_iteration = training_iteration\n        self.past_seq_config = PastSeqParamHandler.get_past_seq_config(\n            look_back)\n        self.epochs = epochs\n\n    def search_space(self, all_available_features):\n        return {\n            # -------- feature related parameters\n            ""selected_features"": tune.sample_from(lambda spec:\n                                                  json.dumps(\n                                                      list(np.random.choice(\n                                                          all_available_features,\n                                                          size=np.random.randint(\n                                                              low=3,\n                                                              high=len(all_available_features)),\n                                                          replace=False)))),\n\n            # -------- model selection TODO add MTNet\n            ""model"": tune.choice([""LSTM"", ""Seq2seq""]),\n\n            # --------- Vanilla LSTM model parameters\n            ""lstm_1_units"": tune.grid_search([16, 32]),\n            ""dropout_1"": 0.2,\n            ""lstm_2_units"": tune.grid_search([16, 32]),\n            ""dropout_2"": tune.uniform(0.2, 0.5),\n\n            # ----------- Seq2Seq model parameters\n            ""latent_dim"": tune.grid_search([32, 64]),\n            ""dropout"": tune.uniform(0.2, 0.5),\n\n            # ----------- optimization parameters\n            ""lr"": tune.uniform(0.001, 0.01),\n            ""batch_size"": tune.choice([32, 64], replace=False),\n            ""epochs"": self.epochs,\n            ""past_seq_len"": self.past_seq_config,\n        }\n\n\nclass LSTMGridRandomRecipe(Recipe):\n    """"""\n    A recipe involves both grid search and random search, only for LSTM.\n       tsp = TimeSequencePredictor(...,recipe = LSTMGridRandomRecipe(1))\n    """"""\n\n    def __init__(\n            self,\n            num_rand_samples=1,\n            epochs=5,\n            training_iteration=10,\n            look_back=2,\n            lstm_1_units=[16, 32, 64, 128],\n            lstm_2_units=[16, 32, 64],\n            batch_size=[32, 64]):\n        """"""\n        Constructor.\n        :param lstm_1_units: random search candidates for num of lstm_1_units\n        :param lstm_2_units: grid search candidates for num of lstm_1_units\n        :param batch_size: grid search candidates for batch size\n        :param num_rand_samples: number of hyper-param configurations sampled randomly\n        :param look_back: the length to look back, either a tuple with 2 int values,\n          which is in format is (min len, max len), or a single int, which is\n          a fixed length to look back.\n        :param training_iteration: no. of iterations for training (n epochs) in trials\n        :param epochs: no. of epochs to train in each iteration\n        """"""\n        super(self.__class__, self).__init__()\n        # -- runtime params\n        self.num_samples = num_rand_samples\n        self.training_iteration = training_iteration\n\n        # -- model params\n        self.past_seq_config = PastSeqParamHandler.get_past_seq_config(\n            look_back)\n        self.lstm_1_units_config = tune.choice(lstm_1_units)\n        self.lstm_2_units_config = tune.grid_search(lstm_2_units)\n        self.dropout_2_config = tune.uniform(0.2, 0.5)\n\n        # -- optimization params\n        self.lr = tune.uniform(0.001, 0.01)\n        self.batch_size = tune.grid_search(batch_size)\n        self.epochs = epochs\n\n    def search_space(self, all_available_features):\n        return {\n            # -------- feature related parameters\n            ""selected_features"": tune.sample_from(lambda spec:\n                                                  json.dumps(\n                                                      list(np.random.choice(\n                                                          all_available_features,\n                                                          size=np.random.randint(\n                                                              low=3,\n                                                              high=len(all_available_features)),\n                                                          replace=False)))),\n\n            ""model"": ""LSTM"",\n\n            # --------- Vanilla LSTM model parameters\n            ""lstm_1_units"": self.lstm_1_units_config,\n            ""dropout_1"": 0.2,\n            ""lstm_2_units"": self.lstm_2_units_config,\n            ""dropout_2"": self.dropout_2_config,\n\n            # ----------- optimization parameters\n            ""lr"": self.lr,\n            ""batch_size"": self.batch_size,\n            ""epochs"": self.epochs,\n            ""past_seq_len"": self.past_seq_config,\n        }\n\n\nclass MTNetGridRandomRecipe(Recipe):\n    """"""\n    Grid+Random Recipe for MTNet\n    """"""\n\n    def __init__(self,\n                 num_rand_samples=1,\n                 epochs=5,\n                 training_iteration=10,\n                 time_step=[3, 4],\n                 filter_size=[2, 4],\n                 long_num=[3, 4],\n                 ar_size=[2, 3],\n                 batch_size=[32, 64]):\n        """"""\n        Constructor.\n        :param num_rand_samples: number of hyper-param configurations sampled randomly\n        :param training_iteration: no. of iterations for training (n epochs) in trials\n        :param epochs: no. of epochs to train in each iteration\n        :param time_step: random search candidates for model param ""time_step""\n        :param filter_size: random search candidates for model param ""filter_size""\n        :param long_num: random search candidates for model param ""long_num""\n        :param ar_size: random search candidates for model param ""ar_size""\n        :param batch_size: grid search candidates for batch size\n        """"""\n        super(self.__class__, self).__init__()\n        # -- run time params\n        self.num_samples = num_rand_samples\n        self.training_iteration = training_iteration\n\n        # -- optimization params\n        self.lr = tune.uniform(0.001, 0.01)\n        self.batch_size = self.batch_size = tune.grid_search(batch_size)\n        self.epochs = epochs\n\n        # ---- model params\n        self.dropout = tune.uniform(0.2, 0.5)\n        self.time_step = tune.choice(time_step)\n        self.filter_size = tune.choice(filter_size)\n        self.long_num = tune.choice(long_num,)\n        self.ar_size = tune.choice(ar_size)\n        self.past_seq_len = tune.sample_from(\n            lambda spec: (\n                spec.config.long_num + 1) * spec.config.time_step)\n\n    def search_space(self, all_available_features):\n        return {\n            ""selected_features"": tune.sample_from(lambda spec:\n                                                  json.dumps(\n                                                      list(np.random.choice(\n                                                          all_available_features,\n                                                          size=np.random.randint(\n                                                              low=3,\n                                                              high=len(all_available_features)),\n                                                          replace=False)))),\n\n            ""model"": ""MTNet"",\n            ""lr"": self.lr,\n            ""batch_size"": self.batch_size,\n            ""epochs"": self.epochs,\n            ""dropout"": self.dropout,\n            ""time_step"": self.time_step,\n            ""filter_size"": self.filter_size,\n            ""long_num"": self.long_num,\n            ""ar_size"": self.ar_size,\n            ""past_seq_len"": self.past_seq_len,\n        }\n\n\nclass RandomRecipe(Recipe):\n    """"""\n    Pure random sample Recipe. Often used as baseline.\n       tsp = TimeSequencePredictor(...,recipe = RandomRecipe(5))\n    """"""\n\n    def __init__(\n            self,\n            num_rand_samples=1,\n            look_back=2,\n            epochs=5,\n            reward_metric=-0.05,\n            training_iteration=10):\n        """"""\n        :param num_rand_samples: number of hyper-param configurations sampled randomly\n        :param look_back:the length to look back, either a tuple with 2 int values,\n          which is in format is (min len, max len), or a single int, which is\n          a fixed length to look back.\n        :param reward_metric: the rewarding metric value, when reached, stop trial\n        :param training_iteration: no. of iterations for training (n epochs) in trials\n        :param epochs: no. of epochs to train in each iteration\n        """"""\n        super(self.__class__, self).__init__()\n        self.num_samples = num_rand_samples\n        self.reward_metric = reward_metric\n        self.training_iteration = training_iteration\n        self.epochs = epochs\n        self.past_seq_config = PastSeqParamHandler.get_past_seq_config(\n            look_back)\n\n    def search_space(self, all_available_features):\n        import random\n        return {\n            # -------- feature related parameters\n            ""selected_features"": tune.sample_from(lambda spec:\n                                                  json.dumps(\n                                                      list(np.random.choice(\n                                                          all_available_features,\n                                                          size=np.random.randint(\n                                                              low=3,\n                                                              high=len(all_available_features)),\n                                                          replace=False)))),\n\n            ""model"": tune.choice([""LSTM"", ""Seq2seq""]),\n            # --------- Vanilla LSTM model parameters\n            ""lstm_1_units"": tune.choice([8, 16, 32, 64, 128]),\n            ""dropout_1"": tune.uniform(0.2, 0.5),\n            ""lstm_2_units"": tune.choice([8, 16, 32, 64, 128]),\n            ""dropout_2"": tune.uniform(0.2, 0.5),\n\n            # ----------- Seq2Seq model parameters\n            ""latent_dim"": tune.choice([32, 64, 128, 256]),\n            ""dropout"": tune.uniform(0.2, 0.5),\n\n            # ----------- optimization parameters\n            ""lr"": tune.uniform(0.001, 0.01),\n            ""batch_size"": tune.choice([32, 64, 1024], replace=False),\n            ""epochs"": self.epochs,\n            ""past_seq_len"": self.past_seq_config,\n        }\n\n\nclass BayesRecipe(Recipe):\n    """"""\n    A Bayes search Recipe. (Experimental)\n       tsp = TimeSequencePredictor(...,recipe = BayesRecipe(5))\n    """"""\n\n    def __init__(\n            self,\n            num_samples=1,\n            look_back=2,\n            epochs=5,\n            reward_metric=-0.05,\n            training_iteration=5):\n        """"""\n        Constructor\n        :param num_samples: number of hyper-param configurations sampled\n        :param look_back: the length to look back, either a tuple with 2 int values,\n          which is in format is (min len, max len), or a single int, which is\n          a fixed length to look back.\n        :param reward_metric: the rewarding metric value, when reached, stop trial\n        :param training_iteration: no. of iterations for training (n epochs) in trials\n        :param epochs: no. of epochs to train in each iteration\n        """"""\n        super(self.__class__, self).__init__()\n        self.num_samples = num_samples\n        self.reward_metric = reward_metric\n        self.training_iteration = training_iteration\n        self.epochs = epochs\n        if isinstance(\n            look_back,\n            tuple) and len(look_back) == 2 and isinstance(\n                look_back[0],\n                int) and isinstance(\n                look_back[1],\n                int):\n            if look_back[1] < 2:\n                raise ValueError(\n                    ""The max look back value should be at least 2"")\n            if look_back[0] < 2:\n                print(\n                    ""The input min look back value is smaller than 2. ""\n                    ""We sample from range (2, {}) instead."".format(\n                        look_back[1]))\n            self.bayes_past_seq_config = {""past_seq_len_float"": look_back}\n            self.fixed_past_seq_config = {}\n        elif isinstance(look_back, int):\n            if look_back < 2:\n                raise ValueError(\n                    ""look back value should not be smaller than 2. ""\n                    ""Current value is "", look_back)\n            self.bayes_past_seq_config = {}\n            self.fixed_past_seq_config = {""past_seq_len"": look_back}\n        else:\n            raise ValueError(\n                ""look back is {}.\\n ""\n                ""look_back should be either a tuple with 2 int values:""\n                "" (min_len, max_len) or a single int"".format(look_back))\n\n    def search_space(self, all_available_features):\n        feature_space = {""bayes_feature_{}"".format(feature): (0.3, 1)\n                         for feature in all_available_features}\n        other_space = {\n            # --------- model parameters\n            ""lstm_1_units_float"": (8, 128),\n            ""dropout_1"": (0.2, 0.5),\n            ""lstm_2_units_float"": (8, 128),\n            ""dropout_2"": (0.2, 0.5),\n\n            # ----------- optimization parameters\n            ""lr"": (0.001, 0.01),\n            ""batch_size_log"": (5, 10),\n        }\n        total_space = other_space.copy()\n        total_space.update(feature_space)\n        total_space.update(self.bayes_past_seq_config)\n        return total_space\n\n    def fixed_params(self):\n        total_fixed_params = {\n            ""epochs"": self.epochs,\n            # ""batch_size"": 1024,\n        }\n        total_fixed_params.update(self.fixed_past_seq_config)\n        return total_fixed_params\n\n    def search_algorithm_params(self):\n        return {\n            ""utility_kwargs"": {\n                ""kind"": ""ucb"",\n                ""kappa"": 2.5,\n                ""xi"": 0.0\n            }\n        }\n\n    def search_algorithm(self):\n        return \'BayesOpt\'\n'"
pyzoo/zoo/automl/feature/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/automl/feature/abstract.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom abc import ABC, abstractmethod\n\n\nclass BaseFeatureTransformer(ABC):\n    """"""\n    Abstract Base class for Feature transformers.\n    """"""\n    check_optional_config = False\n\n    @abstractmethod\n    def fit_transform(self, input_df, **config):\n        """"""\n        fit data with the input dataframe\n        Will refit the scalars to this data if any.\n        :param input_df: input to be fitted\n        :param config: the config\n        :return:\n        """"""\n        pass\n\n    @abstractmethod\n    def transform(self, input_df):\n        """"""\n        transform the data with fitted\n        :param input_df: input dataframe\n        :return:\n        """"""\n        pass\n\n    @abstractmethod\n    def save(self, file_path):\n        """"""\n        save the feature tools internal variables.\n        Some of the variables are derived after fit_transform, so only saving config is not enough.\n        :param: file_path : the file to be saved\n        :param: config: the trial config\n        :return:\n        """"""\n        pass\n\n    @abstractmethod\n    def restore(self, **config):\n        """"""\n        Restore variables from file\n        :param file_path: file contain saved parameters.\n                          i.e. some parameters are obtained during training,\n                          not in trial config, e.g. scaler fit params)\n        :param config: the trial config\n        :return:\n        """"""\n        pass\n\n    @abstractmethod\n    def _get_required_parameters(self):\n        """"""\n        :return: required parameters to be set into config\n        """"""\n        return set()\n\n    @abstractmethod\n    def _get_optional_parameters(self):\n        """"""\n        :return: optional parameters to be set into config\n        """"""\n        return set()\n\n    def _check_config(self, **config):\n        """"""\n        Do necessary checking for config\n        :param config:\n        :return:\n        """"""\n        config_parameters = set(config.keys())\n        if not config_parameters.issuperset(self._get_required_parameters()):\n            raise ValueError(""Missing required parameters in configuration. "" +\n                             ""Required parameters are: "" + str(self._get_required_parameters()))\n        if self.check_optional_config and \\\n                not config_parameters.issuperset(self._get_optional_parameters()):\n            raise ValueError(""Missing optional parameters in configuration. "" +\n                             ""Optional parameters are: "" + str(self._get_optional_parameters()))\n        return True\n'"
pyzoo/zoo/automl/feature/time_sequence.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom featuretools import TransformFeature\n\nfrom zoo.automl.common.util import save_config\nfrom zoo.automl.feature.abstract import BaseFeatureTransformer\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport pandas as pd\nimport numpy as np\nimport featuretools as ft\nfrom featuretools.primitives import make_agg_primitive, make_trans_primitive\nfrom featuretools.variable_types import Text, Numeric, DatetimeTimeIndex\nimport json\n\n\nclass TimeSequenceFeatureTransformer(BaseFeatureTransformer):\n    """"""\n    TimeSequence feature engineering\n    """"""\n\n    def __init__(self, future_seq_len=1,\n                 dt_col=""datetime"",\n                 target_col=""value"",\n                 extra_features_col=None,\n                 drop_missing=True):\n        """"""\n        Constructor.\n        :param future_seq_len: the future sequence length to be predicted\n        :dt_col: name of datetime column in the input data frame\n        :target_col: name of target column in the input data frame\n        :extra_features_col: name of extra feature columns that needs to predict the target column.\n        :param drop_missing: whether to drop missing values in the curve, if this is set to False,\n                             an error will be reported if missing values are found. If True, will\n                             drop the missing values and won\'t raise errors.\n        """"""\n        # self.scaler = MinMaxScaler()\n        self.scaler = StandardScaler()\n        self.config = None\n        self.dt_col = dt_col\n        self.target_col = target_col\n        self.extra_features_col = extra_features_col\n        self.feature_data = None\n        self.drop_missing = drop_missing\n        self.generate_feature_list = None\n        self.past_seq_len = None\n        self.future_seq_len = future_seq_len\n\n    def _fit_transform(self, input_df):\n        """"""\n        Fit data and transform the raw data to features. This is used in training for hyper\n        parameter searching.\n        This method will refresh the parameters (e.g. min and max of the MinMaxScaler) if any\n        :param input_df: The input time series data frame, Example:\n         datetime   value   ""extra feature 1""   ""extra feature 2""\n         2019-01-01 1.9 1   2\n         2019-01-02 2.3 0   2\n        :return: tuple (x,y)\n            x: 3-d array in format (no. of samples, past sequence length, 2+feature length),\n            in the last dimension, the 1st col is the time index\n            (data type needs to be numpy datetime type, e.g. ""datetime64""),\n            the 2nd col is the target value (data type should be numeric)\n            y: y is 2-d numpy array in format (no. of samples, future sequence length)\n            if future sequence length > 1, or 1-d numpy array in format (no. of samples, )\n            if future sequence length = 1\n        """"""\n        self._check_input(input_df, mode=""train"")\n        # print(input_df.shape)\n        feature_data = self._get_features(input_df, self.config)\n        self.scaler.fit(feature_data)\n        data_n = self._scale(feature_data)\n        assert np.mean(data_n[0]) < 1e-5\n        (x, y) = self._roll_train(data_n,\n                                  past_seq_len=self.past_seq_len,\n                                  future_seq_len=self.future_seq_len)\n\n        return x, y\n\n    def fit_transform(self, input_df, **config):\n        """"""\n        Fit data and transform the raw data to features. This is used in training for hyper\n        parameter searching.\n        This method will refresh the parameters (e.g. min and max of the MinMaxScaler) if any\n        :param input_df: The input time series data frame, it can be a list of data frame or just\n         one dataframe\n         Example:\n         datetime   value   ""extra feature 1""   ""extra feature 2""\n         2019-01-01 1.9 1   2\n         2019-01-02 2.3 0   2\n        :return: tuple (x,y)\n            x: 3-d array in format (no. of samples, past sequence length, 2+feature length),\n            in the last dimension, the 1st col is the time index\n            (data type needs to be numpy datetime type, e.g. ""datetime64""),\n            the 2nd col is the target value (data type should be numeric)\n            y: y is 2-d numpy array in format (no. of samples, future sequence length)\n            if future sequence length > 1, or 1-d numpy array in format (no. of samples, )\n            if future sequence length = 1\n        """"""\n        self.config = self._get_feat_config(**config)\n\n        if isinstance(input_df, list):\n            train_x_list = []\n            train_y_list = []\n            for df in input_df:\n                x, y = self._fit_transform(df)\n                train_x_list.append(x)\n                train_y_list.append(y)\n            train_x = np.concatenate(train_x_list, axis=0)\n            train_y = np.concatenate(train_y_list, axis=0)\n        else:\n            train_x, train_y = self._fit_transform(input_df)\n        return train_x, train_y\n\n    def _transform(self, input_df, mode):\n        """"""\n        Transform data into features using the preset of configurations from fit_transform\n        :param input_df: The input time series data frame.\n         Example:\n         datetime   value   ""extra feature 1""   ""extra feature 2""\n         2019-01-01  1.9         1                       2\n         2019-01-02  2.3         0                       2\n        :param mode: \'val\'/\'test\'.\n        :return: tuple (x,y)\n            x: 3-d array in format (no. of samples, past sequence length, 2+feature length),\n            in the last dimension, the 1st col is the time index\n            (data type needs to be numpy datetime type, e.g. ""datetime64""),\n            the 2nd col is the target value (data type should be numeric)\n            y: y is 2-d numpy array in format (no. of samples, future sequence length)\n            if future sequence length > 1, or 1-d numpy array in format (no. of samples, )\n            if future sequence length = 1\n        """"""\n        self._check_input(input_df, mode)\n        # generate features\n        feature_data = self._get_features(input_df, self.config)\n        # select and standardize data\n        data_n = self._scale(feature_data)\n        if mode == \'val\':\n            (x, y) = self._roll_train(data_n,\n                                      past_seq_len=self.past_seq_len,\n                                      future_seq_len=self.future_seq_len)\n            return x, y\n        else:\n            x = self._roll_test(data_n, past_seq_len=self.past_seq_len)\n            return x, None\n\n    def transform(self, input_df, is_train=True):\n        """"""\n        Transform data into features using the preset of configurations from fit_transform\n        :param input_df: The input time series data frame, input_df can be a list of data frame or\n                         one data frame.\n         Example:\n         datetime   value   ""extra feature 1""   ""extra feature 2""\n         2019-01-01  1.9         1                       2\n         2019-01-02  2.3         0                       2\n        :param is_train: If the input_df is for training.\n        :return: tuple (x,y)\n            x: 3-d array in format (no. of samples, past sequence length, 2+feature length),\n            in the last dimension, the 1st col is the time index\n            (data type needs to be numpy datetime type, e.g. ""datetime64""),\n            the 2nd col is the target value (data type should be numeric)\n            y: y is 2-d numpy array in format (no. of samples, future sequence length)\n            if future sequence length > 1, or 1-d numpy array in format (no. of samples, )\n            if future sequence length = 1\n        """"""\n        if self.config is None or self.past_seq_len is None:\n            raise Exception(""Needs to call fit_transform or restore first before calling transform"")\n        mode = ""val"" if is_train else ""test""\n        if isinstance(input_df, list):\n            output_x_list = []\n            output_y_list = []\n            for df in input_df:\n                if mode == \'val\':\n                    x, y = self._transform(df, mode)\n                    output_x_list.append(x)\n                    output_y_list.append(y)\n                else:\n                    x, _ = self._transform(df, mode)\n                    output_x_list.append(x)\n            output_x = np.concatenate(output_x_list, axis=0)\n            if output_y_list:\n                output_y = np.concatenate(output_y_list, axis=0)\n            else:\n                output_y = None\n        else:\n            output_x, output_y = self._transform(input_df, mode)\n        return output_x, output_y\n\n    def _unscale(self, y):\n        # for standard scalar\n        value_mean = self.scaler.mean_[0]\n        value_scale = self.scaler.scale_[0]\n        y_unscale = y * value_scale + value_mean\n        return y_unscale\n\n    def unscale_uncertainty(self, y_uncertainty):\n        value_scale = self.scaler.scale_[0]\n        # print(value_scale)\n        y_uncertainty_unscle = y_uncertainty * value_scale\n        return y_uncertainty_unscle\n\n    def _get_y_pred_df(self, y_pred_dt_df, y_pred_unscale):\n        """"""\n        get prediction data frame with datetime column and target column.\n        :param input_df:\n        :return : prediction data frame. If future_seq_len is 1, the output data frame columns are\n            datetime | {target_col}. Otherwise, the output data frame columns are\n            datetime | {target_col}_0 | {target_col}_1 | ...\n        """"""\n        y_pred_df = y_pred_dt_df\n        if self.future_seq_len > 1:\n            columns = [""{}_{}"".format(self.target_col, i) for i in range(self.future_seq_len)]\n            y_pred_df[columns] = pd.DataFrame(y_pred_unscale)\n        else:\n            y_pred_df[self.target_col] = y_pred_unscale\n        return y_pred_df\n\n    def post_processing(self, input_df, y_pred, is_train):\n        """"""\n        Used only in pipeline predict, after calling self.transform(input_df, is_train=False).\n        Post_processing includes converting the predicted array into data frame and scalar inverse\n        transform.\n        :param input_df: a list of data frames or one data frame.\n        :param y_pred: Model prediction result (ndarray).\n        :param is_train: indicate the output is used to evaluation or prediction.\n        :return:\n         In validation mode (is_train=True), return the unscaled y_pred and rolled input_y.\n         In test mode (is_train=False) return unscaled data frame(s) in the format of\n          {datetime_col} | {target_col(s)}.\n        """"""\n        y_pred_unscale = self._unscale(y_pred)\n        if is_train:\n            # return unscaled y_pred (ndarray) and y (ndarray).\n            if isinstance(input_df, list):\n                y_unscale_list = []\n                for df in input_df:\n                    _, y_unscale = self._roll_train(df[[self.target_col]],\n                                                    self.past_seq_len,\n                                                    self.future_seq_len)\n                    y_unscale_list.append(y_unscale)\n                output_y_unscale = np.concatenate(y_unscale_list, axis=0)\n            else:\n                _, output_y_unscale = self._roll_train(input_df[[self.target_col]],\n                                                       self.past_seq_len,\n                                                       self.future_seq_len)\n            return output_y_unscale, y_pred_unscale\n\n        else:\n            # return data frame or a list of data frames.\n            if isinstance(input_df, list):\n                y_pred_dt_df_list = self._get_y_pred_dt_df(input_df, self.past_seq_len)\n                y_pred_df_list = []\n                y_pred_st_loc = 0\n                for y_pred_dt_df in y_pred_dt_df_list:\n                    df = self._get_y_pred_df(y_pred_dt_df,\n                                             y_pred_unscale[y_pred_st_loc:\n                                                            y_pred_st_loc + len(y_pred_dt_df)])\n                    y_pred_st_loc = y_pred_st_loc + len(y_pred_dt_df)\n                    y_pred_df_list.append(df)\n                assert y_pred_st_loc == len(y_pred_unscale)\n                return y_pred_df_list\n            else:\n                y_pred_dt_df = self._get_y_pred_dt_df(input_df, self.past_seq_len)\n                y_pred_df = self._get_y_pred_df(y_pred_dt_df, y_pred_unscale)\n                return y_pred_df\n\n    def save(self, file_path, replace=False):\n        """"""\n        save the feature tools internal variables as well as the initialization args.\n        Some of the variables are derived after fit_transform, so only saving config is not enough.\n        :param: file : the file to be saved\n        :return:\n        """"""\n        # for StandardScaler()\n        data_to_save = {""mean"": self.scaler.mean_.tolist(),\n                        ""scale"": self.scaler.scale_.tolist(),\n                        ""future_seq_len"": self.future_seq_len,\n                        ""dt_col"": self.dt_col,\n                        ""target_col"": self.target_col,\n                        ""extra_features_col"": self.extra_features_col,\n                        ""drop_missing"": self.drop_missing\n                        }\n        save_config(file_path, data_to_save, replace=replace)\n\n    def restore(self, **config):\n        """"""\n        Restore variables from file\n        :return:\n        """"""\n#         with open(file_path, \'r\') as input_file:\n#             result = json.load(input_file)\n\n        # for StandardScalar()\n        self.scaler = StandardScaler()\n        self.scaler.mean_ = np.asarray(config[""mean""])\n        self.scaler.scale_ = np.asarray(config[""scale""])\n\n        self.config = self._get_feat_config(**config)\n\n        self.future_seq_len = config[""future_seq_len""]\n        self.dt_col = config[""dt_col""]\n        self.target_col = config[""target_col""]\n        self.extra_features_col = config[""extra_features_col""]\n        self.drop_missing = config[""drop_missing""]\n\n        # for MinMaxScalar()\n        # self.scaler = MinMaxScaler()\n        # self.scaler.min_ = np.asarray(result[""min""])\n        # self.scaler.scale_ = np.asarray(result[""scale""])\n        # print(self.scaler.transform(input_data))\n\n    def get_feature_list(self, input_df):\n        if isinstance(input_df, list):\n            feature_matrix, feature_defs = self._generate_features(input_df[0])\n        else:\n            feature_matrix, feature_defs = self._generate_features(input_df)\n    # return [feat.generate_name() for feat in feature_defs if isinstance(feat, TransformFeature)]\n        feature_list = []\n        for feat in feature_defs:\n            feature_name = feat.generate_name()\n            # print(feature_name)\n            # todo: need to change if more than one target cols are supported\n            if isinstance(feat, TransformFeature) \\\n                    or (self.extra_features_col and feature_name in self.extra_features_col):\n                # if feature_name != self.target_col:\n                feature_list.append(feature_name)\n        return feature_list\n\n    def _get_feat_config(self, **config):\n        """"""\n        Get feature related arguments from global hyper parameter config and do necessary error\n        checking\n        :param config: the global config (usually from hyper parameter tuning)\n        :return: config only for feature engineering\n        """"""\n        self._check_config(**config)\n        feature_config_names = [""selected_features"", ""past_seq_len""]\n        feat_config = {}\n        for name in feature_config_names:\n            if name not in config:\n                continue\n                # raise KeyError(""Can not find "" + name + "" in config!"")\n            feat_config[name] = config[name]\n        self.past_seq_len = feat_config.get(""past_seq_len"", 1)\n        return feat_config\n\n    def _check_input(self, input_df, mode=""train""):\n        """"""\n        Check dataframe for integrity. Requires time sequence to come in uniform sampling intervals.\n        :param input_df:\n        :return:\n        """"""\n        # check NaT in datetime\n        input_df = input_df.reset_index()\n        dt = input_df[self.dt_col]\n        if not np.issubdtype(dt, np.datetime64):\n            raise ValueError(""The dtype of datetime column is required to be np.datetime64!"")\n        is_nat = pd.isna(dt)\n        if is_nat.any(axis=None):\n            raise ValueError(""Missing datetime in input dataframe!"")\n\n        # check uniform (is that necessary?)\n        interval = dt[1] - dt[0]\n\n        if not all([dt[i] - dt[i - 1] == interval for i in range(1, len(dt))]):\n            raise ValueError(""Input time sequence intervals are not uniform!"")\n\n        # check missing values\n        if not self.drop_missing:\n            is_nan = pd.isna(input_df)\n            if is_nan.any(axis=None):\n                raise ValueError(""Missing values in input dataframe!"")\n\n        # check if the last datetime is large than current time.\n        # In that case, feature tools generate NaN.\n        last_datetime = dt.iloc[-1]\n        current_time = np.datetime64(\'today\', \'s\')\n        if last_datetime > current_time:\n            raise ValueError(""Last date time is bigger than current time!"")\n\n        # check if the length of input data is smaller than requested.\n        if mode == ""test"":\n            min_input_len = self.past_seq_len\n            error_msg = ""Length of {} data should be larger than "" \\\n                        ""the past sequence length selected by automl.\\n"" \\\n                        ""{} data length: {}\\n"" \\\n                        ""past sequence length selected: {}\\n"" \\\n                .format(mode, mode, len(input_df), self.past_seq_len)\n        else:\n            min_input_len = self.past_seq_len + self.future_seq_len\n            error_msg = ""Length of {} data should be larger than "" \\\n                        ""the sequence length you want to predict "" \\\n                        ""plus the past sequence length selected by automl.\\n""\\\n                        ""{} data length: {}\\n""\\\n                        ""predict sequence length: {}\\n""\\\n                        ""past sequence length selected: {}\\n""\\\n                .format(mode, mode, len(input_df), self.future_seq_len, self.past_seq_len)\n        if len(input_df) < min_input_len:\n            raise ValueError(error_msg)\n\n        return input_df\n\n    def _roll_data(self, data, seq_len):\n        result = []\n        mask = []\n        for i in range(len(data) - seq_len + 1):\n            result.append(data[i: i + seq_len])\n\n            if pd.isna(data[i: i + seq_len]).any(axis=None):\n                mask.append(0)\n            else:\n                mask.append(1)\n\n        return np.asarray(result), np.asarray(mask)\n\n    def _roll_train(self, dataframe, past_seq_len, future_seq_len):\n        """"""\n        roll dataframe into sequence samples to be used in TimeSequencePredictor.\n        roll_train: split the whole dataset apart to build (x, y).\n        :param df: a dataframe which has been resampled in uniform frequency.\n        :param past_seq_len: the length of the past sequence\n        :param future_seq_len: the length of the future sequence\n        :return: tuple (x,y)\n            x: 3-d array in format (no. of samples, past sequence length, 2+feature length), in the\n            last dimension, the 1st col is the time index (data type needs to be numpy datetime type\n            , e.g. ""datetime64""),\n            the 2nd col is the target value (data type should be numeric)\n            y: y is 2-d numpy array in format (no. of samples, future sequence length) if future\n            sequence length > 1, or 1-d numpy array in format (no. of samples, ) if future sequence\n            length = 1\n        """"""\n        x = dataframe[0:-future_seq_len].values\n        y = dataframe.iloc[past_seq_len:, 0].values\n        output_x, mask_x = self._roll_data(x, past_seq_len)\n        output_y, mask_y = self._roll_data(y, future_seq_len)\n        # assert output_x.shape[0] == output_y.shape[0],\n        # ""The shape of output_x and output_y doesn\'t match! ""\n        mask = (mask_x == 1) & (mask_y == 1)\n        return output_x[mask], output_y[mask]\n\n    def _roll_test(self, dataframe, past_seq_len):\n        """"""\n        roll dataframe into sequence samples to be used in TimeSequencePredictor.\n        roll_test: the whole dataframe is regarded as x.\n        :param df: a dataframe which has been resampled in uniform frequency.\n        :param past_seq_len: the length of the past sequence\n        :return: x\n            x: 3-d array in format (no. of samples, past sequence length, 2+feature length), in the\n            last dimension, the 1st col is the time index (data type needs to be numpy datetime type\n            , e.g. ""datetime64""),\n            the 2nd col is the target value (data type should be numeric)\n        """"""\n        x = dataframe.values\n        output_x, mask_x = self._roll_data(x, past_seq_len)\n        # assert output_x.shape[0] == output_y.shape[0],\n        # ""The shape of output_x and output_y doesn\'t match! ""\n        mask = (mask_x == 1)\n        return output_x[mask]\n\n    def __get_y_pred_dt_df(self, input_df, past_seq_len):\n        """"""\n        :param input_df: one data frame\n        :return: a data frame with prediction datetime\n        """"""\n        input_df = input_df.reset_index(drop=True)\n        input_dt_df = input_df.reset_index(drop=True)[[self.dt_col]].copy()\n        time_delta = input_dt_df.iloc[-1] - input_dt_df.iloc[-2]\n        last_time = input_dt_df.iloc[-1] + time_delta\n        last_df = pd.DataFrame({self.dt_col: last_time})\n        pre_pred_dt_df = input_dt_df[past_seq_len:].copy()\n        pre_pred_dt_df = pre_pred_dt_df.reset_index(drop=True)\n        y_pred_dt_df = pre_pred_dt_df.append(last_df, ignore_index=True)\n        # print(y_pred_dt_df)\n        return y_pred_dt_df\n\n    def _get_y_pred_dt_df(self, input_df, past_seq_len):\n        """"""\n        :param input_df: a data frame or a list of data frame\n        :param past_seq_len:\n        :return:\n        """"""\n        if isinstance(input_df, list):\n            y_pred_dt_df_list = []\n            for df in input_df:\n                y_pred_dt_df = self.__get_y_pred_dt_df(df, past_seq_len)\n                y_pred_dt_df_list.append(y_pred_dt_df)\n            return y_pred_dt_df_list\n        else:\n            return self.__get_y_pred_dt_df(input_df, past_seq_len)\n\n    def _scale(self, data):\n        """"""\n        Scale the data\n        :param data:\n        :return:\n        """"""\n        np_scaled = self.scaler.transform(data)\n        data_s = pd.DataFrame(np_scaled)\n        return data_s\n\n    def _rearrange_data(self, input_df):\n        """"""\n        change the input_df column order into [datetime, target, feature1, feature2, ...]\n        :param input_df:\n        :return:\n        """"""\n        cols = input_df.columns.tolist()\n        new_cols = [self.dt_col,\n                    self.target_col] + [col for col in cols\n                                        if col != self.dt_col and col != self.target_col]\n        rearranged_data = input_df[new_cols].copy\n        return rearranged_data\n\n    def _generate_features(self, input_df):\n        df = input_df.copy()\n        df[""id""] = df.index + 1\n\n        es = ft.EntitySet(id=""data"")\n        es = es.entity_from_dataframe(entity_id=""time_seq"",\n                                      dataframe=df,\n                                      index=""id"",\n                                      time_index=self.dt_col)\n\n        def is_awake(column):\n            hour = column.dt.hour\n            return (((hour >= 6) & (hour <= 23)) | (hour == 0)).astype(int)\n\n        def is_busy_hours(column):\n            hour = column.dt.hour\n            return (((hour >= 7) & (hour <= 9)) | (hour >= 16) & (hour <= 19)).astype(int)\n\n        IsAwake = make_trans_primitive(function=is_awake,\n                                       input_types=[DatetimeTimeIndex],\n                                       return_type=Numeric)\n        IsBusyHours = make_trans_primitive(function=is_busy_hours,\n                                           input_types=[DatetimeTimeIndex],\n                                           return_type=Numeric)\n\n        feature_matrix, feature_defs = ft.dfs(entityset=es,\n                                              target_entity=""time_seq"",\n                                              agg_primitives=[""count""],\n                                              trans_primitives=[""month"", ""weekday"", ""day"", ""hour"",\n                                                                ""is_weekend"", IsAwake, IsBusyHours])\n        return feature_matrix, feature_defs\n\n    def _get_features(self, input_df, config):\n        feature_matrix, feature_defs = self._generate_features(input_df)\n        # self.write_generate_feature_list(feature_defs)\n        feature_cols = np.asarray(json.loads(config.get(""selected_features"")))\n        # we do not include target col in candidates.\n        # the first column is designed to be the default position of target column.\n        target_col = np.array([self.target_col])\n        cols = np.concatenate([target_col, feature_cols])\n        target_feature_matrix = feature_matrix[cols]\n        return target_feature_matrix.astype(float)\n\n    def _get_optional_parameters(self):\n        return set([""past_seq_len""])\n\n    def _get_required_parameters(self):\n        return set([""selected_features""])\n'"
pyzoo/zoo/automl/model/MTNet_keras.py,0,"b'# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# MIT License\n#\n# Copyright (c) 2018 Roland Zimmermann\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\nimport numpy as np\nimport time\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.initializers import TruncatedNormal, Constant\nimport tensorflow.keras.backend as K\nimport math\n\nimport tensorflow as tf\n# _Linear = core_rnn_cell._Linear\nfrom zoo.automl.common.metrics import Evaluator\nimport pandas as pd\nfrom zoo.automl.model.abstract import BaseModel\nfrom zoo.automl.common.util import save_config\n\n\nclass AttentionRNNWrapper(Wrapper):\n    """"""\n        This class is modified based on\n        https://github.com/zimmerrol/keras-utility-layer-collection/blob/master/kulc/attention.py.\n        The idea of the implementation is based on the paper:\n            ""Effective Approaches to Attention-based Neural Machine Translation"" by Luong et al.\n        This layer is an attention layer, which can be wrapped around arbitrary RNN layers.\n        This way, after each time step an attention vector is calculated\n        based on the current output of the LSTM and the entire input time series.\n        This attention vector is then used as a weight vector to choose special values\n        from the input data. This data is then finally concatenated to the next input time step\'s\n        data. On this a linear transformation in the same space as the input data\'s space\n        is performed before the data is fed into the RNN cell again.\n        This technique is similar to the input-feeding method described in the paper cited\n    """"""\n\n    def __init__(self, layer, weight_initializer=""glorot_uniform"", **kwargs):\n        assert isinstance(layer, RNN)\n        self.layer = layer\n        self.supports_masking = True\n        self.weight_initializer = weight_initializer\n\n        super(AttentionRNNWrapper, self).__init__(layer, **kwargs)\n\n    def _validate_input_shape(self, input_shape):\n        if len(input_shape) != 3:\n            raise ValueError(\n                ""Layer received an input with shape {0} but expected a Tensor of rank 3."".format(\n                    input_shape[0]))\n\n    def build(self, input_shape):\n        self._validate_input_shape(input_shape)\n\n        self.input_spec = InputSpec(shape=input_shape)\n\n        if not self.layer.built:\n            self.layer.build(input_shape)\n            self.layer.built = True\n\n        input_dim = input_shape[-1]\n\n        if self.layer.return_sequences:\n            output_dim = self.layer.compute_output_shape(input_shape)[0][-1]\n        else:\n            output_dim = self.layer.compute_output_shape(input_shape)[-1]\n\n        input_dim = input_dim.value\n        output_dim = output_dim.value\n\n        self._W1 = self.add_weight(shape=(input_dim, input_dim), name=""{}_W1"".format(self.name),\n                                   initializer=self.weight_initializer)\n        self._W2 = self.add_weight(shape=(output_dim, input_dim), name=""{}_W2"".format(self.name),\n                                   initializer=self.weight_initializer)\n        self._W3 = self.add_weight(shape=(2 * input_dim, input_dim), name=""{}_W3"".format(self.name),\n                                   initializer=self.weight_initializer)\n        self._b2 = self.add_weight(shape=(input_dim,), name=""{}_b2"".format(self.name),\n                                   initializer=self.weight_initializer)\n        self._b3 = self.add_weight(shape=(input_dim,), name=""{}_b3"".format(self.name),\n                                   initializer=self.weight_initializer)\n        self._V = self.add_weight(shape=(input_dim, 1), name=""{}_V"".format(self.name),\n                                  initializer=self.weight_initializer)\n\n        super(AttentionRNNWrapper, self).build()\n\n    def compute_output_shape(self, input_shape):\n        self._validate_input_shape(input_shape)\n\n        return self.layer.compute_output_shape(input_shape)\n\n    @property\n    def trainable_weights(self):\n        return self._trainable_weights + self.layer.trainable_weights\n\n    @property\n    def non_trainable_weights(self):\n        return self._non_trainable_weights + self.layer.non_trainable_weights\n\n    def step(self, x, states):\n        h = states[1]\n        # states[1] necessary?\n\n        # equals K.dot(X, self._W1) + self._b2 with X.shape=[bs, T, input_dim]\n        total_x_prod = states[-1]\n        # comes from the constants (equals the input sequence)\n        X = states[-2]\n\n        # expand dims to add the vector which is only valid for this time step\n        # to total_x_prod which is valid for all time steps\n        hw = K.expand_dims(K.dot(h, self._W2), 1)\n        additive_atn = total_x_prod + hw\n        attention = K.softmax(K.dot(additive_atn, self._V), axis=1)\n        x_weighted = K.sum(attention * X, [1])\n\n        x = K.dot(K.concatenate([x, x_weighted], 1), self._W3) + self._b3\n\n        h, new_states = self.layer.cell.call(x, states[:-2])\n\n        return h, new_states\n\n    def call(self, x, constants=None, mask=None, initial_state=None):\n        # input shape: (n_samples, time (padded with zeros), input_dim)\n        input_shape = self.input_spec.shape\n\n        if self.layer.stateful:\n            initial_states = self.layer.states\n        elif initial_state is not None:\n            initial_states = initial_state\n            if not isinstance(initial_states, (list, tuple)):\n                initial_states = [initial_states]\n\n            base_initial_state = self.layer.get_initial_state(x)\n            if len(base_initial_state) != len(initial_states):\n                raise ValueError(\n                    ""initial_state does not have the correct length. Received length {0} ""\n                    ""but expected {1}"".format(len(initial_states), len(base_initial_state)))\n            else:\n                # check the state\' shape\n                for i in range(len(initial_states)):\n                    # initial_states[i][j] != base_initial_state[i][j]:\n                    if not initial_states[i].shape.is_compatible_with(base_initial_state[i].shape):\n                        raise ValueError(\n                            ""initial_state does not match the default base state of the layer. ""\n                            ""Received {0} but expected {1}"".format(\n                                [x.shape for x in initial_states],\n                                [x.shape for x in base_initial_state]))\n        else:\n            initial_states = self.layer.get_initial_state(x)\n\n        # print(initial_states)\n\n        if not constants:\n            constants = []\n\n        constants += self.get_constants(x)\n\n        last_output, outputs, states = K.rnn(\n            self.step,\n            x,\n            initial_states,\n            go_backwards=self.layer.go_backwards,\n            mask=mask,\n            constants=constants,\n            unroll=self.layer.unroll,\n            input_length=input_shape[1]\n        )\n\n        if self.layer.stateful:\n            self.updates = []\n            for i in range(len(states)):\n                self.updates.append((self.layer.states[i], states[i]))\n\n        if self.layer.return_sequences:\n            output = outputs\n        else:\n            output = last_output\n\n            # Properly set learning phase\n        if getattr(last_output, \'_uses_learning_phase\', False):\n            output._uses_learning_phase = True\n            for state in states:\n                state._uses_learning_phase = True\n\n        if self.layer.return_state:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            else:\n                states = list(states)\n            return [output] + states\n        else:\n            return output\n\n    def get_constants(self, x):\n        # add constants to speed up calculation\n        constants = [x, K.dot(x, self._W1) + self._b2]\n\n        return constants\n\n    def get_config(self):\n        config = {\'weight_initializer\': self.weight_initializer}\n        base_config = super(AttentionRNNWrapper, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass MTNetKeras(BaseModel):\n\n    def __init__(self, check_optional_config=False, future_seq_len=1):\n\n        """"""\n        Constructor of MTNet model\n        """"""\n        self.check_optional_config = check_optional_config\n        self.config = None\n        # config parameter\n        self.time_step = None  # timestep\n        self.cnn_height = None  # convolution window size (convolution filter height)` ?\n        self.long_num = None  # the number of the long-term memory series\n        self.ar_window = None  # the window size of ar model\n        self.feature_num = None  # input\'s variable dimension (convolution filter width)\n        self.output_dim = None  # output\'s variable dimension\n        self.cnn_hid_size = None\n        # last size is equal to en_conv_hidden_size, should be a list\n        self.rnn_hid_sizes = None\n        self.last_rnn_size = None\n        self.dropout = None\n        self.lr = None\n        self.batch_size = None\n\n        self.saved_configs = {""cnn_height"", ""long_num"", ""time_step"", ""ar_window"",\n                              ""cnn_hid_size"", ""rnn_hid_sizes"", ""dropout"", ""lr"", ""batch_size"",\n                              ""epochs"", ""metrics"", ""mc"",\n                              ""feature_num"", ""output_dim""}\n        self.model = None\n        self.metrics = None\n        self.mc = None\n        self.epochs = None\n\n    def apply_config(self, rs=False, config=None):\n        super()._check_config(**config)\n        if rs:\n            config_names = set(config.keys())\n            assert config_names.issuperset(self.saved_configs)\n            # assert config_names.issuperset(self.lr_decay_configs) or \\\n            #        config_names.issuperset(self.lr_configs)\n        self.epochs = config.get(""epochs"")\n        self.metrics = config.get(""metrics"", [""mean_squared_error""])\n        self.mc = config.get(""mc"")\n        self.feature_num = config[""feature_num""]\n        self.output_dim = config[""output_dim""]\n        self.time_step = config.get(""time_step"", 1)\n        self.long_num = config.get(""long_num"", 7)\n        self.ar_window = config.get(""ar_window"", 1)\n        self.cnn_height = config.get(""cnn_height"", 1)\n        self.cnn_hid_size = config.get(""cnn_hid_size"", 32)\n        self.rnn_hid_sizes = config.get(""rnn_hid_sizes"", [16, 32])\n        self.last_rnn_size = self.rnn_hid_sizes[-1]\n        self.dropout = config.get(""dropout"", 0.2)\n\n        self.batch_size = config.get(""batch_size"", 64)\n        self.lr = config.get(\'lr\', 0.001)\n        self._check_configs()\n\n    def _check_configs(self):\n        assert self.time_step >= 1, \\\n            ""Invalid configuration value. \'time_step\' must be larger than 1""\n        assert self.time_step >= self.ar_window, \\\n            ""Invalid configuration value. \'ar_window\' must not exceed \'time_step\'""\n        assert isinstance(self.rnn_hid_sizes, list), \\\n            ""Invalid configuration value. \'rnn_hid_sizes\' must be a list of integers""\n        # assert self.cnn_hid_size == self.last_rnn_size,\\\n        #     ""Invalid configuration value. \'cnn_hid_size\' must be equal to the last element of "" \\\n        #     ""\'rnn_hid_sizes\'""\n\n    def build(self):\n        """"""\n        build MTNet model\n        :param config:\n        :return:\n        """"""\n        training = True if self.mc else None\n        # long-term time series historical data inputs\n        long_input = Input(shape=(self.long_num, self.time_step, self.feature_num))\n        # short-term time series historical data\n        short_input = Input(shape=(self.time_step, self.feature_num))\n\n        # ------- no-linear component----------------\n        # memory and context : (batch, long_num, last_rnn_size)\n        memory = self.__encoder(long_input, num=self.long_num, name=\'memory\', training=training)\n        # memory = memory_model(long_input)\n        context = self.__encoder(long_input, num=self.long_num, name=\'context\', training=training)\n        # context = context_model(long_input)\n        # query: (batch, 1, last_rnn_size)\n        query_input = Reshape((1, self.time_step, self.feature_num),\n                              name=\'reshape_query\')(short_input)\n        query = self.__encoder(query_input, num=1, name=\'query\', training=training)\n        # query = query_model(query_input)\n\n        # prob = memory * query.T, shape is (long_num, 1)\n        query_t = Permute((2, 1))(query)\n        prob = Lambda(lambda xy: tf.matmul(xy[0], xy[1]))([memory, query_t])\n        prob = Softmax(axis=-1)(prob)\n        # out is of the same shape of context: (batch, long_num, last_rnn_size)\n        out = multiply([context, prob])\n        # concat: (batch, long_num + 1, last_rnn_size)\n\n        pred_x = concatenate([out, query], axis=1)\n        reshaped_pred_x = Reshape((self.last_rnn_size * (self.long_num + 1),),\n                                  name=""reshape_pred_x"")(pred_x)\n        nonlinear_pred = Dense(units=self.output_dim,\n                               kernel_initializer=TruncatedNormal(stddev=0.1),\n                               bias_initializer=Constant(0.1),)(reshaped_pred_x)\n\n        # ------------ ar component ------------\n        if self.ar_window > 0:\n            ar_pred_x = Reshape((self.ar_window * self.feature_num,),\n                                name=""reshape_ar"")(short_input[:, -self.ar_window:])\n            linear_pred = Dense(units=self.output_dim,\n                                kernel_initializer=TruncatedNormal(stddev=0.1),\n                                bias_initializer=Constant(0.1),)(ar_pred_x)\n        else:\n            linear_pred = 0\n        y_pred = Add()([nonlinear_pred, linear_pred])\n        self.model = Model(inputs=[long_input, short_input], outputs=y_pred)\n        # lr decay\n        # def lr_scheduler(epoch, r):\n        #     max_lr = 0.03\n        #     min_lr = 0.0001\n        #     lr = min_lr + (max_lr - min_lr) * math.exp(-epoch / 60)\n        #     return lr\n        # callbacks = [tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)]\n        # initial_lr = 0.003\n        # rate = math.exp(-1 / 60)\n        # lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        #     initial_lr,\n        #     decay_steps=249,\n        #     decay_rate=rate,\n        #     staircase=True\n        # )\n        #\n        # self.model.compile(loss=""mae"",\n        #                    metrics=metrics,\n        #                    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule))\n\n        self.model.compile(loss=""mae"",\n                           metrics=self.metrics,\n                           optimizer=tf.keras.optimizers.Adam(lr=self.lr))\n\n        return self.model\n\n    def __encoder(self, input, num, name=\'Encoder\', training=None):\n        """"""\n            Treat batch_size dimension and num dimension as one batch_size dimension\n            (batch_size * num).\n        :param input:  <batch_size, num, time_step, input_dim>\n        :param num: the number of input time series data. For short term data, the num is 1.\n        :return: the embedded of the input <batch_size, num, last_rnn_hid_size>\n        """"""\n        # input = Input(shape=(num, self.time_step, self.feature_num))\n        batch_size_new = self.batch_size * num\n        Tc = self.time_step - self.cnn_height + 1\n\n        # CNN\n        # reshaped input: (batch_size_new, time_step, feature_num, 1)\n        reshaped_input = Lambda(lambda x:\n                                K.reshape(x, (-1, self.time_step, self.feature_num, 1),),\n                                name=name+\'reshape_cnn\')(input)\n        # output: <batch_size_new, conv_out, 1, en_conv_hidden_size>\n        cnn_out = Conv2D(filters=self.cnn_hid_size,\n                         kernel_size=(self.cnn_height, self.feature_num),\n                         padding=""valid"",\n                         kernel_initializer=TruncatedNormal(stddev=0.1),\n                         bias_initializer=Constant(0.1),\n                         activation=""relu"")(reshaped_input)\n        cnn_out = Dropout(self.dropout)(cnn_out, training=training)\n\n        rnn_input = Lambda(lambda x:\n                           K.reshape(x, (-1, num, Tc, self.cnn_hid_size)),)(cnn_out)\n\n        # use AttentionRNNWrapper\n        rnn_cells = [GRUCell(h_size, activation=""relu"", dropout=self.dropout)\n                     for h_size in self.rnn_hid_sizes]\n\n        attention_rnn = AttentionRNNWrapper(RNN(rnn_cells),\n                                            weight_initializer=TruncatedNormal(stddev=0.1))\n\n        outputs = []\n        for i in range(num):\n            input_i = rnn_input[:, i]\n            # input_i = (batch, conv_hid_size, Tc)\n            input_i = Permute((2, 1), input_shape=[Tc, self.cnn_hid_size])(input_i)\n            # output = (batch, last_rnn_hid_size)\n            output_i = attention_rnn(input_i, training=training)\n            # output = (batch, 1, last_rnn_hid_size)\n            output_i = Reshape((1, -1))(output_i)\n            outputs.append(output_i)\n        if len(outputs) > 1:\n            output = Lambda(lambda x: concatenate(x, axis=1))(outputs)\n        else:\n            output = outputs[0]\n        return output\n\n    def _reshape_input_x(self, x):\n        long_term = np.reshape(x[:, : self.time_step * self.long_num],\n                               [-1, self.long_num, self.time_step, x.shape[-1]])\n        short_term = np.reshape(x[:, self.time_step * self.long_num:],\n                                [-1, self.time_step, x.shape[-1]])\n        return long_term, short_term\n\n    def _pre_processing(self, x, validation_data=None):\n        long_term, short_term = self._reshape_input_x(x)\n        if validation_data:\n            val_x, val_y = validation_data\n            long_val, short_val = self._reshape_input_x(val_x)\n            validation_data = ([long_val, short_val], val_y)\n        return [long_term, short_term], validation_data\n\n    def _add_config_attributes(self, config, **new_attributes):\n        # new_attributes are among [""metrics"", ""epochs"", ""mc"", ""feature_num"", ""output_dim""]\n        if self.config is None:\n            self.config = config\n        else:\n            if config:\n                raise ValueError(""You can only pass new configuations for \'mc\', \'epochs\' and ""\n                                 ""\'metrics\' during incremental fitting. ""\n                                 ""Additional configs passed are {}"".format(config))\n\n        if new_attributes[""metrics""] is None:\n            del new_attributes[""metrics""]\n        self.config.update(new_attributes)\n\n    def _check_input(self, x, y):\n        input_feature_num = x.shape[-1]\n        input_output_dim = y.shape[-1]\n        if input_feature_num is None:\n            raise ValueError(""input x is None!"")\n        if input_output_dim is None:\n            raise ValueError(""input y is None!"")\n\n        if self.feature_num is not None and self.feature_num != input_feature_num:\n            raise ValueError(""input x has different feature number (the shape of last dimension) ""\n                             ""{} with the fitted model, which is {}.""\n                             .format(input_feature_num, self.feature_num))\n        if self.output_dim is not None and self.output_dim != input_output_dim:\n            raise ValueError(""input y has different prediction size (the shape of last dimension) ""\n                             ""of {} with the fitted model, which is {}.""\n                             .format(input_output_dim, self.output_dim))\n        return input_feature_num, input_output_dim\n\n    def fit_eval(self, x, y, validation_data=None, mc=False, metrics=None,\n                 epochs=10, verbose=0, **config):\n        feature_num, output_dim = self._check_input(x, y)\n        self._add_config_attributes(config, epochs=epochs, mc=mc, metrics=metrics,\n                                    feature_num=feature_num, output_dim=output_dim)\n        self.apply_config(config=self.config)\n        processed_x, processed_validation_data = self._pre_processing(x, validation_data)\n\n        # if model is not initialized, __build the model\n        if self.model is None:\n            st = time.time()\n            self.build()\n            end = time.time()\n            if verbose == 1:\n                print(""Build model took {}s"".format(end - st))\n\n        st = time.time()\n        hist = self.model.fit(processed_x, y, validation_data=processed_validation_data,\n                              batch_size=self.batch_size,\n                              epochs=self.epochs,\n                              verbose=verbose)\n\n        if verbose == 1:\n            print(""Fit model took {}s"".format(time.time() - st))\n        if validation_data is None:\n            # get train metrics\n            # results = self.model.evaluate(x, y)\n            result = hist.history.get(self.metrics[0])[-1]\n        else:\n            result = hist.history.get(\'val_\' + str(self.metrics[0]))[-1]\n        return result\n\n    def evaluate(self, x, y, metrics=[\'mse\']):\n        """"""\n        Evaluate on x, y\n        :param x: input\n        :param y: target\n        :param metric: a list of metrics in string format\n        :return: a list of metric evaluation results\n        """"""\n        y_pred = self.predict(x)\n        if y_pred.shape[1] == 1:\n            multioutput = \'uniform_average\'\n        else:\n            multioutput = \'raw_values\'\n        # y = np.squeeze(y, axis=2)\n        return [Evaluator.evaluate(m, y, y_pred, multioutput=multioutput) for m in metrics]\n\n    def predict(self, x, mc=False):\n        input_x = self._reshape_input_x(x)\n        return self.model.predict(input_x)\n\n    def predict_with_uncertainty(self, x, n_iter=100):\n        result = np.zeros((n_iter,) + (x.shape[0], self.output_dim))\n\n        for i in range(n_iter):\n            result[i, :, :] = self.predict(x, mc=True)\n\n        prediction = result.mean(axis=0)\n        uncertainty = result.std(axis=0)\n        return prediction, uncertainty\n\n    def save(self, model_path, config_path):\n        self.model.save_weights(model_path)\n        config_to_save = {""cnn_height"": self.cnn_height,\n                          ""long_num"": self.long_num,\n                          ""time_step"": self.time_step,\n                          ""ar_window"": self.ar_window,\n                          ""cnn_hid_size"": self.cnn_hid_size,\n                          ""rnn_hid_sizes"": self.rnn_hid_sizes,\n                          ""dropout"": self.dropout,\n                          ""lr"": self.lr,\n                          ""batch_size"": self.batch_size,\n                          # for fit eval\n                          ""epochs"": self.epochs,\n                          # todo: can not serialize metrics unless all elements are str\n                          ""metrics"": self.metrics,\n                          ""mc"": self.mc,\n                          ""feature_num"": self.feature_num,\n                          ""output_dim"": self.output_dim,\n                          }\n        assert set(config_to_save.keys()) == self.saved_configs, \\\n            ""The keys in config_to_save is not the same as self.saved_configs."" \\\n            ""Please keep them consistent""\n        # if self.decay_epochs > 0:\n        #     lr_decay_configs = {""min_lr"": self.min_lr,\n        #                         ""max_lr"": self.max_lr}\n        #     assert set(lr_decay_configs.keys()) == self.lr_decay_configs, \\\n        #         ""The keys in lr_decay_configs is not the same as self.lr_decay_configs."" \\\n        #         ""Please keep them consistent""\n        #     config_to_save.update(lr_decay_configs)\n        # else:\n        #     lr_configs = {""lr"": self.lr_value}\n        #     assert set(lr_configs.keys()) == self.lr_configs, \\\n        #         ""The keys in lr_configs is not the same as self.lr_configs."" \\\n        #         ""Please keep them consistent""\n        #     config_to_save.update(lr_configs)\n\n        save_config(config_path, config_to_save)\n\n    def restore(self, model_path, **config):\n        """"""\n        restore model from file\n        :param model_path: the model file\n        :param config: the trial config\n        """"""\n        self.config = config\n        self.apply_config(rs=True, config=config)\n        self.build()\n        self.model.load_weights(model_path)\n\n    def _get_optional_parameters(self):\n        return {\n            ""batch_size"",\n            ""dropout"",\n            ""time_step"",\n            ""filter_size"",\n            ""long_num"",\n            ""ar_size"",\n        }\n\n    def _get_required_parameters(self):\n        return {\n            ""feature_num"",\n            ""output_dim""\n        }\n'"
pyzoo/zoo/automl/model/Seq2Seq.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport json\n\nfrom time import time\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense\nimport tensorflow.keras as keras\n\nimport os\n\nfrom zoo.automl.feature.time_sequence import TimeSequenceFeatureTransformer\nfrom zoo.automl.model.abstract import BaseModel\nfrom zoo.automl.common.util import *\nfrom zoo.automl.common.metrics import Evaluator\n\n\nclass LSTMSeq2Seq(BaseModel):\n\n    def __init__(self, check_optional_config=True, future_seq_len=2):\n        """"""\n        Constructor of LSTM Seq2Seq model\n        """"""\n        self.model = None\n        self.past_seq_len = None\n        self.future_seq_len = future_seq_len\n        self.feature_num = None\n        self.target_col_num = None\n        self.metric = None\n        self.latent_dim = None\n        self.batch_size = None\n        self.check_optional_config = check_optional_config\n\n    def _build_train(self, mc=False, **config):\n        """"""\n        build LSTM Seq2Seq model\n        :param config:\n        :return:\n        """"""\n        super()._check_config(**config)\n        self.metric = config.get(\'metric\', \'mean_squared_error\')\n        self.latent_dim = config.get(\'latent_dim\', 128)\n        self.dropout = config.get(\'dropout\', 0.2)\n        self.lr = config.get(\'lr\', 0.001)\n        # for restore in continuous training\n        self.batch_size = config.get(\'batch_size\', 64)\n        training = True if mc else None\n\n        # Define an input sequence and process it.\n        self.encoder_inputs = Input(shape=(None, self.feature_num), name=""encoder_inputs"")\n        encoder = LSTM(units=self.latent_dim,\n                       dropout=self.dropout,\n                       return_state=True,\n                       name=""encoder_lstm"")\n        encoder_outputs, state_h, state_c = encoder(self.encoder_inputs, training=training)\n        # We discard `encoder_outputs` and only keep the states.\n        self.encoder_states = [state_h, state_c]\n\n        # Set up the decoder, using `encoder_states` as initial state.\n        self.decoder_inputs = Input(shape=(None, self.target_col_num), name=""decoder_inputs"")\n        # We set up our decoder to return full output sequences,\n        # and to return internal states as well. We don\'t use the\n        # return states in the training model, but we will use them in inference.\n        self.decoder_lstm = LSTM(self.latent_dim,\n                                 dropout=self.dropout,\n                                 return_sequences=True,\n                                 return_state=True,\n                                 name=""decoder_lstm"")\n        decoder_outputs, _, _ = self.decoder_lstm(self.decoder_inputs,\n                                                  training=training,\n                                                  initial_state=self.encoder_states)\n\n        self.decoder_dense = Dense(self.target_col_num, name=""decoder_dense"")\n        decoder_outputs = self.decoder_dense(decoder_outputs)\n\n        # Define the model that will turn\n        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n        self.model = Model([self.encoder_inputs, self.decoder_inputs], decoder_outputs)\n        self.model.compile(loss=\'mse\',\n                           metrics=[self.metric],\n                           optimizer=keras.optimizers.RMSprop(lr=self.lr))\n        return self.model\n\n    def _restore_model(self):\n        self.encoder_inputs = self.model.input[0]  # input_1\n        encoder_outputs, state_h_enc, state_c_enc = self.model.layers[2].output  # lstm_1\n        self.encoder_states = [state_h_enc, state_c_enc]\n\n        self.decoder_inputs = self.model.input[1]  # input_2\n        self.decoder_lstm = self.model.layers[3]\n\n        self.decoder_dense = self.model.layers[4]\n\n    def _build_inference(self, mc=False):\n        training = True if mc else None\n        # from our previous model - mapping encoder sequence to state vectors\n        encoder_model = Model(self.encoder_inputs, self.encoder_states)\n\n        # A modified version of the decoding stage that takes in predicted target inputs\n        # and encoded state vectors, returning predicted target outputs and decoder state vectors.\n        # We need to hang onto these state vectors to run the next step of the inference loop.\n        decoder_state_input_h = Input(shape=(self.latent_dim,))\n        decoder_state_input_c = Input(shape=(self.latent_dim,))\n        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n        decoder_outputs, state_h, state_c = self.decoder_lstm(self.decoder_inputs,\n                                                              training=training,\n                                                              initial_state=decoder_states_inputs)\n        decoder_states = [state_h, state_c]\n\n        decoder_outputs = self.decoder_dense(decoder_outputs)\n        decoder_model = Model([self.decoder_inputs] + decoder_states_inputs,\n                              [decoder_outputs] + decoder_states)\n        return encoder_model, decoder_model\n\n    def _decode_sequence(self, input_seq, mc=False):\n        encoder_model, decoder_model = self._build_inference(mc=mc)\n        # Encode the input as state vectors.\n        states_value = encoder_model.predict(input_seq)\n\n        # Generate empty target sequence of length 1.\n        target_seq = np.zeros((len(input_seq), 1, self.target_col_num))\n\n        # Populate the first target sequence with end of encoding series value\n        target_seq[:, 0] = input_seq[:, -1, :self.target_col_num]\n\n        # Sampling loop for a batch of sequences - we will fill decoded_seq with predictions\n        # (to simplify, here we assume a batch of size 1).\n\n        decoded_seq = np.zeros((len(input_seq), self.future_seq_len, self.target_col_num))\n\n        for i in range(self.future_seq_len):\n            output, h, c = decoder_model.predict([target_seq] + states_value)\n\n            decoded_seq[:, i] = output[:, 0]\n\n            # Update the target sequence (of length 1).\n            target_seq = np.zeros((len(input_seq), 1, self.target_col_num))\n            target_seq[:, 0] = output[:, 0]\n\n            # Update states\n            states_value = [h, c]\n\n        return decoded_seq\n\n    def _get_decoder_inputs(self, x, y):\n        """"""\n        lagged target series for teacher forcing\n        decoder_input data is one timestamp ahead of y\n        :param x: 3-d array in format of (sample_num, past_sequence_len, feature_num)\n        :param y: 3-d array in format of (sample_num, future_sequence_len, target_col_num)\n                  Need to expand dimension if y is a 2-d array with one target col\n        :return: 3-d array of decoder inputs\n        """"""\n        decoder_input_data = np.zeros(y.shape)\n        decoder_input_data[1:, ] = y[:-1, ]\n        decoder_input_data[0, 0] = x[-1, -1, :self.target_col_num]\n        decoder_input_data[0, 1:] = y[0, :-1]\n\n        return decoder_input_data\n\n    def _get_len(self, x, y):\n        self.past_seq_len = x.shape[1]\n        self.feature_num = x.shape[2]\n        # self.future_seq_len = y.shape[1]\n        self.target_col_num = y.shape[2]\n\n    def _expand_y(self, y):\n        """"""\n        expand dims for y.\n        :param y:\n        :return:\n        """"""\n        while len(y.shape) < 3:\n            y = np.expand_dims(y, axis=2)\n        return y\n\n    def _pre_processing(self, x, y, validation_data):\n        """"""\n        pre_process input data.\n        1. expand dims for y and val_y\n        2. get decoder inputs for train data\n        3. get decoder inputs for validation data\n        :param x: train_x\n        :param y: train_y\n        :param validation_data:\n        :return: network input\n        """"""\n        y = self._expand_y(y)\n        self._get_len(x, y)\n        decoder_input_data = self._get_decoder_inputs(x, y)\n        if validation_data is not None:\n            val_x, val_y = validation_data\n            val_y = self._expand_y(val_y)\n            val_decoder_input = self._get_decoder_inputs(val_x, val_y)\n            validation_data = ([val_x, val_decoder_input], val_y)\n        return x, y, decoder_input_data, validation_data\n\n    def fit_eval(self, x, y, validation_data=None, mc=False, verbose=0, **config):\n        """"""\n        fit for one iteration\n        :param x: 3-d array in format (no. of samples, past sequence length, 2+feature length),\n        in the last dimension, the 1st col is the time index (data type needs to be numpy datetime\n        type, e.g. ""datetime64""),\n        the 2nd col is the target value (data type should be numeric)\n        :param y: 2-d numpy array in format (no. of samples, future sequence length)\n        if future sequence length > 1,\n        or 1-d numpy array in format (no. of samples, ) if future sequence length = 1\n        :param validation_data: tuple in format (x_test,y_test), data used for validation.\n        If this is specified, validation result will be the optimization target for automl.\n        Otherwise, train metric will be the optimization target.\n        :param config: optimization hyper parameters\n        :return: the resulting metric\n        """"""\n        x, y, decoder_input_data, validation_data = self._pre_processing(x, y, validation_data)\n\n        # if model is not initialized, __build the model\n        if self.model is None:\n            self._build_train(mc=mc, **config)\n\n        # batch_size = config.get(\'batch_size\', 64)\n        # lr = self.lr\n        # name = ""seq2seq-batch_size-{}-epochs-{}-lr-{}-time-{}""\\\n        #     .format(batch_size, epochs, lr, time())\n        # tensorboard = TensorBoard(log_dir=""logs/"" + name)\n\n        hist = self.model.fit([x, decoder_input_data], y,\n                              validation_data=validation_data,\n                              batch_size=self.batch_size,\n                              epochs=config.get(""epochs"", 10),\n                              verbose=verbose,\n                              # callbacks=[tensorboard]\n                              )\n        # print(hist.history)\n\n        if validation_data is None:\n            # get train metrics\n            # results = self.model.evaluate(x, y)\n            result = hist.history.get(self.metric)[-1]\n        else:\n            result = hist.history.get(\'val_\' + str(self.metric))[-1]\n        return result\n\n    def evaluate(self, x, y, metric=[\'mse\']):\n        """"""\n        Evaluate on x, y\n        :param x: input\n        :param y: target\n        :param metric: a list of metrics in string format\n        :return: a list of metric evaluation results\n        """"""\n        y_pred = self.predict(x)\n        # y = np.squeeze(y, axis=2)\n        return [Evaluator.evaluate(m, y, y_pred) for m in metric]\n\n    def predict(self, x, mc=False):\n        """"""\n        Prediction on x.\n        :param x: input\n        :return: predicted y (expected dimension = 2)\n        """"""\n        y_pred = self._decode_sequence(x, mc=mc)\n        y_pred = np.squeeze(y_pred, axis=2)\n        return y_pred\n\n    def predict_with_uncertainty(self, x, n_iter=100):\n        result = np.zeros((n_iter,) + (x.shape[0], self.future_seq_len))\n\n        for i in range(n_iter):\n            result[i, :, :] = self.predict(x, mc=True)\n\n        prediction = result.mean(axis=0)\n        uncertainty = result.std(axis=0)\n        return prediction, uncertainty\n\n    def save(self, model_path, config_path):\n        """"""\n        save model to file.\n        :param model_path: the model file path to be saved to.\n        :param config_path: the config file path to be saved to.\n        :return:\n        """"""\n\n        self.model.save(model_path)\n\n        config_to_save = {""past_seq_len"": self.past_seq_len,\n                          ""feature_num"": self.feature_num,\n                          ""future_seq_len"": self.future_seq_len,\n                          ""target_col_num"": self.target_col_num,\n                          ""metric"": self.metric,\n                          ""latent_dim"": self.latent_dim,\n                          ""batch_size"": self.batch_size}\n        save_config(config_path, config_to_save)\n\n    def restore(self, model_path, **config):\n        """"""\n        restore model from file\n        :param model_path: the model file\n        :param config: the trial config\n        :return: the restored model\n        """"""\n\n        self.past_seq_len = config[""past_seq_len""]\n        self.feature_num = config[""feature_num""]\n        self.future_seq_len = config[""future_seq_len""]\n        self.target_col_num = config[""target_col_num""]\n        self.metric = config[""metric""]\n        self.latent_dim = config[""latent_dim""]\n        self.batch_size = config[""batch_size""]\n\n        self.model = keras.models.load_model(model_path)\n        self._restore_model()\n        # self.model.load_weights(file_path)\n\n    def _get_required_parameters(self):\n        return {\n            # \'input_shape_x\',\n            # \'input_shape_y\',\n            # \'out_units\'\n        }\n\n    def _get_optional_parameters(self):\n        return {\n            \'past_seq_len\'\n            \'latent_dim\'\n            \'dropout\',\n            \'metric\',\n            \'lr\',\n            \'epochs\',\n            \'batch_size\'\n        }\n'"
pyzoo/zoo/automl/model/Seq2Seq_pytorch.py,11,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom zoo.automl.feature.time_sequence import TimeSequenceFeatureTransformer\nfrom zoo.automl.model.abstract import BaseModel\nfrom zoo.automl.common.util import *\nfrom zoo.automl.common.metrics import Evaluator\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_num, dropout):\n        super().__init__()\n\n        self.hidden_dim = hidden_dim\n        self.layer_num = layer_num\n\n        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_num, dropout=dropout, batch_first=True)\n\n        for name, param in self.rnn.named_parameters():\n            if \'bias\' in name:\n                nn.init.constant_(param, 0.0)\n            elif \'weight_ih\' in name:\n                nn.init.xavier_normal_(param)\n            elif \'weight_hh\' in name:\n                nn.init.orthogonal_(param)\n\n    def forward(self, input_seq):\n        # input_seq = [batch_size, seq_len, feature_num]\n        outputs, (hidden, cell) = self.rnn(input_seq)\n        # outputs = [batch size, seq len, hidden dim]\n        # hidden = [batch size, layer num, hidden dim]\n        # cell = [batch size, layer num, hidden dim]\n\n        # outputs are always from the top hidden layer\n        return hidden, cell\n\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, hidden_dim, layer_num, dropout):\n        super().__init__()\n\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.layer_num = layer_num\n\n        self.rnn = nn.LSTM(output_dim, hidden_dim, layer_num, dropout=dropout, batch_first=True)\n\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n        for name, param in self.rnn.named_parameters():\n            if \'bias\' in name:\n                nn.init.constant_(param, 0.0)\n            elif \'weight_ih\' in name:\n                nn.init.xavier_normal_(param)\n            elif \'weight_hh\' in name:\n                nn.init.orthogonal_(param)\n\n    def forward(self, decoder_input, hidden, cell):\n        # input = [batch size]\n        # hidden = [batch size, layer num, hidden dim]\n        # cell = [batch size, layer num, hidden dim]\n\n        # input = decoder_input.view(-1, 1)\n        # decoder_input = [batch size, 1], since output_dim is 1\n        decoder_input = decoder_input.unsqueeze(1)\n        # decoder_input = [batch_size, 1, 1]\n\n        output, (hidden, cell) = self.rnn(decoder_input, (hidden, cell))\n\n        # output = [batch size, seq len, hidden dim]\n        # hidden = [batch size, layer num, hidden dim]\n        # cell = [batch size, layer num, hidden dim]\n\n        # seq len will always be 1 in the decoder, therefore:\n        # output = [batch size, 1, hidden dim]\n        # hidden = [batch size, layer num, hidden dim]\n        # cell = [batch size, layer num, hidden dim]\n        prediction = self.fc_out(output.squeeze())\n        # prediction = [batch size, output dim]\n\n        return prediction, hidden, cell\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, target_seq_len=1):\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        self.target_seq_len = target_seq_len\n\n    def forward(self, source, target=None):\n        # past_seq_len\n        batch_size = source.shape[0]\n\n        output_dim = self.decoder.output_dim\n\n        # tensor to store the predicted outputs\n        target_seq = torch.zeros(batch_size, self.target_seq_len, output_dim)\n\n        # last hidden state of the encoder is used as the initial hidden state of the decoder\n        hidden, cell = self.encoder(source)\n\n        # Populate the first target sequence with end of encoding series value\n        # decoder_input : [batch_size, output_dim]\n        decoder_input = source[:, -1, :output_dim]\n\n        for i in range(self.target_seq_len):\n            decoder_output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n            target_seq[:, i] = decoder_output\n            if target is None:\n                # in test mode\n                decoder_input = decoder_output\n            else:\n                decoder_input = target[:, i]\n        return target_seq\n\n\nclass Seq2SeqPytorch(BaseModel):\n\n    def __init__(self, check_optional_config=True, future_seq_len=1):\n        """"""\n        Constructor of Vanilla LSTM model\n        """"""\n        self.model = None\n        self.check_optional_config = check_optional_config\n        self.future_seq_len = future_seq_len\n        self.feature_num = None\n        self.output_dim = None\n        self.metric = None\n        self.batch_size = None\n        self.criterion = None\n        self.optimizer = None\n\n    def _get_configs(self, config):\n        super()._check_config(**config)\n        self.metric = config.get(\'metric\', \'mean_squared_error\')\n        self.batch_size = config.get(\'batch_size\', 32)\n        self.hidden_dim = config.get(\'hidden_dim\', 32)\n        self.layer_num = config.get(\'layer_num\', 2)\n        self.dropout = config.get(\'dropout\', 0.2)\n        self.lr = config.get(""lr"", 0.001)\n\n    def _load_data(self, input_data, batch_size):\n        x, y = input_data\n        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))\n        data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n        return data_loader\n\n    def _train_one_epoch(self, train_loader):\n        self.model.train()\n        train_losses = []\n        for input_seqs, target_seqs in train_loader:\n            self.model.zero_grad()\n\n            outputs = self.model(input_seqs, target_seqs)\n            loss = self.criterion(outputs, target_seqs)\n\n            # get gradients\n            loss.backward()\n\n            # update parameters\n            self.optimizer.step()\n\n            train_losses.append(loss.item())\n        return np.mean(train_losses)\n\n    def _val_one_epoch(self, val_loader):\n        self.model.eval()\n        val_losses = []\n        for val_input, val_target in val_loader:\n            val_out = self.model(val_input)\n            val_loss = self.criterion(val_out, val_target)\n            val_losses.append(val_loss.item())\n        return np.mean(val_losses)\n\n    def _test_one_epoch(self, test_loader, mc=False):\n        if not mc:\n            self.model.eval()\n        else:\n            self.model.train()\n        test_out_list = []\n        for test_input in test_loader:\n            # test_input is a list with one element\n            test_out = self.model(test_input[0])\n            test_out_list.append(test_out.detach().numpy())\n        predictions = np.concatenate(test_out_list)\n        y_pred = np.squeeze(predictions, axis=2)\n        return y_pred\n\n    def _print_model(self):\n        # print model and parameters\n        print(self.model)\n        print(len(list(self.model.parameters())))\n        for i in range(len(list(self.model.parameters()))):\n            print(list(self.model.parameters())[i].size())\n\n    def _expand_y(self, y):\n        """"""\n        expand dims for y.\n        :param y:\n        :return:\n        """"""\n        while len(y.shape) < 3:\n            y = np.expand_dims(y, axis=2)\n        return y\n\n    def _pre_processing(self, x, y, validation_data):\n        """"""\n        pre_process input data\n        1. expand dims for y and vay_y\n        2. get input lengths\n        :param x:\n        :param y:\n        :param validation_data:\n        :return:\n        """"""\n        y = self._expand_y(y)\n        self.feature_num = x.shape[2]\n        self.output_dim = y.shape[2]\n        if validation_data is not None:\n            val_x, val_y = validation_data\n            val_y = self._expand_y(val_y)\n        return x, y, (val_x, val_y)\n\n    def fit_eval(self, x, y, validation_data=None, mc=False, verbose=0, **config):\n        self._get_configs(config)\n        x, y, validation_data = self._pre_processing(x, y, validation_data)\n        # get data\n        train_loader = self._load_data((x, y), self.batch_size)\n        if validation_data:\n            val_loader = self._load_data(validation_data, self.batch_size)\n\n        encoder = Encoder(self.feature_num, self.hidden_dim, self.layer_num, self.dropout)\n        decoder = Decoder(self.output_dim, self.hidden_dim, self.layer_num, self.dropout)\n\n        self.model = Seq2Seq(encoder, decoder, target_seq_len=self.future_seq_len)\n        print(encoder)\n        print(decoder)\n        # self._print_model()\n\n        self.criterion = nn.MSELoss()\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n\n        epochs = config.get(\'epochs\', 20)\n        assert (epochs > 0)\n        val_epoch = 1\n        for i in range(epochs):\n            train_loss = self._train_one_epoch(train_loader)\n            if verbose == 1:\n                print(""Epoch : {}/{}..."".format(i, epochs),\n                      ""Loss: {:.6f}..."".format(train_loss),\n                      )\n            if i % val_epoch == 0:\n                if validation_data:\n                    val_loss = self._val_one_epoch(val_loader)\n                if verbose == 1:\n                    print(""Val loss: {:.6f}..."".format(val_loss))\n        if validation_data:\n            result = val_loss\n        else:\n            result = train_loss\n        return result\n\n    def evaluate(self, x, y, metric=[\'mse\']):\n        """"""\n        Evaluate on x, y\n        :param x: input\n        :param y: target\n        :param metric: a list of metrics in string format\n        :return: a list of metric evaluation results\n        """"""\n        y_pred = self.predict(x)\n        assert y_pred.shape == y.shape\n        return [Evaluator.evaluate(m, y, y_pred) for m in metric]\n\n    def predict(self, x, mc=False):\n        """"""\n        Prediction on x.\n        :param x: input\n        :return: predicted y\n        """"""\n        test_x = TensorDataset(torch.from_numpy(x))\n        test_loader = DataLoader(test_x, shuffle=False, batch_size=self.batch_size)\n        y_pred = self._test_one_epoch(test_loader, mc=mc)\n        return y_pred\n\n    def predict_with_uncertainty(self, x, n_iter=100):\n        test_x = TensorDataset(torch.from_numpy(x))\n        test_loader = DataLoader(test_x, shuffle=False, batch_size=self.batch_size)\n        result = np.zeros((n_iter,) + (x.shape[0], self.future_seq_len))\n\n        for i in range(n_iter):\n            result[i, :, :] = self._test_one_epoch(test_loader, mc=True)\n\n        prediction = result.mean(axis=0)\n        uncertainty = result.std(axis=0)\n        return prediction, uncertainty\n\n    def save(self, model_path, config_path):\n        """"""\n        save model to file.\n        :param model_path: the model file.\n        :param config_path: the config file\n        :return:\n        """"""\n        torch.save(self.model.state_dict(), model_path)\n        # os.rename(""vanilla_lstm_tmp.h5"", model_path)\n\n        config_to_save = {\n            ""future_seq_len"": self.future_seq_len,\n            ""feature_num"": self.feature_num,\n            ""metric"": self.metric,\n            ""batch_size"": self.batch_size,\n            ""hidden_dim"": self.hidden_dim,\n            ""dropout"": self.dropout,\n            ""layer_num"": self.layer_num,\n            ""output_dim"": self.output_dim,\n            # ""lr"": self.lr\n        }\n        save_config(config_path, config_to_save)\n\n    def restore(self, model_path, **config):\n        """"""\n        restore model from file\n        :param model_path: the model file\n        :param config: the trial config\n        :return: the restored model\n        """"""\n        # self.model = None\n        # self._build(**config)\n        # self.model = keras.models.load_model(model_path)\n        # self.model.load_weights(file_path)\n\n        self.future_seq_len = config[""future_seq_len""]\n        self.feature_num = config[""feature_num""]\n        self.output_dim = config[""output_dim""]\n        # for continuous training\n        saved_configs = [""future_seq_len"", ""metric"", ""batch_size"", ""hidden_dim"",\n                         ""dropout"", ""layer_num"", ""output_dim""]\n        assert all([c in config for c in saved_configs])\n        self._get_configs(config)\n\n        encoder = Encoder(self.feature_num, self.hidden_dim, self.layer_num, self.dropout)\n        decoder = Decoder(self.output_dim, self.hidden_dim, self.layer_num, self.dropout)\n\n        self.model = Seq2Seq(encoder, decoder, target_seq_len=self.future_seq_len)\n        self.model.load_state_dict(torch.load(model_path))\n        self.model.eval()\n\n    def _get_required_parameters(self):\n        return {\n            # \'input_shape_x\',\n            # \'input_shape_y\',\n            # \'out_units\'\n        }\n\n    def _get_optional_parameters(self):\n        return {\n            \'hidden_dim\',\n            \'layer_num\',\n            \'hidden_dim\',\n            \'dropout\',\n            \'lr\',\n            \'epochs\',\n            \'batch_size\'\n        }\n'"
pyzoo/zoo/automl/model/VanillaLSTM.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Dropout\nimport tensorflow.keras as keras\nimport os\n\nfrom zoo.automl.model.abstract import BaseModel\nfrom zoo.automl.common.util import *\nfrom zoo.automl.common.metrics import Evaluator\n\n\nclass VanillaLSTM(BaseModel):\n\n    def __init__(self, check_optional_config=False, future_seq_len=1):\n        """"""\n        Constructor of Vanilla LSTM model\n        """"""\n        self.model = None\n        self.check_optional_config = check_optional_config\n        self.future_seq_len = future_seq_len\n        self.feature_num = None\n        self.metric = None\n        self.batch_size = None\n\n    def _get_dropout(self, input_tensor, p=0.5, mc=False):\n        if mc:\n            return Dropout(p)(input_tensor, training=True)\n        else:\n            return Dropout(p)(input_tensor)\n\n    def _build(self, mc=False, **config):\n        """"""\n        build vanilla LSTM model\n        :param config: model hyper parameters\n        :return: self\n        """"""\n        super()._check_config(**config)\n        self.metric = config.get(\'metric\', \'mean_squared_error\')\n        self.batch_size = config.get(\'batch_size\', 1024)\n        self.feature_num = config[""feature_num""]\n\n        inp = Input(shape=(None, self.feature_num))\n        lstm_1 = LSTM(units=config.get(\'lstm_1_units\', 20),\n                      return_sequences=True)(inp)\n        dropout_1 = self._get_dropout(lstm_1,\n                                      p=config.get(\'dropout_1\', 0.2),\n                                      mc=mc)\n        lstm_2 = LSTM(units=config.get(\'lstm_2_units\', 10),\n                      return_sequences=False)(dropout_1)\n        dropout_2 = self._get_dropout(lstm_2,\n                                      p=config.get(\'dropout_2\', 0.2),\n                                      mc=mc)\n        out = Dense(self.future_seq_len)(dropout_2)\n        self.model = Model(inputs=inp, outputs=out)\n\n        # self.model = Sequential()\n        # self.model.add(LSTM(\n        #     # input_shape=(config.get(\'input_shape_x\', 20),\n        #     #             config.get(\'input_shape_y\', 20)),\n        #     units=config.get(\'lstm_1_units\', 20),\n        #     return_sequences=True))\n        # self.model.add(Dropout(config.get(\'dropout_1\', 0.2)))\n        #\n        # self.model.add(LSTM(\n        #     units=config.get(\'lstm_2_units\', 10),\n        #     return_sequences=False))\n        # self.model.add(Dropout(config.get(\'dropout_2\', 0.2)))\n\n        # self.model.add(Dense(self.future_seq_len))\n        self.model.compile(loss=\'mse\',\n                           metrics=[self.metric],\n                           optimizer=keras.optimizers.RMSprop(lr=config.get(\'lr\', 0.001)))\n        return self.model\n\n    def fit_eval(self, x, y, validation_data=None, mc=False, verbose=0, **config):\n        """"""\n        fit for one iteration\n        :param x: 3-d array in format (no. of samples, past sequence length, 2+feature length),\n        in the last dimension, the 1st col is the time index (data type needs to be numpy datetime\n        type, e.g. ""datetime64""),\n        the 2nd col is the target value (data type should be numeric)\n        :param y: 2-d numpy array in format (no. of samples, future sequence length)\n        if future sequence length > 1, or 1-d numpy array in format (no. of samples, )\n        if future sequence length = 1\n        :param validation_data: tuple in format (x_test,y_test), data used for validation.\n        If this is specified, validation result will be the optimization target for automl.\n        Otherwise, train metric will be the optimization target.\n        :param config: optimization hyper parameters\n        :return: the resulting metric\n        """"""\n        config.update({""feature_num"": x.shape[2]})\n        # if model is not initialized, __build the model\n        if self.model is None:\n            self._build(mc=mc, **config)\n\n        hist = self.model.fit(x, y,\n                              validation_data=validation_data,\n                              batch_size=self.batch_size,\n                              epochs=config.get(""epochs"", 10),\n                              verbose=verbose\n                              )\n        # print(hist.history)\n\n        if validation_data is None:\n            # get train metrics\n            # results = self.model.evaluate(x, y)\n            result = hist.history.get(self.metric)[0]\n        else:\n            result = hist.history.get(\'val_\' + str(self.metric))[0]\n        return result\n\n    def evaluate(self, x, y, metric=[\'mse\']):\n        """"""\n        Evaluate on x, y\n        :param x: input\n        :param y: target\n        :param metric: a list of metrics in string format\n        :return: a list of metric evaluation results\n        """"""\n        y_pred = self.predict(x)\n        return [Evaluator.evaluate(m, y, y_pred) for m in metric]\n\n    def predict(self, x, mc=False):\n        """"""\n        Prediction on x.\n        :param x: input\n        :return: predicted y\n        """"""\n        return self.model.predict(x)\n\n    def predict_with_uncertainty(self, x, n_iter=100):\n        result = np.zeros((n_iter,) + (x.shape[0], self.future_seq_len))\n\n        for i in range(n_iter):\n            result[i, :, :] = self.predict(x)\n\n        prediction = result.mean(axis=0)\n        uncertainty = result.std(axis=0)\n        return prediction, uncertainty\n\n    def save(self, model_path, config_path):\n        """"""\n        save model to file.\n        :param model_path: the model file.\n        :param config_path: the config file\n        :return:\n        """"""\n        self.model.save(model_path)\n        # os.rename(""vanilla_lstm_tmp.h5"", model_path)\n\n        config_to_save = {\n            # ""future_seq_len"": self.future_seq_len,\n            ""metric"": self.metric,\n            ""batch_size"": self.batch_size\n        }\n        save_config(config_path, config_to_save)\n\n    def restore(self, model_path, **config):\n        """"""\n        restore model from file\n        :param model_path: the model file\n        :param config: the trial config\n        :return: the restored model\n        """"""\n        # self.model = None\n        # self._build(**config)\n        self.model = keras.models.load_model(model_path)\n        # self.model.load_weights(file_path)\n\n        # self.future_seq_len = config[""future_seq_len""]\n        # for continuous training\n        self.metric = config[""metric""]\n        self.batch_size = config[""batch_size""]\n\n    def _get_required_parameters(self):\n        return {\n            ""feature_num""\n        }\n\n    def _get_optional_parameters(self):\n        return {\n            \'lstm_1_units\',\n            \'dropout_1\',\n            \'lstm_2_units\',\n            \'dropout_2\',\n            \'metric\',\n            \'lr\',\n            \'epochs\',\n            \'batch_size\'\n        }\n'"
pyzoo/zoo/automl/model/VanillaLSTM_pytorch.py,12,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom zoo.automl.feature.time_sequence import TimeSequenceFeatureTransformer\nfrom zoo.automl.model.abstract import BaseModel\nfrom zoo.automl.common.util import *\nfrom zoo.automl.common.metrics import Evaluator\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_num, dropout, output_dim):\n        super(LSTMModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.layer_num = layer_num\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_num, dropout=dropout, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n        for name, param in self.lstm.named_parameters():\n            if \'bias\' in name:\n                nn.init.constant_(param, 0.0)\n            elif \'weight_ih\' in name:\n                nn.init.xavier_normal_(param)\n            elif \'weight_hh\' in name:\n                nn.init.orthogonal_(param)\n\n    def forward(self, input_seq):\n        # init hidden\n        # input_seq: (batch_size, seq_len, feature_num)\n        # h0 = torch.zeros(self.layer_num, input_seq.size[0], self.hidden_dim)\n        # c0 = torch.zeros(self.layer_num, input_seq.size[0], self.hidden_dim)\n        lstm_out, hidden = self.lstm(input_seq)\n        # lstm_out: (batch_size, hidden_dim\n        # reshaping the outputs to feed in fully connected layer\n        # out = lstm_out[-1].contiguous().view(-1, self.hidden_dim)\n        # out = self.linear(out, len(input_seq), -1)\n        out = self.fc(lstm_out[:, -1, :])\n        return out\n\n\nclass VanillaLSTMPytorch(BaseModel):\n\n    def __init__(self, check_optional_config=True, future_seq_len=1):\n        """"""\n        Constructor of Vanilla LSTM model\n        """"""\n        self.model = None\n        self.check_optional_config = check_optional_config\n        self.future_seq_len = future_seq_len\n        self.feature_num = None\n        self.output_dim = 1\n        self.metric = None\n        self.criterion = None\n        self.optimizer = None\n\n    def _get_configs(self, config):\n        super()._check_config(**config)\n        self.metric = config.get(\'metric\', \'mean_squared_error\')\n        self.batch_size = config.get(\'batch_size\', 1024)\n        self.hidden_dim = config.get(\'hidden_dim\', 32)\n        self.layer_num = config.get(\'layer_num\', 2)\n        self.dropout = config.get(\'dropout\', 0.2)\n        self.lr = config.get(""lr"", 0.001)\n\n    def _load_data(self, input_data, batch_size):\n        x, y = input_data\n        data = TensorDataset(torch.from_numpy(x), torch.from_numpy(y))\n        data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n        return data_loader\n\n    def _train_one_epoch(self, train_loader):\n        self.model.train()\n        train_losses = []\n        for input_seqs, target_seqs in train_loader:\n            self.model.zero_grad()\n\n            outputs = self.model(input_seqs)\n            loss = self.criterion(outputs, target_seqs)\n\n            # get gradients\n            loss.backward()\n\n            # update parameters\n            self.optimizer.step()\n\n            train_losses.append(loss.item())\n        return np.mean(train_losses)\n\n    def _val_one_epoch(self, val_loader):\n        self.model.eval()\n        val_losses = []\n        for val_input, val_target in val_loader:\n            val_out = self.model(val_input)\n            val_loss = self.criterion(val_out, val_target)\n            val_losses.append(val_loss.item())\n        return np.mean(val_losses)\n\n    def _test_one_epoch(self, test_loader, mc=False):\n        if not mc:\n            self.model.eval()\n        else:\n            self.model.train()\n        test_out_list = []\n        for test_input in test_loader:\n            test_out = self.model(test_input[0])\n            test_out_list.append(test_out.detach().numpy())\n        return np.concatenate(test_out_list)\n\n    def _print_model(self):\n        # print model and parameters\n        print(self.model)\n        print(len(list(self.model.parameters())))\n        for i in range(len(list(self.model.parameters()))):\n            print(list(self.model.parameters())[i].size())\n\n    def fit_eval(self, x, y, validation_data, mc=False, verbose=0, **config):\n        self._get_configs(config)\n        # get data\n        train_loader = self._load_data((x, y), self.batch_size)\n        if validation_data:\n            val_loader = self._load_data(validation_data, self.batch_size)\n\n        self.feature_num = x.shape[2]\n        self.output_dim = 1\n        self.model = LSTMModel(self.feature_num, self.hidden_dim, self.layer_num, self.dropout,\n                               self.output_dim)\n        # self._print_model()\n\n        self.criterion = nn.MSELoss()\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n\n        epochs = config.get(\'epochs\', 20)\n        assert(epochs > 0)\n        val_epoch = 1\n        for i in range(epochs):\n            train_loss = self._train_one_epoch(train_loader)\n            if verbose == 1:\n                print(""Epoch : {}/{}..."".format(i, epochs),\n                      ""Loss: {:.6f}..."".format(train_loss),\n                      )\n            if i % val_epoch == 0:\n                if validation_data:\n                    val_loss = self._val_one_epoch(val_loader)\n                if verbose == 1:\n                    print(""Val loss: {:.6f}..."".format(val_loss))\n        if validation_data:\n            result = val_loss\n        else:\n            result = train_loss\n        return result\n\n    def evaluate(self, x, y, metric=[\'mse\']):\n        """"""\n        Evaluate on x, y\n        :param x: input\n        :param y: target\n        :param metric: a list of metrics in string format\n        :return: a list of metric evaluation results\n        """"""\n        y_pred = self.predict(x)\n        assert y_pred.shape == y.shape\n        return [Evaluator.evaluate(m, y, y_pred) for m in metric]\n\n    def predict(self, x, mc=False):\n        """"""\n        Prediction on x.\n        :param x: input\n        :return: predicted y\n        """"""\n        test_x = TensorDataset(torch.from_numpy(x))\n        test_loader = DataLoader(test_x, shuffle=False, batch_size=self.batch_size)\n        predictions = self._test_one_epoch(test_loader, mc=mc)\n        return predictions\n\n    def predict_with_uncertainty(self, x, n_iter=100):\n        test_x = TensorDataset(torch.from_numpy(x))\n        test_loader = DataLoader(test_x, shuffle=False, batch_size=self.batch_size)\n        result = np.zeros((n_iter,) + (x.shape[0], self.future_seq_len))\n\n        for i in range(n_iter):\n            result[i, :, :] = self._test_one_epoch(test_loader, mc=True)\n\n        prediction = result.mean(axis=0)\n        uncertainty = result.std(axis=0)\n        return prediction, uncertainty\n\n    def save(self, model_path, config_path):\n        """"""\n        save model to file.\n        :param model_path: the model file.\n        :param config_path: the config file\n        :return:\n        """"""\n        torch.save(self.model.state_dict(), model_path)\n        # os.rename(""vanilla_lstm_tmp.h5"", model_path)\n\n        config_to_save = {\n            ""future_seq_len"": self.future_seq_len,\n            ""feature_num"": self.feature_num,\n            ""metric"": self.metric,\n            ""batch_size"": self.batch_size,\n            ""hidden_dim"": self.hidden_dim,\n            ""dropout"": self.dropout,\n            ""layer_num"": self.layer_num,\n            ""output_dim"": self.output_dim,\n            # ""lr"": self.lr\n        }\n        save_config(config_path, config_to_save)\n\n    def restore(self, model_path, **config):\n        """"""\n        restore model from file\n        :param model_path: the model file\n        :param config: the trial config\n        :return: the restored model\n        """"""\n        # self.model = None\n        # self._build(**config)\n        # self.model = keras.models.load_model(model_path)\n        # self.model.load_weights(file_path)\n\n        self.future_seq_len = config[""future_seq_len""]\n        self.feature_num = config[""feature_num""]\n        self.output_dim = config[""output_dim""]\n        # for continuous training\n        saved_configs = [""future_seq_len"", ""metric"", ""batch_size"", ""hidden_dim"",\n                         ""dropout"", ""layer_num"", ""output_dim""]\n        assert all([c in config for c in saved_configs])\n        self._get_configs(config)\n\n        self.model = LSTMModel(self.feature_num, self.hidden_dim, self.layer_num, self.dropout,\n                               self.output_dim)\n        self.model.load_state_dict(torch.load(model_path))\n        self.model.eval()\n\n    def _get_required_parameters(self):\n        return {\n            # \'input_shape_x\',\n            # \'input_shape_y\',\n            # \'out_units\'\n        }\n\n    def _get_optional_parameters(self):\n        return {\n            \'hidden_dim\',\n            \'layer_num\',\n            \'hidden_dim\',\n            \'dropout\',\n            \'lr\',\n            \'epochs\',\n            \'batch_size\'\n        }\n'"
pyzoo/zoo/automl/model/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .abstract import *\nfrom .VanillaLSTM import *\nfrom .Seq2Seq import *\nfrom .time_sequence import *\n'"
pyzoo/zoo/automl/model/abstract.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom abc import ABC, abstractmethod\n\n\nclass BaseModel(ABC):\n    """"""\n    base model for automl tuning\n    """"""\n\n    check_optional_config = False\n    future_seq_len = None\n\n    @abstractmethod\n    def fit_eval(self, x, y, validation_data=None, mc=False, verbose=0, **config):\n        """"""\n        optimize and evaluate for one iteration for tuning\n        :param config: tunable parameters for optimization\n        :return:\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def evaluate(self, x, y, metric=None):\n        """"""\n        Evaluate the model\n        :param x: input\n        :param y: target\n        :param metric:\n        :return: a list of metric evaluation results\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, x, mc=False):\n        """"""\n        Prediction.\n        :param x: input\n        :return: result\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def save(self, model_path, config_path):\n        """"""\n        save model to file.\n        :param model_path: the model file path to be saved to.\n        :param config_path: the config file path to be saved to.\n        :return:\n        """"""\n        pass\n\n    @abstractmethod\n    def restore(self, model_path, **config):\n        """"""\n        restore model from model file and config.\n        :param model_path: the model file\n        :param config: the config\n        :return: the restored model\n        """"""\n        pass\n\n    @abstractmethod\n    def _get_required_parameters(self):\n        """"""\n        :return: required parameters to be set into config\n        """"""\n        return set()\n\n    @abstractmethod\n    def _get_optional_parameters(self):\n        """"""\n        :return: optional parameters to be set into config\n        """"""\n        return set()\n\n    def _check_config(self, **config):\n        """"""\n        Do necessary checking for config\n        :param config:\n        :return:\n        """"""\n        config_parameters = set(config.keys())\n        if not config_parameters.issuperset(self._get_required_parameters()):\n            raise ValueError(""Missing required parameters in configuration. "" +\n                             ""Required parameters are: "" + str(self._get_required_parameters()))\n        if self.check_optional_config and \\\n                not config_parameters.issuperset(self._get_optional_parameters()):\n            raise ValueError(""Missing optional parameters in configuration. "" +\n                             ""Optional parameters are: "" + str(self._get_optional_parameters()))\n        return True\n'"
pyzoo/zoo/automl/model/time_sequence.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom .abstract import BaseModel\nfrom .VanillaLSTM import VanillaLSTM\nfrom .Seq2Seq import LSTMSeq2Seq\nfrom .MTNet_keras import MTNetKeras\nfrom zoo.automl.common.util import *\n\nMODEL_MAP = {""LSTM"": VanillaLSTM,\n             ""Seq2seq"": LSTMSeq2Seq,\n             ""MTNet"": MTNetKeras,\n             }\n\n\nclass TimeSequenceModel(BaseModel):\n    """"""\n    Time Sequence Model is used to do model selection.\n    """"""\n    def __init__(self, check_optional_config=False, future_seq_len=None):\n        """"""\n        Contructor of time sequence model\n        :param check_optional_config:\n        :param future_seq_len:\n        """"""\n        self.check_optional_config = check_optional_config\n        self.future_seq_len = future_seq_len\n        self.model = None\n        self.selected_model = None\n\n    #     if future_seq_len:\n    #         self._model_selection(future_seq_len, check_optional_config)\n    #\n    # def _model_selection(self, future_seq_len, check_optional_config=False, verbose=1):\n    #     if future_seq_len == 1:\n    #         self.model = VanillaLSTM(check_optional_config=check_optional_config,\n    #                                  future_seq_len=future_seq_len)\n    #         if verbose == 1:\n    #             print(""Model selection: Vanilla LSTM model is selected."")\n    #     else:\n    #         self.model = LSTMSeq2Seq(check_optional_config=check_optional_config,\n    #                                  future_seq_len=future_seq_len)\n    #         if verbose == 1:\n    #             print(""Model selection: LSTM Seq2Seq model is selected."")\n\n    def fit_eval(self, x, y, validation_data=None, mc=False, metric=""mse"", verbose=0, **config):\n        """"""\n        fit for one iteration\n        :param x: 3-d array in format (no. of samples, past sequence length, 2+feature length),\n        in the last dimension, the 1st col is the time index (data type needs to be numpy datetime\n        type, e.g. ""datetime64""),\n        the 2nd col is the target value (data type should be numeric)\n        :param y: 2-d numpy array in format (no. of samples, future sequence length)\n        if future sequence length > 1, or 1-d numpy array in format (no. of samples, )\n        if future sequence length = 1\n        :param validation_data: tuple in format (x_test,y_test), data used for validation.\n        If this is specified, validation result will be the optimization target for automl.\n        Otherwise, train metric will be the optimization target.\n        :param metric: the way to measure the performance of model\n        :param config: optimization hyper parameters\n        :return: the resulting metric\n        """"""\n        if not self.model:\n            self._sel_model(config, verbose=1)\n\n        return self.model.fit_eval(x, y,\n                                   validation_data=validation_data,\n                                   mc=mc,\n                                   verbose=verbose,\n                                   **config)\n\n    def _sel_model(self, config, verbose=0):\n        self.selected_model = config.get(""model"", ""LSTM"")\n        self.model = MODEL_MAP[self.selected_model](\n            check_optional_config=self.check_optional_config,\n            future_seq_len=self.future_seq_len)\n        if verbose != 0:\n            print(self.selected_model, ""is selected."")\n\n    def evaluate(self, x, y, metric=[\'mse\']):\n        """"""\n        Evaluate on x, y\n        :param x: input\n        :param y: target\n        :param metric: a list of metrics in string format\n        :return: a list of metric evaluation results\n        """"""\n        return self.model.evaluate(x, y, metric)\n\n    def predict(self, x, mc=False):\n        """"""\n        Prediction on x.\n        :param x: input\n        :return: predicted y\n        """"""\n        return self.model.predict(x)\n\n    def predict_with_uncertainty(self, x, n_iter=100):\n        return self.model.predict_with_uncertainty(x, n_iter)\n\n    def save(self, model_path, config_path):\n        """"""\n        save model to file.\n        :param model_path: the model file.\n        :param config_path: the config file\n        :return:\n        """"""\n        model_config = {""future_seq_len"": self.future_seq_len,\n                        ""model"": self.selected_model}\n        save_config(config_path, model_config)\n        self.model.save(model_path, config_path)\n\n    def restore(self, model_path, **config):\n        assert ""future_seq_len"" in config\n        assert ""model"" in config\n        self.future_seq_len = config[""future_seq_len""]\n        self._sel_model(config=config, verbose=0)\n        # self._model_selection(future_seq_len=config[""future_seq_len""], verbose=0)\n        self.model.restore(model_path, **config)\n\n    def _get_required_parameters(self):\n        return self.model._get_required_parameters()\n\n    def _get_optional_parameters(self):\n        return self.model._get_optional_parameters()\n'"
pyzoo/zoo/automl/pipeline/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/automl/pipeline/abstract.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom abc import ABC, abstractmethod\n\n\nclass Pipeline(ABC):\n    """"""\n    The pipeline object which is used to store the series of transformation of features and model\n    """"""\n\n    @abstractmethod\n    def evaluate(self, input_df, metric=None):\n        """"""\n        evaluate the pipeline\n        :param input_df: input data frame\n        :param metric: the evaluation metric\n        :return:\n        """"""\n        pass\n\n    @abstractmethod\n    def predict(self, input_df):\n        """"""\n        predict using the pipeline\n        :param input_df: input data frame\n        :return: the prediction result\n        """"""\n        pass\n\n    @abstractmethod\n    def save(self, file):\n        """"""\n        save the pipeline to a file\n        :param file: the pipeline file\n        :return: a pipeline object\n        """"""\n        pass\n'"
pyzoo/zoo/automl/pipeline/time_sequence.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport tempfile\nimport time\n\nfrom zoo.automl.common.metrics import Evaluator\nfrom zoo.automl.pipeline.abstract import Pipeline\nfrom zoo.automl.common.util import *\nfrom zoo.automl.feature.time_sequence import TimeSequenceFeatureTransformer\nfrom zoo.automl.model import TimeSequenceModel\nfrom zoo.automl.common.parameters import *\n\n\nclass TimeSequencePipeline(Pipeline):\n\n    def __init__(self, feature_transformers=None, model=None, config=None, name=None):\n        """"""\n        initialize a pipeline\n        :param model: the internal model\n        :param feature_transformers: the feature transformers\n        """"""\n        self.feature_transformers = feature_transformers\n        self.model = model\n        self.config = config\n        self.name = name\n        self.time = time.strftime(""%Y%m%d-%H%M%S"")\n\n    def describe(self):\n        init_info = [\'future_seq_len\', \'dt_col\', \'target_col\', \'extra_features_col\', \'drop_missing\']\n        print(""**** Initialization info ****"")\n        for info in init_info:\n            print(info + "":"", self.config[info])\n        print("""")\n\n    def fit(self, input_df, validation_df=None, mc=False, epoch_num=20):\n        x, y = self.feature_transformers.transform(input_df, is_train=True)\n        if validation_df is not None and not validation_df.empty:\n            validation_data = self.feature_transformers.transform(validation_df)\n        else:\n            validation_data = None\n        new_config = {\'epochs\': epoch_num}\n        self.model.fit_eval(x, y, validation_data, mc=mc, verbose=1, **new_config)\n        print(\'Fit done!\')\n\n    def _is_val_df_valid(self, validation_df):\n        df_not_empty = isinstance(validation_df, pd.DataFrame) and not validation_df.empty\n        df_list_not_empty = isinstance(validation_df, list) \\\n            and validation_df and not all([d.empty for d in validation_df])\n        if validation_df is not None and (df_not_empty or df_list_not_empty):\n            return True\n        else:\n            return False\n\n    def _check_configs(self):\n        required_configs = {\'future_seq_len\'}\n        if not self.config.keys() & required_configs:\n            raise ValueError(""Missing required parameters in configuration. "" +\n                             ""Required parameters are: "" + str(required_configs))\n        default_config = {\'dt_col\': \'datetime\', \'target_col\': \'value\', \'extra_features_col\': None,\n                          \'drop_missing\': True, \'past_seq_len\': 2, \'batch_size\': 64, \'lr\': 0.001,\n                          \'dropout\': 0.2, \'epochs\': 10, \'metric\': \'mse\'}\n        for config, value in default_config.items():\n            if config not in self.config:\n                print(\'Config: \\\'{}\\\' is not specified. \'\n                      \'A default value of {} will be used.\'.format(config, value))\n\n    def get_default_configs(self):\n        default_configs = {\'dt_col\': \'datetime\',\n                           \'target_col\': \'value\',\n                           \'extra_features_col\': None,\n                           \'drop_missing\': True,\n                           \'future_seq_len\': 1,\n                           \'past_seq_len\': 2,\n                           \'batch_size\': 64,\n                           \'lr\': 0.001,\n                           \'dropout\': 0.2,\n                           \'epochs\': 10,\n                           \'metric\': \'mean_squared_error\'}\n        print(""**** default config: ****"")\n        for config in default_configs:\n            print(config + "":"", default_configs[config])\n        print(""You can change any fields in the default configs by passing into ""\n              ""fit_with_fixed_configs(). Otherwise, the default values will be used."")\n        return default_configs\n\n    def fit_with_fixed_configs(self, input_df, validation_df=None, mc=False, **user_configs):\n        """"""\n        Fit pipeline with fixed configs. The model will be trained from initialization\n        with the hyper-parameter specified in configs. The configs contain both identity configs\n        (Eg. ""future_seq_len"", ""dt_col"", ""target_col"", ""metric"") and automl tunable configs\n        (Eg. ""past_seq_len"", ""batch_size"").\n        We recommend calling get_default_configs to see the name and default values of configs you\n        you can specify.\n        :param input_df: one data frame or a list of data frames\n        :param validation_df: one data frame or a list of data frames\n        :param user_configs: you can overwrite or add more configs with user_configs. Eg. ""epochs""\n        :return:\n        """"""\n        # self._check_configs()\n        if self.config is None:\n            self.config = self.get_default_configs()\n        if user_configs is not None:\n            self.config.update(user_configs)\n        ft_id_config_set = {\'future_seq_len\', \'dt_col\', \'target_col\',\n                            \'extra_features_col\', \'drop_missing\'}\n        ft_id_configs = {a: self.config[a] for a in ft_id_config_set}\n        self.feature_transformers = TimeSequenceFeatureTransformer(**ft_id_configs)\n        model_id_config_set = {\'future_seq_len\'}\n        ft_id_configs = {a: self.config[a] for a in model_id_config_set}\n        self.model = TimeSequenceModel(check_optional_config=False, **ft_id_configs)\n        all_available_features = self.feature_transformers.get_feature_list(input_df)\n        self.config.update({""selected_features"": all_available_features})\n        (x_train, y_train) = self.feature_transformers.fit_transform(input_df, **self.config)\n        if self._is_val_df_valid(validation_df):\n            validation_data = self.feature_transformers.transform(validation_df)\n        else:\n            validation_data = None\n\n        self.model.fit_eval(x_train, y_train,\n                            validation_data=validation_data,\n                            mc=mc,\n                            verbose=1, **self.config)\n\n    def evaluate(self,\n                 input_df,\n                 metrics=[""mse""],\n                 multioutput=\'raw_values\'\n                 ):\n        """"""\n        evaluate the pipeline\n        :param input_df:\n        :param metrics: subset of [\'mean_squared_error\', \'r_square\', \'sMAPE\']\n        :param multioutput: string in [\'raw_values\', \'uniform_average\']\n                \'raw_values\' :\n                    Returns a full set of errors in case of multioutput input.\n                \'uniform_average\' :\n                    Errors of all outputs are averaged with uniform weight.\n        :return:\n        """"""\n        if isinstance(metrics, str):\n            metrics = [metrics]\n        # if not isinstance(metrics, list):\n        #    raise ValueError(""Expected metrics to be a list!"")\n\n        x, y = self.feature_transformers.transform(input_df, is_train=True)\n        y_pred = self.model.predict(x)\n        if y_pred.shape[1] == 1:\n            multioutput = \'uniform_average\'\n        y_unscale, y_pred_unscale = self.feature_transformers.post_processing(input_df,\n                                                                              y_pred,\n                                                                              is_train=True)\n\n        return [Evaluator.evaluate(m, y_unscale, y_pred_unscale, multioutput=multioutput)\n                for m in metrics]\n\n    def predict(self, input_df):\n        """"""\n        predict test data with the pipeline fitted\n        :param input_df:\n        :return:\n        """"""\n        x, _ = self.feature_transformers.transform(input_df, is_train=False)\n        y_pred = self.model.predict(x)\n        y_output = self.feature_transformers.post_processing(input_df, y_pred, is_train=False)\n        return y_output\n\n    def predict_with_uncertainty(self, input_df, n_iter=100):\n        x, _ = self.feature_transformers.transform(input_df, is_train=False)\n        y_pred, y_pred_uncertainty = self.model.predict_with_uncertainty(x=x, n_iter=n_iter)\n        y_output = self.feature_transformers.post_processing(input_df, y_pred, is_train=False)\n        y_uncertainty = self.feature_transformers.unscale_uncertainty(y_pred_uncertainty)\n        return y_output, y_uncertainty\n\n    def save(self, ppl_file=None):\n        """"""\n        save pipeline to file, contains feature transformer, model, trial config.\n        :param ppl_file:\n        :return:\n        """"""\n        ppl_file = ppl_file or os.path.join(DEFAULT_PPL_DIR, ""{}_{}.ppl"".\n                                            format(self.name, self.time))\n        save_zip(ppl_file, self.feature_transformers, self.model, self.config)\n        print(""Pipeline is saved in"", ppl_file)\n        return ppl_file\n\n    def config_save(self, config_file=None):\n        """"""\n        save all configs to file.\n        :param config_file:\n        :return:\n        """"""\n        config_file = config_file or os.path.join(DEFAULT_CONFIG_DIR, ""{}_{}.json"".\n                                                  format(self.name, self.time))\n        save_config(config_file, self.config, replace=True)\n        return config_file\n\n\ndef load_ts_pipeline(file):\n    feature_transformers = TimeSequenceFeatureTransformer()\n    model = TimeSequenceModel(check_optional_config=False)\n\n    all_config = restore_zip(file, feature_transformers, model)\n    ts_pipeline = TimeSequencePipeline(feature_transformers=feature_transformers,\n                                       model=model,\n                                       config=all_config)\n    print(""Restore pipeline from"", file)\n    return ts_pipeline\n'"
pyzoo/zoo/automl/regression/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/automl/regression/time_sequence_predictor.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either exp\'\n# ress or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport numpy as np\nimport tempfile\nimport zipfile\nimport os\nimport shutil\nimport ray\n\nfrom zoo.automl.search.abstract import *\nfrom zoo.automl.search.RayTuneSearchEngine import RayTuneSearchEngine\nfrom zoo.automl.common.metrics import Evaluator\nfrom zoo.automl.feature.time_sequence import TimeSequenceFeatureTransformer\n\nfrom zoo.automl.model import TimeSequenceModel\nfrom zoo.automl.pipeline.time_sequence import TimeSequencePipeline, load_ts_pipeline\nfrom zoo.automl.common.util import *\nfrom zoo.automl.config.recipe import *\n\n\nclass TimeSequencePredictor(object):\n    """"""\n    Trains a model that predicts future time sequence from past sequence.\n    Past sequence should be > 1. Future sequence can be > 1.\n    For example, predict the next 2 data points from past 5 data points.\n    Output have only one target value (a scalar) for each data point in the sequence.\n    Input can have more than one features (value plus several features)\n    Example usage:\n        tsp = TimeSequencePredictor()\n        tsp.fit(input_df)\n        result = tsp.predict(test_df)\n\n    """"""\n\n    def __init__(self,\n                 name=""automl"",\n                 logs_dir=""~/zoo_automl_logs"",\n                 future_seq_len=1,\n                 dt_col=""datetime"",\n                 target_col=""value"",\n                 extra_features_col=None,\n                 drop_missing=True,\n                 ):\n        """"""\n        Constructor of Time Sequence Predictor\n        :param logs_dir where the automl tune logs file located\n        :param future_seq_len: the future sequence length to be predicted\n        :param dt_col: the datetime index column\n        :param target_col: the target col (to be predicted)\n        :param extra_features_col: extra features\n        :param drop_missing: whether to drop missing values in the input\n        """"""\n        self.logs_dir = logs_dir\n        self.pipeline = None\n        self.future_seq_len = future_seq_len\n        self.dt_col = dt_col\n        self.target_col = target_col\n        self.extra_features_col = extra_features_col\n        self.drop_missing = drop_missing\n        self.name = name\n\n    def fit(self,\n            input_df,\n            validation_df=None,\n            metric=""mse"",\n            recipe=SmokeRecipe(),\n            mc=False,\n            resources_per_trial={""cpu"": 2},\n            distributed=False,\n            hdfs_url=None\n            ):\n        """"""\n        Trains the model for time sequence prediction.\n        If future sequence length > 1, use seq2seq model, else use vanilla LSTM model.\n        :param input_df: The input time series data frame, Example:\n         datetime   value   ""extra feature 1""   ""extra feature 2""\n         2019-01-01 1.9 1   2\n         2019-01-02 2.3 0   2\n        :param validation_df: validation data\n        :param metric: String. Metric used for train and validation. Available values are\n                       ""mean_squared_error"" or ""r_square""\n        :param recipe: a Recipe object. Various recipes covers different search space and stopping\n                      criteria. Default is SmokeRecipe().\n        :param resources_per_trial: Machine resources to allocate per trial,\n            e.g. ``{""cpu"": 64, ""gpu"": 8}`\n        :param distributed: bool. Indicate if running in distributed mode. If true, we will upload\n                            models to HDFS.\n        :param hdfs_url: the hdfs url used to save file in distributed model. If None, the default\n                         hdfs_url will be used.\n        :return: self\n        """"""\n\n        self._check_input(input_df, validation_df, metric)\n        if distributed:\n            if hdfs_url is not None:\n                remote_dir = os.path.join(hdfs_url, ""ray_results"", self.name)\n            else:\n                remote_dir = os.path.join(os.sep, ""ray_results"", self.name)\n            if self.name not in get_remote_list(os.path.dirname(remote_dir)):\n                cmd = ""hadoop fs -mkdir -p {}"".format(remote_dir)\n                process(cmd)\n        else:\n            remote_dir = None\n\n        self.pipeline = self._hp_search(\n            input_df,\n            validation_df=validation_df,\n            metric=metric,\n            recipe=recipe,\n            mc=mc,\n            resources_per_trial=resources_per_trial,\n            remote_dir=remote_dir)\n        return self.pipeline\n\n    def evaluate(self,\n                 input_df,\n                 metric=None\n                 ):\n        """"""\n        Evaluate the model on a list of metrics.\n        :param input_df: The input time series data frame, Example:\n         datetime   value   ""extra feature 1""   ""extra feature 2""\n         2019-01-01 1.9 1   2\n         2019-01-02 2.3 0   2\n        :param metric: A list of Strings Available string values are ""mean_squared_error"",\n                      ""r_square"".\n        :return: a list of metric evaluation results.\n        """"""\n        if not Evaluator.check_metric(metric):\n            raise ValueError(""metric"" + metric + ""is not supported"")\n        return self.pipeline.evaluate(input_df, metric)\n\n    def predict(self,\n                input_df):\n        """"""\n        Predict future sequence from past sequence.\n        :param input_df: The input time series data frame, Example:\n         datetime   value   ""extra feature 1""   ""extra feature 2""\n         2019-01-01 1.9 1   2\n         2019-01-02 2.3 0   2\n        :return: a data frame with 2 columns, the 1st is the datetime, which is the last datetime of\n            the past sequence.\n            values are the predicted future sequence values.\n            Example :\n            datetime    value_0     value_1   ...     value_2\n            2019-01-03  2           3                   9\n        """"""\n        return self.pipeline.predict(input_df)\n\n    def _check_input_format(self, input_df):\n        if isinstance(input_df, list) and all(\n                [isinstance(d, pd.DataFrame) for d in input_df]):\n            input_is_list = True\n            return input_is_list\n        elif isinstance(input_df, pd.DataFrame):\n            input_is_list = False\n            return input_is_list\n        else:\n            raise ValueError(\n                ""input_df should be a data frame or a list of data frames"")\n\n    def _check_missing_col(self, input_df):\n        cols_list = [self.dt_col, self.target_col]\n        if self.extra_features_col is not None:\n            if not isinstance(self.extra_features_col, (list,)):\n                raise ValueError(\n                    ""extra_features_col needs to be either None or a list"")\n            cols_list.extend(self.extra_features_col)\n\n        missing_cols = set(cols_list) - set(input_df.columns)\n        if len(missing_cols) != 0:\n            raise ValueError(""Missing Columns in the input data frame:"" +\n                             \',\'.join(list(missing_cols)))\n\n    def _check_input(self, input_df, validation_df, metric):\n        input_is_list = self._check_input_format(input_df)\n        if not input_is_list:\n            self._check_missing_col(input_df)\n            if validation_df is not None:\n                self._check_missing_col(validation_df)\n        else:\n            for d in input_df:\n                self._check_missing_col(d)\n            if validation_df is not None:\n                for val_d in validation_df:\n                    self._check_missing_col(val_d)\n\n        allowed_fit_metrics = [""mse"", ""mae"", ""r2""]\n        if metric not in allowed_fit_metrics:\n            raise ValueError(""metric "" + metric + "" is not supported"")\n\n    @staticmethod\n    def _get_metric_mode(metric):\n        max_mode_metrics = [""r2""]\n        min_mode_metrics = [""mse"", ""mae""]\n        if metric in min_mode_metrics:\n            return ""min""\n        elif metric in max_mode_metrics:\n            return ""max""\n        else:\n            return ValueError, ""metric "" + metric + "" is not supported""\n\n    def _hp_search(self,\n                   input_df,\n                   validation_df,\n                   metric,\n                   recipe,\n                   mc,\n                   resources_per_trial,\n                   remote_dir):\n\n        ft = TimeSequenceFeatureTransformer(self.future_seq_len,\n                                            self.dt_col,\n                                            self.target_col,\n                                            self.extra_features_col,\n                                            self.drop_missing)\n        if isinstance(input_df, list):\n            feature_list = ft.get_feature_list(input_df[0])\n        else:\n            feature_list = ft.get_feature_list(input_df)\n\n        # model = VanillaLSTM(check_optional_config=False)\n        model = TimeSequenceModel(\n            check_optional_config=False,\n            future_seq_len=self.future_seq_len)\n\n        # prepare parameters for search engine\n        search_space = recipe.search_space(feature_list)\n        runtime_params = recipe.runtime_params()\n        num_samples = runtime_params[\'num_samples\']\n        stop = dict(runtime_params)\n        search_algorithm_params = recipe.search_algorithm_params()\n        search_algorithm = recipe.search_algorithm()\n        fixed_params = recipe.fixed_params()\n        del stop[\'num_samples\']\n\n        metric_mode = TimeSequencePredictor._get_metric_mode(metric)\n        searcher = RayTuneSearchEngine(logs_dir=self.logs_dir,\n                                       resources_per_trial=resources_per_trial,\n                                       name=self.name,\n                                       remote_dir=remote_dir,\n                                       )\n        searcher.compile(input_df,\n                         search_space=search_space,\n                         stop=stop,\n                         search_algorithm_params=search_algorithm_params,\n                         search_algorithm=search_algorithm,\n                         fixed_params=fixed_params,\n                         feature_transformers=ft,\n                         future_seq_len=self.future_seq_len,\n                         validation_df=validation_df,\n                         metric=metric,\n                         metric_mode=metric_mode,\n                         mc=mc,\n                         num_samples=num_samples)\n        # searcher.test_run()\n        analysis = searcher.run()\n\n        pipeline = self._make_pipeline(analysis,\n                                       metric_mode,\n                                       feature_transformers=ft,\n                                       model=model,\n                                       remote_dir=remote_dir)\n        return pipeline\n\n    def _print_config(self, best_config):\n        print(""The best configurations are:"")\n        for name, value in best_config.items():\n            print(name, "":"", value)\n\n    def _make_pipeline(self, analysis, metric_mode, feature_transformers, model, remote_dir):\n        metric = ""reward_metric""\n        best_config = analysis.get_best_config(metric=metric, mode=metric_mode)\n        best_logdir = analysis.get_best_logdir(metric=metric, mode=metric_mode)\n        print(""best log dir is "", best_logdir)\n        dataframe = analysis.dataframe(metric=metric, mode=metric_mode)\n        # print(dataframe)\n        model_path = os.path.join(best_logdir, dataframe[""checkpoint""].iloc[0])\n        config = convert_bayes_configs(best_config).copy()\n        self._print_config(config)\n        if remote_dir is not None:\n            all_config = restore_hdfs(model_path,\n                                      remote_dir,\n                                      feature_transformers,\n                                      model,\n                                      # config)\n                                      )\n        else:\n            all_config = restore_zip(model_path,\n                                     feature_transformers,\n                                     model,\n                                     # config)\n                                     )\n        return TimeSequencePipeline(name=self.name,\n                                    feature_transformers=feature_transformers,\n                                    model=model,\n                                    config=all_config)\n'"
pyzoo/zoo/automl/search/RayTuneSearchEngine.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport ray\nfrom ray import tune\nfrom copy import copy, deepcopy\n\nfrom zoo.automl.model import TimeSequenceModel\nfrom zoo.automl.search.abstract import *\nfrom zoo.automl.common.util import *\nfrom ray.tune import Trainable\nfrom ray.tune.suggest.bayesopt import BayesOptSearch\n\n\nclass RayTuneSearchEngine(SearchEngine):\n    """"""\n    Tune driver\n    """"""\n\n    def __init__(self,\n                 logs_dir="""",\n                 resources_per_trial=None,\n                 name="""",\n                 remote_dir=None,\n                 ):\n        """"""\n        Constructor\n        :param resources_per_trial: resources for each trial\n        """"""\n        self.pipeline = None\n        self.train_func = None\n        self.trainable_class = None\n        self.resources_per_trail = resources_per_trial\n        self.trials = None\n        self.remote_dir = remote_dir\n        self.name = name\n\n    def compile(self,\n                input_df,\n                search_space,\n                num_samples=1,\n                stop=None,\n                search_algorithm=None,\n                search_algorithm_params=None,\n                fixed_params=None,\n                feature_transformers=None,\n                # model=None,\n                future_seq_len=1,\n                validation_df=None,\n                mc=False,\n                metric=""mse"",\n                metric_mode=""min""):\n        """"""\n        Do necessary preparations for the engine\n        :param input_df:\n        :param search_space:\n        :param num_samples:\n        :param stop:\n        :param search_algorithm:\n        :param search_algorithm_params:\n        :param fixed_params:\n        :param feature_transformers:\n        :param model:\n        :param validation_df:\n        :param metric:\n        :return:\n        """"""\n        self.search_space = self._prepare_tune_config(search_space)\n        self.stop_criteria = stop\n        self.num_samples = num_samples\n\n        if search_algorithm == \'BayesOpt\':\n            self.search_algorithm = BayesOptSearch(\n                self.search_space,\n                metric=""reward_metric"",\n                mode=metric_mode,\n                utility_kwargs=search_algorithm_params[""utility_kwargs""]\n            )\n        else:\n            self.search_algorithm = None\n        self.fixed_params = fixed_params\n\n        self.train_func = self._prepare_train_func(input_df=input_df,\n                                                   feature_transformers=feature_transformers,\n                                                   future_seq_len=future_seq_len,\n                                                   validation_df=validation_df,\n                                                   metric=metric,\n                                                   metric_mode=metric_mode,\n                                                   mc=mc,\n                                                   remote_dir=self.remote_dir)\n        # self.trainable_class = self._prepare_trainable_class(input_df,\n        #                                                      feature_transformers,\n        #                                                      # model,\n        #                                                      future_seq_len,\n        #                                                      validation_df,\n        #                                                      metric_op,\n        #                                                      self.remote_dir)\n\n    def run(self):\n        """"""\n        Run trials\n        :return: trials result\n        """"""\n        # function based\n        if not self.search_algorithm:\n            analysis = tune.run(\n                self.train_func,\n                name=self.name,\n                stop=self.stop_criteria,\n                config=self.search_space,\n                num_samples=self.num_samples,\n                resources_per_trial=self.resources_per_trail,\n                verbose=1,\n                reuse_actors=True\n            )\n        else:\n            analysis = tune.run(\n                self.train_func,\n                name=self.name,\n                config=self.fixed_params,\n                stop=self.stop_criteria,\n                search_alg=self.search_algorithm,\n                num_samples=self.num_samples,\n                resources_per_trial=self.resources_per_trail,\n                verbose=1,\n                reuse_actors=True\n            )\n        self.trials = analysis.trials\n        return analysis\n\n    def get_best_trials(self, k=1):\n        sorted_trials = RayTuneSearchEngine._get_sorted_trials(self.trials, metric=""reward_metric"")\n        best_trials = sorted_trials[:k]\n        return [self._make_trial_output(t) for t in best_trials]\n\n    def _make_trial_output(self, trial):\n        return TrialOutput(config=trial.config,\n                           model_path=os.path.join(trial.logdir, trial.last_result[""checkpoint""]))\n\n    @staticmethod\n    def _get_best_trial(trial_list, metric):\n        """"""Retrieve the best trial.""""""\n        return max(trial_list, key=lambda trial: trial.last_result.get(metric, 0))\n\n    @staticmethod\n    def _get_sorted_trials(trial_list, metric):\n        return sorted(\n            trial_list,\n            key=lambda trial: trial.last_result.get(metric, 0),\n            reverse=True)\n\n    @staticmethod\n    def _get_best_result(trial_list, metric):\n        """"""Retrieve the last result from the best trial.""""""\n        return {metric: RayTuneSearchEngine._get_best_trial(trial_list, metric).last_result[metric]}\n\n    def test_run(self):\n        def mock_reporter(**kwargs):\n            assert ""reward_metric"" in kwargs, ""Did not report proper metric""\n            assert ""checkpoint"" in kwargs, ""Accidentally removed `checkpoint`?""\n            raise GoodError(""This works."")\n\n        try:\n            self.train_func({\'out_units\': 1,\n                             \'selected_features\': [""MONTH(datetime)"", ""WEEKDAY(datetime)""]},\n                            mock_reporter)\n            # self.train_func(self.search_space, mock_reporter)\n\n        except TypeError as e:\n            print(""Forgot to modify function signature?"")\n            raise e\n        except GoodError:\n            print(""Works!"")\n            return 1\n        raise Exception(""Didn\'t call reporter..."")\n\n    @staticmethod\n    def _is_validation_df_valid(validation_df):\n        df_not_empty = isinstance(validation_df, pd.DataFrame) and not validation_df.empty\n        df_list_not_empty = isinstance(validation_df, list) and validation_df \\\n            and not all([d.empty for d in validation_df])\n        return validation_df is not None and not (df_not_empty or df_list_not_empty)\n\n    @staticmethod\n    def _prepare_train_func(input_df,\n                            feature_transformers,\n                            future_seq_len,\n                            metric,\n                            metric_mode,\n                            validation_df=None,\n                            mc=False,\n                            remote_dir=None\n                            ):\n        """"""\n        Prepare the train function for ray tune\n        :param input_df: input dataframe\n        :param feature_transformers: feature transformers\n        :param model: model or model selector\n        :param validation_df: validation dataframe\n        :param metric: the rewarding metric\n        :param metric_mode: the mode of rewarding metric. ""min"" or ""max""\n        :return: the train function\n        """"""\n        input_df_id = ray.put(input_df)\n        ft_id = ray.put(feature_transformers)\n        # model_id = ray.put(model)\n\n        df_not_empty = isinstance(validation_df, pd.DataFrame) and not validation_df.empty\n        df_list_not_empty = isinstance(validation_df, list) and validation_df \\\n            and not all([d.empty for d in validation_df])\n        if validation_df is not None and (df_not_empty or df_list_not_empty):\n            validation_df_id = ray.put(validation_df)\n            is_val_df_valid = True\n        else:\n            is_val_df_valid = False\n\n        def train_func(config):\n            # make a copy from global variables for trial to make changes\n            global_ft = ray.get(ft_id)\n            # global_model = ray.get(model_id)\n            trial_ft = deepcopy(global_ft)\n            # trial_model = deepcopy(global_model)\n            trial_model = TimeSequenceModel(check_optional_config=False,\n                                            future_seq_len=future_seq_len)\n\n            # handling input\n            global_input_df = ray.get(input_df_id)\n            trial_input_df = deepcopy(global_input_df)\n            config = convert_bayes_configs(config).copy()\n            # print(""config is "", config)\n            (x_train, y_train) = trial_ft.fit_transform(trial_input_df, **config)\n            # trial_ft.fit(trial_input_df, **config)\n\n            # handling validation data\n            validation_data = None\n            if is_val_df_valid:\n                global_validation_df = ray.get(validation_df_id)\n                trial_validation_df = deepcopy(global_validation_df)\n                validation_data = trial_ft.transform(trial_validation_df)\n\n            # no need to call build since it is called the first time fit_eval is called.\n            # callbacks = [TuneCallback(tune_reporter)]\n            # fit model\n            best_reward_m = -999\n            metric_op = 1 if metric_mode is ""max"" else -1\n            for i in range(1, 101):\n                result = trial_model.fit_eval(x_train,\n                                              y_train,\n                                              validation_data=validation_data,\n                                              mc=mc,\n                                              metric=metric,\n                                              # verbose=1,\n                                              **config)\n                reward_m = metric_op * result\n                ckpt_name = ""best.ckpt""\n                if reward_m > best_reward_m:\n                    best_reward_m = reward_m\n                    save_zip(ckpt_name, trial_ft, trial_model, config)\n                    if remote_dir is not None:\n                        upload_ppl_hdfs(remote_dir, ckpt_name)\n\n                tune.track.log(training_iteration=i,\n                               reward_metric=reward_m,\n                               checkpoint=""best.ckpt"")\n\n        return train_func\n\n    @staticmethod\n    def _prepare_trainable_class(input_df,\n                                 feature_transformers,\n                                 future_seq_len,\n                                 metric,\n                                 metric_mode,\n                                 validation_df=None,\n                                 mc=False,\n                                 remote_dir=None\n                                 ):\n        """"""\n        Prepare the train function for ray tune\n        :param input_df: input dataframe\n        :param feature_transformers: feature transformers\n        :param model: model or model selector\n        :param validation_df: validation dataframe\n        :param metric: the rewarding metric\n        :param metric_mode: the mode of rewarding metric. ""min"" or ""max""\n        :return: the train function\n        """"""\n        input_df_id = ray.put(input_df)\n        ft_id = ray.put(feature_transformers)\n        # model_id = ray.put(model)\n\n        df_not_empty = isinstance(validation_df, pd.DataFrame) and not validation_df.empty\n        df_list_not_empty = isinstance(validation_df, list) and validation_df \\\n            and not all([d.empty for d in validation_df])\n        if validation_df is not None and (df_not_empty or df_list_not_empty):\n            validation_df_id = ray.put(validation_df)\n            is_val_df_valid = True\n        else:\n            is_val_df_valid = False\n\n        class TrainableClass(Trainable):\n\n            def _setup(self, config):\n                # print(""config in set up is"", config)\n                global_ft = ray.get(ft_id)\n                # global_model = ray.get(model_id)\n                self.trial_ft = deepcopy(global_ft)\n                self.trial_model = TimeSequenceModel(check_optional_config=False,\n                                                     future_seq_len=future_seq_len)\n\n                # handling input\n                global_input_df = ray.get(input_df_id)\n                trial_input_df = deepcopy(global_input_df)\n                self.config = convert_bayes_configs(config).copy()\n                (self.x_train, self.y_train) = self.trial_ft.fit_transform(trial_input_df,\n                                                                           **self.config)\n                # trial_ft.fit(trial_input_df, **config)\n\n                # handling validation data\n                self.validation_data = None\n                if is_val_df_valid:\n                    global_validation_df = ray.get(validation_df_id)\n                    trial_validation_df = deepcopy(global_validation_df)\n                    self.validation_data = self.trial_ft.transform(trial_validation_df)\n\n                # no need to call build since it is called the first time fit_eval is called.\n                # callbacks = [TuneCallback(tune_reporter)]\n                # fit model\n                self.best_reward_m = -999\n                self.reward_m = -999\n                self.ckpt_name = ""pipeline.ckpt""\n                self.metric_op = 1 if metric_mode is ""max"" else -1\n\n            def _train(self):\n                # print(""self.config in train is "", self.config)\n                result = self.trial_model.fit_eval(self.x_train, self.y_train,\n                                                   validation_data=self.validation_data,\n                                                   # verbose=1,\n                                                   **self.config)\n                self.reward_m = self.metric_op * result\n                # if metric == ""mean_squared_error"":\n                #     self.reward_m = (-1) * result\n                #     # print(""running iteration: "",i)\n                # elif metric == ""r_square"":\n                #     self.reward_m = result\n                # else:\n                #     raise ValueError(""metric can only be \\""mean_squared_error\\"" or \\""r_square\\"""")\n                return {""reward_metric"": self.reward_m, ""checkpoint"": self.ckpt_name}\n\n            def _save(self, checkpoint_dir):\n                # print(""checkpoint dir is "", checkpoint_dir)\n                ckpt_name = self.ckpt_name\n                # save in the working dir (without ""checkpoint_{}"".format(training_iteration))\n                path = os.path.join(checkpoint_dir, "".."", ckpt_name)\n                # path = os.path.join(checkpoint_dir, ckpt_name)\n                # print(""checkpoint save path is "", checkpoint_dir)\n                if self.reward_m > self.best_reward_m:\n                    self.best_reward_m = self.reward_m\n                    print(""****this reward is"", self.reward_m)\n                    print(""*********saving checkpoint"")\n                    save_zip(ckpt_name, self.trial_ft, self.trial_model, self.config)\n                    if remote_dir is not None:\n                        upload_ppl_hdfs(remote_dir, ckpt_name)\n                return path\n\n            def _restore(self, checkpoint_path):\n                # print(""checkpoint path in restore is "", checkpoint_path)\n                if remote_dir is not None:\n                    restore_hdfs(checkpoint_path, remote_dir, self.trial_ft, self.trial_model)\n                else:\n                    restore_zip(checkpoint_path, self.trial_ft, self.trial_model)\n\n        return TrainableClass\n\n    def _prepare_tune_config(self, space):\n        tune_config = {}\n        for k, v in space.items():\n            if isinstance(v, RandomSample):\n                tune_config[k] = tune.sample_from(v.func)\n            elif isinstance(v, GridSearch):\n                tune_config[k] = tune.grid_search(v.values)\n            else:\n                tune_config[k] = v\n        return tune_config\n'"
pyzoo/zoo/automl/search/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .abstract import *\n'"
pyzoo/zoo/automl/search/abstract.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom abc import ABC, abstractmethod\n\n\nclass GoodError(Exception):\n    pass\n\n\nclass SearchEngine(ABC):\n    """"""\n    Abstract Base Search Engine class. For hyper paramter tuning.\n    """"""\n\n    @abstractmethod\n    def run(self):\n        """"""\n        Run the trials with searched parameters\n        :return:\n        """"""\n        pass\n\n    @abstractmethod\n    def get_best_trials(self, k):\n        """"""\n        Get the best trials from .\n        :param k: trials to be selected\n        :return: the config of best k trials\n        """"""\n        pass\n\n\nclass GridSearch(object):\n    def __init__(self, values):\n        self.values = values\n\n\nclass RandomSample(object):\n    def __init__(self, func):\n        self.func = func\n\n\nclass BayersianOpt(object):\n    def __init__(self):\n        pass\n\n\nclass TrialOutput(object):\n    def __init__(self, config, model_path):\n        self.config = config\n        self.model_path = model_path\n'"
pyzoo/zoo/examples/anomalydetection/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/anomalydetection/anomaly_detection.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.common.nncontext import init_nncontext\nfrom zoo.models.anomalydetection import AnomalyDetector\nimport pandas as pd\nfrom pyspark.sql import SQLContext\nfrom pyspark import sql\nfrom optparse import OptionParser\nimport sys\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--input_dir"", dest=""input_dir"",\n                      help=""Required. The path where NBA nyc_taxi.csv locates."")\n    parser.add_option(""-b"", ""--batch_size"", dest=""batch_size"", default=""1024"",\n                      help=""The number of samples per gradient update. Default is 1024."")\n    parser.add_option(""--nb_epoch"", dest=""nb_epoch"", default=""20"",\n                      help=""The number of epochs to train the model. Default is 20."")\n    parser.add_option(""--unroll_len"", dest=""unroll_len"", default=""24"",\n                      help=""The length of precious values to predict future value. Default is 24."")\n\n    (options, args) = parser.parse_args(sys.argv)\n\n    # if input_dir is not given\n    if not options.input_dir:\n        parser.print_help()\n        parser.error(\'input_dir is required\')\n\n    sc = init_nncontext(""Anomaly Detection Example"")\n\n    sqlContext = sql.SQLContext(sc)\n\n    def load_and_scale(input_path):\n        df = pd.read_csv(input_path)\n        df[\'datetime\'] = pd.to_datetime(df[\'timestamp\'])\n        df[\'hours\'] = df[\'datetime\'].dt.hour\n        df[\'awake\'] = (((df[\'hours\'] >= 6) & (df[\'hours\'] <= 23)) | (df[\'hours\'] == 0)).astype(int)\n        print(df.head(10))\n        sqlContext = SQLContext(sc)\n        dfspark = sqlContext.createDataFrame(df[[""value"", ""hours"", ""awake""]])\n        feature_size = len([""value"", ""hours"", ""awake""])\n        return AnomalyDetector.standardScale(dfspark), feature_size\n\n    df_scaled, feature_size = load_and_scale(options.input_dir)\n    data_rdd = df_scaled.rdd.map(lambda row: [x for x in row])\n    unrolled = AnomalyDetector.unroll(data_rdd, int(options.unroll_len), predict_step=1)\n    [train, test] = AnomalyDetector.train_test_split(unrolled, 1000)\n\n    model = AnomalyDetector(feature_shape=(int(options.unroll_len), feature_size),\n                            hidden_layers=[8, 32, 15], dropouts=[0.2, 0.2, 0.2])\n    model.compile(loss=\'mse\', optimizer=\'rmsprop\', metrics=[\'mae\'])\n    model.fit(train, batch_size=int(options.batch_size), nb_epoch=int(options.nb_epoch))\n    test.cache()\n    y_predict = model.predict(test, batch_per_thread=int(options.batch_size))\\\n        .map(lambda x: float(x[0]))\n    y_truth = test.map(lambda x: float(x.label.to_ndarray()[0]))\n    anomalies = AnomalyDetector.detect_anomalies(y_predict, y_truth, 50)\n\n    print(""anomalies: "", anomalies.take(10)[0:10])\n    print(""finished..."")\n    sc.stop()\n'"
pyzoo/zoo/examples/attention/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/attention/transformer.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom bigdl.optim.optimizer import Adam\nfrom tensorflow.python.keras.datasets import imdb\nfrom tensorflow.python.keras.preprocessing import sequence\nfrom zoo.pipeline.api.keras.models import Model\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.common.nncontext import init_spark_conf\nfrom zoo.common.nncontext import init_nncontext\n\n\nconf = init_spark_conf()\nconf.set(""spark.executor.extraJavaOptions"", ""-Xss512m"")\nconf.set(""spark.driver.extraJavaOptions"", ""-Xss512m"")\nsc = init_nncontext(conf)\nmax_features = 20000\nmax_len = 200\n\nprint(\'Loading data...\')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), \'train sequences\')\nprint(len(x_test), \'test sequences\')\n\nprint(\'Pad sequences (samples x time)\')\nx_train = sequence.pad_sequences(x_train, maxlen=max_len)\nx_test = sequence.pad_sequences(x_test, maxlen=max_len)\nprint(\'x_train shape:\', x_train.shape)\nprint(\'x_test shape:\', x_test.shape)\n\n\ntrain_pos = np.zeros((len(x_train), max_len), dtype=np.int32)\nval_pos = np.zeros((len(x_test), max_len), dtype=np.int32)\nfor i in range(0, len(x_train)):\n    train_pos[i, :] = np.arange(max_len)\n    val_pos[i, :] = np.arange(max_len)\n\n\ndef build_sample(token_id, position_id, label):\n    samples = []\n    for i in range(label.shape[0]):\n        sample = Sample.from_ndarray([token_id[i], position_id[i]], np.array(label[i]))\n        samples.append(sample)\n    return samples\n\n\ntrain_samples = build_sample(x_train, train_pos, y_train)\ntrain_rdd = sc.parallelize(train_samples)\nval_samples = build_sample(x_test, val_pos, y_test)\nval_rdd = sc.parallelize(val_samples)\n\ntoken_shape = (max_len,)\nposition_shape = (max_len,)\ntoken_input = Input(shape=token_shape)\nposition_input = Input(shape=position_shape)\nO_seq = TransformerLayer.init(\n    vocab=max_features, hidden_size=128, n_head=8, seq_len=max_len)([token_input, position_input])\n# Select the first output of the Transformer. The second is the pooled output.\nO_seq = SelectTable(0)(O_seq)\nO_seq = GlobalAveragePooling1D()(O_seq)\nO_seq = Dropout(0.2)(O_seq)\noutputs = Dense(2, activation=\'softmax\')(O_seq)\n\nmodel = Model([token_input, position_input], outputs)\nmodel.summary()\n\nmodel.compile(optimizer=Adam(),\n              loss=""sparse_categorical_crossentropy"",\n              metrics=[\'accuracy\'])\n\nbatch_size = 128\nprint(\'Train...\')\nmodel.fit(train_rdd,\n          batch_size=batch_size,\n          nb_epoch=1)\nprint(""Train finished."")\n\nprint(\'Evaluating...\')\nscore = model.evaluate(val_rdd, batch_size=128)[0]\nprint(score)\n\nprint(""finished..."")\nsc.stop()\n'"
pyzoo/zoo/examples/autograd/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/autograd/custom.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.common.nncontext import *\nfrom zoo.pipeline.api.autograd import *\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import *\nfrom optparse import OptionParser\n\n\ndef mean_absolute_error(y_true, y_pred):\n    result = mean(abs(y_true - y_pred), axis=1)\n    return result\n\n\ndef add_one_func(x):\n    return x + 1.0\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--nb_epoch"", dest=""nb_epoch"", default=""500"")\n\n    (options, args) = parser.parse_args(sys.argv)\n\n    sc = init_nncontext(""custom example"")\n\n    data_len = 1000\n    X_ = np.random.uniform(0, 1, (1000, 2))\n    Y_ = ((2 * X_).sum(1) + 0.4).reshape([data_len, 1])\n\n    a = Input(shape=(2,))\n    b = Dense(1)(a)\n    c = Lambda(function=add_one_func)(b)\n    model = Model(input=a, output=c)\n\n    model.compile(optimizer=SGD(learningrate=1e-2),\n                  loss=mean_absolute_error)\n\n    model.set_tensorboard(\'./log\', \'customized layer and loss\')\n\n    model.fit(x=X_,\n              y=Y_,\n              batch_size=32,\n              nb_epoch=int(options.nb_epoch),\n              distributed=False)\n\n    model.save_graph_topology(\'./log\')\n\n    w = model.get_weights()\n    print(w)\n    pred = model.predict_local(X_)\n    print(""finished..."")\n    sc.stop()\n'"
pyzoo/zoo/examples/autograd/customloss.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.common.nncontext import *\nfrom zoo.pipeline.api.autograd import *\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import *\n\n\ndef mean_absolute_error(y_true, y_pred):\n    result = mean(abs(y_true - y_pred), axis=1)\n    return result\n\n\nif __name__ == ""__main__"":\n    sc = init_nncontext(""customloss example"")\n    data_len = 1000\n    X_ = np.random.uniform(0, 1, (1000, 2))\n    Y_ = ((2 * X_).sum(1) + 0.4).reshape([data_len, 1])\n    model = Sequential()\n    model.add(Dense(1, input_shape=(2,)))\n    model.compile(optimizer=SGD(learningrate=1e-2),\n                  loss=mean_absolute_error,\n                  metrics=None)\n    model.fit(x=X_,\n              y=Y_,\n              batch_size=32,\n              nb_epoch=500,\n              validation_data=None,\n              distributed=False)\n    w = model.get_weights()\n    print(w)\n    pred = model.predict_local(X_)\n    print(""finished..."")\n    sc.stop()\n'"
pyzoo/zoo/examples/horovod/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/horovod/simple_horovod_pytorch.py,11,"b'# This file is adapted from https://github.com/horovod/horovod/blob/master/examples/pytorch_mnist.py\n\nfrom __future__ import print_function\n\nimport argparse\n\nimport horovod.torch as hvd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data.distributed\nfrom torchvision import datasets, transforms\nfrom zoo.ray import RayContext\n\nfrom zoo import init_spark_on_yarn, init_spark_on_local\nfrom zoo.orca.learn.horovod import HorovodRayTrainer\n\n\ndef run_horovod():\n    # Temporary patch this script until the MNIST dataset download issue get resolved\n    # https://github.com/pytorch/vision/issues/1938\n    import urllib\n    try:\n        # For python 2\n        class AppURLopener(urllib.FancyURLopener):\n            version = ""Mozilla/5.0""\n\n        urllib._urlopener = AppURLopener()\n    except AttributeError:\n        # For python 3\n        opener = urllib.request.build_opener()\n        opener.addheaders = [(\'User-agent\', \'Mozilla/5.0\')]\n        urllib.request.install_opener(opener)\n\n    batch_size = 64\n    test_batch_size = 1000\n    epochs = 10\n    lr = 0.01\n    momentum = 0.5\n    seed = 43\n    log_interval = 10\n    fp16_allreduce = False\n    use_adasum = False\n\n    # Horovod: initialize library.\n    hvd.init()\n    torch.manual_seed(seed)\n\n    # Horovod: limit # of CPU threads to be used per worker.\n    torch.set_num_threads(4)\n\n    kwargs = {}\n    train_dataset = \\\n        datasets.MNIST(\'data-%d\' % hvd.rank(), train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ]))\n    # Horovod: use DistributedSampler to partition the training data.\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, sampler=train_sampler, **kwargs)\n\n    test_dataset = \\\n        datasets.MNIST(\'data-%d\' % hvd.rank(), train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ]))\n    # Horovod: use DistributedSampler to partition the test data.\n    test_sampler = torch.utils.data.distributed.DistributedSampler(\n        test_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n                                              sampler=test_sampler, **kwargs)\n\n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n            self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n            self.conv2_drop = nn.Dropout2d()\n            self.fc1 = nn.Linear(320, 50)\n            self.fc2 = nn.Linear(50, 10)\n\n        def forward(self, x):\n            x = F.relu(F.max_pool2d(self.conv1(x), 2))\n            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n            x = x.view(-1, 320)\n            x = F.relu(self.fc1(x))\n            x = F.dropout(x, training=self.training)\n            x = self.fc2(x)\n            return F.log_softmax(x)\n\n    model = Net()\n\n    # By default, Adasum doesn\'t need scaling up learning rate.\n    lr_scaler = hvd.size() if not use_adasum else 1\n\n    # Horovod: scale learning rate by lr_scaler.\n    optimizer = optim.SGD(model.parameters(), lr=lr * lr_scaler,\n                          momentum=momentum)\n\n    # Horovod: broadcast parameters & optimizer state.\n    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n    hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n\n    # Horovod: (optional) compression algorithm.\n    compression = hvd.Compression.fp16 if fp16_allreduce else hvd.Compression.none\n\n    # Horovod: wrap optimizer with DistributedOptimizer.\n    optimizer = hvd.DistributedOptimizer(optimizer,\n                                         named_parameters=model.named_parameters(),\n                                         compression=compression,\n                                         op=hvd.Adasum if use_adasum else hvd.Average)\n\n    def train(epoch):\n        model.train()\n        # Horovod: set epoch to sampler for shuffling.\n        train_sampler.set_epoch(epoch)\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            loss.backward()\n            optimizer.step()\n            if batch_idx % log_interval == 0:\n                # Horovod: use train_sampler to determine the number of examples in\n                # this worker\'s partition.\n                print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                    epoch, batch_idx * len(data), len(train_sampler),\n                    100. * batch_idx / len(train_loader), loss.item()))\n\n    def metric_average(val, name):\n        tensor = torch.tensor(val)\n        avg_tensor = hvd.allreduce(tensor, name=name)\n        return avg_tensor.item()\n\n    def test():\n        model.eval()\n        test_loss = 0.\n        test_accuracy = 0.\n        for data, target in test_loader:\n            output = model(data)\n            # sum up batch loss\n            test_loss += F.nll_loss(output, target, size_average=False).item()\n            # get the index of the max log-probability\n            pred = output.data.max(1, keepdim=True)[1]\n            test_accuracy += pred.eq(target.data.view_as(pred)).cpu().float().sum()\n\n        # Horovod: use test_sampler to determine the number of examples in\n        # this worker\'s partition.\n        test_loss /= len(test_sampler)\n        test_accuracy /= len(test_sampler)\n\n        # Horovod: average metric values across workers.\n        test_loss = metric_average(test_loss, \'avg_loss\')\n        test_accuracy = metric_average(test_accuracy, \'avg_accuracy\')\n\n        # Horovod: print output only on first rank.\n        if hvd.rank() == 0:\n            print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n\'.format(\n                test_loss, 100. * test_accuracy))\n\n    for epoch in range(1, epochs + 1):\n        train(epoch)\n        test()\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--hadoop_conf"", type=str,\n                    help=""turn on yarn mode by passing the path to the hadoop""\n                         "" configuration folder. Otherwise, turn on local mode."")\nparser.add_argument(""--slave_num"", type=int, default=2,\n                    help=""The number of slave nodes"")\nparser.add_argument(""--conda_name"", type=str,\n                    help=""The name of conda environment."")\nparser.add_argument(""--iface"", type=str, default=""eth0"")\nparser.add_argument(""--executor_cores"", type=int, default=8,\n                    help=""The number of driver\'s cpu cores you want to use.""\n                         ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--executor_memory"", type=str, default=""10g"",\n                    help=""The size of slave(executor)\'s memory you want to use.""\n                         ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--driver_memory"", type=str, default=""2g"",\n                    help=""The size of driver\'s memory you want to use.""\n                         ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--driver_cores"", type=int, default=8,\n                    help=""The number of driver\'s cpu cores you want to use.""\n                         ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--extra_executor_memory_for_ray"", type=str, default=""20g"",\n                    help=""The extra executor memory to store some data.""\n                         ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--object_store_memory"", type=str, default=""4g"",\n                    help=""The memory to store data on local.""\n                         ""You can change it depending on your own cluster setting."")\n\nif __name__ == ""__main__"":\n\n    args = parser.parse_args()\n    if args.hadoop_conf:\n        sc = init_spark_on_yarn(\n            hadoop_conf=args.hadoop_conf,\n            conda_name=args.conda_name,\n            num_executor=args.slave_num,\n            executor_cores=args.executor_cores,\n            executor_memory=args.executor_memory,\n            driver_memory=args.driver_memory,\n            driver_cores=args.driver_cores,\n            extra_executor_memory_for_ray=args.extra_executor_memory_for_ray)\n        ray_ctx = RayContext(\n            sc=sc,\n            object_store_memory=args.object_store_memory)\n        ray_ctx.init()\n    else:\n        sc = init_spark_on_local()\n        ray_ctx = RayContext(\n            sc=sc,\n            object_store_memory=args.object_store_memory)\n        ray_ctx.init()\n\n    runner = HorovodRayTrainer(ray_ctx)\n    runner.train(func=run_horovod)\n'"
pyzoo/zoo/examples/imageclassification/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/imageclassification/predict.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom optparse import OptionParser\nfrom zoo.common.nncontext import init_nncontext\nfrom zoo.models.image.imageclassification import *\n\n\ndef predict(model_path, img_path, topN, partition_num):\n    print(""ImageClassification prediction"")\n    print(""Model Path %s"" % model_path)\n    print(""Image Path %s"" % img_path)\n    print(""Top N : %d"" % topN)\n    imc = ImageClassifier.load_model(model_path)\n    image_set = ImageSet.read(img_path, sc, partition_num)\n    output = imc.predict_image_set(image_set)\n    labelMap = imc.get_config().label_map()\n    predicts = output.get_predict().collect()\n    for predict in predicts:\n        (uri, probs) = predict\n        sortedProbs = [(prob, index) for index, prob in enumerate(probs[0])]\n        sortedProbs.sort()\n        print(""Image : %s, top %d prediction result"" % (uri, topN))\n        for i in range(topN):\n            print(""\\t%s, %f"" % (labelMap[sortedProbs[999 - i][1]], sortedProbs[999 - i][0]))\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""-f"", ""--folder"", type=str, dest=""img_path"", default=""."",\n                      help=""Path where the images are stored"")\n    parser.add_option(""--model"", type=str, dest=""model_path"", default="""",\n                      help=""Path where the model is stored"")\n    parser.add_option(""--topN"", type=int, dest=""topN"", default=1, help=""top N number"")\n    parser.add_option(""--partition_num"", type=int, dest=""partition_num"", default=4,\n                      help=""The number of partitions"")\n    (options, args) = parser.parse_args(sys.argv)\n\n    sc = init_nncontext(""Image Classification Example"")\n\n    predict(options.model_path, options.img_path, options.topN, options.partition_num)\n    print(""finished..."")\n    sc.stop()\n'"
pyzoo/zoo/examples/inception/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/inception/inception.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom optparse import OptionParser\nfrom math import ceil\nfrom datetime import datetime\n\nfrom zoo.common.nncontext import *\nfrom zoo.feature.image import *\nfrom zoo.pipeline.nnframes import *\nfrom zoo.pipeline.estimator import *\nfrom bigdl.optim.optimizer import *\nfrom bigdl.util.common import *\nfrom bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nfrom bigdl.nn.initialization_method import *\n\n\ndef config_option_parser():\n    parser = OptionParser()\n    parser.add_option(""-f"", ""--folder"", type=str, dest=""folder"", default="""",\n                      help=""url of hdf+s folder store the hadoop sequence files"")\n    parser.add_option(""--model"", type=str, dest=""model"", default="""", help=""model snapshot location"")\n    parser.add_option(""--state"", type=str, dest=""state"", default="""", help=""state snapshot location"")\n    parser.add_option(""--checkpoint"", type=str, dest=""checkpoint"", default="""",\n                      help=""where to cache the model"")\n    parser.add_option(""-o"", ""--overwrite"", action=""store_true"", dest=""overwrite"", default=False,\n                      help=""overwrite checkpoint files"")\n    parser.add_option(""-e"", ""--maxEpoch"", type=int, dest=""maxEpoch"", default=0,\n                      help=""epoch numbers"")\n    parser.add_option(""-i"", ""--maxIteration"", type=int, dest=""maxIteration"", default=3100,\n                      help=""iteration numbers"")\n    parser.add_option(""-l"", ""--learningRate"", type=float, dest=""learningRate"", default=0.01,\n                      help=""learning rate"")\n    parser.add_option(""--warmupEpoch"", type=int, dest=""warmupEpoch"", default=0,\n                      help=""warm up epoch numbers"")\n    parser.add_option(""--maxLr"", type=float, dest=""maxLr"", default=0.0, help=""max Lr after warm up"")\n    parser.add_option(""-b"", ""--batchSize"", type=int, dest=""batchSize"", help=""batch size"")\n    parser.add_option(""--classNum"", type=int, dest=""classNum"", default=1000, help=""class number"")\n    parser.add_option(""--weightDecay"", type=float, dest=""weightDecay"", default=0.0001,\n                      help=""weight decay"")\n    parser.add_option(""--checkpointIteration"", type=int, dest=""checkpointIteration"", default=620,\n                      help=""checkpoint interval of iterations"")\n    parser.add_option(""--gradientMin"", type=float, dest=""gradientMin"", default=0.0,\n                      help=""min gradient clipping by"")\n    parser.add_option(""--gradientMax"", type=float, dest=""gradientMax"", default=0.0,\n                      help=""max gradient clipping by"")\n    parser.add_option(""--gradientL2NormThreshold"", type=float, dest=""gradientL2NormThreshold"",\n                      default=0.0, help=""gradient L2-Norm threshold"")\n    parser.add_option(""--memoryType"", type=str, dest=""memoryType"", default=""DRAM"",\n                      help=""memory storage type, DRAM or PMEM"")\n\n    return parser\n\n\ndef get_inception_data(url, sc=None, data_type=""train""):\n    path = os.path.join(url, data_type)\n    return SeqFileFolder.files_to_image_frame(url=path, sc=sc, class_num=1000)\n\n\ndef t(input_t):\n    if type(input_t) is list:\n        # insert into index 0 spot, such that the real data starts from index 1\n        temp = [0]\n        temp.extend(input_t)\n        return dict(enumerate(temp))\n    # if dictionary, return it back\n    return input_t\n\n\ndef inception_layer_v1(input_size, config, name_prefix=""""):\n    concat = Concat(2)\n    conv1 = Sequential()\n    conv1.add(SpatialConvolution(input_size, config[1][1], 1, 1, 1, 1)\n              .set_init_method(weight_init_method=Xavier(), bias_init_method=ConstInitMethod(0.1))\n              .set_name(name_prefix + ""1x1""))\n    conv1.add(ReLU(True).set_name(name_prefix + ""relu_1x1""))\n    concat.add(conv1)\n    conv3 = Sequential()\n    conv3.add(SpatialConvolution(input_size, config[2][1], 1, 1, 1, 1)\n              .set_init_method(weight_init_method=Xavier(), bias_init_method=ConstInitMethod(0.1))\n              .set_name(name_prefix + ""3x3_reduce""))\n    conv3.add(ReLU(True).set_name(name_prefix + ""relu_3x3_reduce""))\n    conv3.add(SpatialConvolution(config[2][1], config[2][2], 3, 3, 1, 1, 1, 1)\n              .set_init_method(weight_init_method=Xavier(), bias_init_method=ConstInitMethod(0.1))\n              .set_name(name_prefix + ""3x3""))\n    conv3.add(ReLU(True).set_name(name_prefix + ""relu_3x3""))\n    concat.add(conv3)\n    conv5 = Sequential()\n    conv5.add(SpatialConvolution(input_size, config[3][1], 1, 1, 1, 1)\n              .set_init_method(weight_init_method=Xavier(), bias_init_method=ConstInitMethod(0.1))\n              .set_name(name_prefix + ""5x5_reduce""))\n    conv5.add(ReLU(True).set_name(name_prefix + ""relu_5x5_reduce""))\n    conv5.add(SpatialConvolution(config[3][1], config[3][2], 5, 5, 1, 1, 2, 2)\n              .set_init_method(weight_init_method=Xavier(), bias_init_method=ConstInitMethod(0.1))\n              .set_name(name_prefix + ""5x5""))\n    conv5.add(ReLU(True).set_name(name_prefix + ""relu_5x5""))\n    concat.add(conv5)\n    pool = Sequential()\n    pool.add(SpatialMaxPooling(3, 3, 1, 1, 1, 1, to_ceil=True).set_name(name_prefix + ""pool""))\n    pool.add(SpatialConvolution(input_size, config[4][1], 1, 1, 1, 1)\n             .set_init_method(weight_init_method=Xavier(), bias_init_method=ConstInitMethod(0.1))\n             .set_name(name_prefix + ""pool_proj""))\n    pool.add(ReLU(True).set_name(name_prefix + ""relu_pool_proj""))\n    concat.add(pool).set_name(name_prefix + ""output"")\n    return concat\n\n\ndef inception_v1_no_aux_classifier(class_num, has_dropout=True):\n    model = Sequential()\n    model.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1, False)\n              .set_init_method(weight_init_method=Xavier(), bias_init_method=ConstInitMethod(0.1))\n              .set_name(""conv1/7x7_s2""))\n    model.add(ReLU(True).set_name(""conv1/relu_7x7""))\n    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(""pool1/3x3_s2""))\n    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(""pool1/norm1""))\n    model.add(SpatialConvolution(64, 64, 1, 1, 1, 1)\n              .set_init_method(weight_init_method=Xavier(), bias_init_method=ConstInitMethod(0.1))\n              .set_name(""conv2/3x3_reduce""))\n    model.add(ReLU(True).set_name(""conv2/relu_3x3_reduce""))\n    model.add(SpatialConvolution(64, 192, 3, 3, 1, 1, 1, 1)\n              .set_init_method(weight_init_method=Xavier(), bias_init_method=ConstInitMethod(0.1))\n              .set_name(""conv2/3x3""))\n    model.add(ReLU(True).set_name(""conv2/relu_3x3""))\n    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(""conv2/norm2""))\n    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(""pool2/3x3_s2""))\n    model.add(inception_layer_v1(192, t([t([64]), t(\n        [96, 128]), t([16, 32]), t([32])]), ""inception_3a/""))\n    model.add(inception_layer_v1(256, t([t([128]), t(\n        [128, 192]), t([32, 96]), t([64])]), ""inception_3b/""))\n    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n    model.add(inception_layer_v1(480, t([t([192]), t(\n        [96, 208]), t([16, 48]), t([64])]), ""inception_4a/""))\n    model.add(inception_layer_v1(512, t([t([160]), t(\n        [112, 224]), t([24, 64]), t([64])]), ""inception_4b/""))\n    model.add(inception_layer_v1(512, t([t([128]), t(\n        [128, 256]), t([24, 64]), t([64])]), ""inception_4c/""))\n    model.add(inception_layer_v1(512, t([t([112]), t(\n        [144, 288]), t([32, 64]), t([64])]), ""inception_4d/""))\n    model.add(inception_layer_v1(528, t([t([256]), t(\n        [160, 320]), t([32, 128]), t([128])]), ""inception_4e/""))\n    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n    model.add(inception_layer_v1(832, t([t([256]), t(\n        [160, 320]), t([32, 128]), t([128])]), ""inception_5a/""))\n    model.add(inception_layer_v1(832, t([t([384]), t(\n        [192, 384]), t([48, 128]), t([128])]), ""inception_5b/""))\n    model.add(SpatialAveragePooling(7, 7, 1, 1).set_name(""pool5/7x7_s1""))\n    if has_dropout:\n        model.add(Dropout(0.4).set_name(""pool5/drop_7x7_s1""))\n    model.add(View([1024], num_input_dims=3))\n    model.add(Linear(1024, class_num)\n              .set_init_method(weight_init_method=Xavier(), bias_init_method=Zeros())\n              .set_name(""loss3/classifier""))\n    model.add(LogSoftMax().set_name(""loss3/loss3""))\n    model.reset()\n    return model\n\n\nif __name__ == ""__main__"":\n    # parse options\n    parser = config_option_parser()\n    (options, args) = parser.parse_args(sys.argv)\n\n    if not options.learningRate:\n        parser.error(""-l --learningRate is a mandatory opt"")\n    if not options.batchSize:\n        parser.error(""-b --batchSize is a mandatory opt"")\n\n    # init\n    sc = init_nncontext(""inception v1"")\n\n    image_size = 224  # create dataset\n    train_transformer = ChainedPreprocessing([ImagePixelBytesToMat(),\n                                              ImageResize(256, 256),\n                                              ImageRandomCrop(image_size, image_size),\n                                              ImageRandomPreprocessing(ImageHFlip(), 0.5),\n                                              ImageChannelNormalize(123.0, 117.0, 104.0),\n                                              ImageMatToTensor(format=""NCHW"", to_RGB=False),\n                                              ImageSetToSample(input_keys=[""imageTensor""],\n                                                               target_keys=[""label""])\n                                              ])\n\n    raw_train_data = get_inception_data(options.folder, sc, ""train"")\n    train_data = FeatureSet.image_frame(raw_train_data).transform(train_transformer)\n\n    val_transformer = ChainedPreprocessing([ImagePixelBytesToMat(),\n                                            ImageResize(256, 256),\n                                            ImageCenterCrop(image_size, image_size),\n                                            ImageChannelNormalize(123.0, 117.0, 104.0),\n                                            ImageMatToTensor(format=""NCHW"", to_RGB=False),\n                                            ImageSetToSample(input_keys=[""imageTensor""],\n                                                             target_keys=[""label""])\n                                            ])\n\n    raw_val_data = get_inception_data(options.folder, sc, ""val"")\n    val_data = FeatureSet.image_frame(raw_val_data).transform(val_transformer)\n\n    # build model\n    if options.model != """":\n        # load model snapshot\n        inception_model = Model.load(options.model)\n    else:\n        inception_model = inception_v1_no_aux_classifier(options.classNum)\n\n    # set optimization method\n    iterationPerEpoch = int(ceil(float(1281167) / options.batchSize))\n    if options.maxEpoch:\n        maxIteration = iterationPerEpoch * options.maxEpoch\n    else:\n        maxIteration = options.maxIteration\n    warmup_iteration = options.warmupEpoch * iterationPerEpoch\n    if options.state != """":\n        # load state snapshot\n        optim = OptimMethod.load(options.state)\n    else:\n        if warmup_iteration == 0:\n            warmupDelta = 0.0\n        else:\n            if options.maxLr:\n                maxlr = options.maxLr\n            else:\n                maxlr = options.learningRate\n            warmupDelta = (maxlr - options.learningRate)/warmup_iteration\n        polyIteration = maxIteration - warmup_iteration\n        lrSchedule = SequentialSchedule(iterationPerEpoch)\n        lrSchedule.add(Warmup(warmupDelta), warmup_iteration)\n        lrSchedule.add(Poly(0.5, maxIteration), polyIteration)\n        optim = SGD(learningrate=options.learningRate, learningrate_decay=0.0,\n                    weightdecay=options.weightDecay,\n                    momentum=0.9, dampening=0.0, nesterov=False,\n                    leaningrate_schedule=lrSchedule)\n\n    # create triggers\n    if options.maxEpoch:\n        checkpoint_trigger = EveryEpoch()\n        test_trigger = EveryEpoch()\n        end_trigger = MaxEpoch(options.maxEpoch)\n    else:\n        checkpoint_trigger = SeveralIteration(options.checkpointIteration)\n        test_trigger = SeveralIteration(options.checkpointIteration)\n        end_trigger = MaxIteration(options.maxIteration)\n\n    timeStamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")\n\n    # Estimator\n    estimator = Estimator(inception_model, optim_methods=optim, model_dir=options.checkpoint)\n\n    if options.gradientMin and options.gradientMax:\n        estimator.set_constant_gradient_clipping(options.gradientMin, options.gradientMax)\n\n    if options.gradientL2NormThreshold:\n        estimator.set_l2_norm_gradient_clipping(options.gradientL2NormThreshold)\n\n    estimator.train_imagefeature(train_set=train_data,\n                                 criterion=ClassNLLCriterion(),\n                                 end_trigger=end_trigger,\n                                 checkpoint_trigger=checkpoint_trigger,\n                                 validation_set=val_data,\n                                 validation_method=[Top1Accuracy(), Top5Accuracy()],\n                                 batch_size=options.batchSize)\n\n    inception_model.saveModel(""/tmp/inception/model.bigdl"", ""/tmp/inception/model.bin"", True)\n\n    sc.stop()\n'"
pyzoo/zoo/examples/nnframes/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/objectdetection/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/objectdetection/predict.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport argparse\nimport cv2\n\nfrom zoo.common.nncontext import init_nncontext\nfrom zoo.models.image.objectdetection import *\n\n\nsc = init_nncontext(""Object Detection Example"")\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'model_path\', help=""Path where the model is stored"")\nparser.add_argument(\'img_path\', help=""Path where the images are stored"")\nparser.add_argument(\'output_path\', help=""Path to store the detection results"")\nparser.add_argument(""--partition_num"", type=int, default=1, help=""The number of partitions"")\n\n\ndef predict(model_path, img_path, output_path, partition_num):\n    model = ObjectDetector.load_model(model_path)\n    image_set = ImageSet.read(img_path, sc, image_codec=1, min_partitions=partition_num)\n    output = model.predict_image_set(image_set)\n\n    config = model.get_config()\n    visualizer = Visualizer(config.label_map(), encoding=""jpg"")\n    visualized = visualizer(output).get_image(to_chw=False).collect()\n    for img_id in range(len(visualized)):\n        cv2.imwrite(output_path + \'/\' + str(img_id) + \'.jpg\', visualized[img_id])\n\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n    predict(args.model_path, args.img_path, args.output_path, args.partition_num)\n\nprint(""finished..."")\nsc.stop()\n'"
pyzoo/zoo/examples/openvino/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/openvino/predict.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nimport numpy as np\nfrom os.path import join\nfrom optparse import OptionParser\n\nfrom zoo.common.nncontext import init_nncontext\nfrom zoo.feature.image import ImageSet\nfrom zoo.pipeline.inference import InferenceModel\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--image"", type=str, dest=""img_path"",\n                      help=""The path where the images are stored, ""\n                           ""can be either a folder or an image path"")\n    parser.add_option(""--model"", type=str, dest=""model_path"",\n                      help=""Path to the TensorFlow model file"")\n\n    (options, args) = parser.parse_args(sys.argv)\n\n    sc = init_nncontext(""OpenVINO Object Detection Inference Example"")\n    images = ImageSet.read(options.img_path, sc,\n                           resize_height=600, resize_width=600).get_image().collect()\n    input_data = np.concatenate([image.reshape((1, 1) + image.shape) for image in images], axis=0)\n    model_path = options.model_path\n    model = InferenceModel()\n    model.load_openvino(model_path,\n                        weight_path=model_path[:model_path.rindex(""."")] +\n                        "".bin"")\n    predictions = model.predict(input_data)\n    # Print the detection result of the first image.\n    print(predictions[0])\n    print(""finished..."")\n    sc.stop()\n'"
pyzoo/zoo/examples/orca/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/pytorch/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/qaranker/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/qaranker/qa_ranker.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nfrom optparse import OptionParser\n\nfrom bigdl.optim.optimizer import SGD\nfrom zoo.common.nncontext import init_nncontext\nfrom zoo.feature.common import Relations\nfrom zoo.feature.text import TextSet\nfrom zoo.models.textmatching import KNRM\nfrom zoo.pipeline.api.keras.models import Sequential\nfrom zoo.pipeline.api.keras.layers import TimeDistributed\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--data_path"", dest=""data_path"")\n    parser.add_option(""--embedding_file"", dest=""embedding_file"")\n    parser.add_option(""--question_length"", dest=""question_length"", default=""10"")\n    parser.add_option(""--answer_length"", dest=""answer_length"", default=""40"")\n    parser.add_option(""--partition_num"", dest=""partition_num"", default=""4"")\n    parser.add_option(""-b"", ""--batch_size"", dest=""batch_size"", default=""200"")\n    parser.add_option(""-e"", ""--nb_epoch"", dest=""nb_epoch"", default=""30"")\n    parser.add_option(""-l"", ""--learning_rate"", dest=""learning_rate"", default=""0.001"")\n    parser.add_option(""-m"", ""--model"", dest=""model"")\n    parser.add_option(""--output_path"", dest=""output_path"")\n\n    (options, args) = parser.parse_args(sys.argv)\n    sc = init_nncontext(""QARanker Example"")\n\n    q_set = TextSet.read_csv(options.data_path + ""/question_corpus.csv"",\n                             sc, int(options.partition_num)).tokenize().normalize()\\\n        .word2idx(min_freq=2).shape_sequence(int(options.question_length))\n    a_set = TextSet.read_csv(options.data_path+""/answer_corpus.csv"",\n                             sc, int(options.partition_num)).tokenize().normalize()\\\n        .word2idx(min_freq=2, existing_map=q_set.get_word_index())\\\n        .shape_sequence(int(options.answer_length))\n\n    train_relations = Relations.read(options.data_path + ""/relation_train.csv"",\n                                     sc, int(options.partition_num))\n    train_set = TextSet.from_relation_pairs(train_relations, q_set, a_set)\n    validate_relations = Relations.read(options.data_path + ""/relation_valid.csv"",\n                                        sc, int(options.partition_num))\n    validate_set = TextSet.from_relation_lists(validate_relations, q_set, a_set)\n\n    if options.model:\n        knrm = KNRM.load_model(options.model)\n    else:\n        word_index = a_set.get_word_index()\n        knrm = KNRM(int(options.question_length), int(options.answer_length),\n                    options.embedding_file, word_index)\n    model = Sequential().add(\n        TimeDistributed(\n            knrm,\n            input_shape=(2, int(options.question_length) + int(options.answer_length))))\n    model.compile(optimizer=SGD(learningrate=float(options.learning_rate)),\n                  loss=""rank_hinge"")\n    for i in range(0, int(options.nb_epoch)):\n        model.fit(train_set, batch_size=int(options.batch_size), nb_epoch=1)\n        knrm.evaluate_ndcg(validate_set, 3)\n        knrm.evaluate_ndcg(validate_set, 5)\n        knrm.evaluate_map(validate_set)\n\n    if options.output_path:\n        knrm.save_model(options.output_path + ""/knrm.model"")\n        a_set.save_word_index(options.output_path + ""/word_index.txt"")\n        print(""Trained model and word dictionary saved"")\n    sc.stop()\n'"
pyzoo/zoo/examples/ray/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/tensorflow/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/textclassification/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/textclassification/news20.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tarfile\nimport zipfile\nfrom bigdl.dataset import base\nfrom bigdl.util.common import *\n\nNEWS20_URL = \'http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz\'\nGLOVE_URL = \'http://nlp.stanford.edu/data/glove.6B.zip\'\n\n\ndef get_news20(base_dir=""./data/20news-18828""):\n    """"""\n    Parse 20 Newsgroup dataset and return a list of (tokens, label).\n    The dataset will be downloaded automatically if not found in the target base_dir.\n    """"""\n    news20_dir = base_dir + ""/20news-18828/""\n    if not os.path.isdir(news20_dir):\n        download_news20(base_dir)\n    texts = []\n    label_id = 0\n    for category in sorted(os.listdir(news20_dir)):\n        category_dir = os.path.join(news20_dir, category)\n        if os.path.isdir(category_dir):\n            for text_file in sorted(os.listdir(category_dir)):\n                if text_file.isdigit():\n                    text_file_path = os.path.join(category_dir, text_file)\n                    if sys.version_info < (3,):\n                        f = open(text_file_path)\n                    else:\n                        f = open(text_file_path, encoding=\'latin-1\')\n                    content = f.read()\n                    texts.append((content, label_id))\n                    f.close()\n        label_id += 1\n    class_num = label_id\n    print(\'Found %s texts.\' % len(texts))\n    return texts, class_num\n\n\ndef get_glove(base_dir=""./data/glove.6B"", dim=100):\n    """"""\n    Parse the pre-trained glove6B word2vec and return a dict mapping from word to vector,\n    given the dim of a vector.\n    The word embeddings will be downloaded automatically if not found in the target base_dir.\n    """"""\n    glove_dir = base_dir + ""/glove.6B""\n    if not os.path.isdir(glove_dir):\n        download_glove(base_dir)\n    glove_path = os.path.join(glove_dir, ""glove.6B.%sd.txt"" % dim)\n    if sys.version_info < (3,):\n        w2v_f = open(glove_path)\n    else:\n        w2v_f = open(glove_path, encoding=\'latin-1\')\n    pre_w2v = {}\n    for line in w2v_f.readlines():\n        items = line.split("" "")\n        pre_w2v[items[0]] = [float(i) for i in items[1:]]\n    w2v_f.close()\n    return pre_w2v\n\n\ndef download_news20(dest_dir):\n    news20 = ""20news-18828.tar.gz""\n    news20_path = base.maybe_download(news20, dest_dir, NEWS20_URL)\n    tar = tarfile.open(news20_path, ""r:gz"")\n    news20_dir = os.path.join(dest_dir, ""20news-18828"")\n    if not os.path.exists(news20_dir):\n        print(""Extracting %s to %s"" % (news20_path, news20_dir))\n        tar.extractall(dest_dir)\n        tar.close()\n\n\ndef download_glove(dest_dir):\n    glove = ""glove.6B.zip""\n    glove_path = base.maybe_download(glove, dest_dir, GLOVE_URL)\n    zip_ref = zipfile.ZipFile(glove_path, \'r\')\n    glove_dir = os.path.join(dest_dir, ""glove.6B"")\n    if not os.path.exists(glove_dir):\n        print(""Extracting %s to %s"" % (glove_path, glove_dir))\n        zip_ref.extractall(glove_dir)\n        zip_ref.close()\n'"
pyzoo/zoo/examples/textclassification/text_classification.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nimport datetime as dt\nfrom optparse import OptionParser\n\nfrom bigdl.optim.optimizer import Adagrad\nfrom zoo.common.nncontext import init_nncontext\nfrom zoo.feature.text import TextSet\nfrom zoo.models.textclassification import TextClassifier\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--data_path"", dest=""data_path"")\n    parser.add_option(""--embedding_path"", dest=""embedding_path"")\n    parser.add_option(""--class_num"", dest=""class_num"", default=""20"")\n    parser.add_option(""--partition_num"", dest=""partition_num"", default=""4"")\n    parser.add_option(""--token_length"", dest=""token_length"", default=""200"")\n    parser.add_option(""--sequence_length"", dest=""sequence_length"", default=""500"")\n    parser.add_option(""--max_words_num"", dest=""max_words_num"", default=""5000"")\n    parser.add_option(""--encoder"", dest=""encoder"", default=""cnn"")\n    parser.add_option(""--encoder_output_dim"", dest=""encoder_output_dim"", default=""256"")\n    parser.add_option(""--training_split"", dest=""training_split"", default=""0.8"")\n    parser.add_option(""-b"", ""--batch_size"", dest=""batch_size"", default=""128"")\n    parser.add_option(""-e"", ""--nb_epoch"", dest=""nb_epoch"", default=""20"")\n    parser.add_option(""-l"", ""--learning_rate"", dest=""learning_rate"", default=""0.01"")\n    parser.add_option(""--log_dir"", dest=""log_dir"", default=""/tmp/.analytics-zoo"")\n    parser.add_option(""-m"", ""--model"", dest=""model"")\n    parser.add_option(""--output_path"", dest=""output_path"")\n\n    (options, args) = parser.parse_args(sys.argv)\n    sc = init_nncontext(""Text Classification Example"")\n\n    text_set = TextSet.read(path=options.data_path).to_distributed(sc, int(options.partition_num))\n    print(""Processing text dataset..."")\n    transformed = text_set.tokenize().normalize()\\\n        .word2idx(remove_topN=10, max_words_num=int(options.max_words_num))\\\n        .shape_sequence(len=int(options.sequence_length)).generate_sample()\n    train_set, val_set = transformed.random_split(\n        [float(options.training_split), 1 - float(options.training_split)])\n\n    if options.model:\n        model = TextClassifier.load_model(options.model)\n    else:\n        token_length = int(options.token_length)\n        if not (token_length == 50 or token_length == 100\n                or token_length == 200 or token_length == 300):\n            raise ValueError(\'token_length for GloVe can only be 50, 100, 200, 300, but got \'\n                             + str(token_length))\n        embedding_file = options.embedding_path + ""/glove.6B."" + str(token_length) + ""d.txt""\n        word_index = transformed.get_word_index()\n        model = TextClassifier(int(options.class_num), embedding_file,\n                               word_index, int(options.sequence_length),\n                               options.encoder, int(options.encoder_output_dim))\n\n    model.compile(optimizer=Adagrad(learningrate=float(options.learning_rate),\n                                    learningrate_decay=0.001),\n                  loss=""sparse_categorical_crossentropy"",\n                  metrics=[\'accuracy\'])\n    app_name = \'textclassification-\' + dt.datetime.now().strftime(""%Y%m%d-%H%M%S"")\n    model.set_tensorboard(options.log_dir, app_name)\n    model.fit(train_set, batch_size=int(options.batch_size),\n              nb_epoch=int(options.nb_epoch), validation_data=val_set)\n    predict_set = model.predict(val_set, batch_per_thread=int(options.partition_num))\n    # Get the first five prediction probability distributions\n    predicts = predict_set.get_predicts().take(5)\n    print(""Probability distributions of the first five texts in the validation set:"")\n    for predict in predicts:\n        (uri, probs) = predict\n        print(""Prediction for "" + uri + "": "")\n        print(probs)\n\n    if options.output_path:\n        model.save_model(options.output_path + ""/text_classifier.model"")\n        transformed.save_word_index(options.output_path + ""/word_index.txt"")\n        print(""Trained model and word dictionary saved"")\n    sc.stop()\n'"
pyzoo/zoo/feature/image/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .imagePreprocessing import *\nfrom .imageset import *\n'"
pyzoo/zoo/feature/image/imagePreprocessing.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.util.common import *\nfrom zoo.feature.common import Preprocessing\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass ImagePreprocessing(Preprocessing):\n    """"""\n    ImagePreprocessing is a transformer that transform ImageFeature\n    """"""\n    def __init__(self, bigdl_type=""float"", *args):\n        super(ImagePreprocessing, self).__init__(bigdl_type, *args)\n\n\nclass ImageBytesToMat(ImagePreprocessing):\n    """"""\n    Transform byte array(original image file in byte) to OpenCVMat\n    :param byte_key key that maps byte array\n    :param image_codec specifying the color type of a loaded image, same as in OpenCV.imread.\n     By default is Imgcodecs.CV_LOAD_IMAGE_UNCHANGED\n    """"""\n    def __init__(self, byte_key=""bytes"", image_codec=-1, bigdl_type=""float""):\n        super(ImageBytesToMat, self).__init__(bigdl_type, byte_key, image_codec)\n\n\nclass ImagePixelBytesToMat(ImagePreprocessing):\n    """"""\n    Transform byte array(pixels in byte) to OpenCVMat\n    :param byte_key key that maps byte array\n    """"""\n    def __init__(self, byte_key=""bytes"", bigdl_type=""float""):\n        super(ImagePixelBytesToMat, self).__init__(bigdl_type, byte_key)\n\n\nclass ImageResize(ImagePreprocessing):\n    """"""\n    Resize image\n    :param resize_h height after resize\n    :param resize_w width after resize\n    :param resize_mode if resizeMode = -1, random select a mode from (Imgproc.INTER_LINEAR,\n     Imgproc.INTER_CUBIC, Imgproc.INTER_AREA, Imgproc.INTER_NEAREST, Imgproc.INTER_LANCZOS4)\n    :param use_scale_factor if true, scale factor fx and fy is used, fx = fy = 0\n    note that the result of the following are different\n    Imgproc.resize(mat, mat, new Size(resizeWH, resizeWH), 0, 0, Imgproc.INTER_LINEAR)\n    Imgproc.resize(mat, mat, new Size(resizeWH, resizeWH))\n    """"""\n    def __init__(self, resize_h, resize_w, resize_mode=1, use_scale_factor=True,\n                 bigdl_type=""float""):\n        super(ImageResize, self).__init__(bigdl_type, resize_h, resize_w,\n                                          resize_mode, use_scale_factor)\n\n\nclass ImageBrightness(ImagePreprocessing):\n    """"""\n    adjust the image brightness\n    :param deltaLow brightness parameter: low bound\n    :param deltaHigh brightness parameter: high bound\n    """"""\n    def __init__(self, delta_low, delta_high, bigdl_type=""float""):\n        super(ImageBrightness, self).__init__(bigdl_type, float(delta_low), float(delta_high))\n\n\nclass ImageChannelNormalize(ImagePreprocessing):\n    """"""\n    image channel normalize\n    :param mean_r mean value in R channel\n    :param mean_g mean value in G channel\n    :param meanB_b mean value in B channel\n    :param std_r std value in R channel\n    :param std_g std value in G channel\n    :param std_b std value in B channel\n    """"""\n    def __init__(self, mean_r, mean_g, mean_b, std_r=1.0,\n                 std_g=1.0, std_b=1.0, bigdl_type=""float""):\n        super(ImageChannelNormalize, self).__init__(bigdl_type, float(mean_r), float(mean_g),\n                                                    float(mean_b), float(std_r), float(std_g),\n                                                    float(std_b))\n\n\nclass PerImageNormalize(ImagePreprocessing):\n    """"""\n    Normalizes the norm or value range per image, similar to opencv::normalize\n    https://docs.opencv.org/ref/master/d2/de8/group__core__array.html\n    #ga87eef7ee3970f86906d69a92cbf064bd\n    ImageNormalize normalizes scale and shift the input features. Various normalize\n    methods are supported,\n    Eg. NORM_INF, NORM_L1, NORM_L2 or NORM_MINMAX\n    Pleas notice it\'s a per image normalization.\n    :param min lower range boundary in case of the range normalization or\n    norm value to normalize\n    :param max upper range boundary in case of the range normalization.\n    It is not used for the norm normalization.\n    :param norm_type normalization type, see opencv:NormTypes.\n    https://docs.opencv.org/ref/master/d2/de8/group__core__array.html\n    #gad12cefbcb5291cf958a85b4b67b6149f\n    Default Core.NORM_MINMAX\n    """"""\n    def __init__(self, min, max, norm_type=32, bigdl_type=""float""):\n        super(PerImageNormalize, self).__init__(bigdl_type, float(min), float(max), norm_type)\n\n\nclass ImageMatToTensor(ImagePreprocessing):\n    """"""\n    MatToTensor\n    :param toRGB BGR to RGB (default is BGR)\n    :param tensorKey key to store transformed tensor\n    :param format DataFormat.NCHW or DataFormat.NHWC\n    """"""\n    def __init__(self, to_RGB=False, tensor_key=""imageTensor"",\n                 share_buffer=True, format=""NCHW"", bigdl_type=""float""):\n        super(ImageMatToTensor, self).__init__(bigdl_type, to_RGB, tensor_key,\n                                               share_buffer, format)\n\n\nclass ImageSetToSample(ImagePreprocessing):\n    """"""\n    transform imageframe to samples\n    :param input_keys keys that maps inputs (each input should be a tensor)\n    :param target_keys keys that maps targets (each target should be a tensor)\n    :param sample_key key to store sample\n    """"""\n    def __init__(self, input_keys=[""imageTensor""], target_keys=[""label""],\n                 sample_key=""sample"", bigdl_type=""float""):\n        super(ImageSetToSample, self).__init__(bigdl_type, input_keys, target_keys, sample_key)\n\n\nclass ImageHue(ImagePreprocessing):\n    """"""\n    adjust the image hue\n    :param deltaLow hue parameter: low bound\n    :param deltaHigh hue parameter: high bound\n    """"""\n    def __init__(self, delta_low, delta_high, bigdl_type=""float""):\n        super(ImageHue, self).__init__(bigdl_type, float(delta_low), float(delta_high))\n\n\nclass ImageSaturation(ImagePreprocessing):\n    """"""\n    adjust the image Saturation\n    :param deltaLow brightness parameter: low bound\n    :param deltaHigh brightness parameter: high bound\n    """"""\n    def __init__(self, delta_low, delta_high, bigdl_type=""float""):\n        super(ImageSaturation, self).__init__(bigdl_type, float(delta_low), float(delta_high))\n\n\nclass ImageChannelOrder(ImagePreprocessing):\n    """"""\n    random change the channel of an image\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(ImageChannelOrder, self).__init__(bigdl_type)\n\n\nclass ImageColorJitter(ImagePreprocessing):\n    """"""\n    Random adjust brightness, contrast, hue, saturation\n    :param brightness_prob probability to adjust brightness\n    :param brightness_delta brightness parameter\n    :param contrast_prob probability to adjust contrast\n    :param contrast_lower contrast lower parameter\n    :param contrast_upper contrast upper parameter\n    :param hue_prob probability to adjust hue\n    :param hue_delta hue parameter\n    :param saturation_prob probability to adjust saturation\n    :param saturation_lower saturation lower parameter\n    :param saturation_upper saturation upper parameter\n    :param random_order_prob random order for different operation\n    :param shuffle  shuffle the transformers\n    """"""\n    def __init__(self, brightness_prob=0.5,\n                 brightness_delta=32.0,\n                 contrast_prob=0.5,\n                 contrast_lower=0.5,\n                 contrast_upper=1.5,\n                 hue_prob=0.5,\n                 hue_delta=18.0,\n                 saturation_prob=0.5,\n                 saturation_lower=0.5,\n                 saturation_upper=1.5,\n                 random_order_prob=0.0,\n                 shuffle=False,\n                 bigdl_type=""float""):\n        super(ImageColorJitter, self).__init__(bigdl_type,\n                                               float(brightness_prob), float(brightness_delta),\n                                               float(contrast_prob), float(contrast_lower),\n                                               float(contrast_upper), float(hue_prob),\n                                               float(hue_delta), float(saturation_prob),\n                                               float(saturation_lower), float(saturation_upper),\n                                               float(random_order_prob), shuffle)\n\n\nclass ImageAspectScale(ImagePreprocessing):\n    """"""\n    Resize the image, keep the aspect ratio. scale according to the short edge\n    :param min_size scale size, apply to short edge\n    :param scale_multiple_of make the scaled size multiple of some value\n    :param max_size max size after scale\n    :param resize_mode if resizeMode = -1, random select a mode from\n    (Imgproc.INTER_LINEAR, Imgproc.INTER_CUBIC, Imgproc.INTER_AREA,\n    Imgproc.INTER_NEAREST, Imgproc.INTER_LANCZOS4)\n    :param use_scale_factor if true, scale factor fx and fy is used, fx = fy = 0\n    :aram min_scale control the minimum scale up for image\n    """"""\n\n    def __init__(self, min_size, scale_multiple_of=1, max_size=1000,\n                 resize_mode=1, use_scale_factor=True, min_scale=-1.0,\n                 bigdl_type=""float""):\n        super(ImageAspectScale, self).__init__(bigdl_type,\n                                               min_size, scale_multiple_of, max_size,\n                                               resize_mode, use_scale_factor, min_scale)\n\n\nclass ImageRandomAspectScale(ImagePreprocessing):\n    """"""\n    resize the image by randomly choosing a scale\n    :param scales array of scale options that for random choice\n    :param scaleMultipleOf Resize test images so that its width and height are multiples of\n    :param maxSize Max pixel size of the longest side of a scaled input image\n    """"""\n    def __init__(self, scales, scale_multiple_of=1, max_size=1000, bigdl_type=""float""):\n        super(ImageRandomAspectScale, self).__init__(bigdl_type,\n                                                     scales, scale_multiple_of, max_size)\n\n\nclass ImagePixelNormalize(ImagePreprocessing):\n    """"""\n    Pixel level normalizer, data(i) = data(i) - mean(i)\n\n    :param means pixel level mean, following H * W * C order\n    """"""\n\n    def __init__(self, means, bigdl_type=""float""):\n        super(ImagePixelNormalize, self).__init__(bigdl_type, means)\n\n\nclass ImageRandomCrop(ImagePreprocessing):\n    """"""\n    Random crop a `cropWidth` x `cropHeight` patch from an image.\n    The patch size should be less than the image size.\n\n    :param crop_width width after crop\n    :param crop_height height after crop\n    :param is_clip whether to clip the roi to image boundaries\n    """"""\n\n    def __init__(self, crop_width, crop_height, is_clip=True, bigdl_type=""float""):\n        super(ImageRandomCrop, self).__init__(bigdl_type,\n                                              crop_width, crop_height, is_clip)\n\n\nclass ImageCenterCrop(ImagePreprocessing):\n    """"""\n    Crop a `cropWidth` x `cropHeight` patch from center of image.\n    The patch size should be less than the image size.\n    :param crop_width width after crop\n    :param crop_height height after crop\n    :param is_clip  clip cropping box boundary\n    """"""\n\n    def __init__(self, crop_width, crop_height, is_clip=True, bigdl_type=""float""):\n        super(ImageCenterCrop, self).__init__(bigdl_type,\n                                              crop_width, crop_height, is_clip)\n\n\nclass ImageFixedCrop(ImagePreprocessing):\n    """"""\n    Crop a fixed area of image\n\n    :param x1 start in width\n    :param y1 start in height\n    :param x2 end in width\n    :param y2 end in height\n    :param normalized whether args are normalized, i.e. in range [0, 1]\n    :param is_clip whether to clip the roi to image boundaries\n    """"""\n\n    def __init__(self, x1, y1, x2, y2, normalized=True, is_clip=True, bigdl_type=""float""):\n        super(ImageFixedCrop, self).__init__(bigdl_type,\n                                             x1, y1, x2, y2, normalized, is_clip)\n\n\nclass ImageExpand(ImagePreprocessing):\n    """"""\n    expand image, fill the blank part with the meanR, meanG, meanB\n\n    :param means_r means in R channel\n    :param means_g means in G channel\n    :param means_b means in B channel\n    :param min_expand_ratio min expand ratio\n    :param max_expand_ratio max expand ratio\n    """"""\n\n    def __init__(self, means_r=123, means_g=117, means_b=104,\n                 min_expand_ratio=1.0,\n                 max_expand_ratio=4.0, bigdl_type=""float""):\n        super(ImageExpand, self).__init__(bigdl_type, means_r, means_g, means_b,\n                                          min_expand_ratio, max_expand_ratio)\n\n\nclass ImageFiller(ImagePreprocessing):\n    """"""\n    Fill part of image with certain pixel value\n    :param start_x start x ratio\n    :param start_y start y ratio\n    :param end_x end x ratio\n    :param end_y end y ratio\n    :param value filling value\n    """"""\n\n    def __init__(self, start_x, start_y, end_x, end_y, value=255, bigdl_type=""float""):\n        super(ImageFiller, self).__init__(bigdl_type, start_x, start_y,\n                                          end_x, end_y, value)\n\n\nclass ImageHFlip(ImagePreprocessing):\n    """"""\n    Flip the image horizontally\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        super(ImageHFlip, self).__init__(bigdl_type)\n\n\nclass ImageMirror(ImagePreprocessing):\n    """"""\n    Flip the image horizontally and vertically\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(ImageMirror, self).__init__(bigdl_type)\n\n\nclass ImageFeatureToTensor(Preprocessing):\n    """"""\n    a Transformer that convert ImageFeature to a Tensor.\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(ImageFeatureToTensor, self).__init__(bigdl_type)\n\n\nclass ImageFeatureToSample(Preprocessing):\n    """"""\n    A transformer that get Sample from ImageFeature.\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(ImageFeatureToSample, self).__init__(bigdl_type)\n\n\nclass RowToImageFeature(Preprocessing):\n    """"""\n    a Transformer that converts a Spark Row to a BigDL ImageFeature.\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(RowToImageFeature, self).__init__(bigdl_type)\n\n\nclass ImageRandomPreprocessing(Preprocessing):\n    """"""\n    Randomly apply the preprocessing to some of the input ImageFeatures, with probability specified.\n    E.g. if prob = 0.5, the preprocessing will apply to half of the input ImageFeatures.\n    :param preprocessing preprocessing to apply.\n    :param prob probability to apply the preprocessing action.\n    """"""\n\n    def __init__(self, preprocessing, prob, bigdl_type=""float""):\n        super(ImageRandomPreprocessing, self).__init__(bigdl_type, preprocessing, float(prob))\n'"
pyzoo/zoo/feature/image/imageset.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom bigdl.transform.vision.image import ImageFrame\nfrom bigdl.util.common import *\nfrom zoo.common.utils import callZooFunc\n\n\nclass ImageSet(JavaValue):\n    """"""\n    ImageSet wraps a set of ImageFeature\n    """"""\n\n    def __init__(self, jvalue, bigdl_type=""float""):\n        self.value = jvalue\n        self.bigdl_type = bigdl_type\n        if self.is_local():\n            self.image_set = LocalImageSet(jvalue=self.value)\n        else:\n            self.image_set = DistributedImageSet(jvalue=self.value)\n\n    def is_local(self):\n        """"""\n        whether this is a LocalImageSet\n        """"""\n        return callZooFunc(self.bigdl_type, ""isLocalImageSet"", self.value)\n\n    def is_distributed(self):\n        """"""\n        whether this is a DistributedImageSet\n        """"""\n        return callZooFunc(self.bigdl_type, ""isDistributedImageSet"", self.value)\n\n    @property\n    def label_map(self):\n        """"""\n        :return: the labelMap of this ImageSet, None if the ImageSet does not have a labelMap\n        """"""\n        return callZooFunc(self.bigdl_type, ""imageSetGetLabelMap"", self.value)\n\n    @classmethod\n    def read(cls, path, sc=None, min_partitions=1, resize_height=-1,\n             resize_width=-1, image_codec=-1, with_label=False, one_based_label=True,\n             bigdl_type=""float""):\n        """"""\n        Read images as Image Set\n        if sc is defined, Read image as DistributedImageSet from local file system or HDFS\n        if sc is null, Read image as LocalImageSet from local file system\n        :param path path to read images\n        if sc is defined, path can be local or HDFS. Wildcard character are supported.\n        if sc is null, path is local directory/image file/image file with wildcard character\n\n        if withLabel is set to true, path should be a directory that have two levels. The\n        first level is class folders, and the second is images. All images belong to a same\n        class should be put into the same class folder. So each image in the path is labeled by the\n        folder it belongs.\n\n        :param sc SparkContext\n        :param min_partitions A suggestion value of the minimal splitting number for input data.\n        :param resize_height height after resize, by default is -1 which will not resize the image\n        :param resize_width width after resize, by default is -1 which will not resize the image\n        :param image_codec specifying the color type of a loaded image, same as in OpenCV.imread.\n               By default is Imgcodecs.CV_LOAD_IMAGE_UNCHANGED(-1)\n        :param with_label whether to treat folders in the path as image classification labels\n               and read the labels into ImageSet.\n        :param one_based_label whether to use one based label\n        :return ImageSet\n        """"""\n        return ImageSet(jvalue=callZooFunc(bigdl_type, ""readImageSet"", path,\n                                           sc, min_partitions, resize_height,\n                                           resize_width, image_codec, with_label,\n                                           one_based_label))\n\n    @classmethod\n    def from_image_frame(cls, image_frame, bigdl_type=""float""):\n        return ImageSet(jvalue=callZooFunc(bigdl_type, ""imageFrameToImageSet"", image_frame))\n\n    @classmethod\n    def from_rdds(cls, image_rdd, label_rdd=None, bigdl_type=""float""):\n        """"""\n        Create a ImageSet from rdds of ndarray.\n\n        :param image_rdd: a rdd of ndarray, each ndarray should has dimension of 3 or 4 (3D images)\n        :param label_rdd: a rdd of ndarray\n        :return: a DistributedImageSet\n        """"""\n        image_rdd = image_rdd.map(lambda x: JTensor.from_ndarray(x))\n        if label_rdd is not None:\n            label_rdd = label_rdd.map(lambda x: JTensor.from_ndarray(x))\n        return ImageSet(jvalue=callZooFunc(bigdl_type, ""createDistributedImageSet"",\n                                           image_rdd, label_rdd), bigdl_type=bigdl_type)\n\n    def transform(self, transformer):\n        """"""\n        transformImageSet\n        """"""\n        return ImageSet(callZooFunc(self.bigdl_type, ""transformImageSet"",\n                                    transformer, self.value), self.bigdl_type)\n\n    def get_image(self, key=""floats"", to_chw=True):\n        """"""\n        get image from ImageSet\n        """"""\n        return self.image_set.get_image(key, to_chw)\n\n    def get_label(self):\n        """"""\n        get label from ImageSet\n        """"""\n        return self.image_set.get_label()\n\n    def get_predict(self, key=""predict""):\n        """"""\n        get prediction from ImageSet\n        """"""\n        return self.image_set.get_predict(key)\n\n    def to_image_frame(self, bigdl_type=""float""):\n        return ImageFrame(callZooFunc(bigdl_type, ""imageSetToImageFrame"", self.value), bigdl_type)\n\n\nclass LocalImageSet(ImageSet):\n    """"""\n    LocalImageSet wraps a list of ImageFeature\n    """"""\n\n    def __init__(self, image_list=None, label_list=None, jvalue=None, bigdl_type=""float""):\n        assert jvalue or image_list, ""jvalue and image_list cannot be None in the same time""\n        if jvalue:\n            self.value = jvalue\n        else:\n            # init from image ndarray list and label rdd(optional)\n            image_tensor_list = list(map(lambda image: JTensor.from_ndarray(image), image_list))\n            label_tensor_list = list(map(lambda label: JTensor.from_ndarray(label), label_list)) \\\n                if label_list else None\n            self.value = callZooFunc(bigdl_type, JavaValue.jvm_class_constructor(self),\n                                     image_tensor_list, label_tensor_list)\n        self.bigdl_type = bigdl_type\n\n    def get_image(self, key=""floats"", to_chw=True):\n        """"""\n        get image list from ImageSet\n        """"""\n        tensors = callZooFunc(self.bigdl_type, ""localImageSetToImageTensor"",\n                              self.value, key, to_chw)\n        return list(map(lambda tensor: tensor.to_ndarray(), tensors))\n\n    def get_label(self):\n        """"""\n        get label list from ImageSet\n        """"""\n        labels = callZooFunc(self.bigdl_type, ""localImageSetToLabelTensor"", self.value)\n        return map(lambda tensor: tensor.to_ndarray(), labels)\n\n    def get_predict(self, key=""predict""):\n        """"""\n        get prediction list from ImageSet\n        """"""\n        predicts = callZooFunc(self.bigdl_type, ""localImageSetToPredict"", self.value, key)\n        return list(map(lambda predict:\n                        (predict[0], list(map(lambda x: x.to_ndarray(), predict[1]))) if predict[1]\n                        else (predict[0], None), predicts))\n\n\nclass DistributedImageSet(ImageSet):\n    """"""\n    DistributedImageSet wraps an RDD of ImageFeature\n    """"""\n\n    def __init__(self, image_rdd=None, label_rdd=None, jvalue=None, bigdl_type=""float""):\n        assert jvalue or image_rdd, ""jvalue and image_rdd cannot be None in the same time""\n        if jvalue:\n            self.value = jvalue\n        else:\n            # init from image ndarray rdd and label rdd(optional)\n            image_tensor_rdd = image_rdd.map(lambda image: JTensor.from_ndarray(image))\n            label_tensor_rdd = label_rdd.map(lambda label: JTensor.from_ndarray(label)) \\\n                if label_rdd else None\n            self.value = callZooFunc(bigdl_type, JavaValue.jvm_class_constructor(self),\n                                     image_tensor_rdd, label_tensor_rdd)\n        self.bigdl_type = bigdl_type\n\n    def get_image(self, key=""floats"", to_chw=True):\n        """"""\n        get image rdd from ImageSet\n        """"""\n        tensor_rdd = callZooFunc(self.bigdl_type, ""distributedImageSetToImageTensorRdd"",\n                                 self.value, key, to_chw)\n        return tensor_rdd.map(lambda tensor: tensor.to_ndarray())\n\n    def get_label(self):\n        """"""\n        get label rdd from ImageSet\n        """"""\n        tensor_rdd = callZooFunc(self.bigdl_type, ""distributedImageSetToLabelTensorRdd"",\n                                 self.value)\n        return tensor_rdd.map(lambda tensor: tensor.to_ndarray())\n\n    def get_predict(self, key=""predict""):\n        """"""\n        get prediction rdd from ImageSet\n        """"""\n        predicts = callZooFunc(self.bigdl_type, ""distributedImageSetToPredict"", self.value, key)\n        return predicts.map(lambda predict:\n                            (predict[0],\n                             list(map(lambda x: x.to_ndarray(), predict[1]))) if predict[1]\n                            else (predict[0], None))\n'"
pyzoo/zoo/feature/image3d/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/feature/image3d/transformation.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nimport numpy as np\n\nfrom bigdl.util.common import *\nfrom zoo.feature.image.imagePreprocessing import *\nfrom zoo.feature.image.imageset import *\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass ImagePreprocessing3D(ImagePreprocessing):\n    """"""\n    ImagePreprocessing3D is a transformer that transform ImageFeature for 3D image\n    """"""\n    def __init__(self, bigdl_type=""float"", *args):\n        super(ImagePreprocessing3D, self).__init__(bigdl_type, *args)\n\n\nclass Crop3D(ImagePreprocessing3D):\n    """"""\n    Crop a patch from a 3D image from \'start\' of patch size.\n    The patch size should be less than the image size.\n\n    :param start start point list[depth, height, width] for cropping\n    :param patchSize patch size list[depth, height, width]\n    """"""\n    def __init__(self, start, patch_size, bigdl_type=""float""):\n        super(Crop3D, self).__init__(bigdl_type, start, patch_size)\n\n\nclass RandomCrop3D(ImagePreprocessing3D):\n    """"""\n    Random crop a `cropDepth` x `cropHeight` x `cropWidth` patch from an image.\n    The patch size should be less than the image size.\n\n    :param crop_depth depth after crop\n    :param crop_height height after crop\n    :param crop_width width after crop\n    """"""\n    def __init__(self, crop_depth, crop_height, crop_width, bigdl_type=""float""):\n        super(RandomCrop3D, self).__init__(bigdl_type, crop_depth, crop_height, crop_width)\n\n\nclass CenterCrop3D(ImagePreprocessing3D):\n    """"""\n    Center crop a `cropDepth` x `cropHeight` x `cropWidth` patch from an image.\n    The patch size should be less than the image size.\n\n    :param crop_depth depth after crop\n    :param crop_height height after crop\n    :param crop_width width after crop\n    """"""\n    def __init__(self, crop_depth, crop_height, crop_width, bigdl_type=""float""):\n        super(CenterCrop3D, self).__init__(bigdl_type, crop_depth, crop_height, crop_width)\n\n\nclass Rotate3D(ImagePreprocessing3D):\n    """"""\n    Rotate a 3D image with specified angles.\n\n    :param rotation_angles the angles for rotation.\n    Which are the yaw(a counterclockwise rotation angle about the z-axis),\n    pitch(a counterclockwise rotation angle about the y-axis),\n    and roll(a counterclockwise rotation angle about the x-axis).\n    """"""\n    def __init__(self, rotation_angles, bigdl_type=""float""):\n        super(Rotate3D, self).__init__(bigdl_type, rotation_angles)\n\n\nclass AffineTransform3D(ImagePreprocessing3D):\n    """"""\n    Affine transformer implements affine transformation on a given tensor.\n    To avoid defects in resampling, the mapping is from destination to source.\n    dst(z,y,x) = src(f(z),f(y),f(x)) where f: dst -> src\n    :param affine_mat: numpy array in 3x3 shape.Define affine transformation from dst to src.\n    :param translation: numpy array in 3 dimension.Default value is np.zero(3).\n            Define translation in each axis.\n    :param clampMode: str, default value is ""clamp"".\n            Define how to handle interpolation off the input image.\n    :param padVal: float, default is 0.0. Define padding value when clampMode=""padding"".\n            Setting this value when clampMode=""clamp"" will cause an error.\n    """"""\n\n    def __init__(self, affine_mat, translation=np.zeros(3), clamp_mode=""clamp"",\n                 pad_val=0.0, bigdl_type=""float""):\n        affine_mat_tensor = JTensor.from_ndarray(affine_mat)\n        translation_tensor = JTensor.from_ndarray(translation)\n        super(AffineTransform3D, self).__init__(bigdl_type, affine_mat_tensor, translation_tensor,\n                                                clamp_mode, pad_val)\n'"
pyzoo/zoo/feature/text/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .text_feature import *\nfrom .text_set import *\nfrom .transformer import *\n'"
pyzoo/zoo/feature/text/text_feature.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nimport six\nfrom bigdl.util.common import JavaValue\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass TextFeature(JavaValue):\n    """"""\n    Each TextFeature keeps information of a single text record.\n    It can include various status (if any) of a text,\n    e.g. original text content, uri, category label, tokens, index representation\n    of tokens, BigDL Sample representation, prediction result and so on.\n    """"""\n\n    def __init__(self, text=None, label=None, uri=None, jvalue=None, bigdl_type=""float""):\n        if text is not None:\n            assert isinstance(text, six.string_types), ""text of a TextFeature should be a string""\n        if uri is not None:\n            assert isinstance(uri, six.string_types), ""uri of a TextFeature should be a string""\n        if label is not None:\n            super(TextFeature, self).__init__(jvalue, bigdl_type, text, int(label), uri)\n        else:\n            super(TextFeature, self).__init__(jvalue, bigdl_type, text, uri)\n\n    def get_text(self):\n        """"""\n        Get the text content of the TextFeature.\n\n        :return: String\n        """"""\n        return callZooFunc(self.bigdl_type, ""textFeatureGetText"", self.value)\n\n    def get_label(self):\n        """"""\n        Get the label of the TextFeature.\n        If no label is stored, -1 will be returned.\n\n        :return: Int\n        """"""\n        return callZooFunc(self.bigdl_type, ""textFeatureGetLabel"", self.value)\n\n    def get_uri(self):\n        """"""\n        Get the identifier of the TextFeature.\n        If no id is stored, None will be returned.\n\n        :return: String\n        """"""\n        return callZooFunc(self.bigdl_type, ""textFeatureGetURI"", self.value)\n\n    def has_label(self):\n        """"""\n        Whether the TextFeature contains label.\n\n        :return: Boolean\n        """"""\n        return callZooFunc(self.bigdl_type, ""textFeatureHasLabel"", self.value)\n\n    def set_label(self, label):\n        """"""\n        Set the label for the TextFeature.\n\n        :param label: Int\n        :return: The TextFeature with label.\n        """"""\n        self.value = callZooFunc(self.bigdl_type, ""textFeatureSetLabel"", self.value, int(label))\n        return self\n\n    def get_tokens(self):\n        """"""\n        Get the tokens of the TextFeature.\n        If text hasn\'t been segmented, None will be returned.\n\n        :return: List of String\n        """"""\n        return callZooFunc(self.bigdl_type, ""textFeatureGetTokens"", self.value)\n\n    def get_sample(self):\n        """"""\n        Get the Sample representation of the TextFeature.\n        If the TextFeature hasn\'t been transformed to Sample, None will be returned.\n\n        :return: BigDL Sample\n        """"""\n        return callZooFunc(self.bigdl_type, ""textFeatureGetSample"", self.value)\n\n    def keys(self):\n        """"""\n        Get the keys that the TextFeature contains.\n\n        :return: List of String\n        """"""\n        return callZooFunc(self.bigdl_type, ""textFeatureGetKeys"", self.value)\n'"
pyzoo/zoo/feature/text/text_set.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport six\nfrom bigdl.util.common import JavaValue\nfrom zoo.common.utils import callZooFunc\nfrom pyspark import RDD\n\n\nclass TextSet(JavaValue):\n    """"""\n    TextSet wraps a set of texts with status.\n    """"""\n\n    def __init__(self, jvalue, bigdl_type=""float"", *args):\n        super(TextSet, self).__init__(jvalue, bigdl_type, *args)\n\n    def is_local(self):\n        """"""\n        Whether it is a LocalTextSet.\n\n        :return: Boolean\n        """"""\n        return callZooFunc(self.bigdl_type, ""textSetIsLocal"", self.value)\n\n    def is_distributed(self):\n        """"""\n        Whether it is a DistributedTextSet.\n\n        :return: Boolean\n        """"""\n        return callZooFunc(self.bigdl_type, ""textSetIsDistributed"", self.value)\n\n    def to_distributed(self, sc=None, partition_num=4):\n        """"""\n        Convert to a DistributedTextSet.\n\n        Need to specify SparkContext to convert a LocalTextSet to a DistributedTextSet.\n        In this case, you may also want to specify partition_num, the default of which is 4.\n\n        :return: DistributedTextSet\n        """"""\n        if self.is_distributed():\n            jvalue = self.value\n        else:\n            assert sc, ""sc cannot be null to transform a LocalTextSet to a DistributedTextSet""\n            jvalue = callZooFunc(self.bigdl_type, ""textSetToDistributed"", self.value,\n                                 sc, partition_num)\n        return DistributedTextSet(jvalue=jvalue)\n\n    def to_local(self):\n        """"""\n        Convert to a LocalTextSet.\n\n        :return: LocalTextSet\n        """"""\n        if self.is_local():\n            jvalue = self.value\n        else:\n            jvalue = callZooFunc(self.bigdl_type, ""textSetToLocal"", self.value)\n        return LocalTextSet(jvalue=jvalue)\n\n    def get_word_index(self):\n        """"""\n        Get the word_index dictionary of the TextSet.\n        If the TextSet hasn\'t been transformed from word to index, None will be returned.\n\n        :return: Dictionary {word: id}\n        """"""\n        return callZooFunc(self.bigdl_type, ""textSetGetWordIndex"", self.value)\n\n    def save_word_index(self, path):\n        """"""\n        Save the word_index dictionary to text file, which can be used for future inference.\n        Each separate line will be ""word id"".\n\n        For LocalTextSet, save txt to a local file system.\n        For DistributedTextSet, save txt to a local or distributed file system (such as HDFS).\n\n        :param path: The path to the text file.\n        """"""\n        callZooFunc(self.bigdl_type, ""textSetSaveWordIndex"", self.value, path)\n\n    def load_word_index(self, path):\n        """"""\n        Load the word_index map which was saved after the training, so that this TextSet can\n        directly use this word_index during inference.\n        Each separate line should be ""word id"".\n\n        Note that after calling `load_word_index`, you do not need to specify any argument when\n        calling `word2idx` in the preprocessing pipeline as now you are using exactly the loaded\n        word_index for transformation.\n\n        For LocalTextSet, load txt from a local file system.\n        For DistributedTextSet, load txt from a local or distributed file system (such as HDFS).\n\n        :return: TextSet with the loaded word_index.\n        """"""\n        jvalue = callZooFunc(self.bigdl_type, ""textSetLoadWordIndex"", self.value, path)\n        return TextSet(jvalue=jvalue)\n\n    def set_word_index(self, vocab):\n        """"""\n        Assign a word_index dictionary for this TextSet to use during word2idx.\n        If you load the word_index from the saved file, you are recommended to use `load_word_index`\n        directly.\n\n        :return: TextSet with the word_index set.\n        """"""\n        jvalue = callZooFunc(self.bigdl_type, ""textSetSetWordIndex"", self.value, vocab)\n        return TextSet(jvalue=jvalue)\n\n    def generate_word_index_map(self, remove_topN=0, max_words_num=-1,\n                                min_freq=1, existing_map=None):\n        """"""\n        Generate word_index map based on sorted word frequencies in descending order.\n        Return the result dictionary, which can also be retrieved by \'get_word_index()\'.\n        Make sure you call this after tokenize. Otherwise you will get an error.\n        See word2idx for more details.\n\n        :return: Dictionary {word: id}\n        """"""\n        return callZooFunc(self.bigdl_type, ""textSetGenerateWordIndexMap"", self.value,\n                           remove_topN, max_words_num, min_freq, existing_map)\n\n    def get_texts(self):\n        """"""\n        Get the text contents of a TextSet.\n\n        :return: List of String for LocalTextSet.\n                 RDD of String for DistributedTextSet.\n        """"""\n        return callZooFunc(self.bigdl_type, ""textSetGetTexts"", self.value)\n\n    def get_uris(self):\n        """"""\n        Get the identifiers of a TextSet.\n        If a text doesn\'t have a uri, its corresponding position will be None.\n\n        :return: List of String for LocalTextSet.\n                 RDD of String for DistributedTextSet.\n        """"""\n        return callZooFunc(self.bigdl_type, ""textSetGetURIs"", self.value)\n\n    def get_labels(self):\n        """"""\n        Get the labels of a TextSet (if any).\n        If a text doesn\'t have a label, its corresponding position will be -1.\n\n        :return: List of int for LocalTextSet.\n                 RDD of int for DistributedTextSet.\n        """"""\n        return callZooFunc(self.bigdl_type, ""textSetGetLabels"", self.value)\n\n    def get_predicts(self):\n        """"""\n        Get the prediction results (if any) combined with uris (if any) of a TextSet.\n        If a text doesn\'t have a uri, its corresponding uri will be None.\n        If a text hasn\'t been predicted by a model, its corresponding prediction will be None.\n\n        :return: List of (uri, prediction as a list of numpy array) for LocalTextSet.\n                 RDD of (uri, prediction as a list of numpy array) for DistributedTextSet.\n        """"""\n        predicts = callZooFunc(self.bigdl_type, ""textSetGetPredicts"", self.value)\n        if isinstance(predicts, RDD):\n            return predicts.map(lambda predict: (predict[0], _process_predict_result(predict[1])))\n        else:\n            return [(predict[0], _process_predict_result(predict[1])) for predict in predicts]\n\n    def get_samples(self):\n        """"""\n        Get the BigDL Sample representations of a TextSet (if any).\n        If a text hasn\'t been transformed to Sample, its corresponding position will be None.\n\n        :return: List of Sample for LocalTextSet.\n                 RDD of Sample for DistributedTextSet.\n        """"""\n        return callZooFunc(self.bigdl_type, ""textSetGetSamples"", self.value)\n\n    def random_split(self, weights):\n        """"""\n        Randomly split into list of TextSet with provided weights.\n        Only available for DistributedTextSet for now.\n\n        :param weights: List of float indicating the split portions.\n        """"""\n        jvalues = callZooFunc(self.bigdl_type, ""textSetRandomSplit"", self.value, weights)\n        return [TextSet(jvalue=jvalue) for jvalue in list(jvalues)]\n\n    def tokenize(self):\n        """"""\n        Do tokenization on original text.\n        See Tokenizer for more details.\n\n        :return: TextSet after tokenization.\n        """"""\n        jvalue = callZooFunc(self.bigdl_type, ""textSetTokenize"", self.value)\n        return TextSet(jvalue=jvalue)\n\n    def normalize(self):\n        """"""\n        Do normalization on tokens.\n        Need to tokenize first.\n        See Normalizer for more details.\n\n        :return: TextSet after normalization.\n        """"""\n        jvalue = callZooFunc(self.bigdl_type, ""textSetNormalize"", self.value)\n        return TextSet(jvalue=jvalue)\n\n    def word2idx(self, remove_topN=0, max_words_num=-1, min_freq=1, existing_map=None):\n        """"""\n        Map word tokens to indices.\n        Important: Take care that this method behaves a bit differently for training and inference.\n\n        ---------------------------------------Training--------------------------------------------\n        During the training, you need to generate a new word_index dictionary according to the texts\n        you are dealing with. Thus this method will first do the dictionary generation and then\n        convert words to indices based on the generated dictionary.\n\n        You can specify the following arguments which pose some constraints when generating\n        the dictionary.\n        In the result dictionary, index will start from 1 and corresponds to the occurrence\n        frequency of each word sorted in descending order.\n        Here we adopt the convention that index 0 will be reserved for unknown words.\n        After word2idx, you can get the generated word_index dictionary by calling \'get_word_index\'.\n        Also, you can call `save_word_index` to save this word_index dictionary to be used in\n        future training.\n\n        :param remove_topN: Non-negative int. Remove the topN words with highest frequencies\n                            in the case where those are treated as stopwords.\n                            Default is 0, namely remove nothing.\n        :param max_words_num: Int. The maximum number of words to be taken into consideration.\n                              Default is -1, namely all words will be considered.\n                              Otherwise, it should be a positive int.\n        :param min_freq: Positive int. Only those words with frequency >= min_freq will be taken\n                         into consideration.\n                         Default is 1, namely all words that occur will be considered.\n        :param existing_map: Existing dictionary of word_index if any.\n                             Default is None and in this case a new dictionary with index starting\n                             from 1 will be generated.\n                             If not None, then the generated dictionary will preserve the word_index\n                             in existing_map and assign subsequent indices to new words.\n\n        ---------------------------------------Inference--------------------------------------------\n        During the inference, you are supposed to use exactly the same word_index dictionary as in\n        the training stage instead of generating a new one.\n        Thus please be aware that you do not need to specify any of the above arguments.\n        You need to call `load_word_index` or `set_word_index` beforehand for dictionary loading.\n\n        Need to tokenize first.\n        See WordIndexer for more details.\n\n        :return: TextSet after word2idx.\n        """"""\n        jvalue = callZooFunc(self.bigdl_type, ""textSetWord2idx"", self.value,\n                             remove_topN, max_words_num, min_freq, existing_map)\n        return TextSet(jvalue=jvalue)\n\n    def shape_sequence(self, len, trunc_mode=""pre"", pad_element=0):\n        """"""\n        Shape the sequence of indices to a fixed length.\n        Need to word2idx first.\n        See SequenceShaper for more details.\n\n        :return: TextSet after sequence shaping.\n        """"""\n        assert isinstance(pad_element, int), ""pad_element should be an int""\n        jvalue = callZooFunc(self.bigdl_type, ""textSetShapeSequence"", self.value,\n                             len, trunc_mode, pad_element)\n        return TextSet(jvalue=jvalue)\n\n    def generate_sample(self):\n        """"""\n        Generate BigDL Sample.\n        Need to word2idx first.\n        See TextFeatureToSample for more details.\n\n        :return: TextSet with Samples.\n        """"""\n        jvalue = callZooFunc(self.bigdl_type, ""textSetGenerateSample"", self.value)\n        return TextSet(jvalue=jvalue)\n\n    def transform(self, transformer):\n        return TextSet(callZooFunc(self.bigdl_type, ""transformTextSet"",\n                                   transformer, self.value), self.bigdl_type)\n\n    @classmethod\n    def read(cls, path, sc=None, min_partitions=1, bigdl_type=""float""):\n        """"""\n        Read text files with labels from a directory.\n        The folder structure is expected to be the following:\n        path\n          |dir1 - text1, text2, ...\n          |dir2 - text1, text2, ...\n          |dir3 - text1, text2, ...\n        Under the target path, there ought to be N subdirectories (dir1 to dirN). Each\n        subdirectory represents a category and contains all texts that belong to such\n        category. Each category will be a given a label according to its position in the\n        ascending order sorted among all subdirectories.\n        All texts will be given a label according to the subdirectory where it is located.\n        Labels start from 0.\n\n        :param path: The folder path to texts. Local or distributed file system (such as HDFS)\n                     are supported. If you want to read from a distributed file system, sc\n                     needs to be specified.\n        :param sc: An instance of SparkContext.\n                   If specified, texts will be read as a DistributedTextSet.\n                   Default is None and in this case texts will be read as a LocalTextSet.\n        :param min_partitions: Int. A suggestion value of the minimal partition number for input\n                               texts. Only need to specify this when sc is not None. Default is 1.\n\n        :return: TextSet.\n        """"""\n        jvalue = callZooFunc(bigdl_type, ""readTextSet"", path, sc, min_partitions)\n        return TextSet(jvalue=jvalue)\n\n    @classmethod\n    def read_csv(cls, path, sc=None, min_partitions=1, bigdl_type=""float""):\n        """"""\n        Read texts with id from csv file.\n        Each record is supposed to contain the following two fields in order:\n        id(string) and text(string).\n        Note that the csv file should be without header.\n\n        :param path: The path to the csv file. Local or distributed file system (such as HDFS)\n                     are supported. If you want to read from a distributed file system, sc\n                     needs to be specified.\n        :param sc: An instance of SparkContext.\n                   If specified, texts will be read as a DistributedTextSet.\n                   Default is None and in this case texts will be read as a LocalTextSet.\n        :param min_partitions: Int. A suggestion value of the minimal partition number for input\n                               texts. Only need to specify this when sc is not None. Default is 1.\n\n        :return: TextSet.\n        """"""\n        jvalue = callZooFunc(bigdl_type, ""textSetReadCSV"", path, sc, min_partitions)\n        return TextSet(jvalue=jvalue)\n\n    @classmethod\n    def read_parquet(cls, path, sc, bigdl_type=""float""):\n        """"""\n        Read texts with id from parquet file.\n        Schema should be the following:\n        ""id""(string) and ""text""(string).\n\n        :param path: The path to the parquet file.\n        :param sc: An instance of SparkContext.\n\n        :return: DistributedTextSet.\n        """"""\n        jvalue = callZooFunc(bigdl_type, ""textSetReadParquet"", path, sc)\n        return DistributedTextSet(jvalue=jvalue)\n\n    @classmethod\n    def from_relation_pairs(cls, relations, corpus1, corpus2, bigdl_type=""float""):\n        """"""\n        Used to generate a TextSet for pairwise training.\n\n        This method does the following:\n        1. Generate all RelationPairs: (id1, id2Positive, id2Negative) from Relations.\n        2. Join RelationPairs with corpus to transform id to indexedTokens.\n        Note: Make sure that the corpus has been transformed by SequenceShaper and WordIndexer.\n        3. For each pair, generate a TextFeature having Sample with:\n        - feature of shape (2, text1Length + text2Length).\n        - label of value [1 0] as the positive relation is placed before the negative one.\n\n        :param relations: List or RDD of Relation.\n        :param corpus1: TextSet that contains all id1 in relations. For each TextFeature in corpus1,\n                        text must have been transformed to indexedTokens of the same length.\n        :param corpus2: TextSet that contains all id2 in relations. For each TextFeature in corpus2,\n                        text must have been transformed to indexedTokens of the same length.\n        Note that if relations is a list, then corpus1 and corpus2 must both be LocalTextSet.\n        If relations is RDD, then corpus1 and corpus2 must both be DistributedTextSet.\n\n        :return: TextSet.\n        """"""\n        if isinstance(relations, RDD):\n            relations = relations.map(lambda x: x.to_tuple())\n        elif isinstance(relations, list):\n            relations = [relation.to_tuple() for relation in relations]\n        else:\n            raise TypeError(""relations should be RDD or list of Relation"")\n        jvalue = callZooFunc(bigdl_type, ""textSetFromRelationPairs"", relations, corpus1, corpus2)\n        return TextSet(jvalue=jvalue)\n\n    @classmethod\n    def from_relation_lists(cls, relations, corpus1, corpus2, bigdl_type=""float""):\n        """"""\n        Used to generate a TextSet for ranking.\n\n        This method does the following:\n        1. For each id1 in relations, find the list of id2 with corresponding label that\n        comes together with id1.\n        In other words, group relations by id1.\n        2. Join with corpus to transform each id to indexedTokens.\n        Note: Make sure that the corpus has been transformed by SequenceShaper and WordIndexer.\n        3. For each list, generate a TextFeature having Sample with:\n        - feature of shape (list_length, text1_length + text2_length).\n        - label of shape (list_length, 1).\n\n        :param relations: List or RDD of Relation.\n        :param corpus1: TextSet that contains all id1 in relations. For each TextFeature in corpus1,\n                        text must have been transformed to indexedTokens of the same length.\n        :param corpus2: TextSet that contains all id2 in relations. For each TextFeature in corpus2,\n                        text must have been transformed to indexedTokens of the same length.\n        Note that if relations is a list, then corpus1 and corpus2 must both be LocalTextSet.\n        If relations is RDD, then corpus1 and corpus2 must both be DistributedTextSet.\n\n        :return: TextSet.\n        """"""\n        if isinstance(relations, RDD):\n            relations = relations.map(lambda x: x.to_tuple())\n        elif isinstance(relations, list):\n            relations = [relation.to_tuple() for relation in relations]\n        else:\n            raise TypeError(""relations should be RDD or list of Relation"")\n        jvalue = callZooFunc(bigdl_type, ""textSetFromRelationLists"", relations, corpus1, corpus2)\n        return TextSet(jvalue=jvalue)\n\n\nclass LocalTextSet(TextSet):\n    """"""\n    LocalTextSet is comprised of lists.\n    """"""\n\n    def __init__(self, texts=None, labels=None, jvalue=None, bigdl_type=""float""):\n        """"""\n        Create a LocalTextSet using texts and labels.\n\n        # Arguments:\n        texts: List of String. Each element is the content of a text.\n        labels: List of int or None if texts don\'t have labels.\n        """"""\n        if texts is not None:\n            assert all(isinstance(text, six.string_types) for text in texts), \\\n                ""texts for LocalTextSet should be list of string""\n        if labels is not None:\n            labels = [int(label) for label in labels]\n        super(LocalTextSet, self).__init__(jvalue, bigdl_type, texts, labels)\n\n\nclass DistributedTextSet(TextSet):\n    """"""\n    DistributedTextSet is comprised of RDDs.\n    """"""\n\n    def __init__(self, texts=None, labels=None, jvalue=None, bigdl_type=""float""):\n        """"""\n        Create a DistributedTextSet using texts and labels.\n\n        # Arguments:\n        texts: RDD of String. Each element is the content of a text.\n        labels: RDD of int or None if texts don\'t have labels.\n        """"""\n        if texts is not None:\n            assert isinstance(texts, RDD), ""texts for DistributedTextSet should be RDD of String""\n        if labels is not None:\n            assert isinstance(labels, RDD), ""labels for DistributedTextSet should be RDD of int""\n            labels = labels.map(lambda x: int(x))\n        super(DistributedTextSet, self).__init__(jvalue, bigdl_type, texts, labels)\n\n\ndef _process_predict_result(predict):\n    # \'predict\' is a list of JTensors or None\n    # convert to a list of ndarray\n    if predict is not None:\n        return [res.to_ndarray() for res in predict]\n    else:\n        return None\n'"
pyzoo/zoo/feature/text/transformer.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nimport six\nfrom zoo.feature.common import Preprocessing\nfrom zoo.feature.text import TextFeature\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass TextTransformer(Preprocessing):\n    """"""\n    Base class of Transformers that transform TextFeature.\n    """"""\n\n    def __init__(self, bigdl_type=""float"", *args):\n        super(TextTransformer, self).__init__(bigdl_type, *args)\n\n    def transform(self, text_feature):\n        """"""\n        Transform a TextFeature.\n        """"""\n        res = callZooFunc(self.bigdl_type, ""transformTextFeature"", self.value, text_feature.value)\n        return TextFeature(jvalue=res)\n\n\nclass Tokenizer(TextTransformer):\n    """"""\n    Transform text to array of string tokens.\n\n    >>> tokenizer = Tokenizer()\n    creating: createTokenizer\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        super(Tokenizer, self).__init__(bigdl_type)\n\n\nclass Normalizer(TextTransformer):\n    """"""\n    Removes all dirty characters (non English alphabet) from tokens and converts words to\n    lower case. Need to tokenize first.\n    Original tokens will be replaced by normalized tokens.\n\n    >>> normalizer = Normalizer()\n    creating: createNormalizer\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        super(Normalizer, self).__init__(bigdl_type)\n\n\nclass WordIndexer(TextTransformer):\n    """"""\n    Given a wordIndex map, transform tokens to corresponding indices.\n    Those words not in the map will be aborted.\n    Need to tokenize first.\n\n    # Arguments\n    map: Dict with word (string) as its key and index (int) as its value.\n\n    >>> word_indexer = WordIndexer(map={""it"": 1, ""me"": 2})\n    creating: createWordIndexer\n    """"""\n\n    def __init__(self, map, bigdl_type=""float""):\n        super(WordIndexer, self).__init__(bigdl_type, map)\n\n\nclass SequenceShaper(TextTransformer):\n    """"""\n    Shape the sequence of indices to a fixed length.\n    If the original sequence is longer than the target length, it will be truncated from\n    the beginning or the end.\n    If the original sequence is shorter than the target length, it will be padded to the end.\n    Need to word2idx first.\n    The original indices sequence will be replaced by the shaped sequence.\n\n    # Arguments\n    len: Positive int. The target length.\n    trunc_mode: Truncation mode. String. Either \'pre\' or \'post\'. Default is \'pre\'.\n                If \'pre\', the sequence will be truncated from the beginning.\n                If \'post\', the sequence will be truncated from the end.\n    pad_element: Int. The element to be padded to the sequence if the original length is\n                 smaller than the target length.\n                 Default is 0 with the convention that we reserve index 0 for unknown words.\n    >>> sequence_shaper = SequenceShaper(len=6, trunc_mode=""post"", pad_element=10000)\n    creating: createSequenceShaper\n    """"""\n\n    def __init__(self, len, trunc_mode=""pre"", pad_element=0, bigdl_type=""float""):\n        assert isinstance(pad_element, int), ""pad_element should be an int""\n        super(SequenceShaper, self).__init__(bigdl_type, len, trunc_mode, pad_element)\n\n\nclass TextFeatureToSample(TextTransformer):\n    """"""\n    Transform indexedTokens and label (if any) of a TextFeature to a BigDL Sample.\n    Need to word2idx first.\n\n    >>> to_sample = TextFeatureToSample()\n    creating: createTextFeatureToSample\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        super(TextFeatureToSample, self).__init__(bigdl_type)\n'"
pyzoo/zoo/models/anomalydetection/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .anomaly_detector import *\n'"
pyzoo/zoo/models/anomalydetection/anomaly_detector.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport warnings\n\nfrom zoo.pipeline.api.keras.models import Sequential\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.models.common import *\nfrom zoo.common.utils import callZooFunc\nfrom bigdl.util.common import Sample\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass AnomalyDetector(KerasZooModel):\n    """"""\n    The anomaly detector model for sequence data based on LSTM.\n\n    # Arguments\n    feature_shape: The input shape of features, including unroll_length and feature_size.\n    hidden_layers: Units of hidden layers of LSTM.\n    dropouts:     Fraction of the input units to drop out. Float between 0 and 1.\n    """"""\n\n    def __init__(self, feature_shape, hidden_layers=[8, 32, 15],\n                 dropouts=[0.2, 0.2, 0.2], **kwargs):\n        assert len(hidden_layers) == len(dropouts), \\\n            ""sizes of dropouts and hidden_layers should be equal""\n        if \'bigdl_type\' in kwargs:\n            kwargs.pop(\'bigdl_type\')\n            self.bigdl_type = kwargs.get(""bigdl_type"")\n        else:\n            self.bigdl_type = ""float""\n        if kwargs:\n            raise TypeError(\'Wrong arguments for AnomalyDetector: \' + str(kwargs))\n        self.feature_shape = feature_shape\n        self.hidden_layers = hidden_layers\n        self.dropouts = dropouts\n        self.model = self.build_model()\n        super(AnomalyDetector, self).__init__(None, self.bigdl_type,\n                                              feature_shape,\n                                              hidden_layers,\n                                              dropouts,\n                                              self.model)\n\n    def build_model(self):\n        model = Sequential()\n        model.add(InputLayer(input_shape=self.feature_shape)) \\\n            .add(LSTM(input_shape=self.feature_shape, output_dim=self.hidden_layers[0],\n                      return_sequences=True))\n\n        for ilayer in range(1, len(self.hidden_layers) - 1):\n            model.add(LSTM(output_dim=self.hidden_layers[ilayer], return_sequences=True)) \\\n                .add(Dropout(self.dropouts[ilayer]))\n\n        model.add(LSTM(self.hidden_layers[-1], return_sequences=False)) \\\n            .add(Dropout(self.dropouts[-1]))\n\n        model.add(Dense(output_dim=1))\n        return model\n\n    @staticmethod\n    def load_model(path, weight_path=None, bigdl_type=""float""):\n        """"""\n        Load an existing AnomalyDetector model (with weights).\n\n        # Arguments\n        path: The path for the pre-defined model.\n              Local file system, HDFS and Amazon S3 are supported.\n              HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n              Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        weight_path: The path for pre-trained weights if any. Default is None.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""loadAnomalyDetector"", path, weight_path)\n        model = KerasZooModel._do_load(jmodel, bigdl_type)\n        model.__class__ = AnomalyDetector\n        return model\n\n    def predict(self, x, batch_per_thread=8):\n        """"""\n        Precict on RDD[Sample].\n        """"""\n        results = callZooFunc(self.bigdl_type, ""modelPredictRDD"",\n                              self.value,\n                              x,\n                              batch_per_thread)\n        return results.map(lambda data: data.to_ndarray())\n\n    @classmethod\n    def unroll(cls, data_rdd, unroll_length, predict_step=1):\n        """"""\n        Unroll a rdd of arrays to prepare features and labels.\n\n        # Arguments\n        data_rdd: RDD[Array]. data to be unrolled, it holds original time series features\n        unroll_length: Int. the length of precious values to predict future value.\n        predict_step: Int. How many time steps to predict future value, default is 1.\n        return: an rdd of FeatureLableIndex\n        a simple example\n                     data: (1,2,3,4,5,6); unrollLength: 2, predictStep: 1\n                     features, label, index\n                     (1,2), 3, 0\n                     (2,3), 4, 1\n                     (3,4), 5, 2\n                     (4,5), 6, 3\n        """"""\n        result = callZooFunc(""float"", ""unroll"", data_rdd, unroll_length, predict_step)\n        return cls._to_indexed_rdd(result)\n\n    @classmethod\n    def detect_anomalies(cls, ytruth, ypredict, anomaly_size):\n        """"""\n        # Arguments\n        :param ytruth: RDD of float or double values. Truth to be compared.\n        :param ypredict: RDD of float or double values. Predictions.\n        :param anomaly_size: Int. The size to be considered as anomalies.\n        :return: RDD of [ytruth, ypredict, anomaly], anomaly is None or ytruth\n        """"""\n        return callZooFunc(""float"", ""detectAnomalies"", ytruth, ypredict, anomaly_size)\n\n    @staticmethod\n    def standardScale(df):\n        return callZooFunc(""float"", ""standardScaleDF"", df)\n\n    @staticmethod\n    def train_test_split(unrolled, test_size):\n        cutPoint = unrolled.count() - test_size\n        train = unrolled.filter(lambda x: x.index < cutPoint) \\\n            .map(lambda x: Sample.from_ndarray(np.array(x.feature), np.array(x.label)))\n        test = unrolled.filter(lambda x: x.index >= cutPoint) \\\n            .map(lambda x: Sample.from_ndarray(np.array(x.feature), np.array(x.label)))\n        return [train, test]\n\n    @staticmethod\n    def _to_indexed_rdd(unrolled_rdd):\n        def row_to_feature(feature_str):\n            feature = [x.split(""|"") for x in feature_str.split("","")]\n            matrix = []\n            for i in range(0, len(feature)):\n                line = []\n                for j in range(0, len(feature[0])):\n                    line.append(float(feature[i][j]))\n            matrix.append(line)\n            return matrix\n\n        return unrolled_rdd \\\n            .map(lambda y: FeatureLableIndex(row_to_feature(y[0]), float(y[1]), long(y[2])))\n\n\nclass FeatureLableIndex(object):\n    """"""\n    Each record should contain the following fields:\n    feature: List[List[float]].\n    label: float.\n    index: long.\n    """"""\n\n    def __init__(self, feature, label, index, bigdl_type=""float""):\n        self.feature = feature\n        self.label = label\n        self.index = index\n        self.bigdl_type = bigdl_type\n\n    def __reduce__(self):\n        return FeatureLableIndex, (self.feature, self.label, self.index)\n\n    def __str__(self):\n        return ""FeatureLableIndex [feature: %s, label: %s, index: %s]"" % (\n            self.feature, self.label, self.index)\n'"
pyzoo/zoo/models/common/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .zoo_model import *\nfrom .ranker import *\n'"
pyzoo/zoo/models/common/ranker.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom bigdl.util.common import JavaValue\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass Ranker(JavaValue):\n    """"""\n    Base class for Ranking models (e.g., TextMatcher and Ranker) that\n    provides validation methods with different metrics.\n    """"""\n\n    def evaluate_ndcg(self, x, k, threshold=0.0):\n        """"""\n        Evaluate using normalized discounted cumulative gain on TextSet.\n\n        :param x: TextSet. Each TextFeature should contain Sample with batch features and labels.\n                  In other words, each Sample should be a batch of records having both positive\n                  and negative labels.\n        :param k: Positive int. Rank position.\n        :param threshold: Float. If label > threshold, then it will be considered as\n                          a positive record. Default is 0.0.\n\n        :return: Float. NDCG result.\n        """"""\n        return callZooFunc(self.bigdl_type, ""evaluateNDCG"",\n                           self.value, x, k, threshold)\n\n    def evaluate_map(self, x, threshold=0.0):\n        """"""\n        Evaluate using mean average precision on TextSet.\n\n        :param x: TextSet. Each TextFeature should contain Sample with batch features and labels.\n                  In other words, each Sample should be a batch of records having both positive\n                  and negative labels.\n        :param threshold: Float. If label > threshold, then it will be considered as\n                          a positive record. Default is 0.0.\n\n        :return: Float. MAP result.\n        """"""\n        return callZooFunc(self.bigdl_type, ""evaluateMAP"",\n                           self.value, x, threshold)\n'"
pyzoo/zoo/models/common/zoo_model.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.nn.layer import Container, Layer\nfrom bigdl.util.common import *\nfrom zoo.pipeline.api.keras.engine.topology import KerasNet\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass ZooModelCreator(JavaValue):\n    def jvm_class_constructor(self):\n        name = ""createZoo"" + self.__class__.__name__\n        print(""creating: "" + name)\n        return name\n\n\nclass ZooModel(ZooModelCreator, Container):\n    """"""\n    The base class for models in Analytics Zoo.\n    """"""\n\n    def predict_classes(self, x, batch_size=32, zero_based_label=True):\n        """"""\n        Predict for classes. By default, label predictions start from 0.\n\n        # Arguments\n        x: Prediction data. A Numpy array or RDD of Sample.\n        batch_size: Number of samples per batch. Default is 32.\n        zero_based_label: Boolean. Whether result labels start from 0.\n                          Default is True. If False, result labels start from 1.\n        """"""\n        if isinstance(x, np.ndarray):\n            data_rdd = to_sample_rdd(x, np.zeros([x.shape[0]]))\n        elif isinstance(x, RDD):\n            data_rdd = x\n        else:\n            raise TypeError(""Unsupported prediction data type: %s"" % type(x))\n        return callZooFunc(self.bigdl_type, ""zooModelPredictClasses"",\n                           self.value,\n                           data_rdd,\n                           batch_size,\n                           zero_based_label)\n\n    def save_model(self, path, weight_path=None, over_write=False):\n        """"""\n        Save the model to the specified path.\n\n        # Arguments\n        path: The path to save the model. Local file system, HDFS and Amazon S3 are supported.\n              HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n              Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        weight_path: The path to save weights. Default is None.\n        over_write: Whether to overwrite the file if it already exists. Default is False.\n        """"""\n        callZooFunc(self.bigdl_type, ""saveZooModel"",\n                    self.value, path, weight_path, over_write)\n\n    def summary(self):\n        """"""\n        Print out the summary of the model.\n        """"""\n        callZooFunc(self.bigdl_type, ""zooModelSummary"",\n                    self.value)\n\n    def set_evaluate_status(self):\n        """"""\n        Set the model to be in evaluate status, i.e. remove the effect of Dropout, etc.\n        """"""\n        callZooFunc(self.bigdl_type, ""zooModelSetEvaluateStatus"",\n                    self.value)\n        return self\n\n    @staticmethod\n    def _do_load(jmodel, bigdl_type=""float""):\n        model = Layer(jvalue=jmodel, bigdl_type=bigdl_type)\n        model.value = jmodel\n        return model\n\n\nclass KerasZooModel(ZooModel):\n    """"""\n    The base class for Keras style models in Analytics Zoo.\n    """"""\n\n    # For the following method, please see documentation of KerasNet for details\n    def compile(self, optimizer, loss, metrics=None):\n        self.model.compile(optimizer, loss, metrics)\n\n    def fit(self, x, y=None, batch_size=32, nb_epoch=10,\n            validation_split=0, validation_data=None, distributed=True):\n        self.model.fit(x, y, batch_size, nb_epoch, validation_split, validation_data, distributed)\n\n    def set_checkpoint(self, path, over_write=True):\n        self.model.set_checkpoint(path, over_write)\n\n    def set_tensorboard(self, log_dir, app_name):\n        self.model.set_tensorboard(log_dir, app_name)\n\n    def get_train_summary(self, tag=None):\n        return self.model.get_train_summary(tag)\n\n    def get_validation_summary(self, tag=None):\n        return self.model.get_validation_summary(tag)\n\n    def clear_gradient_clipping(self):\n        self.model.clear_gradient_clipping()\n\n    def set_constant_gradient_clipping(self, min, max):\n        self.model.set_constant_gradient_clipping(min, max)\n\n    def set_gradient_clipping_by_l2_norm(self, clip_norm):\n        self.model.set_gradient_clipping_by_l2_norm(clip_norm)\n\n    def set_evaluate_status(self):\n        return self.model.set_evaluate_status()\n\n    def evaluate(self, x, y=None, batch_size=32):\n        return self.model.evaluate(x, y, batch_size)\n\n    def predict(self, x, batch_per_thread=4, distributed=True):\n        return self.model.predict(x, batch_per_thread, distributed)\n\n    def predict_classes(self, x, batch_per_thread=4, distributed=True):\n        return self.model.predict_classes(x, batch_per_thread, distributed)\n\n    @staticmethod\n    def _do_load(jmodel, bigdl_type=""float""):\n        model = ZooModel._do_load(jmodel, bigdl_type)\n        labor_model = callZooFunc(bigdl_type, ""getModule"", jmodel)\n        model.model = KerasNet(labor_model)\n        return model\n'"
pyzoo/zoo/models/image/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/models/recommendation/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .recommender import *\nfrom .neuralcf import *\nfrom .wide_and_deep import *\nfrom .session_recommender import *\n'"
pyzoo/zoo/models/recommendation/neuralcf.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom zoo.models.common import KerasZooModel\nfrom zoo.models.recommendation import Recommender\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass NeuralCF(Recommender):\n    """"""\n    The neural collaborative filtering model used for recommendation.\n\n    # Arguments\n    user_count: The number of users. Positive int.\n    item_count: The number of classes. Positive int.\n    class_num: The number of classes. Positive int.\n    user_embed: Units of user embedding. Positive int. Default is 20.\n    item_embed: itemEmbed Units of item embedding. Positive int. Default is 20.\n    hidden_layers: Units of hidden layers for MLP. Tuple of positive int. Default is (40, 20, 10).\n    include_mf: Whether to include Matrix Factorization. Boolean. Default is True.\n    mf_embed: Units of matrix factorization embedding. Positive int. Default is 20.\n    """"""\n\n    def __init__(self, user_count, item_count, class_num, user_embed=20,\n                 item_embed=20, hidden_layers=[40, 20, 10], include_mf=True,\n                 mf_embed=20, bigdl_type=""float""):\n        self.user_count = int(user_count)\n        self.item_count = int(item_count)\n        self.class_num = int(class_num)\n        self.user_embed = int(user_embed)\n        self.item_embed = int(item_embed)\n        self.hidden_layers = [int(unit) for unit in hidden_layers]\n        self.include_mf = include_mf\n        self.mf_embed = int(mf_embed)\n        self.bigdl_type = bigdl_type\n        self.model = self.build_model()\n        super(NeuralCF, self).__init__(None, self.bigdl_type,\n                                       self.user_count,\n                                       self.item_count,\n                                       self.class_num,\n                                       self.user_embed,\n                                       self.item_embed,\n                                       self.hidden_layers,\n                                       self.include_mf,\n                                       self.mf_embed,\n                                       self.model)\n\n    def build_model(self):\n        input = Input(shape=(2,))\n        user_flat = Flatten()(Select(1, 0)(input))\n        item_flat = Flatten()(Select(1, 1)(input))\n        mlp_user_embed = Embedding(self.user_count + 1, self.user_embed, init=""uniform"")(user_flat)\n        mlp_item_embed = Embedding(self.item_count + 1, self.item_embed, init=""uniform"")(item_flat)\n        mlp_user_flat = Flatten()(mlp_user_embed)\n        mlp_item_flat = Flatten()(mlp_item_embed)\n        mlp_latent = merge(inputs=[mlp_user_flat, mlp_item_flat], mode=""concat"")\n        linear1 = Dense(self.hidden_layers[0], activation=""relu"")(mlp_latent)\n        mlp_linear = linear1\n        for ilayer in range(1, len(self.hidden_layers)):\n            linear_mid = Dense(self.hidden_layers[ilayer], activation=""relu"")(mlp_linear)\n            mlp_linear = linear_mid\n\n        if (self.include_mf):\n            assert (self.mf_embed > 0)\n            mf_user_embed = Embedding(self.user_count + 1, self.mf_embed, init=""uniform"")(user_flat)\n            mf_item_embed = Embedding(self.item_count + 1, self.mf_embed, init=""uniform"")(item_flat)\n            mf_user_flatten = Flatten()(mf_user_embed)\n            mf_item_flatten = Flatten()(mf_item_embed)\n            mf_latent = merge(inputs=[mf_user_flatten, mf_item_flatten], mode=""mul"")\n            concated_model = merge(inputs=[mlp_linear, mf_latent], mode=""concat"")\n            linear_last = Dense(self.class_num, activation=""softmax"")(concated_model)\n        else:\n            linear_last = Dense(self.class_num, activation=""softmax"")(mlp_linear)\n        model = Model(input, linear_last)\n        return model\n\n    @staticmethod\n    def load_model(path, weight_path=None, bigdl_type=""float""):\n        """"""\n        Load an existing NeuralCF model (with weights).\n\n        # Arguments\n        path: The path for the pre-defined model.\n              Local file system, HDFS and Amazon S3 are supported.\n              HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n              Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        weight_path: The path for pre-trained weights if any. Default is None.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""loadNeuralCF"", path, weight_path)\n        model = KerasZooModel._do_load(jmodel, bigdl_type)\n        model.__class__ = NeuralCF\n        return model\n'"
pyzoo/zoo/models/recommendation/recommender.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom pyspark import RDD\n\nfrom zoo.models.common import *\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass UserItemFeature(object):\n    """"""\n    Represent records of user-item with features.\n\n    Each record should contain the following fields:\n    user_id: Positive int.\n    item_id: Positive int.\n    sample: Sample which consists of feature(s) and label(s).\n    """"""\n\n    def __init__(self, user_id, item_id, sample, bigdl_type=""float""):\n        self.user_id = int(user_id)\n        self.item_id = int(item_id)\n        self.sample = sample\n        self.bigdl_type = bigdl_type\n\n    def __reduce__(self):\n        return UserItemFeature, (self.user_id, self.item_id, self.sample)\n\n    def __str__(self):\n        return ""UserItemFeature [user_id: %s, item_id: %s, %s]"" % (\n            self.user_id, self.item_id, self.sample)\n\n\nclass UserItemPrediction(object):\n    """"""\n    Represent the prediction results of user-item pairs.\n\n    Each prediction record will contain the following information:\n    user_id: Positive int.\n    item_id: Positive int.\n    prediction: The prediction (rating) for the user on the item.\n    probability: The probability for the prediction.\n    """"""\n\n    def __init__(self, user_id, item_id, prediction, probability, bigdl_type=""float""):\n        self.user_id = user_id\n        self.item_id = item_id\n        self.prediction = prediction\n        self.probability = probability\n        self.bigdl_type = bigdl_type\n\n    def __reduce__(self):\n        return UserItemPrediction, (self.user_id, self.item_id, self.prediction, self.probability)\n\n    def __str__(self):\n        return ""UserItemPrediction [user_id: %s, item_id: %s, prediction: %s, probability: %s]"" % (\n            self.user_id, self.item_id, self.prediction, self.probability)\n\n\nclass Recommender(KerasZooModel):\n    """"""\n    The base class for recommendation models in Analytics Zoo.\n    """"""\n\n    def predict_user_item_pair(self, feature_rdd):\n        """"""\n        Predict for user-item pairs.\n\n        # Arguments\n        feature_rdd: RDD of UserItemFeature.\n        :return RDD of UserItemPrediction.\n        """"""\n        result_rdd = callZooFunc(self.bigdl_type, ""predictUserItemPair"",\n                                 self.value,\n                                 self._to_tuple_rdd(feature_rdd))\n        return self._to_prediction_rdd(result_rdd)\n\n    def recommend_for_user(self, feature_rdd, max_items):\n        """"""\n        Recommend a number of items for each user.\n\n        # Arguments\n        feature_rdd: RDD of UserItemFeature.\n        max_items: The number of items to be recommended to each user. Positive int.\n        :return RDD of UserItemPrediction.\n        """"""\n        result_rdd = callZooFunc(self.bigdl_type, ""recommendForUser"",\n                                 self.value,\n                                 self._to_tuple_rdd(feature_rdd),\n                                 int(max_items))\n        return self._to_prediction_rdd(result_rdd)\n\n    def recommend_for_item(self, feature_rdd, max_users):\n        """"""\n        Recommend a number of users for each item.\n\n        # Arguments\n        feature_rdd: RDD of UserItemFeature.\n        max_users: The number of users to be recommended to each item. Positive int.\n        :return RDD of UserItemPrediction.\n        """"""\n        result_rdd = callZooFunc(self.bigdl_type, ""recommendForItem"",\n                                 self.value,\n                                 self._to_tuple_rdd(feature_rdd),\n                                 int(max_users))\n        return self._to_prediction_rdd(result_rdd)\n\n    @staticmethod\n    def _to_tuple_rdd(feature_rdd):\n        assert isinstance(feature_rdd, RDD), ""feature_rdd should be RDD of UserItemFeature""\n        return feature_rdd.map(lambda x: (x.user_id, x.item_id, x.sample))\n\n    @staticmethod\n    def _to_prediction_rdd(result_rdd):\n        return result_rdd.map(lambda y: UserItemPrediction(int(y[0]), int(y[1]), int(y[2]), y[3]))\n'"
pyzoo/zoo/models/recommendation/session_recommender.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom zoo.models.common import KerasZooModel\nfrom zoo.models.recommendation import Recommender\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass SessionRecommender(Recommender):\n    """"""\n    The Session Recommender model used for recommendation.\n\n    # Arguments\n     item_ount: The number of distinct items. Positive integer.\n     item_embed: The output size of embedding layer. Positive integer.\n     rnn_hidden_layers: Units of hidden layers for the mlp model. Array of positive integers.\n     session_length: The max number of items in the sequence of a session\n     include_history: Whether to include purchase history. Boolean. Default is true.\n     mlp_hidden_layers: Units of hidden layers for the mlp model. Array of positive integers.\n     history_length: The max number of items in the sequence of historical purchase\n     """"""\n\n    def __init__(self, item_count, item_embed, rnn_hidden_layers=[40, 20], session_length=0,\n                 include_history=False, mlp_hidden_layers=[40, 20], history_length=0,\n                 bigdl_type=""float""):\n        assert session_length > 0, ""session_length should align with input features""\n        if include_history:\n            assert history_length > 0, ""history_length should align with input features""\n        self.item_count = int(item_count)\n        self.item_embed = int(item_embed)\n        self.mlp_hidden_layers = [int(unit) for unit in mlp_hidden_layers]\n        self.rnn_hidden_layers = [int(unit) for unit in rnn_hidden_layers]\n        self.include_history = include_history\n        self.session_length = int(session_length)\n        self.history_length = int(history_length)\n        self.bigdl_type = bigdl_type\n        self.model = self.build_model()\n        super(SessionRecommender, self).__init__(None, self.bigdl_type,\n                                                 self.item_count,\n                                                 self.item_embed,\n                                                 self.rnn_hidden_layers,\n                                                 self.session_length,\n                                                 self.include_history,\n                                                 self.mlp_hidden_layers,\n                                                 self.history_length,\n                                                 self.model)\n\n    def build_model(self):\n        input_rnn = Input(shape=(self.session_length,))\n        session_table = Embedding(self.item_count + 1, self.item_embed, init=""uniform"")(input_rnn)\n\n        gru = GRU(self.rnn_hidden_layers[0], return_sequences=True)(session_table)\n        for hidden in range(1, len(self.rnn_hidden_layers) - 1):\n            gru = GRU(self.rnn_hidden_layers[hidden], return_sequences=True)(gru)\n        gru_last = GRU(self.rnn_hidden_layers[-1], return_sequences=False)(gru)\n        rnn = Dense(self.item_count)(gru_last)\n\n        if self.include_history:\n            input_mlp = Input(shape=(self.history_length,))\n            his_table = Embedding(self.item_count + 1, self.item_embed, init=""uniform"")(input_mlp)\n            embedSum = KerasLayerWrapper(Sum(dimension=2))(his_table)\n            flatten = Flatten()(embedSum)\n            mlp = Dense(self.mlp_hidden_layers[0], activation=""relu"")(flatten)\n            for hidden in range(1, len(self.mlp_hidden_layers)):\n                mlp = Dense(self.mlp_hidden_layers[hidden], activation=""relu"")(mlp)\n            mlp_last = Dense(self.item_count)(mlp)\n            merged = merge(inputs=[rnn, mlp_last], mode=""sum"")\n            out = Activation(activation=""softmax"")(merged)\n            model = Model(input=[input_rnn, input_mlp], output=out)\n        else:\n            out = Activation(activation=""softmax"")(rnn)\n            model = Model(input=input_rnn, output=out)\n        return model\n\n    def recommend_for_user(self, feature_rdd, max_items):\n        raise Exception(""recommend_for_user: Unsupported for SessionRecommender"")\n\n    def recommend_for_item(self, feature_rdd, max_users):\n        raise Exception(""recommend_for_item: Unsupported for SessionRecommender"")\n\n    def predict_user_item_pair(self, feature_rdd):\n        raise Exception(""predict_user_item_pair: Unsupported for SessionRecommender"")\n\n    def recommend_for_session(self, sessions, max_items, zero_based_label):\n        """"""\n        recommend for sessions given rdd of samples or list of samples.\n\n        # Arguments\n        sessions: rdd of samples or list of samples.\n        max_items:   Number of items to be recommended to each user. Positive integer.\n        zero_based_label: True if data starts from 0, False if data starts from 1\n        :return rdd of list of list(item, probability),\n        """"""\n        if isinstance(sessions, list):\n            sc = get_spark_context()\n            sessions_rdd = sc.parallelize(sessions)\n        elif (isinstance(sessions, RDD)):\n            sessions_rdd = sessions\n        else:\n            raise TypeError(""Unsupported training data type: %s"" % type(sessions))\n        results = callZooFunc(self.bigdl_type, ""recommendForSession"",\n                              self.value,\n                              sessions_rdd,\n                              max_items,\n                              zero_based_label)\n\n        if isinstance(sessions, list):\n            return results.collect()\n        else:\n            return results\n\n    @staticmethod\n    def load_model(path, weight_path=None, bigdl_type=""float""):\n        """"""\n        Load an existing SessionRecommender model (with weights).\n\n        # Arguments\n        path: The path for the pre-defined model.\n              Local file system, HDFS and Amazon S3 are supported.\n              HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n              Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        weight_path: The path for pre-trained weights if any. Default is None.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""loadSessionRecommender"", path, weight_path)\n        model = KerasZooModel._do_load(jmodel, bigdl_type)\n        model.__class__ = SessionRecommender\n        return model\n'"
pyzoo/zoo/models/recommendation/utils.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\n\nfrom bigdl.util.common import JTensor, Sample\n\nfrom zoo.common.utils import callZooFunc\nfrom zoo.models.recommendation import UserItemFeature\n\n\ndef hash_bucket(content, bucket_size=1000, start=0):\n    return (hash(str(content)) % bucket_size + bucket_size) % bucket_size + start\n\n\ndef categorical_from_vocab_list(sth, vocab_list, default=-1, start=0):\n    if sth in vocab_list:\n        return vocab_list.index(sth) + start\n    else:\n        return default + start\n\n\ndef get_boundaries(target, boundaries, default=-1, start=0):\n    if target == \'?\':\n        return default + start\n    else:\n        for i in range(len(boundaries)):\n            if target < boundaries[i]:\n                return i + start\n        return len(boundaries) + start\n\n\ndef get_negative_samples(indexed):\n    return callZooFunc(""float"", ""getNegativeSamples"",\n                       indexed)\n\n\ndef get_wide_tensor(row, column_info):\n    """"""\n    convert a row to tensor given column feature information of a WideAndDeep model\n\n    :param row: Row of userId, itemId, features and label\n    :param column_info: ColumnFeatureInfo specify information of different features\n    :return: an array of tensors as input for wide part of a WideAndDeep model\n    """"""\n\n    wide_columns = column_info.wide_base_cols + column_info.wide_cross_cols\n    wide_dims = column_info.wide_base_dims + column_info.wide_cross_dims\n    wide_length = len(wide_columns)\n    acc = 0\n    indices = []\n    for i in range(0, wide_length):\n        index = row[wide_columns[i]]\n        if i == 0:\n            res = index\n        else:\n            acc += wide_dims[i - 1]\n            res = acc + index\n        indices.append(res)\n    values = np.array([i + 1 for i in indices])\n    shape = np.array([sum(wide_dims)])\n    return JTensor.sparse(values, np.array(indices), shape)\n\n\ndef get_deep_tensors(row, column_info):\n    """"""\n    convert a row to tensors given column feature information of a WideAndDeep model\n\n    :param row: Row of userId, itemId, features and label\n    :param column_info: ColumnFeatureInfo specify information of different features\n    :return: an array of tensors as input for deep part of a WideAndDeep model\n    """"""\n\n    ind_col = column_info.indicator_cols\n    emb_col = column_info.embed_cols\n    cont_col = column_info.continuous_cols\n\n    ind_tensor = np.zeros(sum(column_info.indicator_dims), )\n    # setup indicators\n    acc = 0\n    for i in range(0, len(ind_col)):\n        index = row[ind_col[i]]\n        if i == 0:\n            res = index\n        else:\n            acc += column_info.indicator_dims[i - 1]\n            res = acc + index\n        ind_tensor[res] = 1\n\n    emb_tensor = np.zeros(len(emb_col), )\n    for i in range(0, len(emb_col)):\n        emb_tensor[i] = float(row[emb_col[i]])\n\n    cont_tensor = np.zeros(len(cont_col), )\n    for i in range(0, len(cont_col)):\n        cont_tensor[i] = float(row[cont_col[i]])\n\n    has_ind = len(ind_col) > 0\n    has_emd = len(emb_col) > 0\n    has_cont = len(cont_col) > 0\n    if (has_ind and has_emd and has_cont):\n        deep_tensor = [ind_tensor, emb_tensor, cont_tensor]\n    elif ((not has_ind) and has_emd and has_cont):\n        deep_tensor = [emb_tensor, cont_tensor]\n    elif (has_ind and (not has_emd) and has_cont):\n        deep_tensor = [ind_tensor, cont_tensor]\n    elif (has_ind and has_emd and (not has_cont)):\n        deep_tensor = [ind_tensor, emb_tensor]\n    elif ((not has_ind) and (not has_emd) and has_cont):\n        deep_tensor = [cont_tensor]\n    elif ((not has_ind) and has_emd and (not has_cont)):\n        deep_tensor = [emb_tensor]\n    elif (has_ind and (not has_emd) and (not has_cont)):\n        deep_tensor = [ind_tensor]\n    else:\n        raise TypeError(""Empty deep tensors"")\n    return deep_tensor\n\n\ndef row_to_sample(row, column_info, model_type=""wide_n_deep""):\n    """"""\n    convert a row to sample given column feature information of a WideAndDeep model\n\n    :param row: Row of userId, itemId, features and label\n    :param column_info: ColumnFeatureInfo specify information of different features\n    :return: TensorSample as input for WideAndDeep model\n    """"""\n\n    wide_tensor = get_wide_tensor(row, column_info)\n    deep_tensor = get_deep_tensors(row, column_info)\n    deep_tensors = [JTensor.from_ndarray(ele) for ele in deep_tensor]\n    label = row[column_info.label]\n    model_type = model_type.lower()\n    if model_type == ""wide_n_deep"":\n        feature = [wide_tensor] + deep_tensors\n    elif model_type == ""wide"":\n        feature = wide_tensor\n    elif model_type == ""deep"":\n        feature = deep_tensors\n    else:\n        raise TypeError(""Unsupported model_type: %s"" % model_type)\n    return Sample.from_jtensor(feature, label)\n\n\ndef to_user_item_feature(row, column_info, model_type=""wide_n_deep""):\n    """"""\n    convert a row to UserItemFeature given column feature information of a WideAndDeep model\n\n    :param row: Row of userId, itemId, features and label\n    :param column_info: ColumnFeatureInfo specify information of different features\n    :return: UserItemFeature for recommender model\n    """"""\n    return UserItemFeature(row[""userId""], row[""itemId""],\n                           row_to_sample(row, column_info, model_type))\n'"
pyzoo/zoo/models/recommendation/wide_and_deep.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom zoo.models.common import *\nfrom zoo.models.recommendation import Recommender\nfrom zoo.common.utils import callZooFunc\nfrom zoo.pipeline.api.keras.layers import *\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass ColumnFeatureInfo(object):\n    """"""\n    The same data information shared by the WideAndDeep model and its feature generation part.\n\n    Each instance could contain the following fields:\n    wide_base_cols: Data of wide_base_cols together with wide_cross_cols will be fed\n                    into the wide model. List of String. Default is an empty list.\n    wide_base_dims: Dimensions of wide_base_cols. The dimensions of the data in\n                    wide_base_cols should be within the range of wide_base_dims.\n                    List of int. Default is an empty list.\n    wide_cross_cols: Data of wide_cross_cols will be fed into the wide model.\n                     List of String. Default is an empty list.\n    wide_cross_dims: Dimensions of wide_cross_cols. The dimensions of the data in\n                     wide_cross_cols should be within the range of wide_cross_dims.\n                     List of int. Default is an empty list.\n    indicator_cols: Data of indicator_cols will be fed into the deep model as multi-hot vectors.\n                    List of String. Default is an empty list.\n    indicator_dims: Dimensions of indicator_cols. The dimensions of the data in\n                    indicator_cols should be within the range of indicator_dims.\n                    List of int. Default is an empty list.\n    embed_cols: Data of embed_cols will be fed into the deep model as embeddings.\n                List of String. Default is an empty list.\n    embed_in_dims: Input dimension of the data in embed_cols. The dimensions of the data in\n                   embed_cols should be within the range of embed_in_dims.\n                   List of int. Default is an empty list.\n    embed_out_dims: The dimensions of embeddings. List of int. Default is an empty list.\n    continuous_cols: Data of continuous_cols will be treated as continuous values for\n                     the deep model. List of String. Default is an empty list.\n    label: The name of the \'label\' column. String. Default is \'label\'.\n    """"""\n\n    def __init__(self, wide_base_cols=None, wide_base_dims=None, wide_cross_cols=None,\n                 wide_cross_dims=None, indicator_cols=None, indicator_dims=None,\n                 embed_cols=None, embed_in_dims=None, embed_out_dims=None,\n                 continuous_cols=None, label=""label"", bigdl_type=""float""):\n        self.wide_base_cols = [] if not wide_base_cols else wide_base_cols\n        self.wide_base_dims = [] if not wide_base_dims else [int(d) for d in wide_base_dims]\n        self.wide_cross_cols = [] if not wide_cross_cols else wide_cross_cols\n        self.wide_cross_dims = [] if not wide_cross_dims else [int(d) for d in wide_cross_dims]\n        self.indicator_cols = [] if not indicator_cols else indicator_cols\n        self.indicator_dims = [] if not indicator_dims else [int(d) for d in indicator_dims]\n        self.embed_cols = [] if not embed_cols else embed_cols\n        self.embed_in_dims = [] if not embed_in_dims else [int(d) for d in embed_in_dims]\n        self.embed_out_dims = [] if not embed_out_dims else [int(d) for d in embed_out_dims]\n        self.continuous_cols = [] if not continuous_cols else continuous_cols\n        self.label = label\n        self.bigdl_type = bigdl_type\n\n    def __reduce__(self):\n        return ColumnFeatureInfo, (self.wide_base_cols, self.wide_base_dims, self.wide_cross_cols,\n                                   self.wide_cross_dims, self.indicator_cols, self.indicator_dims,\n                                   self.embed_cols, self.embed_in_dims, self.embed_out_dims,\n                                   self.continuous_cols, self.label)\n\n    def __str__(self):\n        return ""ColumnFeatureInfo {wide_base_cols: %s, wide_base_dims: %s, wide_cross_cols: %s, "" \\\n               ""wide_cross_dims: %s, indicator_cols: %s, indicator_dims: %s, embed_cols: %s, "" \\\n               ""embed_cols: %s, embed_in_dims: %s, embed_out_dims: %s, continuous_cols: %s, "" \\\n               ""label: \'%s\'}"" \\\n               % (self.wide_base_cols, self.wide_base_dims, self.wide_cross_cols,\n                  self.wide_cross_dims, self.indicator_cols, self.indicator_dims,\n                  self.embed_cols, self.embed_cols, self.embed_in_dims,\n                  self.embed_out_dims, self.continuous_cols, self.label)\n\n\nclass WideAndDeep(Recommender):\n    """"""\n    The Wide and Deep model used for recommendation.\n\n    # Arguments\n    class_num: The number of classes. Positive int.\n    column_info: An instance of ColumnFeatureInfo.\n    model_type: String. \'wide\', \'deep\' and \'wide_n_deep\' are supported. Default is \'wide_n_deep\'.\n    hidden_layers: Units of hidden layers for the deep model.\n                   Tuple of positive int. Default is (40, 20, 10).\n    """"""\n\n    def __init__(self, class_num, column_info, model_type=""wide_n_deep"",\n                 hidden_layers=[40, 20, 10], bigdl_type=""float""):\n        assert len(column_info.wide_base_cols) == len(column_info.wide_base_dims), \\\n            ""size of wide_base_columns should match""\n        assert len(column_info.wide_cross_cols) == len(column_info.wide_cross_dims), \\\n            ""size of wide_cross_columns should match""\n        assert len(column_info.indicator_cols) == len(column_info.indicator_dims), \\\n            ""size of wide_indicator_columns should match""\n        assert len(column_info.embed_cols) == len(column_info.embed_in_dims) \\\n            == len(column_info.embed_out_dims), ""size of wide_indicator_columns should match""\n\n        self.class_num = int(class_num)\n        self.wide_base_dims = column_info.wide_base_dims\n        self.wide_cross_dims = column_info.wide_cross_dims\n        self.indicator_dims = column_info.indicator_dims\n        self.embed_in_dims = column_info.embed_in_dims\n        self.embed_out_dims = column_info.embed_out_dims\n        self.continuous_cols = column_info.continuous_cols\n        self.model_type = model_type\n        self.hidden_layers = [int(unit) for unit in hidden_layers]\n        self.bigdl_type = bigdl_type\n        self.model = self.build_model()\n        super(WideAndDeep, self).__init__(None, self.bigdl_type,\n                                          self.model_type,\n                                          self.class_num,\n                                          self.hidden_layers,\n                                          self.wide_base_dims,\n                                          self.wide_cross_dims,\n                                          self.indicator_dims,\n                                          self.embed_in_dims,\n                                          self.embed_out_dims,\n                                          self.continuous_cols,\n                                          self.model)\n\n    def build_model(self):\n        wide_dims = sum(self.wide_base_dims) + sum(self.wide_cross_dims)\n        input_wide = Input(shape=(wide_dims,))\n        input_ind = Input(shape=(sum(self.indicator_dims),))\n        input_emb = Input(shape=(len(self.embed_in_dims),))\n        input_con = Input(shape=(len(self.continuous_cols),))\n\n        wide_linear = SparseDense(self.class_num)(input_wide)\n\n        if (self.model_type == ""wide""):\n            out = Activation(""softmax"")(wide_linear)\n            model = Model(input_wide, out)\n        elif (self.model_type == ""deep""):\n            (input_deep, merge_list) = self._deep_merge(input_ind, input_emb, input_con)\n            deep_linear = self._deep_hidden(merge_list)\n            out = Activation(""softmax"")(deep_linear)\n            model = Model(input_deep, out)\n        elif (self.model_type == ""wide_n_deep""):\n            (input_deep, merge_list) = self._deep_merge(input_ind, input_emb, input_con)\n            deep_linear = self._deep_hidden(merge_list)\n            merged = merge([wide_linear, deep_linear], ""sum"")\n            out = Activation(""softmax"")(merged)\n            model = Model([input_wide] + input_deep, out)\n\n        else:\n            raise TypeError(""Unsupported model_type: %s"" % self.model_type)\n\n        return model\n\n    def _deep_hidden(self, merge_list):\n        if (len(merge_list) == 1):\n            merged = merge_list[0]\n        else:\n            merged = merge(merge_list, ""concat"")\n        linear = Dense(self.hidden_layers[0], activation=""relu"")(merged)\n\n        for ilayer in range(1, len(self.hidden_layers)):\n            linear_mid = Dense(self.hidden_layers[ilayer], activation=""relu"")(linear)\n            linear = linear_mid\n        last = Dense(self.class_num, activation=""relu"")(linear)\n        return last\n\n    def _deep_merge(self, input_ind, input_emb, input_cont):\n        embed_width = 0\n        embed = []\n        for i in range(0, len(self.embed_in_dims)):\n            flat_select = Flatten()(Select(1, embed_width)(input_emb))\n            iembed = Embedding(self.embed_in_dims[i] + 1, self.embed_out_dims[i],\n                               init=""normal"")(flat_select)\n            flat_embed = Flatten()(iembed)\n            embed.append(flat_embed)\n            embed_width = embed_width + 1\n\n        has_ind = len(self.indicator_dims) > 0\n        has_emd = len(self.embed_in_dims) > 0\n        has_cont = len(self.continuous_cols) > 0\n        if (has_ind and has_emd and has_cont):\n            input = [input_ind, input_emb, input_cont]\n            merged_list = [input_ind] + embed + [input_cont]\n        elif (not has_ind and has_emd and has_cont):\n            input = [input_emb, input_cont]\n            merged_list = embed + [input_cont]\n        elif (has_ind and (not has_emd) and has_cont):\n            input = [input_ind, input_cont]\n            merged_list = [input_ind, input_cont]\n        elif (has_ind and has_emd and (not has_cont)):\n            input = [input_ind, input_emb]\n            merged_list = [input_ind] + embed\n        elif ((not has_ind) and (not has_emd) and has_cont):\n            input = [input_cont]\n            merged_list = [input_cont]\n        elif ((not has_ind) and has_emd and (not has_cont)):\n            input = [input_emb]\n            merged_list = embed\n        elif (has_ind and (not has_emd) and not (has_cont)):\n            input = [input_ind]\n            merged_list = [input_ind]\n        else:\n            raise TypeError(""Empty deep model for: %s"" % self.model_type)\n\n        return (input, merged_list)\n\n    @staticmethod\n    def load_model(path, weight_path=None, bigdl_type=""float""):\n        """"""\n        Load an existing WideAndDeep model (with weights).\n\n        # Arguments\n        path: The path for the pre-defined model.\n              Local file system, HDFS and Amazon S3 are supported.\n              HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n              Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        weight_path: The path for pre-trained weights if any. Default is None.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""loadWideAndDeep"", path, weight_path)\n        model = ZooModel._do_load(jmodel, bigdl_type)\n        labor_model = KerasZooModel._do_load(jmodel, bigdl_type)\n        model.model = labor_model\n        model.__class__ = WideAndDeep\n        return model\n'"
pyzoo/zoo/models/seq2seq/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .seq2seq import *\n'"
pyzoo/zoo/models/seq2seq/seq2seq.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.nn.layer import Layer\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.models.common import ZooModel\nfrom zoo.common.utils import callZooFunc\n\nfrom zoo.pipeline.api.keras.engine import ZooKerasLayer\nfrom zoo.pipeline.api.keras.models import Model\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\ndef createRNN(rnn_type, nlayers, hidden_size):\n    _rnn_type = rnn_type.lower()\n    if (_rnn_type == ""lstm""):\n        return [LSTM(hidden_size, return_sequences=True) for layer in range(nlayers)]\n    elif (_rnn_type == ""gru""):\n        return [GRU(hidden_size, return_sequences=True) for layer in range(nlayers)]\n    elif (_rnn_type == ""simplernn""):\n        return [SimpleRNN(hidden_size, return_sequences=True) for layer in range(nlayers)]\n    else:\n        raise Exception(\'Only support lstm|gru|simplernn\')\n\n\nclass RNNEncoder(ZooKerasLayer):\n    """"""\n    A generic recurrent neural network encoder\n\n    # Arguments\n    rnns: rnn layers used for encoder, support stacked rnn layers\n    embedding: embedding layer in encoder\n    input_shape: shape of input, not including batch\n\n    >>> encoder = RNNEncoder.initialize(""lstm"", 2, 3)\n    creating: createZooKerasLSTM\n    creating: createZooKerasLSTM\n    creating: createZooKerasRNNEncoder\n\n    >>> lstm = LSTM(3)\n    creating: createZooKerasLSTM\n    >>> embedding = Embedding(1000, 32, input_length=10, name=""embedding1"")\n    creating: createZooKerasEmbedding\n    >>> encoder = RNNEncoder([lstm], embedding)\n    creating: createZooKerasRNNEncoder\n    """"""\n\n    def __init__(self, rnns, embedding=None, input_shape=None):\n        super(RNNEncoder, self).__init__(None,\n                                         rnns,\n                                         embedding,\n                                         list(input_shape) if input_shape else None)\n\n    @classmethod\n    def initialize(cls, rnn_type, nlayers, hidden_size, embedding=None, input_shape=None):\n        """"""\n        rnn_type: currently support ""simplernn | lstm | gru""\n        nlayers: number of layers used in encoder\n        hidden_size: hidden size of encoder\n        embedding: embedding layer in encoder, `None` is supported\n        """"""\n        rnns = createRNN(rnn_type, nlayers, hidden_size)\n        return RNNEncoder(rnns, embedding, input_shape)\n\n\nclass RNNDecoder(ZooKerasLayer):\n    """"""\n    A generic recurrent neural network decoder\n\n    # Arguments\n    rnns: rnn layers used for decoder, support stacked rnn layers\n    embedding: embedding layer in decoder\n    input_shape: shape of input, not including batch\n\n    >>> decoder = RNNDecoder.initialize(""lstm"", 2, 3)\n    creating: createZooKerasLSTM\n    creating: createZooKerasLSTM\n    creating: createZooKerasRNNDecoder\n\n    >>> lstm = LSTM(3)\n    creating: createZooKerasLSTM\n    >>> embedding = Embedding(1000, 32, input_length=10, name=""embedding1"")\n    creating: createZooKerasEmbedding\n    >>> encoder = RNNDecoder([lstm], embedding)\n    creating: createZooKerasRNNDecoder\n    """"""\n\n    def __init__(self, rnns, embedding=None, input_shape=None):\n        super(RNNDecoder, self).__init__(None,\n                                         rnns,\n                                         embedding,\n                                         list(input_shape) if input_shape else None)\n\n    @classmethod\n    def initialize(cls, rnn_type, nlayers, hidden_size, embedding=None, input_shape=None):\n        """"""\n        rnn_type: currently support ""simplernn | lstm | gru""\n        nlayers: number of layers used in decoder\n        hidden_size: hidden size of decoder\n        embedding: embedding layer in decoder, `None` is supported\n        """"""\n        rnns = createRNN(rnn_type, nlayers, hidden_size)\n        return RNNDecoder(rnns, embedding, input_shape)\n\n\nclass Bridge(ZooKerasLayer):\n    """"""\n    defines how to transform encoder to decoder\n\n    # Arguments\n    bridge_type: currently only support ""dense | densenonlinear""\n    decoder_hiddenSize: hidden size of decoder\n    bridge: keras layers used to do the transformation\n\n    >>> bridge = Bridge.initialize(""dense"", 2)\n    creating: createZooKerasBridge\n    >>> dense = Dense(3)\n    creating: createZooKerasDense\n    >>> bridge = Bridge.initialize_from_keras_layer(dense)\n    creating: createZooKerasBridge\n    """"""\n\n    def __init__(self, bridge_type, decoder_hidden_size, bridge):\n        super(Bridge, self).__init__(None, bridge_type, decoder_hidden_size, bridge)\n\n    @classmethod\n    def initialize(cls, bridge_type, decoder_hidden_size):\n        """"""\n        bridge_type: currently only support ""dense | densenonlinear""\n        decoder_hiddenSize: hidden size of decoder\n        """"""\n        return Bridge(bridge_type, decoder_hidden_size, None)\n\n    @classmethod\n    def initialize_from_keras_layer(cls, bridge):\n        """"""\n        bridge: keras layers used to do the transformation\n        """"""\n        return Bridge(""customized"", 0, bridge)\n\n\nclass Seq2seq(ZooModel):\n    """"""\n    A trainable interface for a simple, generic encoder + decoder model\n\n    # Arguments\n    encoder: an encoder object\n    decoder: a decoder object\n    input_shape: shape of encoder input, for variable length, please use -1 as seq len\n    output_shape: shape of decoder input, for variable length, please use -1 as seq len\n    bridge: connect encoder and decoder\n    generator: Feeding decoder output to generator to generate final result, `None` is supported\n\n    >>> encoder = RNNEncoder.initialize(""LSTM"", 1, 4)\n    creating: createZooKerasLSTM\n    creating: createZooKerasRNNEncoder\n    >>> decoder = RNNDecoder.initialize(""LSTM"", 1, 4)\n    creating: createZooKerasLSTM\n    creating: createZooKerasRNNDecoder\n    >>> bridge = Bridge.initialize(""dense"", 4)\n    creating: createZooKerasBridge\n    >>> seq2seq = Seq2seq(encoder, decoder, [2, 4], [2, 4], bridge)\n    creating: createZooKerasInput\n    creating: createZooKerasInput\n    creating: createZooKerasSelectTable\n    creating: createZooKerasModel\n    creating: createZooSeq2seq\n    """"""\n\n    def __init__(self, encoder, decoder, input_shape, output_shape, bridge=None,\n                 generator=None, bigdl_type=""float""):\n        if (input_shape is None) or (output_shape is None):\n            raise TypeError(\'input_shape and output_shape cannot be None\')\n        self.encoder = encoder\n        self.decoder = decoder\n        self.input_shape = list(input_shape)\n        self.output_shape = list(output_shape)\n        self.bridge = bridge\n        self.generator = generator\n        self.bigdl_type = bigdl_type\n        self.model = self.build_model()\n        super(Seq2seq, self).__init__(None, self.bigdl_type,\n                                      self.encoder,\n                                      self.decoder,\n                                      self.input_shape,\n                                      self.output_shape,\n                                      self.bridge,\n                                      self.generator,\n                                      self.model)\n\n    def build_model(self):\n        encoder_input = Input(name=""encoder_input"", shape=self.input_shape)\n        decoder_input = Input(name=""decoder_input"", shape=self.output_shape)\n        encoder_output = self.encoder(encoder_input)\n\n        encoder_final_states = SelectTable(1)(encoder_output)\n        decoder_init_states = \\\n            self.bridge(encoder_final_states) if self.bridge else encoder_final_states\n\n        decoder_output = self.decoder([decoder_input, decoder_init_states])\n\n        output = self.generator(decoder_output) if self.generator else decoder_output\n\n        return Model([encoder_input, decoder_input], output)\n\n    def set_checkpoint(self, path, over_write=True):\n        callZooFunc(self.bigdl_type, ""seq2seqSetCheckpoint"",\n                    self.value,\n                    path,\n                    over_write)\n\n    @staticmethod\n    def load_model(path, weight_path=None, bigdl_type=""float""):\n        """"""\n        Load an existing Seq2seq model (with weights).\n\n        # Arguments\n        path: The path for the pre-defined model.\n              Local file system, HDFS and Amazon S3 are supported.\n              HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n              Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        weight_path: The path for pre-trained weights if any. Default is None.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""loadSeq2seq"", path, weight_path)\n        model = ZooModel._do_load(jmodel, bigdl_type)\n        model.__class__ = Seq2seq\n        return model\n\n    # For the following methods, please refer to KerasNet for documentation.\n    def compile(self, optimizer, loss, metrics=None):\n        if isinstance(optimizer, six.string_types):\n            optimizer = to_bigdl_optim_method(optimizer)\n        if isinstance(loss, six.string_types):\n            loss = to_bigdl_criterion(loss)\n        if metrics and all(isinstance(metric, six.string_types) for metric in metrics):\n            metrics = to_bigdl_metrics(metrics, loss)\n        callZooFunc(self.bigdl_type, ""seq2seqCompile"",\n                    self.value,\n                    optimizer,\n                    loss,\n                    metrics)\n\n    def fit(self, x, batch_size=32, nb_epoch=10, validation_data=None):\n        callZooFunc(self.bigdl_type, ""seq2seqFit"",\n                    self.value,\n                    x,\n                    batch_size,\n                    nb_epoch,\n                    validation_data)\n\n    def infer(self, input, start_sign, max_seq_len=30, stop_sign=None, build_output=None):\n        """"""\n        Inference API for given input\n\n        # Arguments\n        input: a sequence of data feed into encoder, eg: batch x seqLen x featureSize\n        start_sign: a ndarray which represents start and is fed into decoder\n        max_seq_len: max sequence length for final output\n        stop_sign: a ndarray that indicates model should stop infer further if current output\n        is the same with stopSign\n        build_output: Feeding model output to buildOutput to generate final result\n        """"""\n        jinput, input_is_table = Layer.check_input(input)\n        assert not input_is_table\n        jstart_sign, start_sign_is_table = Layer.check_input(start_sign)\n        assert not start_sign_is_table\n        if stop_sign:\n            jstop_sign, stop_sign_is_table = Layer.check_input(stop_sign)\n            assert not start_sign_is_table\n        else:\n            jstop_sign = None\n        results = callZooFunc(self.bigdl_type, ""seq2seqInfer"",\n                              self.value,\n                              jinput[0],\n                              jstart_sign[0],\n                              max_seq_len,\n                              jstop_sign[0] if jstop_sign else None,\n                              build_output)\n        return results\n'"
pyzoo/zoo/models/textclassification/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .text_classifier import *\n'"
pyzoo/zoo/models/textclassification/text_classifier.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport warnings\n\nfrom zoo.pipeline.api.keras.models import Sequential\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.models.common import ZooModel\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass TextClassifier(ZooModel):\n    """"""\n    The model used for text classification with WordEmbedding as its first layer.\n\n    # Arguments\n    class_num: The number of text categories to be classified. Positive int.\n    embedding_file: The path to the word embedding file.\n                    Currently only the following GloVe files are supported:\n                    ""glove.6B.50d.txt"", ""glove.6B.100d.txt"", ""glove.6B.200d.txt""\n                    ""glove.6B.300d.txt"", ""glove.42B.300d.txt"", ""glove.840B.300d.txt"".\n                    You can download from: https://nlp.stanford.edu/projects/glove/.\n    word_index: Dictionary of word (string) and its corresponding index (int).\n                The index is supposed to start from 1 with 0 reserved for unknown words.\n                During the prediction, if you have words that are not in the word_index\n                for the training, you can map them to index 0.\n                Default is None. In this case, all the words in the embedding_file will\n                be taken into account and you can call\n                WordEmbedding.get_word_index(embedding_file) to retrieve the dictionary.\n    sequence_length: The length of a sequence. Positive int. Default is 500.\n    encoder: The encoder for input sequences. String. \'cnn\' or \'lstm\' or \'gru\' are supported.\n             Default is \'cnn\'.\n    encoder_output_dim: The output dimension for the encoder. Positive int. Default is 256.\n    """"""\n\n    def __init__(self, class_num, embedding_file, word_index=None, sequence_length=500,\n                 encoder=""cnn"", encoder_output_dim=256, **kwargs):\n        if \'token_length\' in kwargs:\n            kwargs.pop(\'token_length\')\n            warnings.warn(\'The ""token_length"" argument in TextClassifier has been deprecated \'\n                          \'since 0.3.0, instead you should pass the arguments ""embedding_file"" \'\n                          \'and ""word_index"" to construct a TextClassifier with WordEmbedding \'\n                          \'as the first layer.\')\n        if \'bigdl_type\' in kwargs:\n            kwargs.pop(\'bigdl_type\')\n            self.bigdl_type = kwargs.get(""bigdl_type"")\n        else:\n            self.bigdl_type = ""float""\n        if kwargs:\n            raise TypeError(\'Wrong arguments for TextClassifier: \' + str(kwargs))\n        self.class_num = class_num\n        self.embedding = WordEmbedding(embedding_file, word_index, input_length=sequence_length)\n        self.sequence_length = sequence_length\n        self.encoder = encoder\n        self.encoder_output_dim = encoder_output_dim\n        self.model = self.build_model()\n        super(TextClassifier, self).__init__(None, self.bigdl_type,\n                                             int(class_num),\n                                             self.embedding,\n                                             int(sequence_length),\n                                             encoder,\n                                             int(encoder_output_dim),\n                                             self.model)\n\n    def build_model(self):\n        model = Sequential()\n        model.add(self.embedding)\n        if self.encoder.lower() == \'cnn\':\n            model.add(Convolution1D(self.encoder_output_dim, 5, activation=\'relu\'))\n            model.add(GlobalMaxPooling1D())\n        elif self.encoder.lower() == \'lstm\':\n            model.add(LSTM(self.encoder_output_dim))\n        elif self.encoder.lower() == \'gru\':\n            model.add(GRU(self.encoder_output_dim))\n        else:\n            raise ValueError(\'Unsupported encoder for TextClassifier: \' + self.encoder)\n        model.add(Dense(128))\n        model.add(Dropout(0.2))\n        model.add(Activation(\'relu\'))\n        model.add(Dense(self.class_num, activation=\'softmax\'))\n        return model\n\n    @staticmethod\n    def load_model(path, weight_path=None, bigdl_type=""float""):\n        """"""\n        Load an existing TextClassifier model (with weights).\n\n        # Arguments\n        path: The path for the pre-defined model.\n              Local file system, HDFS and Amazon S3 are supported.\n              HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n              Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        weight_path: The path for pre-trained weights if any. Default is None.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""loadTextClassifier"", path, weight_path)\n        model = ZooModel._do_load(jmodel, bigdl_type)\n        model.__class__ = TextClassifier\n        return model\n\n    # For the following methods, please refer to KerasNet for documentation.\n    def compile(self, optimizer, loss, metrics=None):\n        if isinstance(optimizer, six.string_types):\n            optimizer = to_bigdl_optim_method(optimizer)\n        if isinstance(loss, six.string_types):\n            loss = to_bigdl_criterion(loss)\n        if metrics and all(isinstance(metric, six.string_types) for metric in metrics):\n            metrics = to_bigdl_metrics(metrics, loss)\n        callZooFunc(self.bigdl_type, ""textClassifierCompile"",\n                    self.value,\n                    optimizer,\n                    loss,\n                    metrics)\n\n    def set_tensorboard(self, log_dir, app_name):\n        callZooFunc(self.bigdl_type, ""textClassifierSetTensorBoard"",\n                    self.value,\n                    log_dir,\n                    app_name)\n\n    def set_checkpoint(self, path, over_write=True):\n        callZooFunc(self.bigdl_type, ""textClassifierSetCheckpoint"",\n                    self.value,\n                    path,\n                    over_write)\n\n    def fit(self, x, batch_size=32, nb_epoch=10, validation_data=None):\n        """"""\n        Fit on TextSet.\n        """"""\n        assert isinstance(x, TextSet), ""x should be a TextSet""\n        if validation_data:\n            assert isinstance(validation_data, TextSet), ""validation_data should be a TextSet""\n        callZooFunc(self.bigdl_type, ""textClassifierFit"",\n                    self.value,\n                    x,\n                    batch_size,\n                    nb_epoch,\n                    validation_data)\n\n    def evaluate(self, x, batch_size=32):\n        """"""\n        Evaluate on TextSet.\n        """"""\n        assert isinstance(x, TextSet), ""x should be a TextSet""\n        return callZooFunc(self.bigdl_type, ""textClassifierEvaluate"",\n                           self.value,\n                           x,\n                           batch_size)\n\n    def predict(self, x, batch_per_thread=4):\n        """"""\n        Predict on TextSet.\n        """"""\n        assert isinstance(x, TextSet), ""x should be a TextSet""\n        results = callZooFunc(self.bigdl_type, ""textClassifierPredict"",\n                              self.value,\n                              x,\n                              batch_per_thread)\n        return TextSet(results)\n'"
pyzoo/zoo/models/textmatching/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .text_matcher import *\nfrom .knrm import *\n'"
pyzoo/zoo/models/textmatching/knrm.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nimport zoo.pipeline.api.autograd as A\nfrom zoo.models.common import ZooModel\nfrom zoo.models.textmatching import TextMatcher\nfrom zoo.pipeline.api.keras.layers import Input, Embedding, Dense, Squeeze, prepare_embedding\nfrom zoo.pipeline.api.keras.models import Model\nfrom bigdl.util.common import JTensor\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass KNRM(TextMatcher):\n    """"""\n    Kernel-pooling Neural Ranking Model with RBF kernel.\n    https://arxiv.org/abs/1706.06613\n\n    # Arguments:\n    text1_length: Sequence length of text1 (query).\n    text2_length: Sequence length of text2 (doc).\n    embedding_file: The path to the word embedding file.\n                    Currently only the following GloVe files are supported:\n                    ""glove.6B.50d.txt"", ""glove.6B.100d.txt"", ""glove.6B.200d.txt""\n                    ""glove.6B.300d.txt"", ""glove.42B.300d.txt"", ""glove.840B.300d.txt"".\n                    You can download from: https://nlp.stanford.edu/projects/glove/.\n    word_index: Dictionary of word (string) and its corresponding index (int).\n                The index is supposed to start from 1 with 0 reserved for unknown words.\n                During the prediction, if you have words that are not in the word_index\n                for the training, you can map them to index 0.\n                Default is None. In this case, all the words in the embedding_file will\n                be taken into account and you can call\n                WordEmbedding.get_word_index(embedding_file) to retrieve the dictionary.\n    train_embed: Boolean. Whether to train the embedding layer or not. Default is True.\n    kernel_num: Int > 1. The number of kernels to use. Default is 21.\n    sigma: Float. Defines the kernel width, or the range of its softTF count. Default is 0.1.\n    exact_sigma: Float. The sigma used for the kernel that harvests exact matches\n                 in the case where RBF mu=1.0. Default is 0.001.\n    target_mode: String. The target mode of the model. Either \'ranking\' or \'classification\'.\n                 For ranking, the output will be the relevance score between text1 and text2 and\n                 you are recommended to use \'rank_hinge\' as loss for pairwise training.\n                 For classification, the last layer will be sigmoid and the output will be the\n                 probability between 0 and 1 indicating whether text1 is related to text2 and\n                 you are recommended to use \'binary_crossentropy\' as loss for binary classification.\n                 Default mode is \'ranking\'.\n    """"""\n\n    def __init__(self, text1_length, text2_length, embedding_file, word_index=None,\n                 train_embed=True, kernel_num=21, sigma=0.1, exact_sigma=0.001,\n                 target_mode=""ranking"", bigdl_type=""float""):\n        embed_weights = prepare_embedding(embedding_file, word_index,\n                                          randomize_unknown=True, normalize=True)\n        vocab_size, embed_size = embed_weights.shape\n        super(KNRM, self).__init__(text1_length, vocab_size, embed_size,\n                                   embed_weights, train_embed, target_mode, bigdl_type)\n        self.text2_length = text2_length\n        assert kernel_num > 1, ""kernel_num must be an int larger than 1""\n        self.kernel_num = kernel_num\n        self.sigma = float(sigma)\n        self.exact_sigma = float(exact_sigma)\n        self.model = self.build_model()\n        super(TextMatcher, self).__init__(None, self.bigdl_type,\n                                          self.text1_length,\n                                          self.text2_length,\n                                          self.vocab_size,\n                                          self.embed_size,\n                                          JTensor.from_ndarray(embed_weights),\n                                          self.train_embed,\n                                          self.kernel_num,\n                                          self.sigma,\n                                          self.exact_sigma,\n                                          self.target_mode,\n                                          self.model)\n\n    def build_model(self):\n        # Remark: Share weights for embedding is not supported.\n        # Thus here the model takes concatenated input and slice to split the input.\n        input = Input(name=\'input\', shape=(self.text1_length + self.text2_length,))\n        embedding = Embedding(self.vocab_size, self.embed_size,\n                              weights=self.embed_weights, trainable=self.train_embed)(input)\n        query_embed = embedding.slice(1, 0, self.text1_length)\n        doc_embed = embedding.slice(1, self.text1_length, self.text2_length)\n        mm = A.batch_dot(query_embed, doc_embed, axes=[2, 2])  # Translation Matrix.\n        KM = []\n        for i in range(self.kernel_num):\n            mu = 1. / (self.kernel_num - 1) + (2. * i) / (self.kernel_num - 1) - 1.0\n            sigma = self.sigma\n            if mu > 1.0:  # Exact match.\n                sigma = self.exact_sigma\n                mu = 1.0\n            mm_exp = A.exp((-0.5) * (mm - mu) * (mm - mu) / sigma / sigma)\n            mm_doc_sum = A.sum(mm_exp, axis=2)\n            mm_log = A.log(mm_doc_sum + 1.0)\n            # Remark: Keep the reduced dimension for the last sum and squeeze after stack.\n            # Otherwise, when batch=1, the output will become a Scalar not compatible for stack.\n            mm_sum = A.sum(mm_log, axis=1, keepDims=True)\n            KM.append(mm_sum)\n        Phi = Squeeze(2)(A.stack(KM))\n        if self.target_mode == ""ranking"":\n            output = Dense(1, init=""uniform"")(Phi)\n        else:\n            output = Dense(1, init=""uniform"", activation=""sigmoid"")(Phi)\n        model = Model(input=input, output=output)\n        return model\n\n    @staticmethod\n    def load_model(path, weight_path=None, bigdl_type=""float""):\n        """"""\n        Load an existing KNRM model (with weights).\n\n        # Arguments\n        path: The path for the pre-defined model.\n              Local file system, HDFS and Amazon S3 are supported.\n              HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n              Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        weight_path: The path for pre-trained weights if any. Default is None.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""loadKNRM"", path, weight_path)\n        model = ZooModel._do_load(jmodel, bigdl_type)\n        model.__class__ = KNRM\n        return model\n'"
pyzoo/zoo/models/textmatching/text_matcher.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.models.common import *\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass TextMatcher(ZooModel, Ranker):\n    """"""\n    The base class for text matching models in Analytics Zoo.\n    Referred to MatchZoo implementation: https://github.com/NTMC-Community/MatchZoo\n    """"""\n    def __init__(self, text1_length, vocab_size, embed_size=300, embed_weights=None,\n                 train_embed=True, target_mode=""ranking"", bigdl_type=""float""):\n        self.text1_length = text1_length\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.embed_weights = embed_weights\n        self.train_embed = train_embed\n        assert target_mode == ""ranking"" or target_mode == ""classification"",\\\n            ""target_mode should be either ranking or classification""\n        self.target_mode = target_mode\n        self.bigdl_type = bigdl_type\n'"
pyzoo/zoo/orca/data/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.orca.data.shard import XShards, RayXShards, SparkXShards\n'"
pyzoo/zoo/orca/data/file.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\n\n\ndef open_text(path):\n    # Return a list of lines\n    if path.startswith(""hdfs""):  # hdfs://url:port/file_path\n        import pyarrow as pa\n        fs = pa.hdfs.connect()\n        with fs.open(path, \'rb\') as f:\n            lines = f.read().decode(""utf-8"").split(""\\n"")\n    elif path.startswith(""s3""):  # s3://bucket/file_path\n        access_key_id = os.environ[""AWS_ACCESS_KEY_ID""]\n        secret_access_key = os.environ[""AWS_SECRET_ACCESS_KEY""]\n        import boto3\n        s3_client = boto3.Session(\n            aws_access_key_id=access_key_id,\n            aws_secret_access_key=secret_access_key).client(\'s3\', verify=False)\n        path_parts = path.split(""://"")[1].split(\'/\')\n        bucket = path_parts.pop(0)\n        key = ""/"".join(path_parts)\n        data = s3_client.get_object(Bucket=bucket, Key=key)\n        lines = data[""Body""].read().decode(""utf-8"").split(""\\n"")\n    else:  # Local path\n        lines = []\n        for line in open(path):\n            lines.append(line)\n    return [line.strip() for line in lines]\n\n\ndef open_image(path):\n    from PIL import Image\n    if path.startswith(""hdfs""):  # hdfs://url:port/file_path\n        import pyarrow as pa\n        fs = pa.hdfs.connect()\n        with fs.open(path, \'rb\') as f:\n            return Image.open(f)\n    elif path.startswith(""s3""):  # s3://bucket/file_path\n        access_key_id = os.environ[""AWS_ACCESS_KEY_ID""]\n        secret_access_key = os.environ[""AWS_SECRET_ACCESS_KEY""]\n        import boto3\n        from io import BytesIO\n        s3_client = boto3.Session(\n            aws_access_key_id=access_key_id,\n            aws_secret_access_key=secret_access_key).client(\'s3\', verify=False)\n        path_parts = path.split(""://"")[1].split(\'/\')\n        bucket = path_parts.pop(0)\n        key = ""/"".join(path_parts)\n        data = s3_client.get_object(Bucket=bucket, Key=key)\n        return Image.open(BytesIO(data[""Body""].read()))\n    else:  # Local path\n        return Image.open(path)\n\n\ndef load_numpy(path):\n    import numpy as np\n    if path.startswith(""hdfs""):  # hdfs://url:port/file_path\n        import pyarrow as pa\n        fs = pa.hdfs.connect()\n        with fs.open(path, \'rb\') as f:\n            return np.load(f)\n    elif path.startswith(""s3""):  # s3://bucket/file_path\n        access_key_id = os.environ[""AWS_ACCESS_KEY_ID""]\n        secret_access_key = os.environ[""AWS_SECRET_ACCESS_KEY""]\n        import boto3\n        from io import BytesIO\n        s3_client = boto3.Session(\n            aws_access_key_id=access_key_id,\n            aws_secret_access_key=secret_access_key).client(\'s3\', verify=False)\n        path_parts = path.split(""://"")[1].split(\'/\')\n        bucket = path_parts.pop(0)\n        key = ""/"".join(path_parts)\n        data = s3_client.get_object(Bucket=bucket, Key=key)\n        return np.load(BytesIO(data[""Body""].read()))\n    else:  # Local path\n        return np.load(path)\n\n\ndef exists(path):\n    if path.startswith(""hdfs""):  # hdfs://url:port/file_path\n        import pyarrow as pa\n        fs = pa.hdfs.connect()\n        return fs.exists(path)\n    elif path.startswith(""s3""):  # s3://bucket/file_path\n        access_key_id = os.environ[""AWS_ACCESS_KEY_ID""]\n        secret_access_key = os.environ[""AWS_SECRET_ACCESS_KEY""]\n        import boto3\n        s3_client = boto3.Session(\n            aws_access_key_id=access_key_id,\n            aws_secret_access_key=secret_access_key).client(\'s3\', verify=False)\n        path_parts = path.split(""://"")[1].split(\'/\')\n        bucket = path_parts.pop(0)\n        key = ""/"".join(path_parts)\n        try:\n            s3_client.get_object(Bucket=bucket, Key=key)\n        except Exception as ex:\n            if ex.response[\'Error\'][\'Code\'] == \'NoSuchKey\':\n                return False\n            raise ex\n        return True\n    else:\n        return os.path.exists(path)\n\n\ndef makedirs(path):\n    if path.startswith(""hdfs""):  # hdfs://url:port/file_path\n        import pyarrow as pa\n        fs = pa.hdfs.connect()\n        if not fs.exists(path):\n            return fs.mkdir(path)\n    elif path.startswith(""s3""):  # s3://bucket/file_path\n        access_key_id = os.environ[""AWS_ACCESS_KEY_ID""]\n        secret_access_key = os.environ[""AWS_SECRET_ACCESS_KEY""]\n        import boto3\n        s3_client = boto3.Session(\n            aws_access_key_id=access_key_id,\n            aws_secret_access_key=secret_access_key).client(\'s3\', verify=False)\n        path_parts = path.split(""://"")[1].split(\'/\')\n        bucket = path_parts.pop(0)\n        key = ""/"".join(path_parts)\n        return s3_client.put_object(Bucket=bucket, Key=key, Body=\'\')\n    else:\n        return os.makedirs(path)\n'"
pyzoo/zoo/orca/data/shard.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.orca.data.utils import *\nimport os\n\n\nclass XShards(object):\n    """"""\n    A collection of data which can be pre-processed parallelly.\n    """"""\n\n    def transform_shard(self, func, *args):\n        """"""\n        Transform each shard in the XShards using func\n        :param func: pre-processing function\n        :param args: arguments for the pre-processing function\n        :return: DataShard\n        """"""\n        pass\n\n    def collect(self):\n        """"""\n        Returns a list that contains all of the elements in this XShards\n        :return: list of elements\n        """"""\n        pass\n\n    def num_partitions(self):\n        """"""\n        return the number of partitions in this XShards\n        :return: an int\n        """"""\n        pass\n\n\nclass RayXShards(XShards):\n    """"""\n    A collection of data which can be pre-processed parallelly on Ray\n    """"""\n    def __init__(self, partitions):\n        self.partitions = partitions\n        self.shard_list = flatten([partition.shard_list for partition in partitions])\n\n    def transform_shard(self, func, *args):\n        """"""\n        Transform each shard in the XShards using func\n        :param func: pre-processing function.\n        In the function, the element object should be the first argument\n        :param args: rest arguments for the pre-processing function\n        :return: this DataShard\n        """"""\n        import ray\n        done_ids, undone_ids = ray.wait([shard.transform.remote(func, *args)\n                                         for shard in self.shard_list],\n                                        num_returns=len(self.shard_list))\n        assert len(undone_ids) == 0\n        return self\n\n    def collect(self):\n        """"""\n        Returns a list that contains all of the elements in this XShards\n        :return: list of elements\n        """"""\n        import ray\n        return ray.get([shard.get_data.remote() for shard in self.shard_list])\n\n    def num_partitions(self):\n        return len(self.partitions)\n\n    def repartition(self, num_partitions):\n        """"""\n        Repartition XShards.\n        :param num_partitions: number of partitions\n        :return: this XShards\n        """"""\n        shards_partitions = list(chunk(self.shard_list, num_partitions))\n        self.partitions = [RayPartition(shards) for shards in shards_partitions]\n        return self\n\n    def get_partitions(self):\n        """"""\n        Return partition list of the XShards\n        :return: partition list\n        """"""\n        return self.partitions\n\n\nclass RayPartition(object):\n    """"""\n    Partition of RayXShards\n    """"""\n\n    def __init__(self, shard_list):\n        self.shard_list = shard_list\n\n    def get_data(self):\n        return [shard.get_data.remote() for shard in self.shard_list]\n\n\ndef get_eager_mode():\n    is_eager = True\n    if os.getenv(""EAGER_EXECUTION""):\n        eager_execution = os.getenv(""EAGER_EXECUTION"").lower()\n        if eager_execution == ""false"":\n            is_eager = False\n    return is_eager\n\n\nclass SparkXShards(XShards):\n    def __init__(self, rdd):\n        self.rdd = rdd\n        self.user_cached = False\n        self.eager = get_eager_mode()\n        self.rdd.cache()\n        if self.eager:\n            self.compute()\n\n    def transform_shard(self, func, *args):\n        transformed_shard = SparkXShards(self.rdd.map(lambda data: func(data, *args)))\n        self._uncache()\n        return transformed_shard\n\n    def collect(self):\n        return self.rdd.collect()\n\n    def cache(self):\n        self.user_cached = True\n        self.rdd.cache()\n        return self\n\n    def uncache(self):\n        self.user_cached = False\n        self.rdd.unpersist()\n        return self\n\n    def _uncache(self):\n        if not self.user_cached:\n            self.uncache()\n\n    def is_cached(self):\n        return self.rdd.is_cached\n\n    def compute(self):\n        self.rdd.count()\n        return self\n\n    def num_partitions(self):\n        return self.rdd.getNumPartitions()\n\n    def repartition(self, num_partitions):\n        repartitioned_shard = SparkXShards(self.rdd.repartition(num_partitions))\n        self._uncache()\n        return repartitioned_shard\n\n    def partition_by(self, cols, num_partitions=None):\n        import pandas as pd\n        elem_class, columns = self.rdd.map(\n            lambda data: (type(data), data.columns) if isinstance(data, pd.DataFrame)\n            else (type(data), None)).first()\n        if issubclass(elem_class, pd.DataFrame):\n            # if partition by a column\n            if isinstance(cols, str):\n                if cols not in columns:\n                    raise Exception(""The partition column is not in the DataFrame"")\n                # change data to key value pairs\n                rdd = self.rdd.flatMap(\n                    lambda df: df.apply(lambda row: (row[cols], row.values.tolist()), axis=1)\n                    .values.tolist())\n\n                partition_num = self.rdd.getNumPartitions() if not num_partitions \\\n                    else num_partitions\n                # partition with key\n                partitioned_rdd = rdd.partitionBy(partition_num)\n            else:\n                raise Exception(""Only support partition by a column name"")\n\n            def merge(iterator):\n                data = [value[1] for value in list(iterator)]\n                if data:\n                    df = pd.DataFrame(data=data, columns=columns)\n                    return [df]\n                else:\n                    # no data in this partition\n                    return []\n            # merge records to df in each partition\n            partitioned_shard = SparkXShards(partitioned_rdd.mapPartitions(merge))\n            self._uncache()\n            return partitioned_shard\n        else:\n            raise Exception(""Currently only support partition by for XShards""\n                            "" of Pandas DataFrame"")\n\n    def unique(self, key):\n        import pandas as pd\n        elem_class, columns = self.rdd.map(\n            lambda data: (type(data), data.columns) if isinstance(data, pd.DataFrame)\n            else (type(data), None)).first()\n        if issubclass(elem_class, pd.DataFrame):\n            if key is None:\n                raise Exception(""Cannot apply unique operation on XShards of Pandas Dataframe""\n                                "" without column name"")\n            if key in columns:\n                rdd = self.rdd.map(lambda df: df[key].unique())\n                import numpy as np\n                result = rdd.reduce(lambda list1, list2: pd.unique(np.concatenate((list1, list2),\n                                                                                  axis=0)))\n                return result\n            else:\n                raise Exception(""The select key is not in the DataFrame in this XShards"")\n        else:\n            # we may support numpy or other types later\n            raise Exception(""Currently only support unique() on XShards of Pandas DataFrame"")\n\n    def split(self):\n        """"""\n        Split SparkXShards into multiple SparkXShards.\n        Each element in the SparkXShards needs be a list or tuple with same length.\n        :return: Splits of SparkXShards. If element in the input SparkDataShard is not\n                list or tuple, return list of input SparkDataShards.\n        """"""\n        # get number of splits\n        list_split_length = self.rdd.map(lambda data: len(data) if isinstance(data, list) or\n                                         isinstance(data, tuple) else 1).collect()\n        # check if each element has same splits\n        if list_split_length.count(list_split_length[0]) != len(list_split_length):\n            raise Exception(""Cannot split this XShards because its partitions ""\n                            ""have different split length"")\n        else:\n            if list_split_length[0] > 1:\n                def get_data(order):\n                    def transform(data):\n                        return data[order]\n                    return transform\n                split_shard_list = [SparkXShards(self.rdd.map(get_data(i)))\n                                    for i in range(list_split_length[0])]\n                self._uncache()\n                return split_shard_list\n            else:\n                return [self]\n\n    def len(self, key=None):\n        if key is None:\n            return self.rdd.map(lambda data: len(data) if hasattr(data, \'__len__\') else 1)\\\n                .reduce(lambda l1, l2: l1 + l2)\n        else:\n\n            def get_len(data):\n                assert hasattr(data, \'__getitem__\'), \\\n                    ""No selection operation available for this XShards""\n                try:\n                    value = data[key]\n                except:\n                    raise Exception(""Invalid key for this XShards"")\n                return len(value) if hasattr(value, \'__len__\') else 1\n            return self.rdd.map(get_len).reduce(lambda l1, l2: l1 + l2)\n\n    def save_pickle(self, path, batchSize=10):\n        self.rdd.saveAsPickleFile(path, batchSize)\n        return self\n\n    @classmethod\n    def load_pickle(cls, path, sc, minPartitions=None):\n        return SparkXShards(sc.pickleFile(path, minPartitions))\n\n    def __del__(self):\n        self.uncache()\n'"
pyzoo/zoo/orca/data/utils.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\n\nfrom zoo.common import get_file_list\n\n\n# split list into n chunks\ndef chunk(lst, n):\n    size = len(lst) // n\n    leftovers = lst[size * n:]\n    for c in range(n):\n        if leftovers:\n            extra = [leftovers.pop()]\n        else:\n            extra = []\n        yield lst[c * size:(c + 1) * size] + extra\n\n\ndef flatten(list_of_list):\n    flattend = [item for sublist in list_of_list for item in sublist]\n    return flattend\n\n\ndef list_s3_file(file_path, file_type, env):\n    path_parts = file_path.split(\'/\')\n    bucket = path_parts.pop(0)\n    key = ""/"".join(path_parts)\n\n    access_key_id = env[""AWS_ACCESS_KEY_ID""]\n    secret_access_key = env[""AWS_SECRET_ACCESS_KEY""]\n\n    import boto3\n    s3_client = boto3.Session(\n        aws_access_key_id=access_key_id,\n        aws_secret_access_key=secret_access_key,\n    ).client(\'s3\', verify=False)\n    # file\n    if os.path.splitext(file_path)[1] != \'\':\n        return [""s3://"" + file_path]\n    else:\n        keys = []\n        resp = s3_client.list_objects_v2(Bucket=bucket,\n                                         Prefix=key)\n        for obj in resp[\'Contents\']:\n            keys.append(obj[\'Key\'])\n        # only get json/csv files\n        files = [file for file in keys if os.path.splitext(file)[1] == ""."" + file_type]\n        file_paths = [os.path.join(""s3://"" + bucket, file) for file in files]\n        return file_paths\n\n\ndef extract_one_path(file_path, file_type, env):\n    file_url_splits = file_path.split(""://"")\n    prefix = file_url_splits[0]\n    if prefix == ""s3"":\n        file_paths = list_s3_file(file_url_splits[1], file_type, env)\n    elif prefix == ""hdfs"":\n        import pyarrow as pa\n        fs = pa.hdfs.connect()\n        if fs.isfile(file_path):\n            return [file_path]\n        else:\n            file_paths = get_file_list(file_path)\n            # only get json/csv files\n            file_paths = [file for file in file_paths\n                          if os.path.splitext(file)[1] == ""."" + file_type]\n    else:\n        if os.path.isfile(file_path):\n            return [file_path]\n        else:\n            file_paths = get_file_list(file_path)\n            # only get json/csv files\n            file_paths = [file for file in file_paths\n                          if os.path.splitext(file)[1] == ""."" + file_type]\n    return file_paths\n'"
pyzoo/zoo/orca/learn/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/pipeline/api/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/pipeline/api/autograd.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom bigdl.nn.layer import Layer, Node\nfrom bigdl.util.common import to_list, JTensor\n\nfrom zoo.common.utils import callZooFunc\nimport zoo.pipeline.api.keras.base as kbase\nfrom zoo.pipeline.api.keras.objectives import LossFunction\nfrom zoo.pipeline.api.utils import remove_batch, toMultiShape\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\ndef mean(x, axis=0, keepDims=False):\n    """"""\n    Mean of a variable, alongside the specified axis.\n    :param x: A variable.\n    :param axis: A list of integer. Axes to compute the mean.\n    :param keepDims: A boolean, whether to keep the dimensions or not.\n            If `keepDims` is `False`, the rank of the variable is reduced\n            by 1 for each entry in `axis`. If `keepDims` is `True`,\n            the reduced dimensions are retained with length 1.\n    :return: A variable with the mean of elements of `x`.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""mean"", x, axis, keepDims))\n\n\ndef abs(x):\n    """"""\n    Element-wise absolute value.\n    :param x: A variable.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""abs"", x))\n\n\ndef batch_dot(x, y, axes=1, normalize=False):\n    """"""\n    Operator that computes a dot product between samples in two tensors.\n\n    E.g. if applied to two tensors `a` and `b` of shape `(batch_size, n)`,\n    the output will be a tensor of shape `(batch_size, 1)`\n    where each entry `i` will be the dot product between\n    `a[i]` and `b[i]`.\n\n    :param x: Shape should only be [batch, xx]\n    :param y: Shape should only be [batch, xx]\n    :param axes: Integer or tuple of integers,\n                axis or axes along which to take the dot product.\n    :param normalize: Whether to L2-normalize samples along the\n                dot product axis before taking the dot product.\n                If set to True, then the output of the dot product\n                is the cosine proximity between the two samples.\n    :return: A variable.\n    """"""\n    if not normalize:\n        if isinstance(axes, int):\n            axes = [axes] * 2\n    return Variable.from_jvalue(callZooFunc(""float"", ""batchDot"", x, y, axes, normalize))\n\n\ndef l2_normalize(x, axis):\n    """"""\n    Normalizes a tensor wrt the L2 norm alongside the specified axis.\n    :param x: A variable. Shape should only be [batch, xx]\n    :param axis: axis along which to perform normalization.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""l2Normalize"", x, int(axis)))\n\n\ndef sum(x, axis=0, keepDims=False):\n    """"""\n    Sum of the values in a a variable, alongside the specified axis.\n    :param x: A variable.\n    :param axis: An integer. Axes to compute the sum over.\n    :param keepDims: A boolean, whether to keep the dimensions or not.\n            If `keepDims` is `False`, the rank of the variable is reduced\n            by 1 for each entry in `axis`. If `keepDims` is `True`,\n            the reduced dimensions are retained with length 1.\n    :return: A variable with sum of `x`.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""sum"", x, axis, keepDims))\n\n\ndef stack(inputs, axis=1):\n    """"""\n    Stacks a list of rank `R` tensors into a rank `R+1` tensor.\n    You should start from 1 as dim 0 is for batch.\n    :param inputs: List of variables (tensors).\n    :param axis: axis along which to perform stacking.\n    :return:\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""stack"", inputs, axis))\n\n\ndef expand_dims(x, axis):\n    """"""\n   Adds a 1-sized dimension at index ""axis"".\n    :param x: a Variable to be expanded\n    :param axis: axis Position where to add a new axis.\n    The axis is 0 based and if you set the axis to 0, you would change the batch dim.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""expandDims"", x, axis))\n\n\ndef clip(x, min, max):\n    """"""\n    Element-wise value clipping.\n    :param x: A variable.\n    :param min: Python float or integer.\n    :param max: Python float or integer.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""clip"", x, float(min), float(max)))\n\n\ndef contiguous(x):\n    """"""\n    Turn the output and grad to be contiguous for the input Variable\n    :param x: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""contiguous"", x))\n\n\ndef square(x):\n    """"""\n    Element-wise square.\n    :param x: A variable.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""square"", x))\n\n\ndef sqrt(x):\n    """"""\n    Element-wise square root.\n    :param x: A variable.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""sqrt"", x))\n\n\ndef exp(x):\n    """"""\n    Element-wise exponential.\n    :param x: A variable.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""exp"", x))\n\n\ndef maximum(x, y):\n    """"""\n    Element-wise maximum of two variables.\n    :param x: A variable.\n    :param y: A variable.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""maximum"", x, y))\n\n\ndef log(x):\n    """"""\n    Element-wise log.\n    :param x: A variable.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""log"", x))\n\n\ndef pow(x, a):\n    """"""\n    Element-wise exponentiation.\n    :param x: A variable.\n    :param a: Python integer.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""pow"", x, float(a)))\n\n\ndef epsilon():\n    """"""\n    Define the value of epsilon.\n    :return: A value of type Double.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""epsilon""))\n\n\ndef neg(x):\n    """"""\n    Computes numerical negative value element-wise.\n    :param x: A variable.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""neg"", x))\n\n\ndef softsign(x):\n    """"""\n    Softsign of a variable.\n    :param x: A variable.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""softsign"", x))\n\n\ndef softplus(x):\n    """"""\n    Softplus of a variable.\n    :param x: A variable.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""softplus"", x))\n\n\ndef mm(x, y, axes=None):\n    """"""\n    Module to perform matrix multiplication on two mini-batch inputs,\n    producing a mini-batch.\n    :param x: A variable.\n    :param y: A variable.\n    :param axes: Axes along which to perform multiplication.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""mm"", x, y, axes))\n\n\ndef erf(x):\n    """"""\n    Computes the error function(Gauss error function) of each element.\n    :param x: A variable.\n    :return: A variable.\n    """"""\n    return Variable.from_jvalue(callZooFunc(""float"", ""erf"", x))\n\n\nclass VariableOperator(object):\n    # TODO: we need to add a mapping for Shape here.\n    def __to_batch_shape(cls, shape):\n        return tuple([None] + shape[1:])\n\n    def __process_shape(self, shape):\n        if len(shape) == 1:\n            return self.__to_batch_shape(shape[0])\n        else:\n            return [self.__to_batch_shape(s) for s in shape]\n\n    def get_input_shape(self):\n        return self.__process_shape(callZooFunc(""float"", ""varGetInputShape"", self))\n\n    def get_output_shape(self):\n        return self.__process_shape(callZooFunc(""float"", ""varGetOutputShape"", self))\n\n    @property\n    def shape(self):\n        return self.get_output_shape()\n\n    @staticmethod\n    def from_jvalue(jvalue):\n        return Variable(input_shape=None, node=None, jvalue=jvalue)\n\n    def add(self, var):\n        return Variable.from_jvalue(callZooFunc(""float"", ""add"", self, var))\n        # self.value.getClass().getSimpleName()\n\n    def sub(self, var):\n        return Variable.from_jvalue(callZooFunc(""float"", ""sub"", self, var))\n\n    def __sub__(self, other):\n        return self.sub(other)\n\n    def __rsub__(self, other):\n        return Variable.from_jvalue(callZooFunc(""float"", ""sub"", other, self))\n\n    def __add__(self, other):\n        return self.add(other)\n\n    __radd__ = __add__\n\n    def __mul__(self, other):\n        return Variable.from_jvalue(callZooFunc(""float"", ""mul"", self, other))\n\n    __rmul__ = __mul__\n\n    def __div__(self, other):\n        return Variable.from_jvalue(callZooFunc(""float"", ""div"", self, other))\n\n    __truediv__ = __div__\n\n    def __rdiv__(self, other):\n        return Variable.from_jvalue(callZooFunc(""float"", ""div"", other, self))\n\n    __rtruediv__ = __rdiv__\n\n    def __neg__(self):\n        return neg(self)\n\n    def slice(self, dim, start_index, length):\n        """"""\n        Same as narrow in Torch.\n        Slice the input with the number of dimensions not being reduced.\n        The batch dimension needs to be unchanged.\n        For example, if input is:\n        1 2 3\n        4 5 6\n        slice(1, 1, 2) will give output\n        2 3\n        5 6\n        slice(1, 2, -1) will give output\n        3\n        6\n        :param  dim The dimension to narrow. 0-based index. Cannot narrow the batch dimension.\n                -1 means the last dimension of the input.\n        :param  startIndex Non-negative integer.\n                The start index on the given dimension. 0-based index.\n        :param length The length to be sliced. Default is 1.\n        """"""\n        return Variable.from_jvalue(\n            callZooFunc(""float"", ""slice"", self, dim, start_index, length))\n\n    def index_select(self, dim, index):\n        """"""\n           Select an index of the input in the given dim and return the subset part.\n           The batch dimension needs to be unchanged.\n           The selected dim would be remove after this operation.\n           For example, if input is:\n           1 2 3\n           4 5 6\n           Select(1, 1) will give output [2 5]\n           Select(1, -1) will give output [3 6]\n        :param dim: The dimension to select. 0-based index. Cannot select the batch dimension.\n                -1 means the last dimension of the input.\n        :param index: The index of the dimension to be selected. 0-based index.\n               -1 means the last dimension of the input.\n        :return:\n        """"""\n        return Variable.from_jvalue(callZooFunc(""float"", ""indexSelect"", self, dim, index))\n\n    def squeeze(self, dim=None):\n        """"""\n        Delete the singleton dimension(s).\n        The dim can be zero, and if so you would change the batch dim.\n        For example, if input has size (2, 1, 3, 4, 1):\n        Squeeze(dim = 1) will give output size (2, 3, 4, 1)\n        Squeeze(dims = null) will give output size (2, 3, 4)\n        """"""\n        return Variable.from_jvalue(callZooFunc(""float"", ""squeeze"", self, dim))\n\n\nclass Variable(kbase.ZooKerasCreator, VariableOperator):\n    def __init__(self, input_shape, node=None, jvalue=None, name=None):\n        self.name = name\n        if jvalue:\n            self.value = jvalue\n            self.bigdl_type = ""float""\n        else:\n            if node:\n                super(Variable, self).__init__(jvalue, ""float"", node, name)\n            else:\n                super(Variable, self).__init__(jvalue, ""float"", toMultiShape(input_shape), name)\n\n    def set_name(self, name):\n        self.node.element().set_name(name)\n\n    @classmethod\n    def from_node(cls, node):\n        return cls(input_shape=None, node=node)\n\n    @property\n    def node(self):\n        return Node.of(self.value.node())\n\n\nclass Lambda(kbase.ZooKerasCreator):\n    """"""Used for evaluating an arbitrary expressions on an input.\n\n       # Examples\n\n       ```python\n           # add a x -> x + 2 layer\n           model.add(Lambda(lambda x: x + 2))\n       ```\n       # Arguments\n           function: The function to be evaluated.\n               Takes input tensor as first argument.\n\n       # Input shape\n           Arbitrary. Use the keyword argument input_shape\n           (tuple of integers, does not include the samples axis)\n           when using this layer as the first layer in a model.\n       """"""\n\n    def __init__(self, function, input_shape=None, bigdl_type=""float""):\n        self.function = function\n        self.input_shape = input_shape\n        self.bigdl_type = bigdl_type\n\n    def __call__(self, x=None):\n        """"""\n        Some other modules point to current module\n        :param x: upstream module nodes. x is either a Node or list of Node.\n        :return: node containing current module\n        """"""\n        x = to_list(x if x else [])\n        layer = self\n        if isinstance(self, Lambda):\n            input_shapes = [var.get_output_shape() for var in x]\n            layer = self.create(remove_batch(input_shapes))\n        return Variable.from_jvalue(callZooFunc(self.bigdl_type,\n                                                ""connectInputs"",\n                                                layer,\n                                                to_list(x)))\n\n    # input_shapes should not contain batch dim\n    def create(self, input_shapes):\n        input_shapes = toMultiShape(input_shapes)\n        inputs = [Variable(list(output_shape)) for output_shape in input_shapes]\n        return LambdaLayer(input_vars=inputs,\n                           out_var=self.function(*inputs),\n                           input_shape=input_shapes)\n\n\nclass LambdaLayer(kbase.ZooKerasLayer):\n    def __init__(self, input_vars, out_var, input_shape=None, **kwargs):\n        super(LambdaLayer, self).__init__(None,\n                                          input_vars,\n                                          out_var,\n                                          list(input_shape) if input_shape else None,\n                                          **kwargs)\n\n\nclass Parameter(kbase.ZooKerasLayer, VariableOperator):\n    """"""\n    A trainable Variable. The default init_method is RandomUniform(-0.05, 0.05).\n    You can also specify the init_weight by passing a ndarray.\n    :param shape: Shape of this Parameter\n    :param init_method: A method used to initialize the Parameter.\n                        The default value is RandomUniform(-0.05, 0.05)\n    :param init_weight: A ndarray as the init value.\n    :param trainable It\'s true by default, meaning the value would be updated by gradient.\n    """"""\n\n    def __init__(self, shape, init_method=None,\n                 init_weight=None, trainable=True, **kwargs):\n        if not init_method:\n            from bigdl.nn.initialization_method import RandomUniform\n            init_method = RandomUniform(-0.05, 0.05)\n        super(Parameter, self).__init__(None,\n                                        list(shape),\n                                        init_method,\n                                        kbase.JTensor.from_ndarray(init_weight),\n                                        trainable,\n                                        **kwargs)\n\n    @property\n    def shape(self):\n        return self.get_weight().shape\n\n    def get_weight(self):\n        """"""\n        :return: the ndarray for the current weight\n        """"""\n        jtensor = callZooFunc(self.bigdl_type,\n                              ""getParameterWeight"",\n                              self)\n        return jtensor.to_ndarray()\n\n    def set_weight(self, value):\n        """"""\n        :param value: value is a ndarray\n        :return:\n        """"""\n        callZooFunc(self.bigdl_type,\n                    ""setParameterWeight"",\n                    self,\n                    kbase.JTensor.from_ndarray(value))\n\n\nclass Constant(kbase.ZooKerasCreator, VariableOperator):\n    """"""\n    A constant Variable without weights.\n    :param data: value of the Variable.\n    :param name: Optional. Name of the Variable\n    """"""\n\n    def __init__(self, data, name=None, bigdl_type=""float""):\n        self.data = data\n        super(Constant, self).__init__(None, bigdl_type, JTensor.from_ndarray(data), name)\n\n\nclass CustomLoss(LossFunction):\n    def __init__(self, loss_func, y_pred_shape, y_true_shape=None):\n        """"""\n        :param loss_func: a function which accept y_true and y_pred\n        :param y_pred_shape: The pred shape without batch dim.\n        :param y_true_shape: The target shape without batch dim.\n               It should be the same as y_pred_shape by default.\n        i.e input_shape=[3], then the feeding data would be [None, 3]\n        """"""\n\n        y_real = Variable(input_shape=y_true_shape if y_true_shape else y_pred_shape)\n        y_pred = Variable(input_shape=y_pred_shape)\n        loss_var = loss_func(y_real, y_pred)\n        super(CustomLoss, self).__init__(None, ""float"", [y_real, y_pred], loss_var)\n\n    def forward(self, y_true, y_pred):\n        """"""\n        NB: It\'s for debug only, please use optimizer.optimize() in production.\n        Takes an input object, and computes the corresponding loss of the criterion,\n        compared with `target`\n\n        :param input: ndarray or list of ndarray\n        :param target: ndarray or list of ndarray\n        :return: value of loss\n        """"""\n        input = y_pred\n        target = y_true\n        jinput, input_is_table = Layer.check_input(input)\n        jtarget, target_is_table = Layer.check_input(target)\n        output = callZooFunc(self.bigdl_type,\n                             ""criterionForward"",\n                             self.value,\n                             jinput,\n                             input_is_table,\n                             jtarget,\n                             target_is_table)\n        return output\n\n    def backward(self, y_true, y_pred):\n        """"""\n        NB: It\'s for debug only, please use optimizer.optimize() in production.\n        Performs a back-propagation step through the criterion, with respect to the given input.\n\n        :param input: ndarray or list of ndarray\n        :param target: ndarray or list of ndarray\n        :return: ndarray\n        """"""\n        input = y_pred\n        target = y_true\n        jinput, input_is_table = Layer.check_input(input)\n        jtarget, target_is_table = Layer.check_input(target)\n        output = callZooFunc(self.bigdl_type,\n                             ""criterionBackward"",\n                             self.value,\n                             jinput,\n                             input_is_table,\n                             jtarget,\n                             target_is_table)\n        return Layer.convert_output(output)\n'"
pyzoo/zoo/pipeline/api/utils.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\ndef toMultiShape(shape):\n    if any(isinstance(i, list) for i in shape):  # multi shape\n        return shape\n    elif any(isinstance(i, tuple) for i in shape):\n        return [list(s) for s in shape]\n    elif isinstance(shape, tuple):\n        return [list(shape)]\n    else:\n        return [shape]\n\n\n# TODO: create a shape mapping here.\ndef remove_batch(shape):\n    if any(isinstance(i, list) or isinstance(i, tuple) for i in shape):  # multi shape\n        return [remove_batch(s) for s in shape]\n    else:\n        return list(shape[1:])\n'"
pyzoo/zoo/pipeline/estimator/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .estimator import *\n'"
pyzoo/zoo/pipeline/estimator/estimator.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.util.common import JavaValue\nfrom zoo.common.utils import callZooFunc\n\n\nclass Estimator(JavaValue):\n    """"""\n    Estimator class for training and evaluation BigDL models.\n\n    Estimator wraps a model, and provide an uniform training, evaluation or prediction operation on\n    both local host and distributed spark environment.\n    """"""\n\n    def __init__(self, model, optim_methods=None, model_dir=None, jvalue=None, bigdl_type=""float""):\n        self.bigdl_type = bigdl_type\n        self.value = jvalue if jvalue else callZooFunc(\n            bigdl_type, self.jvm_class_constructor(), model, optim_methods, model_dir)\n\n    def clear_gradient_clipping(self):\n        """"""\n        Clear gradient clipping parameters. In this case, gradient clipping will not be applied.\n        In order to take effect, it needs to be called before fit.\n        :return:\n        """"""\n        callZooFunc(self.bigdl_type, ""clearGradientClipping"")\n\n    def set_constant_gradient_clipping(self, min, max):\n        """"""\n        Set constant gradient clipping during the training process.\n        In order to take effect, it needs to be called before fit.\n        :param min: The minimum value to clip by.\n        :param max: The maximum value to clip by.\n        :return:\n        """"""\n        callZooFunc(self.bigdl_type, ""setConstantGradientClipping"", self.value, min, max)\n\n    def set_l2_norm_gradient_clipping(self, clip_norm):\n        """"""\n        Clip gradient to a maximum L2-Norm during the training process.\n        In order to take effect, it needs to be called before fit.\n        :param clip_norm: Gradient L2-Norm threshold.\n        :return:\n        """"""\n        callZooFunc(self.bigdl_type, ""setGradientClippingByL2Norm"", self.value, clip_norm)\n\n    def train(self, train_set, criterion, end_trigger=None, checkpoint_trigger=None,\n              validation_set=None, validation_method=None, batch_size=32):\n        """"""\n        Train model with provided trainSet and criterion.\n        The training will end until the endTrigger is triggered.\n        During the training, if checkPointTrigger is defined and triggered,\n        the model will be saved to modelDir. And if validationSet and validationMethod\n        is defined, the model will be evaluated at the checkpoint.\n        :param train_set: training FeatureSet, a FeatureSet[Sample[T]]\n        :param criterion: Loss function\n        :param end_trigger: When to finish the training\n        :param checkpoint_trigger: When to save a checkpoint and evaluate model.\n        :param validation_set: Validation FeatureSet, a FeatureSet[Sample[T]]\n        :param validation_method: Validation Methods.\n        :param batch_size:\n        :return: Estimator\n        """"""\n        callZooFunc(self.bigdl_type, ""estimatorTrain"", self.value, train_set,\n                    criterion, end_trigger, checkpoint_trigger, validation_set,\n                    validation_method, batch_size)\n        return self\n\n    def train_minibatch(self, train_set, criterion, end_trigger=None, checkpoint_trigger=None,\n                        validation_set=None, validation_method=None):\n\n        """"""\n        Train model with provided trainSet and criterion.\n        The training will end until the endTrigger is triggered.\n        During the training, if checkPointTrigger is defined and triggered,\n        the model will be saved to modelDir. And if validationSet and validationMethod\n        is defined, the model will be evaluated at the checkpoint.\n        :param train_set: training FeatureSet, a FeatureSet[MiniBatch[T]]\n        :param criterion: Loss function\n        :param end_trigger: When to finish the training\n        :param checkpoint_trigger: When to save a checkpoint and evaluate model.\n        :param validation_set: Validation FeatureSet, a FeatureSet[MiniBatch[T]]\n        :param validation_method: Validation Methods.\n        :return: Estimator\n        """"""\n\n        callZooFunc(self.bigdl_type, ""estimatorTrainMiniBatch"", self.value, train_set,\n                    criterion, end_trigger, checkpoint_trigger, validation_set,\n                    validation_method)\n        return self\n\n    def train_imagefeature(self, train_set, criterion, end_trigger=None, checkpoint_trigger=None,\n                           validation_set=None, validation_method=None, batch_size=32):\n        """"""\n        Train model with provided imageFeature trainSet and criterion.\n        The training will end until the endTrigger is triggered.\n        During the training, if checkPointTrigger is defined and triggered,\n        the model will be saved to modelDir. And if validationSet and validationMethod\n        is defined, the model will be evaluated at the checkpoint.\n        :param train_set: training FeatureSet, a FeatureSet[ImageFeature]\n        :param criterion: Loss function\n        :param end_trigger: When to finish the training\n        :param checkpoint_trigger: When to save a checkpoint and evaluate model.\n        :param validation_set: Validation FeatureSet, a FeatureSet[Sample[T]]\n        :param validation_method: Validation Methods.\n        :param batch_size: Batch size\n        :return:\n        """"""\n        callZooFunc(self.bigdl_type, ""estimatorTrainImageFeature"", self.value, train_set,\n                    criterion, end_trigger, checkpoint_trigger, validation_set,\n                    validation_method, batch_size)\n        return self\n\n    def evaluate(self, validation_set, validation_method, batch_size=32):\n        """"""\n        Evaluate the model on the validationSet with the validationMethods.\n        :param validation_set: validation FeatureSet, a FeatureSet[Sample[T]]\n        :param validation_method: validation methods\n        :param batch_size: batch size\n        :return: validation results\n        """"""\n        return callZooFunc(self.bigdl_type, ""estimatorEvaluate"", self.value,\n                           validation_set, validation_method, batch_size)\n\n    def evaluate_imagefeature(self, validation_set, validation_method, batch_size=32):\n        """"""\n        Evaluate the model on the validationSet with the validationMethods.\n        :param validation_set: validation FeatureSet, a FeatureSet[Sample[T]]\n        :param validation_method: validation methods\n        :param batch_size: batch size\n        :return: validation results\n        """"""\n        return callZooFunc(self.bigdl_type, ""estimatorEvaluateImageFeature"", self.value,\n                           validation_set, validation_method, batch_size)\n'"
pyzoo/zoo/pipeline/inference/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .inference_model import *\n'"
pyzoo/zoo/pipeline/inference/inference_model.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.util.common import JavaValue\nfrom zoo.common.utils import callZooFunc\nfrom bigdl.nn.layer import Layer\nfrom zoo.pipeline.api.keras.engine import KerasNet\nimport warnings\n\n\nclass InferenceModel(JavaValue):\n    """"""\n    Model for thread-safe inference.\n    To do inference, you need to first initiate an InferenceModel instance, then call\n    load|load_caffe|load_tf|load_openvino to load a pre-trained model, and finally call predict.\n\n    # Arguments\n    supported_concurrent_num: Int. How many concurrent threads to invoke. Default is 1.\n    """"""\n\n    def __init__(self, supported_concurrent_num=1, bigdl_type=""float""):\n        super(InferenceModel, self).__init__(None, bigdl_type, supported_concurrent_num)\n\n    def load_bigdl(self, model_path, weight_path=None):\n        """"""\n        Load a pre-trained Analytics Zoo or BigDL model.\n\n        :param model_path: String. The file path to the model.\n        :param weight_path: String. The file path to the weights if any. Default is None.\n        """"""\n        callZooFunc(self.bigdl_type, ""inferenceModelLoadBigDL"",\n                    self.value, model_path, weight_path)\n\n    # deprecated in ""0.8.0""\n    def load(self, model_path, weight_path=None):\n        """"""\n        Load a pre-trained Analytics Zoo or BigDL model.\n\n        :param model_path: String. The file path to the model.\n        :param weight_path: String. The file path to the weights if any. Default is None.\n        """"""\n        warnings.warn(""deprecated in 0.8.0"")\n        callZooFunc(self.bigdl_type, ""inferenceModelLoad"",\n                    self.value, model_path, weight_path)\n\n    def load_caffe(self, model_path, weight_path):\n        """"""\n        Load a pre-trained Caffe model.\n\n        :param model_path: String. The file path to the prototxt file.\n        :param weight_path: String. The file path to the Caffe model.\n        """"""\n        callZooFunc(self.bigdl_type, ""inferenceModelLoadCaffe"",\n                    self.value, model_path, weight_path)\n\n    def load_openvino(self, model_path, weight_path, batch_size=0):\n        """"""\n        Load an OpenVINI IR.\n\n        :param model_path: String. The file path to the OpenVINO IR xml file.\n        :param weight_path: String. The file path to the OpenVINO IR bin file.\n        :param batch_size: Int. Set batch Size, default is 0 (use default batch size).\n        """"""\n        callZooFunc(self.bigdl_type, ""inferenceModelLoadOpenVINO"",\n                    self.value, model_path, weight_path, batch_size)\n\n    def load_tensorflow(self, model_path, model_type=""frozenModel"", intra_op_parallelism_threads=1,\n                        inter_op_parallelism_threads=1, use_per_session_threads=True):\n        """"""\n        Load an TensorFlow model using tensorflow.\n\n        :param model_path: String. The file path to the TensorFlow model.\n        :param model_type: String. The type of the tensorflow model file. Default is ""frozenModel""\n        :param intra_op_parallelism_threads: Int. The number of intraOpParallelismThreads.\n                                             Default is 1.\n        :param inter_op_parallelism_threads: Int. The number of interOpParallelismThreads.\n                                             Default is 1.\n        :param use_per_session_threads: Boolean. Whether to use perSessionThreads. Default is True.\n        """"""\n        callZooFunc(self.bigdl_type, ""inferenceModelLoadTensorFlow"",\n                    self.value, model_path, model_type, intra_op_parallelism_threads,\n                    inter_op_parallelism_threads, use_per_session_threads)\n\n    def load_tensorflow(self, model_path, model_type,\n                        inputs, outputs, intra_op_parallelism_threads=1,\n                        inter_op_parallelism_threads=1, use_per_session_threads=True):\n        """"""\n        Load an TensorFlow model using tensorflow.\n\n        :param model_path: String. The file path to the TensorFlow model.\n        :param model_type: String. The type of the tensorflow model file: ""frozenModel"" or\n         ""savedModel"".\n        :param inputs: Array[String]. the inputs of the model.\n        inputs outputs: Array[String]. the outputs of the model.\n        :param intra_op_parallelism_threads: Int. The number of intraOpParallelismThreads.\n                                                Default is 1.\n        :param inter_op_parallelism_threads: Int. The number of interOpParallelismThreads.\n                                                Default is 1.\n        :param use_per_session_threads: Boolean. Whether to use perSessionThreads. Default is True.\n           """"""\n        callZooFunc(self.bigdl_type, ""inferenceModelLoadTensorFlow"",\n                    self.value, model_path, model_type,\n                    inputs, outputs, intra_op_parallelism_threads,\n                    inter_op_parallelism_threads, use_per_session_threads)\n\n    # deprecated in ""0.8.0""\n    def load_tf(self, model_path, backend=""tensorflow"",\n                intra_op_parallelism_threads=1, inter_op_parallelism_threads=1,\n                use_per_session_threads=True, model_type=None,\n                ov_pipeline_config_path=None, ov_extensions_config_path=None):\n        """"""\n        Load an TensorFlow model using tensorflow or openvino backend.\n\n        :param model_path: String. The file path to the TensorFlow model.\n        :param backend: String. The backend to use for inference. Either \'tensorflow\' or \'openvino\'.\n                        For \'tensorflow\' backend, only need to specify arguments\n                        intra_op_parallelism_threads, inter_op_parallelism_threads\n                        and use_per_session_threads.\n                        For \'openvino\' backend, only need to specify either model_type or\n                        pipeline_config_path together with extensions_config_path.\n                        Default is \'tensorflow\'.\n        :param intra_op_parallelism_threads: For \'tensorflow\' backend only. Int.\n                                             The number of intraOpParallelismThreads. Default is 1.\n        :param inter_op_parallelism_threads: For \'tensorflow\' backend only. Int.\n                                             The number of interOpParallelismThreads. Default is 1.\n        :param use_per_session_threads: For \'tensorflow\' backend only. Boolean.\n                                        Whether to use perSessionThreads. Default is True.\n        :param model_type: For \'openvino\' backend only. The type of the TensorFlow model,\n                           e.g. faster_rcnn_resnet101_coco, ssd_inception_v2_coco, etc.\n        :param ov_pipeline_config_path: For \'openvino\' backend only. String.\n                                        The file path to the pipeline configure file.\n        :param ov_extensions_config_path: For \'openvino\' backend only. String.\n                                          The file path to the extensions configure file.\n                                          Need pipeline_config_path and extensions_config_path\n                                          for \'openvino\' backend if model_type is not specified.\n        """"""\n        warnings.warn(""deprecated in 0.8.0"")\n        backend = backend.lower()\n        if backend == ""tensorflow"" or backend == ""tf"":\n            callZooFunc(self.bigdl_type, ""inferenceModelLoadTensorFlow"",\n                        self.value, model_path, ""frozenModel"", intra_op_parallelism_threads,\n                        inter_op_parallelism_threads, use_per_session_threads)\n        elif backend == ""openvino"" or backend == ""ov"":\n            if model_type:\n                if ov_pipeline_config_path:\n                    callZooFunc(self.bigdl_type, ""inferenceModelOpenVINOLoadTF"",\n                                self.value, model_path, model_type, ov_pipeline_config_path, None)\n                else:\n                    callZooFunc(self.bigdl_type, ""inferenceModelOpenVINOLoadTF"",\n                                self.value, model_path, model_type)\n            else:\n                if ov_pipeline_config_path is None and ov_extensions_config_path is None:\n                    raise Exception(""For openvino backend, you must provide either model_type or ""\n                                    ""both pipeline_config_path and extensions_config_path"")\n                callZooFunc(self.bigdl_type, ""inferenceModelOpenVINOLoadTF"",\n                            self.value, model_path, ov_pipeline_config_path,\n                            ov_extensions_config_path)\n        else:\n            raise ValueError(""Currently only tensorflow and openvino are supported as backend"")\n\n    # deprecated in ""0.8.0""\n    def load_tf_object_detection_as_openvino(self,\n                                             model_path,\n                                             object_detection_model_type,\n                                             pipeline_config_path,\n                                             extensions_config_path\n                                             ):\n        """"""\n        load object detection TF model as OpenVINO IR\n        :param model_path: String, the path of the tensorflow model\n        :param object_detection_model_type: String, the type of the tensorflow model\n        :param pipeline_config_path: String, the path of the pipeline configure file\n        :param extensions_config_path: String, the path of the extensions configure file\n        :return:\n        """"""\n        warnings.warn(""deprecated in 0.8.0"")\n        callZooFunc(self.bigdl_type,\n                    ""inferenceModelOpenVINOLoadTF"",\n                    self.value,\n                    model_path,\n                    object_detection_model_type,\n                    pipeline_config_path,\n                    extensions_config_path)\n\n    # deprecated in ""0.8.0""\n    def load_tf_image_classification_as_openvino(self,\n                                                 model_path,\n                                                 image_classification_model_type,\n                                                 checkpoint_path,\n                                                 input_shape,\n                                                 if_reverse_input_channels,\n                                                 mean_values,\n                                                 scale):\n        """"""\n        load image classification TF model as OpenVINO IR\n        :param model_path: String, the path of the tensorflow model\n        :param image_classification_model_type: String, the type of the tensorflow model\n        :param checkpoint_path: String, the path of the tensorflow checkpoint file\n        :param input_shape: List of Int,\n                input shape that should be fed to an input node(s) of the model\n        :param if_reverse_input_channels: Boolean,\n                the boolean value of if need reverse input channels.\n        :param mean_values: List of Float, all input values coming from original network inputs\n                            will be divided by this value.\n        :param scale: Float, the scale value, to be used for the input image per channel.\n        :return:\n        """"""\n        warnings.warn(""deprecated in 0.8.0"")\n        callZooFunc(self.bigdl_type,\n                    ""inferenceModelOpenVINOLoadTF"",\n                    self.value,\n                    model_path,\n                    image_classification_model_type,\n                    checkpoint_path,\n                    input_shape,\n                    if_reverse_input_channels,\n                    [float(value) for value in mean_values],\n                    float(scale))\n\n    def predict(self, inputs):\n        """"""\n        Do prediction on inputs.\n\n        :param inputs: A numpy array or a list of numpy arrays or JTensor or a list of JTensors.\n        """"""\n        jinputs, input_is_table = Layer.check_input(inputs)\n        output = callZooFunc(self.bigdl_type,\n                             ""inferenceModelPredict"",\n                             self.value,\n                             jinputs,\n                             input_is_table)\n        return KerasNet.convert_output(output)\n'"
pyzoo/zoo/pipeline/nnframes/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .nn_classifier import *\nfrom .nn_image_reader import *\nfrom .nn_image_schema import *\n'"
pyzoo/zoo/pipeline/nnframes/nn_classifier.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom pyspark.ml.param.shared import *\nfrom pyspark.ml.wrapper import JavaModel, JavaEstimator, JavaTransformer\nfrom bigdl.optim.optimizer import SGD\nfrom zoo.common.utils import callZooFunc\nfrom bigdl.util.common import *\nfrom zoo.feature.common import *\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass HasBatchSize(Params):\n    """"""\n    Mixin for param batchSize: batch size.\n    """"""\n\n    # a placeholder to make it appear in the generated doc\n    batchSize = Param(Params._dummy(), ""batchSize"", ""batchSize (>= 0)."")\n\n    def __init__(self):\n        super(HasBatchSize, self).__init__()\n        #: param for batch size.\n        self.batchSize = Param(self, ""batchSize"", ""batchSize"")\n        self._setDefault(batchSize=1)\n\n    def setBatchSize(self, val):\n        """"""\n        Sets the value of :py:attr:`batchSize`.\n        """"""\n        self._paramMap[self.batchSize] = val\n        return self\n\n    def getBatchSize(self):\n        """"""\n        Gets the value of batchSize or its default value.\n        """"""\n        return self.getOrDefault(self.batchSize)\n\n\nclass HasSamplePreprocessing:\n    """"""\n    Mixin for param samplePreprocessing\n    """"""\n    samplePreprocessing = None\n\n    def __init__(self):\n        super(HasSamplePreprocessing, self).__init__()\n\n    def setSamplePreprocessing(self, val):\n        """"""\n        Sets samplePreprocessing\n        """"""\n        pythonBigDL_method_name = ""setSamplePreprocessing""\n        callZooFunc(self.bigdl_type, pythonBigDL_method_name, self.value, val)\n        self.samplePreprocessing = val\n        return self\n\n    def getSamplePreprocessing(self):\n        return self.samplePreprocessing\n\n\nclass HasOptimMethod:\n\n    def __init__(self):\n        super(HasOptimMethod, self).__init__()\n        self.optimMethod = SGD()\n\n    def setOptimMethod(self, val):\n        """"""\n        Sets optimization method. E.g. SGD, Adam, LBFGS etc. from bigdl.optim.optimizer.\n        default: SGD()\n        """"""\n        pythonBigDL_method_name = ""setOptimMethod""\n        callZooFunc(self.bigdl_type, pythonBigDL_method_name, self.value, val)\n        self.optimMethod = val\n        return self\n\n    def getOptimMethod(self):\n        """"""\n        Gets the optimization method\n        """"""\n        return self.optimMethod\n\n\nclass HasThreshold(Params):\n    """"""\n    Mixin for param Threshold in binary classification.\n\n    The threshold applies to the raw output of the model. If the output is greater than\n    threshold, then predict 1, else 0. A high threshold encourages the model to predict 0\n    more often; a low threshold encourages the model to predict 1 more often.\n\n    Note: the param is different from the one in Spark ProbabilisticClassifier which is compared\n    against estimated probability.\n\n    Default is 0.5.\n    """"""\n\n    def __init__(self):\n        super(HasThreshold, self).__init__()\n        self.threshold = Param(self, ""threshold"", ""threshold"")\n        self._setDefault(threshold=0.5)\n\n    def setThreshold(self, val):\n        """"""\n        Sets the value of :py:attr:`threshold`.\n        """"""\n        self._paramMap[self.threshold] = val\n        return self\n\n    def getThreshold(self):\n        """"""\n        Gets the value of threshold or its default value.\n        """"""\n        return self.getOrDefault(self.threshold)\n\n\nclass NNEstimator(JavaEstimator, HasFeaturesCol, HasLabelCol, HasPredictionCol, HasBatchSize,\n                  HasOptimMethod, HasSamplePreprocessing, JavaValue):\n    """"""\n    NNEstimator extends org.apache.spark.ml.Estimator and supports training a BigDL model with\n    Spark DataFrame data. It can be integrated into a standard Spark ML Pipeline to enable\n    users for combined usage with Spark MLlib.\n\n    NNEstimator supports different feature and label data type through operation defined in\n    Preprocessing. We provide pre-defined Preprocessing for popular data types like Array\n    or Vector in package zoo.feature, while user can also develop customized Preprocess\n    which extends from feature.common.Preprocessing. During fit, NNEstimator\n    will extract feature and label data from input DataFrame and use the Preprocessing to prepare\n    data for the model.\n    Using the Preprocessing allows NNEstimator to cache only the raw data and decrease the\n    memory consumption during feature conversion and training.\n\n    More concrete examples are available in package com.intel.analytics.zoo.examples.nnframes\n    """"""\n\n    def __init__(self, model, criterion,\n                 feature_preprocessing=None,\n                 label_preprocessing=None,\n                 jvalue=None, bigdl_type=""float""):\n        """"""\n        Construct a NNEstimator with BigDL model, criterion and Preprocessing for feature and label\n        data.\n        :param model: BigDL Model to be trained.\n        :param criterion: BigDL criterion.\n        :param feature_preprocessing: The param converts the data in feature column to a\n               Tensor or to a Sample directly. It expects a List of Int as the size of the\n               converted Tensor, or a Preprocessing[F, Tensor[T]]\n\n               If a List of Int is set as feature_preprocessing, it can only handle the case that\n               feature column contains the following data types:\n               Float, Double, Int, Array[Float], Array[Double], Array[Int] and MLlib Vector. The\n               feature data are converted to Tensors with the specified sizes before\n               sending to the model. Internally, a SeqToTensor is generated according to the\n               size, and used as the feature_preprocessing.\n\n               Alternatively, user can set feature_preprocessing as Preprocessing[F, Tensor[T]]\n               that transforms the feature data to a Tensor[T]. Some pre-defined Preprocessing are\n               provided in package zoo.feature. Multiple Preprocessing can be combined as a\n               ChainedPreprocessing.\n\n               The feature_preprocessing will also be copied to the generated NNModel and applied\n               to feature column during transform.\n        :param label_preprocessing: similar to feature_preprocessing, but applies to Label data.\n        :param jvalue: Java object create by Py4j\n        :param bigdl_type: optional parameter. data type of model, ""float""(default) or ""double"".\n        """"""\n        super(NNEstimator, self).__init__()\n\n        # avoid initialization during import.\n        if not feature_preprocessing:\n            feature_preprocessing = SeqToTensor()\n        if not label_preprocessing:\n            label_preprocessing = SeqToTensor()\n\n        if type(feature_preprocessing) is list:\n            if type(feature_preprocessing[0]) is list:\n                feature_preprocessing = SeqToMultipleTensors(feature_preprocessing)\n            elif isinstance(feature_preprocessing[0], int):\n                feature_preprocessing = SeqToTensor(feature_preprocessing)\n\n        if type(label_preprocessing) is list:\n            assert (all(isinstance(x, int) for x in label_preprocessing))\n            label_preprocessing = SeqToTensor(label_preprocessing)\n\n        sample_preprocessing = FeatureLabelPreprocessing(feature_preprocessing, label_preprocessing)\n\n        self.value = jvalue if jvalue else callZooFunc(\n            bigdl_type, self.jvm_class_constructor(), model, criterion, sample_preprocessing)\n        self.model = model\n        self.samplePreprocessing = sample_preprocessing\n        self.bigdl_type = bigdl_type\n        self._java_obj = self.value\n\n        self.maxEpoch = Param(self, ""maxEpoch"", ""number of max Epoch"")\n        self.learningRate = Param(self, ""learningRate"", ""learning rate"")\n        self.learningRateDecay = Param(self, ""learningRateDecay"", ""learning rate decay"")\n        self.cachingSample = Param(self, ""cachingSample"", ""cachingSample"")\n\n        self.train_summary = None\n        self.validation_config = None\n        self.checkpoint_config = None\n        self.validation_summary = None\n        self.endWhen = None\n        self.dataCacheLevel = ""DRAM""\n\n    def setSamplePreprocessing(self, val):\n        """"""\n        Sets the value of sample_preprocessing\n        :param val: a Preprocesing[(Feature, Option(Label), Sample]\n        """"""\n        super(NNEstimator, self).setSamplePreprocessing(val)\n        return self\n\n    def setMaxEpoch(self, val):\n        """"""\n        Sets the value of :py:attr:`maxEpoch`.\n        """"""\n        self._paramMap[self.maxEpoch] = val\n        return self\n\n    def getMaxEpoch(self):\n        """"""\n        Gets the value of maxEpoch or its default value.\n        """"""\n        return self.getOrDefault(self.maxEpoch)\n\n    def setEndWhen(self, trigger):\n        """"""\n        When to stop the training, passed in a Trigger. E.g. maxIterations(100)\n        """"""\n        pythonBigDL_method_name = ""setEndWhen""\n        callZooFunc(self.bigdl_type, pythonBigDL_method_name, self.value, trigger)\n        self.endWhen = trigger\n        return self\n\n    def getEndWhen(self):\n        """"""\n        Gets the value of endWhen or its default value.\n        """"""\n        return self.endWhen\n\n    def setDataCacheLevel(self, level, numSlice=None):\n        """"""\n        :param level: string, ""DRAM"", ""PMEM"" or ""DISK_AND_DRAM"".\n                If it\'s DRAM, will cache dataset into dynamic random-access memory\n                If it\'s PMEM, will cache dataset into Intel Optane DC Persistent Memory\n                If it\'s DISK_AND_DRAM, will cache dataset into disk, and only hold 1/numSlice\n                  of the data into memory during the training. After going through the\n                  1/numSlice, we will release the current cache, and load another slice into\n                  memory.\n        """"""\n        pythonBigDL_method_name = ""setDataCacheLevel""\n        callZooFunc(self.bigdl_type, pythonBigDL_method_name, self.value, level, numSlice)\n        self.dataCacheLevel = level if numSlice is None else (level, numSlice)\n        return self\n\n    def getDataCacheLevel(self):\n        return self.dataCacheLevel\n\n    def setLearningRate(self, val):\n        """"""\n        Sets the value of :py:attr:`learningRate`.\n        .. note:: Deprecated in 0.4.0. Please set learning rate with optimMethod directly.\n        """"""\n        self._paramMap[self.learningRate] = val\n        return self\n\n    def getLearningRate(self):\n        """"""\n        Gets the value of learningRate or its default value.\n        """"""\n        return self.getOrDefault(self.learningRate)\n\n    def setLearningRateDecay(self, val):\n        """"""\n        Sets the value of :py:attr:`learningRateDecay`.\n        .. note:: Deprecated in 0.4.0. Please set learning rate decay with optimMethod directly.\n        """"""\n        self._paramMap[self.learningRateDecay] = val\n        return self\n\n    def getLearningRateDecay(self):\n        """"""\n        Gets the value of learningRateDecay or its default value.\n        """"""\n        return self.getOrDefault(self.learningRateDecay)\n\n    def setCachingSample(self, val):\n        """"""\n        whether to cache the Samples after preprocessing. Default: True\n        """"""\n        self._paramMap[self.cachingSample] = val\n        return self\n\n    def isCachingSample(self):\n        """"""\n        Gets the value of cachingSample or its default value.\n        """"""\n        return self.getOrDefault(self.cachingSample)\n\n    def setTrainSummary(self, val):\n        """"""\n        Statistics (LearningRate, Loss, Throughput, Parameters) collected during training for the\n        training data, which can be used for visualization via Tensorboard.\n        Use setTrainSummary to enable train logger. Then the log will be saved to\n        logDir/appName/train as specified by the parameters of TrainSummary.\n        Default: Not enabled\n\n        :param summary: a TrainSummary object\n        """"""\n        pythonBigDL_method_name = ""setTrainSummary""\n        callZooFunc(self.bigdl_type, pythonBigDL_method_name, self.value, val)\n        self.train_summary = val\n        return self\n\n    def getTrainSummary(self):\n        """"""\n        Gets the train summary\n        """"""\n        return self.train_summary\n\n    def setValidationSummary(self, val):\n        """"""\n        Statistics (LearningRate, Loss, Throughput, Parameters) collected during training for the\n        validation data if validation data is set, which can be used for visualization via\n        Tensorboard. Use setValidationSummary to enable validation logger. Then the log will be\n        saved to logDir/appName/ as specified by the parameters of validationSummary.\n        Default: None\n        """"""\n        pythonBigDL_method_name = ""setValidationSummary""\n        callZooFunc(self.bigdl_type, pythonBigDL_method_name, self.value, val)\n        self.validation_summary = val\n        return self\n\n    def getValidationSummary(self):\n        """"""\n        Gets the Validation summary\n        """"""\n        return self.validation_summary\n\n    def setValidation(self, trigger, val_df, val_method, batch_size):\n        """"""\n        Set a validate evaluation during training\n\n        :param trigger: validation interval\n        :param val_df: validation dataset\n        :param val_method: the ValidationMethod to use,e.g. ""Top1Accuracy"", ""Top5Accuracy"", ""Loss""\n        :param batch_size: validation batch size\n        """"""\n        pythonBigDL_method_name = ""setValidation""\n        callZooFunc(self.bigdl_type, pythonBigDL_method_name, self.value,\n                    trigger, val_df, val_method, batch_size)\n        self.validation_config = [trigger, val_df, val_method, batch_size]\n        return self\n\n    def getValidation(self):\n        """"""\n        Gets the validate configuration. If validation config has been set, getValidation will\n        return a List of [ValidationTrigger, Validation data, Array[ValidationMethod[T]],\n        batchsize]\n        """"""\n        return self.validation_config\n\n    def clearGradientClipping(self):\n        """"""\n        Clear clipping params, in this case, clipping will not be applied.\n        In order to take effect, it needs to be called before fit.\n        """"""\n        callZooFunc(self.bigdl_type, ""nnEstimatorClearGradientClipping"",\n                    self.value)\n        return self\n\n    def setConstantGradientClipping(self, min, max):\n        """"""\n        Set constant gradient clipping during the training process.\n        In order to take effect, it needs to be called before fit.\n\n        # Arguments\n        min: The minimum value to clip by. Float.\n        max: The maximum value to clip by. Float.\n        """"""\n        callZooFunc(self.bigdl_type, ""nnEstimatorSetConstantGradientClipping"",\n                    self.value,\n                    float(min),\n                    float(max))\n        return self\n\n    def setGradientClippingByL2Norm(self, clip_norm):\n        """"""\n        Clip gradient to a maximum L2-Norm during the training process.\n        In order to take effect, it needs to be called before fit.\n\n        # Arguments\n        clip_norm: Gradient L2-Norm threshold. Float.\n        """"""\n        callZooFunc(self.bigdl_type, ""nnEstimatorSetGradientClippingByL2Norm"",\n                    self.value,\n                    float(clip_norm))\n        return self\n\n    def setCheckpoint(self, path, trigger, isOverWrite=True):\n        """"""\n        Set check points during training. Not enabled by default\n        :param path: the directory to save the model\n        :param trigger: how often to save the check point\n        :param isOverWrite: whether to overwrite existing snapshots in path. Default is True\n        :return: self\n        """"""\n        pythonBigDL_method_name = ""setCheckpoint""\n        callZooFunc(self.bigdl_type, pythonBigDL_method_name, self.value,\n                    path, trigger, isOverWrite)\n        self.checkpoint_config = [path, trigger, isOverWrite]\n        return self\n\n    def getCheckpoint(self):\n        """"""\n        :return: a tuple containing (checkpointPath, checkpointTrigger, checkpointOverwrite)\n        """"""\n        return self.checkpoint_config\n\n    def _create_model(self, java_model):\n        # explicity reset SamplePreprocessing even though java_model already has the preprocessing,\n        # so that python NNModel also has sample_preprocessing\n        estPreprocessing = self.getSamplePreprocessing()\n        nnModel = NNModel(model=self.model, feature_preprocessing=None, jvalue=java_model,\n                          bigdl_type=self.bigdl_type) \\\n            .setSamplePreprocessing(ChainedPreprocessing([ToTuple(), estPreprocessing]))\n\n        nnModel.setFeaturesCol(self.getFeaturesCol()) \\\n            .setPredictionCol(self.getPredictionCol()) \\\n            .setBatchSize(java_model.getBatchSize())\n        return nnModel\n\n\nclass NNModel(JavaTransformer, HasFeaturesCol, HasPredictionCol, HasBatchSize,\n              HasSamplePreprocessing, JavaValue):\n    """"""\n    NNModel extends Spark ML Transformer and supports BigDL model with Spark DataFrame.\n\n    NNModel supports different feature data type through Preprocessing. Some common\n    Preprocessing have been defined in com.intel.analytics.zoo.feature.\n\n    After transform, the prediction column contains the output of the model as Array[T], where\n    T (Double or Float) is decided by the model type.\n    """"""\n\n    def __init__(self, model, feature_preprocessing=None, jvalue=None, bigdl_type=""float""):\n        """"""\n        create a NNModel with a BigDL model\n        :param model: trained BigDL model to use in prediction.\n        :param feature_preprocessing: The param converts the data in feature column to a\n                                      Tensor. It expects a List of Int as\n                                      the size of the converted Tensor, or a\n                                      Preprocessing[F, Tensor[T]]\n        :param jvalue: Java object create by Py4j\n        :param bigdl_type: optional parameter. data type of model, ""float""(default) or ""double"".\n        """"""\n        super(NNModel, self).__init__()\n        # initialize with Java NNModel\n        if jvalue:\n            assert feature_preprocessing is None\n            self.value = jvalue\n        # initialize with Python Model and preprocessing\n        else:\n            if not feature_preprocessing:\n                feature_preprocessing = SeqToTensor()\n\n            if type(feature_preprocessing) is list:\n                if type(feature_preprocessing[0]) is list:\n                    feature_preprocessing = SeqToMultipleTensors(feature_preprocessing)\n                elif isinstance(feature_preprocessing[0], int):\n                    feature_preprocessing = SeqToTensor(feature_preprocessing)\n\n            sample_preprocessing = ChainedPreprocessing([feature_preprocessing, TensorToSample()])\n            self.value = callZooFunc(\n                bigdl_type, self.jvm_class_constructor(), model, sample_preprocessing)\n            self.samplePreprocessing = sample_preprocessing\n\n        self.model = model\n        self._java_obj = self.value\n        self.bigdl_type = bigdl_type\n        self.setBatchSize(self.value.getBatchSize())\n\n    def save(self, path):\n        self._transfer_params_to_java()\n        callZooFunc(self.bigdl_type, ""saveNNModel"", self.value, path)\n        return self\n\n    @staticmethod\n    def load(path):\n        jvalue = callZooFunc(""float"", ""loadNNModel"", path)\n        return NNModel(model=None, feature_preprocessing=None, jvalue=jvalue)\n\n\nclass NNClassifier(NNEstimator):\n    """"""\n    NNClassifier is a specialized NNEstimator that simplifies the data format for\n    classification tasks. It only supports label column of DoubleType, and the fitted\n    NNClassifierModel will have the prediction column of DoubleType.\n    """"""\n\n    def __init__(self, model, criterion, feature_preprocessing=None,\n                 jvalue=None, bigdl_type=""float""):\n        """"""\n        :param model: BigDL module to be optimized\n        :param criterion: BigDL criterion method\n        :param feature_preprocessing: The param converts the data in feature column to a\n                                      Tensor. It expects a List of Int as\n                                      the size of the converted Tensor, or a\n                                      Preprocessing[F, Tensor[T]]\n        :param bigdl_type(optional): Data type of BigDL model, ""float""(default) or ""double"".\n        """"""\n        if not feature_preprocessing:\n            feature_preprocessing = SeqToTensor()\n\n        super(NNClassifier, self).__init__(\n            model, criterion, feature_preprocessing, ScalarToTensor(), jvalue, bigdl_type)\n\n    def setSamplePreprocessing(self, val):\n        """"""\n        Sets the value of sample_preprocessing\n        :param val: a Preprocesing[(Feature, Option(Label), Sample]\n        """"""\n        super(NNClassifier, self).setSamplePreprocessing(val)\n        return self\n\n    def _create_model(self, java_model):\n        # explicity reset SamplePreprocessing even though java_model already has the preprocessing,\n        # so that python NNClassifierModel also has sample_preprocessing\n        estPreprocessing = self.getSamplePreprocessing()\n        classifierModel = NNClassifierModel(model=self.model, feature_preprocessing=None,\n                                            jvalue=java_model, bigdl_type=self.bigdl_type) \\\n            .setSamplePreprocessing(ChainedPreprocessing([ToTuple(), estPreprocessing]))\n\n        classifierModel.setFeaturesCol(self.getFeaturesCol()) \\\n            .setPredictionCol(self.getPredictionCol()) \\\n            .setBatchSize(java_model.getBatchSize())\n        return classifierModel\n\n\nclass NNClassifierModel(NNModel, HasThreshold):\n    """"""\n    NNClassifierModel is a specialized [[NNModel]] for classification tasks. The prediction\n    column will have the datatype of Double.\n    """"""\n\n    def __init__(self, model, feature_preprocessing=None, jvalue=None,\n                 bigdl_type=""float""):\n        """"""\n        :param model: trained BigDL model to use in prediction.\n        :param feature_preprocessing: The param converts the data in feature column to a\n                                      Tensor. It expects a List of Int as\n                                      the size of the converted Tensor, or a\n                                      Preprocessing[F, Tensor[T]]\n        :param jvalue: Java object create by Py4j\n        :param bigdl_type(optional): Data type of BigDL model, ""float""(default) or ""double"".\n        """"""\n        super(NNClassifierModel, self).__init__(model, feature_preprocessing, jvalue, bigdl_type)\n\n    @staticmethod\n    def load(path):\n        jvalue = callZooFunc(""float"", ""loadNNClassifierModel"", path)\n        return NNClassifierModel(model=None, feature_preprocessing=None, jvalue=jvalue)\n\n\nclass XGBClassifierModel:\n    \'\'\'\n    XGBClassifierModel is a trained XGBoost classification model. The prediction column\n    will have the prediction results.\n    \'\'\'\n\n    def __init__(self, jvalue):\n        super(XGBClassifierModel, self).__init__()\n        assert jvalue is not None\n        self.value = jvalue\n\n    def setFeaturesCol(self, features):\n        callZooFunc(""float"", ""setFeaturesXGBClassifierModel"", self.value, features)\n\n    def setPredictionCol(self, prediction):\n        callZooFunc(""float"", ""setPredictionXGBClassifierModel"", self.value, prediction)\n\n    def transform(self, dataset):\n        df = callZooFunc(""float"", ""transformXGBClassifierModel"", self.value, dataset)\n        return df\n\n    @staticmethod\n    def loadModel(path, numClasses):\n        """"""\n        load a pretrained XGBoostClassificationModel\n        :param path: pretrained model path\n        :param numClasses: number of classes for classification\n        """"""\n        jvalue = callZooFunc(""float"", ""loadXGBClassifierModel"", path, numClasses)\n        return XGBClassifierModel(jvalue=jvalue)\n'"
pyzoo/zoo/pipeline/nnframes/nn_image_reader.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass NNImageReader:\n    """"""\n    Primary DataFrame-based image loading interface, defining API to read images from files\n    to DataFrame.\n    """"""\n\n    @staticmethod\n    def readImages(path, sc=None, minPartitions=1, resizeH=-1, resizeW=-1,\n                   image_codec=-1, bigdl_type=""float""):\n        """"""\n        Read the directory of images into DataFrame from the local or remote source.\n        :param path Directory to the input data files, the path can be comma separated paths as the\n                list of inputs. Wildcards path are supported similarly to sc.binaryFiles(path).\n        :param min_partitions A suggestion value of the minimal splitting number for input data.\n        :param resizeH height after resize, by default is -1 which will not resize the image\n        :param resizeW width after resize, by default is -1 which will not resize the image\n        :param image_codec specifying the color type of a loaded image, same as in OpenCV.imread.\n               By default is Imgcodecs.CV_LOAD_IMAGE_UNCHANGED(-1).\n               >0 Return a 3-channel color image. Note In the current implementation the\n                  alpha channel, if any, is stripped from the output image. Use negative value\n                  if you need the alpha channel.\n               =0 Return a grayscale image.\n               <0 Return the loaded image as is (with alpha channel if any).\n        :return DataFrame with a single column ""image""; Each record in the column represents\n                one image record: Row (uri, height, width, channels, CvType, bytes).\n        """"""\n        df = callZooFunc(bigdl_type, ""nnReadImage"", path, sc, minPartitions, resizeH,\n                         resizeW, image_codec)\n        df._sc._jsc = sc._jsc\n        return df\n'"
pyzoo/zoo/pipeline/nnframes/nn_image_schema.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\ndef with_origin_column(dataset, imageColumn=""image"", originColumn=""origin"", bigdl_type=""float""):\n    return callZooFunc(bigdl_type, ""withOriginColumn"", dataset, imageColumn, originColumn)\n'"
pyzoo/zoo/tfpark/gan/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/tfpark/gan/common.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom bigdl.optim.optimizer import OptimMethod\n\n\nclass GanOptimMethod(OptimMethod):\n    def __init__(self, d_optim, g_optim, g_param_size, d_steps=1, g_steps=1):\n        super(GanOptimMethod, self).__init__(None, ""float"",\n                                             d_optim, g_optim, d_steps, g_steps, g_param_size)\n'"
pyzoo/zoo/tfpark/gan/gan_estimator.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport inspect\nimport os\nimport tempfile\n\nimport tensorflow as tf\n\nfrom zoo.tfpark import TFOptimizer, ZooOptimizer\n# todo make it inherit Estimator\nfrom zoo.tfpark.zoo_optimizer import FakeOptimMethod\nfrom zoo.util import nest\n\n\nclass GANEstimator(object):\n\n    def __init__(self,\n                 generator_fn,\n                 discriminator_fn,\n                 generator_loss_fn,\n                 discriminator_loss_fn,\n                 generator_optimizer,\n                 discriminator_optimizer,\n                 generator_steps=1,\n                 discriminator_steps=1,\n                 model_dir=None,\n                 session_config=None,\n                 ):\n        assert isinstance(generator_optimizer, ZooOptimizer),\\\n            ""generator_optimizer should be a ZooOptimizer""\n        assert isinstance(discriminator_optimizer, ZooOptimizer),\\\n            ""discriminator_optimizer should be a ZooOptimizer""\n        self._generator_fn = generator_fn\n        self._discriminator_fn = discriminator_fn\n        self._generator_loss_fn = generator_loss_fn\n        self._discriminator_loss_fn = discriminator_loss_fn\n        self._generator_steps = generator_steps\n        self._discriminator_steps = discriminator_steps\n        self._gen_opt = generator_optimizer\n        self._dis_opt = discriminator_optimizer\n        self._session_config = session_config\n\n        if model_dir is None:\n            folder = tempfile.mkdtemp()\n            self.checkpoint_path = os.path.join(folder, ""model"")\n            self.model_dir = folder\n        else:\n            self.checkpoint_path = os.path.join(model_dir, ""model"")\n            self.model_dir = model_dir\n\n    @staticmethod\n    def _call_fn_maybe_with_counter(fn, counter, *args):\n        fn_args = inspect.getargspec(fn).args\n        if ""counter"" in fn_args:\n            return fn(*args, counter=counter)\n        else:\n            return fn(*args)\n\n    def train(self, input_fn, end_trigger):\n\n        with tf.Graph().as_default() as g:\n\n            dataset = input_fn()\n\n            generator_inputs = dataset.tensors[0]\n            real_data = dataset.tensors[1]\n\n            counter = tf.train.get_or_create_global_step()\n\n            period = self._discriminator_steps + self._generator_steps\n\n            is_discriminator_phase = tf.less(tf.mod(counter, period), self._discriminator_steps)\n\n            with tf.variable_scope(""Generator""):\n                gen_data = self._call_fn_maybe_with_counter(self._generator_fn, counter,\n                                                            generator_inputs)\n\n            with tf.variable_scope(""Discriminator""):\n                fake_d_outputs = self._call_fn_maybe_with_counter(self._discriminator_fn,\n                                                                  counter,\n                                                                  gen_data, generator_inputs)\n\n            with tf.variable_scope(""Discriminator"", reuse=True):\n                real_d_outputs = self._call_fn_maybe_with_counter(self._discriminator_fn,\n                                                                  counter,\n                                                                  real_data, generator_inputs)\n\n            with tf.name_scope(""Generator_loss""):\n                generator_loss = self._call_fn_maybe_with_counter(self._generator_loss_fn,\n                                                                  counter,\n                                                                  fake_d_outputs)\n                gen_reg_loss = tf.losses.get_regularization_loss(""Generator"")\n\n                generator_loss = generator_loss + gen_reg_loss\n\n            with tf.name_scope(""Discriminator_loss""):\n                discriminator_loss = self._call_fn_maybe_with_counter(self._discriminator_loss_fn,\n                                                                      counter,\n                                                                      real_d_outputs,\n                                                                      fake_d_outputs)\n                dis_reg_loss = tf.losses.get_regularization_loss(""Discriminator"")\n                discriminator_loss = discriminator_loss + dis_reg_loss\n\n            generator_variables = tf.trainable_variables(""Generator"")\n            discriminator_variables = tf.trainable_variables(""Discriminator"")\n\n            def run_gen_compute():\n                gen_grads_vars = self._gen_opt.compute_gradients(generator_loss,\n                                                                 var_list=generator_variables)\n                gen_grads = [grad for grad, var in gen_grads_vars]\n                dis_grads = [tf.zeros_like(var) for var in discriminator_variables]\n\n                return gen_grads + dis_grads\n\n            def run_dis_compute():\n                dis_grads_vars = self._gen_opt.compute_gradients(discriminator_loss,\n                                                                 var_list=discriminator_variables)\n                dis_grads = [grad for grad, var in dis_grads_vars]\n                gen_gards = [tf.zeros_like(var) for var in generator_variables]\n                return gen_gards + dis_grads\n\n            grads = tf.cond(is_discriminator_phase, run_dis_compute, run_gen_compute)\n\n            grads_vars = list(zip(grads, generator_variables + discriminator_variables))\n\n            gen_grads_vars = grads_vars[:len(generator_variables)]\n            dis_grads_vars = grads_vars[len(generator_variables):]\n\n            grads = [grad for grad, var in grads_vars]\n\n            _train_op = tf.cond(is_discriminator_phase,\n                                lambda: self._dis_opt.apply_gradients(dis_grads_vars),\n                                lambda: self._gen_opt.apply_gradients(gen_grads_vars))\n\n            variables = generator_variables + discriminator_variables\n\n            loss = tf.cond(is_discriminator_phase,\n                           lambda: discriminator_loss,\n                           lambda: generator_loss)\n\n            with tf.control_dependencies([_train_op]):\n                increase_counter = tf.assign_add(counter, 1)\n\n            with tf.control_dependencies([increase_counter]):\n                train_op = tf.no_op()\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                saver = tf.train.Saver()\n                kpt = tf.train.latest_checkpoint(self.model_dir)\n                if kpt is not None:\n                    saver.restore(sess, kpt)\n                opt = TFOptimizer._from_grads(loss, sess,\n                                              inputs=nest.flatten(dataset._original_tensors),\n                                              labels=[],\n                                              grads=grads, variables=variables, dataset=dataset,\n                                              optim_method=FakeOptimMethod(),\n                                              session_config=self._session_config,\n                                              model_dir=os.path.join(self.model_dir, ""tmp""),\n                                              train_op=train_op)\n                opt.optimize(end_trigger)\n                saver = tf.train.Saver()\n                saver.save(sess, self.checkpoint_path, global_step=counter)\n'"
pyzoo/zoo/tfpark/text/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/zouwu/autots/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/zouwu/autots/forecast.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.automl.regression.time_sequence_predictor import TimeSequencePredictor\nfrom zoo.automl.config.recipe import *\nfrom zoo.automl.pipeline.time_sequence import load_ts_pipeline\n\n\nclass AutoTSTrainer:\n    """"""\n    The Automated Time Series Forecast Trainer\n    """"""\n\n    def __init__(self,\n                 horizon=1,\n                 dt_col=""datetime"",\n                 target_col=""value"",\n                 extra_features_col=None\n                 ):\n        """"""\n        Initialize the AutoTS Trainer.\n\n        :param horizon: steps to look forward\n        :param dt_col: the datetime column\n        :param target_col: the target column to forecast\n        :param extra_features_col: extra feature columns\n        """"""\n        self.internal = TimeSequencePredictor(\n            dt_col=dt_col,\n            target_col=target_col,\n            future_seq_len=horizon,\n            extra_features_col=extra_features_col,\n        )\n\n    def fit(self,\n            train_df,\n            validation_df=None,\n            metric=""mse"",\n            recipe: Recipe = SmokeRecipe(),\n            uncertainty: bool = False,\n            distributed: bool = False,\n            hdfs_url=None\n            ):\n        """"""\n        Fit a time series forecasting pipeline w/ automl\n        :param train_df: the input dataframe (as pandas.dataframe)\n        :param validation_df: the validation dataframe (as pandas.dataframe)\n        :param recipe: the configuration of searching\n        :param metric: the evaluation metric to optimize\n        :param uncertainty: whether to enable uncertainty calculation\n                            (will output an uncertainty sigma)\n        :param hdfs_url: the hdfs_url to use for storing trail and intermediate results\n        :param distributed: whether to enable distributed training\n        :return a TSPipeline\n        """"""\n        zoo_pipeline = self.internal.fit(train_df,\n                                         validation_df,\n                                         metric,\n                                         recipe,\n                                         mc=uncertainty,\n                                         distributed=distributed,\n                                         hdfs_url=hdfs_url)\n        ppl = TSPipeline()\n        ppl.internal = zoo_pipeline\n        return ppl\n\n\nclass TSPipeline:\n    """"""\n    A pipeline for time series forecasting.\n    """"""\n\n    def __init__(self):\n        """"""\n        Initialize an emtpy TSPipeline.\n        Usually it is not called by user directly.\n        An TSPipeline either obtained from AutoTrainer.fit or load from file\n        """"""\n        self.internal = None\n        self.uncertainty = False\n\n    def save(self, pipeline_file):\n        """"""\n        save the pipeline to a file\n        :param pipeline_file: the file path\n        :return:\n        """"""\n        return self.internal.save(pipeline_file)\n\n    @staticmethod\n    def load(pipeline_file):\n        """"""\n        load pipeline from a file\n        :param pipeline_file: the pipeline file\n        :return: a TSPipeline object\n        """"""\n        tsppl = TSPipeline()\n        tsppl.internal = load_ts_pipeline(pipeline_file)\n        return tsppl\n\n    def fit(self,\n            input_df,\n            validation_df=None,\n            uncertainty: bool = False,\n            epochs=1,\n            **user_config):\n        """"""\n        Incremental Fitting\n\n        :param input_df: the input dataframe\n        :param validation_df: the validation dataframe\n        :param uncertainty: whether to calculate uncertainty\n        :param epochs: number of epochs to train\n        :param user_config: user configurations\n        :return:\n        """"""\n        # TODO refactor automl.Pipeline fit methods to merge the two\n        # maybe use another method to apply configs.\n        # distinguish between incremental and fit from scratch\n        self.uncertainty = uncertainty\n        if user_config:\n            self.internal.fit_with_fixed_configs(input_df=input_df,\n                                                 validation_df=validation_df,\n                                                 mc=uncertainty,\n                                                 epoch_num=epochs,\n                                                 **user_config)\n        else:\n            self.internal.fit(input_df=input_df,\n                              validation_df=validation_df,\n                              mc=uncertainty,\n                              epoch_num=epochs)\n\n    def predict(self, input_df):\n        """"""\n        predict the result\n        :param input_df: the input dataframe\n        :return: the forecast results\n        """"""\n        if self.uncertainty is True:\n            return self.internal.predict_with_uncertainty(input_df)\n        else:\n            return self.internal.predict(input_df)\n\n    def evaluate(self,\n                 input_df,\n                 metrics=[""mse""],\n                 multioutput=\'raw_values\'):\n        """"""\n        evaluate the results\n        :param input_df: the input dataframe\n        :param metrics: the evaluation metrics\n        :param multioutput: output mode of multiple output, whether to aggregate\n        :return: the evaluation results\n        """"""\n        return self.internal.evaluate(input_df, metrics, multioutput)\n'"
pyzoo/zoo/zouwu/model/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/zouwu/model/anomaly.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport math\nimport numpy as np\nimport pandas as pd\n\nfrom abc import ABCMeta, abstractmethod\n\n\nclass Distance(metaclass=ABCMeta):\n    """"""\n    The Base Distance Class.\n    """"""\n    @abstractmethod\n    def distance(self, x, y):\n        """"""\n        Calculate the distance between x and y. a and b should be in same shape.\n        :param x: the first tensor\n        :param y: the second tensor\n        :return: the absolute distance between x and y\n        """"""\n        pass\n\n\nclass EuclideanDistance(Distance):\n    """"""\n    Euclidean Distance Measure\n    """"""\n\n    def __init__(self):\n        pass\n\n    def distance(self, x, y):\n        return np.linalg.norm(x - y)\n\n\nclass ThresholdEstimator:\n    """"""\n    An estimator to find the proper threshold.\n    """"""\n    def fit(self,\n            y,\n            yhat,\n            mode=""default"",\n            ratio=0.01,\n            dist_measure=EuclideanDistance()\n            ):\n        """"""\n        fit the y and yhat and find the proper threshold\n        :param y: actual values\n        :param yhat: predicted values\n        :param mode: types of ways to find threshold\n            ""default"" : fit data to a uniform distribution (the percentile way)\n            ""gaussian"": fit data to a gaussian distribution *TBD\n        :param ratio: the ratio of anomaly to consider as anomaly.\n        :return: the threshold\n        """"""\n        assert y.shape == yhat.shape\n        diff = [dist_measure.distance(m, n) for m, n in zip(y, yhat)]\n        if mode == ""default"":\n            threshold = np.percentile(diff, (1-ratio)*100)\n            return threshold\n        elif mode == ""gaussian"":\n            from scipy.stats import norm\n            mu, sigma = norm.fit(diff)\n            t = norm.ppf(1-ratio)\n            return t*sigma+mu\n        else:\n            raise Exception(""Does not support"", mode)\n\n\nclass DetectorBase(metaclass=ABCMeta):\n    """"""\n    Base class for detector\n    """"""\n    @abstractmethod\n    def detect(self, y, **kwargs):\n        """"""\n        Detect anomalies in dataset\n        :param y: the dataset\n        :return: the anomaly indexes in y\n        """"""\n        pass\n\n\nclass ThresholdDetector(DetectorBase):\n    """"""\n    Anomaly detector\n    """"""\n\n    def __init__(self):\n        pass\n\n    def detect(self,\n               y,\n               yhat=None,\n               threshold=math.inf,\n               dist_measure=EuclideanDistance()):\n        """"""\n        Detect anomalies. Each sample can have 1 or more dimensions.\n        :param dist_measure:\n        :param y: the values to detect. shape could be\n                1-D (num_samples,)\n                or 2-D array (num_samples, features)\n        :param yhat: the estimated values, a tensor with same shape as y,\n                could be None when threshold is a tuple\n        :param threshold: threshold, could be\n            1. a single value -  absolute distance threshold, same for all samples\n            2. a 1-D array in shape (num_samples,) - per sample absolute distance threshold\n            3. a tensor in same shape as y and yhat - per dimension absolute distance threshold\n            4. a tuple (min, max) min and max tensors, same shape as y, yhat is ignored in this case\n        :return: the anomaly values indexes in the samples, i.e. num_samples dimension.\n        """"""\n        self.threshold = threshold\n        self.dist_measure = dist_measure\n        if isinstance(threshold, int) or \\\n                isinstance(threshold, float):\n            return self._check_all_distance(y, yhat)\n        elif isinstance(threshold, np.ndarray):\n            if len(threshold.shape) == 2:\n                self._check_per_dim_distance(y, yhat, threshold)\n            elif len(threshold.shape) == 1:\n                self._check_per_sample_distance(y, yhat, threshold)\n            else:\n                raise ValueError(\n                    ""threshold shape"", str(\n                        threshold.shape), ""is not valid"")\n        elif isinstance(threshold, tuple) \\\n                and len(threshold) == 2 \\\n                and threshold[0].shape == y.shape \\\n                and threshold[-1].shape == y.shape:\n            return self._check_range(y, threshold)\n        else:\n            raise ValueError(\n                ""threshold shape"", str(threshold),\n                ""is not valid"")\n\n    def _check_all_distance(self, y, yhat):\n        index = []\n        for i in range(y.shape[0]):\n            diff = self.dist_measure.distance(y[i], yhat[i])\n            if diff >= self.threshold:\n                index.append(i)\n        return index\n\n    def _check_per_dim_distance(self, y, yhat, threshold):\n        raise NotImplementedError(""Does not support check per dim distance"")\n\n    def _check_per_sample_distance(self, y, yhat, threshold):\n        raise NotImplementedError(""Does not support check per sample distance"")\n\n    def _check_range(self, y, threshold):\n        min_diff = y - threshold[0]\n        anomaly_index = set(np.where(min_diff < 0)[0])\n        max_diff = y - threshold[1]\n        anomaly_index.update(np.where(max_diff > 0)[0])\n        return list(anomaly_index)\n'"
pyzoo/zoo/zouwu/model/forecast.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom abc import ABCMeta, abstractmethod\n\nfrom zoo.automl.model.MTNet_keras import MTNetKeras as MTNetKerasModel\nfrom zoo.automl.model.VanillaLSTM import VanillaLSTM as LSTMKerasModel\nfrom zoo.tfpark import KerasModel as TFParkKerasModel\n\nimport tensorflow as tf\n\n\nclass Forecaster(TFParkKerasModel, metaclass=ABCMeta):\n    """"""\n    Base class for TFPark KerasModel based Forecast models.\n    """"""\n\n    def __init__(self):\n        """"""\n        Initializer.\n        Turns the tf.keras model returned from _build into a tfpark.KerasModel\n        """"""\n        self.model = self._build()\n        assert (isinstance(self.model, tf.keras.Model))\n        super().__init__(self.model)\n\n    @abstractmethod\n    def _build(self):\n        """"""\n        Build a tf.keras model.\n        :return: a tf.keras model (compiled)\n        """"""\n        pass\n\n\nclass LSTMForecaster(Forecaster):\n    """"""\n    Vanilla LSTM Forecaster\n    """"""\n\n    def __init__(self,\n                 target_dim=1,\n                 feature_dim=1,\n                 lstm_1_units=16,\n                 dropout_1=0.2,\n                 lstm_2_units=8,\n                 dropout_2=0.2,\n                 metric=""mean_squared_error"",\n                 lr=0.001,\n                 uncertainty: bool = False\n                 ):\n        """"""\n        Build a LSTM Forecast Model.\n\n        :param target_dim: dimension of model output\n        :param feature_dim: dimension of input feature\n        :param lstm_1_units: num of units for the 1st LSTM layer\n        :param dropout_1: p for the 1st dropout layer\n        :param lstm_2_units: num of units for the 2nd LSTM layer\n        :param dropout_2: p for the 2nd dropout layer\n        :param metric: the metric for validation and evaluation\n        :param lr: learning rate\n        :param uncertainty: whether to return uncertainty\n        """"""\n        #\n        self.target_dim = target_dim\n        self.check_optional_config = False\n        self.uncertainty = uncertainty\n\n        self.model_config = {\n            ""lr"": lr,\n            ""lstm_1_units"": lstm_1_units,\n            ""dropout_1"": dropout_1,\n            ""lstm_2_units"": lstm_2_units,\n            ""dropout_2"": dropout_2,\n            ""metric"": metric,\n            ""feature_num"": feature_dim\n        }\n        self.internal = None\n\n        super().__init__()\n\n    def _build(self):\n        """"""\n        Build LSTM Model in tf.keras\n        """"""\n        # build model with TF/Keras\n        self.internal = LSTMKerasModel(\n            check_optional_config=self.check_optional_config,\n            future_seq_len=self.target_dim)\n        return self.internal._build(mc=self.uncertainty,\n                                    **self.model_config)\n\n\nclass MTNetForecaster(Forecaster):\n    """"""\n    MTNet Forecast Model\n    """"""\n\n    def __init__(self,\n                 target_dim=1,\n                 feature_dim=1,\n                 lb_long_steps=1,\n                 lb_long_stepsize=1,\n                 ar_window_size=1,\n                 cnn_kernel_size=1,\n                 metric=""mean_squared_error"",\n                 uncertainty: bool = False,\n                 ):\n        """"""\n        Build a MTNet Forecast Model.\n        :param target_dim: the dimension of model output\n        :param feature_dim: the dimension of input feature\n        :param lb_long_steps: the number of steps for the long-term memory series\n        :param lb_long_stepsize: the step size for long-term memory series\n        :param ar_window_size\xef\xbc\x9athe auto regression window size in MTNet\n        :param cnn_kernel_size: cnn filter height in MTNet\n        :param metric: the metric for validation and evaluation\n        :param uncertainty: whether to enable calculation of uncertainty\n        """"""\n        self.check_optional_config = False\n        self.mc = uncertainty\n        self.model_config = {\n            ""feature_num"": feature_dim,\n            ""output_dim"": target_dim,\n            ""metrics"": [metric],\n            ""mc"": uncertainty,\n            ""time_step"": lb_long_stepsize,\n            ""long_num"": lb_long_steps,\n            ""ar_window"": ar_window_size,\n            ""cnn_height"": cnn_kernel_size,\n            ""past_seq_len"": (lb_long_steps + 1) * lb_long_stepsize\n\n        }\n        self._internal = None\n\n        super().__init__()\n\n    def _build(self):\n        """"""\n        build a MTNet model in tf.keras\n        :return: a tf.keras MTNet model\n        """"""\n        # TODO change this function call after MTNet fixes\n        self.internal = MTNetKerasModel(\n            check_optional_config=self.check_optional_config,\n            future_seq_len=self.model_config.get(\'output_dim\'))\n        self.internal.apply_config(config=self.model_config)\n        return self.internal.build()\n\n    def preprocess_input(self, x):\n        """"""\n        The original rolled features needs an extra step to process.\n        This should be called before train_x, validation_x, and test_x\n        :param x: the original samples from rolling\n        :return: a tuple (long_term_x, short_term_x)\n                which are long term and short term history respectively\n        """"""\n        return self.internal._reshape_input_x(x)\n'"
pyzoo/test/zoo/automl/common/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/automl/common/test_metrics.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport numpy as np\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.automl.common.metrics import ME, MAE, MSE, RMSE, MSLE, R2\n\nfrom zoo.automl.common.metrics import MPE, MAPE, MSPE, sMAPE, MDAPE, sMDAPE\nfrom sklearn.utils.testing import assert_almost_equal\nfrom sklearn.utils.testing import assert_array_almost_equal\n\n\nclass TestMetrics(ZooTestCase):\n\n    def setup_method(self, method):\n        pass\n\n    def teardown_method(self, method):\n        pass\n\n    def test_metrics(self, n_samples=50):\n        y_true = np.arange(n_samples) + 1\n        y_pred = y_true + 1\n\n        assert_almost_equal(MSE(y_true, y_pred), [1.])\n        assert_almost_equal(MSLE(y_true, y_pred),\n                            MSE(np.log(1 + y_true),\n                            np.log(1 + y_pred)))\n        assert_almost_equal(MAE(y_true, y_pred), [1.])\n        assert_almost_equal(R2(y_true, y_pred), [0.995], 2)\n\n        assert_almost_equal(sMAPE(y_true, y_pred), [3.89], decimal=2)\n\n        y_true = [3, -0.5, 2, 7]\n        y_pred = [2.5, -0.3, 2, 8]\n\n        assert_almost_equal(MAPE(y_true, y_pred), [17.74], decimal=2)\n        assert_almost_equal(MPE(y_true, y_pred), [10.6], decimal=2)\n        assert_almost_equal(RMSE(y_true, y_pred), [0.57], decimal=2)\n        assert_almost_equal(ME(y_true, y_pred), [-0.17], decimal=2)\n        assert_almost_equal(MSPE(y_true, y_pred), [0.32], decimal=2)\n        assert_almost_equal(MDAPE(y_true, y_pred), [15.48], decimal=2)\n        assert_almost_equal(sMDAPE(y_true, y_pred), [7.88], decimal=2)\n\n    def test_multioutput_metrics(self):\n        y_true = np.array([[1, 0, 0, 1], [0, 1, 1, 1], [1, 1, 0, 1]])\n        y_pred = np.array([[0, 0, 0, 1], [1, 0, 1, 1], [0, 0, 0, 1]])\n        assert_almost_equal(MSE(y_true, y_pred), [(1. / 3 + 2. / 3 + 2. / 3) / 4.])\n\n        assert_almost_equal(MSLE(y_true, y_pred), [0.200], decimal=2)\n\n        assert_almost_equal(MAE(y_true, y_pred), [(1. + 2. / 3) / 4.])\n\n        assert_almost_equal(R2(y_true, y_pred, multioutput=\'variance_weighted\'), [1. - 5. / 2])\n\n        assert_almost_equal(R2(y_true, y_pred, multioutput=\'uniform_average\'), [-.875])\n\n        y_true = ([[3, -0.5, 2, 7], [3, -0.5, 2, 7], [3, -0.5, 2, 7]])\n        y_pred = ([[2.5, -0.3, 2, 8], [2.5, -0.3, 2, 8], [2.5, -0.3, 2, 8]])\n\n        assert_almost_equal(sMAPE(y_true, y_pred, multioutput=\'uniform_average\'),\n                            [10.19], decimal=2)\n        assert_almost_equal(MAPE(y_true, y_pred, multioutput=\'uniform_average\'), [17.74], decimal=2)\n        assert_almost_equal(MPE(y_true, y_pred, multioutput=\'uniform_average\'), [10.6], decimal=2)\n        assert_almost_equal(RMSE(y_true, y_pred, multioutput=\'uniform_average\'), [0.57], decimal=2)\n\n        assert_almost_equal(ME(y_true, y_pred, multioutput=\'uniform_average\'), [-0.18], decimal=2)\n        assert_almost_equal(MSPE(y_true, y_pred, multioutput=\'uniform_average\'), [0.32], decimal=2)\n        assert_almost_equal(MDAPE(y_true, y_pred, multioutput=\'uniform_average\'),\n                            [17.74], decimal=2)\n        assert_almost_equal(sMDAPE(y_true, y_pred, multioutput=\'uniform_average\'),\n                            [10.19], decimal=2)\n\n    def test_multioutput_array_metrics(self):\n        y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n        y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n\n        assert_array_almost_equal(MSE(y_true, y_pred, multioutput=\'raw_values\'),\n                                  [0.125, 0.5625], decimal=2)\n        assert_array_almost_equal(MAE(y_true, y_pred, multioutput=\'raw_values\'),\n                                  [0.25, 0.625], decimal=2)\n        assert_array_almost_equal(R2(y_true, y_pred, multioutput=\'raw_values\'),\n                                  [0.95, 0.93], decimal=2)\n\n        assert_array_almost_equal(sMAPE(y_true, y_pred, multioutput=\'raw_values\'),\n                                  [4.09, 12.83], decimal=2)\n        assert_array_almost_equal(MAPE(y_true, y_pred, multioutput=\'raw_values\'),\n                                  [7.78, 22.62], decimal=2)\n        assert_array_almost_equal(MPE(y_true, y_pred, multioutput=\'raw_values\'),\n                                  [2.22, 5.95], decimal=2)\n        assert_array_almost_equal(RMSE(y_true, y_pred, multioutput=\'raw_values\'),\n                                  [0.35, 0.75], decimal=2)\n\n        assert_array_almost_equal(ME(y_true, y_pred, multioutput=\'raw_values\'),\n                                  [0., 0.12], decimal=2)\n        assert_array_almost_equal(MSPE(y_true, y_pred, multioutput=\'raw_values\'),\n                                  [0.12, 0.56], decimal=2)\n        assert_array_almost_equal(MDAPE(y_true, y_pred, multioutput=\'raw_values\'),\n                                  [5.56, 20.24], decimal=2)\n        assert_array_almost_equal(sMDAPE(y_true, y_pred, multioutput=\'raw_values\'),\n                                  [2.63, 8.99], decimal=2)\n'"
pyzoo/test/zoo/automl/common/test_util.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.automl.common.util import *\n\n\nclass TestUtil(ZooTestCase):\n    def setup_method(self, method):\n        pass\n\n    def teardown_method(self, method):\n        pass\n\n    def test_train_val_test_split(self):\n        # length test\n        sample_num = 100\n        look_back = 10\n        horizon = 1\n        dates = pd.date_range(\'1/1/2020\', periods=sample_num)\n        values = np.random.randn(sample_num)\n        df = pd.DataFrame({""values"": values}, index=dates)\n        train_df, val_df, test_df = train_val_test_split(df,\n                                                         val_ratio=0.1,\n                                                         test_ratio=0.1,\n                                                         look_back=look_back,\n                                                         horizon=horizon)\n        assert len(train_df) == sample_num * 0.8\n        assert len(val_df) == sample_num * 0.1 + look_back + horizon - 1\n        assert len(test_df) == sample_num * 0.1 + look_back + horizon - 1\n        # index test\n        assert pd.api.types.is_datetime64_any_dtype(test_df.index.dtype)\n        assert pd.api.types.is_datetime64_any_dtype(val_df.index.dtype)\n'"
pyzoo/test/zoo/automl/config/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/automl/config/test_recipe.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.automl.config.recipe import *\n\n\nclass TestTimeSequencePredictor(ZooTestCase):\n\n    def setup_method(self, method):\n        pass\n\n    def teardown_method(self, method):\n        pass\n\n    def test_SmokeRecipe(self):\n        recipe = SmokeRecipe()\n        assert recipe.num_samples == 1\n        assert recipe.training_iteration == 1\n        assert recipe.reward_metric is None\n\n    def test_MTNetSmokeRecipe(self):\n        recipe = MTNetSmokeRecipe()\n        assert recipe.num_samples == 1\n        assert recipe.training_iteration == 1\n        assert recipe.reward_metric is None\n\n    def test_GridRandomRecipe(self):\n        recipe = GridRandomRecipe(num_rand_samples=100)\n        search_space = recipe.search_space(\n            all_available_features=[\n                ""f1"", ""f2"", ""f3"", ""target1"", ""target2""])\n        assert search_space is not None\n\n    def test_RandomRecipe(self):\n        recipe = GridRandomRecipe(num_rand_samples=100)\n        search_space = recipe.search_space(\n            all_available_features=[\n                ""f1"", ""f2"", ""f3"", ""target1"", ""target2""])\n        assert search_space is not None\n\n    def test_LSTMGridRandomRecipe(self):\n        recipe = LSTMGridRandomRecipe(num_rand_samples=100)\n        search_space = recipe.search_space(\n            all_available_features=[\n                ""f1"", ""f2"", ""f3"", ""target1"", ""target2""])\n        assert search_space is not None\n\n    def test_MTNetGridRandomRecipe(self):\n        recipe = MTNetGridRandomRecipe()\n        search_space = recipe.search_space(\n            all_available_features=[\n                ""f1"", ""f2"", ""f3"", ""target1"", ""target2""])\n        assert search_space is not None\n\n    def test_BayesRecipe(self):\n        recipe = BayesRecipe(num_samples=10)\n        search_space = recipe.search_space(\n            all_available_features=[\n                ""f1"", ""f2"", ""f3"", ""target1"", ""target2""])\n        assert search_space is not None\n        assert recipe.reward_metric is not None\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/automl/feature/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/automl/feature/test_time_sequence_feature.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport shutil\nimport tempfile\n\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.automl.common.util import *\nfrom zoo.automl.feature.time_sequence import *\nfrom numpy.testing import assert_array_almost_equal\nimport json\n\n\nclass TestTimeSequenceFeature(ZooTestCase):\n    def setup_method(self, method):\n        pass\n\n    def teardown_method(self, method):\n        pass\n\n    def test_get_feature_list(self):\n        dates = pd.date_range(\'1/1/2019\', periods=8)\n        data = np.random.randn(8, 3)\n        df = pd.DataFrame({""datetime"": dates, ""values"": data[:, 0],\n                           ""A"": data[:, 1], ""B"": data[:, 2]})\n        feat = TimeSequenceFeatureTransformer(dt_col=""datetime"",\n                                              target_col=""values"",\n                                              extra_features_col=[""A"", ""B""],\n                                              drop_missing=True)\n        feature_list = feat.get_feature_list(df)\n        assert set(feature_list) == {\'IS_AWAKE(datetime)\',\n                                     \'IS_BUSY_HOURS(datetime)\',\n                                     \'HOUR(datetime)\',\n                                     \'DAY(datetime)\',\n                                     \'IS_WEEKEND(datetime)\',\n                                     \'WEEKDAY(datetime)\',\n                                     \'MONTH(datetime)\',\n                                     \'A\',\n                                     \'B\'}\n\n    def test_fit_transform(self):\n        sample_num = 8\n        past_seq_len = 2\n        dates = pd.date_range(\'1/1/2019\', periods=sample_num)\n        data = np.random.randn(sample_num, 3)\n        df = pd.DataFrame({""datetime"": dates, ""values"": data[:, 0],\n                           ""A"": data[:, 1], ""B"": data[:, 2]})\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\',\n                                                   \'A\']),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=1, dt_col=""datetime"",\n                                              target_col=""values"", drop_missing=True)\n        x, y = feat.fit_transform(df, **config)\n        assert x.shape == (sample_num - past_seq_len,\n                           past_seq_len,\n                           len(json.loads(config[""selected_features""])) + 1)\n        assert y.shape == (sample_num - past_seq_len, 1)\n        assert np.mean(np.concatenate((x[0, :, 0], y[:, 0]), axis=None)) < 1e-5\n\n    def test_fit_transform_df_list(self):\n        sample_num = 8\n        past_seq_len = 2\n        dates = pd.date_range(\'1/1/2019\', periods=sample_num)\n        data = np.random.randn(sample_num, 3)\n        df = pd.DataFrame({""datetime"": dates, ""values"": data[:, 0],\n                           ""A"": data[:, 1], ""B"": data[:, 2]})\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\',\n                                                   \'A\']),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=1, dt_col=""datetime"",\n                                              target_col=""values"", drop_missing=True)\n\n        df_list = [df] * 3\n        x, y = feat.fit_transform(df_list, **config)\n        single_result_len = sample_num - past_seq_len\n        assert x.shape == (single_result_len * 3,\n                           past_seq_len,\n                           len(json.loads(config[""selected_features""])) + 1)\n        assert y.shape == (single_result_len * 3, 1)\n        assert_array_almost_equal(x[:single_result_len],\n                                  x[single_result_len: 2 * single_result_len], decimal=2)\n        assert_array_almost_equal(x[:single_result_len], x[2 * single_result_len:], decimal=2)\n        assert_array_almost_equal(y[:single_result_len],\n                                  y[single_result_len: 2 * single_result_len],\n                                  decimal=2)\n        assert_array_almost_equal(y[:single_result_len], y[2 * single_result_len:], decimal=2)\n\n        assert np.mean(np.concatenate((x[0, :, 0], y[:single_result_len, 0]), axis=None)) < 1e-5\n\n    def test_fit_transform_input_datetime(self):\n        # if the type of input datetime is not datetime64, raise an error\n        dates = pd.date_range(\'1/1/2019\', periods=8)\n        values = np.random.randn(8)\n        df = pd.DataFrame({""datetime"": dates.strftime(\'%m/%d/%Y\'), ""values"": values})\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\']),\n                  ""past_seq_len"": 2}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=1, dt_col=""datetime"",\n                                              target_col=""values"", drop_missing=True)\n\n        with pytest.raises(ValueError) as excinfo:\n            feat.fit_transform(df, **config)\n        assert \'np.datetime64\' in str(excinfo.value)\n\n        # if there is NaT in datetime, raise an error\n        df.loc[1, ""datetime""] = None\n        with pytest.raises(ValueError, match=r"".* datetime .*""):\n            feat.fit_transform(df, **config)\n\n        # if the last datetime is larger than current time, raise an error\n        dates = pd.date_range(\'1/1/2119\', periods=8)\n        values = np.random.randn(8)\n        df = pd.DataFrame({""datetime"": dates, ""values"": values})\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\']),\n                  ""past_seq_len"": 2}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=1, dt_col=""datetime"",\n                                              target_col=""values"", drop_missing=True)\n\n        with pytest.raises(ValueError, match=r"".* current .*""):\n            feat.fit_transform(df, **config)\n\n    def test_input_data_len(self):\n        sample_num = 100\n        past_seq_len = 20\n        dates = pd.date_range(\'1/1/2019\', periods=sample_num)\n        values = np.random.randn(sample_num)\n        df = pd.DataFrame({""datetime"": dates, ""values"": values})\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\']),\n                  ""past_seq_len"": past_seq_len}\n        train_df, val_df, test_df = train_val_test_split(df,\n                                                         val_ratio=0.1,\n                                                         test_ratio=0.1,\n                                                         look_back=10)\n\n        feat = TimeSequenceFeatureTransformer(future_seq_len=1, dt_col=""datetime"",\n                                              target_col=""values"", drop_missing=True)\n        with pytest.raises(ValueError, match=r"".*past sequence length.*""):\n            feat.fit_transform(train_df[:20], **config)\n\n        feat.fit_transform(train_df, **config)\n        with pytest.raises(ValueError, match=r"".*past sequence length.*""):\n            feat.transform(val_df, is_train=True)\n\n        with pytest.raises(ValueError, match=r"".*past sequence length.*""):\n            feat.transform(test_df[:-1], is_train=False)\n        out_x, out_y = feat.transform(test_df, is_train=False)\n        assert len(out_x) == 1\n        assert out_y is None\n\n    def test_fit_transform_input_data(self):\n        # if there is NaN in data other than datetime, drop the training sample.\n        num_samples = 8\n        dates = pd.date_range(\'1/1/2019\', periods=num_samples)\n        values = np.random.randn(num_samples)\n        df = pd.DataFrame({""datetime"": dates, ""values"": values})\n        df.loc[2, ""values""] = None\n        past_seq_len = 2\n\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\']),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=1, dt_col=""datetime"",\n                                              target_col=""values"", drop_missing=True)\n\n        x, y = feat.fit_transform(df, **config)\n        # mask_x = [1, 0, 0, 1, 1, 1]\n        # mask_y = [0, 1, 1, 1, 1, 1]\n        # mask   = [0, 0, 0, 1, 1, 1]\n        assert x.shape == (3, past_seq_len, len(json.loads(config[""selected_features""])) + 1)\n        assert y.shape == (3, 1)\n\n    def test_transform_train_true(self):\n        num_samples = 16\n        dates = pd.date_range(\'1/1/2019\', periods=num_samples)\n        values = np.random.randn(num_samples, 2)\n        df = pd.DataFrame({""datetime"": dates, ""values"": values[:, 0], ""feature_1"": values[:, 1]})\n        train_sample_num = 10\n        train_df = df[:train_sample_num]\n        val_df = df[train_sample_num:]\n        past_seq_len = 2\n\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\',\n                                                   ""feature_1""]),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=1, dt_col=""datetime"",\n                                              target_col=""values"",\n                                              extra_features_col=""feature_1"",\n                                              drop_missing=True)\n\n        feat.fit_transform(train_df, **config)\n        val_x, val_y = feat.transform(val_df, is_train=True)\n        assert val_x.shape == (val_df.shape[0] - past_seq_len,\n                               past_seq_len,\n                               len(json.loads(config[""selected_features""])) + 1)\n        assert val_y.shape == (val_df.shape[0] - past_seq_len, 1)\n\n    def test_transform_train_true_df_list(self):\n        num_samples = 16\n        dates = pd.date_range(\'1/1/2019\', periods=num_samples)\n        values = np.random.randn(num_samples, 2)\n        df = pd.DataFrame({""datetime"": dates, ""values"": values[:, 0], ""feature_1"": values[:, 1]})\n        train_sample_num = 10\n        train_df = df[:train_sample_num]\n        val_df = df[train_sample_num:]\n        past_seq_len = 2\n\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\',\n                                                   ""feature_1""]),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=1, dt_col=""datetime"",\n                                              target_col=""values"",\n                                              extra_features_col=""feature_1"",\n                                              drop_missing=True)\n\n        train_df_list = [train_df] * 3\n        feat.fit_transform(train_df_list, **config)\n        val_df_list = [val_df] * 3\n        val_x, val_y = feat.transform(val_df_list, is_train=True)\n        single_result_len = val_df.shape[0] - past_seq_len\n        assert val_x.shape == (single_result_len * 3,\n                               past_seq_len,\n                               len(json.loads(config[""selected_features""])) + 1)\n        assert val_y.shape == (single_result_len * 3, 1)\n\n    def test_transform_train_false(self):\n        num_samples = 16\n        dates = pd.date_range(\'1/1/2019\', periods=num_samples)\n        values = np.random.randn(num_samples, 2)\n        df = pd.DataFrame({""datetime"": dates, ""values"": values[:, 0], ""feature_1"": values[:, 1]})\n        train_sample_num = 10\n        train_df = df[:train_sample_num]\n        test_df = df[train_sample_num:]\n        past_seq_len = 2\n\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\',\n                                                   ""feature_1""]),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=1, dt_col=""datetime"",\n                                              target_col=""values"",\n                                              extra_features_col=""feature_1"",\n                                              drop_missing=True)\n        feat.fit_transform(train_df, **config)\n        test_x, _ = feat.transform(test_df, is_train=False)\n        assert test_x.shape == (test_df.shape[0] - past_seq_len + 1,\n                                past_seq_len,\n                                len(json.loads(config[""selected_features""])) + 1)\n\n    def test_transform_train_false_df_list(self):\n        num_samples = 16\n        dates = pd.date_range(\'1/1/2019\', periods=num_samples)\n        values = np.random.randn(num_samples, 2)\n        df = pd.DataFrame({""datetime"": dates, ""values"": values[:, 0], ""feature_1"": values[:, 1]})\n        train_sample_num = 10\n        train_df = df[:train_sample_num]\n        test_df = df[train_sample_num:]\n        past_seq_len = 2\n\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\',\n                                                   ""feature_1""]),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=1, dt_col=""datetime"",\n                                              target_col=""values"",\n                                              extra_features_col=""feature_1"",\n                                              drop_missing=True)\n        train_df_list = [train_df] * 3\n        feat.fit_transform(train_df_list, **config)\n        test_df_list = [test_df] * 3\n        test_x, _ = feat.transform(test_df_list, is_train=False)\n        assert test_x.shape == ((test_df.shape[0] - past_seq_len + 1) * 3,\n                                past_seq_len,\n                                len(json.loads(config[""selected_features""])) + 1)\n\n    def test_save_restore(self):\n        dates = pd.date_range(\'1/1/2019\', periods=8)\n        values = np.random.randn(8)\n        df = pd.DataFrame({""dt"": dates, ""v"": values})\n\n        future_seq_len = 2\n        dt_col = ""dt""\n        target_col = ""v""\n        drop_missing = True\n        feat = TimeSequenceFeatureTransformer(future_seq_len=future_seq_len,\n                                              dt_col=dt_col,\n                                              target_col=target_col,\n                                              drop_missing=drop_missing)\n\n        feature_list = feat.get_feature_list(df)\n        config = {""selected_features"": json.dumps(feature_list),\n                  ""past_seq_len"": 2\n                  }\n\n        train_x, train_y = feat.fit_transform(df, **config)\n\n        dirname = tempfile.mkdtemp(prefix=""automl_test_feature"")\n        try:\n            save(dirname, feature_transformers=feat)\n            new_ft = TimeSequenceFeatureTransformer()\n            restore(dirname, feature_transformers=new_ft, config=config)\n\n            assert new_ft.future_seq_len == future_seq_len\n            assert new_ft.dt_col == dt_col\n            assert new_ft.target_col == target_col\n            assert new_ft.extra_features_col is None\n            assert new_ft.drop_missing == drop_missing\n\n            test_x, _ = new_ft.transform(df[:-future_seq_len], is_train=False)\n\n            assert_array_almost_equal(test_x, train_x, decimal=2)\n\n        finally:\n            shutil.rmtree(dirname)\n\n    def test_post_processing_train(self):\n        dates = pd.date_range(\'1/1/2019\', periods=8)\n        values = np.random.randn(8)\n        dt_col = ""datetime""\n        value_col = ""values""\n        df = pd.DataFrame({dt_col: dates, value_col: values})\n\n        past_seq_len = 2\n        future_seq_len = 1\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\']),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=future_seq_len, dt_col=""datetime"",\n                                              target_col=""values"", drop_missing=True)\n\n        train_x, train_y = feat.fit_transform(df, **config)\n        y_unscale, y_unscale_1 = feat.post_processing(df, train_y, is_train=True)\n        y_input = df[past_seq_len:][[value_col]].values\n        msg = ""y_unscale is {}, y_unscale_1 is {}"".format(y_unscale, y_unscale_1)\n        assert_array_almost_equal(y_unscale, y_unscale_1, decimal=2), msg\n        msg = ""y_unscale is {}, y_input is {}"".format(y_unscale, y_input)\n        assert_array_almost_equal(y_unscale, y_input, decimal=2), msg\n\n    def test_post_processing_train_df_list(self):\n        dates = pd.date_range(\'1/1/2019\', periods=8)\n        values = np.random.randn(8)\n        dt_col = ""datetime""\n        value_col = ""values""\n        df = pd.DataFrame({dt_col: dates, value_col: values})\n\n        past_seq_len = 2\n        future_seq_len = 1\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\']),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=future_seq_len, dt_col=""datetime"",\n                                              target_col=""values"", drop_missing=True)\n        df_list = [df] * 3\n        train_x, train_y = feat.fit_transform(df_list, **config)\n        y_unscale, y_unscale_1 = feat.post_processing(df_list, train_y, is_train=True)\n        y_input = df[past_seq_len:][[value_col]].values\n        target_y = np.concatenate([y_input] * 3)\n        msg = ""y_unscale is {}, y_unscale_1 is {}"".format(y_unscale, y_unscale_1)\n        assert_array_almost_equal(y_unscale, y_unscale_1, decimal=2), msg\n        msg = ""y_unscale is {}, y_input is {}"".format(y_unscale, target_y)\n        assert_array_almost_equal(y_unscale, target_y, decimal=2), msg\n\n    def test_post_processing_test_1(self):\n        dates = pd.date_range(\'1/1/2019\', periods=8)\n        values = np.random.randn(8)\n        dt_col = ""datetime""\n        value_col = ""values""\n        df = pd.DataFrame({dt_col: dates, value_col: values})\n\n        past_seq_len = 2\n        future_seq_len = 1\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\']),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=future_seq_len, dt_col=""datetime"",\n                                              target_col=""values"", drop_missing=True)\n\n        train_x, train_y = feat.fit_transform(df, **config)\n\n        dirname = tempfile.mkdtemp(prefix=""automl_test_feature_"")\n        try:\n            save(dirname, feature_transformers=feat)\n            new_ft = TimeSequenceFeatureTransformer()\n            restore(dirname, feature_transformers=new_ft, config=config)\n\n            test_df = df[:-future_seq_len]\n            new_ft.transform(test_df, is_train=False)\n            output_value_df = new_ft.post_processing(test_df, train_y, is_train=False)\n\n            # train_y is generated from df[past_seq_len:]\n            target_df = df[past_seq_len:].copy().reset_index(drop=True)\n\n            assert output_value_df[dt_col].equals(target_df[dt_col])\n            assert_array_almost_equal(output_value_df[value_col].values,\n                                      target_df[value_col].values, decimal=2)\n\n        finally:\n            shutil.rmtree(dirname)\n\n    def test_post_processing_test_df_list(self):\n        dates = pd.date_range(\'1/1/2019\', periods=8)\n        values = np.random.randn(8)\n        dt_col = ""datetime""\n        value_col = ""values""\n        df = pd.DataFrame({dt_col: dates, value_col: values})\n\n        past_seq_len = 2\n        future_seq_len = 1\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\']),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=future_seq_len, dt_col=""datetime"",\n                                              target_col=""values"", drop_missing=True)\n        df_list = [df] * 3\n        train_x, train_y = feat.fit_transform(df_list, **config)\n\n        dirname = tempfile.mkdtemp(prefix=""automl_test_feature_"")\n        try:\n            save(dirname, feature_transformers=feat)\n            new_ft = TimeSequenceFeatureTransformer()\n            restore(dirname, feature_transformers=new_ft, config=config)\n\n            test_df = df[:-future_seq_len]\n            test_df_list = [test_df] * 3\n            new_ft.transform(test_df_list, is_train=False)\n            output_value_df_list = new_ft.post_processing(test_df_list, train_y, is_train=False)\n\n            # train_y is generated from df[past_seq_len:]\n            target_df = df[past_seq_len:].copy().reset_index(drop=True)\n\n            assert output_value_df_list[0].equals(output_value_df_list[1])\n            assert output_value_df_list[0].equals(output_value_df_list[2])\n            assert output_value_df_list[0][dt_col].equals(target_df[dt_col])\n            assert_array_almost_equal(output_value_df_list[0][value_col].values,\n                                      target_df[value_col].values, decimal=2)\n\n        finally:\n            shutil.rmtree(dirname)\n\n    def test_post_processing_test_2(self):\n        sample_num = 8\n        dates = pd.date_range(\'1/1/2019\', periods=sample_num)\n        values = np.random.randn(sample_num)\n        dt_col = ""datetime""\n        value_col = ""values""\n        df = pd.DataFrame({dt_col: dates, value_col: values})\n\n        past_seq_len = 2\n        future_seq_len = 2\n        config = {""selected_features"": json.dumps([\'IS_AWAKE(datetime)\',\n                                                   \'IS_BUSY_HOURS(datetime)\',\n                                                   \'HOUR(datetime)\']),\n                  ""past_seq_len"": past_seq_len}\n        feat = TimeSequenceFeatureTransformer(future_seq_len=future_seq_len, dt_col=""datetime"",\n                                              target_col=""values"", drop_missing=True)\n\n        train_x, train_y = feat.fit_transform(df, **config)\n\n        dirname = tempfile.mkdtemp(prefix=""automl_test_feature_"")\n        try:\n            save(dirname, feature_transformers=feat)\n            new_ft = TimeSequenceFeatureTransformer()\n            restore(dirname, feature_transformers=new_ft, config=config)\n\n            test_df = df[:-future_seq_len]\n            new_ft.transform(test_df, is_train=False)\n            output_value_df = new_ft.post_processing(test_df, train_y, is_train=False)\n            assert output_value_df.shape == (sample_num - past_seq_len - future_seq_len + 1,\n                                             future_seq_len + 1)\n\n            columns = [""{}_{}"".format(value_col, i) for i in range(future_seq_len)]\n            output_value = output_value_df[columns].values\n            target_df = df[past_seq_len:].copy().reset_index(drop=True)\n            target_value = feat._roll_test(target_df[""values""], future_seq_len)\n\n            assert output_value_df[dt_col].equals(target_df[:-future_seq_len + 1][dt_col])\n            msg = ""output_value is {}, target_value is {}"".format(output_value, target_value)\n            assert_array_almost_equal(output_value, target_value, decimal=2), msg\n\n        finally:\n            shutil.rmtree(dirname)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/automl/model/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/automl/model/test_Seq2Seq.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport shutil\nimport tempfile\n\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.automl.model.Seq2Seq import *\nfrom zoo.automl.feature.time_sequence import TimeSequenceFeatureTransformer\nfrom numpy.testing import assert_array_almost_equal\n\n\nclass TestSeq2Seq(ZooTestCase):\n\n    def setup_method(self, method):\n        # super().setup_method(method)\n        self.train_data = pd.DataFrame(data=np.random.randn(64, 4))\n        self.val_data = pd.DataFrame(data=np.random.randn(16, 4))\n        self.test_data = pd.DataFrame(data=np.random.randn(16, 4))\n\n        self.past_seq_len = 6\n        self.future_seq_len_1 = 1\n        self.future_seq_len_2 = 2\n\n        # use roll method in time_sequence\n        self.feat = TimeSequenceFeatureTransformer()\n\n        self.config = {\n            \'batch_size\': 32,\n            \'epochs\': 1\n        }\n\n        self.model_1 = LSTMSeq2Seq(check_optional_config=False,\n                                   future_seq_len=self.future_seq_len_1)\n        self.model_2 = LSTMSeq2Seq(check_optional_config=False,\n                                   future_seq_len=self.future_seq_len_2)\n\n        self.fitted = False\n        self.predict_1 = None\n        self.predict_2 = None\n\n    def teardown_method(self, method):\n        pass\n\n    def test_fit_eval_1(self):\n        x_train_1, y_train_1 = self.feat._roll_train(self.train_data,\n                                                     past_seq_len=self.past_seq_len,\n                                                     future_seq_len=self.future_seq_len_1)\n        print(""fit_eval_future_seq_len_1:"",\n              self.model_1.fit_eval(x_train_1, y_train_1, **self.config))\n        assert self.model_1.past_seq_len == 6\n        assert self.model_1.feature_num == 4\n        assert self.model_1.future_seq_len == 1\n        assert self.model_1.target_col_num == 1\n\n    def test_fit_eval_2(self):\n        x_train_2, y_train_2 = self.feat._roll_train(self.train_data,\n                                                     past_seq_len=self.past_seq_len,\n                                                     future_seq_len=self.future_seq_len_2)\n        print(""fit_eval_future_seq_len_2:"",\n              self.model_2.fit_eval(x_train_2, y_train_2, **self.config))\n        assert self.model_2.future_seq_len == 2\n\n        self.fitted = True\n\n    def test_evaluate_1(self):\n        x_train_1, y_train_1 = self.feat._roll_train(self.train_data,\n                                                     past_seq_len=self.past_seq_len,\n                                                     future_seq_len=self.future_seq_len_1)\n        x_val_1, y_val_1 = self.feat._roll_train(self.val_data,\n                                                 past_seq_len=self.past_seq_len,\n                                                 future_seq_len=self.future_seq_len_1)\n\n        self.model_1.fit_eval(x_train_1, y_train_1, **self.config)\n\n        print(""evaluate_future_seq_len_1:"", self.model_1.evaluate(x_val_1,\n                                                                  y_val_1,\n                                                                  metric=[\'mse\',\n                                                                          \'r2\']))\n\n    def test_evaluate_2(self):\n        x_train_2, y_train_2 = self.feat._roll_train(self.train_data,\n                                                     past_seq_len=self.past_seq_len,\n                                                     future_seq_len=self.future_seq_len_2)\n        x_val_2, y_val_2 = self.feat._roll_train(self.val_data,\n                                                 past_seq_len=self.past_seq_len,\n                                                 future_seq_len=self.future_seq_len_2)\n\n        self.model_2.fit_eval(x_train_2, y_train_2, **self.config)\n\n        print(""evaluate_future_seq_len_2:"", self.model_2.evaluate(x_val_2,\n                                                                  y_val_2,\n                                                                  metric=[\'mse\',\n                                                                          \'r2\']))\n\n    def test_predict_1(self):\n        x_train_1, y_train_1 = self.feat._roll_train(self.train_data,\n                                                     past_seq_len=self.past_seq_len,\n                                                     future_seq_len=self.future_seq_len_1)\n        x_test_1 = self.feat._roll_test(self.test_data, past_seq_len=self.past_seq_len)\n        self.model_1.fit_eval(x_train_1, y_train_1, **self.config)\n\n        predict_1 = self.model_1.predict(x_test_1)\n        assert predict_1.shape == (x_test_1.shape[0], self.future_seq_len_1)\n\n    def test_predict_2(self):\n        x_train_2, y_train_2 = self.feat._roll_train(self.train_data,\n                                                     past_seq_len=self.past_seq_len,\n                                                     future_seq_len=self.future_seq_len_2)\n        x_test_2 = self.feat._roll_test(self.test_data, past_seq_len=self.past_seq_len)\n        self.model_2.fit_eval(x_train_2, y_train_2, **self.config)\n\n        predict_2 = self.model_2.predict(x_test_2)\n        assert predict_2.shape == (x_test_2.shape[0], self.future_seq_len_2)\n\n    def test_save_restore_1(self):\n        x_train_1, y_train_1 = self.feat._roll_train(self.train_data,\n                                                     past_seq_len=self.past_seq_len,\n                                                     future_seq_len=self.future_seq_len_1)\n        x_test_1 = self.feat._roll_test(self.test_data, past_seq_len=self.past_seq_len)\n        self.model_1.fit_eval(x_train_1, y_train_1, **self.config)\n\n        predict_1_before = self.model_1.predict(x_test_1)\n        new_model_1 = LSTMSeq2Seq(check_optional_config=False)\n\n        dirname = tempfile.mkdtemp(prefix=""automl_test_feature"")\n        try:\n            save(dirname, model=self.model_1)\n            restore(dirname, model=new_model_1, config=self.config)\n            predict_1_after = new_model_1.predict(x_test_1)\n            assert_array_almost_equal(predict_1_before, predict_1_after, decimal=2), \\\n                ""Prediction values are not the same after restore: "" \\\n                ""predict before is {}, and predict after is {}"".format(predict_1_before,\n                                                                       predict_1_after)\n            new_config = {\'epochs\': 1}\n            new_model_1.fit_eval(x_train_1, y_train_1, **new_config)\n        finally:\n            shutil.rmtree(dirname)\n\n    def test_save_restore_2(self):\n        x_train_2, y_train_2 = self.feat._roll_train(self.train_data,\n                                                     past_seq_len=self.past_seq_len,\n                                                     future_seq_len=self.future_seq_len_2)\n        x_test_2 = self.feat._roll_test(self.test_data, past_seq_len=self.past_seq_len)\n        self.model_2.fit_eval(x_train_2, y_train_2, **self.config)\n\n        predict_2_before = self.model_2.predict(x_test_2)\n        new_model_2 = LSTMSeq2Seq(check_optional_config=False)\n\n        dirname = tempfile.mkdtemp(prefix=""automl_test_feature"")\n        try:\n            save(dirname, model=self.model_2)\n            restore(dirname, model=new_model_2, config=self.config)\n            predict_2_after = new_model_2.predict(x_test_2)\n            assert_array_almost_equal(predict_2_before, predict_2_after, decimal=2), \\\n                ""Prediction values are not the same after restore: "" \\\n                ""predict before is {}, and predict after is {}"".format(predict_2_before,\n                                                                       predict_2_after)\n            new_config = {\'epochs\': 2}\n            new_model_2.fit_eval(x_train_2, y_train_2, **new_config)\n        finally:\n            shutil.rmtree(dirname)\n\n    def test_predict_with_uncertainty(self,):\n        x_train_2, y_train_2 = self.feat._roll_train(self.train_data,\n                                                     past_seq_len=self.past_seq_len,\n                                                     future_seq_len=self.future_seq_len_2)\n        x_test_2 = self.feat._roll_test(self.test_data, past_seq_len=self.past_seq_len)\n        self.model_2.fit_eval(x_train_2, y_train_2, mc=True, **self.config)\n        prediction, uncertainty = self.model_2.predict_with_uncertainty(x_test_2, n_iter=2)\n        assert prediction.shape == (x_test_2.shape[0], self.future_seq_len_2)\n        assert uncertainty.shape == (x_test_2.shape[0], self.future_seq_len_2)\n        assert np.any(uncertainty)\n\n        new_model_2 = LSTMSeq2Seq(check_optional_config=False)\n        dirname = tempfile.mkdtemp(prefix=""automl_test_feature"")\n        try:\n            save(dirname, model=self.model_2)\n            restore(dirname, model=new_model_2, config=self.config)\n            prediction, uncertainty = new_model_2.predict_with_uncertainty(x_test_2, n_iter=2)\n            assert prediction.shape == (x_test_2.shape[0], self.future_seq_len_2)\n            assert uncertainty.shape == (x_test_2.shape[0], self.future_seq_len_2)\n            assert np.any(uncertainty)\n        finally:\n            shutil.rmtree(dirname)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/automl/model/test_VanillaLSTM.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.automl.model.VanillaLSTM import *\nfrom zoo.automl.feature.time_sequence import TimeSequenceFeatureTransformer\nfrom numpy.testing import assert_array_almost_equal\n\n\nclass TestVanillaLSTM(ZooTestCase):\n\n    def setup_method(self, method):\n        # super().setup_method(method)\n        train_data = pd.DataFrame(data=np.random.randn(64, 4))\n        val_data = pd.DataFrame(data=np.random.randn(16, 4))\n        test_data = pd.DataFrame(data=np.random.randn(16, 4))\n\n        future_seq_len = 1\n        past_seq_len = 6\n\n        # use roll method in time_sequence\n        tsft = TimeSequenceFeatureTransformer()\n        self.x_train, self.y_train = tsft._roll_train(train_data,\n                                                      past_seq_len=past_seq_len,\n                                                      future_seq_len=future_seq_len)\n        self.x_val, self.y_val = tsft._roll_train(val_data,\n                                                  past_seq_len=past_seq_len,\n                                                  future_seq_len=future_seq_len)\n        self.x_test = tsft._roll_test(test_data, past_seq_len=past_seq_len)\n        self.config = {\n            \'epochs\': 1,\n            ""lr"": 0.001,\n            ""lstm_1_units"": 16,\n            ""dropout_1"": 0.2,\n            ""lstm_2_units"": 10,\n            ""dropout_2"": 0.2,\n            ""batch_size"": 32,\n        }\n        self.model = VanillaLSTM(check_optional_config=False, future_seq_len=future_seq_len)\n\n    def teardown_method(self, method):\n        pass\n\n    def test_fit_eval(self):\n        print(""fit_eval:"", self.model.fit_eval(self.x_train,\n                                               self.y_train,\n                                               **self.config))\n\n    def test_fit_eval_mc(self):\n        print(""fit_eval:"", self.model.fit_eval(self.x_train,\n                                               self.y_train,\n                                               mc=True,\n                                               **self.config))\n\n    def test_evaluate(self):\n        self.model.fit_eval(self.x_train, self.y_train, **self.config)\n        mse, rs = self.model.evaluate(self.x_val,\n                                      self.y_val,\n                                      metric=[\'mse\', \'r2\'])\n        print(""Mean squared error is:"", mse)\n        print(""R square is:"", rs)\n\n    def test_predict(self):\n        self.model.fit_eval(self.x_train, self.y_train, **self.config)\n        self.y_pred = self.model.predict(self.x_test)\n        assert self.y_pred.shape == (self.x_test.shape[0], 1)\n\n    def test_save_restore(self):\n        new_model = VanillaLSTM(check_optional_config=False)\n        self.model.fit_eval(self.x_train, self.y_train, **self.config)\n        predict_before = self.model.predict(self.x_test)\n\n        dirname = tempfile.mkdtemp(prefix=""automl_test_vanilla"")\n        try:\n            save(dirname, model=self.model)\n            restore(dirname, model=new_model, config=self.config)\n            predict_after = new_model.predict(self.x_test)\n            assert_array_almost_equal(predict_before, predict_after, decimal=2)\n            new_config = {\'epochs\': 2}\n            new_model.fit_eval(self.x_train, self.y_train, **new_config)\n\n        finally:\n            shutil.rmtree(dirname)\n\n    def test_predict_with_uncertainty(self,):\n        self.model.fit_eval(self.x_train, self.y_train, mc=True, **self.config)\n        prediction, uncertainty = self.model.predict_with_uncertainty(self.x_test, n_iter=10)\n        assert prediction.shape == (self.x_test.shape[0], 1)\n        assert uncertainty.shape == (self.x_test.shape[0], 1)\n        assert np.any(uncertainty)\n\n        new_model = VanillaLSTM(check_optional_config=False)\n        dirname = tempfile.mkdtemp(prefix=""automl_test_feature"")\n        try:\n            save(dirname, model=self.model)\n            restore(dirname, model=new_model, config=self.config)\n            prediction, uncertainty = new_model.predict_with_uncertainty(self.x_test, n_iter=2)\n            assert prediction.shape == (self.x_test.shape[0], 1)\n            assert uncertainty.shape == (self.x_test.shape[0], 1)\n            assert np.any(uncertainty)\n        finally:\n            shutil.rmtree(dirname)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/automl/model/test_mtnet.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport shutil\n\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.automl.model.MTNet_keras import MTNetKeras\nfrom zoo.automl.feature.time_sequence import TimeSequenceFeatureTransformer\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom zoo.automl.common.util import save, restore\nfrom numpy.testing import assert_array_almost_equal\n\n\nclass TestMTNetKeras(ZooTestCase):\n\n    def setup_method(self, method):\n        tf.keras.backend.clear_session()\n        self.ft = TimeSequenceFeatureTransformer()\n        self.create_data()\n        self.model = MTNetKeras()\n        self.config = {""long_num"": self.long_num,\n                       ""time_step"": self.time_step,\n                       ""ar_window"": np.random.randint(1, 3),\n                       ""cnn_height"": np.random.randint(1, 3),\n                       ""epochs"": 1}\n\n    def teardown_method(self, method):\n        pass\n\n    def create_data(self):\n        def gen_train_sample(data, past_seq_len, future_seq_len):\n            data = pd.DataFrame(data)\n            x, y = self.ft._roll_train(data,\n                                       past_seq_len=past_seq_len,\n                                       future_seq_len=future_seq_len\n                                       )\n            return x, y\n\n        def gen_test_sample(data, past_seq_len):\n            test_data = pd.DataFrame(data)\n            x = self.ft._roll_test(test_data, past_seq_len=past_seq_len)\n            return x\n\n        self.long_num = 6\n        self.time_step = 2\n        look_back = (self.long_num + 1) * self.time_step\n        look_forward = 1\n        self.x_train, self.y_train = gen_train_sample(data=np.random.randn(\n            64, 4), past_seq_len=look_back, future_seq_len=look_forward)\n        self.x_val, self.y_val = gen_train_sample(data=np.random.randn(16, 4),\n                                                  past_seq_len=look_back,\n                                                  future_seq_len=look_forward)\n        self.x_test = gen_test_sample(data=np.random.randn(16, 4),\n                                      past_seq_len=look_back)\n\n    def test_fit_evaluate(self):\n        self.model.fit_eval(self.x_train, self.y_train,\n                            validation_data=(self.x_val, self.y_val),\n                            **self.config)\n        self.model.evaluate(self.x_val, self.y_val)\n\n    def test_save_restore(self):\n        self.model.fit_eval(self.x_train, self.y_train,\n                            validation_data=(self.x_val, self.y_val),\n                            **self.config)\n        y_pred = self.model.predict(self.x_test)\n        assert y_pred.shape == (self.x_test.shape[0], self.y_train.shape[1])\n        dirname = ""tmp""\n        restored_model = MTNetKeras()\n        try:\n            save(dirname, model=self.model)\n            restore(dirname, model=restored_model, config=self.config)\n            predict_after = restored_model.predict(self.x_test)\n            assert_array_almost_equal(y_pred, predict_after, decimal=2), \\\n                ""Prediction values are not the same after restore: "" \\\n                ""predict before is {}, and predict after is {}"".format(y_pred, predict_after)\n            restored_model.fit_eval(self.x_train, self.y_train, epochs=1)\n            restored_model.evaluate(self.x_val, self.y_val)\n        finally:\n            shutil.rmtree(""tmp"")\n\n    def test_predict_with_uncertainty(self):\n        self.model.fit_eval(self.x_train, self.y_train,\n                            validation_data=(self.x_val, self.y_val),\n                            mc=True,\n                            **self.config)\n        pred, uncertainty = self.model.predict_with_uncertainty(self.x_test, n_iter=2)\n        assert pred.shape == (self.x_test.shape[0], self.y_train.shape[1])\n        assert uncertainty.shape == pred.shape\n        assert np.any(uncertainty)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/automl/pipeline/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/automl/pipeline/test_time_sequence.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport shutil\nimport tempfile\n\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.automl.model import BaseModel\nfrom zoo.automl.pipeline.time_sequence import *\nfrom zoo.automl.regression.time_sequence_predictor import *\nimport numpy as np\nimport pandas as pd\nimport ray\nfrom pandas.util.testing import assert_frame_equal\nfrom numpy.testing import assert_array_almost_equal\n\n\ndefault_past_seq_len = 2\n\n\nclass TestTimeSequencePipeline(ZooTestCase):\n\n    def setup_method(self, method):\n        pass\n\n    def teardown_method(self, method):\n        """"""\n        Teardown any state that was previously setup with a setup_method call.\n        """"""\n        pass\n\n    def get_input_tsp(self, future_seq_len, target_col):\n        sample_num = np.random.randint(100, 200)\n        train_df = pd.DataFrame({""datetime"": pd.date_range(\'1/1/2019\',\n                                                           periods=sample_num),\n                                 target_col: np.random.randn(sample_num)})\n        test_sample_num = np.random.randint(20, 30)\n        test_df = pd.DataFrame({""datetime"": pd.date_range(\'1/1/2019\',\n                                                          periods=test_sample_num),\n                                target_col: np.random.randn(test_sample_num)})\n        tsp = TimeSequencePredictor(dt_col=""datetime"",\n                                    target_col=target_col,\n                                    future_seq_len=future_seq_len,\n                                    extra_features_col=None, )\n        return train_df, test_df, tsp, test_sample_num\n\n    # def test_evaluate_predict_future_equal_1(self):\n    #     target_col = ""values""\n    #     metrics = [""mse"", ""r2""]\n    #     future_seq_len = 1\n    #     train_df, test_df, tsp, test_sample_num = self.get_input_tsp(future_seq_len, target_col)\n    #     pipeline = tsp.fit(train_df, test_df)\n    #     mse, rs = pipeline.evaluate(test_df, metrics=metrics)\n    #     assert isinstance(mse, np.float)\n    #     assert isinstance(rs, np.float)\n    #     y_pred = pipeline.predict(test_df)\n    #     assert y_pred.shape == (test_sample_num - default_past_seq_len + 1,\n    #                             future_seq_len + 1)\n    #\n    #     y_pred_df = pipeline.predict(test_df[:-future_seq_len])\n    #     y_df = test_df[default_past_seq_len:]\n    #\n    #     mse_pred_eval, rs_pred_eval = [Evaluator.evaluate(m,\n    #                                                       y_df[target_col].values,\n    #                                                       y_pred_df[target_col].values)\n    #                                    for m in metrics]\n    #     mse_eval, rs_eval = pipeline.evaluate(test_df, metrics)\n    #     assert mse_pred_eval == mse_eval\n    #     assert rs_pred_eval == rs_eval\n\n    def test_save_restore_future_equal_1(self):\n        target_col = ""values""\n        metrics = [""mse"", ""r2""]\n        future_seq_len = 1\n        train_df, test_df, tsp, test_sample_num = self.get_input_tsp(future_seq_len, target_col)\n        pipeline = tsp.fit(train_df, test_df)\n        y_pred = pipeline.predict(test_df)\n        mse, rs = pipeline.evaluate(test_df, metrics=metrics)\n        print(""Evaluation result: Mean square error is: {}, R square is: {}."".format(mse, rs))\n\n        dirname = tempfile.mkdtemp(prefix=""saved_pipeline"")\n        try:\n            save_pipeline_file = os.path.join(dirname, ""my.ppl"")\n            pipeline.save(save_pipeline_file)\n            assert os.path.isfile(save_pipeline_file)\n            new_pipeline = load_ts_pipeline(save_pipeline_file)\n            assert new_pipeline.config is not None\n            assert isinstance(new_pipeline.feature_transformers, TimeSequenceFeatureTransformer)\n            assert isinstance(new_pipeline.model, TimeSequenceModel)\n            new_pipeline.describe()\n            new_pred = new_pipeline.predict(test_df)\n            assert_array_almost_equal(y_pred[target_col].values, new_pred[target_col].values,\n                                      decimal=2)\n        finally:\n            shutil.rmtree(dirname)\n\n        new_pipeline.fit(train_df, epoch_num=1)\n        new_mse, new_rs = new_pipeline.evaluate(test_df,\n                                                metrics=[""mse"", ""r2""])\n        print(""Evaluation result after restore and fit: ""\n              ""Mean square error is: {}, R square is: {}."".format(new_mse, new_rs))\n\n    def test_evaluate_predict_future_more_1(self):\n        target_col = ""values""\n        metrics = [""mse"", ""r2""]\n        future_seq_len = np.random.randint(2, 6)\n        train_df, test_df, tsp, test_sample_num = self.get_input_tsp(future_seq_len, target_col)\n        pipeline = tsp.fit(train_df, test_df)\n        mse, rs = pipeline.evaluate(test_df, metrics=metrics)\n        assert len(mse) == future_seq_len\n        assert len(rs) == future_seq_len\n        y_pred = pipeline.predict(test_df)\n        assert y_pred.shape == (test_sample_num - default_past_seq_len + 1,\n                                future_seq_len + 1)\n\n        y_pred_df = pipeline.predict(test_df[:-future_seq_len])\n        columns = [""{}_{}"".format(target_col, i) for i in range(future_seq_len)]\n        y_pred_value = y_pred_df[columns].values\n\n        y_df = test_df[default_past_seq_len:]\n        y_value = TimeSequenceFeatureTransformer()._roll_test(y_df[target_col], future_seq_len)\n\n        mse_pred_eval, rs_pred_eval = [Evaluator.evaluate(m, y_value, y_pred_value)\n                                       for m in metrics]\n        mse_eval, rs_eval = pipeline.evaluate(test_df, metrics)\n        assert_array_almost_equal(mse_pred_eval, mse_eval, decimal=2)\n        assert_array_almost_equal(rs_pred_eval, rs_eval, decimal=2)\n\n    # def test_save_restore_future_more_1(self):\n    #     target_col = ""values""\n    #     metrics = [""mse"", ""r2""]\n    #     future_seq_len = np.random.randint(2, 6)\n    #     train_df, test_df, tsp, test_sample_num = self.get_input_tsp(future_seq_len, target_col)\n    #     pipeline = tsp.fit(train_df, test_df)\n    #     y_pred = pipeline.predict(test_df)\n    #     mse, rs = pipeline.evaluate(test_df, metrics=metrics)\n    #     print(""Evaluation result: Mean square error is: {}, R square is: {}."".format(mse, rs))\n    #\n    #     dirname = tempfile.mkdtemp(prefix=""saved_pipeline"")\n    #     try:\n    #         save_pipeline_file = os.path.join(dirname, ""my.ppl"")\n    #         pipeline.save(save_pipeline_file)\n    #         assert os.path.isfile(save_pipeline_file)\n    #         new_pipeline = load_ts_pipeline(save_pipeline_file)\n    #\n    #         new_pred = new_pipeline.predict(test_df)\n    #         columns = [""{}_{}"".format(target_col, i) for i in range(future_seq_len)]\n    #         assert_array_almost_equal(y_pred[columns].values, new_pred[columns].values, decimal=2)\n    #\n    #     finally:\n    #         shutil.rmtree(dirname)\n    #\n    #     new_pipeline.fit(train_df, epoch_num=1)\n    #     new_mse, new_rs = new_pipeline.evaluate(test_df,\n    #                                             metrics=[""mse"", ""r2""])\n    #     print(""Evaluation result after restore and fit: ""\n    #           ""Mean square error is: {}, R square is: {}."".format(new_mse, new_rs))\n\n    def test_predict_df_list(self):\n        target_col = ""values""\n        future_seq_len = np.random.randint(2, 6)\n        train_df, test_df, tsp, test_sample_num = self.get_input_tsp(future_seq_len, target_col)\n        df_num = 2\n        train_df_list = [train_df] * df_num\n        test_df_list = [test_df] * df_num\n        pipeline = tsp.fit(train_df_list, test_df_list)\n        y_pred = pipeline.predict(test_df_list)\n        assert len(y_pred) == df_num\n        assert_frame_equal(y_pred[0], y_pred[1], check_exact=False, check_less_precise=2)\n        assert y_pred[0].shape == (test_sample_num - default_past_seq_len + 1,\n                                   future_seq_len + 1)\n\n    # def test_look_back_future_1(self):\n    #     target_col = ""values""\n    #     min_past_seq_len = np.random.randint(3, 5)\n    #     max_past_seq_len = np.random.randint(5, 8)\n    #     future_seq_len = 1\n    #     train_df, test_df, tsp, test_sample_num = self.get_input_tsp(future_seq_len, target_col)\n    #\n    #     random_pipeline = tsp.fit(train_df, test_df, recipe=RandomRecipe(\n    #         look_back=(min_past_seq_len, max_past_seq_len)))\n    #     y_pred_random = random_pipeline.predict(test_df)\n    #     assert y_pred_random.shape[0] >= test_sample_num - max_past_seq_len + 1\n    #     assert y_pred_random.shape[0] <= test_sample_num - min_past_seq_len + 1\n    #     assert y_pred_random.shape[1] == future_seq_len + 1\n    #     mse, rs = random_pipeline.evaluate(test_df, metrics=[""mse"", ""r2""])\n    #     assert isinstance(mse, np.float)\n    #     assert isinstance(rs, np.float)\n    #\n    # def test_look_back_future_more_1(self):\n    #     target_col = ""values""\n    #     min_past_seq_len = np.random.randint(3, 5)\n    #     max_past_seq_len = np.random.randint(5, 8)\n    #     future_seq_len = np.random.randint(2, 6)\n    #     train_df, test_df, tsp, test_sample_num = self.get_input_tsp(future_seq_len, target_col)\n    #\n    #     random_pipeline = tsp.fit(train_df, test_df, recipe=RandomRecipe(\n    #         look_back=(min_past_seq_len, max_past_seq_len)))\n    #     y_pred_random = random_pipeline.predict(test_df)\n    #     assert y_pred_random.shape[0] >= test_sample_num - max_past_seq_len + 1\n    #     assert y_pred_random.shape[0] <= test_sample_num - min_past_seq_len + 1\n    #     assert y_pred_random.shape[1] == future_seq_len + 1\n    #     mse, rs = random_pipeline.evaluate(test_df, metrics=[""mse"", ""r2""])\n    #     assert len(mse) == future_seq_len\n    #     assert len(rs) == future_seq_len\n\n    def test_look_back_value(self):\n        target_col = ""values""\n        future_seq_len = np.random.randint(2, 6)\n        train_df, test_df, tsp, test_sample_num = self.get_input_tsp(future_seq_len, target_col)\n        # test min_past_seq_len < 2\n        tsp.fit(train_df, test_df, recipe=RandomRecipe(look_back=(1, 2)))\n        # test max_past_seq_len < 2\n        with pytest.raises(ValueError, match=r"".*max look back value*.""):\n            tsp.fit(train_df, test_df, recipe=RandomRecipe(look_back=(0, 1)))\n        # test look_back value < 2\n        with pytest.raises(ValueError, match=r"".*look back value should not be smaller than 2*.""):\n            tsp.fit(train_df, test_df, recipe=RandomRecipe(look_back=1))\n\n        # test look back is None\n        with pytest.raises(ValueError, match=r"".*look_back should be either*.""):\n            tsp.fit(train_df, test_df, recipe=RandomRecipe(look_back=None))\n        # test look back is str\n        with pytest.raises(ValueError, match=r"".*look_back should be either*.""):\n            tsp.fit(train_df, test_df, recipe=RandomRecipe(look_back=""a""))\n        # test look back is float\n        with pytest.raises(ValueError, match=r"".*look_back should be either*.""):\n            tsp.fit(train_df, test_df, recipe=RandomRecipe(look_back=2.5))\n        # test look back range is float\n        with pytest.raises(ValueError, match=r"".*look_back should be either*.""):\n            tsp.fit(train_df, test_df, recipe=RandomRecipe(look_back=(2.5, 3)))\n\n    def test_get_default_configs(self):\n        ppl = TimeSequencePipeline(name=\'test\')\n        ppl.get_default_configs()\n\n    # def test_fit_with_fixed_configs_save(self):\n    #     target_col = ""values""\n    #     future_seq_len = np.random.randint(2, 6)\n    #     train_df, test_df, tsp, test_sample_num = self.get_input_tsp(future_seq_len, target_col)\n    #     ppl = TimeSequencePipeline(name=\'test\')\n    #     ppl.fit_with_fixed_configs(train_df, test_df, target_col=target_col,\n    #                                batch_size=64, epochs=1, future_seq_len=future_seq_len)\n    #     mse, rs = ppl.evaluate(test_df, metrics=[""mse"", ""r2""])\n    #     assert len(mse) == future_seq_len\n    #     assert len(rs) == future_seq_len\n    #     y_pred = ppl.predict(test_df)\n    #     ppl_file = ppl.save()\n    #     config_file = ppl.config_save()\n    #     assert os.path.isfile(config_file)\n    #     reload_configs = load_config(config_file)\n    #     reload_ppl = load_ts_pipeline(ppl_file)\n    #     os.remove(ppl_file)\n    #     os.remove(config_file)\n    #     os.rmdir(os.path.dirname(os.path.abspath(ppl_file)))\n    #     reload_y_pred = reload_ppl.predict(test_df)\n    #     assert reload_configs[""batch_size""] == 64\n    #     assert_frame_equal(y_pred, reload_y_pred, check_exact=False, check_less_precise=5)\n\n    # def test_predict_with_uncertainty(self):\n    #     target_col = ""values""\n    #     future_seq_len = np.random.randint(2, 6)\n    #     train_df, test_df, tsp, test_sample_num = self.get_input_tsp(future_seq_len, target_col)\n    #\n    #     # test future_seq_len = 3\n    #     pipeline = tsp.fit(train_df, mc=True, validation_df=test_df)\n    #     y_out, y_pred_uncertainty = pipeline.predict_with_uncertainty(test_df, n_iter=2)\n    #     assert y_out.shape == (test_sample_num - default_past_seq_len + 1,\n    #                            future_seq_len + 1)\n    #     assert y_pred_uncertainty.shape == (test_sample_num - default_past_seq_len + 1,\n    #                                         future_seq_len)\n    #     assert np.any(y_pred_uncertainty)\n    #\n    # def test_fit_predict_with_uncertainty(self):\n    #     # test future_seq_len = 1\n    #     self.pipeline_1 = self.tsp_1.fit(self.train_df, mc=True, validation_df=self.validation_df)\n    #     self.pipeline_1.fit(self.validation_df, mc=True, epoch_num=1)\n    #     y_out, y_pred_uncertainty = self.pipeline_1.predict_with_uncertainty(self.test_df,\n    #                                                                          n_iter=2)\n    #     assert y_out.shape == (self.test_sample_num - self.default_past_seq_len + 1,\n    #                            self.future_seq_len_1 + 1)\n    #     assert y_pred_uncertainty.shape == (self.test_sample_num - self.default_past_seq_len + 1,\n    #                                         self.future_seq_len_1)\n    #     assert np.any(y_pred_uncertainty)\n    #\n    #     # test future_seq_len = 3\n    #     self.pipeline_3 = self.tsp_3.fit(self.train_df, mc=True, validation_df=self.validation_df)\n    #     self.pipeline_3.fit(self.validation_df, mc=True, epoch_num=1)\n    #     y_out, y_pred_uncertainty = self.pipeline_3.predict_with_uncertainty(self.test_df,\n    #                                                                          n_iter=2)\n    #     assert y_out.shape == (self.test_sample_num - self.default_past_seq_len + 1,\n    #                            self.future_seq_len_3 + 1)\n    #     assert y_pred_uncertainty.shape == (self.test_sample_num - self.default_past_seq_len + 1,\n    #                                         self.future_seq_len_3)\n    #     assert np.any(y_pred_uncertainty)\n    #\n    # def test_fit_fixed_configs_predict_with_uncertainty(self):\n    #     # test future_seq_len = 1\n    #     self.pipeline_1 = self.tsp_1.fit(self.train_df, validation_df=self.validation_df)\n    #     config_file = self.pipeline_1.config_save()\n    #     assert os.path.isfile(config_file)\n    #     configs = load_config(config_file)\n    #     os.remove(config_file)\n    #     os.rmdir(os.path.dirname(os.path.abspath(config_file)))\n    #     ppl = TimeSequencePipeline(name=\'test\', config=configs)\n    #     ppl.fit_with_fixed_configs(self.train_df, self.validation_df, mc=True)\n    #     y_out, y_pred_uncertainty = ppl.predict_with_uncertainty(self.test_df, n_iter=2)\n    #     assert y_out.shape == (self.test_sample_num - self.default_past_seq_len + 1,\n    #                            self.future_seq_len_1 + 1)\n    #     assert y_pred_uncertainty.shape == (self.test_sample_num - self.default_past_seq_len + 1,\n    #                                         self.future_seq_len_1)\n    #     assert np.any(y_pred_uncertainty)\n    #\n    #     # test future_seq_len = 3\n    #     self.pipeline_3 = self.tsp_3.fit(self.train_df, validation_df=self.validation_df)\n    #     config_file = self.pipeline_3.config_save()\n    #     assert os.path.isfile(config_file)\n    #     configs = load_config(config_file)\n    #     os.remove(config_file)\n    #     os.rmdir(os.path.dirname(os.path.abspath(config_file)))\n    #     ppl = TimeSequencePipeline(name=\'test\', config=configs)\n    #     ppl.fit_with_fixed_configs(self.train_df, self.validation_df, mc=True)\n    #     y_out, y_pred_uncertainty = ppl.predict_with_uncertainty(self.test_df, n_iter=2)\n    #     assert y_out.shape == (self.test_sample_num - self.default_past_seq_len + 1,\n    #                            self.future_seq_len_3 + 1)\n    #     assert y_pred_uncertainty.shape == (self.test_sample_num - self.default_past_seq_len + 1,\n    #                                         self.future_seq_len_3)\n    #     assert np.any(y_pred_uncertainty)\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/automl/regression/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/automl/regression/test_time_sequence_predictor.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.automl.config.recipe import *\nfrom zoo.automl.model import BaseModel\nfrom zoo.automl.regression.time_sequence_predictor import *\n\n\nclass TestTimeSequencePredictor(ZooTestCase):\n\n    def setup_method(self, method):\n        pass\n\n    def teardown_method(self, method):\n        pass\n\n    def create_dataset(self):\n        sample_num = np.random.randint(100, 200)\n        train_df = pd.DataFrame({""datetime"": pd.date_range(\n            \'1/1/2019\', periods=sample_num), ""value"": np.random.randn(sample_num)})\n        val_sample_num = np.random.randint(20, 30)\n        validation_df = pd.DataFrame({""datetime"": pd.date_range(\n            \'1/1/2019\', periods=val_sample_num), ""value"": np.random.randn(val_sample_num)})\n        future_seq_len = np.random.randint(1, 6)\n        return train_df, validation_df, future_seq_len\n\n    def test_fit_SmokeRecipe(self):\n        train_df, validation_df, future_seq_len = self.create_dataset()\n        tsp = TimeSequencePredictor(dt_col=""datetime"",\n                                    target_col=""value"",\n                                    future_seq_len=future_seq_len,\n                                    extra_features_col=None, )\n        pipeline = tsp.fit(train_df, validation_df)\n        assert isinstance(pipeline, TimeSequencePipeline)\n        assert isinstance(\n            pipeline.feature_transformers,\n            TimeSequenceFeatureTransformer)\n        assert isinstance(pipeline.model, BaseModel)\n        assert pipeline.config is not None\n\n    def test_fit_LSTMGridRandomRecipe(self):\n        train_df, _, future_seq_len = self.create_dataset()\n        tsp = TimeSequencePredictor(dt_col=""datetime"",\n                                    target_col=""value"",\n                                    future_seq_len=future_seq_len,\n                                    extra_features_col=None, )\n        pipeline = tsp.fit(train_df,\n                           recipe=LSTMGridRandomRecipe(\n                               lstm_2_units=[4],\n                               batch_size=[1024],\n                               num_rand_samples=5,\n                               look_back=2,\n                               training_iteration=1,\n                               epochs=1))\n        assert isinstance(pipeline, TimeSequencePipeline)\n        assert isinstance(\n            pipeline.feature_transformers,\n            TimeSequenceFeatureTransformer)\n        assert isinstance(pipeline.model, BaseModel)\n        assert pipeline.config is not None\n        assert \'past_seq_len\' in pipeline.config\n        assert pipeline.config[""past_seq_len""] == 2\n\n    def test_fit_BayesRecipe(self):\n        train_df, _, future_seq_len = self.create_dataset()\n        tsp = TimeSequencePredictor(dt_col=""datetime"",\n                                    target_col=""value"",\n                                    future_seq_len=future_seq_len,\n                                    extra_features_col=None, )\n        pipeline = tsp.fit(\n            train_df, recipe=BayesRecipe(\n                num_samples=1,\n                training_iteration=2,\n                epochs=1,\n                look_back=(3, 5)\n            ))\n        assert isinstance(pipeline, TimeSequencePipeline)\n        assert isinstance(\n            pipeline.feature_transformers,\n            TimeSequenceFeatureTransformer)\n        assert isinstance(pipeline.model, BaseModel)\n        assert pipeline.config is not None\n        assert ""epochs"" in pipeline.config\n        assert [config_name for config_name in pipeline.config\n                if config_name.startswith(\'bayes_feature\')] == []\n        assert [config_name for config_name in pipeline.config\n                if config_name.endswith(\'float\')] == []\n        assert \'past_seq_len\' in pipeline.config\n        assert 3 <= pipeline.config[""past_seq_len""] <= 5\n\n    def test_fit_df_list(self):\n        train_df, validation_df, future_seq_len = self.create_dataset()\n        tsp = TimeSequencePredictor(dt_col=""datetime"",\n                                    target_col=""value"",\n                                    future_seq_len=future_seq_len,\n                                    extra_features_col=None, )\n        train_df_list = [train_df] * 3\n        val_df_list = [validation_df] * 3\n        pipeline = tsp.fit(train_df_list, val_df_list)\n        assert isinstance(pipeline, TimeSequencePipeline)\n        assert isinstance(\n            pipeline.feature_transformers,\n            TimeSequenceFeatureTransformer)\n        assert isinstance(pipeline.model, BaseModel)\n        assert pipeline.config is not None\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/feature/image/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/feature/image/test_image_set.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nimport cv2\n\nfrom zoo.common.nncontext import *\nfrom zoo.feature.image import *\n\n\nclass Test_Image_Set():\n\n    def setup_method(self, method):\n        """""" setup any state tied to the execution of the given method in a\n        class.  setup_method is invoked for every test method of a class.\n        """"""\n        self.sc = init_nncontext(init_spark_conf().setMaster(""local[4]"")\n                                 .setAppName(""test image set""))\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\n        self.image_path = os.path.join(resource_path, ""pascal/000025.jpg"")\n        self.grayimage_path = os.path.join(resource_path, ""gray/gray.bmp"")\n        self.image_folder = os.path.join(resource_path, ""imagenet"")\n\n    def teardown_method(self, method):\n        """""" teardown any state that was previously setup with a setup_method\n        call.\n        """"""\n        self.sc.stop()\n\n    def transformer_test(self, transformer):\n        image_set = ImageSet.read(self.image_path)\n        transformed = transformer(image_set)\n        transformed.get_image()\n\n        image_set = ImageSet.read(self.image_path, self.sc)\n        transformed = transformer(image_set)\n        images = transformed.get_image()\n        images.count()\n\n    def test_get_image(self):\n        image_set = ImageSet.read(self.image_path, resize_height=128, resize_width=128)\n        image = image_set.get_image()\n\n        image_set = ImageSet.read(self.image_path)\n        image = image_set.get_image()\n        assert image[0].shape[0] is 3\n\n        image_set = ImageSet.read(self.image_path, self.sc)\n        image = image_set.get_image().collect()\n        assert image[0].shape[0] is 3\n\n        image_set = ImageSet.read(self.grayimage_path)\n        image = image_set.get_image()\n        assert image[0].shape[0] is 1\n\n        image_set = ImageSet.read(self.grayimage_path, self.sc)\n        image = image_set.get_image().collect()\n        assert image[0].shape[0] is 1\n\n    def test_get_label(self):\n        image_set = ImageSet.read(self.image_path)\n        image_set.get_label()\n\n    def test_is_local(self):\n        image_set = ImageSet.read(self.image_path)\n        assert image_set.is_local() is True\n        image_set = ImageSet.read(self.image_path, self.sc)\n        assert image_set.is_local() is False\n\n    def test_is_distributed(self):\n        image_set = ImageSet.read(self.image_path)\n        assert image_set.is_distributed() is False\n        image_set = ImageSet.read(self.image_path, self.sc)\n        assert image_set.is_distributed() is True\n\n    def test_image_set_transform(self):\n        transformer = ImageMatToTensor()\n        image_set = ImageSet.read(self.image_path)\n        transformed = image_set.transform(transformer)\n        transformed.get_image()\n\n    def test_empty_get_predict_local(self):\n        image_set = ImageSet.read(self.image_path)\n        image_set.get_predict()\n\n    def test_empty_get_predict_distributed(self):\n        image_set = ImageSet.read(self.image_path, self.sc)\n        image_set.get_predict()\n\n    def test_local_image_set(self):\n        image = cv2.imread(self.image_path)\n        local_image_set = LocalImageSet([image])\n        print(local_image_set.get_image())\n\n    def test_image_set_random_preprocess(self):\n        transformer = ImageRandomPreprocessing(ImageResize(10, 10), 1.0)\n        image_set = ImageSet.read(self.image_path)\n        transformed = image_set.transform(transformer)\n        img = transformed.get_image()[0]\n        assert img.shape == (3, 10, 10)\n\n    def test_image_set_from_image_folder_with_sc(self):\n        image_set = ImageSet.read(self.image_folder, sc=self.sc, with_label=True)\n        label_map = image_set.label_map\n        assert len(label_map) == 4\n        imgs = image_set.get_image().collect()\n        assert len(imgs) == 11\n        labels = image_set.get_label().collect()\n        labels = [l[0] for l in labels]\n        assert len(labels) == 11\n        assert len(set(labels)) == 4\n\n    def test_image_set_from_image_folder_without_sc(self):\n        image_set = ImageSet.read(self.image_folder, with_label=True)\n        label_map = image_set.label_map\n        assert len(label_map) == 4\n        imgs = image_set.get_image()\n        assert len(imgs) == 11\n        labels = image_set.get_label()\n        labels = [l[0] for l in labels]\n        assert len(labels) == 11\n        assert len(set(labels)) == 4\n\n    def test_local_image_set_with_zero_based_label(self):\n        image_set = ImageSet.read(self.image_folder,\n                                  with_label=True, one_based_label=False)\n        label_map = image_set.label_map\n        for k in label_map:\n            assert label_map[k] < 4.0\n\n        for label in image_set.get_label():\n            assert label < 4.0\n\n    def test_distributed_image_set_with_zero_based_label(self):\n        image_set = ImageSet.read(self.image_folder, sc=self.sc,\n                                  with_label=True, one_based_label=False)\n        label_map = image_set.label_map\n        for k in label_map:\n            assert label_map[k] < 4.0\n\n        for label in image_set.get_label().collect():\n            assert label < 4.0\n\n    def test_local_image_set_with_one_based_label(self):\n        image_set = ImageSet.read(self.image_folder,\n                                  with_label=True, one_based_label=True)\n        label_map = image_set.label_map\n        for k in label_map:\n            assert label_map[k] <= 4.0 and label_map[k] > 0.0\n\n        for label in image_set.get_label():\n            assert label <= 4.0 and label > 0.0\n\n    def test_distributed_image_set_with_one_based_label(self):\n        image_set = ImageSet.read(self.image_folder, sc=self.sc,\n                                  with_label=True, one_based_label=True)\n        label_map = image_set.label_map\n        for k in label_map:\n            assert label_map[k] > 0.0 and label_map[k] <= 4.0\n\n        for label in image_set.get_label().collect():\n            assert label > 0.0 and label <= 4.0\n\n    def test_distributed_image_set_from_rdds(self):\n        image_rdd = self.sc.parallelize(np.zeros((4, 32, 32, 3)))\n        label_rdd = self.sc.parallelize(np.random.randint(0, 4, size=(4, 1)))\n        image_set = ImageSet.from_rdds(image_rdd, label_rdd)\n\n        for image in image_set.get_image().collect():\n            assert image.sum() == 0.0\n\n        for label in image_set.get_label().collect():\n            assert label >= 0.0 and label < 4.0\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/feature/image3d/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/feature/image3d/test_image3d.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nimport h5py\nfrom math import pi\nfrom zoo.common.nncontext import *\nfrom zoo.feature.common import *\nfrom zoo.feature.image3d.transformation import *\n\n\nclass Test_Image3D():\n\n    def setup_method(self):\n        """""" setup any state tied to the execution of the given method in a\n        class.  setup_method is invoked for every test method of a class.\n        """"""\n        self.sc = init_nncontext(create_spark_conf().setMaster(""local[4]"")\n                                 .setAppName(""test image set"")\n                                 .set(""spark.shuffle.reduceLocality.enabled"", ""false"")\n                                 .set(""spark.shuffle.blockTransferService"", ""nio"")\n                                 .set(""spark.scheduler.minRegisteredResourcesRatio"", ""1.0"")\n                                 .set(""spark.speculation"", ""false"")\n                                 )\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\n        image_path = os.path.join(resource_path, ""image3d/a.mat"")\n        img = h5py.File(image_path)[\'meniscus_im\']\n        sample = np.array(img)\n        self.sample = np.expand_dims(sample, 3)\n\n    def teardown_method(self, method):\n        """""" teardown any state that was previously setup with a setup_method\n        call.\n        """"""\n        self.sc.stop()\n\n    def test_crop(self):\n        start_loc = [13, 80, 125]\n        patch = [5, 40, 40]\n        crop = Crop3D(start=start_loc, patch_size=patch)\n        data_rdd = self.sc.parallelize([self.sample])\n        image_set = DistributedImageSet(image_rdd=data_rdd)\n        transformed = crop(image_set)\n        image = transformed.get_image(key=""imageTensor"").first()\n        assert image.shape == (5, 40, 40, 1)\n\n    def test_crop_random(self):\n        patch = [5, 40, 40]\n        data_rdd = self.sc.parallelize([self.sample])\n        image_set = DistributedImageSet(image_rdd=data_rdd)\n        crop = RandomCrop3D(patch[0], patch[1], patch[2])\n        transformed = image_set.transform(crop)\n        image = transformed.get_image(key=""imageTensor"").first()\n        assert image.shape == (5, 40, 40, 1)\n\n    def test_crop_centor(self):\n        patch = [5, 40, 40]\n        data_rdd = self.sc.parallelize([self.sample])\n        image_set = DistributedImageSet(image_rdd=data_rdd)\n        crop = CenterCrop3D(patch[0], patch[1], patch[2])\n        transformed = image_set.transform(crop)\n        image = transformed.get_image(key=""imageTensor"").first()\n        assert image.shape == (5, 40, 40, 1)\n\n    def test_rotation_1(self):\n        crop = CenterCrop3D(5, 40, 40)\n        data_rdd = self.sc.parallelize([self.sample])\n        image_set = DistributedImageSet(image_rdd=data_rdd)\n        cropped = image_set.transform(crop)\n        yaw = 0.0\n        pitch = 0.0\n        roll = pi / 6\n        rotate_30 = Rotate3D([yaw, pitch, roll])\n        transformed = cropped.transform(rotate_30)\n        image = transformed.get_image(key=""imageTensor"").first()\n        assert image.shape == (5, 40, 40, 1)\n\n    def test_rotation_2(self):\n        crop = CenterCrop3D(5, 40, 40)\n        data_rdd = self.sc.parallelize([self.sample])\n        image_set = DistributedImageSet(image_rdd=data_rdd)\n        cropped = image_set.transform(crop)\n        yaw = 0.0\n        pitch = 0.0\n        roll = pi / 2\n        rotate_90 = Rotate3D([yaw, pitch, roll])\n        transformed = cropped.transform(rotate_90)\n        image = transformed.get_image(key=""imageTensor"").first()\n        assert image.shape == (5, 40, 40, 1)\n\n    def test_affine_transformation(self):\n        data_rdd = self.sc.parallelize([self.sample])\n        image_set = DistributedImageSet(image_rdd=data_rdd)\n        crop = CenterCrop3D(5, 40, 40)\n        cropped = image_set.transform(crop)\n        affine = AffineTransform3D(np.random.rand(3, 3))\n        transformed = cropped.transform(affine)\n        image = transformed.get_image(key=""imageTensor"").first()\n        assert image.shape == (5, 40, 40, 1)\n\n    def test_pipeline(self):\n        data_rdd = self.sc.parallelize([self.sample])\n        image_set = DistributedImageSet(image_rdd=data_rdd)\n        yaw = 0.0\n        pitch = 0.0\n        roll = pi / 6\n        transformer = ChainedPreprocessing(\n            [CenterCrop3D(5, 40, 40), Rotate3D([yaw, pitch, roll])])\n        transformed = transformer(image_set)\n        assert transformed.is_distributed() is True\n        image = transformed.get_image(key=""imageTensor"").first()\n        assert image.shape == (5, 40, 40, 1)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/feature/text/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/feature/text/test_text_feature.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nfrom zoo.feature.text import *\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\n\ntext = ""Hello my friend, please annotate my text""\n\n\nclass TestTextFeature(ZooTestCase):\n\n    def test_text_feature_with_label(self):\n        feature = TextFeature(text, 1)\n        assert feature.get_text() == text\n        assert feature.get_label() == 1\n        assert feature.has_label()\n        assert set(feature.keys()) == {\'text\', \'label\'}\n        assert feature.get_tokens() is None\n        assert feature.get_sample() is None\n\n    def test_text_feature_without_label(self):\n        feature = TextFeature(text)\n        assert feature.get_text() == text\n        assert feature.get_label() == -1\n        assert not feature.has_label()\n        assert feature.keys() == [\'text\']\n        feature.set_label(0.)\n        assert feature.get_label() == 0\n        assert feature.has_label()\n        assert set(feature.keys()) == {\'text\', \'label\'}\n        assert feature.get_tokens() is None\n        assert feature.get_sample() is None\n\n    def test_text_feature_transformation(self):\n        feature = TextFeature(text, 0)\n        tokenizer = Tokenizer()\n        tokenized = tokenizer.transform(feature)\n        assert tokenized.get_tokens() == \\\n            [\'Hello\', \'my\', \'friend,\', \'please\', \'annotate\', \'my\', \'text\']\n        normalizer = Normalizer()\n        normalized = normalizer.transform(tokenized)\n        assert normalized.get_tokens() == \\\n            [\'hello\', \'my\', \'friend\', \'please\', \'annotate\', \'my\', \'text\']\n        word_index = {""my"": 1, ""please"": 2, ""friend"": 3}\n        indexed = WordIndexer(word_index).transform(normalized)\n        shaped = SequenceShaper(5).transform(indexed)\n        transformed = TextFeatureToSample().transform(shaped)\n        assert set(transformed.keys()) == {\'text\', \'label\', \'tokens\', \'indexedTokens\', \'sample\'}\n        sample = transformed.get_sample()\n        assert list(sample.feature.storage) == [1., 3., 2., 1., 0.]\n        assert list(sample.label.storage) == [0.]\n\n    def test_text_feature_with_uri(self):\n        feature = TextFeature(uri=""A1"")\n        assert feature.get_text() is None\n        assert feature.get_uri() == ""A1""\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/feature/text/test_text_set.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nimport shutil\n\nfrom bigdl.optim.optimizer import SGD\nfrom zoo.feature.common import ChainedPreprocessing, Relations\nfrom zoo.feature.text import *\nfrom zoo.common.nncontext import *\nfrom zoo.models.textclassification import TextClassifier\nfrom zoo.models.textmatching import KNRM\nfrom zoo.pipeline.api.keras.models import Sequential\nfrom zoo.pipeline.api.keras.layers import TimeDistributed\nfrom zoo.pipeline.api.keras.objectives import SparseCategoricalCrossEntropy\n\n\nclass TestTextSet:\n\n    def setup_method(self, method):\n        """""" setup any state tied to the execution of the given method in a\n        class.  setup_method is invoked for every test method of a class.\n        """"""\n        self.sc = init_nncontext(init_spark_conf().setMaster(""local[1]"")\n                                 .setAppName(""test text set""))\n        text1 = ""Hello my friend, please annotate my text""\n        text2 = ""hello world, this is some sentence for my test""\n        text3 = ""another text for test""\n        self.texts = [text1, text2, text3]\n        self.labels = [0., 1, 1]\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\n        self.path = os.path.join(resource_path, ""news20"")\n        self.glove_path = os.path.join(resource_path, ""glove.6B/glove.6B.50d.txt"")\n        self.qa_path = os.path.join(resource_path, ""qa"")\n\n    def teardown_method(self, method):\n        """""" teardown any state that was previously setup with a setup_method\n        call.\n        """"""\n        self.sc.stop()\n\n    def test_textset_without_label(self):\n        local_set = LocalTextSet(self.texts)\n        assert local_set.get_labels() == [-1, -1, -1]\n        distributed_set = DistributedTextSet(self.sc.parallelize(self.texts))\n        assert distributed_set.get_labels().collect() == [-1, -1, -1]\n\n    def test_textset_convertion(self):\n        local_set = LocalTextSet(self.texts, self.labels)\n        local1 = local_set.to_local()\n        distributed1 = local_set.to_distributed(self.sc)\n        assert local1.is_local()\n        assert distributed1.is_distributed()\n        assert local1.get_texts() == distributed1.get_texts().collect()\n\n        texts_rdd = self.sc.parallelize(self.texts)\n        labels_rdd = self.sc.parallelize(self.labels)\n        distributed_set = DistributedTextSet(texts_rdd, labels_rdd)\n        local2 = distributed_set.to_local()\n        distributed2 = distributed_set.to_distributed()\n        assert local2.is_local()\n        assert distributed2.is_distributed()\n        assert local2.get_texts() == distributed2.get_texts().collect()\n\n    def test_local_textset_integration(self):\n        local_set = LocalTextSet(self.texts, self.labels)\n        assert local_set.is_local()\n        assert not local_set.is_distributed()\n        assert local_set.get_texts() == self.texts\n        assert local_set.get_labels() == self.labels\n        tokenized = ChainedPreprocessing([Tokenizer(), Normalizer()])(local_set)\n        word_index = tokenized.generate_word_index_map(max_words_num=10)\n        transformed = ChainedPreprocessing([WordIndexer(word_index), SequenceShaper(10),\n                                            TextFeatureToSample()])(tokenized)\n        assert transformed.is_local()\n        word_index = transformed.get_word_index()\n        assert len(word_index) == 10\n        assert word_index[""my""] == 1\n        samples = transformed.get_samples()\n        assert len(samples) == 3\n        for sample in samples:\n            assert sample.feature.shape[0] == 10\n\n        vocab_file = create_tmp_path() + "".txt""\n        transformed.save_word_index(vocab_file)\n        local_set2 = LocalTextSet(self.texts, self.labels)\n        local_set2.load_word_index(vocab_file)\n        transformed2 = local_set2.tokenize().normalize().word2idx()\\\n            .shape_sequence(10).generate_sample()\n        samples2 = transformed2.get_samples()\n        for s1, s2 in zip(samples, samples2):\n            assert np.allclose(s1.feature.to_ndarray(), s2.feature.to_ndarray())\n        os.remove(vocab_file)\n\n        model = TextClassifier(5, self.glove_path, word_index, 10)\n        model.compile(""adagrad"", ""sparse_categorical_crossentropy"", [\'accuracy\'])\n        tmp_log_dir = create_tmp_path()\n        tmp_checkpoint_path = create_tmp_path()\n        os.mkdir(tmp_checkpoint_path)\n        model.set_tensorboard(tmp_log_dir, ""textclassification"")\n        model.set_checkpoint(tmp_checkpoint_path)\n        model.fit(transformed, batch_size=2, nb_epoch=2, validation_data=transformed)\n        acc = model.evaluate(transformed, batch_size=2)\n        res_set = model.predict(transformed, batch_per_thread=2)\n        predicts = res_set.get_predicts()\n\n        # Test for loaded model predict on TextSet\n        tmp_path = create_tmp_path() + "".bigdl""\n        model.save_model(tmp_path, over_write=True)\n        loaded_model = TextClassifier.load_model(tmp_path)\n        loaded_res_set = loaded_model.predict(transformed, batch_per_thread=2)\n        loaded_predicts = loaded_res_set.get_predicts()\n        assert len(predicts) == len(loaded_predicts)\n\n        for i in range(0, len(predicts)):  # (uri, prediction)\n            assert not predicts[i][0]\n            assert not loaded_predicts[i][0]  # uri is not recorded and thus None\n            assert len(predicts[i][1]) == 1\n            assert len(loaded_predicts[i][1]) == 1\n            assert predicts[i][1][0].shape == (5, )\n            assert np.allclose(predicts[i][1][0], loaded_predicts[i][1][0])\n        shutil.rmtree(tmp_log_dir)\n        shutil.rmtree(tmp_checkpoint_path)\n        os.remove(tmp_path)\n\n    def test_distributed_textset_integration(self):\n        texts_rdd = self.sc.parallelize(self.texts)\n        labels_rdd = self.sc.parallelize(self.labels)\n        distributed_set = DistributedTextSet(texts_rdd, labels_rdd)\n        assert distributed_set.is_distributed()\n        assert not distributed_set.is_local()\n        assert distributed_set.get_texts().collect() == self.texts\n        assert distributed_set.get_labels().collect() == self.labels\n\n        sets = distributed_set.random_split([0.5, 0.5])\n        train_texts = sets[0].get_texts().collect()\n        test_texts = sets[1].get_texts().collect()\n        assert set(train_texts + test_texts) == set(self.texts)\n\n        tokenized = Tokenizer()(distributed_set)\n        transformed = tokenized.normalize().word2idx().shape_sequence(5).generate_sample()\n        word_index = transformed.get_word_index()\n        assert len(word_index) == 14\n        samples = transformed.get_samples().collect()\n        assert len(samples) == 3\n        for sample in samples:\n            assert sample.feature.shape[0] == 5\n\n        vocab_file = create_tmp_path() + "".txt""\n        transformed.save_word_index(vocab_file)\n        distributed_set2 = DistributedTextSet(texts_rdd, labels_rdd)\n        distributed_set2.load_word_index(vocab_file)\n        transformed2 = distributed_set2.tokenize().normalize().word2idx()\\\n            .shape_sequence(5).generate_sample()\n        samples2 = transformed2.get_samples().collect()\n        for s1, s2 in zip(samples, samples2):\n            assert np.allclose(s1.feature.to_ndarray(), s2.feature.to_ndarray())\n        os.remove(vocab_file)\n\n        model = TextClassifier(5, self.glove_path, word_index, 5, encoder=""lstm"")\n        model.compile(SGD(), SparseCategoricalCrossEntropy())\n        model.fit(transformed, batch_size=2, nb_epoch=2)\n        res_set = model.predict(transformed, batch_per_thread=2)\n        predicts = res_set.get_predicts().collect()\n        for predict in predicts:  # (uri, prediction)\n            assert not predict[0]  # uri is not recorded and thus None\n            assert len(predict[1]) == 1\n            assert predict[1][0].shape == (5, )\n\n        tmp_path = create_tmp_path() + "".bigdl""\n        model.save_model(tmp_path, over_write=True)\n        loaded_model = TextClassifier.load_model(tmp_path)\n        loaded_res_set = loaded_model.predict(transformed, batch_per_thread=2)\n        loaded_predicts = loaded_res_set.get_predicts().collect()\n        assert len(loaded_predicts) == len(predicts)\n        os.remove(tmp_path)\n\n    def test_read_local(self):\n        local_set = TextSet.read(self.path)\n        assert local_set.is_local()\n        assert not local_set.get_word_index()  # should be None\n        assert len(local_set.get_texts()) == 3\n        assert local_set.get_labels() == [0, 0, 1]\n        assert local_set.get_samples() == [None, None, None]\n        assert local_set.get_predicts() == [(uri, None) for uri in local_set.get_uris()]\n\n    def test_read_distributed(self):\n        distributed_set = TextSet.read(self.path, self.sc, 4)\n        assert distributed_set.is_distributed()\n        assert not distributed_set.get_word_index()\n        assert len(distributed_set.get_texts().collect()) == 3\n        assert sorted(distributed_set.get_labels().collect()) == [0, 0, 1]\n        assert distributed_set.get_samples().collect() == [None, None, None]\n        assert distributed_set.get_predicts().collect() ==\\\n            [(uri, None) for uri in distributed_set.get_uris().collect()]\n\n    def test_read_csv_parquet(self):\n        text_set = TextSet.read_csv(self.qa_path + ""/question_corpus.csv"", self.sc)\n        text_set2 = TextSet.read_csv(self.qa_path + ""/question_corpus.csv"")\n        text_set3 = TextSet.read_parquet(self.qa_path + ""/question_corpus.parquet"", self.sc)\n        assert text_set.is_distributed()\n        assert text_set2.is_local()\n        assert text_set3.is_distributed()\n\n    def test_qaranker_distributed_integration(self):\n        relations = Relations.read(self.qa_path+""/relations.txt"", self.sc)\n        assert relations.count() == 4\n        text_set = TextSet.read_csv(self.qa_path+""/question_corpus.csv"", self.sc)\n        assert text_set.get_uris().collect() == [""Q1"", ""Q2""]\n        transformed = text_set.tokenize().normalize().word2idx().shape_sequence(5)\n        relation_pairs = TextSet.from_relation_pairs(relations, transformed, transformed)\n        pair_samples = relation_pairs.get_samples().collect()\n        assert len(pair_samples) == 2\n        for sample in pair_samples:\n            assert list(sample.feature.shape) == [2, 10]\n            assert np.allclose(sample.label.to_ndarray(), np.array([[1.0], [0.0]]))\n        relation_lists = TextSet.from_relation_lists(relations, transformed, transformed)\n        relation_samples = relation_lists.get_samples().collect()\n        assert len(relation_samples) == 2\n        for sample in relation_samples:\n            assert list(sample.feature.shape) == [2, 10]\n            assert list(sample.label.shape) == [2, 1]\n        knrm = KNRM(5, 5, self.glove_path, word_index=transformed.get_word_index())\n        model = Sequential().add(TimeDistributed(knrm, input_shape=(2, 10)))\n        model.compile(""sgd"", ""rank_hinge"")\n        model.fit(relation_pairs, batch_size=2, nb_epoch=2)\n        print(knrm.evaluate_ndcg(relation_lists, 3))\n        print(knrm.evaluate_map(relation_lists))\n\n    def test_qaranker_local_integration(self):\n        relations = Relations.read(self.qa_path+""/relations.txt"")\n        assert len(relations) == 4\n        text_set = TextSet.read_csv(self.qa_path+""/question_corpus.csv"")\n        assert text_set.get_uris() == [""Q1"", ""Q2""]\n        transformed = text_set.tokenize().normalize().word2idx().shape_sequence(5)\n        relation_pairs = TextSet.from_relation_pairs(relations, transformed, transformed)\n        pair_samples = relation_pairs.get_samples()\n        assert len(pair_samples) == 2\n        for sample in pair_samples:\n            assert list(sample.feature.shape) == [2, 10]\n            assert np.allclose(sample.label.to_ndarray(), np.array([[1.0], [0.0]]))\n        relation_lists = TextSet.from_relation_lists(relations, transformed, transformed)\n        relation_samples = relation_lists.get_samples()\n        assert len(relation_samples) == 2\n        for sample in relation_samples:\n            assert list(sample.feature.shape) == [2, 10]\n            assert list(sample.label.shape) == [2, 1]\n        knrm = KNRM(5, 5, self.glove_path, word_index=transformed.get_word_index())\n        model = Sequential().add(TimeDistributed(knrm, input_shape=(2, 10)))\n        model.compile(""sgd"", ""rank_hinge"")\n        model.fit(relation_pairs, batch_size=2, nb_epoch=2)\n        print(knrm.evaluate_ndcg(relation_lists, 3))\n        print(knrm.evaluate_map(relation_lists))\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/models/anomalydetection/test_anomalydetector.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nimport numpy as np\nfrom zoo.models.anomalydetection import AnomalyDetector\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestAnomalyDetector(ZooTestCase):\n\n    def test_forward_backward(self):\n        model = AnomalyDetector(feature_shape=(10, 3), hidden_layers=[8, 32, 15],\n                                dropouts=[0.2, 0.2, 0.2])\n        model.summary()\n        input_data = np.random.rand(100, 10, 3)\n        self.assert_forward_backward(model, input_data)\n        model.set_evaluate_status()\n        # Forward twice will get the same output\n        output1 = model.forward(input_data)\n        output2 = model.forward(input_data)\n        assert np.allclose(output1, output2)\n\n    def test_save_load(self):\n        model = AnomalyDetector(feature_shape=(10, 3), hidden_layers=[8, 32, 15],\n                                dropouts=[0.2, 0.2, 0.2])\n        input_data = np.random.rand(100, 10, 3)\n        self.assert_zoo_model_save_load(model, input_data)\n\n    def test_compile_fit(self):\n        model = AnomalyDetector(feature_shape=(5, 3), hidden_layers=[8],\n                                dropouts=[0.2])\n        input_data = [[float(i), float(i + 1), float(i + 2)] for i in range(20)]\n        rdd = self.sc.parallelize(input_data)\n        unrolled = AnomalyDetector.unroll(rdd, 5, 1)\n        [train, validation] = AnomalyDetector.train_test_split(unrolled, 10)\n        model.compile(loss=\'mse\', optimizer=\'rmsprop\', metrics=[\'mae\'])\n        model.fit(train, batch_size=4, nb_epoch=1, validation_data=validation)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/models/recommendation/test_neuralcf.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nimport random\n\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.models.recommendation import UserItemFeature\nfrom zoo.models.recommendation import NeuralCF\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestNeuralCF(ZooTestCase):\n\n    def test_forward_backward_without_mf(self):\n        model = NeuralCF(30, 30, 2, include_mf=False)\n        input_data = np.random.randint(1, 30,  size=(10, 2))\n        self.assert_forward_backward(model, input_data)\n\n    def test_forward_backward_with_mf(self):\n        model = NeuralCF(10, 10, 5, 5, 5)\n        input_data = np.random.randint(1, 10, size=(3, 2))\n        self.assert_forward_backward(model, input_data)\n\n    def test_save_load(self):\n        model = NeuralCF(10000, 2000, 10)\n        input_data = np.random.randint(100, 2000, size=(300, 2))\n        self.assert_zoo_model_save_load(model, input_data)\n\n    def test_predict_recommend(self):\n        def gen_rand_user_item_feature(user_num, item_num, class_num):\n            user_id = random.randint(1, user_num)\n            item_id = random.randint(1, item_num)\n            rating = random.randint(1, class_num)\n            sample = Sample.from_ndarray(np.array([user_id, item_id]), np.array([rating]))\n            return UserItemFeature(user_id, item_id, sample)\n        model = NeuralCF(200, 80, 5)\n        data = self.sc.parallelize(range(0, 50))\\\n            .map(lambda i: gen_rand_user_item_feature(200, 80, 5))\n        predictions = model.predict_user_item_pair(data).collect()\n        print(predictions[0])\n        recommended_items = model.recommend_for_user(data, max_items=3).collect()\n        print(recommended_items[0])\n        recommended_users = model.recommend_for_item(data, max_users=4).collect()\n        print(recommended_users[0])\n\n    def test_compile_fit(self):\n        def gen_rand_user_item_feature(user_num, item_num, class_num):\n            user_id = random.randint(1, user_num)\n            item_id = random.randint(1, item_num)\n            rating = random.randint(1, class_num)\n            sample = Sample.from_ndarray(np.array([user_id, item_id]), np.array([rating]))\n            return UserItemFeature(user_id, item_id, sample)\n        model = NeuralCF(200, 80, 5)\n        model.summary()\n        data = self.sc.parallelize(range(0, 50)) \\\n            .map(lambda i: gen_rand_user_item_feature(200, 80, 5)) \\\n            .map(lambda pair: pair.sample)\n        model.compile(optimizer=""adam"",\n                      loss=SparseCategoricalCrossEntropy(zero_based_label=False),\n                      metrics=[\'accuracy\'])\n        tmp_log_dir = create_tmp_path()\n        model.set_tensorboard(tmp_log_dir, ""training_test"")\n        model.fit(data, nb_epoch=1, batch_size=32, validation_data=data)\n        train_loss = model.get_train_summary(""Loss"")\n        val_loss = model.get_validation_summary(""Loss"")\n        print(np.array(train_loss))\n        print(np.array(val_loss))\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/models/recommendation/test_recommender_utils.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nfrom zoo.models.recommendation.utils import *\nfrom zoo.common.nncontext import *\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestRecommenderUtils:\n\n    def test_get_boundaries(self):\n        index = get_boundaries(42, [18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n        assert index == 5\n\n    def test_categorical_from_vocab_list(self):\n        MARITAL_STATUS_VOCAB = [""Married-civ-spouse"", ""Divorced"", ""Married-spouse-absent"",\n                                ""Never-married"", ""Separated"", ""Married-AF-spouse"", ""Widowed""]\n        index = categorical_from_vocab_list(""Never-married"", MARITAL_STATUS_VOCAB)\n        assert index == 3\n\n    def test_hash_bucket(self):\n        np.random.seed(1337)\n        res = hash_bucket(""Prof-specialty"", 1000)\n        assert res < 1000\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/models/recommendation/test_sessionrecommender.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nimport random\n\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.models.recommendation import SessionRecommender\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestSessionRecommender(ZooTestCase):\n    def test_forward_backward_without_history(self):\n        model = SessionRecommender(30, 5, [10, 5], 2)\n        input_data = np.random.randint(1, 30, size=(10, 2))\n        self.assert_forward_backward(model, input_data)\n\n    def test_forward_backward_with_history(self):\n        model = SessionRecommender(30, 5, [10, 5], 2, True, [6, 3], 5)\n        input_data = [np.random.randint(1, 30, size=(10, 2)),\n                      np.random.randint(1, 30, size=(10, 5))]\n        self.assert_forward_backward(model, input_data)\n\n    def test_save_load(self):\n        model = SessionRecommender(30, 5, [10, 5], 2, True, [6, 3], 5)\n        input_data = [np.random.randint(1, 30, size=(10, 2)),\n                      np.random.randint(1, 30, size=(10, 5))]\n        self.assert_zoo_model_save_load(model, input_data)\n\n    def test_compile_fit(self):\n        model = SessionRecommender(30, 5, [10, 5], 2, True, [6, 3], 5)\n        input_data = [[np.random.randint(1, 30, size=(2)),\n                       np.random.randint(1, 30, size=(5)),\n                       np.random.randint(1, 30)] for i in range(100)]\n        samples = self.sc.parallelize(input_data)\\\n            .map(lambda x: Sample.from_ndarray((x[0], x[1]), np.array(x[2])))\n        train, test = samples.randomSplit([0.8, 0.2], seed=1)\n        model.compile(loss=\'sparse_categorical_crossentropy\',\n                      optimizer=\'rmsprop\',\n                      metrics=[\'top5Accuracy\'])\n        model.fit(train, batch_size=4, nb_epoch=1, validation_data=test)\n\n    def test_recommed_predict(self):\n        model = SessionRecommender(30, 5, [10, 5], 2, True, [6, 3], 5)\n        input_data = [[np.random.randint(1, 30, size=(2)),\n                       np.random.randint(1, 30, size=(5)),\n                       np.random.randint(1, 30)] for i in range(100)]\n        samples = [Sample.from_ndarray((input_data[i][0], input_data[i][1]),\n                                       np.array(input_data[i][2])) for i in range(100)]\n        rdd = self.sc.parallelize(samples)\n        results1 = model.predict(rdd).collect()\n        print(results1[0])\n\n        recommendations1 = model.recommend_for_session(rdd, 3, zero_based_label=False).collect()\n        print(recommendations1[0])\n\n        recommendations2 = model.recommend_for_session(samples, 3, zero_based_label=False)\n        print(recommendations2[0])\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/models/recommendation/test_wideanddeep.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nfrom pyspark.sql.functions import col, udf\nfrom zoo.models.recommendation import *\nfrom zoo.models.recommendation.utils import *\nfrom zoo.common.nncontext import *\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestWideAndDeep(ZooTestCase):\n    def setup_method(self, method):\n        sparkConf = init_spark_conf().setMaster(""local[4]"") \\\n            .setAppName(""test wide and deep"")\n        self.sc = init_nncontext(sparkConf)\n        self.sqlContext = SQLContext(self.sc)\n        data_path = os.path.join(os.path.split(__file__)[0], ""../../resources/recommender"")\n        categorical_gender_udf = udf(lambda gender:\n                                     categorical_from_vocab_list(gender, [""F"", ""M""], start=1))\n        bucket_udf = udf(lambda feature1, feature2:\n                         hash_bucket(str(feature1) + ""_"" + str(feature2), bucket_size=100))\n        self.data_in = self.sqlContext.read.parquet(data_path) \\\n            .withColumn(""gender"", categorical_gender_udf(col(""gender"")).cast(""int"")) \\\n            .withColumn(""occupation-gender"",\n                        bucket_udf(col(""occupation""), col(""gender"")).cast(""int""))\n        self.column_info = ColumnFeatureInfo(\n            wide_base_cols=[""occupation"", ""gender""],\n            wide_base_dims=[21, 3],\n            wide_cross_cols=[""occupation-gender""],\n            wide_cross_dims=[100],\n            indicator_cols=[""occupation"", ""gender""],\n            indicator_dims=[21, 3],\n            embed_cols=[""userId"", ""itemId""],\n            embed_in_dims=[100, 100],\n            embed_out_dims=[20, 20],\n            continuous_cols=[""age""])\n\n    def test_wide_forward_backward(self):\n        input = JTensor.sparse(np.array([1, 3, 5, 2, 4, 6]),\n                               np.array([[0, 0, 0, 0, 0, 0], [1, 2, 5, 8, 90, 100]]),\n                               np.array([2, 124]))\n        model = WideAndDeep(5, self.column_info, ""wide"")\n        output = model.forward(input)\n\n    def test_deep_indicator_forward_backward(self):\n        column_info = ColumnFeatureInfo(\n            indicator_cols=[""occupation"", ""gender""],\n            indicator_dims=[21, 3])\n        model = WideAndDeep(5, column_info, ""deep"")\n        input = np.random.randint(2, size=(2, 24))\n        self.assert_forward_backward(model, input)\n\n    def test_deep_embedding_forward_backward(self):\n        column_info = ColumnFeatureInfo(\n            embed_cols=[""userId"", ""itemId""],\n            embed_in_dims=[100, 100],\n            embed_out_dims=[20, 20])\n        model = WideAndDeep(5, column_info, ""deep"")\n        model.summary()\n        input = np.random.randint(1, 100, size=(10, 2))\n        self.assert_forward_backward(model, input)\n\n    def test_deep_continous_forward_backward(self):\n        column_info = ColumnFeatureInfo(\n            continuous_cols=[""age"", ""whatever""])\n        model = WideAndDeep(5, column_info, ""deep"")\n        model.summary()\n        input = np.random.randint(1, 100, size=(100, 2))\n        self.assert_forward_backward(model, input)\n\n    def test_save_load(self):\n        column_info = ColumnFeatureInfo(\n            indicator_cols=[""occupation"", ""gender""],\n            indicator_dims=[21, 3])\n        model = WideAndDeep(5, column_info, ""deep"")\n        input_data = np.random.randint(2, size=(2, 24))\n        self.assert_zoo_model_save_load(model, input_data)\n\n    def test_predict_recommend(self):\n        column_info = self.column_info\n        model = WideAndDeep(5, column_info, ""wide_n_deep"")\n        data = self.data_in.rdd.map(lambda row: to_user_item_feature(row, column_info))\n        predictions = model.predict_user_item_pair(data)\n        print(predictions.take(1)[0])\n        recommended_items = model.recommend_for_user(data, max_items=3)\n        print(recommended_items.take(1)[0])\n        recommended_users = model.recommend_for_item(data, max_users=4)\n        print(recommended_users.take(1)[0])\n\n    def test_negative_sample(self):\n        negative_df = get_negative_samples(self.data_in)\n\n    def test_compile_fit(self):\n        column_info = self.column_info\n        model = WideAndDeep(5, column_info, ""wide_n_deep"")\n        data = self.data_in.rdd.map(lambda row: to_user_item_feature(row, column_info)) \\\n            .map(lambda user_item_feature: user_item_feature.sample)\n        model.compile(loss=SparseCategoricalCrossEntropy(zero_based_label=False),\n                      optimizer=""adam"",\n                      metrics=[""mae""])\n        model.fit(data, nb_epoch=1)\n\n    def test_deep_merge(self):\n        def get_input(column_info):\n            input_ind = Input(shape=(sum(column_info.indicator_dims),))\n            input_emb = Input(shape=(len(column_info.embed_in_dims),))\n            input_con = Input(shape=(len(column_info.continuous_cols),))\n            return(input_ind, input_emb, input_con)\n\n        column_info1 = self.column_info\n        model1 = WideAndDeep(5, column_info1, ""deep"")\n        (input_ind1, input_emb1, input_con1) = get_input(column_info1)\n        (input1, merged_list1) = model1._deep_merge(input_ind1, input_emb1, input_con1)\n        assert len(input1) == 3\n        assert len(merged_list1) == 4\n\n        column_info2 = ColumnFeatureInfo(\n            indicator_cols=[""occupation"", ""gender""],\n            indicator_dims=[21, 3],\n            embed_cols=[""userId"", ""itemId""],\n            embed_in_dims=[100, 100],\n            embed_out_dims=[20, 20])\n        model2 = WideAndDeep(5, column_info2, ""deep"")\n        (input_ind2, input_emb2, input_con2) = get_input(column_info2)\n        (input2, merged_list2) = model2._deep_merge(input_ind2, input_emb2, input_con2)\n        assert len(input2) == 2\n        assert len(merged_list2) == 3\n\n        column_info3 = ColumnFeatureInfo(\n            indicator_cols=[""occupation"", ""gender""],\n            indicator_dims=[21, 3],\n            continuous_cols=[""age""])\n        model3 = WideAndDeep(5, column_info3, ""deep"")\n        (input_ind3, input_emb3, input_con3) = get_input(column_info3)\n        (input3, merged_list3) = model3._deep_merge(input_ind3, input_emb3, input_con3)\n        assert len(input3) == 2\n        assert len(merged_list3) == 2\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/models/seq2seq/test_seq2seq.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nimport numpy as np\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.models.seq2seq import *\n\n\nclass TestSeq2seq(ZooTestCase):\n\n    def test_forward_backward(self):\n        input_data = [np.random.randint(20, size=(1, 2, 4)),\n                      np.random.randint(20, size=(1, 2, 4))]\n        encoder = RNNEncoder.initialize(""LSTM"", 1, 4)\n        decoder = RNNDecoder.initialize(""LSTM"", 1, 4)\n        bridge = Bridge.initialize(""dense"", 4)\n        model = Seq2seq(encoder, decoder, [2, 4], [2, 4], bridge)\n        self.assert_forward_backward(model, input_data)\n        sent1 = np.random.randint(20, size=(1, 2, 4))\n        sent2 = np.random.randint(20, size=(1, 4))\n        result = model.infer(sent1, sent2, 3)\n\n    def test_save_load(self):\n        input_data = [np.random.randint(20, size=(1, 2, 4)),\n                      np.random.randint(20, size=(1, 2, 4))]\n        encoder = RNNEncoder.initialize(""LSTM"", 1, 4)\n        decoder = RNNDecoder.initialize(""LSTM"", 1, 4)\n        bridge = Bridge.initialize(""dense"", 4)\n        model = Seq2seq(encoder, decoder, [2, 4], [2, 4], bridge)\n        self.assert_zoo_model_save_load(model, input_data)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/models/textclassification/test_textclassifier.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nimport os\n\nimport numpy as np\nfrom zoo.models.textclassification import TextClassifier\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\n\nnp.random.seed(1337)  # for reproducibility\n\nresource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\nglove_path = os.path.join(resource_path, ""glove.6B/glove.6B.50d.txt"")\n\n\nclass TestTextClassifier(ZooTestCase):\n\n    def test_forward_backward(self):\n        model = TextClassifier(10, glove_path)\n        model.summary()\n        input_data = np.random.randint(20, size=(4, 500))\n        self.assert_forward_backward(model, input_data)\n        model.set_evaluate_status()\n        # Forward twice will get the same output\n        output1 = model.forward(input_data)\n        output2 = model.forward(input_data)\n        assert np.allclose(output1, output2)\n\n    def test_save_load(self):\n        model = TextClassifier(20, glove_path, sequence_length=100)\n        input_data = np.random.randint(20, size=(3, 100))\n        self.assert_zoo_model_save_load(model, input_data)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/models/textmatching/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/models/textmatching/test_knrm.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport pytest\n\nfrom keras.layers import *\nfrom keras.models import Model\nfrom zoo.models.textmatching import KNRM\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\n\nnp.random.seed(1337)  # for reproducibility\n\nresource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\nglove_path = os.path.join(resource_path, ""glove.6B/glove.6B.50d.txt"")\n\n\nclass TestKNRM(ZooTestCase):\n\n    # Model definition from MatchZoo rewritten in Keras 1.2.2\n    @staticmethod\n    def keras_knrm(text1_length, text2_length, vocab_size, embed_size,\n                   kernel_num=21, sigma=0.1, exact_sigma=0.001):\n        def kernel_layer(mu, sigma):\n            def kernel(x):\n                return K.tf.exp(-0.5 * (x - mu) * (x - mu) / sigma / sigma)\n            return Activation(kernel)\n\n        query = Input(name=\'query\', shape=(text1_length, ))\n        doc = Input(name=\'doc\', shape=(text2_length, ))\n        embedding = Embedding(vocab_size, embed_size, name=""embedding"")\n        q_embed = embedding(query)\n        d_embed = embedding(doc)\n        mm = merge([q_embed, d_embed], mode=""dot"", dot_axes=[2, 2])\n\n        KM = []\n        for i in range(kernel_num):\n            mu = 1. / (kernel_num - 1) + (2. * i) / (kernel_num - 1) - 1.0\n            sigma = sigma\n            if mu > 1.0:\n                sigma = exact_sigma\n                mu = 1.0\n            mm_exp = kernel_layer(mu, sigma)(mm)\n            mm_doc_sum = Lambda(lambda x: K.tf.reduce_sum(x, 2))(mm_exp)\n            mm_log = Activation(K.tf.log1p)(mm_doc_sum)\n            mm_sum = Lambda(lambda x: K.tf.reduce_sum(x, 1))(mm_log)\n            KM.append(mm_sum)\n\n        Phi = Lambda(lambda x: K.tf.stack(x, 1))(KM)\n        out_ = Dense(1, init=""uniform"", activation=""sigmoid"", name=""dense"")(Phi)\n\n        model = Model([query, doc], out_)\n        return model\n\n    def test_with_keras(self):\n        kmodel = self.keras_knrm(5, 10, 22, 50)\n        input_data = np.random.randint(20, size=(4, 15))\n        koutput = kmodel.predict([input_data[:, :5], input_data[:, 5:]])\n        kweights = kmodel.get_weights()\n        bweights = [kweights[0], np.transpose(kweights[1]), kweights[2]]\n        model = KNRM(5, 10, glove_path, target_mode=""classification"")\n        model.set_weights(bweights)\n        output = model.forward(input_data)\n        self.assert_allclose(output, koutput)\n\n    def test_forward_backward(self):\n        model = KNRM(15, 60, glove_path, word_index={""is"": 1, ""to"": 2, ""the"": 3, ""for"": 4},\n                     kernel_num=10, sigma=0.15, exact_sigma=1e-4)\n        input_data = np.random.randint(5, size=(1, 75))\n        self.assert_forward_backward(model, input_data)\n\n    def test_save_load(self):\n        model = KNRM(5, 10, glove_path)\n        input_data = np.random.randint(20, size=(3, 15))\n        self.assert_zoo_model_save_load(model, input_data)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/orca/data/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/orca/data/conftest.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os.path\n\nimport pytest\n\nsc = None\nray_ctx = None\n\n\n@pytest.fixture(autouse=True, scope=\'package\')\ndef orca_data_fixture():\n    from zoo import init_spark_on_local\n    from zoo.ray import RayContext\n    global ray_ctx\n    sc = init_spark_on_local(cores=4, spark_log_level=""INFO"")\n    access_key_id = os.getenv(""AWS_ACCESS_KEY_ID"")\n    secret_access_key = os.getenv(""AWS_SECRET_ACCESS_KEY"")\n    if access_key_id is not None and secret_access_key is not None:\n        ray_ctx = RayContext(sc=sc,\n                             object_store_memory=""1g"",\n                             env={""AWS_ACCESS_KEY_ID"": access_key_id,\n                                  ""AWS_SECRET_ACCESS_KEY"": secret_access_key}\n                             )\n    else:\n        ray_ctx = RayContext(sc=sc, object_store_memory=""1g"")\n    ray_ctx.init()\n    yield\n    ray_ctx.stop()\n    sc.stop()\n\n\n# @pytest.fixture()\n# def setUpModule():\n#     sc = init_spark_on_local(cores=4, spark_log_level=""INFO"")\n#     ray_ctx = RayContext(sc=sc)\n#     ray_ctx.init()\n#\n#\n# def tearDownModule():\n#     ray_ctx.stop()\n#     sc.stop()\n\ndef get_ray_ctx():\n    return ray_ctx\n'"
pyzoo/test/zoo/orca/data/test_file.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os.path\nimport shutil\nimport tempfile\n\nfrom zoo.orca.data.file import open_image, open_text, load_numpy, exists, makedirs\n\n\nclass TestFile:\n    def setup_method(self, method):\n        self.resource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\n\n    def test_open_local_text(self):\n        file_path = os.path.join(self.resource_path, ""qa/relations.txt"")\n        lines = open_text(file_path)\n        assert lines == [""Q1,Q1,1"", ""Q1,Q2,0"", ""Q2,Q1,0"", ""Q2,Q2,1""]\n\n    def test_open_s3_text(self):\n        access_key_id = os.getenv(""AWS_ACCESS_KEY_ID"")\n        secret_access_key = os.getenv(""AWS_SECRET_ACCESS_KEY"")\n        if access_key_id and secret_access_key:\n            file_path = ""s3://analytics-zoo-data/hyperseg/trainingData/train_tiled.txt""\n            lines = open_text(file_path)\n            assert lines[0] == ""CONTENTAI_000001""\n\n    def test_open_local_image(self):\n        file_path = os.path.join(self.resource_path, ""cat_dog/cats/cat.7000.jpg"")\n        image = open_image(file_path)\n\n    def test_open_s3_image(self):\n        access_key_id = os.getenv(""AWS_ACCESS_KEY_ID"")\n        secret_access_key = os.getenv(""AWS_SECRET_ACCESS_KEY"")\n        if access_key_id and secret_access_key:\n            file_path = ""s3://analytics-zoo-data/dogs-vs-cats/samples/cat.7000.jpg""\n            image = open_image(file_path)\n\n    def test_load_local_numpy(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/random.npy"")\n        res = load_numpy(file_path)\n        assert res.shape == (2, 5)\n\n    def test_load_s3_numpy(self):\n        access_key_id = os.getenv(""AWS_ACCESS_KEY_ID"")\n        secret_access_key = os.getenv(""AWS_SECRET_ACCESS_KEY"")\n        if access_key_id and secret_access_key:\n            file_path = ""s3://analytics-zoo-data/hyperseg/VGGcompression/core1.npy""\n            res = load_numpy(file_path)\n            assert res.shape == (32, 64, 3, 3)\n\n    def test_exists_local(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/random.npy"")\n        assert exists(file_path)\n        file_path = os.path.join(self.resource_path, ""orca/data/abc.npy"")\n        assert not exists(file_path)\n\n    def test_exists_s3(self):\n        access_key_id = os.getenv(""AWS_ACCESS_KEY_ID"")\n        secret_access_key = os.getenv(""AWS_SECRET_ACCESS_KEY"")\n        if access_key_id and secret_access_key:\n            file_path = ""s3://analytics-zoo-data/nyc_taxi.csv""\n            assert exists(file_path)\n            file_path = ""s3://analytics-zoo-data/abc.csv""\n            assert not exists(file_path)\n\n    def test_mkdirs_local(self):\n        temp = tempfile.mkdtemp()\n        path = os.path.join(temp, ""dir1"")\n        makedirs(path)\n        assert exists(path)\n        path = os.path.join(temp, ""dir2/dir3"")\n        makedirs(path)\n        assert exists(path)\n        shutil.rmtree(temp)\n\n    def test_mkdirs_s3(self):\n        access_key_id = os.getenv(""AWS_ACCESS_KEY_ID"")\n        secret_access_key = os.getenv(""AWS_SECRET_ACCESS_KEY"")\n        if access_key_id and secret_access_key:\n            file_path = ""s3://analytics-zoo-data/temp/abc/""\n            makedirs(file_path)\n            assert exists(file_path)\n            import boto3\n            s3_client = boto3.Session(\n                aws_access_key_id=access_key_id,\n                aws_secret_access_key=secret_access_key).client(\'s3\', verify=False)\n            path_parts = file_path.split(""://"")[1].split(\'/\')\n            bucket = path_parts.pop(0)\n            key = ""/"".join(path_parts)\n            s3_client.delete_object(Bucket=bucket, Key=key)\n'"
pyzoo/test/zoo/orca/data/test_ray_pandas.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os.path\n\nimport pytest\nimport ray\n\nimport zoo.orca.data.pandas\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom test.zoo.orca.data.conftest import get_ray_ctx\n\n\nclass TestRayXShards(ZooTestCase):\n    def setup_method(self, method):\n        self.resource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\n        self.ray_ctx = get_ray_ctx()\n\n    def teardown_method(self, method):\n        """""" teardown any state that was previously setup with a setup_method\n        call.\n        """"""\n        pass\n\n    def test_read_local_csv(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/csv"")\n        data_shard = zoo.orca.data.pandas.read_csv(file_path, self.ray_ctx)\n        data = data_shard.collect()\n        assert len(data) == 2, ""number of shard should be 2""\n        df = data[0]\n        assert ""location"" in df.columns, ""location is not in columns""\n\n    def test_read_local_json(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/json"")\n        data_shard = zoo.orca.data.pandas.read_json(file_path, self.ray_ctx, orient=\'columns\',\n                                                    lines=True)\n        data = data_shard.collect()\n        assert len(data) == 2, ""number of shard should be 2""\n        df = data[0]\n        assert ""value"" in df.columns, ""value is not in columns""\n\n    def test_read_s3(self):\n        access_key_id = os.getenv(""AWS_ACCESS_KEY_ID"")\n        secret_access_key = os.getenv(""AWS_SECRET_ACCESS_KEY"")\n        if access_key_id and secret_access_key:\n            file_path = ""s3://analytics-zoo-data/nyc_taxi.csv""\n            data_shard = zoo.orca.data.pandas.read_csv(file_path, self.ray_ctx)\n            data = data_shard.collect()\n            df = data[0]\n            assert ""value"" in df.columns, ""value is not in columns""\n\n    def test_repartition(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/json"")\n        data_shard = zoo.orca.data.pandas.read_json(file_path, self.ray_ctx)\n        partitions1 = data_shard.get_partitions()\n        assert len(partitions1) == 2, ""number of partition should be 2""\n        data_shard.repartition(1)\n        partitions2 = data_shard.get_partitions()\n        assert len(partitions2) == 1, ""number of partition should be 1""\n        partition_data = ray.get(partitions2[0].get_data())\n        assert len(partition_data) == 2, ""partition 0 should have 2 objects""\n\n    def test_transform_shard(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/json"")\n        data_shard = zoo.orca.data.pandas.read_json(file_path, self.ray_ctx, orient=\'columns\',\n                                                    lines=True)\n        data = data_shard.collect()\n        assert data[0][""value""].values[0] > 0, ""value should be positive""\n\n        def negative(df, column_name):\n            df[column_name] = df[column_name] * (-1)\n            return df\n\n        data_shard.transform_shard(negative, ""value"")\n        data2 = data_shard.collect()\n        assert data2[0][""value""].values[0] < 0, ""value should be negative""\n\n    def test_read_csv_with_args(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/csv"")\n        data_shard = zoo.orca.data.pandas.read_csv(file_path, self.ray_ctx, sep=\',\', header=0)\n        data = data_shard.collect()\n        assert len(data) == 2, ""number of shard should be 2""\n        df = data[0]\n        assert ""location"" in df.columns, ""location is not in columns""\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/orca/data/test_spark_pandas.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os.path\nimport shutil\n\nimport pytest\n\nimport zoo.orca.data\nimport zoo.orca.data.pandas\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.common.nncontext import *\n\n\nclass TestSparkXShards(ZooTestCase):\n    def setup_method(self, method):\n        self.resource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\n        sparkConf = init_spark_conf().setMaster(""local[4]"").setAppName(""testSparkXShards"")\n        self.sc = init_nncontext(sparkConf)\n\n    def teardown_method(self, method):\n        """""" teardown any state that was previously setup with a setup_method\n        call.\n        """"""\n        self.sc.stop()\n\n    def test_read_local_csv(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/csv"")\n        data_shard = zoo.orca.data.pandas.read_csv(file_path, self.sc)\n        data = data_shard.collect()\n        assert len(data) == 2, ""number of shard should be 2""\n        df = data[0]\n        assert ""location"" in df.columns, ""location is not in columns""\n\n    def test_read_local_json(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/json"")\n        data_shard = zoo.orca.data.pandas.read_json(file_path, self.sc,\n                                                    orient=\'columns\', lines=True)\n        data = data_shard.collect()\n        assert len(data) == 2, ""number of shard should be 2""\n        df = data[0]\n        assert ""value"" in df.columns, ""value is not in columns""\n\n    def test_read_s3(self):\n        access_key_id = os.getenv(""AWS_ACCESS_KEY_ID"")\n        secret_access_key = os.getenv(""AWS_SECRET_ACCESS_KEY"")\n        if access_key_id and secret_access_key:\n            file_path = ""s3://analytics-zoo-data/nyc_taxi.csv""\n            data_shard = zoo.orca.data.pandas.read_csv(file_path, self.sc)\n            data = data_shard.collect()\n            df = data[0]\n            assert ""value"" in df.columns, ""value is not in columns""\n\n    def test_repartition(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/json"")\n        data_shard = zoo.orca.data.pandas.read_json(file_path, self.sc,\n                                                    orient=\'columns\', lines=True)\n        partitions_num_1 = data_shard.rdd.getNumPartitions()\n        assert partitions_num_1 == 4, ""number of partition should be 4""\n        data_shard.cache()\n        partitioned_shard = data_shard.repartition(1)\n        assert data_shard.is_cached(), ""data_shard should be cached""\n        assert partitioned_shard.is_cached(), ""partitioned_shard should be cached""\n        data_shard.uncache()\n        assert not data_shard.is_cached(), ""data_shard should be uncached""\n        partitions_num_2 = partitioned_shard.rdd.getNumPartitions()\n        assert partitions_num_2 == 1, ""number of partition should be 1""\n\n    def test_apply(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/json"")\n        data_shard = zoo.orca.data.pandas.read_json(file_path, self.sc,\n                                                    orient=\'columns\', lines=True)\n        data = data_shard.collect()\n        assert data[0][""value""].values[0] > 0, ""value should be positive""\n\n        def negative(df, column_name):\n            df[column_name] = df[column_name] * (-1)\n            return df\n\n        trans_data_shard = data_shard.transform_shard(negative, ""value"")\n        data2 = trans_data_shard.collect()\n        assert data2[0][""value""].values[0] < 0, ""value should be negative""\n\n    def test_read_csv_with_args(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/csv"")\n        data_shard = zoo.orca.data.pandas.read_csv(file_path, self.sc, sep=\',\', header=0)\n        data = data_shard.collect()\n        assert len(data) == 2, ""number of shard should be 2""\n        df = data[0]\n        assert ""location"" in df.columns, ""location is not in columns""\n\n    def test_partition_by_single_column(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/csv"")\n        data_shard = zoo.orca.data.pandas.read_csv(file_path, self.sc)\n        partitioned_shard = data_shard.partition_by(cols=""location"")\n        partitions = partitioned_shard.rdd.glom().collect()\n        assert len(partitions) == 4\n\n        data_shard = zoo.orca.data.pandas.read_csv(file_path, self.sc)\n        partitioned_shard = data_shard.partition_by(cols=""location"", num_partitions=3)\n        assert not data_shard.is_cached(), ""data_shard should be uncached""\n        assert partitioned_shard.is_cached(), ""partitioned_shard should be cached""\n        partitions = partitioned_shard.rdd.glom().collect()\n        assert len(partitions) == 3\n\n    def test_unique(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/csv"")\n        data_shard = zoo.orca.data.pandas.read_csv(file_path, self.sc)\n        location_list = data_shard.unique(""location"")\n        assert len(location_list) == 6\n\n    def test_split(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/csv"")\n        data_shard = zoo.orca.data.pandas.read_csv(file_path, self.sc)\n        trans_data_shard = data_shard.transform_shard(lambda df: (df[0:-1], df[-1:]))\n        assert trans_data_shard.is_cached(), ""trans_data_shard should be cached""\n        shards_splits = trans_data_shard.split()\n        assert not trans_data_shard.is_cached(), ""shards_splits should be uncached""\n        assert len(shards_splits) == 2\n        assert shards_splits[0].is_cached(), ""shards in shards_splits should be cached""\n        data1 = shards_splits[0].collect()\n        data2 = shards_splits[1].collect()\n        assert len(data1[0].index) > 1\n        assert len(data2[0].index) == 1\n\n    def test_len(self):\n        file_path = os.path.join(self.resource_path, ""orca/data/csv"")\n        data_shard = zoo.orca.data.pandas.read_csv(file_path, self.sc)\n        assert data_shard.len() == 14\n        assert data_shard.len(\'ID\') == 14\n        with self.assertRaises(Exception) as context:\n            data_shard.len(\'abc\')\n        self.assertTrue(\'Invalid key for this XShards\' in str(context.exception))\n\n        def to_dict(df):\n            return {\'ID\': df[\'ID\'].to_numpy(), \'location\': df[\'location\'].to_numpy()}\n        data_shard = data_shard.transform_shard(to_dict)\n        assert data_shard.len(\'ID\') == 14\n        assert data_shard.len() == 4\n        with self.assertRaises(Exception) as context:\n            data_shard.len(\'abc\')\n        self.assertTrue(\'Invalid key for this XShards\' in str(context.exception))\n\n        def to_number(d):\n            return 4\n        data_shard = data_shard.transform_shard(to_number)\n        assert data_shard.len() == 2\n        with self.assertRaises(Exception) as context:\n            data_shard.len(\'abc\')\n        self.assertTrue(\'No selection operation available for this XShards\' in\n                        str(context.exception))\n\n    def test_save(self):\n        temp = tempfile.mkdtemp()\n        file_path = os.path.join(self.resource_path, ""orca/data/csv"")\n        data_shard = zoo.orca.data.pandas.read_csv(file_path, self.sc)\n        path = os.path.join(temp, ""data.pkl"")\n        data_shard.save_pickle(path)\n        shards = zoo.orca.data.SparkXShards.load_pickle(path, self.sc)\n        assert isinstance(shards, zoo.orca.data.SparkXShards)\n        shutil.rmtree(temp)\n\n    def test_transform(self):\n        def trans_func(df):\n            data1 = {\'ID\': df[\'ID\'].values, \'price\': df[\'sale_price\'].values}\n            data2 = {\'location\': df[\'location\'].values}\n            return {\'x\': data1, \'y\': data2}\n        file_path = os.path.join(self.resource_path, ""orca/data/csv"")\n        data_shard = zoo.orca.data.pandas.read_csv(file_path, self.sc)\n        assert data_shard.is_cached(), ""data_shard should be cached""\n        transformed_data_shard = data_shard.transform_shard(trans_func)\n        assert not data_shard.is_cached(), ""data_shard should be uncached""\n        assert transformed_data_shard.is_cached(), ""transformed_data_shard should be cached""\n        data = data_shard.collect()\n        assert len(data) == 2, ""number of shard should be 2""\n        df = data[0]\n        assert ""location"" in df.columns, ""location is not in columns""\n        trans_data = transformed_data_shard.collect()\n        assert len(trans_data) == 2, ""number of shard should be 2""\n        trans_dict = trans_data[0]\n        assert ""x"" in trans_dict, ""x is not in the dictionary""\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/orca/learn/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/pipeline/api/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/pipeline/api/test_torch_net.py,32,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport shutil\nimport errno\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision\nimport numpy as np\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.pipeline.api.net.torch_net import TorchNet\nfrom zoo.pipeline.api.net.torch_criterion import TorchCriterion\nfrom zoo.pipeline.nnframes import *\nfrom zoo.common.nncontext import *\n\nfrom pyspark.ml.linalg import Vectors\n\n\nclass TestPytorch(ZooTestCase):\n\n    def setup_method(self, method):\n        """""" setup any state tied to the execution of the given method in a\n        class.  setup_method is invoked for every test method of a class.\n        """"""\n        sparkConf = init_spark_conf().setMaster(""local[1]"").setAppName(""TestPytorch"")\n        self.sc = init_nncontext(sparkConf)\n        self.sqlContext = SQLContext(self.sc)\n        assert(self.sc.appName == ""TestPytorch"")\n        if self.sc.version.startswith(""2""):\n            from pyspark.sql import SparkSession\n            spark = SparkSession \\\n                .builder \\\n                .getOrCreate()\n\n    def teardown_method(self, method):\n        """""" teardown any state that was previously setup with a setup_method\n        call.\n        """"""\n        self.sc.stop()\n\n    def test_torchnet_constructor(self):\n        # two inputs test\n        class TwoInputModel(nn.Module):\n            def __init__(self):\n                super(TwoInputModel, self).__init__()\n                self.dense1 = nn.Linear(2, 2)\n                self.dense2 = nn.Linear(3, 1)\n\n            def forward(self, x1, x2):\n                x1 = self.dense1(x1)\n                x2 = self.dense2(x2)\n                return x1, x2\n\n        TorchNet.from_pytorch(TwoInputModel(), (torch.ones(2, 2), torch.ones(2, 3)))\n        TorchNet.from_pytorch(TwoInputModel(), ([2, 2], [2, 3]))\n        TorchNet.from_pytorch(TwoInputModel(), [torch.ones(2, 2), torch.ones(2, 3)])\n        TorchNet.from_pytorch(TwoInputModel(), [[2, 2], [2, 3]])\n\n        # one input\n        input = [[0.5, 1.], [-0.3, 1.2]]\n        torch_input = torch.tensor(input)\n        model = nn.Linear(2, 1)\n        TorchNet.from_pytorch(model, torch_input)\n        TorchNet.from_pytorch(model, [1, 2])\n\n    def test_torchcriterion_constructor(self):\n        # two inputs test\n        criterion = nn.MSELoss()\n\n        def lossFunc(input, label):\n            loss1 = criterion(input[0], label[0])\n            loss2 = criterion(input[1], label[1])\n            loss = loss1 + 0.4 * loss2\n            return loss\n\n        TorchCriterion.from_pytorch(lossFunc,\n                                    (torch.ones(2, 2), torch.ones(2, 3)),\n                                    (torch.ones(2, 2), torch.ones(2, 3)))\n        TorchCriterion.from_pytorch(lossFunc, ([2, 2], [2, 3]), ([2, 2], [2, 3]))\n        TorchCriterion.from_pytorch(lossFunc,\n                                    [torch.ones(2, 2), torch.ones(2, 3)],\n                                    [torch.ones(2, 2), torch.ones(2, 3)])\n        TorchCriterion.from_pytorch(lossFunc, [[2, 2], [2, 3]], [[2, 2], [2, 3]])\n\n        # one inputs test\n        TorchCriterion.from_pytorch(criterion, [2, 1], [2, 1])\n        TorchCriterion.from_pytorch(criterion, torch.ones(2, 2), torch.ones(2, 2))\n\n    def test_torch_net_predict_resnet(self):\n        try:\n            model = torchvision.models.resnet18(pretrained=True).eval()\n        except:\n            # try again to work around connection error\n            import time\n            import random\n            time.sleep(random.randint(0, 20))\n            model = torchvision.models.resnet18(pretrained=True).eval()\n        net = TorchNet.from_pytorch(model, [1, 3, 224, 224])\n\n        dummpy_input = torch.ones(1, 3, 224, 224)\n        result = net.predict(dummpy_input.numpy()).collect()\n        assert np.allclose(result[0][0:5].tolist(),\n                           np.asarray(\n                               [-0.03913354128599167, 0.11446280777454376, -1.7967549562454224,\n                                -1.2342952489852905, -0.819004476070404]))\n\n    def test_linear_gradient_match(self):\n        input = [[0.5, 1.], [-0.3, 1.2]]\n        label = [[0.6], [-0.9]]\n        torch_input = torch.tensor(input)\n        torch_label = torch.tensor(label)\n\n        model = nn.Linear(2, 1)\n        criterion = nn.MSELoss()\n\n        torch_output = model.forward(torch_input)\n        torch_loss = criterion.forward(torch_output, torch_label)\n        torch_loss.backward()\n        torch_grad = model.weight.grad.tolist()[0] + model.bias.grad.tolist()\n\n        # AZ part\n        az_net = TorchNet.from_pytorch(model, [1, 2])\n        az_criterion = TorchCriterion.from_pytorch(criterion, [1, 1], [1, 1])\n\n        az_input = np.array(input)\n        az_label = np.array(label)\n\n        az_output = az_net.forward(az_input)\n        az_loss_output = az_criterion.forward(az_output, az_label)\n        az_loss_backward = az_criterion.backward(az_output, az_label)\n        az_model_backward = az_net.backward(az_input, az_loss_backward)\n\n        az_grad = list(az_net.parameters().values())[0][\'gradWeight\']\n\n        assert np.allclose(torch_loss.tolist(), az_loss_output)\n        assert np.allclose(torch_grad, az_grad.tolist())\n\n    def test_conv2D_gradient_match(self):\n        class SimpleTorchModel(nn.Module):\n            def __init__(self):\n                super(SimpleTorchModel, self).__init__()\n                self.dense1 = nn.Linear(2, 48)\n                self.conv1 = nn.Conv2d(3, 2, 2)\n                self.dense2 = nn.Linear(2, 1)\n\n            def forward(self, x):\n                x = self.dense1(x)\n                x = x.view(-1, 3, 4, 4)\n                x = torch.relu(self.conv1(x))\n                x = F.max_pool2d(x, 2)\n                x = x.view(x.size(0), -1)\n                x = torch.sigmoid(self.dense2(x))\n                return x\n\n        input = [[1., -0.5], [0.5, -1.]]\n        label = [[1., -0.5]]\n        torch_input = torch.tensor(input)\n        torch_label = torch.tensor(label)\n\n        torch_model = SimpleTorchModel()\n        torch_criterion = nn.MSELoss()\n\n        torch_output = torch_model.forward(torch_input)\n        torch_loss = torch_criterion.forward(torch_output, torch_label)\n        torch_loss.backward()\n        torch_grad = torch_model.dense1.weight.grad.flatten().tolist() + \\\n            torch_model.dense1.bias.grad.flatten().tolist() + \\\n            torch_model.conv1.weight.grad.flatten().tolist() + \\\n            torch_model.conv1.bias.grad.flatten().tolist() + \\\n            torch_model.dense2.weight.grad.flatten().tolist() + \\\n            torch_model.dense2.bias.grad.flatten().tolist()\n\n        # AZ part\n        az_net = TorchNet.from_pytorch(torch_model, [1, 2])\n        az_criterion = TorchCriterion.from_pytorch(torch_criterion.forward, [1, 1], [1, 1])\n\n        az_input = np.array(input)\n        az_label = np.array(label)\n        az_output = az_net.forward(np.array(input))\n        az_loss_output = az_criterion.forward(az_output, az_label)\n        az_loss_backward = az_criterion.backward(az_output, az_label)\n        az_model_backward = az_net.backward(az_input, az_loss_backward)\n\n        az_grad = list(az_net.parameters().values())[0][\'gradWeight\']\n\n        assert np.allclose(torch_loss.tolist(), az_loss_output)\n        assert np.allclose(torch_grad, az_grad.tolist())\n\n    def test_cross_entrophy_match(self):\n        input = [[0.5, 1.], [-0.3, 1.2]]\n        label = [3, 6]\n        torch_input = torch.tensor(input)\n        torch_label = torch.tensor(label).long()\n\n        model = nn.Linear(2, 10)\n        criterion = nn.CrossEntropyLoss()\n\n        def lossFunc(input, target):\n            return criterion.forward(input, target.flatten().long())\n\n        torch_output = model.forward(torch_input)\n        torch_loss = criterion.forward(torch_output, torch_label)\n        torch_loss.backward()\n        torch_grad = model.weight.grad.flatten().tolist() + model.bias.grad.tolist()\n\n        # AZ part\n        az_net = TorchNet.from_pytorch(model, [1, 2])\n        az_criterion = TorchCriterion.from_pytorch(lossFunc, [1, 10], [1, 1])\n\n        az_input = np.array(input)\n        az_label = np.array(label)\n\n        az_output = az_net.forward(az_input)\n        az_loss_output = az_criterion.forward(az_output, az_label)\n        az_loss_backward = az_criterion.backward(az_output, az_label)\n        az_model_backward = az_net.backward(az_input, az_loss_backward)\n\n        az_grad = list(az_net.parameters().values())[0][\'gradWeight\']\n\n        assert np.allclose(torch_loss.tolist(), az_loss_output)\n        assert np.allclose(torch_grad, az_grad.tolist())\n\n    def test_Lenet_gradient_match(self):\n        class LeNet(nn.Module):\n            def __init__(self):\n                super(LeNet, self).__init__()\n                self.conv1 = nn.Conv2d(1, 20, 5, 1)\n                self.conv2 = nn.Conv2d(20, 50, 5, 1)\n                self.fc1 = nn.Linear(4 * 4 * 50, 500)\n                self.fc2 = nn.Linear(500, 10)\n\n            def forward(self, x):\n                x = F.relu(self.conv1(x))\n                x = F.max_pool2d(x, 2, 2)\n                x = F.relu(self.conv2(x))\n                x = F.max_pool2d(x, 2, 2)\n                x = x.view(-1, 4 * 4 * 50)\n                x = F.relu(self.fc1(x))\n                x = self.fc2(x)\n                return F.log_softmax(x, dim=1)\n\n        input = np.random.rand(2, 1, 28, 28)\n        label = [7, 3]\n        torch_input = torch.tensor(input).float()\n        torch_label = torch.tensor(label).long()\n\n        torch_model = LeNet()\n        torch_criterion = nn.CrossEntropyLoss()\n\n        torch_output = torch_model.forward(torch_input)\n        torch_loss = torch_criterion.forward(torch_output, torch_label)\n        torch_loss.backward()\n        torch_grad = torch_model.conv1.weight.grad.flatten().tolist() + \\\n            torch_model.conv1.bias.grad.flatten().tolist() + \\\n            torch_model.conv2.weight.grad.flatten().tolist() + \\\n            torch_model.conv2.bias.grad.flatten().tolist() + \\\n            torch_model.fc1.weight.grad.flatten().tolist() + \\\n            torch_model.fc1.bias.grad.flatten().tolist() + \\\n            torch_model.fc2.weight.grad.flatten().tolist() + \\\n            torch_model.fc2.bias.grad.flatten().tolist()\n\n        # AZ part\n        az_net = TorchNet.from_pytorch(torch_model, [1, 1, 28, 28])\n\n        def lossFunc(input, target):\n            return torch_criterion.forward(input, target.flatten().long())\n\n        az_criterion = TorchCriterion.from_pytorch(lossFunc, [1, 10], [1, 1])\n\n        az_input = np.array(input)\n        az_label = np.array(label)\n        az_output = az_net.forward(np.array(input))\n        az_loss_output = az_criterion.forward(az_output, az_label)\n        az_loss_backward = az_criterion.backward(az_output, az_label)\n        az_model_backward = az_net.backward(az_input, az_loss_backward)\n\n        az_grad = list(az_net.parameters().values())[0][\'gradWeight\']\n\n        assert np.allclose(torch_loss.tolist(), az_loss_output)\n        assert np.allclose(torch_grad, az_grad.tolist(), atol=1.e-5, rtol=1.e-3)\n\n    def test_model_inference_with_multiple_output(self):\n        class TwoOutputModel(nn.Module):\n            def __init__(self):\n                super(TwoOutputModel, self).__init__()\n                self.dense1 = nn.Linear(2, 1)\n\n            def forward(self, x):\n                x1 = self.dense1(x)\n                return x, x1\n\n        torch_model = TwoOutputModel()\n        az_net = TorchNet.from_pytorch(TwoOutputModel(), [1, 2])\n\n        az_input = np.array([[0.5, 1.], [-0.3, 1.2]])\n        az_output = az_net.forward(az_input)\n        assert (len(az_output) == 2)\n        assert (az_output[0].shape == (2, 2))\n        assert (az_output[1].shape == (2, 1))\n\n    def test_model_train_with_multiple_output(self):\n        class TwoOutputModel(nn.Module):\n            def __init__(self):\n                super(TwoOutputModel, self).__init__()\n                self.dense1 = nn.Linear(2, 1)\n\n            def forward(self, x):\n                x1 = self.dense1(x)\n                return x, x1\n\n        input = [[0.5, 1.], [-0.3, 1.2]]\n        torch_input = torch.tensor(input)\n        torch_label = (torch.ones(2, 2), torch.ones(2, 1))\n\n        model = TwoOutputModel()\n        criterion = nn.MSELoss()\n\n        def lossFunc(input, label):\n            loss1 = criterion(input[0], label[0])\n            loss2 = criterion(input[1], label[1])\n            loss = loss1 + 0.4 * loss2\n            return loss\n\n        torch_output = model.forward(torch_input)\n        torch_loss = lossFunc(torch_output, torch_label)\n        torch_loss.backward()\n        torch_grad = model.dense1.weight.grad.tolist()[0] + model.dense1.bias.grad.tolist()\n\n        az_net = TorchNet.from_pytorch(model, [1, 2])\n        az_criterion = TorchCriterion.from_pytorch(\n            lossFunc,\n            (torch.ones(2, 2), torch.ones(2, 1)),\n            (torch.ones(2, 2), torch.ones(2, 1)))\n\n        az_input = np.array(input)\n        az_label = [np.ones([2, 2]), np.ones([2, 1])]\n        az_output = az_net.forward(az_input)\n        az_loss_output = az_criterion.forward(az_output, az_label)\n        az_loss_backward = az_criterion.backward(az_output, az_label)\n        az_model_backward = az_net.backward(az_input, az_loss_backward)\n\n        az_grad = list(az_net.parameters().values())[0][\'gradWeight\']\n\n        assert np.allclose(torch_loss.tolist(), az_loss_output)\n        assert np.allclose(torch_grad, az_grad.tolist())\n\n    def test_model_train_with_multiple_input(self):\n        class TwoInputModel(nn.Module):\n            def __init__(self):\n                super(TwoInputModel, self).__init__()\n                self.dense1 = nn.Linear(2, 2)\n                self.dense2 = nn.Linear(2, 1)\n\n            def forward(self, x1, x2):\n                x1 = self.dense1(x1)\n                x2 = self.dense2(x2)\n                return x1, x2\n\n        input = [[0.5, 1.], [-0.3, 1.2]]\n        torch_input1 = torch.tensor(input, requires_grad=True)\n        torch_input2 = torch.tensor(input, requires_grad=True)\n        torch_label = (torch.ones(2, 2), torch.ones(2, 1))\n\n        model = TwoInputModel()\n        criterion = nn.MSELoss()\n\n        def lossFunc(input, label):\n            loss1 = criterion(input[0], label[0])\n            loss2 = criterion(input[1], label[1])\n            loss = loss1 + 0.4 * loss2\n            return loss\n\n        torch_output = model.forward(torch_input1, torch_input2)\n        torch_loss = lossFunc(torch_output, torch_label)\n        torch_loss.backward()\n        torch_grad = model.dense1.weight.grad.tolist()[0] + \\\n            model.dense1.weight.grad.tolist()[1] + \\\n            model.dense1.bias.grad.tolist() + \\\n            model.dense2.weight.grad.tolist()[0] + \\\n            model.dense2.bias.grad.tolist()\n\n        az_net = TorchNet.from_pytorch(model, (torch.ones(2, 2), torch.ones(2, 2)))\n        az_criterion = TorchCriterion.from_pytorch(\n            lossFunc,\n            (torch.ones(2, 2), torch.ones(2, 1)),\n            (torch.ones(2, 2), torch.ones(2, 1)))\n\n        az_input = [np.array(input), np.array(input)]\n        az_label = [np.ones([2, 2]), np.ones([2, 1])]\n        az_output = az_net.forward(az_input)\n        az_loss_output = az_criterion.forward(az_output, az_label)\n        az_loss_backward = az_criterion.backward(az_output, az_label)\n        az_model_backward = az_net.backward(az_input, az_loss_backward)\n\n        az_grad = list(az_net.parameters().values())[0][\'gradWeight\']\n\n        assert np.allclose(torch_loss.tolist(), az_loss_output)\n        assert np.allclose(torch_grad, az_grad.tolist())\n        assert np.allclose(az_model_backward[0], torch_input1.grad)\n        assert np.allclose(az_model_backward[1], torch_input2.grad)\n\n    def test_model_save_load(self):\n        class SimpleTorchModel(nn.Module):\n            def __init__(self):\n                super(SimpleTorchModel, self).__init__()\n                self.dense1 = nn.Linear(2, 4)\n                self.dense2 = nn.Linear(4, 1)\n\n            def forward(self, x):\n                x = self.dense1(x)\n                x = torch.sigmoid(self.dense2(x))\n                return x\n\n        df = self.sqlContext.createDataFrame(\n            [(Vectors.dense([2.0, 1.0]), 1.0),\n             (Vectors.dense([1.0, 2.0]), 0.0),\n             (Vectors.dense([2.0, 1.0]), 1.0),\n             (Vectors.dense([1.0, 2.0]), 0.0)],\n            [""features"", ""label""])\n\n        torch_model = SimpleTorchModel()\n        torch_criterion = nn.MSELoss()\n\n        az_model = TorchNet.from_pytorch(torch_model, [1, 2])\n        az_criterion = TorchCriterion.from_pytorch(torch_criterion, [1, 1], [1, 1])\n        estimator = NNEstimator(az_model, az_criterion) \\\n            .setBatchSize(4) \\\n            .setLearningRate(0.01) \\\n            .setMaxEpoch(10)\n\n        nnModel = estimator.fit(df)\n        res = nnModel.transform(df)\n\n        try:\n            tmp_dir = tempfile.mkdtemp()\n            modelPath = os.path.join(tmp_dir, ""model"")\n            az_model.savePytorch(modelPath)\n            loaded = TorchNet(modelPath)\n            resDF = NNModel(loaded).setPredictionCol(""loaded"").transform(res)\n            assert resDF.filter(""prediction==loaded"").count() == resDF.count()\n        finally:\n            try:\n                shutil.rmtree(tmp_dir)  # delete directory\n            except OSError as exc:\n                if exc.errno != errno.ENOENT:  # ENOENT - no such file or directory\n                    raise  # re-raise exception\n\n    def test_model_save_load_nnframe(self):\n        class SimpleTorchModel(nn.Module):\n            def __init__(self):\n                super(SimpleTorchModel, self).__init__()\n                self.dense1 = nn.Linear(2, 4)\n                self.dense2 = nn.Linear(4, 1)\n\n            def forward(self, x):\n                x = self.dense1(x)\n                x = torch.sigmoid(self.dense2(x))\n                return x\n\n        df = self.sqlContext.createDataFrame(\n            [(Vectors.dense([2.0, 1.0]), 1.0),\n             (Vectors.dense([1.0, 2.0]), 0.0),\n             (Vectors.dense([2.0, 1.0]), 1.0),\n             (Vectors.dense([1.0, 2.0]), 0.0)],\n            [""features"", ""label""])\n\n        torch_model = SimpleTorchModel()\n        torch_criterion = nn.MSELoss()\n\n        az_model = TorchNet.from_pytorch(torch_model, [1, 2])\n        az_criterion = TorchCriterion.from_pytorch(torch_criterion, [1, 1], [1, 1])\n        estimator = NNEstimator(az_model, az_criterion) \\\n            .setBatchSize(4) \\\n            .setLearningRate(0.01) \\\n            .setMaxEpoch(10)\n\n        nnModel = estimator.fit(df)\n        try:\n            tmp_dir = tempfile.mkdtemp()\n            modelPath = os.path.join(tmp_dir, ""model"")\n            nnModel.save(modelPath)\n            loaded = NNModel.load(modelPath)\n            resDF = loaded.transform(df)\n        finally:\n            try:\n                shutil.rmtree(tmp_dir)  # delete directory\n            except OSError as exc:\n                if exc.errno != errno.ENOENT:  # ENOENT - no such file or directory\n                    raise  # re-raise exception\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/pipeline/autograd/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/pipeline/autograd/test_loss.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport keras.backend as KK\nimport keras.layers as klayers\nimport keras.objectives as kloss\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.pipeline.api.autograd import *\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.pipeline.api.keras.layers import *\n\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestLoss(ZooTestCase):\n    # kkloss is a function which accept y_pred and y_true.\n    # y_pred and y_true are all keras_tensor\n    # zloss is a AbstractCriterion\n    # The first dim of shape is batch\n    def compareLossWithKeras(self, kkloss_func, zloss, shape, sizeAverageKerasLoss=True):\n\n        y_pred = klayers.Input(shape=shape[1:])\n        y_true = klayers.Input(shape=shape[1:])\n\n        batch = shape[0]\n\n        kkloss = kkloss_func(y_true, y_pred)\n        y_true_value = np.random.uniform(0, 1, shape)\n        y_pred_value = np.random.uniform(0, 1, shape)\n\n        k_grad_y_pred = KK.get_session().run(\n            KK.gradients(kkloss, y_pred),\n            feed_dict={y_true: y_true_value, y_pred: y_pred_value})[0]\n        k_output = KK.get_session().run(kkloss,\n                                        feed_dict={y_true: y_true_value, y_pred: y_pred_value})\n        z_output = zloss.forward(y_true_value, y_pred_value)\n        z_grad_y_pred = zloss.backward(y_true_value, y_pred_value)\n\n        assert(z_output == pytest.approx(np.mean(k_output), 1e-5, 1e-5))\n        if sizeAverageKerasLoss:\n            self.assert_allclose(k_grad_y_pred / batch, z_grad_y_pred)\n        else:\n            self.assert_allclose(k_grad_y_pred, z_grad_y_pred)\n\n    def test_abs(self):\n        def mean_absolute_error(y_true, y_pred):\n            result = mean(abs(y_true - y_pred), axis=1)\n            return result\n        self.compareLossWithKeras(kloss.mean_absolute_error,\n                                  CustomLoss(mean_absolute_error, [3]), [2, 3],\n                                  sizeAverageKerasLoss=True)\n\n    def test_rank_hinge_loss(self):\n        def rank_hinge_loss(**kwargs):\n            if isinstance(kwargs, dict) and \'batch\' in kwargs:\n                batch = kwargs[\'batch\']\n\n            def _rank_hinge_loss(y_true, y_pred):\n                y_pred = y_pred + y_true - y_true\n                margin = 1.0\n                pos = merge([y_pred.slice(0, i, 1) for i in range(0, batch, 2)],\n                            mode=""concat"", concat_axis=0)\n                neg = merge([y_pred.slice(0, i, 1) for i in range(1, batch, 2)],\n                            mode=""concat"", concat_axis=0)\n                loss = maximum(margin + neg - pos, 0.)\n                return loss\n            return _rank_hinge_loss\n\n        def keras_rank_hinge_loss(y_true, y_pred):\n            margin = 1.0\n            y_pos = klayers.Lambda(lambda a: a[::2, :], output_shape=(1,))(y_pred)\n            y_neg = klayers.Lambda(lambda a: a[1::2, :], output_shape=(1,))(y_pred)\n            loss = KK.maximum(0., margin + y_neg - y_pos)\n            return KK.mean(loss)\n\n        batch = 32\n        self.compareLossWithKeras(keras_rank_hinge_loss,\n                                  CustomLoss(rank_hinge_loss(batch=batch), [1]),\n                                  [batch, 1], sizeAverageKerasLoss=False)\n\n    def test_abs_with_fit(self):\n        def mean_absolute_error(y_true, y_pred):\n            result = mean(abs(y_true - y_pred), axis=1)\n            return result\n        data_len = 1000\n        X_ = np.random.uniform(0, 1, (1000, 2))\n        Y_ = ((2 * X_).sum(1) + 0.4).reshape([data_len, 1])\n        model = Sequential()\n        model.add(Dense(1, input_shape=(2, )))\n        model.compile(optimizer=SGD(learningrate=1e-2),\n                      loss=mean_absolute_error,\n                      metrics=[""auc""])\n        model.fit(x=X_,\n                  y=Y_,\n                  batch_size=32,\n                  nb_epoch=500,\n                  validation_data=None,\n                  distributed=False)\n        w = model.get_weights()\n        self.assert_allclose(w[0], np.array([2, 2]).reshape([1, 2]), rtol=1e-1)\n        self.assert_allclose(w[1], np.array([0.4]), rtol=1e-1)\n        predict_result = model.predict_local(X_)\n        self.assert_allclose(Y_, predict_result.reshape((data_len, 1)), rtol=1e-1)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/pipeline/autograd/test_model.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import *\nimport zoo.pipeline.api.autograd as auto\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestAutoGradModel(ZooTestCase):\n    def test_var(self):\n        input = Input(shape=[2, 20])\n        time = TimeDistributed(layer=Dense(30))(input)\n        t1 = time.index_select(1, 0)\n        t2 = time.index_select(1, 1)\n        diff = auto.abs(t1 - t2)\n        assert diff.get_output_shape() == (None, 30)\n        assert diff.get_input_shape() == (None, 30)\n        model = Model(input, diff)\n        data = np.random.uniform(0, 1, [10, 2, 20])\n        output = model.forward(data)\n        print(output.shape)\n\n    def test_var_model(self):\n        input = Input(shape=[2, 3, 16, 16])\n\n        vgg_mock = Sequential()\n        vgg_mock.add(Conv2D(2, 4, 4, input_shape=[3, 16, 16]))  # output: 2, 13, 13\n        vgg_mock.add(Reshape([2 * 13 * 13]))\n        vgg_mock.add(Dense(100))\n        vgg_mock.add(Reshape([100, 1, 1]))\n\n        time = TimeDistributed(layer=vgg_mock)(input)\n        t1 = time.index_select(1, 0)\n        t2 = time.index_select(1, 1)\n        diff = t1 - t2\n        model = Model(input, diff)\n        data = np.random.uniform(0, 1, [10, 2, 3, 16, 16])\n        output = model.forward(data)\n        print(output.shape)\n\n    def test_zoo_keras_layer_of(self):\n        input = Input(shape=[2, 3])\n        dense = Dense(3)\n        ZooKerasLayer.of(dense.value)(input)\n\n    def test_parameter_create(self):\n        w = auto.Parameter(shape=(3, 2))\n        value = w.get_weight()\n        w.set_weight(value)\n        x = auto.Variable(input_shape=(3,))\n        b = auto.Parameter(shape=(2,))\n        out = auto.mm(x, w, axes=(1, 0)) + b\n        model = Model(input=x, output=out)\n        input_data = np.random.uniform(0, 1, (4, 3))\n        model.forward(input_data)\n'"
pyzoo/test/zoo/pipeline/autograd/test_operator.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport keras.backend as KK\nimport keras.layers as klayers\nimport numpy as np\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.pipeline.api.autograd import *\nimport zoo.pipeline.api.autograd as A\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import Sequential, Model\nfrom zoo.pipeline.api.utils import remove_batch\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestOperator(ZooTestCase):\n    # shape including batch\n    def compare_binary_op(self, kk_func, z_layer, shape, rtol=1e-5, atol=1e-5):\n        x = klayers.Input(shape=shape[0][1:])\n        y = klayers.Input(shape=shape[1][1:])\n\n        batch = shape[0][0]\n\n        kkresult = kk_func(x, y)\n        x_value = np.random.uniform(0, 1, shape[0])\n        y_value = np.random.uniform(0, 1, shape[1])\n\n        k_grads = KK.get_session().run(KK.gradients(kkresult, [x, y]),\n                                       feed_dict={x: x_value, y: y_value})\n        k_output = KK.get_session().run(kkresult,\n                                        feed_dict={x: x_value, y: y_value})\n        inputs = [Input(s) for s in remove_batch(shape)]\n        model = Model(inputs, z_layer(inputs))\n        z_output = model.forward([x_value, y_value])\n        grad_output = np.array(z_output)\n        grad_output.fill(1.0)\n        z_grads = model.backward([x_value, y_value], grad_output)\n\n        # Check if the model can be forward/backward multiple times or not\n        z_output2 = model.forward([x_value, y_value])\n        z_grads2 = model.backward([x_value, y_value], grad_output)\n        self.assert_allclose(z_output, z_output2, rtol, atol)\n        [self.assert_allclose(z, k, rtol, atol) for (z, k) in zip(z_grads, z_grads2)]\n\n        self.assert_allclose(z_output, k_output, rtol, atol)\n        [self.assert_allclose(z, k, rtol, atol) for (z, k) in zip(z_grads, k_grads)]\n\n    # shape including batch\n    def compare_unary_op(self, kk_func, z_layer, shape, rtol=1e-5, atol=1e-5):\n        x = klayers.Input(shape=shape[1:])\n\n        batch = shape[0]\n\n        kkresult = kk_func(x)\n        x_value = np.random.uniform(0, 1, shape)\n\n        k_grads = KK.get_session().run(KK.gradients(kkresult, x),\n                                       feed_dict={x: x_value})\n        k_output = KK.get_session().run(kkresult,\n                                        feed_dict={x: x_value})\n        model = Sequential()\n        model.add(InputLayer(shape[1:]))\n        model.add(z_layer)\n        z_output = model.forward(x_value)\n        grad_output = np.array(z_output)\n        grad_output.fill(1.0)\n        z_grad = model.backward(x_value, grad_output)\n\n        z_output2 = model.forward(x_value)\n        z_grad2 = model.backward(x_value, grad_output)\n        self.assert_allclose(z_output, z_output2, rtol, atol)\n        self.assert_allclose(z_grad, z_grad2, rtol, atol)\n\n        self.assert_allclose(z_output, k_output, rtol, atol)\n        self.assert_allclose(z_grad, k_grads[0], rtol, atol)\n\n    def test_add(self):\n        def z_add_func(x, y):\n            return x + y\n\n        def k_add_func(x, y):\n            return x + y\n\n        self.compare_binary_op(k_add_func,\n                               Lambda(function=z_add_func), [[2, 3], [2, 3]])\n\n    def test_add_constant(self):\n        def z_add_func(x):\n            return x + 3.0\n\n        def k_add_func(x):\n            return x + 3.0\n\n        self.compare_unary_op(k_add_func,\n                              Lambda(function=z_add_func), [2, 3])\n\n    def test_radd_constant(self):\n        def z_add_func(x):\n            return 3.0 + x\n\n        def k_add_func(x):\n            return 3.0 + x\n\n        self.compare_unary_op(k_add_func,\n                              Lambda(function=z_add_func), [2, 3])\n\n    def test_sub(self):\n        def z_func(x, y):\n            return x - y\n\n        def k_func(x, y):\n            return x - y\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func), [[2, 3], [2, 3]])\n\n    def test_sub_constant(self):\n        def z_func(x):\n            return x - 3.0\n\n        def k_func(x):\n            return x - 3.0\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func), [2, 3])\n\n    def test_rsub_constant(self):\n        def z_func(x):\n            return 3.0 - x\n\n        def k_func(x):\n            return 3.0 - x\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func), [2, 3])\n\n    def test_div(self):\n        def z_func(x, y):\n            return x / y\n\n        def k_func(x, y):\n            return x / y\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func), [[2, 3], [2, 3]])\n\n    def test_div_constant(self):\n        def z_func(x):\n            return x / 3.0\n\n        def k_func(x):\n            return x / 3.0\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func), [2, 3])\n\n    def test_rdiv_constant(self):\n        def z_func(x):\n            return 3.0 / x\n\n        def k_func(x):\n            return 3.0 / x\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func), [2, 3])\n\n    def test_mul(self):\n        def z_func(x, y):\n            return x * y\n\n        def k_func(x, y):\n            return x * y\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func), [[2, 3], [2, 3]])\n\n    def test_mul_constant(self):\n        def z_func(x):\n            return x * 3.0\n\n        def k_func(x):\n            return x * 3.0\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func), [2, 3])\n\n    def test_rmul_constant(self):\n        def z_func(x):\n            return 3.0 * x\n\n        def k_func(x):\n            return 3.0 * x\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func), [2, 3])\n\n    def test_neg(self):\n        def z_func(x):\n            return - x\n\n        def k_func(x):\n            return - x\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func), [2, 3])\n\n    def test_clip(self):\n        def z_func(x):\n            return clip(x, 0.5, 0.8)\n\n        def k_func(x):\n            return KK.clip(x, 0.5, 0.8)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3])\n\n    def test_pow(self):\n        def z_func(x):\n            return pow(x, 3.0)\n\n        def k_func(x):\n            return KK.pow(x, 3.0)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3])\n\n    def test_square(self):\n        def z_func(x):\n            return square(x)\n\n        def k_func(x):\n            return KK.square(x)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3])\n\n    def test_square_as_first_layer(self):\n        def z_func(x):\n            return square(x)\n\n        ll = Lambda(function=z_func, input_shape=[2, 3])\n        seq = Sequential()\n        seq.add(ll)\n        result = seq.forward(np.ones([2, 3]))\n        assert (result == np.ones([2, 3])).all()\n\n    def test_expose_node(self):\n        image_shape = [3, 16, 16]\n        input_shape = [2] + image_shape\n        input = Input(shape=input_shape, name=""input1"")\n\n        def l1(x):\n            x1 = x.index_select(1, 0)  # input is [B, 2, 3, 16, 16]\n            x2 = x.index_select(1, 0)\n            return abs(x1 - x2)\n\n        output = Lambda(function=l1)(input)\n        model = Model(input, output)\n\n        mock_data = np.random.uniform(0, 1, [10] + input_shape)\n        out_data = model.forward(mock_data)\n        assert out_data.shape == (10, 3, 16, 16)\n\n    def test_softsign(self):\n        def z_func(x):\n            return softsign(x)\n\n        def k_func(x):\n            return KK.softsign(x)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3])\n\n    def test_softplus(self):\n        def z_func(x):\n            return softplus(x)\n\n        def k_func(x):\n            return KK.softplus(x)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3])\n\n    def test_exp(self):\n        def z_func(x):\n            return exp(x)\n\n        def k_func(x):\n            return KK.exp(x)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3])\n\n    def test_abs(self):\n        def z_func(x):\n            return abs(x)\n\n        def k_func(x):\n            return KK.abs(x)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3])\n\n    def test_log(self):\n        def z_func(x):\n            return log(x)\n\n        def k_func(x):\n            return KK.log(x)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3])\n\n    def test_sqrt(self):\n        def z_func(x):\n            return sqrt(x)\n\n        def k_func(x):\n            return KK.sqrt(x)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3])\n\n    def test_mean(self):\n        def z_func(x):\n            return mean(x, 0, False)\n\n        def k_func(x):\n            return KK.mean(x, 0, False)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3])\n\n    def test_sum(self):\n        def z_func(x):\n            return sum(x, 0, False)\n\n        def k_func(x):\n            return KK.sum(x, 0, False)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3])\n\n    def test_maximum(self):\n        def z_func(x, y):\n            return maximum(x, y)\n\n        def k_func(x, y):\n            return KK.maximum(x, y)\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func, ), [[2, 3], [2, 3]])\n\n    def test_expand_dim1(self):\n        def z_func(x):\n            return expand_dims(x, 1)\n\n        def k_func(x):\n            return KK.expand_dims(x, 1)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3, 4])\n\n    def test_expand_dim2(self):\n        def z_func(x):\n            return expand_dims(x, 2)\n\n        def k_func(x):\n            return KK.expand_dims(x, 2)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [2, 3, 4, 5])\n\n    def test_stack(self):\n        def z_func(x, y):\n            return stack([x, y], axis=1)\n\n        def k_func(x, y):\n            return KK.stack([x, y], axis=1)\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func, ), [[3, 2, 4], [3, 2, 4]])\n\n    def test_stack2(self):\n        def z_func(x, y):\n            return stack([x, y], axis=2)\n\n        def k_func(x, y):\n            return KK.stack([x, y], axis=2)\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func, ), [[3, 2, 4], [3, 2, 4]])\n\n    def test_slice(self):\n        def z_func(x):\n            return x.slice(1, 1, 2)\n\n        def k_func(x):\n            return x[:, 1:3, :]\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [3, 5, 4])\n\n    def test_slice2(self):\n        def z_func(x):\n            return x.slice(2, 1, 2)\n\n        def k_func(x):\n            return x[:, :, 1:3]\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [3, 2, 4])\n\n    def test_index_select(self):\n        def z_func(x):\n            return x.index_select(2, 2)\n\n        def k_func(x):\n            return x[:, :, 2]\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [3, 2, 4])\n\n    def test_dot_2D(self):\n        def z_func(x, y):\n            return A.batch_dot(x, y, axes=1, normalize=False)\n\n        def k_func(x, y):\n            return klayers.Dot(axes=[1, 1], normalize=False)([x, y])\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func, ), [[3, 2], [3, 2]])\n\n    def test_dot_3D_2(self):\n        def z_func(x, y):\n            return A.batch_dot(x, y, axes=2, normalize=False)\n\n        def k_func(x, y):\n            return klayers.Dot(axes=2, normalize=False)([x, y])\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func, ), [[3, 2, 4], [3, 2, 4]])\n\n    def test_dot_3D_1(self):\n        def z_func(x, y):\n            return A.batch_dot(x, y, axes=1, normalize=False)\n\n        def k_func(x, y):\n            return klayers.Dot(axes=1, normalize=False)([x, y])\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func, ), [[3, 2, 4], [3, 2, 4]])\n\n    def test_dot_3D_1_2(self):\n        def z_func(x, y):\n            return A.batch_dot(x, y, axes=[1, 2], normalize=False)\n\n        def k_func(x, y):\n            return klayers.Dot(axes=[1, 2], normalize=False)([x, y])\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func, ), [[3, 2, 4], [3, 4, 2]])\n\n    def test_dot_3D_2_2_norm(self):\n        def z_func(x, y):\n            return A.batch_dot(x, y, axes=[2, 2], normalize=True)\n\n        def k_func(x, y):\n            return klayers.Dot(axes=[2, 2], normalize=True)([x, y])\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func, ), [[3, 2, 4], [3, 2, 4]])\n\n    def test_dot_3D_1_2_norm(self):\n        def z_func(x, y):\n            return A.batch_dot(x, y, axes=[1, 2], normalize=True)\n\n        def k_func(x, y):\n            return klayers.Dot(axes=[1, 2], normalize=True)([x, y])\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func, ), [[3, 2, 4], [3, 4, 2]])\n\n    def test_l2_normalize(self):\n        def z_func(x):\n            return A.l2_normalize(x, axis=1)\n\n        def k_func(x):\n            return KK.l2_normalize(x, axis=1)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [3, 2])\n\n    def test_l2_normalize_2(self):\n        def z_func(x):\n            return A.l2_normalize(x, axis=2)\n\n        def k_func(x):\n            return KK.l2_normalize(x, axis=2)\n\n        self.compare_unary_op(k_func,\n                              Lambda(function=z_func, ), [3, 2, 4])\n\n    def test_mm(self):\n        def z_func(x, y):\n            return A.mm(x, y, axes=[2, 2])\n\n        def k_func(x, y):\n            return klayers.Dot(axes=[2, 2], normalize=False)([x, y])\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func, ), [[2, 3, 4], [2, 3, 4]])\n\n    def test_mm2(self):\n        def z_func(x, y):\n            return A.mm(x, y, axes=[1, 1])\n\n        def k_func(x, y):\n            return klayers.Dot(axes=[1, 1], normalize=False)([x, y])\n\n        self.compare_binary_op(k_func,\n                               Lambda(function=z_func, ), [[2, 3, 4], [2, 3, 4]])\n\n    def test_ExpandDim(self):\n        inputdata = np.array([2, 1, 6])\n        input = Parameter(shape=(3,), init_weight=inputdata)\n        expand = ExpandDim(dim=0)(input)\n        model = Model(input, expand)\n        assert model.get_output_shape() == (1, 3)\n        desired = inputdata.reshape(1, 3)\n        outputdata = model.forward(inputdata)\n        np.testing.assert_almost_equal(outputdata, desired, decimal=4)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/pipeline/autograd/test_operator_special.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport keras.backend as KK\nimport keras.layers as klayers\nimport numpy as np\nimport pytest\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.pipeline.api.autograd import *\nimport zoo.pipeline.api.autograd as A\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import Sequential, Model\nfrom zoo.pipeline.api.autograd import Parameter, Constant\n\n\nclass TestOperatorSpecial(ZooTestCase):\n    def test_mean_1D(self):\n        data = np.random.randn(2, )\n        parameter = Parameter(shape=(2,), init_weight=data)\n        out = autograd.mean(parameter, axis=0)\n        model = Model(input=parameter, output=out)\n        result = model.forward(data)\n        np.testing.assert_almost_equal(result, data.mean(axis=0), decimal=5)\n\n    def test_mean_2D(self):\n        data = np.random.randn(2, 3)\n        parameter = Parameter(shape=(2, 3), init_weight=data)\n        out = autograd.mean(parameter, axis=0)\n        model = Model(input=parameter, output=out)\n        result = model.forward(data)\n        np.testing.assert_almost_equal(result, data.mean(axis=0), decimal=5)\n\n    def test_slice_1D(self):\n        data = np.random.randn(4, )\n        parameter = Parameter(shape=(4,), init_weight=data)\n        out = parameter.slice(0, 0, 2)\n        model = Model(input=parameter, output=out)\n        result = model.forward(data)\n        np.testing.assert_almost_equal(result, data[0:2], decimal=5)\n\n    def test_slice_2D(self):\n        data = np.random.randn(2, 3)\n        parameter = Parameter(shape=(2, 3), init_weight=data)\n        out = parameter.slice(0, 0, 2)\n        model = Model(input=parameter, output=out)\n        result = model.forward(data)\n        np.testing.assert_almost_equal(result, data[0:2], decimal=5)\n\n    def test_unsqueeze_1D(self):\n        data = np.random.randn(4, )\n        parameter = Parameter(shape=(4,), init_weight=data)\n        out = autograd.expand_dims(parameter, axis=0)\n        model = Model(input=parameter, output=out)\n        result = model.forward(data)\n        np.testing.assert_almost_equal(result, np.expand_dims(data, axis=0), decimal=5)\n\n    def test_unsqueeze_2D(self):\n        data = np.random.randn(2, 3)\n        parameter = Parameter(shape=(2, 3), init_weight=data)\n        out = autograd.expand_dims(parameter, axis=0)\n        model = Model(input=parameter, output=out)\n        result = model.forward(data)\n        np.testing.assert_almost_equal(result, np.expand_dims(data, axis=0), decimal=5)\n\n    def test_sum_1D(self):\n        data = np.random.randn(2, )\n        parameter = Parameter(shape=(2,), init_weight=data)\n        out = autograd.sum(parameter, axis=0)\n        model = Model(input=parameter, output=out)\n        result = model.forward(data)\n        np.testing.assert_almost_equal(result, data.sum(axis=0), decimal=5)\n\n    def test_sum_2D(self):\n        data = np.random.randn(2, 3)\n        parameter = Parameter(shape=(2, 3), init_weight=data)\n        out = autograd.sum(parameter, axis=0)\n        model = Model(input=parameter, output=out)\n        result = model.forward(data)\n        np.testing.assert_almost_equal(result, data.sum(axis=0), decimal=5)\n'"
pyzoo/test/zoo/pipeline/estimator/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/pipeline/estimator/test_estimator.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nfrom pyspark.ml import Pipeline\n\nfrom zoo.pipeline.estimator import *\n\nfrom bigdl.nn.layer import Sequential, View, Linear, LogSoftMax, SpatialConvolution\nfrom bigdl.nn.criterion import ClassNLLCriterion\nfrom bigdl.optim.optimizer import *\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.feature.common import FeatureSet\nfrom zoo import init_nncontext, init_spark_conf\nimport zoo.common\n\n\nclass TestEstimator(ZooTestCase):\n    def setup_method(self, method):\n        """""" setup any state tied to the execution of the given method in a\n        class.  setup_method is invoked for every test method of a class.\n        """"""\n        sparkConf = init_spark_conf().setMaster(""local[1]"").setAppName(""testEstimator"")\n        self.sc = init_nncontext(sparkConf)\n        assert (self.sc.appName == ""testEstimator"")\n\n    def teardown_method(self, method):\n        """""" teardown any state that was previously setup with a setup_method\n        call.\n        """"""\n        self.sc.stop()\n\n    @staticmethod\n    def _create_cnn_model():\n        model = Sequential()\n        model.add(SpatialConvolution(3, 1, 5, 5))\n        model.add(View([1 * 220 * 220]))\n        model.add(Linear(1 * 220 * 220, 20))\n        model.add(LogSoftMax())\n        return model\n\n    @staticmethod\n    def _generate_image_data(data_num, img_shape):\n        images = []\n        labels = []\n        for i in range(0, data_num):\n            features = np.random.uniform(0, 1, img_shape)\n            label = np.array([2])\n            images.append(features)\n            labels.append(label)\n        return images, labels\n\n    def test_estimator_train_imagefeature(self):\n        batch_size = 8\n        epoch_num = 5\n        images, labels = TestEstimator._generate_image_data(data_num=8, img_shape=(200, 200, 3))\n\n        image_frame = DistributedImageFrame(self.sc.parallelize(images),\n                                            self.sc.parallelize(labels))\n\n        transformer = Pipeline([BytesToMat(), Resize(256, 256), CenterCrop(224, 224),\n                                ChannelNormalize(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n                                MatToTensor(), ImageFrameToSample(target_keys=[\'label\'])])\n        data_set = FeatureSet.image_frame(image_frame).transform(transformer)\n\n        model = TestEstimator._create_cnn_model()\n\n        optim_method = SGD(learningrate=0.01)\n\n        estimator = Estimator(model, optim_method, """")\n        estimator.set_constant_gradient_clipping(0.1, 1.2)\n        estimator.train_imagefeature(train_set=data_set, criterion=ClassNLLCriterion(),\n                                     end_trigger=MaxEpoch(epoch_num),\n                                     checkpoint_trigger=EveryEpoch(),\n                                     validation_set=data_set,\n                                     validation_method=[Top1Accuracy()],\n                                     batch_size=batch_size)\n        predict_result = model.predict_image(image_frame.transform(transformer))\n        assert (predict_result.get_predict().count(), 8)\n\n    def test_estimator_train(self):\n        batch_size = 8\n        epoch_num = 5\n\n        images, labels = TestEstimator._generate_image_data(data_num=8, img_shape=(3, 224, 224))\n\n        image_rdd = self.sc.parallelize(images)\n        labels = self.sc.parallelize(labels)\n\n        sample_rdd = image_rdd.zip(labels).map(\n            lambda img_label: zoo.common.Sample.from_ndarray(img_label[0], img_label[1]))\n\n        data_set = FeatureSet.sample_rdd(sample_rdd)\n\n        model = TestEstimator._create_cnn_model()\n\n        optim_method = SGD(learningrate=0.01)\n\n        estimator = Estimator(model, optim_method, """")\n        estimator.set_constant_gradient_clipping(0.1, 1.2)\n        estimator.train(train_set=data_set, criterion=ClassNLLCriterion(),\n                        end_trigger=MaxEpoch(epoch_num),\n                        checkpoint_trigger=EveryEpoch(),\n                        validation_set=data_set,\n                        validation_method=[Top1Accuracy()],\n                        batch_size=batch_size)\n        predict_result = model.predict(sample_rdd)\n        assert (predict_result.count(), 8)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/pipeline/inference/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/pipeline/inference/test_inference_model.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport pytest\nimport numpy as np\n\nfrom bigdl.dataset.base import maybe_download\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nfrom zoo.pipeline.inference import InferenceModel\n\nimport tarfile\n\nnp.random.seed(1337)  # for reproducibility\n\nresource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\nproperty_path = os.path.join(os.path.split(__file__)[0],\n                             ""../../../../../zoo/target/classes/app.properties"")\ndata_url = ""http://download.tensorflow.org""\ns3_url = ""https://s3-ap-southeast-1.amazonaws.com""\nwith open(property_path) as f:\n    for _ in range(2):  # skip the first two lines\n        next(f)\n    for line in f:\n        if ""data-store-url"" in line:\n            line = line.strip()\n            data_url = line.split(""="")[1].replace(""\\\\"", """")\n\n\nclass TestInferenceModel(ZooTestCase):\n\n    def test_load_bigdl(self):\n        model = InferenceModel(3)\n        model.load_bigdl(os.path.join(resource_path, ""models/bigdl/bigdl_lenet.model""))\n        input_data = np.random.random([4, 28, 28, 1])\n        output_data = model.predict(input_data)\n\n    def test_load_caffe(self):\n        model = InferenceModel(10)\n        model.load_caffe(os.path.join(resource_path, ""models/caffe/test_persist.prototxt""),\n                         os.path.join(resource_path, ""models/caffe/test_persist.caffemodel""))\n        input_data = np.random.random([4, 3, 8, 8])\n        output_data = model.predict(input_data)\n\n    def test_load_openvino(self):\n        local_path = self.create_temp_dir()\n        model = InferenceModel(1)\n        model_url = s3_url + ""/analytics-zoo-models/openvino/2018_R5/resnet_v1_50.xml""\n        weight_url = s3_url + ""/analytics-zoo-models/openvino/2018_R5/resnet_v1_50.bin""\n        model_path = maybe_download(""resnet_v1_50.xml"",\n                                    local_path, model_url)\n        weight_path = maybe_download(""resnet_v1_50.bin"",\n                                     local_path, weight_url)\n        model.load_openvino(model_path, weight_path)\n        input_data = np.random.random([4, 1, 224, 224, 3])\n        model.predict(input_data)\n\n    def test_load_tf_openvino_od(self):\n        local_path = self.create_temp_dir()\n        url = data_url + ""/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz""\n        file_abs_path = maybe_download(""faster_rcnn_resnet101_coco_2018_01_28.tar.gz"",\n                                       local_path, url)\n        tar = tarfile.open(file_abs_path, ""r:gz"")\n        extracted_to = os.path.join(local_path, ""faster_rcnn_resnet101_coco_2018_01_28"")\n        if not os.path.exists(extracted_to):\n            print(""Extracting %s to %s"" % (file_abs_path, extracted_to))\n            tar.extractall(local_path)\n            tar.close()\n        model = InferenceModel(3)\n        model.load_tf(model_path=extracted_to + ""/frozen_inference_graph.pb"",\n                      backend=""openvino"",\n                      model_type=""faster_rcnn_resnet101_coco"",\n                      ov_pipeline_config_path=extracted_to + ""/pipeline.config"",\n                      ov_extensions_config_path=None)\n        input_data = np.random.random([4, 1, 3, 600, 600])\n        output_data = model.predict(input_data)\n        model2 = InferenceModel(3)\n        model2.load_tf_object_detection_as_openvino(\n            model_path=extracted_to + ""/frozen_inference_graph.pb"",\n            object_detection_model_type=""faster_rcnn_resnet101_coco"",\n            pipeline_config_path=extracted_to + ""/pipeline.config"",\n            extensions_config_path=None)\n        model2.predict(input_data)\n\n    # def test_load_tf_openvino_ic(self):\n    #     local_path = self.create_temp_dir()\n    #     print(local_path)\n    #     url = data_url + ""/models/resnet_v1_50_2016_08_28.tar.gz""\n    #     file_abs_path = maybe_download(""resnet_v1_50_2016_08_28.tar.gz"", local_path, url)\n    #     tar = tarfile.open(file_abs_path, ""r:gz"")\n    #     print(""Extracting %s to %s"" % (file_abs_path, local_path))\n    #     tar.extractall(local_path)\n    #     tar.close()\n    #     model = InferenceModel(3)\n    #     model.load_tf_image_classification_as_openvino(\n    #         model_path=None,\n    #         image_classification_model_type=""resnet_v1_50"",\n    #         checkpoint_path=local_path + ""/resnet_v1_50.ckpt"",\n    #         input_shape=[4, 224, 224, 3],\n    #         if_reverse_input_channels=True,\n    #         mean_values=[123.68, 116.78, 103.94],\n    #         scale=1)\n    #     print(model)\n    #     input_data = np.random.random([4, 1, 224, 224, 3])\n    #     model.predict(input_data)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/pipeline/nnframes/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/pipeline/nnframes/test_nn_classifier.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nimport shutil\nimport errno\nimport string\nfrom bigdl.nn.criterion import *\nfrom bigdl.nn.layer import *\nfrom bigdl.optim.optimizer import *\nfrom numpy.testing import assert_allclose\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.sql.types import *\n\nfrom zoo.common.nncontext import *\nfrom zoo.pipeline.nnframes import *\nfrom zoo.pipeline.api.keras.optimizers import Adam as KAdam\nfrom zoo.pipeline.api.keras import layers as ZLayer\nfrom zoo.pipeline.api.keras.models import Model as ZModel\nfrom zoo.feature.common import *\nfrom zoo.feature.image import *\nfrom zoo.util.tf import *\n\n\nclass TestNNClassifer():\n    def setup_method(self, method):\n        """""" setup any state tied to the execution of the given method in a\n        class.  setup_method is invoked for every test method of a class.\n        """"""\n        sparkConf = init_spark_conf().setMaster(""local[1]"").setAppName(""testNNClassifer"")\n        self.sc = init_nncontext(sparkConf)\n        self.sqlContext = SQLContext(self.sc)\n        assert(self.sc.appName == ""testNNClassifer"")\n\n    def teardown_method(self, method):\n        """""" teardown any state that was previously setup with a setup_method\n        call.\n        """"""\n        self.sc.stop()\n\n    def get_estimator_df(self):\n        data = self.sc.parallelize([\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0)),\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0))])\n\n        schema = StructType([\n            StructField(""features"", ArrayType(DoubleType(), False), False),\n            StructField(""label"", ArrayType(DoubleType(), False), False)])\n        df = self.sqlContext.createDataFrame(data, schema)\n        return df\n\n    def get_classifier_df(self):\n        data = self.sc.parallelize([\n            ((2.0, 1.0), 1.0),\n            ((1.0, 2.0), 2.0),\n            ((2.0, 1.0), 1.0),\n            ((1.0, 2.0), 2.0)])\n\n        schema = StructType([\n            StructField(""features"", ArrayType(DoubleType(), False), False),\n            StructField(""label"", DoubleType(), False)])\n        df = self.sqlContext.createDataFrame(data, schema)\n        return df\n\n    def test_nnEstimator_construct_with_differnt_params(self):\n        linear_model = Sequential().add(Linear(2, 2))\n        mse_criterion = MSECriterion()\n        df = self.get_estimator_df()\n        for e in [NNEstimator(linear_model, mse_criterion),\n                  NNEstimator(linear_model, mse_criterion, [2], [2]),\n                  NNEstimator(linear_model, mse_criterion, SeqToTensor([2]), SeqToTensor([2]))]:\n            nnModel = e.setBatchSize(4).setMaxEpoch(1).fit(df)\n            res = nnModel.transform(df)\n            assert type(res).__name__ == \'DataFrame\'\n\n    def test_nnClassifier_construct_with_differnt_params(self):\n        linear_model = Sequential().add(Linear(2, 2))\n        mse_criterion = MSECriterion()\n        df = self.get_classifier_df()\n        for e in [NNClassifier(linear_model, mse_criterion),\n                  NNClassifier(linear_model, mse_criterion, [2]),\n                  NNClassifier(linear_model, mse_criterion, SeqToTensor([2]))]:\n            nnModel = e.setBatchSize(4).setMaxEpoch(1).fit(df)\n            res = nnModel.transform(df)\n            assert type(res).__name__ == \'DataFrame\'\n\n    def test_nnModel_construct_with_differnt_params(self):\n        linear_model = Sequential().add(Linear(2, 2))\n        df = self.get_estimator_df()\n        for e in [NNModel(linear_model),\n                  NNModel(linear_model, [2]),\n                  NNModel(linear_model, SeqToTensor([2]))]:\n            res = e.transform(df)\n            assert type(res).__name__ == \'DataFrame\'\n            assert e.getBatchSize() == 4\n\n    def test_nnClassiferModel_construct_with_differnt_params(self):\n        linear_model = Sequential().add(Linear(2, 2))\n        df = self.get_classifier_df()\n        for e in [NNClassifierModel(linear_model),\n                  NNClassifierModel(linear_model, [2]),\n                  NNClassifierModel(linear_model, SeqToTensor([2]))]:\n            res = e.transform(df)\n            assert type(res).__name__ == \'DataFrame\'\n            assert e.getBatchSize() == 4\n\n    def test_all_set_get_methods(self):\n        linear_model = Sequential().add(Linear(2, 2))\n        mse_criterion = MSECriterion()\n\n        estimator = NNEstimator(linear_model, mse_criterion, SeqToTensor([2]), SeqToTensor([2]))\n        assert estimator.setBatchSize(30).getBatchSize() == 30\n        assert estimator.setMaxEpoch(40).getMaxEpoch() == 40\n        assert estimator.setLearningRate(1e-4).getLearningRate() == 1e-4\n        assert estimator.setFeaturesCol(""abcd"").getFeaturesCol() == ""abcd""\n        assert estimator.setLabelCol(""xyz"").getLabelCol() == ""xyz""\n        assert isinstance(estimator.setOptimMethod(Adam()).getOptimMethod(), Adam)\n\n        nn_model = NNModel(linear_model, SeqToTensor([2]))\n        assert nn_model.setBatchSize(20).getBatchSize() == 20\n\n        linear_model = Sequential().add(Linear(2, 2))\n        classNLL_criterion = ClassNLLCriterion()\n        classifier = NNClassifier(linear_model, classNLL_criterion, SeqToTensor([2]))\n        assert classifier.setBatchSize(20).getBatchSize() == 20\n        assert classifier.setMaxEpoch(50).getMaxEpoch() == 50\n        assert classifier.setLearningRate(1e-5).getLearningRate() == 1e-5\n        assert classifier.setLearningRateDecay(1e-9).getLearningRateDecay() == 1e-9\n        assert classifier.setCachingSample(False).isCachingSample() is False\n\n        nn_classifier_model = NNClassifierModel(linear_model, SeqToTensor([2]))\n        assert nn_classifier_model.setBatchSize((20)).getBatchSize() == 20\n\n    def test_nnEstimator_fit_nnmodel_transform(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        estimator = NNEstimator(model, criterion, SeqToTensor([2]), ArrayToTensor([2]))\\\n            .setBatchSize(4).setLearningRate(0.2).setMaxEpoch(40)\n\n        df = self.get_estimator_df()\n        nnModel = estimator.fit(df)\n        assert nnModel.getBatchSize() == 4\n\n        res = nnModel.transform(df)\n        assert type(res).__name__ == \'DataFrame\'\n        res.registerTempTable(""nnModelDF"")  # Compatible with spark 1.6\n        results = self.sqlContext.table(""nnModelDF"")\n\n        count = results.rdd.count()\n        data = results.rdd.collect()\n\n        for i in range(count):\n            row_label = data[i][1]\n            row_prediction = data[i][2]\n            assert_allclose(row_label[0], row_prediction[0], atol=0, rtol=1e-1)\n            assert_allclose(row_label[1], row_prediction[1], atol=0, rtol=1e-1)\n\n    def test_nnEstimator_fit_gradient_clipping(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        estimator = NNEstimator(model, criterion, SeqToTensor([2]), ArrayToTensor([2])) \\\n            .setBatchSize(4).setLearningRate(0.2).setMaxEpoch(2)\\\n            .setConstantGradientClipping(0.1, 0.2)\n\n        df = self.get_estimator_df()\n        estimator.fit(df)\n        estimator.clearGradientClipping()\n        estimator.fit(df)\n        estimator.setGradientClippingByL2Norm(1.2)\n        estimator.fit(df)\n\n    def test_nnEstimator_fit_with_Cache_Disk(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        estimator = NNEstimator(model, criterion, SeqToTensor([2]), ArrayToTensor([2])) \\\n            .setBatchSize(1).setLearningRate(0.2).setMaxEpoch(2) \\\n            .setDataCacheLevel(""DISK_AND_DRAM"", 2)\n\n        data = self.sc.parallelize([\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0)),\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0)),\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0)),\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0))])\n\n        schema = StructType([\n            StructField(""features"", ArrayType(DoubleType(), False), False),\n            StructField(""label"", ArrayType(DoubleType(), False), False)])\n        df = self.sqlContext.createDataFrame(data, schema)\n        estimator.fit(df)\n\n    def test_nnEstimator_fit_with_non_default_featureCol(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        estimator = NNEstimator(model, criterion, SeqToTensor([2]), SeqToTensor([2]))\\\n            .setBatchSize(4)\\\n            .setLearningRate(0.01).setMaxEpoch(1) \\\n            .setFeaturesCol(""abcd"").setLabelCol(""xyz"").setPredictionCol(""tt"")\n\n        data = self.sc.parallelize([\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0)),\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0))])\n\n        schema = StructType([\n            StructField(""abcd"", ArrayType(DoubleType(), False), False),\n            StructField(""xyz"", ArrayType(DoubleType(), False), False)])\n        df = self.sqlContext.createDataFrame(data, schema)\n        nnModel = estimator.fit(df)\n\n        res = nnModel.transform(df)\n        assert type(res).__name__ == \'DataFrame\'\n        assert res.select(""abcd"", ""xyz"", ""tt"").count() == 4\n\n    def test_nnEstimator_fit_with_different_OptimMethods(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        estimator = NNEstimator(model, criterion, SeqToTensor([2]), SeqToTensor([2]))\\\n            .setBatchSize(4)\\\n            .setLearningRate(0.01).setMaxEpoch(1) \\\n            .setPredictionCol(""tt"")\n\n        df = self.get_estimator_df()\n        for opt in [SGD(learningrate=1e-3, learningrate_decay=0.0,),\n                    Adam(), LBFGS(), Adagrad(), Adadelta()]:\n            nnModel = estimator.setOptimMethod(opt).fit(df)\n            res = nnModel.transform(df)\n            assert type(res).__name__ == \'DataFrame\'\n            assert res.select(""features"", ""label"", ""tt"").count() == 4\n\n    def test_nnEstimator_fit_with_adam_lr_schedile(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        df = self.get_estimator_df()\n        nnModel = NNEstimator(model, criterion, SeqToTensor([2]), SeqToTensor([2])) \\\n            .setBatchSize(4) \\\n            .setLearningRate(0.01).setMaxEpoch(1) \\\n            .setPredictionCol(""tt"") \\\n            .setOptimMethod(KAdam(\n                schedule=Plateau(""Loss"", factor=0.1, patience=2, mode=""min"", epsilon=0.01,\n                                 cooldown=0, min_lr=1e-15))) \\\n            .fit(df)\n        res = nnModel.transform(df)\n        assert type(res).__name__ == \'DataFrame\'\n\n    def test_nnEstimator_create_with_feature_size(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        estimator = NNEstimator(model, criterion, [2], [2])\\\n            .setBatchSize(4).setLearningRate(0.2).setMaxEpoch(1)\n\n        df = self.get_estimator_df()\n        nnModel = estimator.fit(df)\n        assert nnModel.getBatchSize() == 4\n\n    def test_nnEstimator_fit_with_train_val_summary(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        data = self.sc.parallelize([\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0)),\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0))])\n        val_data = self.sc.parallelize([\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0))])\n        schema = StructType([\n            StructField(""features"", ArrayType(DoubleType(), False), False),\n            StructField(""label"", ArrayType(DoubleType(), False), False)])\n        df = self.sqlContext.createDataFrame(data, schema)\n        val_df = self.sqlContext.createDataFrame(val_data, schema)\n\n        tmp_dir = tempfile.mkdtemp()\n        train_summary = TrainSummary(log_dir=tmp_dir, app_name=""estTest"")\n        train_summary.set_summary_trigger(""LearningRate"", SeveralIteration(1))\n        val_summary = ValidationSummary(log_dir=tmp_dir, app_name=""estTest"")\n        estimator = NNEstimator(model, criterion, SeqToTensor([2]), SeqToTensor([2]))\\\n            .setBatchSize(4) \\\n            .setMaxEpoch(5) \\\n            .setTrainSummary(train_summary)\n        assert (estimator.getValidation() is None)\n        estimator.setValidation(EveryEpoch(), val_df, [MAE()], 2) \\\n            .setValidationSummary(val_summary)\n        assert (estimator.getValidation() is not None)\n\n        nnModel = estimator.fit(df)\n        res = nnModel.transform(df)\n        lr_result = train_summary.read_scalar(""LearningRate"")\n        mae_result = val_summary.read_scalar(""MAE"")\n        assert isinstance(estimator.getTrainSummary(), TrainSummary)\n        assert type(res).__name__ == \'DataFrame\'\n        assert len(lr_result) == 5\n        assert len(mae_result) == 4\n\n    def test_NNEstimator_checkpoint(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        df = self.get_estimator_df()\n        try:\n            tmp_dir = tempfile.mkdtemp()\n            estimator = NNEstimator(model, criterion).setMaxEpoch(5)\\\n                .setBatchSize(4)\\\n                .setCheckpoint(tmp_dir, EveryEpoch(), False)\n\n            checkpoint_config = estimator.getCheckpoint()\n            assert checkpoint_config[0] == tmp_dir\n            assert ""EveryEpoch"" in str(checkpoint_config)\n            assert checkpoint_config[2] is False\n\n            estimator.fit(df)\n            assert len(os.listdir(tmp_dir)) > 0\n        finally:\n            try:\n                shutil.rmtree(tmp_dir)  # delete directory\n            except OSError as exc:\n                if exc.errno != errno.ENOENT:  # ENOENT - no such file or directory\n                    raise  # re-raise exception\n\n    def test_NNEstimator_multi_input(self):\n        zx1 = ZLayer.Input(shape=(1, ))\n        zx2 = ZLayer.Input(shape=(1, ))\n        zz = ZLayer.merge([zx1, zx2], mode=""concat"")\n        zy = ZLayer.Dense(2)(zz)\n        zmodel = ZModel([zx1, zx2], zy)\n\n        criterion = MSECriterion()\n        df = self.get_estimator_df()\n        estimator = NNEstimator(zmodel, criterion, [[1], [1]]).setMaxEpoch(5) \\\n            .setBatchSize(4)\n        nnmodel = estimator.fit(df)\n        nnmodel.transform(df).collect()\n\n    def test_NNEstimator_works_with_VectorAssembler_multi_input(self):\n        if self.sc.version.startswith(""2""):\n            from pyspark.ml.linalg import Vectors\n            from pyspark.ml.feature import VectorAssembler\n            from pyspark.sql import SparkSession\n\n            spark = SparkSession \\\n                .builder \\\n                .getOrCreate()\n\n            df = spark.createDataFrame(\n                [(1, 35, 109.0, Vectors.dense([2.0, 5.0, 0.5, 0.5]), 1.0),\n                 (2, 58, 2998.0, Vectors.dense([4.0, 10.0, 0.5, 0.5]), 2.0),\n                 (3, 18, 123.0, Vectors.dense([3.0, 15.0, 0.5, 0.5]), 1.0)],\n                [""user"", ""age"", ""income"", ""history"", ""label""])\n\n            assembler = VectorAssembler(\n                inputCols=[""user"", ""age"", ""income"", ""history""],\n                outputCol=""features"")\n\n            df = assembler.transform(df)\n\n            x1 = ZLayer.Input(shape=(1,))\n            x2 = ZLayer.Input(shape=(2,))\n            x3 = ZLayer.Input(shape=(2, 2,))\n\n            user_embedding = ZLayer.Embedding(5, 10)(x1)\n            flatten = ZLayer.Flatten()(user_embedding)\n            dense1 = ZLayer.Dense(2)(x2)\n            gru = ZLayer.LSTM(4, input_shape=(2, 2))(x3)\n\n            merged = ZLayer.merge([flatten, dense1, gru], mode=""concat"")\n            zy = ZLayer.Dense(2)(merged)\n\n            zmodel = ZModel([x1, x2, x3], zy)\n            criterion = ClassNLLCriterion()\n            classifier = NNClassifier(zmodel, criterion, [[1], [2], [2, 2]]) \\\n                .setOptimMethod(Adam()) \\\n                .setLearningRate(0.1) \\\n                .setBatchSize(2) \\\n                .setMaxEpoch(10)\n\n            nnClassifierModel = classifier.fit(df)\n            print(nnClassifierModel.getBatchSize())\n            res = nnClassifierModel.transform(df).collect()\n\n    def test_NNModel_transform_with_nonDefault_featureCol(self):\n        model = Sequential().add(Linear(2, 2))\n        nnModel = NNModel(model, SeqToTensor([2]))\\\n            .setFeaturesCol(""abcd"").setPredictionCol(""dcba"")\n\n        data = self.sc.parallelize([\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0)),\n            ((2.0, 1.0), (1.0, 2.0)),\n            ((1.0, 2.0), (2.0, 1.0))])\n\n        schema = StructType([\n            StructField(""abcd"", ArrayType(DoubleType(), False), False),\n            StructField(""xyz"", ArrayType(DoubleType(), False), False)])\n        df = self.sqlContext.createDataFrame(data, schema)\n        res = nnModel.transform(df)\n        assert type(res).__name__ == \'DataFrame\'\n        assert res.select(""abcd"", ""dcba"").count() == 4\n\n    def test_nnModel_set_Preprocessing(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        estimator = NNEstimator(model, criterion, [2], [2])\\\n            .setBatchSize(4).setLearningRate(0.2).setMaxEpoch(1)\n\n        df = self.get_estimator_df()\n        nnModel = estimator.fit(df)\n        newTransformer = ChainedPreprocessing([SeqToTensor([2]), TensorToSample()])\n        nnModel.setSamplePreprocessing(newTransformer)\n\n        res = nnModel.transform(df)\n        assert type(res).__name__ == \'DataFrame\'\n        assert res.count() == 4\n\n    def test_NNModel_save_load_BigDL_model(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        estimator = NNEstimator(model, criterion).setMaxEpoch(1).setBatchSize(4)\n\n        df = self.get_estimator_df()\n        nnModel = estimator.fit(df)\n        try:\n            tmp_dir = tempfile.mkdtemp()\n            modelPath = os.path.join(tmp_dir, ""model"")\n            nnModel.model.save(modelPath)\n            loaded_model = Model.load(modelPath)\n            resultDF = NNModel(loaded_model).transform(df)\n            assert resultDF.count() == 4\n        finally:\n            try:\n                shutil.rmtree(tmp_dir)  # delete directory\n            except OSError as exc:\n                if exc.errno != errno.ENOENT:  # ENOENT - no such file or directory\n                    raise  # re-raise exception\n\n    def test_NNModel_save_load(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        estimator = NNEstimator(model, criterion).setMaxEpoch(1).setBatchSize(4)\n\n        df = self.get_estimator_df()\n        nnModel = estimator.fit(df)\n        try:\n            tmp_dir = tempfile.mkdtemp()\n            modelPath = os.path.join(tmp_dir, ""model"")\n            nnModel.save(modelPath)\n            loaded_model = NNModel.load(modelPath)\n            assert loaded_model.transform(df).count() == 4\n        finally:\n            try:\n                shutil.rmtree(tmp_dir)  # delete directory\n            except OSError as exc:\n                if exc.errno != errno.ENOENT:  # ENOENT - no such file or directory\n                    raise  # re-raise exception\n\n    def test_nnclassifier_fit_nnclassifiermodel_transform(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = ClassNLLCriterion()\n        classifier = NNClassifier(model, criterion, SeqToTensor([2])) \\\n            .setBatchSize(4) \\\n            .setLearningRate(0.2).setMaxEpoch(40)\n\n        df = self.get_classifier_df()\n        nnClassifierModel = classifier.fit(df)\n        assert(isinstance(nnClassifierModel, NNClassifierModel))\n        res = nnClassifierModel.transform(df)\n        assert type(res).__name__ == \'DataFrame\'\n        res.registerTempTable(""nnClassifierModelDF"")\n        results = self.sqlContext.table(""nnClassifierModelDF"")\n\n        count = results.rdd.count()\n        data = results.rdd.collect()\n\n        for i in range(count):\n            row_label = data[i][1]\n            row_prediction = data[i][2]\n            assert row_label == row_prediction\n\n    def test_nnclassifier_fit_with_Sigmoid(self):\n        model = Sequential().add(Linear(2, 1)).add(Sigmoid())\n        criterion = BCECriterion()\n        classifier = NNClassifier(model, criterion, SeqToTensor([2])) \\\n            .setBatchSize(4) \\\n            .setLearningRate(0.2).setMaxEpoch(40)\n\n        data = self.sc.parallelize([\n            ((2.0, 1.0), 0.0),\n            ((1.0, 2.0), 1.0),\n            ((2.0, 1.0), 0.0),\n            ((1.0, 2.0), 1.0)])\n\n        schema = StructType([\n            StructField(""features"", ArrayType(DoubleType(), False), False),\n            StructField(""label"", DoubleType(), False)])\n        df = self.sqlContext.createDataFrame(data, schema)\n        nnClassifierModel = classifier.fit(df)\n        assert(isinstance(nnClassifierModel, NNClassifierModel))\n        res = nnClassifierModel.transform(df)\n        res.registerTempTable(""nnClassifierModelDF"")\n        results = self.sqlContext.table(""nnClassifierModelDF"")\n\n        count = results.rdd.count()\n        data = results.rdd.collect()\n\n        for i in range(count):\n            row_label = data[i][1]\n            row_prediction = data[i][2]\n            assert row_label == row_prediction\n\n    def test_nnclassifierModel_set_Preprocessing(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = ClassNLLCriterion()\n        classifier = NNClassifier(model, criterion, SeqToTensor([2])) \\\n            .setBatchSize(4) \\\n            .setLearningRate(0.2).setMaxEpoch(1)\n\n        df = self.get_classifier_df()\n        nnClassifierModel = classifier.fit(df)\n\n        newTransformer = ChainedPreprocessing([SeqToTensor([2]), TensorToSample()])\n        nnClassifierModel.setSamplePreprocessing(newTransformer)\n\n        res = nnClassifierModel.transform(df)\n        assert type(res).__name__ == \'DataFrame\'\n        assert res.count() == 4\n\n    def test_nnclassifier_create_with_size_fit_transform(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = ClassNLLCriterion()\n        classifier = NNClassifier(model, criterion, [2]) \\\n            .setBatchSize(4) \\\n            .setLearningRate(0.2).setMaxEpoch(40)\n\n        df = self.get_classifier_df()\n        nnClassifierModel = classifier.fit(df)\n\n        res = nnClassifierModel.transform(df)\n        assert type(res).__name__ == \'DataFrame\'\n\n    def test_nnclassifier_fit_different_optimMethods(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = ClassNLLCriterion()\n        classifier = NNClassifier(model, criterion, SeqToTensor([2]))\\\n            .setBatchSize(4) \\\n            .setLearningRate(0.2).setMaxEpoch(1)\n\n        df = self.get_classifier_df()\n        for opt in [Adam(), SGD(learningrate=1e-2, learningrate_decay=1e-6,),\n                    LBFGS(), Adagrad(), Adadelta()]:\n            nnClassifierModel = classifier.setOptimMethod(opt).fit(df)\n            res = nnClassifierModel.transform(df)\n            res.collect()\n            assert type(res).__name__ == \'DataFrame\'\n\n    def test_nnClassifier_fit_with_train_val_summary(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        data = self.sc.parallelize([\n            ((2.0, 1.0), 1.0),\n            ((1.0, 2.0), 2.0),\n            ((2.0, 1.0), 1.0),\n            ((1.0, 2.0), 2.0)])\n\n        val_data = self.sc.parallelize([\n            ((2.0, 1.0), 1.0),\n            ((1.0, 2.0), 2.0)])\n\n        schema = StructType([\n            StructField(""features"", ArrayType(DoubleType(), False), False),\n            StructField(""label"", DoubleType(), False)])\n        df = self.sqlContext.createDataFrame(data, schema)\n        val_df = self.sqlContext.createDataFrame(val_data, schema)\n\n        tmp_dir = tempfile.mkdtemp()\n        train_summary = TrainSummary(log_dir=tmp_dir, app_name=""nnTest"")\n        train_summary.set_summary_trigger(""LearningRate"", SeveralIteration(1))\n        val_summary = ValidationSummary(log_dir=tmp_dir, app_name=""nnTest"")\n\n        classfier = NNClassifier(model, criterion, SeqToTensor([2]))\\\n            .setBatchSize(4) \\\n            .setTrainSummary(train_summary).setMaxEpoch(5) \\\n            .setValidation(EveryEpoch(), val_df, [Top1Accuracy()], 2) \\\n            .setValidationSummary(val_summary)\n\n        nnModel = classfier.fit(df)\n        res = nnModel.transform(df)\n        lr_result = train_summary.read_scalar(""LearningRate"")\n        top1_result = val_summary.read_scalar(""Top1Accuracy"")\n\n        assert isinstance(classfier.getTrainSummary(), TrainSummary)\n        assert type(res).__name__ == \'DataFrame\'\n        assert len(lr_result) == 5\n        assert len(top1_result) == 4\n\n    def test_nnclassifier_in_pipeline(self):\n\n        if self.sc.version.startswith(""1""):\n            from pyspark.mllib.linalg import Vectors\n\n            df = self.sqlContext.createDataFrame(\n                [(Vectors.dense([2.0, 1.0]), 1.0),\n                 (Vectors.dense([1.0, 2.0]), 2.0),\n                 (Vectors.dense([2.0, 1.0]), 1.0),\n                 (Vectors.dense([1.0, 2.0]), 2.0),\n                 ], [""features"", ""label""])\n\n            scaler = MinMaxScaler().setInputCol(""features"").setOutputCol(""scaled"")\n            model = Sequential().add(Linear(2, 2))\n            criterion = ClassNLLCriterion()\n            classifier = NNClassifier(model, criterion)\\\n                .setBatchSize(4) \\\n                .setLearningRate(0.01).setMaxEpoch(1).setFeaturesCol(""scaled"")\n\n            pipeline = Pipeline(stages=[scaler, classifier])\n\n            pipelineModel = pipeline.fit(df)\n\n            res = pipelineModel.transform(df)\n            assert type(res).__name__ == \'DataFrame\'\n        # TODO: Add test for ML Vector once infra is ready.\n\n    def test_NNClassifierModel_save_load_BigDL_model(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = MSECriterion()\n        classifier = NNClassifier(model, criterion).setMaxEpoch(1).setBatchSize(4)\n\n        df = self.get_classifier_df()\n        nnClassifierModel = classifier.fit(df)\n        try:\n            tmp_dir = tempfile.mkdtemp()\n            modelPath = os.path.join(tmp_dir, ""model"")\n            nnClassifierModel.model.save(modelPath)\n            loaded_model = Model.load(modelPath)\n            resultDF = NNClassifierModel(loaded_model).transform(df)\n            assert resultDF.count() == 4\n        finally:\n            try:\n                shutil.rmtree(tmp_dir)  # delete directory\n            except OSError as exc:\n                if exc.errno != errno.ENOENT:  # ENOENT - no such file or directory\n                    raise  # re-raise exception\n\n    def test_NNClassifierModel_save_load(self):\n        model = Sequential().add(Linear(2, 2))\n        criterion = ClassNLLCriterion()\n        classifier = NNClassifier(model, criterion, [2]).setMaxEpoch(1).setBatchSize(4)\n\n        df = self.get_classifier_df()\n        nnClassifierModel = classifier.fit(df)\n        try:\n            tmp_dir = tempfile.mkdtemp()\n            modelPath = os.path.join(tmp_dir, ""model"")\n            nnClassifierModel.save(modelPath)\n            loaded_model = NNClassifierModel.load(modelPath)\n            assert (isinstance(loaded_model, NNClassifierModel))\n            assert loaded_model.transform(df).count() == 4\n        finally:\n            try:\n                shutil.rmtree(tmp_dir)  # delete directory\n            except OSError as exc:\n                if exc.errno != errno.ENOENT:  # ENOENT - no such file or directory\n                    raise  # re-raise exception\n\n    def test_input_node_of_tfnet_from_session(self):\n        import tensorflow as tff\n        input1 = tff.placeholder(dtype=tff.float32, shape=(None, 2))\n        input2 = tff.placeholder(dtype=tff.float32, shape=(None, 2))\n        hidden = tff.layers.dense(input1, 4)\n        output = tff.layers.dense(hidden, 1)\n        sess = tff.Session()\n        sess.run(tff.global_variables_initializer())\n        tmp_dir = tempfile.mkdtemp()\n        modelPath = os.path.join(tmp_dir, ""model"")\n        raised_error = False\n        try:\n            export_tf(sess, modelPath, inputs=[input1, input2], outputs=[output])\n        except ValueError as v:\n            assert (((str(v)).find((input2.name)[0:-2])) != -1)\n            raised_error = True\n        finally:\n            try:\n                shutil.rmtree(modelPath)  # delete directory\n            except OSError as exc:\n                if exc.errno != errno.ENOENT:  # ENOENT - no such file or directory\n                    raise  # re-raise exception\n\n        if not raised_error:\n            raise ValueError(""we do not find this error, test failed"")\n\n    def test_XGBClassifierModel_predict(self):\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\n        path = os.path.join(resource_path, ""xgbclassifier/"")\n        modelPath = path + ""XGBClassifer.bin""\n        filePath = path + ""test.csv""\n        model = XGBClassifierModel.loadModel(modelPath, 2)\n\n        from pyspark.sql import SparkSession\n\n        spark = SparkSession \\\n            .builder \\\n            .getOrCreate()\n        df = spark.read.csv(filePath, sep="","", inferSchema=True, header=True)\n        model.setFeaturesCol([""age"", ""gender"", ""jointime"", ""star""])\n        predict = model.transform(df)\n        predict.count()\n\n\nif __name__ == ""__main__"":\n    pytest.main()\n'"
pyzoo/test/zoo/pipeline/nnframes/test_nn_image_reader.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport pytest\nfrom bigdl.transform.vision.image import *\nfrom bigdl.util.common import *\nfrom pyspark.sql.types import *\n\nfrom zoo.common.nncontext import *\nfrom zoo.pipeline.nnframes import *\n\n\nclass TestNNImageReader():\n\n    def setup_method(self, method):\n        """"""\n        setup any state tied to the execution of the given method in a\n        class. setup_method is invoked for every test method of a class.\n        """"""\n        sparkConf = init_spark_conf().setMaster(""local[1]"").setAppName(""TestNNImageReader"")\n        self.sc = init_nncontext(sparkConf)\n        self.resource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\n\n    def teardown_method(self, method):\n        """"""\n        teardown any state that was previously setup with a setup_method\n        call.\n        """"""\n        self.sc.stop()\n\n    def test_get_pascal_image(self):\n        image_path = os.path.join(self.resource_path, ""pascal/000025.jpg"")\n        image_frame = NNImageReader.readImages(image_path, self.sc)\n        assert image_frame.count() == 1\n        assert type(image_frame).__name__ == \'DataFrame\'\n        first_row = image_frame.take(1)[0][0]\n        assert first_row[0].endswith(""pascal/000025.jpg"")\n        assert first_row[1] == 375\n        assert first_row[2] == 500\n        assert first_row[3] == 3\n        assert first_row[4] == 16\n        assert len(first_row[5]) == 562500\n\n    def test_read_image_withOriginColumn(self):\n        image_path = os.path.join(self.resource_path, ""pascal/000025.jpg"")\n        image_frame = NNImageReader.readImages(image_path, self.sc)\n        first_row = image_frame.take(1)[0][0]\n        image_origin = first_row[0]\n\n        originDF = with_origin_column(image_frame).select(""origin"")\n        origin = originDF.take(1)[0][0]\n\n        assert image_origin == origin\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/pipeline/onnx/test_model_loading.py,92,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom test.zoo.pipeline.utils.test_utils_onnx import OnnxTestCase\nfrom zoo.pipeline.api.keras.layers import *\nimport numpy as np\n\nnp.random.seed(1337)  # for reproducibility\nimport torch\nimport onnx.helper as helper\nimport onnx\nimport pytest\nfrom zoo.pipeline.api.onnx.onnx_loader import OnnxLoader\nfrom onnx import backend\nfrom onnx.backend import test\nfrom onnx.backend.test.case import node\nfrom onnx.backend.test.case.node import pool_op_common\n\n\nclass Squeeze(torch.nn.Module):\n    def __init__(self, *dim):\n        super(Squeeze, self).__init__()\n        if dim:\n            self.dim = dim[0]\n        else:\n            self.dim = -1\n\n    def forward(self, x):\n        if (self.dim >= 0):\n            return torch.squeeze(x, dim=self.dim)\n        else:\n            return torch.squeeze(x)\n\n\nclass Transpose(torch.nn.Module):\n    def __init__(self, *parameter):\n        super(Transpose, self).__init__()\n        self.dim0 = parameter[0]\n        self.dim1 = parameter[1]\n\n    def forward(self, x):\n        return torch.transpose(x, dim0=self.dim0, dim1=self.dim1)\n\n\nclass TestModelLoading(OnnxTestCase):\n    def test_onnx_conv2d(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3)\n        )\n        input_shape_with_batch = (1, 3, 224, 224)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_conv2d_2(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3),\n            torch.nn.Conv2d(in_channels=64, out_channels=4, kernel_size=3)\n        )\n        input_shape_with_batch = (1, 3, 224, 224)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def _batchnorm_test_mode(self, x, s, bias, mean, var, epsilon=1e-5):\n        dims_x = len(x.shape)\n        dim_ones = (1,) * (dims_x - 2)\n        s = s.reshape(-1, *dim_ones)\n        bias = bias.reshape(-1, *dim_ones)\n        mean = mean.reshape(-1, *dim_ones)\n        var = var.reshape(-1, *dim_ones)\n        return s * (x - mean) / np.sqrt(var + epsilon) + bias\n\n    # Momentum is always equal to 1 no matter what value we set\n    def test_onnx_batch_norm1(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(num_features=3, momentum=1, affine=False)\n        )\n        input_shape_with_batch = (1, 3, 224, 224)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch, rtol=1e-3, atol=1e-3)\n\n    # Momentum is always equal to 1 no matter what value we set\n    def test_onnx_batch_norm2(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(num_features=3, momentum=1, affine=True)\n        )\n        input_shape_with_batch = (1, 3, 224, 224)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch, rtol=1e-3, atol=1e-3)\n\n    def test_batch_norm(self):\n        x = np.array([[[[-1, 0, 1]], [[2, 3, 4]]]]).astype(np.float32).reshape((3, 2, 1, 1))\n        s = np.array([1.0, 1.0]).astype(np.float32).reshape((2, 1))\n        bias = np.array([0, 0]).astype(np.float32).reshape((2, 1))\n        mean = np.array([0, 3]).astype(np.float32).reshape((2, 1))\n        var = np.array([1, 1.5]).astype(np.float32).reshape((2, 1))\n        y = self._batchnorm_test_mode(x, s, bias, mean, var).astype(np.float32)\n\n        node = onnx.helper.make_node(\n            \'BatchNormalization\',\n            inputs=[\'x\', \'s\', \'bias\', \'mean\', \'var\'],\n            outputs=[\'y\'],\n        )\n        output = OnnxLoader.run_node(node, [x, s, bias, mean, var])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=3)\n\n    def test_conv_with_padding(self):\n        x = np.array([[[[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n                        [5., 6., 7., 8., 9.],\n                        [10., 11., 12., 13., 14.],\n                        [15., 16., 17., 18., 19.],\n                        [20., 21., 22., 23., 24.]]]]).astype(np.float32)\n        W = np.array([[[[1., 1., 1.],  # (1, 1, 3, 3) tensor for convolution weights\n                        [1., 1., 1.],\n                        [1., 1., 1.]]]]).astype(np.float32)\n\n        # Convolution with padding\n        node_with_padding = helper.make_node(\n            \'Conv\',\n            inputs=[\'x\', \'W\'],\n            outputs=[\'y\'],\n            kernel_shape=[3, 3],\n            # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n            pads=[1, 1, 1, 1],\n        )\n        y_with_padding = np.array([[[[12., 21., 27., 33., 24.],  # (1, 1, 5, 5) output tensor\n                                     [33., 54., 63., 72., 51.],\n                                     [63., 99., 108., 117., 81.],\n                                     [93., 144., 153., 162., 111.],\n                                     [72., 111., 117., 123., 84.]]]]).astype(np.float32)\n        output = OnnxLoader.run_node(node_with_padding, [x, W])\n        np.testing.assert_almost_equal(output[""y""], y_with_padding, decimal=5)\n\n    def test_conv_without_padding(self):\n        x = np.array([[[[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n                        [5., 6., 7., 8., 9.],\n                        [10., 11., 12., 13., 14.],\n                        [15., 16., 17., 18., 19.],\n                        [20., 21., 22., 23., 24.]]]]).astype(np.float32)\n        W = np.array([[[[1., 1., 1.],  # (1, 1, 3, 3) tensor for convolution weights\n                        [1., 1., 1.],\n                        [1., 1., 1.]]]]).astype(np.float32)\n        # Convolution without padding\n        node_without_padding = onnx.helper.make_node(\n            \'Conv\',\n            inputs=[\'x\', \'W\'],\n            outputs=[\'y\'],\n            kernel_shape=[3, 3],\n            # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n            pads=[0, 0, 0, 0],\n        )\n        y_without_padding = np.array([[[[54., 63., 72.],  # (1, 1, 3, 3) output tensor\n                                        [99., 108., 117.],\n                                        [144., 153., 162.]]]]).astype(np.float32)\n        output = OnnxLoader.run_node(node_without_padding, [x, W])\n        np.testing.assert_almost_equal(output[""y""], y_without_padding, decimal=5)\n\n    def test_onnx_gemm(self):\n        # TODO: Linear(bias = Flase) is mapped to Transpose + MatMul, not GEMM\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.Linear(in_features=3, out_features=4, bias=True)\n        )\n        input_shape_with_batch = (1, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_add(self):\n        class Add(torch.nn.Module):\n            def forward(self, x):\n                return x[0] + x[1]\n\n        pytorch_model = Add()\n        input_shape_with_batch = [(1, 3), (1, 3)]\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_abs(self):\n        class Abs(torch.nn.Module):\n            def forward(self, x):\n                return abs(x)\n\n        pytorch_model = Abs()\n        input_shape_with_batch = (1, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_abs(self):\n        node = onnx.helper.make_node(\n            \'Abs\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.abs(x)\n\n    def test_onnx_neg(self):\n        class Neg(torch.nn.Module):\n            def forward(self, x):\n                return -x\n\n        pytorch_model = Neg()\n        input_shape_with_batch = (1, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_neg(self):\n        node = onnx.helper.make_node(\n            \'Neg\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n\n        x = np.array([-4, 2]).astype(np.float32).reshape([2, 1])\n        y = np.negative(x)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.negative(x)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_averagepool2d(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.AvgPool2d(kernel_size=3, count_include_pad=False)\n        )\n        input_shape_with_batch = (1, 3, 224, 224)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_averagepool2d_padding(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.AvgPool2d(kernel_size=10, padding=4, count_include_pad=False)\n        )\n        input_shape_with_batch = (1, 3, 224, 224)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_relu(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.ReLU()\n        )\n        input_shape_with_batch = (1, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_relu(self):\n        node = helper.make_node(\n            \'Relu\',\n            inputs=[\'x\'],\n            outputs=[\'y\']\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, 0, np.inf)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_softmax(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.Softmax()\n        )\n        input_shape_with_batch = (1, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_softmax(self):\n        node = helper.make_node(\n            \'Softmax\',\n            inputs=[\'x\'],\n            outputs=[\'y\']\n        )\n        x = np.array([[-1, 0, 1]]).astype(np.float32)\n        # expected output [[0.09003058, 0.24472848, 0.66524094]]\n        y = np.exp(x) / np.sum(np.exp(x), axis=1)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_reshape(self):\n        original_shape = [2, 3, 4]\n        test_cases = {\n            \'reordered_dims\': np.array([4, 2, 3], dtype=np.int64),\n            \'reduced_dims\': np.array([3, 8], dtype=np.int64),\n            \'extended_dims\': np.array([3, 2, 2, 2], dtype=np.int64),\n            \'one_dim\': np.array([24], dtype=np.int64)\n            # \'negative_dim\': np.array([6, -1, 2], dtype=np.int64),\n        }\n        data = np.random.random_sample(original_shape).astype(np.float32)\n\n        for test_name, shape in test_cases.items():\n            node = onnx.helper.make_node(\n                \'Reshape\',\n                inputs=[\'data\', \'shape\'],\n                outputs=[\'reshaped\'],\n            )\n\n            output = OnnxLoader.run_node(node, [data, shape])\n            reshaped = np.reshape(data, shape)\n            np.testing.assert_almost_equal(output[""reshaped""], reshaped, decimal=5)\n\n    def test_reshape_pytorch(self):\n        class View(torch.nn.Module):\n            def __init__(self, *shape):\n                super(View, self).__init__()\n                self.shape = shape\n\n            def forward(self, input):\n                return input.view(self.shape)\n\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.Linear(20, 20),\n            View(2, 5, 4))\n        input_shape_with_batch = (2, 20)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_constant(self):\n        values = np.random.randn(5, 5).astype(np.float32)\n        node = onnx.helper.make_node(\n            \'Constant\',\n            inputs=[],\n            outputs=[\'values\'],\n            value=onnx.helper.make_tensor(\n                name=\'const_tensor\',\n                data_type=onnx.TensorProto.FLOAT,\n                dims=values.shape,\n                vals=values.flatten().astype(float),\n            ),\n        )\n        output = OnnxLoader.run_node(node, [])\n        np.testing.assert_almost_equal(output[""values""], values, decimal=5)\n\n    def test_onnx_maxpool2d(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.MaxPool2d(kernel_size=3)\n        )\n        input_shape_with_batch = (1, 3, 224, 224)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_maxpool2d_pads(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.MaxPool2d(kernel_size=3, padding=(0, 1))\n        )\n        input_shape_with_batch = (1, 3, 224, 224)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_maxpool2d_pads(self):\n        node = helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[5, 5],\n            pads=[2, 2, 2, 2]\n\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[\n            [13, 14, 15, 15, 15],\n            [18, 19, 20, 20, 20],\n            [23, 24, 25, 25, 25],\n            [23, 24, 25, 25, 25],\n            [23, 24, 25, 25, 25]]]]).astype(np.float32)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_maxpool2d_pads01(self):\n        import pytest\n        with pytest.raises(Exception) as e_info:\n            node = onnx.helper.make_node(\n                \'MaxPool\',\n                inputs=[\'x\'],\n                outputs=[\'y\'],\n                kernel_shape=[3, 3],\n                pads=[0, 0, 1, 1]\n            )\n            x = np.random.randn(1, 3, 28, 28).astype(np.float32)\n            x_shape = np.shape(x)\n            kernel_shape = (3, 3)\n            strides = (1, 1)\n            pad_top = pad_left = 0\n            pad_bottom = pad_right = 1\n            pad_shape = [pad_top + pad_bottom, pad_left + pad_right]\n            out_shape = pool_op_common.get_output_shape(\'VALID\', np.add(x_shape[2:], pad_shape),\n                                                        kernel_shape, strides)\n            padded = np.pad(x, ((0, 0), (0, 0), (pad_top, pad_bottom), (pad_left, pad_right)),\n                            mode=\'constant\', constant_values=np.nan)\n            y = pool_op_common.pool(padded, x_shape, kernel_shape, strides,\n                                    out_shape, pad_shape, \'MAX\')\n            output = OnnxLoader.run_node(node, [x])\n            np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_maxpool2d_same_upper(self):\n        node = helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[3, 3],\n            strides=[2, 2],\n            auto_pad=""SAME_UPPER""\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[[7, 9, 10],\n                        [17, 19, 20],\n                        [22, 24, 25]]]]).astype(np.float32)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_maxpool2d_strides(self):\n        node = helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2],\n            strides=[2, 2]\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[[7, 9],\n                        [17, 19]]]]).astype(np.float32)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_logsoftmax(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.LogSoftmax()\n        )\n        input_shape_with_batch = (1, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_tanh(self):\n        node = onnx.helper.make_node(\n            \'Tanh\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.tanh(x)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y)\n\n    def test_onnx_exp(self):\n        node = onnx.helper.make_node(\n            \'Exp\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.exp(x)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_flatten(self):\n        node = onnx.helper.make_node(\n            \'Flatten\',\n            inputs=[\'a\'],\n            outputs=[\'b\'],\n        )\n        shape = (5, 4, 3, 2)\n        a = np.random.random_sample(shape).astype(np.float32)\n        new_shape = (5, 24)\n        b = np.reshape(a, new_shape)\n        output = OnnxLoader.run_node(node, [a])\n        np.testing.assert_almost_equal(output[""b""], b, decimal=5)\n\n    def test_onnx_sqrt(self):\n        node = onnx.helper.make_node(\n            \'Sqrt\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n        x = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\n        y = np.sqrt(x)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_log(self):\n        node = onnx.helper.make_node(\n            \'Log\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n        x = np.exp(np.random.randn(3, 4, 5).astype(np.float32))\n        y = np.log(x)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_hardsigmoid(self):\n        default_alpha = 0.2\n        default_beta = 0.5\n        node = onnx.helper.make_node(\n            \'HardSigmoid\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x * default_alpha + default_beta, 0, 1)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_matmul_2d(self):\n        node = onnx.helper.make_node(\n            \'MatMul\',\n            inputs=[\'a\', \'b\'],\n            outputs=[\'c\'],\n        )\n\n        # 2d\n        a = np.random.randn(3, 4).astype(np.float32).reshape((3, 4))\n        b = np.random.randn(4, 3).astype(np.float32).reshape((4, 3))\n        c = np.matmul(a, b)\n        output = OnnxLoader.run_node(node, [a, b])\n        np.testing.assert_almost_equal(output[""c""], c, decimal=5)\n\n    def test_matmul_3d(self):\n        node = onnx.helper.make_node(\n            \'MatMul\',\n            inputs=[\'a\', \'b\'],\n            outputs=[\'c\'],\n        )\n        # 3d\n        a = np.random.randn(2, 3, 4).astype(np.float32)\n        b = np.random.randn(2, 4, 3).astype(np.float32)\n        c = np.matmul(a, b)\n        output = OnnxLoader.run_node(node, [a, b])\n        np.testing.assert_almost_equal(output[""c""], c, decimal=5)\n\n    def test_minit(self):\n        import torch.nn as nn\n        import torch.nn.functional as F\n\n        class MnistNet(nn.Module):\n            def __init__(self):\n                super(MnistNet, self).__init__()\n                self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n                self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n                self.conv2_drop = nn.Dropout2d()\n                self.fc1 = nn.Linear(320, 50)\n                self.fc2 = nn.Linear(50, 10)\n\n            def forward(self, x):\n                x = F.relu(F.max_pool2d(self.conv1(x), 2))\n                x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n                x = x.view(-1, 320)\n                x = F.relu(self.fc1(x))\n                x = F.dropout2d(x, training=self.training)\n                x = self.fc2(x)\n                return F.log_softmax(x, dim=1)\n\n        pytorch_model = MnistNet()\n        pytorch_model.train(mode=False)\n        self.compare_with_pytorch(pytorch_model, [(1, 1, 28, 28)])\n\n    def test_onnx_sub(self):\n        class Sub(torch.nn.Module):\n            def forward(self, x):\n                return x[0] - x[1]\n\n        pytorch_model = Sub()\n        input_shape_with_batch = [(1, 3), (1, 3)]\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_sub(self):\n        node = onnx.helper.make_node(\n            \'Sub\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n        )\n\n        x = np.array([1, 2, 3]).astype(np.float32).reshape([3, 1])\n        y = np.array([3, 2, 1]).astype(np.float32).reshape([3, 1])\n        z = x - y\n        output = OnnxLoader.run_node(node, [x, y])\n        np.testing.assert_almost_equal(output[""z""], z, decimal=5)\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(3, 4, 5).astype(np.float32)\n        z = x - y\n        output = OnnxLoader.run_node(node, [x, y])\n        np.testing.assert_almost_equal(output[""z""], z, decimal=5)\n\n    def test_onnx_squeeze(self):\n        pytorch_model = Squeeze()\n        input_shape_with_batch = (2, 1, 2, 1, 2)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_squeeze_dim0(self):\n        pytorch_model = Squeeze(0)\n        input_shape_with_batch = (1, 2, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_squeeze_dim1(self):\n        pytorch_model = Squeeze(1)\n        input_shape_with_batch = (2, 1, 3, 1, 2)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_squeeze(self):\n        node = onnx.helper.make_node(\n            \'Squeeze\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            axes=[0],\n        )\n        x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n        y = np.squeeze(x, axis=0)\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_squeeze_none(self):\n        node = onnx.helper.make_node(\n            \'Squeeze\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n        x = np.random.randn(1, 1, 4, 5).astype(np.float32)\n        y = np.squeeze(x)\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_squeeze_list(self):\n        node = onnx.helper.make_node(\n            \'Squeeze\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            axes=[0, 1],\n        )\n        x = np.random.randn(1, 1, 4, 5).astype(np.float32)\n        y = np.squeeze(x)\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_squeeze_axis(self):\n        node = onnx.helper.make_node(\n            \'Squeeze\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            axes=[1],\n        )\n        x = np.random.randn(3, 1, 4, 5).astype(np.float32)\n        y = np.squeeze(x, axis=1)\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_sigmoid(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.Sigmoid()\n        )\n        input_shape_with_batch = (1, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_sigmoid(self):\n        node = helper.make_node(\n            \'Sigmoid\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n        x = np.array([[-1, 0, 1]]).astype(np.float32)\n        y = 1.0 / (1.0 + np.exp(np.negative(x)))  # expected output [0.26894143, 0.5, 0.7310586]\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_index_select(self):\n        class IndexSelect(torch.nn.Module):\n            def __init__(self, *parameter):\n                super(IndexSelect, self).__init__()\n                self.dim = parameter[0]\n                self.index = parameter[1]\n\n            def forward(self, x):\n                return torch.index_select(x, dim=self.dim, index=torch.tensor(self.index))\n\n        pytorch_model = IndexSelect(3, 2)\n        input_shape_with_batch = (3, 4, 5, 6)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_index_select_axis0(self):\n        import pytest\n        with pytest.raises(Exception) as e_info:\n            class IndexSelect(torch.nn.Module):\n                def __init__(self, *parameter):\n                    super(IndexSelect, self).__init__()\n                    self.dim = parameter[0]\n                    self.index = parameter[1]\n\n                def forward(self, x):\n                    return torch.index_select(x, dim=self.dim, index=torch.tensor(self.index))\n\n            pytorch_model = IndexSelect(0, 2)\n            input_shape_with_batch = (3, 4, 5, 6)\n            self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_concat(self):\n        class Concat(torch.nn.Module):\n            def forward(self, x):\n                return torch.cat([v for v in x], 1)\n\n        pytorch_model = Concat()\n        input_shape_with_batch = [(1, 3), (1, 3)]\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_concat(self):\n        test_cases = {\n            \'1d\': ([1, 2],\n                   [3, 4]),\n            \'2d\': ([[1, 2], [3, 4]],\n                   [[5, 6], [7, 8]]),\n            \'3d\': ([[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n                   [[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n        }  # type: Dict[Text, Sequence[Any]]\n\n        for test_case, values_ in test_cases.items():\n            values = [np.asarray(v, dtype=np.float32) for v in values_]\n            for i in range(1, len(values[0].shape)):\n                in_args = [\'value\' + str(k) for k in range(len(values))]\n                node = onnx.helper.make_node(\n                    \'Concat\',\n                    inputs=[s for s in in_args],\n                    outputs=[\'output\'],\n                    axis=i\n                )\n                y = np.concatenate(values, i)\n                output = OnnxLoader.run_node(node, [v for v in values])\n                np.testing.assert_almost_equal(output[""output""], y, decimal=5)\n\n    def test_concat_axis(self):\n        test_cases = {\n            \'1d\': ([1, 2],\n                   [3, 4]),\n            \'2d\': ([[1, 2], [3, 4]],\n                   [[5, 6], [7, 8]]),\n            \'3d\': ([[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n                   [[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n        }  # type: Dict[Text, Sequence[Any]]\n\n        for test_case, values_ in test_cases.items():\n            values = [np.asarray(v, dtype=np.float32) for v in values_]\n            for i in range(1, len(values[0].shape)):\n                in_args = [\'value\' + str(k) for k in range(len(values))]\n                node = onnx.helper.make_node(\n                    \'Concat\',\n                    inputs=[s for s in in_args],\n                    outputs=[\'output\'],\n                    axis=0\n                )\n                y = np.concatenate(values, 0)\n                output = OnnxLoader.run_node(node, [v for v in values])\n                np.testing.assert_almost_equal(output[""output""], y, decimal=5)\n\n    def test_torch_add(self):\n        class Add(torch.nn.Module):\n            def forward(self, x):\n                return torch.add(x[0], 1, x[1])\n\n        pytorch_model = Add()\n        input_shape_with_batch = [(1, 3), (1, 3)]\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_leakyrelu(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.LeakyReLU()\n        )\n        input_shape_with_batch = (1, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_leakyrelu(self):\n        node = helper.make_node(\n            \'LeakyRelu\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            alpha=0.1\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        # expected output [-0.1, 0., 1.]\n        y = np.clip(x, 0, np.inf) + np.clip(x, -np.inf, 0) * 0.1\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_gt(self):\n        class gt(torch.nn.Module):\n            def forward(self, x):\n                return torch.gt(x[0], x[1])\n\n        pytorch_model = gt()\n        input_shape_with_batch = [(1, 3), (1, 3)]\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_gt(self):\n        node = helper.make_node(\n            \'Greater\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'greater\'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(3, 4, 5).astype(np.float32)\n        z = np.greater(x, y)\n\n        output = OnnxLoader.run_node(node, [x, y])\n        np.testing.assert_almost_equal(output[\'greater\'], z, decimal=5)\n\n    def test_maxpool1d(self):\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2],\n        )\n        x = np.random.randn(1, 3, 32).astype(np.float32)\n        x_shape = np.array(np.shape(x))\n        kernel_shape = np.array([2])\n        strides = [1]\n        out_shape = pool_op_common.get_output_shape(\'VALID\', x_shape[2:], kernel_shape, strides)\n        padded = x\n        y = pool_op_common.pool(padded, x_shape, kernel_shape, strides, out_shape, [0], \'MAX\')\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_maxpool1d_strides(self):\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2],\n            strides=[2]\n        )\n        x = np.random.randn(1, 3, 32).astype(np.float32)\n        x_shape = np.array(np.shape(x))\n        kernel_shape = np.array([2])\n        strides = [2]\n        out_shape = pool_op_common.get_output_shape(\'VALID\', x_shape[2:], kernel_shape, strides)\n        padded = x\n        y = pool_op_common.pool(padded, x_shape, kernel_shape, strides, out_shape, [0], \'MAX\')\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_maxpool1d(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.MaxPool1d(2)\n        )\n        input_shape_with_batch = (1, 3, 32)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_maxpool1d_pads(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.MaxPool1d(2, padding=1)\n        )\n        input_shape_with_batch = (1, 3, 32)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_threshold(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.Threshold(0, 0))\n        input_shape_with_batch = (2, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_mul(self):\n        class Mul(torch.nn.Module):\n            def forward(self, x):\n                return x[0] * x[1]\n\n        pytorch_model = Mul()\n        input_shape_with_batch = [(1, 3), (1, 3)]\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_mul1(self):\n        node = onnx.helper.make_node(\n            \'Mul\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n        )\n\n        x = np.array([1, 2, 3]).astype(np.float32).reshape([3, 1])\n        y = np.array([4, 5, 6]).astype(np.float32).reshape([3, 1])\n        z = x * y  # expected output [4., 10., 18.]\n        output = OnnxLoader.run_node(node, [x, y])\n        np.testing.assert_almost_equal(output[\'z\'], z, decimal=5)\n\n    def test_mul2(self):\n        node = onnx.helper.make_node(\n            \'Mul\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(3, 4, 5).astype(np.float32)\n        z = x * y\n        output = OnnxLoader.run_node(node, [x, y])\n        np.testing.assert_almost_equal(output[\'z\'], z, decimal=5)\n\n    def test_onnx_div(self):\n        class Div(torch.nn.Module):\n            def forward(self, x):\n                return x[0] / x[1]\n\n        pytorch_model = Div()\n        input_shape_with_batch = [(1, 3), (1, 3)]\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_div1(self):\n        node = onnx.helper.make_node(\n            \'Div\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n        )\n\n        x = np.array([3, 4]).astype(np.float32).reshape([2, 1])\n        y = np.array([1, 2]).astype(np.float32).reshape([2, 1])\n        z = x / y\n        output = OnnxLoader.run_node(node, [x, y])\n        np.testing.assert_almost_equal(output[""z""], z, decimal=5)\n\n    def test_div2(self):\n        node = onnx.helper.make_node(\n            \'Div\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\n        z = x / y\n        output = OnnxLoader.run_node(node, [x, y])\n        np.testing.assert_almost_equal(output[""z""], z, decimal=5)\n\n    def test_pow(self):\n        class Power(torch.nn.Module):\n            def forward(self, x):\n                return torch.pow(x, 2)\n\n        pytorch_model = Power()\n        input_shape_with_batch = (1, 2, 2)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_elu(self):\n        node = onnx.helper.make_node(\n            \'Elu\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            alpha=2.0\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, 0, np.inf) + (np.exp(np.clip(x, -np.inf, 0)) - 1) * 2.0\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_elu_default(self):\n        node = onnx.helper.make_node(\n            \'Elu\',\n            inputs=[\'x\'],\n            outputs=[\'y\']\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, 0, np.inf) + (np.exp(np.clip(x, -np.inf, 0)) - 1) * 1.0\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_elu_default(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.ELU()\n        )\n        input_shape_with_batch = (1, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_elu(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.ELU(alpha=2)\n        )\n        input_shape_with_batch = (1, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_torch_clip(self):\n        class clamp(torch.nn.Module):\n            def forward(self, x):\n                return torch.clamp(x, -1, 1)\n\n        pytorch_model = torch.nn.Sequential(\n            clamp()\n        )\n        input_shape_with_batch = (1, 3, 32)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_exception_clip(self):\n        import pytest\n        with pytest.raises(Exception) as e_info:\n            class clamp(torch.nn.Module):\n                def forward(self, x):\n                    return torch.clamp(x, 1, -1)\n\n            pytorch_model = torch.nn.Sequential(\n                clamp()\n            )\n            input_shape_with_batch = (1, 3, 32)\n            self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_embedding(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.Embedding(num_embeddings=10, embedding_dim=3)\n        )\n        input_shape_with_batch = (2, 4)\n        input_data_with_batch = [[[1, 2, 4, 5], [4, 3, 2, 9]]]\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch, input_data_with_batch)\n\n    def test_onnx_slice1(self):\n        class Slice(torch.nn.Module):\n            def __init__(self, *parameter):\n                super(Slice, self).__init__()\n                self.axes = parameter[0]\n                self.starts = parameter[1]\n                self.ends = parameter[2]\n\n            def forward(self, x):\n                return x[self.starts:self.ends]\n\n        pytorch_model = Slice(0, 0, 2)\n        input_shape_with_batch = (3, 3, 3)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_slice1_start_out_of_bounds(self):\n        with pytest.raises(Exception) as e_info:\n            node = onnx.helper.make_node(\n                \'Slice\',\n                inputs=[\'x\'],\n                outputs=[\'y\'],\n                axes=[0],\n                starts=[1000],\n                ends=[1000],\n            )\n\n            x = np.random.randn(3, 3, 3).astype(np.float32)\n            y = x[1000:1000]\n            output = OnnxLoader.run_node(node, [x])\n            np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_slice2(self):\n        class Slice(torch.nn.Module):\n            def __init__(self, *parameter):\n                super(Slice, self).__init__()\n                self.axes = parameter[0]\n                self.starts = parameter[1]\n                self.ends = parameter[2]\n\n            def forward(self, x):\n                return x[self.starts[0]:self.ends[0], self.starts[1]:self.ends[1]]\n\n        pytorch_model = Slice([0, 1], [0, 0], [2, -2])\n        input_shape_with_batch = (20, 10, 5)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_slice2_neg(self):\n        node = onnx.helper.make_node(\n            \'Slice\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            axes=[0, 1],\n            starts=[0, 0],\n            ends=[2, -2],\n        )\n\n        x = np.random.randn(20, 10, 5).astype(np.float32)\n        y = x[0:2, 0:-2]\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_slice3(self):\n        class Slice(torch.nn.Module):\n            def __init__(self, *parameter):\n                super(Slice, self).__init__()\n                self.axes = parameter[0]\n                self.starts = parameter[1]\n                self.ends = parameter[2]\n\n            def forward(self, x):\n                return x[self.starts[0]:self.ends[0], self.starts[1]:self.ends[1],\n                         self.starts[2]:self.ends[2]]\n\n        pytorch_model = Slice([0, 1, 2], [0, 0, 3], [20, 10, 4])\n        input_shape_with_batch = (20, 10, 5)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_slice3_default_axes(self):\n        node = onnx.helper.make_node(\n            \'Slice\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            starts=[0, 0, 3],\n            ends=[20, 10, 4],\n        )\n\n        x = np.random.randn(20, 10, 5).astype(np.float32)\n        y = x[:, :, 3:4]\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_reducemean_keepdims(self):\n        class ReduceMean(torch.nn.Module):\n            def __init__(self, *parameter):\n                super(ReduceMean, self).__init__()\n                self.dim = parameter[0]\n                self.keepdim = parameter[1]\n\n            def forward(self, x):\n                return torch.mean(x, dim=self.dim, keepdim=self.keepdim)\n\n        pytorch_model = ReduceMean(1, True)\n        input_shape_with_batch = (1, 2, 2)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_reducemean(self):\n        class ReduceMean(torch.nn.Module):\n            def __init__(self, *parameter):\n                super(ReduceMean, self).__init__()\n                self.dim = parameter[0]\n                self.keepdim = parameter[1]\n\n            def forward(self, x):\n                return torch.mean(x, dim=self.dim, keepdim=self.keepdim)\n\n        pytorch_model = ReduceMean(1, False)\n        input_shape_with_batch = (1, 2, 2)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_reducemean_do_not_keepdims(self):\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 0\n\n        node = onnx.helper.make_node(\n            \'ReduceMean\',\n            inputs=[\'data\'],\n            outputs=[\'reduced\'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n                        dtype=np.float32)\n        reduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n        output = OnnxLoader.run_node(node, [data])\n        np.testing.assert_almost_equal(output[""reduced""], reduced, decimal=5)\n\n    def test_reducemean_keepdims(self):\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            \'ReduceMean\',\n            inputs=[\'data\'],\n            outputs=[\'reduced\'],\n            axes=axes,\n            keepdims=keepdims)\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n        output = OnnxLoader.run_node(node, [data])\n        np.testing.assert_almost_equal(output[""reduced""], reduced, decimal=5)\n\n    def test_onnx_reducesum_keepdims(self):\n        class ReduceSum(torch.nn.Module):\n            def __init__(self, *parameter):\n                super(ReduceSum, self).__init__()\n                self.dim = parameter[0]\n                self.keepdim = parameter[1]\n\n            def forward(self, x):\n                return torch.sum(x, dim=self.dim, keepdim=self.keepdim)\n\n        pytorch_model = ReduceSum(1, True)\n        input_shape_with_batch = (20, 10, 5)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_reducesum(self):\n        class ReduceSum(torch.nn.Module):\n            def __init__(self, *parameter):\n                super(ReduceSum, self).__init__()\n                self.dim = parameter[0]\n                self.keepdim = parameter[1]\n\n            def forward(self, x):\n                return torch.sum(x, dim=self.dim, keepdim=self.keepdim)\n\n        pytorch_model = ReduceSum(1, False)\n        input_shape_with_batch = (20, 10, 5)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_reducesum_do_not_keepdims(self):\n        axes = [1]\n        keepdims = 0\n\n        node = onnx.helper.make_node(\n            \'ReduceSum\',\n            inputs=[\'data\'],\n            outputs=[\'reduced\'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]],\n                        dtype=np.float32)\n        reduced = np.sum(data, axis=tuple(axes), keepdims=keepdims == 1)\n        output = OnnxLoader.run_node(node, [data])\n        np.testing.assert_almost_equal(output[""reduced""], reduced, decimal=5)\n\n    def test_reducesum_keepdims(self):\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            \'ReduceSum\',\n            inputs=[\'data\'],\n            outputs=[\'reduced\'],\n            axes=axes,\n            keepdims=keepdims)\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(data, axis=tuple(axes), keepdims=keepdims == 1)\n        output = OnnxLoader.run_node(node, [data])\n        np.testing.assert_almost_equal(output[""reduced""], reduced, decimal=5)\n\n    def test_onnx_unsqueeze_axis0(self):\n        class Unsqueeze(torch.nn.Module):\n            def __init__(self, *parameter):\n                super(Unsqueeze, self).__init__()\n                self.dim = parameter[0]\n\n            def forward(self, x):\n                return torch.unsqueeze(x, dim=self.dim)\n\n        pytorch_model = Unsqueeze(0)\n        input_shape_with_batch = (1, 2, 2)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_unsqueeze_axis0(self):\n        node = onnx.helper.make_node(\n            \'Unsqueeze\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            axes=[0],\n        )\n        x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n        y = np.expand_dims(x, axis=0)\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_unsqueeze_axis1(self):\n        class Unsqueeze(torch.nn.Module):\n            def __init__(self, *parameter):\n                super(Unsqueeze, self).__init__()\n                self.dim = parameter[0]\n\n            def forward(self, x):\n                return torch.unsqueeze(x, dim=self.dim)\n\n        pytorch_model = Unsqueeze(1)\n        input_shape_with_batch = (1, 2, 2)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_unsqueeze_axis1(self):\n        node = onnx.helper.make_node(\n            \'Unsqueeze\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            axes=[1],\n        )\n        x = np.random.randn(3, 1, 4, 5).astype(np.float32)\n        y = np.expand_dims(x, axis=1)\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_unsqueeze_list(self):\n        node = onnx.helper.make_node(\n            \'Unsqueeze\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            axes=[0, 4],\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.expand_dims(x, axis=0)\n        y = np.expand_dims(y, axis=4)\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_transpose(self):\n        pytorch_model = Transpose(2, 3)\n        input_shape_with_batch = (3, 7, 8, 9)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_onnx_transpose0(self):\n        import pytest\n        with pytest.raises(Exception) as e_info:\n            pytorch_model = Transpose(0, 3)\n            input_shape_with_batch = (3, 7, 8, 9)\n            self.compare_with_pytorch(pytorch_model, input_shape_with_batch)\n\n    def test_transpose(self):\n        shape = (2, 3, 4)\n        data = np.random.random_sample(shape).astype(np.float32)\n        permutation = (0, 2, 1)\n\n        node = onnx.helper.make_node(\n            \'Transpose\',\n            inputs=[\'data\'],\n            outputs=[\'transposed\'],\n            perm=permutation\n        )\n        transposed = np.transpose(data, permutation)\n        output = OnnxLoader.run_node(node, [data])\n        np.testing.assert_almost_equal(output[""transposed""], transposed, decimal=5)\n\n    def test_transpose_default(self):\n        import pytest\n        with pytest.raises(Exception) as e_info:\n            shape = (2, 3, 4)\n            data = np.random.random_sample(shape).astype(np.float32)\n\n            node = onnx.helper.make_node(\n                \'Transpose\',\n                inputs=[\'data\'],\n                outputs=[\'transposed\']\n            )\n\n            transposed = np.transpose(data)\n            output = OnnxLoader.run_node(node, [data])\n            np.testing.assert_almost_equal(output[""transposed""], transposed, decimal=5)\n\n    def test_shape(self):\n        node = onnx.helper.make_node(\n            \'Shape\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n        x = np.array([\n            [1, 2, 3],\n            [4, 5, 6],\n        ]).astype(np.float32)\n        y = np.array([\n            2, 3,\n        ]).astype(np.int64)\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.array(x.shape).astype(np.int64)\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_globalaveragepool(self):\n        node = onnx.helper.make_node(\n            \'GlobalAveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n        x = np.random.randn(2, 3, 7, 5).astype(np.float32)\n        spatial_shape = np.ndim(x) - 2\n        y = np.average(x, axis=tuple(range(spatial_shape, spatial_shape + 2)))\n        for _ in range(spatial_shape):\n            y = np.expand_dims(y, -1)\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_onnx_globalaveragepool2(self):\n        pytorch_model = torch.nn.Sequential(\n            torch.nn.AdaptiveAvgPool2d((1, 1))\n        )\n        input_shape_with_batch = (1, 3, 224, 224)\n        self.compare_with_pytorch(pytorch_model, input_shape_with_batch, rtol=1e-4, atol=1e-4)\n\n    def test_lrn_default(self):\n        import math\n        alpha = 0.0001\n        beta = 0.75\n        bias = 1.0\n        nsize = 3\n        node = onnx.helper.make_node(\n            \'LRN\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            size=3\n        )\n        x = np.random.randn(5, 5, 5, 5).astype(np.float32)\n        square_sum = np.zeros((5, 5, 5, 5)).astype(np.float32)\n        for n, c, h, w in np.ndindex(x.shape):\n            square_sum[n, c, h, w] = sum(\n                x[n, max(0, c - int(math.floor((nsize - 1) / 2))):\n                  min(5, c + int(math.ceil((nsize - 1) / 2)) + 1),\n                  h, w] ** 2)\n        y = x / ((bias + (alpha / nsize) * square_sum) ** beta)\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n\n    def test_lrn(self):\n        import math\n        alpha = 0.0002\n        beta = 0.5\n        bias = 2.0\n        nsize = 3\n        node = onnx.helper.make_node(\n            \'LRN\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            alpha=alpha,\n            beta=beta,\n            bias=bias,\n            size=nsize\n        )\n        x = np.random.randn(5, 5, 5, 5).astype(np.float32)\n        square_sum = np.zeros((5, 5, 5, 5)).astype(np.float32)\n        for n, c, h, w in np.ndindex(x.shape):\n            square_sum[n, c, h, w] = sum(\n                x[n, max(0, c - int(math.floor((nsize - 1) / 2))):\n                  min(5, c + int(math.ceil((nsize - 1) / 2)) + 1),\n                  h, w] ** 2)\n        y = x / ((bias + (alpha / nsize) * square_sum) ** beta)\n\n        output = OnnxLoader.run_node(node, [x])\n        np.testing.assert_almost_equal(output[""y""], y, decimal=5)\n'"
pyzoo/test/zoo/pipeline/utils/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/pipeline/utils/test_utils.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import print_function\n\nimport logging\nimport shutil\nfrom unittest import TestCase\n\nimport keras.backend as K\n\nfrom zoo.common.nncontext import *\nfrom zoo.feature.image import ImageSet\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass ZooTestCase(TestCase):\n    logging.basicConfig(level=logging.DEBUG)\n    logging.getLogger(\'py4j\').setLevel(logging.INFO)\n\n    def setup_method(self, method):\n        """"""\n        Setup any state tied to the execution of the given method in a class.\n        It is invoked for every test method of a class.\n        """"""\n        K.set_image_dim_ordering(""th"")\n        sparkConf = init_spark_conf().setMaster(""local[4]"").setAppName(""zoo test case"")\\\n            .set(""spark.driver.memory"", ""5g"")\n        assert str(sparkConf.get(""spark.shuffle.reduceLocality.enabled"")) == ""false""\n        assert \\\n            str(sparkConf.get(""spark.serializer"")) == ""org.apache.spark.serializer.JavaSerializer""\n        assert SparkContext._active_spark_context is None\n        self.sc = init_nncontext(sparkConf)\n        self.sc.setLogLevel(""ERROR"")\n        self.sqlContext = SQLContext(self.sc)\n        self.tmp_dirs = []\n\n    def teardown_method(self, method):\n        """"""\n        Teardown any state that was previously setup with a setup_method call.\n        """"""\n        K.set_image_dim_ordering(""th"")\n        self.sc.stop()\n        if hasattr(self, ""tmp_dirs""):\n            for d in self.tmp_dirs:\n                shutil.rmtree(d)\n\n    def create_temp_dir(self):\n        tmp_dir = tempfile.mkdtemp()\n        self.tmp_dirs.append(tmp_dir)\n        return tmp_dir\n\n    def assert_allclose(self, a, b, rtol=1e-6, atol=1e-6, msg=None):\n        # from tensorflow\n        self.assertEqual(a.shape, b.shape, ""Shape mismatch: expected %s, got %s."" %\n                         (a.shape, b.shape))\n        if not np.allclose(a, b, rtol=rtol, atol=atol):\n            cond = np.logical_or(\n                np.abs(a - b) > atol + rtol * np.abs(b), np.isnan(a) != np.isnan(b))\n            if a.ndim:\n                x = a[np.where(cond)]\n                y = b[np.where(cond)]\n                print(""not close where = "", np.where(cond))\n            else:\n                # np.where is broken for scalars\n                x, y = a, b\n            print(""not close lhs = "", x)\n            print(""not close rhs = "", y)\n            print(""not close dif = "", np.abs(x - y))\n            print(""not close tol = "", atol + rtol * np.abs(y))\n            print(""dtype = %s, shape = %s"" % (a.dtype, a.shape))\n            np.testing.assert_allclose(a, b, rtol=rtol, atol=atol, err_msg=msg)\n\n    def assert_list_allclose(self, a, b, rtol=1e-6, atol=1e-6, msg=None):\n        for (i1, i2) in zip(a, b):\n            self.assert_allclose(i1, i2, rtol, atol, msg)\n\n    def compare_loss(self, y_a, y_b, kloss, zloss, rtol=1e-6, atol=1e-6):\n        """"""\n        Compare forward results for Keras loss against Zoo loss.\n\n        # Arguments\n        y_a: input/y_pred\n        y_b: target/y_true\n        """"""\n        keras_output = np.mean(K.eval(kloss(K.variable(y_b), K.variable(y_a))))\n        zoo_output = zloss.forward(y_a, y_b)\n        np.testing.assert_allclose(zoo_output, keras_output, rtol=rtol, atol=atol)\n\n    def compare_layer(self, klayer, zlayer, input_data, weight_converter=None,\n                      is_training=False, rtol=1e-6, atol=1e-6):\n        """"""\n        Compare forward results for Keras layer against Zoo Keras API layer.\n        """"""\n        from keras.models import Sequential as KSequential\n        from zoo.pipeline.api.keras.models import Sequential as ZSequential\n        zmodel = ZSequential()\n        zmodel.add(zlayer)\n        kmodel = KSequential()\n        kmodel.add(klayer)\n        koutput = kmodel.predict(input_data)\n        from zoo.pipeline.api.keras.layers import BatchNormalization\n        if isinstance(zlayer, BatchNormalization):\n            k_running_mean = K.eval(klayer.running_mean)\n            k_running_std = K.eval(klayer.running_std)\n            zlayer.set_running_mean(k_running_mean)\n            zlayer.set_running_std(k_running_std)\n        if kmodel.get_weights():\n            zmodel.set_weights(weight_converter(klayer, kmodel.get_weights()))\n        zmodel.training(is_training)\n        zoutput = zmodel.forward(input_data)\n        self.assert_allclose(zoutput, koutput, rtol=rtol, atol=atol)\n\n    def compare_model(self, zmodel, kmodel, input_data, rtol=1e-5, atol=1e-5):\n        """"""\n        Compare forward results for Keras model against Zoo Keras API model.\n        """"""\n        from bigdl.keras.converter import WeightLoader\n        WeightLoader.load_weights_from_kmodel(zmodel, kmodel)\n        zmodel.training(is_training=False)\n        bigdl_output = zmodel.forward(input_data)\n        keras_output = kmodel.predict(input_data)\n        self.assert_allclose(bigdl_output, keras_output, rtol=rtol, atol=atol)\n\n    def assert_forward_backward(self, model, input_data):\n        """"""\n        Test whether forward and backward can work properly.\n        """"""\n        output = model.forward(input_data)\n        grad_input = model.backward(input_data, output)\n\n    def assert_zoo_model_save_load(self, model, input_data, rtol=1e-6, atol=1e-6):\n        """"""\n        Test for ZooModel save and load.\n        The loaded model should have the same class as the original model.\n        The loaded model should produce the same forward and backward results as the original model.\n        """"""\n        model_class = model.__class__\n        tmp_path = create_tmp_path() + "".bigdl""\n        model.save_model(tmp_path, over_write=True)\n        loaded_model = model_class.load_model(tmp_path)\n        assert isinstance(loaded_model, model_class)\n        self.compare_output_and_grad_input(model, loaded_model, input_data, rtol, atol)\n        os.remove(tmp_path)\n\n    def assert_tfpark_model_save_load(self, model, input_data, rtol=1e-6, atol=1e-6):\n        model_class = model.__class__\n        tmp_path = create_tmp_path() + "".h5""\n        model.save_model(tmp_path)\n        loaded_model = model_class.load_model(tmp_path)\n        assert isinstance(loaded_model, model_class)\n        # Calling predict will remove the impact of dropout.\n        output1 = model.predict(input_data)\n        output2 = loaded_model.predict(input_data, distributed=True)\n        if isinstance(output1, list):\n            self.assert_list_allclose(output1, output2, rtol, atol)\n        else:\n            self.assert_allclose(output1, output2, rtol, atol)\n        os.remove(tmp_path)\n\n    def compare_output_and_grad_input(self, model1, model2, input_data, rtol=1e-6, atol=1e-6):\n        # Set seed in case of random factors such as dropout.\n        rng = RNG()\n        rng.set_seed(1000)\n        output1 = model1.forward(input_data)\n        rng.set_seed(1000)\n        output2 = model2.forward(input_data)\n        if isinstance(output1, list):\n            self.assert_list_allclose(output1, output2, rtol, atol)\n        else:\n            self.assert_allclose(output1, output2, rtol, atol)\n        rng.set_seed(1000)\n        grad_input1 = model1.backward(input_data, output1)\n        rng.set_seed(1000)\n        grad_input2 = model2.backward(input_data, output1)\n        if isinstance(grad_input1, list):\n            self.assert_list_allclose(grad_input1, grad_input2, rtol, atol)\n        else:\n            self.assert_allclose(grad_input1, grad_input2, rtol, atol)\n\n    def compare_output_and_grad_input_set_weights(self, model1, model2, input_data,\n                                                  rtol=1e-6, atol=1e-6):\n        if model1.get_weights():\n            model2.set_weights(model1.get_weights())\n        self.compare_output_and_grad_input(model1, model2, input_data, rtol, atol)\n\n    def intercept(self, func, error_message):\n\n        error = False\n        try:\n            func()\n        except Exception as e:\n            if error_message not in str(e):\n                raise Exception(""error_message not in the exception raised. "" +\n                                ""error_message: %s, exception: %s"" % (error_message, e))\n            error = True\n\n        if not error:\n            raise Exception(""exception is not raised"")\n\n    def get_raw_image_set(self, with_label):\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../resources"")\n        if with_label:\n            image_folder = os.path.join(resource_path, ""cat_dog"")\n        else:\n            image_folder = os.path.join(resource_path, ""cat_dog/*"")\n        image_set = ImageSet.read(image_folder, with_label=with_label, sc=get_spark_context(),\n                                  one_based_label=False)\n        return image_set\n'"
pyzoo/test/zoo/pipeline/utils/test_utils_onnx.py,4,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import print_function\n\nimport numpy as np\nimport onnx\nimport torch\n\nfrom zoo.pipeline.api.onnx.onnx_loader import OnnxLoader\nfrom .test_utils import ZooTestCase\nfrom bigdl.util.common import to_list\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass OnnxTestCase(ZooTestCase):\n\n    def dump_pytorch_to_onnx(self, pytorch_model, input_shape_with_batch, input_data_with_batch):\n        import uuid\n        tmp_dir = self.create_temp_dir()\n        onnx_path = ""%s/%s"" % (tmp_dir, uuid.uuid4().hex + "".onnx"")\n        dummy_input = self._generate_pytorch_input(input_shape_with_batch, input_data_with_batch)\n        torch.onnx.export(pytorch_model, dummy_input, onnx_path)\n        print(""Creating an Onnx model: "" + onnx_path)\n        return onnx_path\n\n    def _generate_pytorch_input(self, input_shape_with_batch, input_data_with_batch):\n        if input_data_with_batch is None:\n            dummy_input = [torch.autograd.Variable(torch.randn(shape))\n                           for shape in input_shape_with_batch]\n        else:\n            dummy_input = [torch.autograd.Variable(torch.LongTensor(data))\n                           for data in input_data_with_batch]\n        if len(dummy_input) == 1:\n            dummy_input = dummy_input[0]\n        return dummy_input\n\n    def _convert_ndarray_to_tensor(self, input_data_with_batch):\n        tensors = [torch.from_numpy(input) for input in input_data_with_batch]\n        if len(tensors) == 1:\n            return tensors[0]\n        else:\n            return tensors\n\n    def compare_with_pytorch(self, pytorch_model, input_shape_with_batch,\n                             input_data_with_batch=None, compare_result=True,\n                             rtol=1e-6, atol=1e-6):\n        input_shape_with_batch = to_list(input_shape_with_batch)\n        if input_data_with_batch is not None:\n            input_data_with_batch = to_list(input_data_with_batch)\n        onnx_path = self.dump_pytorch_to_onnx(pytorch_model, input_shape_with_batch,\n                                              input_data_with_batch)\n        # TODO: we only consider single  output for now\n        if input_data_with_batch is None:\n            input_data_with_batch = [np.random.uniform(0, 1, shape).astype(np.float32)\n                                     for shape in input_shape_with_batch]\n        else:\n            input_data_with_batch = [np.array(data).astype(np.long)\n                                     for data in input_data_with_batch]\n        # coutput = caffe2.python.onnx.backend.run_model(onnx_model, input_data_with_batch)[0]\n\n        pytorch_model.eval()\n        pytorch_out = pytorch_model.forward(self._convert_ndarray_to_tensor(input_data_with_batch))\n        zmodel = OnnxLoader.from_path(onnx_path, is_training=False)\n        zoutput = zmodel.forward(\n            input_data_with_batch[0] if len(input_data_with_batch) == 1 else input_data_with_batch)\n        if compare_result:\n            self.assert_allclose(pytorch_out.detach().numpy(), zoutput, rtol, atol)\n            assert tuple(pytorch_out.size()[1:]) == zmodel.get_output_shape()[1:]\n\n    def gen_rnd(self, shape, low=-1.0, high=1.0):\n        return np.random.uniform(low, high, np.prod(shape)) \\\n            .reshape(shape) \\\n            .astype(np.float32)\n'"
pyzoo/test/zoo/ray/integration/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/ray/integration/ray_on_yarn.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport ray\n\nfrom zoo import init_spark_on_yarn\nfrom zoo.ray import RayContext\n\nslave_num = 2\n\nsc = init_spark_on_yarn(\n    hadoop_conf=""/opt/work/almaren-yarn-config/"",\n    conda_name=""ray_train"",\n    num_executor=slave_num,\n    executor_cores=28,\n    executor_memory=""10g"",\n    driver_memory=""2g"",\n    driver_cores=4,\n    extra_executor_memory_for_ray=""30g"",\n    spark_conf={""hello"": ""world""})\n\nray_ctx = RayContext(sc=sc,\n                     object_store_memory=""25g"",\n                     extra_params={""temp-dir"": ""/tmp/hello/""},\n                     env={""http_proxy"": ""http://child-prc.intel.com:913"",\n                          ""http_proxys"": ""http://child-prc.intel.com:913""})\nray_ctx.init(object_store_memory=""2g"",\n             num_cores=0,\n             labels="""",\n             extra_params={})\n\n\n@ray.remote\nclass TestRay():\n    def hostname(self):\n        import socket\n        return socket.gethostname()\n\n    def check_cv2(self):\n        # conda install -c conda-forge opencv==3.4.2\n        import cv2\n        return cv2.__version__\n\n    def ip(self):\n        import ray.services as rservices\n        return rservices.get_node_ip_address()\n\n    def network(self):\n        from urllib.request import urlopen\n        try:\n            urlopen(\'http://www.baidu.com\', timeout=3)\n            return True\n        except Exception as err:\n            return False\n\n\nactors = [TestRay.remote() for i in range(0, slave_num)]\nprint(ray.get([actor.hostname.remote() for actor in actors]))\nprint(ray.get([actor.ip.remote() for actor in actors]))\n# print(ray.get([actor.network.remote() for actor in actors]))\n\nray_ctx.stop()\n'"
pyzoo/test/zoo/ray/integration/test_yarn_reinit_raycontext.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport time\n\nimport numpy as np\nimport ray\n\nfrom zoo import init_spark_on_yarn\nfrom zoo.ray import RayContext\n\nnp.random.seed(1337)  # for reproducibility\n\n\n@ray.remote\nclass TestRay:\n    def hostname(self):\n        import socket\n        return socket.gethostname()\n\n\nnode_num = 4\nsc = init_spark_on_yarn(\n    hadoop_conf=""/opt/work/hadoop-2.7.2/etc/hadoop/"",\n    conda_name=""rayexample"",\n    num_executor=node_num,\n    executor_cores=28,\n    executor_memory=""10g"",\n    driver_memory=""2g"",\n    driver_cores=4,\n    extra_executor_memory_for_ray=""30g"")\nray_ctx = RayContext(sc=sc, object_store_memory=""2g"")\nray_ctx.init()\nactors = [TestRay.remote() for i in range(0, node_num)]\nprint(ray.get([actor.hostname.remote() for actor in actors]))\nray_ctx.stop()\n# repeat\nray_ctx = RayContext(sc=sc, object_store_memory=""1g"")\nray_ctx.init()\nactors = [TestRay.remote() for i in range(0, node_num)]\nprint(ray.get([actor.hostname.remote() for actor in actors]))\nray_ctx.stop()\n\nsc.stop()\ntime.sleep(3)\n'"
pyzoo/zoo/examples/nnframes/finetune/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/nnframes/finetune/image_finetuning_example.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\n\nfrom bigdl.nn.criterion import CrossEntropyCriterion\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import DoubleType, StringType\n\nfrom zoo.common.nncontext import *\nfrom zoo.feature.image import *\nfrom zoo.pipeline.api.keras.layers import Dense, Input, Flatten\nfrom zoo.pipeline.api.keras.models import *\nfrom zoo.pipeline.api.net import *\nfrom zoo.pipeline.nnframes import *\nfrom optparse import OptionParser\n\nif __name__ == ""__main__"":\n\n    parser = OptionParser()\n    parser.add_option(""-m"", dest=""model_path"",\n                      help=""Required. pretrained model path."")\n    parser.add_option(""-f"", dest=""image_path"",\n                      help=""training data path."")\n    parser.add_option(""--b"", ""--batch_size"", type=int, dest=""batch_size"", default=""56"",\n                      help=""The number of samples per gradient update. Default is 56."")\n    parser.add_option(""--nb_epoch"", type=int, dest=""nb_epoch"", default=""2"",\n                      help=""The number of epochs to train the model. Default is 2."")\n    parser.add_option(""--r"", ""--learning_rate"", type=float, dest=""learning_rate"", default=""0.003"",\n                      help=""The learning rate for the model. Default is 0.003."")\n\n    (options, args) = parser.parse_args(sys.argv)\n\n    if not options.model_path:\n        parser.print_help()\n        parser.error(\'model_path is required\')\n\n    if not options.image_path:\n        parser.print_help()\n        parser.error(\'image_path is required\')\n\n    sc = init_nncontext(""ImageFineTuningExample"")\n\n    imageDF = NNImageReader.readImages(options.image_path, sc, resizeH=300, resizeW=300,\n                                       image_codec=1)\n\n    getName = udf(lambda row: os.path.basename(row[0]), StringType())\n    getLabel = udf(lambda name: 1.0 if name.startswith(\'cat\') else 2.0, DoubleType())\n    labelDF = imageDF.withColumn(""name"", getName(col(""image""))) \\\n        .withColumn(""label"", getLabel(col(\'name\')))\n    (trainingDF, validationDF) = labelDF.randomSplit([0.9, 0.1])\n\n    # compose a pipeline that includes feature transform, pretrained model and Logistic Regression\n    transformer = ChainedPreprocessing(\n        [RowToImageFeature(), ImageResize(256, 256), ImageCenterCrop(224, 224),\n         ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(), ImageFeatureToTensor()])\n\n    full_model = Net.load_bigdl(options.model_path)\n    # create a new model by remove layers after pool5/drop_7x7_s1\n    model = full_model.new_graph([""pool5/drop_7x7_s1""])\n    # freeze layers from input to pool4/3x3_s2 inclusive\n    model.freeze_up_to([""pool4/3x3_s2""])\n\n    inputNode = Input(name=""input"", shape=(3, 224, 224))\n    inception = model.to_keras()(inputNode)\n    flatten = Flatten()(inception)\n    logits = Dense(2)(flatten)\n\n    lrModel = Model(inputNode, logits)\n\n    classifier = NNClassifier(lrModel, CrossEntropyCriterion(), transformer) \\\n        .setLearningRate(options.learning_rate) \\\n        .setBatchSize(options.batch_size) \\\n        .setMaxEpoch(options.nb_epoch) \\\n        .setFeaturesCol(""image"") \\\n        .setCachingSample(False)\n\n    pipeline = Pipeline(stages=[classifier])\n\n    catdogModel = pipeline.fit(trainingDF)\n    predictionDF = catdogModel.transform(validationDF).cache()\n    predictionDF.sample(False, 0.1).show()\n\n    correct = predictionDF.filter(""label=prediction"").count()\n    overall = predictionDF.count()\n    accuracy = correct * 1.0 / overall\n\n    print(""Test Error = %g "" % (1.0 - accuracy))\n\n    print(""finished..."")\n    sc.stop()\n'"
pyzoo/zoo/examples/nnframes/imageInference/ImageInferenceExample.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.nn.layer import Model\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import StringType\n\nfrom zoo.common.nncontext import *\nfrom zoo.feature.image import *\nfrom zoo.pipeline.nnframes import *\n\nfrom optparse import OptionParser\nimport sys\n\n\ndef inference(image_path, model_path, batch_size, sc):\n    imageDF = NNImageReader.readImages(image_path, sc, resizeH=300, resizeW=300, image_codec=1)\n    getName = udf(lambda row: row[0], StringType())\n    transformer = ChainedPreprocessing(\n        [RowToImageFeature(), ImageResize(256, 256), ImageCenterCrop(224, 224),\n         ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(), ImageFeatureToTensor()])\n\n    model = Model.loadModel(model_path)\n    classifier_model = NNClassifierModel(model, transformer)\\\n        .setFeaturesCol(""image"").setBatchSize(batch_size)\n    predictionDF = classifier_model.transform(imageDF).withColumn(""name"", getName(col(""image"")))\n    return predictionDF\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""-m"", dest=""model_path"",\n                      help=""Required. pretrained model path."")\n    parser.add_option(""-f"", dest=""image_path"",\n                      help=""training data path."")\n    parser.add_option(""--b"", ""--batch_size"", type=int, dest=""batch_size"", default=""56"",\n                      help=""The number of samples per gradient update. Default is 56."")\n\n    (options, args) = parser.parse_args(sys.argv)\n\n    if not options.model_path:\n        parser.print_help()\n        parser.error(\'model_path is required\')\n\n    if not options.image_path:\n        parser.print_help()\n        parser.error(\'image_path is required\')\n\n    sc = init_nncontext(""image_inference"")\n\n    image_path = options.image_path\n    model_path = options.model_path\n    batch_size = options.batch_size\n\n    predictionDF = inference(image_path, model_path, batch_size, sc)\n    predictionDF.select(""name"", ""prediction"").orderBy(""name"").show(20, False)\n\n    print(""finished..."")\n    sc.stop()\n'"
pyzoo/zoo/examples/nnframes/imageInference/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/nnframes/imageTransferLearning/ImageTransferLearningExample.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\n\nfrom bigdl.nn.criterion import *\nfrom bigdl.nn.layer import *\nfrom bigdl.optim.optimizer import Adam\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import DoubleType, StringType\n\nfrom zoo.common.nncontext import *\nfrom zoo.feature.image import *\nfrom zoo.pipeline.nnframes import *\n\nfrom optparse import OptionParser\n\nif __name__ == ""__main__"":\n\n    parser = OptionParser()\n    parser.add_option(""-m"", dest=""model_path"",\n                      help=""Required. pretrained model path."")\n    parser.add_option(""-f"", dest=""image_path"",\n                      help=""training data path."")\n    parser.add_option(""--b"", ""--batch_size"", type=int, dest=""batch_size"", default=""56"",\n                      help=""The number of samples per gradient update. Default is 56."")\n    parser.add_option(""--nb_epoch"", type=int, dest=""nb_epoch"", default=""20"",\n                      help=""The number of epochs to train the model. Default is 20."")\n    parser.add_option(""--r"", ""--learning_rate"", type=float, dest=""learning_rate"", default=""0.002"",\n                      help=""The learning rate for the model. Default is 0.002."")\n\n    (options, args) = parser.parse_args(sys.argv)\n\n    if not options.model_path:\n        parser.print_help()\n        parser.error(\'model_path is required\')\n\n    if not options.image_path:\n        parser.print_help()\n        parser.error(\'image_path is required\')\n\n    sc = init_nncontext(""ImageTransferLearningExample "")\n\n    imageDF = NNImageReader.readImages(options.image_path, sc, resizeH=300, resizeW=300,\n                                       image_codec=1)\n\n    getName = udf(lambda row: os.path.basename(row[0]), StringType())\n    getLabel = udf(lambda name: 1.0 if name.startswith(\'cat\') else 2.0, DoubleType())\n    labelDF = imageDF.withColumn(""name"", getName(col(""image""))) \\\n        .withColumn(""label"", getLabel(col(\'name\')))\n    (trainingDF, validationDF) = labelDF.randomSplit([0.9, 0.1])\n\n    # compose a pipeline that includes feature transform, pretrained model and Logistic Regression\n    transformer = ChainedPreprocessing(\n        [RowToImageFeature(), ImageResize(256, 256), ImageCenterCrop(224, 224),\n         ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(), ImageFeatureToTensor()])\n\n    preTrainedNNModel = NNModel(Model.loadModel(options.model_path), transformer) \\\n        .setFeaturesCol(""image"") \\\n        .setPredictionCol(""embedding"")\n\n    lrModel = Sequential().add(Linear(1000, 2)).add(LogSoftMax())\n    classifier = NNClassifier(lrModel, ClassNLLCriterion(), SeqToTensor([1000])) \\\n        .setLearningRate(options.learning_rate) \\\n        .setOptimMethod(Adam()) \\\n        .setBatchSize(options.batch_size) \\\n        .setMaxEpoch(options.nb_epoch) \\\n        .setFeaturesCol(""embedding"") \\\n        .setCachingSample(False) \\\n\n    pipeline = Pipeline(stages=[preTrainedNNModel, classifier])\n\n    catdogModel = pipeline.fit(trainingDF)\n    predictionDF = catdogModel.transform(validationDF).cache()\n    predictionDF.sample(False, 0.1).show()\n\n    evaluator = MulticlassClassificationEvaluator(\n        labelCol=""label"", predictionCol=""prediction"", metricName=""accuracy"")\n    accuracy = evaluator.evaluate(predictionDF)\n    # expected error should be less than 10%\n    print(""Test Error = %g "" % (1.0 - accuracy))\n\n    print(""finished..."")\n    sc.stop()\n'"
pyzoo/zoo/examples/nnframes/imageTransferLearning/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/orca/data/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/orca/data/ray_pandas.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nfrom optparse import OptionParser\n\nimport pandas as pd\nfrom bigdl.util.common import get_node_and_core_number\nfrom pyspark.sql import SQLContext\n\nimport zoo.orca.data.pandas\nfrom zoo import init_spark_on_local\nfrom zoo.ray import RayContext\n\n\ndef process_feature(df, awake_begin=6, awake_end=23):\n    df[\'datetime\'] = pd.to_datetime(df[\'timestamp\'])\n    df[\'hours\'] = df[\'datetime\'].dt.hour\n    df[\'awake\'] = (((df[\'hours\'] >= awake_begin) & (df[\'hours\'] <= awake_end))\n                   | (df[\'hours\'] == 0)).astype(int)\n    return df\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""-f"", type=str, dest=""file_path"",\n                      help=""The file path to be read"")\n\n    (options, args) = parser.parse_args(sys.argv)\n\n    # Prepare csv files\n    df = pd.read_csv(options.file_path)\n    sc = init_spark_on_local(cores=""*"")\n    sqlContext = SQLContext(sc)\n    num_nodes, num_cores = get_node_and_core_number()\n    df_spark = sqlContext.createDataFrame(df)\n    df_spark.printSchema()\n    df_spark.repartition(num_cores).write.\\\n        format(\'json\').mode(""overwrite"").save(""/tmp/ray-pandas-example"")\n\n    # init ray context\n    ray_ctx = RayContext(sc=sc,\n                         object_store_memory=""5g""\n                         )\n    ray_ctx.init()\n\n    # read data\n    data_shard = zoo.orca.data.pandas.read_json(""/tmp/ray-pandas-example"", ray_ctx,\n                                                orient=\'columns\', lines=True)\n\n    # collect data\n    data = data_shard.collect()\n    print(""collected data :"")\n    print(data[0].head())\n\n    # repartition\n    partitions = data_shard.get_partitions()\n    print(""get %d partitions"" % len(partitions))\n    data_shard.repartition(2)\n    new_partitions = data_shard.get_partitions()\n    print(""get %d partitions after repartition"" % len(new_partitions))\n\n    # apply function on each element\n    data_shards = data_shard.transform_shard(process_feature, 6, 24)\n    data2 = data_shards.collect()\n    print(""collected new data :"")\n    print(data2[0].head())\n\n    ray_ctx.stop()\n    sc.stop()\n'"
pyzoo/zoo/examples/orca/data/spark_pandas.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nfrom optparse import OptionParser\n\nimport zoo.orca.data.pandas\nfrom zoo.common.nncontext import init_nncontext\n\n\ndef process_feature(df, awake_begin=6, awake_end=23):\n    import pandas as pd\n    df[\'datetime\'] = pd.to_datetime(df[\'timestamp\'])\n    df[\'hours\'] = df[\'datetime\'].dt.hour\n    df[\'awake\'] = (((df[\'hours\'] >= awake_begin) & (df[\'hours\'] <= awake_end))\n                   | (df[\'hours\'] == 0)).astype(int)\n    return df\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""-f"", type=str, dest=""file_path"",\n                      help=""The file path to be read"")\n\n    (options, args) = parser.parse_args(sys.argv)\n\n    sc = init_nncontext()\n\n    # read data\n    file_path = options.file_path\n    data_shard = zoo.orca.data.pandas.read_csv(file_path, sc)\n    data = data_shard.collect()\n\n    # repartition\n    data_shard = data_shard.repartition(2)\n\n    # apply function on each element\n    trans_data_shard = data_shard.transform_shard(process_feature)\n    data2 = trans_data_shard.collect()\n\n    sc.stop()\n'"
pyzoo/zoo/examples/orca/learn/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/pytorch/inference/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/pytorch/inference/predict.py,1,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom optparse import OptionParser\n\nimport torch\nfrom torchvision import datasets, models, transforms\nfrom zoo.common.nncontext import init_nncontext\nfrom zoo.feature.common import *\nfrom zoo.feature.image import *\nfrom zoo.pipeline.api.net.torch_net import TorchNet\n\n\ndef predict(img_path):\n    val_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(img_path, transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])),\n        batch_size=8, shuffle=False,\n        num_workers=1, pin_memory=True)\n\n    model = models.resnet18(pretrained=True).eval()\n    net = TorchNet.from_pytorch(model, [1, 3, 224, 224])\n\n    for inputs, labels in val_loader:\n        output = net.predict(inputs.numpy(), distributed=True).collect()\n        index = [o.argmax() for o in output]\n        print(index)\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--image"", type=str, dest=""img_path"",\n                      help=""The path where the images are stored, ""\n                           ""can be either a folder or an image path"")\n    (options, args) = parser.parse_args(sys.argv)\n\n    sc = init_nncontext(""Torch ResNet Prediction Example"")\n    predict(options.img_path)\n'"
pyzoo/zoo/examples/pytorch/train/Lenet_mnist.py,3,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom bigdl.optim.optimizer import *\nfrom torchvision import datasets\nfrom zoo.common.nncontext import *\nfrom zoo.pipeline.nnframes import *\nfrom zoo.pipeline.api.net.torch_net import TorchNet\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import DoubleType\nfrom zoo.pipeline.api.net.torch_criterion import TorchCriterion\n\n\n# define model with Pytorch API\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\nif __name__ == \'__main__\':\n    sparkConf = init_spark_conf().setAppName(""test_pytorch_lenet"")\n    sc = init_nncontext(sparkConf)\n    spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n\n    mnist = datasets.MNIST(\'../data\', train=True, download=True)\n    X_train = mnist.data.numpy() / 255.0\n    Y_train = mnist.train_labels.float().numpy()\n    pd_df = pd.DataFrame()\n    pd_df[\'features\'] = X_train.reshape((X_train.shape[0], 784)).tolist()\n    pd_df[\'label\'] = Y_train.reshape((Y_train.shape[0])).tolist()\n\n    mnistDF = spark.createDataFrame(pd_df)\n    (trainingDF, validationDF) = mnistDF.randomSplit([0.8, 0.2])\n    trainingDF.show()\n\n    # define loss with Pytorch API\n    def lossFunc(input, target):\n        return nn.CrossEntropyLoss().forward(input, target.flatten().long())\n\n    torch_model = LeNet()\n    model = TorchNet.from_pytorch(torch_model, [1, 1, 28, 28])\n    criterion = TorchCriterion.from_pytorch(lossFunc, [1, 10], torch.LongTensor([5]))\n    classifier = NNClassifier(model, criterion, SeqToTensor([1, 28, 28])) \\\n        .setBatchSize(256) \\\n        .setOptimMethod(Adam()) \\\n        .setLearningRate(0.001)\\\n        .setMaxEpoch(2)\n\n    nnClassifierModel = classifier.fit(trainingDF)\n\n    print(""After training: "")\n    shift = udf(lambda p: p - 1, DoubleType())\n    res = nnClassifierModel.transform(validationDF) \\\n        .withColumn(""prediction"", shift(col(\'prediction\')))\n    res.show(100)\n\n    correct = res.filter(""label=prediction"").count()\n    overall = res.count()\n    accuracy = correct * 1.0 / overall\n    print(""Validation accuracy = %g "" % accuracy)\n'"
pyzoo/zoo/examples/pytorch/train/SimpleTrainingExample.py,2,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport torch\nimport torch.nn as nn\nfrom bigdl.optim.optimizer import Adam\nfrom zoo.common.nncontext import *\nfrom zoo.pipeline.api.net.torch_net import TorchNet\nfrom zoo.pipeline.api.net.torch_criterion import TorchCriterion\nfrom zoo.pipeline.nnframes import *\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql import SparkSession\n\n\n# define model with Pytorch\nclass SimpleTorchModel(nn.Module):\n    def __init__(self):\n        super(SimpleTorchModel, self).__init__()\n        self.dense1 = nn.Linear(2, 4)\n        self.dense2 = nn.Linear(4, 1)\n\n    def forward(self, x):\n        x = self.dense1(x)\n        x = torch.sigmoid(self.dense2(x))\n        return x\n\nif __name__ == \'__main__\':\n    sparkConf = init_spark_conf().setAppName(""testNNClassifer"").setMaster(\'local[1]\')\n    sc = init_nncontext(sparkConf)\n    spark = SparkSession \\\n        .builder \\\n        .getOrCreate()\n\n    df = spark.createDataFrame(\n        [(Vectors.dense([2.0, 1.0]), 1.0),\n         (Vectors.dense([1.0, 2.0]), 0.0),\n         (Vectors.dense([2.0, 1.0]), 1.0),\n         (Vectors.dense([1.0, 2.0]), 0.0)],\n        [""features"", ""label""])\n\n    torch_model = SimpleTorchModel()\n    torch_criterion = nn.MSELoss()\n\n    az_model = TorchNet.from_pytorch(torch_model, [1, 2])\n    az_criterion = TorchCriterion.from_pytorch(torch_criterion, [1, 1], [1, 1])\n\n    classifier = NNClassifier(az_model, az_criterion) \\\n        .setBatchSize(4) \\\n        .setOptimMethod(Adam()) \\\n        .setLearningRate(0.01) \\\n        .setMaxEpoch(10)\n\n    nnClassifierModel = classifier.fit(df)\n\n    print(""After training: "")\n    res = nnClassifierModel.transform(df)\n    res.show(10, False)\n'"
pyzoo/zoo/examples/ray/parameter_server/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/ray/parameter_server/async_parameter_server.py,0,"b'# This file is adapted from https://github.com/ray-project/ray/blob\n# /master/examples/parameter_server/async_parameter_server.py\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport time\n\nimport model\nimport ray\n\nfrom zoo import init_spark_on_yarn, init_spark_on_local\nfrom zoo.ray import RayContext\n\nos.environ[""LANG""] = ""C.UTF-8""\nparser = argparse.ArgumentParser(description=""Run the asynchronous parameter ""\n                                             ""server example."")\nparser.add_argument(""--num_workers"", default=4, type=int,\n                    help=""The number of workers to use."")\nparser.add_argument(""--iterations"", default=50, type=int,\n                    help=""Iteration time."")\nparser.add_argument(""--hadoop_conf"", type=str,\n                    help=""turn on yarn mode by passing the path to the hadoop""\n                    ""Configuration folder. Otherwise, turn on local mode."")\nparser.add_argument(""--conda_name"", type=str,\n                    help=""The name of conda environment."")\nparser.add_argument(""--executor_cores"", type=int, default=8,\n                    help=""The number of driver\'s cpu cores you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--executor_memory"", type=str, default=""10g"",\n                    help=""The size of slave(executor)\'s memory you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--driver_memory"", type=str, default=""2g"",\n                    help=""The size of driver\'s memory you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--driver_cores"", type=int, default=8,\n                    help=""The number of driver\'s cpu cores you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--extra_executor_memory_for_ray"", type=str, default=""20g"",\n                    help=""The extra executor memory to store some data.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--object_store_memory"", type=str, default=""4g"",\n                    help=""The memory to store data on local.""\n                    ""You can change it depending on your own cluster setting."")\n\n\n@ray.remote\nclass ParameterServer(object):\n    def __init__(self, keys, values):\n        # These values will be mutated, so we must create a copy that is not\n        # backed by the object store.\n        values = [value.copy() for value in values]\n        self.weights = dict(zip(keys, values))\n\n    def push(self, keys, values):\n        for key, value in zip(keys, values):\n            self.weights[key] += value\n\n    def pull(self, keys):\n        return [self.weights[key] for key in keys]\n\n\n@ray.remote\ndef worker_task(ps, worker_index, batch_size=50):\n    # Download MNIST.\n    print(""Worker "" + str(worker_index))\n    mnist = model.download_mnist_retry(seed=worker_index)\n\n    # Initialize the model.\n    net = model.SimpleCNN()\n    keys = net.get_weights()[0]\n\n    while True:\n        # Get the current weights from the parameter server.\n        weights = ray.get(ps.pull.remote(keys))\n        net.set_weights(keys, weights)\n        # Compute an update and push it to the parameter server.\n        xs, ys = mnist.train.next_batch(batch_size)\n        gradients = net.compute_update(xs, ys)\n        ps.push.remote(keys, gradients)\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n    if args.hadoop_conf:\n        sc = init_spark_on_yarn(\n            hadoop_conf=args.hadoop_conf,\n            conda_name=args.conda_name,\n            num_executor=args.num_workers,\n            executor_cores=args.executor_cores,\n            executor_memory=args.executor_memory,\n            driver_memory=args.driver_memory,\n            driver_cores=args.driver_cores,\n            extra_executor_memory_for_ray=args.extra_executor_memory_for_ray)\n        ray_ctx = RayContext(\n            sc=sc,\n            object_store_memory=args.object_store_memory)\n    else:\n        sc = init_spark_on_local(cores=args.driver_cores)\n        ray_ctx = RayContext(sc=sc, object_store_memory=args.object_store_memory)\n    ray_ctx.init()\n\n    # Create a parameter server with some random weights.\n    net = model.SimpleCNN()\n    all_keys, all_values = net.get_weights()\n    ps = ParameterServer.remote(all_keys, all_values)\n\n    # Start some training tasks.\n    worker_tasks = [worker_task.remote(ps, i) for i in range(args.num_workers)]\n\n    # Download MNIST.\n    mnist = model.download_mnist_retry()\n    print(""Begin iteration"")\n    i = 0\n    while i < args.iterations:\n        # Get and evaluate the current model.\n        print(""-----Iteration"" + str(i) + ""------"")\n        current_weights = ray.get(ps.pull.remote(all_keys))\n        net.set_weights(all_keys, current_weights)\n        test_xs, test_ys = mnist.test.next_batch(1000)\n        accuracy = net.compute_accuracy(test_xs, test_ys)\n        print(""Iteration {}: accuracy is {}"".format(i, accuracy))\n        i += 1\n        time.sleep(1)\n    ray_ctx.stop()\n    sc.stop()\n'"
pyzoo/zoo/examples/ray/parameter_server/model.py,0,"b'# This file is adapted from https://github.com/ray-project/ray/blob\n# /master/examples/parameter_server/model.py\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Most of the tensorflow code is adapted from Tensorflow\'s tutorial on using\n# CNNs to train MNIST\n# https://www.tensorflow.org/get_started/mnist/pros#build-a-multilayer-convolutional-network.  # noqa: E501\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport ray\nimport ray.experimental.tf_utils\n\n\ndef download_mnist_retry(seed=0, max_num_retries=20):\n    for _ in range(max_num_retries):\n        try:\n            return input_data.read_data_sets(\n                ""MNIST_data"", one_hot=True, seed=seed)\n        except tf.errors.AlreadyExistsError:\n            time.sleep(1)\n    raise Exception(""Failed to download MNIST."")\n\n\nclass SimpleCNN(object):\n    def __init__(self, learning_rate=1e-4):\n        with tf.Graph().as_default():\n\n            # Create the model\n            self.x = tf.placeholder(tf.float32, [None, 784])\n\n            # Define loss and optimizer\n            self.y_ = tf.placeholder(tf.float32, [None, 10])\n\n            # Build the graph for the deep net\n            self.y_conv, self.keep_prob = deepnn(self.x)\n\n            with tf.name_scope(""loss""):\n                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n                    labels=self.y_, logits=self.y_conv)\n            self.cross_entropy = tf.reduce_mean(cross_entropy)\n\n            with tf.name_scope(""adam_optimizer""):\n                self.optimizer = tf.train.AdamOptimizer(learning_rate)\n                self.train_step = self.optimizer.minimize(self.cross_entropy)\n\n            with tf.name_scope(""accuracy""):\n                correct_prediction = tf.equal(\n                    tf.argmax(self.y_conv, 1), tf.argmax(self.y_, 1))\n                correct_prediction = tf.cast(correct_prediction, tf.float32)\n            self.accuracy = tf.reduce_mean(correct_prediction)\n\n            self.sess = tf.Session(\n                config=tf.ConfigProto(\n                    intra_op_parallelism_threads=1,\n                    inter_op_parallelism_threads=1))\n            self.sess.run(tf.global_variables_initializer())\n\n            # Helper values.\n\n            self.variables = ray.experimental.tf_utils.TensorFlowVariables(\n                self.cross_entropy, self.sess)\n\n            self.grads = self.optimizer.compute_gradients(self.cross_entropy)\n            self.grads_placeholder = [(tf.placeholder(\n                ""float"", shape=grad[1].get_shape()), grad[1])\n                for grad in self.grads]\n            self.apply_grads_placeholder = self.optimizer.apply_gradients(\n                self.grads_placeholder)\n\n    def compute_update(self, x, y):\n        # TODO(rkn): Computing the weights before and after the training step\n        # and taking the diff is awful.\n        weights = self.get_weights()[1]\n        self.sess.run(\n            self.train_step,\n            feed_dict={\n                self.x: x,\n                self.y_: y,\n                self.keep_prob: 0.5\n            })\n        new_weights = self.get_weights()[1]\n        return [x - y for x, y in zip(new_weights, weights)]\n\n    def compute_gradients(self, x, y):\n        return self.sess.run(\n            [grad[0] for grad in self.grads],\n            feed_dict={\n                self.x: x,\n                self.y_: y,\n                self.keep_prob: 0.5\n            })\n\n    def apply_gradients(self, gradients):\n        feed_dict = {}\n        for i in range(len(self.grads_placeholder)):\n            feed_dict[self.grads_placeholder[i][0]] = gradients[i]\n        self.sess.run(self.apply_grads_placeholder, feed_dict=feed_dict)\n\n    def compute_accuracy(self, x, y):\n        return self.sess.run(\n            self.accuracy,\n            feed_dict={\n                self.x: x,\n                self.y_: y,\n                self.keep_prob: 1.0\n            })\n\n    def set_weights(self, variable_names, weights):\n        self.variables.set_weights(dict(zip(variable_names, weights)))\n\n    def get_weights(self):\n        weights = self.variables.get_weights()\n        return list(weights.keys()), list(weights.values())\n\n\ndef deepnn(x):\n    """"""deepnn builds the graph for a deep net for classifying digits.\n    Args:\n        x: an input tensor with the dimensions (N_examples, 784), where 784 is\n            the number of pixels in a standard MNIST image.\n    Returns:\n        A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with\n            values equal to the logits of classifying the digit into one of 10\n            classes (the digits 0-9). keep_prob is a scalar placeholder for the\n            probability of dropout.\n    """"""\n    # Reshape to use within a convolutional neural net.\n    # Last dimension is for ""features"" - there is only one here, since images\n    # are grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n    with tf.name_scope(""reshape""):\n        x_image = tf.reshape(x, [-1, 28, 28, 1])\n\n    # First convolutional layer - maps one grayscale image to 32 feature maps.\n    with tf.name_scope(""conv1""):\n        W_conv1 = weight_variable([5, 5, 1, 32])\n        b_conv1 = bias_variable([32])\n        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\n    # Pooling layer - downsamples by 2X.\n    with tf.name_scope(""pool1""):\n        h_pool1 = max_pool_2x2(h_conv1)\n\n    # Second convolutional layer -- maps 32 feature maps to 64.\n    with tf.name_scope(""conv2""):\n        W_conv2 = weight_variable([5, 5, 32, 64])\n        b_conv2 = bias_variable([64])\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n\n    # Second pooling layer.\n    with tf.name_scope(""pool2""):\n        h_pool2 = max_pool_2x2(h_conv2)\n\n    # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n    # is down to 7x7x64 feature maps -- maps this to 1024 features.\n    with tf.name_scope(""fc1""):\n        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n        b_fc1 = bias_variable([1024])\n\n        h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n    # Dropout - controls the complexity of the model, prevents co-adaptation of\n    # features.\n    with tf.name_scope(""dropout""):\n        keep_prob = tf.placeholder(tf.float32)\n        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n    # Map the 1024 features to 10 classes, one for each digit\n    with tf.name_scope(""fc2""):\n        W_fc2 = weight_variable([1024, 10])\n        b_fc2 = bias_variable([10])\n\n        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n    return y_conv, keep_prob\n\n\ndef conv2d(x, W):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=""SAME"")\n\n\ndef max_pool_2x2(x):\n    """"""max_pool_2x2 downsamples a feature map by 2X.""""""\n    return tf.nn.max_pool(\n        x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"")\n\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n'"
pyzoo/zoo/examples/ray/parameter_server/sync_parameter_server.py,0,"b'# This file is adapted from https://github.com/ray-project/ray/blob\n# /master/examples/parameter_server/sync_parameter_server.py\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nimport model\nimport numpy as np\nimport ray\n\nfrom zoo import init_spark_on_yarn, init_spark_on_local\nfrom zoo.ray import RayContext\n\nos.environ[""LANG""] = ""C.UTF-8""\nparser = argparse.ArgumentParser(description=""Run the synchronous parameter ""\n                                             ""server example."")\nparser.add_argument(""--num_workers"", default=4, type=int,\n                    help=""The number of workers to use."")\nparser.add_argument(""--iterations"", default=50, type=int,\n                    help=""Iteration time."")\nparser.add_argument(""--hadoop_conf"", type=str,\n                    help=""turn on yarn mode by passing the path to the hadoop""\n                    ""configuration folder. Otherwise, turn on local mode."")\nparser.add_argument(""--conda_name"", type=str,\n                    help=""The name of conda environment."")\nparser.add_argument(""--executor_cores"", type=int, default=8,\n                    help=""The number of driver\'s cpu cores you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--executor_memory"", type=str, default=""10g"",\n                    help=""The size of slave(executor)\'s memory you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--driver_memory"", type=str, default=""2g"",\n                    help=""The size of driver\'s memory you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--driver_cores"", type=int, default=8,\n                    help=""The number of driver\'s cpu cores you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--extra_executor_memory_for_ray"", type=str, default=""20g"",\n                    help=""The extra executor memory to store some data.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--object_store_memory"", type=str, default=""4g"",\n                    help=""The memory to store data on local.""\n                    ""You can change it depending on your own cluster setting."")\n\n\n@ray.remote\nclass ParameterServer(object):\n    def __init__(self, learning_rate):\n        self.net = model.SimpleCNN(learning_rate=learning_rate)\n\n    def apply_gradients(self, *gradients):\n        self.net.apply_gradients(np.mean(gradients, axis=0))\n        return self.net.variables.get_flat()\n\n    def get_weights(self):\n        return self.net.variables.get_flat()\n\n\n@ray.remote\nclass Worker(object):\n    def __init__(self, worker_index, batch_size=50):\n        self.worker_index = worker_index\n        self.batch_size = batch_size\n        self.mnist = model.download_mnist_retry(seed=worker_index)\n        self.net = model.SimpleCNN()\n\n    def compute_gradients(self, weights):\n        self.net.variables.set_flat(weights)\n        xs, ys = self.mnist.train.next_batch(self.batch_size)\n        return self.net.compute_gradients(xs, ys)\n\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n    if args.hadoop_conf:\n        sc = init_spark_on_yarn(\n            hadoop_conf=args.hadoop_conf,\n            conda_name=args.conda_name,\n            num_executor=args.num_workers,\n            executor_cores=args.executor_cores,\n            executor_memory=args.executor_memory,\n            driver_memory=args.driver_memory,\n            driver_cores=args.driver_cores,\n            extra_executor_memory_for_ray=args.extra_executor_memory_for_ray)\n        ray_ctx = RayContext(\n            sc=sc,\n            object_store_memory=args.object_store_memory)\n    else:\n        sc = init_spark_on_local(cores=args.driver_cores)\n        ray_ctx = RayContext(sc=sc, object_store_memory=args.object_store_memory)\n    ray_ctx.init()\n\n    # Create a parameter server.\n    net = model.SimpleCNN()\n    ps = ParameterServer.remote(1e-4 * args.num_workers)\n\n    # Create workers.\n    workers = [Worker.remote(worker_index)\n               for worker_index in range(args.num_workers)]\n\n    # Download MNIST.\n    mnist = model.download_mnist_retry()\n\n    i = 0\n    current_weights = ps.get_weights.remote()\n    print(""Begin iteration"")\n    while i < args.iterations:\n        # Compute and apply gradients.\n        gradients = [worker.compute_gradients.remote(current_weights)\n                     for worker in workers]\n        current_weights = ps.apply_gradients.remote(*gradients)\n\n        if i % 10 == 0:\n            # Evaluate the current model.\n            net.variables.set_flat(ray.get(current_weights))\n            test_xs, test_ys = mnist.test.next_batch(1000)\n            accuracy = net.compute_accuracy(test_xs, test_ys)\n            print(""Iteration {}: accuracy is {}"".format(i, accuracy))\n        i += 1\n    ray_ctx.stop()\n    sc.stop()\n'"
pyzoo/zoo/examples/ray/rl_pong/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/ray/rl_pong/rl_pong.py,0,"b'# This file is adapted from https://github.com/ray-project/ray/blob/master\n# /examples/rl_pong/driver.py\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# play Pong https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport time\n\nimport gym\nimport numpy as np\nimport ray\n\nfrom zoo import init_spark_on_yarn, init_spark_on_local\nfrom zoo.ray import RayContext\n\nos.environ[""LANG""] = ""C.UTF-8""\n# Define some hyperparameters.\n\n# The number of hidden layer neurons.\nH = 200\nlearning_rate = 1e-4\n# Discount factor for reward.\ngamma = 0.99\n# The decay factor for RMSProp leaky sum of grad^2.\ndecay_rate = 0.99\n\n# The input dimensionality: 80x80 grid.\nD = 80 * 80\n\n\ndef sigmoid(x):\n    # Sigmoid ""squashing"" function to interval [0, 1].\n    return 1.0 / (1.0 + np.exp(-x))\n\n\ndef preprocess(img):\n    """"""Preprocess 210x160x3 uint8 frame into 6400 (80x80) 1D float vector.""""""\n    # Crop the image.\n    img = img[35:195]\n    # Downsample by factor of 2.\n    img = img[::2, ::2, 0]\n    # Erase background (background type 1).\n    img[img == 144] = 0\n    # Erase background (background type 2).\n    img[img == 109] = 0\n    # Set everything else (paddles, ball) to 1.\n    img[img != 0] = 1\n    return img.astype(np.float).ravel()\n\n\ndef discount_rewards(r):\n    """"""take 1D float array of rewards and compute discounted reward""""""\n    discounted_r = np.zeros_like(r)\n    running_add = 0\n    for t in reversed(range(0, r.size)):\n        # Reset the sum, since this was a game boundary (pong specific!).\n        if r[t] != 0:\n            running_add = 0\n        running_add = running_add * gamma + r[t]\n        discounted_r[t] = running_add\n    return discounted_r\n\n\n# defines the policy network\n# x is a vector that holds the preprocessed pixel information\ndef policy_forward(x, model):\n    # neurons in the hidden layer (W1) can detect various game senarios\n    h = np.dot(model[""W1""], x)   # compute hidden layer neuron activations\n    h[h < 0] = 0  # ReLU nonlinearity. threhold at zero\n    # weights in W2 can then decide if each case we should go UP or DOWN\n    logp = np.dot(model[""W2""], h)   # compuate the log probability of going up\n    p = sigmoid(logp)\n    # Return probability of taking action 2, and hidden state.\n    return p, h\n\n\ndef policy_backward(eph, epx, epdlogp, model):\n    """"""backward pass. (eph is array of intermediate hidden states)""""""\n# the way to change the policy parameters is to\n# do some rollouts, take the gradient of the sampled actions\n#  multiply it by the score and add everything\n    dW2 = np.dot(eph.T, epdlogp).ravel()\n    dh = np.outer(epdlogp, model[""W2""])\n    # Backprop relu.\n    dh[eph <= 0] = 0\n    dW1 = np.dot(dh.T, epx)\n    return {""W1"": dW1, ""W2"": dW2}\n\n\n@ray.remote\nclass PongEnv(object):\n    def __init__(self):\n        # Tell numpy to only use one core. If we don\'t do this, each actor may\n        # try to use all of the cores and the resulting contention may result\n        # in no speedup over the serial version. Note that if numpy is using\n        # OpenBLAS, then you need to set OPENBLAS_NUM_THREADS=1, and you\n        # probably need to do it from the command line (so it happens before\n        # numpy is imported).\n        os.environ[""MKL_NUM_THREADS""] = ""1""\n        self.env = gym.make(""Pong-v0"")\n\n    def compute_gradient(self, model):\n        # model = {\'W1\':W1, \'W2\':W2}\n        # given a model, run for one episode and return the parameter\n        # to be updated and sum(reward)\n        # Reset the game.\n        observation = self.env.reset()\n        # Note that prev_x is used in computing the difference frame.\n        prev_x = None\n        xs, hs, dlogps, drs = [], [], [], []\n        reward_sum = 0\n        done = False\n        while not done:\n            cur_x = preprocess(observation)\n            x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n            prev_x = cur_x\n\n            # feed difference frames into the network\n            # so that it can detect motion\n            aprob, h = policy_forward(x, model)\n            # Sample an action.\n            action = 2 if np.random.uniform() < aprob else 3\n\n            # The observation.\n            xs.append(x)\n            # The hidden state.\n            hs.append(h)\n            y = 1 if action == 2 else 0  # A ""fake label"".\n            # The gradient that encourages the action that was taken to be\n            # taken (see http://cs231n.github.io/neural-networks-2/#losses if\n            # confused).\n            dlogps.append(y - aprob)\n\n            observation, reward, done, info = self.env.step(action)\n            reward_sum += reward\n\n            # Record reward (has to be done after we call step() to get reward\n            # for previous action).\n            drs.append(reward)\n\n        epx = np.vstack(xs)\n        eph = np.vstack(hs)\n        epdlogp = np.vstack(dlogps)\n        epr = np.vstack(drs)\n        # Reset the array memory.\n        xs, hs, dlogps, drs = [], [], [], []\n\n        # Compute the discounted reward backward through time.\n        discounted_epr = discount_rewards(epr)\n        # Standardize the rewards to be unit normal (helps control the gradient\n        # estimator variance).\n        discounted_epr -= np.mean(discounted_epr)\n        discounted_epr /= np.std(discounted_epr)\n        # Modulate the gradient with advantage (the policy gradient magic\n        # happens right here).\n        epdlogp *= discounted_epr\n        return policy_backward(eph, epx, epdlogp, model), reward_sum\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Train an RL agent"")\n\n    parser.add_argument(""--hadoop_conf"", type=str,\n                        help=""turn on yarn mode by passing the hadoop path""\n                        ""configuration folder. Otherwise, turn on local mode."")\n    parser.add_argument(""--batch_size"", default=10, type=int,\n                        help=""The number of roll-outs to do per batch."")\n    parser.add_argument(""--iterations"", default=-1, type=int,\n                        help=""The number of model updates to perform. By ""\n                        ""default, training will not terminate."")\n\n    parser.add_argument(""--conda_name"", type=str,\n                        help=""The name of conda environment."")\n    parser.add_argument(""--slave_num"", type=int, default=2,\n                        help=""The number of slave nodes"")\n    parser.add_argument(""--executor_cores"", type=int, default=8,\n                        help=""The number of driver\'s cpu cores you want to use.""\n                             ""You can change it depending on your own cluster setting."")\n    parser.add_argument(""--executor_memory"", type=str, default=""10g"",\n                        help=""The size of slave(executor)\'s memory you want to use.""\n                             ""You can change it depending on your own cluster setting."")\n    parser.add_argument(""--driver_memory"", type=str, default=""2g"",\n                        help=""The size of driver\'s memory you want to use.""\n                             ""You can change it depending on your own cluster setting."")\n    parser.add_argument(""--driver_cores"", type=int, default=8,\n                        help=""The number of driver\'s cpu cores you want to use.""\n                             ""You can change it depending on your own cluster setting."")\n    parser.add_argument(""--extra_executor_memory_for_ray"", type=str, default=""20g"",\n                        help=""The extra executor memory to store some data.""\n                             ""You can change it depending on your own cluster setting."")\n    parser.add_argument(""--object_store_memory"", type=str, default=""4g"",\n                        help=""The memory to store data on local.""\n                             ""You can change it depending on your own cluster setting."")\n\n    args = parser.parse_args()\n    if args.hadoop_conf:\n        sc = init_spark_on_yarn(\n            hadoop_conf=args.hadoop_conf,\n            conda_name=args.conda_name,\n            num_executor=args.slave_num,\n            executor_cores=args.executor_cores,\n            executor_memory=args.executor_memory,\n            driver_memory=args.driver_memory,\n            driver_cores=args.driver_cores,\n            extra_executor_memory_for_ray=args.extra_executor_memory_for_ray)\n        ray_ctx = RayContext(\n            sc=sc,\n            object_store_memory=args.object_store_memory)\n    else:\n        sc = init_spark_on_local(cores=args.driver_cores)\n        ray_ctx = RayContext(sc=sc, object_store_memory=args.object_store_memory)\n    ray_ctx.init()\n\n    batch_size = args.batch_size\n    # Run the reinforcement learning.\n    running_reward = None\n    batch_num = 1\n    model = {}\n    # ""Xavier"" initialization.\n    model[""W1""] = np.random.randn(H, D) / np.sqrt(D)\n    model[""W2""] = np.random.randn(H) / np.sqrt(H)\n    # Update buffers that add up gradients over a batch.\n    grad_buffer = {k: np.zeros_like(v) for k, v in model.items()}\n    # Update the rmsprop memory.\n    rmsprop_cache = {k: np.zeros_like(v) for k, v in model.items()}\n    actors = [PongEnv.remote() for _ in range(batch_size)]\n    iteration = 0\n    while iteration != args.iterations:\n        iteration += 1\n        model_id = ray.put(model)\n        actions = []\n        # Launch tasks to compute gradients from multiple rollouts in parallel.\n        start_time = time.time()\n        # run rall_out for batch_size times\n        for i in range(batch_size):\n            # compute_gradient returns two variables, so action_id is a list\n            action_id = actors[i].compute_gradient.remote(model_id)\n            actions.append(action_id)\n        for i in range(batch_size):\n            # wait for one actor to finish its operation\n            # action_id is the ready object id\n            action_id, actions = ray.wait(actions)\n            grad, reward_sum = ray.get(action_id[0])\n            # Accumulate the gradient of each weight parameter over batch.\n            for k in model:\n                grad_buffer[k] += grad[k]\n            running_reward = (reward_sum if running_reward is None else\n                              running_reward * 0.99 + reward_sum * 0.01)\n        end_time = time.time()\n        print(""Batch {} computed {} rollouts in {} seconds, ""\n              ""running mean is {}"".format(batch_num, batch_size,\n                                          end_time - start_time,\n                                          running_reward))\n        # update gradient after one iteration\n        for k, v in model.items():\n            g = grad_buffer[k]\n            rmsprop_cache[k] = (\n                decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2)\n            model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n            # Reset the batch gradient buffer.\n            grad_buffer[k] = np.zeros_like(v)\n        batch_num += 1\n\n    ray_ctx.stop()\n    sc.stop()\n'"
pyzoo/zoo/examples/ray/rllib/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/ray/rllib/multiagent_two_trainers.py,0,"b'# This file is adapted from https://github.com/ray-project/ray/blob\n# /master/python/ray/rllib/examples/multiagent_two_trainers.py\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n""""""Example of using two different training methods at once in multi-agent.\nHere we create a number of CartPole agents, some of which are trained with\nDQN, and some of which are trained with PPO. We periodically sync weights\nbetween the two trainers (note that no such syncing is needed when using just\na single training method).\nFor a simpler example, see also: multiagent_cartpole.py\n""""""\n\nimport argparse\nimport gym\nimport os\n\n\nfrom ray.rllib.agents.dqn.dqn import DQNTrainer\nfrom ray.rllib.agents.dqn.dqn_policy import DQNTFPolicy\nfrom ray.rllib.agents.ppo.ppo import PPOTrainer\nfrom ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\nfrom ray.rllib.tests.test_multi_agent_env import MultiCartpole\nfrom ray.tune.logger import pretty_print\nfrom ray.tune.registry import register_env\nfrom zoo import init_spark_on_yarn, init_spark_on_local\nfrom zoo.ray import RayContext\nos.environ[""LANG""] = ""C.UTF-8""\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--iterations"", type=int, default=10,\n                    help=""The number of iterations to train the model"")\nparser.add_argument(""--hadoop_conf"", type=str,\n                    help=""turn on yarn mode by passing the path to the hadoop""\n                    "" configuration folder. Otherwise, turn on local mode."")\nparser.add_argument(""--slave_num"", type=int, default=2,\n                    help=""The number of slave nodes"")\nparser.add_argument(""--conda_name"", type=str,\n                    help=""The name of conda environment."")\nparser.add_argument(""--executor_cores"", type=int, default=8,\n                    help=""The number of driver\'s cpu cores you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--executor_memory"", type=str, default=""10g"",\n                    help=""The size of slave(executor)\'s memory you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--driver_memory"", type=str, default=""2g"",\n                    help=""The size of driver\'s memory you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--driver_cores"", type=int, default=8,\n                    help=""The number of driver\'s cpu cores you want to use.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--extra_executor_memory_for_ray"", type=str, default=""20g"",\n                    help=""The extra executor memory to store some data.""\n                    ""You can change it depending on your own cluster setting."")\nparser.add_argument(""--object_store_memory"", type=str, default=""4g"",\n                    help=""The memory to store data on local.""\n                    ""You can change it depending on your own cluster setting."")\n\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n    if args.hadoop_conf:\n        sc = init_spark_on_yarn(\n            hadoop_conf=args.hadoop_conf,\n            conda_name=args.conda_name,\n            num_executor=args.slave_num,\n            executor_cores=args.executor_cores,\n            executor_memory=args.executor_memory,\n            driver_memory=args.driver_memory,\n            driver_cores=args.driver_cores,\n            extra_executor_memory_for_ray=args.extra_executor_memory_for_ray)\n        ray_ctx = RayContext(\n            sc=sc,\n            object_store_memory=args.object_store_memory)\n    else:\n        sc = init_spark_on_local(cores=args.driver_cores)\n        ray_ctx = RayContext(sc=sc, object_store_memory=args.object_store_memory)\n    ray_ctx.init()\n\n    # Simple environment with 4 independent cartpole entities\n    register_env(""multi_cartpole"", lambda _: MultiCartpole(4))\n    single_env = gym.make(""CartPole-v0"")\n    obs_space = single_env.observation_space\n    act_space = single_env.action_space\n\n    # You can also have multiple policies per trainer, but here we just\n    # show one each for PPO and DQN.\n    policies = {\n        ""ppo_policy"": (PPOTFPolicy, obs_space, act_space, {}),\n        ""dqn_policy"": (DQNTFPolicy, obs_space, act_space, {}),\n    }\n\n    def policy_mapping_fn(agent_id):\n        if agent_id % 2 == 0:\n            return ""ppo_policy""\n        else:\n            return ""dqn_policy""\n\n    ppo_trainer = PPOTrainer(\n        env=""multi_cartpole"",\n        config={\n            ""multiagent"": {\n                ""policies"": policies,\n                ""policy_mapping_fn"": policy_mapping_fn,\n                ""policies_to_train"": [""ppo_policy""],\n            },\n            # disable filters, otherwise we would need to synchronize those\n            # as well to the DQN agent\n            ""observation_filter"": ""NoFilter"",\n        })\n\n    dqn_trainer = DQNTrainer(\n        env=""multi_cartpole"",\n        config={\n            ""multiagent"": {\n                ""policies"": policies,\n                ""policy_mapping_fn"": policy_mapping_fn,\n                ""policies_to_train"": [""dqn_policy""],\n            },\n            ""gamma"": 0.95,\n            ""n_step"": 3,\n        })\n\n    # You should see both the printed X and Y approach 200 as this trains:\n    # info:\n    #   policy_reward_mean:\n    #     dqn_policy: X\n    #     ppo_policy: Y\n    for i in range(args.iterations):\n        print(""== Iteration"", i, ""=="")\n\n        # improve the DQN policy\n        print(""-- DQN --"")\n        print(pretty_print(dqn_trainer.train()))\n\n        # improve the PPO policy\n        print(""-- PPO --"")\n        print(pretty_print(ppo_trainer.train()))\n\n        # swap weights to synchronize\n        dqn_trainer.set_weights(ppo_trainer.get_weights([""ppo_policy""]))\n        ppo_trainer.set_weights(dqn_trainer.get_weights([""dqn_policy""]))\n    ray_ctx.stop()\n    sc.stop()\n'"
pyzoo/zoo/examples/serving/Recommendation-ncf/input.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.serving.client import InputQueue, OutputQueue\nimport numpy as np\n# input tensor\nx = np.array([2, 3], dtype=np.float32)\n\ninput_api = InputQueue()\ninput_api.enqueue_tensor(\'my_input\', input=x)\n\noutput_api = OutputQueue()\nresult = output_api.query(\'my_input\')\nprint(""Result is :"" + str(result))\n'"
pyzoo/zoo/examples/streaming/objectdetection/image_path_writer.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.common import convert_to_safe_path\nimport argparse\nimport uuid\nimport sys\nfrom time import sleep\nfrom os import listdir, mkdir, rmdir, access, R_OK, W_OK\nfrom os.path import isfile, join\nimport shutil\n\n\ndef package_path_to_text(streaming_path, file_path, batch=10, delay=3):\n    """"""\n    Package {batch} image paths into text files, such that spark\n    Streaming can read these paths\n    :param streaming_path:\n    :param file_path:\n    :param batch:\n    :param delay:\n    :return:\n    """"""\n    files = []\n\n    # Convert to abs\n    file_path = convert_to_safe_path(file_path, False)\n    streaming_path = convert_to_safe_path(streaming_path, False)\n\n    # check access\n    if not access(file_path, R_OK):\n        sys.stdout.write(\'Not allowed!\\n\')\n        sys.exit()\n\n    if not access(streaming_path, W_OK):\n        sys.stdout.write(\'Not allowed!\\n\')\n        sys.exit()\n\n    for f in listdir(file_path):\n        if isfile(join(file_path, f)):\n            files.append(join(file_path, f) + \'\\n\')\n    index = 0\n    curr = 0\n    tmp_dir = join(""/tmp"", str(uuid.uuid4()))\n    mkdir(tmp_dir)\n    print(""Tmp dir at "" + tmp_dir)\n    while curr < len(files):\n        last = min(curr + batch, len(files))\n        # Because spark textFileStream requires create and move\n        # Write to tmp location\n        batch_file_name = join(streaming_path, str(index))\n        with open(join(tmp_dir, str(index) + "".txt""), ""w"") as text_file:\n            text_file.writelines(files[curr:last])\n        # Move to streaming location\n        shutil.move(text_file.name,\n                    batch_file_name)\n        print(""Writing to "" + batch_file_name)\n        index += 1\n        curr = last\n        sleep(delay)\n    rmdir(tmp_dir)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--img_path\', help=""Path where the images are stored"")\n    parser.add_argument(\'--streaming_path\', help=""Path for streaming text"",\n                        default=""/tmp/zoo/streaming"")\n    args = parser.parse_args()\n    package_path_to_text(args.streaming_path, args.img_path)\n'"
pyzoo/zoo/examples/streaming/objectdetection/streaming_object_detection.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport argparse\nimport cv2\nfrom pyspark.streaming import StreamingContext\n\nfrom PIL import Image\nfrom datetime import datetime\nfrom zoo.common.nncontext import *\nfrom zoo.models.image.objectdetection import *\n\n\ndef read_image_file(path):\n    """"""\n    Read image file into NDArray\n    :param path: String\n    :return: NDArray\n    """"""\n    print(""Reading image from "" + path)\n    img = Image.open(path)\n    nd_img = np.array(img)\n    # print(nd_img.shape)\n    return nd_img\n\n\ndef write_image_file(image, output_path):\n    """"""\n    Write image (NDArray) to file\n    :param image: NDArrary\n    :param output_path: String\n    :return:\n    """"""\n    # The only problem of output result is that\n    # image path is lost after converting to ND array.\n    # So, we set current filename as timestamp with first 10 number.\n    write_path = output_path + \'/\' \\\n        + datetime.now().strftime(""%Y%m%d-%H%M%S"") + \'-\'\\\n        + """".join([str(int(i * 100)) for i in image[:8, 0, 0]])\n    print(""Writing image to "" + write_path)\n    cv2.imwrite(write_path + \'.jpg\', image)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--model\', help=""Path where the model is stored"")\n    parser.add_argument(\'--output_path\', help=""Path for detection results"",\n                        default=""/tmp/zoo/output"")\n    parser.add_argument(\'--streaming_path\', help=""Path for streaming text"",\n                        default=""/tmp/zoo/streaming"")\n    parser.add_argument(""--partition_num"", type=int,\n                        default=1, help=""The number of partitions"")\n\n    args = parser.parse_args()\n\n    sc = init_nncontext(""Streaming Object Detection Example"")\n    ssc = StreamingContext(sc, 3)\n    lines = ssc.textFileStream(args.streaming_path)\n\n    model = ObjectDetector.load_model(args.model)\n\n    def predict(batch_path):\n        if batch_path.getNumPartitions() == 0:\n            return\n        # print(batch_path.top(1))\n        # Read local\n        image_set = DistributedImageSet(batch_path.map(read_image_file))\n        output = model.predict_image_set(image_set)\n        # Save to output\n        config = model.get_config()\n        visualizer = Visualizer(config.label_map(), encoding=""jpg"")\n        visualizer(output).get_image(to_chw=False)\\\n            .foreach(lambda x: write_image_file(x, args.output_path))\n\n    lines.foreachRDD(predict)\n    # Start the computation\n    ssc.start()\n    # Wait for the computation to terminate\n    ssc.awaitTermination()\n'"
pyzoo/zoo/examples/streaming/textclassification/streaming_text_classification.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nfrom optparse import OptionParser\nfrom pyspark.streaming import StreamingContext\n\nfrom zoo.common.nncontext import init_nncontext\nfrom zoo.feature.text import DistributedTextSet\nfrom zoo.models.textclassification import TextClassifier\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--host"", dest=""host"",\n                      default=""localhost"")\n    parser.add_option(""--port"", dest=""port"",\n                      default=""9999"")\n    parser.add_option(""--index_path"", dest=""index_path"")\n    parser.add_option(""--partition_num"", dest=""partition_num"",\n                      default=""4"")\n    parser.add_option(""--sequence_length"", dest=""sequence_length"",\n                      default=""500"")\n    parser.add_option(""-b"", ""--batch_size"", dest=""batch_size"",\n                      default=""128"")\n    parser.add_option(""-m"", ""--model"", dest=""model"")\n    parser.add_option(""--input_file"", dest=""input_file"")\n\n    (options, args) = parser.parse_args(sys.argv)\n\n    sc = init_nncontext(""Text Classification Example"")\n    ssc = StreamingContext(sc, 3)\n    lines = ssc.textFileStream(options.input_file)\n    if not options.input_file:\n        lines = ssc.socketTextStream(options.host, int(options.port))\n\n    model = TextClassifier.load_model(options.model)\n\n    # Labels of 20 Newsgroup dataset\n    labels = [""alt.atheism"",\n              ""comp.graphics"",\n              ""comp.os.ms-windows.misc"",\n              ""comp.sys.ibm.pc.hardware"",\n              ""comp.sys.mac.hardware"",\n              ""comp.windows.x"",\n              ""misc.forsale"",\n              ""rec.autos"",\n              ""rec.motorcycles"",\n              ""rec.sport.baseball"",\n              ""rec.sport.hockey"",\n              ""sci.crypt"",\n              ""sci.electronics"",\n              ""sci.med"",\n              ""sci.space"",\n              ""soc.religion.christian"",\n              ""talk.politics.guns"",\n              ""talk.politics.mideast"",\n              ""talk.politics.misc"",\n              ""talk.religion.misc""]\n\n    def predict(record):\n        if record.getNumPartitions() == 0:\n            return\n        text_set = DistributedTextSet(record)\n        text_set.load_word_index(options.index_path)\n        print(""Processing text..."")\n        transformed = text_set.tokenize().normalize()\\\n            .word2idx()\\\n            .shape_sequence(len=int(options.sequence_length)).generate_sample()\n        predict_set = model.predict(transformed, int(options.partition_num))\n        # Get the first five prediction probability distributions\n        predicts = predict_set.get_predicts().collect()\n        print(""Probability distributions of top-5:"")\n        for p in predicts:\n            (uri, probs) = p\n            for k, v in sorted(enumerate(probs[0]), key=lambda x: x[1])[:5]:\n                print(labels[k] + "" "" + str(v))\n\n    lines.foreachRDD(predict)\n    # Start the computation\n    ssc.start()\n    # Wait for the computation to terminate\n    ssc.awaitTermination()\n'"
pyzoo/zoo/examples/tensorflow/freeze_checkpoint/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/tensorflow/freeze_checkpoint/freeze_checkpoint.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\n\nfrom zoo.util.tf import export_tf\nfrom optparse import OptionParser\n\n\ndef ckpt_to_frozen_graph(options):\n    with tf.gfile.GFile(options.pbPath, \'rb\') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n        var_list_name = [node.name + "":0""\n                         for node in graph_def.node\n                         if node.op in [""Variable"", ""VariableV2"", ""VarHandleOp""]]\n\n    # now build the graph in the memory and visualize it\n    with tf.Session() as sess:\n        graph = tf.get_default_graph()\n        tf.import_graph_def(graph_def, name="""")\n\n        var_list = [graph.get_tensor_by_name(name) for name in var_list_name]\n\n        for v in var_list:\n            tf.add_to_collection(tf.GraphKeys.TRAINABLE_VARIABLES, v)\n\n        saver = tf.train.Saver(var_list)\n        saver.restore(sess, options.ckptPath)\n\n        input_names = options.inputsName.split("","")\n        output_names = options.outputsName.split("","")\n\n        input_tensors = [graph.get_tensor_by_name(name) for name in input_names]\n        output_tensors = [graph.get_tensor_by_name(name) for name in output_names]\n\n        export_tf(sess, options.outputDir, inputs=input_tensors, outputs=output_tensors)\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--pbPath"", dest=""pbPath"",\n                      help=""The path to a TensorFlow pb file"")\n    parser.add_option(""--ckptPath"", dest=""ckptPath"",\n                      help=""The path to a TensorFlow chekpoint file"")\n    parser.add_option(""--inputsName"", dest=""inputsName"",\n                      help=""A comma separated list of Tensor names as the model inputs, ""\n                           ""e.g. input_0:0,input_1:0."")\n    parser.add_option(""--outputsName"", dest=""outputsName"",\n                      help=""A comma separated list of Tensor names as the model outputs, ""\n                           ""e.g. output_0:0,output_1:0."")\n    parser.add_option(""-o"", ""--outputDir"", dest=""outputDir"", default=""."")\n    import sys\n    (options, args) = parser.parse_args(sys.argv)\n    assert options.pbPath is not None, ""--pbPath must be provided""\n    assert options.ckptPath is not None, ""--ckptPath must be provided""\n    assert options.inputsName is not None, ""--inputsName must be provided""\n    assert options.outputsName is not None, ""--outputsName must be provided""\n    ckpt_to_frozen_graph(options)\n'"
pyzoo/zoo/examples/tensorflow/freeze_saved_model/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/tensorflow/freeze_saved_model/freeze.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom optparse import OptionParser\nimport sys\nimport tensorflow as tf\n\nfrom zoo.util.tf import export_tf\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--saved_model_path"", dest=""saved_model_path"",\n                      help=""The path to a TensorFlow saved model"")\n    parser.add_option(""--tag"", default=""serve"", dest=""tag"",\n                      help=""The tag used to load from saved model"")\n    parser.add_option(""--signature"", default=""serving_default"", dest=""signature"",\n                      help=""The signature to find input and output tensors"")\n    parser.add_option(""--input_tensors"", default=None, dest=""input_tensors"",\n                      help=""A comma separated list of Tensor"" +\n                           "" names as the model inputs, e.g. input_0:0,input_1:0.""\n                           "" This will override the ones found in signature."")\n    parser.add_option(""--output_tensors"", default=None, dest=""output_tensors"",\n                      help=""A comma separated list of Tensor"" +\n                           "" names as the model outputs, e.g. output_0:0,output_1:0.""\n                           "" This will override the ones found in signature."")\n    parser.add_option(""--output_path"", dest=""output_path"",\n                      help=""The output frozen model path"")\n    (options, args) = parser.parse_args(sys.argv)\n\n    assert options.saved_model_path is not None, ""--saved_model_path must be provided""\n    assert options.output_path is not None, ""--output_path must be provided""\n\n    with tf.Session() as sess:\n        loaded = tf.saved_model.load(sess, tags=[options.tag],\n                                     export_dir=options.saved_model_path)\n\n        signature = loaded.signature_def[options.signature]\n        if options.input_tensors is None:\n            input_keys = signature.inputs.keys()\n            input_names = [signature.inputs[key].name for key in input_keys]\n            print(""Found inputs in signature {}"".format(options.signature))\n            print(""Inputs are \\n{}"".format(signature.inputs))\n        else:\n            input_names = options.input_tensors.split("","")\n        input_tensors = [tf.get_default_graph().get_tensor_by_name(name) for name in input_names]\n\n        if options.output_tensors is None:\n            outputs_keys = signature.outputs.keys()\n            output_names = [signature.outputs[key].name for key in outputs_keys]\n            print(""Found outputs in signature {}"".format(options.signature))\n            print(""outputs are \\n{}"".format(signature.inputs))\n        else:\n            output_names = options.output_tensors.split("","")\n        output_tensors = [tf.get_default_graph().get_tensor_by_name(name) for name in output_names]\n\n        export_tf(sess, folder=options.output_path, inputs=input_tensors, outputs=output_tensors)\n'"
pyzoo/zoo/examples/tensorflow/tfnet/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/tensorflow/tfnet/predict.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom optparse import OptionParser\n\nfrom zoo.tfpark import TFNet\nfrom zoo.common.nncontext import init_nncontext\nfrom zoo.feature.common import *\nfrom zoo.models.image.objectdetection import *\n\n\ndef predict(model_path, img_path, partition_num=4):\n    inputs = ""image_tensor:0""\n    outputs = [""num_detections:0"", ""detection_boxes:0"",\n               ""detection_scores:0"", ""detection_classes:0""]\n\n    model = TFNet(model_path, inputs, outputs)\n    image_set = ImageSet.read(img_path, sc, partition_num)\n    transformer = ChainedPreprocessing([ImageResize(256, 256), ImageMatToTensor(format=""NHWC""),\n                                        ImageSetToSample()])\n    transformed_image_set = image_set.transform(transformer)\n    output = model.predict_image(transformed_image_set.to_image_frame(), batch_per_partition=1)\n    # Print the detection result of the first image.\n    result = ImageSet.from_image_frame(output).get_predict().first()\n    print(result)\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--image"", type=str, dest=""img_path"",\n                      help=""The path where the images are stored, ""\n                           ""can be either a folder or an image path"")\n    parser.add_option(""--model"", type=str, dest=""model_path"",\n                      help=""The path of the TensorFlow object detection model"")\n    parser.add_option(""--partition_num"", type=int, dest=""partition_num"", default=4,\n                      help=""The number of partitions"")\n    (options, args) = parser.parse_args(sys.argv)\n\n    sc = init_nncontext(""TFNet Object Detection Example"")\n\n    predict(options.model_path, options.img_path, options.partition_num)\n    print(""finished..."")\n    sc.stop()\n'"
pyzoo/zoo/examples/tensorflow/tfpark/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/vnni/openvino/predict.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom optparse import OptionParser\n\nfrom zoo.pipeline.inference import InferenceModel\nfrom zoo.common.nncontext import init_nncontext\nfrom zoo.feature.image import *\nfrom zoo.pipeline.nnframes import *\n\nBATCH_SIZE = 4\n\n\ndef predict(model_path, img_path):\n    model = InferenceModel()\n    model.load_openvino(model_path,\n                        weight_path=model_path[:model_path.rindex(""."")] + "".bin"",\n                        batch_size=BATCH_SIZE)\n    sc = init_nncontext(""OpenVINO Python resnet_v1_50 Inference Example"")\n    # pre-processing\n    infer_transformer = ChainedPreprocessing([ImageBytesToMat(),\n                                             ImageResize(256, 256),\n                                             ImageCenterCrop(224, 224),\n                                             ImageMatToTensor(format=""NHWC"", to_RGB=True)])\n    image_set = ImageSet.read(img_path, sc).\\\n        transform(infer_transformer).get_image().collect()\n    image_set = np.expand_dims(image_set, axis=1)\n\n    for i in range(len(image_set) // BATCH_SIZE + 1):\n        index = i * BATCH_SIZE\n        # check whether out of index\n        if index >= len(image_set):\n            break\n        batch = image_set[index]\n        # put 4 images in one batch\n        for j in range(index + 1, min(index + BATCH_SIZE, len(image_set))):\n            batch = np.vstack((batch, image_set[j]))\n        batch = np.expand_dims(batch, axis=0)\n        # predict batch\n        predictions = model.predict(batch)\n        result = predictions[0]\n\n        # post-processing for Top-1\n        print(""batch_"" + str(i))\n        for r in result:\n            output = {}\n            max_index = np.argmax(r)\n            output[""Top-1""] = str(max_index)\n            print(""* Predict result "" + str(output))\n    print(""finished..."")\n    sc.stop()\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--image"", type=str, dest=""img_path"",\n                      help=""The path where the images are stored, ""\n                           ""can be either a folder or an image path"")\n    parser.add_option(""--model"", type=str, dest=""model_path"",\n                      help=""Zoo Model Path"")\n\n    (options, args) = parser.parse_args(sys.argv)\n    predict(options.model_path, options.img_path)\n'"
pyzoo/zoo/models/image/common/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/models/image/common/image_config.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nfrom bigdl.util.common import JavaValue\nfrom zoo.common.utils import callZooFunc\n\nfrom zoo.feature.common import Preprocessing\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass ImageConfigure(JavaValue):\n    """"""\n    predictor configure\n    :param pre_processor preprocessor of ImageSet before model inference\n    :param post_processor postprocessor of ImageSet after model inference\n    :param batch_per_partition batch size per partition\n    :param label_map mapping from prediction result indexes to real dataset labels\n    :param feature_padding_param featurePaddingParam if the inputs have variant size\n    """"""\n\n    def __init__(self, pre_processor=None,\n                 post_processor=None,\n                 batch_per_partition=4,\n                 label_map=None, feature_padding_param=None, jvalue=None, bigdl_type=""float""):\n        self.bigdl_type = bigdl_type\n        if jvalue:\n            self.value = jvalue\n        else:\n            if pre_processor:\n                assert issubclass(pre_processor.__class__, Preprocessing), \\\n                    ""the pre_processor should be subclass of Preprocessing""\n            if post_processor:\n                assert issubclass(post_processor.__class__, Preprocessing), \\\n                    ""the post_processor should be subclass of Preprocessing""\n            self.value = callZooFunc(\n                bigdl_type, JavaValue.jvm_class_constructor(self),\n                pre_processor,\n                post_processor,\n                batch_per_partition,\n                label_map,\n                feature_padding_param)\n\n    def label_map(self):\n        return callZooFunc(self.bigdl_type, ""getLabelMap"", self.value)\n\n\nclass PaddingParam(JavaValue):\n\n    def __init__(self, bigdl_type=""float""):\n        self.value = callZooFunc(\n            bigdl_type, JavaValue.jvm_class_constructor(self))\n'"
pyzoo/zoo/models/image/common/image_model.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.models.common import ZooModel\nfrom zoo.models.image.common.image_config import ImageConfigure\nfrom zoo.feature.image.imageset import *\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass ImageModel(ZooModel):\n    """"""\n    The basic class for image model.\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        super(ImageModel, self).__init__(None, bigdl_type)\n\n    def predict_image_set(self, image, configure=None):\n        res = callZooFunc(self.bigdl_type, ""imageModelPredict"", self.value,\n                          image, configure)\n        return ImageSet(res)\n\n    def get_config(self):\n        config = callZooFunc(self.bigdl_type, ""getImageConfig"", self.value)\n        return ImageConfigure(jvalue=config)\n'"
pyzoo/zoo/models/image/imageclassification/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom .image_classification import *\n'"
pyzoo/zoo/models/image/imageclassification/image_classification.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.transform.vision.image import FeatureTransformer\nfrom zoo.models.image.common.image_model import ImageModel\nfrom zoo.feature.image.imageset import *\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\ndef read_imagenet_label_map():\n    """"""\n    load imagenet label map\n    """"""\n    return callZooFunc(""float"", ""readImagenetLabelMap"")\n\n\nclass ImageClassifier(ImageModel):\n    """"""\n    A pre-trained image classifier model.\n\n    :param model_path The path containing the pre-trained model\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        self.bigdl_type = bigdl_type\n        super(ImageClassifier, self).__init__(None, bigdl_type)\n\n    @staticmethod\n    def load_model(path, weight_path=None, bigdl_type=""float""):\n        """"""\n        Load an existing object detection model (with weights).\n\n        # Arguments\n        path: The path to save the model. Local file system, HDFS and Amazon S3 are supported.\n              HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n              Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""loadImageClassifier"", path, weight_path)\n        model = ImageModel._do_load(jmodel, bigdl_type)\n        model.__class__ = ImageClassifier\n        return model\n\n\nclass LabelOutput(FeatureTransformer):\n    """"""\n    Label Output tensor with corresponding real labels on specific dataset\n    clses is the key in ImgFeature where you want to store all sorted mapped labels\n    probs is the key in ImgFeature where you want to store all the sorted probilities for each class\n    """"""\n\n    def __init__(self, label_map, clses, probs, bigdl_type=""float""):\n        self.value = callZooFunc(\n            bigdl_type, JavaValue.jvm_class_constructor(self), label_map, clses, probs)\n'"
pyzoo/zoo/models/image/objectdetection/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .object_detector import *\n'"
pyzoo/zoo/models/image/objectdetection/object_detector.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nfrom bigdl.util.common import JavaValue\n\nfrom zoo.models.image.common.image_model import ImageModel\nfrom zoo.common.utils import callZooFunc\nfrom zoo.feature.image.imageset import *\nfrom zoo.feature.image.imagePreprocessing import *\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\ndef read_pascal_label_map():\n    """"""\n    load pascal label map\n    """"""\n    return callZooFunc(""float"", ""readPascalLabelMap"")\n\n\ndef read_coco_label_map():\n    """"""\n    load coco label map\n    """"""\n    return callZooFunc(""float"", ""readCocoLabelMap"")\n\n\nclass ObjectDetector(ImageModel):\n    """"""\n    A pre-trained object detector model.\n\n    :param model_path The path containing the pre-trained model\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        self.bigdl_type = bigdl_type\n        super(ObjectDetector, self).__init__(None, bigdl_type)\n\n    @staticmethod\n    def load_model(path, weight_path=None, bigdl_type=""float""):\n        """"""\n        Load an existing object detection model (with weights).\n\n        # Arguments\n        path: The path to save the model. Local file system, HDFS and Amazon S3 are supported.\n              HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n              Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""loadObjectDetector"", path, weight_path)\n        model = ImageModel._do_load(jmodel, bigdl_type)\n        model.__class__ = ObjectDetector\n        return model\n\n\nclass ImInfo(ImagePreprocessing):\n    """"""\n    Generate imInfo\n    imInfo is a tensor that contains height, width, scaleInHeight, scaleInWidth\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        super(ImInfo, self).__init__(bigdl_type)\n\n\nclass DecodeOutput(ImagePreprocessing):\n    """"""\n    Decode the detection output\n    The output of the model prediction is a 1-dim tensor\n    The first element of tensor is the number(K) of objects detected,\n    followed by [label score x1 y1 x2 y2] X K\n    For example, if there are 2 detected objects, then K = 2, the tensor may\n    looks like\n    ```2, 1, 0.5, 10, 20, 50, 80, 3, 0.3, 20, 10, 40, 70```\n    After decoding, it returns a 2-dim tensor, each row represents a detected object\n    ```\n    1, 0.5, 10, 20, 50, 80\n    3, 0.3, 20, 10, 40, 70\n    ```\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        super(DecodeOutput, self).__init__(bigdl_type)\n\n\nclass ScaleDetection(ImagePreprocessing):\n    """"""\n    If the detection is normalized, for example, ssd detected bounding box is in [0, 1],\n    need to scale the bbox according to the original image size.\n    Note that in this transformer, the tensor from model output will be decoded,\n    just like `DecodeOutput`\n    """"""\n\n    def __init__(self, bigdl_type=""float""):\n        super(ScaleDetection, self).__init__(bigdl_type)\n\n\nclass Visualizer(ImagePreprocessing):\n    """"""\n    Visualizer is a transformer to visualize the detection results\n    (tensors that encodes label, score, boundingbox)\n    You can call image_frame.get_image() to get the visualized results\n    """"""\n\n    def __init__(self, label_map, thresh=0.3, encoding=""png"",\n                 bigdl_type=""float""):\n        self.value = callZooFunc(\n            bigdl_type, JavaValue.jvm_class_constructor(self), label_map, thresh, encoding)\n\n    def __call__(self, image_set, bigdl_type=""float""):\n        """"""\n        transform ImageSet\n        """"""\n        jset = callZooFunc(bigdl_type,\n                           ""transformImageSet"", self.value, image_set)\n        return ImageSet(jvalue=jset)\n'"
pyzoo/zoo/orca/data/pandas/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.orca.data.pandas.preprocessing import read_csv\nfrom zoo.orca.data.pandas.preprocessing import read_json\n'"
pyzoo/zoo/orca/data/pandas/preprocessing.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport random\n\nfrom bigdl.util.common import get_node_and_core_number\nfrom pyspark.context import SparkContext\n\nfrom zoo.ray import RayContext\nfrom zoo.orca.data.shard import RayXShards, RayPartition, SparkXShards\nfrom zoo.orca.data.utils import *\n\n\ndef read_csv(file_path, context, **kwargs):\n    """"""\n    Read csv files to XShards\n    :param file_path: could be a csv file, multiple csv file paths separated by comma,\n     a directory containing csv files.\n     Supported file systems are local file system, hdfs, and s3.\n    :param context: SparkContext or RayContext\n    :return: XShards\n    """"""\n    if isinstance(context, RayContext):\n        return read_file_ray(context, file_path, ""csv"", **kwargs)\n    elif isinstance(context, SparkContext):\n        return read_file_spark(context, file_path, ""csv"", **kwargs)\n    else:\n        raise Exception(""Context type should be RayContext or SparkContext"")\n\n\ndef read_json(file_path, context, **kwargs):\n    """"""\n    Read json files to XShards\n    :param file_path: could be a json file, multiple json file paths separated by comma,\n     a directory containing json files.\n     Supported file systems are local file system, hdfs, and s3.\n    :param context: SparkContext or RayContext\n    :return: XShards\n    """"""\n    if isinstance(context, RayContext):\n        return read_file_ray(context, file_path, ""json"", **kwargs)\n    elif isinstance(context, SparkContext):\n        return read_file_spark(context, file_path, ""json"", **kwargs)\n    else:\n        raise Exception(""Context type should be RayContext or SparkContext"")\n\n\ndef read_file_ray(context, file_path, file_type, **kwargs):\n    file_paths = []\n    # extract all file paths\n    if isinstance(file_path, list):\n        [file_paths.extend(extract_one_path(path, file_type, context.env)) for path in file_path]\n    else:\n        file_paths = extract_one_path(file_path, file_type, context.env)\n\n    num_executors = context.num_ray_nodes\n    num_cores = context.ray_node_cpu_cores\n    num_partitions = num_executors * num_cores\n\n    # split files to partitions\n    random.shuffle(file_paths)\n    # remove empty partitions\n    file_partition_list = [partition for partition\n                           in list(chunk(file_paths, num_partitions)) if partition]\n    import ray\n    # create shard actor to read data\n    Shard = ray.remote(RayPandasShard)\n    shards = [Shard.remote() for i in range(len(file_partition_list))]\n    done_ids, undone_ids = \\\n        ray.wait([shard.read_file_partitions.remote(file_partition_list[i], file_type, **kwargs)\n                  for i, shard in enumerate(shards)], num_returns=len(shards))\n    assert len(undone_ids) == 0\n\n    # create initial partition\n    partitions = [RayPartition([shard]) for shard in shards]\n    data_shards = RayXShards(partitions)\n    return data_shards\n\n\ndef read_file_spark(context, file_path, file_type, **kwargs):\n    file_url_splits = file_path.split(""://"")\n    prefix = file_url_splits[0]\n    node_num, core_num = get_node_and_core_number()\n\n    file_paths = []\n    if isinstance(file_path, list):\n        [file_paths.extend(extract_one_path(path, file_type, os.environ)) for path in file_path]\n    else:\n        file_paths = extract_one_path(file_path, file_type, os.environ)\n\n    rdd = context.parallelize(file_paths, node_num * core_num)\n\n    if prefix == ""hdfs"":\n        def loadFile(iterator):\n            import pandas as pd\n            import pyarrow as pa\n            fs = pa.hdfs.connect()\n\n            for x in iterator:\n                with fs.open(x, \'rb\') as f:\n                    if file_type == ""csv"":\n                        df = pd.read_csv(f, **kwargs)\n                    elif file_type == ""json"":\n                        df = pd.read_json(f, **kwargs)\n                    else:\n                        raise Exception(""Unsupported file type"")\n                    yield df\n\n        pd_rdd = rdd.mapPartitions(loadFile)\n    elif prefix == ""s3"":\n        def loadFile(iterator):\n            access_key_id = os.environ[""AWS_ACCESS_KEY_ID""]\n            secret_access_key = os.environ[""AWS_SECRET_ACCESS_KEY""]\n            import boto3\n            import pandas as pd\n            s3_client = boto3.Session(\n                aws_access_key_id=access_key_id,\n                aws_secret_access_key=secret_access_key,\n            ).client(\'s3\', verify=False)\n            for x in iterator:\n                path_parts = x.split(""://"")[1].split(\'/\')\n                bucket = path_parts.pop(0)\n                key = ""/"".join(path_parts)\n                obj = s3_client.get_object(Bucket=bucket, Key=key)\n                if file_type == ""json"":\n                    df = pd.read_json(obj[\'Body\'], **kwargs)\n                elif file_type == ""csv"":\n                    df = pd.read_csv(obj[\'Body\'], **kwargs)\n                else:\n                    raise Exception(""Unsupported file type"")\n                yield df\n\n        pd_rdd = rdd.mapPartitions(loadFile)\n    else:\n        def loadFile(iterator):\n            import pandas as pd\n            for x in iterator:\n                if file_type == ""csv"":\n                    df = pd.read_csv(x, **kwargs)\n                elif file_type == ""json"":\n                    df = pd.read_json(x, **kwargs)\n                else:\n                    raise Exception(""Unsupported file type"")\n                yield df\n\n        pd_rdd = rdd.mapPartitions(loadFile)\n\n    data_shards = SparkXShards(pd_rdd)\n    return data_shards\n\n\nclass RayPandasShard(object):\n    """"""\n    Actor to read csv/json file to Pandas DataFrame and manipulate data\n    """"""\n\n    def __init__(self, data=None):\n        self.data = data\n\n    def read_file_partitions(self, paths, file_type, **kwargs):\n        df_list = []\n        import pandas as pd\n        prefix = paths[0].split(""://"")[0]\n        if prefix == ""hdfs"":\n            import pyarrow as pa\n            fs = pa.hdfs.connect()\n            for path in paths:\n                with fs.open(path, \'rb\') as f:\n                    if file_type == ""json"":\n                        df = pd.read_json(f, **kwargs)\n                    elif file_type == ""csv"":\n                        df = pd.read_csv(f, **kwargs)\n                    else:\n                        raise Exception(""Unsupported file type"")\n                    df_list.append(df)\n        elif prefix == ""s3"":\n            import boto3\n            access_key_id = os.environ[""AWS_ACCESS_KEY_ID""]\n            secret_access_key = os.environ[""AWS_SECRET_ACCESS_KEY""]\n            s3_client = boto3.Session(\n                aws_access_key_id=access_key_id,\n                aws_secret_access_key=secret_access_key,\n            ).client(\'s3\', verify=False)\n            for path in paths:\n                path_parts = path.split(""://"")[1].split(\'/\')\n                bucket = path_parts.pop(0)\n                key = ""/"".join(path_parts)\n                obj = s3_client.get_object(Bucket=bucket, Key=key)\n                if file_type == ""json"":\n                    df = pd.read_json(obj[\'Body\'], **kwargs)\n                elif file_type == ""csv"":\n                    df = pd.read_csv(obj[\'Body\'], **kwargs)\n                else:\n                    raise Exception(""Unsupported file type"")\n                df_list.append(df)\n        else:\n            for path in paths:\n                if file_type == ""json"":\n                    df = pd.read_json(path, **kwargs)\n                elif file_type == ""csv"":\n                    df = pd.read_csv(path, **kwargs)\n                else:\n                    raise Exception(""Unsupported file type"")\n                df_list.append(df)\n        self.data = pd.concat(df_list)\n        return 0\n\n    def transform(self, func, *args):\n        self.data = func(self.data, *args)\n        return 0\n\n    def get_data(self):\n        return self.data\n'"
pyzoo/zoo/orca/data/tf/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/orca/data/tf/data.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport tensorflow as tf\n\nfrom zoo.tfpark.tf_dataset import TensorMeta\nfrom zoo.util import nest\nfrom zoo import getOrCreateSparkContext, get_node_and_core_number\nfrom zoo.common import callZooFunc\nfrom zoo.feature.common import FeatureSet\nfrom zoo.orca.data.shard import SparkXShards\nfrom zoo.tfpark import TFDataset\n\n\nclass TFDataDataset2(TFDataset):\n\n    def __init__(self, dataset, batch_size,\n                 batch_per_thread,\n                 validation_dataset=None):\n\n        node_num, core_num = get_node_and_core_number()\n\n        if batch_size > 0:\n            num_parts = dataset.xshards.num_partitions()\n            if num_parts != node_num:\n                dataset.xshards = dataset.xshards.repartition(node_num)\n            assert batch_size % node_num == 0, \\\n                ""batch_size should be a multiple of num_shards, got"" \\\n                "" batch_size {}, node_num {}"".format(batch_size, node_num)\n            batch_per_shard = batch_size // node_num\n        elif batch_per_thread > 0:\n            batch_per_shard = batch_per_thread\n        else:\n            raise ValueError(""one of batch_size or batch_per_thread must be larger than 0"")\n\n        self.rdd = dataset.as_graph_rdd(batch_per_shard).cache()\n        meta_info = self.rdd.map(lambda x: x[1]).first()\n        tensor_structure = meta_info[""tensor_structure""]\n        self.init_op_name = meta_info[""init_op_name""]\n        self.output_names = meta_info[""output_names""]\n        self.output_types = meta_info[""output_types""]\n        self.table_init_op = meta_info[""table_init_op""]\n\n        if validation_dataset is not None:\n            self.val_rdd = validation_dataset.as_graph_rdd(batch_per_shard).cache()\n            meta_info = self.val_rdd.map(lambda x: x[1]).first()\n            self.val_init_op_name = meta_info[""init_op_name""]\n            self.val_output_names = meta_info[""output_names""]\n            self.val_output_types = meta_info[""output_types""]\n        else:\n            self.val_rdd = None\n            self.val_init_op_name = None\n            self.val_output_names = None\n            self.val_output_types = None\n\n        super().__init__(tensor_structure, batch_size=batch_size,\n                         batch_per_thread=batch_per_thread,\n                         hard_code_batch_size=False)\n        self.shard_index_op_name = None\n        self.validation_dataset = validation_dataset\n\n    def get_prediction_data(self):\n        jvalue = callZooFunc(""float"", ""createMiniBatchRDDFromTFDataset"",\n                             self.rdd.map(lambda x: x[0]), self.init_op_name, self.table_init_op,\n                             self.output_names, self.output_types, self.shard_index_op_name)\n        rdd = jvalue.value().toJavaRDD()\n        return rdd\n\n    def get_training_data(self):\n        jvalue = callZooFunc(""float"", ""createTFDataFeatureSet"",\n                             self.rdd.map(lambda x: x[0]), self.init_op_name, self.table_init_op,\n                             self.output_names, self.output_types, self.shard_index_op_name)\n        return FeatureSet(jvalue=jvalue)\n\n    def get_validation_data(self):\n        if self.validation_dataset is not None:\n            jvalue = callZooFunc(""float"", ""createTFDataFeatureSet"",\n                                 self.val_rdd.map(lambda x: x[0]), self.init_op_name,\n                                 self.table_init_op, self.output_names,\n                                 self.output_types, self.shard_index_op_name)\n            return FeatureSet(jvalue=jvalue)\n        return None\n\n\nclass Dataset(object):\n\n    """"""\n    Represents a distributed set of elements backed by an RDD,\n    which is created by applying tensorflow dataset transformations\n    on each partitions.\n    """"""\n\n    def __init__(self, xshards, create_dataset_fn):\n        self.xshards = xshards\n        self.create_dataset_fn = create_dataset_fn\n\n    def as_graph_rdd(self, batch_per_shard):\n\n        create_dataset_fn = self.create_dataset_fn\n\n        def to_dataset(iter):\n            data_list = list(iter)\n\n            import tensorflow as tf\n            if not data_list:\n                return []\n\n            datasets = [create_dataset_fn(data) for data in data_list]\n            from functools import reduce\n            dataset = reduce(lambda x, y: x.concatenate(y), datasets)\n            dataset = dataset.batch(batch_per_shard, True)\n            iterator = dataset.make_initializable_iterator()\n            train_next_ops = nest.flatten(iterator.get_next())\n            output_types = [t.as_datatype_enum\n                            for t in nest.flatten(dataset.output_types)]\n\n            init_op_name = iterator.initializer.name\n            table_init_op = tf.tables_initializer().name\n            output_names = [op.name for op in train_next_ops]\n\n            graph = train_next_ops[0].graph\n\n            flatten_shapes = nest.flatten(dataset.output_shapes)\n\n            flatten_shapes = [shape[1:] for shape in flatten_shapes]\n\n            flatten_tensor_structure = [TensorMeta(dtype=output_types[i],\n                                                   shape=list(flatten_shapes[i]),\n                                                   name=""zoo_input_{}"".format(i))\n                                        for i in range(len(flatten_shapes))]\n            structure = dataset.output_types\n            if isinstance(structure, tf.DType):\n                structure = (structure,)\n            tensor_structure = nest.pack_sequence_as(structure,\n                                                     flatten_tensor_structure)\n\n            meta_info = {\n                ""init_op_name"": init_op_name,\n                ""table_init_op"": table_init_op,\n                ""output_names"": output_names,\n                ""output_types"": output_types,\n                ""tensor_structure"": tensor_structure\n            }\n\n            return [(bytearray(graph.as_graph_def().SerializeToString()), meta_info)]\n\n        graph_rdd_and_meta = self.xshards.rdd.mapPartitions(to_dataset)\n        return graph_rdd_and_meta\n\n    @staticmethod\n    def from_tensor_slices(xshards):\n        return TensorSliceDataset(xshards)\n\n    def map(self, map_func):\n\n        return MapDataset(self, map_func)\n\n\nclass TensorSliceDataset(Dataset):\n\n    def __init__(self, xshards):\n        assert isinstance(xshards, SparkXShards), \\\n            ""only datasets backed by a SparkXShards are supported""\n\n        self.xshards = xshards\n\n        def create_dataset_fn(data):\n            return tf.data.Dataset.from_tensor_slices(data)\n        super().__init__(xshards, create_dataset_fn)\n\n\nclass MapDataset(Dataset):\n\n    def __init__(self, input_dataset, map_func):\n\n        create_pre_dataset_fn = input_dataset.create_dataset_fn\n\n        def create_dataset_fn(data):\n            dataset = create_pre_dataset_fn(data)\n            return dataset.map(map_func)\n        super().__init__(xshards=input_dataset.xshards,\n                         create_dataset_fn=create_dataset_fn)\n'"
pyzoo/zoo/orca/learn/horovod/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .horovod_ray_trainer import HorovodRayTrainer\n'"
pyzoo/zoo/orca/learn/horovod/horovod_ray_trainer.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport ray\nimport os\nfrom horovod.run.gloo_run import RendezvousServer, _allocate\nfrom horovod.run.driver import driver_service\nfrom horovod.run.task_fn import _task_fn\nfrom horovod.run.common.util import settings as hvd_settings\nfrom horovod.run.common.util import timeout, secret\nfrom horovod.run.task import task_service\n\n\ndef make_horovod_worker(cores_per_node):\n\n    # todo how to make user func honor this resource restriction\n    @ray.remote(num_cpus=cores_per_node)\n    class HorovodWorker:\n\n        def hostname(self):\n            import socket\n            return socket.gethostname()\n\n        def run(self, env, func):\n            import os\n            os.environ.update(env)\n            return func()\n\n        def task_fn(self, index, driver_addresses, settings):\n            _task_fn(index, driver_addresses, settings)\n\n    return HorovodWorker\n\n\ndef _get_driver_ip(common_intfs):\n    from socket import AF_INET\n    from psutil import net_if_addrs\n    iface = list(common_intfs)[0]\n    driver_ip = None\n    for addr in net_if_addrs()[iface]:\n        if addr.family == AF_INET:\n            driver_ip = addr.address\n\n    if not driver_ip:\n        raise RuntimeError(\n            \'Cannot find an IPv4 address of the common interface.\')\n\n    return driver_ip\n\n\ndef _hosts_to_hosts_spec(hosts):\n    host_to_size = {}\n    host_and_rank_to_worker_idx = {}\n    for i, host in enumerate(hosts):\n        if host not in host_to_size:\n            host_to_size[host] = 0\n        else:\n            host_to_size[host] = host_to_size[host] + 1\n        host_and_rank_to_worker_idx[(host, host_to_size[host])] = i\n\n    for key in host_to_size:\n        host_to_size[key] += 1\n\n    hosts_spec = [""{}:{}"".format(key, host_to_size[key]) for key in host_to_size]\n    return hosts_spec, host_and_rank_to_worker_idx, host_to_size\n\n\ndef _launch_task_servers(all_host_names, host_rank_to_id, driver_addresses, settings, workers):\n\n    result_ids = []\n    for index in range(len(all_host_names)):\n        host_name = all_host_names[index]\n        worker = workers[host_rank_to_id[(host_name, 0)]]\n\n        result_id = worker.task_fn.remote(index, driver_addresses, settings)\n        result_ids.append(result_id)\n\n    return result_ids\n\n\ndef _find_common_network_interface(host_to_size, host_rank_to_id, workers, settings):\n    all_host_names = [k for k in host_to_size]\n    driver = driver_service.HorovodRunDriverService(len(all_host_names), settings.key, settings.nic)\n\n    _launch_task_servers(all_host_names, host_rank_to_id, driver.addresses(), settings, workers)\n\n    # the following code is copied and modified from horovod.run._driver_fn\n    try:\n        # wait for all the hosts to register with the service service.\n        if settings.verbose >= 2:\n            print(\'Waiting for the hosts to acknowledge.\')\n        driver.wait_for_initial_registration(settings.timeout)\n        tasks = [\n            task_service.HorovodRunTaskClient(\n                index,\n                driver.task_addresses_for_driver(index),\n                settings.key,\n                settings.verbose) for index in range(\n                settings.num_hosts)]\n        # Notify all the drivers that the initial registration is complete.\n        for task in tasks:\n            task.notify_initial_registration_complete()\n        if settings.verbose >= 2:\n            print(\'Notified all the hosts that the registration is complete.\')\n        # Each worker should probe the interfaces of the next worker in a ring\n        # manner and filter only the routed ones -- it should filter out\n        # interfaces that are not really connected to any external networks\n        # such as lo0 with address 127.0.0.1.\n        if settings.verbose >= 2:\n            print(\'Waiting for hosts to perform host-to-host \'\n                  \'interface checking.\')\n        driver.wait_for_task_to_task_address_updates(settings.timeout)\n        if settings.verbose >= 2:\n            print(\'Host-to-host interface checking successful.\')\n        # Determine a set of common interfaces for task-to-task communication.\n        common_intfs = set(driver.task_addresses_for_tasks(0).keys())\n        for index in range(1, settings.num_hosts):\n            common_intfs.intersection_update(\n                driver.task_addresses_for_tasks(index).keys())\n        if not common_intfs:\n            raise Exception(\n                \'Unable to find a set of common task-to-task communication \'\n                \'interfaces: %s\'\n                % [(index, driver.task_addresses_for_tasks(index))\n                   for index in range(settings.num_hosts)])\n        return common_intfs\n    finally:\n        driver.shutdown()\n\n\nclass HorovodRayTrainer:\n\n    # todo check whether horovod is built with gloo\n    def __init__(self, ray_ctx, verbose=None, start_timeout=None):\n\n        self.cores_per_node = ray_ctx.ray_node_cpu_cores\n        self.num_nodes = ray_ctx.num_ray_nodes\n        self.worker_class = make_horovod_worker(self.cores_per_node)\n        self.remote_workers = [self.worker_class.remote() for i in range(0, self.num_nodes)]\n\n        hosts = ray.get([worker.hostname.remote() for worker in self.remote_workers])\n        hosts_spec, name_rank_to_id, host_to_size = _hosts_to_hosts_spec(hosts)\n        self.host_alloc_plan = _allocate("","".join(hosts_spec), self.num_nodes)\n        global_rendezv = RendezvousServer(True)\n        global_rendezv_port = global_rendezv.start_server(self.host_alloc_plan)\n\n        if start_timeout is None:\n            start_timeout = int(os.getenv(\'HOROVOD_START_TIMEOUT\', \'30\'))\n\n        tmout = timeout.Timeout(start_timeout,\n                                message=\'Timed out waiting for {activity}. Please \'\n                                        \'check connectivity between servers. You \'\n                                        \'may need to increase the --start-timeout \'\n                                        \'parameter if you have too many servers.\')\n\n        all_host_names = [k for k in host_to_size]\n\n        settings = hvd_settings.Settings(verbose=2 if verbose else 0,\n                                         key=secret.make_secret_key(),\n                                         timeout=tmout,\n                                         num_hosts=len(all_host_names),\n                                         num_proc=self.num_nodes,\n                                         hosts="","".join(hosts_spec))\n\n        common_intfs = _find_common_network_interface(host_to_size, name_rank_to_id,\n                                                      self.remote_workers, settings)\n        iface = list(common_intfs)[0]\n        driver_ip = _get_driver_ip([iface])\n\n        common_envs = {\n            ""HOROVOD_GLOO_RENDEZVOUS_ADDR"": driver_ip,\n            ""HOROVOD_GLOO_RENDEZVOUS_PORT"": str(global_rendezv_port),\n            ""HOROVOD_CONTROLLER"": ""gloo"",\n            ""HOROVOD_CPU_OPERATIONS"": ""gloo"",\n            ""HOROVOD_GLOO_IFACE"": iface,\n            ""PYTHONUNBUFFERED"": \'1\',\n        }\n\n        for key in os.environ:\n            if key.startswith(""HOROVOD""):\n                common_envs[key] = os.environ[key]\n\n        # todo support other Horovod envs\n        self.per_worker_envs = [common_envs.copy() for _ in range(self.num_nodes)]\n        for alloc_info in self.host_alloc_plan:\n            key = (alloc_info.hostname, alloc_info.local_rank)\n            local_envs = self.per_worker_envs[name_rank_to_id[key]]\n            local_envs[""HOROVOD_RANK""] = str(alloc_info.rank)\n            local_envs[""HOROVOD_SIZE""] = str(alloc_info.size)\n            local_envs[""HOROVOD_LOCAL_RANK""] = str(alloc_info.local_rank)\n            local_envs[""HOROVOD_LOCAL_SIZE""] = str(alloc_info.local_size)\n            local_envs[""HOROVOD_CROSS_RANK""] = str(alloc_info.cross_rank)\n            local_envs[""HOROVOD_CROSS_SIZE""] = str(alloc_info.cross_size)\n\n    def train(self, func):\n        ray.wait([self.remote_workers[i].run.remote(self.per_worker_envs[i], func)\n                  for i in range(self.num_nodes)])\n'"
pyzoo/zoo/orca/learn/mxnet/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .mxnet_trainer import MXNetTrainer\nfrom .utils import create_trainer_config\n'"
pyzoo/zoo/orca/learn/mxnet/mxnet_runner.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport time\nimport logging\nimport subprocess\nimport ray.services\nimport mxnet as mx\nimport numpy as np\nfrom mxnet import gluon\nfrom functools import reduce\nfrom zoo.ray.utils import to_list\nfrom zoo.orca.learn.mxnet.utils import find_free_port\nfrom zoo.orca.data.shard import XShards\n\n\nclass MXNetRunner(object):\n    """"""Manages a MXNet model for training.""""""\n\n    def setup_distributed(self, env, config, train_data, model_creator, loss_creator=None,\n                          validation_metrics_creator=None, test_data=None, train_resize_size=None,\n                          eval_metrics_creator=None):\n        logging.basicConfig(level=logging.INFO)  # This can print log messages to console.\n        self.logger = logging.getLogger()\n        assert isinstance(config, dict), ""config must be a dict""\n        for param in [""batch_size"", ""optimizer"", ""optimizer_params"", ""log_interval""]:\n            assert param in config, param + "" must be specified in config""\n        self.config = config\n        self.model_creator = model_creator\n        self.loss_creator = loss_creator\n        self.validation_metrics_creator = validation_metrics_creator\n        self.eval_metircs_creator = eval_metrics_creator\n        self.is_worker = False\n        env[""DMLC_NODE_HOST""] = self.get_node_ip()\n        if env[""DMLC_ROLE""] == ""worker"":\n            self.is_worker = True\n\n        if self.is_worker:\n            os.environ.update(env)\n            self.kv = mx.kv.create(""dist_sync"")\n            # Set seed so that the model on each worker is initialized with the same weights\n            if ""seed"" in self.config:\n                mx.random.seed(self.config[""seed""])\n\n            if isinstance(train_data, XShards):\n                # XShard input\n                # retrieve train data\n                train_data = train_data.get_partitions()\n                train_partition_data = ray.get(train_data[self.kv.rank].get_data())\n                data, label = get_data_label(train_partition_data)\n                self.train_data = mx.io.NDArrayIter(data=data, label=label,\n                                                    batch_size=config[""batch_size""],\n                                                    shuffle=True)\n                if train_resize_size is not None:\n                    self.train_data = mx.io.ResizeIter(self.train_data, train_resize_size)\n                # retrieve val data\n                if test_data is None:\n                    self.val_data = None\n                else:\n                    assert isinstance(test_data, XShards), ""Test data should be an instance of "" \\\n                                                           ""XShards, please check your input""\n                    test_data = test_data.get_partitions()\n                    val_partition_data = ray.get(test_data[self.kv.rank].get_data())\n                    val_data, val_label = get_data_label(val_partition_data)\n                    self.val_data = mx.io.NDArrayIter(data=val_data,\n                                                      label=val_label,\n                                                      batch_size=config[""batch_size""],\n                                                      shuffle=True)\n            else:\n                # data_creator\n                assert callable(train_data), ""Train data should be an instance of xShards or "" \\\n                                             ""a callable function, please check your input""\n                self.train_data = train_data(self.config, self.kv)\n                if test_data is not None:\n                    assert callable(test_data), ""Test data should be an instance of xShards or "" \\\n                                                ""a callable function, please check your input""\n                    self.val_data = test_data(self.config, self.kv)\n                else:\n                    self.val_data = None\n\n            self.model = self.model_creator(self.config)\n            if self.loss_creator:\n                self.loss = self.loss_creator(self.config)\n            else:\n                self.loss = None\n            if self.eval_metircs_creator:\n                self.eval_metrics = self.eval_metircs_creator(self.config)\n            else:\n                self.eval_metrics = None\n            if self.val_data:\n                assert self.validation_metrics_creator, \\\n                    ""Metrics not defined for validation, please specify metrics_creator""\n                self.val_metrics = self.validation_metrics_creator(self.config)\n            else:\n                self.val_metrics = None\n            # For BaseModule, use symbolic API. Otherwise, use imperative API.\n            # TODO: change to Estimator API?\n            if not isinstance(self.model, mx.module.BaseModule):\n                assert self.loss, ""Loss not defined for gluon model, please specify loss_creator""\n                self.trainer = gluon.Trainer(self.model.collect_params(), self.config[""optimizer""],\n                                             optimizer_params=self.config[""optimizer_params""],\n                                             kvstore=self.kv)\n            else:  # Trainer is not needed for symbolic API.\n                self.trainer = None\n        else:  # server\n            # Need to use the environment on each raylet process for the correct python environment.\n            # TODO: Need to kill this process manually?\n            modified_env = os.environ.copy()\n            modified_env.update(env)\n            # For servers, just import mxnet and no need to do anything else\n            subprocess.Popen(""python -c \'import mxnet\'"", shell=True, env=modified_env)\n\n    def train(self, nb_epoch=1):\n        """"""Train the model and update the model parameters.""""""\n        stats = dict()\n        if self.is_worker:\n            start_time = time.time()\n            if self.trainer:  # Imperative API\n                for epoch in range(nb_epoch):\n                    self.train_data.reset()\n                    if self.eval_metrics:\n                        self.eval_metrics.reset()  # metrics will accumulate for one batch\n                    batch_start_time = time.time()\n                    epoch_start_time = time.time()\n                    for i, batch in enumerate(self.train_data):\n                        data = gluon.utils.split_and_load(\n                            batch.data[0].astype(""float32""), ctx_list=[mx.cpu()], batch_axis=0)\n                        label = gluon.utils.split_and_load(\n                            batch.label[0].astype(""float32""), ctx_list=[mx.cpu()], batch_axis=0)\n                        outputs = []\n                        Ls = []\n                        from mxnet import autograd as ag\n                        with ag.record():\n                            for x, y in zip(data, label):\n                                z = self.model(x)  # forward\n                                L = self.loss(z, y)\n                                # store the loss and do backward on a batch for better speed\n                                Ls.append(L)\n                                outputs.append(z)\n                            ag.backward(Ls)\n                        self.trainer.step(batch.data[0].shape[0])\n                        if self.eval_metrics:\n                            self.eval_metrics.update(label, outputs)\n                        if not (i + 1) % self.config[""log_interval""]:\n                            # This would be logged on driver for each worker process.\n                            iteration_log = \\\n                                ""Epoch[%d] Batch[%d]  Speed: %f samples/sec  %s=%f"" \\\n                                % (epoch, i,\n                                   self.config[""batch_size""] / (time.time() - batch_start_time),\n                                   ""loss"", Ls[0].asnumpy().mean())\n                            if self.eval_metrics:\n                                names, accs = self.eval_metrics.get()\n                                names, accs = to_list(names), to_list(accs)\n                                for name, acc in zip(names, accs):\n                                    iteration_log += ""  %s=%f"" % (name, acc)\n                            self.logger.info(iteration_log)\n                        batch_start_time = time.time()\n                    # Epoch time log\n                    self.logger.info(""[Epoch %d] time cost: %f"" %\n                                     (epoch, time.time() - epoch_start_time))\n                    # Epoch metrics log on train data\n                    if self.eval_metrics:\n                        epoch_train_log = ""[Epoch %d] training: "" % epoch\n                        names, accs = self.eval_metrics.get()\n                        names, accs = to_list(names), to_list(accs)\n                        for name, acc in zip(names, accs):\n                            epoch_train_log += ""%s=%f  "" % (name, acc)\n                        self.logger.info(epoch_train_log)\n                    # Epoch metrics log on validation data if any:\n                    if self.val_data:\n                        self.val_metrics.reset()\n                        self.val_data.reset()\n                        for batch in self.val_data:\n                            data = gluon.utils.split_and_load(\n                                batch.data[0].astype(""float32"", copy=False),\n                                ctx_list=[mx.cpu()], batch_axis=0)\n                            label = gluon.utils.split_and_load(\n                                batch.label[0].astype(""float32"", copy=False),\n                                ctx_list=[mx.cpu()], batch_axis=0)\n                            outputs = [self.model(X) for X in data]\n                            self.val_metrics.update(label, outputs)\n                        epoch_val_log = ""[Epoch %d] validation: "" % epoch\n                        names, accs = self.val_metrics.get()\n                        names, accs = to_list(names), to_list(accs)\n                        for name, acc in zip(names, accs):\n                            epoch_val_log += ""%s=%f  "" % (name, acc)\n                        self.logger.info(epoch_val_log)\n                    # TODO: save checkpoints\n                if self.eval_metrics:\n                    names, accs = self.eval_metrics.get()\n                    names, accs = to_list(names), to_list(accs)\n                    for name, acc in zip(names, accs):\n                        stats[name] = acc\n            else:  # Symbolic API\n                # TODO: seems no history (i.e. validation accuracy) returned by fit?\n                if ""init"" not in self.config:\n                    from mxnet.initializer import Uniform\n                    self.config[""init""] = Uniform(0.01)  # This is the default value for MXNet\n                if self.eval_metrics is None:\n                    self.eval_metrics = \'acc\'\n                self.model.fit(train_data=self.train_data,\n                               num_epoch=nb_epoch,\n                               initializer=self.config[""init""],\n                               kvstore=self.kv,\n                               optimizer=self.config[""optimizer""],\n                               optimizer_params=self.config[""optimizer_params""],\n                               eval_data=self.val_data,\n                               eval_metric=self.eval_metrics,\n                               validation_metric=self.val_metrics,\n                               batch_end_callback=mx.callback.Speedometer(\n                                   self.config[""batch_size""], self.config[""log_interval""]),\n                               epoch_end_callback=None if ""model"" not in self.config\n                               else mx.callback.do_checkpoint(self.config[""model""]))\n            epoch_time = time.time() - start_time\n            stats[""epoch_time""] = epoch_time\n        return stats\n\n    def shutdown(self):\n        """"""Attempts to shut down the runner.""""""\n        del self.logger\n        if self.is_worker:\n            del self.kv\n            del self.model\n            del self.train_data\n            del self.val_data\n            del self.trainer\n            del self.loss\n        # TODO: also delete downloaded data as well?\n\n    def get_node_ip(self):\n        """"""Returns the IP address of the current node.""""""\n        return ray.services.get_node_ip_address()\n\n    def find_free_port(self):\n        """"""Finds a free port on the current node.""""""\n        return find_free_port()\n\n\ndef get_data_label(partition_data):\n    def combine_dict(dict1, dict2):\n        return {key: np.concatenate((value, dict2[key]), axis=0)\n                for (key, value) in dict1.items()}\n\n    def combine_list(list1, list2):\n        return [np.concatenate((list1[index], list2[index]), axis=0)\n                for index in range(0, len(list1))]\n\n    data_list = [data[\'x\'] for data in partition_data]\n    label_list = [data[\'y\'] for data in partition_data]\n    if isinstance(partition_data[0][\'x\'], dict):\n        data = reduce(lambda dict1, dict2: combine_dict(dict1, dict2), data_list)\n    elif isinstance(partition_data[0][\'x\'], np.ndarray):\n        data = reduce(lambda array1, array2: np.concatenate((array1, array2), axis=0),\n                      data_list)\n    elif isinstance(partition_data[0][\'x\'], list):\n        data = reduce(lambda list1, list2: combine_list(list1, list2), data_list)\n\n    if isinstance(partition_data[0][\'y\'], dict):\n        label = reduce(lambda dict1, dict2: combine_dict(dict1, dict2), label_list)\n    elif isinstance(partition_data[0][\'y\'], np.ndarray):\n        label = reduce(lambda array1, array2: np.concatenate((array1, array2), axis=0),\n                       label_list)\n    elif isinstance(partition_data[0][\'y\'], list):\n        label = reduce(lambda list1, list2: combine_list(list1, list2), data_list)\n\n    return data, label\n'"
pyzoo/zoo/orca/learn/mxnet/mxnet_trainer.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport logging\nimport subprocess\nimport ray.services\nfrom dmlc_tracker.tracker import get_host_ip\nfrom zoo.orca.learn.mxnet.mxnet_runner import MXNetRunner\nfrom zoo.orca.learn.mxnet.utils import find_free_port\n\n\nclass MXNetTrainer(object):\n    """"""\n    MXNetTrainer provides an automatic setup for synchronous distributed MXNet training.\n\n    :param config: A dictionary for training configurations. Keys must include the following:\n    batch_size, optimizer, optimizer_params, log_interval.\n    optimizer should be an MXNet optimizer or its string representation.\n    optimizer_params should be a dict in companion with the optimizer. It can contain learning_rate\n    and other optimization configurations.\n    log_interval should be an integer, specifying the interval for logging throughput and metrics\n    information (if any) during the training process.\n    You can call create_trainer_config to create the config easily.\n    You can specify ""seed"" in config to set random seed.\n    You can specify ""init"" in seed to set model initializer.\n\n    :param train_data: An instance of xShards or a function that takes config and kv as arguments\n    and returns an MXNet DataIter/DataLoader for training.\n    You can specify data related configurations for this function in the config argument above.\n    kv is an instance of MXNet distributed key-value store. kv.num_workers and kv.rank\n    can be used in this function to split data for different workers if necessary.\n\n    :param model_creator: A function that takes config as argument and returns an MXNet model.\n    The model can be defined either using MXNet symbolic API or imperative(gluon) API.\n\n    :param loss_creator: A function that takes config as argument and returns an MXNet loss.\n    This is not needed for symbolic API where loss is already defined as model output.\n\n    :param train_resize_batch_num: The number of batches per epoch to resize to. Default is None.\n    You might need to specify this if the size of train_data for each worker varies.\n\n    :param eval_metrics_creator: A function that takes config as argument and returns one or\n    a list of MXNet metrics or corresponding string representations of metrics, for example,\n    \'accuracy\'. This is not needed if you don\'t need evaluation on the training data set.\n\n    :param test_data: An instance of xShards or a function that takes config and kv as arguments\n    and returns an MXNet DataIter/DataLoader for testing.\n    You can specify data related configurations for this function in the config argument above.\n    kv is an instance of MXNet distributed key-value store. kv.num_workers and kv.rank\n    can be used in this function to split data for different workers if necessary.\n\n    :param validation_metrics_creator: A function that takes config as argument and returns one or\n    a list of MXNet metrics or corresponding string representations of metrics, for example,\n    \'accuracy\'. This is not needed if you don\'t have validation data throughout the training.\n\n    :param num_workers: The number of workers for distributed training. Default is 1.\n\n    :param num_servers: The number of servers for distributed training. Default is None and in this\n    case it would be equal to the number of workers.\n\n    :param runner_cores: The number of CPU cores allocated for each MXNet worker and server.\n    Default is None. You may need to specify this for better performance.\n    """"""\n    def __init__(self, config, train_data, model_creator,\n                 loss_creator=None, train_resize_batch_num=None, eval_metrics_creator=None,\n                 test_data=None, validation_metrics_creator=None,\n                 num_workers=1, num_servers=None, runner_cores=None):\n        self.config = config\n        self.train_data = train_data\n        self.test_data = test_data\n        self.model_creator = model_creator\n        self.loss_creator = loss_creator\n        self.validation_metrics_creator = validation_metrics_creator\n        self.eval_metrics_creator = eval_metrics_creator\n        self.num_workers = num_workers\n        self.num_servers = num_servers if num_servers else self.num_workers\n        self.train_resize_batch_num = train_resize_batch_num\n\n        # Generate actor class\n        # Add a dummy custom resource: _mxnet_worker and _mxnet_server to diff worker from server\n        # if runner_cores is specified so that we can place one worker and one server on a node\n        # for better performance.\n        Worker = ray.remote(num_cpus=runner_cores, resources={""_mxnet_worker"": 1})(MXNetRunner) \\\n            if runner_cores else ray.remote(MXNetRunner)\n        Server = ray.remote(num_cpus=runner_cores, resources={""_mxnet_server"": 1})(MXNetRunner) \\\n            if runner_cores else ray.remote(MXNetRunner)\n\n        # Start runners: workers followed by servers\n        self.runners = [\n            Worker.remote()\n            for i in range(self.num_workers)\n        ]\n        self.runners += [\n            Server.remote()\n            for i in range(self.num_servers)\n        ]\n\n        # Compute URL for initializing distributed setup\n        ips = ray.get(\n            [runner.get_node_ip.remote() for runner in self.runners])\n        ports = ray.get(\n            [runner.find_free_port.remote() for runner in self.runners])\n        logger = logging.getLogger()\n        logger.info(ips)\n        logger.info(ports)\n\n        env = {\n            ""DMLC_PS_ROOT_URI"": str(get_host_ip()),\n            ""DMLC_PS_ROOT_PORT"": str(find_free_port()),\n            ""DMLC_NUM_SERVER"": str(self.num_servers),\n            ""DMLC_NUM_WORKER"": str(self.num_workers),\n        }\n        envs = []\n        for i in range(self.num_workers):\n            current_env = env.copy()\n            current_env[\'DMLC_ROLE\'] = \'worker\'\n            envs.append(current_env)\n        for i in range(self.num_servers):\n            current_env = env.copy()\n            current_env[\'DMLC_ROLE\'] = \'server\'\n            envs.append(current_env)\n\n        env[\'DMLC_ROLE\'] = \'scheduler\'\n        modified_env = os.environ.copy()\n        modified_env.update(env)\n        # Need to contain system env to run bash\n        # TODO: Need to kill this process manually?\n        subprocess.Popen(""python -c \'import mxnet\'"", shell=True, env=modified_env)\n\n        ray.get([\n            runner.setup_distributed.remote(envs[i], self.config,\n                self.train_data,\n                self.model_creator,\n                self.loss_creator,\n                self.validation_metrics_creator,\n                self.test_data,\n                self.train_resize_batch_num,\n                self.eval_metrics_creator)\n            for i, runner in enumerate(self.runners)\n        ])\n\n    def train(self, nb_epoch=1):\n        """"""Trains an MXNet model for several epochs.""""""\n        stats = ray.get([w.train.remote(nb_epoch) for w in self.runners])\n        return stats\n\n    def shutdown(self):\n        """"""Shuts down runners and releases resources.""""""\n        for runner in self.runners:\n            runner.shutdown.remote()\n            runner.__ray_terminate__.remote()\n\n# TODO: add model save and restore\n# TODO: add predict, evaluate\n'"
pyzoo/zoo/orca/learn/mxnet/utils.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport socket\nfrom contextlib import closing\n\n\ndef find_free_port():\n    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n        s.bind(("""", 0))\n        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        return s.getsockname()[1]\n\n\ndef create_trainer_config(batch_size=32, optimizer=""sgd"", optimizer_params=None,\n                          log_interval=10, seed=None, extra_config=None):\n    if not optimizer_params:\n        optimizer_params = {\'learning_rate\': 0.01}\n    config = {\n        ""batch_size"": batch_size,\n        ""optimizer"": optimizer,\n        ""optimizer_params"": optimizer_params,\n        ""log_interval"": log_interval,\n    }\n    if seed:\n        config[""seed""] = seed\n    if extra_config:\n        assert isinstance(extra_config, dict), ""extra_config must be a dict""\n        config.update(extra_config)\n    return config\n'"
pyzoo/zoo/orca/learn/pytorch/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .pytorch_trainer import PyTorchTrainer\n'"
pyzoo/zoo/orca/learn/pytorch/pytorch_trainer.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport ray\nfrom ray.util.sgd.torch import TorchTrainer\n\n\nclass PyTorchTrainer(object):\n    def __init__(self, model_creator, data_creator, optimizer_creator,\n                 loss_creator=None, scheduler_creator=None, training_operator_cls=None,\n                 initialization_hook=None, config=None, num_workers=1,\n                 use_fp16=False, use_tqdm=False, scheduler_step_freq=""batch""):\n        # Lift TorchTrainer to an Actor so that its local worker would be\n        # created on the cluster as well.\n        RemoteTrainer = ray.remote(TorchTrainer)\n        self.trainer = RemoteTrainer.remote(model_creator=model_creator,\n                                            data_creator=data_creator,\n                                            optimizer_creator=optimizer_creator,\n                                            loss_creator=loss_creator,\n                                            scheduler_creator=scheduler_creator,\n                                            training_operator_cls=training_operator_cls,\n                                            initialization_hook=initialization_hook,\n                                            config=config,\n                                            num_workers=num_workers,\n                                            backend=""gloo"",\n                                            use_fp16=use_fp16,\n                                            use_tqdm=use_tqdm,\n                                            scheduler_step_freq=scheduler_step_freq)\n\n    def train(self, nb_epoch=1):\n        """"""Trains a PyTorch model for several epochs.""""""\n        for i in range(nb_epoch):\n            stats = ray.get(self.trainer.train.remote())\n        return stats\n\n    def shutdown(self, force=False):\n        self.trainer.shutdown(force)\n'"
pyzoo/zoo/orca/learn/tf/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/orca/learn/tf/estimator.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom bigdl.optim.optimizer import MaxIteration, SGD\nfrom zoo.orca.data.shard import SparkXShards\nfrom zoo.orca.data.tf.data import Dataset, TFDataDataset2\nfrom zoo.tfpark import TFEstimator, TFOptimizer, TFPredictor, TFNet, ZooOptimizer\nimport pandas as pd\nimport tensorflow as tf\n\nfrom zoo.tfpark.tf_dataset import TFDataset, TensorMeta\nfrom zoo.util import nest\n\n\nclass Estimator(object):\n    def fit(self, data, steps, **kwargs):\n        pass\n\n    def predict(self, data, **kwargs):\n        pass\n\n    @staticmethod\n    def from_graph(*, inputs, outputs=None,\n                   labels=None, loss=None, optimizer=None,\n                   metrics=None, updates=None,\n                   sess=None, model_dir=None, backend=""spark""):\n        assert backend == ""spark"", ""only spark backend is supported for now""\n        return TFOptimizerWrapper(inputs=inputs,\n                                  outputs=outputs,\n                                  labels=labels,\n                                  loss=loss,\n                                  optimizer=optimizer,\n                                  metrics=metrics, updates=updates,\n                                  sess=sess,\n                                  model_dir=model_dir)\n\n\ndef _xshards_to_tf_dataset(data_shard,\n                           batch_size=-1, batch_per_thread=-1,\n                           validation_data_shard=None):\n    # todo data_shard.head ?\n    import numpy as np\n\n    def check_data_type_and_to_list(data):\n        result = {}\n        assert isinstance(data, dict), ""each shard should be an dict""\n        assert ""x"" in data, ""key x should in each shard""\n        x = data[""x""]\n        if isinstance(x, np.ndarray):\n            new_x = [x]\n        elif isinstance(x, tuple) and all([isinstance(xi, np.ndarray) for xi in x]):\n            new_x = x\n        else:\n            raise ValueError(""value of x should be a ndarray or a tuple of ndarrays"")\n        result[""x""] = new_x\n        if ""y"" in data:\n            y = data[""y""]\n            if isinstance(y, np.ndarray):\n                new_y = [y]\n            elif isinstance(y, tuple) and all([isinstance(yi, np.ndarray) for yi in y]):\n                new_y = y\n            else:\n                raise ValueError(""value of x should be a ndarray or a tuple of ndarrays"")\n            result[""y""] = new_y\n        return result\n\n    def get_spec(data):\n        data = check_data_type_and_to_list(data)\n        feature_spec = [(feat.dtype, feat.shape[1:])\n                        for feat in data[""x""]]\n        if ""y"" in data:\n            label_spec = [(label.dtype, label.shape[1:])\n                          for label in data[""y""]]\n        else:\n            label_spec = None\n        return (feature_spec, label_spec)\n\n    (feature_spec, label_spec) = data_shard.rdd.map(get_spec).first()\n\n    feature_spec = [(tf.dtypes.as_dtype(spec[0]), spec[1]) for spec in feature_spec]\n    label_spec = [(tf.dtypes.as_dtype(spec[0]), spec[1]) for spec in label_spec] \\\n        if label_spec is not None else None\n\n    assert batch_size != -1 or batch_per_thread != -1, \\\n        ""one of batch_size and batch_per_thread should be specified""\n\n    # todo this might be very slow\n    def flatten(data):\n        data = check_data_type_and_to_list(data)\n        features = data[""x""]\n\n        has_label = ""y"" in data\n        labels = data[""y""] if has_label else None\n        length = features[0].shape[0]\n\n        for i in range(length):\n            fs = [feat[i] for feat in features]\n            if has_label:\n                ls = [l[i] for l in labels]\n                yield (fs, ls)\n            else:\n                yield (fs,)\n\n    val_rdd = None if validation_data_shard is None \\\n        else validation_data_shard.rdd.flatMap(flatten)\n\n    dataset = TFDataset.from_rdd(data_shard.rdd.flatMap(flatten),\n                                 features=feature_spec,\n                                 labels=label_spec,\n                                 batch_size=batch_size,\n                                 batch_per_thread=batch_per_thread,\n                                 val_rdd=val_rdd)\n\n    return dataset\n\n\nclass TFOptimizerWrapper(Estimator):\n\n    def __init__(self, *, inputs, outputs, labels, loss,\n                 optimizer,\n                 metrics,\n                 updates, sess,\n                 model_dir):\n        self.inputs = inputs\n        self.outputs = outputs\n        self.labels = labels\n        self.loss = loss\n        if optimizer is not None:\n            assert isinstance(optimizer, tf.train.Optimizer), \\\n                ""optimizer is of type {}, "".format(type(self.optimizer)) + \\\n                ""it should be an instance of tf.train.Optimizer""\n            self.optimizer = ZooOptimizer(optimizer)\n            self.train_op = self.optimizer.minimize(self.loss)\n        else:\n            self.optimizer = None\n            self.train_op = None\n        self.metrics = metrics\n        self.updates = updates\n        if sess is None:\n            self.sess = tf.Session()\n            self.sess.run(tf.global_variables_initializer())\n        else:\n            self.sess = sess\n        self.model_dir = model_dir\n\n    def fit(self, data, steps,\n            batch_size=32,\n            validation_data=None,\n            feed_dict=None,\n            session_config=None):\n\n        assert self.labels is not None, \\\n            ""labels is None; it should not be None in training""\n        assert self.loss is not None, \\\n            ""loss is None; it should not be None in training""\n        assert self.optimizer is not None, \\\n            ""optimizer is None; it not None in training""\n\n        if isinstance(data, SparkXShards):\n            dataset = _xshards_to_tf_dataset(data,\n                                             batch_size=batch_size,\n                                             validation_data_shard=validation_data)\n        elif isinstance(data, Dataset):\n            dataset = TFDataDataset2(data, batch_size=batch_size,\n                                     batch_per_thread=-1,\n                                     validation_dataset=validation_data)\n        else:\n            raise ValueError(""data type {} is not supported; ""\n                             ""it must be created by zoo.orca.data.package"")\n\n        if feed_dict is not None:\n            tensor_with_value = {key: (value, value) for key, value in feed_dict.items()}\n        else:\n            tensor_with_value = None\n\n        optimizer = TFOptimizer.from_train_op(\n            train_op=self.train_op,\n            loss=self.loss,\n            inputs=self.inputs,\n            labels=self.labels,\n            dataset=dataset,\n            metrics=self.metrics,\n            updates=self.updates, sess=self.sess,\n            tensor_with_value=tensor_with_value,\n            session_config=session_config,\n            model_dir=self.model_dir)\n\n        optimizer.optimize(end_trigger=MaxIteration(steps))\n        return self\n\n    def predict(self, data, batch_size=32):\n        assert self.outputs is not None, \\\n            ""output is None, it should not be None in prediction""\n\n        if isinstance(data, SparkXShards):\n            dataset = _xshards_to_tf_dataset(data,\n                                             batch_per_thread=batch_size)\n        elif isinstance(data, Dataset):\n            dataset = TFDataDataset2(data, batch_size=-1,\n                                     batch_per_thread=batch_size)\n        else:\n            raise ValueError(""data must be a SparkXShards or an orca.data.tf.Dataset"")\n\n        flat_inputs = nest.flatten(self.inputs)\n        flat_outputs = nest.flatten(self.outputs)\n        tfnet = TFNet.from_session(sess=self.sess, inputs=flat_inputs, outputs=flat_outputs)\n        return tfnet.predict(dataset)\n'"
pyzoo/zoo/pipeline/api/keras/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/pipeline/api/keras/base.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.nn.layer import Layer\nfrom bigdl.util.common import *\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass ZooKerasCreator(JavaValue):\n    def jvm_class_constructor(self):\n        name = ""createZooKeras"" + self.__class__.__name__\n        print(""creating: "" + name)\n        return name\n\n\nclass ZooCallable(object):\n    def __call__(self, x):\n        """"""\n        Some other modules point to current module\n        :param x: input variables. x is either a Variable or list of Variable.\n        :return: Variable containing current module\n        """"""\n        from zoo.pipeline.api.autograd import Variable\n        return Variable.from_jvalue(callZooFunc(self.bigdl_type,\n                                                ""connectInputs"",\n                                                self,\n                                                to_list(x)))\n\n\nclass InferShape(JavaValue):\n    def __init__(self, bigdl_type=""float""):\n        self.bigdl_type = bigdl_type\n\n    @classmethod\n    def __to_keras_shape(cls, shape):\n        if shape[0] == -1:\n            return tuple([None] + shape[1:])\n        else:\n            return tuple(shape)\n\n    def __process_shape(self, shape):\n        if len(shape) == 1:\n            return self.__to_keras_shape(shape[0])\n        else:\n            return [self.__to_keras_shape(s) for s in shape]\n\n    def get_input_shape(self):\n        """"""\n        Return a list of shape tuples if there are multiple inputs.\n        Return one shape tuple otherwise.\n        """"""\n        input = callZooFunc(self.bigdl_type, ""getInputShape"",\n                            self.value)\n        return self.__process_shape(input)\n\n    def get_output_shape(self):\n        """"""\n        Return a list of shape tuples if there are multiple outputs.\n        Return one shape tuple otherwise.\n        """"""\n        output = callZooFunc(self.bigdl_type, ""getOutputShape"",\n                             self.value)\n        return self.__process_shape(output)\n\n\nclass ZooKerasLayer(ZooKerasCreator, ZooCallable, Layer, InferShape):\n    def __init__(self, jvalue, *args, **kwargs):\n        allowed_kwargs = {""name"", ""bigdl_type""}\n        for kwarg in kwargs.keys():\n            if kwarg not in allowed_kwargs:\n                raise TypeError(""Wrong argument for the layer:"", kwarg)\n        bigdl_type = kwargs.get(""bigdl_type"")\n        if not bigdl_type:\n            bigdl_type = ""float""\n        super(ZooKerasCreator, self).__init__(jvalue, bigdl_type, *args)\n        name = kwargs.get(""name"")\n        if name:\n            self.set_name(name)\n\n    def get_weights_shape(self):\n        """"""\n        :return: None if without weights\n        """"""\n        jshapes = callZooFunc(self.bigdl_type, ""zooGetWeightsShape"",\n                              self.value)\n        return [tuple(jshape) for jshape in jshapes]\n\n    def set_weights(self, weights):\n        """"""\n        Set weights for this layer\n\n        :param weights: a list of numpy arrays which represent weight and bias\n        """"""\n        current_shapes = self.get_weights_shape()\n        assert len(current_shapes) == len(weights), ""The parameters number should be the same""\n        for w, cws in zip(weights, current_shapes):\n            assert w.shape == cws, \\\n                ""The shape of parameter should be the same, but got %s, %s"" % (w.shape, cws)\n\n        tensors = [JTensor.from_ndarray(param, self.bigdl_type) for param in to_list(weights)]\n        callZooFunc(self.bigdl_type, ""zooSetWeights"", self.value, tensors)\n\n    @classmethod\n    def of(cls, jvalue, bigdl_type=""float""):\n        return ZooKerasLayer(jvalue, bigdl_type)\n'"
pyzoo/zoo/pipeline/api/keras/metrics.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.util.common import *\nfrom zoo.pipeline.api.keras.base import ZooKerasCreator\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass AUC(JavaValue):\n    """"""\n    Metric for binary(0/1) classification, support single label and multiple labels.\n\n    # Arguments\n    threshold_num: The number of thresholds. The quality of approximation\n                   may vary depending on threshold_num.\n\n    >>> meter = AUC(20)\n    creating: createAUC\n    """"""\n    def __init__(self, threshold_num=200, bigdl_type=""float""):\n        JavaValue.__init__(self, None, bigdl_type, threshold_num)\n\n\nclass MAE(ZooKerasCreator, JavaValue):\n    """"""\n    Metric for mean absoluate error, similar from MAE criterion\n\n    >>> mae = MAE()\n    creating: createZooKerasMAE\n\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(MAE, self).__init__(None, bigdl_type)\n\n\nclass Accuracy(ZooKerasCreator, JavaValue):\n    """"""\n    Measures top1 accuracy for multi-class classification\n    or accuracy for binary classification.\n\n    # Arguments\n    zero_based_label: Boolean. Whether target labels start from 0. Default is True.\n                      If False, labels start from 1.\n                      Note that this only takes effect for multi-class classification.\n                      For binary classification, labels ought to be 0 or 1.\n\n    >>> acc = Accuracy()\n    creating: createZooKerasAccuracy\n    """"""\n    def __init__(self, zero_based_label=True, bigdl_type=""float""):\n        super(Accuracy, self).__init__(None, bigdl_type,\n                                       zero_based_label)\n\n\nclass SparseCategoricalAccuracy(ZooKerasCreator, JavaValue):\n    """"""\n    Measures top1 accuracy for multi-class classification with sparse target.\n\n    >>> acc = SparseCategoricalAccuracy()\n    creating: createZooKerasSparseCategoricalAccuracy\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(SparseCategoricalAccuracy, self).__init__(None, bigdl_type)\n\n\nclass CategoricalAccuracy(ZooKerasCreator, JavaValue):\n    """"""\n    Measures top1 accuracy for multi-class classification when target is one-hot encoded.\n\n    >>> acc = CategoricalAccuracy()\n    creating: createZooKerasCategoricalAccuracy\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(CategoricalAccuracy, self).__init__(None, bigdl_type)\n\n\nclass BinaryAccuracy(ZooKerasCreator, JavaValue):\n    """"""\n    Measures top1 accuracy for binary classification with zero-based index.\n\n    >>> acc = BinaryAccuracy()\n    creating: createZooKerasBinaryAccuracy\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(BinaryAccuracy, self).__init__(None, bigdl_type)\n\n\nclass Top5Accuracy(ZooKerasCreator, JavaValue):\n    """"""\n    Measures top5 accuracy for multi-class classification.\n\n    # Arguments\n    zero_based_label: Boolean. Whether target labels start from 0. Default is True.\n                      If False, labels start from 1.\n\n    >>> acc = Top5Accuracy()\n    creating: createZooKerasTop5Accuracy\n    """"""\n    def __init__(self, zero_based_label=True, bigdl_type=""float""):\n        super(Top5Accuracy, self).__init__(None, bigdl_type,\n                                           zero_based_label)\n'"
pyzoo/zoo/pipeline/api/keras/models.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom zoo.pipeline.api.utils import remove_batch\nfrom .engine.topology import KerasNet\nfrom bigdl.util.common import to_list\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass Sequential(KerasNet):\n    """"""\n    Container for a sequential model.\n\n    # Arguments\n    name: String to specify the name of the sequential model. Default is None.\n\n    >>> sequential = Sequential(name=""seq1"")\n    creating: createZooKerasSequential\n    """"""\n\n    def __init__(self, jvalue=None, **kwargs):\n        super(Sequential, self).__init__(jvalue, **kwargs)\n\n    # TODO: expose is_built from scala side\n    def is_built(self):\n        try:\n            self.get_output_shape()\n            return True\n        except:\n            return False\n\n    def add(self, model):\n        from zoo.pipeline.api.autograd import Lambda\n        if (isinstance(model, Lambda)):\n            if not self.is_built():\n                if not model.input_shape:\n                    raise Exception(""You should specify inputShape for the first layer"")\n                input_shapes = model.input_shape\n            else:\n                input_shapes = self.get_output_shape()\n            model = model.create(remove_batch(input_shapes))\n        self.value.add(model.value)\n        return self\n\n    @staticmethod\n    def from_jvalue(jvalue, bigdl_type=""float""):\n        """"""\n        Create a Python Model base on the given java value\n        :param jvalue: Java object create by Py4j\n        :return: A Python Model\n        """"""\n        model = Sequential(jvalue=jvalue)\n        model.value = jvalue\n        return model\n\n\nclass Model(KerasNet):\n    """"""\n    Container for a graph model.\n\n    # Arguments\n    input: An input node or a list of input nodes.\n    output: An output node or a list of output nodes.\n    name: String to specify the name of the graph model. Default is None.\n    """"""\n\n    def __init__(self, input, output, jvalue=None, **kwargs):\n        super(Model, self).__init__(jvalue,\n                                    to_list(input),\n                                    to_list(output),\n                                    **kwargs)\n\n    def save_graph_topology(self, log_path, backward=False):\n        """"""\n        Save the current model graph to a folder, which can be displayed in TensorBoard\n        by running the command:\n        tensorboard --logdir log_path\n\n        # Arguments\n        log_path: The path to save the model graph.\n        backward: The name of the application.\n        """"""\n        callZooFunc(self.bigdl_type, ""zooSaveGraphTopology"",\n                    self.value,\n                    log_path,\n                    backward)\n\n    def new_graph(self, outputs):\n        value = callZooFunc(self.bigdl_type, ""newGraph"", self.value, outputs)\n        return self.from_jvalue(value)\n\n    def freeze_up_to(self, names):\n        callZooFunc(self.bigdl_type, ""freezeUpTo"", self.value, names)\n\n    def unfreeze(self, names):\n        callZooFunc(self.bigdl_type, ""unFreeze"", self.value, names)\n\n    @staticmethod\n    def from_jvalue(jvalue, bigdl_type=""float""):\n        """"""\n        Create a Python Model base on the given java value\n        :param jvalue: Java object create by Py4j\n        :return: A Python Model\n        """"""\n        model = Model([], [], jvalue=jvalue)\n        model.value = jvalue\n        return model\n'"
pyzoo/zoo/pipeline/api/keras/objectives.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom zoo.pipeline.api.keras.base import ZooKerasCreator\nfrom bigdl.nn.criterion import Criterion\nfrom bigdl.util.common import JTensor\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass LossFunction(ZooKerasCreator, Criterion):\n    """"""\n    The base class for Keras-style API objectives in Analytics Zoo.\n    """"""\n    def __init__(self, jvalue, bigdl_type, *args):\n        super(Criterion, self).__init__(jvalue, bigdl_type, *args)\n\n    @classmethod\n    def of(cls, jloss, bigdl_type=""float""):\n        """"""\n        Create a Python LossFunction from a JavaObject.\n\n        # Arguments\n        jloss: A java criterion object which created by Py4j\n        """"""\n        loss = LossFunction(bigdl_type, jloss)\n        loss.value = jloss\n        loss.bigdl_type = bigdl_type\n        return loss\n\n\nclass SparseCategoricalCrossEntropy(LossFunction):\n    """"""\n    A loss often used in multi-class classification problems with SoftMax\n    as the last layer of the neural network.\n\n    By default, same as Keras, input(y_pred) is supposed to be probabilities of each class,\n    and target(y_true) is supposed to be the class label starting from 0.\n\n    # Arguments\n    log_prob_as_input: Boolean. Whether to accept log-probabilities or probabilities\n                       as input. Default is False and inputs should be probabilities.\n    zero_based_label: Boolean. Whether target labels start from 0. Default is True.\n                      If False, labels start from 1.\n    weights: A Numpy array. Weights of each class if you have an unbalanced training set.\n    size_average: Boolean. Whether losses are averaged over observations for each\n                  mini-batch. Default is True. If False, the losses are instead\n                  summed for each mini-batch.\n    padding_value: Int. If the target is set to this value, the training process\n                   will skip this sample. In other words, the forward process will\n                   return zero output and the backward process will also return\n                   zero grad_input. Default is -1.\n\n    >>> loss = SparseCategoricalCrossEntropy()\n    creating: createZooKerasSparseCategoricalCrossEntropy\n    >>> import numpy as np\n    >>> np.random.seed(1128)\n    >>> weights = np.random.uniform(0, 1, (2,)).astype(""float32"")\n    >>> loss = SparseCategoricalCrossEntropy(weights=weights)\n    creating: createZooKerasSparseCategoricalCrossEntropy\n    """"""\n    def __init__(self, log_prob_as_input=False, zero_based_label=True,\n                 weights=None, size_average=True, padding_value=-1, bigdl_type=""float""):\n        super(SparseCategoricalCrossEntropy, self).__init__(None, bigdl_type,\n                                                            log_prob_as_input,\n                                                            zero_based_label,\n                                                            JTensor.from_ndarray(weights),\n                                                            size_average,\n                                                            padding_value)\n\n\nclass MeanAbsoluteError(LossFunction):\n    """"""\n    A loss that measures the mean absolute value of the element-wise difference\n    between the input and the target.\n\n    # Arguments\n    size_average: Boolean. Whether losses are averaged over observations for each\n              mini-batch. Default is True. If False, the losses are instead\n              summed for each mini-batch.\n\n    >>> loss = MeanAbsoluteError()\n    creating: createZooKerasMeanAbsoluteError\n    """"""\n    def __init__(self, size_average=True, bigdl_type=""float""):\n        super(MeanAbsoluteError, self).__init__(None, bigdl_type,\n                                                size_average)\n\n\nmae = MAE = MeanAbsoluteError\n\n\nclass BinaryCrossEntropy(LossFunction):\n    """"""\n    A loss that measures the Binary Cross Entropy between the target and the output\n\n    # Arguments\n    size_average: Boolean. Whether losses are averaged over observations for each\n                mini-batch. Default is True. If False, the losses are instead\n                summed for each mini-batch.\n    weights: weights over the input dimension\n\n    >>> loss = BinaryCrossEntropy()\n    creating: createZooKerasBinaryCrossEntropy\n    """"""\n    def __init__(self, weights=None, size_average=True, bigdl_type=""float""):\n        super(BinaryCrossEntropy, self).__init__(None, bigdl_type,\n                                                 JTensor.from_ndarray(weights),\n                                                 size_average)\n\n\nclass CategoricalCrossEntropy(LossFunction):\n    """"""\n    This is same with cross entropy criterion, except the target tensor is a one-hot tensor\n\n    >>> loss = CategoricalCrossEntropy()\n    creating: createZooKerasCategoricalCrossEntropy\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(CategoricalCrossEntropy, self).__init__(None, bigdl_type)\n\n\nclass CosineProximity(LossFunction):\n    """"""\n    The negative of the mean cosine proximity between predictions and targets.\n    The cosine proximity is defined as below:\n        x\'(i) = x(i) / sqrt(max(sum(x(i)^2), 1e-12))\n        y\'(i) = y(i) / sqrt(max(sum(x(i)^2), 1e-12))\n        cosine_proximity(x, y) = mean(-1 * x\'(i) * y\'(i))\n\n    >>> loss = CosineProximity()\n    creating: createZooKerasCosineProximity\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(CosineProximity, self).__init__(None, bigdl_type)\n\n\nclass Hinge(LossFunction):\n    """"""\n    Creates a criterion that optimizes a two-class classification\n    hinge loss (margin-based loss) between input x (a Tensor of dimension 1) and output y.\n\n    # Arguments:\n    margin: Float. Default is 1.0.\n    size_average: Boolean. Whether losses are averaged over observations for each\n                  mini-batch. Default is True. If False, the losses are instead\n                  summed for each mini-batch.\n\n    >>> loss = Hinge()\n    creating: createZooKerasHinge\n    """"""\n    def __init__(self, margin=1.0, size_average=True, bigdl_type=""float""):\n        super(Hinge, self).__init__(None, bigdl_type, float(margin), size_average)\n\n\nclass KullbackLeiblerDivergence(LossFunction):\n    """"""\n    Loss calculated as:y_true = K.clip(y_true, K.epsilon(), 1)\n    y_pred = K.clip(y_pred, K.epsilon(), 1)\n    and output K.sum(y_true * K.log(y_true / y_pred), axis=-1)\n\n    >>> loss = KullbackLeiblerDivergence()\n    creating: createZooKerasKullbackLeiblerDivergence\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(KullbackLeiblerDivergence, self).__init__(None, bigdl_type)\n\n\nclass MeanAbsolutePercentageError(LossFunction):\n    """"""\n    It caculates diff = K.abs((y - x) / K.clip(K.abs(y), K.epsilon(), Double.MaxValue))\n    and return 100 * K.mean(diff) as outpout\n\n    >>> loss = MeanAbsolutePercentageError()\n    creating: createZooKerasMeanAbsolutePercentageError\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(MeanAbsolutePercentageError, self).__init__(None, bigdl_type)\n\n\nmape = MAPE = MeanAbsolutePercentageError\n\n\nclass MeanSquaredError(LossFunction):\n    """"""\n    A loss that measures the mean squared value of the element-wise difference\n    between the input and the target.\n\n    # Arguments\n    size_average: Boolean. Whether losses are averaged over observations for each\n              mini-batch. Default is True. If False, the losses are instead\n              summed for each mini-batch.\n\n    >>> loss = MeanSquaredError()\n    creating: createZooKerasMeanSquaredError\n    """"""\n    def __init__(self, size_average=True, bigdl_type=""float""):\n        super(MeanSquaredError, self).__init__(None, bigdl_type,\n                                               size_average)\n\n\nmse = MSE = MeanSquaredError\n\n\nclass MeanSquaredLogarithmicError(LossFunction):\n    """"""\n    It calculates:\n    first_log = K.log(K.clip(y, K.epsilon(), Double.MaxValue) + 1.)\n    second_log = K.log(K.clip(x, K.epsilon(), Double.MaxValue) + 1.)\n    and output K.mean(K.square(first_log - second_log))\n\n    >>> loss = MeanSquaredLogarithmicError()\n    creating: createZooKerasMeanSquaredLogarithmicError\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(MeanSquaredLogarithmicError, self).__init__(None, bigdl_type)\n\n\nmsle = MSLE = MeanSquaredLogarithmicError\n\n\nclass Poisson(LossFunction):\n    """"""\n    Loss calculated as: K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()), axis=-1)\n\n    >>> loss = Poisson()\n    creating: createZooKerasPoisson\n    """"""\n    def __init__(self, bigdl_type=""float""):\n        super(Poisson, self).__init__(None, bigdl_type)\n\n\nclass SquaredHinge(LossFunction):\n    """"""\n    Creates a criterion that optimizes a two-class classification\n    squared hinge loss (margin-based loss)\n    between input x (a Tensor of dimension 1) and output y.\n\n    # Arguments:\n    margin: Float. Default is 1.0.\n    size_average: Boolean. Whether losses are averaged over observations for each\n                  mini-batch. Default is True. If False, the losses are instead\n                  summed for each mini-batch.\n\n    >>> loss = SquaredHinge()\n    creating: createZooKerasSquaredHinge\n    """"""\n    def __init__(self, margin=1.0, size_average=False, bigdl_type=""float""):\n        super(SquaredHinge, self).__init__(None, bigdl_type, float(margin), size_average)\n\n\nclass RankHinge(LossFunction):\n    """"""\n    Hinge loss for pairwise ranking problems.\n\n    # Arguments:\n    margin: Float. Default is 1.0.\n\n    >>> loss = RankHinge()\n    creating: createZooKerasRankHinge\n    """"""\n    def __init__(self, margin=1.0, bigdl_type=""float""):\n        super(RankHinge, self).__init__(None, bigdl_type, float(margin))\n'"
pyzoo/zoo/pipeline/api/keras/optimizers.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.util.common import *\nfrom bigdl.optim.optimizer import OptimMethod, Default\nfrom zoo.pipeline.api.keras.base import ZooKerasCreator\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass Adam(OptimMethod, ZooKerasCreator):\n    """"""\n    An implementation of Adam with learning rate schedule.\n    >>> adam = Adam()\n    creating: createZooKerasAdam\n    creating: createDefault\n    """"""\n\n    def __init__(self,\n                 lr=1e-3,\n                 beta_1=0.9,\n                 beta_2=0.999,\n                 epsilon=1e-8,\n                 decay=0.0,\n                 schedule=None,\n                 bigdl_type=""float""):\n        """"""\n        :param lr learning rate\n        :param beta_1 first moment coefficient\n        :param beta_2 second moment coefficient\n        :param epsilon for numerical stability\n        :param decay learning rate decay\n        :param schedule learning rate schedule, e.g. Warmup or Poly from BigDL\n        """"""\n\n        # explicitly reimplement the constructor since:\n        # 1. This class need to be a subclass of OptimMethod\n        # 2. The constructor of OptimMethod invokes JavaValue.jvm_class_constructor() directly\n        #    and does not take the polymorphism.\n        self.value = callZooFunc(\n            bigdl_type, ZooKerasCreator.jvm_class_constructor(self),\n            lr,\n            beta_1,\n            beta_2,\n            epsilon,\n            decay,\n            schedule if (schedule) else Default()\n        )\n        self.bigdl_type = bigdl_type\n\n\nclass AdamWeightDecay(OptimMethod, ZooKerasCreator):\n    """"""\n    >>> adam = AdamWeightDecay()\n    creating: createZooKerasAdamWeightDecay\n    """"""\n\n    def __init__(self,\n                 lr=1e-3,\n                 warmup_portion=-1.0,\n                 total=-1,\n                 schedule=""linear"",\n                 beta1=0.9,\n                 beta2=0.999,\n                 epsilon=1e-6,\n                 weight_decay=0.01,\n                 bigdl_type=""float""):\n        """"""\n        :param lr learning rate\n        :param warmupPortion portion of total for the warmup, -1 means no warmup. Default: -1\n        :param total total number of training steps for the learning\n         rate schedule, -1 means constant learning rate. Default: -1\n        :param schedule schedule to use for the warmup. Default: \'linear\'\n        :param beta1 first moment coefficient\n        :param beta2 second moment coefficient\n        :param epsilon for numerical stability\n        :param weightDecay weight decay\n        """"""\n\n        # explicitly reimplement the constructor since:\n        # 1. This class need to be a subclass of OptimMethod\n        # 2. The constructor of OptimMethod invokes JavaValue.jvm_class_constructor() directly\n        #    and does not take the polymorphism.\n        self.value = callZooFunc(\n            bigdl_type, ZooKerasCreator.jvm_class_constructor(self),\n            lr,\n            warmup_portion,\n            total,\n            schedule,\n            beta1,\n            beta2,\n            epsilon,\n            weight_decay)\n        self.bigdl_type = bigdl_type\n\n\nclass PolyEpochDecay(ZooKerasCreator):\n    """"""\n    A learning rate decay policy, where the effective learning rate\n    follows a polynomial decay, to be zero by the max_epochs.\n    Calculation: init_lr * (1 - epoch/max_iteration) ^ (power)\n\n\n    :param power: The coefficient of decay.\n    :param max_epochs: The maximum number of epochs when lr becomes zero.\n\n    >>> poly = PolyEpochDecay(0.5, 5)\n    creating: createZooKerasPolyEpochDecay\n    """"""\n\n    def __init__(self, power, max_epochs, bigdl_type=""float""):\n        JavaValue.__init__(self, None, bigdl_type, power, max_epochs)\n'"
pyzoo/zoo/pipeline/api/keras/regularizers.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom bigdl.optim.optimizer import *\n\n\ndef l1(l1=0.01):\n    return L1Regularizer(l1=l1)\n\n\ndef l2(l2=0.01):\n    return L2Regularizer(l2=l2)\n\n\ndef l1l2(l1=0.01, l2=0.01):\n    return L1L2Regularizer(l1=l1, l2=l2)\n'"
pyzoo/zoo/pipeline/api/keras/utils.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.optim.optimizer import *\nfrom zoo.pipeline.api.keras.objectives import *\nfrom zoo.pipeline.api.keras import metrics\n\n\ndef to_bigdl_optim_method(optimizer):\n    optimizer = optimizer.lower()\n    if optimizer == ""adagrad"":\n        return Adagrad(learningrate=0.01)\n    elif optimizer == ""sgd"":\n        return SGD(learningrate=0.01)\n    elif optimizer == ""adam"":\n        return Adam()\n    elif optimizer == ""rmsprop"":\n        return RMSprop(learningrate=0.001, decayrate=0.9)\n    elif optimizer == ""adadelta"":\n        return Adadelta(decayrate=0.95, epsilon=1e-8)\n    elif optimizer == ""adamax"":\n        return Adamax(epsilon=1e-8)\n    else:\n        raise TypeError(""Unsupported optimizer: %s"" % optimizer)\n\n\ndef to_bigdl_criterion(criterion):\n    criterion = criterion.lower()\n    if criterion == ""categorical_crossentropy"":\n        return CategoricalCrossEntropy()\n    elif criterion == ""mse"" or criterion == ""mean_squared_error"":\n        return MeanSquaredError()\n    elif criterion == ""binary_crossentropy"":\n        return BinaryCrossEntropy()\n    elif criterion == ""mae"" or criterion == ""mean_absolute_error"":\n        return mae()\n    elif criterion == ""hinge"":\n        return Hinge()\n    elif criterion == ""mean_absolute_percentage_error"" or criterion == ""mape"":\n        return MeanAbsolutePercentageError()\n    elif criterion == ""mean_squared_logarithmic_error"" or criterion == ""msle"":\n        return MeanSquaredLogarithmicError()\n    elif criterion == ""squared_hinge"":\n        return SquaredHinge()\n    elif criterion == ""sparse_categorical_crossentropy"":\n        return SparseCategoricalCrossEntropy()\n    elif criterion == ""kullback_leibler_divergence"" or criterion == ""kld"":\n        return KullbackLeiblerDivergence()\n    elif criterion == ""poisson"":\n        return Poisson()\n    elif criterion == ""cosine_proximity"" or criterion == ""cosine"":\n        return CosineProximity()\n    elif criterion == ""rank_hinge"":\n        return RankHinge()\n    else:\n        raise TypeError(""Unsupported loss: %s"" % criterion)\n\n\ndef to_bigdl_metric(metric, loss):\n    metric = metric.lower()\n    loss_str = (loss if isinstance(loss, six.string_types) else loss.__class__.__name__).lower()\n    if metric == ""accuracy"" or metric == ""acc"":\n        if loss_str == ""sparse_categorical_crossentropy""\\\n                or loss_str == ""sparsecategoricalcrossentropy"":\n            return metrics.SparseCategoricalAccuracy()\n        elif loss_str == ""categorical_crossentropy""\\\n                or loss_str == ""categoricalcrossentropy"":\n            return metrics.CategoricalAccuracy()\n        elif loss_str == ""binary_crossentropy""\\\n                or loss_str == ""binarycrossentropy"":\n            return metrics.BinaryAccuracy()\n        else:\n            raise TypeError(\n                ""Not supported combination: metric {} and loss {}"".format(metric, loss_str))\n    elif metric == ""top5accuracy"" or metric == ""top5acc"":\n        return metrics.Top5Accuracy()\n    elif metric == ""mae"":\n        return metrics.MAE()\n    elif metric == ""auc"":\n        return metrics.AUC()\n    elif metric == ""loss"":\n        return Loss(to_bigdl_criterion(loss_str))\n    elif metric == ""treennaccuracy"":\n        return TreeNNAccuracy()\n    else:\n        raise TypeError(""Unsupported metric: %s"" % metric)\n\n\ndef to_bigdl_metrics(metrics, loss):\n    return [to_bigdl_metric(m, loss) for m in metrics]\n'"
pyzoo/zoo/pipeline/api/keras2/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/pipeline/api/keras2/base.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom bigdl.nn.layer import Layer\nfrom bigdl.util.common import JavaValue\n\nfrom zoo.pipeline.api.keras.base import ZooKerasLayer\n\n\nclass ZooKeras2Creator(JavaValue):\n    def jvm_class_constructor(self):\n        name = ""createZooKeras2"" + self.__class__.__name__\n        print(""creating: "" + name)\n        return name\n\n\nclass ZooKeras2Layer(ZooKeras2Creator, ZooKerasLayer):\n    pass\n'"
pyzoo/zoo/pipeline/api/net/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .graph_net import GraphNet\nfrom .net_load import Net\nfrom zoo.tfpark.tfnet import TFNet\n'"
pyzoo/zoo/pipeline/api/net/graph_net.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom bigdl.nn.layer import Model as BModel\nfrom zoo.feature.image import ImageSet\nfrom zoo.feature.text import TextSet\nfrom zoo.pipeline.api.keras.base import ZooKerasLayer\nfrom zoo.pipeline.api.keras.utils import *\nfrom bigdl.nn.layer import Layer\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass GraphNet(BModel):\n    def __init__(self, input, output, jvalue=None, bigdl_type=""float"", **kwargs):\n        super(BModel, self).__init__(jvalue,\n                                     to_list(input),\n                                     to_list(output),\n                                     bigdl_type,\n                                     **kwargs)\n\n    def predict(self, x, batch_per_thread=4, distributed=True):\n        """"""\n        Use a model to do prediction.\n\n        # Arguments\n        x: Prediction data. A Numpy array or RDD of Sample or ImageSet.\n        batch_per_thread:\n          The default value is 4.\n          When distributed is True,the total batch size is batch_per_thread * rdd.getNumPartitions.\n          When distributed is False the total batch size is batch_per_thread * numOfCores.\n        distributed: Boolean. Whether to do prediction in distributed mode or local mode.\n                     Default is True. In local mode, x must be a Numpy array.\n        """"""\n        if isinstance(x, ImageSet) or isinstance(x, TextSet):\n            results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                  self.value,\n                                  x,\n                                  batch_per_thread)\n            return ImageSet(results) if isinstance(x, ImageSet) else TextSet(results)\n        if distributed:\n            if isinstance(x, np.ndarray):\n                data_rdd = to_sample_rdd(x, np.zeros([x.shape[0]]))\n            elif isinstance(x, RDD):\n                data_rdd = x\n            else:\n                raise TypeError(""Unsupported prediction data type: %s"" % type(x))\n            results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                  self.value,\n                                  data_rdd,\n                                  batch_per_thread)\n            return results.map(lambda result: Layer.convert_output(result))\n        else:\n            if isinstance(x, np.ndarray) or isinstance(x, list):\n                results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                      self.value,\n                                      self._to_jtensors(x),\n                                      batch_per_thread)\n                return [Layer.convert_output(result) for result in results]\n            else:\n                raise TypeError(""Unsupported prediction data type: %s"" % type(x))\n\n    def flattened_layers(self, include_container=False):\n        jlayers = callZooFunc(self.bigdl_type, ""getFlattenSubModules"", self, include_container)\n        layers = [Layer.of(jlayer) for jlayer in jlayers]\n        return layers\n\n    @property\n    def layers(self):\n        jlayers = callZooFunc(self.bigdl_type, ""getSubModules"", self)\n        layers = [Layer.of(jlayer) for jlayer in jlayers]\n        return layers\n\n    @staticmethod\n    def from_jvalue(jvalue, bigdl_type=""float""):\n        """"""\n        Create a Python Model base on the given java value\n\n        :param jvalue: Java object create by Py4j\n        :return: A Python Model\n        """"""\n        model = GraphNet([], [], jvalue=jvalue, bigdl_type=bigdl_type)\n        model.value = jvalue\n        return model\n\n    def new_graph(self, outputs):\n        """"""\n        Specify a list of nodes as output and return a new graph using the existing nodes\n\n        :param outputs: A list of nodes specified\n        :return: A graph model\n        """"""\n        value = callZooFunc(self.bigdl_type, ""newGraph"", self.value, outputs)\n        return self.from_jvalue(value, self.bigdl_type)\n\n    def freeze_up_to(self, names):\n        """"""\n        Freeze the model from the bottom up to the layers specified by names (inclusive).\n        This is useful for finetuning a model\n\n        :param names: A list of module names to be Freezed\n        :return: current graph model\n        """"""\n        callZooFunc(self.bigdl_type, ""freezeUpTo"", self.value, names)\n\n    def unfreeze(self, names=None):\n        """"""\n        ""unfreeze"" module, i.e. make the module parameters(weight/bias, if exists)\n        to be trained(updated) in training process.\n        If \'names\' is a non-empty list, unfreeze layers that match given names\n\n        :param names: list of module names to be unFreezed. Default is None.\n        :return: current graph model\n        """"""\n        callZooFunc(self.bigdl_type, ""unFreeze"", self.value, names)\n\n    def to_keras(self):\n        value = callZooFunc(self.bigdl_type, ""netToKeras"", self.value)\n        return ZooKerasLayer.of(value, self.bigdl_type)\n'"
pyzoo/zoo/pipeline/api/net/net_load.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport importlib\nimport os\nimport sys\n\nfrom zoo.common.utils import callZooFunc\nfrom bigdl.nn.layer import Model as BModel\nfrom zoo.pipeline.api.net.graph_net import GraphNet\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass JavaToPython:\n    # TODO: Add more mapping here as it only support Model and Sequential for now.\n    def __init__(self, jvalue, bigdl_type=""float""):\n        self.jvaule = jvalue\n        self.jfullname = callZooFunc(bigdl_type,\n                                     ""getRealClassNameOfJValue"",\n                                     jvalue)\n\n    def get_python_class(self):\n        """"""\n        Redirect the jvalue to the proper python class.\n        :param jvalue: Java object create by Py4j\n        :return: A proper Python wrapper which would be a Model, Sequential...\n        """"""\n\n        jpackage_name = ""."".join(self.jfullname.split(""."")[:-1])\n        pclass_name = self._get_py_name(self.jfullname.split(""."")[-1])\n        base_module = self._load_ppackage_by_jpackage(jpackage_name)\n        if pclass_name in dir(base_module):\n            pclass = getattr(base_module, pclass_name)\n            assert ""from_jvalue"" in dir(pclass), \\\n                ""pclass: {} should implement from_jvalue method"".format(pclass)\n            return pclass\n        raise Exception(""No proper python class for: {}"".format(self.jfullname))\n\n    def _get_py_name(self, jclass_name):\n        if jclass_name == ""Model"":\n            return ""Model""\n        elif jclass_name == ""Sequential"":\n            return ""Sequential""\n        else:\n            raise Exception(""Not supported type: {}"".format(jclass_name))\n\n    def _load_ppackage_by_jpackage(self, jpackage_name):\n        if ""com.intel.analytics.zoo.pipeline.api.keras.models"":\n            return importlib.import_module(\'zoo.pipeline.api.keras.models\')\n        raise Exception(""Not supported package: {}"".format(jpackage_name))\n\n\nclass Net:\n\n    @staticmethod\n    def from_jvalue(jvalue, bigdl_type=""float""):\n        pclass = JavaToPython(jvalue).get_python_class()\n        return getattr(pclass, ""from_jvalue"")(jvalue, bigdl_type)\n\n    @staticmethod\n    def load_bigdl(model_path, weight_path=None, bigdl_type=""float""):\n        """"""\n        Load a pre-trained BigDL model.\n\n        :param model_path: The path to the pre-trained model.\n        :param weight_path: The path to the weights of the pre-trained model. Default is None.\n        :return: A pre-trained model.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""netLoadBigDL"", model_path, weight_path)\n        return GraphNet.from_jvalue(jmodel)\n\n    @staticmethod\n    def load(model_path, weight_path=None, bigdl_type=""float""):\n        """"""\n        Load an existing Analytics Zoo model defined in Keras-style(with weights).\n\n        :param model_path: The path to load the saved model.\n                          Local file system, HDFS and Amazon S3 are supported.\n                          HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n                          Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        :param weight_path: The path for pre-trained weights if any. Default is None.\n        :return: An Analytics Zoo model.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""netLoad"", model_path, weight_path)\n        return Net.from_jvalue(jmodel, bigdl_type)\n\n    @staticmethod\n    def load_torch(path, bigdl_type=""float""):\n        """"""\n        Load a pre-trained Torch model.\n\n        :param path: The path containing the pre-trained model.\n        :return: A pre-trained model.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""netLoadTorch"", path)\n        return GraphNet.from_jvalue(jmodel, bigdl_type)\n\n    @staticmethod\n    def load_caffe(def_path, model_path, bigdl_type=""float""):\n        """"""\n        Load a pre-trained Caffe model.\n\n        :param def_path: The path containing the caffe model definition.\n        :param model_path: The path containing the pre-trained caffe model.\n        :return: A pre-trained model.\n        """"""\n        jmodel = callZooFunc(bigdl_type, ""netLoadCaffe"", def_path, model_path)\n        return GraphNet.from_jvalue(jmodel, bigdl_type)\n\n    @staticmethod\n    def load_keras(json_path=None, hdf5_path=None, by_name=False):\n        """"""\n        Load a pre-trained Keras model.\n\n        :param json_path: The json path containing the keras model definition. Default is None.\n        :param hdf5_path: The HDF5 path containing the pre-trained keras model weights\n                        with or without the model architecture. Default is None.\n        :param by_name: by default the architecture should be unchanged.\n                        If set as True, only layers with the same name will be loaded.\n        :return: A BigDL model.\n        """"""\n        return BModel.load_keras(json_path, hdf5_path, by_name)\n'"
pyzoo/zoo/pipeline/api/net/torch_criterion.py,2,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport torch\nimport torch.nn as nn\nimport sys\nimport os\nimport tempfile\nimport shutil\nfrom bigdl.nn.criterion import Criterion\nfrom .torch_net import TorchNet\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass LossWrapperModule(nn.Module):\n    def __init__(self, lossFunc):\n        super(LossWrapperModule, self).__init__()\n        self.func = lossFunc\n\n    def forward(self, x, y):\n        return self.func(x, y)\n\n\nclass TorchCriterion(Criterion):\n    """"""\n    TorchCriterion wraps a loss function for distributed inference or training.\n    Use TorchCriterion.from_pytorch to initialize.\n    """"""\n\n    def __init__(self, path, bigdl_type=""float""):\n        """"""\n        :param path: path to the TorchScript model.\n        :param bigdl_type:\n        """"""\n        super(TorchCriterion, self).__init__(None, bigdl_type, path)\n\n    @staticmethod\n    def from_pytorch(loss, input, label=None):\n        """"""\n        Create a TorchCriterion directly from PyTorch function. We need users to provide example\n        input and label (or just their sizes) to trace the loss function.\n\n        :param loss: this can be a torch loss (e.g. nn.MSELoss()) or\n                     a function that takes two Tensor parameters: input and label. E.g.\n                     def lossFunc(input, target):\n                         return nn.CrossEntropyLoss().forward(input, target.flatten().long())\n        :param input: example input. It can be:\n                        1. a torch tensor, or tuple of torch tensors for multi-input models\n                        2. list of integers, or tuple of int list for multi-input models. E.g. For\n                           ResNet, this can be [1, 3, 224, 224]. A random tensor with the\n                           specified size will be used as the example input.\n        :param label: example label. It can be:\n                        1. a torch tensor, or tuple of torch tensors for multi-input models\n                        2. list of integers, or tuple of int list for multi-input models. E.g. For\n                           ResNet, this can be [1, 3, 224, 224]. A random tensor with the\n                           specified size will be used as the example input.\n                      When label is None, input will also be used as label.\n        """"""\n        if input is None:\n            raise Exception(""please specify input and label"")\n\n        temp = tempfile.mkdtemp()\n        # use input_shape as label shape when label_shape is not specified\n        if label is None:\n            label = input\n\n        sample_input = TorchNet.get_sample_input(input)\n        sample_label = TorchNet.get_sample_input(label)\n\n        traced_script_loss = torch.jit.trace(LossWrapperModule(loss),\n                                             (sample_input, sample_label))\n        lossPath = os.path.join(temp, ""loss.pt"")\n        traced_script_loss.save(lossPath)\n\n        criterion = TorchCriterion(lossPath)\n        shutil.rmtree(temp)\n\n        return criterion\n'"
pyzoo/zoo/pipeline/api/net/torch_net.py,5,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport shutil\nimport sys\nimport tempfile\n\nimport numpy as np\nimport torch\nfrom pyspark import RDD\n\nfrom bigdl.nn.layer import Layer\nfrom zoo import getOrCreateSparkContext\nfrom zoo.common.utils import callZooFunc\nfrom zoo.feature.image import ImageSet\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass TorchNet(Layer):\n    """"""\n    TorchNet wraps a TorchScript model as a single layer, thus the Pytorch model can be used for\n    distributed inference or training.\n    :param path: path to the TorchScript model.\n    """"""\n\n    def __init__(self, path, bigdl_type=""float""):\n        super(TorchNet, self).__init__(None, bigdl_type, path)\n\n    @staticmethod\n    def from_pytorch(module, input, check_trace=True):\n        """"""\n        Create a TorchNet directly from PyTorch model, e.g. model in torchvision.models.\n        Users need to provide an example input or the input tensor shape.\n        :param module: a PyTorch model\n        :param input: To trace the tensor operations, torch jit trace requires users to\n                      provide an example input. Here the input parameter can be:\n                        1. a torch tensor, or tuple of torch tensors for multi-input models\n                        2. list of integers, or tuple of int list for multi-input models. E.g. For\n                           ResNet, this can be [1, 3, 224, 224]. A random tensor with the\n                           specified size will be used as the example input.\n        :param check_trace: boolean value, optional. check if the same inputs run through\n                            traced module produce the same outputs. Default: ``True``. You\n                            might want to disable this if, for example, your network contains\n                            non-deterministic ops or if you are sure that the network is\n                            correct despite a checker failure.\n        """"""\n        if input is None:\n            raise Exception(""please provide an example input or input Tensor size"")\n\n        sample = TorchNet.get_sample_input(input)\n        temp = tempfile.mkdtemp()\n\n        # save model\n        traced_script_module = torch.jit.trace(module, sample, check_trace=check_trace)\n        path = os.path.join(temp, ""model.pt"")\n        traced_script_module.save(path)\n\n        net = TorchNet(path)\n        shutil.rmtree(temp)\n\n        return net\n\n    @staticmethod\n    def get_sample_input(input):\n        if isinstance(input, torch.Tensor):\n            return input\n\n        elif isinstance(input, (list, tuple)) and len(input) > 0:\n            if all(isinstance(x, torch.Tensor) for x in input):  # tensors\n                return tuple(input)\n            elif all(isinstance(x, int) for x in input):  # ints\n                return torch.rand(input)\n            elif all(isinstance(x, (list, tuple)) for x in input) and \\\n                    all(isinstance(y, int) for x in input for y in x):  # nested int list (tuple)\n                return tuple(map(lambda size: torch.rand(size), input))\n\n        raise Exception(""Unsupported input type: "" + str(input))\n\n    def savePytorch(self, path):\n        \'\'\'\n        save the model as a torch script module\n        \'\'\'\n        pythonBigDL_method_name = ""torchNetSavePytorch""\n        callZooFunc(self.bigdl_type, pythonBigDL_method_name, self.value, path)\n        return\n\n    def predict(self, x, batch_per_thread=1, distributed=True):\n        """"""\n        Use a model to do prediction.\n        """"""\n        if isinstance(x, ImageSet):\n            results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                  self.value,\n                                  x,\n                                  batch_per_thread)\n            return ImageSet(results)\n        if distributed:\n            if isinstance(x, np.ndarray):\n                from zoo.tfpark.tfnet import to_sample_rdd\n                data_rdd = to_sample_rdd(x, np.zeros([x.shape[0]]), getOrCreateSparkContext())\n            elif isinstance(x, RDD):\n                data_rdd = x\n            else:\n                raise TypeError(""Unsupported prediction data type: %s"" % type(x))\n            results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                  self.value,\n                                  data_rdd,\n                                  batch_per_thread)\n            return results.map(lambda result: Layer.convert_output(result))\n        else:\n            if isinstance(x, np.ndarray) or isinstance(x, list):\n                results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                      self.value,\n                                      self._to_jtensors(x),\n                                      batch_per_thread)\n                return [Layer.convert_output(result) for result in results]\n            else:\n                raise TypeError(""Unsupported prediction data type: %s"" % type(x))\n'"
pyzoo/zoo/pipeline/api/net/utils.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nimport warnings\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\ndef find_tensors(sources, predicate):\n    \'\'\'\n    find all the tensors that are used for computing grads and has been\n    computed during forward\n    :param grads:\n    :param forward_ops:\n    :return:\n    \'\'\'\n    from collections import deque\n    import tensorflow as tf\n    queue = deque([])\n    for source in sources:\n        queue.append(source)\n\n    results = set()\n    visited = set()\n    while len(queue) > 0:\n        node = queue.popleft()\n        # this is necessary, because input may not be differentiable\n        if node is None:\n            continue\n        else:\n            visited.add(node.name)\n            if predicate(node):\n                results.add(node)\n            if isinstance(node, tf.Tensor):\n                inputs = list(node.op.inputs) + list(node.op.control_inputs)\n            elif isinstance(node, tf.Operation):\n                inputs = list(node.inputs) + list(node.control_inputs)\n            else:\n                raise ValueError(""Unrecognized Node: {}"".format(node))\n            for input_tensor in inputs:\n                # this is necessary because there may be a cycle in the graph such as tf.while_loop\n                if input_tensor.name not in visited:\n                    queue.append(input_tensor)\n    return list(results)\n\n\ndef find_placeholders(grads):\n    import tensorflow as tf\n\n    def predicate(t):\n        if not isinstance(t, tf.Operation):\n            return t.op.type.startswith(""Placeholder"")\n        else:\n            return False\n\n    return find_tensors(grads, predicate)\n\n\ndef _check_the_same(all_required_inputs, inputs_in_datasets):\n    inputs_not_in_dataset = [i for i in all_required_inputs if i not in inputs_in_datasets]\n    if inputs_not_in_dataset:\n        raise ValueError(""You should not use any placeholder that are not defined in dataset, "" +\n                         ""found %s"" % inputs_not_in_dataset)\n    if len(inputs_in_datasets) != len(all_required_inputs):\n        inputs_not_require_by_loss = [i for i in inputs_in_datasets if i not in\n                                      all_required_inputs]\n        raise ValueError(""You should use all the placeholders that are defined in dataset, "" +\n                         ""%s are not used"" % inputs_not_require_by_loss)\n\n\ndef to_bigdl_optim_method(koptim_method):\n    # koptim_method is always an object\n    import tensorflow.keras.backend as K\n    import tensorflow.keras.optimizers as koptimizers\n    import bigdl.optim.optimizer as boptimizer\n    import tensorflow.train as tftrain\n    import tensorflow as tf\n    from tensorflow.python.keras.optimizers import TFOptimizer\n\n    if isinstance(koptim_method, dict):\n        res = dict()\n        for name, optim_method in koptim_method.items():\n            res[name] = to_bigdl_optim_method(optim_method)\n        return res\n\n    if isinstance(koptim_method, TFOptimizer):\n        koptim_method = koptim_method.optimizer\n\n    if isinstance(koptim_method, boptimizer.OptimMethod):\n        return koptim_method\n    elif isinstance(koptim_method, koptimizers.Optimizer):\n        lr = float(K.eval(koptim_method.lr))\n        decay = float(K.eval(koptim_method.decay))\n        if isinstance(koptim_method, koptimizers.Adagrad):\n            warnings.warn(""For Adagrad, we don\'t support epsilon for now"")\n            return boptimizer.Adagrad(learningrate=lr,\n                                      learningrate_decay=decay)\n        elif isinstance(koptim_method, koptimizers.SGD):\n            momentum = float(K.eval(koptim_method.momentum))\n            return boptimizer.SGD(learningrate=lr,\n                                  learningrate_decay=decay,\n                                  momentum=momentum,\n                                  nesterov=koptim_method.nesterov)\n        elif isinstance(koptim_method, koptimizers.Adam):\n            beta1 = float(K.eval(koptim_method.beta_1))\n            beta2 = float(K.eval(koptim_method.beta_2))\n            return boptimizer.Adam(learningrate=lr,\n                                   learningrate_decay=decay,\n                                   beta1=beta1,\n                                   beta2=beta2,\n                                   epsilon=koptim_method.epsilon)\n        elif isinstance(koptim_method, koptimizers.RMSprop):\n            rho = float(K.eval(koptim_method.rho))\n            return boptimizer.RMSprop(learningrate=lr,\n                                      learningrate_decay=decay,\n                                      decayrate=rho,\n                                      epsilon=koptim_method.epsilon)\n        elif isinstance(koptim_method, koptimizers.Adadelta):\n            warnings.warn(\n                ""For Adadelta, we don\'t support learning rate and learning rate decay for now"")\n            return boptimizer.Adadelta(decayrate=koptim_method.rho,\n                                       epsilon=koptim_method.epsilon)\n        elif isinstance(koptim_method, koptimizers.Adamax):\n            beta1 = float(K.eval(koptim_method.beta_1))\n            beta2 = float(K.eval(koptim_method.beta_2))\n            warnings.warn(""For Adamax, we don\'t support learning rate decay for now"")\n            return boptimizer.Adamax(learningrate=lr,\n                                     beta1=beta1,\n                                     beta2=beta2,\n                                     epsilon=koptim_method.epsilon)\n    elif isinstance(koptim_method, tftrain.Optimizer):\n        def get_value(v):\n            if isinstance(v, (tf.Tensor, tf.SparseTensor, tf.Variable)):\n                return float(K.eval(v))\n            else:\n                return float(v)\n\n        if isinstance(koptim_method, tftrain.GradientDescentOptimizer):\n            lr = get_value(koptim_method._learning_rate)\n            return boptimizer.SGD(learningrate=lr)\n        elif isinstance(koptim_method, tftrain.MomentumOptimizer):\n            lr = get_value(koptim_method._learning_rate)\n            momentum = get_value(koptim_method._momentum)\n            use_nesterov = koptim_method._use_nesterov\n            return boptimizer.SGD(learningrate=lr, momentum=momentum, nesterov=use_nesterov)\n        elif isinstance(koptim_method, tftrain.AdagradOptimizer):\n            lr = get_value(koptim_method._learning_rate)\n            return boptimizer.Adagrad(learningrate=lr)\n        elif isinstance(koptim_method, tftrain.AdamOptimizer):\n            lr = get_value(koptim_method._lr)\n            beta1 = get_value(koptim_method._beta1)\n            beta2 = get_value(koptim_method._beta2)\n            epsilon = get_value(koptim_method._epsilon)\n            return boptimizer.Adam(learningrate=lr, beta1=beta1, beta2=beta2, epsilon=epsilon)\n        elif isinstance(koptim_method, tftrain.RMSPropOptimizer):\n            lr = get_value(koptim_method._learning_rate)\n            decay = get_value(koptim_method._decay)\n            momentum = get_value(koptim_method._momentum)\n            epsilon = get_value(koptim_method._epsilon)\n            centered = get_value(koptim_method._centered)\n            if momentum != 0.0 or centered:\n                warnings.warn(\n                    ""For RMSPropOptimizer, we don\'t support momentum and centered for now"")\n            return boptimizer.RMSprop(learningrate=lr,\n                                      learningrate_decay=decay,\n                                      epsilon=epsilon)\n        elif isinstance(koptim_method, tftrain.AdadeltaOptimizer):\n            lr = get_value(koptim_method._lr)\n            rho = get_value(koptim_method._rho)\n            epsilon = get_value(koptim_method._epsilon)\n            warnings.warn(\n                ""For Adadelta, we don\'t support learning rate for now"")\n            return boptimizer.Adadelta(decayrate=rho, epsilon=epsilon)\n\n    raise ValueError(""We don\'t support %s for now"" % koptim_method)\n'"
pyzoo/zoo/pipeline/api/onnx/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/pipeline/api/onnx/onnx_helper.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nclass OnnxHelper:\n    @staticmethod\n    def parse_attr(attr_proto):\n        """"""Convert a list of AttributeProto to a dict, with names as keys.""""""\n        attrs = {}\n        for a in attr_proto:\n            for f in [\'f\', \'i\', \'s\']:\n                if a.HasField(f):\n                    attrs[a.name] = getattr(a, f)\n            for f in [\'floats\', \'ints\', \'strings\']:\n                if list(getattr(a, f)):\n                    assert a.name not in attrs, ""Only one type of attr is allowed""\n                    attrs[a.name] = tuple(getattr(a, f))\n            for f in [\'t\', \'g\']:\n                if a.HasField(f):\n                    attrs[a.name] = getattr(a, f)\n            for f in [\'tensors\', \'graphs\']:\n                if list(getattr(a, f)):\n                    raise NotImplementedError(""Filed {} is not supported in mxnet."".format(f))\n            if a.name not in attrs:\n                raise ValueError(""Cannot parse attribute: \\n{}\\n."".format(a))\n        return attrs\n\n    @staticmethod\n    def to_numpy(tensor_proto):\n        """"""Grab data in TensorProto and to_tensor to numpy array.""""""\n        try:\n            from onnx.numpy_helper import to_array\n        except ImportError as e:\n            raise ImportError(""Unable to import onnx which is required {}"".format(e))\n        np_array = to_array(tensor_proto).reshape(tuple(tensor_proto.dims))\n        return np_array\n\n    @staticmethod\n    def get_shape_from_node(valueInfoProto):\n        return [int(dim.dim_value) for dim in valueInfoProto.type.tensor_type.shape.dim]\n\n    @staticmethod\n    def get_padds(onnx_attr):\n        border_mode = None\n        pads = None\n\n        if ""auto_pad"" in onnx_attr.keys():\n            if onnx_attr[\'auto_pad\'].decode() == \'SAME_UPPER\':\n                border_mode = \'same\'\n            elif onnx_attr[\'auto_pad\'].decode() == \'VALID\':\n                border_mode = \'valid\'\n            elif onnx_attr[\'auto_pad\'].decode() == \'NOTSET\':\n                assert ""pads"" in onnx_attr.keys(), ""you should specify pads explicitly""\n            else:\n                raise NotImplementedError(\'Unknown auto_pad mode ""%s"", \'\n                                          \'only SAME_UPPER and VALID are supported.\'\n                                          % onnx_attr[\'auto_pad\'])\n\n        # In ONNX, pads format is [x1_begin, x2_begin...x1_end, x2_end,...].\n        # While pads format we supported should be [x1_begin, x1_end, x2_begin, x2_end...]\n        if ""pads"" in onnx_attr.keys():\n            pads = [int(i) for i in onnx_attr[""pads""]]\n            if len(pads) == 4:\n                assert pads[0] == pads[2]\n                assert pads[1] == pads[3]\n                pads = [pads[0], pads[1]]\n            elif len(pads) == 2:\n                assert pads[0] == pads[1]\n                pads = pads[0]\n\n        return border_mode, pads\n'"
pyzoo/zoo/pipeline/api/onnx/onnx_loader.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom collections import OrderedDict\n\nimport onnx\n\nimport zoo.pipeline.api.keras.models as zmodels\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\n\n\nclass OnnxInput(object):\n    def __init__(self, name, zvalue, data=None):\n        self.name = name\n        self.zvalue = zvalue  # zvalue is a Input or Parameter\n        self.data = data  # data is a ndarray\n\n\nclass OnnxLoader(object):\n\n    training = False\n\n    def __init__(self, onnx_graph):\n        self.graph = onnx_graph\n        self._all_tensors = {}  # including the original input tensor or the immediate tensor.\n        self.initializer = {}  # name -> ndarray\n        self._inputs = OrderedDict()  # the original input tensor only.\n\n    @classmethod\n    def from_path(cls, onnx_path, is_training=False):\n        onnx_model = onnx.load(onnx_path)\n        try:\n            zmodel = OnnxLoader(onnx_model.graph).to_keras()\n        except Exception as e:\n            print(onnx_model)\n            raise e\n        zmodel.training(is_training=is_training)\n        return zmodel\n\n    @staticmethod\n    # inputs_dict is a list of batch data\n    def run_node(node, inputs, is_training=False):\n        inputs_list = []\n        assert len(inputs) == len(list(node.input))\n        for node_input, input_data in zip(node.input, inputs):\n            inputs_list.append(OnnxInput(node_input, input_data))\n        mapper = OperatorMapper.of(node, set(), inputs_list)\n        out_tensor = mapper.to_tensor()\n\n        model = zmodels.Model(input=[i.zvalue for i in mapper.model_inputs], output=out_tensor)\n        data = [i.data for i in mapper.model_inputs]\n        model.training(is_training)\n        output = model.forward(data if len(data) > 1 else data[0])\n        result = {}\n        if isinstance(output, list):\n            assert len(output) == len(node.output)\n            for o, no in zip(output, node.output):\n                result[no] = o\n        else:\n            result[node.output[0]] = output\n        return result\n\n    def to_keras(self):\n        """"""Convert a Onnx model to KerasNet model.\n      """"""\n        # parse network inputs, aka parameters\n        for init_tensor in self.graph.initializer:\n            if not init_tensor.name.strip():\n                raise ValueError(""Tensor\'s name is required."")\n            self.initializer[init_tensor.name] = OnnxInput(name=init_tensor.name,\n                                                           zvalue=OnnxHelper.to_numpy(init_tensor))\n\n        # converting GraphProto message\n        # i: ValueInfoProto\n        for i in self.graph.input:\n            if i.name in self.initializer:\n                # we should have added that via graph._initializer\n                self._all_tensors[i.name] = self.initializer[i.name]\n            else:\n                self._inputs[i.name] = OnnxInput(name=i.name,\n                                                 zvalue=OnnxHelper.get_shape_from_node(i))\n                self._all_tensors[i.name] = self._inputs[i.name]\n\n        # constructing nodes, nodes are stored as directed acyclic graph\n        # converting NodeProto message\n        for node in self.graph.node:\n            inputs = []\n            for i in node.input:\n                if i == """":\n                    continue\n                if i not in self._all_tensors:\n                    raise Exception(""Cannot find {}"".format(i))\n                inputs.append(self._all_tensors[i])\n\n            mapper = OperatorMapper.of(node,\n                                       self.initializer, inputs)\n            # update inputs and all_tensors\n            for input in mapper.model_inputs:\n                # Only update the original inputs here.\n                if input.name in self._inputs:\n                    self._inputs[input.name] = input.zvalue\n                self._all_tensors[input.name] = input.zvalue\n            tensor = mapper.to_tensor()\n            output_ids = list(node.output)\n            assert len(output_ids) == 1 or node.op_type == ""Dropout"",\\\n                ""Only support single output for now""\n            self._all_tensors[output_ids[0]] = OnnxInput(name=tensor.name, zvalue=tensor)\n\n        output_tensors = []\n        for i in self.graph.output:\n            if i.name not in self._all_tensors:\n                raise Exception(""The output haven\'t been calculate"")\n            output_tensors.append(self._all_tensors[i.name].zvalue)\n        model = zmodels.Model(input=list(self._inputs.values()), output=output_tensors)\n        return model\n'"
pyzoo/zoo/pipeline/api/torch/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .torch_loss import TorchLoss\nfrom .torch_model import TorchModel\n'"
pyzoo/zoo/pipeline/api/torch/torch_loss.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\nfrom bigdl.nn.criterion import Criterion\nfrom pyspark.serializers import CloudPickleSerializer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass TorchLoss(Criterion):\n    """"""\n    TorchLoss wraps a loss function for distributed inference or training.\n    This TorchLoss should be used with TorchModel.\n    """"""\n\n    def __init__(self, criterion_bytes, bigdl_type=""float""):\n        """"""\n        :param bigdl_type:\n        """"""\n        super(TorchLoss, self).__init__(None, bigdl_type, criterion_bytes)\n\n    @staticmethod\n    def from_pytorch(criterion):\n        bys = CloudPickleSerializer.dumps(CloudPickleSerializer, criterion)\n        net = TorchLoss(bys)\n        return net\n'"
pyzoo/zoo/pipeline/api/torch/torch_model.py,4,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nimport torch\n\nfrom bigdl.nn.layer import Layer\nfrom bigdl.util.common import JTensor\nfrom zoo.common.utils import callZooFunc\nfrom pyspark.serializers import CloudPickleSerializer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass TorchModel(Layer):\n    """"""\n    TorchModel wraps a PyTorch model as a single layer, thus the PyTorch model can be used for\n    distributed inference or training.\n    The implement of TorchModel is different from TorchNet, this TorchModel is running running\n    pytorch model in an embedding Cpython interpreter, while TorchNet transfer the pytorch model\n    to TorchScript and run with libtorch.\n    """"""\n\n    def __init__(self, module_bytes, weights, bigdl_type=""float""):\n        weights = JTensor.from_ndarray(weights)\n        self.module_bytes = module_bytes\n        self.value = callZooFunc(\n            bigdl_type, self.jvm_class_constructor(), module_bytes, weights)\n        self.bigdl_type = bigdl_type\n\n    @staticmethod\n    def from_pytorch(model):\n        """"""\n        Create a TorchNet directly from PyTorch model, e.g. model in torchvision.models.\n        :param model: a PyTorch model\n        """"""\n        weights = []\n        for param in model.parameters():\n            weights.append(param.view(-1))\n        flatten_weight = torch.nn.utils.parameters_to_vector(weights).data.numpy()\n        bys = CloudPickleSerializer.dumps(CloudPickleSerializer, model)\n        net = TorchModel(bys, flatten_weight)\n        return net\n\n    def to_pytorch(self):\n        """"""\n        Convert to pytorch model\n        :return: a pytorch model\n        """"""\n        new_weight = self.get_weights()\n        assert(len(new_weight) == 1, ""TorchModel\'s weights should be one tensor"")\n        m = CloudPickleSerializer.loads(CloudPickleSerializer, self.module_bytes)\n        w = torch.Tensor(new_weight[0])\n        torch.nn.utils.vector_to_parameters(w, m.parameters())\n        return m\n'"
pyzoo/zoo/pipeline/api/torch/utils.py,1,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom torch.utils.data.sampler import Sampler\nimport math\n\n\nclass DistributedSequentialSampler(Sampler):\n    """"""\n    A sequential sampler used in FeatureSet when get (train=false) iterator .\n    """"""\n    def __init__(self, dataset, num_replicas, rank):\n        self.dataset = dataset\n        self.num_samples = int(math.floor(len(self.dataset) * 1.0 / num_replicas))\n        extra_samples = len(self.dataset) % num_replicas\n        self.epoch = 0\n        if extra_samples > rank:\n            self.num_samples += 1\n            self.offset = self.num_samples * rank\n        else:\n            self.offset = self.num_samples * rank + extra_samples\n        self.total_size = len(dataset)\n\n    def __iter__(self):\n        indices = list(range(self.offset, self.num_samples + self.offset))\n\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
pyzoo/zoo/tfpark/text/estimator/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .bert_base import *\nfrom .bert_classifier import *\nfrom .bert_squad import *\nfrom .bert_ner import *\n'"
pyzoo/zoo/tfpark/text/estimator/bert_base.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.tfpark.estimator import *\nfrom bert import modeling\n\n\ndef bert_model(features, labels, mode, params):\n    """"""\n    Return an instance of BertModel and one can take its different outputs to\n    perform specific tasks.\n    """"""\n    input_ids = features[""input_ids""]\n    if ""input_mask"" in features:\n        input_mask = features[""input_mask""]\n    else:\n        input_mask = None\n    if ""token_type_ids"" in features:\n        token_type_ids = features[""token_type_ids""]\n    else:\n        token_type_ids = None\n    bert_config = modeling.BertConfig.from_json_file(params[""bert_config_file""])\n    model = modeling.BertModel(\n        config=bert_config,\n        is_training=(mode == tf.estimator.ModeKeys.TRAIN),\n        input_ids=input_ids,\n        input_mask=input_mask,\n        token_type_ids=token_type_ids,\n        use_one_hot_embeddings=params[""use_one_hot_embeddings""])\n    tvars = tf.trainable_variables()\n    if params[""init_checkpoint""]:\n        assignment_map, initialized_variable_names = \\\n            modeling.get_assignment_map_from_checkpoint(tvars, params[""init_checkpoint""])\n        tf.train.init_from_checkpoint(params[""init_checkpoint""], assignment_map)\n    return model\n\n\ndef bert_input_fn(rdd, max_seq_length, batch_size,\n                  features={""input_ids"", ""input_mask"", ""token_type_ids""},\n                  extra_features=None, labels=None, label_size=None):\n    """"""\n    Takes an RDD to create the input function for BERT related TFEstimators.\n    For training and evaluation, each element in rdd should be a tuple:\n    (dict of features, a single label or dict of labels)\n    Note that currently only integer or integer array labels are supported.\n    For prediction, each element in rdd should be a dict of features.\n\n    Features in each RDD element should contain ""input_ids"", ""input_mask"" and ""token_type_ids"",\n    each of shape max_seq_length.\n    If you have other extra features in your dict of features, you need to explicitly specify\n    the argument `extra_features`, which is supposed to be the dict with feature name as key\n    and tuple of (dtype, shape) as its value.\n    """"""\n    assert features.issubset({""input_ids"", ""input_mask"", ""token_type_ids""})\n    features_dict = {}\n    for feature in features:\n        features_dict[feature] = (tf.int32, [max_seq_length])\n    if extra_features is not None:\n        assert isinstance(extra_features, dict), ""extra_features should be a dictionary""\n        for k, v in extra_features.items():\n            assert isinstance(k, six.string_types)\n            assert isinstance(v, tuple)\n            features_dict[k] = v\n    if label_size is None:\n        label_size = []\n    else:\n        label_size = [label_size]\n    if labels is None:\n        res_labels = (tf.int32, label_size)\n    elif isinstance(labels, list) or isinstance(labels, set):\n        labels = set(labels)\n        if len(labels) == 1:\n            res_labels = (tf.int32, label_size)\n        else:\n            res_labels = {}\n            for label in labels:\n                res_labels[label] = (tf.int32, label_size)\n    else:\n        raise ValueError(""Wrong labels. ""\n                         ""labels should be a set of label names if you have multiple labels"")\n\n    def input_fn(mode):\n        if mode == tf.estimator.ModeKeys.EVAL or mode == tf.estimator.ModeKeys.TRAIN:\n            return TFDataset.from_rdd(rdd,\n                                      features=features_dict,\n                                      labels=res_labels,\n                                      batch_size=batch_size)\n        else:\n            return TFDataset.from_rdd(rdd,\n                                      features=features_dict,\n                                      batch_per_thread=batch_size // rdd.getNumPartitions())\n    return input_fn\n\n\nclass BERTBaseEstimator(TFEstimator):\n    """"""\n    The base class for BERT related TFEstimators.\n    Common arguments:\n    bert_config_file, init_checkpoint, use_one_hot_embeddings, optimizer, model_dir.\n\n    For its subclass:\n    - One can add additional arguments and access them via `params`.\n    - One can utilize `_bert_model` to create model_fn and `bert_input_fn` to create input_fn.\n    """"""\n    def __init__(self, model_fn, bert_config_file, init_checkpoint=None,\n                 use_one_hot_embeddings=False, model_dir=None, **kwargs):\n        params = {""bert_config_file"": bert_config_file,\n                  ""init_checkpoint"": init_checkpoint,\n                  ""use_one_hot_embeddings"": use_one_hot_embeddings}\n        for k, v in kwargs.items():\n            params[k] = v\n        estimator = tf.estimator.Estimator(model_fn, model_dir=model_dir, params=params)\n        super(BERTBaseEstimator, self).__init__(estimator)\n'"
pyzoo/zoo/tfpark/text/estimator/bert_classifier.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.tfpark import ZooOptimizer\nfrom zoo.tfpark.text.estimator import *\n\n\ndef make_bert_classifier_model_fn(optimizer):\n    def _bert_classifier_model_fn(features, labels, mode, params):\n        """"""\n        Model function for BERTClassifier.\n\n        :param features: Dict of feature tensors. Must include the key ""input_ids"".\n        :param labels: Label tensor for training.\n        :param mode: \'train\', \'eval\' or \'infer\'.\n        :param params: Must include the key ""num_classes"".\n        :return: tf.estimator.EstimatorSpec.\n        """"""\n        output_layer = bert_model(features, labels, mode, params).get_pooled_output()\n        hidden_size = output_layer.shape[-1].value\n        output_weights = tf.get_variable(\n            ""output_weights"", [params[""num_classes""], hidden_size],\n            initializer=tf.truncated_normal_initializer(stddev=0.02))\n        output_bias = tf.get_variable(\n            ""output_bias"", [params[""num_classes""]], initializer=tf.zeros_initializer())\n        with tf.variable_scope(""loss""):\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n\n            logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n            logits = tf.nn.bias_add(logits, output_bias)\n            probabilities = tf.nn.softmax(logits, axis=-1)\n            if mode == tf.estimator.ModeKeys.PREDICT or mode == tf.estimator.ModeKeys.EVAL:\n                return tf.estimator.EstimatorSpec(mode=mode, predictions=probabilities)\n            else:\n                log_probs = tf.nn.log_softmax(logits, axis=-1)\n                one_hot_labels = tf.one_hot(labels, depth=params[""num_classes""], dtype=tf.float32)\n                per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n                loss = tf.reduce_mean(per_example_loss)\n                train_op = ZooOptimizer(optimizer).minimize(loss)\n                return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)\n    return _bert_classifier_model_fn\n\n\nclass BERTClassifier(BERTBaseEstimator):\n    """"""\n    A pre-built TFEstimator that takes the hidden state of the first token of BERT\n    to do classification.\n\n    :param num_classes: Positive int. The number of classes to be classified.\n    :param bert_config_file: The path to the json file for BERT configurations.\n    :param init_checkpoint: The path to the initial checkpoint of the pre-trained BERT model if any.\n                            Default is None.\n    :param use_one_hot_embeddings: Boolean. Whether to use one-hot for word embeddings.\n                                   Default is False.\n    :param optimizer: The optimizer used to train the estimator. It should be an instance of\n                      tf.train.Optimizer.\n                      Default is None if no training is involved.\n    :param model_dir: The output directory for model checkpoints to be written if any.\n                      Default is None.\n    """"""\n    def __init__(self, num_classes, bert_config_file, init_checkpoint=None,\n                 use_one_hot_embeddings=False, optimizer=None, model_dir=None):\n        super(BERTClassifier, self).__init__(\n            model_fn=make_bert_classifier_model_fn(optimizer),\n            bert_config_file=bert_config_file,\n            init_checkpoint=init_checkpoint,\n            use_one_hot_embeddings=use_one_hot_embeddings,\n            model_dir=model_dir,\n            num_classes=num_classes)\n'"
pyzoo/zoo/tfpark/text/estimator/bert_ner.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.tfpark import ZooOptimizer\nfrom zoo.tfpark.text.estimator import *\n\n\ndef make_bert_ner_model_fn(optimizer):\n    def _bert_ner_model_fn(features, labels, mode, params):\n        output_layer = bert_model(features, labels, mode, params).get_sequence_output()\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n        logits = tf.layers.dense(output_layer, params[""num_entities""])\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            logits = tf.reshape(logits, [-1, params[""num_entities""]])\n            labels = tf.reshape(labels, [-1])\n            mask = tf.cast(features[""input_mask""], dtype=tf.float32)\n            one_hot_labels = tf.one_hot(labels, depth=params[""num_entities""], dtype=tf.float32)\n            loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=one_hot_labels)\n            loss *= tf.reshape(mask, [-1])\n            loss = tf.reduce_sum(loss)\n            total_size = tf.reduce_sum(mask)\n            total_size += 1e-12  # to avoid division by 0 for all-0 weights\n            loss /= total_size\n            train_op = ZooOptimizer(optimizer).minimize(loss)\n            return tf.estimator.EstimatorSpec(mode=mode,\n                                              train_op=train_op, loss=loss)\n        elif mode == tf.estimator.ModeKeys.PREDICT:\n            probabilities = tf.nn.softmax(logits, axis=-1)\n            predict = tf.argmax(probabilities, axis=-1)\n            return tf.estimator.EstimatorSpec(mode=mode, predictions=predict)\n        else:\n            raise ValueError(""Currently only TRAIN and PREDICT modes are supported for NER"")\n    return _bert_ner_model_fn\n\n\nclass BERTNER(BERTBaseEstimator):\n    """"""\n    A pre-built TFEstimator that takes the hidden state of the final encoder layer of BERT\n    for named entity recognition based on SoftMax classification.\n    Note that cased BERT models are recommended for NER.\n\n    :param num_entities: Positive int. The number of entity labels to be classified.\n    :param bert_config_file: The path to the json file for BERT configurations.\n    :param init_checkpoint: The path to the initial checkpoint of the pre-trained BERT model if any.\n                            Default is None.\n    :param use_one_hot_embeddings: Boolean. Whether to use one-hot for word embeddings.\n                                   Default is False.\n    :param optimizer: The optimizer used to train the estimator. It should be an instance of\n                      tf.train.Optimizer.\n                      Default is None if no training is involved.\n    :param model_dir: The output directory for model checkpoints to be written if any.\n                      Default is None.\n    """"""\n    def __init__(self, num_entities, bert_config_file, init_checkpoint=None,\n                 use_one_hot_embeddings=False, optimizer=None, model_dir=None):\n        super(BERTNER, self).__init__(\n            model_fn=make_bert_ner_model_fn(optimizer),\n            bert_config_file=bert_config_file,\n            init_checkpoint=init_checkpoint,\n            use_one_hot_embeddings=use_one_hot_embeddings,\n            model_dir=model_dir,\n            num_entities=num_entities)\n'"
pyzoo/zoo/tfpark/text/estimator/bert_squad.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.tfpark import ZooOptimizer\nfrom zoo.tfpark.text.estimator import *\n\n\ndef make_bert_squad_model_fn(optimizer):\n    def _bert_squad_model_fn(features, labels, mode, params):\n        final_hidden = bert_model(features, labels, mode, params).get_sequence_output()\n        final_hidden_shape = modeling.get_shape_list(final_hidden, expected_rank=3)\n        batch_size = final_hidden_shape[0]\n        seq_length = final_hidden_shape[1]\n        hidden_size = final_hidden_shape[2]\n\n        output_weights = tf.get_variable(\n            ""cls/squad/output_weights"", [2, hidden_size],\n            initializer=tf.truncated_normal_initializer(stddev=0.02))\n        output_bias = tf.get_variable(\n            ""cls/squad/output_bias"", [2], initializer=tf.zeros_initializer())\n\n        final_hidden_matrix = tf.reshape(final_hidden,\n                                         [batch_size * seq_length, hidden_size])\n        logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n\n        logits = tf.reshape(logits, [batch_size, seq_length, 2])\n        logits = tf.transpose(logits, [2, 0, 1])\n        unstacked_logits = tf.unstack(logits, axis=0)\n        (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            def compute_loss(logits, positions):\n                one_hot_positions = tf.one_hot(\n                    positions, depth=seq_length, dtype=tf.float32)\n                log_probs = tf.nn.log_softmax(logits, axis=-1)\n                loss = -tf.reduce_mean(\n                    tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n                return loss\n\n            start_positions = labels[""start_positions""]\n            end_positions = labels[""end_positions""]\n\n            start_loss = compute_loss(start_logits, start_positions)\n            end_loss = compute_loss(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2.0\n            train_op = ZooOptimizer(optimizer).minimize(total_loss)\n            return tf.estimator.EstimatorSpec(mode=mode,\n                                              train_op=train_op, loss=total_loss)\n        elif mode == tf.estimator.ModeKeys.PREDICT:\n            predictions = {\n                ""unique_ids"": features[""unique_ids""],\n                ""start_logits"": start_logits,\n                ""end_logits"": end_logits,\n            }\n            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n        else:\n            raise ValueError(""Currently only TRAIN and PREDICT modes are supported. ""\n                             ""SQuAD uses a separate script for EVAL"")\n\n    return _bert_squad_model_fn\n\n\nclass BERTSQuAD(BERTBaseEstimator):\n    """"""\n    A pre-built TFEstimator that that takes the hidden state of the final encoder layer of BERT\n    to perform training and prediction on SQuAD dataset.\n    The Stanford Question Answering Dataset (SQuAD) is a popular question answering\n    benchmark dataset.\n\n    :param bert_config_file: The path to the json file for BERT configurations.\n    :param init_checkpoint: The path to the initial checkpoint of the pre-trained BERT model if any.\n                            Default is None.\n    :param use_one_hot_embeddings: Boolean. Whether to use one-hot for word embeddings.\n                                   Default is False.\n    :param optimizer: The optimizer used to train the estimator. It should be an instance of\n                      tf.train.Optimizer.\n                      Default is None if no training is involved.\n    :param model_dir: The output directory for model checkpoints to be written if any.\n                      Default is None.\n    """"""\n    def __init__(self, bert_config_file, init_checkpoint=None,\n                 use_one_hot_embeddings=False, optimizer=None, model_dir=None):\n        super(BERTSQuAD, self).__init__(\n            model_fn=make_bert_squad_model_fn(optimizer),\n            bert_config_file=bert_config_file,\n            init_checkpoint=init_checkpoint,\n            use_one_hot_embeddings=use_one_hot_embeddings,\n            model_dir=model_dir)\n'"
pyzoo/zoo/tfpark/text/keras/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .intent_extraction import *\nfrom .ner import *\nfrom .pos_tagging import *\n'"
pyzoo/zoo/tfpark/text/keras/intent_extraction.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport nlp_architect.models.intent_extraction as intent_models\nfrom zoo.tfpark.text.keras.text_model import TextKerasModel\n\n\nclass IntentEntity(TextKerasModel):\n    """"""\n    A multi-task model used for joint intent extraction and slot filling.\n\n    This model has two inputs:\n    - word indices of shape (batch, sequence_length)\n    - character indices of shape (batch, sequence_length, word_length)\n    This model has two outputs:\n    - intent labels of shape (batch, num_intents)\n    - entity tags of shape (batch, sequence_length, num_entities)\n\n    :param num_intents: Positive int. The number of intent classes to be classified.\n    :param num_entities: Positive int. The number of slot labels to be classified.\n    :param word_vocab_size: Positive int. The size of the word dictionary.\n    :param char_vocab_size: Positive int. The size of the character dictionary.\n    :param word_length: Positive int. The max word length in characters. Default is 12.\n    :param word_emb_dim: Positive int. The dimension of word embeddings. Default is 100.\n    :param char_emb_dim: Positive int. The dimension of character embeddings. Default is 30.\n    :param char_lstm_dim: Positive int. The hidden size of character feature Bi-LSTM layer.\n                          Default is 30.\n    :param tagger_lstm_dim: Positive int. The hidden size of tagger Bi-LSTM layers. Default is 100.\n    :param dropout: Dropout rate. Default is 0.2.\n    :param optimizer: Optimizer to train the model.\n                      If not specified, it will by default to be tf.train.AdamOptimizer().\n    """"""\n    def __init__(self, num_intents, num_entities, word_vocab_size,\n                 char_vocab_size, word_length=12, word_emb_dim=100, char_emb_dim=30,\n                 char_lstm_dim=30, tagger_lstm_dim=100, dropout=0.2, optimizer=None):\n        super(IntentEntity, self).__init__(intent_models.MultiTaskIntentModel(use_cudnn=False),\n                                           optimizer,\n                                           word_length=word_length,\n                                           num_labels=num_entities,\n                                           num_intent_labels=num_intents,\n                                           word_vocab_size=word_vocab_size,\n                                           char_vocab_size=char_vocab_size,\n                                           word_emb_dims=word_emb_dim,\n                                           char_emb_dims=char_emb_dim,\n                                           char_lstm_dims=char_lstm_dim,\n                                           tagger_lstm_dims=tagger_lstm_dim,\n                                           dropout=dropout)\n\n    @staticmethod\n    def load_model(path):\n        """"""\n        Load an existing IntentEntity model (with weights) from HDF5 file.\n\n        :param path: String. The path to the pre-defined model.\n        :return: IntentEntity.\n        """"""\n        labor = intent_models.MultiTaskIntentModel(use_cudnn=False)\n        model = TextKerasModel._load_model(labor, path)\n        model.__class__ = IntentEntity\n        return model\n'"
pyzoo/zoo/tfpark/text/keras/ner.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport nlp_architect.models.ner_crf as ner_model\nfrom zoo.tfpark.text.keras.text_model import TextKerasModel\n\n\nclass NER(TextKerasModel):\n    """"""\n    The model used for named entity recognition using Bidirectional LSTM with\n    Conditional Random Field (CRF) sequence classifier.\n\n    This model has two inputs:\n    - word indices of shape (batch, sequence_length)\n    - character indices of shape (batch, sequence_length, word_length)\n    This model outputs entity tags of shape (batch, sequence_length, num_entities).\n\n    :param num_entities: Positive int. The number of entity labels to be classified.\n    :param word_vocab_size: Positive int. The size of the word dictionary.\n    :param char_vocab_size: Positive int. The size of the character dictionary.\n    :param word_length: Positive int. The max word length in characters. Default is 12.\n    :param word_emb_dim: Positive int. The dimension of word embeddings. Default is 100.\n    :param char_emb_dim: Positive int. The dimension of character embeddings. Default is 30.\n    :param tagger_lstm_dim: Positive int. The hidden size of tagger Bi-LSTM layers. Default is 100.\n    :param dropout: Dropout rate. Default is 0.5.\n    :param crf_mode: String. CRF operation mode. Either \'reg\' or \'pad\'. Default is \'reg\'.\n                     \'reg\' for regular full sequence learning (all sequences have equal length).\n                     \'pad\' for supplied sequence lengths (useful for padded sequences).\n                     For \'pad\' mode, a third input for sequence_length (batch, 1) is needed.\n    :param optimizer: Optimizer to train the model. If not specified, it will by default\n                      to be tf.keras.optimizers.Adam(0.001, clipnorm=5.).\n    """"""\n    def __init__(self, num_entities, word_vocab_size, char_vocab_size, word_length=12,\n                 word_emb_dim=100, char_emb_dim=30, tagger_lstm_dim=100, dropout=0.5,\n                 crf_mode=\'reg\', optimizer=None):\n        super(NER, self).__init__(ner_model.NERCRF(use_cudnn=False), optimizer,\n                                  word_length=word_length,\n                                  target_label_dims=num_entities,\n                                  word_vocab_size=word_vocab_size,\n                                  char_vocab_size=char_vocab_size,\n                                  word_embedding_dims=word_emb_dim,\n                                  char_embedding_dims=char_emb_dim,\n                                  tagger_lstm_dims=tagger_lstm_dim,\n                                  dropout=dropout,\n                                  crf_mode=crf_mode)\n        # Remark: In nlp-architect NERCRF.build(..), word_lstm_dims is never used.\n        # Thus, removed this argument here to avoid ambiguity.\n\n    @staticmethod\n    def load_model(path):\n        """"""\n        Load an existing NER model (with weights) from HDF5 file.\n\n        :param path: String. The path to the pre-defined model.\n        :return: NER.\n        """"""\n        labor = ner_model.NERCRF(use_cudnn=False)\n        model = TextKerasModel._load_model(labor, path)\n        model.__class__ = NER\n        return model\n'"
pyzoo/zoo/tfpark/text/keras/pos_tagging.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom nlp_architect.models import chunker\nfrom zoo.tfpark.text.keras.text_model import TextKerasModel\n\n\nclass SequenceTagger(TextKerasModel):\n    """"""\n    The model used as POS-tagger and chunker for sentence tagging, which contains three\n    Bidirectional LSTM layers.\n\n    This model can have one or two input(s):\n    - word indices of shape (batch, sequence_length)\n    *If char_vocab_size is not None:\n    - character indices of shape (batch, sequence_length, word_length)\n    This model has two outputs:\n    - pos tags of shape (batch, sequence_length, num_pos_labels)\n    - chunk tags of shape (batch, sequence_length, num_chunk_labels)\n\n    :param num_pos_labels: Positive int. The number of pos labels to be classified.\n    :param num_chunk_labels: Positive int. The number of chunk labels to be classified.\n    :param word_vocab_size: Positive int. The size of the word dictionary.\n    :param char_vocab_size: Positive int. The size of the character dictionary.\n                            Default is None and in this case only one input, namely word indices\n                            is expected.\n    :param word_length: Positive int. The max word length in characters. Default is 12.\n    :param feature_size: Positive int. The size of Embedding and Bi-LSTM layers. Default is 100.\n    :param dropout: Dropout rate. Default is 0.5.\n    :param classifier: String. The classification layer used for tagging chunks.\n                       Either \'softmax\' or \'crf\' (Conditional Random Field). Default is \'softmax\'.\n    :param optimizer: Optimizer to train the model. If not specified, it will by default\n                      to be tf.train.AdamOptimizer().\n    """"""\n    def __init__(self, num_pos_labels, num_chunk_labels, word_vocab_size,\n                 char_vocab_size=None, word_length=12, feature_size=100, dropout=0.2,\n                 classifier=\'softmax\', optimizer=None):\n        classifier = classifier.lower()\n        assert classifier in [\'softmax\', \'crf\'], ""classifier should be either softmax or crf""\n        super(SequenceTagger, self).__init__(chunker.SequenceTagger(use_cudnn=False),\n                                             vocabulary_size=word_vocab_size,\n                                             num_pos_labels=num_pos_labels,\n                                             num_chunk_labels=num_chunk_labels,\n                                             char_vocab_size=char_vocab_size,\n                                             max_word_len=word_length,\n                                             feature_size=feature_size,\n                                             dropout=dropout,\n                                             classifier=classifier,\n                                             optimizer=optimizer)\n\n    @staticmethod\n    def load_model(path):\n        """"""\n        Load an existing SequenceTagger model (with weights) from HDF5 file.\n\n        :param path: String. The path to the pre-defined model.\n        :return: NER.\n        """"""\n        labor = chunker.SequenceTagger(use_cudnn=False)\n        model = TextKerasModel._load_model(labor, path)\n        model.__class__ = SequenceTagger\n        return model\n'"
pyzoo/zoo/tfpark/text/keras/text_model.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.tfpark import KerasModel\n\n\n# TODO: add word embedding file support\nclass TextKerasModel(KerasModel):\n    """"""\n    The base class for text models in tfpark.\n    """"""\n    def __init__(self, labor, optimizer=None, **kwargs):\n        self.labor = labor\n        self.labor.build(**kwargs)\n        model = self.labor.model\n        # Recompile the model if user uses a different optimizer other than the default one.\n        if optimizer:\n            model.compile(loss=model.loss, optimizer=optimizer, metrics=model.metrics)\n        super(TextKerasModel, self).__init__(model)\n\n    # Remark: nlp-architect CRF layer has error when directly loaded by tf.keras.models.load_model.\n    # Thus we keep the nlp-architect class as labor and uses its save/load,\n    # which only saves the weights with model parameters\n    # and reconstruct the model using the exact parameters and setting weights when loading.\n    def save_model(self, path):\n        """"""\n        Save the model to a single HDF5 file.\n\n        :param path: String. The path to save the model.\n        """"""\n        self.labor.save(path)\n\n    @staticmethod\n    def _load_model(labor, path):\n        labor.load(path)\n        model = KerasModel(labor.model)\n        model.labor = labor\n        return model\n'"
zoo/src/test/resources/serving/imagenet_enqueuer.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.serving.client import InputQueue, OutputQueue\nimport cv2\nimport os\nimport json\nimport yaml\nimport time\nimport subprocess\nimport sys\nfrom multiprocessing import Process\nfrom optparse import OptionParser\n\n\n# To enqueue all image in specified directory and validate them using label txt\n# using 8 threads to accelerate enqueueing image\nif __name__ == ""__main__"":\n\n    def inqueue_part(in_api: InputQueue, base_path, path_list):\n        print(""this thread got "", len(path_list), "" images"")\n        for p in path_list:\n            if not p.endswith(""jpeg"") and not p.endswith(""JPEG""):\n                continue\n            img = cv2.imread(os.path.join(base_path, p))\n            img = cv2.resize(img, (224, 224))\n            input_api.enqueue_image(p, img)\n    # params are set here, only need to set model and image path\n\n    parser = OptionParser()\n    parser.add_option(""--img_path"", type=str, dest=""img_path"",\n                      help=""The path of images you want to validate"")\n\n    parser.add_option(""--img_num"", type=int, dest=""img_num"",\n                      help=""The total number of images you validate"")\n    (options, args) = parser.parse_args(sys.argv)\n\n    val_img_path = options.img_path\n    total_img_num = options.img_num\n\n    val_txt = os.path.join(val_img_path, ""val.txt"")\n\n    path_list = os.listdir(val_img_path)\n\n    input_api = InputQueue()\n\n    # push image in queue\n    top1_dict = {}\n    top5_dict = {}\n    # for p in path_list:\n    #     if not p.endswith(format_wanted):\n    #         continue\n    #     img = cv2.imread(os.path.join(val_img_path, p))\n    #     img = cv2.resize(img, (224, 224))\n    #     redis_queue.enqueue_image(p, img)\n    piece_len = int(len(path_list) / 8)\n    s = b = 0\n    procs = []\n    for i in range(8):\n        a = s\n        s += piece_len\n        if i == 7:\n            b = len(path_list)\n        else:\n            b = s\n        proc = Process(target=inqueue_part,\n                       args=(input_api, val_img_path, path_list[a:b]))\n        procs.append(proc)\n        proc.start()\n    for proc in procs:\n        proc.join()\n\n    # after image all pushed, start check from queue\n    # get result from queue, wait until all written\n    res_list = None\n    while not res_list or len(res_list) < total_img_num:\n        time.sleep(5)\n        res_list = input_api.db.keys(\'result:*\')\n        print(""Current records in Redis:"", len(res_list))\n\n    # prepare for validation, store result to dict\n    output_api = OutputQueue()\n    res_dict = output_api.dequeue()\n\n    for uri in res_dict.keys():\n\n        tmp_list = json.loads(res_dict[uri])\n        top1_s, top5_s = set(), set()\n        top1_s.add(tmp_list[0][0])\n        for i in range(len(tmp_list)):\n            top5_s.add(tmp_list[i][0])\n        top5_dict[uri] = top5_s\n        top1_dict[uri] = top1_s\n\n    total, top1, top5 = 0, 0, 0\n\n    # open label txt file and count the validation result\n    with open(val_txt) as f:\n        for line in f:\n            line = line.strip().split(\' \')\n            img_uri, img_cls = line[0], int(line[1])\n            if img_cls in top1_dict[img_uri]:\n                top1 += 1\n            if img_cls in top5_dict[img_uri]:\n                top5 += 1\n            total += 1\n    print(""top 1 accuracy is "", float(top1) / total)\n    print(""top 5 accuracy is "", float(top5) / total)\n\n    # shutdown serving and re-initialize all\n    # serving_process.terminate()\n'"
zoo/src/test/resources/tf/test.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# script to generate multi_type_inputs_outputs.pb\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\n\nfloat_input = tf.placeholder(dtype=tf.float32, name=""float_input"", shape=(None, 1))\ndouble_input = tf.placeholder(dtype=tf.float64, name=""double_input"", shape=(None, 1))\nint_input = tf.placeholder(dtype=tf.int32, name=""int_input"", shape=(None, 1))\nlong_input = tf.placeholder(dtype=tf.int64, name=""long_input"", shape=(None, 1))\nuint8_input = tf.placeholder(dtype=tf.uint8, name=""uint8_input"", shape=(None, 1))\n\nfloat_output = tf.identity(float_input, name=""float_output"")\ndouble_output = tf.identity(double_input, name=""double_output"")\nint_output = tf.identity(int_input, name=""int_output"")\nlong_output = tf.identity(long_input, name=""long_output"")\nuint8_output = tf.identity(uint8_input, name=""uint8_output"")\n\nwith gfile.GFile(""./multi_type_inputs_outputs.pb"", ""wb"") as f:\n    f.write(tf.get_default_graph().as_graph_def().SerializeToString())'"
pyzoo/test/zoo/orca/learn/mxnet/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/orca/learn/mxnet/conftest.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pytest\n\nsc = None\nray_ctx = None\n\n\n@pytest.fixture(autouse=True, scope=\'package\')\ndef rayonspark_fixture():\n    from zoo import init_spark_on_local\n    from zoo.ray import RayContext\n    global ray_ctx\n    sc = init_spark_on_local(cores=8, spark_log_level=""INFO"")\n    ray_ctx = RayContext(sc=sc, object_store_memory=""1g"")\n    ray_ctx.init()\n    yield\n    ray_ctx.stop()\n    sc.stop()\n\n\ndef get_ray_ctx():\n    return ray_ctx\n'"
pyzoo/test/zoo/orca/learn/mxnet/test_mxnet_gluon.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom unittest import TestCase\n\nimport numpy as np\nimport pytest\n\nimport mxnet as mx\nfrom mxnet import gluon\nfrom mxnet.gluon import nn\nfrom zoo.ray import RayContext\nfrom zoo.orca.learn.mxnet import MXNetTrainer, create_trainer_config\n\nnp.random.seed(1337)  # for reproducibility\n\n\ndef get_train_data_iter(config, kv):\n    train_data = np.random.rand(200, 30)\n    train_label = np.random.randint(0, 10, (200,))\n    train = mx.io.NDArrayIter(train_data, train_label,\n                              batch_size=config[""batch_size""], shuffle=True)\n    return train\n\n\ndef get_test_data_iter(config, kv):\n    test_data = np.random.rand(80, 30)\n    test_label = np.random.randint(0, 10, (80,))\n    test = mx.io.NDArrayIter(test_data, test_label,\n                             batch_size=config[""batch_size""], shuffle=True)\n    return test\n\n\ndef get_model(config):\n    class SimpleModel(gluon.Block):\n        def __init__(self, **kwargs):\n            super(SimpleModel, self).__init__(**kwargs)\n            self.fc1 = nn.Dense(20)\n            self.fc2 = nn.Dense(10)\n\n        def forward(self, x):\n            x = self.fc1(x)\n            x = self.fc2(x)\n            return x\n\n    net = SimpleModel()\n    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=[mx.cpu()])\n    return net\n\n\ndef get_loss(config):\n    return gluon.loss.SoftmaxCrossEntropyLoss()\n\n\ndef get_metrics(config):\n    return mx.metric.Accuracy()\n\n\nclass TestMXNetGluon(TestCase):\n    def test_gluon(self):\n        current_ray_ctx = RayContext.get()\n        address_info = current_ray_ctx.address_info\n        assert ""object_store_address"" in address_info\n        config = create_trainer_config(batch_size=32, log_interval=2, optimizer=""adam"",\n                                       optimizer_params={\'learning_rate\': 0.02})\n        trainer = MXNetTrainer(config, get_train_data_iter, get_model, get_loss,\n                               eval_metrics_creator=get_metrics,\n                               validation_metrics_creator=get_metrics,\n                               num_workers=2, test_data=get_test_data_iter)\n        trainer.train(nb_epoch=2)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/orca/learn/mxnet/test_mxnet_symbol.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom unittest import TestCase\n\nimport numpy as np\nimport pytest\n\nimport mxnet as mx\nfrom zoo.orca.learn.mxnet import MXNetTrainer, create_trainer_config\n\nnp.random.seed(1337)  # for reproducibility\n\n\ndef get_train_data_iter(config, kv):\n    train_data = np.random.rand(200, 30)\n    train_label = np.random.randint(0, 10, (200,))\n    train = mx.io.NDArrayIter({""input"": train_data}, {""label"": train_label},\n                              batch_size=config[""batch_size""], shuffle=True)\n    return train\n\n\ndef get_test_data_iter(config, kv):\n    test_data = np.random.rand(80, 30)\n    test_label = np.random.randint(0, 10, (80,))\n    test = mx.io.NDArrayIter({""input"": test_data}, {""label"": test_label},\n                             batch_size=config[""batch_size""], shuffle=True)\n    return test\n\n\ndef get_model(config):\n    input_data = mx.symbol.Variable(\'input\')\n    y_true = mx.symbol.Variable(\'label\')\n    fc1 = mx.symbol.FullyConnected(data=input_data, num_hidden=20, name=\'fc1\')\n    fc2 = mx.symbol.FullyConnected(data=fc1, num_hidden=10, name=\'fc2\')\n    output = mx.symbol.SoftmaxOutput(data=fc2, label=y_true, name=\'output\')\n    mod = mx.mod.Module(symbol=output,\n                        data_names=[\'input\'],\n                        label_names=[\'label\'],\n                        context=mx.cpu())\n    return mod\n\n\ndef get_metrics(config):\n    return \'accuracy\'\n\n\nclass TestMXNetSymbol(TestCase):\n    def test_symbol(self):\n        config = create_trainer_config(batch_size=32, log_interval=2, seed=42)\n        trainer = MXNetTrainer(config, get_train_data_iter, get_model,\n                               validation_metrics_creator=get_metrics,\n                               test_data=get_test_data_iter, eval_metrics_creator=get_metrics)\n        trainer.train(nb_epoch=2)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/orca/learn/mxnet/test_mxnet_xshards.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom unittest import TestCase\n\nimport os.path\nimport pytest\n\nimport numpy as np\nimport mxnet as mx\nfrom mxnet import gluon\nfrom mxnet.gluon import nn\nimport zoo.orca.data.pandas\nfrom zoo.orca.learn.mxnet import MXNetTrainer, create_trainer_config\nfrom test.zoo.orca.learn.mxnet.conftest import get_ray_ctx\n\n\ndef prepare_data_symbol(df):\n    data = {\'input\': np.array(df[\'data\'].values.tolist())}\n    label = {\'label\': df[\'label\'].values}\n    return {\'x\': data, \'y\': label}\n\n\ndef prepare_data_gluon(df):\n    data = np.array(df[\'data\'].values.tolist())\n    label = df[\'label\'].values\n    return {\'x\': data, \'y\': label}\n\n\ndef prepare_data_list(df):\n    data = [np.array(df[\'data_x\'].values.tolist()), np.array(df[\'data_y\'].values.tolist())]\n    label = df[\'label\'].values\n    return {\'x\': data, \'y\': label}\n\n\ndef get_loss(config):\n    return gluon.loss.SoftmaxCrossEntropyLoss()\n\n\ndef get_gluon_metrics(config):\n    return mx.metric.Accuracy()\n\n\ndef get_metrics(config):\n    return \'accuracy\'\n\n\ndef get_symbol_model(config):\n    input_data = mx.symbol.Variable(\'input\')\n    y_true = mx.symbol.Variable(\'label\')\n    fc1 = mx.symbol.FullyConnected(data=input_data, num_hidden=20, name=\'fc1\')\n    fc2 = mx.symbol.FullyConnected(data=fc1, num_hidden=10, name=\'fc2\')\n    output = mx.symbol.SoftmaxOutput(data=fc2, label=y_true, name=\'output\')\n    mod = mx.mod.Module(symbol=output,\n                        data_names=[\'input\'],\n                        label_names=[\'label\'],\n                        context=mx.cpu())\n    return mod\n\n\ndef get_gluon_model(config):\n    class SimpleModel(gluon.Block):\n        def __init__(self, **kwargs):\n            super(SimpleModel, self).__init__(**kwargs)\n            self.fc1 = nn.Dense(20)\n            self.fc2 = nn.Dense(10)\n\n        def forward(self, x):\n            x = self.fc1(x)\n            x = self.fc2(x)\n            return x\n\n    net = SimpleModel()\n    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=[mx.cpu()])\n    return net\n\n\nclass TestMXNetXShards(TestCase):\n    def test_xshards_symbol(self):\n        # prepare data\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../../resources"")\n        self.ray_ctx = get_ray_ctx()\n        train_file_path = os.path.join(resource_path, ""orca/learn/train_data.json"")\n        train_data_shard = zoo.orca.data.pandas.read_json(train_file_path, self.ray_ctx,\n                                                          orient=\'records\', lines=False)\n        train_data_shard.transform_shard(prepare_data_symbol)\n        test_file_path = os.path.join(resource_path, ""orca/learn/test_data.json"")\n        test_data_shard = zoo.orca.data.pandas.read_json(test_file_path, self.ray_ctx,\n                                                         orient=\'records\', lines=False)\n        test_data_shard.transform_shard(prepare_data_symbol)\n        config = create_trainer_config(batch_size=32, log_interval=1, seed=42)\n        trainer = MXNetTrainer(config, train_data_shard, get_symbol_model,\n                               validation_metrics_creator=get_metrics, test_data=test_data_shard,\n                               eval_metrics_creator=get_metrics)\n        trainer.train(nb_epoch=2)\n\n    def test_xshards_gluon(self):\n        # prepare data\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../../resources"")\n        self.ray_ctx = get_ray_ctx()\n        train_file_path = os.path.join(resource_path, ""orca/learn/train_data.json"")\n        train_data_shard = zoo.orca.data.pandas.read_json(train_file_path, self.ray_ctx,\n                                                          orient=\'records\', lines=False)\n        train_data_shard.transform_shard(prepare_data_gluon)\n        test_file_path = os.path.join(resource_path, ""orca/learn/test_data.json"")\n        test_data_shard = zoo.orca.data.pandas.read_json(test_file_path, self.ray_ctx,\n                                                         orient=\'records\', lines=False)\n        test_data_shard.transform_shard(prepare_data_gluon)\n        config = create_trainer_config(batch_size=32, log_interval=1, seed=42)\n        trainer = MXNetTrainer(config, train_data_shard, get_gluon_model, get_loss,\n                               validation_metrics_creator=get_gluon_metrics,\n                               test_data=test_data_shard, eval_metrics_creator=get_gluon_metrics)\n        trainer.train(nb_epoch=2)\n\n    def test_xshard_list(self):\n        # prepare data\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../../resources"")\n        self.ray_ctx = get_ray_ctx()\n        train_file_path = os.path.join(resource_path, ""orca/learn/train_data_list.json"")\n        train_data_shard = zoo.orca.data.pandas.read_json(train_file_path, self.ray_ctx,\n                                                          orient=""records"", lines=False)\n        train_data_shard.transform_shard(prepare_data_list)\n        test_file_path = os.path.join(resource_path, ""orca/learn/test_data_list.json"")\n        test_data_shard = zoo.orca.data.pandas.read_json(test_file_path, self.ray_ctx,\n                                                         orient=""records"", lines=False)\n        test_data_shard.transform_shard(prepare_data_list)\n        config = create_trainer_config(batch_size=32, log_interval=1, seed=42)\n        trainer = MXNetTrainer(config, train_data_shard, get_gluon_model, get_loss,\n                               validation_metrics_creator=get_gluon_metrics,\n                               test_data=test_data_shard, eval_metrics_creator=get_gluon_metrics)\n        trainer.train(nb_epoch=2)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/orca/learn/pytorch/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/orca/learn/pytorch/conftest.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pytest\n\nsc = None\nray_ctx = None\n\n\n@pytest.fixture(autouse=True, scope=\'package\')\ndef rayonspark_fixture():\n    from zoo import init_spark_on_local\n    from zoo.ray import RayContext\n    sc = init_spark_on_local(cores=8, spark_log_level=""INFO"")\n    ray_ctx = RayContext(sc=sc, object_store_memory=""1g"")\n    ray_ctx.init()\n    yield\n    ray_ctx.stop()\n    sc.stop()\n'"
pyzoo/test/zoo/orca/learn/pytorch/test_pytorch_trainer.py,7,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom unittest import TestCase\n\nimport numpy as np\nimport pytest\n\nimport torch\nimport torch.nn as nn\nfrom zoo.orca.learn.pytorch import PyTorchTrainer\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass LinearDataset(torch.utils.data.Dataset):\n    """"""y = a * x + b""""""\n\n    def __init__(self, a, b, size=1000):\n        x = np.arange(0, 10, 10 / size, dtype=np.float32)\n        self.x = torch.from_numpy(x)\n        self.y = torch.from_numpy(a * x + b)\n\n    def __getitem__(self, index):\n        return self.x[index, None], self.y[index, None]\n\n    def __len__(self):\n        return len(self.x)\n\n\ndef get_data_loaders(config):\n    train_dataset = LinearDataset(2, 5, size=config.get(""data_size"", 1000))\n    val_dataset = LinearDataset(2, 5, size=config.get(""val_size"", 400))\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.get(""batch_size"", 32),\n    )\n    validation_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=config.get(""batch_size"", 32))\n    return train_loader, validation_loader\n\n\ndef get_model(config):\n    return nn.Linear(1, config.get(""hidden_size"", 1))\n\n\ndef get_optimizer(model, config):\n    return torch.optim.SGD(model.parameters(), lr=config.get(""lr"", 1e-2))\n\n\nclass TestPyTorchTrainer(TestCase):\n    def test_linear(self):\n        trainer = PyTorchTrainer(model_creator=get_model,\n                                 data_creator=get_data_loaders,\n                                 optimizer_creator=get_optimizer,\n                                 loss_creator=nn.MSELoss,\n                                 config={""lr"": 1e-2, ""hidden_size"": 1,\n                                         ""batch_size"": 128})\n        stats = trainer.train(nb_epoch=2)\n        print(stats)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/orca/learn/spark/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/orca/learn/spark/conftest.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport pytest\n\n\n@pytest.fixture(autouse=True, scope=\'package\')\ndef estimator_for_spark_fixture():\n    from zoo import init_spark_on_local\n    sc = init_spark_on_local(cores=4, spark_log_level=""INFO"")\n    yield sc\n    sc.stop()\n'"
pyzoo/test/zoo/orca/learn/spark/test_estimator_for_spark.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport tensorflow as tf\n\nfrom zoo.orca.data.tf.data import Dataset\nfrom zoo.orca.learn.tf.estimator import Estimator\nimport zoo.orca.data.pandas\n\nresource_path = os.path.join(os.path.split(__file__)[0], ""../../../resources"")\n\n\nclass SimpleModel(object):\n\n    def __init__(self):\n        self.user = tf.placeholder(dtype=tf.int32, shape=(None,))\n        self.item = tf.placeholder(dtype=tf.int32, shape=(None,))\n        self.label = tf.placeholder(dtype=tf.int32, shape=(None,))\n\n        feat = tf.stack([self.user, self.item], axis=1)\n        self.logits = tf.layers.dense(tf.to_float(feat), 2)\n\n        self.loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=self.logits,\n                                                                          labels=self.label))\n\n\ndef test_estimator_graph(estimator_for_spark_fixture):\n    import zoo.orca.data.pandas\n\n    sc = estimator_for_spark_fixture\n\n    tf.reset_default_graph()\n\n    model = SimpleModel()\n    file_path = os.path.join(resource_path, ""orca/learn/ncf.csv"")\n    data_shard = zoo.orca.data.pandas.read_csv(file_path, sc)\n\n    def transform(df):\n        result = {\n            ""x"": (df[\'user\'].to_numpy(), df[\'item\'].to_numpy()),\n            ""y"": df[\'label\'].to_numpy()\n        }\n        return result\n\n    data_shard = data_shard.transform_shard(transform)\n\n    est = Estimator.from_graph(\n        inputs=[model.user, model.item],\n        labels=[model.label],\n        outputs=[model.logits],\n        loss=model.loss,\n        optimizer=tf.train.AdamOptimizer(),\n        metrics={""loss"": model.loss})\n    est.fit(data=data_shard,\n            batch_size=8,\n            steps=10,\n            validation_data=data_shard)\n\n    data_shard = zoo.orca.data.pandas.read_csv(file_path, sc)\n\n    def transform(df):\n        result = {\n            ""x"": (df[\'user\'].to_numpy(), df[\'item\'].to_numpy()),\n        }\n        return result\n\n    data_shard = data_shard.transform_shard(transform)\n    predictions = est.predict(data_shard).collect()\n    print(predictions)\n\n\ndef test_estimator_graph_fit(estimator_for_spark_fixture):\n    import zoo.orca.data.pandas\n    tf.reset_default_graph()\n\n    model = SimpleModel()\n    sc = estimator_for_spark_fixture\n    file_path = os.path.join(resource_path, ""orca/learn/ncf.csv"")\n    data_shard = zoo.orca.data.pandas.read_csv(file_path, sc)\n\n    def transform(df):\n        result = {\n            ""x"": (df[\'user\'].to_numpy(), df[\'item\'].to_numpy()),\n            ""y"": df[\'label\'].to_numpy()\n        }\n        return result\n\n    data_shard = data_shard.transform_shard(transform)\n\n    est = Estimator.from_graph(\n        inputs=[model.user, model.item],\n        labels=[model.label],\n        loss=model.loss,\n        optimizer=tf.train.AdamOptimizer(),\n        metrics={""loss"": model.loss})\n    est.fit(data=data_shard,\n            batch_size=8,\n            steps=10,\n            validation_data=data_shard)\n\n\ndef test_estimator_graph_predict(estimator_for_spark_fixture):\n    import zoo.orca.data.pandas\n    tf.reset_default_graph()\n\n    sc = estimator_for_spark_fixture\n\n    model = SimpleModel()\n    file_path = os.path.join(resource_path, ""orca/learn/ncf.csv"")\n    data_shard = zoo.orca.data.pandas.read_csv(file_path, sc)\n\n    est = Estimator.from_graph(\n        inputs=[model.user, model.item],\n        outputs=[model.logits])\n\n    def transform(df):\n        result = {\n            ""x"": (df[\'user\'].to_numpy(), df[\'item\'].to_numpy()),\n        }\n        return result\n\n    data_shard = data_shard.transform_shard(transform)\n    predictions = est.predict(data_shard).collect()\n    print(predictions)\n\n\ndef test_estimator_graph_fit_dataset(estimator_for_spark_fixture):\n    import zoo.orca.data.pandas\n    tf.reset_default_graph()\n    model = SimpleModel()\n    sc = estimator_for_spark_fixture\n    file_path = os.path.join(resource_path, ""orca/learn/ncf.csv"")\n    data_shard = zoo.orca.data.pandas.read_csv(file_path, sc)\n\n    def transform(df):\n        result = {\n            ""x"": (df[\'user\'].to_numpy(), df[\'item\'].to_numpy()),\n            ""y"": df[\'label\'].to_numpy()\n        }\n        return result\n\n    data_shard = data_shard.transform_shard(transform)\n    dataset = Dataset.from_tensor_slices(data_shard)\n\n    est = Estimator.from_graph(\n        inputs=[model.user, model.item],\n        labels=[model.label],\n        loss=model.loss,\n        optimizer=tf.train.AdamOptimizer(),\n        metrics={""loss"": model.loss})\n    est.fit(data=dataset,\n            batch_size=8,\n            steps=10,\n            validation_data=dataset)\n\n\ndef test_estimator_graph_predict_dataset(estimator_for_spark_fixture):\n\n    sc = estimator_for_spark_fixture\n    tf.reset_default_graph()\n\n    model = SimpleModel()\n    file_path = os.path.join(resource_path, ""orca/learn/ncf.csv"")\n    data_shard = zoo.orca.data.pandas.read_csv(file_path, sc)\n\n    est = Estimator.from_graph(\n        inputs=[model.user, model.item],\n        outputs=[model.logits])\n\n    def transform(df):\n        result = {\n            ""x"": (df[\'user\'].to_numpy(), df[\'item\'].to_numpy()),\n        }\n        return result\n\n    data_shard = data_shard.transform_shard(transform)\n    dataset = Dataset.from_tensor_slices(data_shard)\n    predictions = est.predict(dataset).collect()\n    print(predictions)\n\n\nif __name__ == ""__main__"":\n    import pytest\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/pipeline/api/keras/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/test/zoo/pipeline/api/keras/test_layer.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nimport numpy as np\n\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nimport zoo.pipeline.api.keras.layers as ZLayer\nfrom zoo.pipeline.api.keras.models import Model as ZModel, Sequential as ZSequential\nimport keras.layers as KLayer\nfrom keras.engine import merge as kmerge, Model as KModel\nfrom keras.models import Sequential as KSequential\nimport keras.backend as K\nfrom bigdl.keras.converter import WeightsConverter\nfrom zoo.pipeline.api.keras import regularizers\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestLayer(ZooTestCase):\n\n    def test_embedding(self):\n        input_data = np.random.randint(1000, size=(32, 10))\n        zlayer = ZLayer.Embedding(1000, 64, input_shape=(10, ))\n        klayer = KLayer.Embedding(1000, 64, input_length=10)\n        self.compare_layer(klayer, zlayer, input_data,\n                           WeightsConverter.convert_embedding)\n\n    def test_batchnormalization(self):\n        print(""Running batch normal test"")\n        K.set_image_dim_ordering(""th"")\n        input_data = np.random.random_sample([2, 5, 32, 32])\n        zlayer = ZLayer.BatchNormalization(axis=1, input_shape=(5, 32, 32))\n        klayer = KLayer.BatchNormalization(axis=1, input_shape=(5, 32, 32))\n        self.compare_layer(klayer, zlayer, input_data,\n                           WeightsConverter.convert_batchnormalization)\n        K.set_image_dim_ordering(""tf"")\n        input_data2 = np.random.random_sample([2, 32, 32, 4])\n        zlayer = ZLayer.BatchNormalization(axis=-1, dim_ordering=""tf"", input_shape=(32, 32, 4))\n        klayer = KLayer.BatchNormalization(axis=-1, input_shape=(32, 32, 4))\n        self.compare_layer(klayer, zlayer, input_data2,\n                           WeightsConverter.convert_batchnormalization)\n        K.set_image_dim_ordering(""th"")\n        input_data = np.random.random_sample([2, 5])\n        zlayer = ZLayer.BatchNormalization(axis=1, input_shape=(5,))\n        klayer = KLayer.BatchNormalization(axis=1, input_shape=(5,))\n        self.compare_layer(klayer, zlayer, input_data,\n                           WeightsConverter.convert_batchnormalization)\n\n    def test_merge_sum(self):\n        z1 = ZLayer.InputLayer(input_shape=(3, 5))\n        z2 = ZLayer.InputLayer(input_shape=(3, 5))\n        zlayer = ZLayer.Merge(layers=[z1, z2], mode=""sum"")\n        k1 = KLayer.InputLayer(input_shape=(3, 5))\n        k2 = KLayer.InputLayer(input_shape=(3, 5))\n        klayer = KLayer.Merge(layers=[k1, k2], mode=""sum"")\n        input_data = [np.random.random([2, 3, 5]), np.random.random([2, 3, 5])]\n        self.compare_layer(klayer, zlayer, input_data)\n\n    def test_merge_mul(self):\n        z1 = ZLayer.InputLayer(input_shape=(3, 5))\n        z2 = ZLayer.InputLayer(input_shape=(3, 5))\n        zlayer = ZLayer.Merge(layers=[z1, z2], mode=""mul"")\n        k1 = KLayer.InputLayer(input_shape=(3, 5))\n        k2 = KLayer.InputLayer(input_shape=(3, 5))\n        klayer = KLayer.Merge(layers=[k1, k2], mode=""mul"")\n        input_data = [np.random.random([2, 3, 5]), np.random.random([2, 3, 5])]\n        self.compare_layer(klayer, zlayer, input_data)\n\n    def test_merge_ave(self):\n        z1 = ZLayer.InputLayer(input_shape=(2, 5, 8))\n        z2 = ZLayer.InputLayer(input_shape=(2, 5, 8))\n        zlayer = ZLayer.Merge(layers=[z1, z2], mode=""ave"")\n        k1 = KLayer.InputLayer(input_shape=(2, 5, 8))\n        k2 = KLayer.InputLayer(input_shape=(2, 5, 8))\n        klayer = KLayer.Merge(layers=[k1, k2], mode=""ave"")\n        input_data = [np.random.random([3, 2, 5, 8]), np.random.random([3, 2, 5, 8])]\n        self.compare_layer(klayer, zlayer, input_data)\n\n    def test_merge_max(self):\n        z1 = ZLayer.InputLayer(input_shape=(2, 5, 8))\n        z2 = ZLayer.InputLayer(input_shape=(2, 5, 8))\n        zlayer = ZLayer.Merge(layers=[z1, z2], mode=""max"")\n        k1 = KLayer.InputLayer(input_shape=(2, 5, 8))\n        k2 = KLayer.InputLayer(input_shape=(2, 5, 8))\n        klayer = KLayer.Merge(layers=[k1, k2], mode=""max"")\n        input_data = [np.random.random([3, 2, 5, 8]), np.random.random([3, 2, 5, 8])]\n        self.compare_layer(klayer, zlayer, input_data)\n\n    def test_merge_concat(self):\n        z1 = ZLayer.InputLayer(input_shape=(2, 5, 11))\n        z2 = ZLayer.InputLayer(input_shape=(2, 5, 8))\n        zlayer = ZLayer.Merge(layers=[z1, z2], mode=""concat"")\n        k1 = KLayer.InputLayer(input_shape=(2, 5, 11))\n        k2 = KLayer.InputLayer(input_shape=(2, 5, 8))\n        klayer = KLayer.Merge(layers=[k1, k2], mode=""concat"")\n        input_data = [np.random.random([3, 2, 5, 11]), np.random.random([3, 2, 5, 8])]\n        self.compare_layer(klayer, zlayer, input_data)\n\n    def test_merge_dot(self):\n        z1 = ZLayer.InputLayer(input_shape=(4, ))\n        z2 = ZLayer.InputLayer(input_shape=(4, ))\n        zlayer = ZLayer.Merge(layers=[z1, z2], mode=""dot"")\n        k1 = KLayer.InputLayer(input_shape=(4, ))\n        k2 = KLayer.InputLayer(input_shape=(4, ))\n        klayer = KLayer.Merge(layers=[k1, k2], mode=""dot"")\n        input_data = [np.random.random([2, 4]), np.random.random([2, 4])]\n        self.compare_layer(klayer, zlayer, input_data)\n\n    def test_merge_cos(self):\n        z1 = ZLayer.InputLayer(input_shape=(3, ))\n        z2 = ZLayer.InputLayer(input_shape=(3, ))\n        zlayer = ZLayer.Merge(layers=[z1, z2], mode=""cos"")\n        k1 = KLayer.InputLayer(input_shape=(3, ))\n        k2 = KLayer.InputLayer(input_shape=(3, ))\n        klayer = KLayer.Merge(layers=[k1, k2], mode=""cos"")\n        input_data = [np.random.random([2, 3]), np.random.random([2, 3])]\n        self.compare_layer(klayer, zlayer, input_data)\n\n    def convert_two_dense(self, kmodel, weights):\n        return [weights[2].T, weights[3], weights[0].T, weights[1]]\n\n    def test_merge_method_sum(self):\n        zx1 = ZLayer.Input(shape=(8, ))\n        zx2 = ZLayer.Input(shape=(6, ))\n        zy1 = ZLayer.Dense(10)(zx1)\n        zy2 = ZLayer.Dense(10)(zx2)\n        zz = ZLayer.merge([zy1, zy2], mode=""sum"")\n        zmodel = ZModel([zx1, zx2], zz, name=""graph1"")\n\n        kx1 = KLayer.Input(shape=(8, ))\n        kx2 = KLayer.Input(shape=(6, ))\n        ky1 = KLayer.Dense(10)(kx1)\n        ky2 = KLayer.Dense(10)(kx2)\n        kz = kmerge([ky1, ky2], mode=""sum"")\n        kmodel = KModel([kx1, kx2], kz)\n\n        input_data = [np.random.random([2, 8]), np.random.random([2, 6])]\n        self.compare_layer(kmodel, zmodel, input_data, self.convert_two_dense)\n\n    def test_merge_method_model_concat(self):\n        zx1 = ZLayer.Input(shape=(4, ))\n        zx2 = ZLayer.Input(shape=(5, ))\n        zy1 = ZLayer.Dense(6, activation=""sigmoid"")(zx1)\n        zbranch1 = ZModel(zx1, zy1)(zx1)\n        zbranch2 = ZLayer.Dense(8)(zx2)\n        zz = ZLayer.merge([zbranch1, zbranch2], mode=""concat"")\n        zmodel = ZModel([zx1, zx2], zz)\n\n        kx1 = KLayer.Input(shape=(4, ))\n        kx2 = KLayer.Input(shape=(5, ))\n        ky1 = KLayer.Dense(6, activation=""sigmoid"")(kx1)\n        kbranch1 = KModel(kx1, ky1)(kx1)\n        kbranch2 = KLayer.Dense(8)(kx2)\n        kz = KLayer.merge([kbranch1, kbranch2], mode=""concat"")\n        kmodel = KModel([kx1, kx2], kz)\n\n        input_data = [np.random.random([2, 4]), np.random.random([2, 5])]\n        self.compare_layer(kmodel, zmodel, input_data, self.convert_two_dense)\n\n    def test_merge_method_seq_concat(self):\n        zx1 = ZLayer.Input(shape=(10, ))\n        zx2 = ZLayer.Input(shape=(10, ))\n        zy1 = ZLayer.Dense(12, activation=""sigmoid"")(zx1)\n        zbranch1_node = ZModel(zx1, zy1)(zx1)\n        zbranch2 = ZSequential()\n        zbranch2.add(ZLayer.Dense(12, input_dim=10))\n        zbranch2_node = zbranch2(zx2)\n        zz = ZLayer.merge([zbranch1_node, zbranch2_node], mode=""concat"")\n        zmodel = ZModel([zx1, zx2], zz)\n\n        kx1 = KLayer.Input(shape=(10, ))\n        kx2 = KLayer.Input(shape=(10, ))\n        ky1 = KLayer.Dense(12, activation=""sigmoid"")(kx1)\n        kbranch1_node = KModel(kx1, ky1)(kx1)\n        kbranch2 = KSequential()\n        kbranch2.add(KLayer.Dense(12, input_dim=10))\n        kbranch2_node = kbranch2(kx2)\n        kz = KLayer.merge([kbranch1_node, kbranch2_node], mode=""concat"")\n        kmodel = KModel([kx1, kx2], kz)\n\n        input_data = [np.random.random([2, 10]), np.random.random([2, 10])]\n        self.compare_layer(kmodel, zmodel, input_data, self.convert_two_dense)\n\n    def test_reshape(self):\n        a = np.random.random((2, 2, 3, 4))\n        i1 = ZLayer.Input(shape=(2, 3, 4))\n        s = ZLayer.Reshape((-1, 2, 12))(i1)\n        m = ZModel(i1, s)\n        # predict should not generate exception\n        y = m.predict(a, distributed=False)\n\n    def test_regularizer(self):\n        model = ZSequential()\n        model.add(ZLayer.Dense(16, W_regularizer=regularizers.l2(0.001),\n                               activation=\'relu\', input_shape=(10000,)))\n        model.summary()\n        model.compile(optimizer=\'rmsprop\',\n                      loss=\'binary_crossentropy\',\n                      metrics=[\'acc\'])\n\n    def test_transformer_forward_backward(self):\n        layer = ZLayer.TransformerLayer.init(\n            vocab=200, hidden_size=128, n_head=4, seq_len=20)\n\n        train_token = np.random.randint(20, size=(2, 20))\n        train_pos = np.zeros((2, 20), dtype=np.int32)\n        input = [train_token, train_pos]\n        self.assert_forward_backward(layer, input)\n\n    def test_bert_forward_backward(self):\n        layer = ZLayer.BERT.init(\n            vocab=200, hidden_size=128, n_head=4, seq_len=20, intermediate_size=20)\n\n        train_token = np.random.randint(20, size=(2, 20))\n        token_type_id = np.zeros((2, 20), dtype=np.int32)\n        train_pos = np.zeros((2, 20), dtype=np.int32)\n        mask_attention = np.ones((2, 1, 1, 20), dtype=np.int32)\n        input = [train_token, token_type_id, train_pos, mask_attention]\n        self.assert_forward_backward(layer, input)\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/pipeline/api/keras/test_net.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\n\nimport keras.layers as KLayer\nfrom keras.models import Sequential as KSequential\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nimport zoo.pipeline.api.keras.layers as ZLayer\nfrom zoo.pipeline.api.keras.models import Model as ZModel\nfrom zoo.pipeline.api.keras.models import Sequential as ZSequential\nfrom zoo.pipeline.api.net import Net, TFNet\nfrom bigdl.nn.layer import Linear, Sigmoid, SoftMax, Model as BModel\nfrom bigdl.util.common import *\nfrom bigdl.nn.layer import Sequential\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestLayer(ZooTestCase):\n\n    def test_load_bigdl_model(self):\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../../resources"")\n        model_path = os.path.join(resource_path, ""models/bigdl/bigdl_lenet.model"")\n        model = Net.load_bigdl(model_path)\n        model2 = model.new_graph([""reshape2""])\n        model2.freeze_up_to([""pool3""])\n        model2.unfreeze()\n        import numpy as np\n        data = np.zeros([1, 1, 28, 28])\n        output = model2.forward(data)\n        assert output.shape == (1, 192)\n\n    def test_load_caffe_model(self):\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../../resources"")\n        model_path = os.path.join(resource_path, ""models/caffe/test_persist.caffemodel"")\n        def_path = os.path.join(resource_path, ""models/caffe/test_persist.prototxt"")\n        model = Net.load_caffe(def_path, model_path)\n        model2 = model.new_graph([""ip""])\n        model2.freeze_up_to([""conv2""])\n        model2.unfreeze()\n\n    def test_deprecated_save(self):\n        with pytest.raises(Exception) as e_info:\n            input = ZLayer.Input(shape=(5,))\n            output = ZLayer.Dense(10)(input)\n            zmodel = ZModel(input, output, name=""graph1"")\n            zmodel.save(create_tmp_path())\n\n    def test_save_load_Model(self):\n        input = ZLayer.Input(shape=(5,))\n        output = ZLayer.Dense(10)(input)\n        zmodel = ZModel(input, output, name=""graph1"")\n        tmp_path = create_tmp_path()\n        zmodel.saveModel(tmp_path, None, True)\n        model_reloaded = Net.load(tmp_path)\n        input_data = np.random.random([10, 5])\n        y = np.random.random([10, 10])\n        model_reloaded.compile(optimizer=""adam"",\n                               loss=""mse"")\n        model_reloaded.fit(x=input_data, y=y, batch_size=8, nb_epoch=2)\n\n    def test_save_load_Sequential(self):\n        zmodel = ZSequential()\n        dense = ZLayer.Dense(10, input_dim=5)\n        zmodel.add(dense)\n        tmp_path = create_tmp_path()\n        zmodel.saveModel(tmp_path, None, True)\n        model_reloaded = Net.load(tmp_path)\n        input_data = np.random.random([10, 5])\n        y = np.random.random([10, 10])\n        model_reloaded.compile(optimizer=""adam"",\n                               loss=""mse"")\n        model_reloaded.fit(x=input_data, y=y, batch_size=8, nb_epoch=1)\n\n    def test_load(self):\n        input = ZLayer.Input(shape=(5,))\n        output = ZLayer.Dense(10)(input)\n        zmodel = ZModel(input, output, name=""graph1"")\n        tmp_path = create_tmp_path()\n        zmodel.saveModel(tmp_path, None, True)\n        model_reloaded = Net.load(tmp_path)\n        input_data = np.random.random([3, 5])\n        self.compare_output_and_grad_input(zmodel, model_reloaded, input_data)\n\n    def test_load_keras(self):\n        model = KSequential()\n        model.add(KLayer.Dense(32, activation=\'relu\', input_dim=100))\n\n        tmp_path_json = create_tmp_path() + "".json""\n        model_json = model.to_json()\n        with open(tmp_path_json, ""w"") as json_file:\n            json_file.write(model_json)\n        zmodel = Net.load_keras(json_path=tmp_path_json)\n        assert isinstance(zmodel, Sequential)\n\n        tmp_path_hdf5 = create_tmp_path() + "".h5""\n        model.save(tmp_path_hdf5)\n        zmodel2 = Net.load_keras(hdf5_path=tmp_path_hdf5)\n        assert isinstance(zmodel2, Sequential)\n\n    def test_layers_method(self):\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../../resources"")\n        model_path = os.path.join(resource_path, ""models/bigdl/bigdl_lenet.model"")\n        model = Net.load_bigdl(model_path)\n        assert len(model.layers) == 12\n\n    def test_flatten_layers_method(self):\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../../resources"")\n        model_path = os.path.join(resource_path, ""models/bigdl/bigdl_lenet.model"")\n        model = Net.load_bigdl(model_path)\n\n        assert len(Sequential().add(model).flattened_layers()) == 12\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/pipeline/api/keras/test_simple_integration.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nimport shutil\n\nfrom zoo.feature.common import ChainedPreprocessing\nfrom zoo.feature.image import *\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import *\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestSimpleIntegration(ZooTestCase):\n\n    def test_sequential(self):\n        model = Sequential()\n        model.add(InputLayer(input_shape=(8, )))\n        model.add(Dense(10))\n        model.add(Dense(12))\n        input_shape = model.get_input_shape()\n        output_shape = model.get_output_shape()\n        np.testing.assert_allclose((8,), input_shape[1:])\n        np.testing.assert_allclose((12,), output_shape[1:])\n\n    def test_graph(self):\n        x1 = Input(shape=(8, ))\n        x2 = Input(shape=(6, ))\n        y1 = Dense(10)(x1)\n        y2 = Dense(10)(x2)\n        model = Model([x1, x2], [y1, y2])\n        tmp_log_dir = create_tmp_path()\n        model.save_graph_topology(tmp_log_dir)\n        input_shapes = model.get_input_shape()\n        output_shapes = model.get_output_shape()\n        np.testing.assert_allclose((8, ), input_shapes[0][1:])\n        np.testing.assert_allclose((6, ), input_shapes[1][1:])\n        np.testing.assert_allclose((10, ), output_shapes[0][1:])\n        np.testing.assert_allclose((10, ), output_shapes[1][1:])\n        shutil.rmtree(tmp_log_dir)\n\n    def test_training_with_tensorboard_checkpoint_gradientclipping(self):\n        model = Sequential()\n        model.add(Dense(8, input_shape=(32, 32, )))\n        model.add(Flatten())\n        model.add(Dense(4, activation=""softmax""))\n        X_train = np.random.random([200, 32, 32])\n        y_train = np.random.randint(4, size=(200, ))\n        X_test = np.random.random([40, 32, 32])\n        y_test = np.random.randint(4, size=(40, ))\n        model.compile(optimizer=""adam"",\n                      loss=""sparse_categorical_crossentropy"",\n                      metrics=[\'accuracy\'])\n        tmp_log_dir = create_tmp_path()\n        tmp_checkpoint_path = create_tmp_path()\n        os.mkdir(tmp_checkpoint_path)\n        model.set_tensorboard(tmp_log_dir, ""training_test"")\n        model.set_checkpoint(tmp_checkpoint_path)\n        model.set_constant_gradient_clipping(0.01, 0.03)\n        model.fit(X_train, y_train, batch_size=112, nb_epoch=2, validation_data=(X_test, y_test))\n        model.clear_gradient_clipping()\n        model.fit(X_train, y_train, batch_size=112, nb_epoch=2, validation_data=(X_test, y_test))\n        model.set_gradient_clipping_by_l2_norm(0.2)\n        model.fit(X_train, y_train, batch_size=112, nb_epoch=2, validation_data=(X_test, y_test))\n        train_loss = model.get_train_summary(""Loss"")\n        val_loss = model.get_validation_summary(""Loss"")\n        np.array(train_loss)\n        np.array(val_loss)\n        eval = model.evaluate(X_test, y_test, batch_size=112)\n        result = model.predict(X_test).collect()\n        for res in result:\n            assert isinstance(res, np.ndarray)\n        result2 = model.predict(X_test, distributed=False)\n        result_classes = model.predict_classes(X_test)\n        shutil.rmtree(tmp_log_dir)\n        shutil.rmtree(tmp_checkpoint_path)\n\n    def test_multiple_outputs_predict(self):\n        input = Input(shape=(32, ))\n        dense1 = Dense(10)(input)\n        dense2 = Dense(12)(input)\n        model = Model(input, [dense1, dense2])\n        data = np.random.random([10, 32])\n        result = model.predict(data).collect()\n        for res in result:\n            assert isinstance(res, list) and len(res) == 2\n        result2 = model.predict(data, distributed=False)\n        for res in result2:\n            assert isinstance(res, list) and len(res) == 2\n\n    def test_training_without_validation(self):\n        model = Sequential()\n        model.add(Dense(4, activation=""relu"", input_shape=(10, )))\n        x = np.random.random([300, 10])\n        y = np.random.random([300, ])\n        model.compile(optimizer=""sgd"", loss=""mae"")\n        model.fit(x, y, batch_size=112, nb_epoch=2)\n        model.predict(x)\n\n    def test_training_imageset(self):\n        images = []\n        labels = []\n        for i in range(0, 32):\n            features = np.random.uniform(0, 1, (200, 200, 3))\n            label = np.array([2])\n            images.append(features)\n            labels.append(label)\n        image_set = DistributedImageSet(self.sc.parallelize(images),\n                                        self.sc.parallelize(labels))\n\n        transformer = ChainedPreprocessing(\n            [ImageBytesToMat(), ImageResize(256, 256), ImageCenterCrop(224, 224),\n             ImageChannelNormalize(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n             ImageMatToTensor(), ImageSetToSample(target_keys=[\'label\'])])\n        data = image_set.transform(transformer)\n\n        model = Sequential()\n        model.add(Convolution2D(1, 5, 5, input_shape=(3, 224, 224)))\n        model.add(Reshape((1*220*220, )))\n        model.add(Dense(20, activation=""softmax""))\n        model.compile(optimizer=""sgd"", loss=""sparse_categorical_crossentropy"", metrics=[""accuracy""])\n        model.fit(data, batch_size=8, nb_epoch=2, validation_data=data)\n        result = model.predict(data, batch_per_thread=8)\n        accuracy = model.evaluate(data, batch_size=8)\n\n    def test_remove_batch(self):\n        from zoo.pipeline.api.utils import remove_batch\n        assert remove_batch([2, 3, 4]) == [3, 4]\n        assert remove_batch([[2, 6, 7], [2, 3, 4]]) == [[6, 7], [3, 4]]\n\n    def test_sequential_to_model(self):\n        seq = Sequential()\n        seq.add(Dense(8, input_shape=(32, 32, )))\n        seq.add(Flatten())\n        seq.add(Dense(4, activation=""softmax""))\n        seq.to_model()\n\n    def test_keras_net_layers(self):\n        x1 = Input(shape=(8, ))\n        x2 = Input(shape=(6, ))\n        y1 = Dense(10)(x1)\n        y2 = Dense(10)(x2)\n        model = Model([x1, x2], [y1, y2])\n        assert len(model.layers) == 4\n\n    def test_keras_net_flatten_layers(self):\n        x1 = Input(shape=(8, ))\n        x2 = Input(shape=(6, ))\n        y1 = Dense(10)(x1)\n        y2 = Dense(10)(x2)\n        model = Model([x1, x2], [y1, y2])\n        assert len(model.flattened_layers()) == 4\n\n    def test_keras_get_layer(self):\n        x1 = Input(shape=(8,))\n        y1 = Dense(10, name=""Dense"")(x1)\n        model = Model([x1], [y1])\n        layer = model.get_layer(""Dense"")\n        assert layer.name() == ""Dense""\n\n    def test_create_image_config(self):\n        from zoo.models.image.common.image_config import ImageConfigure\n        from zoo.feature.image.imagePreprocessing import ImageResize\n        from zoo.feature.common import ChainedPreprocessing\n        ImageConfigure(\n            pre_processor=ImageResize(224, 224))\n        ImageConfigure(\n            pre_processor=ChainedPreprocessing([ImageResize(224, 224), ImageResize(224, 224)]))\n\n    def test_model_summary_sequential(self):\n        model = Sequential()\n        model.add(LSTM(input_shape=(16, 32), output_dim=8, return_sequences=True))\n        model.add(Dropout(0.2))\n        model.add(LSTM(32, return_sequences=True))\n        model.add(Dropout(0.2))\n        model.add(LSTM(15, return_sequences=False))\n        model.add(Dropout(0.2))\n        model.add(Dense(output_dim=1))\n        model.summary()\n\n    def test_model_summary_graph(self):\n        x = Input(shape=(8, ))\n        y = Dense(10)(x)\n        z = Dense(12)(y)\n        model = Model(x, z)\n        model.summary()\n\n    def test_word_embedding_without_word_index(self):\n        model = Sequential()\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../../resources"")\n        glove_path = os.path.join(resource_path, ""glove.6B/glove.6B.50d.txt"")\n        embedding = WordEmbedding(glove_path, input_length=10)\n        model.add(embedding)\n        input_data = np.random.randint(20, size=(32, 10))\n        self.assert_forward_backward(model, input_data)\n\n    def test_word_embedding_with_word_index(self):\n        model = Sequential()\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../../resources"")\n        glove_path = os.path.join(resource_path, ""glove.6B/glove.6B.50d.txt"")\n        word_index = {""with"": 1, ""is"": 2, ""him"": 3, ""have"": 4}\n        embedding = WordEmbedding(glove_path, word_index, input_length=30)\n        model.add(embedding)\n        input_data = np.random.randint(5, size=(4, 30))\n        self.assert_forward_backward(model, input_data)\n\n    def test_get_word_index(self):\n        resource_path = os.path.join(os.path.split(__file__)[0], ""../../../resources"")\n        glove_path = os.path.join(resource_path, ""glove.6B/glove.6B.50d.txt"")\n        word_index = WordEmbedding.get_word_index(glove_path)\n        assert word_index[""the""] == 1\n        assert word_index[""for""] == 11\n        assert word_index[""as""] == 20\n\n    def test_set_evaluate_status(self):\n        model = Sequential().add(Dropout(0.2, input_shape=(4, ))).set_evaluate_status()\n        input_data = np.random.random([3, 4])\n        output = model.forward(input_data)\n        assert np.allclose(input_data, output)\n\n    def test_training_with_loss_metrics(self):\n        model = Sequential()\n        model.add(Dense(8, input_shape=(32, 32, )))\n        model.add(Flatten())\n        model.add(Dense(4, activation=""softmax""))\n        X_train = np.random.random([200, 32, 32])\n        y_train = np.random.randint(4, size=(200, ))\n        X_test = np.random.random([40, 32, 32])\n        y_test = np.random.randint(4, size=(40, ))\n        model.compile(optimizer=""adam"",\n                      loss=""sparse_categorical_crossentropy"",\n                      metrics=[\'accuracy\', \'loss\'])\n        model.fit(X_train, y_train, validation_data=(X_test, y_test))\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/pipeline/api/keras2/test_simple_integration.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport pytest\nimport shutil\n\nfrom zoo.pipeline.api.keras2.layers import *\nfrom zoo.pipeline.api.keras.models import *\nfrom test.zoo.pipeline.utils.test_utils import ZooTestCase\nimport numpy as np\n\nnp.random.seed(1337)  # for reproducibility\n\n\nclass TestSimpleIntegration(ZooTestCase):\n\n    def test_sequential(self):\n        model = Sequential()\n        model.add(Dense(10, kernel_initializer=""glorot_uniform"", input_shape=(8, )))\n        model.add(Dense(12))\n        input_shape = model.get_input_shape()\n        output_shape = model.get_output_shape()\n\n        np.testing.assert_allclose((8,), input_shape[1:])\n        np.testing.assert_allclose((12,), output_shape[1:])\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/test/zoo/pipeline/api/torch/test_torch.py,3,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport torch\nfrom torch import nn\nimport torchvision\nimport pytest\n\nfrom unittest import TestCase\nfrom zoo.pipeline.api.torch import TorchModel, TorchLoss\nfrom zoo.common.nncontext import *\n\n\nclass TestPytorch(TestCase):\n\n    def setUp(self):\n        """""" setup any state tied to the execution of the given method in a\n        class.  setup_method is invoked for every test method of a class.\n        """"""\n        self.sc = init_spark_on_local(4)\n\n    def tearDown(self):\n        """""" teardown any state that was previously setup with a setup_method\n        call.\n        """"""\n        self.sc.stop()\n\n    def test_torchmodel_constructor(self):\n        class TwoInputModel(nn.Module):\n            def __init__(self):\n                super(TwoInputModel, self).__init__()\n                self.dense1 = nn.Linear(2, 2)\n                self.dense2 = nn.Linear(3, 1)\n\n            def forward(self, x1, x2):\n                x1 = self.dense1(x1)\n                x2 = self.dense2(x2)\n                return x1, x2\n\n        TorchModel.from_pytorch(TwoInputModel())\n\n    def test_torchloss_constructor(self):\n        criterion = nn.MSELoss()\n        TorchLoss.from_pytorch(criterion)\n\n    def test_torch_net_predict_resnet(self):\n        torch.random.manual_seed(1)\n        pytorch_model = torchvision.models.resnet18(pretrained=False).eval()\n        zoo_model = TorchModel.from_pytorch(pytorch_model)\n        zoo_model.evaluate()\n\n        dummy_input = torch.ones(1, 3, 224, 224)\n        pytorch_result = pytorch_model(dummy_input).data.numpy()\n        zoo_result = zoo_model.forward(dummy_input.numpy())\n        print(pytorch_result)\n        print(zoo_result)\n        assert np.allclose(pytorch_result, zoo_result, rtol=1.e-6, atol=1.e-6)\n\n    def test_model_to_pytorch(self):\n        class SimpleTorchModel(nn.Module):\n            def __init__(self):\n                super(SimpleTorchModel, self).__init__()\n                self.dense1 = nn.Linear(2, 4)\n                self.dense2 = nn.Linear(4, 1)\n\n            def forward(self, x):\n                x = self.dense1(x)\n                x = torch.sigmoid(self.dense2(x))\n                return x\n\n        torch_model = SimpleTorchModel()\n        az_model = TorchModel.from_pytorch(torch_model)\n\n        weights = az_model.get_weights()\n        weights[0][0] = 1.0\n        az_model.set_weights(weights)\n\n        exported_model = az_model.to_pytorch()\n        p = list(exported_model.parameters())\n        assert p[0][0][0] == 1.0\n\n\nif __name__ == ""__main__"":\n    pytest.main([__file__])\n'"
pyzoo/zoo/examples/orca/learn/mxnet/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/orca/learn/mxnet/lenet_mnist.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Reference: https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html\n\nimport argparse\n\nfrom zoo import init_spark_on_local, init_spark_on_yarn\nfrom zoo.ray import RayContext\nfrom zoo.orca.learn.mxnet import MXNetTrainer, create_trainer_config\n\n\ndef get_train_data_iter(config, kv):\n    from mxnet.test_utils import get_mnist_iterator\n    from filelock import FileLock\n    with FileLock(""data.lock""):\n        iters = get_mnist_iterator(config[""batch_size""], (1, 28, 28),\n                                   num_parts=kv.num_workers, part_index=kv.rank)\n        return iters[0]\n\n\ndef get_test_data_iter(config, kv):\n    from mxnet.test_utils import get_mnist_iterator\n    from filelock import FileLock\n    with FileLock(""data.lock""):\n        iters = get_mnist_iterator(config[""batch_size""], (1, 28, 28),\n                                   num_parts=kv.num_workers, part_index=kv.rank)\n        return iters[1]\n\n\ndef get_model(config):\n    import mxnet as mx\n    from mxnet import gluon\n    from mxnet.gluon import nn\n    import mxnet.ndarray as F\n\n    class LeNet(gluon.Block):\n        def __init__(self, **kwargs):\n            super(LeNet, self).__init__(**kwargs)\n            with self.name_scope():\n                # layers created in name_scope will inherit name space\n                # from parent layer.\n                self.conv1 = nn.Conv2D(20, kernel_size=(5, 5))\n                self.pool1 = nn.MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n                self.conv2 = nn.Conv2D(50, kernel_size=(5, 5))\n                self.pool2 = nn.MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n                self.fc1 = nn.Dense(500)\n                self.fc2 = nn.Dense(10)\n\n        def forward(self, x):\n            x = self.pool1(F.tanh(self.conv1(x)))\n            x = self.pool2(F.tanh(self.conv2(x)))\n            # 0 means copy over size from corresponding dimension.\n            # -1 means infer size from the rest of dimensions.\n            x = x.reshape((0, -1))\n            x = F.tanh(self.fc1(x))\n            x = F.tanh(self.fc2(x))\n            return x\n\n    net = LeNet()\n    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=[mx.cpu()])\n    return net\n\n\ndef get_loss(config):\n    from mxnet import gluon\n    return gluon.loss.SoftmaxCrossEntropyLoss()\n\n\ndef get_metrics(config):\n    import mxnet as mx\n    return mx.metric.Accuracy()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'Train a LeNet model for handwritten digit recognition.\')\n    parser.add_argument(\'--hadoop_conf\', type=str,\n                        help=\'The path to the hadoop configuration folder. Required if you \'\n                             \'wish to run on yarn clusters. Otherwise, run in local mode.\')\n    parser.add_argument(\'--conda_name\', type=str,\n                        help=\'The name of conda environment. Required if you \'\n                             \'wish to run on yarn clusters.\')\n    parser.add_argument(\'--executor_cores\', type=int, default=4,\n                        help=\'The number of executor cores you want to use.\')\n    parser.add_argument(\'-n\', \'--num_workers\', type=int, default=2,\n                        help=\'The number of MXNet workers to be launched.\')\n    parser.add_argument(\'-s\', \'--num_servers\', type=int,\n                        help=\'The number of MXNet servers to be launched. If not specified, \'\n                        \'default to be equal to the number of workers.\')\n    parser.add_argument(\'-b\', \'--batch_size\', type=int, default=100,\n                        help=\'The number of samples per gradient update for each worker.\')\n    parser.add_argument(\'-e\', \'--epochs\', type=int, default=10,\n                        help=\'The number of epochs to train the model.\')\n    parser.add_argument(\'-l\', \'--learning_rate\', type=float, default=0.02,\n                        help=\'Learning rate for the LeNet model.\')\n    parser.add_argument(\'--log_interval\', type=int, default=100,\n                        help=\'The number of batches to wait before logging throughput and \'\n                             \'metrics information during the training process.\')\n    opt = parser.parse_args()\n\n    if opt.hadoop_conf:\n        assert opt.conda_name is not None, ""conda_name must be specified for yarn mode""\n        sc = init_spark_on_yarn(\n            hadoop_conf=opt.hadoop_conf,\n            conda_name=opt.conda_name,\n            num_executor=opt.num_workers,\n            executor_cores=opt.executor_cores)\n    else:\n        sc = init_spark_on_local(cores=""*"")\n    ray_ctx = RayContext(sc=sc)\n    ray_ctx.init()\n\n    config = create_trainer_config(opt.batch_size, optimizer=""sgd"",\n                                   optimizer_params={\'learning_rate\': opt.learning_rate},\n                                   log_interval=opt.log_interval, seed=42)\n    trainer = MXNetTrainer(config, train_data=get_train_data_iter, model_creator=get_model,\n                           loss_creator=get_loss, validation_metrics_creator=get_metrics,\n                           num_workers=opt.num_workers, num_servers=opt.num_servers,\n                           test_data=get_test_data_iter, eval_metrics_creator=get_metrics)\n    trainer.train(nb_epoch=opt.epochs)\n    ray_ctx.stop()\n    sc.stop()\n'"
pyzoo/zoo/examples/pytorch/train/mnist/main.py,5,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom zoo.pipeline.api.torch import TorchModel, TorchLoss\nfrom zoo.pipeline.estimator import *\nfrom bigdl.optim.optimizer import SGD, Adam\nfrom zoo.common.nncontext import *\nfrom zoo.feature.common import FeatureSet\nfrom zoo.pipeline.api.keras.metrics import Accuracy\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description=\'PyTorch MNIST Example\')\n    parser.add_argument(\'--dir\', default=\'/tmp/data\', metavar=\'N\',\n                        help=\'the folder store mnist data\')\n    parser.add_argument(\'--batch-size\', type=int, default=256, metavar=\'N\',\n                        help=\'input batch size for training per executor(default: 256)\')\n    parser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                        help=\'input batch size for testing per executor(default: 1000)\')\n    parser.add_argument(\'--epochs\', type=int, default=2, metavar=\'N\',\n                        help=\'number of epochs to train (default: 2)\')\n    parser.add_argument(\'--lr\', type=float, default=0.001, metavar=\'LR\',\n                        help=\'learning rate (default: 0.001)\')\n    parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                        help=\'random seed (default: 1)\')\n    parser.add_argument(\'--save-model\', action=\'store_true\', default=False,\n                        help=\'For Saving the current Model\')\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(args.dir, train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args.batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(args.dir, train=False,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args.test_batch_size, shuffle=False)\n\n    # init on yarn when HADOOP_CONF_DIR and ZOO_CONDA_NAME is provided.\n    if os.environ.get(\'HADOOP_CONF_DIR\') is None:\n        sc = init_spark_on_local(cores=1, conf={""spark.driver.memory"": ""20g""})\n    else:\n        num_executors = 2\n        num_cores_per_executor = 4\n        hadoop_conf_dir = os.environ.get(\'HADOOP_CONF_DIR\')\n        zoo_conda_name = os.environ.get(\'ZOO_CONDA_NAME\')  # The name of the created conda-env\n        sc = init_spark_on_yarn(\n            hadoop_conf=hadoop_conf_dir,\n            conda_name=zoo_conda_name,\n            num_executor=num_executors,\n            executor_cores=num_cores_per_executor,\n            executor_memory=""2g"",\n            driver_memory=""10g"",\n            driver_cores=1,\n            spark_conf={""spark.rpc.message.maxSize"": ""1024"",\n                        ""spark.task.maxFailures"":  ""1"",\n                        ""spark.driver.extraJavaOptions"": ""-Dbigdl.failure.retryTimes=1""})\n\n    model = Net()\n    model.train()\n    criterion = nn.NLLLoss()\n\n    adam = Adam(args.lr)\n    zoo_model = TorchModel.from_pytorch(model)\n    zoo_criterion = TorchLoss.from_pytorch(criterion)\n    zoo_estimator = Estimator(zoo_model, optim_methods=adam)\n    train_featureset = FeatureSet.pytorch_dataloader(train_loader)\n    test_featureset = FeatureSet.pytorch_dataloader(test_loader)\n    from bigdl.optim.optimizer import MaxEpoch, EveryEpoch\n    zoo_estimator.train_minibatch(train_featureset, zoo_criterion,\n                                  end_trigger=MaxEpoch(args.epochs),\n                                  checkpoint_trigger=EveryEpoch(),\n                                  validation_set=test_featureset,\n                                  validation_method=[Accuracy()])\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pyzoo/zoo/examples/pytorch/train/resnet_finetune/resnet_finetune.py,3,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType, DoubleType\nfrom pyspark.sql.functions import col, udf\nfrom bigdl.optim.optimizer import *\nfrom zoo.common.nncontext import *\nfrom zoo.feature.image import *\nfrom zoo.pipeline.api.net.torch_net import TorchNet\nfrom zoo.pipeline.api.net.torch_criterion import TorchCriterion\nfrom zoo.pipeline.nnframes import *\nfrom zoo.pipeline.api.keras.metrics import Accuracy\n\n\n# Define model with Pytorch\nclass CatDogModel(nn.Module):\n    def __init__(self):\n        super(CatDogModel, self).__init__()\n        self.features = torchvision.models.resnet18(pretrained=True).eval()\n        self.dense1 = nn.Linear(1000, 2)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = F.log_softmax(self.dense1(x), dim=1)\n        return x\n\n\nif __name__ == \'__main__\':\n\n    if len(sys.argv) != 2:\n        print(sys.argv)\n        print(""Need parameters: <imagePath>"")\n        exit(-1)\n\n    hadoop_conf_dir = os.environ.get(\'HADOOP_CONF_DIR\')\n\n    if hadoop_conf_dir:\n        num_executors = 2\n        num_cores_per_executor = 4\n        sc = init_spark_on_yarn(\n            hadoop_conf=hadoop_conf_dir,\n            conda_name=os.environ[""ZOO_CONDA_NAME""],  # The name of the created conda-env\n            num_executor=num_executors,\n            executor_cores=num_cores_per_executor,\n            executor_memory=""8g"",\n            driver_memory=""2g"",\n            driver_cores=1)\n    else:\n        num_executors = 1\n        num_cores_per_executor = 4\n        sc = init_spark_on_local(cores=4, conf={""spark.driver.memory"": ""10g""})\n\n    torchnet = TorchNet.from_pytorch(CatDogModel(), [4, 3, 224, 224])\n\n    def lossFunc(input, target):\n        return nn.NLLLoss().forward(input, target.flatten().long())\n\n    torchcriterion = TorchCriterion.from_pytorch(lossFunc, [1, 2], torch.LongTensor([1]))\n\n    # prepare training data as Spark DataFrame\n    image_path = sys.argv[1]\n    imageDF = NNImageReader.readImages(image_path, sc, resizeH=256, resizeW=256, image_codec=1)\n    getName = udf(lambda row: os.path.basename(row[0]), StringType())\n    getLabel = udf(lambda name: 1.0 if name.startswith(\'cat\') else 0.0, DoubleType())\n    labelDF = imageDF.withColumn(""name"", getName(col(""image""))) \\\n        .withColumn(""label"", getLabel(col(\'name\'))).cache()\n    (trainingDF, validationDF) = labelDF.randomSplit([0.9, 0.1])\n\n    # run training and evaluation\n    featureTransformer = ChainedPreprocessing(\n        [RowToImageFeature(), ImageCenterCrop(224, 224),\n         ImageChannelNormalize(123.0, 117.0, 104.0, 255.0, 255.0, 255.0),\n         ImageMatToTensor(), ImageFeatureToTensor()])\n\n    classifier = NNClassifier(torchnet, torchcriterion, featureTransformer) \\\n        .setLearningRate(0.001) \\\n        .setBatchSize(16) \\\n        .setMaxEpoch(1) \\\n        .setFeaturesCol(""image"") \\\n        .setCachingSample(False) \\\n        .setValidation(EveryEpoch(), validationDF, [Accuracy()], 16)\n\n    catdogModel = classifier.fit(trainingDF)\n\n    shift = udf(lambda p: p - 1, DoubleType())\n    predictionDF = catdogModel.transform(validationDF) \\\n        .withColumn(""prediction"", shift(col(\'prediction\'))).cache()\n    predictionDF.sample(False, 0.1).show()\n\n    correct = predictionDF.filter(""label=prediction"").count()\n    overall = predictionDF.count()\n    accuracy = correct * 1.0 / overall\n\n    # expecting: accuracy > 96%\n    print(""Validation accuracy = %g "" % accuracy)\n'"
pyzoo/zoo/examples/tensorflow/tfpark/estimator/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/tensorflow/tfpark/estimator/estimator_dataset.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\nimport numpy as np\nfrom zoo import init_nncontext\nfrom zoo.tfpark import TFDataset, TFEstimator, ZooOptimizer\n\n\ndef get_data(dataset):\n    from bigdl.dataset import mnist\n    (images_data, labels_data) = mnist.read_data_sets(""/tmp/mnist"", dataset)\n    images_data = (images_data - mnist.TRAIN_MEAN) / mnist.TRAIN_STD\n    return (images_data, labels_data.astype(np.int32))\n\n\ndef main():\n    sc = init_nncontext()\n\n    def model_fn(features, labels, mode):\n        from nets import lenet\n        slim = tf.contrib.slim\n        with slim.arg_scope(lenet.lenet_arg_scope()):\n            logits, end_points = lenet.lenet(features, num_classes=10, is_training=True)\n\n        if mode == tf.estimator.ModeKeys.EVAL or mode == tf.estimator.ModeKeys.TRAIN:\n            loss = tf.reduce_mean(\n                tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels))\n\n            optimizer = ZooOptimizer(tf.train.AdamOptimizer())\n            train_op = optimizer.minimize(loss)\n            return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n        else:\n            return tf.estimator.EstimatorSpec(mode, predictions=logits)\n\n    def input_fn(mode):\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            training_data = get_data(""train"")\n            dataset = TFDataset.from_ndarrays(training_data, batch_size=320)\n        elif mode == tf.estimator.ModeKeys.EVAL:\n            testing_data = get_data(""test"")\n            dataset = TFDataset.from_ndarrays(testing_data, batch_per_thread=80)\n        else:\n            images, _ = get_data(""test"")\n            dataset = TFDataset.from_ndarrays(images, batch_per_thread=80)\n\n        return dataset\n    estimator = TFEstimator.from_model_fn(model_fn, model_dir=""/tmp/estimator"")\n\n    estimator.train(input_fn, steps=60000//320)\n\n    metrics = estimator.evaluate(input_fn, [""acc""])\n    print(metrics)\n\n    predictions = estimator.predict(input_fn)\n\n    print(predictions.first())\n    print(""finished..."")\n    sc.stop()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pyzoo/zoo/examples/tensorflow/tfpark/estimator/estimator_inception.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom optparse import OptionParser\n\nimport tensorflow as tf\n\nfrom zoo import init_nncontext\nfrom zoo.feature.common import *\nfrom zoo.feature.image.imagePreprocessing import *\nfrom zoo.feature.image.imageset import *\nfrom zoo.tfpark import TFDataset, TFEstimator, ZooOptimizer\n\n\ndef main(option):\n    sc = init_nncontext()\n\n    def input_fn(mode, params):\n\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            image_set = ImageSet.read(params[""image_path""],\n                                      sc=sc, with_label=True, one_based_label=False)\n            train_transformer = ChainedPreprocessing([ImageBytesToMat(),\n                                                      ImageResize(256, 256),\n                                                      ImageRandomCrop(224, 224),\n                                                      ImageRandomPreprocessing(ImageHFlip(), 0.5),\n                                                      ImageChannelNormalize(\n                                                          0.485, 0.456, 0.406,\n                                                          0.229, 0.224, 0.225),\n                                                      ImageMatToTensor(to_RGB=True, format=""NHWC""),\n                                                      ImageSetToSample(input_keys=[""imageTensor""],\n                                                                       target_keys=[""label""])\n                                                      ])\n            feature_set = FeatureSet.image_frame(image_set.to_image_frame())\n            feature_set = feature_set.transform(train_transformer)\n            feature_set = feature_set.transform(ImageFeatureToSample())\n            dataset = TFDataset.from_feature_set(feature_set,\n                                                 features=(tf.float32, [224, 224, 3]),\n                                                 labels=(tf.int32, [1]), batch_size=16)\n        else:\n            raise NotImplementedError\n\n        return dataset\n\n    def model_fn(features, labels, mode, params):\n        from nets import inception\n        slim = tf.contrib.slim\n        labels = tf.squeeze(labels, axis=1)\n        with slim.arg_scope(inception.inception_v1_arg_scope()):\n            logits, end_points = inception.inception_v1(features,\n                                                        num_classes=int(params[""num_classes""]),\n                                                        is_training=True)\n\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            loss = tf.reduce_mean(\n                tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels))\n            train_op = ZooOptimizer(tf.train.AdamOptimizer()).minimize(loss)\n            return tf.estimator.EstimatorSpec(mode, train_op=train_op,\n                                              predictions=logits, loss=loss)\n        else:\n            raise NotImplementedError\n\n    estimator = TFEstimator.from_model_fn(model_fn,\n                                          params={""image_path"": option.image_path,\n                                                  ""num_classes"": option.num_classes})\n\n    estimator.train(input_fn, steps=100)\n    print(""finished..."")\n    sc.stop()\n\nif __name__ == \'__main__\':\n    parser = OptionParser()\n    parser.add_option(""--image-path"", dest=""image_path"")\n    parser.add_option(""--num-classes"", dest=""num_classes"")\n\n    (options, args) = parser.parse_args(sys.argv)\n    main(options)\n'"
pyzoo/zoo/examples/tensorflow/tfpark/estimator/pre-made-estimator.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport sys\nimport pandas as pd\nimport tensorflow as tf\nfrom optparse import OptionParser\nfrom tensorflow_estimator.python.estimator.canned import prediction_keys\nfrom zoo import init_nncontext\nfrom zoo.tfpark import TFDataset, ZooOptimizer, TFEstimator\n\n\ndef make_input_fn(data_df, label_df, mode, batch_size=-1, batch_per_thread=-1):\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        def input_function():\n            ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n            ds = ds.shuffle(1000)\n            ds = TFDataset.from_tf_data_dataset(dataset=ds, batch_size=batch_size)\n            return ds\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        def input_function():\n            ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n            ds = TFDataset.from_tf_data_dataset(dataset=ds, batch_per_thread=batch_per_thread)\n            return ds\n    else:\n        def input_function():\n            ds = tf.data.Dataset.from_tensor_slices((dict(data_df),))\n            ds = TFDataset.from_tf_data_dataset(dataset=ds, batch_size=batch_size,\n                                                batch_per_thread=batch_per_thread)\n            return ds\n    return input_function\n\n\nif __name__ == ""__main__"":\n    parser = OptionParser()\n    parser.add_option(""--data_dir"", dest=""data_dir"")\n    (options, args) = parser.parse_args(sys.argv)\n\n    dftrain = pd.read_csv(os.path.join(options.data_dir, \'train.csv\'))\n    dfeval = pd.read_csv(os.path.join(options.data_dir, \'eval.csv\'))\n    y_train = dftrain.pop(\'survived\')\n    y_eval = dfeval.pop(\'survived\')\n\n    CATEGORICAL_COLUMNS = [\'sex\', \'n_siblings_spouses\', \'parch\', \'class\', \'deck\',\n                           \'embark_town\', \'alone\']\n    NUMERIC_COLUMNS = [\'age\', \'fare\']\n\n    feature_columns = []\n    for feature_name in CATEGORICAL_COLUMNS:\n        vocabulary = dftrain[feature_name].unique()\n        feature_columns.append(tf.feature_column.\n                               categorical_column_with_vocabulary_list(feature_name, vocabulary))\n\n    for feature_name in NUMERIC_COLUMNS:\n        feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n\n    sc = init_nncontext()\n\n    linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns,\n                                               optimizer=ZooOptimizer(tf.train.FtrlOptimizer(0.2)),\n                                               model_dir=""/tmp/estimator/linear"")\n    zoo_est = TFEstimator(linear_est)\n    train_input_fn = make_input_fn(dftrain, y_train,\n                                   mode=tf.estimator.ModeKeys.TRAIN,\n                                   batch_size=32)\n    zoo_est.train(train_input_fn, steps=200)\n\n    eval_input_fn = make_input_fn(dfeval, y_eval,\n                                  mode=tf.estimator.ModeKeys.EVAL,\n                                  batch_per_thread=8)\n    eval_result = zoo_est.evaluate(eval_input_fn, [""acc""])\n    print(eval_result)\n\n    pred_input_fn = make_input_fn(dfeval, y_eval,\n                                  mode=tf.estimator.ModeKeys.PREDICT,\n                                  batch_per_thread=8)\n    predictions = zoo_est.predict(pred_input_fn,\n                                  predict_keys=[prediction_keys.PredictionKeys.CLASS_IDS])\n    print(predictions.collect())\n    print(""finished..."")\n    sc.stop()\n'"
pyzoo/zoo/examples/tensorflow/tfpark/gan/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/tensorflow/tfpark/gan/gan_train_and_evaluate.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom bigdl.optim.optimizer import MaxIteration\nfrom zoo.tfpark.gan.gan_estimator import GANEstimator\n\nfrom zoo import init_nncontext\nfrom zoo.tfpark import TFDataset, ZooOptimizer\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow_gan.examples.mnist.networks import *\nfrom tensorflow_gan.python.losses.losses_impl import *\nimport tensorflow_datasets as tfds\n\nMODEL_DIR = ""/tmp/gan_model""\nNOISE_DIM = 64\n\n\ndef eval():\n\n    with tf.Graph().as_default() as g:\n        noise = tf.random.normal(mean=0.0, stddev=1.0, shape=(50, NOISE_DIM))\n        step = tf.train.get_or_create_global_step()\n        with tf.variable_scope(""Generator""):\n            one_hot = tf.one_hot(tf.concat([tf.range(0, 10)] * 5, axis=0), 10)\n            fake_img = conditional_generator((noise, one_hot), is_training=False)\n            fake_img = (fake_img * 128.0) + 128.0\n            fake_img = tf.cast(fake_img, tf.uint8)\n            tiled = tfgan.eval.image_grid(fake_img, grid_shape=(5, 10),\n                                          image_shape=(28, 28),\n                                          num_channels=1)\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            saver = tf.train.Saver()\n            ckpt = tf.train.latest_checkpoint(MODEL_DIR)\n            saver.restore(sess, ckpt)\n            outputs, step_value = sess.run([tiled, step])\n            plt.imsave(""./image_{}.png"".format(step_value), np.squeeze(outputs), cmap=""gray"")\n\n\nif __name__ == ""__main__"":\n    sc = init_nncontext()\n\n    def input_fn():\n        def map_func(data):\n            image = data[\'image\']\n            label = data[\'label\']\n            one_hot_label = tf.one_hot(label, depth=10)\n            noise = tf.random.normal(mean=0.0, stddev=1.0, shape=(NOISE_DIM,))\n            generator_inputs = (noise, one_hot_label)\n            discriminator_inputs = ((tf.to_float(image) / 255.0) - 0.5) * 2\n            return (generator_inputs, discriminator_inputs)\n\n        ds = tfds.load(""mnist"", split=""train"")\n        ds = ds.map(map_func)\n        dataset = TFDataset.from_tf_data_dataset(ds, batch_size=36)\n        return dataset\n\n    opt = GANEstimator(\n        generator_fn=conditional_generator,\n        discriminator_fn=conditional_discriminator,\n        generator_loss_fn=wasserstein_generator_loss,\n        discriminator_loss_fn=wasserstein_discriminator_loss,\n        generator_optimizer=ZooOptimizer(tf.train.AdamOptimizer(1e-5, 0.5)),\n        discriminator_optimizer=ZooOptimizer(tf.train.AdamOptimizer(1e-4, 0.5)),\n        model_dir=MODEL_DIR,\n        session_config=tf.ConfigProto()\n    )\n\n    for i in range(20):\n        opt.train(input_fn, MaxIteration(1000))\n        eval()\n\n    print(""finished..."")\n    sc.stop()\n'"
pyzoo/zoo/examples/tensorflow/tfpark/inception/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/tensorflow/tfpark/inception/inception.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom math import ceil\nfrom optparse import OptionParser\nfrom bigdl.optim.optimizer import *\nfrom zoo.common.nncontext import *\nfrom zoo.feature.image import *\nfrom zoo.pipeline.api.keras.metrics import *\nfrom zoo.pipeline.nnframes import *\nfrom zoo.tfpark import TFDataset, TFOptimizer\nfrom nets import inception_v1\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef get_inception_data(url, sc=None, data_type=""train""):\n    path = os.path.join(url, data_type)\n    return SeqFileFolder.files_to_image_frame(url=path, sc=sc, class_num=1000)\n\n\ndef config_option_parser():\n    parser = OptionParser()\n    parser.add_option(""-f"", ""--folder"", type=str, dest=""folder"", default="""",\n                      help=""url of hdf+s folder store the hadoop sequence files"")\n    parser.add_option(""--model"", type=str, dest=""model"", default="""",\n                      help=""model snapshot location"")\n    parser.add_option(""--checkpoint"", type=str, dest=""checkpoint"", default="""",\n                      help=""where to cache the model"")\n    parser.add_option(""-e"", ""--maxEpoch"", type=int, dest=""maxEpoch"", default=0,\n                      help=""epoch numbers"")\n    parser.add_option(""-i"", ""--maxIteration"", type=int, dest=""maxIteration"", default=3100,\n                      help=""iteration numbers"")\n    parser.add_option(""-l"", ""--learningRate"", type=float, dest=""learningRate"", default=0.01,\n                      help=""learning rate"")\n    parser.add_option(""--warmupEpoch"", type=int, dest=""warmupEpoch"", default=0,\n                      help=""warm up epoch numbers"")\n    parser.add_option(""--maxLr"", type=float, dest=""maxLr"", default=0.0,\n                      help=""max Lr after warm up"")\n    parser.add_option(""-b"", ""--batchSize"", type=int, dest=""batchSize"", help=""batch size"")\n    parser.add_option(""--weightDecay"", type=float, dest=""weightDecay"", default=0.0001,\n                      help=""weight decay"")\n    parser.add_option(""--checkpointIteration"", type=int, dest=""checkpointIteration"", default=620,\n                      help=""checkpoint interval of iterations"")\n    parser.add_option(""--resumeTrainingCheckpoint"", type=str, dest=""resumeTrainingCheckpoint"",\n                      default=None,\n                      help=""an analytics zoo checkpoint path for resume training, usually contains""\n                           + ""a file named model.$iter_num and a file named""\n                           + "" optimMethod-TFParkTraining.$iter_num"")\n    parser.add_option(""--resumeTrainingVersion"", type=int, dest=""resumeTrainingVersion"",\n                      default=None,\n                      help=""the version of checkpoint file, should be the $iter_num""\n                           + "" in model.$iter_num"")\n    return parser\n\nif __name__ == ""__main__"":\n    # parse options\n    parser = config_option_parser()\n    (options, args) = parser.parse_args(sys.argv)\n\n    if not options.learningRate:\n        parser.error(""-l --learningRate is a mandatory opt"")\n    if not options.batchSize:\n        parser.error(""-b --batchSize is a mandatory opt"")\n\n    sc = init_nncontext(""inception v1"")\n\n    image_size = 224  # create dataset\n    train_transformer = ChainedPreprocessing([ImagePixelBytesToMat(),\n                                              ImageResize(256, 256),\n                                              ImageRandomCrop(image_size, image_size),\n                                              ImageRandomPreprocessing(ImageHFlip(), 0.5),\n                                              ImageChannelNormalize(123.0, 117.0, 104.0),\n                                              ImageMatToTensor(format=""NHWC"", to_RGB=False),\n                                              ImageSetToSample(input_keys=[""imageTensor""],\n                                                               target_keys=[""label""])])\n    raw_train_data = get_inception_data(options.folder, sc, ""train"")\n    train_data = FeatureSet.image_frame(raw_train_data).transform(train_transformer)\n\n    val_transformer = ChainedPreprocessing([ImagePixelBytesToMat(),\n                                            ImageResize(256, 256),\n                                            ImageCenterCrop(image_size, image_size),\n                                            ImageChannelNormalize(123.0, 117.0, 104.0),\n                                            ImageMatToTensor(format=""NHWC"", to_RGB=False),\n                                            ImageSetToSample(input_keys=[""imageTensor""],\n                                                             target_keys=[""label""])])\n    raw_val_data = get_inception_data(options.folder, sc, ""val"")\n    val_data = FeatureSet.image_frame(raw_val_data).transform(val_transformer)\n    val_data = val_data.transform(ImageFeatureToSample())\n\n    train_data = train_data.transform(ImageFeatureToSample())\n\n    dataset = TFDataset.from_feature_set(train_data,\n                                         features=(tf.float32, [224, 224, 3]),\n                                         labels=(tf.int32, [1]),\n                                         batch_size=options.batchSize,\n                                         validation_dataset=val_data)\n\n    images, labels = dataset.tensors\n\n    # As sequence file\'s label is one-based, so labels need to subtract 1.\n    zero_based_label = labels - 1\n\n    is_training = tf.placeholder(dtype=tf.bool, shape=())\n\n    with slim.arg_scope(inception_v1.inception_v1_arg_scope(weight_decay=0.0,\n                                                            use_batch_norm=False)):\n        logits, end_points = inception_v1.inception_v1(images,\n                                                       dropout_keep_prob=0.6,\n                                                       num_classes=1000,\n                                                       is_training=is_training)\n\n    loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits,\n                                                                 labels=zero_based_label))\n\n    iterationPerEpoch = int(ceil(float(1281167) / options.batchSize))\n    if options.maxEpoch:\n        maxIteration = iterationPerEpoch * options.maxEpoch\n    else:\n        maxIteration = options.maxIteration\n    warmup_iteration = options.warmupEpoch * iterationPerEpoch\n\n    if warmup_iteration == 0:\n        warmupDelta = 0.0\n    else:\n        if options.maxLr:\n            maxlr = options.maxLr\n        else:\n            maxlr = options.learningRate\n        warmupDelta = (maxlr - options.learningRate) / warmup_iteration\n    polyIteration = maxIteration - warmup_iteration\n    lrSchedule = SequentialSchedule(iterationPerEpoch)\n    lrSchedule.add(Warmup(warmupDelta), warmup_iteration)\n    lrSchedule.add(Poly(0.5, maxIteration), polyIteration)\n    optim = SGD(learningrate=options.learningRate, learningrate_decay=0.0,\n                weightdecay=options.weightDecay, momentum=0.9, dampening=0.0,\n                nesterov=False,\n                leaningrate_schedule=lrSchedule)\n\n    if options.maxEpoch:\n        checkpoint_trigger = EveryEpoch()\n        end_trigger = MaxEpoch(options.maxEpoch)\n    else:\n        checkpoint_trigger = SeveralIteration(options.checkpointIteration)\n        end_trigger = MaxIteration(options.maxIteration)\n\n    optimizer = TFOptimizer.from_loss(loss, optim,\n                                      val_outputs=[logits],\n                                      val_labels=[zero_based_label],\n                                      val_method=[Accuracy(), Top5Accuracy(), Loss()],\n                                      tensor_with_value={is_training: [True, False]},\n                                      model_dir=""/tmp/logs"")\n\n    if options.resumeTrainingCheckpoint is not None:\n        assert options.resumeTrainingVersion is not None,\\\n            ""--resumeTrainingVersion must be specified when --resumeTrainingCheckpoint is.""\n        optimizer.load_checkpoint(options.resumeTrainingCheckpoint,\n                                  options.resumeTrainingVersion)\n\n    optimizer.optimize(end_trigger=end_trigger, checkpoint_trigger=checkpoint_trigger)\n\n    if options.checkpoint:\n        saver = tf.train.Saver()\n        saver.save(optimizer.sess, options.checkpoint)\n\n    sc.stop()\n'"
pyzoo/zoo/examples/tensorflow/tfpark/keras/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/tensorflow/tfpark/keras/keras_dataset.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nimport tensorflow as tf\nimport numpy as np\nfrom zoo import init_nncontext\nfrom zoo.tfpark import KerasModel, TFDataset\n\n\ndef get_data_rdd(dataset, sc):\n    from bigdl.dataset import mnist\n    (images_data, labels_data) = mnist.read_data_sets(""/tmp/mnist"", dataset)\n    image_rdd = sc.parallelize(images_data)\n    labels_rdd = sc.parallelize(labels_data)\n    rdd = image_rdd.zip(labels_rdd) \\\n        .map(lambda rec_tuple: ((rec_tuple[0] - mnist.TRAIN_MEAN) / mnist.TRAIN_STD,\n                                np.array(rec_tuple[1])))\n    return rdd\n\n\ndef main(max_epoch):\n    sc = init_nncontext()\n\n    training_rdd = get_data_rdd(""train"", sc)\n    testing_rdd = get_data_rdd(""test"", sc)\n\n    dataset = TFDataset.from_rdd(training_rdd,\n                                 features=(tf.float32, [28, 28, 1]),\n                                 labels=(tf.int32, []),\n                                 batch_size=320,\n                                 val_rdd=testing_rdd)\n\n    model = tf.keras.Sequential(\n        [tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n         tf.keras.layers.Dense(64, activation=\'relu\'),\n         tf.keras.layers.Dense(64, activation=\'relu\'),\n         tf.keras.layers.Dense(10, activation=\'softmax\'),\n         ]\n    )\n\n    model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n                  loss=\'sparse_categorical_crossentropy\',\n                  metrics=[\'accuracy\'])\n\n    keras_model = KerasModel(model)\n\n    keras_model.fit(dataset,\n                    epochs=max_epoch,\n                    distributed=True)\n\n    eval_dataset = TFDataset.from_rdd(\n        testing_rdd,\n        features=(tf.float32, [28, 28, 1]),\n        labels=(tf.int32, []), batch_per_thread=80)\n    result = keras_model.evaluate(eval_dataset)\n\n    print(model.metrics_names)\n    print(result)\n    # >> [\'loss\', \'acc\']\n    # >> [0.08865142822265625, 0.9722]\n\n    model.save_weights(""/tmp/mnist_keras.h5"")\n\n\nif __name__ == \'__main__\':\n\n    max_epoch = 5\n\n    if len(sys.argv) > 1:\n        max_epoch = int(sys.argv[1])\n    main(max_epoch)\n'"
pyzoo/zoo/examples/tensorflow/tfpark/keras/keras_ndarray.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nimport tensorflow as tf\nfrom zoo import init_nncontext\nfrom bigdl.dataset import mnist\nfrom zoo.tfpark import KerasModel\n\n\ndef main(max_epoch):\n    _ = init_nncontext()\n\n    (training_images_data, training_labels_data) = mnist.read_data_sets(""/tmp/mnist"", ""train"")\n    (testing_images_data, testing_labels_data) = mnist.read_data_sets(""/tmp/mnist"", ""test"")\n\n    training_images_data = (training_images_data - mnist.TRAIN_MEAN) / mnist.TRAIN_STD\n    testing_images_data = (testing_images_data - mnist.TRAIN_MEAN) / mnist.TRAIN_STD\n\n    model = tf.keras.Sequential(\n        [tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n         tf.keras.layers.Dense(64, activation=\'relu\'),\n         tf.keras.layers.Dense(64, activation=\'relu\'),\n         tf.keras.layers.Dense(10, activation=\'softmax\'),\n         ]\n    )\n\n    model.compile(optimizer=\'rmsprop\',\n                  loss=\'sparse_categorical_crossentropy\',\n                  metrics=[\'accuracy\'])\n\n    keras_model = KerasModel(model)\n\n    keras_model.fit(training_images_data,\n                    training_labels_data,\n                    validation_data=(testing_images_data, testing_labels_data),\n                    epochs=max_epoch,\n                    batch_size=320,\n                    distributed=True)\n\n    result = keras_model.evaluate(testing_images_data, testing_labels_data,\n                                  distributed=False, batch_per_thread=80)\n\n    print(keras_model.metrics_names)\n    print(result)\n    # >> [\'loss\', \'acc\']\n    # >> [0.08865142822265625, 0.9722]\n\n    keras_model.save_weights(""/tmp/mnist_keras.h5"")\n\n\nif __name__ == \'__main__\':\n\n    max_epoch = 5\n\n    if len(sys.argv) > 1:\n        max_epoch = int(sys.argv[1])\n    main(max_epoch)\n'"
pyzoo/zoo/examples/tensorflow/tfpark/tf_optimizer/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/examples/tensorflow/tfpark/tf_optimizer/evaluate.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport tensorflow as tf\nfrom zoo import init_nncontext\nfrom zoo.tfpark import TFDataset, TFPredictor\nimport numpy as np\nimport sys\n\nfrom bigdl.dataset import mnist\n\nsys.path.append(""/tmp/models/slim"")  # add the slim library\nfrom nets import lenet\n\nslim = tf.contrib.slim\n\n\ndef main(data_num):\n\n    sc = init_nncontext()\n\n    # get data, pre-process and create TFDataset\n    (images_data, labels_data) = mnist.read_data_sets(""/tmp/mnist"", ""test"")\n    images_data = (images_data[:data_num] - mnist.TRAIN_MEAN) / mnist.TRAIN_STD\n    labels_data = labels_data[:data_num].astype(np.int32)\n    dataset = TFDataset.from_ndarrays((images_data, labels_data), batch_per_thread=20)\n\n    # construct the model from TFDataset\n    images, labels = dataset.tensors\n\n    labels = tf.squeeze(labels)\n\n    with slim.arg_scope(lenet.lenet_arg_scope()):\n        logits, end_points = lenet.lenet(images, num_classes=10, is_training=False)\n\n    predictions = tf.to_int32(tf.argmax(logits, axis=1))\n    correct = tf.expand_dims(tf.to_int32(tf.equal(predictions, labels)), axis=1)\n\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        saver.restore(sess, ""/tmp/lenet/model"")\n\n        predictor = TFPredictor(sess, [correct])\n\n        accuracy = predictor.predict().mean()\n\n        print(""predict accuracy is %s"" % accuracy)\n\n\nif __name__ == \'__main__\':\n\n    data_num = 10000\n\n    if len(sys.argv) > 1:\n        data_num = int(sys.argv[1])\n    main(data_num)\n'"
pyzoo/zoo/examples/tensorflow/tfpark/tf_optimizer/train.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport tensorflow as tf\nfrom zoo import init_nncontext\nfrom zoo.tfpark import TFOptimizer, TFDataset\nfrom bigdl.optim.optimizer import *\nimport numpy as np\nimport sys\n\nfrom bigdl.dataset import mnist\nfrom bigdl.dataset.transformer import *\n\nsys.path.append(""/tmp/models/slim"")  # add the slim library\nfrom nets import lenet\n\nslim = tf.contrib.slim\n\n\ndef accuracy(logits, labels):\n    predictions = tf.argmax(logits, axis=1, output_type=labels.dtype)\n    is_correct = tf.cast(tf.equal(predictions, labels), dtype=tf.float32)\n    return tf.reduce_mean(is_correct)\n\n\ndef main(max_epoch, data_num):\n    sc = init_nncontext()\n\n    # get data, pre-process and create TFDataset\n    (train_images_data, train_labels_data) = mnist.read_data_sets(""/tmp/mnist"", ""train"")\n    (test_images_data, test_labels_data) = mnist.read_data_sets(""/tmp/mnist"", ""train"")\n\n    train_images_data = (train_images_data[:data_num] - mnist.TRAIN_MEAN) / mnist.TRAIN_STD\n    train_labels_data = train_labels_data[:data_num].astype(np.int)\n    test_images_data = (test_images_data[:data_num] - mnist.TRAIN_MEAN) / mnist.TRAIN_STD\n    test_labels_data = (test_labels_data[:data_num]).astype(np.int)\n    dataset = TFDataset.from_ndarrays((train_images_data, train_labels_data),\n                                      batch_size=360,\n                                      val_tensors=(test_images_data, test_labels_data))\n\n    # construct the model from TFDataset\n    images, labels = dataset.tensors\n\n    with slim.arg_scope(lenet.lenet_arg_scope()):\n        logits, end_points = lenet.lenet(images, num_classes=10, is_training=True)\n\n    loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels))\n\n    acc = accuracy(logits, labels)\n\n    # create a optimizer\n    optimizer = TFOptimizer.from_loss(loss, Adam(1e-3),\n                                      metrics={""acc"": acc},\n                                      model_dir=""/tmp/lenet/"")\n    # kick off training\n    optimizer.optimize(end_trigger=MaxEpoch(max_epoch))\n\n    saver = tf.train.Saver()\n    saver.save(optimizer.sess, ""/tmp/lenet/model"")\n\n\nif __name__ == \'__main__\':\n\n    max_epoch = 5\n    data_num = 60000\n\n    if len(sys.argv) > 1:\n        max_epoch = int(sys.argv[1])\n        data_num = int(sys.argv[2])\n    main(max_epoch, data_num)\n'"
pyzoo/zoo/pipeline/api/keras/datasets/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/pipeline/api/keras/datasets/boston_housing.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Source url of dataset is from boston_housing.py from Keras Open Source Project\n#\n# The MIT License (MIT)\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\nimport numpy as np\n\nfrom bigdl.dataset import base\n\n\ndef load_data(path=\'boston_housing.npz\', dest_dir=\'/tmp/.zoo/dataset\', test_split=0.2):\n    """"""Loads the Boston Housing dataset, the source url of download\n       is copied from keras.datasets\n    # Arguments\n        dest_dir: where to cache the data (relative to `~/.zoo/dataset`).\n        nb_words: number of words to keep, the words are already indexed by frequency\n                  so that the less frequent words would be abandoned\n        oov_char: index to pad the abandoned words, if None, one abandoned word\n                  would be taken place with its next word and total length -= 1\n        test_split: the ratio to split part of dataset to test data,\n                    the remained data would be train data\n\n    # Returns\n        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n    """"""\n\n    path = base.maybe_download(path,\n                               dest_dir,\n                               \'https://s3.amazonaws.com/keras-datasets/boston_housing.npz\')\n\n    with np.load(path) as f:\n        x = f[\'x\']\n        y = f[\'y\']\n\n    shuffle_by_seed([x, y])\n    split_index = int(len(x) * (1 - test_split))\n\n    x_train, y_train = x[:split_index], y[:split_index]\n\n    x_test, y_test = x[split_index:], y[split_index:]\n\n    return (x_train, y_train), (x_test, y_test)\n\n\ndef shuffle_by_seed(arr_list, seed=0):\n    for arr in arr_list:\n        np.random.seed(seed)\n        np.random.shuffle(arr)\n'"
pyzoo/zoo/pipeline/api/keras/datasets/imdb.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom bigdl.dataset import base\nimport numpy as np\n\nfrom six.moves import cPickle\n\n\ndef download_imdb(dest_dir):\n    """"""Download pre-processed IMDB movie review data\n\n    :argument\n        dest_dir: destination directory to store the data\n\n    :return\n        The absolute path of the stored data\n    """"""\n    file_name = ""imdb_full.pkl""\n    file_abs_path = base.maybe_download(file_name,\n                                        dest_dir,\n                                        \'https://s3.amazonaws.com/text-datasets/imdb_full.pkl\')\n    return file_abs_path\n\n\ndef load_data(dest_dir=\'/tmp/.zoo/dataset\', nb_words=None, oov_char=2):\n    """"""Load IMDB dataset.\n\n    :argument\n        dest_dir: where to cache the data (relative to `~/.zoo/dataset`).\n        nb_words: number of words to keep, the words are already indexed by frequency\n                  so that the less frequent words would be abandoned\n        oov_char: index to pad the abandoned words, if None, one abandoned word\n                  would be taken place with its next word and total length -= 1\n    :return\n        the train, test separated IMDB dataset.\n    """"""\n    path = download_imdb(dest_dir)\n\n    f = open(path, \'rb\')\n\n    (x_train, y_train), (x_test, y_test) = cPickle.load(f)\n    # imdb.pkl would return different numbers of variables, not 4\n\n    f.close()\n    shuffle_by_seed([x_train, y_train, x_test, y_test])\n    x = x_train + x_test\n\n    if not nb_words:\n        nb_words = max([max(s) for s in x])\n\n    if oov_char is not None:\n        new_x = []\n        for s in x:\n            new_s = []\n            for word in s:\n                if word >= nb_words:\n                    new_s.append(oov_char)\n                else:\n                    new_s.append(word)\n            new_x.append(new_s)\n    else:\n        new_x = []\n        for s in x:\n            new_s = []\n            for word in s:\n                if word < nb_words:\n                    new_s.append(word)\n            new_x.append(new_s)\n    x = new_x\n\n    return (np.array(x[:len(x_train)]), np.array(y_train)), \\\n           (np.array(x[len(x_train):]), np.array(y_test))\n\n\ndef shuffle_by_seed(arr_list, seed=0):\n    for arr in arr_list:\n        np.random.seed(seed)\n        np.random.shuffle(arr)\n\n\ndef get_word_index(dest_dir=\'/tmp/.zoo/dataset\', filename=\'imdb_word_index.pkl\'):\n    """"""Retrieves the dictionary mapping word indices back to words.\n\n    # Arguments\n        dest_dir: where to cache the data (relative to `~/.zoo/dataset`).\n        filename: dataset file name\n\n    # Returns\n        The word index dictionary.\n    """"""\n\n    path = base.maybe_download(filename,\n                               dest_dir,\n                               \'https://s3.amazonaws.com/text-datasets/imdb_word_index.pkl\',\n                               )\n    f = open(path, \'rb\')\n\n    data = cPickle.load(f, encoding=\'latin1\')\n\n    f.close()\n    return data\n\n\nif __name__ == ""__main__"":\n    print(\'Processing text dataset\')\n    (x_train, y_train), (x_test, y_test) = load_data()\n    print(\'finished processing text\')\n'"
pyzoo/zoo/pipeline/api/keras/datasets/mnist.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport gzip\nimport numpy\n\nfrom bigdl.dataset import base\n\n\nSOURCE_URL = \'http://yann.lecun.com/exdb/mnist/\'\n\nTRAIN_MEAN = 0.13066047740239506 * 255\nTRAIN_STD = 0.3081078 * 255\nTEST_MEAN = 0.13251460696903547 * 255\nTEST_STD = 0.31048024 * 255\n\n\ndef _read32(bytestream):\n    dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n    return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n\n\ndef extract_images(f):\n    """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].\n\n    :param: f: A file object that can be passed into a gzip reader.\n    :return: data: A 4D unit8 numpy array [index, y, x, depth].\n    :raise: ValueError: If the bytestream does not start with 2051.\n\n    """"""\n    print(\'Extracting\', f.name)\n    with gzip.GzipFile(fileobj=f) as bytestream:\n        magic = _read32(bytestream)\n        if magic != 2051:\n            raise ValueError(\n                \'Invalid magic number %d in MNIST image file: %s\' %\n                (magic, f.name))\n        num_images = _read32(bytestream)\n        rows = _read32(bytestream)\n        cols = _read32(bytestream)\n        buf = bytestream.read(rows * cols * num_images)\n        data = numpy.frombuffer(buf, dtype=numpy.uint8)\n        data = data.reshape(num_images, rows, cols, 1)\n        return data\n\n\ndef extract_labels(f):\n    print(\'Extracting\', f.name)\n    with gzip.GzipFile(fileobj=f) as bytestream:\n        magic = _read32(bytestream)\n        if magic != 2049:\n            raise ValueError(\n                \'Invalid magic number %d in MNIST label file: %s\' %\n                (magic, f.name))\n        num_items = _read32(bytestream)\n        buf = bytestream.read(num_items)\n        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n        return labels\n\n\ndef read_data_sets(train_dir, data_type=""train""):\n    """"""\n    Parse or download mnist data if train_dir is empty.\n\n    :param: train_dir: The directory storing the mnist data\n\n    :param: data_type: Reading training set or testing set.It can be either ""train"" or ""test""\n\n    :return:\n\n    ```\n    (ndarray, ndarray) representing (features, labels)\n    features is a 4D unit8 numpy array [index, y, x, depth]\n    representing each pixel valued from 0 to 255.\n    labels is 1D unit8 nunpy array representing the label valued from 0 to 9.\n    ```\n\n    """"""\n    TRAIN_IMAGES = \'train-images-idx3-ubyte.gz\'\n    TRAIN_LABELS = \'train-labels-idx1-ubyte.gz\'\n    TEST_IMAGES = \'t10k-images-idx3-ubyte.gz\'\n    TEST_LABELS = \'t10k-labels-idx1-ubyte.gz\'\n\n    if data_type == ""train"":\n        local_file = base.maybe_download(TRAIN_IMAGES, train_dir,\n                                         SOURCE_URL + TRAIN_IMAGES)\n        with open(local_file, \'rb\') as f:\n            train_images = extract_images(f)\n\n        local_file = base.maybe_download(TRAIN_LABELS, train_dir,\n                                         SOURCE_URL + TRAIN_LABELS)\n        with open(local_file, \'rb\') as f:\n            train_labels = extract_labels(f)\n        return train_images, train_labels\n\n    else:\n        local_file = base.maybe_download(TEST_IMAGES, train_dir,\n                                         SOURCE_URL + TEST_IMAGES)\n        with open(local_file, \'rb\') as f:\n            test_images = extract_images(f)\n\n        local_file = base.maybe_download(TEST_LABELS, train_dir,\n                                         SOURCE_URL + TEST_LABELS)\n        with open(local_file, \'rb\') as f:\n            test_labels = extract_labels(f)\n        return test_images, test_labels\n\n\ndef load_data(location=""/tmp/.zoo/dataset/mnist""):\n    (train_images, train_labels) = read_data_sets(location, ""train"")\n    (test_images, test_labels) = read_data_sets(location, ""test"")\n\n    return (train_images, train_labels), (test_images, test_labels)\n\n\nif __name__ == ""__main__"":\n    (train_images, train_labels), (test_images, test_labels) = load_data()\n    train, _ = read_data_sets(""/tmp/.zoo/dataset/mnist/"", ""train"")\n    test, _ = read_data_sets(""/tmp/.zoo/dataset/mnist/"", ""test"")\n'"
pyzoo/zoo/pipeline/api/keras/datasets/reuters.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nfrom bigdl.dataset import base\nimport numpy as np\n\nfrom six.moves import cPickle\n\n\ndef download_reuters(dest_dir):\n    """"""Download pre-processed reuters newswire data\n\n    :argument\n        dest_dir: destination directory to store the data\n\n    :return\n        The absolute path of the stored data\n    """"""\n    file_name = \'reuters.pkl\'\n    file_abs_path = base.maybe_download(file_name,\n                                        dest_dir,\n                                        \'https://s3.amazonaws.com/text-datasets/reuters.pkl\')\n    return file_abs_path\n\n\ndef load_data(dest_dir=\'/tmp/.zoo/dataset\', nb_words=None, oov_char=2, test_split=0.2):\n    """"""Load reuters dataset.\n\n    :argument\n        dest_dir: where to cache the data (relative to `~/.zoo/dataset`).\n        nb_words: number of words to keep, the words are already indexed by frequency\n                  so that the less frequent words would be abandoned\n        oov_char: index to pad the abandoned words, if None, one abandoned word\n                  would be taken place with its next word and total length -= 1\n        test_split: the ratio to split part of dataset to test data,\n                    the remained data would be train data\n\n    :return\n        the train, test separated reuters dataset.\n    """"""\n    path = download_reuters(dest_dir)\n\n    f = open(path, \'rb\')\n\n    x, y = cPickle.load(f)\n    # return data and label, need to separate to train and test\n\n    f.close()\n\n    shuffle_by_seed([x, y])\n\n    if not nb_words:\n        nb_words = max([max(s) for s in x])\n\n    if oov_char is not None:\n        new_x = []\n        for s in x:\n            new_s = []\n            for word in s:\n                if word >= nb_words:\n                    new_s.append(oov_char)\n                else:\n                    new_s.append(word)\n            new_x.append(new_s)\n    else:\n        new_x = []\n        for s in x:\n            new_s = []\n            for word in s:\n                if word < nb_words:\n                    new_s.append(word)\n            new_x.append(new_s)\n    x = new_x\n\n    split_index = int(len(x) * (1 - test_split))\n\n    x_train, y_train = x[:split_index], y[:split_index]\n\n    x_test, y_test = x[split_index:], y[split_index:]\n\n    return (x_train, y_train), (x_test, y_test)\n\n\ndef shuffle_by_seed(arr_list, seed=0):\n    for arr in arr_list:\n        np.random.seed(seed)\n        np.random.shuffle(arr)\n\n\ndef get_word_index(dest_dir=\'/tmp/.zoo/dataset\', filename=\'reuters_word_index.pkl\'):\n    """"""Retrieves the dictionary mapping word indices back to words.\n\n    # Arguments\n        dest_dir: where to cache the data (relative to `~/.zoo/dataset`).\n        filename: dataset file name\n\n    # Returns\n        The word index dictionary.\n    """"""\n\n    path = base.maybe_download(filename,\n                               dest_dir,\n                               \'https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl\')\n\n    f = open(path, \'rb\')\n\n    data = cPickle.load(f, encoding=\'latin1\')\n\n    f.close()\n    return data\n\n\nif __name__ == ""__main__"":\n    print(\'Processing text dataset\')\n    (x_train, y_train), (x_test, y_test) = load_data()\n    print(\'finished processing text\')\n'"
pyzoo/zoo/pipeline/api/keras/engine/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom .topology import *\n'"
pyzoo/zoo/pipeline/api/keras/engine/topology.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport zoo.pipeline.api.autograd as autograd\nfrom zoo.feature.image import ImageSet\nfrom zoo.feature.text import TextSet\nfrom zoo.feature.common import FeatureSet\nfrom zoo.pipeline.api.keras.base import ZooKerasLayer\nfrom zoo.pipeline.api.keras.utils import *\nfrom bigdl.nn.layer import Layer\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass KerasNet(ZooKerasLayer):\n    def save(self, path, over_write=False):\n        raise Exception(""This is a deprecated method. Please use saveModel instead."")\n\n    def saveModel(self, modelPath, weightPath=None, over_write=False):\n        """"""\n        Save this module to path with protobuf format.\n        :param modelPath: The path to save module, local file system,\n                          HDFS and Amazon S3 is supported.\n                          HDFS path should be like ""hdfs://[host]:[port]/xxx""\n                          Amazon S3 path should be like ""s3a://bucket/xxx""\n        :param weightPath: The Path for the parameters\n        :param over_write: override the existing model on modelPath or not.\n        """"""\n        super(KerasNet, self).saveModel(modelPath=modelPath,\n                                        weightPath=weightPath,\n                                        over_write=over_write)\n\n    def compile(self, optimizer, loss, metrics=None):\n        """"""\n        Configure the learning process. It MUST be called before fit or evaluate.\n\n        # Arguments\n        optimizer: Optimization method to be used. One can alternatively pass in the corresponding\n                   string representation, such as \'sgd\'.\n        loss: Criterion to be used. One can alternatively pass in the corresponding string\n              representation, such as \'mse\'.\n        metrics: List of validation methods to be used. Default is None if no validation is needed.\n                 For convenience, string representations are supported: \'accuracy\' (or \'acc\'),\n                 \'top5accuracy\' (or \'top5acc\'), \'mae\', \'auc\', \'treennaccuracy\' and \'loss\'.\n                 For example, you can either use [Accuracy()] or [\'accuracy\'].\n        """"""\n        if isinstance(optimizer, six.string_types):\n            optimizer = to_bigdl_optim_method(optimizer)\n        criterion = loss\n        if isinstance(loss, six.string_types):\n            criterion = to_bigdl_criterion(loss)\n        if callable(loss):\n            from zoo.pipeline.api.autograd import CustomLoss\n            criterion = CustomLoss(loss, self.get_output_shape()[1:])\n        if metrics and all(isinstance(metric, six.string_types) for metric in metrics):\n            metrics = to_bigdl_metrics(metrics, loss)\n        callZooFunc(self.bigdl_type, ""zooCompile"",\n                    self.value,\n                    optimizer,\n                    criterion,\n                    metrics)\n\n    def set_tensorboard(self, log_dir, app_name):\n        """"""\n        Set summary information during the training process for visualization purposes.\n        Saved summary can be viewed via TensorBoard.\n        In order to take effect, it needs to be called before fit.\n\n        Training summary will be saved to \'log_dir/app_name/train\'\n        and validation summary (if any) will be saved to \'log_dir/app_name/validation\'.\n\n        # Arguments\n        log_dir: The base directory path to store training and validation logs.\n        app_name: The name of the application.\n        """"""\n        callZooFunc(self.bigdl_type, ""zooSetTensorBoard"",\n                    self.value,\n                    log_dir,\n                    app_name)\n\n    def get_train_summary(self, tag=None):\n        """"""\n        Get the scalar from model train summary\n        Return 2-D array like object which could be converted\n        by nd.array()\n        # Arguments\n        tag: The string variable represents the scalar wanted\n        """"""\n        # exception handle\n        if tag != ""Loss"" and tag != ""LearningRate"" and tag != ""Throughput"":\n            raise TypeError(\'Only ""Loss"", ""LearningRate"", ""Throughput""\'\n                            + \'are supported in train summary\')\n\n        return callZooFunc(self.bigdl_type, ""zooGetScalarFromSummary"",\n                           self.value, tag, ""Train"")\n\n    def get_validation_summary(self, tag=None):\n        """"""\n        Get the scalar from model validation summary\n        Return 2-D array like object which could be converted\n        by np.array()\n        # Arguments\n        tag: The string variable represents the scalar wanted\n        """"""\n        validation_set = set((\'AUC\', \'Accuracy\', \'BinaryAccuracy\', \'CategoricalAccuracy\',\n                              \'HitRatio\', \'Loss\', \'MAE\', \'NDCG\', \'SparseCategoricalAccuracy\',\n                              \'TFValidationMethod\', \'Top1Accuracy\',\n                              \'Top5Accuracy\', \'TreeNNAccuracy\'))\n        if tag not in validation_set:\n            raise TypeError(\'Only subclasses of ValidationMethod are supported,\'\n                            + \'which are \' + str(validation_set))\n        return callZooFunc(self.bigdl_type, ""zooGetScalarFromSummary"",\n                           self.value, tag, ""Validation"")\n\n    def set_checkpoint(self, path, over_write=True):\n        """"""\n        Configure checkpoint settings to write snapshots every epoch during the training process.\n        In order to take effect, it needs to be called before fit.\n\n        # Arguments\n        path: The path to save snapshots. Make sure this path exists beforehand.\n        over_write: Whether to overwrite existing snapshots in the given path. Default is True.\n        """"""\n        callZooFunc(self.bigdl_type, ""zooSetCheckpoint"",\n                    self.value,\n                    path,\n                    over_write)\n\n    def clear_gradient_clipping(self):\n        """"""\n        Clear gradient clipping parameters. In this case, gradient clipping will not be applied.\n        In order to take effect, it needs to be called before fit.\n        """"""\n        callZooFunc(self.bigdl_type, ""zooClearGradientClipping"",\n                    self.value)\n\n    def set_constant_gradient_clipping(self, min, max):\n        """"""\n        Set constant gradient clipping during the training process.\n        In order to take effect, it needs to be called before fit.\n\n        # Arguments\n        min: The minimum value to clip by. Float.\n        max: The maximum value to clip by. Float.\n        """"""\n        callZooFunc(self.bigdl_type, ""zooSetConstantGradientClipping"",\n                    self.value,\n                    float(min),\n                    float(max))\n\n    def set_gradient_clipping_by_l2_norm(self, clip_norm):\n        """"""\n        Clip gradient to a maximum L2-Norm during the training process.\n        In order to take effect, it needs to be called before fit.\n\n        # Arguments\n        clip_norm: Gradient L2-Norm threshold. Float.\n        """"""\n        callZooFunc(self.bigdl_type, ""zooSetGradientClippingByL2Norm"",\n                    self.value,\n                    float(clip_norm))\n\n    def set_evaluate_status(self):\n        """"""\n        Set the model to be in evaluate status, i.e. remove the effect of Dropout, etc.\n        """"""\n        callZooFunc(self.bigdl_type, ""zooSetEvaluateStatus"",\n                    self.value)\n        return self\n\n    def fit(self, x, y=None, batch_size=32, nb_epoch=10,\n            validation_split=0, validation_data=None, distributed=True):\n        """"""\n        Train a model for a fixed number of epochs on a DataSet.\n\n        # Arguments\n        x: Input data. A Numpy array or RDD of Sample, ImageSet or TextSet.\n        y: Labels. A Numpy array. Default is None if x is already Sample RDD or ImageSet or TextSet.\n        batch_size: Number of samples per gradient update. Default is 32.\n        nb_epoch: Number of epochs to train.\n        validation_data: Tuple (x_val, y_val) where x_val and y_val are both Numpy arrays.\n                         Can also be RDD of Sample or ImageSet or TextSet.\n                         Default is None if no validation is involved.\n        distributed: Boolean. Whether to train the model in distributed mode or local mode.\n                     Default is True. In local mode, x and y must both be Numpy arrays.\n        """"""\n\n        if distributed:\n            if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n                if validation_data:\n                    validation_data = to_sample_rdd(*validation_data)\n                elif validation_split != 0:\n                    if validation_split > 1 or validation_split < 0:\n                        raise TypeError(""validation split must in range [0, 1]"")\n                    split_index = int(len(x) * (1 - validation_split))\n                    validation_data = (x[split_index:], y[split_index:])\n                    x, y = x[:split_index], y[:split_index]\n                    validation_data = to_sample_rdd(*validation_data)\n                training_data = to_sample_rdd(x, y)\n            elif (isinstance(x, RDD) or isinstance(x, ImageSet) or isinstance(x, TextSet)) \\\n                    or isinstance(x, FeatureSet) and not y:\n                training_data = x\n            else:\n                raise TypeError(""Unsupported training data type: %s"" % type(x))\n            callZooFunc(self.bigdl_type, ""zooFit"",\n                        self.value,\n                        training_data,\n                        batch_size,\n                        nb_epoch,\n                        validation_data)\n        else:\n            if validation_data:\n                val_x = [JTensor.from_ndarray(x) for x in to_list(validation_data[0])]\n                val_y = JTensor.from_ndarray(validation_data[1])\n            else:\n                val_x, val_y = None, None\n            callZooFunc(self.bigdl_type, ""zooFit"",\n                        self.value,\n                        [JTensor.from_ndarray(x) for x in to_list(x)],\n                        JTensor.from_ndarray(y),\n                        batch_size,\n                        nb_epoch,\n                        val_x,\n                        val_y)\n\n    def evaluate(self, x, y=None, batch_size=32):\n        """"""\n        Evaluate a model on a given dataset in distributed mode.\n\n        # Arguments\n        x: Evaluation data. A Numpy array or RDD of Sample or ImageSet or TextSet.\n        y: Labels. A Numpy array.\n           Default is None if x is already Sample RDD or ImageSet or TextSet.\n        batch_size: Number of samples per batch. Default is 32.\n        """"""\n        if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n            data = to_sample_rdd(x, y)\n        elif (isinstance(x, RDD) or isinstance(x, ImageSet) or isinstance(x, TextSet)) and not y:\n            data = x\n        else:\n            raise TypeError(""Unsupported evaluation data type: %s"" % type(x))\n        return callZooFunc(self.bigdl_type, ""zooEvaluate"",\n                           self.value,\n                           data,\n                           batch_size)\n\n    def forward(self, input):\n        """"""\n        NB: It\'s for debug only, please use optimizer.optimize() in production.\n        Takes an input object, and computes the corresponding output of the module\n        :param input: ndarray or list of ndarray\n        :param input: ndarray or list of ndarray or JTensor or list of JTensor.\n        :return: ndarray or list of ndarray\n        """"""\n        jinput, input_is_table = self.check_input(input)\n        output = callZooFunc(self.bigdl_type,\n                             ""zooForward"",\n                             self.value,\n                             jinput,\n                             input_is_table)\n        return self.convert_output(output)\n\n    @staticmethod\n    def convert_output(output):\n        if type(output) is JTensor:\n            return output.to_ndarray()\n        elif len(output) == 1:\n            return KerasNet.convert_output(output[0])\n        else:\n            return [KerasNet.convert_output(x) for x in output]\n\n    def predict(self, x, batch_per_thread=4, distributed=True):\n        """"""\n        Use a model to do prediction.\n\n        # Arguments\n        x: Prediction data. A Numpy array or RDD of Sample or ImageSet.\n        batch_per_thread:\n          The default value is 4.\n          When distributed is True,the total batch size is batch_per_thread * rdd.getNumPartitions.\n          When distributed is False the total batch size is batch_per_thread * numOfCores.\n        distributed: Boolean. Whether to do prediction in distributed mode or local mode.\n                     Default is True. In local mode, x must be a Numpy array.\n        """"""\n        if isinstance(x, ImageSet) or isinstance(x, TextSet):\n            results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                  self.value,\n                                  x,\n                                  batch_per_thread)\n            return ImageSet(results) if isinstance(x, ImageSet) else TextSet(results)\n        if distributed:\n            if isinstance(x, np.ndarray):\n                data_rdd = to_sample_rdd(x, np.zeros([x.shape[0]]))\n            elif isinstance(x, RDD):\n                data_rdd = x\n            else:\n                raise TypeError(""Unsupported prediction data type: %s"" % type(x))\n            results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                  self.value,\n                                  data_rdd,\n                                  batch_per_thread)\n            return results.map(lambda result: Layer.convert_output(result))\n        else:\n            if isinstance(x, np.ndarray) or isinstance(x, list):\n                results = callZooFunc(self.bigdl_type, ""zooPredict"",\n                                      self.value,\n                                      self._to_jtensors(x),\n                                      batch_per_thread)\n                return [Layer.convert_output(result) for result in results]\n            else:\n                raise TypeError(""Unsupported prediction data type: %s"" % type(x))\n\n    def predict_classes(self, x, batch_per_thread=4, zero_based_label=True):\n        """"""\n        Use a model to predict for classes. By default, label predictions start from 0.\n\n        # Arguments\n        x: Prediction data. A Numpy array or RDD of Sample.\n        batch_per_partition:\n          The default value is 4.\n          When distributed is True, the total batch size is batch_per_thread * rdd.getNumPartitions.\n          When distributed is False the total batch size is batch_per_thread * numOfCores.\n        zero_based_label: Boolean. Whether result labels start from 0.\n                          Default is True. If False, result labels start from 1.\n        """"""\n        if isinstance(x, np.ndarray):\n            data_rdd = to_sample_rdd(x, np.zeros([x.shape[0]]))\n        elif isinstance(x, RDD):\n            data_rdd = x\n        else:\n            raise TypeError(""Unsupported prediction data type: %s"" % type(x))\n        return callZooFunc(self.bigdl_type, ""zooPredictClasses"",\n                           self.value,\n                           data_rdd,\n                           batch_per_thread,\n                           zero_based_label)\n\n    def get_layer(self, name):\n        layer = [l for l in self.layers if l.name() == name]\n        if (len(layer) == 0):\n            raise Exception(""Could not find a layer named: %s"" + name)\n        elif (len(layer) > 1):\n            raise Exception(""There are multiple layers named: %s"" + name)\n        else:\n            return layer[0]\n\n    def summary(self, line_length=120, positions=[.33, .55, .67, 1.]):\n        """"""\n        Print out the summary information of an Analytics Zoo Keras Model.\n\n        For each layer in the model, there will be a separate row containing four columns:\n        ________________________________________________________________________________\n        Layer (type)          Output Shape          Param #     Connected to\n        ================================================================================\n\n        In addition, total number of parameters of this model, separated into trainable and\n        non-trainable counts, will be printed out after the table.\n\n        # Arguments\n        line_length The total length of one row. Default is 120.\n        positions: The maximum absolute length proportion(%) of each field.\n                   List of Float of length 4.\n                   Usually you don\'t need to adjust this parameter.\n                   Default is [.33, .55, .67, 1.], meaning that\n                   the first field will occupy up to 33% of line_length,\n                   the second field will occupy up to (55-33)% of line_length,\n                   the third field will occupy up to (67-55)% of line_length,\n                   the fourth field will occupy the remaining line (100-67)%.\n                   If the field has a larger length, the remaining part will be trimmed.\n                   If the field has a smaller length, the remaining part will be white spaces.\n        """"""\n        callZooFunc(self.bigdl_type, ""zooKerasNetSummary"",\n                    self.value,\n                    line_length,\n                    [float(p) for p in positions])\n\n    def to_model(self):\n        from zoo.pipeline.api.keras.models import Model\n        return Model.from_jvalue(callZooFunc(self.bigdl_type, ""kerasNetToModel"", self.value))\n\n    @property\n    def layers(self):\n        jlayers = callZooFunc(self.bigdl_type, ""getSubModules"", self)\n        layers = [Layer.of(jlayer) for jlayer in jlayers]\n        return layers\n\n    def flattened_layers(self, include_container=False):\n        jlayers = callZooFunc(self.bigdl_type, ""getFlattenSubModules"", self, include_container)\n        layers = [Layer.of(jlayer) for jlayer in jlayers]\n        return layers\n\n\nclass Input(autograd.Variable):\n    """"""\n    Used to instantiate an input node.\n\n    # Arguments\n    shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> input = Input(name=""input1"", shape=(3, 5))\n    creating: createZooKerasInput\n    """"""\n\n    def __init__(self, shape=None, name=None, bigdl_type=""float""):\n        super(Input, self).__init__(input_shape=list(shape) if shape else None,\n                                    node=None, jvalue=None, name=name)\n\n\nclass InputLayer(ZooKerasLayer):\n    """"""\n    Used as an entry point into a model.\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> inputlayer = InputLayer(input_shape=(3, 5), name=""input1"")\n    creating: createZooKerasInputLayer\n    """"""\n\n    def __init__(self, input_shape=None, **kwargs):\n        super(InputLayer, self).__init__(None,\n                                         list(input_shape) if input_shape else None,\n                                         **kwargs)\n\n\nclass Merge(ZooKerasLayer):\n    """"""\n    Used to merge a list of inputs into a single output, following some merge mode.\n    Merge must have at least two input layers.\n\n    When using this layer as the first layer in a model, you need to provide the argument\n    input_shape for input layers (a list of shape tuples, does not include the batch dimension).\n\n    # Arguments\n    layers: A list of layer instances. Must be more than one layer.\n    mode: Merge mode. String, must be one of: \'sum\', \'mul\', \'concat\', \'ave\', \'cos\',\n          \'dot\', \'max\', \'sub\', \'div\', \'min\'. Default is \'sum\'.\n    concat_axis: Int, axis to use when concatenating layers.\n                 Only specify this when merge mode is \'concat\'.\n                 Default is -1, meaning the last axis of the input.\n    input_shape: A list of shape tuples, each not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> l1 = InputLayer(input_shape=(3, 5))\n    creating: createZooKerasInputLayer\n    >>> l2 = InputLayer(input_shape=(3, 5))\n    creating: createZooKerasInputLayer\n    >>> merge = Merge(layers=[l1, l2], mode=\'sum\', name=""merge1"")\n    creating: createZooKerasMerge\n    """"""\n\n    def __init__(self, layers=None, mode=""sum"", concat_axis=-1,\n                 input_shape=None, **kwargs):\n        super(Merge, self).__init__(None,\n                                    list(layers) if layers else None,\n                                    mode,\n                                    concat_axis,\n                                    input_shape,\n                                    **kwargs)\n\n\ndef merge(inputs, mode=""sum"", concat_axis=-1, name=None):\n    """"""\n    Functional merge. Only use this method if you are defining a graph model.\n    Used to merge a list of input nodes into a single output node (NOT layers!),\n    following some merge mode.\n\n    # Arguments\n    inputs: A list of node instances. Must be more than one node.\n    mode: Merge mode. String, must be one of: \'sum\', \'mul\', \'concat\', \'ave\', \'cos\',\n          \'dot\', \'max\', \'sub\', \'div\', \'min\'. Default is \'sum\'.\n    concat_axis: Int, axis to use when concatenating nodes.\n                 Only specify this when merge mode is \'concat\'.\n                 Default is -1, meaning the last axis of the input.\n    name: String to set the name of the functional merge.\n          If not specified, its name will by default to be a generated string.\n    """"""\n    return Merge(mode=mode, concat_axis=concat_axis, name=name)(list(inputs))\n'"
pyzoo/zoo/pipeline/api/keras/layers/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom ..engine import *\nfrom .core import *\nfrom .convolutional import *\nfrom .pooling import *\nfrom .local import *\nfrom .recurrent import *\nfrom .normalization import *\nfrom .embeddings import *\nfrom .noise import *\nfrom .advanced_activations import *\nfrom .wrappers import *\nfrom .convolutional_recurrent import *\nfrom .torch import *\nfrom .self_attention import *\n'"
pyzoo/zoo/pipeline/api/keras/layers/advanced_activations.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom ..engine.topology import ZooKerasLayer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass LeakyReLU(ZooKerasLayer):\n    """"""\n    Leaky version of a Rectified Linear Unit.\n    It allows a small gradient when the unit is not active:\n    f(x) = alpha * x for x < 0,\n    f(x) = x for x >= 0.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    alpha: Float >= 0. Negative slope coefficient. Default is 0.3.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> leakyrelu = LeakyReLU(0.02, input_shape=(4, 5))\n    creating: createZooKerasLeakyReLU\n    """"""\n    def __init__(self, alpha=0.01, input_shape=None, **kwargs):\n        super(LeakyReLU, self).__init__(None,\n                                        float(alpha),\n                                        list(input_shape) if input_shape else None,\n                                        **kwargs)\n\n\nclass ELU(ZooKerasLayer):\n    """"""\n    Exponential Linear Unit.\n    It follows:\n    f(x) =  alpha * (exp(x) - 1.) for x < 0,\n    f(x) = x for x >= 0.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    alpha: Float, scale for the negative factor. Default is 1.0.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> elu = ELU(1.2, input_shape=(4, 5))\n    creating: createZooKerasELU\n    """"""\n    def __init__(self, alpha=1.0, input_shape=None, **kwargs):\n        super(ELU, self).__init__(None,\n                                  float(alpha),\n                                  list(input_shape) if input_shape else None,\n                                  **kwargs)\n\n\nclass ThresholdedReLU(ZooKerasLayer):\n    """"""\n    Thresholded Rectified Linear Unit.\n    It follows:\n    f(x) = x for x > theta,\n    f(x) = 0 otherwise.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    theta: Float >= 0. Threshold location of activation. Default is 1.0.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> thresholdedrelu = ThresholdedReLU(input_shape=(10, 12))\n    creating: createZooKerasThresholdedReLU\n    """"""\n    def __init__(self, theta=1.0, input_shape=None, **kwargs):\n        super(ThresholdedReLU, self).__init__(None,\n                                              float(theta),\n                                              list(input_shape) if input_shape else None,\n                                              **kwargs)\n\n\nclass SReLU(ZooKerasLayer):\n    """"""\n    S-shaped Rectified Linear Unit.\n    It follows:\n    f(x) = t^r + a^r(x - t^r) for x >= t^r\n    f(x) = x for t^r > x > t^l,\n    f(x) = t^l + a^l(x - t^l) for x <= t^l\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    t_left_init: String representation of the initialization method for the left part intercept.\n                 Default is \'zero\'.\n    a_left_init: String representation of the initialization method for the left part slope.\n                 Default is \'glorot_uniform\'.\n    t_right_init: String representation of the nitialization method for the right part intercept.\n                  Default is \'glorot_uniform\'.\n    a_right_init: String representation of the initialization method for the right part slope.\n                  Default is \'one\'.\n    shared_axes: Int tuple. The axes along which to share learnable parameters for the\n                 activation function. Default is None.\n                 For example, if the incoming feature maps are from a 2D convolution with output\n                 shape (batch, height, width, channels), and you wish to share parameters across\n                 space so that each filter only has one set of parameters, set \'shared_axes=(1,2)\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> srelu = SReLU(input_shape=(4, 5))\n    creating: createZooKerasSReLU\n    """"""\n    def __init__(self, t_left_init=""zero"", a_left_init=""glorot_uniform"",\n                 t_right_init=""glorot_uniform"", a_right_init=""one"",\n                 shared_axes=None, input_shape=None, **kwargs):\n        super(SReLU, self).__init__(None,\n                                    t_left_init,\n                                    a_left_init,\n                                    t_right_init,\n                                    a_right_init,\n                                    shared_axes,\n                                    list(input_shape) if input_shape else None,\n                                    **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras/layers/convolutional.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom ..engine.topology import ZooKerasLayer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass Convolution1D(ZooKerasLayer):\n    """"""\n    Applies convolution operator for filtering neighborhoods of 1D inputs.\n    You can also use Conv1D as an alias of this layer.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    nb_filter: Number of convolution filters to use.\n    filter_length: The extension (spatial or temporal) of each filter.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'glorot_uniform\'.\n    limits: Optional. Limit value for initialization method.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    border_mode: Either \'valid\' or \'same\'. Default is \'valid\'.\n    subsample_length: Factor by which to subsample output. Int. Default is 1.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> conv1d = Convolution1D(12, 4, input_shape=(3, 16))\n    creating: createZooKerasConvolution1D\n    """"""\n    def __init__(self, nb_filter, filter_length, init=""glorot_uniform"", limits=None,\n                 activation=None, border_mode=""valid"", subsample_length=1,\n                 W_regularizer=None, b_regularizer=None, bias=True,\n                 input_shape=None, **kwargs):\n        super(Convolution1D, self).__init__(None,\n                                            nb_filter,\n                                            filter_length,\n                                            init,\n                                            list(limits) if limits else None,\n                                            activation,\n                                            border_mode,\n                                            subsample_length,\n                                            W_regularizer,\n                                            b_regularizer,\n                                            bias,\n                                            list(input_shape) if input_shape else None,\n                                            **kwargs)\n\n\nclass AtrousConvolution1D(ZooKerasLayer):\n    """"""\n    Applies an atrous convolution operator for filtering neighborhoods of 1D inputs.\n    A.k.a dilated convolution or convolution with holes.\n    Border mode currently supported for this layer is \'valid\'.\n    Bias will be included in this layer.\n    You can also use AtrousConv1D as an alias of this layer.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    nb_filter: Number of convolution filters to use.\n    filter_length: The extension (spatial or temporal) of each filter.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'glorot_uniform\'.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    border_mode: Only \'valid\' is supported for now.\n    subsample_length: Factor by which to subsample output. Int. Default is 1.\n    atrous_rate: Factor for kernel dilation. Also called filter_dilation elsewhere.\n                 Int. Default is 1.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Only \'True\' is supported for now.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> atrousconv1d = AtrousConvolution1D(8, 3, input_shape=(3, 12))\n    creating: createZooKerasAtrousConvolution1D\n    """"""\n    def __init__(self, nb_filter, filter_length, init=""glorot_uniform"", activation=None,\n                 border_mode=""valid"", subsample_length=1, atrous_rate=1, W_regularizer=None,\n                 b_regularizer=None, bias=True, input_shape=None, **kwargs):\n        if border_mode != ""valid"":\n            raise ValueError(""For AtrousConvolution1D, ""\n                             ""only border_mode=\'valid\' is supported for now"")\n        if not bias:\n            raise ValueError(""For AtrousConvolution1D, ""\n                             ""only bias=True is supported for now"")\n        super(AtrousConvolution1D, self).__init__(None,\n                                                  nb_filter,\n                                                  filter_length,\n                                                  init,\n                                                  activation,\n                                                  subsample_length,\n                                                  atrous_rate,\n                                                  W_regularizer,\n                                                  b_regularizer,\n                                                  list(input_shape) if input_shape else None,\n                                                  **kwargs)\n\n\nclass Convolution2D(ZooKerasLayer):\n    """"""\n    Applies a 2D convolution over an input image composed of several input planes.\n    You can also use Conv2D as an alias of this layer.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n    e.g. input_shape=(3, 128, 128) for 128x128 RGB pictures.\n\n    # Arguments\n    nb_filter: Number of convolution filters to use.\n    nb_row: Number of rows in the convolution kernel.\n    nb_col: Number of cols in the convolution kernel.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'glorot_uniform\'.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    border_mode: Either \'valid\' or \'same\'. Default is \'valid\'.\n    subsample: Int tuple of length 2 corresponding to the step of the convolution in the\n               height and width dimension. Also called strides elsewhere. Default is (1, 1).\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> conv2d = Convolution2D(32, 3, 3, input_shape=(3, 128, 128), name=""convolution2d_1"")\n    creating: createZooKerasConvolution2D\n    """"""\n    def __init__(self, nb_filter, nb_row, nb_col,\n                 init=""glorot_uniform"", activation=None,\n                 border_mode=""valid"", subsample=(1, 1), dim_ordering=""th"",\n                 W_regularizer=None, b_regularizer=None, bias=True,\n                 input_shape=None, pads=None,  **kwargs):\n        super(Convolution2D, self).__init__(None,\n                                            nb_filter,\n                                            nb_row,\n                                            nb_col,\n                                            init,\n                                            activation,\n                                            border_mode,\n                                            subsample,\n                                            dim_ordering,\n                                            W_regularizer,\n                                            b_regularizer,\n                                            bias,\n                                            list(input_shape) if input_shape else None,\n                                            pads,\n                                            **kwargs)\n\n\nclass Deconvolution2D(ZooKerasLayer):\n    """"""\n    Transposed convolution operator for filtering windows of 2D inputs.\n    The need for transposed convolutions generally arises from the desire to use a transformation\n    going in the opposite direction of a normal convolution, i.e., from something that has\n    the shape of the output of some convolution to something that has the shape of its input\n    while maintaining a connectivity pattern that is compatible with said convolution.\n    Data format currently supported for this layer is dim_ordering=\'th\' (Channel First).\n    Border mode currently supported for this layer is \'valid\'.\n    You can also use Deconv2D as an alias of this layer.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n    e.g. input_shape=(3, 128, 128) for 128x128 RGB pictures.\n\n    # Arguments\n    nb_filter: Number of transposed convolution filters to use.\n    nb_row: Number of rows in the convolution kernel.\n    nb_col: Number of cols in the convolution kernel.\n    output_shape: Output shape of the transposed convolution operation. Tuple of int.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'glorot_uniform\'.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    border_mode: Only \'valid\' is supported for now.\n    subsample: Int tuple of length 2 corresponding to the step of the convolution in the\n               height and width dimension. Also called strides elsewhere. Default is (1, 1).\n    dim_ordering: Format of input data. Only \'th\' (Channel First) is supported for now.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> deconv2d = Deconvolution2D(3, 3, 3, output_shape=(None, 3, 14, 14), input_shape=(3, 12, 12))\n    creating: createZooKerasDeconvolution2D\n    """"""\n    def __init__(self, nb_filter, nb_row, nb_col, output_shape, init=""glorot_uniform"",\n                 activation=None, border_mode=""valid"", subsample=(1, 1), dim_ordering=""th"",\n                 W_regularizer=None, b_regularizer=None, bias=True, input_shape=None, **kwargs):\n        if border_mode != ""valid"":\n            raise ValueError(""For Deconvolution2D, only border_mode=\'valid\' is supported for now"")\n        super(Deconvolution2D, self).__init__(None,\n                                              nb_filter,\n                                              nb_row,\n                                              nb_col,\n                                              init,\n                                              activation,\n                                              subsample,\n                                              dim_ordering,\n                                              W_regularizer,\n                                              b_regularizer,\n                                              bias,\n                                              list(input_shape) if input_shape else None,\n                                              **kwargs)\n\n\nclass AtrousConvolution2D(ZooKerasLayer):\n    """"""\n    Applies an atrous Convolution operator for filtering windows of 2D inputs.\n    A.k.a dilated convolution or convolution with holes.\n    Data format currently supported for this layer is dim_ordering=\'th\' (Channel First).\n    Border mode currently supported for this layer is \'valid\'.\n    Bias will be included in this layer.\n    You can also use AtrousConv2D as an alias of this layer.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n    e.g. input_shape=(3, 128, 128) for 128x128 RGB pictures.\n\n    # Arguments\n    nb_filter: Number of convolution filters to use.\n    nb_row: Number of rows in the convolution kernel.\n    nb_col: Number of cols in the convolution kernel.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'glorot_uniform\'.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    border_mode: Only \'valid\' is supported for now.\n    subsample: Int tuple of length 2 corresponding to the step of the convolution in the\n               height and width dimension. Also called strides elsewhere. Default is (1, 1).\n    atrous_rate: Int tuple of length 2. Factor for kernel dilation.\n                 Also called filter_dilation elsewhere. Default is (1, 1).\n    dim_ordering: Format of input data. Only \'th\' (Channel First) is supported for now.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Only \'True\' is supported for now.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> atrousconv2d = AtrousConvolution2D(12, 4, 3, input_shape=(3, 64, 64))\n    creating: createZooKerasAtrousConvolution2D\n    """"""\n    def __init__(self, nb_filter, nb_row, nb_col, init=""glorot_uniform"",\n                 activation=None, border_mode=""valid"", subsample=(1, 1),\n                 atrous_rate=(1, 1), dim_ordering=""th"", W_regularizer=None,\n                 b_regularizer=None, bias=True, input_shape=None, **kwargs):\n        if border_mode != ""valid"":\n            raise ValueError(""For AtrousConvolution2D, ""\n                             ""only border_mode=\'valid\' is supported for now"")\n        if not bias:\n            raise ValueError(""For AtrousConvolution2D, only bias=True is supported for now"")\n        super(AtrousConvolution2D, self).__init__(None,\n                                                  nb_filter,\n                                                  nb_row,\n                                                  nb_col,\n                                                  init,\n                                                  activation,\n                                                  subsample,\n                                                  atrous_rate,\n                                                  dim_ordering,\n                                                  W_regularizer,\n                                                  b_regularizer,\n                                                  list(input_shape) if input_shape else None,\n                                                  **kwargs)\n\n\nclass SeparableConvolution2D(ZooKerasLayer):\n    """"""\n    Applies separable convolution operator for 2D inputs.\n    Separable convolutions consist in first performing a depthwise spatial convolution (which acts\n    on each input channel separately) followed by a pointwise convolution which mixes together the\n    resulting output channels. The depth_multiplier argument controls how many output channels are\n    generated per input channel in the depthwise step.\n    You can also use SeparableConv2D as an alias of this layer.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n    e.g. input_shape=(3, 128, 128) for 128x128 RGB pictures.\n\n    # Arguments\n    nb_filter: Number of convolution filters to use.\n    nb_row: Number of rows in the convolution kernel.\n    nb_col: Number of cols in the convolution kernel.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'glorot_uniform\'.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    border_mode: Either \'valid\' or \'same\'. Default is \'valid\'.\n    subsample: Int tuple of length 2 corresponding to the step of the convolution in the\n               height and width dimension. Also called strides elsewhere. Default is (1, 1).\n    depth_multiplier: How many output channel to use per input channel for the depthwise\n                      convolution step. Int. Default is 1.\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    depthwise_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                           applied to the depthwise weights matrices. Default is None.\n    pointwise_regularizer: An instance of [[Regularizer]], applied to the pointwise weights\n                           matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> separableconv2d = SeparableConvolution2D(12, 3, 4, input_shape=(3, 32, 32))\n    creating: createZooKerasSeparableConvolution2D\n    """"""\n    def __init__(self, nb_filter, nb_row, nb_col, init=""glorot_uniform"",\n                 activation=None, border_mode=""valid"", subsample=(1, 1), depth_multiplier=1,\n                 dim_ordering=""th"", depthwise_regularizer=None, pointwise_regularizer=None,\n                 b_regularizer=None, bias=True, input_shape=None, **kwargs):\n        super(SeparableConvolution2D, self).__init__(None,\n                                                     nb_filter,\n                                                     nb_row,\n                                                     nb_col,\n                                                     init,\n                                                     activation,\n                                                     border_mode,\n                                                     subsample,\n                                                     depth_multiplier,\n                                                     dim_ordering,\n                                                     depthwise_regularizer,\n                                                     pointwise_regularizer,\n                                                     b_regularizer,\n                                                     bias,\n                                                     list(input_shape) if input_shape else None,\n                                                     **kwargs)\n\n\nclass Convolution3D(ZooKerasLayer):\n    """"""\n    Applies convolution operator for filtering windows of three-dimensional inputs.\n    You can also use Conv3D as an alias of this layer.\n    Data format currently supported for this layer is dim_ordering=\'th\' (Channel First).\n    The input of this layer should be 5D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    nb_filter: Number of convolution filters to use.\n    kernel_dim1: Length of the first dimension in the convolution kernel.\n    kernel_dim2: Length of the second dimension in the convolution kernel.\n    kernel_dim3: Length of the third dimension in the convolution kernel.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'glorot_uniform\'.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    border_mode: Either \'valid\' or \'same\'. Default is \'valid\'.\n    subsample: Int tuple of length 3. Factor by which to subsample output.\n               Also called strides elsewhere. Default is (1, 1, 1).\n    dim_ordering: Format of input data. Only \'th\' (Channel First) is supported for now.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> conv3d = Convolution3D(32, 3, 4, 5, input_shape=(3, 64, 64, 64))\n    creating: createZooKerasConvolution3D\n    """"""\n    def __init__(self, nb_filter, kernel_dim1, kernel_dim2, kernel_dim3,\n                 init=""glorot_uniform"", activation=None, border_mode=""valid"",\n                 subsample=(1, 1, 1), dim_ordering=""th"", W_regularizer=None,\n                 b_regularizer=None, bias=True, input_shape=None, **kwargs):\n        super(Convolution3D, self).__init__(None,\n                                            nb_filter,\n                                            kernel_dim1,\n                                            kernel_dim2,\n                                            kernel_dim3,\n                                            init,\n                                            activation,\n                                            border_mode,\n                                            subsample,\n                                            dim_ordering,\n                                            W_regularizer,\n                                            b_regularizer,\n                                            bias,\n                                            list(input_shape) if input_shape else None,\n                                            **kwargs)\n\n\nclass UpSampling1D(ZooKerasLayer):\n    """"""\n    UpSampling layer for 1D inputs.\n    Repeats each temporal step \'length\' times along the time axis.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    length: Int. UpSampling factor. Default is 2.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> upsampling1d = UpSampling1D(length=3, input_shape=(3, 12))\n    creating: createZooKerasUpSampling1D\n    """"""\n    def __init__(self, length=2, input_shape=None, **kwargs):\n        super(UpSampling1D, self).__init__(None,\n                                           length,\n                                           list(input_shape) if input_shape else None,\n                                           **kwargs)\n\n\nclass UpSampling2D(ZooKerasLayer):\n    """"""\n    UpSampling layer for 2D inputs.\n    Repeats the rows and columns of the data by size[0] and size[1] respectively.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    size: Int tuple of length 2. UpSampling factors for rows and columns. Default is (2, 2).\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> upsampling2d = UpSampling2D(size=(1, 3), input_shape=(3, 16, 16))\n    creating: createZooKerasUpSampling2D\n    """"""\n    def __init__(self, size=(2, 2), dim_ordering=""th"", input_shape=None, **kwargs):\n        super(UpSampling2D, self).__init__(None,\n                                           size,\n                                           dim_ordering,\n                                           list(input_shape) if input_shape else None,\n                                           **kwargs)\n\n\nclass UpSampling3D(ZooKerasLayer):\n    """"""\n    UpSampling layer for 2D inputs.\n    Repeats the 1st, 2nd and 3rd dimensions of the data by\n    size[0], size[1] and size[2] respectively.\n    Data format currently supported for this layer is dim_ordering=\'th\' (Channel First).\n    The input of this layer should be 5D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    size: Int tuple of length 3. UpSampling factors for dim1, dim2 and dim3. Default is (2, 2, 2).\n    dim_ordering: Format of input data. Only \'th\' (Channel First) is supported for now.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> upsampling3d = UpSampling3D(size=(1, 2, 3), input_shape=(3, 16, 16, 16))\n    creating: createZooKerasUpSampling3D\n    """"""\n    def __init__(self, size=(2, 2, 2), dim_ordering=""th"", input_shape=None, **kwargs):\n        super(UpSampling3D, self).__init__(None,\n                                           size,\n                                           dim_ordering,\n                                           list(input_shape) if input_shape else None,\n                                           **kwargs)\n\n\nclass ZeroPadding1D(ZooKerasLayer):\n    """"""\n    Zero-padding layer for 1D input (e.g. temporal sequence).\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    padding: Int or int tuple of length 2.\n             If int, how many zeros to add both at the beginning and at the end of\n             the padding dimension.\n             If tuple of length 2, how many zeros to add in the order \'(left_pad, right_pad)\'.\n             Default is 1.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> zeropadding1d = ZeroPadding1D(padding=2, input_shape=(3, 6))\n    creating: createZooKerasZeroPadding1D\n    """"""\n    def __init__(self, padding=1, input_shape=None, **kwargs):\n        if isinstance(padding, int):\n            padding = (padding, padding)\n        super(ZeroPadding1D, self).__init__(None,\n                                            padding,\n                                            list(input_shape) if input_shape else None,\n                                            **kwargs)\n\n\nclass ZeroPadding2D(ZooKerasLayer):\n    """"""\n    Zero-padding layer for 2D input (e.g. picture).\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    padding: Int tuple of length 2 or length 4.\n             If tuple of length 2, how many zeros to add both at the beginning and\n             at the end of rows and cols.\n             If tuple of length 4, how many zeros to add in the order\n             \'(top_pad, bottom_pad, left_pad, right_pad)\'.\n             Default is (1, 1).\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> zeropadding2d = ZeroPadding2D(padding=(2, 1), input_shape=(2, 8, 8))\n    creating: createZooKerasZeroPadding2D\n    """"""\n    def __init__(self, padding=(1, 1), dim_ordering=""th"", input_shape=None, **kwargs):\n        if len(padding) == 2:\n            padding = (padding[0], padding[0], padding[1], padding[1])\n        super(ZeroPadding2D, self).__init__(None,\n                                            padding,\n                                            dim_ordering,\n                                            list(input_shape) if input_shape else None,\n                                            **kwargs)\n\n\nclass ZeroPadding3D(ZooKerasLayer):\n    """"""\n    Zero-padding layer for 3D data (spatial or spatio-temporal).\n    The input of this layer should be 5D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    padding: Int tuple of length 3. How many zeros to add at the beginning and\n             at the end of the 3 padding dimensions.\n             Symmetric padding will be applied to each dimension. Default is (1, 1, 1).\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> zeropadding3d = ZeroPadding3D(padding=(2, 1, 2), input_shape=(2, 8, 8, 10))\n    creating: createZooKerasZeroPadding3D\n    """"""\n    def __init__(self, padding=(1, 1, 1), dim_ordering=""th"", input_shape=None, **kwargs):\n        super(ZeroPadding3D, self).__init__(None,\n                                            padding,\n                                            dim_ordering,\n                                            list(input_shape) if input_shape else None,\n                                            **kwargs)\n\n\nclass Cropping1D(ZooKerasLayer):\n    """"""\n    Cropping layer for 1D input (e.g. temporal sequence).\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    cropping: Int tuple of length 2. How many units should be trimmed off at the beginning and\n              end of the cropping dimension. Default is (1, 1).\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> cropping1d = Cropping1D(cropping=(1, 2), input_shape=(8, 8))\n    creating: createZooKerasCropping1D\n    """"""\n    def __init__(self, cropping=(1, 1), input_shape=None, **kwargs):\n        super(Cropping1D, self).__init__(None,\n                                         cropping,\n                                         list(input_shape) if input_shape else None,\n                                         **kwargs)\n\n\nclass Cropping2D(ZooKerasLayer):\n    """"""\n    Cropping layer for 2D input (e.g. picture).\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    cropping: Int tuple of tuple of length 2. How many units should be trimmed off\n              at the beginning and end of the 2 cropping dimensions (i.e. height and width).\n              Default is ((0, 0), (0, 0)).\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> cropping2d = Cropping2D(cropping=((1, 2), (0, 1)), input_shape=(12, 12, 12))\n    creating: createZooKerasCropping2D\n    """"""\n    def __init__(self, cropping=((0, 0), (0, 0)), dim_ordering=""th"",\n                 input_shape=None, **kwargs):\n        super(Cropping2D, self).__init__(None,\n                                         cropping[0],\n                                         cropping[1],\n                                         dim_ordering,\n                                         list(input_shape) if input_shape else None,\n                                         **kwargs)\n\n\nclass Cropping3D(ZooKerasLayer):\n    """"""\n    Cropping layer for 3D data (e.g. spatial or spatio-temporal).\n    The input of this layer should be 5D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    cropping: Int tuple of tuple of length 3. How many units should be trimmed off\n              at the beginning and end of the 3 cropping dimensions\n              (i.e. kernel_dim1, kernel_dim2 and kernel_dim3).\n              Default is ((1, 1), (1, 1), (1, 1)).\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> cropping3d = Cropping3D(cropping=((0, 2), (1, 1), (3, 1)), input_shape=(4, 12, 12, 16))\n    creating: createZooKerasCropping3D\n    """"""\n    def __init__(self, cropping=((1, 1), (1, 1), (1, 1)), dim_ordering=""th"",\n                 input_shape=None, **kwargs):\n        super(Cropping3D, self).__init__(None,\n                                         cropping[0],\n                                         cropping[1],\n                                         cropping[2],\n                                         dim_ordering,\n                                         list(input_shape) if input_shape else None,\n                                         **kwargs)\n\n\nConv1D = Convolution1D\nConv2D = Convolution2D\nConv3D = Convolution3D\nDeconv2D = Deconvolution2D\nAtrousConv1D = AtrousConvolution1D\nAtrousConv2D = AtrousConvolution2D\nSeparableConv2D = SeparableConvolution2D\n'"
pyzoo/zoo/pipeline/api/keras/layers/convolutional_recurrent.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom ..engine.topology import ZooKerasLayer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass ConvLSTM2D(ZooKerasLayer):\n    """"""\n    Convolutional LSTM.\n    The convolution kernel for this layer is a square kernel with equal strides \'subsample\'.\n    The input of this layer should be 5D, i.e. (samples, time, channels, rows, cols) and\n    dim_ordering=\'th\' (Channel First) is expected.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    nb_filter: Number of convolution filters to use.\n    nb_row: Number of rows in the convolution kernel.\n    nb_col: Number of cols in the convolution kernel.\n            Should be equal to nb_row as for a square kernel.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is \'tanh\'.\n    inner_activation: String representation of the activation function for inner cells.\n                      Default is \'hard_sigmoid\'.\n    dim_ordering: Format of input data. Only \'th\' (Channel First) is supported for now.\n    subsample: Tuple of length 2. Factor by which to subsample output.\n               Also called strides elsewhere.\n               Only support subsample[0] equal to subsample[1] for now. Default is (1, 1).\n    border_mode: One of ""same"" or ""valid"". Also called padding elsewhere. Default is ""valid"".\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    U_regularizer: An instance of [[Regularizer]], applied the recurrent weights matrices.\n                   Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    return_sequences: Whether to return the full sequence or only return the last output\n                      in the output sequence. Default is False.\n    go_backwards: Whether the input sequence will be processed backwards. Default is False.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> convlstm2d = ConvLSTM2D(24, 3, 3, input_shape=(4, 32, 32, 32))\n    creating: createZooKerasConvLSTM2D\n    """"""\n    def __init__(self, nb_filter, nb_row, nb_col, activation=""tanh"",\n                 inner_activation=""hard_sigmoid"", dim_ordering=""th"", border_mode=""valid"",\n                 subsample=(1, 1), W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                 return_sequences=False, go_backwards=False, input_shape=None, **kwargs):\n        if nb_row != nb_col:\n            raise ValueError(""For ConvLSTM2D, only square kernel is supported for now"")\n        if border_mode != ""same"" and border_mode != ""valid"":\n            raise ValueError(""For ConvLSTM2D, only support border_mode as \'same\' and \'valid\'"")\n        if subsample[0] != subsample[1]:\n            raise ValueError(""For ConvLSTM2D, only equal strides is supported for now"")\n        super(ConvLSTM2D, self).__init__(None,\n                                         nb_filter,\n                                         nb_row,\n                                         activation,\n                                         inner_activation,\n                                         dim_ordering,\n                                         subsample[0],\n                                         border_mode,\n                                         W_regularizer,\n                                         U_regularizer,\n                                         b_regularizer,\n                                         return_sequences,\n                                         go_backwards,\n                                         list(input_shape) if input_shape else None,\n                                         **kwargs)\n\n\nclass ConvLSTM3D(ZooKerasLayer):\n    """"""\n    Convolutional LSTM for 3D input.\n    The convolution kernel for this layer is a cubic kernel with equal strides for all dimensions.\n    The input of this layer should be 6D, i.e. (samples, time, channels, dim1, dim2, dim3),\n    and \'CHANNEL_FIRST\' (dimOrdering=\'th\') is expected.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    nb_filter: Number of convolution filters to use.\n    nb_kernel: Length of the first, second and third dimensions in the convolution kernel.\n               Cubic kernel.\n    dim_ordering: Format of input data. Only \'th\' (Channel First) is supported for now.\n    border_mode: Only \'same\' is supported for now.\n    subsample: Tuple of length 3. Factor by which to subsample output.\n               Also called strides elsewhere. Default is (1, 1, 1).\n               Only support subsample[0] equal to subsample[1] equal to subsample[2] for now.\n    border_mode: One of ""same"" or ""valid"". Also called padding elsewhere. Default is ""valid"".\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    U_regularizer: An instance of [[Regularizer]], applied the recurrent weights matrices.\n                   Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    return_sequences: Whether to return the full sequence or only return the last output\n                      in the output sequence. Default is False.\n    go_backwards: Whether the input sequence will be processed backwards. Default is False.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> convlstm3d = ConvLSTM3D(10, 4, input_shape=(8, 4, 10, 32, 32))\n    creating: createZooKerasConvLSTM3D\n    """"""\n    def __init__(self, nb_filter, nb_kernel, dim_ordering=""th"", border_mode=""valid"",\n                 subsample=(1, 1, 1), W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                 return_sequences=False, go_backwards=False, input_shape=None, **kwargs):\n        if dim_ordering != ""th"":\n            raise ValueError(""For ConvLSTM3D, only dim_ordering=\'th\' is supported for now"")\n        if border_mode != ""same"" and border_mode != ""valid"":\n            raise ValueError(""For ConvLSTM3D, only support border_mode as \'same\' and \'valid\'"")\n        if subsample[0] != subsample[1] or subsample[1] != subsample[2]:\n            raise ValueError(""For ConvLSTM3D, only equal strides is supported for now"")\n        super(ConvLSTM3D, self).__init__(None,\n                                         nb_filter,\n                                         nb_kernel,\n                                         subsample[0],\n                                         border_mode,\n                                         W_regularizer,\n                                         U_regularizer,\n                                         b_regularizer,\n                                         return_sequences,\n                                         go_backwards,\n                                         list(input_shape) if input_shape else None,\n                                         **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras/layers/core.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom bigdl.util.common import INTMIN\nfrom ..engine.topology import ZooKerasLayer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass Masking(ZooKerasLayer):\n    """"""\n    Use a mask value to skip timesteps for a sequence.\n    Masks a sequence by using a mask value to skip timesteps.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    mask_value: Float, mask value. For each timestep in the input (the second dimension),\n                if all values in the input at that timestep are equal to \'mask_value\',\n                then the timestep will masked (skipped) in all downstream layers.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> masking = Masking(0.3, input_shape=(6, 8))\n    creating: createZooKerasMasking\n    """"""\n    def __init__(self, mask_value=0.0, input_shape=None, **kwargs):\n        super(Masking, self).__init__(None,\n                                      float(mask_value),\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n\n\nclass Dropout(ZooKerasLayer):\n    """"""\n    Applies Dropout to the input by randomly setting a fraction \'p\' of input units to 0 at each\n    update during training time in order to prevent overfitting.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    p: Fraction of the input units to drop. Float between 0 and 1.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> dropout = Dropout(0.25, input_shape=(2, 3))\n    creating: createZooKerasDropout\n    """"""\n    def __init__(self, p, input_shape=None, **kwargs):\n        super(Dropout, self).__init__(None,\n                                      float(p),\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n\n\nclass SpatialDropout1D(ZooKerasLayer):\n    """"""\n    Spatial 1D version of Dropout.\n    This version performs the same function as Dropout, however it drops entire 1D feature maps\n    instead of individual elements. If adjacent frames within feature maps are strongly correlated\n    (as is normally the case in early convolution layers) then regular dropout will not regularize\n    the activations and will otherwise just result in an effective learning rate decrease.\n    In this case, SpatialDropout1D will help promote independence between feature maps and\n    should be used instead.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    p: Fraction of the input units to drop. Float between 0 and 1.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> spatialdropout1d = SpatialDropout1D(0.4, input_shape=(10, 12))\n    creating: createZooKerasSpatialDropout1D\n    """"""\n    def __init__(self, p=0.5, input_shape=None, **kwargs):\n        super(SpatialDropout1D, self).__init__(None,\n                                               float(p),\n                                               list(input_shape) if input_shape else None,\n                                               **kwargs)\n\n\nclass SpatialDropout2D(ZooKerasLayer):\n    """"""\n    Spatial 2D version of Dropout.\n    This version performs the same function as Dropout, however it drops entire 2D feature maps\n    instead of individual elements. If adjacent pixels within feature maps are strongly correlated\n    (as is normally the case in early convolution layers) then regular dropout will not regularize\n    the activations and will otherwise just result in an effective learning rate decrease.\n    In this case, SpatialDropout2D will help promote independence between feature maps and\n    should be used instead.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    p: Fraction of the input units to drop. Float between 0 and 1.\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> spatialdropout2d = SpatialDropout2D(0.25, input_shape=(5, 12, 12))\n    creating: createZooKerasSpatialDropout2D\n    """"""\n    def __init__(self, p=0.5, dim_ordering=""th"", input_shape=None, **kwargs):\n        super(SpatialDropout2D, self).__init__(None,\n                                               float(p),\n                                               dim_ordering,\n                                               list(input_shape) if input_shape else None,\n                                               **kwargs)\n\n\nclass SpatialDropout3D(ZooKerasLayer):\n    """"""\n    Spatial 3D version of Dropout.\n    This version performs the same function as Dropout, however it drops entire 3D feature maps\n    instead of individual elements. If adjacent voxels within feature maps are strongly correlated\n    (as is normally the case in early convolution layers) then regular dropout will not regularize\n    the activations and will otherwise just result in an effective learning rate decrease.\n    In this case, SpatialDropout3D will help promote independence between feature maps and\n    should be used instead.\n    The input of this layer should be 5D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    p: Fraction of the input units to drop. Float between 0 and 1.\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> spatialdropout3d = SpatialDropout3D(0.6, input_shape=(4, 12, 12, 16))\n    creating: createZooKerasSpatialDropout3D\n    """"""\n    def __init__(self, p=0.5, dim_ordering=""th"", input_shape=None, **kwargs):\n        super(SpatialDropout3D, self).__init__(None,\n                                               float(p),\n                                               dim_ordering,\n                                               list(input_shape) if input_shape else None,\n                                               **kwargs)\n\n\nclass Activation(ZooKerasLayer):\n    """"""\n    Simple activation function to be applied to the output.\n    Available activations: \'tanh\', \'relu\', \'sigmoid\', \'softmax\', \'softplus\', \'softsign\',\n                           \'hard_sigmoid\', \'linear\', \'relu6\', \'tanh_shrink\', \'softmin\',\n                           \'log_sigmoid\' and \'log_softmax\'.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    activation: Name of the activation function as string.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> activation = Activation(""relu"", input_shape=(3, 4))\n    creating: createZooKerasActivation\n    """"""\n    def __init__(self, activation, input_shape=None, **kwargs):\n        super(Activation, self).__init__(None,\n                                         activation,\n                                         list(input_shape) if input_shape else None,\n                                         **kwargs)\n\n\nclass Reshape(ZooKerasLayer):\n    """"""\n    Reshapes an output to a certain shape.\n    Supports shape inference by allowing one -1 in the target shape.\n    For example, if input_shape = (2, 3, 4), target_shape = (3, -1),\n    then output_shape will be (3, 8).\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    target_shape: A shape tuple. The target shape that you desire to have.\n                  Batch dimension should be excluded.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> reshape = Reshape((2, 10), input_shape=(5, 4))\n    creating: createZooKerasReshape\n    """"""\n    def __init__(self, target_shape, input_shape=None, **kwargs):\n        super(Reshape, self).__init__(None,\n                                      target_shape,\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n\n\nclass Permute(ZooKerasLayer):\n    """"""\n    Permutes the dimensions of the input according to a given pattern.\n    Useful for connecting RNNs and convnets together.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    dims: Tuple of int. Permutation pattern, does not include the batch dimension.\n          Indexing starts at 1.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> permute = Permute((2, 1, 3), input_shape=(3, 4, 5))\n    creating: createZooKerasPermute\n    """"""\n    def __init__(self, dims, input_shape=None, **kwargs):\n        super(Permute, self).__init__(None,\n                                      dims,\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n\n\nclass Flatten(ZooKerasLayer):\n    """"""\n    Flattens the input without affecting the batch size.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> flatten = Flatten(input_shape=(3, 10, 2))\n    creating: createZooKerasFlatten\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(Flatten, self).__init__(None,\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n\n\nclass RepeatVector(ZooKerasLayer):\n    """"""\n    Repeats the input n times.\n    The input of this layer should be 2D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    n: Repetition factor. Int.\n    input_dim: Dimensionality of the input. Alternatively, you can specify \'input_shape\'\n               when using this layer as the first layer.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> repeatvector = RepeatVector(5, input_shape=(3, ))\n    creating: createZooKerasRepeatVector\n    """"""\n    def __init__(self, n, input_dim=None, input_shape=None, **kwargs):\n        if input_dim:\n            input_shape = (input_dim, )\n        super(RepeatVector, self).__init__(None,\n                                           n,\n                                           list(input_shape) if input_shape else None,\n                                           **kwargs)\n\n\nclass Dense(ZooKerasLayer):\n    """"""\n    A densely-connected NN layer.\n    The most common input is 2D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    output_dim: The size of output dimension.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'glorot_uniform\'.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_dim: Dimensionality of the input for 2D input. For nD input, you can alternatively\n               specify \'input_shape\' when using this layer as the first layer.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> dense = Dense(10, input_dim=8, name=""dense1"")\n    creating: createZooKerasDense\n    """"""\n    def __init__(self, output_dim, init=""glorot_uniform"", limits=None, activation=None,\n                 W_regularizer=None, b_regularizer=None,\n                 bias=True, input_dim=None, input_shape=None, **kwargs):\n        if input_dim:\n            input_shape = (input_dim, )\n        super(Dense, self).__init__(None,\n                                    output_dim,\n                                    init,\n                                    list(limits) if limits else None,\n                                    activation,\n                                    W_regularizer,\n                                    b_regularizer,\n                                    bias,\n                                    list(input_shape) if input_shape else None,\n                                    **kwargs)\n\n\nclass GetShape(ZooKerasLayer):\n    """"""\n    GetShape gets the value of input_shape.\n    For example, if input_shape = (2, 3, 4),\n    then output will be (2, 3, 4).\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    >>> getShape = GetShape(input_shape=(3, 4, 5))\n    creating: createZooKerasGetShape\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(GetShape, self).__init__(None,\n                                       list(input_shape) if input_shape else None,\n                                       **kwargs)\n\n\nclass SparseDense(ZooKerasLayer):\n    """"""\n    SparseDense is the sparse version of layer Dense. SparseDense has two different from Dense:\n    firstly, SparseDense\'s input Tensor is a SparseTensor. Secondly, SparseDense doesn\'t backward\n    gradient to next layer in the backpropagation by default, as the gradInput of SparseDense is\n    useless and very big in most cases.\n\n    But, considering model like Wide&Deep, we provide backwardStart and backwardLength to backward\n    part of the gradient to next layer.\n\n    The most common input is 2D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    inputShape (a Single Shape, does not include the batch dimension).\n\n    # Arguments\n    output_dim: The size of output dimension.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'glorot_uniform\'.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    backward_start: backward start index, counting from 1.\n    backward_length: backward length.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> sparseDense = SparseDense(10, input_shape=(10, 4), name=""sparseDense"")\n    creating: createZooKerasSparseDense\n    """"""\n\n    def __init__(self, output_dim, init=""glorot_uniform"", activation=None,\n                 W_regularizer=None, b_regularizer=None, backward_start=-1,\n                 backward_length=-1, init_weight=None, init_bias=None,\n                 init_grad_weight=None, init_grad_bias=None,\n                 bias=True, input_shape=None, **kwargs):\n        super(SparseDense, self).__init__(None,\n                                          output_dim,\n                                          init,\n                                          activation,\n                                          W_regularizer,\n                                          b_regularizer,\n                                          backward_start,\n                                          backward_length,\n                                          init_weight,\n                                          init_bias,\n                                          init_grad_weight,\n                                          init_grad_bias,\n                                          bias,\n                                          list(input_shape) if input_shape else None,\n                                          **kwargs)\n\n\nclass MaxoutDense(ZooKerasLayer):\n    """"""\n    A dense maxout layer that takes the element-wise maximum of linear layers.\n    This allows the layer to learn a convex, piecewise linear activation function over the inputs.\n    The input of this layer should be 2D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    output_dim: The size of output dimension.\n    nb_feature: Number of Dense layers to use internally. Int. Default is 4.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_dim: Dimensionality of the input. Alternatively, you can specify \'input_shape\'\n               when using this layer as the first layer.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> maxoutdense = MaxoutDense(6, input_shape=(10, ))\n    creating: createZooKerasMaxoutDense\n    """"""\n    def __init__(self, output_dim, nb_feature=4, W_regularizer=None, b_regularizer=None,\n                 bias=True, input_dim=None, input_shape=None, **kwargs):\n        if input_dim:\n            input_shape = (input_dim, )\n        super(MaxoutDense, self).__init__(None,\n                                          output_dim,\n                                          nb_feature,\n                                          W_regularizer,\n                                          b_regularizer,\n                                          bias,\n                                          list(input_shape) if input_shape else None,\n                                          **kwargs)\n\n\nclass Highway(ZooKerasLayer):\n    """"""\n    Densely connected highway network. Highway layers are a natural extension of LSTMs\n    to feedforward networks.\n    The input of this layer should be 2D, i.e. (batch, input dim).\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_dim: Dimensionality of the input. Alternatively, you can specify \'input_shape\'\n               when using this layer as the first layer.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> highway = Highway(activation=\'relu\', input_shape=(8, ))\n    creating: createZooKerasHighway\n    """"""\n    def __init__(self, activation=None, W_regularizer=None, b_regularizer=None,\n                 bias=True, input_dim=None, input_shape=None, **kwargs):\n        if input_dim:\n            input_shape = (input_dim, )\n        super(Highway, self).__init__(None,\n                                      activation,\n                                      W_regularizer,\n                                      b_regularizer,\n                                      bias,\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n\n\nclass Max(ZooKerasLayer):\n    """"""\n    Applies a max operation over dimension `dim`\n\n    # Arguments\n    dim: max along this dimension\n    num_input_dims: Optional. If in a batch model, set to the inputDims.\n    return_value: Optional. Config whether return value or indices\n    input_shape: A shape tuple, not including batch.\n\n    >>> max = Max(dim=1, input_shape=(3, 5))\n    creating: createZooKerasMax\n    """"""\n    def __init__(self, dim, num_input_dims=INTMIN, return_value=True, input_shape=None, **kwargs):\n        super(Max, self).__init__(None,\n                                  dim,\n                                  num_input_dims,\n                                  return_value,\n                                  list(input_shape) if input_shape else None,\n                                  **kwargs)\n\n\nclass ExpandDim(ZooKerasLayer):\n    """"""\n    Expand_dim is an improved layer to suuport 1D input.\n    For example, if we get an 1D input with shape(3),\n    we will return the shape(1, 3) after we use expand_dim(0, input).\n    # Arguments\n    dim: The specified axis to expand dimension on.\n    input_shape: A shape tuple, not including batch.\n\n    >>> expandDim = ExpandDim(dim=0, input_shape=(3, 2))\n    creating: createZooKerasExpandDim\n    """"""\n    def __init__(self, dim, input_shape=None, **kwargs):\n        super(ExpandDim, self).__init__(None,\n                                        dim,\n                                        list(input_shape) if input_shape else None,\n                                        **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras/layers/embeddings.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom bigdl.util.common import JTensor\nfrom zoo.common.utils import callZooFunc\nfrom ..engine.topology import ZooKerasLayer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass Embedding(ZooKerasLayer):\n    """"""\n    Turn positive integers (indexes) into dense vectors of fixed size.\n    The input of this layer should be 2D.\n\n    This layer can only be used as the first layer in a model, you need to provide the argument\n    input_length (an integer) or input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_dim: Size of the vocabulary. Int > 0.\n    output_dim: Dimension of the dense embedding. Int > 0.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'uniform\'.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the embedding matrix. Default is None.\n    weights: Initial weights set to this layer, which should be a numpy array of\n             size (inputDim, outputDim). Default is None and in this case weights are\n             initialized by the initialization method specified by \'init\'.\n             Otherwise, \'weights\' will override \'init\' to take effect.\n    trainable: Whether this layer is trainable or not. Default is True.\n    input_length: Positive int. The sequence length of each input.\n    mask_zero: if maskZero is set to true, the input whose value equals `paddingValue`\n    the output will be masked to zero vector.\n    padding_value: padding value, default 0\n    zero_based_id: default True and input should be 0 based. Otherwise need to be 1 base\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> embedding = Embedding(1000, 32, input_length=10, name=""embedding1"")\n    creating: createZooKerasEmbedding\n\n    >>> import numpy as np\n    >>> embedding = Embedding(10, 200, weights=np.random.random([10, 200]), input_length=10)\n    creating: createZooKerasEmbedding\n    """"""\n\n    def __init__(self, input_dim, output_dim, init=""uniform"", weights=None, trainable=True,\n                 input_length=None, W_regularizer=None, input_shape=None, mask_zero=False,\n                 padding_value=0, zero_based_id=True, **kwargs):\n        if input_length:\n            input_shape = (input_length,)\n        super(Embedding, self).__init__(None,\n                                        input_dim,\n                                        output_dim,\n                                        init,\n                                        JTensor.from_ndarray(weights),\n                                        trainable,\n                                        W_regularizer,\n                                        list(input_shape) if input_shape else None,\n                                        mask_zero,\n                                        padding_value,\n                                        zero_based_id,\n                                        **kwargs)\n\n\nclass WordEmbedding(ZooKerasLayer):\n    """"""\n    Embedding layer that directly loads pre-trained word vectors as weights.\n    Turn non-negative integers (indices) into dense vectors of fixed size.\n    Currently only GloVe embedding is supported.\n    The input of this layer should be 2D.\n\n    This layer can only be used as the first layer in a model, you need to provide the argument\n    input_length (an integer) or input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    embedding_file: The path to the embedding file.\n                    Currently the following GloVe files are supported:\n                    ""glove.6B.50d.txt"", ""glove.6B.100d.txt"", ""glove.6B.200d.txt""\n                    ""glove.6B.300d.txt"", ""glove.42B.300d.txt"", ""glove.840B.300d.txt"".\n                    You can download them from: https://nlp.stanford.edu/projects/glove/.\n    word_index: Dictionary of word (string) and its corresponding index (int).\n                The index is supposed to start from 1 with 0 reserved for unknown words.\n                During the prediction, if you have words that are not in the word_index\n                for the training, you can map them to index 0.\n                Default is None. In this case, all the words in the embedding_file will\n                be taken into account and you can call\n                WordEmbedding.get_word_index(embedding_file) to retrieve the dictionary.\n    trainable: To configure whether the weights of this layer will be updated or not.\n               Only False is supported for now.\n    input_length: Positive int. The sequence length of each input.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n    """"""\n\n    def __init__(self, embedding_file, word_index=None, trainable=False, input_length=None,\n                 input_shape=None, **kwargs):\n        if input_length:\n            input_shape = (input_length,)\n        super(WordEmbedding, self).__init__(None,\n                                            embedding_file,\n                                            word_index,\n                                            trainable,\n                                            list(input_shape) if input_shape else None,\n                                            **kwargs)\n\n    @staticmethod\n    def get_word_index(embedding_file, bigdl_type=""float""):\n        """"""\n        Get the full wordIndex map from the given embedding_file.\n\n        # Arguments\n        embedding_file: The path to the embedding file.\n                        Currently only the following GloVe files are supported:\n                        ""glove.6B.50d.txt"", ""glove.6B.100d.txt"", ""glove.6B.200d.txt""\n                        ""glove.6B.300d.txt"", ""glove.42B.300d.txt"", ""glove.840B.300d.txt"".\n                        You can download them from: https://nlp.stanford.edu/projects/glove/.\n\n        # Return\n        Dictionary of word (string) and its corresponding index (int) obtained from\n        the given embedding file.\n        """"""\n        return callZooFunc(bigdl_type, ""wordEmbeddingGetWordIndex"",\n                           embedding_file)\n\n\ndef prepare_embedding(embedding_file, word_index=None,\n                      randomize_unknown=False, normalize=False):\n    """"""\n    Prepare embedding weights from embedding_file given word_index.\n\n    # Arguments\n    embedding_file and word_index: See WordEmbedding.\n    randomize_unknown: Boolean. Whether to randomly initialize words that don\'t exist in\n                       embedding_file. Default is False and in this case corresponding entries\n                       to unknown words will be zero vectors.\n    normalize: Boolean. Whether to normalize word vectors. Default is False.\n\n    # Return\n    Pretrained embedding weights as a numpy array.\n    """"""\n    return callZooFunc(""float"", ""prepareEmbedding"",\n                       embedding_file,\n                       word_index,\n                       randomize_unknown,\n                       normalize).to_ndarray()\n\n\nclass SparseEmbedding(ZooKerasLayer):\n    """"""\n    SparseEmbedding is the sparse version of layer Embedding.\n\n    The input of SparseEmbedding should be a 2D SparseTensor or two 2D sparseTensors.\n    If the input is a SparseTensor, the values are positive integer ids,\n    values in each row of this SparseTensor will be turned into a dense vector.\n    If the input is two SparseTensors, the first tensor should be the integer ids, just\n    like the SparseTensor input. And the second tensor is the corresponding\n    weights of the integer ids.\n\n    This layer can only be used as the first layer in a model, you need to provide the argument\n    inputShape (a Single Shape, does not include the batch dimension).\n\n    # Arguments\n    input_dim: Size of the vocabulary. Int > 0.\n    output_dim: Dimension of the dense embedding. Int >= 0.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'uniform\'.\n    combiner: A string specifying the reduce type.\n              Currently ""mean"", ""sum"", ""sqrtn"" is supported.\n    max_norm: If provided, each embedding is normalized to have l2 norm equal to\n               maxNorm before combining.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the embedding matrix. Default is None.\n    input_shape: A Single Shape, does not include the batch dimension.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> sparse_embedding = SparseEmbedding(input_dim=10, output_dim=4, input_shape=(10, ))\n    creating: createZooKerasSparseEmbedding\n    """"""\n\n    def __init__(self, input_dim, output_dim, combiner=""sum"", max_norm=-1.0, init=""uniform"",\n                 W_regularizer=None, input_shape=None, **kwargs):\n        super(SparseEmbedding, self).__init__(None,\n                                              input_dim,\n                                              output_dim,\n                                              combiner,\n                                              max_norm,\n                                              init,\n                                              W_regularizer,\n                                              list(input_shape) if input_shape else None,\n                                              **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras/layers/local.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom ..engine.topology import ZooKerasLayer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass LocallyConnected1D(ZooKerasLayer):\n    """"""\n    Locally-connected layer for 1D inputs which works similarly to the TemporalConvolution\n    layer, except that weights are unshared, that is, a different set of filters is applied\n    at each different patch of the input.\n    Border mode currently supported for this layer is \'valid\'.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    nb_filter: Dimensionality of the output.\n    filter_length: The extension (spatial or temporal) of each filter.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    border_mode: Only \'valid\' is supported for now.\n    subsample_length: Factor by which to subsample output. Int. Default is 1.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> locallyconnected1d = LocallyConnected1D(6, 3, input_shape=(8, 12))\n    creating: createZooKerasLocallyConnected1D\n    """"""\n    def __init__(self, nb_filter, filter_length, activation=None, border_mode=""valid"",\n                 subsample_length=1, W_regularizer=None, b_regularizer=None,\n                 bias=True, input_shape=None, **kwargs):\n        if border_mode != ""valid"":\n            raise ValueError(""For LocallyConnected1D, ""\n                             ""only border_mode=\'valid\' is supported for now"")\n        super(LocallyConnected1D, self).__init__(None,\n                                                 nb_filter,\n                                                 filter_length,\n                                                 activation,\n                                                 subsample_length,\n                                                 W_regularizer,\n                                                 b_regularizer,\n                                                 bias,\n                                                 list(input_shape) if input_shape else None,\n                                                 **kwargs)\n\n\nclass LocallyConnected2D(ZooKerasLayer):\n    """"""\n    Locally-connected layer for 2D inputs that works similarly to the SpatialConvolution\n    layer, except that weights are unshared, that is, a different set of filters is applied\n    at each different patch of the input.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    nb_filter: Number of convolution filters to use.\n    nb_row: Number of rows in the convolution kernel.\n    nb_col: Number of cols in the convolution kernel.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    border_mode: Either \'valid\' or \'same\'. Default is \'valid\'.\n    subsample: Int tuple of length 2 corresponding to the step of the convolution in the\n               height and width dimension. Also called strides elsewhere. Default is (1, 1).\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> locallyconnected2d = LocallyConnected2D(12, 3, 4, input_shape=(3, 128, 128))\n    creating: createZooKerasLocallyConnected2D\n    """"""\n    def __init__(self, nb_filter, nb_row, nb_col, activation=None,\n                 border_mode=""valid"", subsample=(1, 1), dim_ordering=""th"",\n                 W_regularizer=None, b_regularizer=None, bias=True,\n                 input_shape=None, **kwargs):\n        super(LocallyConnected2D, self).__init__(None,\n                                                 nb_filter,\n                                                 nb_row,\n                                                 nb_col,\n                                                 activation,\n                                                 border_mode,\n                                                 subsample,\n                                                 dim_ordering,\n                                                 W_regularizer,\n                                                 b_regularizer,\n                                                 bias,\n                                                 list(input_shape) if input_shape else None,\n                                                 **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras/layers/noise.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom ..engine.topology import ZooKerasLayer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass GaussianNoise(ZooKerasLayer):\n    """"""\n    Apply additive zero-centered Gaussian noise.\n    This is useful to mitigate overfitting (you could see it as a form of random data augmentation).\n    Gaussian Noise is a natural choice as corruption process for real valued inputs.\n    As it is a regularization layer, it is only active at training time.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    sigma: Float, standard deviation of the noise distribution.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> gaussiannoise = GaussianNoise(0.45, input_shape=(3, 4, 5), name=""gaussiannoise1"")\n    creating: createZooKerasGaussianNoise\n    """"""\n    def __init__(self, sigma, input_shape=None, **kwargs):\n        super(GaussianNoise, self).__init__(None,\n                                            float(sigma),\n                                            list(input_shape) if input_shape else None,\n                                            **kwargs)\n\n\nclass GaussianDropout(ZooKerasLayer):\n    """"""\n    Apply multiplicative 1-centered Gaussian noise.\n    As it is a regularization layer, it is only active at training time.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    p: Drop probability. Float between 0 and 1.\n       The multiplicative noise will have standard deviation \'sqrt(p/(1-p))\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> gaussiandropout = GaussianDropout(0.45, input_shape=(4, 8))\n    creating: createZooKerasGaussianDropout\n    """"""\n    def __init__(self, p, input_shape=None, **kwargs):\n        super(GaussianDropout, self).__init__(None,\n                                              float(p),\n                                              list(input_shape) if input_shape else None,\n                                              **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras/layers/normalization.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom ..engine.topology import ZooKerasLayer\nfrom bigdl.util.common import JTensor\nfrom zoo.common.utils import callZooFunc\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass BatchNormalization(ZooKerasLayer):\n    """"""\n    Batch normalization layer.\n    Normalize the activations of the previous layer at each batch, i.e. applies a transformation\n    that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n    It is a feature-wise normalization, each feature map in the input will be normalized separately.\n    The input of this layer should be 4D or 2D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    epsilon: Small float > 0. Fuzz parameter. Default is 0.001.\n    momentum: Float. Momentum in the computation of the exponential average of the mean and\n              standard deviation of the data, for feature-wise normalization. Default is 0.99.\n    beta_init: Name of the initialization function for shift parameter. Default is \'zero\'.\n    gamma_init: Name of the initialization function for scale parameter. Default is \'one\'.\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'. For \'th\', axis along which to normalize is 1.\n                  For \'tf\', axis is 3.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> batchnormalization = BatchNormalization(input_shape=(3, 12, 12), name=""bn1"")\n    creating: createZooKerasBatchNormalization\n    """"""\n\n    def __init__(self, epsilon=0.001, mode=0, axis=1, momentum=0.99, beta_init=""zero"",\n                 gamma_init=""one"", dim_ordering=""th"", input_shape=None, **kwargs):\n        if mode != 0:\n            raise ValueError(""For BatchNormalization, only mode=0 is supported for now"")\n        if dim_ordering == ""th"" and axis != 1:\n            raise ValueError(""For BatchNormalization with th dim ordering, ""\n                             ""only axis=1 is supported for now"")\n        if dim_ordering == ""tf"" and axis != -1 and axis != 3:\n            raise ValueError(""For BatchNormalization with tf dim ordering, ""\n                             ""only axis=-1 is supported for now"")\n        super(BatchNormalization, self).__init__(None,\n                                                 float(epsilon),\n                                                 float(momentum),\n                                                 beta_init,\n                                                 gamma_init,\n                                                 dim_ordering,\n                                                 list(input_shape) if input_shape else None,\n                                                 **kwargs)\n\n    def set_running_mean(self, running_mean):\n        """"""\n        Set the running mean of the BatchNormalization layer.\n        :param running_mean: a Numpy array.\n        """"""\n        callZooFunc(self.bigdl_type, ""setRunningMean"",\n                    self.value, JTensor.from_ndarray(running_mean))\n        return self\n\n    def set_running_std(self, running_std):\n        """"""\n        Set the running variance of the BatchNormalization layer.\n        :param running_std: a Numpy array.\n        """"""\n        callZooFunc(self.bigdl_type, ""setRunningStd"",\n                    self.value, JTensor.from_ndarray(running_std))\n        return self\n\n    def get_running_mean(self):\n        """"""\n        Get the running meaning of the BatchNormalization layer.\n        """"""\n        return callZooFunc(self.bigdl_type, ""getRunningMean"",\n                           self.value).to_ndarray()\n\n    def get_running_std(self):\n        """"""\n        Get the running variance of the BatchNormalization layer.\n        """"""\n        return callZooFunc(self.bigdl_type, ""getRunningStd"",\n                           self.value).to_ndarray()\n'"
pyzoo/zoo/pipeline/api/keras/layers/pooling.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom ..engine.topology import ZooKerasLayer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass MaxPooling1D(ZooKerasLayer):\n    """"""\n    Applies max pooling operation for temporal data.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    pool_length: Size of the region to which max pooling is applied. Integer. Default is 2.\n    strides: Factor by which to downscale. 2 will halve the input.\n             Default is None, and in this case it will be equal to pool_length..\n    border_mode: Either \'valid\' or \'same\'. Default is \'valid\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> maxpooling1d = MaxPooling1D(3, input_shape=(3, 24))\n    creating: createZooKerasMaxPooling1D\n    """"""\n    def __init__(self, pool_length=2, stride=None, border_mode=""valid"",\n                 input_shape=None, pad=0, **kwargs):\n        if not stride:\n            stride = -1\n        super(MaxPooling1D, self).__init__(None,\n                                           pool_length,\n                                           stride,\n                                           border_mode,\n                                           list(input_shape) if input_shape else None,\n                                           pad,\n                                           **kwargs)\n\n\nclass AveragePooling1D(ZooKerasLayer):\n    """"""\n    Applies average pooling operation for temporal data.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    pool_length: Size of the region to which max pooling is applied.\n    strides: Factor by which to downscale. 2 will halve the input.\n             Default is None, and in this case it will be equal to pool_length..\n    border_mode: Either \'valid\' or \'same\'. Default is \'valid\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> averagepooling1d = AveragePooling1D(input_shape=(3, 24))\n    creating: createZooKerasAveragePooling1D\n    """"""\n    def __init__(self, pool_length=2, stride=None, border_mode=""valid"",\n                 input_shape=None, **kwargs):\n        if not stride:\n            stride = -1\n        super(AveragePooling1D, self).__init__(None,\n                                               pool_length,\n                                               stride,\n                                               border_mode,\n                                               list(input_shape) if input_shape else None,\n                                               **kwargs)\n\n\nclass MaxPooling2D(ZooKerasLayer):\n    """"""\n    Applies max pooling operation for spatial data.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    pool_size: Int tuple of length 2 corresponding to the downscale vertically and horizontally.\n               Default is (2, 2), which will halve the image in each dimension.\n    strides: Int tuple of length 2. Stride values.\n             Default is None, and in this case it will be equal to pool_size.\n    border_mode: Either \'valid\' or \'same\'. Default is \'valid\'.\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> maxpooling2d = MaxPooling2D((2, 2), input_shape=(3, 32, 32), name=""maxpooling2d_1"")\n    creating: createZooKerasMaxPooling2D\n    """"""\n    def __init__(self, pool_size=(2, 2), strides=None,\n                 border_mode=""valid"", dim_ordering=""th"",\n                 input_shape=None, pads=None, **kwargs):\n        super(MaxPooling2D, self).__init__(None,\n                                           pool_size,\n                                           strides,\n                                           border_mode,\n                                           dim_ordering,\n                                           list(input_shape) if input_shape else None,\n                                           pads,\n                                           **kwargs)\n\n\nclass AveragePooling2D(ZooKerasLayer):\n    """"""\n    Applies average pooling operation for spatial data.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    pool_size: Int tuple of length 2 corresponding to the downscale vertically and horizontally.\n               Default is (2, 2), which will halve the image in each dimension.\n    strides: Int tuple of length 2. Stride values.\n             Default is None, and in this case it will be equal to pool_size.\n    border_mode: Either \'valid\' or \'same\'. Default is \'valid\'.\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> averagepooling2d = AveragePooling2D((1, 2), input_shape=(2, 28, 32))\n    creating: createZooKerasAveragePooling2D\n    """"""\n    def __init__(self, pool_size=(2, 2), strides=None, border_mode=""valid"",\n                 dim_ordering=""th"", input_shape=None, pads=None, count_include_pad=False, **kwargs):\n        super(AveragePooling2D, self).__init__(None,\n                                               pool_size,\n                                               strides,\n                                               border_mode,\n                                               dim_ordering,\n                                               list(input_shape) if input_shape else None,\n                                               pads,\n                                               count_include_pad,\n                                               **kwargs)\n\n\nclass MaxPooling3D(ZooKerasLayer):\n    """"""\n    Applies max pooling operation for 3D data (spatial or spatio-temporal).\n    Data format currently supported for this layer is dim_ordering=\'th\' (Channel First).\n    Border mode currently supported for this layer is \'valid\'.\n    The input of this layer should be 5D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    pool_size: Int tuple of length 3. Factors by which to downscale (dim1, dim2, dim3).\n               Default is (2, 2, 2), which will halve the image in each dimension.\n    strides: Int tuple of length 3. Stride values.\n             Default is None, and in this case it will be equal to pool_size.\n    border_mode: Only \'valid\' is supported for now.\n    dim_ordering: Format of input data. Only \'th\' (Channel First) is supported for now.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> maxpooling3d = MaxPooling3D((2, 1, 3), input_shape=(3, 32, 32, 32))\n    creating: createZooKerasMaxPooling3D\n    """"""\n    def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode=""valid"",\n                 dim_ordering=""th"", input_shape=None, **kwargs):\n        if border_mode != ""valid"":\n            raise ValueError(""For MaxPooling3D, only border_mode=\'valid\' is supported for now"")\n        super(MaxPooling3D, self).__init__(None,\n                                           pool_size,\n                                           strides,\n                                           dim_ordering,\n                                           list(input_shape) if input_shape else None,\n                                           **kwargs)\n\n\nclass AveragePooling3D(ZooKerasLayer):\n    """"""\n    Applies average pooling operation for 3D data (spatial or spatio-temporal).\n    Data format currently supported for this layer is dim_ordering=\'th\' (Channel First).\n    Border mode currently supported for this layer is \'valid\'.\n    The input of this layer should be 5D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    pool_size: Int tuple of length 3. Factors by which to downscale (dim1, dim2, dim3).\n               Default is (2, 2, 2), which will halve the image in each dimension.\n    strides: Int tuple of length 3. Stride values.\n             Default is None, and in this case it will be equal to pool_size.\n    border_mode: Only \'valid\' is supported for now.\n    dim_ordering: Format of input data. Only \'th\' (Channel First) is supported for now.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> averagepooling3d = AveragePooling3D((1, 1, 2), input_shape=(3, 28, 32, 36))\n    creating: createZooKerasAveragePooling3D\n    """"""\n    def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode=""valid"",\n                 dim_ordering=""th"", input_shape=None, **kwargs):\n        if border_mode != ""valid"":\n            raise ValueError(""For AveragePooling3D, only border_mode=\'valid\' is supported for now"")\n        super(AveragePooling3D, self).__init__(None,\n                                               pool_size,\n                                               strides,\n                                               dim_ordering,\n                                               list(input_shape) if input_shape else None,\n                                               **kwargs)\n\n\nclass GlobalAveragePooling1D(ZooKerasLayer):\n    """"""\n    Applies global average pooling operation for temporal data.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> globalaveragepooling1d = GlobalAveragePooling1D(input_shape=(12, 12))\n    creating: createZooKerasGlobalAveragePooling1D\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(GlobalAveragePooling1D, self).__init__(None,\n                                                     list(input_shape) if input_shape else None,\n                                                     **kwargs)\n\n\nclass GlobalMaxPooling1D(ZooKerasLayer):\n    """"""\n    Applies global max pooling operation for temporal data.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> globalmaxpooling1d = GlobalMaxPooling1D(input_shape=(4, 8))\n    creating: createZooKerasGlobalMaxPooling1D\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(GlobalMaxPooling1D, self).__init__(None,\n                                                 list(input_shape) if input_shape else None,\n                                                 **kwargs)\n\n\nclass GlobalAveragePooling2D(ZooKerasLayer):\n    """"""\n    Applies global average pooling operation for spatial data.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> globalaveragepooling2d = GlobalAveragePooling2D(input_shape=(4, 32, 32))\n    creating: createZooKerasGlobalAveragePooling2D\n    """"""\n    def __init__(self, dim_ordering=""th"", input_shape=None, **kwargs):\n        super(GlobalAveragePooling2D, self).__init__(None,\n                                                     dim_ordering,\n                                                     list(input_shape) if input_shape else None,\n                                                     **kwargs)\n\n\nclass GlobalMaxPooling2D(ZooKerasLayer):\n    """"""\n    Applies global max pooling operation for spatial data.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> globalmaxpooling2d = GlobalMaxPooling2D(input_shape=(4, 32, 32))\n    creating: createZooKerasGlobalMaxPooling2D\n    """"""\n    def __init__(self, dim_ordering=""th"", input_shape=None, **kwargs):\n        super(GlobalMaxPooling2D, self).__init__(None,\n                                                 dim_ordering,\n                                                 list(input_shape) if input_shape else None,\n                                                 **kwargs)\n\n\nclass GlobalAveragePooling3D(ZooKerasLayer):\n    """"""\n    Applies global average pooling operation for 3D data.\n    Data format currently supported for this layer is dim_ordering=\'th\' (Channel First).\n    Border mode currently supported for this layer is \'valid\'.\n    The input of this layer should be 5D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    dim_ordering: Format of input data. Only \'th\' (Channel First) is supported for now.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> globalaveragepooling3d = GlobalAveragePooling3D(input_shape=(4, 16, 16, 20))\n    creating: createZooKerasGlobalAveragePooling3D\n    """"""\n    def __init__(self, dim_ordering=""th"", input_shape=None, **kwargs):\n        super(GlobalAveragePooling3D, self).__init__(None,\n                                                     dim_ordering,\n                                                     list(input_shape) if input_shape else None,\n                                                     **kwargs)\n\n\nclass GlobalMaxPooling3D(ZooKerasLayer):\n    """"""\n    Applies global max pooling operation for 3D data.\n    Data format currently supported for this layer is dim_ordering=\'th\' (Channel First).\n    Border mode currently supported for this layer is \'valid\'.\n    The input of this layer should be 5D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    dim_ordering: Format of input data. Only \'th\' (Channel First) is supported for now.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> globalmaxpooling3d = GlobalMaxPooling3D(input_shape=(4, 32, 32, 32))\n    creating: createZooKerasGlobalMaxPooling3D\n    """"""\n    def __init__(self, dim_ordering=""th"", input_shape=None, **kwargs):\n        super(GlobalMaxPooling3D, self).__init__(None,\n                                                 dim_ordering,\n                                                 list(input_shape) if input_shape else None,\n                                                 **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras/layers/recurrent.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom ..engine.topology import ZooKerasLayer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass SimpleRNN(ZooKerasLayer):\n    """"""\n    A fully-connected recurrent neural network cell. The output is to be fed back to input.\n    The input of this layer should be 3D, i.e. (batch, time steps, input dim).\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    output_dim: Hidden unit size. Dimension of internal projections and final output.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is \'tanh\'.\n    return_sequences: Whether to return the full sequence or only return the last output\n                      in the output sequence. Default is False.\n    go_backwards: Whether the input sequence will be processed backwards. Default is False.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    U_regularizer: An instance of [[Regularizer]], applied the recurrent weights matrices.\n                   Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> simplernn = SimpleRNN(16, input_shape=(3, 32))\n    creating: createZooKerasSimpleRNN\n    """"""\n    def __init__(self, output_dim, activation=""tanh"", return_sequences=False,\n                 go_backwards=False, W_regularizer=None, U_regularizer=None,\n                 b_regularizer=None, input_shape=None, **kwargs):\n        super(SimpleRNN, self).__init__(None,\n                                        output_dim,\n                                        activation,\n                                        return_sequences,\n                                        go_backwards,\n                                        W_regularizer,\n                                        U_regularizer,\n                                        b_regularizer,\n                                        list(input_shape) if input_shape else None,\n                                        **kwargs)\n\n\nclass GRU(ZooKerasLayer):\n    """"""\n    Gated Recurrent Unit architecture.\n    The input of this layer should be 3D, i.e. (batch, time steps, input dim).\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    output_dim: Hidden unit size. Dimension of internal projections and final output.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is \'tanh\'.\n    inner_activation: String representation of the activation function for inner cells.\n                      Default is \'hard_sigmoid\'.\n    return_sequences: Whether to return the full sequence or only return the last output\n                      in the output sequence. Default is False.\n    go_backwards: Whether the input sequence will be processed backwards. Default is False.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    U_regularizer: An instance of [[Regularizer]], applied the recurrent weights matrices.\n                   Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> gru = GRU(24, input_shape=(32, 32))\n    creating: createZooKerasGRU\n    """"""\n    def __init__(self, output_dim, activation=""tanh"", inner_activation=""hard_sigmoid"",\n                 return_sequences=False, go_backwards=False, W_regularizer=None,\n                 U_regularizer=None, b_regularizer=None, input_shape=None, **kwargs):\n        super(GRU, self).__init__(None,\n                                  output_dim,\n                                  activation,\n                                  inner_activation,\n                                  return_sequences,\n                                  go_backwards,\n                                  W_regularizer,\n                                  U_regularizer,\n                                  b_regularizer,\n                                  list(input_shape) if input_shape else None,\n                                  **kwargs)\n\n\nclass LSTM(ZooKerasLayer):\n    """"""\n    Long Short Term Memory unit architecture.\n    The input of this layer should be 3D, i.e. (batch, time steps, input dim).\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    output_dim: Hidden unit size. Dimension of internal projections and final output.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is \'tanh\'.\n    inner_activation: String representation of the activation function for inner cells.\n                      Default is \'hard_sigmoid\'.\n    return_sequences: Whether to return the full sequence or only return the last output\n                      in the output sequence. Default is False.\n    go_backwards: Whether the input sequence will be processed backwards. Default is False.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    U_regularizer: An instance of [[Regularizer]], applied the recurrent weights matrices.\n                   Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> lstm = LSTM(32, input_shape=(8, 16), name=""lstm1"")\n    creating: createZooKerasLSTM\n    """"""\n    def __init__(self, output_dim, activation=""tanh"", inner_activation=""hard_sigmoid"",\n                 return_sequences=False, go_backwards=False, W_regularizer=None,\n                 U_regularizer=None, b_regularizer=None, input_shape=None, **kwargs):\n        super(LSTM, self).__init__(None,\n                                   output_dim,\n                                   activation,\n                                   inner_activation,\n                                   return_sequences,\n                                   go_backwards,\n                                   W_regularizer,\n                                   U_regularizer,\n                                   b_regularizer,\n                                   list(input_shape) if input_shape else None,\n                                   **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras/layers/self_attention.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport numpy as np\nimport math\n\nfrom bigdl.nn.layer import Sum\nfrom bigdl.nn.layer import Layer\nfrom zoo.common.utils import callZooFunc\n\nfrom zoo.models.common import ZooModel\nfrom zoo.pipeline.api.keras.engine import ZooKerasLayer\nfrom zoo.pipeline.api.keras.layers import *\nfrom zoo.pipeline.api.keras.models import Sequential\nfrom zoo.pipeline.api.keras.models import Model\nimport zoo.pipeline.api.autograd as auto\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\ndef layer_norm(x, w, b, e=1e-5):\n    sizes = x.get_output_shape()[1:]\n    u = auto.mean(x, len(sizes), True)\n    s = auto.mean(auto.square(x - u), len(sizes), True)\n    y = (x - u) / auto.sqrt(s + e)\n    y = y * w + b\n    return y\n\n\nclass TransformerLayer(ZooKerasLayer):\n    """"""\n    A self attention layer\n\n    Input is a list which consists of 2 ndarrays.\n    1. Token id ndarray: shape [batch, seqLen] with the word token indices in the vocabulary\n    2. Position id ndarray: shape [batch, seqLen] with positions in the sentence.\n    Output is a list which contains:\n    1. The states of Transformer layer.\n    2. The pooled output which processes the hidden state of the last layer with regard to the first\n    token of the sequence. This would be useful for segment-level tasks.\n\n    # Arguments\n    nBlock: block number\n    hidden_drop: drop probability off projection\n    attn_drop: drop probability of attention\n    n_head: head number\n    initializer_range: weight initialization range\n    bidirectional: whether unidirectional or bidirectional\n    output_all_block: whether output all blocks\' output\n    embedding_layer: embedding layer\n    input_shape: input shape\n    """"""\n\n    def __init__(self, n_block, hidden_drop, attn_drop, n_head, initializer_range, bidirectional,\n                 output_all_block, embedding_layer, input_shape, intermediate_size=0,\n                 bigdl_type=""float""):\n        self.hidden_drop = hidden_drop\n        self.attn_drop = attn_drop\n        self.n_head = n_head\n        self.initializer_range = initializer_range\n        self.output_all_block = output_all_block\n        self.bidirectional = bidirectional\n        self.intermediate_size = intermediate_size\n        self.seq_len = input_shape[0][0]\n        self.bigdl_type = bigdl_type\n        if not bidirectional:\n            mask_value = np.tril(np.ones((self.seq_len, self.seq_len), dtype=bigdl_type))\n            self.mask_value = auto.Constant(data=mask_value.reshape((1, 1,\n                                                                     self.seq_len, self.seq_len)))\n\n        (extended_attention_mask, embedding_inputs, inputs) = self.build_input(input_shape)\n        embedding = embedding_layer(embedding_inputs)\n        hidden_size = embedding.get_output_shape()[-1]\n\n        next_input = embedding\n\n        output = [None] * n_block\n        output[0] = self.block(next_input, hidden_size, extended_attention_mask)\n\n        for index in range(n_block - 1):\n            o = self.block(output[index], hidden_size, extended_attention_mask)\n            output[index + 1] = o\n\n        pooler_output = self.pooler(output[-1], hidden_size)\n        model = Model(inputs, output.append(pooler_output)) if output_all_block \\\n            else Model(inputs, [output[-1], pooler_output])\n        self.value = model.value\n\n    def build_input(self, input_shape):\n        if any(not isinstance(i, tuple) and not isinstance(i, list) for i in input_shape):\n            raise TypeError(\'TransformerLayer input must be a list of ndarray (consisting\'\n                            \' of input sequence, sequence positions, etc.)\')\n\n        inputs = [Input(list(shape)) for shape in input_shape]\n        return None, inputs, inputs\n\n    def block(self, x, size, attention_mask=None, eplision=1e-5):\n        g = auto.Parameter(shape=(1, size), init_weight=np.ones((1, size), dtype=self.bigdl_type))\n        b = auto.Parameter(shape=(1, size), init_weight=np.zeros((1, size), dtype=self.bigdl_type))\n        g2 = auto.Parameter(shape=(1, size), init_weight=np.ones((1, size), dtype=self.bigdl_type))\n        b2 = auto.Parameter(shape=(1, size), init_weight=np.zeros((1, size), dtype=self.bigdl_type))\n\n        a = self.multi_head_self_attention(x, size, attention_mask)\n        n = layer_norm(x + a, w=g, b=b, e=eplision)\n        m = self.mlp(n, size)\n        h = layer_norm(n + m, w=g2, b=b2, e=eplision)\n        return h\n\n    def projection_layer(self, output_size):\n        return Convolution1D(output_size, 1, ""normal"", (0.0, self.initializer_range))\n\n    def multi_head_self_attention(self, x, size, attention_mask=None):\n        c = self.projection_layer(size * 3)(x)\n        query = c.slice(2, 0, size)\n        key = c.slice(2, size, size)\n        value = c.slice(2, size * 2, size)\n        q = self.split_heads(query, self.n_head)\n        k = self.split_heads(key, self.n_head, k=True)\n        v = self.split_heads(value, self.n_head)\n        a = self.attn(q, k, v, True, attention_mask)\n        m = self.merge_heads(a)\n        n = self.projection_layer(size)(m)\n        d = Dropout(self.hidden_drop)(n)\n        return d\n\n    def attn(self, q, k, v, scale=False, attention_mask=None):\n        w = auto.mm(q, k)\n        if scale:\n            w = w / math.sqrt(v.get_output_shape()[-1])\n\n        if not self.bidirectional:\n            w = w * self.mask_value + (self.mask_value * (-1.0) + 1.0) * (-1e9)\n        if attention_mask:\n            w = w + attention_mask\n\n        w = Activation(""softmax"")(w)\n        w = Dropout(self.attn_drop)(w)\n        w = auto.mm(w, v)\n        return w\n\n    def mlp(self, x, hidden_size):\n        size = self.intermediate_size if self.intermediate_size > 0 else hidden_size * 4\n        h = self.projection_layer(size)(x)\n        a = self.gelu(h)\n        h2 = self.projection_layer(hidden_size)(a)\n        y = Dropout(self.hidden_drop)(h2)\n        return y\n\n    def gelu(self, x):\n        y = (auto.square(x) * x * 0.044715 + x) * (math.sqrt(2 / math.pi))\n        y = Activation(""tanh"")(y) + 1.0\n        y = x * 0.5 * y\n        return y\n\n    def split_heads(self, x, n_head, k=False):\n        sizes = x.get_output_shape()[1:]\n        shape = list(sizes + (int(sizes[-1] / n_head),))\n        shape[-2] = n_head\n        r = Reshape(shape)(x)\n        if k:\n            f = Permute((2, 3, 1))(r)\n        else:\n            f = Permute((2, 1, 3))(r)\n        return f\n\n    def merge_heads(self, x):\n        p = auto.contiguous(Permute((2, 1, 3))(x))\n        sizes = p.get_output_shape()[1:]\n        merge_sizes = list(sizes[:-2] + (sizes[-1] * sizes[-2],))\n        m = Reshape(merge_sizes)(p)\n        return m\n\n    def pooler(self, x, hidden_size):\n        first_token = Select(1, 0)(x)\n        pooler_output = Dense(hidden_size)(first_token)\n        o = Activation(""tanh"")(pooler_output)\n        return o\n\n    @classmethod\n    def init(cls, vocab=40990, seq_len=77, n_block=12, hidden_drop=0.1,\n             attn_drop=0.1, n_head=12, hidden_size=768,\n             embedding_drop=0.1, initializer_range=0.02,\n             bidirectional=False, output_all_block=False):\n        """"""\n        vocab: vocabulary size of training data, default is 40990\n        seq_len: max sequence length of training data, default is 77\n        n_block: block number, default is 12\n        hidden_drop: drop probability of projection, default is 0.1\n        attn_drop: drop probability of attention, default is 0.1\n        n_head: head number, default is 12\n        hidden_size: is also embedding size\n        embedding_drop: drop probability of embedding layer, default is 0.1\n        initializer_range: weight initialization range, default is 0.02\n        bidirectional: whether unidirectional or bidirectional, default is unidirectional\n        output_all_block: whether output all blocks\' output\n        """"""\n        if hidden_size < 0:\n            raise TypeError(\'hidden_size must be greater than 0 with default embedding layer\')\n        from bigdl.nn.layer import Squeeze\n        word_input = InputLayer(input_shape=(seq_len,))\n        postion_input = InputLayer(input_shape=(seq_len,))\n\n        embedding = Sequential()\n        embedding.add(Merge(layers=[word_input, postion_input], mode=\'concat\')) \\\n            .add(Reshape([seq_len * 2])) \\\n            .add(Embedding(vocab, hidden_size, input_length=seq_len * 2,\n                           weights=np.random.normal(0.0, initializer_range, (vocab, hidden_size))))\\\n            .add(Dropout(embedding_drop)) \\\n            .add(Reshape((seq_len, 2, hidden_size))) \\\n            .add(KerasLayerWrapper(Sum(dimension=3, squeeze=True)))\n        # walk around for bug #1208, need remove this line after the bug fixed\n        embedding.add(KerasLayerWrapper(Squeeze(dim=3)))\n\n        shape = ((seq_len,), (seq_len,))\n        return TransformerLayer(n_block, hidden_drop, attn_drop, n_head, initializer_range,\n                                bidirectional, output_all_block, embedding, input_shape=shape)\n\n\nclass BERT(TransformerLayer):\n    """"""\n    A self attention layer.\n    Input is a list which consists of 4 ndarrays.\n    1. Token id ndarray: shape [batch, seqLen] with the word token indices in the vocabulary\n    2. Token type id ndarray: shape [batch, seqLen] with the token types in [0, 1].\n       0 means `sentence A` and 1 means a `sentence B` (see BERT paper for more details).\n    3. Position id ndarray: shape [batch, seqLen] with positions in the sentence.\n    4. Attention_mask ndarray: shape [batch, seqLen] with indices in [0, 1].\n       It\'s a mask to be used if the input sequence length is smaller than seqLen in\n       the current batch.\n    Output is a list which contains:\n    1. The states of BERT layer.\n    2. The pooled output which processes the hidden state of the last layer with regard to the first\n    token of the sequence. This would be useful for segment-level tasks.\n\n    # Arguments\n    n_block: block number\n    n_head: head number\n    intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n    hidden_drop: The dropout probability for all fully connected layers\n    attn_drop: drop probability of attention\n    initializer_ranger: weight initialization range\n    output_all_block: whether output all blocks\' output\n    embedding_layer: embedding layer\n    input_shape: input shape\n    """"""\n\n    def __init__(self, n_block, n_head, intermediate_size, hidden_drop, attn_drop,\n                 initializer_range, output_all_block, embedding_layer,\n                 input_shape, bigdl_type=""float""):\n        self.hidden_drop = hidden_drop\n        self.attn_drop = attn_drop\n        self.n_head = n_head\n        self.intermediate_size = intermediate_size\n        self.output_all_block = output_all_block\n        self.bigdl_type = bigdl_type\n        self.seq_len = input_shape[0][0]\n        self.initializer_range = initializer_range\n        self.bidirectional = True\n        self.n_block = n_block\n\n        word_input = Input(shape=input_shape[0])\n        token_type_input = Input(shape=input_shape[1])\n        position_input = Input(shape=input_shape[2])\n        attention_mask = Input(shape=input_shape[3])\n\n        e = embedding_layer([word_input, token_type_input, position_input])\n        self.hidden_size = e.get_output_shape()[-1]\n        extended_attention_mask = (- attention_mask + 1.0) * -10000.0\n\n        next_input = e\n        model_output = [None] * n_block\n        model_output[0] = self.block(next_input, self.hidden_size, extended_attention_mask)\n\n        for _ in range(n_block - 1):\n            output = self.block(model_output[_], self.hidden_size, extended_attention_mask)\n            model_output[_ + 1] = output\n\n        pooler_output = self.pooler(model_output[-1], self.hidden_size)\n\n        if output_all_block:\n            model_output.append(pooler_output)\n            model = Model([word_input, token_type_input, position_input, attention_mask],\n                          model_output)\n        else:\n            model = Model([word_input, token_type_input, position_input, attention_mask],\n                          [model_output[-1], pooler_output])\n        self.value = model.value\n\n    def projection_layer(self, output_size):\n        return Dense(output_size, ""normal"", (0.0, self.initializer_range))\n\n    def build_input(self, input_shape):\n        if any(not isinstance(i, list) and not isinstance(i, tuple) for i in input_shape) \\\n                and len(input_shape) != 4:\n            raise TypeError(\'BERT input must be a list of 4 ndarray (consisting of input\'\n                            \' sequence, sequence positions, segment id, attention mask)\')\n        inputs = [Input(list(shape)) for shape in input_shape]\n        return (- inputs[-1] + 1.0) * -10000.0, inputs[:-1], inputs\n\n    def gelu(self, x):\n        y = x / math.sqrt(2.0)\n        e = auto.erf(y)\n        y = x * 0.5 * (e + 1.0)\n        return y\n\n    @classmethod\n    def init(cls, vocab=40990, hidden_size=768, n_block=12, n_head=12,\n             seq_len=512, intermediate_size=3072, hidden_drop=0.1,\n             attn_drop=0.1, initializer_range=0.02, output_all_block=True,\n             bigdl_type=""float""):\n        """"""\n        vocab: vocabulary size of training data, default is 40990\n        hidden_size: size of the encoder layers, default is 768\n        n_block: block number, default is 12\n        n_head: head number, default is 12\n        seq_len: max sequence length of training data, default is 77\n        intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n        hidden_drop: drop probability of full connected layers, default is 0.1\n        attn_drop: drop probability of attention, default is 0.1\n        initializer_ranger: weight initialization range, default is 0.02\n        output_all_block: whether output all blocks\' output, default is True\n        """"""\n        word_input = Input(shape=(seq_len,))\n        token_type_input = Input(shape=(seq_len,))\n        position_input = Input(shape=(seq_len,))\n        word_embedding = Embedding(vocab, hidden_size, input_length=seq_len,\n                                   weights=np.random.normal(0.0, initializer_range,\n                                                            (vocab, hidden_size)))(word_input)\n        position_embedding = Embedding(seq_len, hidden_size, input_length=seq_len,\n                                       weights=np.random.normal(0.0, initializer_range,\n                                                                (seq_len, hidden_size)))(\n            position_input)\n        token_type_embedding = Embedding(2, hidden_size, input_length=seq_len,\n                                         weights=np.random.normal(0.0, initializer_range,\n                                                                  (2, hidden_size)))(\n            token_type_input)\n        embedding = word_embedding + position_embedding + token_type_embedding\n\n        w = auto.Parameter(shape=(1, hidden_size),\n                           init_weight=np.ones((1, hidden_size), dtype=bigdl_type))\n        b = auto.Parameter(shape=(1, hidden_size),\n                           init_weight=np.zeros((1, hidden_size), dtype=bigdl_type))\n        after_norm = layer_norm(embedding, w, b, 1e-12)\n        h = Dropout(hidden_drop)(after_norm)\n\n        embedding_layer = Model([word_input, token_type_input, position_input], h)\n        shape = ((seq_len,), (seq_len,), (seq_len,), (1, 1, seq_len))\n\n        return BERT(n_block, n_head, intermediate_size, hidden_drop, attn_drop, initializer_range,\n                    output_all_block, embedding_layer, input_shape=shape)\n\n    @staticmethod\n    def init_from_existing_model(path, weight_path=None, input_seq_len=-1.0, hidden_drop=-1.0,\n                                 attn_drop=-1.0, output_all_block=True, bigdl_type=""float""):\n        """"""\n        Load an existing BERT model (with weights).\n\n        # Arguments\n        path: The path for the pre-defined model.\n              Local file system, HDFS and Amazon S3 are supported.\n              HDFS path should be like \'hdfs://[host]:[port]/xxx\'.\n              Amazon S3 path should be like \'s3a://bucket/xxx\'.\n        weight_path: The path for pre-trained weights if any. Default is None.\n        """"""\n        jlayer = callZooFunc(bigdl_type, ""loadBERT"", path, weight_path, input_seq_len,\n                             hidden_drop, attn_drop, output_all_block)\n\n        model = Layer(jvalue=jlayer, bigdl_type=bigdl_type)\n        model.__class__ = BERT\n        return model\n'"
pyzoo/zoo/pipeline/api/keras/layers/torch.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Layers from Torch wrapped in Keras style.\n\nimport sys\n\nfrom ..engine.topology import ZooKerasLayer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass Select(ZooKerasLayer):\n    """"""\n    Select an index of the input in the given dim and return the subset part.\n    The batch dimension needs to be unchanged.\n    The returned tensor has one less dimension: the dimension dim is removed.\n    As a result, it is not possible to select() on a 1D tensor.\n    For example, if input is: [[1 2 3], [4 5 6]]\n    Select(1, 1) will give output [2 5]\n    Select(1, -1) will give output [3 6]\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    dim: The dimension to select. 0-based index. Cannot select the batch dimension.\n         -1 means the last dimension of the input.\n    index: The index of the dimension to be selected. 0-based index.\n           -1 means the last dimension of the input.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the wrapper.\n          If not specified, its name will by default to be a generated string.\n\n    >>> select = Select(0, -1, input_shape=(3, 4), name=""select1"")\n    creating: createZooKerasSelect\n    """"""\n    def __init__(self, dim, index, input_shape=None, **kwargs):\n        super(Select, self).__init__(None,\n                                     dim,\n                                     index,\n                                     list(input_shape) if input_shape else None,\n                                     **kwargs)\n\n\nclass Narrow(ZooKerasLayer):\n    """"""\n    Narrow the input with the number of dimensions not being reduced.\n    The batch dimension needs to be unchanged.\n    For example, if input is: [[1 2 3], [4 5 6]]\n    Narrow(1, 1, 2) will give output [[2 3], [5 6]]\n    Narrow(1, 2, -1) will give output [[3], [6]]\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    dim: The dimension to narrow. 0-based index. Cannot narrow the batch dimension.\n         -1 means the last dimension of the input.\n    offset: Non-negative integer. The start index on the given dimension. 0-based index.\n    length: The length to narrow. Default is 1.\n            Can use a negative length such as -1 in the case where input size is unknown.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> narrow = Narrow(1, 3, input_shape=(5, 6, 7), name=""narrow1"")\n    creating: createZooKerasNarrow\n    """"""\n    def __init__(self, dim, offset, length=1, input_shape=None, **kwargs):\n        super(Narrow, self).__init__(None,\n                                     dim,\n                                     offset,\n                                     length,\n                                     list(input_shape) if input_shape else None,\n                                     **kwargs)\n\n\nclass Squeeze(ZooKerasLayer):\n    """"""\n    Delete the singleton dimension(s).\n    The batch dimension needs to be unchanged.\n    For example, if input has size (2, 1, 3, 4, 1):\n    Squeeze(1) will give output size (2, 3, 4, 1)\n    Squeeze() will give output size (2, 3, 4)\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    dim: The dimension(s) to squeeze. Can be either int or tuple of int.\n         0-based index. Cannot squeeze the batch dimension.\n         The selected dimensions must be singleton, i.e. having size 1.\n         Default is None, and in this case all the non-batch singleton dimensions will be deleted.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> squeeze1 = Squeeze(1, input_shape=(1, 4, 5))\n    creating: createZooKerasSqueeze\n    >>> squeeze2 = Squeeze(input_shape=(1, 8, 1, 4))\n    creating: createZooKerasSqueeze\n    >>> squeeze3 = Squeeze((1, 2), input_shape=(1, 1, 1, 32))\n    creating: createZooKerasSqueeze\n    """"""\n    def __init__(self, dim=None, input_shape=None, **kwargs):\n        if isinstance(dim, int):\n            dim = (dim, )\n        super(Squeeze, self).__init__(None,\n                                      dim,\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n\n\nclass AddConstant(ZooKerasLayer):\n    """"""\n    Add a (non-learnable) scalar constant to the input.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    constant: The scalar constant to be added.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> addconstant = AddConstant(1, input_shape=(1, 4, 5))\n    creating: createZooKerasAddConstant\n    """"""\n    def __init__(self, constant, input_shape=None, **kwargs):\n        super(AddConstant, self).__init__(None,\n                                          float(constant),\n                                          list(input_shape) if input_shape else None,\n                                          **kwargs)\n\n\nclass MulConstant(ZooKerasLayer):\n    """"""\n    Multiply the input by a (non-learnable) scalar constant.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    constant: The scalar constant to be multiplied.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> mulconstant = MulConstant(2.2, input_shape=(3, 4))\n    creating: createZooKerasMulConstant\n    """"""\n    def __init__(self, constant, input_shape=None, **kwargs):\n        super(MulConstant, self).__init__(None,\n                                          float(constant),\n                                          list(input_shape) if input_shape else None,\n                                          **kwargs)\n\n\nclass LRN2D(ZooKerasLayer):\n    """"""\n    Local Response Normalization between different feature maps.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    alpha: Float. The scaling parameter. Default is 0.0001.\n    k: Float. A constant.\n    beta: Float. The exponent. Default is 0.75.\n    n: The number of channels to sum over.\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> lrn2d = LRN2D(1e-3, 1.2, 0.4, 4, dim_ordering=""tf"", input_shape=(4, 5, 6))\n    creating: createZooKerasLRN2D\n    """"""\n    def __init__(self, alpha=1e-4, k=1.0, beta=0.75, n=5,\n                 dim_ordering=""th"", input_shape=None, **kwargs):\n        super(LRN2D, self).__init__(None,\n                                    float(alpha),\n                                    float(k),\n                                    float(beta),\n                                    n,\n                                    dim_ordering,\n                                    list(input_shape) if input_shape else None,\n                                    **kwargs)\n\n\nclass ShareConvolution2D(ZooKerasLayer):\n    """"""\n    Applies a 2D convolution over an input image composed of several input planes.\n    You can also use ShareConv2D as an alias of this layer.\n    Data format currently supported for this layer is dim_ordering=\'th\' (Channel First).\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n    e.g. input_shape=(3, 128, 128) for 128x128 RGB pictures.\n\n    # Arguments\n    nb_filter: Number of convolution filters to use.\n    nb_row: Number of rows in the convolution kernel.\n    nb_col: Number of cols in the convolution kernel.\n    init: String representation of the initialization method for the weights of the layer.\n          Default is \'glorot_uniform\'.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    subsample: Int tuple of length 2 corresponding to the step of the convolution in the\n               height and width dimension. Also called strides elsewhere. Default is (1, 1).\n    pad_h: The additional zeros added to the height dimension. Default is 0.\n    pad_w: The additional zeros added to the width dimension. Default is 0.\n    propagate_back: Whether to propagate gradient back. Default is True.\n    dim_ordering: Format of input data. Only \'th\' (Channel First) is supported for now.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> shareconv2d = ShareConvolution2D(32, 3, 4, activation=""tanh"", input_shape=(3, 128, 128))\n    creating: createZooKerasShareConvolution2D\n    """"""\n    def __init__(self, nb_filter, nb_row, nb_col, init=""glorot_uniform"",\n                 activation=None, subsample=(1, 1), pad_h=0, pad_w=0, propagate_back=True,\n                 dim_ordering=""th"", W_regularizer=None, b_regularizer=None,\n                 bias=True, input_shape=None, **kwargs):\n        super(ShareConvolution2D, self).__init__(None,\n                                                 nb_filter,\n                                                 nb_row,\n                                                 nb_col,\n                                                 init,\n                                                 activation,\n                                                 subsample,\n                                                 pad_h,\n                                                 pad_w,\n                                                 propagate_back,\n                                                 dim_ordering,\n                                                 W_regularizer,\n                                                 b_regularizer,\n                                                 bias,\n                                                 list(input_shape) if input_shape else None,\n                                                 **kwargs)\n\n\nShareConv2D = ShareConvolution2D\n\n\nclass CAdd(ZooKerasLayer):\n    """"""\n    This layer has a bias with given size.\n    The bias will be added element-wise to the input.\n    If the element number of the bias matches the input, a simple element-wise addition\n    will be done.\n    Or the bias will be expanded to the same size of the input.\n    The expand means repeat on unmatched singleton dimension (if some unmatched dimension\n    isn\'t a singleton dimension, an error will be raised).\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    size: The size of the bias.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is null.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> cadd = CAdd((2, 1), input_shape=(3, ))\n    creating: createZooKerasCAdd\n    """"""\n    def __init__(self, size, b_regularizer=None, input_shape=None, **kwargs):\n        super(CAdd, self).__init__(None,\n                                   size,\n                                   b_regularizer,\n                                   list(input_shape) if input_shape else None,\n                                   **kwargs)\n\n\nclass CMul(ZooKerasLayer):\n    """"""\n    This layer has a weight with given size.\n    The weight will be multiplied element-wise to the input.\n    If the element number of the weight matches the input,\n    a simple element-wise multiplication will be done.\n    Or the bias will be expanded to the same size of the input.\n    The expand means repeat on unmatched singleton dimension (if some unmatched dimension isn\'t\n    singleton dimension, an error will be raised).\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    size: The size of the bias.\n    W_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is null.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> cmul = CMul((2, 1), input_shape=(3, ))\n    creating: createZooKerasCMul\n    """"""\n    def __init__(self, size, W_regularizer=None, input_shape=None, **kwargs):\n        super(CMul, self).__init__(None,\n                                   size,\n                                   W_regularizer,\n                                   list(input_shape) if input_shape else None,\n                                   **kwargs)\n\n\nclass Exp(ZooKerasLayer):\n    """"""\n    Applies element-wise exp to the input.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> exp = Exp(input_shape=(2, 3, 4))\n    creating: createZooKerasExp\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(Exp, self).__init__(None,\n                                  list(input_shape) if input_shape else None,\n                                  **kwargs)\n\n\nclass Identity(ZooKerasLayer):\n    """"""\n    Identity just return the input to output.\n    It\'s useful in same parallel container to get an origin input.\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> identity = Identity(input_shape=(3, ))\n    creating: createZooKerasIdentity\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(Identity, self).__init__(None,\n                                       list(input_shape) if input_shape else None,\n                                       **kwargs)\n\n\nclass Log(ZooKerasLayer):\n    """"""\n    Applies a log transformation to the input.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> log = Log(input_shape=(4, 8, 8))\n    creating: createZooKerasLog\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(Log, self).__init__(None,\n                                  list(input_shape) if input_shape else None,\n                                  **kwargs)\n\n\nclass Mul(ZooKerasLayer):\n    """"""\n    Multiply a single scalar factor to the input.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> mul = Mul(input_shape=(3, 4, 5))\n    creating: createZooKerasMul\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(Mul, self).__init__(None,\n                                  list(input_shape) if input_shape else None,\n                                  **kwargs)\n\n\nclass Power(ZooKerasLayer):\n    """"""\n    Applies an element-wise power operation with scale and shift to the input.\n\n    f(x) = (shift + scale * x)^power^\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    power: The exponent.\n    scale: The scale parameter. Default is 1.\n    shift: The shift parameter. Default is 0.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> power = Power(3, input_shape=(3, ))\n    creating: createZooKerasPower\n    """"""\n    def __init__(self, power, scale=1, shift=0, input_shape=None, **kwargs):\n        super(Power, self).__init__(None,\n                                    float(power),\n                                    float(scale),\n                                    float(shift),\n                                    list(input_shape) if input_shape else None,\n                                    **kwargs)\n\n\nclass Scale(ZooKerasLayer):\n    """"""\n    Scale is the combination of CMul and CAdd.\n\n    Computes the element-wise product of the input and weight,\n    with the shape of the weight ""expand"" to match the shape of the input.\n    Similarly, perform an expanded bias and perform an element-wise add.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    size: Size of the weight and bias.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> scale = Scale((2, 1), input_shape=(3, ))\n    creating: createZooKerasScale\n    """"""\n    def __init__(self, size, input_shape=None, **kwargs):\n        super(Scale, self).__init__(None,\n                                    size,\n                                    list(input_shape) if input_shape else None,\n                                    **kwargs)\n\n\nclass Sqrt(ZooKerasLayer):\n    """"""\n    Applies an element-wise square root operation to the input.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> sqrt = Sqrt(input_shape=(3, ))\n    creating: createZooKerasSqrt\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(Sqrt, self).__init__(None,\n                                   list(input_shape) if input_shape else None,\n                                   **kwargs)\n\n\nclass Square(ZooKerasLayer):\n    """"""\n    Applies an element-wise square operation to the input.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> square = Square(input_shape=(5, ))\n    creating: createZooKerasSquare\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(Square, self).__init__(None,\n                                     list(input_shape) if input_shape else None,\n                                     **kwargs)\n\n\nclass HardShrink(ZooKerasLayer):\n    """"""\n    Applies the hard shrinkage function element-wise to the input.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    value: The threshold value. Default is 0.5.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> hardshrink = HardShrink(input_shape=(2, 4, 8))\n    creating: createZooKerasHardShrink\n    """"""\n    def __init__(self, value=0.5, input_shape=None, **kwargs):\n        super(HardShrink, self).__init__(None,\n                                         float(value),\n                                         list(input_shape) if input_shape else None,\n                                         **kwargs)\n\n\nclass HardTanh(ZooKerasLayer):\n    """"""\n    Applies the hard tanh function element-wise to the input.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    min_value: The minimum threshold value. Default is -1.\n    max_value: The maximum threshold value. Default is 1.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> hardtanh = HardTanh(input_shape=(3, 4))\n    creating: createZooKerasHardTanh\n    """"""\n    def __init__(self, min_value=-1, max_value=1, input_shape=None, **kwargs):\n        super(HardTanh, self).__init__(None,\n                                       float(min_value),\n                                       float(max_value),\n                                       list(input_shape) if input_shape else None,\n                                       **kwargs)\n\n\nclass Negative(ZooKerasLayer):\n    """"""\n    Computes the negative value of each element of the input.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> negative = Negative(input_shape=(4, 5, 8))\n    creating: createZooKerasNegative\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(Negative, self).__init__(None,\n                                       list(input_shape) if input_shape else None,\n                                       **kwargs)\n\n\nclass PReLU(ZooKerasLayer):\n    """"""\n    Applies parametric ReLU, where parameter varies the slope of the negative part.\n\n    Notice: Please don\'t use weight decay on this.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    n_output_plane: Input map number. Default is 0,\n                    which means using PReLU in shared version and has only one parameter.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> prelu = PReLU(input_shape=(3, 4, 8, 8))\n    creating: createZooKerasPReLU\n    """"""\n    def __init__(self, n_output_plane=0, input_shape=None, **kwargs):\n        super(PReLU, self).__init__(None,\n                                    n_output_plane,\n                                    list(input_shape) if input_shape else None,\n                                    **kwargs)\n\n\nclass RReLU(ZooKerasLayer):\n    """"""\n    Applies the randomized leaky rectified linear unit element-wise to the input.\n\n    In the training mode, negative inputs are multiplied by a factor drawn\n    from a uniform random distribution U(l, u).\n    In the evaluation mode, a RReLU behaves like a LeakyReLU with a constant mean\n    factor a = (l + u) / 2.\n    If l == u, a RReLU essentially becomes a LeakyReLU.\n    Regardless of operating in in-place mode a RReLU will internally\n    allocate an input-sized noise tensor to store random factors for negative inputs.\n    For reference see [Empirical Evaluation of Rectified Activations in Convolutional\n    Network](http://arxiv.org/abs/1505.00853).\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    lower: Lower boundary of the uniform random distribution. Default is 1.0/8.\n    upper: Upper boundary of the uniform random distribution. Default is 1.0/3.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> rrelu = RReLU(input_shape=(3, 4))\n    creating: createZooKerasRReLU\n    """"""\n    def __init__(self, lower=1.0/8, upper=1.0/3, input_shape=None, **kwargs):\n        super(RReLU, self).__init__(None,\n                                    float(lower),\n                                    float(upper),\n                                    list(input_shape) if input_shape else None,\n                                    **kwargs)\n\n\nclass SoftShrink(ZooKerasLayer):\n    """"""\n    Applies the soft shrinkage function element-wise to the input.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    value: The threshold value. Default is 0.5.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> softshrink = SoftShrink(input_shape=(4, 4, 8, 8))\n    creating: createZooKerasSoftShrink\n    """"""\n    def __init__(self, value=0.5, input_shape=None, **kwargs):\n        super(SoftShrink, self).__init__(None,\n                                         float(value),\n                                         list(input_shape) if input_shape else None,\n                                         **kwargs)\n\n\nclass WithinChannelLRN2D(ZooKerasLayer):\n    """"""\n    The local response normalization layer performs a kind of ""lateral inhibition""\n    by normalizing over local input regions. The local regions extend spatially,\n    in separate channels (i.e., they have shape 1 x size x size).\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    size: The side length of the square region to sum over. Default is 5.\n    alpha: The scaling parameter. Default is 1.0.\n    beta: The exponent. Default is 0.75.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> withinchannellrn2d = WithinChannelLRN2D(input_shape=(2, 3, 8, 8))\n    creating: createZooKerasWithinChannelLRN2D\n    """"""\n    def __init__(self, size=5, alpha=1.0, beta=0.75, input_shape=None, **kwargs):\n        super(WithinChannelLRN2D, self).__init__(None,\n                                                 size,\n                                                 float(alpha),\n                                                 float(beta),\n                                                 list(input_shape) if input_shape else None,\n                                                 **kwargs)\n\n\nclass BinaryThreshold(ZooKerasLayer):\n    """"""\n    Threshold the input.\n    If an input element is smaller than the threshold value,\n    it will be replaced by 0; otherwise, it will be replaced by 1.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    value: The threshold value to compare with. Default is 1e-6.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> binarythreshold = BinaryThreshold(input_shape=(2, 3, 4, 5))\n    creating: createZooKerasBinaryThreshold\n    """"""\n    def __init__(self, value=1e-6, input_shape=None, **kwargs):\n        super(BinaryThreshold, self).__init__(None,\n                                              float(value),\n                                              list(input_shape) if input_shape else None,\n                                              **kwargs)\n\n\nclass Threshold(ZooKerasLayer):\n    """"""\n    Threshold input Tensor.\n    If values in the Tensor smaller than or equal to th, then replace it with v.\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n    # Arguments\n    th: The threshold value to compare with. Default is 1e-6.\n    v: the value to replace with.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n    >>> threshold = Threshold(input_shape=(2, 3, 4, 5))\n    creating: createZooKerasThreshold\n    """"""\n    def __init__(self, th=1e-6, v=0.0, input_shape=None, **kwargs):\n        super(Threshold, self).__init__(None,\n                                        float(th),\n                                        float(v),\n                                        list(input_shape) if input_shape else None,\n                                        **kwargs)\n\n\nclass GaussianSampler(ZooKerasLayer):\n    """"""\n    Takes {mean, log_variance} as input and samples from the Gaussian distribution\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n\n    >>> gaussianSampler = GaussianSampler(input_shape=[(3,),(3,)])\n    creating: createZooKerasGaussianSampler\n    """"""\n    def __init__(self, input_shape=None, **kwargs):\n        super(GaussianSampler, self).__init__(None,\n                                              list(input_shape) if input_shape else None,\n                                              **kwargs)\n\n\nclass ResizeBilinear(ZooKerasLayer):\n    """"""\n    Resize the input image with bilinear interpolation. The input image must be a float tensor with\n    NHWC or NCHW layout\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    output_height: output height\n    output_width: output width\n    align_corner: align corner or not\n    dim_ordering: Format of input data. Either \'th\' (Channel First) or \'tf\' (Channel Last).\n                  Default is \'th\'.\n    input_shape: A shape tuple, not including batch.\n\n    >>> resizeBilinear = ResizeBilinear(10, 20, input_shape=(2, 3, 5, 7))\n    creating: createZooKerasResizeBilinear\n    """"""\n    def __init__(self, output_height, output_width, align_corner=False,\n                 dim_ordering=""th"", input_shape=None, **kwargs):\n        super(ResizeBilinear, self).__init__(None,\n                                             output_height,\n                                             output_width,\n                                             align_corner,\n                                             dim_ordering,\n                                             list(input_shape) if input_shape else None,\n                                             **kwargs)\n\n\nclass SelectTable(ZooKerasLayer):\n    """"""\n    Creates a module that takes a list of JTensors as input and outputs the element at index `index`\n\n    # Arguments\n    index: the index to be selected. 0-based index\n    input_shape: a list of shape tuples, not including batch.\n\n    >>> selectTable = SelectTable(0, input_shape=[[2, 3], [5, 7]])\n    creating: createZooKerasSelectTable\n    """"""\n    def __init__(self, index, input_shape=None, **kwargs):\n        super(SelectTable, self).__init__(None,\n                                          index,\n                                          list(input_shape) if input_shape else None,\n                                          **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras/layers/wrappers.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\n\nfrom ..engine.topology import ZooKerasLayer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass TimeDistributed(ZooKerasLayer):\n    """"""\n    TimeDistributed wrapper.\n    Apply a layer to every temporal slice of an input.\n    The input should be at least 3D.\n    The dimension of index one will be considered as the temporal dimension.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n    name: String to specify the name of the wrapper. Default is None.\n\n    # Arguments\n    layer: A layer instance.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the wrapper.\n          If not specified, its name will by default to be a generated string.\n\n    >>> from zoo.pipeline.api.keras.layers import Dense\n    >>> timedistributed = TimeDistributed(Dense(8), input_shape=(10, 12))\n    creating: createZooKerasDense\n    creating: createZooKerasTimeDistributed\n    """"""\n    def __init__(self, layer, input_shape=None, **kwargs):\n        super(TimeDistributed, self).__init__(None,\n                                              layer,\n                                              list(input_shape) if input_shape else None,\n                                              **kwargs)\n\n\nclass Bidirectional(ZooKerasLayer):\n    """"""\n    Bidirectional wrapper for RNNs.\n    Bidirectional currently requires RNNs to return the full sequence, i.e. return_sequences = True.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n    Example of creating a bidirectional LSTM:\n    Bidirectiona(LSTM(12, return_sequences=True), merge_mode=""sum"", input_shape=(32, 32))\n\n    # Arguments\n    layer: An instance of a recurrent layer.\n    merge_mode: Mode by which outputs of the forward and backward RNNs will be combined.\n                Must be one of: \'sum\', \'mul\', \'concat\', \'ave\'. Default is \'concat\'.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the wrapper.\n          If not specified, its name will by default to be a generated string.\n\n    >>> from zoo.pipeline.api.keras.layers import LSTM\n    >>> bidiretional = Bidirectional(LSTM(10, return_sequences=True), input_shape=(12, 16))\n    creating: createZooKerasLSTM\n    creating: createZooKerasBidirectional\n    """"""\n    def __init__(self, layer, merge_mode=""concat"", input_shape=None, **kwargs):\n        super(Bidirectional, self).__init__(None,\n                                            layer,\n                                            merge_mode,\n                                            list(input_shape) if input_shape else None,\n                                            **kwargs)\n\n\nclass KerasLayerWrapper(ZooKerasLayer):\n    """"""\n    Wrap a torch style layer to keras style layer.\n    This layer can be built multiple times.\n    This layer will return a keras compatible layer\n\n    # Arguments\n    torch_layer: a torch style layer.\n    input_shape: A shape tuple, not including batch.\n    i.e If the input data is (2, 3, 4) and 2 is the batch size, you should input: (3, 4) here.\n    >>> from zoo.pipeline.api.keras.layers import KerasLayerWrapper\n    >>> from bigdl.nn.layer import Linear\n    >>> linear = Linear(100, 10, with_bias=True)\n    creating: createLinear\n    >>> kerasLayer = KerasLayerWrapper(linear, input_shape=(100, ))\n    creating: createZooKerasKerasLayerWrapper\n    """"""\n    def __init__(self, torch_layer, input_shape=None, **kwargs):\n        super(KerasLayerWrapper, self).__init__(None,\n                                                torch_layer,\n                                                list(input_shape) if input_shape else None,\n                                                **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras2/engine/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/pipeline/api/keras2/engine/topology.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n'"
pyzoo/zoo/pipeline/api/keras2/engine/training.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n'"
pyzoo/zoo/pipeline/api/keras2/layers/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import absolute_import\n\nfrom .merge import *\nfrom .core import *\nfrom .convolutional import *\nfrom .pooling import *\nfrom .local import *\nfrom .recurrent import *\nfrom .normalization import *\nfrom .embeddings import *\nfrom .noise import *\nfrom .advanced_activations import *\nfrom .wrappers import *\nfrom .convolutional_recurrent import *\n'"
pyzoo/zoo/pipeline/api/keras2/layers/advanced_activations.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n'"
pyzoo/zoo/pipeline/api/keras2/layers/convolutional.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\nfrom zoo.pipeline.api.keras2.base import ZooKeras2Layer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass Conv1D(ZooKeras2Layer):\n    """"""1D convolution layer (e.g. temporal convolution).\n\n    This layer creates a convolution kernel that is convolved\n    with the layer input over a single spatial (or temporal) dimension\n    to produce a tensor of outputs.\n    If `use_bias` is True, a bias vector is created and added to the outputs.\n    Finally, if `activation` is not `None`,\n    it is applied to the outputs as well.\n\n    When using this layer as the first layer in a model,\n    provide an `input_shape` argument\n    (tuple of integers or `None`, e.g.\n    `(10, 128)` for sequences of 10 vectors of 128-dimensional vectors,\n    or `(None, 128)` for variable-length sequences of 128-dimensional vectors.\n\n    # Arguments\n        filters: Integer, the dimensionality of the output space\n            (i.e. the number of output filters in the convolution).\n        kernel_size: An integer or tuple/list of a single integer,\n            specifying the length of the 1D convolution window.\n        strides: An integer or tuple/list of a single integer,\n            specifying the stride length of the convolution.\n            Specifying any stride value != 1 is incompatible with specifying\n            any `dilation_rate` value != 1.\n        padding: One of `""valid""` or `""same""` (case-insensitive).\n            `""valid""` means ""no padding"".\n            `""same""` results in padding the input such that\n            the output has the same length as the original input.\n            (see [activations](../activations.md)).\n            If you don\'t specify anything, no activation is applied\n            (ie. ""linear"" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix\n        bias_initializer: Initializer for the bias vector\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n        bias_regularizer: Regularizer function applied to the bias vector\n\n    # Input shape\n        3D tensor with shape: `(batch_size, steps, input_dim)`\n\n    # Output shape\n        3D tensor with shape: `(batch_size, new_steps, filters)`\n        `steps` value might have changed due to padding or strides.\n\n    >>> conv1d = Conv1D(12, 4, input_shape=(3, 16))\n    creating: createZooKeras2Conv1D\n    """"""\n    def __init__(self,\n                 filters,\n                 kernel_size,\n                 strides=1,\n                 padding=""valid"",\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=""glorot_uniform"",\n                 bias_initializer=""zero"",\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 input_shape=None, **kwargs):\n        super(Conv1D, self).__init__(None,\n                                     filters,\n                                     kernel_size,\n                                     strides,\n                                     padding,\n                                     activation,\n                                     use_bias,\n                                     kernel_initializer,\n                                     bias_initializer,\n                                     kernel_regularizer,\n                                     bias_regularizer,\n                                     list(input_shape) if input_shape else None,\n                                     **kwargs)\n\n\nclass Conv2D(ZooKeras2Layer):\n    """"""2D convolution layer (e.g. spatial convolution over images).\n\n    This layer creates a convolution kernel that is convolved\n    with the layer input to produce a tensor of\n    outputs. If `use_bias` is True,\n    a bias vector is created and added to the outputs. Finally, if\n    `activation` is not `None`, it is applied to the outputs as well.\n\n    When using this layer as the first layer in a model,\n    provide the keyword argument `input_shape`\n    (tuple of integers, does not include the sample axis),\n    e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n    in `data_format=""channels_last""`.\n\n    # Arguments\n        filters: Integer, the dimensionality of the output space\n            (i.e. the number of output filters in the convolution).\n        kernel_size: An integer or tuple/list of 2 integers, specifying the\n            width and height of the 2D convolution window.\n            Can be a single integer to specify the same value for\n            all spatial dimensions.\n        strides: An integer or tuple/list of 2 integers,\n            specifying the strides of the convolution along the width and height.\n            Can be a single integer to specify the same value for\n            all spatial dimensions.\n            Specifying any stride value != 1 is incompatible with specifying\n            any `dilation_rate` value != 1.\n        padding: one of `""valid""` or `""same""` (case-insensitive).\n            Note that `""same""` is slightly inconsistent across backends with\n            `strides` != 1.\n        data_format: A string,\n            one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs.\n            `channels_last` corresponds to inputs with shape\n            `(batch, height, width, channels)` while `channels_first`\n            corresponds to inputs with shape\n            `(batch, channels, height, width)`.\n            It defaults to the `image_data_format` value found in your\n            Keras config file at `~/.keras/keras.json`.\n            If you never set it, then it will be ""channels_last"".\n        activation: Activation function to use.\n            If you don\'t specify anything, no activation is applied\n            (ie. ""linear"" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix\n        bias_initializer: Initializer for the bias vector\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n        bias_regularizer: Regularizer function applied to the bias vector\n\n    # Input shape\n        4D tensor with shape:\n        `(samples, channels, rows, cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, rows, cols, channels)` if data_format=\'channels_last\'.\n\n    # Output shape\n        4D tensor with shape:\n        `(samples, filters, new_rows, new_cols)` if data_format=\'channels_first\'\n        or 4D tensor with shape:\n        `(samples, new_rows, new_cols, filters)` if data_format=\'channels_last\'.\n        `rows` and `cols` values might have changed due to padding.\n\n    >>> conv2d = Conv2D(12, kernel_size=(2, 5), input_shape=(3, 16, 16))\n    creating: createZooKeras2Conv2D\n    """"""\n    def __init__(self,\n                 filters,\n                 kernel_size,\n                 strides=(1, 1),\n                 padding=""valid"",\n                 data_format=""channels_first"",\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=""glorot_uniform"",\n                 bias_initializer=""zero"",\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 input_shape=None, **kwargs):\n        super(Conv2D, self).__init__(None,\n                                     filters,\n                                     kernel_size,\n                                     strides,\n                                     padding,\n                                     data_format,\n                                     activation,\n                                     use_bias,\n                                     kernel_initializer,\n                                     bias_initializer,\n                                     kernel_regularizer,\n                                     bias_regularizer,\n                                     list(input_shape) if input_shape else None,\n                                     **kwargs)\n\n\nclass Cropping1D(ZooKeras2Layer):\n    """"""\n    Cropping layer for 1D input (e.g. temporal sequence).\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    cropping: Int tuple of length 2. How many units should be trimmed off at the beginning and\n              end of the cropping dimension. Default is (1, 1).\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> cropping1d = Cropping1D(cropping=(1, 2), input_shape=(8, 8))\n    creating: createZooKeras2Cropping1D\n    """"""\n    def __init__(self, cropping=(1, 1), input_shape=None, **kwargs):\n        super(Cropping1D, self).__init__(None,\n                                         cropping,\n                                         list(input_shape) if input_shape else None,\n                                         **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras2/layers/convolutional_recurrent.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n'"
pyzoo/zoo/pipeline/api/keras2/layers/core.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nimport zoo.pipeline.api.keras.layers as klayers1\nfrom zoo.pipeline.api.keras2.base import ZooKeras2Layer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass Dense(ZooKeras2Layer):\n    """"""\n    A densely-connected NN layer.\n    The most common input is 2D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    units: The size of output dimension.\n    kernel_initializer: String representation of the initialization method \\\n          for the weights of the layer.\n          Default is \'glorot_uniform\'.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    kernel_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    b_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    bias_regularizer: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_dim: Dimensionality of the input for 2D input. For nD input, you can alternatively\n               specify \'input_shape\' when using this layer as the first layer.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> dense = Dense(10, kernel_initializer=""glorot_uniform"", input_dim=8, name=""dense1"")\n    creating: createZooKeras2Dense\n    """"""\n    def __init__(self, units, kernel_initializer=""glorot_uniform"",\n                 bias_initializer=""zero"", activation=None,\n                 kernel_regularizer=None, bias_regularizer=None,\n                 use_bias=True, input_dim=None, input_shape=None, **kwargs):\n        if input_dim:\n            input_shape = (input_dim, )\n        super(Dense, self).__init__(None,\n                                    units,\n                                    kernel_initializer,\n                                    bias_initializer,\n                                    activation,\n                                    kernel_regularizer,\n                                    bias_regularizer,\n                                    use_bias,\n                                    list(input_shape) if input_shape else None,\n                                    **kwargs)\n\n\nclass Activation(ZooKeras2Layer):\n    """"""\n    Simple activation function to be applied to the output.\n    Available activations: \'tanh\', \'relu\', \'sigmoid\', \'softmax\', \'softplus\', \'softsign\',\n                           \'hard_sigmoid\', \'linear\', \'relu6\', \'tanh_shrink\', \'softmin\',\n                           \'log_sigmoid\' and \'log_softmax\'.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    activation: Name of the activation function as string.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> activation = Activation(""relu"", input_shape=(3, 4))\n    creating: createZooKeras2Activation\n    """"""\n    def __init__(self,\n                 activation,\n                 input_shape=None,\n                 **kwargs):\n        super(Activation, self).__init__(None,\n                                         activation,\n                                         list(input_shape) if input_shape else None,\n                                         **kwargs)\n\n\nclass Dropout(ZooKeras2Layer):\n    """"""\n    Applies Dropout to the input by randomly setting a fraction \'rate\' of input units to 0 at each\n    update during training time in order to prevent overfitting.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    rate: Fraction of the input units to drop. Float between 0 and 1.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> dropout = Dropout(0.25, input_shape=(2, 3))\n    creating: createZooKeras2Dropout\n    """"""\n    def __init__(self,\n                 rate,\n                 input_shape=None,\n                 **kwargs):\n        super(Dropout, self).__init__(None,\n                                      float(rate),\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n\n\nclass Flatten(ZooKeras2Layer):\n    """"""\n    Flattens the input without affecting the batch size.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> flatten = Flatten(input_shape=(3, 10, 2))\n    creating: createZooKeras2Flatten\n    """"""\n    def __init__(self,\n                 input_shape=None,\n                 **kwargs):\n        super(Flatten, self).__init__(None,\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras2/layers/embeddings.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n'"
pyzoo/zoo/pipeline/api/keras2/layers/local.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\nfrom zoo.pipeline.api.keras2.base import ZooKeras2Layer\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass LocallyConnected1D(ZooKeras2Layer):\n    """"""\n    Locally-connected layer for 1D inputs which works similarly to the TemporalConvolution\n    layer, except that weights are unshared, that is, a different set of filters is applied\n    at each different patch of the input..\n    Padding currently supported for this layer is \'valid\'.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    filters: Dimensionality of the output.\n    kernel_size: The extension (spatial or temporal) of each filter.\n    strides: Factor by which to subsample output. Int. Default is 1.\n    padding: Only \'valid\' is supported for now.\n    activation: String representation of the activation function to use\n                (such as \'relu\' or \'sigmoid\'). Default is None.\n    kernel_regularizer: An instance of [[Regularizer]], (eg. L1 or L2 regularization),\n                   applied to the input weights matrices. Default is None.\n    bias_regularizer: An instance of [[Regularizer]], applied to the bias. Default is None.\n    use_bias: Whether to include a bias (i.e. make the layer affine rather than linear).\n          Default is True.\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> locallyconnected1d = LocallyConnected1D(6, 3, input_shape=(8, 12))\n    creating: createZooKeras2LocallyConnected1D\n    """"""\n    def __init__(self,\n                 filters,\n                 kernel_size,\n                 strides=1,\n                 padding=""valid"",\n                 activation=None,\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 use_bias=True,\n                 input_shape=None,\n                 **kwargs):\n        if padding != ""valid"":\n            raise ValueError(""For LocallyConnected1D, ""\n                             ""only padding=\'valid\' is supported for now"")\n        super(LocallyConnected1D, self).__init__(None,\n                                                 filters,\n                                                 kernel_size,\n                                                 strides,\n                                                 padding,\n                                                 activation,\n                                                 kernel_regularizer,\n                                                 bias_regularizer,\n                                                 use_bias,\n                                                 list(input_shape) if input_shape else None,\n                                                 **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras2/layers/merge.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\nfrom zoo.pipeline.api.keras2.base import ZooKeras2Layer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass Maximum(ZooKeras2Layer):\n    """"""\n    Layer that computes the maximum (element-wise) a list of inputs.\n\n    It takes as input a list of tensors,\n    all of the same shape, and returns\n    a single tensor (also of the same shape).\n    >>> from zoo.pipeline.api.keras.layers import Input\n    >>> max = Maximum()([Input(shape=(4, 5)), Input(shape=(4, 5))])\n    creating: createZooKeras2Maximum\n    creating: createZooKerasInput\n    creating: createZooKerasInput\n    """"""\n    def __init__(self,\n                 input_shape=None, **kwargs):\n        super(Maximum, self).__init__(None,\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n\n\ndef maximum(inputs, **kwargs):\n    """"""Functional interface to the `Maximum` layer.\n\n    # Arguments\n        inputs: A list of input tensors (at least 2).\n        **kwargs: Standard layer keyword arguments.\n\n    # Returns\n        A tensor, the element-wise maximum of the inputs.\n    >>> from zoo.pipeline.api.keras.layers import Input\n    >>> max = maximum([Input(shape=(4, 5)), Input(shape=(4, 5))])\n    creating: createZooKerasInput\n    creating: createZooKerasInput\n    creating: createZooKeras2Maximum\n    """"""\n    return Maximum(**kwargs)(inputs)\n\n\nclass Minimum(ZooKeras2Layer):\n    """"""\n    Layer that computes the minimum (element-wise) a list of inputs.\n\n    It takes as input a list of tensors,\n    all of the same shape, and returns\n    a single tensor (also of the same shape).\n    >>> from zoo.pipeline.api.keras.layers import Input\n    >>> max = Minimum()([Input(shape=(4, 5)), Input(shape=(4, 5))])\n    creating: createZooKeras2Minimum\n    creating: createZooKerasInput\n    creating: createZooKerasInput\n    """"""\n    def __init__(self,\n                 input_shape=None, **kwargs):\n        super(Minimum, self).__init__(None,\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n\n\ndef minimum(inputs, **kwargs):\n    """"""Functional interface to the `Minimum` layer.\n\n    # Arguments\n        inputs: A list of input tensors (at least 2).\n        **kwargs: Standard layer keyword arguments.\n\n    # Returns\n        A tensor, the element-wise minimum of the inputs.\n    >>> from zoo.pipeline.api.keras.layers import Input\n    >>> min = minimum([Input(shape=(4, 5)), Input(shape=(4, 5))])\n    creating: createZooKerasInput\n    creating: createZooKerasInput\n    creating: createZooKeras2Minimum\n    """"""\n    return Minimum(**kwargs)(inputs)\n\n\nclass Average(ZooKeras2Layer):\n    """"""\n    Layer that computes the average (element-wise) a list of inputs.\n\n    It takes as input a list of tensors,\n    all of the same shape, and returns\n    a single tensor (also of the same shape).\n    >>> from zoo.pipeline.api.keras.layers import Input\n    >>> ave = Average()([Input(shape=(4, 5)), Input(shape=(4, 5)), Input(shape=(4, 5))])\n    creating: createZooKeras2Average\n    creating: createZooKerasInput\n    creating: createZooKerasInput\n    creating: createZooKerasInput\n    """"""\n    def __init__(self,\n                 input_shape=None, **kwargs):\n        super(Average, self).__init__(None,\n                                      list(input_shape) if input_shape else None,\n                                      **kwargs)\n\n\ndef average(inputs, **kwargs):\n    """"""Functional interface to the `Average` layer.\n\n    # Arguments\n        inputs: A list of input tensors (at least 2).\n        **kwargs: Standard layer keyword arguments.\n\n    # Returns\n        A tensor, the element-wise minimum of the inputs.\n    >>> from zoo.pipeline.api.keras.layers import Input\n    >>> ave = average([Input(shape=(4, 5)), Input(shape=(4, 5)), Input(shape=(4, 5))])\n    creating: createZooKerasInput\n    creating: createZooKerasInput\n    creating: createZooKerasInput\n    creating: createZooKeras2Average\n    """"""\n    return Average(**kwargs)(inputs)\n'"
pyzoo/zoo/pipeline/api/keras2/layers/noise.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n'"
pyzoo/zoo/pipeline/api/keras2/layers/normalization.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n'"
pyzoo/zoo/pipeline/api/keras2/layers/pooling.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\nfrom zoo.pipeline.api.keras2.base import ZooKeras2Layer\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n\n\nclass MaxPooling1D(ZooKeras2Layer):\n    """"""\n    Max pooling operation for temporal data.\n\n    # Arguments\n        pool_size: Integer, size of the max pooling windows.\n        strides: Integer, or None. Factor by which to downscale.\n            E.g. 2 will halve the input.\n            If None, it will be set to -1, which will be default to pool_size.\n        padding: One of `""valid""` or `""same""` (case-insensitive).\n\n    # Input shape\n        3D tensor with shape: `(batch_size, steps, features)`.\n\n    # Output shape\n        3D tensor with shape: `(batch_size, downsampled_steps, features)`.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    >>> maxpooling1d = MaxPooling1D(3, input_shape=(3, 24))\n    creating: createZooKeras2MaxPooling1D\n    """"""\n    def __init__(self,\n                 pool_size=2,\n                 strides=None,\n                 padding=""valid"",\n                 input_shape=None, **kwargs):\n        if not strides:\n            strides = -1\n        super(MaxPooling1D, self).__init__(None,\n                                           pool_size,\n                                           strides,\n                                           padding,\n                                           list(input_shape) if input_shape else None,\n                                           **kwargs)\n\n\nclass AveragePooling1D(ZooKeras2Layer):\n    """"""\n    Average pooling operation for temporal data.\n\n    # Arguments\n        pool_size: Integer, size of the average pooling windows.\n        strides: Integer, or None. Factor by which to downscale.\n            E.g. 2 will halve the input.\n            If None, it will be set to -1, which will be default to pool_size.\n        padding: One of `""valid""` or `""same""` (case-insensitive).\n\n    # Input shape\n        3D tensor with shape: `(batch_size, steps, features)`.\n\n    # Output shape\n        3D tensor with shape: `(batch_size, downsampled_steps, features)`.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    >>> averagepooling1d = AveragePooling1D(input_shape=(3, 24))\n    creating: createZooKeras2AveragePooling1D\n    """"""\n    def __init__(self,\n                 pool_size=2,\n                 strides=None,\n                 padding=""valid"",\n                 input_shape=None, **kwargs):\n        if not strides:\n            strides = -1\n        super(AveragePooling1D, self).__init__(None,\n                                               pool_size,\n                                               strides,\n                                               padding,\n                                               list(input_shape) if input_shape else None,\n                                               **kwargs)\n\n\nclass GlobalAveragePooling1D(ZooKeras2Layer):\n\n    """"""\n    Applies global average pooling operation for temporal data.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> globalaveragepooling1d = GlobalAveragePooling1D(input_shape=(12, 12))\n    creating: createZooKeras2GlobalAveragePooling1D\n    """"""\n    def __init__(self,\n                 input_shape=None,\n                 **kwargs):\n\n        super(GlobalAveragePooling1D, self).__init__(None,\n                                                     list(input_shape) if input_shape else None,\n                                                     **kwargs)\n\n\nclass GlobalMaxPooling1D(ZooKeras2Layer):\n    """"""\n    Applies global max pooling operation for temporal data.\n    The input of this layer should be 3D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension) .\n\n    # Arguments\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> globalmaxpooling1d = GlobalMaxPooling1D(input_shape=(4, 8))\n    creating: createZooKeras2GlobalMaxPooling1D\n    """"""\n\n    def __init__(self, input_shape=None, **kwargs):\n        super(GlobalMaxPooling1D, self).__init__(None,\n                                                 list(input_shape) if input_shape else None,\n                                                 **kwargs)\n\n\nclass GlobalAveragePooling2D(ZooKeras2Layer):\n    """"""\n    Applies global average pooling operation for spatial data.\n    The input of this layer should be 4D.\n\n    When you use this layer as the first layer of a model, you need to provide the argument\n    input_shape (a shape tuple, does not include the batch dimension).\n\n    # Arguments\n    data_format: Format of input data. Either channels_first  or channels_last.\n\n    input_shape: A shape tuple, not including batch.\n    name: String to set the name of the layer.\n          If not specified, its name will by default to be a generated string.\n\n    >>> globalaveragepooling2d = GlobalAveragePooling2D(input_shape=(4, 32, 32))\n    creating: createZooKeras2GlobalAveragePooling2D\n    """"""\n    def __init__(self,\n                 data_format=""channels_first"",\n                 input_shape=None,\n                 **kwargs):\n        super(GlobalAveragePooling2D, self).__init__(None,\n                                                     data_format,\n                                                     list(input_shape) if input_shape else None,\n                                                     **kwargs)\n'"
pyzoo/zoo/pipeline/api/keras2/layers/recurrent.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n'"
pyzoo/zoo/pipeline/api/keras2/layers/wrappers.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport sys\n\nif sys.version >= \'3\':\n    long = int\n    unicode = str\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/__init__.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/abs.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.autograd as autograd\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass AbsMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(AbsMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        x = self.model_inputs[0].zvalue\n        return autograd.abs(x)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/add.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\n\n\nclass AddMapper(OperatorMapper):\n    def __init__(self, node, initializer, _all_tensors):\n        super(AddMapper, self).__init__(node, initializer, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of inputs\n        """"""\n        assert len(self._input_list) == 2, ""Add should have 2 inputs""\n        return [self._to_zoo_input(oi) for oi in self._input_list]\n\n    def _to_tensor(self):\n        x = self.model_inputs[0].zvalue\n        y = self.model_inputs[1].zvalue\n        return x + y\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/averagepool.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass AveragePoolMapper (OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(AveragePoolMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        assert len(self.model_inputs) == 1, ""AveragePool accept single input only""\n        rank = len(self.model_inputs[0].zvalue.shape)\n        if (rank == 4):  # NCHW\n            poolSize = [int(i) for i in self.onnx_attr[\'kernel_shape\']]\n            strides = [int(i) for i in self.onnx_attr[\'strides\']]\n            count_include_pad = bool(self.onnx_attr[\'count_include_pad\'])\\\n                if ""count_include_pad"" in self.onnx_attr else False\n            dim_ordering = ""th""\n            border_mode, pads = OnnxHelper.get_padds(self.onnx_attr)\n            averagepool2d = zlayers.AveragePooling2D(pool_size=poolSize,\n                                                     strides=strides,\n                                                     dim_ordering=dim_ordering,\n                                                     pads=pads,\n                                                     count_include_pad=count_include_pad)\n            return averagepool2d(self.model_inputs[0].zvalue)\n        else:\n            raise Exception(""not supported."")\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/batchnormalization.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport zoo.pipeline.api.autograd as autograd\nimport numpy as np\n\n\nclass BatchNormalizationMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(BatchNormalizationMapper, self).__init__(node, _params, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of OnnxInput\n        """"""\n        return [self._to_zoo_input(self._input_list[0])]\n\n    def _extract_trainable_values(self):\n        if len(self._input_list) == 5:\n            if isinstance(self._input_list[1].zvalue, autograd.Parameter):\n                return [self._input_list[1].zvalue.get_weight(),\n                        self._input_list[2].zvalue.get_weight()]\n            else:\n                return [self._input_list[1].zvalue, self._input_list[2].zvalue]\n        else:\n            return None\n\n    def to_zoo_format(self, trainable_values):\n        """"""\n        Convert ONNX _initializer to Zoo format\n        :return: list of ndarray\n        """"""\n        return trainable_values\n\n    def _to_tensor(self):\n        input = self.model_inputs[0]\n        rank = len(input.zvalue.shape)\n\n        if (rank == 4):\n            epsilon = float(self.onnx_attr[\'epsilon\']) if ""epsilon"" in self.onnx_attr else 0.001\n            momentum = float(self.onnx_attr[\'momentum\'] if ""momentum"" in self.onnx_attr else 0.99)\n            dim_ordering = ""th""\n            if len(self._input_list) == 5:\n                mean = self._input_list[3].zvalue\n                variance = self._input_list[4].zvalue\n            else:\n                mean = self._input_list[1].zvalue\n                variance = self._input_list[2].zvalue\n            batch_norm = zlayers.BatchNormalization(epsilon=epsilon,\n                                                    momentum=momentum,\n                                                    dim_ordering=dim_ordering)\n            norm_tensor = batch_norm(input.zvalue)\n            norm_tensor.node.element().set_running_mean(mean)\n            norm_tensor.node.element().set_running_std(variance)\n            return norm_tensor\n        else:\n            raise Exception(""not supported."")\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/cast.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport bigdl.nn.layer as blayer\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass CastMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(CastMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        identity = zlayers.KerasLayerWrapper(blayer.Identity())\n        return identity(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/clip.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.keras.layers import KerasLayerWrapper\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport numpy as np\nimport zoo.pipeline.api.keras.layers as zlayers\nfrom bigdl.nn.layer import Clamp\n\n\nclass ClipMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(ClipMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        assert len(self.model_inputs) == 1, ""Clip accept single input only""\n        min = int(self.onnx_attr[\'min\'])\n        max = int(self.onnx_attr[\'max\'])\n        assert min <= max, ""Min must be smaller or equal than Max""\n        clip = KerasLayerWrapper(Clamp(min, max))\n        return clip(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/concat.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass ConcatMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(ConcatMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        assert \'axis\' in self.onnx_attr, ""axis is a required attribute""\n        axis = int(self.onnx_attr[\'axis\'])\n        data = [i.zvalue for i in self.model_inputs]\n        return zlayers.Merge(mode=""concat"", concat_axis=axis)(data)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/constant.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport bigdl.nn.layer as blayer\nimport numpy as np\n\nimport zoo.pipeline.api.keras.layers as zlayers\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nfrom zoo.pipeline.api.onnx.onnx_loader import OnnxInput\n\n\nclass ConstantMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(ConstantMapper, self).__init__(node, _params, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of OnnxInput\n        """"""\n        input = OnnxInput(name=self.op_name, zvalue=OnnxHelper.to_numpy(self.onnx_attr[\'value\']))\n        return [self._to_zoo_input(input, is_constant=True)]\n\n    def _to_tensor(self):\n        return self.model_inputs[0].zvalue\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/conv.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport numpy as np\n\n\nclass ConvMapper(OperatorMapper):\n    def __init__(self, node, initializer, inputs):\n        super(ConvMapper, self).__init__(node, initializer, inputs)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of OnnxInput\n        """"""\n        return [self._to_zoo_input(self._input_list[0])]\n\n    def _extract_trainable_values(self):\n        if len(self._input_list) > 2:\n            return [self._input_list[1].zvalue, self._input_list[2].zvalue]\n        else:\n            return [self._input_list[1].zvalue]  # without bias\n\n    def to_zoo_format(self, trainable_values):\n        """"""\n        Convert ONNX _initializer to Zoo format\n        :return: list of ndarray\n        """"""\n        if len(trainable_values) > 1:\n            return [np.expand_dims(trainable_values[0], 0), trainable_values[1]]\n        else:\n            return np.expand_dims(trainable_values[0], 0)\n\n    def _to_tensor(self):\n        input = self.model_inputs[0]\n        W_weights = self.model_trainable_values[0]\n        rank = len(input.zvalue.shape)\n\n        if (rank == 4):  # NCHW\n            nb_filter = W_weights.shape[0]\n            nb_row = int(self.onnx_attr[\'kernel_shape\'][0])\n            nb_col = int(self.onnx_attr[\'kernel_shape\'][1])\n            subSample = [int(i) for i in\n                         self.onnx_attr[\'strides\']] if ""strides"" in self.onnx_attr else (1, 1)\n            dim_ordering = ""th""\n            assert \'dilations\' not in self.onnx_attr or self.onnx_attr[\'dilations\'] == (\n                1, 1), ""we only support dilations == (1, 1)""\n            assert \'group\' not in self.onnx_attr or self.onnx_attr[\n                \'group\'] == 1, ""we only support group == 1""\n            bias = True if (len(self._input_list) > 2) else False\n\n            border_mode, pads = OnnxHelper.get_padds(self.onnx_attr)\n\n            conv = zlayers.Convolution2D(nb_filter=nb_filter,\n                                         nb_row=nb_row,\n                                         nb_col=nb_col,\n                                         subsample=subSample,\n                                         dim_ordering=dim_ordering,\n                                         bias=bias,\n                                         border_mode=border_mode,\n                                         pads=pads)\n            return conv(input.zvalue)\n        else:\n            raise Exception(""not supported."")\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/div.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.autograd as autograd\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass DivMapper(OperatorMapper):\n    def __init__(self, node, initializer, _all_tensors):\n        super(DivMapper, self).__init__(node, initializer, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of inputs\n        """"""\n        assert len(self._input_list) == 2, ""Mul should have 2 inputs""\n        return [self._to_zoo_input(oi) for oi in self._input_list]\n\n    def _to_tensor(self):\n        x = self.model_inputs[0].zvalue\n        y = self.model_inputs[1].zvalue\n        return x / y\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/dropout.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport zoo.pipeline.api.keras.layers as zlayers\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_loader import OnnxLoader\n\n\nclass DropoutMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(DropoutMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        if ""mask"" in self.onnx_attr:\n            raise Exception(""We don\'t support mask for now"")\n        ratio = float(self.onnx_attr[""ratio""])\n        if (not OnnxLoader.training) and ""is_test"" in self.onnx_attr and self.onnx_attr[\'is_test\']:\n            return self.model_inputs[0].zvalue\n        dropout = zlayers.Dropout(p=ratio)\n        return dropout(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/elu.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass EluMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(EluMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        if ""alpha"" in self.onnx_attr:\n            alpha = float(self.onnx_attr[\'alpha\'])\n        else:\n            alpha = 1.0\n        elu = zlayers.ELU(alpha=alpha)\n        return elu(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/exp.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass ExpMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(ExpMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        exp = zlayers.Exp()\n        return exp(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/flatten.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass FlattenMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(FlattenMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        flatten = zlayers.Flatten()\n        return flatten(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/gather.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport zoo.pipeline.api.autograd as zautograd\nimport bigdl.nn.layer as blayer\n\n\nclass GatherMapper(OperatorMapper):\n    def __init__(self, node, initializer, _all_tensors):\n        super(GatherMapper, self).__init__(node, initializer, _all_tensors)\n\n    def _extract_model_inputs(self):\n        return [self._to_zoo_input(i) for i in self._input_list]\n\n    def _to_tensor(self):\n        data = self.model_inputs[0].zvalue\n        indices = self.model_inputs[1].zvalue\n\n        if self._initializer and isinstance(data, zautograd.Parameter):\n            embedding = zlayers.Embedding(input_dim=data.shape[0],\n                                          output_dim=data.shape[1],\n                                          weights=data.get_weight(),\n                                          input_length=indices.shape[1])\n            return embedding(indices)\n        else:\n            dim = int(self.onnx_attr[\'axis\'])\n            assert dim >= 1, ""Currently only dim>=1 is supported.""\n            assert indices.shape == (1,), ""Currently only one index is supported.""\n            index = int(indices.get_weight().max())\n            return zautograd.expand_dims(data.index_select(dim=dim, index=index), axis=dim)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/gemm.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport numpy as np\n\nimport zoo.pipeline.api.keras.layers as zlayers\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\n\n\nclass GemmMapper(OperatorMapper):\n    def __init__(self, node, initializer, _all_tensors):\n        super(GemmMapper, self).__init__(node, initializer, _all_tensors)\n\n    def _extract_model_inputs(self):\n        return [self._to_zoo_input(self._input_list[0])]\n\n    def _extract_trainable_values(self):\n        y = self._input_list[1]\n        z = self._input_list[2]\n\n        if ""transB"" in self.onnx_attr and self.onnx_attr[\'transB\']:\n            y.zvalue = np.transpose(y.zvalue)\n        alpha = self.onnx_attr[""alpha""] if ""alpha"" in self.onnx_attr else 1.0\n        beta = self.onnx_attr[""beta""] if ""beta"" in self.onnx_attr else 1.0\n        return [alpha * y.zvalue, beta * z.zvalue]\n\n    def to_zoo_format(self, trainable_values):\n        """"""\n        Convert ONNX _initializer to Zoo format\n        :return: list of ndarray\n        """"""\n\n        # The format of weight in BigDL is : input * output, so we need to transpose the y here.\n        # There\'s no exception if you don\'t transpose it\n        # as the `set_weights` method doesn\'t check the shape and respect the total items only.\n        return [np.transpose(trainable_values[0]), trainable_values[1]]\n\n    def _to_tensor(self):\n        x = self.model_inputs[0]\n        z = self.model_trainable_values[1]\n        assert len(x.zvalue.shape) == 2, ""we only accept 2D input""\n\n        if ""transA"" in self.onnx_attr and self.onnx_attr[\'transA\']:\n            # TODO: add transpose operator for this x = x.transpose()\n            raise Exception(""we don\'t support this for now"")\n        layer = zlayers.Dense(len(z))\n        return layer(x.zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/globalaveragepool.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport zoo.pipeline.api.autograd as zautograd\nimport numpy as np\n\n\nclass GlobalAveragePoolMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(GlobalAveragePoolMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        x = self.model_inputs[0].zvalue\n        y = zlayers.GlobalAveragePooling2D()(x)\n        \'\'\'\n        Input data tensor from the previous operator; dimensions for image case are (N x C x H x W),\n        where N is the batch size, C is the number of channels, and H and W are the height\n        and the width of the data.\n        Output data tensor from pooling across the input tensor. Dimensions will be N x C x 1 x 1.\n        \'\'\'\n        return zautograd.expand_dims(zautograd.expand_dims(y, axis=2), axis=3)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/greater.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport bigdl.nn.layer as blayer\n\n\nclass GreaterMapper(OperatorMapper):\n    def __init__(self, node, initializer, _all_tensors):\n        super(GreaterMapper, self).__init__(node, initializer, _all_tensors)\n\n    def _to_tensor(self):\n        x = self.model_inputs[0].zvalue\n        y = self.model_inputs[1].zvalue\n        truemap = zlayers.KerasLayerWrapper(blayer.Threshold(th=0.0, v=0.0))(x - y)\n        falsemap = zlayers.KerasLayerWrapper(blayer.Threshold(th=-1e-6, v=1.0))(-truemap)\n        return falsemap\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/hardsigmoid.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass HardSigmoidMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(HardSigmoidMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        hardsigmoid = zlayers.Activation(""hard_sigmoid"")\n        return hardsigmoid(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/leakyrelu.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport numpy as np\n\n\nclass LeakyReluMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(LeakyReluMapper, self).__init__(node, _params, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of OnnxInput\n        """"""\n        input = self._input_list[0]\n        if isinstance(input.zvalue, np.ndarray):\n            self.is_batch = False\n            return [self._to_zoo_input(input, is_constant=True)]\n        else:\n            self.is_batch = True\n            return [self._to_zoo_input(input)]\n\n    def _to_tensor(self):\n        alpha = 0.01\n\n        if ""alpha"" in self.onnx_attr.keys():\n            alpha = self.onnx_attr[\'alpha\']\n\n        leakyrelu = zlayers.LeakyReLU(alpha)\n        return leakyrelu(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/log.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass LogMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(LogMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        log = zlayers.Log()\n        return log(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/logsoftmax.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass LogSoftmaxMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(LogSoftmaxMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        assert len(self.model_inputs) == 1, ""LogSoftmax accept single input only""\n        assert int(self.onnx_attr[\'axis\']) == 1, ""LofSoftware only accept the default axis""\n        rank = len(self.model_inputs[0].zvalue.shape)\n        if (rank == 2):\n            logsoftmax = zlayers.Activation(""log_softmax"")\n            return logsoftmax(self.model_inputs[0].zvalue)\n        else:\n            raise Exception(""not supported."")\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/lrn.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass LRNMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(LRNMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        data = self.model_inputs[0].zvalue\n        alpha = float(self.onnx_attr[\'alpha\']) if ""alpha"" in self.onnx_attr else 0.0001\n        beta = float(self.onnx_attr[\'beta\']) if ""beta"" in self.onnx_attr else 0.75\n        bias = float(self.onnx_attr[\'bias\']) if ""bias"" in self.onnx_attr else 1.0\n        size = int(self.onnx_attr[\'size\'])\n        dim_ordering = ""th""\n\n        return zlayers.LRN2D(alpha=alpha, k=bias, beta=beta,\n                             n=size, dim_ordering=dim_ordering)(data)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/matmul.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport zoo.pipeline.api.autograd as zautograd\nimport numpy as np\n\n\nclass MatMulMapper(OperatorMapper):\n    def __init__(self, node, initializer, inputs):\n        super(MatMulMapper, self).__init__(node, initializer, inputs)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of OnnxInput\n        """"""\n        def gen_input(input):\n            if isinstance(input.zvalue, np.ndarray):\n                return self._to_zoo_input(input, is_constant=True)\n            else:\n                return self._to_zoo_input(input)\n        return [gen_input(i) for i in self._input_list]\n\n    def _to_tensor(self):\n        x = self.model_inputs[0]\n        y = self.model_inputs[1]\n        return zautograd.mm(x.zvalue, y.zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/maxpool.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport numpy as np\n\n\nclass MaxPoolMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(MaxPoolMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        assert len(self.model_inputs) == 1, ""MaxPool accept single input only""\n        rank = len(self.model_inputs[0].zvalue.shape)\n        if ""storage_order"" in self.onnx_attr.keys():\n            assert self.onnx_attr[\'storage_order\'] == 0\n\n        if (rank == 4):  # NCHW\n            pool_size = [int(i) for i in self.onnx_attr[\'kernel_shape\']]\n            if ""strides"" in self.onnx_attr.keys():\n                strides = [int(i) for i in self.onnx_attr[\'strides\']]\n            else:\n                strides = [1 for i in self.onnx_attr[\'kernel_shape\']]\n\n            border_mode, pads = OnnxHelper.get_padds(self.onnx_attr)\n\n            maxpool = zlayers.MaxPooling2D(pool_size=pool_size,\n                                           strides=strides,\n                                           border_mode=border_mode,\n                                           pads=pads)\n            return maxpool(self.model_inputs[0].zvalue)\n        elif (rank == 3):\n            pool_length = int(self.onnx_attr[\'kernel_shape\'][0])\n            if ""strides"" in self.onnx_attr.keys():\n                stride = int(self.onnx_attr[\'strides\'][0])\n            else:\n                stride = 1\n            border_mode, pads = OnnxHelper.get_padds(self.onnx_attr)\n            if border_mode is None and pads is None:\n                border_mode = \'valid\'\n            if pads is None:\n                pads = 0\n            permute = zlayers.Permute(dims=(2, 1))(self.model_inputs[0].zvalue)\n            maxpool = zlayers.MaxPooling1D(pool_length=pool_length,\n                                           stride=stride,\n                                           border_mode=border_mode,\n                                           pad=pads)(permute)\n            return zlayers.Permute(dims=(2, 1))(maxpool)\n        else:\n            raise Exception(""not supported."")\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/mul.py,0,"b'\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.autograd as autograd\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass MulMapper(OperatorMapper):\n    def __init__(self, node, initializer, _all_tensors):\n        super(MulMapper, self).__init__(node, initializer, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of inputs\n        """"""\n        assert len(self._input_list) == 2, ""Mul should have 2 inputs""\n        return [self._to_zoo_input(oi) for oi in self._input_list]\n\n    def _to_tensor(self):\n        x = self.model_inputs[0].zvalue\n        y = self.model_inputs[1].zvalue\n        return x * y\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/neg.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass NegMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(NegMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        neg = zlayers.Negative()\n        return neg(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/operator_mapper.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport zoo.pipeline.api.keras.layers as zlayers\nimport zoo.pipeline.api.autograd as zautograd\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.onnx.mapper\nimport importlib\nimport numpy as np\n\n\nclass OperatorMapper(object):\n    # converting NodeProto message\n    # we don\'t differentiate the data input and weights here, they are all included into inputs.\n    def __init__(self, node, initializer, inputs):\n        self.node = node\n        self.op_name = node.op_type\n        node_name = node.name.strip()\n        # it would be None if user doesn\'t set it manually\n        self.node_name = node_name if node_name else None\n        self.onnx_attr = OnnxHelper.parse_attr(node.attribute)  # dict(name: value)\n        self._initializer = initializer\n        self._input_list = inputs\n        self.model_inputs = self._extract_model_inputs()\n        self.model_trainable_values = self._extract_trainable_values()\n        self.output = node.output[0]\n\n    @staticmethod\n    def of(node, _params, inputs):\n        m = importlib.import_module(""zoo.pipeline.api.onnx.mapper."" + node.op_type.lower())\n        cls = getattr(m, node.op_type + ""Mapper"")\n        return cls(node, _params, inputs)\n\n    def _to_zoo_input(self, input, is_constant=None):\n        is_parameter = True if input.name in self._initializer else False\n        if isinstance(input.zvalue, zautograd.Variable) or isinstance(input.zvalue,\n                                                                      zautograd.Parameter):\n            return input\n        if isinstance(input.zvalue, np.ndarray):\n            if is_parameter or is_constant:\n                shape = input.zvalue.shape\n            else:\n                shape = input.zvalue.shape[1:]\n        elif isinstance(input.zvalue, list):\n            if is_parameter or is_constant:\n                shape = input.zvalue\n            else:\n                shape = input.zvalue[1:]\n        else:\n            raise Exception(""not supported type "" + str(type(input.zvalue)))\n\n        input.data = input.zvalue\n        if is_constant:\n            input.zvalue = zautograd.Parameter(shape=shape, init_weight=input.zvalue,\n                                               trainable=False)\n        elif is_parameter:\n            input.zvalue = zautograd.Parameter(shape=shape, init_weight=input.zvalue, )\n        else:\n            input.zvalue = zlayers.Input(\n                shape=shape, name=input.name)\n        return input\n\n    def to_tensor(self):\n        """"""\n        Convert a node to tensor\n        """"""\n        out_tensor = self._to_tensor()\n        if self.node_name:\n            out_tensor.set_name(self.node_name)\n        assert isinstance(out_tensor, zautograd.Variable) or isinstance(out_tensor,\n                                                                        zautograd.Parameter)\n        if self.model_trainable_values:\n            out_tensor.node.element().set_weights(\n                self.to_zoo_format(self.model_trainable_values))\n        return out_tensor\n\n    def _to_tensor(self):\n        raise Exception(""Please define the content"")\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of OnnxInput\n        """"""\n        return [self._to_zoo_input(i) for i in self._input_list]\n\n    def _extract_trainable_values(self):\n        """"""\n        :return: list of ndarray for weights\n        """"""\n        return None\n\n    def to_zoo_format(self, trainable_values):\n        """"""\n        Convert ONNX _initializer to Zoo format\n        :return: list of ndarray\n        """"""\n        return trainable_values\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/pow.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport numpy as np\n\n\nclass PowMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(PowMapper, self).__init__(node, _params, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of OnnxInput\n        """"""\n        return [self._to_zoo_input(self._input_list[0])]\n\n    def _to_tensor(self):\n        exponent = self._input_list[1].zvalue.get_weight()[0]\n        pow = zlayers.Power(exponent)\n        return pow(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/reducemean.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport zoo.pipeline.api.autograd as autograd\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass ReduceMeanMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(ReduceMeanMapper, self).__init__(node, _params, _all_tensors)\n\n    def _extract_model_inputs(self):\n        return [self._to_zoo_input(oi) for oi in self._input_list]\n\n    def _to_tensor(self):\n        input = self.model_inputs[0].zvalue\n        assert len(self.onnx_attr[\'axes\']) == 1, ""we only support axes with 1 elements for now""\n        axes = self.onnx_attr[\'axes\'][0]\n        keepdims = True if self.onnx_attr[\'keepdims\'] == 1 else False\n        return autograd.mean(input, axis=int(axes), keepDims=keepdims)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/reducesum.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport zoo.pipeline.api.autograd as autograd\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass ReduceSumMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(ReduceSumMapper, self).__init__(node, _params, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of inputs\n        """"""\n        assert len(self._input_list) > 0\n        return [self._to_zoo_input(oi) for oi in self._input_list]\n\n    def _to_tensor(self):\n        input = self.model_inputs[0].zvalue\n        assert len(self.onnx_attr[\'axes\']) == 1, ""we only support axes with 1 elements for now""\n        axes = self.onnx_attr[\'axes\'][0]\n        keepdims = True if self.onnx_attr[\'keepdims\'] == 1 else False\n        return autograd.sum(input, axis=int(axes), keepDims=keepdims)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/relu.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass ReluMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(ReluMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        relu = zlayers.Activation(""relu"")\n        return relu(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/reshape.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport bigdl.nn.layer as blayer\nimport numpy as np\n\nimport zoo.pipeline.api.keras.layers as zlayers\nfrom zoo.pipeline.api.autograd import Parameter\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\n\n\nclass ReshapeMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(ReshapeMapper, self).__init__(node, _params, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of OnnxInput\n        """"""\n        input = self._input_list[0]\n        if isinstance(input.zvalue, np.ndarray):\n            self.is_batch = False\n            return [self._to_zoo_input(input, is_constant=True)]\n        else:\n            self.is_batch = True\n            return [self._to_zoo_input(input)]\n\n    def _to_tensor(self):\n        data = self.model_inputs[0].zvalue\n        origin_target = self._input_list[1].zvalue\n        target = origin_target.get_weight() if isinstance(origin_target, Parameter) else \\\n            origin_target\n        if self.is_batch:\n            targetshape = [int(i) for i in target][1:]\n            return zlayers.Reshape(targetshape)(self.model_inputs[0].zvalue)\n        else:\n            targetshape = [int(i) for i in target]\n            return zlayers.KerasLayerWrapper(blayer.Reshape(targetshape, batch_mode=False))(data)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/shape.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport bigdl.nn.layer as blayer\n\n\nclass ShapeMapper(OperatorMapper):\n    def __init__(self, node, initializer, _all_tensors):\n        super(ShapeMapper, self).__init__(node, initializer, _all_tensors)\n\n    def _to_tensor(self):\n        data = self.model_inputs[0].zvalue\n        return zlayers.GetShape()(data)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/sigmoid.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass SigmoidMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(SigmoidMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        sigmoid = zlayers.Activation(""sigmoid"")\n        return sigmoid(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/slice.py,0,"b'# -*- coding: utf-8 -*\n#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport zoo.pipeline.api.autograd as autograd\nimport bigdl.nn.layer as blayer\n\n\nclass SliceMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(SliceMapper, self).__init__(node, _params, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of OnnxInput\n        """"""\n        return [self._to_zoo_input(self._input_list[0])]\n\n    def _to_tensor(self):\n        lens = []\n        input = self.model_inputs[0].zvalue\n        ends = self.onnx_attr[\'ends\']\n        starts = self.onnx_attr[\'starts\']\n        if ""axes"" in self.onnx_attr.keys():\n            axes = self.onnx_attr[\'axes\']\n        else:\n            axes = range(len(starts))\n        for j in range(len(starts)):\n            lens.append(ends[j] - starts[j])\n            # slice(, len=-1) equals to slice(, len=length)\n            # y = x[:2,0:-1] means start is(0,0) and ends is(2,-1)\n            # which is equivalent to slice(,len=-2) as ""end=-1"" is exclusive here.\n            if lens[j] < 0:\n                lens[j] -= 1\n        for i in range(len(starts)):\n            input = input.slice(dim=int(axes[i]), start_index=int(starts[i]), length=int(lens[i]))\n        return input\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/softmax.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass SoftmaxMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(SoftmaxMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        softmax = zlayers.Activation(""softmax"")\n        return softmax(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/sqrt.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass SqrtMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(SqrtMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        sqrt = zlayers.Sqrt()\n        return sqrt(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/squeeze.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport numpy as np\nfrom zoo.pipeline.api.autograd import Parameter\nimport bigdl.nn.layer as blayer\n\n\nclass SqueezeMapper(OperatorMapper):\n    def __init__(self, node, initializer, _all_tensors):\n        super(SqueezeMapper, self).__init__(node, initializer, _all_tensors)\n\n    def _to_tensor(self):\n        dim = None\n        if ""axes"" in self.onnx_attr.keys():\n            dim = tuple([int(i) for i in self.onnx_attr[\'axes\']])\n\n        data = self.model_inputs[0].zvalue\n\n        return data.squeeze(dim=dim)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/sub.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\n\n\nclass SubMapper(OperatorMapper):\n    def __init__(self, node, initializer, _all_tensors):\n        super(SubMapper, self).__init__(node, initializer, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of inputs\n        """"""\n        assert len(self._input_list) == 2, ""Sub should have 2 inputs""\n        return [self._to_zoo_input(oi) for oi in self._input_list]\n\n    def _to_tensor(self):\n        x = self.model_inputs[0].zvalue\n        y = self.model_inputs[1].zvalue\n        return x - y\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/tanh.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nfrom zoo.pipeline.api.onnx.onnx_helper import OnnxHelper\nimport zoo.pipeline.api.keras.layers as zlayers\n\n\nclass TanhMapper(OperatorMapper):\n    def __init__(self, node, _params, _all_tensors):\n        super(TanhMapper, self).__init__(node, _params, _all_tensors)\n\n    def _to_tensor(self):\n        tanh = zlayers.Activation(""tanh"")\n        return tanh(self.model_inputs[0].zvalue)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/transpose.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport bigdl.nn.layer as blayer\n\n\nclass TransposeMapper(OperatorMapper):\n    def __init__(self, node, initializer, _all_tensors):\n        super(TransposeMapper, self).__init__(node, initializer, _all_tensors)\n\n    def _to_tensor(self):\n        data = self.model_inputs[0].zvalue\n        if ""perm"" in self.onnx_attr:\n            perm = self.onnx_attr[\'perm\']\n        else:\n            perm = [int(i) for i in range(len(data.shape)-1, -1, -1)]\n\n        assert perm[0] == 0, ""Currently transpose 0 is not supported.""\n        dims = [int(i) for i in perm[1:]]\n        return zlayers.Permute(dims)(data)\n'"
pyzoo/zoo/pipeline/api/onnx/mapper/unsqueeze.py,0,"b'#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom zoo.pipeline.api.onnx.mapper.operator_mapper import OperatorMapper\nimport zoo.pipeline.api.keras.layers as zlayers\nimport numpy as np\nfrom zoo.pipeline.api.autograd import Parameter\nimport bigdl.nn.layer as blayer\nimport zoo.pipeline.api.autograd as autograd\n\n\nclass UnsqueezeMapper(OperatorMapper):\n    def __init__(self, node, initializer, _all_tensors):\n        super(UnsqueezeMapper, self).__init__(node, initializer, _all_tensors)\n\n    def _extract_model_inputs(self):\n        """"""\n        :return: list of OnnxInput\n        """"""\n        input = self._input_list[0]\n        if isinstance(input.zvalue, np.ndarray):\n            self.is_batch = False\n            return [self._to_zoo_input(input, is_constant=True)]\n        else:\n            self.is_batch = True\n            return [self._to_zoo_input(input)]\n\n    def _to_tensor(self):\n        data = self.model_inputs[0].zvalue\n        dim = sorted(tuple([int(i) for i in self.onnx_attr[\'axes\']]))\n        for i in dim:\n            data = autograd.expand_dims(data, axis=i)\n        return data\n'"
pyzoo/zoo/examples/tensorflow/tfpark/inception/nets/__init__.py,0,b'# This file is adapted from\n# https://github.com/tensorflow/models/blob/master/research/slim/nets/__init__.py\n'
pyzoo/zoo/examples/tensorflow/tfpark/inception/nets/inception_utils.py,0,"b'# This file is adapted from\n# https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_utils.py\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001,\n                        activation_fn=tf.nn.relu,\n                        batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,\n                        batch_norm_scale=False):\n    """"""Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n    activation_fn: Activation function for conv2d.\n    batch_norm_updates_collections: Collection for the update ops for\n      batch norm.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  """"""\n    batch_norm_params = {\n        # Decay for the moving averages.\n        \'decay\': batch_norm_decay,\n        # epsilon to prevent 0s in variance.\n        \'epsilon\': batch_norm_epsilon,\n        # collection containing update_ops.\n        \'updates_collections\': batch_norm_updates_collections,\n        # use fused batch norm if possible.\n        \'fused\': None,\n        \'scale\': batch_norm_scale,\n    }\n    if use_batch_norm:\n        normalizer_fn = slim.batch_norm\n        normalizer_params = batch_norm_params\n    else:\n        normalizer_fn = None\n        normalizer_params = {}\n    # Set weight_decay for weights in Conv and FC layers.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_regularizer=slim.l2_regularizer(weight_decay)):\n        with slim.arg_scope(\n                [slim.conv2d],\n                weights_initializer=slim.variance_scaling_initializer(),\n                activation_fn=activation_fn,\n                normalizer_fn=normalizer_fn,\n                normalizer_params=normalizer_params) as sc:\n            return sc\n'"
pyzoo/zoo/examples/tensorflow/tfpark/inception/nets/inception_v1.py,0,"b'# This file is adapted from\n# https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v1.py\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v1 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v1_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      include_root_block=True,\n                      scope=\'InceptionV1\'):\n    """"""Defines the Inception V1 base architecture.\n\n  This architecture is defined in:\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n      \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n      \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\', \'Mixed_5c\']. If\n      include_root_block is False, [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\',\n      \'Conv2d_2b_1x1\', \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\'] will not be available.\n    include_root_block: If True, include the convolution and max-pooling layers\n      before the inception modules. If False, excludes those layers.\n    scope: Optional variable_scope.\n\n  Returns:\n    A dictionary from components of the network to the corresponding activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values.\n  """"""\n    end_points = {}\n    with tf.variable_scope(scope, \'InceptionV1\', [inputs]):\n        with slim.arg_scope(\n                [slim.conv2d, slim.fully_connected],\n                weights_initializer=tf.contrib.layers.xavier_initializer(),\n                biases_initializer=tf.initializers.constant(0.1)):\n            with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                                stride=1, padding=\'SAME\'):\n                net = inputs\n                if include_root_block:\n                    end_point = \'Conv2d_1a_7x7\'\n                    net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n                    end_points[end_point] = net\n                    if final_endpoint == end_point:\n                        return net, end_points\n                    end_point = \'MaxPool_2a_3x3\'\n                    net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n                    net = tf.nn.local_response_normalization(net, 5, 0.0001, 0.75)\n                    end_points[end_point] = net\n                    if final_endpoint == end_point:\n                        return net, end_points\n                    end_point = \'Conv2d_2b_1x1\'\n                    net = slim.conv2d(net, 64, [1, 1], scope=end_point)\n                    end_points[end_point] = net\n                    if final_endpoint == end_point:\n                        return net, end_points\n                    end_point = \'Conv2d_2c_3x3\'\n                    net = slim.conv2d(net, 192, [3, 3], scope=end_point)\n                    end_points[end_point] = net\n                    net = tf.nn.local_response_normalization(net, 5, 0.0001, 0.75)\n                    if final_endpoint == end_point:\n                        return net, end_points\n                    end_point = \'MaxPool_3a_3x3\'\n                    net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n                    end_points[end_point] = net\n                    if final_endpoint == end_point:\n                        return net, end_points\n\n                end_point = \'Mixed_3b\'\n                with tf.variable_scope(end_point):\n                    with tf.variable_scope(\'Branch_0\'):\n                        branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_2 = slim.conv2d(branch_2, 32, [5, 5], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_3\'):\n                        branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n                        branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\'Conv2d_0b_1x1\')\n                    net = tf.concat(\n                        axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n                end_points[end_point] = net\n                if final_endpoint == end_point:\n                    return net, end_points\n\n                end_point = \'Mixed_3c\'\n                with tf.variable_scope(end_point):\n                    with tf.variable_scope(\'Branch_0\'):\n                        branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_2 = slim.conv2d(branch_2, 96, [5, 5], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_3\'):\n                        branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n                        branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n                    net = tf.concat(\n                        axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n                end_points[end_point] = net\n                if final_endpoint == end_point:\n                    return net, end_points\n\n                end_point = \'MaxPool_4a_3x3\'\n                net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n                end_points[end_point] = net\n                if final_endpoint == end_point:\n                    return net, end_points\n\n                end_point = \'Mixed_4b\'\n                with tf.variable_scope(end_point):\n                    with tf.variable_scope(\'Branch_0\'):\n                        branch_0 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_2 = slim.conv2d(branch_2, 48, [5, 5], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_3\'):\n                        branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n                        branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n                    net = tf.concat(\n                        axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n                end_points[end_point] = net\n                if final_endpoint == end_point:\n                    return net, end_points\n\n                end_point = \'Mixed_4c\'\n                with tf.variable_scope(end_point):\n                    with tf.variable_scope(\'Branch_0\'):\n                        branch_0 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        branch_1 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_2 = slim.conv2d(branch_2, 64, [5, 5], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_3\'):\n                        branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n                        branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n                    net = tf.concat(\n                        axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n                end_points[end_point] = net\n                if final_endpoint == end_point:\n                    return net, end_points\n\n                end_point = \'Mixed_4d\'\n                with tf.variable_scope(end_point):\n                    with tf.variable_scope(\'Branch_0\'):\n                        branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_2 = slim.conv2d(branch_2, 64, [5, 5], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_3\'):\n                        branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n                        branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n                    net = tf.concat(\n                        axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n                end_points[end_point] = net\n                if final_endpoint == end_point:\n                    return net, end_points\n\n                end_point = \'Mixed_4e\'\n                with tf.variable_scope(end_point):\n                    with tf.variable_scope(\'Branch_0\'):\n                        branch_0 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        branch_1 = slim.conv2d(net, 144, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_2 = slim.conv2d(branch_2, 64, [5, 5], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_3\'):\n                        branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n                        branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n                    net = tf.concat(\n                        axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n                end_points[end_point] = net\n                if final_endpoint == end_point:\n                    return net, end_points\n\n                end_point = \'Mixed_4f\'\n                with tf.variable_scope(end_point):\n                    with tf.variable_scope(\'Branch_0\'):\n                        branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_2 = slim.conv2d(branch_2, 128, [5, 5], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_3\'):\n                        branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n                        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n                    net = tf.concat(\n                        axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n                end_points[end_point] = net\n                if final_endpoint == end_point:\n                    return net, end_points\n\n                end_point = \'MaxPool_5a_2x2\'\n                net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n                end_points[end_point] = net\n                if final_endpoint == end_point:\n                    return net, end_points\n\n                end_point = \'Mixed_5b\'\n                with tf.variable_scope(end_point):\n                    with tf.variable_scope(\'Branch_0\'):\n                        branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_2 = slim.conv2d(branch_2, 128, [5, 5], scope=\'Conv2d_0a_3x3\')\n                    with tf.variable_scope(\'Branch_3\'):\n                        branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n                        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n                    net = tf.concat(\n                        axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n                end_points[end_point] = net\n                if final_endpoint == end_point:\n                    return net, end_points\n\n                end_point = \'Mixed_5c\'\n                with tf.variable_scope(end_point):\n                    with tf.variable_scope(\'Branch_0\'):\n                        branch_0 = slim.conv2d(net, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n                    with tf.variable_scope(\'Branch_1\'):\n                        branch_1 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_2\'):\n                        branch_2 = slim.conv2d(net, 48, [1, 1], scope=\'Conv2d_0a_1x1\')\n                        branch_2 = slim.conv2d(branch_2, 128, [5, 5], scope=\'Conv2d_0b_3x3\')\n                    with tf.variable_scope(\'Branch_3\'):\n                        branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n                        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n                    net = tf.concat(\n                        axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n                end_points[end_point] = net\n                if final_endpoint == end_point:\n                    return net, end_points\n        raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v1(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV1\',\n                 global_pool=False):\n    """"""Defines the Inception V1 architecture.\n\n  This architecture is defined in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is of\n        shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    global_pool: Optional boolean flag to control the avgpooling before the\n      logits layer. If false or unset, pooling is done with a fixed window\n      that reduces default-sized inputs to 1x1, while larger inputs lead to\n      larger outputs. If true, any input size is pooled down to 1x1.\n\n  Returns:\n    net: a Tensor with the logits (pre-softmax activations) if num_classes\n      is a non-zero integer, or the non-dropped-out input to the logits layer\n      if num_classes is 0 or None.\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n    # Final pooling and prediction\n    with tf.variable_scope(scope, \'InceptionV1\', [inputs], reuse=reuse) as scope:\n        with slim.arg_scope([slim.batch_norm, slim.dropout],\n                            is_training=is_training):\n            net, end_points = inception_v1_base(inputs, scope=scope)\n            with tf.variable_scope(\'Logits\'):\n                if global_pool:\n                    # Global average pooling.\n                    net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=\'global_pool\')\n                    end_points[\'global_pool\'] = net\n                else:\n                    # Pooling with a fixed kernel size.\n                    net = slim.avg_pool2d(net, [7, 7], stride=1, scope=\'AvgPool_0a_7x7\')\n                    end_points[\'AvgPool_0a_7x7\'] = net\n                if not num_classes:\n                    return net, end_points\n                net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_0b\')\n                logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                                     normalizer_fn=None, scope=\'Conv2d_0c_1x1\')\n                if spatial_squeeze:\n                    logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n\n                end_points[\'Logits\'] = logits\n                end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n    return logits, end_points\n\n\ninception_v1.default_image_size = 224\n\ninception_v1_arg_scope = inception_utils.inception_arg_scope\n'"
