file_path,api_count,code
configs.py,0,"b'# -*- coding: utf-8 -*-\n\nimport argparse\nfrom pathlib import Path\nimport pprint\n\nproject_dir = Path(__file__).resolve().parent\ndataset_dir = Path(\'/data1/jysung710/tmp_sum/360video/\').resolve()\nvideo_list = [\'360airballoon\', \'360parade\', \'360rowing\', \'360scuba\', \'360wedding\']\nsave_dir = Path(\'/data1/jmcho/SUM_GAN/\')\nscore_dir = Path(\'/data1/common_datasets/tmp_sum/360video/results/SUM-GAN/\')\n\n\ndef str2bool(v):\n    """"""string to boolean""""""\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\nclass Config(object):\n    def __init__(self, **kwargs):\n        """"""Configuration Class: set kwargs as class attributes with setattr""""""\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n        self.set_dataset_dir(self.video_type)\n\n    def set_dataset_dir(self, video_type=\'360airballon\'):\n        if self.preprocessed:\n            self.video_root_dir = dataset_dir.joinpath(\'resnet101_feature\', video_type, self.mode)\n        else:\n            self.video_root_dir = dataset_dir.joinpath(\'video_subshot\', video_type, \'test\')\n        self.save_dir = save_dir.joinpath(video_type)\n        self.log_dir = self.save_dir\n        self.ckpt_path = self.save_dir.joinpath(f\'epoch-{self.epoch}.pkl\')\n        self.score_dir = score_dir\n\n    def __repr__(self):\n        """"""Pretty-print configurations in alphabetical order""""""\n        config_str = \'Configurations\\n\'\n        config_str += pprint.pformat(self.__dict__)\n        return config_str\n\n\ndef get_config(parse=True, **optional_kwargs):\n    """"""\n    Get configurations as attributes of class\n    1. Parse configurations with argparse.\n    2. Create Config class initilized with parsed kwargs.\n    3. Return Config class.\n    """"""\n    parser = argparse.ArgumentParser()\n\n    # Mode\n    parser.add_argument(\'--mode\', type=str, default=\'train\')\n    parser.add_argument(\'--verbose\', type=str2bool, default=\'true\')\n    parser.add_argument(\'--preprocessed\', type=str2bool, default=\'True\')\n    parser.add_argument(\'--video_type\', type=str, default=\'360airballoon\')\n\n    # Model\n    parser.add_argument(\'--input_size\', type=int, default=2048)\n    parser.add_argument(\'--hidden_size\', type=int, default=500)\n    parser.add_argument(\'--num_layers\', type=int, default=2)\n    parser.add_argument(\'--summary_rate\', type=float, default=0.3)\n\n    # Train\n    parser.add_argument(\'--n_epochs\', type=int, default=50)\n    parser.add_argument(\'--clip\', type=float, default=5.0)\n    parser.add_argument(\'--lr\', type=float, default=1e-4)\n    parser.add_argument(\'--discriminator_lr\', type=float, default=1e-5)\n    parser.add_argument(\'--discriminator_slow_start\', type=int, default=15)\n\n    # load epoch\n    parser.add_argument(\'--epoch\', type=int, default=2)\n\n    if parse:\n        kwargs = parser.parse_args()\n    else:\n        kwargs = parser.parse_known_args()[0]\n\n    # Namespace => Dictionary\n    kwargs = vars(kwargs)\n    kwargs.update(optional_kwargs)\n\n    return Config(**kwargs)\n\n\nif __name__ == \'__main__\':\n    config = get_config()\n    import ipdb\n    ipdb.set_trace()\n'"
data_loader.py,4,"b""# -*- coding: utf-8 -*-\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets.folder import default_loader\nfrom pathlib import Path\n\nfrom feature_extraction import resnet_transform\nimport h5py\nimport numpy as np\n\n\nclass VideoData(Dataset):\n    def __init__(self, root, preprocessed=True, transform=resnet_transform, with_name=False):\n        self.root = root\n        self.preprocessed = preprocessed\n        self.transform = transform\n        self.with_name = with_name\n        self.video_list = list(self.root.iterdir())\n\n    def __len__(self):\n        return len(self.video_list)\n\n    def __getitem__(self, index):\n        if self.preprocessed:\n            image_path = self.video_list[index]\n            with h5py.File(image_path, 'r') as f:\n                if self.with_name:\n                    return torch.Tensor(np.array(f['pool5'])), image_path.name[:-5]\n                else:\n                    return torch.Tensor(np.array(f['pool5']))\n\n        else:\n            images = []\n            for img_path in Path(self.video_list[index]).glob('*.jpg'):\n                img = default_loader(img_path)\n                img_tensor = self.transform(img)\n                images.append(img_tensor)\n\n            return torch.stack(images), img_path.parent.name[4:]\n\n\ndef get_loader(root, mode):\n    if mode.lower() == 'train':\n        return DataLoader(VideoData(root), batch_size=1)\n    else:\n        return VideoData(root, with_name=True)\n\n\nif __name__ == '__main__':\n    pass\n"""
feature_extraction.py,1,"b'# -*- coding: utf-8 -*-\n\nfrom PIL import Image\nfrom torchvision import transforms, models\nimport torch.nn as nn\n\n\nclass Rescale(object):\n    """"""Rescale a image to a given size.\n\n    Args:\n        output_size (tuple or tuple): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    """"""\n\n    def __init__(self, *output_size):\n        self.output_size = output_size\n\n    def __call__(self, image):\n        """"""\n        Args:\n            image (PIL.Image) : PIL.Image object to rescale\n        """"""\n        new_h, new_w = self.output_size\n        new_h, new_w = int(new_h), int(new_w)\n        img = image.resize((new_w, new_h), resample=Image.BILINEAR)\n        return img\n\n\nclass ResNetFeature(nn.Module):\n    def __init__(self, feature=\'resnet101\'):\n        """"""\n        Args:\n            feature (string): resnet101 or resnet152\n        """"""\n        super(ResNetFeature, self).__init__()\n        if feature == \'resnet101\':\n            resnet = models.resnet101(pretrained=True)\n        else:\n            resnet = models.resnet152(pretrained=True)\n        resnet.float()\n        resnet.cuda()\n        resnet.eval()\n\n        module_list = list(resnet.children())\n        self.conv5 = nn.Sequential(*module_list[:-2])\n        self.pool5 = module_list[-2]\n\n    def forward(self, x):\n        res5c = self.conv5(x)\n        pool5 = self.pool5(res5c)\n        pool5 = pool5.view(pool5.size(0), -1)\n        return res5c, pool5\n\n\nresnet_transform = transforms.Compose([\n    Rescale(224, 224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n'"
solver.py,16,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nimport numpy as np\nimport json\nfrom tqdm import tqdm, trange\n\nfrom layers import Summarizer, Discriminator  # , apply_weight_norm\nfrom utils import TensorboardWriter\n# from feature_extraction import ResNetFeature\n\n\nclass Solver(object):\n    def __init__(self, config=None, train_loader=None, test_loader=None):\n        """"""Class that Builds, Trains and Evaluates SUM-GAN model""""""\n        self.config = config\n        self.train_loader = train_loader\n        self.test_loader = test_loader\n\n    def build(self):\n\n        # Build Modules\n        self.linear_compress = nn.Linear(\n            self.config.input_size,\n            self.config.hidden_size).cuda()\n        self.summarizer = Summarizer(\n            input_size=self.config.hidden_size,\n            hidden_size=self.config.hidden_size,\n            num_layers=self.config.num_layers).cuda()\n        self.discriminator = Discriminator(\n            input_size=self.config.hidden_size,\n            hidden_size=self.config.hidden_size,\n            num_layers=self.config.num_layers).cuda()\n        self.model = nn.ModuleList([\n            self.linear_compress, self.summarizer, self.discriminator])\n\n        if self.config.mode == \'train\':\n            # Build Optimizers\n            self.s_e_optimizer = optim.Adam(\n                list(self.summarizer.s_lstm.parameters())\n                + list(self.summarizer.vae.e_lstm.parameters())\n                + list(self.linear_compress.parameters()),\n                lr=self.config.lr)\n            self.d_optimizer = optim.Adam(\n                list(self.summarizer.vae.d_lstm.parameters())\n                + list(self.linear_compress.parameters()),\n                lr=self.config.lr)\n            self.c_optimizer = optim.Adam(\n                list(self.discriminator.parameters())\n                + list(self.linear_compress.parameters()),\n                lr=self.config.discriminator_lr)\n\n            self.model.train()\n            # self.model.apply(apply_weight_norm)\n\n            # Overview Parameters\n            # print(\'Model Parameters\')\n            # for name, param in self.model.named_parameters():\n            #     print(\'\\t\' + name + \'\\t\', list(param.size()))\n\n            # Tensorboard\n            self.writer = TensorboardWriter(self.config.log_dir)\n\n    @staticmethod\n    def freeze_model(module):\n        for p in module.parameters():\n            p.requires_grad = False\n\n    def reconstruction_loss(self, h_origin, h_fake):\n        """"""L2 loss between original-regenerated features at cLSTM\'s last hidden layer""""""\n\n        return torch.norm(h_origin - h_fake, p=2)\n\n    def prior_loss(self, mu, log_variance):\n        """"""KL( q(e|x) || N(0,1) )""""""\n        return 0.5 * torch.sum(-1 + log_variance.exp() + mu.pow(2) - log_variance)\n\n    def sparsity_loss(self, scores):\n        """"""Summary-Length Regularization""""""\n\n        return torch.abs(torch.mean(scores) - self.config.summary_rate)\n\n    def gan_loss(self, original_prob, fake_prob, uniform_prob):\n        """"""Typical GAN loss + Classify uniformly scored features""""""\n\n        gan_loss = torch.mean(torch.log(original_prob) + torch.log(1 - fake_prob)\n                              + torch.log(1 - uniform_prob))  # Discriminate uniform score\n\n        return gan_loss\n\n    def train(self):\n        step = 0\n        for epoch_i in trange(self.config.n_epochs, desc=\'Epoch\', ncols=80):\n            s_e_loss_history = []\n            d_loss_history = []\n            c_loss_history = []\n            for batch_i, image_features in enumerate(tqdm(\n                    self.train_loader, desc=\'Batch\', ncols=80, leave=False)):\n\n                if image_features.size(1) > 10000:\n                    continue\n\n                # [batch_size=1, seq_len, 2048]\n                # [seq_len, 2048]\n                image_features = image_features.view(-1, self.config.input_size)\n\n                # [seq_len, 2048]\n                image_features_ = Variable(image_features).cuda()\n\n                #---- Train sLSTM, eLSTM ----#\n                if self.config.verbose:\n                    tqdm.write(\'\\nTraining sLSTM and eLSTM...\')\n\n                # [seq_len, 1, hidden_size]\n                original_features = self.linear_compress(image_features_.detach()).unsqueeze(1)\n\n                scores, h_mu, h_log_variance, generated_features = self.summarizer(\n                    original_features)\n                _, _, _, uniform_features = self.summarizer(\n                    original_features, uniform=True)\n\n                h_origin, original_prob = self.discriminator(original_features)\n                h_fake, fake_prob = self.discriminator(generated_features)\n                h_uniform, uniform_prob = self.discriminator(uniform_features)\n\n                tqdm.write(\n                    f\'original_p: {original_prob.data[0]:.3f}, fake_p: {fake_prob.data[0]:.3f}, uniform_p: {uniform_prob.data[0]:.3f}\')\n\n                reconstruction_loss = self.reconstruction_loss(h_origin, h_fake)\n                prior_loss = self.prior_loss(h_mu, h_log_variance)\n                sparsity_loss = self.sparsity_loss(scores)\n\n                tqdm.write(\n                    f\'recon loss {reconstruction_loss.data[0]:.3f}, prior loss: {prior_loss.data[0]:.3f}, sparsity loss: {sparsity_loss.data[0]:.3f}\')\n\n                s_e_loss = reconstruction_loss + prior_loss + sparsity_loss\n\n                self.s_e_optimizer.zero_grad()\n                s_e_loss.backward()  # retain_graph=True)\n                # Gradient cliping\n                torch.nn.utils.clip_grad_norm(self.model.parameters(), self.config.clip)\n                self.s_e_optimizer.step()\n\n                s_e_loss_history.append(s_e_loss.data)\n\n                #---- Train dLSTM ----#\n                if self.config.verbose:\n                    tqdm.write(\'Training dLSTM...\')\n\n                # [seq_len, 1, hidden_size]\n                original_features = self.linear_compress(image_features_.detach()).unsqueeze(1)\n\n                scores, h_mu, h_log_variance, generated_features = self.summarizer(\n                    original_features)\n                _, _, _, uniform_features = self.summarizer(\n                    original_features, uniform=True)\n\n                h_origin, original_prob = self.discriminator(original_features)\n                h_fake, fake_prob = self.discriminator(generated_features)\n                h_uniform, uniform_prob = self.discriminator(uniform_features)\n\n                tqdm.write(\n                    f\'original_p: {original_prob.data[0]:.3f}, fake_p: {fake_prob.data[0]:.3f}, uniform_p: {uniform_prob.data[0]:.3f}\')\n\n                reconstruction_loss = self.reconstruction_loss(h_origin, h_fake)\n                gan_loss = self.gan_loss(original_prob, fake_prob, uniform_prob)\n\n                tqdm.write(\n                    f\'recon loss {reconstruction_loss.data[0]:.3f}, gan loss: {gan_loss.data[0]:.3f}\')\n\n                d_loss = reconstruction_loss + gan_loss\n\n                self.d_optimizer.zero_grad()\n                d_loss.backward()  # retain_graph=True)\n                # Gradient cliping\n                torch.nn.utils.clip_grad_norm(self.model.parameters(), self.config.clip)\n                self.d_optimizer.step()\n\n                d_loss_history.append(d_loss.data)\n\n                #---- Train cLSTM ----#\n                if batch_i > self.config.discriminator_slow_start:\n                    if self.config.verbose:\n                        tqdm.write(\'Training cLSTM...\')\n                    # [seq_len, 1, hidden_size]\n                    original_features = self.linear_compress(image_features_.detach()).unsqueeze(1)\n\n                    scores, h_mu, h_log_variance, generated_features = self.summarizer(\n                        original_features)\n                    _, _, _, uniform_features = self.summarizer(\n                        original_features, uniform=True)\n\n                    h_origin, original_prob = self.discriminator(original_features)\n                    h_fake, fake_prob = self.discriminator(generated_features)\n                    h_uniform, uniform_prob = self.discriminator(uniform_features)\n                    tqdm.write(\n                        f\'original_p: {original_prob.data[0]:.3f}, fake_p: {fake_prob.data[0]:.3f}, uniform_p: {uniform_prob.data[0]:.3f}\')\n\n                    # Maximization\n                    c_loss = -1 * self.gan_loss(original_prob, fake_prob, uniform_prob)\n\n                    tqdm.write(f\'gan loss: {gan_loss.data[0]:.3f}\')\n\n                    self.c_optimizer.zero_grad()\n                    c_loss.backward()\n                    # Gradient cliping\n                    torch.nn.utils.clip_grad_norm(self.model.parameters(), self.config.clip)\n                    self.c_optimizer.step()\n\n                    c_loss_history.append(c_loss.data)\n\n                if self.config.verbose:\n                    tqdm.write(\'Plotting...\')\n\n                self.writer.update_loss(reconstruction_loss.data, step, \'recon_loss\')\n                self.writer.update_loss(prior_loss.data, step, \'prior_loss\')\n                self.writer.update_loss(sparsity_loss.data, step, \'sparsity_loss\')\n                self.writer.update_loss(gan_loss.data, step, \'gan_loss\')\n\n                # self.writer.update_loss(s_e_loss.data, step, \'s_e_loss\')\n                # self.writer.update_loss(d_loss.data, step, \'d_loss\')\n                # self.writer.update_loss(c_loss.data, step, \'c_loss\')\n\n                self.writer.update_loss(original_prob.data, step, \'original_prob\')\n                self.writer.update_loss(fake_prob.data, step, \'fake_prob\')\n                self.writer.update_loss(uniform_prob.data, step, \'uniform_prob\')\n\n                step += 1\n\n            s_e_loss = torch.stack(s_e_loss_history).mean()\n            d_loss = torch.stack(d_loss_history).mean()\n            c_loss = torch.stack(c_loss_history).mean()\n\n            # Plot\n            if self.config.verbose:\n                tqdm.write(\'Plotting...\')\n            self.writer.update_loss(s_e_loss, epoch_i, \'s_e_loss_epoch\')\n            self.writer.update_loss(d_loss, epoch_i, \'d_loss_epoch\')\n            self.writer.update_loss(c_loss, epoch_i, \'c_loss_epoch\')\n\n            # Save parameters at checkpoint\n            ckpt_path = str(self.config.save_dir) + f\'_epoch-{epoch_i}.pkl\'\n            tqdm.write(f\'Save parameters at {ckpt_path}\')\n            torch.save(self.model.state_dict(), ckpt_path)\n\n            self.evaluate(epoch_i)\n\n            self.model.train()\n\n    def evaluate(self, epoch_i):\n        # checkpoint = self.config.ckpt_path\n        # print(f\'Load parameters from {checkpoint}\')\n        # self.model.load_state_dict(torch.load(checkpoint))\n\n        self.model.eval()\n\n        out_dict = {}\n\n        for video_tensor, video_name in tqdm(\n                self.test_loader, desc=\'Evaluate\', ncols=80, leave=False):\n\n            # [seq_len, batch=1, 2048]\n            video_tensor = video_tensor.view(-1, self.config.input_size)\n            video_feature = Variable(video_tensor, volatile=True).cuda()\n\n            # [seq_len, 1, hidden_size]\n            video_feature = self.linear_compress(video_feature.detach()).unsqueeze(1)\n\n            # [seq_len]\n            scores = self.summarizer.s_lstm(video_feature).squeeze(1)\n\n            scores = np.array(scores.data).tolist()\n\n            out_dict[video_name] = scores\n\n            score_save_path = self.config.score_dir.joinpath(\n                f\'{self.config.video_type}_{epoch_i}.json\')\n            with open(score_save_path, \'w\') as f:\n                tqdm.write(f\'Saving score at {str(score_save_path)}.\')\n                json.dump(out_dict, f)\n            score_save_path.chmod(0o777)\n\n    def pretrain(self):\n        pass\n\n\nif __name__ == \'__main__\':\n    pass\n'"
train.py,0,"b""from configs import get_config\nfrom solver import Solver\nfrom data_loader import get_loader\n\n\nif __name__ == '__main__':\n    config = get_config(mode='train')\n    test_config = get_config(mode='test')\n    print(config)\n    train_loader = get_loader(config.video_root_dir, config.mode)\n    test_loader = get_loader(test_config.video_root_dir, test_config.mode)\n    solver = Solver(config, train_loader, test_loader)\n\n    solver.build()\n    solver.train()\n"""
utils.py,0,"b'# -*- coding: utf-8 -*-\nfrom tensorboardX import SummaryWriter\n\n\nclass TensorboardWriter(SummaryWriter):\n    def __init__(self, logdir):\n        """"""\n        Extended SummaryWriter Class from tensorboard-pytorch (tensorbaordX)\n        https://github.com/lanpa/tensorboard-pytorch/blob/master/tensorboardX/writer.py\n        Internally calls self.file_writer\n        """"""\n        super(TensorboardWriter, self).__init__(logdir)\n        self.logdir = self.file_writer.get_logdir()\n\n    def update_parameters(self, module, step_i):\n        """"""\n        module: nn.Module\n        """"""\n        for name, param in module.named_parameters():\n            self.add_histogram(name, param.clone().cpu().data.numpy(), step_i)\n\n    def update_loss(self, loss, step_i, name=\'loss\'):\n        self.add_scalar(name, loss, step_i)\n\n    def update_histogram(self, values, step_i, name=\'hist\'):\n        self.add_histogram(name, values, step_i)\n'"
layers/__init__.py,0,b'from .discriminator import Discriminator\nfrom .summarizer import Summarizer\nfrom .weight_norm import apply_weight_norm\n'
layers/discriminator.py,1,"b'# -*- coding: utf-8 -*-\nimport torch.nn as nn\n\n\nclass cLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=2):\n        """"""Discriminator LSTM""""""\n        super().__init__()\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n\n    def forward(self, features, init_hidden=None):\n        """"""\n        Args:\n            features: [seq_len, 1, input_size]\n        Return:\n            last_h: [1, hidden_size]\n        """"""\n        self.lstm.flatten_parameters()\n\n        # output: seq_len, batch, hidden_size * num_directions\n        # h_n, c_n: num_layers * num_directions, batch_size, hidden_size\n        output, (h_n, c_n) = self.lstm(features, init_hidden)\n\n        # [batch_size, hidden_size]\n        last_h = h_n[-1]\n\n        return last_h\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=2):\n        """"""Discriminator: cLSTM + output projection to probability""""""\n        super().__init__()\n        self.cLSTM = cLSTM(input_size, hidden_size, num_layers)\n        self.out = nn.Sequential(\n            nn.Linear(hidden_size, 1),\n            nn.Sigmoid())\n\n    def forward(self, features):\n        """"""\n        Args:\n            features: [seq_len, 1, hidden_size]\n        Return:\n            h : [1, hidden_size]\n                Last h from top layer of discriminator\n            prob: [1=batch_size, 1]\n                Probability to be original feature from CNN\n        """"""\n\n        # [1, hidden_size]\n        h = self.cLSTM(features)\n\n        # [1]\n        prob = self.out(h).squeeze()\n\n        return h, prob\n\n\nif __name__ == \'__main__\':\n\n    pass\n'"
layers/lstmcell.py,3,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\n\n\nclass StackedLSTMCell(nn.Module):\n\n    def __init__(self, num_layers, input_size, rnn_size, dropout=0.0):\n        super(StackedLSTMCell, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n\n        for i in range(num_layers):\n            self.layers.append(nn.LSTMCell(input_size, rnn_size))\n            input_size = rnn_size\n\n    def forward(self, x, h_c):\n        """"""\n        Args:\n            x: [batch_size, input_size]\n            h_c: [2, num_layers, batch_size, hidden_size]\n        Return:\n            last_h_c: [2, batch_size, hidden_size] (h from last layer)\n            h_c_list: [2, num_layers, batch_size, hidden_size] (h and c from all layers)\n        """"""\n        h_0, c_0 = h_c\n        h_list, c_list = [], []\n        for i, layer in enumerate(self.layers):\n            # h of i-th layer\n            h_i, c_i = layer(x, (h_0[i], c_0[i]))\n\n            # x for next layer\n            x = h_i\n            if i + 1 != self.num_layers:\n                x = self.dropout(x)\n            h_list += [h_i]\n            c_list += [c_i]\n\n        last_h_c = (h_list[-1], c_list[-1])\n        h_list = torch.stack(h_list)\n        c_list = torch.stack(c_list)\n        h_c_list = (h_list, c_list)\n\n        return last_h_c, h_c_list\n'"
layers/summarizer.py,7,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom .lstmcell import StackedLSTMCell\n\n\nclass sLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=2):\n        """"""Scoring LSTM""""""\n        super().__init__()\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True)\n        self.out = nn.Sequential(\n            nn.Linear(hidden_size * 2, 1),  # bidirection => scalar\n            nn.Sigmoid())\n\n    def forward(self, features, init_hidden=None):\n        """"""\n        Args:\n            features: [seq_len, 1, 100] (compressed pool5 features)\n        Return:\n            scores [seq_len, 1]\n        """"""\n        self.lstm.flatten_parameters()\n\n        # [seq_len, 1, hidden_size * 2]\n        features, (h_n, c_n) = self.lstm(features)\n\n        # [seq_len, 1]\n        scores = self.out(features.squeeze(1))\n\n        return scores\n\n\nclass eLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=2):\n        """"""Encoder LSTM""""""\n        super().__init__()\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n\n        self.linear_mu = nn.Linear(hidden_size, hidden_size)\n        self.linear_var = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, frame_features):\n        """"""\n        Args:\n            frame_features: [seq_len, 1, hidden_size]\n        Return:\n            last hidden\n                h_last [num_layers=2, 1, hidden_size]\n                c_last [num_layers=2, 1, hidden_size]\n        """"""\n        self.lstm.flatten_parameters()\n        _, (h_last, c_last) = self.lstm(frame_features)\n\n        return (h_last, c_last)\n\n\nclass dLSTM(nn.Module):\n    def __init__(self, input_size=2048, hidden_size=2048, num_layers=2):\n        """"""Decoder LSTM""""""\n        super().__init__()\n\n        self.lstm_cell = StackedLSTMCell(num_layers, input_size, hidden_size)\n        self.out = nn.Linear(hidden_size, input_size)\n\n    def forward(self, seq_len, init_hidden):\n        """"""\n        Args:\n            seq_len (int)\n            init_hidden\n                h [num_layers=2, 1, hidden_size]\n                c [num_layers=2, 1, hidden_size]\n        Return:\n            out_features: [seq_len, 1, hidden_size]\n        """"""\n\n        batch_size = init_hidden[0].size(1)\n        hidden_size = init_hidden[0].size(2)\n\n        x = Variable(torch.zeros(batch_size, hidden_size)).cuda()\n        h, c = init_hidden  # (h_0, c_0): last state of eLSTM\n\n        out_features = []\n        for i in range(seq_len):\n            # last_h: [1, hidden_size] (h from last layer)\n            # last_c: [1, hidden_size] (c from last layer)\n            # h: [2=num_layers, 1, hidden_size] (h from all layers)\n            # c: [2=num_layers, 1, hidden_size] (c from all layers)\n            (last_h, last_c), (h, c) = self.lstm_cell(x, (h, c))\n            x = self.out(last_h)\n            out_features.append(last_h)\n        # list of seq_len \'[1, hidden_size]-sized Variables\'\n        return out_features\n\n\nclass VAE(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=2):\n        super().__init__()\n        self.e_lstm = eLSTM(input_size, hidden_size, num_layers)\n        self.d_lstm = dLSTM(input_size, hidden_size, num_layers)\n\n        self.softplus = nn.Softplus()\n\n    def reparameterize(self, mu, log_variance):\n        """"""Sample z via reparameterization trick\n        Args:\n            mu: [num_layers, hidden_size]\n            log_var: [num_layers, hidden_size]\n        Return:\n            h: [num_layers, 1, hidden_size]\n        """"""\n        std = torch.exp(0.5 * log_variance)\n\n        # e ~ N(0,1)\n        epsilon = Variable(torch.randn(std.size())).cuda()\n\n        # [num_layers, 1, hidden_size]\n        return (mu + epsilon * std).unsqueeze(1)\n\n    def forward(self, features):\n        """"""\n        Args:\n            features: [seq_len, 1, hidden_size]\n        Return:\n            h: [2=num_layers, 1, hidden_size]\n            decoded_features: [seq_len, 1, 2048]\n        """"""\n        seq_len = features.size(0)\n\n        # [num_layers, 1, hidden_size]\n        h, c = self.e_lstm(features)\n\n        # [num_layers, hidden_size]\n        h = h.squeeze(1)\n\n        # [num_layers, hidden_size]\n        h_mu = self.e_lstm.linear_mu(h)\n        h_log_variance = torch.log(self.softplus(self.e_lstm.linear_var(h)))\n\n        # [num_layers, 1, hidden_size]\n        h = self.reparameterize(h_mu, h_log_variance)\n\n        # [seq_len, 1, hidden_size]\n        decoded_features = self.d_lstm(seq_len, init_hidden=(h, c))\n\n        # [seq_len, 1, hidden_size]\n        # reverse\n        decoded_features.reverse()\n        decoded_features = torch.stack(decoded_features)\n        return h_mu, h_log_variance, decoded_features\n\n\nclass Summarizer(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers=2):\n        super().__init__()\n        self.s_lstm = sLSTM(input_size, hidden_size, num_layers)\n        self.vae = VAE(input_size, hidden_size, num_layers)\n\n    def forward(self, image_features, uniform=False):\n        # Apply weights\n        if not uniform:\n            # [seq_len, 1]\n            scores = self.s_lstm(image_features)\n\n            # [seq_len, 1, hidden_size]\n            weighted_features = image_features * scores.view(-1, 1, 1)\n        else:\n            scores = None\n            weighted_features = image_features\n\n        h_mu, h_log_variance, decoded_features = self.vae(weighted_features)\n\n        return scores, h_mu, h_log_variance, decoded_features\n\n\nif __name__ == \'__main__\':\n\n    pass\n'"
layers/weight_norm.py,2,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.utils import weight_norm\n\n\ndef apply_weight_norm(module):\n    """"""Recursively apply weight norm to children of given module""""""\n    if isinstance(module, nn.Linear):\n        weight_norm(module, \'weight\')\n    if isinstance(module, (nn.RNNCell, nn.GRUCell, nn.LSTMCell)):\n        weight_norm(module, \'weight_ih\')\n        weight_norm(module, \'weight_hh\')\n    if isinstance(module, (nn.RNN, nn.GRU, nn.LSTM)):\n        for i in range(module.num_layers):\n            weight_norm(module, f\'weight_ih_l{i}\')\n            weight_norm(module, f\'weight_hh_l{i}\')\n            if module.bidirectional:\n                weight_norm(module, f\'weight_ih_l{i}_reverse\')\n                weight_norm(module, f\'weight_hh_l{i}_reverse\')\n'"
