file_path,api_count,code
Day_01/katalk_bot/run.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nfrom flask import Flask, request, jsonify\nimport random\nfrom sentiment import SentimentEngine, mecab_tokenizer\n\napp = Flask(__name__)\n\n@app.route(\'/\')\ndef Welcome():\n    return app.send_static_file(\'index.html\')\n\n@app.route(\'/myapp\')\ndef WelcomeToMyapp():\n    return \'\xeb\xb0\x98\xea\xb0\x91\xec\x8a\xb5\xeb\x8b\x88\xeb\x8b\xa4 :)\'\n\n@app.route(\'/keyboard\')\ndef Keyboard():\n    message = {\n            \'type\': \'buttons\',\n            \'buttons\': [""\xec\x84\xa0\xed\x83\x9d1"", ""\xec\x84\xa0\xed\x83\x9d2""],\n    }\n    return jsonify(message)\n\n@app.route(\'/message\', methods=[\'POST\'])\ndef GetMessage():\n    # \xea\xb3\xa0\xea\xb0\x9d\xec\x9d\xb4 \xeb\xb3\xb4\xeb\x82\xb8 \xeb\xa9\x94\xec\x8b\x9c\xec\xa7\x80 \xec\xa0\x95\xeb\xb3\xb4 (dict)\n    received_data = request.get_json()\n\n    # \xea\xb3\xa0\xea\xb0\x9d \xeb\xa9\x94\xec\x8b\x9c\xec\xa7\x80 \xec\xa4\x91 \xed\x85\x8d\xec\x8a\xa4\xed\x8a\xb8 \xeb\xb6\x80\xeb\xb6\x84 (string)\n    text = received_data[\'content\']\n\n    # \xea\xb0\x90\xec\xa0\x95 \xeb\xb6\x84\xec\x84\x9d \xec\xa0\x90\xec\x88\x98 (0 ~ 1)\n    score = sentiment.score(text)\n\n    # \xea\xb8\x8d\xec\xa0\x95\xeb\xac\xb8\n    if score > 0.5:\n        answer = random.choice(\n                [\'\xec\xa0\x80\xeb\x8f\x84 \xea\xb7\xb8 \xec\x98\x81\xed\x99\x94 \xec\xa2\x8b\xec\x95\x84\xed\x95\xb4\xec\x9a\x94\', \'\xec\xa0\x80\xeb\x8f\x84 \xec\xa2\x8b\xec\x95\x84\xec\x9a\x94\']\n        )\n    # \xeb\xb6\x80\xec\xa0\x95\xeb\xac\xb8\n    else:\n        answer = random.choice(\n                [\'\xec\xa0\x80\xeb\x8f\x84 \xea\xb7\xb8 \xec\x98\x81\xed\x99\x94 \xec\x8b\xab\xec\x96\xb4\xed\x95\xb4\xec\x9a\x94\', \'\xeb\xb3\x84\xeb\xa1\x9c\xec\x97\x90\xec\x9a\x94\']\n        )\n\n    message = {\n        ""message"": {\n            ""text"": answer,\n        }\n    }\n    return jsonify(message)\n\n@app.errorhandler(404)\ndef page_not_found(e):\n    error_message = {\n        ""message"": {\n            ""text"": \'\xec\x9e\x98\xeb\xaa\xbb\xeb\x90\x9c \xec\xa0\x91\xea\xb7\xbc\xec\x9e\x85\xeb\x8b\x88\xeb\x8b\xa4!\'\n        }\n     }\n    return jsonify(error_message)\n\nif __name__ == ""__main__"":\n    # \xea\xb0\x90\xec\xa0\x95 \xeb\xb6\x84\xec\x84\x9d \xec\x97\x94\xec\xa7\x84 \xeb\xb6\x88\xeb\x9f\xac\xec\x98\xa4\xea\xb8\xb0\n    sentiment = SentimentEngine()\n\n    # \xeb\xaf\xb8\xeb\xa6\xac \xec\x84\xa4\xec\xa0\x95\xeb\x90\x9c \xed\x8f\xac\xed\x8a\xb8\xea\xb0\x80 \xec\x97\x86\xec\x9c\xbc\xeb\xa9\xb4 5000\xeb\xb2\x88 \xec\x9d\xb4\xec\x9a\xa9\n    port = os.getenv(\'PORT\', \'5000\')\n\n    # \xed\x94\x8c\xeb\x9d\xbc\xec\x8a\xa4\xed\x81\xac \xec\x84\x9c\xeb\xb2\x84 \xec\x8b\xa4\xed\x96\x89\n    app.run(host=\'0.0.0.0\', port=int(port))\n\n    # ./ngrok http 5000\n'"
Day_01/katalk_bot/sentiment.py,0,"b""from sklearn.externals import joblib\n\n################################################\n# joblib cannot fully serialize external modules..\nfrom konlpy.tag import Mecab\nmecab = Mecab()\ndef mecab_tokenizer(text):\n    tokens = mecab.morphs(text)\n    return tokens\n#################################################\n\nclass SentimentEngine(object):\n    def __init__(self):\n        # \xec\xa0\x80\xec\x9e\xa5\xeb\x90\x9c \xeb\xaa\xa8\xeb\x8d\xb8 \xeb\xb6\x88\xeb\x9f\xac\xec\x98\xa4\xea\xb8\xb0\n        self.model = joblib.load('sentiment_engine.pkl')\n\n    def score(self, text):\n        # \xea\xb3\xa0\xea\xb0\x9d\xec\x9d\x98 \xea\xb0\x90\xec\xa0\x95 (0~1)\n        # 0: \xeb\xb6\x80\xec\xa0\x95 / 1: \xea\xb8\x8d\xec\xa0\x95 \n        score = self.model.predict_proba([text])[:,1][0]\n        return score\n\nif __name__ == '__main__':\n    scorer = Scorer()\n"""
Day_02/CNN/configs.py,0,"b'import argparse\nimport pprint\nfrom pathlib import Path\nfrom torch import optim\nfrom torch import nn\n\nproject_dir = Path(__file__).resolve().parent\n\noptimizer_dict = {\n    \'sgd\': optim.SGD,\n    \'adam\': optim.Adam,\n}\n\n\nclass Config(object):\n    def __init__(self, **kwargs):\n        if kwargs is not None:\n            for key, value in kwargs.items():\n                if key == \'optimizer\':\n                    value = optimizer_dict[value]\n                setattr(self, key, value)\n\n        self.loss_fn = nn.CrossEntropyLoss\n\n    def __repr__(self):\n        """"""Pretty-print configurations in alphabetical order""""""\n        config_str = \'Configurations\\n\'\n        config_str += pprint.pformat(self.__dict__)\n        return config_str\n\n\ndef get_config(parse=True, **optional_kwargs):\n    """"""\n    Get configurations as attributes of class\n    1. Parse configurations with argparse.\n    2. Create Config class initilized with parsed kwargs.\n    3. Return Config class.\n    """"""\n    parser = argparse.ArgumentParser()\n\n    #================ Train ==============#\n    parser.add_argument(\'--batch_size\', type=int, default=100)\n    parser.add_argument(\'--lr\', type=int, default=1e-3)\n    parser.add_argument(\'--optimizer\', type=str, default=\'sgd\')\n    parser.add_argument(\'--epochs\', type=int, default=20)\n\n    #================ Model ==============#\n    parser.add_argument(\'--hidden_size\', type=int, default=300)\n    parser.add_argument(\'--n_channel_per_window\', type=int, default=2)\n    parser.add_argument(\'--label_size\', type=int, default=2)\n    parser.add_argument(\'--vocab_size\', type=int, default=20000)\n    parser.add_argument(\'--dropout\', type=float, default=0.5)\n\n    #================ Path  ==============#\n    parser.add_argument(\'--save_dir\', type=str, default=project_dir.joinpath(\'log\'))\n    parser.add_argument(\'--data_dir\', type=str, default=project_dir.joinpath(\'datasets\'))\n\n    #================ Misc. ==============#\n    parser.add_argument(\'--log_every_epoch\', type=int, default=1)\n    parser.add_argument(\'--save_every_epoch\', type=int, default=1)\n\n    #=============== Parse Arguments===============#\n    if parse:\n        kwargs = parser.parse_args()\n    else:\n        kwargs = parser.parse_known_args()[0]\n\n    # Namespace => Dictionary\n    kwargs = vars(kwargs)\n\n    kwargs.update(optional_kwargs)\n\n    return Config(**kwargs)\n\n\n\n\nif __name__ == \'__main__\':\n    config = get_config()\n    print(config)\n'"
Day_02/CNN/data_loader.py,0,"b""from torchtext.vocab import Vocab\nfrom torchtext import data\nfrom utils import tokenizer\nfrom pathlib import Path\nimport pickle\n\n\ndef postprocess(x, train=True):\n    x = int(x)\n    return x\n\n\ndef filter_pred(example):\n    if example.label in ['0', '1']:\n        if len(example.text) > 1:\n            return True\n    return False\n\n\ndef get_loader(batch_size=100, max_size=20000, is_train=True, data_dir=None):\n\n    text_field = data.Field(tokenize=tokenizer, sequential=True)\n    label_field = data.Field(sequential=False, use_vocab=False,\n                             postprocessing=data.Pipeline(postprocess))\n\n    train_file_path = Path(data_dir).joinpath('naver_train.txt')\n    test_file_path = Path(data_dir).joinpath('naver_test.txt')\n\n    train_dataset = data.TabularDataset(\n        path=train_file_path,\n        format='tsv',\n        fields=[\n            ('id', None),\n            ('text', text_field),\n            ('label', label_field)\n        ],\n        filter_pred=filter_pred)\n\n    print('Building Vocabulary \\n')\n    text_field.build_vocab(train_dataset, max_size=max_size - 2)\n\n    if is_train:\n        loader = data.Iterator(\n            dataset=train_dataset,\n            batch_size=batch_size,\n            sort_key=lambda x: len(x.text),\n            train=True,  # if training set => repeat and shuffle : True\n            repeat=False,\n            device=-1  # CPU: -1\n        )\n        # vocab = text_field.vocab\n        # with open('./vocab.pkl', 'wb') as f:\n        #     pickle.dump(vocab, f)\n\n    else:\n        test_dataset = data.TabularDataset(\n            path=test_file_path,\n            format='tsv',\n            fields=[\n                ('id', None),\n                ('text', text_field),\n                ('label', label_field)\n            ],\n            filter_pred=filter_pred)\n\n        loader = data.Iterator(\n            dataset=test_dataset,\n            batch_size=batch_size,\n            sort=False,\n            train=False,\n            device=-1)\n\n    return loader\n\n\ndef get_vocab():\n    with open('./vocab.pkl', 'rb') as f:\n        vocab = pickle.load(f)\n    return vocab\n"""
Day_02/CNN/eval.py,0,"b""from solver import Solver\nfrom data_loader import get_loader\nfrom configs import get_config\n\nif __name__ == '__main__':\n    config = get_config()\n    print(config)\n\n    data_loader = get_loader(\n        batch_size=config.batch_size,\n        max_size=config.vocab_size,\n        is_train=False,\n        data_dir=config.data_dir)\n\n    solver = Solver(config, data_loader)\n    solver.build(is_train=False)\n    solver.load(epoch=2)\n    solver.eval()\n"""
Day_02/CNN/inference.py,0,"b""from solver import Solver\nfrom data_loader import get_loader, get_vocab\nfrom configs import get_config\nfrom utils import tokenizer\n\nif __name__ == '__main__':\n    config = get_config(batch_size=1)\n    print(config)\n\n    data_loader = get_loader(\n        batch_size=config.batch_size,\n        max_size=config.vocab_size,\n        is_train=False,\n        data_dir=config.data_dir)\n\n    solver = Solver(config, data_loader)\n    solver.build(is_train=False)\n    solver.load(epoch=2)\n    vocab = get_vocab()\n\n    while True:\n        text = input('Insert Sentence: ')\n        text = tokenizer(text)\n        text = [vocab.stoi[word] for word in text]\n\n        prediction = solver.inference(text)\n\n        if prediction == 0:\n            print('Positive!')\n        else:\n            print('Negative')\n"""
Day_02/CNN/models.py,2,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass CNN(nn.Module):\n    def __init__(self, config):\n        super(CNN, self).__init__()\n        self.config = config\n\n        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n\n        self.conv = nn.ModuleList([\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=config.n_channel_per_window,\n                kernel_size=(3, config.hidden_size)),\n\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=config.n_channel_per_window,\n                kernel_size=(4, config.hidden_size)),\n\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=config.n_channel_per_window,\n                kernel_size=(5, config.hidden_size))\n        ])\n\n        n_total_channels = len(self.conv) * config.n_channel_per_window\n\n        self.dropout = nn.Dropout(config.dropout)\n        self.fc = nn.Linear(n_total_channels, config.label_size)\n\n    def forward(self, x):\n        """"""\n        Args:\n            x: [batch_size, max_seq_len]\n        Return:\n            logit: [batch_size, label_size]\n        """"""\n\n        # [batch_size, max_seq_len, hidden_size]\n        x = self.embedding(x)\n\n        # [batch_size, 1, max_seq_len, hidden_size]\n        x = x.unsqueeze(1)\n\n        # Apply Convolution filter followed by Max-pool\n        out_list = []\n        for conv in self.conv:\n\n            ########## Convolution #########\n\n            # [batch_size, n_kernels, _, 1]\n            x_ = F.relu(conv(x))\n\n            # [batch_size, n_kernels, _]\n            x_ = x_.squeeze(3)\n\n            ########## Max-pool #########\n\n            # [batch_size, n_kernels, 1]\n            x_ = F.max_pool1d(x_, x_.size(2))\n\n            # [batch_size, n_kernels]\n            x_ = x_.squeeze(2)\n\n            out_list.append(x_)\n\n        # [batch_size, 3 x n_kernels]\n        out = torch.cat(out_list, 1)\n\n        ######## Dropout ########\n        out = self.dropout(out)\n\n        # [batch_size, label_size]\n        logit = self.fc(out)\n\n        return logit\n'"
Day_02/CNN/solver.py,5,"b'import torch\nfrom torch.autograd import Variable\nfrom models import CNN\n\nfrom tqdm import tqdm\nimport os\nimport numpy as np\n\n\nclass Solver(object):\n    def __init__(self, config, data_loader):\n        self.config = config\n        self.data_loader = data_loader\n\n    def build(self, is_train):\n        self.model = CNN(self.config)\n        self.loss_fn = self.config.loss_fn()\n\n        if is_train:\n            self.model.train()\n            self.optimizer = self.config.optimizer(self.model.parameters(), lr=self.config.lr)\n        else:\n            self.model.eval()\n\n    def save(self, ckpt_path):\n        """"""Save model parameters""""""\n        print(\'Save parameters at \', ckpt_path)\n        torch.save(self.model.state_dict(), ckpt_path)\n\n    def load(self, ckpt_path=None, epoch=None):\n        """"""Load model parameters""""""\n        if not (ckpt_path or epoch):\n            epoch = self.config.epochs\n        if epoch:\n            ckpt_path = os.path.join(self.config.save_dir, f\'epoch-{epoch}.pkl\')\n        print(\'Load parameters from \', ckpt_path)\n        self.model.load_state_dict(torch.load(ckpt_path))\n\n    def train(self):\n        """"""Train model with training data""""""\n        for epoch in tqdm(range(self.config.epochs)):\n            loss_history = []\n\n            for batch_i, batch in enumerate(tqdm(self.data_loader)):\n                # text: [max_seq_len, batch_size]\n                # label: [batch_size]\n                text, label = batch.text, batch.label\n\n                # [batch_size, max_seq_len]\n                text.data.t_()\n\n                # [batch_size, 2]\n                logit = self.model(text)\n\n                # Calculate loss\n                average_batch_loss = self.loss_fn(logit, label)  # [1]\n                loss_history.append(average_batch_loss.data[0])  # Variable -> Tensor\n\n                # Flush out remaining gradient\n                self.optimizer.zero_grad()\n\n                # Backpropagation\n                average_batch_loss.backward()\n\n                # Gradient descent\n                self.optimizer.step()\n\n            # Log intermediate loss\n            if (epoch + 1) % self.config.log_every_epoch == 0:\n                epoch_loss = np.mean(loss_history)\n                log_str = f\'Epoch {epoch + 1} | loss: {epoch_loss:.2f}\\n\'\n                print(log_str)\n\n            # Save model parameters\n            if (epoch + 1) % self.config.save_every_epoch == 0:\n                ckpt_path = os.path.join(self.config.save_dir, f\'epoch-{epoch+1}.pkl\')\n                self.save(ckpt_path)\n\n    def eval(self):\n        """"""Evaluate model from text data""""""\n\n        n_total_data = 0\n        n_correct = 0\n        loss_history = []\n        import ipdb\n        ipdb.set_trace()\n        for _, batch in enumerate(tqdm(self.data_loader)):\n            # text: [max_seq_len, batch_size]\n            # label: [batch_size]\n            text, label = batch.text, batch.label\n\n            # [batch_size, max_seq_len]\n            text.data.t_()\n\n            # [batch_size, 2]\n            logit = self.model(text)\n\n            # Calculate loss\n            average_batch_loss = self.loss_fn(logit, label)  # [1]\n            loss_history.append(average_batch_loss.data[0])  # Variable -> Tensor\n\n            # Calculate accuracy\n            n_total_data += len(label)\n\n            # [batch_size]\n            _, prediction = logit.max(1)\n\n            n_correct += (prediction == label).sum().data\n\n        epoch_loss = np.mean(loss_history)\n\n        accuracy = n_correct / n_total_data\n\n        print(f\'Loss: {epoch_loss:.2f}\')\n\n        print(f\'Accuracy: {accuracy}\')\n\n    def inference(self, text):\n\n        text = Variable(torch.LongTensor([text]))\n\n        # [batch_size, 2]\n        logit = self.model(text)\n\n        _, prediction = torch.max(logit)\n\n        return prediction\n'"
Day_02/CNN/train.py,0,"b""from solver import Solver\nfrom data_loader import get_loader\nfrom configs import get_config\n\nif __name__ == '__main__':\n    config = get_config()\n    print(config)\n\n    data_loader = get_loader(\n        batch_size=config.batch_size,\n        max_size=config.vocab_size,\n        is_train=True,\n        data_dir=config.data_dir)\n\n    solver = Solver(config, data_loader)\n    solver.build(is_train=True)\n    solver.train()\n"""
Day_02/CNN/utils.py,0,"b""from konlpy.tag import Mecab\nimport re\n\nprint('Loading Mecab')\nmecab = Mecab()\n\n# hangul = re.compile('[^ \xe3\x84\xb1-\xe3\x85\xa3\xea\xb0\x80-\xed\x9e\xa3]+')\nhangul = re.compile('[^ \xe3\x85\x8b\xe3\x85\x8e\xea\xb0\x80-\xed\x9e\xa3]+')\n\n\ndef clean(sentence):\n    clean_sentence = hangul.sub('', sentence)\n    return clean_sentence\n\n\ndef mecab_tokenizer(sentence):\n    out_list = []\n    for word, pos in mecab.pos(sentence):\n        out_list.append(word)\n    return out_list\n\n\ndef tokenizer(sentence):\n    clean_sentence = clean(sentence)\n    tokens = mecab_tokenizer(clean_sentence)\n    return tokens\n"""
Day_02/pytorch_template/simple.py,2,"b""import torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils import data\nfrom torch.autograd import Variable\nfrom torchvision import datasets\nfrom torchvision import transforms\n\nimport numpy as np\nfrom tqdm import tqdm\n\n#----------------------- Data -----------------------#\ntrain_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,)),  # (0, 1) => (-0.5, 0.5) => (-1, 1)\n\n])\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,)),  # (0, 1) => (-0.5, 0.5) => (-1, 1)\n])\n\ntrain_dataset = datasets.MNIST(root='/Users/jmin/workspace/ml/datasets',\n                               train=True, transform=train_transform)\ntest_dataset = datasets.MNIST(root='/Users/jmin/workspace/ml/datasets',\n                              train=False, transform=test_transform)\n\ntrain_dataloader = data.DataLoader(\n    dataset=train_dataset, batch_size=100, shuffle=True)\ntest_dataloader = data.DataLoader(\n    dataset=test_dataset, batch_size=100, shuffle=False)\n#----------------------- Model -----------------------#\n\n\nclass NNClassifier(nn.Module):\n    def __init__(self):\n        super(NNClassifier, self).__init__()\n        self.layer_1 = nn.Linear(28 * 28, 200)\n        self.layer_2 = nn.Linear(200, 50)\n        self.layer_3 = nn.Linear(50, 10)\n\n        self.lrelu = nn.LeakyReLU()\n\n        self.softmax = nn.Softmax()\n\n        self.net = nn.Sequential(\n            self.layer_1,  # 784 => 200\n            self.lrelu,\n            self.layer_2,  # 200 => 50\n            self.lrelu,\n            self.layer_3,  # 50 => 10\n            self.softmax,\n        )\n\n    def forward(self, x):\n        # [batch_size, 784] => [batch_size, 1]\n        return self.net(x)\n\n\n#----------------------- Build -----------------------#\nclassifier = NNClassifier()\n\nloss_fn = nn.CrossEntropyLoss()\n# Args:\n#     Input: (batch_size, number of classes)\n#     Target: (batch_size)\n\noptimizer = optim.SGD(params=classifier.parameters(), lr=1e-3)\n\n\n#----------------------- Train -----------------------#\nprint('Start training!\\n')\nfor epoch in tqdm(range(20)):\n    # epoch_loss = average of batch losses\n    loss_history = []\n    for images, true_labels in train_dataloader:\n        # images: [batch_size, 1, 28, 28]\n        # true_labels: [batch_size]\n\n        # Tensor -> Variable\n        images = Variable(images)\n        true_labels = Variable(true_labels)\n\n        # Resize (for loss function)\n        images = images.view(-1, 28 * 28)  # [batch_size, 1, 28, 28] => [batch_size, 28x28]\n        true_labels = true_labels.view(-1)  # [batch_size, 1] => [batch_size]\n\n        # [batch_size, 28x28] => [batch_size, 10]\n        predicted_labels = classifier(images)\n\n        # Calculate loss\n        average_batch_loss = loss_fn(predicted_labels, true_labels)  # [1]\n        loss_history.append(average_batch_loss.data[0])  # Variable -> Tensor\n\n        # Flush out remaining gradient\n        optimizer.zero_grad()\n\n        # Backpropagation\n        average_batch_loss.backward()\n\n        # Gradient descent\n        optimizer.step()\n\n    if (epoch + 1) % 2 == 0:\n        epoch_loss = np.mean(loss_history)\n        log_str = 'Epoch {} | loss: {:.2f}\\n'.format(epoch + 1, epoch_loss)\n        print(log_str)\n\n#----------------------- Evaluation -----------------------#\nprint('Start Evaluation!\\n')\ntest_loss_history = []\nfor images, true_labels in tqdm(test_dataloader):\n    # images: [batch_size, 1, 28, 28]\n    # true_labels: [batch_size]\n\n    # Tensor -> Variable\n    images = Variable(images)\n    true_labels = Variable(true_labels)\n\n    # Resize (for loss function)\n    images = images.view(-1, 28 * 28)  # [batch_size, 1, 28, 28] => [batch_size, 28x28]\n    true_labels = true_labels.view(-1)  # [batch_size, 1] => [batch_size]\n\n    # [batch_size, 28x28] => [batch_size, 10]\n    predicted_labels = classifier(images)\n\n    # Calculate loss\n    average_batch_loss = loss_fn(predicted_labels, true_labels)  # [1]\n    test_loss_history.append(average_batch_loss.data[0])  # Variable -> Tensor\n\ntest_loss = np.mean(test_loss_history)\nlog_str = 'Test loss: {:.2f}\\n'.format(test_loss)\nprint(log_str)\n"""
Day_03/03_seq2seq_attention/masked_cross_entropy.py,5,"b'import torch\nfrom torch.nn import functional\nfrom torch.autograd import Variable\n\ndef sequence_mask(sequence_length, max_len=None):\n    if max_len is None:\n        max_len = sequence_length.data.max()\n    batch_size = sequence_length.size(0)\n    seq_range = torch.range(0, max_len - 1).long()\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    seq_range_expand = Variable(seq_range_expand)\n    if sequence_length.is_cuda:\n        seq_range_expand = seq_range_expand.cuda()\n    seq_length_expand = (sequence_length.unsqueeze(1)\n                         .expand_as(seq_range_expand))\n    return seq_range_expand < seq_length_expand\n\n\ndef masked_cross_entropy(logits, target, length):\n    length = Variable(torch.LongTensor(length)).cuda()\n\n    """"""\n    Args:\n        logits: A Variable containing a FloatTensor of size\n            (batch, max_len, num_classes) which contains the\n            unnormalized probability for each class.\n        target: A Variable containing a LongTensor of size\n            (batch, max_len) which contains the index of the true\n            class for each corresponding step.\n        length: A Variable containing a LongTensor of size (batch,)\n            which contains the length of each data in a batch.\n\n    Returns:\n        loss: An average loss value masked by the length.\n    """"""\n\n    # logits_flat: (batch * max_len, num_classes)\n    logits_flat = logits.view(-1, logits.size(-1))\n    # log_probs_flat: (batch * max_len, num_classes)\n    log_probs_flat = functional.log_softmax(logits_flat)\n    # target_flat: (batch * max_len, 1)\n    target_flat = target.view(-1, 1)\n    # losses_flat: (batch * max_len, 1)\n    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n    # losses: (batch, max_len)\n    losses = losses_flat.view(*target.size())\n    # mask: (batch, max_len)\n    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n    losses = losses * mask.float()\n    loss = losses.sum() / length.float().sum()\n    return loss\n'"
Day_04/memn2n/data_utils.py,0,"b'""""""\nData util codes based on https://github.com/domluna/memn2n\n""""""\n\nimport os\nimport re\nimport numpy as np\n\ndef load_task(data_dir, task_id, only_supporting=False):\n    """"""\n    Load the nth task. There are 20 tasks in total.\n    Returns a tuple containing the training and testing data for the task.\n    """"""\n    assert task_id > 0 and task_id < 21\n\n    files = os.listdir(data_dir)\n    files = [os.path.join(data_dir, f) for f in files]\n    s = ""qa{}_"".format(task_id)\n    train_file = [f for f in files if s in f and \'train\' in f][0]\n    test_file = [f for f in files if s in f and \'test\' in f][0]\n    train_data = get_stories(train_file, only_supporting)\n    test_data = get_stories(test_file, only_supporting)\n    return train_data, test_data\n\n\ndef tokenize(sent):\n    """"""\n    Return the tokens of a sentence including punctuation.\n    >>> tokenize(\'Bob dropped the apple. Where is the apple?\')\n    [\'Bob\', \'dropped\', \'the\', \'apple\', \'.\', \'Where\', \'is\', \'the\', \'apple\', \'?\']\n    """"""\n    return [x.strip() for x in re.split(""(\\W+)?"", sent) if x.strip()]\n\n\ndef parse_stories(lines, only_supporting=False):\n    """"""\n    Parse stories provided in the bAbI tasks format\n    If only_supporting is true, only the sentences that support the answer are kept.\n    """"""\n    data = []\n    story = []\n    for line in lines:\n        line = str.lower(line)\n        nid, line = line.split("" "", 1)\n        nid = int(nid)\n        if nid == 1:\n            story = []\n        if ""\\t"" in line: # question\n            q, a, supporting = line.split(""\\t"")\n            q = tokenize(q)\n            # a = tokenize(a)\n            # answer is one vocab word even if it\'s actually multiple words\n            a = [a]\n            substory = None\n\n            # remove question marks\n            if q[-1] == ""?"":\n                q = q[:-1]\n\n            if only_supporting:\n                # Only select the related substory\n                supporting = map(int, supporting.split())\n                substory = [story[i - 1] for i in supporting]\n            else:\n                # Provide all the substories\n                substory = [x for x in story if x]\n\n            data.append((substory, q, a))\n            story.append("""")\n        else: # regular sentence\n            # remove periods\n            sent = tokenize(line)\n            if sent[-1] == ""."":\n                sent = sent[:-1]\n            story.append(sent)\n    return data\n\n\ndef get_stories(f, only_supporting=False):\n    """"""\n    Given a file name, read the file, retrieve the stories,\n    and then convert the sentences into a single story.\n    If max_length is supplied, any stories longer than max_length\n    tokens will be discarded.\n    """"""\n    with open(f) as f:\n        return parse_stories(f.readlines(), only_supporting=only_supporting)\n\n\ndef vectorize_data(data, word_idx, sentence_size, memory_size):\n    """"""\n    Vectorize stories and queries.\n    If a sentence length < sentence_size, the sentence will be padded with 0\'s.\n    If a story length < memory_size, the story will be padded with empty memories.\n    Empty memories are 1-D arrays of length sentence_size filled with 0\'s.\n    The answer array is returned as a one-hot encoding.\n    """"""\n    S, Q, A = [], [], []\n    for story, query, answer in data:\n        ss = []\n        for i, sentence in enumerate(story, 1):\n            ls = max(0, sentence_size - len(sentence))\n            ss.append([word_idx[w] for w in sentence] + [0] * ls)\n\n        # take only the most recent sentences that fit in memory\n        ss = ss[::-1][:memory_size][::-1]\n\n        # Make the last word of each sentence the time \'word\' which\n        # corresponds to vector of lookup table\n        for i in range(len(ss)):\n            ss[i][-1] = len(word_idx) - memory_size - i + len(ss)\n\n        # pad to memory_size\n        lm = max(0, memory_size - len(ss))\n        for _ in range(lm):\n            ss.append([0] * sentence_size)\n\n        lq = max(0, sentence_size - len(query))\n        q = [word_idx[w] for w in query] + [0] * lq\n\n        y = np.zeros(len(word_idx) + 1) # 0 is reserved for nil word\n        for a in answer:\n            y[word_idx[a]] = 1\n\n        S.append(ss); Q.append(q); A.append(y)\n    return np.array(S), np.array(Q), np.array(A)\n'"
Day_04/memn2n/dataset.py,4,"b'import os\nimport random\nfrom itertools import chain\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nfrom data_utils import load_task, vectorize_data\nfrom six.moves import range, reduce\n\nclass bAbIDataset(data.Dataset):\n    def __init__(self, dataset_dir, task_id=1, memory_size=50, train=True):\n        self.train = train\n        self.task_id = task_id\n        self.dataset_dir = dataset_dir\n\n        train_data, test_data = load_task(self.dataset_dir, task_id)\n        data = train_data + test_data\n\n        self.vocab = set()\n        for story, query, answer in data:\n            self.vocab = self.vocab | set(list(chain.from_iterable(story))+query+answer)\n        self.vocab = sorted(self.vocab)\n        word_idx = dict((word, i+1) for i, word in enumerate(self.vocab))\n\n        self.max_story_size = max([len(story) for story, _, _ in data])\n        self.query_size = max([len(query) for _, query, _ in data])\n        self.sentence_size = max([len(row) for row in \\\n            chain.from_iterable([story for story, _, _ in data])])\n        self.memory_size = min(memory_size, self.max_story_size)\n\n        # Add time words/indexes\n        for i in range(self.memory_size):\n            word_idx[""time{}"".format(i+1)] = ""time{}"".format(i+1)\n\n        self.num_vocab = len(word_idx) + 1 # +1 for nil word\n        self.sentence_size = max(self.query_size, self.sentence_size) # for the position\n        self.sentence_size += 1  # +1 for time words\n        self.word_idx = word_idx\n\n        self.mean_story_size = int(np.mean([ len(s) for s, _, _ in data ]))\n\n        if train:\n            story, query, answer = vectorize_data(train_data, self.word_idx,\n                self.sentence_size, self.memory_size)\n        else:\n            story, query, answer = vectorize_data(test_data, self.word_idx,\n                self.sentence_size, self.memory_size)\n\n        self.data_story = torch.LongTensor(story)\n        self.data_query = torch.LongTensor(query)\n        self.data_answer = torch.LongTensor(np.argmax(answer, axis=1))\n\n    def __getitem__(self, idx):\n        return self.data_story[idx], self.data_query[idx], self.data_answer[idx]\n\n    def __len__(self):\n        return len(self.data_story)\n'"
Day_02/pytorch_template/example_project_dir/configs.py,0,"b""import argparse\nimport os\nfrom torch import optim\nfrom torch import nn\n\nbase_dir = os.path.dirname(os.path.abspath(__file__))\n\noptimizer_dict = {\n    'sgd': optim.SGD,\n    'adam': optim.Adam,\n}\n\n\nclass Config(object):\n    def __init__(self, **kwargs):\n        if kwargs is not None:\n            for key, value in kwargs.items():\n                if key == 'optimizer':\n                    value = optimizer_dict[value]\n                setattr(self, key, value)\n\n        self.loss_fn = nn.CrossEntropyLoss\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    #================ Train ==============#\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--lr', type=int, default=1e-3)\n    parser.add_argument('--optimizer', type=str, default='sgd')\n    parser.add_argument('--epochs', type=int, default=20)\n\n    #================ Model ==============#\n    parser.add_argument('--x_size', type=int, default=784)\n    parser.add_argument('--h1_size', type=int, default=500)\n    parser.add_argument('--h2_size', type=int, default=200)\n    parser.add_argument('--label_size', type=int, default=10)\n\n    #================ Path  ==============#\n    parser.add_argument('--save_dir', type=str, default=os.path.join(base_dir, 'log'))\n    parser.add_argument('--data_dir', type=str, default=os.path.join(base_dir, 'datasets'))\n\n    #================ Misc. ==============#\n    parser.add_argument('--log_every_epoch', type=int, default=1)\n    parser.add_argument('--save_every_epoch', type=int, default=1)\n\n    #=============== Parse Arguments===============#\n    kwargs = parser.parse_args()\n\n    # Namespace => Dictionary\n    kwargs = vars(kwargs)\n\n    return kwargs\n\n\ndef get_config():\n    kwargs = parse_args()\n    return Config(**kwargs)\n\n\nif __name__ == '__main__':\n    config = get_config()\n    print(config)\n"""
Day_02/pytorch_template/example_project_dir/data_loader.py,1,"b'from torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils import data\n\n\ndef get_loader(batch_size=100, is_train=True, data_dir=None):\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5,), std=(0.5,)),  # (0, 1) => (-0.5, 0.5) => (-1, 1)\n    ])\n\n    dataset = datasets.MNIST(root=data_dir, train=is_train, transform=transform, download=False)\n\n    loader = data.DataLoader(\n        dataset=dataset, batch_size=100, shuffle=True)\n\n    return loader\n'"
Day_02/pytorch_template/example_project_dir/eval.py,0,"b""from solver import Solver\nfrom data_loader import get_loader\nfrom configs import get_config\n\nif __name__ == '__main__':\n    config = get_config()\n\n    data_loader = get_loader(\n        batch_size=config.batch_size,\n        is_train=False,\n        data_dir=config.data_dir)\n\n    solver = Solver(config, data_loader)\n    solver.build(is_train=False)\n    solver.eval()\n"""
Day_02/pytorch_template/example_project_dir/models.py,0,"b'from torch import nn\n\n\nclass NNClassifier(nn.Module):\n    def __init__(self, config):\n        super(NNClassifier, self).__init__()\n\n        self.config = config\n\n        self.layer_1 = nn.Linear(config.x_size, config.h1_size)\n        self.layer_2 = nn.Linear(config.h1_size, config.h2_size)\n        self.layer_3 = nn.Linear(config.h2_size, config.label_size)\n\n        self.lrelu = nn.LeakyReLU()\n\n        self.softmax = nn.Softmax()\n\n        self.net = nn.Sequential(\n            self.layer_1,  # 784 => 200\n            self.lrelu,\n            self.layer_2,  # 200 => 50\n            self.lrelu,\n            self.layer_3,  # 50 => 10\n            self.softmax,\n        )\n\n    def forward(self, x):\n        # [batch_size, 784] => [batch_size, 1]\n        return self.net(x)\n'"
Day_02/pytorch_template/example_project_dir/solver.py,3,"b""import torch\nfrom torch.autograd import Variable\nfrom models import NNClassifier\n\nfrom tqdm import tqdm\nimport os\nimport numpy as np\n\n\nclass Solver(object):\n    def __init__(self, config, data_loader):\n        self.config = config\n        self.data_loader = data_loader\n\n    def build(self, is_train):\n        self.model = NNClassifier(self.config)\n        self.loss_fn = self.config.loss_fn()\n\n        if is_train:\n            self.optimizer = self.config.optimizer(self.model.parameters(), lr=self.config.lr)\n\n    def train(self):\n        for epoch in tqdm(range(self.config.epochs)):\n            loss_history = []\n            for images, true_labels in self.data_loader:\n                # images: [batch_size, 1, 28, 28]\n                # true_labels: [batch_size]\n\n                # Tensor -> Variable\n                images = Variable(images)\n                true_labels = Variable(true_labels)\n\n                # Resize (for loss function)\n                images = images.view(-1, 28 * 28)  # [batch_size, 1, 28, 28] => [batch_size, 28x28]\n                true_labels = true_labels.view(-1)  # [batch_size, 1] => [batch_size]\n\n                # [batch_size, 28x28] => [batch_size, 10]\n                predicted_labels = self.model(images)\n\n                # Calculate loss\n                average_batch_loss = self.loss_fn(predicted_labels, true_labels)  # [1]\n                loss_history.append(average_batch_loss.data[0])  # Variable -> Tensor\n\n                # Flush out remaining gradient\n                self.optimizer.zero_grad()\n\n                # Backpropagation\n                average_batch_loss.backward()\n\n                # Gradient descent\n                self.optimizer.step()\n\n            # Log intermediate loss\n            if (epoch + 1) % self.config.log_every_epoch == 0:\n                epoch_loss = np.mean(loss_history)\n                log_str = f'Epoch {epoch + 1} | loss: {epoch_loss:.2f}\\n'\n                print(log_str)\n\n            # Save model parameters\n            if (epoch + 1) % self.config.save_every_epoch == 0:\n                ckpt_path = os.path.join(self.config.save_dir, f'epoch-{epoch+1}.pkl')\n                print('Save parameters at ', ckpt_path)\n                torch.save(self.model.state_dict(), ckpt_path)\n\n    def eval(self, epoch=None):\n\n        # Load model parameters\n        if not isinstance(epoch, int):\n            epoch = self.config.epochs\n        ckpt_path = os.path.join(self.config.save_dir, f'epoch-{epoch}.pkl')\n        print('Load parameters from ', ckpt_path)\n        self.model.load_state_dict(torch.load(ckpt_path))\n\n        loss_history = []\n        for images, true_labels in self.data_loader:\n            # images: [batch_size, 1, 28, 28]\n            # true_labels: [batch_size]\n\n            # Tensor -> Variable\n            images = Variable(images)\n            true_labels = Variable(true_labels)\n\n            # Resize (for loss function)\n            images = images.view(-1, 28 * 28)  # [batch_size, 1, 28, 28] => [batch_size, 28x28]\n            true_labels = true_labels.view(-1)  # [batch_size, 1] => [batch_size]\n\n            # [batch_size, 28x28] => [batch_size, 10]\n            predicted_labels = self.model(images)\n\n            # Calculate loss\n            average_batch_loss = self.loss_fn(predicted_labels, true_labels)  # [1]\n            loss_history.append(average_batch_loss.data[0])  # Variable -> Tensor\n\n        epoch_loss = np.mean(loss_history)\n\n        print('Loss: {epoch_loss:.2f}')\n"""
Day_02/pytorch_template/example_project_dir/train.py,0,"b""from solver import Solver\nfrom data_loader import get_loader\nfrom configs import get_config\n\nif __name__ == '__main__':\n    config = get_config()\n\n    data_loader = get_loader(\n        batch_size=config.batch_size,\n        is_train=True,\n        data_dir=config.data_dir)\n\n    solver = Solver(config, data_loader)\n    solver.build(is_train=True)\n    solver.train()\n"""
