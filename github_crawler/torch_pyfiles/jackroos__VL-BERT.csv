file_path,api_count,code
common/fast_rcnn.py,17,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\nfrom common.backbone.resnet.resnet import *\nfrom common.backbone.resnet.resnet import Bottleneck, BasicBlock\nfrom common.backbone.resnet.resnet import model_urls\n\nfrom common.lib.roi_pooling.roi_pool import ROIPool\nfrom common.lib.roi_pooling.roi_align import ROIAlign\nfrom common.utils.flatten import Flattener\nfrom common.utils.pad_sequence import pad_sequence\nfrom common.utils.bbox import coordinate_embeddings\n\n\nclass FastRCNN(nn.Module):\n    def __init__(self, config, average_pool=True, final_dim=768, enable_cnn_reg_loss=False):\n        """"""\n        :param config:\n        :param average_pool: whether or not to average pool the representations\n        :param final_dim:\n        :param is_train:\n        """"""\n        super(FastRCNN, self).__init__()\n        self.average_pool = average_pool\n        self.enable_cnn_reg_loss = enable_cnn_reg_loss\n        self.final_dim = final_dim\n        self.image_feat_precomputed = config.NETWORK.IMAGE_FEAT_PRECOMPUTED\n        if self.image_feat_precomputed:\n            if config.NETWORK.IMAGE_SEMANTIC:\n                self.object_embed = torch.nn.Embedding(num_embeddings=81, embedding_dim=128)\n            else:\n                self.object_embed = None\n        else:\n            self.stride_in_1x1 = config.NETWORK.IMAGE_STRIDE_IN_1x1\n            self.c5_dilated = config.NETWORK.IMAGE_C5_DILATED\n            self.num_layers = config.NETWORK.IMAGE_NUM_LAYERS\n            self.pretrained_model_path = \'{}-{:04d}.model\'.format(config.NETWORK.IMAGE_PRETRAINED,\n                                                                  config.NETWORK.IMAGE_PRETRAINED_EPOCH) if config.NETWORK.IMAGE_PRETRAINED != \'\' else None\n            self.output_conv5 = config.NETWORK.OUTPUT_CONV5\n            if self.num_layers == 18:\n                self.backbone = resnet18(pretrained=True, pretrained_model_path=self.pretrained_model_path,\n                                         expose_stages=[4])\n                block = BasicBlock\n            elif self.num_layers == 34:\n                self.backbone = resnet34(pretrained=True, pretrained_model_path=self.pretrained_model_path,\n                                         expose_stages=[4])\n                block = BasicBlock\n            elif self.num_layers == 50:\n                self.backbone = resnet50(pretrained=True, pretrained_model_path=self.pretrained_model_path,\n                                         expose_stages=[4], stride_in_1x1=self.stride_in_1x1)\n                block = Bottleneck\n            elif self.num_layers == 101:\n                self.backbone = resnet101(pretrained=True, pretrained_model_path=self.pretrained_model_path,\n                                          expose_stages=[4], stride_in_1x1=self.stride_in_1x1)\n                block = Bottleneck\n            elif self.num_layers == 152:\n                self.backbone = resnet152(pretrained=True, pretrained_model_path=self.pretrained_model_path,\n                                          expose_stages=[4], stride_in_1x1=self.stride_in_1x1)\n                block = Bottleneck\n            else:\n                raise NotImplemented\n\n            output_size = (14, 14)\n            self.roi_align = ROIAlign(output_size=output_size, spatial_scale=1.0 / 16)\n\n            if config.NETWORK.IMAGE_SEMANTIC:\n                self.object_embed = torch.nn.Embedding(num_embeddings=81, embedding_dim=128)\n            else:\n                self.object_embed = None\n                self.mask_upsample = None\n\n            self.roi_head_feature_extractor = self.backbone._make_layer(block=block, planes=512, blocks=3,\n                                                                        stride=2 if not self.c5_dilated else 1,\n                                                                        dilation=1 if not self.c5_dilated else 2,\n                                                                        stride_in_1x1=self.stride_in_1x1)\n\n            if average_pool:\n                self.head = torch.nn.Sequential(\n                    self.roi_head_feature_extractor,\n                    nn.AvgPool2d(7 if not self.c5_dilated else 14, stride=1),\n                    Flattener()\n                )\n            else:\n                self.head = self.roi_head_feature_extractor\n\n            if config.NETWORK.IMAGE_FROZEN_BN:\n                for module in self.roi_head_feature_extractor.modules():\n                    if isinstance(module, nn.BatchNorm2d):\n                        for param in module.parameters():\n                            param.requires_grad = False\n\n            frozen_stages = config.NETWORK.IMAGE_FROZEN_BACKBONE_STAGES\n            if 5 in frozen_stages:\n                for p in self.roi_head_feature_extractor.parameters():\n                    p.requires_grad = False\n                frozen_stages = [stage for stage in frozen_stages if stage != 5]\n            self.backbone.frozen_parameters(frozen_stages=frozen_stages,\n                                            frozen_bn=config.NETWORK.IMAGE_FROZEN_BN)\n\n            if self.enable_cnn_reg_loss:\n                self.regularizing_predictor = torch.nn.Linear(2048, 81)\n\n        self.obj_downsample = torch.nn.Sequential(\n            torch.nn.Dropout(p=0.1),\n            torch.nn.Linear(2 * 2048 + (128 if config.NETWORK.IMAGE_SEMANTIC else 0), final_dim),\n            torch.nn.ReLU(inplace=True),\n        )\n\n    def init_weight(self):\n        if not self.image_feat_precomputed:\n            if self.pretrained_model_path is None:\n                pretrained_model = model_zoo.load_url(model_urls[\'resnet{}\'.format(self.num_layers)])\n            else:\n                pretrained_model = torch.load(self.pretrained_model_path, map_location=lambda storage, loc: storage)\n            roi_head_feat_dict = {k[len(\'layer4.\'):]: v for k, v in pretrained_model.items() if k.startswith(\'layer4.\')}\n            self.roi_head_feature_extractor.load_state_dict(roi_head_feat_dict)\n            if self.output_conv5:\n                self.conv5.load_state_dict(roi_head_feat_dict)\n\n    def bn_eval(self):\n        if not self.image_feat_precomputed:\n            for module in self.modules():\n                if isinstance(module, nn.BatchNorm2d):\n                    module.eval()\n\n    def forward(self, images, boxes, box_mask, im_info, classes=None, segms=None, mvrc_ops=None, mask_visual_embed=None):\n        """"""\n        :param images: [batch_size, 3, im_height, im_width]\n        :param boxes: [batch_size, max_num_objects, 4] Padded boxes\n        :param box_mask: [batch_size, max_num_objects] Mask for whether or not each box is OK\n        :return: object reps [batch_size, max_num_objects, dim]\n        """"""\n\n        box_inds = box_mask.nonzero()\n        obj_labels = classes[box_inds[:, 0], box_inds[:, 1]].type(torch.long) if classes is not None else None\n        assert box_inds.shape[0] > 0\n\n        if self.image_feat_precomputed:\n            post_roialign = boxes[box_inds[:, 0], box_inds[:, 1]][:, 4:]\n            boxes = boxes[:, :, :4]\n        else:\n            img_feats = self.backbone(images)\n            rois = torch.cat((\n                box_inds[:, 0, None].type(boxes.dtype),\n                boxes[box_inds[:, 0], box_inds[:, 1]],\n            ), 1)\n            roi_align_res = self.roi_align(img_feats[\'body4\'], rois).type(images.dtype)\n\n            if segms is not None:\n                pool_layers = self.head[1:]\n                post_roialign = self.roi_head_feature_extractor(roi_align_res)\n                post_roialign = post_roialign * segms[box_inds[:, 0], None, box_inds[:, 1]].to(dtype=post_roialign.dtype)\n                for _layer in pool_layers:\n                    post_roialign = _layer(post_roialign)\n            else:\n                post_roialign = self.head(roi_align_res)\n\n            # Add some regularization, encouraging the model to keep giving decent enough predictions\n            if self.enable_cnn_reg_loss:\n                obj_logits = self.regularizing_predictor(post_roialign)\n                cnn_regularization = F.cross_entropy(obj_logits, obj_labels)[None]\n\n        feats_to_downsample = post_roialign if (self.object_embed is None or obj_labels is None) else \\\n            torch.cat((post_roialign, self.object_embed(obj_labels)), -1)\n        if mvrc_ops is not None and mask_visual_embed is not None:\n            _to_masked = (mvrc_ops == 1)[box_inds[:, 0], box_inds[:, 1]]\n            feats_to_downsample[_to_masked] = mask_visual_embed\n        coord_embed = coordinate_embeddings(\n            torch.cat((boxes[box_inds[:, 0], box_inds[:, 1]], im_info[box_inds[:, 0], :2]), 1),\n            256\n        )\n        feats_to_downsample = torch.cat((coord_embed.view((coord_embed.shape[0], -1)), feats_to_downsample), -1)\n        final_feats = self.obj_downsample(feats_to_downsample)\n\n        # Reshape into a padded sequence - this is expensive and annoying but easier to implement and debug...\n        obj_reps = pad_sequence(final_feats, box_mask.sum(1).tolist())\n        post_roialign = pad_sequence(post_roialign, box_mask.sum(1).tolist())\n\n        # DataParallel compatibility\n        obj_reps_padded = obj_reps.new_zeros((obj_reps.shape[0], boxes.shape[1], obj_reps.shape[2]))\n        obj_reps_padded[:, :obj_reps.shape[1]] = obj_reps\n        obj_reps = obj_reps_padded\n        post_roialign_padded = post_roialign.new_zeros((post_roialign.shape[0], boxes.shape[1], post_roialign.shape[2]))\n        post_roialign_padded[:, :post_roialign.shape[1]] = post_roialign\n        post_roialign = post_roialign_padded\n\n        # Output\n        output_dict = {\n            \'obj_reps_raw\': post_roialign,\n            \'obj_reps\': obj_reps,\n        }\n        if (not self.image_feat_precomputed) and self.enable_cnn_reg_loss:\n            output_dict.update({\'obj_logits\': obj_logits,\n                                \'obj_labels\': obj_labels,\n                                \'cnn_regularization_loss\': cnn_regularization})\n\n        if (not self.image_feat_precomputed) and self.output_conv5:\n            image_feature = self.img_head(img_feats[\'body4\'])\n            output_dict[\'image_feature\'] = image_feature\n\n        return output_dict\n'"
common/lr_scheduler.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nfrom bisect import bisect_right\n\nimport torch\n\n\n# FIXME ideally this would be achieved with a CombinedLRScheduler,\n# separating MultiStepLR with WarmupLR\n# but the current LRScheduler design doesn\'t allow it\nclass WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self,\n        optimizer,\n        milestones,\n        gamma=0.1,\n        warmup_factor=1.0 / 3,\n        warmup_iters=500,\n        warmup_method=""linear"",\n        last_epoch=-1,\n    ):\n        if not list(milestones) == sorted(milestones):\n            raise ValueError(\n                ""Milestones should be a list of"" "" increasing integers. Got {}"",\n                milestones,\n            )\n\n        if warmup_method not in (""constant"", ""linear""):\n            raise ValueError(\n                ""Only \'constant\' or \'linear\' warmup_method accepted""\n                ""got {}"".format(warmup_method)\n            )\n        self.milestones = milestones\n        self.gamma = gamma\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.warmup_method = warmup_method\n        super(WarmupMultiStepLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        warmup_factor = 1\n        if self.last_epoch < self.warmup_iters:\n            if self.warmup_method == ""constant"":\n                warmup_factor = self.warmup_factor\n            elif self.warmup_method == ""linear"":\n                alpha = self.last_epoch / self.warmup_iters\n                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n        return [\n            base_lr\n            * warmup_factor\n            * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n            for base_lr in self.base_lrs\n        ]\n'"
common/module.py,4,"b'from collections import namedtuple\nfrom typing import Dict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Module(nn.Module):\n    def __init__(self, config):\n        super(Module, self).__init__()\n        self.config = config\n\n    def init_weight(self):\n        raise NotImplementedError()\n\n    def fix_params(self):\n        raise NotImplementedError()\n\n    def forward(self, *inputs, **kwargs):\n        inputs, kwargs = self.preprocess(*inputs, **kwargs)\n        if self.training:\n            return self.train_forward(*inputs, **kwargs)\n        else:\n            return self.inference_forward(*inputs, **kwargs)\n\n    def train_forward(self, *inputs, **kwargs):\n        """"""\n        def train_forward(self, data, label, **kwargs):\n            # this is a toy example for 1 output, 2 loss function\n\n            output = None\n            loss1 = torch.tensor(0.0)\n            loss2 = torch.tensor(0.0)\n\n            outputs = {\'output\': output,\n                       \'loss1\': loss1,\n                       \'loss2\': loss2}\n            loss = loss1 + loss2\n\n            return outputs, loss\n        """"""\n        raise NotImplemented\n\n    def inference_forward(self, *inputs, **kwargs):\n        """"""\n        def inference_forward(self, data, **kwargs):\n            output = None\n            outputs = {\'output\': output}\n            return outputs\n        """"""\n        raise NotImplemented\n\n    def preprocess(self, *inputs, **kwargs):\n        if self.training:\n            return self.train_preprocess(*inputs, **kwargs)\n        else:\n            return self.inference_preprocess(*inputs, **kwargs)\n\n    def train_preprocess(self, *inputs, **kwargs):\n        return inputs, kwargs\n\n    def inference_preprocess(self, *inputs, **kwargs):\n        return inputs, kwargs\n'"
common/trainer.py,7,"b'import os\nimport time\nfrom collections import namedtuple\nimport torch\n\ntry:\n    from apex import amp\n    from apex.amp import _amp_state\nexcept ImportError:\n    pass\n    #raise ImportError(""Please install apex from https://www.github.com/nvidia/apex if you want to use fp16."")\n\n# Parameter to pass to batch_end_callback\nBatchEndParam = namedtuple(\'BatchEndParams\',\n                           [\'epoch\',\n                            \'nbatch\',\n                            \'rank\',\n                            \'add_step\',\n                            \'data_in_time\',\n                            \'data_transfer_time\',\n                            \'forward_time\',\n                            \'backward_time\',\n                            \'optimizer_time\',\n                            \'metric_time\',\n                            \'eval_metric\',\n                            \'locals\'])\n\n\ndef _multiple_callbacks(callbacks, *args, **kwargs):\n    """"""Sends args and kwargs to any configured callbacks.\n    This handles the cases where the \'callbacks\' variable\n    is ``None``, a single function, or a list.\n    """"""\n    if isinstance(callbacks, list):\n        for cb in callbacks:\n            cb(*args, **kwargs)\n        return\n    if callbacks:\n        callbacks(*args, **kwargs)\n\n\ndef to_cuda(batch):\n    batch = list(batch)\n\n    for i in range(len(batch)):\n        if isinstance(batch[i], torch.Tensor):\n            batch[i] = batch[i].cuda(non_blocking=True)\n        elif isinstance(batch[i], list):\n            for j, o in enumerate(batch[i]):\n                if isinstance(batch[i], torch.Tensor):\n                    batch[i][j] = o.cuda(non_blocking=True)\n\n    return batch\n\n\ndef train(net,\n          optimizer,\n          lr_scheduler,\n          train_loader,\n          train_sampler,\n          metrics,\n          begin_epoch,\n          end_epoch,\n          logger,\n          rank=None,\n          batch_end_callbacks=None,\n          epoch_end_callbacks=None,\n          writer=None,\n          validation_monitor=None,\n          fp16=False,\n          clip_grad_norm=-1,\n          gradient_accumulate_steps=1):\n\n    assert isinstance(gradient_accumulate_steps, int) and gradient_accumulate_steps >= 1\n\n    for epoch in range(begin_epoch, end_epoch):\n        print(\'PROGRESS: %.2f%%\' % (100.0 * epoch / end_epoch))\n\n        # set epoch as random seed of sampler while distributed training\n        if train_sampler is not None and hasattr(train_sampler, \'set_epoch\'):\n            train_sampler.set_epoch(epoch)\n\n        # reset metrics\n        metrics.reset()\n\n        # set net to train mode\n        net.train()\n\n        # clear the paramter gradients\n        # optimizer.zero_grad()\n\n        # init end time\n        end_time = time.time()\n\n        if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            name, value = validation_monitor.metrics.get()\n            val = value[name.index(validation_monitor.host_metric_name)]\n            lr_scheduler.step(val, epoch)\n\n        # training\n        for nbatch, batch in enumerate(train_loader):\n            global_steps = len(train_loader) * epoch + nbatch\n            os.environ[\'global_steps\'] = str(global_steps)\n\n            # record time\n            data_in_time = time.time() - end_time\n\n            # transfer data to GPU\n            data_transfer_time = time.time()\n            batch = to_cuda(batch)\n            data_transfer_time = time.time() - data_transfer_time\n\n            # forward\n            forward_time = time.time()\n            outputs, loss = net(*batch)\n            loss = loss.mean()\n            if gradient_accumulate_steps > 1:\n                loss = loss / gradient_accumulate_steps\n            forward_time = time.time() - forward_time\n\n            # backward\n            backward_time = time.time()\n            if fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            backward_time = time.time() - backward_time\n\n            optimizer_time = time.time()\n            if (global_steps + 1) % gradient_accumulate_steps == 0:\n                # step LR scheduler\n                if lr_scheduler is not None and not isinstance(lr_scheduler,\n                                                               torch.optim.lr_scheduler.ReduceLROnPlateau):\n                    lr_scheduler.step()\n\n                # clip gradient\n                if clip_grad_norm > 0:\n                    if fp16:\n                        total_norm = torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer),\n                                                                    clip_grad_norm)\n                    else:\n                        total_norm = torch.nn.utils.clip_grad_norm_(net.parameters(),\n                                                                    clip_grad_norm)\n                    if writer is not None:\n                        writer.add_scalar(tag=\'grad-para/Total-Norm\',\n                                          scalar_value=float(total_norm),\n                                          global_step=global_steps)\n\n                optimizer.step()\n                # clear the parameter gradients\n                optimizer.zero_grad()\n            optimizer_time = time.time() - optimizer_time\n\n            # update metric\n            metric_time = time.time()\n            metrics.update(outputs)\n            if writer is not None:\n                with torch.no_grad():\n                    for group_i, param_group in enumerate(optimizer.param_groups):\n                        writer.add_scalar(tag=\'Initial-LR/Group_{}\'.format(group_i),\n                                          scalar_value=param_group[\'initial_lr\'],\n                                          global_step=global_steps)\n                        writer.add_scalar(tag=\'LR/Group_{}\'.format(group_i),\n                                          scalar_value=param_group[\'lr\'],\n                                          global_step=global_steps)\n                    writer.add_scalar(tag=\'Train-Loss\',\n                                      scalar_value=float(loss.item()),\n                                      global_step=global_steps)\n                    name, value = metrics.get()\n                    for n, v in zip(name, value):\n                        writer.add_scalar(tag=\'Train-\' + n,\n                                          scalar_value=v,\n                                          global_step=global_steps)\n\n            metric_time = time.time() - metric_time\n\n            # execute batch_end_callbacks\n            if batch_end_callbacks is not None:\n                batch_end_params = BatchEndParam(epoch=epoch, nbatch=nbatch, add_step=True, rank=rank,\n                                                 data_in_time=data_in_time, data_transfer_time=data_transfer_time,\n                                                 forward_time=forward_time, backward_time=backward_time,\n                                                 optimizer_time=optimizer_time, metric_time=metric_time,\n                                                 eval_metric=metrics, locals=locals())\n                _multiple_callbacks(batch_end_callbacks, batch_end_params)\n\n            # update end time\n            end_time = time.time()\n\n        # excute epoch_end_callbacks\n        if validation_monitor is not None:\n            validation_monitor(epoch, net, optimizer, writer)\n        if epoch_end_callbacks is not None:\n            _multiple_callbacks(epoch_end_callbacks, epoch, net, optimizer, writer, validation_monitor=validation_monitor)\n\n\n'"
common/visual_linguistic_bert.py,8,"b'import torch\nimport torch.nn as nn\nfrom external.pytorch_pretrained_bert.modeling import BertLayerNorm, BertEncoder, BertPooler, ACT2FN, BertOnlyMLMHead\n\n# todo: add this to config\nNUM_SPECIAL_WORDS = 1000\n\n\nclass BaseModel(nn.Module):\n    def __init__(self, config, **kwargs):\n        self.config = config\n        super(BaseModel, self).__init__()\n\n    def init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, *args, **kwargs):\n        raise NotImplemented\n\n\nclass VisualLinguisticBert(BaseModel):\n    def __init__(self, config, language_pretrained_model_path=None):\n        super(VisualLinguisticBert, self).__init__(config)\n\n        self.config = config\n\n        # embeddings\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.end_embedding = nn.Embedding(1, config.hidden_size)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n        self.embedding_LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.embedding_dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # for compatibility of roberta\n        self.position_padding_idx = config.position_padding_idx\n\n        # visual transform\n        self.visual_1x1_text = None\n        self.visual_1x1_object = None\n        if config.visual_size != config.hidden_size:\n            self.visual_1x1_text = nn.Linear(config.visual_size, config.hidden_size)\n            self.visual_1x1_object = nn.Linear(config.visual_size, config.hidden_size)\n        if config.visual_ln:\n            self.visual_ln_text = BertLayerNorm(config.hidden_size, eps=1e-12)\n            self.visual_ln_object = BertLayerNorm(config.hidden_size, eps=1e-12)\n        else:\n            visual_scale_text = nn.Parameter(torch.as_tensor(self.config.visual_scale_text_init, dtype=torch.float),\n                                             requires_grad=True)\n            self.register_parameter(\'visual_scale_text\', visual_scale_text)\n            visual_scale_object = nn.Parameter(torch.as_tensor(self.config.visual_scale_object_init, dtype=torch.float),\n                                               requires_grad=True)\n            self.register_parameter(\'visual_scale_object\', visual_scale_object)\n\n        self.encoder = BertEncoder(config)\n\n        if self.config.with_pooler:\n            self.pooler = BertPooler(config)\n\n        # init weights\n        self.apply(self.init_weights)\n        if config.visual_ln:\n            self.visual_ln_text.weight.data.fill_(self.config.visual_scale_text_init)\n            self.visual_ln_object.weight.data.fill_(self.config.visual_scale_object_init)\n\n        # load language pretrained model\n        if language_pretrained_model_path is not None:\n            self.load_language_pretrained_model(language_pretrained_model_path)\n\n        if config.word_embedding_frozen:\n            for p in self.word_embeddings.parameters():\n                p.requires_grad = False\n            self.special_word_embeddings = nn.Embedding(NUM_SPECIAL_WORDS, config.hidden_size)\n            self.special_word_embeddings.weight.data.copy_(self.word_embeddings.weight.data[:NUM_SPECIAL_WORDS])\n\n    def word_embeddings_wrapper(self, input_ids):\n        if self.config.word_embedding_frozen:\n            word_embeddings = self.word_embeddings(input_ids)\n            word_embeddings[input_ids < NUM_SPECIAL_WORDS] \\\n                = self.special_word_embeddings(input_ids[input_ids < NUM_SPECIAL_WORDS])\n            return word_embeddings\n        else:\n            return self.word_embeddings(input_ids)\n\n    def forward(self,\n                text_input_ids,\n                text_token_type_ids,\n                text_visual_embeddings,\n                text_mask,\n                object_vl_embeddings,\n                object_mask,\n                output_all_encoded_layers=True,\n                output_text_and_object_separately=False,\n                output_attention_probs=False):\n\n        # get seamless concatenate embeddings and mask\n        embedding_output, attention_mask, text_mask_new, object_mask_new = self.embedding(text_input_ids,\n                                                                                          text_token_type_ids,\n                                                                                          text_visual_embeddings,\n                                                                                          text_mask,\n                                                                                          object_vl_embeddings,\n                                                                                          object_mask)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        # extended_attention_mask = 1.0 - extended_attention_mask\n        # extended_attention_mask[extended_attention_mask != 0] = float(\'-inf\')\n\n        if output_attention_probs:\n            encoded_layers, attention_probs = self.encoder(embedding_output,\n                                                           extended_attention_mask,\n                                                           output_all_encoded_layers=output_all_encoded_layers,\n                                                           output_attention_probs=output_attention_probs)\n        else:\n            encoded_layers = self.encoder(embedding_output,\n                                          extended_attention_mask,\n                                          output_all_encoded_layers=output_all_encoded_layers,\n                                          output_attention_probs=output_attention_probs)\n        sequence_output = encoded_layers[-1]\n        pooled_output = self.pooler(sequence_output) if self.config.with_pooler else None\n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n\n        if output_text_and_object_separately:\n            if not output_all_encoded_layers:\n                encoded_layers = [encoded_layers]\n            encoded_layers_text = []\n            encoded_layers_object = []\n            for encoded_layer in encoded_layers:\n                max_text_len = text_input_ids.shape[1]\n                max_object_len = object_vl_embeddings.shape[1]\n                encoded_layer_text = encoded_layer[:, :max_text_len]\n                encoded_layer_object = encoded_layer.new_zeros(\n                    (encoded_layer.shape[0], max_object_len, encoded_layer.shape[2]))\n                encoded_layer_object[object_mask] = encoded_layer[object_mask_new]\n                encoded_layers_text.append(encoded_layer_text)\n                encoded_layers_object.append(encoded_layer_object)\n            if not output_all_encoded_layers:\n                encoded_layers_text = encoded_layers_text[0]\n                encoded_layers_object = encoded_layers_object[0]\n            if output_attention_probs:\n                return encoded_layers_text, encoded_layers_object, pooled_output, attention_probs\n            else:\n                return encoded_layers_text, encoded_layers_object, pooled_output\n        else:\n            if output_attention_probs:\n                return encoded_layers, pooled_output, attention_probs\n            else:\n                return encoded_layers, pooled_output\n\n    def embedding(self,\n                  text_input_ids,\n                  text_token_type_ids,\n                  text_visual_embeddings,\n                  text_mask,\n                  object_vl_embeddings,\n                  object_mask):\n\n        text_linguistic_embedding = self.word_embeddings_wrapper(text_input_ids)\n        if self.visual_1x1_text is not None:\n            text_visual_embeddings = self.visual_1x1_text(text_visual_embeddings)\n        if self.config.visual_ln:\n            text_visual_embeddings = self.visual_ln_text(text_visual_embeddings)\n        else:\n            text_visual_embeddings *= self.visual_scale_text\n        text_vl_embeddings = text_linguistic_embedding + text_visual_embeddings\n\n        object_visual_embeddings = object_vl_embeddings[:, :, :self.config.visual_size]\n        if self.visual_1x1_object is not None:\n            object_visual_embeddings = self.visual_1x1_object(object_visual_embeddings)\n        if self.config.visual_ln:\n            object_visual_embeddings = self.visual_ln_object(object_visual_embeddings)\n        else:\n            object_visual_embeddings *= self.visual_scale_object\n        object_linguistic_embeddings = object_vl_embeddings[:, :, self.config.visual_size:]\n        object_vl_embeddings = object_linguistic_embeddings + object_visual_embeddings\n\n        bs = text_vl_embeddings.size(0)\n        vl_embed_size = text_vl_embeddings.size(-1)\n        max_length = (text_mask.sum(1) + object_mask.sum(1)).max() + 1\n        grid_ind, grid_pos = torch.meshgrid(torch.arange(bs, dtype=torch.long, device=text_vl_embeddings.device),\n                                            torch.arange(max_length, dtype=torch.long, device=text_vl_embeddings.device))\n        text_end = text_mask.sum(1, keepdim=True)\n        object_end = text_end + object_mask.sum(1, keepdim=True)\n\n        # seamlessly concatenate visual linguistic embeddings of text and object\n        _zero_id = torch.zeros((bs, ), dtype=torch.long, device=text_vl_embeddings.device)\n        vl_embeddings = text_vl_embeddings.new_zeros((bs, max_length, vl_embed_size))\n        vl_embeddings[grid_pos < text_end] = text_vl_embeddings[text_mask]\n        vl_embeddings[(grid_pos >= text_end) & (grid_pos < object_end)]  = object_vl_embeddings[object_mask]\n        vl_embeddings[grid_pos == object_end] = self.end_embedding(_zero_id)\n\n        # token type embeddings/ segment embeddings\n        token_type_ids = text_token_type_ids.new_zeros((bs, max_length))\n        token_type_ids[grid_pos < text_end] = text_token_type_ids[text_mask]\n        token_type_ids[(grid_pos >= text_end) & (grid_pos <= object_end)] = 2\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        # position embeddings\n        position_ids = grid_pos + self.position_padding_idx + 1\n        if self.config.obj_pos_id_relative:\n            position_ids[(grid_pos >= text_end) & (grid_pos < object_end)] \\\n                = text_end.expand((bs, max_length))[(grid_pos >= text_end) & (grid_pos < object_end)] \\\n                + self.position_padding_idx + 1\n            position_ids[grid_pos == object_end] = (text_end + 1).squeeze(1) + self.position_padding_idx + 1\n        else:\n            assert False, ""Don\'t use position id 510/511 for objects and [END]!!!""\n            position_ids[(grid_pos >= text_end) & (grid_pos < object_end)] = self.config.max_position_embeddings - 2\n            position_ids[grid_pos == object_end] = self.config.max_position_embeddings - 1\n\n        position_embeddings = self.position_embeddings(position_ids)\n        mask = text_mask.new_zeros((bs, max_length))\n        mask[grid_pos <= object_end] = 1\n\n        embeddings = vl_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.embedding_LayerNorm(embeddings)\n        embeddings = self.embedding_dropout(embeddings)\n\n        return embeddings, mask, grid_pos < text_end, (grid_pos >= text_end) & (grid_pos < object_end)\n\n    def load_language_pretrained_model(self, language_pretrained_model_path):\n        pretrained_state_dict = torch.load(language_pretrained_model_path, map_location=lambda storage, loc: storage)\n        encoder_pretrained_state_dict = {}\n        pooler_pretrained_state_dict = {}\n        embedding_ln_pretrained_state_dict = {}\n        unexpected_keys = []\n        for k, v in pretrained_state_dict.items():\n            if k.startswith(\'bert.\'):\n                k = k[len(\'bert.\'):]\n            elif k.startswith(\'roberta.\'):\n                k = k[len(\'roberta.\'):]\n            else:\n                unexpected_keys.append(k)\n                continue\n            if \'gamma\' in k:\n                k = k.replace(\'gamma\', \'weight\')\n            if \'beta\' in k:\n                k = k.replace(\'beta\', \'bias\')\n            if k.startswith(\'encoder.\'):\n                k_ = k[len(\'encoder.\'):]\n                if k_ in self.encoder.state_dict():\n                    encoder_pretrained_state_dict[k_] = v\n                else:\n                    unexpected_keys.append(k)\n            elif k.startswith(\'embeddings.\'):\n                k_ = k[len(\'embeddings.\'):]\n                if k_ == \'word_embeddings.weight\':\n                    self.word_embeddings.weight.data = v.to(dtype=self.word_embeddings.weight.data.dtype,\n                                                            device=self.word_embeddings.weight.data.device)\n                elif k_ == \'position_embeddings.weight\':\n                    self.position_embeddings.weight.data = v.to(dtype=self.position_embeddings.weight.data.dtype,\n                                                                device=self.position_embeddings.weight.data.device)\n                elif k_ == \'token_type_embeddings.weight\':\n                    self.token_type_embeddings.weight.data[:v.size(0)] = v.to(\n                        dtype=self.token_type_embeddings.weight.data.dtype,\n                        device=self.token_type_embeddings.weight.data.device)\n                    if v.size(0) == 1:\n                        # Todo: roberta token type embedding\n                        self.token_type_embeddings.weight.data[1] = v[0].clone().to(\n                            dtype=self.token_type_embeddings.weight.data.dtype,\n                            device=self.token_type_embeddings.weight.data.device)\n                        self.token_type_embeddings.weight.data[2] = v[0].clone().to(\n                            dtype=self.token_type_embeddings.weight.data.dtype,\n                            device=self.token_type_embeddings.weight.data.device)\n\n                elif k_.startswith(\'LayerNorm.\'):\n                    k__ = k_[len(\'LayerNorm.\'):]\n                    if k__ in self.embedding_LayerNorm.state_dict():\n                        embedding_ln_pretrained_state_dict[k__] = v\n                    else:\n                        unexpected_keys.append(k)\n                else:\n                    unexpected_keys.append(k)\n            elif self.config.with_pooler and k.startswith(\'pooler.\'):\n                k_ = k[len(\'pooler.\'):]\n                if k_ in self.pooler.state_dict():\n                    pooler_pretrained_state_dict[k_] = v\n                else:\n                    unexpected_keys.append(k)\n            else:\n                unexpected_keys.append(k)\n        if len(unexpected_keys) > 0:\n            print(""Warnings: Unexpected keys: {}."".format(unexpected_keys))\n        self.embedding_LayerNorm.load_state_dict(embedding_ln_pretrained_state_dict)\n        self.encoder.load_state_dict(encoder_pretrained_state_dict)\n        if self.config.with_pooler and len(pooler_pretrained_state_dict) > 0:\n            self.pooler.load_state_dict(pooler_pretrained_state_dict)\n\n\nclass VisualLinguisticBertForPretraining(VisualLinguisticBert):\n    def __init__(self, config, language_pretrained_model_path=None,\n                 with_rel_head=True, with_mlm_head=True, with_mvrc_head=True):\n\n        super(VisualLinguisticBertForPretraining, self).__init__(config, language_pretrained_model_path=None)\n\n        self.with_rel_head = with_rel_head\n        self.with_mlm_head = with_mlm_head\n        self.with_mvrc_head = with_mvrc_head\n        if with_rel_head:\n            self.relationsip_head = VisualLinguisticBertRelationshipPredictionHead(config)\n        if with_mlm_head:\n            self.mlm_head = BertOnlyMLMHead(config, self.word_embeddings.weight)\n        if with_mvrc_head:\n            self.mvrc_head = VisualLinguisticBertMVRCHead(config)\n\n        # init weights\n        self.apply(self.init_weights)\n        if config.visual_ln:\n            self.visual_ln_text.weight.data.fill_(self.config.visual_scale_text_init)\n            self.visual_ln_object.weight.data.fill_(self.config.visual_scale_object_init)\n\n        # load language pretrained model\n        if language_pretrained_model_path is not None:\n            self.load_language_pretrained_model(language_pretrained_model_path)\n\n        if config.word_embedding_frozen:\n            for p in self.word_embeddings.parameters():\n                p.requires_grad = False\n\n        if config.pos_embedding_frozen:\n            for p in self.position_embeddings.parameters():\n                p.requires_grad = False\n\n    def forward(self,\n                text_input_ids,\n                text_token_type_ids,\n                text_visual_embeddings,\n                text_mask,\n                object_vl_embeddings,\n                object_mask,\n                output_all_encoded_layers=True,\n                output_text_and_object_separately=False):\n\n        text_out, object_out, pooled_rep = super(VisualLinguisticBertForPretraining, self).forward(\n            text_input_ids,\n            text_token_type_ids,\n            text_visual_embeddings,\n            text_mask,\n            object_vl_embeddings,\n            object_mask,\n            output_all_encoded_layers=False,\n            output_text_and_object_separately=True\n        )\n\n        if self.with_rel_head:\n            relationship_logits = self.relationsip_head(pooled_rep)\n        else:\n            relationship_logits = None\n        if self.with_mlm_head:\n            mlm_logits = self.mlm_head(text_out)\n        else:\n            mlm_logits = None\n        if self.with_mvrc_head:\n            mvrc_logits = self.mvrc_head(object_out)\n        else:\n            mvrc_logits = None\n\n        return relationship_logits, mlm_logits, mvrc_logits\n\n    def load_language_pretrained_model(self, language_pretrained_model_path):\n        pretrained_state_dict = torch.load(language_pretrained_model_path, map_location=lambda storage, loc: storage)\n        encoder_pretrained_state_dict = {}\n        pooler_pretrained_state_dict = {}\n        embedding_ln_pretrained_state_dict = {}\n        relationship_head_pretrained_state_dict = {}\n        mlm_head_pretrained_state_dict = {}\n        unexpected_keys = []\n        for _k, v in pretrained_state_dict.items():\n            if _k.startswith(\'bert.\') or _k.startswith(\'roberta.\'):\n                k = _k[len(\'bert.\'):] if _k.startswith(\'bert.\') else _k[len(\'roberta.\'):]\n                if \'gamma\' in k:\n                    k = k.replace(\'gamma\', \'weight\')\n                if \'beta\' in k:\n                    k = k.replace(\'beta\', \'bias\')\n                if k.startswith(\'encoder.\'):\n                    k_ = k[len(\'encoder.\'):]\n                    if k_ in self.encoder.state_dict():\n                        encoder_pretrained_state_dict[k_] = v\n                    else:\n                        unexpected_keys.append(_k)\n                elif k.startswith(\'embeddings.\'):\n                    k_ = k[len(\'embeddings.\'):]\n                    if k_ == \'word_embeddings.weight\':\n                        self.word_embeddings.weight.data = v.to(dtype=self.word_embeddings.weight.data.dtype,\n                                                                device=self.word_embeddings.weight.data.device)\n                    elif k_ == \'position_embeddings.weight\':\n                        self.position_embeddings.weight.data = v.to(dtype=self.position_embeddings.weight.data.dtype,\n                                                                    device=self.position_embeddings.weight.data.device)\n                    elif k_ == \'token_type_embeddings.weight\':\n                        self.token_type_embeddings.weight.data[:v.size(0)] = v.to(\n                            dtype=self.token_type_embeddings.weight.data.dtype,\n                            device=self.token_type_embeddings.weight.data.device)\n                        if v.size(0) == 1:\n                            # Todo: roberta token type embedding\n                            self.token_type_embeddings.weight.data[1] = v[0].to(\n                                dtype=self.token_type_embeddings.weight.data.dtype,\n                                device=self.token_type_embeddings.weight.data.device)\n                    elif k_.startswith(\'LayerNorm.\'):\n                        k__ = k_[len(\'LayerNorm.\'):]\n                        if k__ in self.embedding_LayerNorm.state_dict():\n                            embedding_ln_pretrained_state_dict[k__] = v\n                        else:\n                            unexpected_keys.append(_k)\n                    else:\n                        unexpected_keys.append(_k)\n                elif self.config.with_pooler and k.startswith(\'pooler.\'):\n                    k_ = k[len(\'pooler.\'):]\n                    if k_ in self.pooler.state_dict():\n                        pooler_pretrained_state_dict[k_] = v\n                    else:\n                        unexpected_keys.append(_k)\n            elif _k.startswith(\'cls.seq_relationship.\') and self.with_rel_head:\n                k_ = _k[len(\'cls.seq_relationship.\'):]\n                if \'gamma\' in k_:\n                    k_ = k_.replace(\'gamma\', \'weight\')\n                if \'beta\' in k_:\n                    k_ = k_.replace(\'beta\', \'bias\')\n                if k_ in self.relationsip_head.caption_image_relationship.state_dict():\n                    relationship_head_pretrained_state_dict[k_] = v\n                else:\n                    unexpected_keys.append(_k)\n            elif (_k.startswith(\'cls.predictions.\') or _k.startswith(\'lm_head.\')) and self.with_mlm_head:\n                k_ = _k[len(\'cls.predictions.\'):] if _k.startswith(\'cls.predictions.\') else _k[len(\'lm_head.\'):]\n                if _k.startswith(\'lm_head.\'):\n                    if \'dense\' in k_ or \'layer_norm\' in k_:\n                        k_ = \'transform.\' + k_\n                    if \'layer_norm\' in k_:\n                        k_ = k_.replace(\'layer_norm\', \'LayerNorm\')\n                if \'gamma\' in k_:\n                    k_ = k_.replace(\'gamma\', \'weight\')\n                if \'beta\' in k_:\n                    k_ = k_.replace(\'beta\', \'bias\')\n                if k_ in self.mlm_head.predictions.state_dict():\n                    mlm_head_pretrained_state_dict[k_] = v\n                else:\n                    unexpected_keys.append(_k)\n            else:\n                unexpected_keys.append(_k)\n        if len(unexpected_keys) > 0:\n            print(""Warnings: Unexpected keys: {}."".format(unexpected_keys))\n        self.embedding_LayerNorm.load_state_dict(embedding_ln_pretrained_state_dict)\n        self.encoder.load_state_dict(encoder_pretrained_state_dict)\n        if self.config.with_pooler and len(pooler_pretrained_state_dict) > 0:\n            self.pooler.load_state_dict(pooler_pretrained_state_dict)\n        if self.with_rel_head and len(relationship_head_pretrained_state_dict) > 0:\n            self.relationsip_head.caption_image_relationship.load_state_dict(relationship_head_pretrained_state_dict)\n        if self.with_mlm_head:\n            self.mlm_head.predictions.load_state_dict(mlm_head_pretrained_state_dict)\n\n\nclass VisualLinguisticBertMVRCHeadTransform(BaseModel):\n    def __init__(self, config):\n        super(VisualLinguisticBertMVRCHeadTransform, self).__init__(config)\n\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.act = ACT2FN[config.hidden_act]\n\n        self.apply(self.init_weights)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.act(hidden_states)\n\n        return hidden_states\n\n\nclass VisualLinguisticBertMVRCHead(BaseModel):\n    def __init__(self, config):\n        super(VisualLinguisticBertMVRCHead, self).__init__(config)\n\n        self.transform = VisualLinguisticBertMVRCHeadTransform(config)\n        self.region_cls_pred = nn.Linear(config.hidden_size, config.visual_region_classes)\n        self.apply(self.init_weights)\n\n    def forward(self, hidden_states):\n\n        hidden_states = self.transform(hidden_states)\n        logits = self.region_cls_pred(hidden_states)\n\n        return logits\n\n\nclass VisualLinguisticBertRelationshipPredictionHead(BaseModel):\n    def __init__(self, config):\n        super(VisualLinguisticBertRelationshipPredictionHead, self).__init__(config)\n\n        self.caption_image_relationship = nn.Linear(config.hidden_size, 2)\n        self.apply(self.init_weights)\n\n    def forward(self, pooled_rep):\n\n        relationship_logits = self.caption_image_relationship(pooled_rep)\n\n        return relationship_logits\n\n\n\n\n'"
pretrain/train_end2end.py,1,"b""import _init_paths\nimport os\nimport argparse\nimport torch\nimport subprocess\n\nfrom pretrain.function.config import config, update_config\nfrom pretrain.function.train import train_net\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Train Cognition Network')\n    parser.add_argument('--cfg', type=str, help='path to config file')\n    parser.add_argument('--model-dir', type=str, help='root path to store checkpoint')\n    parser.add_argument('--log-dir', type=str, help='tensorboard log dir')\n    parser.add_argument('--dist', help='whether to use distributed training', default=False, action='store_true')\n    parser.add_argument('--slurm', help='whether this is a slurm job', default=False, action='store_true')\n    parser.add_argument('--do-test', help='whether to generate csv result on test set',\n                        default=False, action='store_true')\n    parser.add_argument('--cudnn-off', help='disable cudnn', default=False, action='store_true')\n\n    args = parser.parse_args()\n\n    if args.cfg is not None:\n        update_config(args.cfg)\n    if args.model_dir is not None:\n        config.OUTPUT_PATH = os.path.join(args.model_dir, config.OUTPUT_PATH)\n\n    if args.slurm:\n        proc_id = int(os.environ['SLURM_PROCID'])\n        ntasks = int(os.environ['SLURM_NTASKS'])\n        node_list = os.environ['SLURM_NODELIST']\n        num_gpus = torch.cuda.device_count()\n        addr = subprocess.getoutput(\n            'scontrol show hostname {} | head -n1'.format(node_list))\n        os.environ['MASTER_PORT'] = str(29500)\n        os.environ['MASTER_ADDR'] = addr\n        os.environ['WORLD_SIZE'] = str(ntasks)\n        os.environ['RANK'] = str(proc_id)\n        os.environ['LOCAL_RANK'] = str(proc_id % num_gpus)\n\n    return args, config\n\n\ndef main():\n    args, config = parse_args()\n    rank, model = train_net(args, config)\n\n\nif __name__ == '__main__':\n    main()\n\n\n"""
pretrain/vis_attention_maps.py,1,"b""import _init_paths\nimport os\nimport argparse\nimport torch\nimport subprocess\n\nfrom pretrain.function.config import config, update_config\nfrom pretrain.function.vis import vis_net\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Visualize Attention Maps')\n    parser.add_argument('--cfg', type=str, help='path to config file')\n    parser.add_argument('--dist', help='whether to use distributed training', default=False, action='store_true')\n    parser.add_argument('--slurm', help='whether this is a slurm job', default=False, action='store_true')\n    parser.add_argument('--save-dir', help='directory to save attention maps', type=str, default='./attention_maps')\n\n    args = parser.parse_args()\n\n    if args.cfg is not None:\n        update_config(args.cfg)\n\n    if args.slurm:\n        proc_id = int(os.environ['SLURM_PROCID'])\n        ntasks = int(os.environ['SLURM_NTASKS'])\n        node_list = os.environ['SLURM_NODELIST']\n        num_gpus = torch.cuda.device_count()\n        addr = subprocess.getoutput(\n            'scontrol show hostname {} | head -n1'.format(node_list))\n        os.environ['MASTER_PORT'] = str(29500)\n        os.environ['MASTER_ADDR'] = addr\n        os.environ['WORLD_SIZE'] = str(ntasks)\n        os.environ['RANK'] = str(proc_id)\n        os.environ['LOCAL_RANK'] = str(proc_id % num_gpus)\n\n    return args, config\n\n\ndef main():\n    args, config = parse_args()\n    rank, model = vis_net(args, config, args.save_dir)\n\n\nif __name__ == '__main__':\n    main()"""
refcoco/train_end2end.py,1,"b""import _init_paths\nimport os\nimport argparse\nimport torch\nimport subprocess\n\nfrom refcoco.function.config import config, update_config\nfrom refcoco.function.train import train_net\nfrom refcoco.function.test import test_net\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Train Cognition Network')\n    parser.add_argument('--cfg', type=str, help='path to config file')\n    parser.add_argument('--model-dir', type=str, help='root path to store checkpoint')\n    parser.add_argument('--log-dir', type=str, help='tensorboard log dir')\n    parser.add_argument('--dist', help='whether to use distributed training', default=False, action='store_true')\n    parser.add_argument('--slurm', help='whether this is a slurm job', default=False, action='store_true')\n    parser.add_argument('--do-test', help='whether to generate csv result on test set',\n                        default=False, action='store_true')\n    parser.add_argument('--cudnn-off', help='disable cudnn', default=False, action='store_true')\n\n    # easy test pretrain model\n    parser.add_argument('--partial-pretrain', type=str)\n\n    args = parser.parse_args()\n\n    if args.cfg is not None:\n        update_config(args.cfg)\n    if args.model_dir is not None:\n        config.OUTPUT_PATH = os.path.join(args.model_dir, config.OUTPUT_PATH)\n\n    if args.partial_pretrain is not None:\n        config.NETWORK.PARTIAL_PRETRAIN = args.partial_pretrain\n\n    if args.slurm:\n        proc_id = int(os.environ['SLURM_PROCID'])\n        ntasks = int(os.environ['SLURM_NTASKS'])\n        node_list = os.environ['SLURM_NODELIST']\n        num_gpus = torch.cuda.device_count()\n        addr = subprocess.getoutput(\n            'scontrol show hostname {} | head -n1'.format(node_list))\n        os.environ['MASTER_PORT'] = str(29500)\n        os.environ['MASTER_ADDR'] = addr\n        os.environ['WORLD_SIZE'] = str(ntasks)\n        os.environ['RANK'] = str(proc_id)\n        os.environ['LOCAL_RANK'] = str(proc_id % num_gpus)\n\n    return args, config\n\n\ndef main():\n    args, config = parse_args()\n    rank, model = train_net(args, config)\n    if args.do_test and (rank is None or rank == 0):\n        test_net(args, config)\n\n\nif __name__ == '__main__':\n    main()\n\n\n"""
scripts/launch.py,11,"b'r""""""\n`torch.distributed.launch` is a module that spawns up multiple distributed\ntraining processes on each of the training nodes.\nThe utility can be used for single-node distributed training, in which one or\nmore processes per node will be spawned. The utility can be used for either\nCPU training or GPU training. If the utility is used for GPU training,\neach distributed process will be operating on a single GPU. This can achieve\nwell-improved single-node training performance. It can also be used in\nmulti-node distributed training, by spawning up multiple processes on each node\nfor well-improved multi-node distributed training performance as well.\nThis will especially be benefitial for systems with multiple Infiniband\ninterfaces that have direct-GPU support, since all of them can be utilized for\naggregated communication bandwidth.\nIn both cases of single-node distributed training or multi-node distributed\ntraining, this utility will launch the given number of processes per node\n(``--nproc_per_node``). If used for GPU training, this number needs to be less\nor euqal to the number of GPUs on the current system (``nproc_per_node``),\nand each process will be operating on a single GPU from *GPU 0 to\nGPU (nproc_per_node - 1)*.\n**How to use this module:**\n1. Single-Node multi-process distributed training\n::\n    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n               YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n               arguments of your training script)\n2. Multi-Node multi-process distributed training: (e.g. two nodes)\nNode 1: *(IP: 192.168.1.1, and has a free port: 1234)*\n::\n    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n               --nnodes=2 --node_rank=0 --master_addr=""192.168.1.1""\n               --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n               and all other arguments of your training script)\nNode 2:\n::\n    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n               --nnodes=2 --node_rank=1 --master_addr=""192.168.1.1""\n               --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n               and all other arguments of your training script)\n3. To look up what optional arguments this module offers:\n::\n    >>> python -m torch.distributed.launch --help\n**Important Notices:**\n1. This utilty and multi-process distributed (single-node or\nmulti-node) GPU training currently only achieves the best performance using\nthe NCCL distributed backend. Thus NCCL backend is the recommended backend to\nuse for GPU training.\n2. In your training program, you must parse the command-line argument:\n``--local_rank=LOCAL_PROCESS_RANK``, which will be provided by this module.\nIf your training program uses GPUs, you should ensure that your code only\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:\nParsing the local_rank argument\n::\n    >>> import argparse\n    >>> parser = argparse.ArgumentParser()\n    >>> parser.add_argument(""--local_rank"", type=int)\n    >>> args = parser.parse_args()\nSet your device to local rank using either\n::\n    >>> torch.cuda.set_device(arg.local_rank)  # before your code runs\nor\n::\n    >>> with torch.cuda.device(arg.local_rank):\n    >>>    # your code to run\n3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses ``env://``, which is the only supported ``init_method``\nby this module.\n::\n    torch.distributed.init_process_group(backend=\'YOUR BACKEND\',\n                                         init_method=\'env://\')\n4. In your training program, you can either use regular distributed functions\nor use :func:`torch.nn.parallel.DistributedDataParallel` module. If your\ntraining program uses GPUs for training and you would like to use\n:func:`torch.nn.parallel.DistributedDataParallel` module,\nhere is how to configure it.\n::\n    model = torch.nn.parallel.DistributedDataParallel(model,\n                                                      device_ids=[arg.local_rank],\n                                                      output_device=arg.local_rank)\nPlease ensure that ``device_ids`` argument is set to be the only GPU device id\nthat your code will be operating on. This is generally the local rank of the\nprocess. In other words, the ``device_ids`` needs to be ``[args.local_rank]``,\nand ``output_device`` needs to be ``args.local_rank`` in order to use this\nutility\n5. Another way to pass ``local_rank`` to the subprocesses via environment variable\n``LOCAL_RANK``. This behavior is enabled when you launch the script with\n``--use_env=True``. You must adjust the subprocess example above to replace\n``args.local_rank`` with ``os.environ[\'LOCAL_RANK\']``; the launcher\nwill not pass ``--local_rank`` when you specify this flag.\n.. warning::\n    ``local_rank`` is NOT globally unique: it is only unique per process\n    on a machine.  Thus, don\'t use it to decide if you should, e.g.,\n    write to a networked filesystem.  See\n    https://github.com/pytorch/pytorch/issues/12042 for an example of\n    how things can go wrong if you don\'t do this correctly.\n""""""\n\n\nimport sys\nimport subprocess\nimport os\nimport socket\nfrom argparse import ArgumentParser, REMAINDER\n\nimport torch\n\n\ndef parse_args():\n    """"""\n    Helper function parsing the command line options\n    @retval ArgumentParser\n    """"""\n    parser = ArgumentParser(description=""PyTorch distributed training launch ""\n                                        ""helper utilty that will spawn up ""\n                                        ""multiple distributed processes"")\n\n    # Optional arguments for the launch helper\n    parser.add_argument(""--nnodes"", type=int, default=1,\n                        help=""The number of nodes to use for distributed ""\n                             ""training"")\n    parser.add_argument(""--node_rank"", type=int, default=0,\n                        help=""The rank of the node for multi-node distributed ""\n                             ""training"")\n    parser.add_argument(""--nproc_per_node"", type=int, default=1,\n                        help=""The number of processes to launch on each node, ""\n                             ""for GPU training, this is recommended to be set ""\n                             ""to the number of GPUs in your system so that ""\n                             ""each process can be bound to a single GPU."")\n    parser.add_argument(""--master_addr"", default=""127.0.0.1"", type=str,\n                        help=""Master node (rank 0)\'s address, should be either ""\n                             ""the IP address or the hostname of node 0, for ""\n                             ""single node multi-proc training, the ""\n                             ""--master_addr can simply be 127.0.0.1"")\n    parser.add_argument(""--master_port"", default=29500, type=int,\n                        help=""Master node (rank 0)\'s free port that needs to ""\n                             ""be used for communciation during distributed ""\n                             ""training"")\n    # parser.add_argument(""--use_env"", default=False, action=""store_true"",\n    #                     help=""Use environment variable to pass ""\n    #                          ""\'local rank\'. For legacy reasons, the default value is False. ""\n    #                          ""If set to True, the script will not pass ""\n    #                          ""--local_rank as argument, and will instead set LOCAL_RANK."")\n\n    # positional\n    parser.add_argument(""training_script"", type=str,\n                        help=""The full path to the single GPU training ""\n                             ""program/script to be launched in parallel, ""\n                             ""followed by all the arguments for the ""\n                             ""training script"")\n\n    # rest from the training program\n    parser.add_argument(\'training_script_args\', nargs=REMAINDER)\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    # world size in terms of number of processes\n    dist_world_size = args.nproc_per_node * args.nnodes\n\n    # set PyTorch distributed related environmental variables\n    current_env = os.environ.copy()\n    current_env[""MASTER_ADDR""] = args.master_addr\n    current_env[""MASTER_PORT""] = str(args.master_port)\n    current_env[""WORLD_SIZE""] = str(dist_world_size)\n\n    processes = []\n\n    for local_rank in range(0, args.nproc_per_node):\n        # each process\'s rank\n        dist_rank = args.nproc_per_node * args.node_rank + local_rank\n        current_env[""RANK""] = str(dist_rank)\n        current_env[""LOCAL_RANK""] = str(local_rank)\n\n        # # spawn the processes\n        # if args.use_env:\n        #     cmd = [sys.executable, ""-u"",\n        #            args.training_script] + args.training_script_args\n        # else:\n        #     cmd = [sys.executable,\n        #            ""-u"",\n        #            args.training_script,\n        #            ""--local_rank={}"".format(local_rank)] + args.training_script_args\n\n        cmd = [sys.executable, ""-u"",\n               args.training_script] + args.training_script_args + [""--dist""]\n\n        process = subprocess.Popen(cmd, env=current_env)\n        processes.append(process)\n\n    for process in processes:\n        process.wait()\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(returncode=process.returncode,\n                                                cmd=process.args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
vcr/train_end2end.py,1,"b""import _init_paths\nimport os\nimport argparse\nimport torch\nimport subprocess\n\nfrom vcr.function.config import config, update_config\nfrom vcr.function.train import train_net\nfrom vcr.function.test import test_net\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Train Cognition Network')\n    parser.add_argument('--cfg', type=str, help='path to config file')\n    parser.add_argument('--model-dir', type=str, help='root path to store checkpoint')\n    parser.add_argument('--log-dir', type=str, help='tensorboard log dir')\n    parser.add_argument('--dist', help='whether to use distributed training', default=False, action='store_true')\n    parser.add_argument('--slurm', help='whether this is a slurm job', default=False, action='store_true')\n    parser.add_argument('--do-test', help='whether to generate csv result on test set',\n                        default=False, action='store_true')\n    parser.add_argument('--cudnn-off', help='disable cudnn', default=False, action='store_true')\n\n    # easy test pretrain model\n    parser.add_argument('--partial-pretrain', type=str)\n\n    args = parser.parse_args()\n\n    if args.cfg is not None:\n        update_config(args.cfg)\n    if args.model_dir is not None:\n        config.OUTPUT_PATH = os.path.join(args.model_dir, config.OUTPUT_PATH)\n\n    if args.partial_pretrain is not None:\n        config.NETWORK.PARTIAL_PRETRAIN = args.partial_pretrain\n\n    if args.slurm:\n        proc_id = int(os.environ['SLURM_PROCID'])\n        ntasks = int(os.environ['SLURM_NTASKS'])\n        node_list = os.environ['SLURM_NODELIST']\n        num_gpus = torch.cuda.device_count()\n        addr = subprocess.getoutput(\n            'scontrol show hostname {} | head -n1'.format(node_list))\n        os.environ['MASTER_PORT'] = str(29500)\n        os.environ['MASTER_ADDR'] = addr\n        os.environ['WORLD_SIZE'] = str(ntasks)\n        os.environ['RANK'] = str(proc_id)\n        os.environ['LOCAL_RANK'] = str(proc_id % num_gpus)\n\n    return args, config\n\n\ndef main():\n    args, config = parse_args()\n    rank, model = train_net(args, config)\n    if args.do_test and (rank is None or rank == 0):\n        test_net(args, config)\n\n\nif __name__ == '__main__':\n    main()\n\n\n"""
vcr/val.py,8,"b'import _init_paths\nimport os\nimport argparse\nfrom copy import deepcopy\n\nimport jsonlines\nfrom tqdm import trange\nimport torch\nimport numpy as np\n\nfrom common.utils.load import smart_load_model_state_dict\nfrom common.trainer import to_cuda\nfrom common.metrics.composite_eval_metric import CompositeEvalMetric\nfrom common.metrics.vcr_metrics import JointAccuracy\nfrom vcr.data.build import make_dataloader\nfrom vcr.function.config import config, update_config\nfrom vcr.modules import *\nfrom vcr.function.val import joint_validation\n\ntry:\n    from apex import amp\n    from apex.parallel import DistributedDataParallel as Apex_DDP\nexcept ImportError:\n    raise ImportError(""Please install apex from https://www.github.com/nvidia/apex if you want to use fp16."")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\'Do Validation of Cognition Network\')\n    parser.add_argument(\'--a-cfg\', type=str, help=\'path to answer net config yaml\')\n    parser.add_argument(\'--r-cfg\', type=str, help=\'path to rationale net config yaml\')\n    parser.add_argument(\'--a-ckpt\', type=str, help=\'path to checkpoint of answer net\')\n    parser.add_argument(\'--r-ckpt\', type=str, help=\'path to checkpoint of rationale net\')\n    parser.add_argument(\'--a-bs\', type=int)\n    parser.add_argument(\'--r-bs\', type=int)\n    parser.add_argument(\'--gpus\', type=int, nargs=\'+\', default=[0])\n    parser.add_argument(\'--result-path\', type=str, default=\'./vcr_val_results\')\n    parser.add_argument(\'--result-name\', type=str, default=\'vl-bert\')\n    parser.add_argument(\'--use-cache\', default=False, action=\'store_true\')\n    parser.add_argument(\'--annot\', type=str, default=\'./data/vcr/val.jsonl\')\n    parser.add_argument(\'--cudnn-off\', default=False, action=\'store_true\')\n    parser.add_argument(\'--fp16\', default=False, action=\'store_true\')\n\n    args = parser.parse_args()\n    a_config = r_config = None\n    reset_config = deepcopy(config)\n    if args.a_cfg is not None:\n        a_config = config\n        if reset_config is not None:\n            a_config.update(deepcopy(reset_config))\n        if args.a_cfg is not None:\n            update_config(args.a_cfg)\n        a_config = deepcopy(a_config)\n    if args.r_cfg is not None:\n        r_config = config\n        if reset_config is not None:\n            r_config.update(deepcopy(reset_config))\n        if args.r_cfg is not None:\n            update_config(args.r_cfg)\n        r_config = deepcopy(r_config)\n    if args.a_bs is not None:\n        a_config.VAL.BATCH_IMAGES = args.a_bs\n    if args.r_bs is not None:\n        r_config.VAL.BATCH_IMAGES = args.r_bs\n\n    return args, a_config, r_config\n\n\n@torch.no_grad()\ndef main():\n    args, a_config, r_config = parse_args()\n\n    if args.cudnn_off:\n        torch.backends.cudnn.enabled = False\n\n    with jsonlines.open(args.annot) as reader:\n        gts = [(obj[\'answer_label\'], obj[\'rationale_label\']) for obj in reader]\n    a_gt = np.array([gt[0] for gt in gts], dtype=np.int64)\n    r_gt = np.array([gt[1] for gt in gts], dtype=np.int64)\n\n    # cache\n    a_cache_fn = os.path.join(args.result_path, \'{}_a_pred.npy\'.format(args.result_name))\n    r_cache_fn = os.path.join(args.result_path, \'{}_r_pred.npy\'.format(args.result_name))\n    a_pred = r_pred = None\n    if not os.path.exists(args.result_path):\n        os.makedirs(args.result_path)\n    if args.use_cache:\n        if os.path.exists(a_cache_fn):\n            print(""Load cached predictions from {}..."".format(a_cache_fn))\n            a_pred = np.load(a_cache_fn)\n        if os.path.exists(r_cache_fn):\n            print(""Load cached predictions from {}..."".format(r_cache_fn))\n            r_pred = np.load(r_cache_fn)\n    else:\n        if a_config is not None and args.a_ckpt is not None:\n\n            print(""Build model and dataloader for Q->A..."")\n\n            # get model\n            device_ids = args.gpus\n            # os.environ[\'CUDA_VISIBLE_DEVICES\'] = \',\'.join([str(k) for k in args.gpus])\n            a_config.GPUS = \',\'.join([str(k) for k in args.gpus])\n            answer_model = eval(a_config.MODULE)(a_config)\n            if len(device_ids) > 1:\n                answer_model = torch.nn.DataParallel(answer_model, device_ids=device_ids).cuda()\n            else:\n                torch.cuda.set_device(device_ids[0])\n                answer_model = answer_model.cuda()\n\n            if args.fp16:\n                [answer_model] = amp.initialize([answer_model],\n                                                opt_level=\'O2\',\n                                                keep_batchnorm_fp32=False)\n\n            a_ckpt = torch.load(args.a_ckpt, map_location=lambda storage, loc: storage)\n            smart_load_model_state_dict(answer_model, a_ckpt[\'state_dict\'])\n            answer_model.eval()\n\n            # get data loader\n            a_config.DATASET.TASK = \'Q2A\'\n            a_config.VAL.SHUFFLE = False\n            answer_loader = make_dataloader(a_config, mode=\'val\', distributed=False)\n            label_index_in_batch = a_config.DATASET.LABEL_INDEX_IN_BATCH\n\n            print(""Inference Q->A..."")\n\n            # inference\n            n_batch = len(answer_loader)\n            a_pred = np.zeros((len(gts), 4), dtype=np.float)\n            i_sample = 0\n            for nbatch, a_batch in zip(trange(len(answer_loader)), answer_loader):\n            # for a_batch in answer_loader:\n                a_batch = to_cuda(a_batch)\n                a_batch = [a_batch[i] for i in range(len(a_batch)) if i != label_index_in_batch % len(a_batch)]\n                a_out = answer_model(*a_batch)\n                a_batch_pred = a_out[\'label_logits\']\n                batch_size = a_batch_pred.shape[0]\n                if a_batch_pred.dim() == 2:\n                    a_pred[i_sample:(i_sample + batch_size)] = a_batch_pred.detach().cpu().numpy().astype(np.float,\n                                                                                                          copy=False)\n                elif a_batch_pred.dim() == 1:\n                    assert a_batch_pred.shape[0] % 4 == 0\n                    a_batch_pred = a_batch_pred.view((-1, 4))\n                    a_pred[int(i_sample / 4):int((i_sample + batch_size) / 4)] \\\n                        = a_batch_pred.float().detach().cpu().numpy().astype(np.float, copy=False)\n                else:\n                    raise ValueError(""Invalid"")\n                i_sample += batch_size\n                # print(""inference {}/{}"".format(i_sample, len(answer_loader.dataset)))\n            np.save(a_cache_fn, a_pred)\n\n        if r_config is not None and args.r_ckpt is not None:\n\n            print(""Build model and dataloader for QA->R..."")\n\n            # get model\n            device_ids = args.gpus\n            # os.environ[\'CUDA_VISIBLE_DEVICES\'] = \',\'.join([str(k) for k in args.gpus])\n            r_config.GPUS = \',\'.join([str(k) for k in args.gpus])\n            rationale_model = eval(r_config.MODULE)(r_config)\n            if len(device_ids) > 1:\n                rationale_model = torch.nn.DataParallel(rationale_model, device_ids=device_ids).cuda()\n            else:\n                torch.cuda.set_device(device_ids[0])\n                rationale_model = rationale_model.cuda()\n\n            if args.fp16:\n                [rationale_model] = amp.initialize([rationale_model],\n                                                   opt_level=\'O2\',\n                                                   keep_batchnorm_fp32=False)\n\n            r_ckpt = torch.load(args.r_ckpt, map_location=lambda storage, loc: storage)\n            smart_load_model_state_dict(rationale_model, r_ckpt[\'state_dict\'])\n            rationale_model.eval()\n\n            # get data loader\n            r_config.DATASET.TASK = \'QA2R\'\n            r_config.VAL.SHUFFLE = False\n            rationale_loader = make_dataloader(r_config, mode=\'val\', distributed=False)\n            label_index_in_batch = r_config.DATASET.LABEL_INDEX_IN_BATCH\n\n            print(""Inference QA->R..."")\n\n            # inference\n            n_batch = len(rationale_loader)\n            r_pred = np.zeros((len(rationale_loader.dataset), 4), dtype=np.float)\n            i_sample = 0\n            for nbatch, r_batch in zip(trange(len(rationale_loader)), rationale_loader):\n            # for r_batch in rationale_loader:\n                r_batch = to_cuda(r_batch)\n                r_batch = [r_batch[i] for i in range(len(r_batch)) if i != label_index_in_batch % len(r_batch)]\n                r_out = rationale_model(*r_batch)\n                r_batch_pred = r_out[\'label_logits\']\n                batch_size = r_batch_pred.shape[0]\n                r_pred[i_sample:(i_sample + batch_size)] =\\\n                    r_batch_pred.float().detach().cpu().numpy().astype(np.float, copy=False)\n                i_sample += batch_size\n                # print(""inference {}/{}"".format(i_sample, len(rationale_loader.dataset)))\n            np.save(r_cache_fn, r_pred)\n\n    # evaluate\n    print(""Evaluate..."")\n    if a_pred is not None:\n        acc_a = (a_pred.argmax(1) == a_gt).sum() * 1.0 / a_gt.size\n        print(""Q->A\\t{:.1f}"".format(acc_a * 100.0))\n    if r_pred is not None:\n        acc_r = (r_pred.argmax(1) == r_gt).sum() * 1.0 / r_gt.size\n        print(""QA->R\\t{:.1f}"".format(acc_r * 100.0))\n    if a_pred is not None and r_pred is not None:\n        acc_joint = ((a_pred.argmax(1) == a_gt) * (r_pred.argmax(1) == r_gt)).sum() * 1.0 / a_gt.size\n        print(""Q->AR\\t{:.1f}"".format(acc_joint * 100.0))\n\n\nif __name__ == \'__main__\':\n    main()\n\n\n'"
vqa/train_end2end.py,1,"b""import _init_paths\nimport os\nimport argparse\nimport torch\nimport subprocess\n\nfrom vqa.function.config import config, update_config\nfrom vqa.function.train import train_net\nfrom vqa.function.test import test_net\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser('Train Cognition Network')\n    parser.add_argument('--cfg', type=str, help='path to config file')\n    parser.add_argument('--model-dir', type=str, help='root path to store checkpoint')\n    parser.add_argument('--log-dir', type=str, help='tensorboard log dir')\n    parser.add_argument('--dist', help='whether to use distributed training', default=False, action='store_true')\n    parser.add_argument('--slurm', help='whether this is a slurm job', default=False, action='store_true')\n    parser.add_argument('--do-test', help='whether to generate csv result on test set',\n                        default=False, action='store_true')\n    parser.add_argument('--cudnn-off', help='disable cudnn', default=False, action='store_true')\n\n    # easy test pretrain model\n    parser.add_argument('--partial-pretrain', type=str)\n\n    args = parser.parse_args()\n\n    if args.cfg is not None:\n        update_config(args.cfg)\n    if args.model_dir is not None:\n        config.OUTPUT_PATH = os.path.join(args.model_dir, config.OUTPUT_PATH)\n\n    if args.partial_pretrain is not None:\n        config.NETWORK.PARTIAL_PRETRAIN = args.partial_pretrain\n\n    if args.slurm:\n        proc_id = int(os.environ['SLURM_PROCID'])\n        ntasks = int(os.environ['SLURM_NTASKS'])\n        node_list = os.environ['SLURM_NODELIST']\n        num_gpus = torch.cuda.device_count()\n        addr = subprocess.getoutput(\n            'scontrol show hostname {} | head -n1'.format(node_list))\n        os.environ['MASTER_PORT'] = str(29500)\n        os.environ['MASTER_ADDR'] = addr\n        os.environ['WORLD_SIZE'] = str(ntasks)\n        os.environ['RANK'] = str(proc_id)\n        os.environ['LOCAL_RANK'] = str(proc_id % num_gpus)\n\n    return args, config\n\n\ndef main():\n    args, config = parse_args()\n    rank, model = train_net(args, config)\n    if args.do_test and (rank is None or rank == 0):\n        test_net(args, config)\n\n\nif __name__ == '__main__':\n    main()\n\n\n"""
common/metrics/composite_eval_metric.py,1,"b'import numpy as np\nfrom .eval_metric import EvalMetric\nimport torch\n\nclass CompositeEvalMetric(EvalMetric):\n    """"""Manages multiple evaluation metrics.\n    Args:\n        metrics (list of EvalMetric): List of child metrics.\n        name (str): Name of this metric instance for display.\n    """"""\n\n    def __init__(self, metrics=None, name=\'composite\'):\n        super(CompositeEvalMetric, self).__init__(name)\n        if metrics is None:\n            metrics = []\n        self.metrics = metrics\n\n    def add(self, metric):\n        """"""Adds a child metric.\n        Args:\n            metric (EvalMetric): A metric instance.\n        """"""\n        self.metrics.append(metric)\n\n    def get_metric(self, index):\n        """"""Returns a child metric.\n        Args:\n            index (int): Index of child metric in the list of metrics.\n        """"""\n        try:\n            return self.metrics[index]\n        except IndexError:\n            return ValueError(""Metric index {} is out of range 0 and {}"".format(\n                index, len(self.metrics)))\n\n    def update(self, outputs):\n        """"""Updates the internal evaluation result.\n        Args:\n            labels (dict of `NDArray`): The labels of the data.\n            preds (dict of `NDArray`): Predicted values.\n        """"""\n        for metric in self.metrics:\n            metric.update(outputs)\n\n    def reset(self):\n        """"""Resets the internal evaluation result to initial state.""""""\n        try:\n            for metric in self.metrics:\n                metric.reset()\n        except AttributeError:\n            pass\n\n    def get(self):\n        """"""Returns the current evaluation result.\n        Returns:\n            names (list of str): Name of the metrics.\n            values (list of float): Value of the evaluations.\n        """"""\n        names = []\n        values = []\n        for metric in self.metrics:\n            name, value = metric.get()\n            if isinstance(name, str):\n                name = [name]\n            if isinstance(value, (float, int, np.generic,torch.Tensor)):\n                value = [value]\n            names.extend(name)\n            values.extend(value)\n        return names, values\n'"
common/metrics/eval_metric.py,3,"b'import torch\nimport torch.distributed as distributed\n\n\nclass EvalMetric(object):\n    """"""Base class for all evaluation metrics.\n    .. note::\n        This is a base class that provides common metric interfaces.\n        One should not use this class directly, but instead create new metric\n        classes that extend it.\n    Args\n        name (str): Name of this metric instance for display.\n    """"""\n\n    def __init__(self, name, allreduce=False, num_replicas=1, **kwargs):\n        self.name = str(name)\n        self.allreduce=allreduce\n        self.num_replicas = num_replicas\n        self._kwargs = kwargs\n        self.reset()\n\n    def __str__(self):\n        return ""EvalMetric: {}"".format(dict(self.get_name_value()))\n\n    def update(self, outputs):\n        """"""Updates the internal evaluation result.\n        Args\n            labels (list of `NDArray`): The labels of the data.\n            preds (list of `NDArray`): Predicted values.\n        """"""\n        raise NotImplementedError()\n\n    def reset(self):\n        """"""Resets the internal evaluation result to initial state.""""""\n        self.num_inst = torch.tensor(0.)\n        self.sum_metric = torch.tensor(0.)\n\n    def get(self):\n        """"""Returns the current evaluation result.\n        Returns:\n            names (list of str): Name of the metrics.\n            values (list of float): Value of the evaluations.\n        """"""\n        if self.num_inst.item() == 0:\n            return (self.name, float(\'nan\'))\n        else:\n            if self.allreduce:\n                num_inst = self.num_inst.clone().cuda()\n                sum_metric = self.sum_metric.clone().cuda()\n                distributed.all_reduce(num_inst, op=distributed.ReduceOp.SUM)\n                distributed.all_reduce(sum_metric, op=distributed.ReduceOp.SUM)\n                metric_tensor = (sum_metric / num_inst).detach().cpu()\n            else:\n                metric_tensor = (self.sum_metric / self.num_inst).detach().cpu()\n\n            return (self.name, metric_tensor.item())\n\n    def get_name_value(self):\n        """"""Returns zipped name and value pairs.\n        Returns\n            A (list of tuples): (name, value) tuple list.\n        """"""\n        name, value = self.get()\n        if not isinstance(name, list):\n            name = [name]\n        if not isinstance(value, list):\n            value = [value]\n        return list(zip(name, value))\n'"
common/metrics/pretrain_metrics.py,6,"b""import torch\nfrom .eval_metric import EvalMetric\n\n\nclass LossLogger(EvalMetric):\n    def __init__(self, output_name, display_name=None,\n                 allreduce=False, num_replicas=1):\n        self.output_name = output_name\n        if display_name is None:\n            display_name = output_name\n        super(LossLogger, self).__init__(display_name, allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            if self.output_name in outputs:\n                self.sum_metric += float(outputs[self.output_name].mean().item())\n            self.num_inst += 1\n\n\nclass RelationshipAccuracy(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(RelationshipAccuracy, self).__init__('RelAcc', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            logits = outputs['relationship_logits']\n            label = outputs['relationship_label']\n            self.sum_metric += float((logits.argmax(dim=1) == label).sum().item())\n            self.num_inst += logits.shape[0]\n\n\nclass MLMAccuracy(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(MLMAccuracy, self).__init__('MLMAcc', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            logits = outputs['mlm_logits']\n            label = outputs['mlm_label']\n            keep = (label != -1)\n            if keep.sum() > 0:\n                self.sum_metric += float((logits[keep].argmax(dim=1) == label[keep]).sum().item())\n                self.num_inst += keep.sum().item()\n\n\nclass MLMAccuracyWVC(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(MLMAccuracyWVC, self).__init__('MLMAccWVC', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            logits = outputs['mlm_logits_wvc']\n            label = outputs['mlm_label_wvc']\n            keep = (label != -1)\n            if keep.sum() > 0:\n                self.sum_metric += float((logits[keep].argmax(dim=1) == label[keep]).sum().item())\n                self.num_inst += keep.sum().item()\n\n\nclass MLMAccuracyAUX(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(MLMAccuracyAUX, self).__init__('MLMAccAUX', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            logits = outputs['mlm_logits_aux']\n            label = outputs['mlm_label_aux']\n            keep = (label != -1)\n            if keep.sum() > 0:\n                self.sum_metric += float((logits[keep].argmax(dim=1) == label[keep]).sum().item())\n                self.num_inst += keep.sum().item()\n\n\nclass MVRCAccuracy(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(MVRCAccuracy, self).__init__('MVRCAccuracy', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            logits = outputs['mvrc_logits']\n            label = outputs['mvrc_label']\n            keep = (label.sum(2) - 1.0).abs() < 0.1\n            if keep.sum() > 0:\n                self.sum_metric += float((logits[keep].argmax(dim=1) == label[keep].argmax(dim=1)).sum().item())\n                self.num_inst += keep.sum().item()\n\n\n\n\n"""
common/metrics/refcoco_metrics.py,6,"b""import torch\nfrom .eval_metric import EvalMetric\n\n\nclass LossLogger(EvalMetric):\n    def __init__(self, output_name, display_name=None,\n                 allreduce=False, num_replicas=1):\n        self.output_name = output_name\n        if display_name is None:\n            display_name = output_name\n        super(LossLogger, self).__init__(display_name, allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            if self.output_name in outputs:\n                self.sum_metric += float(outputs[self.output_name].mean().item())\n            self.num_inst += 1\n\n\nclass RefAccuracy(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(RefAccuracy, self).__init__('RefAcc', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            cls_logits = outputs['label_logits']\n            label = outputs['label']\n            bs, _ = cls_logits.shape\n            batch_inds = torch.arange(bs, device=cls_logits.device)\n            self.sum_metric += float((label[batch_inds, cls_logits.argmax(1)] > 0.5).sum().item())\n            self.num_inst += cls_logits.shape[0]\n\n\nclass ClsAccuracy(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(ClsAccuracy, self).__init__('ClsAcc', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            cls_logits = outputs['label_logits']\n            cls_pred = (cls_logits > 0).long()\n            label = outputs['label'].long()\n            keep = (label >= 0)\n            self.sum_metric += float((cls_pred[keep] == label[keep]).sum().item())\n            self.num_inst += keep.sum().item()\n\n\nclass ClsPosAccuracy(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(ClsPosAccuracy, self).__init__('ClsPosAcc', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            cls_logits = outputs['label_logits']\n            cls_pred = (cls_logits > 0).long()\n            label = outputs['label'].long()\n            keep = (label == 1)\n            self.sum_metric += float((cls_pred[keep] == label[keep]).sum().item())\n            self.num_inst += keep.sum().item()\n\n\nclass ClsPosFraction(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(ClsPosFraction, self).__init__('ClsPosFrac', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            label = outputs['label'].long()\n            num_pos = (label == 1).sum().item()\n            num_valid = (label >= 0).sum().item()\n            self.sum_metric += float(num_pos)\n            self.num_inst += float(num_valid)\n\n\n\n\n\n\n"""
common/metrics/vcr_metrics.py,5,"b""import torch\nfrom .eval_metric import EvalMetric\n\n\nclass LossLogger(EvalMetric):\n    def __init__(self, output_name, display_name=None,\n                 allreduce=False, num_replicas=1):\n        self.output_name = output_name\n        if display_name is None:\n            display_name = output_name\n        super(LossLogger, self).__init__(display_name, allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            if self.output_name in outputs:\n                self.sum_metric += float(outputs[self.output_name].mean().item())\n            self.num_inst += 1\n\n\nclass Accuracy(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(Accuracy, self).__init__('Acc', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            _filter = outputs['label'] != -1\n            cls_logits = outputs['label_logits'][_filter]\n            label = outputs['label'][_filter]\n            if cls_logits.dim() == 1:\n                cls_logits = cls_logits.view((-1, 4))\n                label = label.view((-1, 4)).argmax(1)\n            self.sum_metric += float((cls_logits.argmax(dim=1) == label).sum().item())\n            self.num_inst += cls_logits.shape[0]\n\n\nclass AnsLoss(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(AnsLoss, self).__init__('AnsLoss', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            self.sum_metric += float(outputs['ans_loss'].mean().item())\n            self.num_inst += 1\n\n\nclass CNNRegLoss(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(CNNRegLoss, self).__init__('CNNRegLoss', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            if 'cnn_regularization_loss' in outputs:\n                self.sum_metric += float(outputs['cnn_regularization_loss'].mean().item())\n            self.num_inst += 1\n\n\nclass PositiveFraction(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(PositiveFraction, self).__init__('PosFraction', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            self.sum_metric += float(outputs['positive_fraction'].mean().item())\n            self.num_inst += 1\n\n\nclass JointAccuracy(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(JointAccuracy, self).__init__('JointAcc', allreduce, num_replicas)\n\n    def update(self, outputs):\n        a_cls_logits = outputs['answer_label_logits']\n        a_label = outputs['answer_label']\n        r_cls_logits = outputs['rationale_label_logits']\n        r_label = outputs['rationale_label']\n        self.sum_metric += float(((a_cls_logits.argmax(dim=1) == a_label)\n                                  & (r_cls_logits.argmax(dim=1) == r_label)).sum().item())\n        self.num_inst += a_cls_logits.shape[0]\n\n\n\n"""
common/metrics/vqa_metrics.py,3,"b""import torch\nfrom .eval_metric import EvalMetric\n\n\nclass LossLogger(EvalMetric):\n    def __init__(self, output_name, display_name=None,\n                 allreduce=False, num_replicas=1):\n        self.output_name = output_name\n        if display_name is None:\n            display_name = output_name\n        super(LossLogger, self).__init__(display_name, allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            if self.output_name in outputs:\n                self.sum_metric += float(outputs[self.output_name].mean().item())\n            self.num_inst += 1\n\n\nclass SoftAccuracy(EvalMetric):\n    def __init__(self, allreduce=False, num_replicas=1):\n        super(SoftAccuracy, self).__init__('SoftAcc', allreduce, num_replicas)\n\n    def update(self, outputs):\n        with torch.no_grad():\n            cls_logits = outputs['label_logits']\n            label = outputs['label']\n            bs, num_classes = cls_logits.shape\n            batch_inds = torch.arange(bs, device=cls_logits.device)\n            self.sum_metric += float(label[batch_inds, cls_logits.argmax(1)].sum().item())\n            self.num_inst += cls_logits.shape[0]\n\n\n\n\n"""
common/nlp/bert_encoder_wrapper.py,2,"b'import torch\nimport torch.nn as nn\nfrom external.pytorch_pretrained_bert.modeling import BertEncoder, BertLayerNorm\n\n\nclass BertEncoderWrapper(nn.Module):\n    def __init__(self, bert_config, input_size, output_all_encoded_layers=False):\n        super(BertEncoderWrapper, self).__init__()\n        self.bert_config = bert_config\n        self.output_all_encoded_layers = output_all_encoded_layers\n        self.input_transform = nn.Linear(input_size, bert_config.hidden_size)\n        self.with_position_embeddings = False if \'with_position_embeddings\' not in bert_config \\\n            else bert_config.with_position_embeddings\n        if self.with_position_embeddings:\n            self.position_embedding = nn.Embedding(bert_config.max_position_embeddings, bert_config.hidden_size)\n            self.LayerNorm = BertLayerNorm(bert_config.hidden_size, eps=1e-12)\n            self.dropout = nn.Dropout(bert_config.hidden_dropout_prob)\n        self.bert_encoder = BertEncoder(bert_config)\n\n        self.apply(self.init_bert_weights)\n\n    def init_bert_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.bert_config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def get_output_dim(self):\n        return self.bert_config.hidden_size\n\n    def forward(self, inputs, mask):\n        inputs = self.input_transform(inputs)\n        if self.with_position_embeddings:\n            seq_length = inputs.size(1)\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=inputs.device)\n            position_ids = position_ids.unsqueeze(0).expand((inputs.shape[0], inputs.shape[1]))\n            position_embeddings = self.position_embedding(position_ids)\n            inputs = inputs + position_embeddings\n            inputs = self.LayerNorm(inputs)\n            inputs = self.dropout(inputs)\n\n        extended_attention_mask = mask.unsqueeze(1).unsqueeze(2)\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        output = self.bert_encoder(inputs,\n                                   extended_attention_mask,\n                                   output_all_encoded_layers=self.output_all_encoded_layers)\n        if not self.output_all_encoded_layers:\n            output = output[0]\n        return output\n\n'"
common/nlp/encoder_base.py,34,"b'from typing import Tuple, Union, Optional, Callable\nimport torch\nfrom torch.nn.utils.rnn import pack_padded_sequence, PackedSequence\n\n# We have two types here for the state, because storing the state in something\n# which is Iterable (like a tuple, below), is helpful for internal manipulation\n# - however, the states are consumed as either Tensors or a Tuple of Tensors, so\n# returning them in this format is unhelpful.\nRnnState = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]  # pylint: disable=invalid-name\nRnnStateStorage = Tuple[torch.Tensor, ...]  # pylint: disable=invalid-name\n\n\ndef get_lengths_from_binary_sequence_mask(mask: torch.Tensor):\n    """"""\n    Compute sequence lengths for each batch element in a tensor using a\n    binary mask.\n\n    Parameters\n    ----------\n    mask : torch.Tensor, required.\n        A 2D binary mask of shape (batch_size, sequence_length) to\n        calculate the per-batch sequence lengths from.\n\n    Returns\n    -------\n    A torch.LongTensor of shape (batch_size,) representing the lengths\n    of the sequences in the batch.\n    """"""\n    return mask.long().sum(-1)\n\n\ndef sort_batch_by_length(tensor: torch.Tensor, sequence_lengths: torch.Tensor):\n    """"""\n    Sort a batch first tensor by some specified lengths.\n\n    Parameters\n    ----------\n    tensor : torch.FloatTensor, required.\n        A batch first Pytorch tensor.\n    sequence_lengths : torch.LongTensor, required.\n        A tensor representing the lengths of some dimension of the tensor which\n        we want to sort by.\n\n    Returns\n    -------\n    sorted_tensor : torch.FloatTensor\n        The original tensor sorted along the batch dimension with respect to sequence_lengths.\n    sorted_sequence_lengths : torch.LongTensor\n        The original sequence_lengths sorted by decreasing size.\n    restoration_indices : torch.LongTensor\n        Indices into the sorted_tensor such that\n        ``sorted_tensor.index_select(0, restoration_indices) == original_tensor``\n    permuation_index : torch.LongTensor\n        The indices used to sort the tensor. This is useful if you want to sort many\n        tensors using the same ordering.\n    """"""\n\n    if not isinstance(tensor, torch.Tensor) or not isinstance(sequence_lengths, torch.Tensor):\n        raise Exception(""Both the tensor and sequence lengths must be torch.Tensors."")\n\n    sorted_sequence_lengths, permutation_index = sequence_lengths.sort(0, descending=True)\n    sorted_tensor = tensor.index_select(0, permutation_index)\n\n    index_range = torch.arange(0, len(sequence_lengths), device=sequence_lengths.device)\n    # This is the equivalent of zipping with index, sorting by the original\n    # sequence lengths and returning the now sorted indices.\n    _, reverse_mapping = permutation_index.sort(0, descending=False)\n    restoration_indices = index_range.index_select(0, reverse_mapping)\n    return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index\n\n\nclass _EncoderBase(torch.nn.Module):\n    # pylint: disable=abstract-method\n    """"""\n    This abstract class serves as a base for the 3 ``Encoder`` abstractions in AllenNLP.\n    - :class:`~allennlp.modules.seq2seq_encoders.Seq2SeqEncoders`\n    - :class:`~allennlp.modules.seq2vec_encoders.Seq2VecEncoders`\n\n    Additionally, this class provides functionality for sorting sequences by length\n    so they can be consumed by Pytorch RNN classes, which require their inputs to be\n    sorted by length. Finally, it also provides optional statefulness to all of it\'s\n    subclasses by allowing the caching and retrieving of the hidden states of RNNs.\n    """"""\n\n    def __init__(self, stateful: bool = False) -> None:\n        super(_EncoderBase, self).__init__()\n        self.stateful = stateful\n        self._states: Optional[RnnStateStorage] = None\n\n    def sort_and_run_forward(self,\n                             module: Callable[[PackedSequence, Optional[RnnState]],\n                                              Tuple[Union[PackedSequence, torch.Tensor], RnnState]],\n                             inputs: torch.Tensor,\n                             mask: torch.Tensor,\n                             hidden_state: Optional[RnnState] = None):\n        """"""\n        This function exists because Pytorch RNNs require that their inputs be sorted\n        before being passed as input. As all of our Seq2xxxEncoders use this functionality,\n        it is provided in a base class. This method can be called on any module which\n        takes as input a ``PackedSequence`` and some ``hidden_state``, which can either be a\n        tuple of tensors or a tensor.\n\n        As all of our Seq2xxxEncoders have different return types, we return `sorted`\n        outputs from the module, which is called directly. Additionally, we return the\n        indices into the batch dimension required to restore the tensor to it\'s correct,\n        unsorted order and the number of valid batch elements (i.e the number of elements\n        in the batch which are not completely masked). This un-sorting and re-padding\n        of the module outputs is left to the subclasses because their outputs have different\n        types and handling them smoothly here is difficult.\n\n        Parameters\n        ----------\n        module : ``Callable[[PackedSequence, Optional[RnnState]],\n                            Tuple[Union[PackedSequence, torch.Tensor], RnnState]]``, required.\n            A function to run on the inputs. In most cases, this is a ``torch.nn.Module``.\n        inputs : ``torch.Tensor``, required.\n            A tensor of shape ``(batch_size, sequence_length, embedding_size)`` representing\n            the inputs to the Encoder.\n        mask : ``torch.Tensor``, required.\n            A tensor of shape ``(batch_size, sequence_length)``, representing masked and\n            non-masked elements of the sequence for each element in the batch.\n        hidden_state : ``Optional[RnnState]``, (default = None).\n            A single tensor of shape (num_layers, batch_size, hidden_size) representing the\n            state of an RNN with or a tuple of\n            tensors of shapes (num_layers, batch_size, hidden_size) and\n            (num_layers, batch_size, memory_size), representing the hidden state and memory\n            state of an LSTM-like RNN.\n\n        Returns\n        -------\n        module_output : ``Union[torch.Tensor, PackedSequence]``.\n            A Tensor or PackedSequence representing the output of the Pytorch Module.\n            The batch size dimension will be equal to ``num_valid``, as sequences of zero\n            length are clipped off before the module is called, as Pytorch cannot handle\n            zero length sequences.\n        final_states : ``Optional[RnnState]``\n            A Tensor representing the hidden state of the Pytorch Module. This can either\n            be a single tensor of shape (num_layers, num_valid, hidden_size), for instance in\n            the case of a GRU, or a tuple of tensors, such as those required for an LSTM.\n        restoration_indices : ``torch.LongTensor``\n            A tensor of shape ``(batch_size,)``, describing the re-indexing required to transform\n            the outputs back to their original batch order.\n        """"""\n        # In some circumstances you may have sequences of zero length. ``pack_padded_sequence``\n        # requires all sequence lengths to be > 0, so remove sequences of zero length before\n        # calling self._module, then fill with zeros.\n\n        # First count how many sequences are empty.\n        batch_size = mask.size(0)\n        num_valid = torch.sum(mask[:, 0]).int().item()\n\n        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)\n        sorted_inputs, sorted_sequence_lengths, restoration_indices, sorting_indices = \\\n            sort_batch_by_length(inputs, sequence_lengths)\n\n        # Now create a PackedSequence with only the non-empty, sorted sequences.\n        packed_sequence_input = pack_padded_sequence(sorted_inputs[:num_valid, :, :],\n                                                     sorted_sequence_lengths[:num_valid].data.tolist(),\n                                                     batch_first=True)\n        # Prepare the initial states.\n        if not self.stateful:\n            if hidden_state is None:\n                initial_states = hidden_state\n            elif isinstance(hidden_state, tuple):\n                initial_states = [state.index_select(1, sorting_indices)[:, :num_valid, :].contiguous()\n                                  for state in hidden_state]\n            else:\n                initial_states = hidden_state.index_select(1, sorting_indices)[:, :num_valid, :].contiguous()\n\n        else:\n            initial_states = self._get_initial_states(batch_size, num_valid, sorting_indices)\n\n        # Actually call the module on the sorted PackedSequence.\n        module_output, final_states = module(packed_sequence_input, initial_states)\n\n        return module_output, final_states, restoration_indices\n\n    def _get_initial_states(self,\n                            batch_size: int,\n                            num_valid: int,\n                            sorting_indices: torch.LongTensor) -> Optional[RnnState]:\n        """"""\n        Returns an initial state for use in an RNN. Additionally, this method handles\n        the batch size changing across calls by mutating the state to append initial states\n        for new elements in the batch. Finally, it also handles sorting the states\n        with respect to the sequence lengths of elements in the batch and removing rows\n        which are completely padded. Importantly, this `mutates` the state if the\n        current batch size is larger than when it was previously called.\n\n        Parameters\n        ----------\n        batch_size : ``int``, required.\n            The batch size can change size across calls to stateful RNNs, so we need\n            to know if we need to expand or shrink the states before returning them.\n            Expanded states will be set to zero.\n        num_valid : ``int``, required.\n            The batch may contain completely padded sequences which get removed before\n            the sequence is passed through the encoder. We also need to clip these off\n            of the state too.\n        sorting_indices ``torch.LongTensor``, required.\n            Pytorch RNNs take sequences sorted by length. When we return the states to be\n            used for a given call to ``module.forward``, we need the states to match up to\n            the sorted sequences, so before returning them, we sort the states using the\n            same indices used to sort the sequences.\n\n        Returns\n        -------\n        This method has a complex return type because it has to deal with the first time it\n        is called, when it has no state, and the fact that types of RNN have heterogeneous\n        states.\n\n        If it is the first time the module has been called, it returns ``None``, regardless\n        of the type of the ``Module``.\n\n        Otherwise, for LSTMs, it returns a tuple of ``torch.Tensors`` with shape\n        ``(num_layers, num_valid, state_size)`` and ``(num_layers, num_valid, memory_size)``\n        respectively, or for GRUs, it returns a single ``torch.Tensor`` of shape\n        ``(num_layers, num_valid, state_size)``.\n        """"""\n        # We don\'t know the state sizes the first time calling forward,\n        # so we let the module define what it\'s initial hidden state looks like.\n        if self._states is None:\n            return None\n\n        # Otherwise, we have some previous states.\n        if batch_size > self._states[0].size(1):\n            # This batch is larger than the all previous states.\n            # If so, resize the states.\n            num_states_to_concat = batch_size - self._states[0].size(1)\n            resized_states = []\n            # state has shape (num_layers, batch_size, hidden_size)\n            for state in self._states:\n                # This _must_ be inside the loop because some\n                # RNNs have states with different last dimension sizes.\n                zeros = state.new_zeros(state.size(0),\n                                        num_states_to_concat,\n                                        state.size(2))\n                resized_states.append(torch.cat([state, zeros], 1))\n            self._states = tuple(resized_states)\n            correctly_shaped_states = self._states\n\n        elif batch_size < self._states[0].size(1):\n            # This batch is smaller than the previous one.\n            correctly_shaped_states = tuple(state[:, :batch_size, :] for state in self._states)\n        else:\n            correctly_shaped_states = self._states\n\n        # At this point, our states are of shape (num_layers, batch_size, hidden_size).\n        # However, the encoder uses sorted sequences and additionally removes elements\n        # of the batch which are fully padded. We need the states to match up to these\n        # sorted and filtered sequences, so we do that in the next two blocks before\n        # returning the state/s.\n        if len(self._states) == 1:\n            # GRUs only have a single state. This `unpacks` it from the\n            # tuple and returns the tensor directly.\n            correctly_shaped_state = correctly_shaped_states[0]\n            sorted_state = correctly_shaped_state.index_select(1, sorting_indices)\n            return sorted_state[:, :num_valid, :]\n        else:\n            # LSTMs have a state tuple of (state, memory).\n            sorted_states = [state.index_select(1, sorting_indices)\n                             for state in correctly_shaped_states]\n            return tuple(state[:, :num_valid, :] for state in sorted_states)\n\n    def _update_states(self,\n                       final_states: RnnStateStorage,\n                       restoration_indices: torch.LongTensor) -> None:\n        """"""\n        After the RNN has run forward, the states need to be updated.\n        This method just sets the state to the updated new state, performing\n        several pieces of book-keeping along the way - namely, unsorting the\n        states and ensuring that the states of completely padded sequences are\n        not updated. Finally, it also detaches the state variable from the\n        computational graph, such that the graph can be garbage collected after\n        each batch iteration.\n\n        Parameters\n        ----------\n        final_states : ``RnnStateStorage``, required.\n            The hidden states returned as output from the RNN.\n        restoration_indices : ``torch.LongTensor``, required.\n            The indices that invert the sorting used in ``sort_and_run_forward``\n            to order the states with respect to the lengths of the sequences in\n            the batch.\n        """"""\n        # TODO(Mark): seems weird to sort here, but append zeros in the subclasses.\n        # which way around is best?\n        new_unsorted_states = [state.index_select(1, restoration_indices)\n                               for state in final_states]\n\n        if self._states is None:\n            # We don\'t already have states, so just set the\n            # ones we receive to be the current state.\n            self._states = tuple(state.data for state in new_unsorted_states)\n        else:\n            # Now we\'ve sorted the states back so that they correspond to the original\n            # indices, we need to figure out what states we need to update, because if we\n            # didn\'t use a state for a particular row, we want to preserve its state.\n            # Thankfully, the rows which are all zero in the state correspond exactly\n            # to those which aren\'t used, so we create masks of shape (new_batch_size,),\n            # denoting which states were used in the RNN computation.\n            current_state_batch_size = self._states[0].size(1)\n            new_state_batch_size = final_states[0].size(1)\n            # Masks for the unused states of shape (1, new_batch_size, 1)\n            used_new_rows_mask = [(state[0, :, :].sum(-1)\n                                   != 0.0).float().view(1, new_state_batch_size, 1)\n                                  for state in new_unsorted_states]\n            new_states = []\n            if current_state_batch_size > new_state_batch_size:\n                # The new state is smaller than the old one,\n                # so just update the indices which we used.\n                for old_state, new_state, used_mask in zip(self._states,\n                                                           new_unsorted_states,\n                                                           used_new_rows_mask):\n                    # zero out all rows in the previous state\n                    # which _were_ used in the current state.\n                    masked_old_state = old_state[:, :new_state_batch_size, :] * (1 - used_mask)\n                    # The old state is larger, so update the relevant parts of it.\n                    old_state[:, :new_state_batch_size, :] = new_state + masked_old_state\n                    new_states.append(old_state.detach())\n            else:\n                # The states are the same size, so we just have to\n                # deal with the possibility that some rows weren\'t used.\n                new_states = []\n                for old_state, new_state, used_mask in zip(self._states,\n                                                           new_unsorted_states,\n                                                           used_new_rows_mask):\n                    # zero out all rows which _were_ used in the current state.\n                    masked_old_state = old_state * (1 - used_mask)\n                    # The old state is larger, so update the relevant parts of it.\n                    new_state += masked_old_state\n                    new_states.append(new_state.detach())\n\n            # It looks like there should be another case handled here - when\n            # the current_state_batch_size < new_state_batch_size. However,\n            # this never happens, because the states themeselves are mutated\n            # by appending zeros when calling _get_inital_states, meaning that\n            # the new states are either of equal size, or smaller, in the case\n            # that there are some unused elements (zero-length) for the RNN computation.\n            self._states = tuple(new_states)\n\n    def reset_states(self):\n        self._states = None\n'"
common/nlp/input_variational_dropout.py,4,"b'import torch\n\nclass InputVariationalDropout(torch.nn.Dropout):\n    """"""\n    Apply the dropout technique in Gal and Ghahramani, ""Dropout as a Bayesian Approximation:\n    Representing Model Uncertainty in Deep Learning"" (https://arxiv.org/abs/1506.02142) to a\n    3D tensor.\n\n    This module accepts a 3D tensor of shape ``(batch_size, num_timesteps, embedding_dim)``\n    and samples a single dropout mask of shape ``(batch_size, embedding_dim)`` and applies\n    it to every time step.\n    """"""\n    def forward(self, input_tensor):\n        # pylint: disable=arguments-differ\n        """"""\n        Apply dropout to input tensor.\n\n        Parameters\n        ----------\n        input_tensor: ``torch.FloatTensor``\n            A tensor of shape ``(batch_size, num_timesteps, embedding_dim)``\n\n        Returns\n        -------\n        output: ``torch.FloatTensor``\n            A tensor of shape ``(batch_size, num_timesteps, embedding_dim)`` with dropout applied.\n        """"""\n        ones = input_tensor.data.new_ones(input_tensor.shape[0], input_tensor.shape[-1])\n        dropout_mask = torch.nn.functional.dropout(ones, self.p, self.training, inplace=False)\n        if self.inplace:\n            input_tensor *= dropout_mask.unsqueeze(1)\n            return None\n        else:\n            return dropout_mask.unsqueeze(1) * input_tensor'"
common/nlp/misc.py,4,"b'import torch\nimport random\n\n\ndef get_align_matrix(aligned_ids, sparse=False, device=None, dtype=torch.float32):\n    """"""\n    Get aligned matrix for feature alignment in sentence embedding\n    :param aligned_ids: list, aligned_ids[k] means original index of k-th token\n    :param sparse: whether to return sparse matrix\n    :param device: device of returned align matrix\n    :param dtype: dtype of returned align matrix\n    :return: align_matrix: torch.FloatTensor, shape: (L, L\')\n\n    Example:\n    >> aligned_ids = [0, 0, 1, 2, 2, 2]\n    >> get_align_matrix(aligned_ids)\n    tensor([[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n            [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n            [0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333]])\n    """"""\n\n    l0 = max(aligned_ids) + 1\n    l1 = len(aligned_ids)\n    if sparse:\n        raise NotImplementedError\n    else:\n        align_matrix = torch.zeros((l0, l1), dtype=dtype, device=device)\n        align_matrix[aligned_ids, torch.arange(l1)] = 1\n        align_matrix = align_matrix / align_matrix.sum(dim=1, keepdim=True)\n\n    return align_matrix\n\n\ndef get_all_ngrams(words):\n    """"""\n    Get all n-grams of words\n    :param words: list of str\n    :return: ngrams, list of (list of str)\n    """"""\n    ngrams = []\n    N = len(words)\n    for n in range(1, N + 1):\n        for i in range(0, N - n + 1):\n            ngrams.append([words[j] for j in range(i, i + n)])\n\n    return ngrams\n\n\ndef random_word_with_token_ids(token_ids, tokenizer):\n    """"""\n    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\n    :param token_ids: list of int, list of token id.\n    :param tokenizer: Tokenizer, object used for tokenization (we need it\'s vocab here)\n    :return: (list of str, list of int), masked tokens and related labels for LM prediction\n    """"""\n    output_label = []\n    mask_id = tokenizer.convert_tokens_to_ids([\'[MASK]\'])[0]\n\n    for i, token_id in enumerate(token_ids):\n        prob = random.random()\n        # mask token with 15% probability\n        if prob < 0.15:\n            prob /= 0.15\n\n            # 80% randomly change token to mask token\n            if prob < 0.8:\n                token_ids[i] = mask_id\n\n            # 10% randomly change token to random token\n            elif prob < 0.9:\n                token_ids[i] = random.choice(list(tokenizer.vocab.items()))[1]\n\n            # -> rest 10% randomly keep current token\n\n            # append current token to output (we will predict these later)\n            output_label.append(token_id)\n        else:\n            # no masking token (will be ignored by loss function later)\n            output_label.append(-1)\n\n    return token_ids, output_label\n\n\n\n\n\n'"
common/nlp/time_distributed.py,2,"b'""""""\nA wrapper that unrolls the second (time) dimension of a tensor\ninto the first (batch) dimension, applies some other ``Module``,\nand then rolls the time dimension back up.\n""""""\n\nimport torch\n\n\nclass TimeDistributed(torch.nn.Module):\n    """"""\n    Given an input shaped like ``(batch_size, time_steps, [rest])`` and a ``Module`` that takes\n    inputs like ``(batch_size, [rest])``, ``TimeDistributed`` reshapes the input to be\n    ``(batch_size * time_steps, [rest])``, applies the contained ``Module``, then reshapes it back.\n\n    Note that while the above gives shapes with ``batch_size`` first, this ``Module`` also works if\n    ``batch_size`` is second - we always just combine the first two dimensions, then split them.\n    """"""\n    def __init__(self, module):\n        super(TimeDistributed, self).__init__()\n        self._module = module\n\n    def forward(self, *inputs, **kwargs):  # pylint: disable=arguments-differ\n        reshaped_inputs = []\n        for input_tensor in inputs:\n            input_size = input_tensor.size()\n            if len(input_size) <= 2:\n                raise RuntimeError(""No dimension to distribute: "" + str(input_size))\n\n            # Squash batch_size and time_steps into a single axis; result has shape\n            # (batch_size * time_steps, input_size).\n            squashed_shape = [-1] + [x for x in input_size[2:]]\n            reshaped_inputs.append(input_tensor.contiguous().view(*squashed_shape))\n\n        reshaped_outputs = self._module(*reshaped_inputs, **kwargs)\n\n        if isinstance(reshaped_outputs, torch.Tensor):\n            # Now get the output back into the right shape.\n            # (batch_size, time_steps, [hidden_size])\n            new_shape = [input_size[0], input_size[1]] + [x for x in reshaped_outputs.size()[1:]]\n            outputs = reshaped_outputs.contiguous().view(*new_shape)\n        elif isinstance(reshaped_outputs, tuple):\n            outputs = []\n            for output in reshaped_outputs:\n                new_shape = [input_size[0], input_size[1]] + [x for x in output.size()[1:]]\n                outputs.append(output.contiguous().view(*new_shape))\n            outputs = tuple(outputs)\n        else:\n            raise ValueError(""Not support!"")\n\n        return outputs\n'"
common/utils/bbox.py,11,"b'import torch\n\n\ndef nonlinear_transform(ex_rois, gt_rois):\n    """"""\n    compute bounding box regression targets from ex_rois to gt_rois\n    :param ex_rois: [k, 4] ([x1, y1, x2, y2])\n    :param gt_rois: [k, 4] (corresponding gt_boxes [x1, y1, x2, y2] )\n    :return: bbox_targets: [k, 4]\n    """"""\n    assert ex_rois.shape[0] == gt_rois.shape[0], \'inconsistent rois number\'\n\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * (ex_widths - 1.0)\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * (ex_heights - 1.0)\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * (gt_widths - 1.0)\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * (gt_heights - 1.0)\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / (ex_widths + 1e-6)\n    targets_dy = (gt_ctr_y - ex_ctr_y) / (ex_heights + 1e-6)\n    targets_dw = torch.log(gt_widths / (ex_widths).clamp(min=1e-6))\n    targets_dh = torch.log(gt_heights / ((ex_heights).clamp(min=1e-6)))\n\n    targets = torch.cat(\n        (targets_dx.view(-1, 1), targets_dy.view(-1, 1), targets_dw.view(-1, 1), targets_dh.view(-1, 1)), dim=-1)\n    return targets\n\n\ndef coordinate_embeddings(boxes, dim):\n    """"""\n    Coordinate embeddings of bounding boxes\n    :param boxes: [K, 6] ([x1, y1, x2, y2, w_image, h_image])\n    :param dim: sin/cos embedding dimension\n    :return: [K, 4, 2 * dim]\n    """"""\n\n    num_boxes = boxes.shape[0]\n    w = boxes[:, 4]\n    h = boxes[:, 5]\n\n    # transform to (x_c, y_c, w, h) format\n    boxes_ = boxes.new_zeros((num_boxes, 4))\n    boxes_[:, 0] = (boxes[:, 0] + boxes[:, 2]) / 2\n    boxes_[:, 1] = (boxes[:, 1] + boxes[:, 3]) / 2\n    boxes_[:, 2] = boxes[:, 2] - boxes[:, 0]\n    boxes_[:, 3] = boxes[:, 3] - boxes[:, 1]\n    boxes = boxes_\n\n    # position\n    pos = boxes.new_zeros((num_boxes, 4))\n    pos[:, 0] = boxes[:, 0] / w * 100\n    pos[:, 1] = boxes[:, 1] / h * 100\n    pos[:, 2] = boxes[:, 2] / w * 100\n    pos[:, 3] = boxes[:, 3] / h * 100\n\n    # sin/cos embedding\n    dim_mat = 1000 ** (torch.arange(dim, dtype=boxes.dtype, device=boxes.device) / dim)\n    sin_embedding = (pos.view((num_boxes, 4, 1)) / dim_mat.view((1, 1, -1))).sin()\n    cos_embedding = (pos.view((num_boxes, 4, 1)) / dim_mat.view((1, 1, -1))).cos()\n\n    return torch.cat((sin_embedding, cos_embedding), dim=-1)\n\n\ndef bbox_iou_py_vectorized(boxes, query_boxes):\n    n_ = boxes.shape[0]\n    k_ = query_boxes.shape[0]\n    n_mesh, k_mesh = torch.meshgrid([torch.arange(n_), torch.arange(k_)])\n    n_mesh = n_mesh.contiguous().view(-1)\n    k_mesh = k_mesh.contiguous().view(-1)\n    boxes = boxes[n_mesh]\n    query_boxes = query_boxes[k_mesh]\n\n    x11, y11, x12, y12 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n    x21, y21, x22, y22 = query_boxes[:, 0], query_boxes[:, 1], query_boxes[:, 2], query_boxes[:, 3]\n    xA = torch.max(x11, x21)\n    yA = torch.max(y11, y21)\n    xB = torch.min(x12, x22)\n    yB = torch.min(y12, y22)\n    interArea = torch.clamp(xB - xA + 1, min=0) * torch.clamp(yB - yA + 1, min=0)\n    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n    iou = interArea / (boxAArea + boxBArea - interArea)\n\n    return iou.view(n_, k_).to(boxes.device)\n\n\n\n\n\n\n'"
common/utils/clip_pad.py,12,"b'import torch\n\n\ndef clip_pad_images(tensor, pad_shape, pad=0):\n    """"""\n    Clip clip_pad_images of the pad area.\n    :param tensor: [c, H, W]\n    :param pad_shape: [h, w]\n    :return: [c, h, w]\n    """"""\n    if not isinstance(tensor, torch.Tensor):\n        tensor = torch.as_tensor(tensor)\n    H, W = tensor.shape[1:]\n    h = pad_shape[1]\n    w = pad_shape[2]\n\n    tensor_ret = torch.zeros((tensor.shape[0], h, w), dtype=tensor.dtype) + pad\n    tensor_ret[:, :min(h, H), :min(w, W)] = tensor[:, :min(h, H), :min(w, W)]\n\n    return tensor_ret\n\n\ndef clip_pad_boxes(tensor, pad_length, pad=0):\n    """"""\n        Clip boxes of the pad area.\n        :param tensor: [k, d]\n        :param pad_shape: K\n        :return: [K, d]\n    """"""\n    if not isinstance(tensor, torch.Tensor):\n        tensor = torch.as_tensor(tensor)\n    k = tensor.shape[0]\n    d = tensor.shape[1]\n    K = pad_length\n    tensor_ret = torch.zeros((K, d), dtype=tensor.dtype) + pad\n    tensor_ret[:min(k, K), :] = tensor[:min(k, K), :]\n\n    return tensor_ret\n\n\ndef clip_pad_1d(tensor, pad_length, pad=0):\n    if not isinstance(tensor, torch.Tensor):\n        tensor = torch.as_tensor(tensor)\n    tensor_ret = torch.zeros((pad_length, ), dtype=tensor.dtype) + pad\n    tensor_ret[:min(tensor.shape[0], pad_length)] = tensor[:min(tensor.shape[0], pad_length)]\n\n    return tensor_ret\n\n\ndef clip_pad_2d(tensor, pad_shape, pad=0):\n    if not isinstance(tensor, torch.Tensor):\n        tensor = torch.as_tensor(tensor)\n    tensor_ret = torch.zeros(*pad_shape, dtype=tensor.dtype) + pad\n    tensor_ret[:min(tensor.shape[0], pad_shape[0]), :min(tensor.shape[1], pad_shape[1])] \\\n        = tensor[:min(tensor.shape[0], pad_shape[0]), :min(tensor.shape[1], pad_shape[1])]\n\n    return tensor_ret\n'"
common/utils/flatten.py,1,"b'import torch\n\n\nclass Flattener(torch.nn.Module):\n    def __init__(self):\n        """"""\n        Flattens last 3 dimensions to make it only batch size, -1\n        """"""\n        super(Flattener, self).__init__()\n\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n'"
common/utils/load.py,2,"b'import torch\nimport os\n\n\ndef smart_load_model_state_dict(model, state_dict):\n    parsed_state_dict = {}\n    for k, v in state_dict.items():\n        if k not in model.state_dict():\n            if k.startswith(\'module.\'):\n                k = k[len(\'module.\'):]\n            else:\n                k = \'module.\' + k\n        if k in model.state_dict():\n            parsed_state_dict[k] = v\n        else:\n            raise ValueError(\'failed to match key of state dict smartly!\')\n    model.load_state_dict(parsed_state_dict)\n\n\ndef smart_resume(model, optimizer, validation_monitor, config, model_prefix, logger):\n    if config.TRAIN.RESUME:\n        print((\'continue training from \', config.TRAIN.BEGIN_EPOCH))\n        # load model\n        model_filename = \'{}-{:04d}.model\'.format(model_prefix, config.TRAIN.BEGIN_EPOCH - 1)\n        check_point = torch.load(model_filename, map_location=lambda storage, loc: storage)\n        # model.load_state_dict(check_point[\'state_dict\'])\n        smart_load_model_state_dict(model, check_point[\'state_dict\'])\n        optimizer.load_state_dict(check_point[\'optimizer\'])\n        if \'validation_monitor\' in check_point:\n            validation_monitor.load_state_dict(check_point[\'validation_monitor\'])\n            print(\n                \'Best Val {}: {}, Epoch: {}\'.format(validation_monitor.host_metric_name,\n                                                    validation_monitor.best_val,\n                                                    validation_monitor.best_epoch)\n            )\n    elif config.TRAIN.AUTO_RESUME:\n        for epoch in range(config.TRAIN.END_EPOCH, config.TRAIN.BEGIN_EPOCH, -1):\n            model_filename = \'{}-{:04d}.model\'.format(model_prefix, epoch - 1)\n            if os.path.exists(model_filename):\n                config.TRAIN.BEGIN_EPOCH = epoch\n                check_point = torch.load(model_filename, map_location=lambda storage, loc: storage)\n                # model.load_state_dict(check_point[\'state_dict\'])\n                smart_load_model_state_dict(model, check_point[\'state_dict\'])\n                optimizer.load_state_dict(check_point[\'optimizer\'])\n                if \'validation_monitor\' in check_point:\n                    validation_monitor.load_state_dict(check_point[\'validation_monitor\'])\n                    print(\n                        \'Best Val {}: {}, Epoch: {}\'.format(validation_monitor.host_metric_name,\n                                                            validation_monitor.best_val,\n                                                            validation_monitor.best_epoch)\n                    )\n                logger.info(""Auto continue training from {0}"".format(model_filename))\n                print(""Auto continue training from {0}"".format(model_filename))\n                break\n\n\ndef smart_partial_load_model_state_dict(model, state_dict):\n    parsed_state_dict = {}\n    non_match_keys = []\n    pretrained_keys = []\n    for k, v in state_dict.items():\n        if k not in model.state_dict():\n            if k.startswith(\'module.\'):\n                k = k[len(\'module.\'):]\n            else:\n                k = \'module.\' + k\n        if k in model.state_dict():\n            parsed_state_dict[k] = v\n            pretrained_keys.append(k)\n        else:\n            non_match_keys.append(k)\n            # raise ValueError(\'failed to match key of state dict smartly!\')\n\n    non_pretrain_keys = [k for k in model.state_dict().keys() if k not in pretrained_keys]\n\n    print(""[Partial Load] partial load state dict of keys: {}"".format(parsed_state_dict.keys()))\n    print(""[Partial Load] non matched keys: {}"".format(non_match_keys))\n    print(""[Partial Load] non pretrain keys: {}"".format(non_pretrain_keys))\n    new_state_dict = model.state_dict()\n    new_state_dict.update(parsed_state_dict)\n    model.load_state_dict(new_state_dict)\n\n'"
common/utils/mask.py,5,"b'from skimage.draw import polygon\nimport torch\n\n\ndef generate_instance_mask(seg_polys, box, mask_size=(14, 14), dtype=torch.float32, copy=True):\n    """"""\n    Generate instance mask from polygon\n    :param seg_poly: torch.Tensor, (N, 2), (x, y) coordinate of N vertices of segmented foreground polygon\n    :param box: array-like, (4, ), (xmin, ymin, xmax, ymax), instance bounding box\n    :param mask_size: tuple, (mask_height, mask_weight)\n    :param dtype: data type of generated mask\n    :param copy: whether copy seg_polys to a new tensor first\n    :return: torch.Tensor, of mask_size, instance mask\n    """"""\n    mask = torch.zeros(mask_size, dtype=dtype)\n    w_ratio = float(mask_size[0]) / (box[2] - box[0] + 1)\n    h_ratio = float(mask_size[1]) / (box[3] - box[1] + 1)\n\n    # import IPython\n    # IPython.embed()\n\n    for seg_poly in seg_polys:\n        if copy:\n            seg_poly = seg_poly.detach().clone()\n        seg_poly = seg_poly.type(torch.float32)\n        seg_poly[:, 0] = (seg_poly[:, 0] - box[0]) * w_ratio\n        seg_poly[:, 1] = (seg_poly[:, 1] - box[1]) * h_ratio\n        rr, cc = polygon(seg_poly[:, 1].clamp(min=0, max=mask_size[1] - 1),\n                         seg_poly[:, 0].clamp(min=0, max=mask_size[0] - 1))\n\n        mask[rr, cc] = 1\n    return mask\n\n\n\n\n'"
common/utils/masked_softmax.py,5,"b'import torch\n\n\ndef masked_softmax(vector: torch.Tensor, mask: torch.Tensor, dim: int = -1) -> torch.Tensor:\n    """"""\n    ``torch.nn.functional.softmax(vector)`` does not work if some elements of ``vector`` should be\n    masked.  This performs a softmax on just the non-masked portions of ``vector``.  Passing\n    ``None`` in for the mask is also acceptable; you\'ll just get a regular softmax.\n\n    ``vector`` can have an arbitrary number of dimensions; the only requirement is that ``mask`` is\n    broadcastable to ``vector\'s`` shape.  If ``mask`` has fewer dimensions than ``vector``, we will\n    unsqueeze on dimension 1 until they match.  If you need a different unsqueezing of your mask,\n    do it yourself before passing the mask into this function.\n\n    In the case that the input vector is completely masked, this function returns an array\n    of ``0.0``. This behavior may cause ``NaN`` if this is used as the last layer of a model\n    that uses categorical cross-entropy loss.\n    """"""\n    if mask is None:\n        result = torch.nn.functional.softmax(vector, dim=dim)\n    else:\n        mask = mask.type(vector.dtype)\n        while mask.dim() < vector.dim():\n            mask = mask.unsqueeze(1)\n        # To limit numerical errors from large vector elements outside the mask, we zero these out.\n        result = torch.nn.functional.softmax(vector * mask, dim=dim)\n        result = result * mask\n        result = result / (result.sum(dim=dim, keepdim=True) + (1e-7 if vector.dtype == torch.half else 1e-13))\n    return result\n'"
common/utils/misc.py,3,"b'import os\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport logging\n\n\ndef block_digonal_matrix(*blocks):\n    """"""\n    Construct block diagonal matrix\n    :param blocks: blocks of block diagonal matrix\n    :param device\n    :param dtype\n    :return: block diagonal matrix\n    """"""\n    assert len(blocks) > 0\n    rows = [block.shape[0] for block in blocks]\n    cols = [block.shape[1] for block in blocks]\n    out = torch.zeros((sum(rows), sum(cols)),\n                      device=blocks[0].device,\n                      dtype=blocks[0].dtype)\n    cur_row = 0\n    cur_col = 0\n    for block, row, col in zip(blocks, rows, cols):\n        out[cur_row:(cur_row + row), cur_col:(cur_col + col)] = block\n        cur_row += row\n        cur_col += col\n\n    return out\n\n\ndef print_and_log(string, logger=None):\n    print(string)\n    if logger is None:\n        logging.info(string)\n    else:\n        logger.info(string)\n\n\ndef summary_parameters(model, logger=None):\n    """"""\n    Summary Parameters of Model\n    :param model: torch.nn.module_name\n    :param logger: logger\n    :return: None\n    """"""\n\n    print_and_log(\'>> Trainable Parameters:\', logger)\n    trainable_paramters = [(str(n), str(v.dtype), str(tuple(v.shape)), str(v.numel()))\n                           for n, v in model.named_parameters() if v.requires_grad]\n    max_lens = [max([len(item) + 4 for item in col]) for col in zip(*trainable_paramters)]\n    raw_format = \'|\' + \'|\'.join([\'{{:{}s}}\'.format(max_len) for max_len in max_lens]) + \'|\'\n    raw_split = \'-\' * (sum(max_lens) + len(max_lens) + 1)\n    print_and_log(raw_split, logger)\n    print_and_log(raw_format.format(\'Name\', \'Dtype\', \'Shape\', \'#Params\'), logger)\n    print_and_log(raw_split, logger)\n\n    for name, dtype, shape, number in trainable_paramters:\n        print_and_log(raw_format.format(name, dtype, shape, number), logger)\n        print_and_log(raw_split, logger)\n\n    num_trainable_params = sum([v.numel() for v in model.parameters() if v.requires_grad])\n    total_params = sum([v.numel() for v in model.parameters()])\n    non_trainable_params = total_params - num_trainable_params\n    print_and_log(\'>> {:25s}\\t{:.2f}\\tM\'.format(\'# TrainableParams:\', num_trainable_params / (1.0 * 10 ** 6)), logger)\n    print_and_log(\'>> {:25s}\\t{:.2f}\\tM\'.format(\'# NonTrainableParams:\', non_trainable_params / (1.0 * 10 ** 6)), logger)\n    print_and_log(\'>> {:25s}\\t{:.2f}\\tM\'.format(\'# TotalParams:\', total_params / (1.0 * 10 ** 6)), logger)\n\n\n\n\ndef clip_grad(named_parameters, max_norm, logger=logging, std_verbose=False, log_verbose=False):\n    """"""Clips gradient norm of an iterable of parameters.\n    The norm is computed over all gradients together, as if they were\n    concatenated into a single vector. Gradients are modified in-place.\n    :param named_parameters: dict, named parameters of pytorch module\n    :param max_norm: float or int, max norm of the gradients\n    :param logger: logger to write verbose info\n    :param std_verbose: verbose info in stdout\n    :param log_verbose: verbose info in log\n\n    :return Total norm of the parameters (viewed as a dict: param name -> param grad norm).\n    """"""\n    max_norm = float(max_norm)\n    parameters = [(n, p) for n, p in named_parameters if p.grad is not None]\n    total_norm = 0\n    param_to_norm = {}\n    param_to_shape = {}\n    for n, p in parameters:\n        param_norm = p.grad.data.norm(2)\n        total_norm += param_norm ** 2\n        param_to_norm[n] = param_norm\n        param_to_shape[n] = tuple(p.size())\n        if np.isnan(param_norm.item()):\n            raise ValueError(""the param {} was null."".format(n))\n\n    total_norm = total_norm ** (1. / 2)\n    clip_coef = max_norm / (total_norm + 1e-6)\n    if clip_coef.item() < 1:\n        logger.info(\'---Clip grad! Total norm: {:.3f}, clip coef: {:.3f}.\'.format(total_norm, clip_coef))\n        for n, p in parameters:\n            p.grad.data.mul_(clip_coef)\n\n    if std_verbose:\n        print(\'---Total norm {:.3f} clip coef {:.3f}-----------------\'.format(total_norm, clip_coef))\n        for name, norm in sorted(param_to_norm.items(), key=lambda x: -x[1]):\n            print(""{:<60s}: {:.3f}, ({}: {})"".format(name, norm, np.prod(param_to_shape[name]), param_to_shape[name]))\n        print(\'-------------------------------\', flush=True)\n    if log_verbose:\n        logger.info(\'---Total norm {:.3f} clip coef {:.3f}-----------------\'.format(total_norm, clip_coef))\n        for name, norm in sorted(param_to_norm.items(), key=lambda x: -x[1]):\n            logger.info(""{:<60s}: {:.3f}, ({}: {})"".format(name, norm, np.prod(param_to_shape[name]), param_to_shape[name]))\n        logger.info(\'-------------------------------\')\n\n    return {name: norm.item() for name, norm in param_to_norm.items()}\n\n\ndef bn_fp16_half_eval(m):\n    classname = str(m.__class__)\n    if \'BatchNorm\' in classname and (not m.training):\n        m.half()\n\n\ndef soft_cross_entropy(input, target, reduction=\'mean\'):\n    """"""\n    Cross entropy loss with input logits and soft target\n    :param input: Tensor, size: (N, C)\n    :param target: Tensor, size: (N, C)\n    :param reduction: \'none\' or \'mean\' or \'sum\', default: \'mean\'\n    :return: loss\n    """"""\n    eps = 1.0e-1\n    # debug = False\n    valid = (target.sum(1) - 1).abs() < eps\n    # if debug:\n    #     print(\'valid\', valid.sum().item())\n    #     print(\'all\', valid.numel())\n    #     print(\'non valid\')\n    #     print(target[valid == 0])\n    if valid.sum().item() == 0:\n        return input.new_zeros(())\n    if reduction == \'mean\':\n        return (- F.log_softmax(input[valid], 1) * target[valid]).sum(1).mean(0)\n    elif reduction == \'sum\':\n        return (- F.log_softmax(input[valid], 1) * target[valid]).sum()\n    elif reduction == \'none\':\n        l = input.new_zeros((input.shape[0], ))\n        l[valid] = (- F.log_softmax(input[valid], 1) * target[valid]).sum(1)\n        return l\n    else:\n        raise ValueError(\'Not support reduction type: {}.\'.format(reduction))\n\n\n\n\n\n\n'"
common/utils/multi_task_dataloader.py,1,"b'from functools import reduce\nimport operator\nfrom typing import List\nfrom torch.utils.data import DataLoader\nimport sys\n\nINT_MAX = sys.maxsize\n\n\ndef prod(iterable):\n    if len(list(iterable)) > 0:\n        return reduce(operator.mul, iterable)\n    else:\n        return 1\n\n\nclass MultiTaskDataLoader(object):\n    """"""\n    Multi-task DataLoader, the first dataloader is master dataloader\n    """"""\n    def __init__(self,\n                 loaders: List[DataLoader]):\n        assert len(loaders) > 1, ""Less than 2 loader!""\n        self.loaders = loaders\n        self.iters = [iter(loader) for loader in loaders]\n        self.lens = [len(loader) for loader in loaders]\n        self.global_idx_in_cycle = 0\n\n    def __iter__(self):\n        if self.global_idx_in_cycle > 0:\n            self.iters[0] = iter(self.loaders[0])\n        return self\n\n    def __next__(self):\n        output_tuple = (*next(self.iters[0]), )\n        for k, (loader, _iter) in enumerate(zip(self.loaders[1:], self.iters[1:])):\n            if hasattr(loader.batch_sampler.sampler, \'set_epoch\'):\n                loader.batch_sampler.sampler.set_epoch(int(self.global_idx_in_cycle / self.lens[k+1]))\n            try:\n                output_tuple += (*next(_iter), )\n            except StopIteration:\n                _iter = iter(loader)\n                self.iters[k+1] = _iter\n                output_tuple += (*next(_iter), )\n\n        if self.global_idx_in_cycle < INT_MAX - 1:\n            self.global_idx_in_cycle += 1\n        else:\n            self.global_idx_in_cycle = 0\n\n        return output_tuple\n\n    def __len__(self):\n        return self.lens[0]\n\n\n\n'"
pretrain/data/build.py,5,"b'import torch.utils.data\n\nfrom .datasets import *\nfrom . import samplers\nfrom .transforms.build import build_transforms\nfrom .collate_batch import BatchCollator\nimport pprint\nfrom copy import deepcopy\n\nDATASET_CATALOGS = {\'conceptual_captions\': ConceptualCaptionsDataset,\n                    \'coco_captions\': COCOCaptionsDataset,\n                    \'general_corpus\': GeneralCorpus}\n\n\ndef build_dataset(dataset_name, *args, **kwargs):\n    assert dataset_name in DATASET_CATALOGS, ""dataset not in catalogs""\n    return DATASET_CATALOGS[dataset_name](*args, **kwargs)\n\n\ndef make_data_sampler(dataset, shuffle, distributed, num_replicas, rank):\n    if distributed:\n        return samplers.DistributedSampler(dataset, shuffle=shuffle, num_replicas=num_replicas, rank=rank)\n    if shuffle:\n        sampler = torch.utils.data.sampler.RandomSampler(dataset)\n    else:\n        sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n    return sampler\n\n\ndef make_batch_data_sampler(dataset, sampler, aspect_grouping, batch_size):\n    if aspect_grouping:\n        group_ids = dataset.group_ids\n        batch_sampler = samplers.GroupedBatchSampler(\n            sampler, group_ids, batch_size, drop_uneven=False\n        )\n    else:\n        batch_sampler = torch.utils.data.sampler.BatchSampler(\n            sampler, batch_size, drop_last=False\n        )\n    return batch_sampler\n\n\ndef make_dataloader(cfg, dataset=None, mode=\'train\', distributed=False, num_replicas=None, rank=None,\n                    expose_sampler=False):\n    assert mode in [\'train\', \'val\', \'test\']\n    if mode == \'train\':\n        ann_file = cfg.DATASET.TRAIN_ANNOTATION_FILE\n        image_set = cfg.DATASET.TRAIN_IMAGE_SET\n        aspect_grouping = cfg.TRAIN.ASPECT_GROUPING\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.TRAIN.BATCH_IMAGES * num_gpu\n        shuffle = cfg.TRAIN.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n    elif mode == \'val\':\n        ann_file = cfg.DATASET.VAL_ANNOTATION_FILE\n        image_set = cfg.DATASET.VAL_IMAGE_SET\n        aspect_grouping = False\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.VAL.BATCH_IMAGES * num_gpu\n        shuffle = cfg.VAL.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n    else:\n        ann_file = cfg.DATASET.TEST_ANNOTATION_FILE\n        image_set = cfg.DATASET.TEST_IMAGE_SET\n        aspect_grouping = False\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.TEST.BATCH_IMAGES * num_gpu\n        shuffle = cfg.TEST.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n\n    transform = build_transforms(cfg, mode)\n\n    if dataset is None:\n\n        dataset = build_dataset(dataset_name=cfg.DATASET.DATASET, ann_file=ann_file, image_set=image_set,\n                                seq_len=cfg.DATASET.SEQ_LEN, min_seq_len=cfg.DATASET.MIN_SEQ_LEN,\n                                with_precomputed_visual_feat=cfg.NETWORK.IMAGE_FEAT_PRECOMPUTED,\n                                mask_raw_pixels=cfg.NETWORK.MASK_RAW_PIXELS,\n                                with_rel_task=cfg.NETWORK.WITH_REL_LOSS,\n                                with_mlm_task=cfg.NETWORK.WITH_MLM_LOSS,\n                                with_mvrc_task=cfg.NETWORK.WITH_MVRC_LOSS,\n                                answer_vocab_file=cfg.DATASET.ANSWER_VOCAB_FILE,\n                                root_path=cfg.DATASET.ROOT_PATH, data_path=cfg.DATASET.DATASET_PATH,\n                                test_mode=(mode == \'test\'), transform=transform,\n                                zip_mode=cfg.DATASET.ZIP_MODE, cache_mode=cfg.DATASET.CACHE_MODE,\n                                cache_db=True if (rank is None or rank == 0) else False,\n                                ignore_db_cache=cfg.DATASET.IGNORE_DB_CACHE,\n                                add_image_as_a_box=cfg.DATASET.ADD_IMAGE_AS_A_BOX,\n                                aspect_grouping=aspect_grouping,\n                                mask_size=(cfg.DATASET.MASK_SIZE, cfg.DATASET.MASK_SIZE),\n                                pretrained_model_name=cfg.NETWORK.BERT_MODEL_NAME)\n\n    sampler = make_data_sampler(dataset, shuffle, distributed, num_replicas, rank)\n    batch_sampler = make_batch_data_sampler(dataset, sampler, aspect_grouping, batch_size)\n    collator = BatchCollator(dataset=dataset, append_ind=cfg.DATASET.APPEND_INDEX)\n\n    dataloader = torch.utils.data.DataLoader(dataset=dataset,\n                                             batch_sampler=batch_sampler,\n                                             num_workers=num_workers,\n                                             pin_memory=False,\n                                             collate_fn=collator)\n    if expose_sampler:\n        return dataloader, sampler\n\n    return dataloader\n\n\ndef make_dataloaders(cfg, mode=\'train\', distributed=False, num_replicas=None, rank=None, expose_sampler=False):\n\n    outputs = []\n\n    for i, dataset_cfg in enumerate(cfg.DATASET):\n        cfg_ = deepcopy(cfg)\n        cfg_.DATASET = dataset_cfg\n        cfg_.TRAIN.BATCH_IMAGES = cfg.TRAIN.BATCH_IMAGES[i]\n        cfg_.VAL.BATCH_IMAGES = cfg.VAL.BATCH_IMAGES[i]\n        cfg_.TEST.BATCH_IMAGES = cfg.TEST.BATCH_IMAGES[i]\n        outputs.append(\n            make_dataloader(cfg_,\n                            mode=mode,\n                            distributed=distributed,\n                            num_replicas=num_replicas,\n                            rank=rank,\n                            expose_sampler=expose_sampler)\n        )\n\n    return outputs\n\n'"
pretrain/data/collate_batch.py,3,"b""import torch\nfrom common.utils.clip_pad import *\n\n\nclass BatchCollator(object):\n    def __init__(self, dataset, append_ind=False):\n        self.dataset = dataset\n        self.test_mode = self.dataset.test_mode\n        self.data_names = self.dataset.data_names\n        self.append_ind = append_ind\n\n    def __call__(self, batch):\n        if not isinstance(batch, list):\n            batch = list(batch)\n\n        if 'image' in self.data_names:\n            if batch[0][self.data_names.index('image')] is not None:\n                max_shape = tuple(max(s) for s in zip(*[data[self.data_names.index('image')].shape for data in batch]))\n                image_none = False\n            else:\n                image_none = True\n        if 'boxes' in self.data_names:\n            max_boxes = max([data[self.data_names.index('boxes')].shape[0] for data in batch])\n        if 'text' in self.data_names:\n            max_text_length = max([len(data[self.data_names.index('text')]) for data in batch])\n\n        for i, ibatch in enumerate(batch):\n            out = {}\n\n            if 'image' in self.data_names:\n                if image_none:\n                    out['image'] = None\n                else:\n                    image = ibatch[self.data_names.index('image')]\n                    out['image'] = clip_pad_images(image, max_shape, pad=0)\n\n            if 'boxes' in self.data_names:\n                boxes = ibatch[self.data_names.index('boxes')]\n                out['boxes'] = clip_pad_boxes(boxes, max_boxes, pad=-2)\n\n            if 'text' in self.data_names:\n                text = ibatch[self.data_names.index('text')]\n                out['text'] = clip_pad_1d(text, max_text_length, pad=0)\n\n            if 'mlm_labels' in self.data_names:\n                mlm_labels = ibatch[self.data_names.index('mlm_labels')]\n                out['mlm_labels'] = clip_pad_1d(mlm_labels, max_text_length, pad=-1)\n\n            if 'mvrc_ops' in self.data_names:\n                mvrc_ops = ibatch[self.data_names.index('mvrc_ops')]\n                out['mvrc_ops'] = clip_pad_1d(mvrc_ops, max_boxes, pad=0)\n\n            if 'mvrc_labels' in self.data_names:\n                mvrc_labels = ibatch[self.data_names.index('mvrc_labels')]\n                out['mvrc_labels'] = clip_pad_boxes(mvrc_labels, max_boxes, pad=0)\n\n            other_names = [data_name for data_name in self.data_names if data_name not in out]\n            for name in other_names:\n                out[name] = torch.as_tensor(ibatch[self.data_names.index(name)])\n\n            batch[i] = tuple(out[data_name] for data_name in self.data_names)\n            if self.append_ind:\n                batch[i] += (torch.tensor(i, dtype=torch.int64),)\n\n        out_tuple = ()\n        for items in zip(*batch):\n            if items[0] is None:\n                out_tuple += (None,)\n            else:\n                out_tuple += (torch.stack(tuple(items), dim=0), )\n\n        return out_tuple\n\n"""
pretrain/function/train.py,19,"b'import os\nimport pprint\nimport shutil\nimport inspect\nimport random\n\nfrom tensorboardX import SummaryWriter\nimport numpy as np\nimport torch\nimport torch.nn\nimport torch.optim as optim\nimport torch.distributed as distributed\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nfrom common.utils.create_logger import create_logger\nfrom common.utils.misc import summary_parameters, bn_fp16_half_eval\nfrom common.utils.load import smart_resume, smart_partial_load_model_state_dict\nfrom common.trainer import train\nfrom common.metrics.composite_eval_metric import CompositeEvalMetric\nfrom common.metrics import pretrain_metrics\nfrom common.callbacks.batch_end_callbacks.speedometer import Speedometer\nfrom common.callbacks.epoch_end_callbacks.validation_monitor import ValidationMonitor\nfrom common.callbacks.epoch_end_callbacks.checkpoint import Checkpoint\nfrom common.lr_scheduler import WarmupMultiStepLR\nfrom common.nlp.bert.optimization import AdamW, WarmupLinearSchedule\nfrom common.utils.multi_task_dataloader import MultiTaskDataLoader\nfrom pretrain.data.build import make_dataloader, make_dataloaders\nfrom pretrain.modules import *\nfrom pretrain.function.val import do_validation\n\ntry:\n    from apex import amp\n    from apex.parallel import DistributedDataParallel as Apex_DDP\nexcept ImportError:\n    pass\n    # raise ImportError(""Please install apex from https://www.github.com/nvidia/apex if you want to use fp16."")\n\n\ndef train_net(args, config):\n    # setup logger\n    logger, final_output_path = create_logger(config.OUTPUT_PATH,\n                                              args.cfg,\n                                              config.DATASET[0].TRAIN_IMAGE_SET if isinstance(config.DATASET, list)\n                                              else config.DATASET.TRAIN_IMAGE_SET,\n                                              split=\'train\')\n    model_prefix = os.path.join(final_output_path, config.MODEL_PREFIX)\n    if args.log_dir is None:\n        args.log_dir = os.path.join(final_output_path, \'tensorboard_logs\')\n\n    pprint.pprint(args)\n    logger.info(\'training args:{}\\n\'.format(args))\n    pprint.pprint(config)\n    logger.info(\'training config:{}\\n\'.format(pprint.pformat(config)))\n\n    # manually set random seed\n    if config.RNG_SEED > -1:\n        random.seed(config.RNG_SEED)\n        np.random.seed(config.RNG_SEED)\n        torch.random.manual_seed(config.RNG_SEED)\n        torch.cuda.manual_seed_all(config.RNG_SEED)\n\n    # cudnn\n    torch.backends.cudnn.benchmark = False\n    if args.cudnn_off:\n        torch.backends.cudnn.enabled = False\n\n    if args.dist:\n        model = eval(config.MODULE)(config)\n        local_rank = int(os.environ.get(\'LOCAL_RANK\') or 0)\n        config.GPUS = str(local_rank)\n        torch.cuda.set_device(local_rank)\n        master_address = os.environ[\'MASTER_ADDR\']\n        master_port = int(os.environ[\'MASTER_PORT\'] or 23456)\n        world_size = int(os.environ[\'WORLD_SIZE\'] or 1)\n        rank = int(os.environ[\'RANK\'] or 0)\n        if args.slurm:\n            distributed.init_process_group(backend=\'nccl\')\n        else:\n            distributed.init_process_group(\n                backend=\'nccl\',\n                init_method=\'tcp://{}:{}\'.format(master_address, master_port),\n                world_size=world_size,\n                rank=rank,\n                group_name=\'mtorch\')\n        print(f\'native distributed, size: {world_size}, rank: {rank}, local rank: {local_rank}\')\n        torch.cuda.set_device(local_rank)\n        config.GPUS = str(local_rank)\n        model = model.cuda()\n        if not config.TRAIN.FP16:\n            model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n\n        if rank == 0:\n            summary_parameters(model.module if isinstance(model, torch.nn.parallel.DistributedDataParallel) else model,\n                               logger)\n            shutil.copy(args.cfg, final_output_path)\n            shutil.copy(inspect.getfile(eval(config.MODULE)), final_output_path)\n\n        writer = None\n        if args.log_dir is not None:\n            tb_log_dir = os.path.join(args.log_dir, \'rank{}\'.format(rank))\n            if not os.path.exists(tb_log_dir):\n                os.makedirs(tb_log_dir)\n            writer = SummaryWriter(log_dir=tb_log_dir)\n\n        if isinstance(config.DATASET, list):\n            train_loaders_and_samplers = make_dataloaders(config,\n                                                          mode=\'train\',\n                                                          distributed=True,\n                                                          num_replicas=world_size,\n                                                          rank=rank,\n                                                          expose_sampler=True)\n            val_loaders = make_dataloaders(config,\n                                           mode=\'val\',\n                                           distributed=True,\n                                           num_replicas=world_size,\n                                           rank=rank)\n            train_loader = MultiTaskDataLoader([loader for loader, _ in train_loaders_and_samplers])\n            val_loader = MultiTaskDataLoader(val_loaders)\n            train_sampler = train_loaders_and_samplers[0][1]\n        else:\n            train_loader, train_sampler = make_dataloader(config,\n                                                          mode=\'train\',\n                                                          distributed=True,\n                                                          num_replicas=world_size,\n                                                          rank=rank,\n                                                          expose_sampler=True)\n            val_loader = make_dataloader(config,\n                                         mode=\'val\',\n                                         distributed=True,\n                                         num_replicas=world_size,\n                                         rank=rank)\n\n        batch_size = world_size * (sum(config.TRAIN.BATCH_IMAGES)\n                                   if isinstance(config.TRAIN.BATCH_IMAGES, list)\n                                   else config.TRAIN.BATCH_IMAGES)\n        if config.TRAIN.GRAD_ACCUMULATE_STEPS > 1:\n            batch_size = batch_size * config.TRAIN.GRAD_ACCUMULATE_STEPS\n        base_lr = config.TRAIN.LR * batch_size\n        optimizer_grouped_parameters = [{\'params\': [p for n, p in model.named_parameters() if _k in n],\n                                         \'lr\': base_lr * _lr_mult}\n                                        for _k, _lr_mult in config.TRAIN.LR_MULT]\n        optimizer_grouped_parameters.append({\'params\': [p for n, p in model.named_parameters()\n                                                        if all([_k not in n for _k, _ in config.TRAIN.LR_MULT])]})\n        if config.TRAIN.OPTIMIZER == \'SGD\':\n            optimizer = optim.SGD(optimizer_grouped_parameters,\n                                  lr=config.TRAIN.LR * batch_size,\n                                  momentum=config.TRAIN.MOMENTUM,\n                                  weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'Adam\':\n            optimizer = optim.Adam(optimizer_grouped_parameters,\n                                   lr=config.TRAIN.LR * batch_size,\n                                   weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'AdamW\':\n            optimizer = AdamW(optimizer_grouped_parameters,\n                              lr=config.TRAIN.LR * batch_size,\n                              betas=(0.9, 0.999),\n                              eps=1e-6,\n                              weight_decay=config.TRAIN.WD,\n                              correct_bias=True)\n        else:\n            raise ValueError(\'Not support optimizer {}!\'.format(config.TRAIN.OPTIMIZER))\n        total_gpus = world_size\n\n    else:\n        #os.environ[\'CUDA_VISIBLE_DEVICES\'] = config.GPUS\n        model = eval(config.MODULE)(config)\n        summary_parameters(model, logger)\n        shutil.copy(args.cfg, final_output_path)\n        shutil.copy(inspect.getfile(eval(config.MODULE)), final_output_path)\n        num_gpus = len(config.GPUS.split(\',\'))\n        assert num_gpus <= 1 or (not config.TRAIN.FP16), ""Not support fp16 with torch.nn.DataParallel. "" \\\n                                                         ""Please use amp.parallel.DistributedDataParallel instead.""\n        total_gpus = num_gpus\n        rank = None\n        writer = SummaryWriter(log_dir=args.log_dir) if args.log_dir is not None else None\n\n        # model\n        if num_gpus > 1:\n            model = torch.nn.DataParallel(model, device_ids=[int(d) for d in config.GPUS.split(\',\')]).cuda()\n        else:\n            torch.cuda.set_device(int(config.GPUS))\n            model.cuda()\n\n        # loader\n        if isinstance(config.DATASET, list):\n            train_loaders = make_dataloaders(config, mode=\'train\', distributed=False)\n            val_loaders = make_dataloaders(config, mode=\'val\', distributed=False)\n            train_loader = MultiTaskDataLoader(train_loaders)\n            val_loader = MultiTaskDataLoader(val_loaders)\n        else:\n            train_loader = make_dataloader(config, mode=\'train\', distributed=False)\n            val_loader = make_dataloader(config, mode=\'val\', distributed=False)\n        train_sampler = None\n\n        batch_size = num_gpus * (sum(config.TRAIN.BATCH_IMAGES) if isinstance(config.TRAIN.BATCH_IMAGES, list)\n                                 else config.TRAIN.BATCH_IMAGES)\n        if config.TRAIN.GRAD_ACCUMULATE_STEPS > 1:\n            batch_size = batch_size * config.TRAIN.GRAD_ACCUMULATE_STEPS\n        base_lr = config.TRAIN.LR * batch_size\n        optimizer_grouped_parameters = [{\'params\': [p for n, p in model.named_parameters() if _k in n],\n                                         \'lr\': base_lr * _lr_mult}\n                                        for _k, _lr_mult in config.TRAIN.LR_MULT]\n        optimizer_grouped_parameters.append({\'params\': [p for n, p in model.named_parameters()\n                                                        if all([_k not in n for _k, _ in config.TRAIN.LR_MULT])]})\n\n        if config.TRAIN.OPTIMIZER == \'SGD\':\n            optimizer = optim.SGD(optimizer_grouped_parameters,\n                                  lr=config.TRAIN.LR * batch_size,\n                                  momentum=config.TRAIN.MOMENTUM,\n                                  weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'Adam\':\n            optimizer = optim.Adam(optimizer_grouped_parameters,\n                                   lr=config.TRAIN.LR * batch_size,\n                                   weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'AdamW\':\n            optimizer = AdamW(optimizer_grouped_parameters,\n                              lr=config.TRAIN.LR * batch_size,\n                              betas=(0.9, 0.999),\n                              eps=1e-6,\n                              weight_decay=config.TRAIN.WD,\n                              correct_bias=True)\n        else:\n            raise ValueError(\'Not support optimizer {}!\'.format(config.TRAIN.OPTIMIZER))\n\n    # partial load pretrain state dict\n    if config.NETWORK.PARTIAL_PRETRAIN != """":\n        pretrain_state_dict = torch.load(config.NETWORK.PARTIAL_PRETRAIN, map_location=lambda storage, loc: storage)[\'state_dict\']\n        prefix_change = [prefix_change.split(\'->\') for prefix_change in config.NETWORK.PARTIAL_PRETRAIN_PREFIX_CHANGES]\n        if len(prefix_change) > 0:\n            pretrain_state_dict_parsed = {}\n            for k, v in pretrain_state_dict.items():\n                no_match = True\n                for pretrain_prefix, new_prefix in prefix_change:\n                    if k.startswith(pretrain_prefix):\n                        k = new_prefix + k[len(pretrain_prefix):]\n                        pretrain_state_dict_parsed[k] = v\n                        no_match = False\n                        break\n                if no_match:\n                    pretrain_state_dict_parsed[k] = v\n            pretrain_state_dict = pretrain_state_dict_parsed\n        smart_partial_load_model_state_dict(model, pretrain_state_dict)\n\n    # metrics\n    metric_kwargs = {\'allreduce\': args.dist,\n                     \'num_replicas\': world_size if args.dist else 1}\n    train_metrics_list = []\n    val_metrics_list = []\n    if config.NETWORK.WITH_REL_LOSS:\n        train_metrics_list.append(pretrain_metrics.RelationshipAccuracy(**metric_kwargs))\n        val_metrics_list.append(pretrain_metrics.RelationshipAccuracy(**metric_kwargs))\n    if config.NETWORK.WITH_MLM_LOSS:\n        if config.MODULE == \'ResNetVLBERTForPretrainingMultitask\':\n            train_metrics_list.append(pretrain_metrics.MLMAccuracyWVC(**metric_kwargs))\n            train_metrics_list.append(pretrain_metrics.MLMAccuracyAUX(**metric_kwargs))\n            val_metrics_list.append(pretrain_metrics.MLMAccuracyWVC(**metric_kwargs))\n            val_metrics_list.append(pretrain_metrics.MLMAccuracyAUX(**metric_kwargs))\n        else:\n            train_metrics_list.append(pretrain_metrics.MLMAccuracy(**metric_kwargs))\n            val_metrics_list.append(pretrain_metrics.MLMAccuracy(**metric_kwargs))\n    if config.NETWORK.WITH_MVRC_LOSS:\n        train_metrics_list.append(pretrain_metrics.MVRCAccuracy(**metric_kwargs))\n        val_metrics_list.append(pretrain_metrics.MVRCAccuracy(**metric_kwargs))\n    for output_name, display_name in config.TRAIN.LOSS_LOGGERS:\n        train_metrics_list.append(pretrain_metrics.LossLogger(output_name, display_name=display_name, **metric_kwargs))\n        val_metrics_list.append(pretrain_metrics.LossLogger(output_name, display_name=display_name, **metric_kwargs))\n\n    train_metrics = CompositeEvalMetric()\n    val_metrics = CompositeEvalMetric()\n    for child_metric in train_metrics_list:\n        train_metrics.add(child_metric)\n    for child_metric in val_metrics_list:\n        val_metrics.add(child_metric)\n\n    # epoch end callbacks\n    epoch_end_callbacks = []\n    if (rank is None) or (rank == 0):\n        epoch_end_callbacks = [Checkpoint(model_prefix, config.CHECKPOINT_FREQUENT)]\n    host_metric_name = \'MLMAcc\' if not config.MODULE == \'ResNetVLBERTForPretrainingMultitask\' else \'MLMAccWVC\'\n    validation_monitor = ValidationMonitor(do_validation, val_loader, val_metrics,\n                                           host_metric_name=host_metric_name)\n\n    # optimizer initial lr before\n    for group in optimizer.param_groups:\n        group.setdefault(\'initial_lr\', group[\'lr\'])\n\n    # resume/auto-resume\n    if rank is None or rank == 0:\n        smart_resume(model, optimizer, validation_monitor, config, model_prefix, logger)\n    if args.dist:\n        begin_epoch = torch.tensor(config.TRAIN.BEGIN_EPOCH).cuda()\n        distributed.broadcast(begin_epoch, src=0)\n        config.TRAIN.BEGIN_EPOCH = begin_epoch.item()\n\n    # batch end callbacks\n    batch_size = len(config.GPUS.split(\',\')) * (sum(config.TRAIN.BATCH_IMAGES)\n                                                if isinstance(config.TRAIN.BATCH_IMAGES, list)\n                                                else config.TRAIN.BATCH_IMAGES)\n    batch_end_callbacks = [Speedometer(batch_size, config.LOG_FREQUENT,\n                                       batches_per_epoch=len(train_loader),\n                                       epochs=config.TRAIN.END_EPOCH - config.TRAIN.BEGIN_EPOCH)]\n\n    # setup lr step and lr scheduler\n    if config.TRAIN.LR_SCHEDULE == \'plateau\':\n        print(""Warning: not support resuming on plateau lr schedule!"")\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                                  mode=\'max\',\n                                                                  factor=config.TRAIN.LR_FACTOR,\n                                                                  patience=1,\n                                                                  verbose=True,\n                                                                  threshold=1e-4,\n                                                                  threshold_mode=\'rel\',\n                                                                  cooldown=2,\n                                                                  min_lr=0,\n                                                                  eps=1e-8)\n    elif config.TRAIN.LR_SCHEDULE == \'triangle\':\n        lr_scheduler = WarmupLinearSchedule(optimizer,\n                                            config.TRAIN.WARMUP_STEPS if config.TRAIN.WARMUP else 0,\n                                            t_total=int(config.TRAIN.END_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS),\n                                            last_epoch=int(config.TRAIN.BEGIN_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS)  - 1)\n    elif config.TRAIN.LR_SCHEDULE == \'step\':\n        lr_iters = [int(epoch * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS) for epoch in config.TRAIN.LR_STEP]\n        lr_scheduler = WarmupMultiStepLR(optimizer, milestones=lr_iters, gamma=config.TRAIN.LR_FACTOR,\n                                         warmup_factor=config.TRAIN.WARMUP_FACTOR,\n                                         warmup_iters=config.TRAIN.WARMUP_STEPS if config.TRAIN.WARMUP else 0,\n                                         warmup_method=config.TRAIN.WARMUP_METHOD,\n                                         last_epoch=int(config.TRAIN.BEGIN_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS)  - 1)\n    else:\n        raise ValueError(""Not support lr schedule: {}."".format(config.TRAIN.LR_SCHEDULE))\n\n    # broadcast parameter and optimizer state from rank 0 before training start\n    if args.dist:\n        for v in model.state_dict().values():\n            distributed.broadcast(v, src=0)\n        # for v in optimizer.state_dict().values():\n        #     distributed.broadcast(v, src=0)\n        best_epoch = torch.tensor(validation_monitor.best_epoch).cuda()\n        best_val = torch.tensor(validation_monitor.best_val).cuda()\n        distributed.broadcast(best_epoch, src=0)\n        distributed.broadcast(best_val, src=0)\n        validation_monitor.best_epoch = best_epoch.item()\n        validation_monitor.best_val = best_val.item()\n\n    # apex: amp fp16 mixed-precision training\n    if config.TRAIN.FP16:\n        # model.apply(bn_fp16_half_eval)\n        model, optimizer = amp.initialize(model, optimizer,\n                                          opt_level=\'O2\',\n                                          keep_batchnorm_fp32=False,\n                                          loss_scale=config.TRAIN.FP16_LOSS_SCALE,\n                                          max_loss_scale=128.0,\n                                          min_loss_scale=128.0)\n        if args.dist:\n            model = Apex_DDP(model, delay_allreduce=True)\n\n    train(model, optimizer, lr_scheduler, train_loader, train_sampler, train_metrics,\n          config.TRAIN.BEGIN_EPOCH, config.TRAIN.END_EPOCH, logger,\n          rank=rank, batch_end_callbacks=batch_end_callbacks, epoch_end_callbacks=epoch_end_callbacks,\n          writer=writer, validation_monitor=validation_monitor, fp16=config.TRAIN.FP16,\n          clip_grad_norm=config.TRAIN.CLIP_GRAD_NORM,\n          gradient_accumulate_steps=config.TRAIN.GRAD_ACCUMULATE_STEPS)\n\n    return rank, model\n'"
pretrain/function/val.py,1,"b'from collections import namedtuple\nimport torch\nfrom common.trainer import to_cuda\n\n\n@torch.no_grad()\ndef do_validation(net, val_loader, metrics, label_index_in_batch):\n    net.eval()\n    metrics.reset()\n    for nbatch, batch in enumerate(val_loader):\n        batch = to_cuda(batch)\n        outputs, _ = net(*batch)\n        metrics.update(outputs)\n\n'"
pretrain/function/vis.py,8,"b'import os\nimport pprint\nimport shutil\nimport inspect\nimport random\nimport math\n\nfrom tqdm import trange\nimport numpy as np\nimport torch\nimport torch.nn\nimport torch.distributed as distributed\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nfrom common.utils.load import smart_partial_load_model_state_dict\nfrom common.trainer import to_cuda\nfrom common.utils.multi_task_dataloader import MultiTaskDataLoader\nfrom pretrain.data.build import make_dataloader, make_dataloaders\nfrom pretrain.modules import *\nfrom common.utils.create_logger import makedirsExist\n\n\ndef vis_net(args, config, save_dir):\n    pprint.pprint(config)\n\n    if args.dist:\n        model = eval(config.MODULE)(config)\n        local_rank = int(os.environ.get(\'LOCAL_RANK\') or 0)\n        config.GPUS = str(local_rank)\n        torch.cuda.set_device(local_rank)\n        master_address = os.environ[\'MASTER_ADDR\']\n        master_port = int(os.environ[\'MASTER_PORT\'] or 23456)\n        world_size = int(os.environ[\'WORLD_SIZE\'] or 1)\n        rank = int(os.environ[\'RANK\'] or 0)\n        if args.slurm:\n            distributed.init_process_group(backend=\'nccl\')\n        else:\n            distributed.init_process_group(\n                backend=\'nccl\',\n                init_method=\'tcp://{}:{}\'.format(master_address, master_port),\n                world_size=world_size,\n                rank=rank,\n                group_name=\'mtorch\')\n        print(f\'native distributed, size: {world_size}, rank: {rank}, local rank: {local_rank}\')\n        torch.cuda.set_device(local_rank)\n        config.GPUS = str(local_rank)\n        model = model.cuda()\n        model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n\n        if isinstance(config.DATASET, list):\n            val_loaders = make_dataloaders(config,\n                                           mode=\'val\',\n                                           distributed=True,\n                                           num_replicas=world_size,\n                                           rank=rank)\n            val_loader = MultiTaskDataLoader(val_loaders)\n        else:\n            val_loader = make_dataloader(config,\n                                         mode=\'val\',\n                                         distributed=True,\n                                         num_replicas=world_size,\n                                         rank=rank)\n    else:\n        model = eval(config.MODULE)(config)\n        num_gpus = len(config.GPUS.split(\',\'))\n        rank = None\n        # model\n        if num_gpus > 1:\n            model = torch.nn.DataParallel(model, device_ids=[int(d) for d in config.GPUS.split(\',\')]).cuda()\n        else:\n            torch.cuda.set_device(int(config.GPUS))\n            model.cuda()\n\n        # loader\n        if isinstance(config.DATASET, list):\n            val_loaders = make_dataloaders(config, mode=\'val\', distributed=False)\n            val_loader = MultiTaskDataLoader(val_loaders)\n        else:\n            val_loader = make_dataloader(config, mode=\'val\', distributed=False)\n\n    # partial load pretrain state dict\n    if config.NETWORK.PARTIAL_PRETRAIN != """":\n        pretrain_state_dict = torch.load(config.NETWORK.PARTIAL_PRETRAIN, map_location=lambda storage, loc: storage)[\'state_dict\']\n        prefix_change = [prefix_change.split(\'->\') for prefix_change in config.NETWORK.PARTIAL_PRETRAIN_PREFIX_CHANGES]\n        if len(prefix_change) > 0:\n            pretrain_state_dict_parsed = {}\n            for k, v in pretrain_state_dict.items():\n                no_match = True\n                for pretrain_prefix, new_prefix in prefix_change:\n                    if k.startswith(pretrain_prefix):\n                        k = new_prefix + k[len(pretrain_prefix):]\n                        pretrain_state_dict_parsed[k] = v\n                        no_match = False\n                        break\n                if no_match:\n                    pretrain_state_dict_parsed[k] = v\n            pretrain_state_dict = pretrain_state_dict_parsed\n        smart_partial_load_model_state_dict(model, pretrain_state_dict)\n\n    # broadcast parameter and optimizer state from rank 0 before training start\n    if args.dist:\n        for v in model.state_dict().values():\n            distributed.broadcast(v, src=0)\n\n    vis(model, val_loader, save_dir, rank=rank, world_size=world_size if args.dist else 1)\n\n    return rank, model\n\n\ndef vis(model, loader, save_dir, rank=None, world_size=1):\n    attention_dir = os.path.join(save_dir, \'attention_probs\')\n    hidden_dir = os.path.join(save_dir, \'hidden_states\')\n    cos_dir = os.path.join(save_dir, \'cos_similarity\')\n    # if not os.path.exists(hidden_dir):\n    #     makedirsExist(hidden_dir)\n    # if not os.path.exists(cos_dir):\n    #     makedirsExist(cos_dir)\n    if not os.path.exists(attention_dir):\n        makedirsExist(attention_dir)\n    # offset = 0\n    # if rank is not None:\n    #     num_samples = int(math.ceil(len(loader.dataset) * 1.0 / world_size))\n    #     offset = num_samples * rank\n    # index = offset\n    model.eval()\n    for i, data in zip(trange(len(loader)), loader):\n    # for i, data in enumerate(loader):\n        data = to_cuda(data)\n        output = model(*data)\n        for _i, (attention_probs, hidden_states) in enumerate(zip(output[\'attention_probs\'], output[\'hidden_states\'])):\n            index = int(data[2][_i][-1])\n            if hasattr(loader.dataset, \'ids\'):\n                image_id = loader.dataset.ids[index]\n            else:\n                image_id = loader.dataset.database[index][\'image\'].split(\'/\')[1].split(\'.\')[0]\n            attention_probs_arr = attention_probs.detach().cpu().numpy()\n            hidden_states_arr = hidden_states.detach().cpu().numpy()\n            cos_similarity_arr = (hidden_states @ hidden_states.transpose(1, 2)).detach().cpu().numpy()\n            np.save(os.path.join(attention_dir, \'{}.npy\'.format(image_id)), attention_probs_arr)\n            # np.save(os.path.join(hidden_dir, \'{}.npy\'.format(image_id)), hidden_states_arr)\n            # np.save(os.path.join(cos_dir, \'{}.npy\'.format(image_id)), cos_similarity_arr)\n            # index = (index + 1) % len(loader.dataset)\n\n\n\n'"
pretrain/modules/resnet_vlbert_for_attention_vis.py,7,"b'import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom external.pytorch_pretrained_bert import BertTokenizer\nfrom common.module import Module\nfrom common.fast_rcnn import FastRCNN\nfrom common.visual_linguistic_bert import VisualLinguisticBert\nfrom common.utils.misc import soft_cross_entropy\n\nBERT_WEIGHTS_NAME = \'pytorch_model.bin\'\n\n\nclass ResNetVLBERTForAttentionVis(Module):\n    def __init__(self, config):\n\n        super(ResNetVLBERTForAttentionVis, self).__init__(config)\n\n        self.image_feature_extractor = FastRCNN(config,\n                                                average_pool=True,\n                                                final_dim=config.NETWORK.IMAGE_FINAL_DIM,\n                                                enable_cnn_reg_loss=False)\n        self.object_linguistic_embeddings = nn.Embedding(1, config.NETWORK.VLBERT.hidden_size)\n        if config.NETWORK.IMAGE_FEAT_PRECOMPUTED or (not config.NETWORK.MASK_RAW_PIXELS):\n            self.object_mask_visual_embedding = nn.Embedding(1, 2048)\n        if config.NETWORK.WITH_MVRC_LOSS:\n            self.object_mask_word_embedding = nn.Embedding(1, config.NETWORK.VLBERT.hidden_size)\n        self.aux_text_visual_embedding = nn.Embedding(1, config.NETWORK.VLBERT.hidden_size)\n        self.image_feature_bn_eval = config.NETWORK.IMAGE_FROZEN_BN\n        self.tokenizer = BertTokenizer.from_pretrained(config.NETWORK.BERT_MODEL_NAME)\n        language_pretrained_model_path = None\n        if config.NETWORK.BERT_PRETRAINED != \'\':\n            language_pretrained_model_path = \'{}-{:04d}.model\'.format(config.NETWORK.BERT_PRETRAINED,\n                                                                      config.NETWORK.BERT_PRETRAINED_EPOCH)\n        elif os.path.isdir(config.NETWORK.BERT_MODEL_NAME):\n            weight_path = os.path.join(config.NETWORK.BERT_MODEL_NAME, BERT_WEIGHTS_NAME)\n            if os.path.isfile(weight_path):\n                language_pretrained_model_path = weight_path\n\n        if language_pretrained_model_path is None:\n            print(""Warning: no pretrained language model found, training from scratch!!!"")\n\n        self.vlbert = VisualLinguisticBert(\n            config.NETWORK.VLBERT,\n            language_pretrained_model_path=None if config.NETWORK.VLBERT.from_scratch else language_pretrained_model_path\n        )\n\n        # init weights\n        self.init_weight()\n\n        self.fix_params()\n\n    def init_weight(self):\n        if self.config.NETWORK.IMAGE_FEAT_PRECOMPUTED or (not self.config.NETWORK.MASK_RAW_PIXELS):\n            self.object_mask_visual_embedding.weight.data.fill_(0.0)\n        if self.config.NETWORK.WITH_MVRC_LOSS:\n            self.object_mask_word_embedding.weight.data.normal_(mean=0.0, std=self.config.NETWORK.VLBERT.initializer_range)\n        self.aux_text_visual_embedding.weight.data.normal_(mean=0.0, std=self.config.NETWORK.VLBERT.initializer_range)\n        self.image_feature_extractor.init_weight()\n        if self.object_linguistic_embeddings is not None:\n            self.object_linguistic_embeddings.weight.data.normal_(mean=0.0,\n                                                                  std=self.config.NETWORK.VLBERT.initializer_range)\n\n    def train(self, mode=True):\n        super(ResNetVLBERTForAttentionVis, self).train(mode)\n        # turn some frozen layers to eval mode\n        if self.image_feature_bn_eval:\n            self.image_feature_extractor.bn_eval()\n\n    def fix_params(self):\n        pass\n\n    def _collect_obj_reps(self, span_tags, object_reps):\n        """"""\n        Collect span-level object representations\n        :param span_tags: [batch_size, ..leading_dims.., L]\n        :param object_reps: [batch_size, max_num_objs_per_batch, obj_dim]\n        :return:\n        """"""\n\n        span_tags_fixed = torch.clamp(span_tags, min=0)  # In case there were masked values here\n        row_id = span_tags_fixed.new_zeros(span_tags_fixed.shape)\n        row_id_broadcaster = torch.arange(0, row_id.shape[0], step=1, device=row_id.device)[:, None]\n\n        # Add extra diminsions to the row broadcaster so it matches row_id\n        leading_dims = len(span_tags.shape) - 2\n        for i in range(leading_dims):\n            row_id_broadcaster = row_id_broadcaster[..., None]\n        row_id += row_id_broadcaster\n        return object_reps[row_id.view(-1), span_tags_fixed.view(-1)].view(*span_tags_fixed.shape, -1)\n\n    def forward(self,\n                image,\n                boxes,\n                im_info,\n                text,\n                relationship_label,\n                mlm_labels,\n                mvrc_ops,\n                mvrc_labels,\n                *aux):\n\n        # concat aux texts from different dataset\n        # assert len(aux) > 0 and len(aux) % 2 == 0\n        aux_text_list = aux[0::2]\n        aux_text_mlm_labels_list = aux[1::2]\n        num_aux_text = sum([_text.shape[0] for _text in aux_text_list])\n        max_aux_text_len = max([_text.shape[1] for _text in aux_text_list]) if len(aux_text_list) > 0 else 0\n        aux_text = text.new_zeros((num_aux_text, max_aux_text_len))\n        aux_text_mlm_labels = mlm_labels.new_zeros((num_aux_text, max_aux_text_len)).fill_(-1)\n        _cur = 0\n        for _text, _mlm_labels in zip(aux_text_list, aux_text_mlm_labels_list):\n            _num = _text.shape[0]\n            aux_text[_cur:(_cur + _num), :_text.shape[1]] = _text\n            aux_text_mlm_labels[_cur:(_cur + _num), :_text.shape[1]] = _mlm_labels\n            _cur += _num\n\n        ###########################################\n\n        # visual feature extraction\n        images = image\n        box_mask = (boxes[:, :, 0] > -1.5)\n        origin_len = boxes.shape[1]\n        max_len = int(box_mask.sum(1).max().item())\n        box_mask = box_mask[:, :max_len]\n        boxes = boxes[:, :max_len]\n        mvrc_ops = mvrc_ops[:, :max_len]\n        mvrc_labels = mvrc_labels[:, :max_len]\n\n        if self.config.NETWORK.IMAGE_FEAT_PRECOMPUTED:\n            box_features = boxes[:, :, 4:]\n            box_features[mvrc_ops == 1] = self.object_mask_visual_embedding.weight[0]\n            boxes[:, :, 4:] = box_features\n\n        obj_reps = self.image_feature_extractor(images=images,\n                                                boxes=boxes,\n                                                box_mask=box_mask,\n                                                im_info=im_info,\n                                                classes=None,\n                                                segms=None,\n                                                mvrc_ops=mvrc_ops,\n                                                mask_visual_embed=self.object_mask_visual_embedding.weight[0]\n                                                if (not self.config.NETWORK.IMAGE_FEAT_PRECOMPUTED)\n                                                   and (not self.config.NETWORK.MASK_RAW_PIXELS)\n                                                else None)\n\n        ############################################\n\n        # prepare text\n        text_input_ids = text\n        text_tags = text.new_zeros(text.shape)\n        text_visual_embeddings = self._collect_obj_reps(text_tags, obj_reps[\'obj_reps\'])\n\n        object_linguistic_embeddings = self.object_linguistic_embeddings(\n            boxes.new_zeros((boxes.shape[0], boxes.shape[1])).long()\n        )\n        if self.config.NETWORK.WITH_MVRC_LOSS:\n            object_linguistic_embeddings[mvrc_ops == 1] = self.object_mask_word_embedding.weight[0]\n        object_vl_embeddings = torch.cat((obj_reps[\'obj_reps\'], object_linguistic_embeddings), -1)\n\n        # add auxiliary text\n        max_text_len = max(text_input_ids.shape[1], aux_text.shape[1])\n        text_input_ids_multi = text_input_ids.new_zeros((text_input_ids.shape[0] + aux_text.shape[0], max_text_len))\n        text_input_ids_multi[:text_input_ids.shape[0], :text_input_ids.shape[1]] = text_input_ids\n        text_input_ids_multi[text_input_ids.shape[0]:, :aux_text.shape[1]] = aux_text\n        text_token_type_ids_multi = text_input_ids_multi.new_zeros(text_input_ids_multi.shape)\n        text_mask_multi = (text_input_ids_multi > 0)\n        text_visual_embeddings_multi = text_visual_embeddings.new_zeros((text_input_ids.shape[0] + aux_text.shape[0],\n                                                                         max_text_len,\n                                                                         text_visual_embeddings.shape[-1]))\n        text_visual_embeddings_multi[:text_visual_embeddings.shape[0], :text_visual_embeddings.shape[1]] \\\n            = text_visual_embeddings\n        text_visual_embeddings_multi[text_visual_embeddings.shape[0]:] = self.aux_text_visual_embedding.weight[0]\n        object_vl_embeddings_multi = object_vl_embeddings.new_zeros((text_input_ids.shape[0] + aux_text.shape[0],\n                                                                     *object_vl_embeddings.shape[1:]))\n        object_vl_embeddings_multi[:object_vl_embeddings.shape[0]] = object_vl_embeddings\n        box_mask_multi = box_mask.new_zeros((text_input_ids.shape[0] + aux_text.shape[0], *box_mask.shape[1:]))\n        box_mask_multi[:box_mask.shape[0]] = box_mask\n\n\n        ###########################################\n\n        # Visual Linguistic BERT\n\n        encoder_layers, _, attention_probs = self.vlbert(text_input_ids_multi,\n                                                         text_token_type_ids_multi,\n                                                         text_visual_embeddings_multi,\n                                                         text_mask_multi,\n                                                         object_vl_embeddings_multi,\n                                                         box_mask_multi,\n                                                         output_all_encoded_layers=True,\n                                                         output_attention_probs=True)\n        hidden_states = torch.stack(encoder_layers, dim=0).transpose(0, 1).contiguous()\n        attention_probs = torch.stack(attention_probs, dim=0).transpose(0, 1).contiguous()\n\n        return {\'attention_probs\': attention_probs,\n                \'hidden_states\': hidden_states}\n\n\n\n\n'"
pretrain/modules/resnet_vlbert_for_pretraining.py,5,"b'import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom external.pytorch_pretrained_bert import BertTokenizer\nfrom common.module import Module\nfrom common.fast_rcnn import FastRCNN\nfrom common.visual_linguistic_bert import VisualLinguisticBertForPretraining\nfrom common.utils.misc import soft_cross_entropy\n\nBERT_WEIGHTS_NAME = \'pytorch_model.bin\'\n\n\nclass ResNetVLBERTForPretraining(Module):\n    def __init__(self, config):\n\n        super(ResNetVLBERTForPretraining, self).__init__(config)\n\n        self.image_feature_extractor = FastRCNN(config,\n                                                average_pool=True,\n                                                final_dim=config.NETWORK.IMAGE_FINAL_DIM,\n                                                enable_cnn_reg_loss=False)\n        self.object_linguistic_embeddings = nn.Embedding(1, config.NETWORK.VLBERT.hidden_size)\n        if config.NETWORK.IMAGE_FEAT_PRECOMPUTED:\n            self.object_mask_visual_embedding = nn.Embedding(1, 2048)\n        if config.NETWORK.WITH_MVRC_LOSS:\n            self.object_mask_word_embedding = nn.Embedding(1, config.NETWORK.VLBERT.hidden_size)\n        self.image_feature_bn_eval = config.NETWORK.IMAGE_FROZEN_BN\n        self.tokenizer = BertTokenizer.from_pretrained(config.NETWORK.BERT_MODEL_NAME)\n        language_pretrained_model_path = None\n        if config.NETWORK.BERT_PRETRAINED != \'\':\n            language_pretrained_model_path = \'{}-{:04d}.model\'.format(config.NETWORK.BERT_PRETRAINED,\n                                                                      config.NETWORK.BERT_PRETRAINED_EPOCH)\n        elif os.path.isdir(config.NETWORK.BERT_MODEL_NAME):\n            weight_path = os.path.join(config.NETWORK.BERT_MODEL_NAME, BERT_WEIGHTS_NAME)\n            if os.path.isfile(weight_path):\n                language_pretrained_model_path = weight_path\n\n        if language_pretrained_model_path is None:\n            print(""Warning: no pretrained language model found, training from scratch!!!"")\n\n        self.vlbert = VisualLinguisticBertForPretraining(\n            config.NETWORK.VLBERT,\n            language_pretrained_model_path=None if config.NETWORK.VLBERT.from_scratch else language_pretrained_model_path,\n            with_rel_head=config.NETWORK.WITH_REL_LOSS,\n            with_mlm_head=config.NETWORK.WITH_MLM_LOSS,\n            with_mvrc_head=config.NETWORK.WITH_MVRC_LOSS,\n        )\n\n        # init weights\n        self.init_weight()\n\n        self.fix_params()\n\n    def init_weight(self):\n        if self.config.NETWORK.IMAGE_FEAT_PRECOMPUTED:\n            self.object_mask_visual_embedding.weight.data.fill_(0.0)\n        if self.config.NETWORK.WITH_MVRC_LOSS:\n            self.object_mask_word_embedding.weight.data.normal_(mean=0.0, std=self.config.NETWORK.VLBERT.initializer_range)\n        self.image_feature_extractor.init_weight()\n        if self.object_linguistic_embeddings is not None:\n            self.object_linguistic_embeddings.weight.data.normal_(mean=0.0,\n                                                                  std=self.config.NETWORK.VLBERT.initializer_range)\n\n    def train(self, mode=True):\n        super(ResNetVLBERTForPretraining, self).train(mode)\n        # turn some frozen layers to eval mode\n        if self.image_feature_bn_eval:\n            self.image_feature_extractor.bn_eval()\n\n    def fix_params(self):\n        pass\n\n    def _collect_obj_reps(self, span_tags, object_reps):\n        """"""\n        Collect span-level object representations\n        :param span_tags: [batch_size, ..leading_dims.., L]\n        :param object_reps: [batch_size, max_num_objs_per_batch, obj_dim]\n        :return:\n        """"""\n\n        span_tags_fixed = torch.clamp(span_tags, min=0)  # In case there were masked values here\n        row_id = span_tags_fixed.new_zeros(span_tags_fixed.shape)\n        row_id_broadcaster = torch.arange(0, row_id.shape[0], step=1, device=row_id.device)[:, None]\n\n        # Add extra diminsions to the row broadcaster so it matches row_id\n        leading_dims = len(span_tags.shape) - 2\n        for i in range(leading_dims):\n            row_id_broadcaster = row_id_broadcaster[..., None]\n        row_id += row_id_broadcaster\n        return object_reps[row_id.view(-1), span_tags_fixed.view(-1)].view(*span_tags_fixed.shape, -1)\n\n    def forward(self,\n                image,\n                boxes,\n                im_info,\n                text,\n                relationship_label,\n                mlm_labels,\n                mvrc_ops,\n                mvrc_labels):\n        ###########################################\n\n        # visual feature extraction\n        images = image\n        box_mask = (boxes[:, :, 0] > -1.5)\n        origin_len = boxes.shape[1]\n        max_len = int(box_mask.sum(1).max().item())\n        box_mask = box_mask[:, :max_len]\n        boxes = boxes[:, :max_len]\n        mvrc_ops = mvrc_ops[:, :max_len]\n        mvrc_labels = mvrc_labels[:, :max_len]\n\n        if self.config.NETWORK.IMAGE_FEAT_PRECOMPUTED:\n            box_features = boxes[:, :, 4:]\n            box_features[mvrc_ops == 1] = self.object_mask_visual_embedding.weight[0]\n            boxes[:, :, 4:] = box_features\n\n        obj_reps = self.image_feature_extractor(images=images,\n                                                boxes=boxes,\n                                                box_mask=box_mask,\n                                                im_info=im_info,\n                                                classes=None,\n                                                segms=None,\n                                                mvrc_ops=mvrc_ops,\n                                                mask_visual_embed=None)\n\n        ############################################\n\n        # prepare text\n        text_input_ids = text\n        text_tags = text.new_zeros(text.shape)\n        text_token_type_ids = text.new_zeros(text.shape)\n        text_mask = (text_input_ids > 0)\n        text_visual_embeddings = self._collect_obj_reps(text_tags, obj_reps[\'obj_reps\'])\n\n        object_linguistic_embeddings = self.object_linguistic_embeddings(\n            boxes.new_zeros((boxes.shape[0], boxes.shape[1])).long()\n        )\n        if self.config.NETWORK.WITH_MVRC_LOSS:\n            object_linguistic_embeddings[mvrc_ops == 1] = self.object_mask_word_embedding.weight[0]\n        object_vl_embeddings = torch.cat((obj_reps[\'obj_reps\'], object_linguistic_embeddings), -1)\n\n        ###########################################\n\n        # Visual Linguistic BERT\n\n        relationship_logits, mlm_logits, mvrc_logits = self.vlbert(text_input_ids,\n                                                                   text_token_type_ids,\n                                                                   text_visual_embeddings,\n                                                                   text_mask,\n                                                                   object_vl_embeddings,\n                                                                   box_mask)\n\n        ###########################################\n        outputs = {}\n\n        # loss\n        relationship_loss = im_info.new_zeros(())\n        mlm_loss = im_info.new_zeros(())\n        mvrc_loss = im_info.new_zeros(())\n        if self.config.NETWORK.WITH_REL_LOSS:\n            relationship_loss = F.cross_entropy(relationship_logits, relationship_label)\n        if self.config.NETWORK.WITH_MLM_LOSS:\n            mlm_logits_padded = mlm_logits.new_zeros((*mlm_labels.shape, mlm_logits.shape[-1])).fill_(-10000.0)\n            mlm_logits_padded[:, :mlm_logits.shape[1]] = mlm_logits\n            mlm_logits = mlm_logits_padded\n            if self.config.NETWORK.MLM_LOSS_NORM_IN_BATCH_FIRST:\n                mlm_loss = F.cross_entropy(mlm_logits.transpose(1, 2),\n                                           mlm_labels,\n                                           ignore_index=-1, reduction=\'none\')\n                num_mlm = (mlm_labels != -1).sum(1, keepdim=True).to(dtype=mlm_loss.dtype)\n                num_has_mlm = (num_mlm != 0).sum().to(dtype=mlm_loss.dtype)\n                mlm_loss = (mlm_loss / (num_mlm + 1e-4)).sum() / (num_has_mlm + 1e-4)\n            else:\n                mlm_loss = F.cross_entropy(mlm_logits.view((-1, mlm_logits.shape[-1])),\n                                           mlm_labels.view(-1),\n                                           ignore_index=-1)\n        # mvrc_loss = F.cross_entropy(mvrc_logits.contiguous().view(-1, mvrc_logits.shape[-1]),\n        #                             mvrc_labels.contiguous().view(-1),\n        #                             ignore_index=-1)\n        if self.config.NETWORK.WITH_MVRC_LOSS:\n            if self.config.NETWORK.MVRC_LOSS_NORM_IN_BATCH_FIRST:\n                mvrc_loss = soft_cross_entropy(\n                    mvrc_logits.contiguous().view(-1, mvrc_logits.shape[-1]),\n                    mvrc_labels.contiguous().view(-1, mvrc_logits.shape[-1]),\n                    reduction=\'none\').view(mvrc_logits.shape[:-1])\n                valid = (mvrc_labels.sum(-1) - 1).abs() < 1.0e-1\n                mvrc_loss = (mvrc_loss / (valid.sum(1, keepdim=True).to(dtype=mvrc_loss.dtype) + 1e-4)) \\\n                                .sum() / ((valid.sum(1) != 0).sum().to(dtype=mvrc_loss.dtype) + 1e-4)\n            else:\n                mvrc_loss = soft_cross_entropy(mvrc_logits.contiguous().view(-1, mvrc_logits.shape[-1]),\n                                               mvrc_labels.contiguous().view(-1, mvrc_logits.shape[-1]))\n\n            mvrc_logits_padded = mvrc_logits.new_zeros((mvrc_logits.shape[0], origin_len, mvrc_logits.shape[2])).fill_(-10000.0)\n            mvrc_logits_padded[:, :mvrc_logits.shape[1]] = mvrc_logits\n            mvrc_logits = mvrc_logits_padded\n            mvrc_labels_padded = mvrc_labels.new_zeros((mvrc_labels.shape[0], origin_len, mvrc_labels.shape[2])).fill_(0.0)\n            mvrc_labels_padded[:, :mvrc_labels.shape[1]] = mvrc_labels\n            mvrc_labels = mvrc_labels_padded\n\n        outputs.update({\n            \'relationship_logits\': relationship_logits if self.config.NETWORK.WITH_REL_LOSS else None,\n            \'relationship_label\': relationship_label if self.config.NETWORK.WITH_REL_LOSS else None,\n            \'mlm_logits\': mlm_logits if self.config.NETWORK.WITH_MLM_LOSS else None,\n            \'mlm_label\': mlm_labels if self.config.NETWORK.WITH_MLM_LOSS else None,\n            \'mvrc_logits\': mvrc_logits if self.config.NETWORK.WITH_MVRC_LOSS else None,\n            \'mvrc_label\': mvrc_labels if self.config.NETWORK.WITH_MVRC_LOSS else None,\n            \'relationship_loss\': relationship_loss,\n            \'mlm_loss\': mlm_loss,\n            \'mvrc_loss\': mvrc_loss,\n        })\n\n        loss = relationship_loss.mean() + mlm_loss.mean() + mvrc_loss.mean()\n\n        return outputs, loss\n\n'"
pretrain/modules/resnet_vlbert_for_pretraining_multitask.py,5,"b'import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom external.pytorch_pretrained_bert import BertTokenizer\nfrom common.module import Module\nfrom common.fast_rcnn import FastRCNN\nfrom common.visual_linguistic_bert import VisualLinguisticBertForPretraining\nfrom common.utils.misc import soft_cross_entropy\n\nBERT_WEIGHTS_NAME = \'pytorch_model.bin\'\n\n\nclass ResNetVLBERTForPretrainingMultitask(Module):\n    def __init__(self, config):\n\n        super(ResNetVLBERTForPretrainingMultitask, self).__init__(config)\n\n        self.image_feature_extractor = FastRCNN(config,\n                                                average_pool=True,\n                                                final_dim=config.NETWORK.IMAGE_FINAL_DIM,\n                                                enable_cnn_reg_loss=False)\n        self.object_linguistic_embeddings = nn.Embedding(1, config.NETWORK.VLBERT.hidden_size)\n        if config.NETWORK.IMAGE_FEAT_PRECOMPUTED or (not config.NETWORK.MASK_RAW_PIXELS):\n            self.object_mask_visual_embedding = nn.Embedding(1, 2048)\n        if config.NETWORK.WITH_MVRC_LOSS:\n            self.object_mask_word_embedding = nn.Embedding(1, config.NETWORK.VLBERT.hidden_size)\n        self.aux_text_visual_embedding = nn.Embedding(1, config.NETWORK.VLBERT.hidden_size)\n        self.image_feature_bn_eval = config.NETWORK.IMAGE_FROZEN_BN\n        self.tokenizer = BertTokenizer.from_pretrained(config.NETWORK.BERT_MODEL_NAME)\n        language_pretrained_model_path = None\n        if config.NETWORK.BERT_PRETRAINED != \'\':\n            language_pretrained_model_path = \'{}-{:04d}.model\'.format(config.NETWORK.BERT_PRETRAINED,\n                                                                      config.NETWORK.BERT_PRETRAINED_EPOCH)\n        elif os.path.isdir(config.NETWORK.BERT_MODEL_NAME):\n            weight_path = os.path.join(config.NETWORK.BERT_MODEL_NAME, BERT_WEIGHTS_NAME)\n            if os.path.isfile(weight_path):\n                language_pretrained_model_path = weight_path\n\n        if language_pretrained_model_path is None:\n            print(""Warning: no pretrained language model found, training from scratch!!!"")\n\n        self.vlbert = VisualLinguisticBertForPretraining(\n            config.NETWORK.VLBERT,\n            language_pretrained_model_path=None if config.NETWORK.VLBERT.from_scratch else language_pretrained_model_path,\n            with_rel_head=config.NETWORK.WITH_REL_LOSS,\n            with_mlm_head=config.NETWORK.WITH_MLM_LOSS,\n            with_mvrc_head=config.NETWORK.WITH_MVRC_LOSS,\n        )\n\n        # init weights\n        self.init_weight()\n\n        self.fix_params()\n\n    def init_weight(self):\n        if self.config.NETWORK.IMAGE_FEAT_PRECOMPUTED or (not self.config.NETWORK.MASK_RAW_PIXELS):\n            self.object_mask_visual_embedding.weight.data.fill_(0.0)\n        if self.config.NETWORK.WITH_MVRC_LOSS:\n            self.object_mask_word_embedding.weight.data.normal_(mean=0.0,\n                                                                std=self.config.NETWORK.VLBERT.initializer_range)\n        self.aux_text_visual_embedding.weight.data.normal_(mean=0.0, std=self.config.NETWORK.VLBERT.initializer_range)\n        self.image_feature_extractor.init_weight()\n        if self.object_linguistic_embeddings is not None:\n            self.object_linguistic_embeddings.weight.data.normal_(mean=0.0,\n                                                                  std=self.config.NETWORK.VLBERT.initializer_range)\n\n    def train(self, mode=True):\n        super(ResNetVLBERTForPretrainingMultitask, self).train(mode)\n        # turn some frozen layers to eval mode\n        if self.image_feature_bn_eval:\n            self.image_feature_extractor.bn_eval()\n\n    def fix_params(self):\n        pass\n\n    def _collect_obj_reps(self, span_tags, object_reps):\n        """"""\n        Collect span-level object representations\n        :param span_tags: [batch_size, ..leading_dims.., L]\n        :param object_reps: [batch_size, max_num_objs_per_batch, obj_dim]\n        :return:\n        """"""\n\n        span_tags_fixed = torch.clamp(span_tags, min=0)  # In case there were masked values here\n        row_id = span_tags_fixed.new_zeros(span_tags_fixed.shape)\n        row_id_broadcaster = torch.arange(0, row_id.shape[0], step=1, device=row_id.device)[:, None]\n\n        # Add extra diminsions to the row broadcaster so it matches row_id\n        leading_dims = len(span_tags.shape) - 2\n        for i in range(leading_dims):\n            row_id_broadcaster = row_id_broadcaster[..., None]\n        row_id += row_id_broadcaster\n        return object_reps[row_id.view(-1), span_tags_fixed.view(-1)].view(*span_tags_fixed.shape, -1)\n\n    def forward(self,\n                image,\n                boxes,\n                im_info,\n                text,\n                relationship_label,\n                mlm_labels,\n                mvrc_ops,\n                mvrc_labels,\n                *aux):\n\n        # concat aux texts from different dataset\n        assert len(aux) > 0 and len(aux) % 2 == 0\n        aux_text_list = aux[0::2]\n        aux_text_mlm_labels_list = aux[1::2]\n        num_aux_text = sum([_text.shape[0] for _text in aux_text_list])\n        max_aux_text_len = max([_text.shape[1] for _text in aux_text_list])\n        aux_text = aux_text_list[0].new_zeros((num_aux_text, max_aux_text_len))\n        aux_text_mlm_labels = aux_text_mlm_labels_list[0].new_zeros((num_aux_text, max_aux_text_len)).fill_(-1)\n        _cur = 0\n        for _text, _mlm_labels in zip(aux_text_list, aux_text_mlm_labels_list):\n            _num = _text.shape[0]\n            aux_text[_cur:(_cur + _num), :_text.shape[1]] = _text\n            aux_text_mlm_labels[_cur:(_cur + _num), :_text.shape[1]] = _mlm_labels\n            _cur += _num\n\n        ###########################################\n\n        # visual feature extraction\n        images = image\n        box_mask = (boxes[:, :, 0] > -1.5)\n        origin_len = boxes.shape[1]\n        max_len = int(box_mask.sum(1).max().item())\n        box_mask = box_mask[:, :max_len]\n        boxes = boxes[:, :max_len]\n        mvrc_ops = mvrc_ops[:, :max_len]\n        mvrc_labels = mvrc_labels[:, :max_len]\n\n        if self.config.NETWORK.IMAGE_FEAT_PRECOMPUTED:\n            box_features = boxes[:, :, 4:]\n            box_features[mvrc_ops == 1] = self.object_mask_visual_embedding.weight[0]\n            boxes[:, :, 4:] = box_features\n\n        obj_reps = self.image_feature_extractor(images=images,\n                                                boxes=boxes,\n                                                box_mask=box_mask,\n                                                im_info=im_info,\n                                                classes=None,\n                                                segms=None,\n                                                mvrc_ops=mvrc_ops,\n                                                mask_visual_embed=self.object_mask_visual_embedding.weight[0]\n                                                if (not self.config.NETWORK.IMAGE_FEAT_PRECOMPUTED)\n                                                   and (not self.config.NETWORK.MASK_RAW_PIXELS)\n                                                else None)\n\n        ############################################\n\n        # prepare text\n        text_input_ids = text\n        text_tags = text.new_zeros(text.shape)\n        text_visual_embeddings = self._collect_obj_reps(text_tags, obj_reps[\'obj_reps\'])\n\n        object_linguistic_embeddings = self.object_linguistic_embeddings(\n            boxes.new_zeros((boxes.shape[0], boxes.shape[1])).long()\n        )\n        if self.config.NETWORK.WITH_MVRC_LOSS:\n            object_linguistic_embeddings[mvrc_ops == 1] = self.object_mask_word_embedding.weight[0]\n        object_vl_embeddings = torch.cat((obj_reps[\'obj_reps\'], object_linguistic_embeddings), -1)\n\n        # add auxiliary text\n        max_text_len = max(text_input_ids.shape[1], aux_text.shape[1])\n        text_input_ids_multi = text_input_ids.new_zeros((text_input_ids.shape[0] + aux_text.shape[0], max_text_len))\n        text_input_ids_multi[:text_input_ids.shape[0], :text_input_ids.shape[1]] = text_input_ids\n        text_input_ids_multi[text_input_ids.shape[0]:, :aux_text.shape[1]] = aux_text\n        text_token_type_ids_multi = text_input_ids_multi.new_zeros(text_input_ids_multi.shape)\n        text_mask_multi = (text_input_ids_multi > 0)\n        text_visual_embeddings_multi = text_visual_embeddings.new_zeros((text_input_ids.shape[0] + aux_text.shape[0],\n                                                                         max_text_len,\n                                                                         text_visual_embeddings.shape[-1]))\n        text_visual_embeddings_multi[:text_visual_embeddings.shape[0], :text_visual_embeddings.shape[1]] \\\n            = text_visual_embeddings\n        text_visual_embeddings_multi[text_visual_embeddings.shape[0]:] = self.aux_text_visual_embedding.weight[0]\n        object_vl_embeddings_multi = object_vl_embeddings.new_zeros((text_input_ids.shape[0] + aux_text.shape[0],\n                                                                     *object_vl_embeddings.shape[1:]))\n        object_vl_embeddings_multi[:object_vl_embeddings.shape[0]] = object_vl_embeddings\n        box_mask_multi = box_mask.new_zeros((text_input_ids.shape[0] + aux_text.shape[0], *box_mask.shape[1:]))\n        box_mask_multi[:box_mask.shape[0]] = box_mask\n\n        ###########################################\n\n        # Visual Linguistic BERT\n\n        relationship_logits_multi, mlm_logits_multi, mvrc_logits_multi = self.vlbert(text_input_ids_multi,\n                                                                                     text_token_type_ids_multi,\n                                                                                     text_visual_embeddings_multi,\n                                                                                     text_mask_multi,\n                                                                                     object_vl_embeddings_multi,\n                                                                                     box_mask_multi)\n\n        ###########################################\n        outputs = {}\n\n        # loss\n        relationship_loss = im_info.new_zeros(())\n        mlm_loss = im_info.new_zeros(())\n        mvrc_loss = im_info.new_zeros(())\n        if self.config.NETWORK.WITH_REL_LOSS:\n            relationship_logits = relationship_logits_multi[:text_input_ids.shape[0]]\n            relationship_loss = F.cross_entropy(relationship_logits, relationship_label)\n        if self.config.NETWORK.WITH_MLM_LOSS:\n            mlm_labels_multi = mlm_labels.new_zeros((text_input_ids.shape[0] + aux_text.shape[0], max_text_len)).fill_(\n                -1)\n            mlm_labels_multi[:text_input_ids.shape[0], :mlm_labels.shape[1]] = mlm_labels\n            mlm_labels_multi[text_input_ids.shape[0]:, :aux_text_mlm_labels.shape[1]] = aux_text_mlm_labels\n\n            mlm_logits_multi_padded = \\\n                mlm_logits_multi.new_zeros((*mlm_labels_multi.shape, mlm_logits_multi.shape[-1])).fill_(-10000.0)\n            mlm_logits_multi_padded[:, :mlm_logits_multi.shape[1]] = mlm_logits_multi\n            mlm_logits_multi = mlm_logits_multi_padded\n            mlm_logits_wvc = mlm_logits_multi_padded[:text_input_ids.shape[0]]\n            mlm_labels_wvc = mlm_labels_multi[:text_input_ids.shape[0]]\n            mlm_logits_aux = mlm_logits_multi_padded[text_input_ids.shape[0]:]\n            mlm_labels_aux = mlm_labels_multi[text_input_ids.shape[0]:]\n            if self.config.NETWORK.MLM_LOSS_NORM_IN_BATCH_FIRST:\n                mlm_loss_wvc = F.cross_entropy(mlm_logits_wvc.transpose(1, 2),\n                                               mlm_labels_wvc,\n                                               ignore_index=-1, reduction=\'none\')\n                num_mlm_wvc = (mlm_labels_wvc != -1).sum(1, keepdim=True).to(dtype=mlm_loss_wvc.dtype)\n                num_has_mlm_wvc = (num_mlm_wvc != 0).sum().to(dtype=mlm_loss_wvc.dtype)\n                mlm_loss_wvc = (mlm_loss_wvc / (num_mlm_wvc + 1e-4)).sum() / (num_has_mlm_wvc + 1e-4)\n                mlm_loss_aux = F.cross_entropy(mlm_logits_aux.transpose(1, 2),\n                                               mlm_labels_aux,\n                                               ignore_index=-1, reduction=\'none\')\n                num_mlm_aux = (mlm_labels_aux != -1).sum(1, keepdim=True).to(dtype=mlm_loss_aux.dtype)\n                num_has_mlm_aux = (num_mlm_aux != 0).sum().to(dtype=mlm_loss_aux.dtype)\n                mlm_loss_aux = (mlm_loss_aux / (num_mlm_aux + 1e-4)).sum() / (num_has_mlm_aux + 1e-4)\n            else:\n                # mlm_loss = F.cross_entropy(mlm_logits_multi_padded.view((-1, mlm_logits_multi_padded.shape[-1])),\n                #                            mlm_labels_multi.view(-1),\n                #                            ignore_index=-1)\n                mlm_loss_wvc = F.cross_entropy(\n                    mlm_logits_wvc.view((-1, mlm_logits_multi_padded.shape[-1])),\n                    mlm_labels_wvc.view(-1),\n                    ignore_index=-1\n                )\n                mlm_loss_aux = F.cross_entropy(\n                    mlm_logits_aux.view((-1, mlm_logits_multi_padded.shape[-1])),\n                    mlm_labels_aux.view(-1),\n                    ignore_index=-1\n                )\n\n        # mvrc_loss = F.cross_entropy(mvrc_logits.contiguous().view(-1, mvrc_logits.shape[-1]),\n        #                             mvrc_labels.contiguous().view(-1),\n        #                             ignore_index=-1)\n        if self.config.NETWORK.WITH_MVRC_LOSS:\n            mvrc_logits = mvrc_logits_multi[:mvrc_labels.shape[0], :mvrc_labels.shape[1]]\n            if self.config.NETWORK.MVRC_LOSS_NORM_IN_BATCH_FIRST:\n                mvrc_loss = soft_cross_entropy(\n                    mvrc_logits.contiguous().view(-1, mvrc_logits.shape[-1]),\n                    mvrc_labels.contiguous().view(-1, mvrc_logits.shape[-1]),\n                    reduction=\'none\').view(mvrc_logits.shape[:-1])\n                valid = (mvrc_labels.sum(-1) - 1).abs() < 1.0e-1\n                mvrc_loss = (mvrc_loss / (valid.sum(1, keepdim=True).to(dtype=mvrc_loss.dtype) + 1e-4)) \\\n                                .sum() / ((valid.sum(1) != 0).sum().to(dtype=mvrc_loss.dtype) + 1e-4)\n            else:\n                mvrc_loss = soft_cross_entropy(mvrc_logits.contiguous().view(-1, mvrc_logits.shape[-1]),\n                                               mvrc_labels.contiguous().view(-1, mvrc_logits.shape[-1]))\n\n            mvrc_logits_padded = mvrc_logits.new_zeros((mvrc_logits.shape[0], origin_len, mvrc_logits.shape[2])).fill_(\n                -10000.0)\n            mvrc_logits_padded[:, :mvrc_logits.shape[1]] = mvrc_logits\n            mvrc_logits = mvrc_logits_padded\n            mvrc_labels_padded = mvrc_labels.new_zeros((mvrc_labels.shape[0], origin_len, mvrc_labels.shape[2])).fill_(\n                0.0)\n            mvrc_labels_padded[:, :mvrc_labels.shape[1]] = mvrc_labels\n            mvrc_labels = mvrc_labels_padded\n\n        outputs.update({\n            \'relationship_logits\': relationship_logits if self.config.NETWORK.WITH_REL_LOSS else None,\n            \'relationship_label\': relationship_label if self.config.NETWORK.WITH_REL_LOSS else None,\n            \'mlm_logits_wvc\': mlm_logits_wvc if self.config.NETWORK.WITH_MLM_LOSS else None,\n            \'mlm_label_wvc\': mlm_labels_wvc if self.config.NETWORK.WITH_MLM_LOSS else None,\n            \'mlm_logits_aux\': mlm_logits_aux if self.config.NETWORK.WITH_MLM_LOSS else None,\n            \'mlm_label_aux\': mlm_labels_aux if self.config.NETWORK.WITH_MLM_LOSS else None,\n            \'mvrc_logits\': mvrc_logits if self.config.NETWORK.WITH_MVRC_LOSS else None,\n            \'mvrc_label\': mvrc_labels if self.config.NETWORK.WITH_MVRC_LOSS else None,\n            \'relationship_loss\': relationship_loss,\n            \'mlm_loss_wvc\': mlm_loss_wvc,\n            \'mlm_loss_aux\': mlm_loss_aux,\n            \'mvrc_loss\': mvrc_loss,\n        })\n\n        loss = relationship_loss.mean() + mlm_loss_wvc.mean() + mlm_loss_aux.mean() + mvrc_loss.mean()\n\n        return outputs, loss\n'"
refcoco/data/build.py,5,"b'import torch.utils.data\n\nfrom .datasets import *\nfrom . import samplers\nfrom .transforms.build import build_transforms\nfrom .collate_batch import BatchCollator\nimport pprint\n\nDATASET_CATALOGS = {\'refcoco+\': RefCOCO}\n\n\ndef build_dataset(dataset_name, *args, **kwargs):\n    assert dataset_name in DATASET_CATALOGS, ""dataset not in catalogs""\n    return DATASET_CATALOGS[dataset_name](*args, **kwargs)\n\n\ndef make_data_sampler(dataset, shuffle, distributed, num_replicas, rank):\n    if distributed:\n        return samplers.DistributedSampler(dataset, shuffle=shuffle, num_replicas=num_replicas, rank=rank)\n    if shuffle:\n        sampler = torch.utils.data.sampler.RandomSampler(dataset)\n    else:\n        sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n    return sampler\n\n\ndef make_batch_data_sampler(dataset, sampler, aspect_grouping, batch_size):\n    if aspect_grouping:\n        group_ids = dataset.group_ids\n        batch_sampler = samplers.GroupedBatchSampler(\n            sampler, group_ids, batch_size, drop_uneven=False\n        )\n    else:\n        batch_sampler = torch.utils.data.sampler.BatchSampler(\n            sampler, batch_size, drop_last=False\n        )\n    return batch_sampler\n\n\ndef make_dataloader(cfg, dataset=None, mode=\'train\', distributed=False, num_replicas=None, rank=None,\n                    expose_sampler=False):\n    assert mode in [\'train\', \'val\', \'test\']\n    if mode == \'train\':\n        ann_file = cfg.DATASET.TRAIN_ANNOTATION_FILE\n        image_set = cfg.DATASET.TRAIN_IMAGE_SET\n        aspect_grouping = cfg.TRAIN.ASPECT_GROUPING\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.TRAIN.BATCH_IMAGES * num_gpu\n        shuffle = cfg.TRAIN.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n        boxes = cfg.DATASET.TRAIN_BOXES\n    elif mode == \'val\':\n        ann_file = cfg.DATASET.VAL_ANNOTATION_FILE\n        image_set = cfg.DATASET.VAL_IMAGE_SET\n        aspect_grouping = False\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.VAL.BATCH_IMAGES * num_gpu\n        shuffle = cfg.VAL.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n        boxes = cfg.DATASET.VAL_BOXES\n    else:\n        ann_file = cfg.DATASET.TEST_ANNOTATION_FILE\n        image_set = cfg.DATASET.TEST_IMAGE_SET\n        aspect_grouping = False\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.TEST.BATCH_IMAGES * num_gpu\n        shuffle = cfg.TEST.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n        boxes = cfg.DATASET.TEST_BOXES\n\n    transform = build_transforms(cfg, mode)\n\n    if dataset is None:\n\n        dataset = build_dataset(dataset_name=cfg.DATASET.DATASET, ann_file=ann_file, image_set=image_set,\n                                boxes=boxes, proposal_source=cfg.DATASET.PROPOSAL_SOURCE,\n                                answer_vocab_file=cfg.DATASET.ANSWER_VOCAB_FILE,\n                                root_path=cfg.DATASET.ROOT_PATH, data_path=cfg.DATASET.DATASET_PATH,\n                                test_mode=(mode == \'test\'), transform=transform,\n                                zip_mode=cfg.DATASET.ZIP_MODE, cache_mode=cfg.DATASET.CACHE_MODE,\n                                cache_db=True if (rank is None or rank == 0) else False,\n                                ignore_db_cache=cfg.DATASET.IGNORE_DB_CACHE,\n                                add_image_as_a_box=cfg.DATASET.ADD_IMAGE_AS_A_BOX,\n                                aspect_grouping=aspect_grouping,\n                                pretrained_model_name=cfg.NETWORK.BERT_MODEL_NAME)\n\n    sampler = make_data_sampler(dataset, shuffle, distributed, num_replicas, rank)\n    batch_sampler = make_batch_data_sampler(dataset, sampler, aspect_grouping, batch_size)\n    collator = BatchCollator(dataset=dataset, append_ind=cfg.DATASET.APPEND_INDEX)\n\n    dataloader = torch.utils.data.DataLoader(dataset=dataset,\n                                             batch_sampler=batch_sampler,\n                                             num_workers=num_workers,\n                                             pin_memory=False,\n                                             collate_fn=collator)\n    if expose_sampler:\n        return dataloader, sampler\n\n    return dataloader\n'"
refcoco/data/collate_batch.py,3,"b""import torch\nfrom common.utils.clip_pad import *\n\n\nclass BatchCollator(object):\n    def __init__(self, dataset, append_ind=False):\n        self.dataset = dataset\n        self.test_mode = self.dataset.test_mode\n        self.data_names = self.dataset.data_names\n        self.append_ind = append_ind\n\n    def __call__(self, batch):\n        if not isinstance(batch, list):\n            batch = list(batch)\n\n        if batch[0][self.data_names.index('image')] is not None:\n            max_shape = tuple(max(s) for s in zip(*[data[self.data_names.index('image')].shape for data in batch]))\n            image_none = False\n        else:\n            image_none = True\n        max_boxes = max([data[self.data_names.index('boxes')].shape[0] for data in batch])\n        max_expression_length = max([len(data[self.data_names.index('expression')]) for data in batch])\n\n        for i, ibatch in enumerate(batch):\n            out = {}\n\n            if image_none:\n                out['image'] = None\n            else:\n                image = ibatch[self.data_names.index('image')]\n                out['image'] = clip_pad_images(image, max_shape, pad=0)\n\n            boxes = ibatch[self.data_names.index('boxes')]\n            out['boxes'] = clip_pad_boxes(boxes, max_boxes, pad=-2)\n\n            expression = ibatch[self.data_names.index('expression')]\n            out['expression'] = clip_pad_1d(expression, max_expression_length, pad=0)\n\n            if 'label' in self.data_names:\n                label = ibatch[self.data_names.index('label')]\n                out['label'] = clip_pad_1d(label, max_boxes, pad=-1)\n\n            other_names = [data_name for data_name in self.data_names if data_name not in out]\n            for name in other_names:\n                out[name] = torch.as_tensor(ibatch[self.data_names.index(name)])\n\n            batch[i] = tuple(out[data_name] for data_name in self.data_names)\n            if self.append_ind:\n                batch[i] += (torch.tensor(i, dtype=torch.int64),)\n\n        out_tuple = ()\n        for items in zip(*batch):\n            if items[0] is None:\n                out_tuple += (None,)\n            else:\n                out_tuple += (torch.stack(tuple(items), dim=0), )\n\n        return out_tuple\n\n"""
refcoco/function/test.py,8,"b'import os\nimport pprint\nimport shutil\n\nimport json\nfrom tqdm import tqdm, trange\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom common.utils.load import smart_load_model_state_dict\nfrom common.trainer import to_cuda\nfrom common.utils.create_logger import create_logger\nfrom refcoco.data.build import make_dataloader\nfrom refcoco.modules import *\n\nPOSITIVE_THRESHOLD = 0.5\n\n\ndef cacluate_iou(pred_boxes, gt_boxes):\n    x11, y11, x12, y12 = pred_boxes[:, 0], pred_boxes[:, 1], pred_boxes[:, 2], pred_boxes[:, 3]\n    x21, y21, x22, y22 = gt_boxes[:, 0], gt_boxes[:, 1], gt_boxes[:, 2], gt_boxes[:, 3]\n    xA = np.maximum(x11, x21)\n    yA = np.maximum(y11, y21)\n    xB = np.minimum(x12, x22)\n    yB = np.minimum(y12, y22)\n    interArea = (xB - xA + 1).clip(0) * (yB - yA + 1).clip(0)\n    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n    iou = interArea / (boxAArea + boxBArea - interArea)\n\n    return iou\n\n\n@torch.no_grad()\ndef test_net(args, config):\n    print(\'test net...\')\n    pprint.pprint(args)\n    pprint.pprint(config)\n    device_ids = [int(d) for d in config.GPUS.split(\',\')]\n    #os.environ[\'CUDA_VISIBLE_DEVICES\'] = config.GPUS\n    config.DATASET.TEST_IMAGE_SET = args.split\n    ckpt_path = args.ckpt\n    save_path = args.result_path\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    shutil.copy2(ckpt_path,\n                 os.path.join(save_path, \'{}_test_ckpt_{}.model\'.format(config.MODEL_PREFIX, config.DATASET.TASK)))\n\n    torch.backends.cudnn.enabled = False\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # get network\n    model = eval(config.MODULE)(config)\n    if len(device_ids) > 1:\n        model = torch.nn.DataParallel(model, device_ids=device_ids).cuda()\n    else:\n        torch.cuda.set_device(device_ids[0])\n        model = model.cuda()\n    checkpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n    smart_load_model_state_dict(model, checkpoint[\'state_dict\'])\n\n    # loader\n    test_loader = make_dataloader(config, mode=\'test\', distributed=False)\n    test_dataset = test_loader.dataset\n    test_database = test_dataset.database\n\n    # test\n    ref_ids = []\n    pred_boxes = []\n    model.eval()\n    cur_id = 0\n    for nbatch, batch in zip(trange(len(test_loader)), test_loader):\n    # for nbatch, batch in tqdm(enumerate(test_loader)):\n        bs = test_loader.batch_sampler.batch_size if test_loader.batch_sampler is not None else test_loader.batch_size\n        ref_ids.extend([test_database[id][\'ref_id\'] for id in range(cur_id, min(cur_id + bs, len(test_database)))])\n        batch = to_cuda(batch)\n        output = model(*batch)\n        pred_boxes.extend(output[\'pred_boxes\'].detach().cpu().tolist())\n        cur_id += bs\n\n    result = [{\'ref_id\': ref_id, \'box\': box} for ref_id, box in zip(ref_ids, pred_boxes)]\n\n    result_json_path = os.path.join(save_path, \'{}_refcoco+_{}.json\'.format(\n        config.MODEL_PREFIX if args.result_name is None else args.result_name, config.DATASET.TEST_IMAGE_SET))\n    with open(result_json_path, \'w\') as f:\n        json.dump(result, f)\n    print(\'result json saved to {}.\'.format(result_json_path))\n\n    # evaluate (test label of refcoco+ has been released)\n    print(""Evaluate on split: {}..."".format(config.DATASET.TEST_IMAGE_SET))\n    pred_boxes_arr = np.array(pred_boxes)\n    gt_boxes_arr = np.array([test_dataset.refer.getRefBox(ref_id=ref_id) for ref_id in ref_ids])\n    gt_boxes_arr[:, [2, 3]] += gt_boxes_arr[:, [0, 1]]\n    iou = cacluate_iou(pred_boxes_arr, gt_boxes_arr)\n    acc = float((iou >= POSITIVE_THRESHOLD).sum() * 1.0 / iou.shape[0])\n    print(""Accuracy: {}."".format(acc * 100.0))\n\n    return result_json_path\n'"
refcoco/function/train.py,19,"b'import os\nimport pprint\nimport shutil\nimport inspect\n\nfrom tensorboardX import SummaryWriter\nimport numpy as np\nimport torch\nimport torch.nn\nimport torch.optim as optim\nimport torch.distributed as distributed\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nfrom common.utils.create_logger import create_logger\nfrom common.utils.misc import summary_parameters, bn_fp16_half_eval\nfrom common.utils.load import smart_resume, smart_partial_load_model_state_dict\nfrom common.trainer import train\nfrom common.metrics.composite_eval_metric import CompositeEvalMetric\nfrom common.metrics import refcoco_metrics\nfrom common.callbacks.batch_end_callbacks.speedometer import Speedometer\nfrom common.callbacks.epoch_end_callbacks.validation_monitor import ValidationMonitor\nfrom common.callbacks.epoch_end_callbacks.checkpoint import Checkpoint\nfrom common.lr_scheduler import WarmupMultiStepLR\nfrom common.nlp.bert.optimization import AdamW, WarmupLinearSchedule\nfrom refcoco.data.build import make_dataloader, build_dataset, build_transforms\nfrom refcoco.modules import *\nfrom refcoco.function.val import do_validation\n\ntry:\n    from apex import amp\n    from apex.parallel import DistributedDataParallel as Apex_DDP\nexcept ImportError:\n    pass\n    #raise ImportError(""Please install apex from https://www.github.com/nvidia/apex if you want to use fp16."")\n\n\ndef train_net(args, config):\n    # setup logger\n    logger, final_output_path = create_logger(config.OUTPUT_PATH, args.cfg, config.DATASET.TRAIN_IMAGE_SET,\n                                              split=\'train\')\n    model_prefix = os.path.join(final_output_path, config.MODEL_PREFIX)\n    if args.log_dir is None:\n        args.log_dir = os.path.join(final_output_path, \'tensorboard_logs\')\n\n    pprint.pprint(args)\n    logger.info(\'training args:{}\\n\'.format(args))\n    pprint.pprint(config)\n    logger.info(\'training config:{}\\n\'.format(pprint.pformat(config)))\n\n    # manually set random seed\n    if config.RNG_SEED > -1:\n        np.random.seed(config.RNG_SEED)\n        torch.random.manual_seed(config.RNG_SEED)\n        torch.cuda.manual_seed_all(config.RNG_SEED)\n\n    # cudnn\n    torch.backends.cudnn.benchmark = False\n    if args.cudnn_off:\n        torch.backends.cudnn.enabled = False\n\n    if args.dist:\n        model = eval(config.MODULE)(config)\n        local_rank = int(os.environ.get(\'LOCAL_RANK\') or 0)\n        config.GPUS = str(local_rank)\n        torch.cuda.set_device(local_rank)\n        master_address = os.environ[\'MASTER_ADDR\']\n        master_port = int(os.environ[\'MASTER_PORT\'] or 23456)\n        world_size = int(os.environ[\'WORLD_SIZE\'] or 1)\n        rank = int(os.environ[\'RANK\'] or 0)\n        if args.slurm:\n            distributed.init_process_group(backend=\'nccl\')\n        else:\n            distributed.init_process_group(\n                backend=\'nccl\',\n                init_method=\'tcp://{}:{}\'.format(master_address, master_port),\n                world_size=world_size,\n                rank=rank,\n                group_name=\'mtorch\')\n        print(f\'native distributed, size: {world_size}, rank: {rank}, local rank: {local_rank}\')\n        torch.cuda.set_device(local_rank)\n        config.GPUS = str(local_rank)\n        model = model.cuda()\n        if not config.TRAIN.FP16:\n            model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n\n        if rank == 0:\n            summary_parameters(model.module if isinstance(model, torch.nn.parallel.DistributedDataParallel) else model,\n                               logger)\n            shutil.copy(args.cfg, final_output_path)\n            shutil.copy(inspect.getfile(eval(config.MODULE)), final_output_path)\n\n        writer = None\n        if args.log_dir is not None:\n            tb_log_dir = os.path.join(args.log_dir, \'rank{}\'.format(rank))\n            if not os.path.exists(tb_log_dir):\n                os.makedirs(tb_log_dir)\n            writer = SummaryWriter(log_dir=tb_log_dir)\n\n        train_loader, train_sampler = make_dataloader(config,\n                                                      mode=\'train\',\n                                                      distributed=True,\n                                                      num_replicas=world_size,\n                                                      rank=rank,\n                                                      expose_sampler=True)\n        val_loader = make_dataloader(config,\n                                     mode=\'val\',\n                                     distributed=True,\n                                     num_replicas=world_size,\n                                     rank=rank)\n\n        batch_size = world_size * (sum(config.TRAIN.BATCH_IMAGES)\n                                   if isinstance(config.TRAIN.BATCH_IMAGES, list)\n                                   else config.TRAIN.BATCH_IMAGES)\n        if config.TRAIN.GRAD_ACCUMULATE_STEPS > 1:\n            batch_size = batch_size * config.TRAIN.GRAD_ACCUMULATE_STEPS\n        base_lr = config.TRAIN.LR * batch_size\n        optimizer_grouped_parameters = [{\'params\': [p for n, p in model.named_parameters() if _k in n],\n                                         \'lr\': base_lr * _lr_mult}\n                                        for _k, _lr_mult in config.TRAIN.LR_MULT]\n        optimizer_grouped_parameters.append({\'params\': [p for n, p in model.named_parameters()\n                                                        if all([_k not in n for _k, _ in config.TRAIN.LR_MULT])]})\n        if config.TRAIN.OPTIMIZER == \'SGD\':\n            optimizer = optim.SGD(optimizer_grouped_parameters,\n                                  lr=config.TRAIN.LR * batch_size,\n                                  momentum=config.TRAIN.MOMENTUM,\n                                  weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'Adam\':\n            optimizer = optim.Adam(optimizer_grouped_parameters,\n                                   lr=config.TRAIN.LR * batch_size,\n                                   weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'AdamW\':\n            optimizer = AdamW(optimizer_grouped_parameters,\n                              lr=config.TRAIN.LR * batch_size,\n                              betas=(0.9, 0.999),\n                              eps=1e-6,\n                              weight_decay=config.TRAIN.WD,\n                              correct_bias=True)\n        else:\n            raise ValueError(\'Not support optimizer {}!\'.format(config.TRAIN.OPTIMIZER))\n        total_gpus = world_size\n\n    else:\n        #os.environ[\'CUDA_VISIBLE_DEVICES\'] = config.GPUS\n        model = eval(config.MODULE)(config)\n        summary_parameters(model, logger)\n        shutil.copy(args.cfg, final_output_path)\n        shutil.copy(inspect.getfile(eval(config.MODULE)), final_output_path)\n        num_gpus = len(config.GPUS.split(\',\'))\n        assert num_gpus <= 1 or (not config.TRAIN.FP16), ""Not support fp16 with torch.nn.DataParallel. "" \\\n                                                         ""Please use amp.parallel.DistributedDataParallel instead.""\n        total_gpus = num_gpus\n        rank = None\n        writer = SummaryWriter(log_dir=args.log_dir) if args.log_dir is not None else None\n\n        # model\n        if num_gpus > 1:\n            model = torch.nn.DataParallel(model, device_ids=[int(d) for d in config.GPUS.split(\',\')]).cuda()\n        else:\n            torch.cuda.set_device(int(config.GPUS))\n            model.cuda()\n\n        # loader\n        train_loader = make_dataloader(config, mode=\'train\', distributed=False)\n        val_loader = make_dataloader(config, mode=\'val\', distributed=False)\n        train_sampler = None\n\n        batch_size = num_gpus * (sum(config.TRAIN.BATCH_IMAGES) if isinstance(config.TRAIN.BATCH_IMAGES, list)\n                                 else config.TRAIN.BATCH_IMAGES)\n        if config.TRAIN.GRAD_ACCUMULATE_STEPS > 1:\n            batch_size = batch_size * config.TRAIN.GRAD_ACCUMULATE_STEPS\n        base_lr = config.TRAIN.LR * batch_size\n        optimizer_grouped_parameters = [{\'params\': [p for n, p in model.named_parameters() if _k in n],\n                                         \'lr\': base_lr * _lr_mult}\n                                        for _k, _lr_mult in config.TRAIN.LR_MULT]\n        optimizer_grouped_parameters.append({\'params\': [p for n, p in model.named_parameters()\n                                                        if all([_k not in n for _k, _ in config.TRAIN.LR_MULT])]})\n\n        if config.TRAIN.OPTIMIZER == \'SGD\':\n            optimizer = optim.SGD(optimizer_grouped_parameters,\n                                  lr=config.TRAIN.LR * batch_size,\n                                  momentum=config.TRAIN.MOMENTUM,\n                                  weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'Adam\':\n            optimizer = optim.Adam(optimizer_grouped_parameters,\n                                   lr=config.TRAIN.LR * batch_size,\n                                   weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'AdamW\':\n            optimizer = AdamW(optimizer_grouped_parameters,\n                              lr=config.TRAIN.LR * batch_size,\n                              betas=(0.9, 0.999),\n                              eps=1e-6,\n                              weight_decay=config.TRAIN.WD,\n                              correct_bias=True)\n        else:\n            raise ValueError(\'Not support optimizer {}!\'.format(config.TRAIN.OPTIMIZER))\n\n    # partial load pretrain state dict\n    if config.NETWORK.PARTIAL_PRETRAIN != """":\n        pretrain_state_dict = torch.load(config.NETWORK.PARTIAL_PRETRAIN, map_location=lambda storage, loc: storage)[\'state_dict\']\n        prefix_change = [prefix_change.split(\'->\') for prefix_change in config.NETWORK.PARTIAL_PRETRAIN_PREFIX_CHANGES]\n        if len(prefix_change) > 0:\n            pretrain_state_dict_parsed = {}\n            for k, v in pretrain_state_dict.items():\n                no_match = True\n                for pretrain_prefix, new_prefix in prefix_change:\n                    if k.startswith(pretrain_prefix):\n                        k = new_prefix + k[len(pretrain_prefix):]\n                        pretrain_state_dict_parsed[k] = v\n                        no_match = False\n                        break\n                if no_match:\n                    pretrain_state_dict_parsed[k] = v\n            pretrain_state_dict = pretrain_state_dict_parsed\n        smart_partial_load_model_state_dict(model, pretrain_state_dict)\n\n    # metrics\n    train_metrics_list = [refcoco_metrics.RefAccuracy(allreduce=args.dist,\n                                                      num_replicas=world_size if args.dist else 1),\n                          refcoco_metrics.ClsAccuracy(allreduce=args.dist,\n                                                      num_replicas=world_size if args.dist else 1),\n                          refcoco_metrics.ClsPosAccuracy(allreduce=args.dist,\n                                                         num_replicas=world_size if args.dist else 1),\n                          refcoco_metrics.ClsPosFraction(allreduce=args.dist,\n                                                         num_replicas=world_size if args.dist else 1),\n                          ]\n    val_metrics_list = [refcoco_metrics.RefAccuracy(allreduce=args.dist,\n                                                    num_replicas=world_size if args.dist else 1)]\n    for output_name, display_name in config.TRAIN.LOSS_LOGGERS:\n        train_metrics_list.append(\n            refcoco_metrics.LossLogger(output_name, display_name=display_name, allreduce=args.dist,\n                                       num_replicas=world_size if args.dist else 1))\n\n    train_metrics = CompositeEvalMetric()\n    val_metrics = CompositeEvalMetric()\n    for child_metric in train_metrics_list:\n        train_metrics.add(child_metric)\n    for child_metric in val_metrics_list:\n        val_metrics.add(child_metric)\n\n    # epoch end callbacks\n    epoch_end_callbacks = []\n    if (rank is None) or (rank == 0):\n        epoch_end_callbacks = [Checkpoint(model_prefix, config.CHECKPOINT_FREQUENT)]\n    validation_monitor = ValidationMonitor(do_validation, val_loader, val_metrics,\n                                           host_metric_name=\'RefAcc\',\n                                           label_index_in_batch=config.DATASET.LABEL_INDEX_IN_BATCH)\n\n    # optimizer initial lr before\n    for group in optimizer.param_groups:\n        group.setdefault(\'initial_lr\', group[\'lr\'])\n\n    # resume/auto-resume\n    if rank is None or rank == 0:\n        smart_resume(model, optimizer, validation_monitor, config, model_prefix, logger)\n    if args.dist:\n        begin_epoch = torch.tensor(config.TRAIN.BEGIN_EPOCH).cuda()\n        distributed.broadcast(begin_epoch, src=0)\n        config.TRAIN.BEGIN_EPOCH = begin_epoch.item()\n\n    # batch end callbacks\n    batch_size = len(config.GPUS.split(\',\')) * config.TRAIN.BATCH_IMAGES\n    batch_end_callbacks = [Speedometer(batch_size, config.LOG_FREQUENT,\n                                       batches_per_epoch=len(train_loader),\n                                       epochs=config.TRAIN.END_EPOCH - config.TRAIN.BEGIN_EPOCH)]\n\n    # setup lr step and lr scheduler\n    if config.TRAIN.LR_SCHEDULE == \'plateau\':\n        print(""Warning: not support resuming on plateau lr schedule!"")\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                                  mode=\'max\',\n                                                                  factor=config.TRAIN.LR_FACTOR,\n                                                                  patience=1,\n                                                                  verbose=True,\n                                                                  threshold=1e-4,\n                                                                  threshold_mode=\'rel\',\n                                                                  cooldown=2,\n                                                                  min_lr=0,\n                                                                  eps=1e-8)\n    elif config.TRAIN.LR_SCHEDULE == \'triangle\':\n        lr_scheduler = WarmupLinearSchedule(optimizer,\n                                            config.TRAIN.WARMUP_STEPS if config.TRAIN.WARMUP else 0,\n                                            t_total=int(config.TRAIN.END_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS),\n                                            last_epoch=int(config.TRAIN.BEGIN_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS)  - 1)\n    elif config.TRAIN.LR_SCHEDULE == \'step\':\n        lr_iters = [int(epoch * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS) for epoch in config.TRAIN.LR_STEP]\n        lr_scheduler = WarmupMultiStepLR(optimizer, milestones=lr_iters, gamma=config.TRAIN.LR_FACTOR,\n                                         warmup_factor=config.TRAIN.WARMUP_FACTOR,\n                                         warmup_iters=config.TRAIN.WARMUP_STEPS if config.TRAIN.WARMUP else 0,\n                                         warmup_method=config.TRAIN.WARMUP_METHOD,\n                                         last_epoch=int(config.TRAIN.BEGIN_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS)  - 1)\n    else:\n        raise ValueError(""Not support lr schedule: {}."".format(config.TRAIN.LR_SCHEDULE))\n\n    # broadcast parameter and optimizer state from rank 0 before training start\n    if args.dist:\n        for v in model.state_dict().values():\n            distributed.broadcast(v, src=0)\n        # for v in optimizer.state_dict().values():\n        #     distributed.broadcast(v, src=0)\n        best_epoch = torch.tensor(validation_monitor.best_epoch).cuda()\n        best_val = torch.tensor(validation_monitor.best_val).cuda()\n        distributed.broadcast(best_epoch, src=0)\n        distributed.broadcast(best_val, src=0)\n        validation_monitor.best_epoch = best_epoch.item()\n        validation_monitor.best_val = best_val.item()\n\n    # apex: amp fp16 mixed-precision training\n    if config.TRAIN.FP16:\n        # model.apply(bn_fp16_half_eval)\n        model, optimizer = amp.initialize(model, optimizer,\n                                          opt_level=\'O2\',\n                                          keep_batchnorm_fp32=False,\n                                          loss_scale=config.TRAIN.FP16_LOSS_SCALE,\n                                          min_loss_scale=128.0)\n        if args.dist:\n            model = Apex_DDP(model, delay_allreduce=True)\n\n    train(model, optimizer, lr_scheduler, train_loader, train_sampler, train_metrics,\n          config.TRAIN.BEGIN_EPOCH, config.TRAIN.END_EPOCH, logger,\n          rank=rank, batch_end_callbacks=batch_end_callbacks, epoch_end_callbacks=epoch_end_callbacks,\n          writer=writer, validation_monitor=validation_monitor, fp16=config.TRAIN.FP16,\n          clip_grad_norm=config.TRAIN.CLIP_GRAD_NORM,\n          gradient_accumulate_steps=config.TRAIN.GRAD_ACCUMULATE_STEPS)\n\n    return rank, model\n'"
refcoco/function/val.py,1,"b""from collections import namedtuple\nimport torch\nfrom common.trainer import to_cuda\n\n\n@torch.no_grad()\ndef do_validation(net, val_loader, metrics, label_index_in_batch):\n    net.eval()\n    metrics.reset()\n    for nbatch, batch in enumerate(val_loader):\n        batch = to_cuda(batch)\n        label = batch[label_index_in_batch]\n        datas = [batch[i] for i in range(len(batch)) if i != label_index_in_batch % len(batch)]\n\n        outputs = net(*datas)\n        outputs.update({'label': label})\n        metrics.update(outputs)\n\n"""
refcoco/modules/resnet_vlbert_for_refcoco.py,9,"b'import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom external.pytorch_pretrained_bert import BertTokenizer\nfrom common.module import Module\nfrom common.fast_rcnn import FastRCNN\nfrom common.visual_linguistic_bert import VisualLinguisticBert, VisualLinguisticBertMVRCHeadTransform\n\nBERT_WEIGHTS_NAME = \'pytorch_model.bin\'\n\n\nclass ResNetVLBERT(Module):\n    def __init__(self, config):\n\n        super(ResNetVLBERT, self).__init__(config)\n\n        self.image_feature_extractor = FastRCNN(config,\n                                                average_pool=True,\n                                                final_dim=config.NETWORK.IMAGE_FINAL_DIM,\n                                                enable_cnn_reg_loss=False)\n        self.object_linguistic_embeddings = nn.Embedding(1, config.NETWORK.VLBERT.hidden_size)\n        self.image_feature_bn_eval = config.NETWORK.IMAGE_FROZEN_BN\n        self.tokenizer = BertTokenizer.from_pretrained(config.NETWORK.BERT_MODEL_NAME)\n\n        language_pretrained_model_path = None\n        if config.NETWORK.BERT_PRETRAINED != \'\':\n            language_pretrained_model_path = \'{}-{:04d}.model\'.format(config.NETWORK.BERT_PRETRAINED,\n                                                                      config.NETWORK.BERT_PRETRAINED_EPOCH)\n        elif os.path.isdir(config.NETWORK.BERT_MODEL_NAME):\n            weight_path = os.path.join(config.NETWORK.BERT_MODEL_NAME, BERT_WEIGHTS_NAME)\n            if os.path.isfile(weight_path):\n                language_pretrained_model_path = weight_path\n        self.language_pretrained_model_path = language_pretrained_model_path\n        if language_pretrained_model_path is None:\n            print(""Warning: no pretrained language model found, training from scratch!!!"")\n\n        self.vlbert = VisualLinguisticBert(config.NETWORK.VLBERT,\n                                         language_pretrained_model_path=language_pretrained_model_path)\n\n        transform = VisualLinguisticBertMVRCHeadTransform(config.NETWORK.VLBERT)\n        linear = nn.Linear(config.NETWORK.VLBERT.hidden_size, 1)\n        self.final_mlp = nn.Sequential(\n            transform,\n            nn.Dropout(config.NETWORK.CLASSIFIER_DROPOUT, inplace=False),\n            linear\n        )\n\n        # init weights\n        self.init_weight()\n\n        self.fix_params()\n\n    def init_weight(self):\n        self.image_feature_extractor.init_weight()\n        if self.object_linguistic_embeddings is not None:\n            self.object_linguistic_embeddings.weight.data.normal_(mean=0.0,\n                                                                  std=self.config.NETWORK.VLBERT.initializer_range)\n        for m in self.final_mlp.modules():\n            if isinstance(m, torch.nn.Linear):\n                torch.nn.init.xavier_uniform_(m.weight)\n                torch.nn.init.constant_(m.bias, 0)\n\n    def train(self, mode=True):\n        super(ResNetVLBERT, self).train(mode)\n        # turn some frozen layers to eval mode\n        if self.image_feature_bn_eval:\n            self.image_feature_extractor.bn_eval()\n\n    def fix_params(self):\n        pass\n\n    def train_forward(self,\n                      image,\n                      boxes,\n                      im_info,\n                      expression,\n                      label,\n                      ):\n        ###########################################\n\n        # visual feature extraction\n        images = image\n        box_mask = (boxes[:, :, 0] > - 1.5)\n        max_len = int(box_mask.sum(1).max().item())\n        origin_len = boxes.shape[1]\n        box_mask = box_mask[:, :max_len]\n        boxes = boxes[:, :max_len]\n        label = label[:, :max_len]\n\n        obj_reps = self.image_feature_extractor(images=images,\n                                                boxes=boxes,\n                                                box_mask=box_mask,\n                                                im_info=im_info,\n                                                classes=None,\n                                                segms=None)\n\n        ############################################\n        # prepare text\n        cls_id, sep_id = self.tokenizer.convert_tokens_to_ids([\'[CLS]\', \'[SEP]\'])\n        text_input_ids = expression.new_zeros((expression.shape[0], expression.shape[1] + 2))\n        text_input_ids[:, 0] = cls_id\n        text_input_ids[:, 1:-1] = expression\n        _sep_pos = (text_input_ids > 0).sum(1)\n        _batch_inds = torch.arange(expression.shape[0], device=expression.device)\n        text_input_ids[_batch_inds, _sep_pos] = sep_id\n        text_token_type_ids = text_input_ids.new_zeros(text_input_ids.shape)\n        text_mask = text_input_ids > 0\n        text_visual_embeddings = obj_reps[\'obj_reps\'][:, 0].unsqueeze(1).repeat((1, text_input_ids.shape[1], 1))\n\n        object_linguistic_embeddings = self.object_linguistic_embeddings(\n            boxes.new_zeros((boxes.shape[0], boxes.shape[1])).long()\n        )\n        object_vl_embeddings = torch.cat((obj_reps[\'obj_reps\'], object_linguistic_embeddings), -1)\n\n        ###########################################\n\n        # Visual Linguistic BERT\n\n        hidden_states_text, hidden_states_regions, _ = self.vlbert(text_input_ids,\n                                                                 text_token_type_ids,\n                                                                 text_visual_embeddings,\n                                                                 text_mask,\n                                                                 object_vl_embeddings,\n                                                                 box_mask,\n                                                                 output_all_encoded_layers=False,\n                                                                 output_text_and_object_separately=True)\n\n        ###########################################\n        outputs = {}\n\n        # classifier\n        logits = self.final_mlp(hidden_states_regions).squeeze(-1)\n\n        # loss\n        cls_loss = F.binary_cross_entropy_with_logits(logits[box_mask], label[box_mask])\n\n        # pad back to origin len for compatibility with DataParallel\n        logits_ = logits.new_zeros((logits.shape[0], origin_len)).fill_(-10000.0)\n        logits_[:, :logits.shape[1]] = logits\n        logits = logits_\n        label_ = label.new_zeros((logits.shape[0], origin_len)).fill_(-1)\n        label_[:, :label.shape[1]] = label\n        label = label_\n\n        outputs.update({\'label_logits\': logits,\n                        \'label\': label,\n                        \'cls_loss\': cls_loss})\n\n        loss = cls_loss.mean()\n\n        return outputs, loss\n\n    def inference_forward(self,\n                          image,\n                          boxes,\n                          im_info,\n                          expression):\n\n        ###########################################\n\n        # visual feature extraction\n        images = image\n        box_mask = (boxes[:, :, 0] > - 1.5)\n        max_len = int(box_mask.sum(1).max().item())\n        origin_len = boxes.shape[1]\n        box_mask = box_mask[:, :max_len]\n        boxes = boxes[:, :max_len]\n\n        obj_reps = self.image_feature_extractor(images=images,\n                                                boxes=boxes,\n                                                box_mask=box_mask,\n                                                im_info=im_info,\n                                                classes=None,\n                                                segms=None)\n\n        ############################################\n        # prepare text\n        cls_id, sep_id = self.tokenizer.convert_tokens_to_ids([\'[CLS]\', \'[SEP]\'])\n        text_input_ids = expression.new_zeros((expression.shape[0], expression.shape[1] + 2))\n        text_input_ids[:, 0] = cls_id\n        text_input_ids[:, 1:-1] = expression\n        _sep_pos = (text_input_ids > 0).sum(1)\n        _batch_inds = torch.arange(expression.shape[0], device=expression.device)\n        text_input_ids[_batch_inds, _sep_pos] = sep_id\n        text_token_type_ids = text_input_ids.new_zeros(text_input_ids.shape)\n        text_mask = text_input_ids > 0\n        text_visual_embeddings = obj_reps[\'obj_reps\'][:, 0].unsqueeze(1).repeat((1, text_input_ids.shape[1], 1))\n\n        object_linguistic_embeddings = self.object_linguistic_embeddings(\n            boxes.new_zeros((boxes.shape[0], boxes.shape[1])).long()\n        )\n        object_vl_embeddings = torch.cat((obj_reps[\'obj_reps\'], object_linguistic_embeddings), -1)\n\n        ###########################################\n\n        # Visual Linguistic BERT\n\n        hidden_states_text, hidden_states_regions, _ = self.vlbert(text_input_ids,\n                                                                 text_token_type_ids,\n                                                                 text_visual_embeddings,\n                                                                 text_mask,\n                                                                 object_vl_embeddings,\n                                                                 box_mask,\n                                                                 output_all_encoded_layers=False,\n                                                                 output_text_and_object_separately=True)\n\n        ###########################################\n        outputs = {}\n\n        # classifier\n        logits = self.final_mlp(hidden_states_regions).squeeze(-1)\n\n        # pad back to origin len for compatibility with DataParallel\n        logits_ = logits.new_zeros((logits.shape[0], origin_len)).fill_(-10000.0)\n        logits_[:, :logits.shape[1]] = logits\n        logits = logits_\n\n        w_ratio = im_info[:, 2]\n        h_ratio = im_info[:, 3]\n        pred_boxes = boxes[_batch_inds, logits.argmax(1), :4]\n        pred_boxes[:, [0, 2]] /= w_ratio.unsqueeze(1)\n        pred_boxes[:, [1, 3]] /= h_ratio.unsqueeze(1)\n        outputs.update({\'label_logits\': logits,\n                        \'pred_boxes\': pred_boxes})\n\n        return outputs\n'"
vcr/data/build.py,5,"b'import torch.utils.data\n\nfrom .datasets import *\nfrom . import samplers\nfrom .transforms.build import build_transforms\nfrom .collate_batch import BatchCollator\nimport pprint\n\nDATASET_CATALOGS = {\'vcr\': VCRDataset}\n\n\ndef build_dataset(dataset_name, *args, **kwargs):\n    assert dataset_name in DATASET_CATALOGS, ""dataset not in catalogs""\n    return DATASET_CATALOGS[dataset_name](*args, **kwargs)\n\n\ndef make_data_sampler(dataset, shuffle, distributed, num_replicas, rank):\n    if distributed:\n        return samplers.DistributedSampler(dataset, shuffle=shuffle, num_replicas=num_replicas, rank=rank)\n    if shuffle:\n        sampler = torch.utils.data.sampler.RandomSampler(dataset)\n    else:\n        sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n    return sampler\n\n\ndef make_batch_data_sampler(dataset, sampler, aspect_grouping, batch_size):\n    if aspect_grouping:\n        group_ids = dataset.group_ids\n        batch_sampler = samplers.GroupedBatchSampler(\n            sampler, group_ids, batch_size, drop_uneven=False\n        )\n    else:\n        batch_sampler = torch.utils.data.sampler.BatchSampler(\n            sampler, batch_size, drop_last=False\n        )\n    return batch_sampler\n\n\ndef make_dataloader(cfg, dataset=None, mode=\'train\', distributed=False, num_replicas=None, rank=None,\n                    expose_sampler=False):\n    assert mode in [\'train\', \'val\', \'test\']\n    if mode == \'train\':\n        ann_file = cfg.DATASET.TRAIN_ANNOTATION_FILE\n        image_set = cfg.DATASET.TRAIN_IMAGE_SET\n        aspect_grouping = cfg.TRAIN.ASPECT_GROUPING\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.TRAIN.BATCH_IMAGES * num_gpu\n        shuffle = cfg.TRAIN.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n        mask_vl_modeling = cfg.DATASET.MASK_VL_MODELING if \'MASK_VL_MODELING\' in cfg.DATASET else False\n        mask_language_modeling = cfg.NETWORK.BERT_WITH_MLM_LOSS if \'BERT_WITH_MLM_LOSS\' in cfg.NETWORK else False\n    elif mode == \'val\':\n        ann_file = cfg.DATASET.VAL_ANNOTATION_FILE\n        image_set = cfg.DATASET.VAL_IMAGE_SET\n        aspect_grouping = False\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.VAL.BATCH_IMAGES * num_gpu\n        shuffle = cfg.VAL.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n        mask_vl_modeling = False\n        mask_language_modeling = False\n        if \'MASK_VL_MODELING\' in cfg.DATASET and cfg.DATASET.MASK_VL_MODELING and cfg.NETWORK.FOR_MASK_VL_MODELING_PRETRAIN:\n            mask_vl_modeling = True\n    else:\n        ann_file = cfg.DATASET.TEST_ANNOTATION_FILE\n        image_set = cfg.DATASET.TEST_IMAGE_SET\n        aspect_grouping = False\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.TEST.BATCH_IMAGES * num_gpu\n        shuffle = cfg.TEST.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n        mask_vl_modeling = False\n        mask_language_modeling = False\n\n    transform = build_transforms(cfg, mode)\n\n    if dataset is None:\n        kwargs = {\'mask_vl_modeling\': mask_vl_modeling,\n                  \'mask_language_modeling\': mask_language_modeling}\n        if mask_vl_modeling:\n            kwargs[\'mask_replace_only_same_cls\'] = cfg.DATASET.MASK_REPLACE_ONLY_SAME_CLS\n            kwargs[\'mask_master_ind_random\'] = (not cfg.NETWORK.FOR_MASK_VL_MODELING_PRETRAIN)\n            kwargs[\'mask_vl_modeling_mask_prob\'] = cfg.DATASET.MASK_VL_MODELING_MASK_PROB\n            kwargs[\'mask_vl_modeling_replace_prob\'] = cfg.DATASET.MASK_VL_MODELING_REPLACE_PROB\n        try:\n            kwargs[\'qa2r_noq\'] = cfg.DATASET.QA2R_NOQ\n            kwargs[\'qa2r_aug\'] = cfg.DATASET.QA2R_AUG\n        except AttributeError:\n            pass\n        try:\n            kwargs[\'basic_align\'] = cfg.DATASET.BASIC_ALIGN\n        except AttributeError:\n            pass\n        try:\n            kwargs[\'with_lg\'] = cfg.NETWORK.GNN.WITH_LG_LAYER\n            kwargs[\'with_kg\'] = cfg.NETWORK.GNN.WITH_KG\n            kwargs[\'kg_path\'] = cfg.DATASET.__getattribute__(\'{}_KG_PATH\'.format(mode.upper()))\n            kwargs[\'kg_word_embed\'] = cfg.DATASET.__getattribute__(\'{}_KG_WORD_EMBED\'.format(mode.upper()))\n        except AttributeError:\n            pass\n        try:\n            kwargs[\'kg_path\'] = cfg.DATASET.__getattribute__(\'{}_KG_PATH\'.format(mode.upper()))\n            kwargs[\'fact_path\'] = cfg.DATASET.__getattribute__(\'{}_KG_PATH\'.format(mode.upper()))\n        except AttributeError:\n            pass\n        try:\n            kwargs[\'expression_file\'] = cfg.DATASET.__getattribute__(\'{}_EXPRESSION_FILE\'.format(mode.upper()))\n        except AttributeError:\n            pass\n\n        try:\n            kwargs[\'kg_vocab_file\'] = cfg.NETWORK.KB_NODE_VOCAB\n        except AttributeError:\n            pass\n\n        try:\n            kwargs[\'caption_file\'] = cfg.DATASET.__getattribute__(\'{}_CAPTION_FILE\'.format(mode.upper()))\n        except AttributeError:\n            pass\n\n        print(\'Dataset kwargs:\')\n        pprint.pprint(kwargs)\n\n        dataset = build_dataset(dataset_name=cfg.DATASET.DATASET, ann_file=ann_file, image_set=image_set,\n                                root_path=cfg.DATASET.ROOT_PATH, data_path=cfg.DATASET.DATASET_PATH,\n                                test_mode=(mode == \'test\'), task=cfg.DATASET.TASK, transform=transform,\n                                zip_mode=cfg.DATASET.ZIP_MODE, cache_mode=cfg.DATASET.CACHE_MODE,\n                                ignore_db_cache=cfg.DATASET.IGNORE_DB_CACHE,\n                                only_use_relevant_dets=cfg.DATASET.ONLY_USE_RELEVANT_DETS,\n                                add_image_as_a_box=cfg.DATASET.ADD_IMAGE_AS_A_BOX,\n                                aspect_grouping=aspect_grouping,\n                                mask_size=(cfg.DATASET.MASK_SIZE, cfg.DATASET.MASK_SIZE),\n                                pretrained_model_name=cfg.NETWORK.BERT_MODEL_NAME,\n                                **kwargs)\n\n    sampler = make_data_sampler(dataset, shuffle, distributed, num_replicas, rank)\n    batch_sampler = make_batch_data_sampler(dataset, sampler, aspect_grouping, batch_size)\n    collator = BatchCollator(dataset=dataset, append_ind=cfg.DATASET.APPEND_INDEX)\n\n    dataloader = torch.utils.data.DataLoader(dataset=dataset,\n                                             batch_sampler=batch_sampler,\n                                             num_workers=num_workers,\n                                             pin_memory=False,\n                                             collate_fn=collator)\n    if expose_sampler:\n        return dataloader, sampler\n\n    return dataloader\n'"
vcr/data/collate_batch.py,9,"b'import torch\n\nfrom common.utils.clip_pad import *\n\n\nclass BatchCollator(object):\n    def __init__(self, dataset, append_ind=False):\n        self.dataset = dataset\n        self.test_mode = self.dataset.test_mode\n        self.task = self.dataset.task\n        self.data_names = self.dataset.data_names\n        self.append_ind = append_ind\n\n    def __call__(self, batch):\n        if not isinstance(batch, list):\n            batch = list(batch)\n\n        max_shape = tuple(max(s) for s in zip(*[data[self.data_names.index(\'image\')].shape for data in batch]))\n        max_boxes = max([data[self.data_names.index(\'boxes\')].shape[0] for data in batch])\n        max_masks = max([data[self.data_names.index(\'masks\')].shape[0] for data in batch])\n        if self.test_mode and self.task == \'QA2R\':\n            max_question_length = max([len(q) for data in batch for q in data[self.data_names.index(\'question\')]])\n        else:\n            max_question_length = max([len(data[self.data_names.index(\'question\')]) for data in batch])\n        if \'answer_choices\' in self.data_names:\n            max_answer_length = max([len(answer) for data in batch for answer in data[self.data_names.index(\'answer_choices\')]])\n        if \'answer\' in self.data_names:\n            max_answer_length = max([len(data[self.data_names.index(\'answer\')]) for data in batch])\n        if \'rationale_choices\' in self.data_names:\n            max_rationale_length = max([len(rationale) for data in batch for rationale in data[self.data_names.index(\'rationale_choices\')]])\n        if \'rationale\' in self.data_names:\n            max_rationale_length = max([len(data[self.data_names.index(\'rationale\')]) for data in batch])\n        if \'question_align_matrix\' in self.data_names:\n            if self.test_mode and self.task == \'QA2R\':\n                max_q_align_length = max([m.shape[0]\n                                          for data in batch\n                                          for m in data[self.data_names.index(\'question_align_matrix\')]])\n            else:\n                max_q_align_length = max([data[self.data_names.index(\'question_align_matrix\')].shape[0] for data in batch])\n        if \'answer_align_matrix\' in self.data_names:\n            if isinstance(batch[0][self.data_names.index(\'answer_align_matrix\')], list) or \\\n                    batch[0][self.data_names.index(\'answer_align_matrix\')].dim() == 3:\n                max_a_align_length = max([m.shape[0]\n                                          for data in batch\n                                          for m in data[self.data_names.index(\'answer_align_matrix\')]])\n            elif batch[0][self.data_names.index(\'answer_align_matrix\')].dim() == 2:\n                max_a_align_length = max([data[self.data_names.index(\'answer_align_matrix\')].shape[0]\n                                          for data in batch])\n            else:\n                raise ValueError(""invalid dims of answer_align_matrix"")\n        if \'rationale_align_matrix\' in self.data_names:\n            if isinstance(batch[0][self.data_names.index(\'rationale_align_matrix\')], list) or \\\n                    batch[0][self.data_names.index(\'rationale_align_matrix\')].dim() == 3:\n                max_r_align_length = max([m.shape[0]\n                                          for data in batch\n                                          for m in data[self.data_names.index(\'rationale_align_matrix\')]])\n            elif batch[0][self.data_names.index(\'rationale_align_matrix\')].dim() == 2:\n                max_r_align_length = max([data[self.data_names.index(\'rationale_align_matrix\')].shape[0]\n                                          for data in batch])\n            else:\n                raise ValueError(""invalid dims of rationale_align_matrix!"")\n\n        for i, ibatch in enumerate(batch):\n            out = {}\n            image = ibatch[self.data_names.index(\'image\')]\n            out[\'image\'] = clip_pad_images(image, max_shape, pad=0)\n\n            boxes = ibatch[self.data_names.index(\'boxes\')]\n            out[\'boxes\'] = clip_pad_boxes(boxes, max_boxes, pad=-1)\n\n            masks = ibatch[self.data_names.index(\'masks\')]\n            mask_height, mask_width = masks.shape[1:]\n            out[\'masks\'] = clip_pad_boxes(masks.view(masks.shape[0], -1), max_masks, pad=-1).view(-1, mask_height, mask_width)\n\n            question = ibatch[self.data_names.index(\'question\')]\n            if self.test_mode and self.task == \'QA2R\':\n                out[\'question\'] = torch.stack(tuple(clip_pad_2d(q, (max_question_length, len(q[0])), pad=-2) for q in question),\n                                              dim=0)\n                if \'question_align_matrix\' in self.data_names:\n                    q_align_matrix = ibatch[self.data_names.index(\'question_align_matrix\')]\n                    out[\'question_align_matrix\'] = torch.stack(\n                        tuple(clip_pad_2d(m, (max_q_align_length, max_question_length), pad=0) for m in q_align_matrix),\n                        dim=0)\n            else:\n                out[\'question\'] = clip_pad_2d(question, (max_question_length, len(question[0])), pad=-2)\n                if \'question_align_matrix\' in self.data_names:\n                    q_align_matrix = ibatch[self.data_names.index(\'question_align_matrix\')]\n                    out[\'question_align_matrix\'] = clip_pad_2d(q_align_matrix,\n                                                               (max_q_align_length, max_question_length),\n                                                               pad=0)\n            if \'answer\' in self.data_names:\n                answer = ibatch[self.data_names.index(\'answer\')]\n                out[\'answer\'] = clip_pad_2d(answer, (max_answer_length, len(answer[0])), pad=-2)\n            if \'answer_choices\' in self.data_names:\n                answer_choices = ibatch[self.data_names.index(\'answer_choices\')]\n                out[\'answer_choices\'] = torch.stack(tuple(clip_pad_2d(answer,\n                                                                      (max_answer_length, len(answer[0])),\n                                                                      pad=-2)\n                                                    for answer in answer_choices),\n                                                    dim=0)\n            if \'answer_align_matrix\' in self.data_names:\n                a_align_matrix = ibatch[self.data_names.index(\'answer_align_matrix\')]\n                if isinstance(a_align_matrix, list) or a_align_matrix.dim() == 3:\n                    out[\'answer_align_matrix\'] = torch.stack(\n                        tuple(clip_pad_2d(m, (max_a_align_length, max_answer_length), pad=0) for m in a_align_matrix),\n                        dim=0)\n                elif a_align_matrix.dim() == 2:\n                    out[\'answer_align_matrix\'] = clip_pad_2d(a_align_matrix, (max_a_align_length, max_answer_length),\n                                                             pad=0)\n            if \'rationale\' in self.data_names:\n                rationale = ibatch[self.data_names.index(\'rationale\')]\n                out[\'rationale\'] = clip_pad_2d(rationale, (max_rationale_length, len(rationale[0])), pad=-2)\n            if \'rationale_choices\' in self.data_names:\n                rationale_choices = ibatch[self.data_names.index(\'rationale_choices\')]\n                out[\'rationale_choices\'] = torch.stack(tuple(clip_pad_2d(rationale,\n                                                                         (max_rationale_length, len(rationale[0])),\n                                                                         pad=-2)\n                                                       for rationale in rationale_choices),\n                                                       dim=0)\n            if \'rationale_align_matrix\' in self.data_names:\n                r_align_matrix = ibatch[self.data_names.index(\'rationale_align_matrix\')]\n                if isinstance(r_align_matrix, list) or r_align_matrix.dim() == 3:\n                    out[\'rationale_align_matrix\'] = torch.stack(\n                        tuple(clip_pad_2d(m, (max_r_align_length, max_rationale_length), pad=0) for m in r_align_matrix),\n                        dim=0)\n                elif r_align_matrix.dim() == 2:\n                    out[\'rationale_align_matrix\'] = clip_pad_2d(r_align_matrix,\n                                                                (max_r_align_length, max_rationale_length), pad=0)\n\n            if \'answer_label\' in self.data_names:\n                out[\'answer_label\'] = ibatch[self.data_names.index(\'answer_label\')]\n            if \'rationale_label\' in self.data_names:\n                out[\'rationale_label\'] = ibatch[self.data_names.index(\'rationale_label\')]\n            out[\'im_info\'] = ibatch[self.data_names.index(\'im_info\')]\n\n            batch[i] = tuple(out[data_name] for data_name in self.data_names)\n            if self.append_ind:\n                batch[i] += (torch.tensor(i, dtype=torch.int64),)\n\n        out_tuple = ()\n        for items in zip(*batch):\n            if isinstance(items[0], torch.Tensor):\n                out_tuple += (torch.stack(tuple(items), dim=0), )\n            else:\n                out_tuple += (list(items), )\n\n        return out_tuple\n\n'"
vcr/function/test.py,7,"b'import os\nimport pprint\nimport shutil\n\nimport pandas as pd\nfrom tqdm import tqdm, trange\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom common.utils.load import smart_load_model_state_dict\nfrom common.trainer import to_cuda\nfrom common.utils.create_logger import create_logger\nfrom vcr.data.build import make_dataloader\nfrom vcr.modules import *\n\ntry:\n    from apex import amp\n    from apex.parallel import DistributedDataParallel as Apex_DDP\nexcept ImportError:\n    pass\n    #raise ImportError(""Please install apex from https://www.github.com/nvidia/apex if you want to use fp16."")\n\n# submit csv should contain following columns:\n# annot_id,\n# answer_0,answer_1,answer_2,answer_3,\n# rationale_conditioned_on_a0_0,rationale_conditioned_on_a0_1,rationale_conditioned_on_a0_2,rationale_conditioned_on_a0_3,\n# rationale_conditioned_on_a1_0,rationale_conditioned_on_a1_1,rationale_conditioned_on_a1_2,rationale_conditioned_on_a1_3,\n# rationale_conditioned_on_a2_0,rationale_conditioned_on_a2_1,rationale_conditioned_on_a2_2,rationale_conditioned_on_a2_3,\n# rationale_conditioned_on_a3_0,rationale_conditioned_on_a3_1,rationale_conditioned_on_a3_2,rationale_conditioned_on_a3_3\n\n\n@torch.no_grad()\ndef test_net(args, config, ckpt_path=None, save_path=None, save_name=None):\n    if save_path is None:\n        logger, test_output_path = create_logger(config.OUTPUT_PATH, args.cfg, config.DATASET.TEST_IMAGE_SET,\n                                                 split=\'test\')\n        save_path = test_output_path\n    if save_name is None:\n        save_name = config.MODEL_PREFIX\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    result_csv_path = os.path.join(save_path,\n                                   \'{}_test_result_{}.csv\'.format(save_name, config.DATASET.TASK))\n    if args.use_cache and os.path.isfile(result_csv_path):\n        print(""Cache found in {}, skip test!"".format(result_csv_path))\n        return result_csv_path\n\n    print(\'test net...\')\n    pprint.pprint(args)\n    pprint.pprint(config)\n    device_ids = [int(d) for d in config.GPUS.split(\',\')]\n    # os.environ[\'CUDA_VISIBLE_DEVICES\'] = config.GPUS\n\n    if ckpt_path is None:\n        _, train_output_path = create_logger(config.OUTPUT_PATH, args.cfg, config.DATASET.TRAIN_IMAGE_SET,\n                                             split=\'train\')\n        model_prefix = os.path.join(train_output_path, config.MODEL_PREFIX)\n        ckpt_path = \'{}-best.model\'.format(model_prefix)\n        print(\'Use best checkpoint {}...\'.format(ckpt_path))\n\n    shutil.copy2(ckpt_path, os.path.join(save_path, \'{}_test_ckpt_{}.model\'.format(config.MODEL_PREFIX, config.DATASET.TASK)))\n\n    # torch.backends.cudnn.enabled = False\n    # torch.backends.cudnn.deterministic = True\n    # torch.backends.cudnn.benchmark = False\n\n    # get network\n    model = eval(config.MODULE)(config)\n    if len(device_ids) > 1:\n        model = torch.nn.DataParallel(model, device_ids=device_ids).cuda()\n    else:\n        model = model.cuda()\n    if args.fp16:\n        [model] = amp.initialize([model],\n                                 opt_level=\'O2\',\n                                 keep_batchnorm_fp32=False)\n    checkpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n    smart_load_model_state_dict(model, checkpoint[\'state_dict\'])\n\n    # loader\n    test_loader = make_dataloader(config, mode=\'test\', distributed=False)\n    test_dataset = test_loader.dataset\n    test_database = test_dataset.database\n\n    # test\n    test_probs = []\n    test_ids = []\n    cur_id = 0\n    model.eval()\n    for nbatch, batch in zip(trange(len(test_loader)), test_loader):\n    # for nbatch, batch in tqdm(enumerate(test_loader)):\n        batch = to_cuda(batch)\n        if config.DATASET.TASK == \'Q2A\':\n            output = model(*batch)\n            probs = F.softmax(output[\'label_logits\'].float(), dim=1)\n            batch_size = probs.shape[0]\n            test_probs.append(probs.float().detach().cpu().numpy())\n            test_ids.append([test_database[cur_id + k][\'annot_id\'] for k in range(batch_size)])\n            cur_id += batch_size\n        elif config.DATASET.TASK == \'QA2R\':\n            conditioned_probs = []\n            for a_id in range(4):\n                q_index_in_batch = test_loader.dataset.data_names.index(\'question\')\n                q_align_mat_index_in_batch = test_loader.dataset.data_names.index(\'question_align_matrix\')\n                batch_ = [*batch]\n                batch_[q_index_in_batch] = batch[q_index_in_batch][:, a_id, :, :]\n                batch_[q_align_mat_index_in_batch] = batch[q_align_mat_index_in_batch][:, a_id, :, :]\n                output = model(*batch_)\n                probs = F.softmax(output[\'label_logits\'].float(), dim=1)\n                conditioned_probs.append(probs.float().detach().cpu().numpy())\n            conditioned_probs = np.concatenate(conditioned_probs, axis=1)\n            test_probs.append(conditioned_probs)\n            test_ids.append([test_database[cur_id + k][\'annot_id\'] for k in range(conditioned_probs.shape[0])])\n            cur_id += conditioned_probs.shape[0]\n        else:\n            raise ValueError(\'Not Support Task {}\'.format(config.DATASET.TASK))\n    test_probs = np.concatenate(test_probs, axis=0)\n    test_ids = np.concatenate(test_ids, axis=0)\n\n    result_npy_path = os.path.join(save_path, \'{}_test_result_{}.npy\'.format(save_name, config.DATASET.TASK))\n    np.save(result_npy_path, test_probs)\n    print(\'result npy saved to {}.\'.format(result_npy_path))\n\n    # generate final result csv\n    if config.DATASET.TASK == \'Q2A\':\n        columns = [\'answer_{}\'.format(i) for i in range(4)]\n    else:\n        columns = [\'rationale_conditioned_on_a{}_{}\'.format(i, j) for i in range(4) for j in range(4)]\n    dataframe = pd.DataFrame(data=test_probs, columns=columns)\n    dataframe[\'annot_id\'] = test_ids\n    dataframe = dataframe.set_index(\'annot_id\', drop=True)\n\n    dataframe.to_csv(result_csv_path)\n    print(\'result csv saved to {}.\'.format(result_csv_path))\n    return result_csv_path\n\n\ndef merge_result(q2a_result_file, qa2r_result_file, output_file):\n    left_df = pd.read_csv(q2a_result_file)\n    right_df = pd.read_csv(qa2r_result_file)\n    merged_df = pd.merge(left_df, right_df, on=\'annot_id\')\n    output_dir = os.path.dirname(output_file)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    merged_df.to_csv(output_file, index=False)\n    print(\'merged result csv saved to {}.\'.format(output_file))\n\n\n\n'"
vcr/function/train.py,20,"b'import os\nimport pprint\nimport shutil\nimport inspect\n\nfrom tensorboardX import SummaryWriter\nimport numpy as np\nimport torch\nimport torch.nn\nimport torch.optim as optim\nimport torch.distributed as distributed\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nfrom common.utils.create_logger import create_logger\nfrom common.utils.misc import summary_parameters, bn_fp16_half_eval\nfrom common.utils.load import smart_resume, smart_partial_load_model_state_dict\nfrom common.trainer import train\nfrom common.metrics.composite_eval_metric import CompositeEvalMetric\nfrom common.metrics import vcr_metrics\nfrom common.callbacks.batch_end_callbacks.speedometer import Speedometer\nfrom common.callbacks.epoch_end_callbacks.validation_monitor import ValidationMonitor\nfrom common.callbacks.epoch_end_callbacks.checkpoint import Checkpoint\nfrom common.lr_scheduler import WarmupMultiStepLR\nfrom common.nlp.bert.optimization import AdamW, WarmupLinearSchedule\nfrom vcr.data.build import make_dataloader, build_dataset, build_transforms\nfrom vcr.modules import *\nfrom vcr.function.val import do_validation\n\ntry:\n    from apex import amp\n    from apex.parallel import DistributedDataParallel as Apex_DDP\nexcept ImportError:\n    pass\n    #raise ImportError(""Please install apex from https://www.github.com/nvidia/apex if you want to use fp16."")\n\n\ndef train_net(args, config):\n    # setup logger\n    logger, final_output_path = create_logger(config.OUTPUT_PATH,\n                                              args.cfg,\n                                              config.DATASET.TRAIN_IMAGE_SET,\n                                              split=\'train\')\n    model_prefix = os.path.join(final_output_path, config.MODEL_PREFIX)\n    if args.log_dir is None:\n        args.log_dir = os.path.join(final_output_path, \'tensorboard_logs\')\n\n    pprint.pprint(args)\n    logger.info(\'training args:{}\\n\'.format(args))\n    pprint.pprint(config)\n    logger.info(\'training config:{}\\n\'.format(pprint.pformat(config)))\n\n    # manually set random seed\n    if config.RNG_SEED > -1:\n        np.random.seed(config.RNG_SEED)\n        torch.random.manual_seed(config.RNG_SEED)\n        torch.cuda.manual_seed_all(config.RNG_SEED)\n\n    # cudnn\n    torch.backends.cudnn.benchmark = False\n    if args.cudnn_off:\n        torch.backends.cudnn.enabled = False\n\n    if args.dist:\n        model = eval(config.MODULE)(config)\n        local_rank = int(os.environ.get(\'LOCAL_RANK\') or 0)\n        config.GPUS = str(local_rank)\n        torch.cuda.set_device(local_rank)\n        master_address = os.environ[\'MASTER_ADDR\']\n        master_port = int(os.environ[\'MASTER_PORT\'] or 23456)\n        world_size = int(os.environ[\'WORLD_SIZE\'] or 1)\n        rank = int(os.environ[\'RANK\'] or 0)\n        if args.slurm:\n            distributed.init_process_group(backend=\'nccl\')\n        else:\n            distributed.init_process_group(\n                backend=\'nccl\',\n                init_method=\'tcp://{}:{}\'.format(master_address, master_port),\n                world_size=world_size,\n                rank=rank,\n                group_name=\'mtorch\')\n        print(f\'native distributed, size: {world_size}, rank: {rank}, local rank: {local_rank}\')\n        torch.cuda.set_device(local_rank)\n        config.GPUS = str(local_rank)\n        model = model.cuda()\n        if not config.TRAIN.FP16:\n            model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n\n        if rank == 0:\n            summary_parameters(model.module if isinstance(model, torch.nn.parallel.DistributedDataParallel) else model,\n                               logger)\n            shutil.copy(args.cfg, final_output_path)\n            shutil.copy(inspect.getfile(eval(config.MODULE)), final_output_path)\n\n        writer = None\n        if args.log_dir is not None:\n            tb_log_dir = os.path.join(args.log_dir, \'rank{}\'.format(rank))\n            if not os.path.exists(tb_log_dir):\n                os.makedirs(tb_log_dir)\n            writer = SummaryWriter(log_dir=tb_log_dir)\n\n        train_loader, train_sampler = make_dataloader(config,\n                                                      mode=\'train\',\n                                                      distributed=True,\n                                                      num_replicas=world_size,\n                                                      rank=rank,\n                                                      expose_sampler=True)\n        val_loader = make_dataloader(config,\n                                     mode=\'val\',\n                                     distributed=True,\n                                     num_replicas=world_size,\n                                     rank=rank)\n\n        batch_size = world_size * (sum(config.TRAIN.BATCH_IMAGES)\n                                   if isinstance(config.TRAIN.BATCH_IMAGES, list)\n                                   else config.TRAIN.BATCH_IMAGES)\n        if config.TRAIN.GRAD_ACCUMULATE_STEPS > 1:\n            batch_size = batch_size * config.TRAIN.GRAD_ACCUMULATE_STEPS\n        base_lr = config.TRAIN.LR * batch_size\n        optimizer_grouped_parameters = [{\'params\': [p for n, p in model.named_parameters() if _k in n],\n                                         \'lr\': base_lr * _lr_mult}\n                                        for _k, _lr_mult in config.TRAIN.LR_MULT]\n        optimizer_grouped_parameters.append({\'params\': [p for n, p in model.named_parameters()\n                                                        if all([_k not in n for _k, _ in config.TRAIN.LR_MULT])]})\n        if config.TRAIN.OPTIMIZER == \'SGD\':\n            optimizer = optim.SGD(optimizer_grouped_parameters,\n                                  lr=config.TRAIN.LR * batch_size,\n                                  momentum=config.TRAIN.MOMENTUM,\n                                  weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'Adam\':\n            optimizer = optim.Adam(optimizer_grouped_parameters,\n                                   lr=config.TRAIN.LR * batch_size,\n                                   weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'AdamW\':\n            optimizer = AdamW(optimizer_grouped_parameters,\n                              lr=config.TRAIN.LR * batch_size,\n                              betas=(0.9, 0.999),\n                              eps=1e-6,\n                              weight_decay=config.TRAIN.WD,\n                              correct_bias=True)\n        else:\n            raise ValueError(\'Not support optimizer {}!\'.format(config.TRAIN.OPTIMIZER))\n        total_gpus = world_size\n\n    else:\n        #os.environ[\'CUDA_VISIBLE_DEVICES\'] = config.GPUS\n        model = eval(config.MODULE)(config)\n        summary_parameters(model, logger)\n        shutil.copy(args.cfg, final_output_path)\n        shutil.copy(inspect.getfile(eval(config.MODULE)), final_output_path)\n        num_gpus = len(config.GPUS.split(\',\'))\n        assert num_gpus <= 1 or (not config.TRAIN.FP16), ""Not support fp16 with torch.nn.DataParallel. "" \\\n                                                         ""Please use amp.parallel.DistributedDataParallel instead.""\n        total_gpus = num_gpus\n        rank = None\n        writer = SummaryWriter(log_dir=args.log_dir) if args.log_dir is not None else None\n\n        # model\n        if num_gpus > 1:\n            model = torch.nn.DataParallel(model, device_ids=[int(d) for d in config.GPUS.split(\',\')]).cuda()\n        else:\n            torch.cuda.set_device(int(config.GPUS))\n            model.cuda()\n\n        # loader\n        train_loader = make_dataloader(config, mode=\'train\', distributed=False)\n        val_loader = make_dataloader(config, mode=\'val\', distributed=False)\n        train_sampler = None\n\n        batch_size = num_gpus * (sum(config.TRAIN.BATCH_IMAGES) if isinstance(config.TRAIN.BATCH_IMAGES, list)\n                                 else config.TRAIN.BATCH_IMAGES)\n        if config.TRAIN.GRAD_ACCUMULATE_STEPS > 1:\n            batch_size = batch_size * config.TRAIN.GRAD_ACCUMULATE_STEPS\n        base_lr = config.TRAIN.LR * batch_size\n        optimizer_grouped_parameters = [{\'params\': [p for n, p in model.named_parameters() if _k in n],\n                                         \'lr\': base_lr * _lr_mult}\n                                        for _k, _lr_mult in config.TRAIN.LR_MULT]\n        optimizer_grouped_parameters.append({\'params\': [p for n, p in model.named_parameters()\n                                                        if all([_k not in n for _k, _ in config.TRAIN.LR_MULT])]})\n\n        if config.TRAIN.OPTIMIZER == \'SGD\':\n            optimizer = optim.SGD(optimizer_grouped_parameters,\n                                  lr=config.TRAIN.LR * batch_size,\n                                  momentum=config.TRAIN.MOMENTUM,\n                                  weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'Adam\':\n            optimizer = optim.Adam(optimizer_grouped_parameters,\n                                   lr=config.TRAIN.LR * batch_size,\n                                   weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'AdamW\':\n            optimizer = AdamW(optimizer_grouped_parameters,\n                              lr=config.TRAIN.LR * batch_size,\n                              betas=(0.9, 0.999),\n                              eps=1e-6,\n                              weight_decay=config.TRAIN.WD,\n                              correct_bias=True)\n        else:\n            raise ValueError(\'Not support optimizer {}!\'.format(config.TRAIN.OPTIMIZER))\n\n    # partial load pretrain state dict\n    if config.NETWORK.PARTIAL_PRETRAIN != """":\n        pretrain_state_dict = torch.load(config.NETWORK.PARTIAL_PRETRAIN, map_location=lambda storage, loc: storage)[\'state_dict\']\n        prefix_change = [prefix_change.split(\'->\') for prefix_change in config.NETWORK.PARTIAL_PRETRAIN_PREFIX_CHANGES]\n\n        pretrain_state_dict_parsed = {}\n        for k, v in pretrain_state_dict.items():\n            no_match = True\n            for pretrain_prefix, new_prefix in prefix_change:\n                if k.startswith(pretrain_prefix):\n                    k = new_prefix + k[len(pretrain_prefix):]\n                    pretrain_state_dict_parsed[k] = v\n                    no_match = False\n                    break\n            if no_match:\n                pretrain_state_dict_parsed[k] = v\n        if \'module.vlbert.relationsip_head.caption_image_relationship.weight\' in pretrain_state_dict \\\n                and config.NETWORK.LOAD_REL_HEAD:\n            pretrain_state_dict_parsed[\'module.final_mlp.1.weight\'] \\\n                = pretrain_state_dict[\'module.vlbert.relationsip_head.caption_image_relationship.weight\'][1:2].float() \\\n                - pretrain_state_dict[\'module.vlbert.relationsip_head.caption_image_relationship.weight\'][0:1].float()\n            pretrain_state_dict_parsed[\'module.final_mlp.1.bias\'] \\\n                = pretrain_state_dict[\'module.vlbert.relationsip_head.caption_image_relationship.bias\'][1:2].float() \\\n                  - pretrain_state_dict[\'module.vlbert.relationsip_head.caption_image_relationship.bias\'][0:1].float()\n        if config.NETWORK.PARTIAL_PRETRAIN_SEGMB_INIT:\n            if isinstance(pretrain_state_dict_parsed[\'module.vlbert._module.token_type_embeddings.weight\'],\n                          torch.HalfTensor):\n                pretrain_state_dict_parsed[\'module.vlbert._module.token_type_embeddings.weight\'] = \\\n                    pretrain_state_dict_parsed[\'module.vlbert._module.token_type_embeddings.weight\'].float()\n            pretrain_state_dict_parsed[\'module.vlbert._module.token_type_embeddings.weight\'][1] = \\\n                pretrain_state_dict_parsed[\'module.vlbert._module.token_type_embeddings.weight\'][0]\n        pretrain_state_dict = pretrain_state_dict_parsed\n\n        smart_partial_load_model_state_dict(model, pretrain_state_dict)\n\n    # metrics\n    train_metrics_list = [vcr_metrics.Accuracy(allreduce=args.dist,\n                                               num_replicas=world_size if args.dist else 1)]\n    val_metrics_list = [vcr_metrics.Accuracy(allreduce=args.dist,\n                                             num_replicas=world_size if args.dist else 1)]\n\n    for output_name, display_name in config.TRAIN.LOSS_LOGGERS:\n        train_metrics_list.append(\n            vcr_metrics.LossLogger(output_name, display_name=display_name, allreduce=args.dist,\n                                   num_replicas=world_size if args.dist else 1))\n\n    train_metrics = CompositeEvalMetric()\n    val_metrics = CompositeEvalMetric()\n    for child_metric in train_metrics_list:\n        train_metrics.add(child_metric)\n    for child_metric in val_metrics_list:\n        val_metrics.add(child_metric)\n\n    # epoch end callbacks\n    epoch_end_callbacks = []\n    if (rank is None) or (rank == 0):\n        epoch_end_callbacks = [Checkpoint(model_prefix, config.CHECKPOINT_FREQUENT)]\n    validation_monitor = ValidationMonitor(do_validation, val_loader, val_metrics,\n                                           host_metric_name=\'Acc\',\n                                           label_index_in_batch=config.DATASET.LABEL_INDEX_IN_BATCH)\n\n    # optimizer initial lr before\n    for group in optimizer.param_groups:\n        group.setdefault(\'initial_lr\', group[\'lr\'])\n\n    # resume/auto-resume\n    if rank is None or rank == 0:\n        smart_resume(model, optimizer, validation_monitor, config, model_prefix, logger)\n    if args.dist:\n        begin_epoch = torch.tensor(config.TRAIN.BEGIN_EPOCH).cuda()\n        distributed.broadcast(begin_epoch, src=0)\n        config.TRAIN.BEGIN_EPOCH = begin_epoch.item()\n\n    # batch end callbacks\n    batch_size = len(config.GPUS.split(\',\')) * config.TRAIN.BATCH_IMAGES\n    batch_end_callbacks = [Speedometer(batch_size, config.LOG_FREQUENT,\n                                       batches_per_epoch=len(train_loader),\n                                       epochs=config.TRAIN.END_EPOCH - config.TRAIN.BEGIN_EPOCH)]\n\n    # setup lr step and lr scheduler\n\n\n    if config.TRAIN.LR_SCHEDULE == \'plateau\':\n        print(""Warning: not support resuming on plateau lr schedule!"")\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                                  mode=\'max\',\n                                                                  factor=config.TRAIN.LR_FACTOR,\n                                                                  patience=1,\n                                                                  verbose=True,\n                                                                  threshold=1e-4,\n                                                                  threshold_mode=\'rel\',\n                                                                  cooldown=2,\n                                                                  min_lr=0,\n                                                                  eps=1e-8)\n    elif config.TRAIN.LR_SCHEDULE == \'triangle\':\n        lr_scheduler = WarmupLinearSchedule(optimizer,\n                                            config.TRAIN.WARMUP_STEPS if config.TRAIN.WARMUP else 0,\n                                            t_total=int(config.TRAIN.END_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS),\n                                            last_epoch=int(config.TRAIN.BEGIN_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS)  - 1)\n    elif config.TRAIN.LR_SCHEDULE == \'step\':\n        lr_iters = [int(epoch * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS) for epoch in config.TRAIN.LR_STEP]\n        lr_scheduler = WarmupMultiStepLR(optimizer, milestones=lr_iters, gamma=config.TRAIN.LR_FACTOR,\n                                         warmup_factor=config.TRAIN.WARMUP_FACTOR,\n                                         warmup_iters=config.TRAIN.WARMUP_STEPS if config.TRAIN.WARMUP else 0,\n                                         warmup_method=config.TRAIN.WARMUP_METHOD,\n                                         last_epoch=int(config.TRAIN.BEGIN_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS)  - 1)\n    else:\n        raise ValueError(""Not support lr schedule: {}."".format(config.TRAIN.LR_SCHEDULE))\n\n    # broadcast parameter and optimizer state from rank 0 before training start\n    if args.dist:\n        for v in model.state_dict().values():\n            distributed.broadcast(v, src=0)\n        # for v in optimizer.state_dict().values():\n        #     distributed.broadcast(v, src=0)\n        best_epoch = torch.tensor(validation_monitor.best_epoch).cuda()\n        best_val = torch.tensor(validation_monitor.best_val).cuda()\n        distributed.broadcast(best_epoch, src=0)\n        distributed.broadcast(best_val, src=0)\n        validation_monitor.best_epoch = best_epoch.item()\n        validation_monitor.best_val = best_val.item()\n\n    # apex: amp fp16 mixed-precision training\n    if config.TRAIN.FP16:\n        # model.apply(bn_fp16_half_eval)\n        model, optimizer = amp.initialize(model, optimizer,\n                                          opt_level=\'O2\',\n                                          keep_batchnorm_fp32=False,\n                                          loss_scale=config.TRAIN.FP16_LOSS_SCALE,\n                                          min_loss_scale=128.0)\n        if args.dist:\n            model = Apex_DDP(model, delay_allreduce=True)\n\n    train(model, optimizer, lr_scheduler, train_loader, train_sampler, train_metrics,\n          config.TRAIN.BEGIN_EPOCH, config.TRAIN.END_EPOCH, logger,\n          rank=rank, batch_end_callbacks=batch_end_callbacks, epoch_end_callbacks=epoch_end_callbacks,\n          writer=writer, validation_monitor=validation_monitor, fp16=config.TRAIN.FP16,\n          clip_grad_norm=config.TRAIN.CLIP_GRAD_NORM,\n          gradient_accumulate_steps=config.TRAIN.GRAD_ACCUMULATE_STEPS)\n\n    return rank, model\n'"
vcr/function/val.py,2,"b""from collections import namedtuple\nimport torch\nfrom common.trainer import to_cuda\n\n\n@torch.no_grad()\ndef do_validation(net, val_loader, metrics, label_index_in_batch):\n    net.eval()\n    metrics.reset()\n    for nbatch, batch in enumerate(val_loader):\n        batch = to_cuda(batch)\n        label = batch[label_index_in_batch]\n        datas = [batch[i] for i in range(len(batch)) if i != label_index_in_batch % len(batch)]\n\n        outputs = net(*datas)\n        outputs.update({'label': label})\n        metrics.update(outputs)\n\n\n@torch.no_grad()\ndef joint_validation(answer_net, rationale_net, answer_val_loader, rationale_val_loader, metrics, label_index_in_batch,\n                     show_progress=False):\n    answer_net.eval()\n    rationale_net.eval()\n    metrics.reset()\n\n    def step(a_batch, r_batch):\n        a_batch = to_cuda(a_batch)\n        a_label = a_batch[label_index_in_batch]\n        a_datas = [a_batch[i] for i in range(len(a_batch)) if i != label_index_in_batch % len(a_batch)]\n        r_batch = to_cuda(r_batch)\n        r_label = r_batch[label_index_in_batch]\n        r_datas = [r_batch[i] for i in range(len(r_batch)) if i != label_index_in_batch % len(r_batch)]\n\n        a_outputs = answer_net(*a_datas)\n        r_outputs = rationale_net(*r_datas)\n        outputs = {'answer_' + k: v for k, v in a_outputs.items()}\n        outputs.update({'rationale_' + k: v for k, v in r_outputs.items()})\n        outputs.update({'answer_label': a_label,\n                        'rationale_label': r_label})\n        metrics.update(outputs)\n\n    if show_progress:\n        from tqdm import tqdm\n        for a_batch, r_batch in tqdm(zip(answer_val_loader, rationale_val_loader)):\n            step(a_batch, r_batch)\n    else:\n        for a_batch, r_batch in zip(answer_val_loader, rationale_val_loader):\n            step(a_batch, r_batch)\n"""
vcr/modules/resnet_vlbert_for_vcr.py,39,"b'import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom external.pytorch_pretrained_bert import BertTokenizer\nfrom common.module import Module\nfrom common.fast_rcnn import FastRCNN\nfrom common.nlp.time_distributed import TimeDistributed\nfrom common.visual_linguistic_bert import VisualLinguisticBert, VisualLinguisticBertMVRCHeadTransform\nfrom common.nlp.roberta import RobertaTokenizer\n\nBERT_WEIGHTS_NAME = \'pytorch_model.bin\'\n\n\nclass ResNetVLBERT(Module):\n    def __init__(self, config):\n\n        super(ResNetVLBERT, self).__init__(config)\n\n        self.enable_cnn_reg_loss = config.NETWORK.ENABLE_CNN_REG_LOSS\n        self.cnn_loss_top = config.NETWORK.CNN_LOSS_TOP\n        if not config.NETWORK.BLIND:\n            self.image_feature_extractor = FastRCNN(config,\n                                                    average_pool=True,\n                                                    final_dim=config.NETWORK.IMAGE_FINAL_DIM,\n                                                    enable_cnn_reg_loss=(self.enable_cnn_reg_loss and not self.cnn_loss_top))\n            if config.NETWORK.VLBERT.object_word_embed_mode == 1:\n                self.object_linguistic_embeddings = nn.Embedding(81, config.NETWORK.VLBERT.hidden_size)\n            elif config.NETWORK.VLBERT.object_word_embed_mode == 2:\n                self.object_linguistic_embeddings = nn.Embedding(1, config.NETWORK.VLBERT.hidden_size)\n            elif config.NETWORK.VLBERT.object_word_embed_mode == 3:\n                self.object_linguistic_embeddings = None\n            else:\n                raise NotImplementedError\n            if self.enable_cnn_reg_loss and self.cnn_loss_top:\n                self.cnn_loss_reg = nn.Sequential(\n                    VisualLinguisticBertMVRCHeadTransform(config.NETWORK.VLBERT),\n                    nn.Dropout(config.NETWORK.CNN_REG_DROPOUT, inplace=False),\n                    nn.Linear(config.NETWORK.VLBERT.hidden_size, 81)\n                )\n        self.image_feature_bn_eval = config.NETWORK.IMAGE_FROZEN_BN\n\n        if \'roberta\' in config.NETWORK.BERT_MODEL_NAME:\n            self.tokenizer = RobertaTokenizer.from_pretrained(config.NETWORK.BERT_MODEL_NAME)\n        else:\n            self.tokenizer = BertTokenizer.from_pretrained(config.NETWORK.BERT_MODEL_NAME)\n\n        language_pretrained_model_path = None\n        if config.NETWORK.BERT_PRETRAINED != \'\':\n            language_pretrained_model_path = \'{}-{:04d}.model\'.format(config.NETWORK.BERT_PRETRAINED,\n                                                                      config.NETWORK.BERT_PRETRAINED_EPOCH)\n        elif os.path.isdir(config.NETWORK.BERT_MODEL_NAME):\n            weight_path = os.path.join(config.NETWORK.BERT_MODEL_NAME, BERT_WEIGHTS_NAME)\n            if os.path.isfile(weight_path):\n                language_pretrained_model_path = weight_path\n\n        if language_pretrained_model_path is None:\n            print(""Warning: no pretrained language model found, training from scratch!!!"")\n\n        self.vlbert = TimeDistributed(\n            VisualLinguisticBert(config.NETWORK.VLBERT,\n                                 language_pretrained_model_path=language_pretrained_model_path)\n        )\n\n        self.for_pretrain = config.NETWORK.FOR_MASK_VL_MODELING_PRETRAIN\n        assert not self.for_pretrain, ""Not implement pretrain mode now!""\n\n        if not self.for_pretrain:\n            dim = config.NETWORK.VLBERT.hidden_size\n            if config.NETWORK.CLASSIFIER_TYPE == ""2fc"":\n                self.final_mlp = torch.nn.Sequential(\n                    torch.nn.Dropout(config.NETWORK.CLASSIFIER_DROPOUT, inplace=False),\n                    torch.nn.Linear(dim, config.NETWORK.CLASSIFIER_HIDDEN_SIZE),\n                    torch.nn.ReLU(inplace=True),\n                    torch.nn.Dropout(config.NETWORK.CLASSIFIER_DROPOUT, inplace=False),\n                    torch.nn.Linear(config.NETWORK.CLASSIFIER_HIDDEN_SIZE, 1),\n                )\n            elif config.NETWORK.CLASSIFIER_TYPE == ""1fc"":\n                self.final_mlp = torch.nn.Sequential(\n                    torch.nn.Dropout(config.NETWORK.CLASSIFIER_DROPOUT, inplace=False),\n                    torch.nn.Linear(dim, 1)\n                )\n            else:\n                raise ValueError(""Not support classifier type: {}!"".format(config.NETWORK.CLASSIFIER_TYPE))\n\n        # init weights\n        self.init_weight()\n\n        self.fix_params()\n\n    def init_weight(self):\n        if not self.config.NETWORK.BLIND:\n            self.image_feature_extractor.init_weight()\n            if self.object_linguistic_embeddings is not None:\n                self.object_linguistic_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n            if self.enable_cnn_reg_loss and self.cnn_loss_top:\n                self.cnn_loss_reg.apply(self.vlbert._module.init_weights)\n\n        if not self.for_pretrain:\n            for m in self.final_mlp.modules():\n                if isinstance(m, torch.nn.Linear):\n                    torch.nn.init.xavier_uniform_(m.weight)\n                    torch.nn.init.constant_(m.bias, 0)\n\n    def train(self, mode=True):\n        super(ResNetVLBERT, self).train(mode)\n        # turn some frozen layers to eval mode\n        if (not self.config.NETWORK.BLIND) and self.image_feature_bn_eval:\n            self.image_feature_extractor.bn_eval()\n\n    def fix_params(self):\n        if self.config.NETWORK.BLIND:\n            self.vlbert._module.visual_scale_text.requires_grad = False\n            self.vlbert._module.visual_scale_object.requires_grad = False\n\n    def _collect_obj_reps(self, span_tags, object_reps):\n        """"""\n        Collect span-level object representations\n        :param span_tags: [batch_size, ..leading_dims.., L]\n        :param object_reps: [batch_size, max_num_objs_per_batch, obj_dim]\n        :return:\n        """"""\n\n        span_tags_fixed = torch.clamp(span_tags, min=0)  # In case there were masked values here\n        row_id = span_tags_fixed.new_zeros(span_tags_fixed.shape)\n        row_id_broadcaster = torch.arange(0, row_id.shape[0], step=1, device=row_id.device)[:, None]\n\n        # Add extra diminsions to the row broadcaster so it matches row_id\n        leading_dims = len(span_tags.shape) - 2\n        for i in range(leading_dims):\n            row_id_broadcaster = row_id_broadcaster[..., None]\n        row_id += row_id_broadcaster\n        return object_reps[row_id.view(-1), span_tags_fixed.view(-1)].view(*span_tags_fixed.shape, -1)\n\n    def prepare_text_from_qa(self, question, question_tags, question_mask, answers, answers_tags, answers_mask):\n        batch_size, max_q_len = question.shape\n        _, num_choices, max_a_len = answers.shape\n        max_len = (question_mask.sum(1) + answers_mask.sum(2).max(1)[0]).max() + 3\n        cls_id, sep_id = self.tokenizer.convert_tokens_to_ids([\'[CLS]\', \'[SEP]\'])\n        question = question.repeat(1, num_choices).view(-1, num_choices, max_q_len)\n        question_mask = question_mask.repeat(1, num_choices).view(-1, num_choices, max_q_len)\n        q_end = 1 + question_mask.sum(2, keepdim=True)\n        a_end = q_end + 1 + answers_mask.sum(2, keepdim=True)\n        input_ids = torch.zeros((batch_size, num_choices, max_len), dtype=question.dtype, device=question.device)\n        input_mask = torch.ones((batch_size, num_choices, max_len), dtype=torch.uint8, device=question.device)\n        input_type_ids = torch.zeros((batch_size, num_choices, max_len), dtype=question.dtype, device=question.device)\n        text_tags = input_type_ids.new_zeros((batch_size, num_choices, max_len))\n        grid_i, grid_j, grid_k = torch.meshgrid(torch.arange(batch_size, device=question.device),\n                                                torch.arange(num_choices, device=question.device),\n                                                torch.arange(max_len, device=question.device))\n\n        input_mask[grid_k > a_end] = 0\n        input_type_ids[(grid_k > q_end) & (grid_k <= a_end)] = 1\n        q_input_mask = (grid_k > 0) & (grid_k < q_end)\n        a_input_mask = (grid_k > q_end) & (grid_k < a_end)\n        input_ids[:, :, 0] = cls_id\n        input_ids[grid_k == q_end] = sep_id\n        input_ids[grid_k == a_end] = sep_id\n        input_ids[q_input_mask] = question[question_mask]\n        input_ids[a_input_mask] = answers[answers_mask]\n        text_tags[q_input_mask] = question_tags[question_mask]\n        text_tags[a_input_mask] = answers_tags[answers_mask]\n\n        return input_ids, input_type_ids, text_tags, input_mask\n\n    def prepare_text_from_qa_onesent(self, question, question_tags, question_mask, answers, answers_tags, answers_mask):\n        batch_size, max_q_len = question.shape\n        _, num_choices, max_a_len = answers.shape\n        max_len = (question_mask.sum(1) + answers_mask.sum(2).max(1)[0]).max() + 2\n        cls_id, sep_id = self.tokenizer.convert_tokens_to_ids([\'[CLS]\', \'[SEP]\'])\n        question = question.repeat(1, num_choices).view(-1, num_choices, max_q_len)\n        question_mask = question_mask.repeat(1, num_choices).view(-1, num_choices, max_q_len)\n        q_end = 1 + question_mask.sum(2, keepdim=True)\n        a_end = q_end + answers_mask.sum(2, keepdim=True)\n        input_ids = torch.zeros((batch_size, num_choices, max_len), dtype=question.dtype, device=question.device)\n        input_mask = torch.ones((batch_size, num_choices, max_len), dtype=torch.uint8, device=question.device)\n        input_type_ids = torch.zeros((batch_size, num_choices, max_len), dtype=question.dtype, device=question.device)\n        text_tags = input_type_ids.new_zeros((batch_size, num_choices, max_len))\n        grid_i, grid_j, grid_k = torch.meshgrid(torch.arange(batch_size, device=question.device),\n                                                torch.arange(num_choices, device=question.device),\n                                                torch.arange(max_len, device=question.device))\n\n        input_mask[grid_k > a_end] = 0\n        q_input_mask = (grid_k > 0) & (grid_k < q_end)\n        a_input_mask = (grid_k >= q_end) & (grid_k < a_end)\n        input_ids[:, :, 0] = cls_id\n        input_ids[grid_k == a_end] = sep_id\n        input_ids[q_input_mask] = question[question_mask]\n        input_ids[a_input_mask] = answers[answers_mask]\n        text_tags[q_input_mask] = question_tags[question_mask]\n        text_tags[a_input_mask] = answers_tags[answers_mask]\n\n        return input_ids, input_type_ids, text_tags, input_mask\n\n    def prepare_text_from_aq(self, question, question_tags, question_mask, answers, answers_tags, answers_mask):\n        batch_size, max_q_len = question.shape\n        _, num_choices, max_a_len = answers.shape\n        max_len = (question_mask.sum(1) + answers_mask.sum(2).max(1)[0]).max() + 3\n        cls_id, sep_id = self.tokenizer.convert_tokens_to_ids([\'[CLS]\', \'[SEP]\'])\n        question = question.repeat(1, num_choices).view(-1, num_choices, max_q_len)\n        question_mask = question_mask.repeat(1, num_choices).view(-1, num_choices, max_q_len)\n        a_end = 1 + answers_mask.sum(2, keepdim=True)\n        q_end = a_end + 1 + question_mask.sum(2, keepdim=True)\n        input_ids = torch.zeros((batch_size, num_choices, max_len), dtype=question.dtype, device=question.device)\n        input_mask = torch.ones((batch_size, num_choices, max_len), dtype=torch.uint8, device=question.device)\n        input_type_ids = torch.zeros((batch_size, num_choices, max_len), dtype=question.dtype, device=question.device)\n        text_tags = input_type_ids.new_zeros((batch_size, num_choices, max_len))\n        grid_i, grid_j, grid_k = torch.meshgrid(torch.arange(batch_size, device=question.device),\n                                                torch.arange(num_choices, device=question.device),\n                                                torch.arange(max_len, device=question.device))\n\n        input_mask[grid_k > q_end] = 0\n        input_type_ids[(grid_k > a_end) & (grid_k <= q_end)] = 1\n        q_input_mask = (grid_k > a_end) & (grid_k < q_end)\n        a_input_mask = (grid_k > 0) & (grid_k < a_end)\n        input_ids[:, :, 0] = cls_id\n        input_ids[grid_k == a_end] = sep_id\n        input_ids[grid_k == q_end] = sep_id\n        input_ids[q_input_mask] = question[question_mask]\n        input_ids[a_input_mask] = answers[answers_mask]\n        text_tags[q_input_mask] = question_tags[question_mask]\n        text_tags[a_input_mask] = answers_tags[answers_mask]\n\n        return input_ids, input_type_ids, text_tags, input_mask\n\n    def train_forward(self,\n                      image,\n                      boxes,\n                      masks,\n                      question,\n                      question_align_matrix,\n                      answer_choices,\n                      answer_align_matrix,\n                      answer_label,\n                      im_info,\n                      mask_position=None,\n                      mask_type=None,\n                      mask_label=None):\n        ###########################################\n\n        # visual feature extraction\n        images = image\n        objects = boxes[:, :, -1]\n        segms = masks\n        boxes = boxes[:, :, :4]\n        box_mask = (boxes[:, :, -1] > - 0.5)\n        max_len = int(box_mask.sum(1).max().item())\n        objects = objects[:, :max_len]\n        box_mask = box_mask[:, :max_len]\n        boxes = boxes[:, :max_len]\n        segms = segms[:, :max_len]\n\n        if self.config.NETWORK.BLIND:\n            obj_reps = {\'obj_reps\': boxes.new_zeros((*boxes.shape[:-1], self.config.NETWORK.IMAGE_FINAL_DIM))}\n        else:\n            obj_reps = self.image_feature_extractor(images=images,\n                                                    boxes=boxes,\n                                                    box_mask=box_mask,\n                                                    im_info=im_info,\n                                                    classes=objects,\n                                                    segms=segms)\n\n        num_choices = answer_choices.shape[1]\n        question_ids = question[:, :, 0]\n        question_tags = question[:, :, 1]\n        question_tags = question_tags.repeat(1, num_choices).view(question_tags.shape[0], num_choices, -1)\n        question_mask = (question[:, :, 0] > 0.5)\n        answer_ids = answer_choices[:, :, :,0]\n        answer_tags = answer_choices[:, :, :, 1]\n        answer_mask = (answer_choices[:, :, :, 0] > 0.5)\n\n        ############################################\n\n        # prepare text\n        if self.config.NETWORK.ANSWER_FIRST:\n            if self.config.NETWORK.QA_ONE_SENT:\n                raise NotImplemented\n            else:\n                text_input_ids, text_token_type_ids, text_tags, text_mask = self.prepare_text_from_aq(\n                    question_ids,\n                    question_tags,\n                    question_mask,\n                    answer_ids,\n                    answer_tags,\n                    answer_mask)\n        else:\n            if self.config.NETWORK.QA_ONE_SENT:\n                text_input_ids, text_token_type_ids, text_tags, text_mask = self.prepare_text_from_qa_onesent(\n                    question_ids,\n                    question_tags,\n                    question_mask,\n                    answer_ids,\n                    answer_tags,\n                    answer_mask)\n            else:\n                text_input_ids, text_token_type_ids, text_tags, text_mask = self.prepare_text_from_qa(\n                    question_ids,\n                    question_tags,\n                    question_mask,\n                    answer_ids,\n                    answer_tags,\n                    answer_mask)\n\n        if self.config.NETWORK.NO_GROUNDING:\n            text_tags.zero_()\n        text_visual_embeddings = self._collect_obj_reps(text_tags, obj_reps[\'obj_reps\'])\n        if self.config.NETWORK.BLIND:\n            object_linguistic_embeddings = boxes.new_zeros((*boxes.shape[:-1], self.config.NETWORK.VLBERT.hidden_size))\n            object_linguistic_embeddings = object_linguistic_embeddings.unsqueeze(1).repeat(1, num_choices, 1, 1)\n        else:\n            if self.config.NETWORK.VLBERT.object_word_embed_mode in [1, 2]:\n                object_linguistic_embeddings = self.object_linguistic_embeddings(\n                    objects.long().clamp(min=0, max=self.object_linguistic_embeddings.weight.data.shape[0] - 1)\n                )\n                object_linguistic_embeddings = object_linguistic_embeddings.unsqueeze(1).repeat(1, num_choices, 1, 1)\n            elif self.config.NETWORK.VLBERT.object_word_embed_mode == 3:\n                cls_id, sep_id = self.tokenizer.convert_tokens_to_ids([\'[CLS]\', \'[SEP]\'])\n                global_context_mask = text_mask & (text_input_ids != cls_id) & (text_input_ids != sep_id)\n                word_embedding = self.vlbert._module.word_embeddings(text_input_ids)\n                word_embedding[global_context_mask == 0] = 0\n                object_linguistic_embeddings = word_embedding.sum(dim=2) / global_context_mask.sum(dim=2, keepdim=True).to(dtype=word_embedding.dtype)\n                object_linguistic_embeddings = object_linguistic_embeddings.unsqueeze(2).repeat((1, 1, max_len, 1))\n        object_vl_embeddings = torch.cat((obj_reps[\'obj_reps\'].unsqueeze(1).repeat(1, num_choices, 1, 1),\n                                          object_linguistic_embeddings), -1)\n\n        ###########################################\n\n        # Visual Linguistic BERT\n\n        if self.config.NETWORK.NO_OBJ_ATTENTION or self.config.NETWORK.BLIND:\n            box_mask.zero_()\n\n        hidden_states_text, hidden_states_objects, pooled_rep = self.vlbert(text_input_ids,\n                                                                          text_token_type_ids,\n                                                                          text_visual_embeddings,\n                                                                          text_mask,\n                                                                          object_vl_embeddings,\n                                                                          box_mask.unsqueeze(1).repeat(1, num_choices, 1),\n                                                                          output_all_encoded_layers=False,\n                                                                          output_text_and_object_separately=True)\n\n        ###########################################\n        outputs = {}\n\n        # classifier\n        logits = self.final_mlp(pooled_rep).squeeze(2)\n\n        # loss\n        if self.config.NETWORK.CLASSIFIER_SIGMOID:\n            _, choice_ind = torch.meshgrid(torch.arange(logits.shape[0], device=logits.device),\n                                           torch.arange(num_choices, device=logits.device))\n            label_binary = (choice_ind == answer_label.unsqueeze(1))\n            if mask_type is not None and self.config.NETWORK.REPLACE_OBJECT_CHANGE_LABEL:\n                label_binary = label_binary * (mask_type != 1).unsqueeze(1)\n            weight = logits.new_zeros(logits.shape).fill_(1.0)\n            weight[label_binary == 1] = self.config.NETWORK.CLASSIFIER_SIGMOID_LOSS_POSITIVE_WEIGHT\n            rescale = (self.config.NETWORK.CLASSIFIER_SIGMOID_LOSS_POSITIVE_WEIGHT + 1.0) \\\n                / (2.0 * self.config.NETWORK.CLASSIFIER_SIGMOID_LOSS_POSITIVE_WEIGHT)\n            ans_loss = rescale * F.binary_cross_entropy_with_logits(logits, label_binary.to(dtype=logits.dtype),\n                                                                    weight=weight)\n            outputs[\'positive_fraction\'] = label_binary.to(dtype=logits.dtype).sum() / label_binary.numel()\n        else:\n            ans_loss = F.cross_entropy(logits, answer_label.long().view(-1))\n\n        outputs.update({\'label_logits\': logits,\n                        \'label\': answer_label.long().view(-1),\n                        \'ans_loss\': ans_loss})\n\n        loss = ans_loss.mean() * self.config.NETWORK.ANS_LOSS_WEIGHT\n\n        if mask_position is not None:\n            assert False, ""Todo: align to original position.""\n            _batch_ind = torch.arange(images.shape[0], dtype=torch.long, device=images.device)\n            mask_pos_rep = hidden_states[_batch_ind, answer_label, mask_position]\n            mask_pred_logits = (obj_reps[\'obj_reps\'] @ mask_pos_rep.unsqueeze(-1)).squeeze(-1)\n            mask_pred_logits[1 - box_mask] -= 10000.0\n            mask_object_loss = F.cross_entropy(mask_pred_logits, mask_label, ignore_index=-1)\n            logits_padded = mask_pred_logits.new_zeros((mask_pred_logits.shape[0], origin_len)).fill_(-10000.0)\n            logits_padded[:, :mask_pred_logits.shape[1]] = mask_pred_logits\n            mask_pred_logits = logits_padded\n            outputs.update({\n                \'mask_object_loss\': mask_object_loss,\n                \'mask_object_logits\': mask_pred_logits,\n                \'mask_object_label\': mask_label})\n            loss = loss + mask_object_loss.mean() * self.config.NETWORK.MASK_OBJECT_LOSS_WEIGHT\n\n        if self.enable_cnn_reg_loss:\n            if not self.cnn_loss_top:\n                loss = loss + obj_reps[\'cnn_regularization_loss\'].mean() * self.config.NETWORK.CNN_LOSS_WEIGHT\n                outputs[\'cnn_regularization_loss\'] = obj_reps[\'cnn_regularization_loss\']\n            else:\n                objects = objects.unsqueeze(1).repeat(1, num_choices, 1)\n                box_mask = box_mask.unsqueeze(1).repeat(1, num_choices, 1)\n                cnn_reg_logits = self.cnn_loss_reg(hidden_states_objects[box_mask])\n                cnn_reg_loss = F.cross_entropy(cnn_reg_logits, objects[box_mask].long())\n                loss = loss + cnn_reg_loss.mean() * self.config.NETWORK.CNN_LOSS_WEIGHT\n                outputs[\'cnn_regularization_loss\'] = cnn_reg_loss\n\n        return outputs, loss\n\n    def inference_forward(self,\n                          image,\n                          boxes,\n                          masks,\n                          question,\n                          question_align_matrix,\n                          answer_choices,\n                          answer_align_matrix,\n                          *args):\n\n        if self.for_pretrain:\n            answer_label, im_info, mask_position, mask_type = args\n        else:\n            assert len(args) == 1\n            im_info = args[0]\n\n        ###########################################\n\n        # visual feature extraction\n        images = image\n        objects = boxes[:, :, -1]\n        segms = masks\n        boxes = boxes[:, :, :4]\n        box_mask = (boxes[:, :, -1] > - 0.5)\n        max_len = int(box_mask.sum(1).max().item())\n        objects = objects[:, :max_len]\n        box_mask = box_mask[:, :max_len]\n        boxes = boxes[:, :max_len]\n        segms = segms[:, :max_len]\n\n        if self.config.NETWORK.BLIND:\n            obj_reps = {\'obj_reps\': boxes.new_zeros((*boxes.shape[:-1], self.config.NETWORK.IMAGE_FINAL_DIM))}\n        else:\n            obj_reps = self.image_feature_extractor(images=images,\n                                                    boxes=boxes,\n                                                    box_mask=box_mask,\n                                                    im_info=im_info,\n                                                    classes=objects,\n                                                    segms=segms)\n\n        num_choices = answer_choices.shape[1]\n        question_ids = question[:, :, 0]\n        question_tags = question[:, :, 1]\n        question_tags = question_tags.repeat(1, num_choices).view(question_tags.shape[0], num_choices, -1)\n        question_mask = (question[:, :, 0] > 0.5)\n        answer_ids = answer_choices[:, :, :, 0]\n        answer_tags = answer_choices[:, :, :, 1]\n        answer_mask = (answer_choices[:, :, :, 0] > 0.5)\n\n        ############################################\n\n        # prepare text\n        if self.config.NETWORK.ANSWER_FIRST:\n            if self.config.NETWORK.QA_ONE_SENT:\n                raise NotImplemented\n            else:\n                text_input_ids, text_token_type_ids, text_tags, text_mask = self.prepare_text_from_aq(\n                    question_ids,\n                    question_tags,\n                    question_mask,\n                    answer_ids,\n                    answer_tags,\n                    answer_mask)\n        else:\n            if self.config.NETWORK.QA_ONE_SENT:\n                text_input_ids, text_token_type_ids, text_tags, text_mask = self.prepare_text_from_qa_onesent(\n                    question_ids,\n                    question_tags,\n                    question_mask,\n                    answer_ids,\n                    answer_tags,\n                    answer_mask)\n            else:\n                text_input_ids, text_token_type_ids, text_tags, text_mask = self.prepare_text_from_qa(\n                    question_ids,\n                    question_tags,\n                    question_mask,\n                    answer_ids,\n                    answer_tags,\n                    answer_mask)\n\n        if self.config.NETWORK.NO_GROUNDING:\n            text_tags.zero_()\n        text_visual_embeddings = self._collect_obj_reps(text_tags, obj_reps[\'obj_reps\'])\n        if self.config.NETWORK.BLIND:\n            object_linguistic_embeddings = boxes.new_zeros(\n                (*boxes.shape[:-1], self.config.NETWORK.VLBERT.hidden_size))\n            object_linguistic_embeddings = object_linguistic_embeddings.unsqueeze(1).repeat(1, num_choices, 1, 1)\n        else:\n            if self.config.NETWORK.VLBERT.object_word_embed_mode in [1, 2]:\n                object_linguistic_embeddings = self.object_linguistic_embeddings(\n                    objects.long().clamp(min=0, max=self.object_linguistic_embeddings.weight.data.shape[0] - 1)\n                )\n                object_linguistic_embeddings = object_linguistic_embeddings.unsqueeze(1).repeat(1, num_choices, 1,\n                                                                                                1)\n            elif self.config.NETWORK.VLBERT.object_word_embed_mode == 3:\n                cls_id, sep_id = self.tokenizer.convert_tokens_to_ids([\'[CLS]\', \'[SEP]\'])\n                global_context_mask = text_mask & (text_input_ids != cls_id) & (text_input_ids != sep_id)\n                word_embedding = self.vlbert._module.word_embeddings(text_input_ids)\n                word_embedding[global_context_mask == 0] = 0\n                object_linguistic_embeddings = word_embedding.sum(dim=2) / global_context_mask.sum(dim=2,\n                                                                                                   keepdim=True).to(\n                    dtype=word_embedding.dtype)\n                object_linguistic_embeddings = object_linguistic_embeddings.unsqueeze(2).repeat((1, 1, max_len, 1))\n        object_vl_embeddings = torch.cat((obj_reps[\'obj_reps\'].unsqueeze(1).repeat(1, num_choices, 1, 1),\n                                          object_linguistic_embeddings), -1)\n\n        ###########################################\n\n        # Visual Linguistic BERT\n\n        if self.config.NETWORK.NO_OBJ_ATTENTION or self.config.NETWORK.BLIND:\n            box_mask.zero_()\n\n        hidden_states_text, hidden_states_objects, pooled_rep = self.vlbert(text_input_ids,\n                                                                          text_token_type_ids,\n                                                                          text_visual_embeddings,\n                                                                          text_mask,\n                                                                          object_vl_embeddings,\n                                                                          box_mask.unsqueeze(1).repeat(1,\n                                                                                                       num_choices,\n                                                                                                       1),\n                                                                          output_all_encoded_layers=False,\n                                                                          output_text_and_object_separately=True)\n\n        ###########################################\n\n        # classifier\n        logits = self.final_mlp(pooled_rep).squeeze(2)\n\n        outputs = {\'label_logits\': logits}\n\n        return outputs\n\n'"
viz/bertviz/attention.py,3,"b'import torch\nfrom collections import defaultdict\n\n\ndef get_attention(model, model_type, tokenizer, sentence_a, sentence_b=None, include_queries_and_keys=False):\n\n    """"""Compute representation of attention to pass to the d3 visualization\n\n    Args:\n        model: pytorch-transformers model\n        model_type: type of model. Valid values \'bert\', \'gpt2\', \'xlnet\', \'roberta\' \n        tokenizer: pytorch-transformers tokenizer\n        sentence_a: Sentence A string\n        sentence_b: Sentence B string\n        include_queries_and_keys: Indicates whether to include queries/keys in results\n\n    Returns:\n      Dictionary of attn representations with the structure:\n      {\n        \'all\': All attention (source = AB, target = AB)\n        \'aa\': Sentence A self-attention (source = A, target = A) (if sentence_b is not None)\n        \'bb\': Sentence B self-attention (source = B, target = B) (if sentence_b is not None)\n        \'ab\': Sentence A -> Sentence B attention (source = A, target = B) (if sentence_b is not None)\n        \'ba\': Sentence B -> Sentence A attention (source = B, target = A) (if sentence_b is not None)\n      }\n      where each value is a dictionary:\n      {\n        \'left_text\': list of source tokens, to be displayed on the left of the vis\n        \'right_text\': list of target tokens, to be displayed on the right of the vis\n        \'attn\': list of attention matrices, one for each layer. Each has shape [num_heads, source_seq_len, target_seq_len]\n        \'queries\' (optional): list of query vector arrays, one for each layer. Each has shape (num_heads, source_seq_len, vector_size)\n        \'keys\' (optional): list of key vector arrays, one for each layer. Each has shape (num_heads, target_seq_len, vector_size)\n      }\n    """"""\n    \n    if model_type not in (\'bert\', \'gpt2\', \'xlnet\', \'roberta\'):\n        raise ValueError(""Invalid model type:"", model_type)\n    if not sentence_a:\n        raise ValueError(""Sentence A is required"")\n    is_sentence_pair = bool(sentence_b)\n    if is_sentence_pair and model_type not in (\'bert\', \'roberta\', \'xlnet\'):\n        raise ValueError(f\'Model {model_type} does not support sentence pairs\')\n    if is_sentence_pair and model_type == \'xlnet\':\n        raise NotImplementedError(""Sentence-pair inputs for XLNet not currently supported."")\n\n    # Prepare inputs to model\n    tokens_a = None\n    tokens_b = None\n    token_type_ids = None\n    if not is_sentence_pair: # Single sentence\n        if model_type in (\'bert\', \'roberta\'):\n            tokens_a = [tokenizer.cls_token] + tokenizer.tokenize(sentence_a) + [tokenizer.sep_token]\n        elif model_type == \'xlnet\':\n            tokens_a = tokenizer.tokenize(sentence_a) + [tokenizer.sep_token] + [tokenizer.cls_token]\n        else:\n            tokens_a = tokenizer.tokenize(sentence_a)\n    else:\n        if model_type == \'bert\':\n            tokens_a = [tokenizer.cls_token] + tokenizer.tokenize(sentence_a) + [tokenizer.sep_token]\n            tokens_b = tokenizer.tokenize(sentence_b) + [tokenizer.sep_token]\n            token_type_ids = torch.LongTensor([[0] * len(tokens_a) + [1] * len(tokens_b)])\n        elif model_type == \'roberta\':\n            tokens_a = [tokenizer.cls_token] + tokenizer.tokenize(sentence_a) + [tokenizer.sep_token]\n            tokens_b = [tokenizer.sep_token] + tokenizer.tokenize(sentence_b) + [tokenizer.sep_token]\n            # Roberta doesn\'t use token type embeddings per https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/convert_roberta_checkpoint_to_pytorch.py\n        else:\n            tokens_b = tokenizer.tokenize(sentence_b)\n\n    token_ids = tokenizer.convert_tokens_to_ids(tokens_a + (tokens_b if tokens_b else []))\n    tokens_tensor = torch.tensor(token_ids).unsqueeze(0)\n\n    # Call model to get attention data\n    model.eval()\n    if token_type_ids is not None:\n        output = model(tokens_tensor, token_type_ids=token_type_ids)\n    else:\n        output = model(tokens_tensor)\n    attn_data_list = output[-1]\n\n    # Populate map with attn data and, optionally, query, key data\n    attn_dict = defaultdict(list)\n    if include_queries_and_keys:\n        queries_dict = defaultdict(list)\n        keys_dict = defaultdict(list)\n\n    if is_sentence_pair:\n        slice_a = slice(0, len(tokens_a))  # Positions corresponding to sentence A in input\n        slice_b = slice(len(tokens_a), len(tokens_a) + len(tokens_b))  # Position corresponding to sentence B in input\n    for layer, attn_data in enumerate(attn_data_list):\n        # Process attention\n        attn = attn_data[\'attn\'][0]  # assume batch_size=1; shape = [num_heads, source_seq_len, target_seq_len]\n        attn_dict[\'all\'].append(attn.tolist())\n        if is_sentence_pair:\n            attn_dict[\'aa\'].append(attn[:, slice_a, slice_a].tolist())  # Append A->A attention for layer, across all heads\n            attn_dict[\'bb\'].append(attn[:, slice_b, slice_b].tolist())  # Append B->B attention for layer, across all heads\n            attn_dict[\'ab\'].append(attn[:, slice_a, slice_b].tolist())  # Append A->B attention for layer, across all heads\n            attn_dict[\'ba\'].append(attn[:, slice_b, slice_a].tolist())  # Append B->A attention for layer, across all heads\n        # Process queries and keys\n        if include_queries_and_keys:\n            queries = attn_data[\'queries\'][0]  # assume batch_size=1; shape = [num_heads, seq_len, vector_size]\n            keys = attn_data[\'keys\'][0]  # assume batch_size=1; shape = [num_heads, seq_len, vector_size]\n            queries_dict[\'all\'].append(queries.tolist())\n            keys_dict[\'all\'].append(keys.tolist())\n            if is_sentence_pair:\n                queries_dict[\'a\'].append(queries[:, slice_a, :].tolist())\n                keys_dict[\'a\'].append(keys[:, slice_a, :].tolist())\n                queries_dict[\'b\'].append(queries[:, slice_b, :].tolist())\n                keys_dict[\'b\'].append(keys[:, slice_b, :].tolist())\n\n    tokens_a = format_special_chars(tokens_a)\n    if tokens_b:\n        tokens_b = format_special_chars(tokens_b)\n    if model_type != \'gpt2\':\n        tokens_a = format_delimiters(tokens_a, tokenizer)\n        if tokens_b:\n            tokens_b = format_delimiters(tokens_b, tokenizer)\n\n    results = {\n        \'all\': {\n            \'attn\': attn_dict[\'all\'],\n            \'left_text\': tokens_a + (tokens_b if tokens_b else []),\n            \'right_text\': tokens_a + (tokens_b if tokens_b else [])\n        }\n    }\n    if is_sentence_pair:\n        results.update({\n            \'aa\': {\n                \'attn\': attn_dict[\'aa\'],\n                \'left_text\': tokens_a,\n                \'right_text\': tokens_a\n            },\n            \'bb\': {\n                \'attn\': attn_dict[\'bb\'],\n                \'left_text\': tokens_b,\n                \'right_text\': tokens_b\n            },\n            \'ab\': {\n                \'attn\': attn_dict[\'ab\'],\n                \'left_text\': tokens_a,\n                \'right_text\': tokens_b\n            },\n            \'ba\': {\n                \'attn\': attn_dict[\'ba\'],\n                \'left_text\': tokens_b,\n                \'right_text\': tokens_a\n            }\n        })\n    if include_queries_and_keys:\n        results[\'all\'].update({\n            \'queries\': queries_dict[\'all\'],\n            \'keys\': keys_dict[\'all\'],\n        })\n        if is_sentence_pair:\n            results[\'aa\'].update({\n                \'queries\': queries_dict[\'a\'],\n                \'keys\': keys_dict[\'a\'],\n            })\n            results[\'bb\'].update({\n                \'queries\': queries_dict[\'b\'],\n                \'keys\': keys_dict[\'b\'],\n            })\n            results[\'ab\'].update({\n                \'queries\': queries_dict[\'a\'],\n                \'keys\': keys_dict[\'b\'],\n            })\n            results[\'ba\'].update({\n                \'queries\': queries_dict[\'b\'],\n                \'keys\': keys_dict[\'a\'],\n            })\n    return results\n\n\ndef format_special_chars(tokens):\n    return [t.replace(\'\xc4\xa0\', \' \').replace(\'\xe2\x96\x81\', \' \') for t in tokens]\n\n\ndef format_delimiters(tokens, tokenizer):\n    formatted_tokens = []\n    for t in tokens:\n        if tokenizer.sep_token:\n            t = t.replace(tokenizer.sep_token, \'[SEP]\')\n        if tokenizer.cls_token:\n            t = t.replace(tokenizer.cls_token, \'[CLS]\')\n        formatted_tokens.append(t)\n    return formatted_tokens'"
vqa/data/build.py,5,"b'import torch.utils.data\n\nfrom .datasets import *\nfrom . import samplers\nfrom .transforms.build import build_transforms\nfrom .collate_batch import BatchCollator\nimport pprint\n\nDATASET_CATALOGS = {\'vqa\': VQA}\n\n\ndef build_dataset(dataset_name, *args, **kwargs):\n    assert dataset_name in DATASET_CATALOGS, ""dataset not in catalogs""\n    return DATASET_CATALOGS[dataset_name](*args, **kwargs)\n\n\ndef make_data_sampler(dataset, shuffle, distributed, num_replicas, rank):\n    if distributed:\n        return samplers.DistributedSampler(dataset, shuffle=shuffle, num_replicas=num_replicas, rank=rank)\n    if shuffle:\n        sampler = torch.utils.data.sampler.RandomSampler(dataset)\n    else:\n        sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n    return sampler\n\n\ndef make_batch_data_sampler(dataset, sampler, aspect_grouping, batch_size):\n    if aspect_grouping:\n        group_ids = dataset.group_ids\n        batch_sampler = samplers.GroupedBatchSampler(\n            sampler, group_ids, batch_size, drop_uneven=False\n        )\n    else:\n        batch_sampler = torch.utils.data.sampler.BatchSampler(\n            sampler, batch_size, drop_last=False\n        )\n    return batch_sampler\n\n\ndef make_dataloader(cfg, dataset=None, mode=\'train\', distributed=False, num_replicas=None, rank=None,\n                    expose_sampler=False):\n    assert mode in [\'train\', \'val\', \'test\']\n    if mode == \'train\':\n        ann_file = cfg.DATASET.TRAIN_ANNOTATION_FILE\n        image_set = cfg.DATASET.TRAIN_IMAGE_SET\n        aspect_grouping = cfg.TRAIN.ASPECT_GROUPING\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.TRAIN.BATCH_IMAGES * num_gpu\n        shuffle = cfg.TRAIN.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n    elif mode == \'val\':\n        ann_file = cfg.DATASET.VAL_ANNOTATION_FILE\n        image_set = cfg.DATASET.VAL_IMAGE_SET\n        aspect_grouping = False\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.VAL.BATCH_IMAGES * num_gpu\n        shuffle = cfg.VAL.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n    else:\n        ann_file = cfg.DATASET.TEST_ANNOTATION_FILE\n        image_set = cfg.DATASET.TEST_IMAGE_SET\n        aspect_grouping = False\n        num_gpu = len(cfg.GPUS.split(\',\'))\n        batch_size = cfg.TEST.BATCH_IMAGES * num_gpu\n        shuffle = cfg.TEST.SHUFFLE\n        num_workers = cfg.NUM_WORKERS_PER_GPU * num_gpu\n\n    transform = build_transforms(cfg, mode)\n\n    if dataset is None:\n\n        dataset = build_dataset(dataset_name=cfg.DATASET.DATASET, ann_file=ann_file, image_set=image_set,\n                                use_imdb=cfg.DATASET.USE_IMDB,\n                                with_precomputed_visual_feat=cfg.NETWORK.IMAGE_FEAT_PRECOMPUTED,\n                                boxes=cfg.DATASET.BOXES,\n                                answer_vocab_file=cfg.DATASET.ANSWER_VOCAB_FILE,\n                                root_path=cfg.DATASET.ROOT_PATH, data_path=cfg.DATASET.DATASET_PATH,\n                                test_mode=(mode == \'test\'), transform=transform,\n                                zip_mode=cfg.DATASET.ZIP_MODE, cache_mode=cfg.DATASET.CACHE_MODE,\n                                cache_db=True if (rank is None or rank == 0) else False,\n                                ignore_db_cache=cfg.DATASET.IGNORE_DB_CACHE,\n                                add_image_as_a_box=cfg.DATASET.ADD_IMAGE_AS_A_BOX,\n                                aspect_grouping=aspect_grouping,\n                                mask_size=(cfg.DATASET.MASK_SIZE, cfg.DATASET.MASK_SIZE),\n                                pretrained_model_name=cfg.NETWORK.BERT_MODEL_NAME)\n\n    sampler = make_data_sampler(dataset, shuffle, distributed, num_replicas, rank)\n    batch_sampler = make_batch_data_sampler(dataset, sampler, aspect_grouping, batch_size)\n    collator = BatchCollator(dataset=dataset, append_ind=cfg.DATASET.APPEND_INDEX)\n\n    dataloader = torch.utils.data.DataLoader(dataset=dataset,\n                                             batch_sampler=batch_sampler,\n                                             num_workers=num_workers,\n                                             pin_memory=False,\n                                             collate_fn=collator)\n    if expose_sampler:\n        return dataloader, sampler\n\n    return dataloader\n'"
vqa/data/collate_batch.py,3,"b""import torch\nfrom common.utils.clip_pad import *\n\n\nclass BatchCollator(object):\n    def __init__(self, dataset, append_ind=False):\n        self.dataset = dataset\n        self.test_mode = self.dataset.test_mode\n        self.data_names = self.dataset.data_names\n        self.append_ind = append_ind\n\n    def __call__(self, batch):\n        if not isinstance(batch, list):\n            batch = list(batch)\n\n        if batch[0][self.data_names.index('image')] is not None:\n            max_shape = tuple(max(s) for s in zip(*[data[self.data_names.index('image')].shape for data in batch]))\n            image_none = False\n        else:\n            image_none = True\n        max_boxes = max([data[self.data_names.index('boxes')].shape[0] for data in batch])\n        max_question_length = max([len(data[self.data_names.index('question')]) for data in batch])\n\n        for i, ibatch in enumerate(batch):\n            out = {}\n\n            if image_none:\n                out['image'] = None\n            else:\n                image = ibatch[self.data_names.index('image')]\n                out['image'] = clip_pad_images(image, max_shape, pad=0)\n\n            boxes = ibatch[self.data_names.index('boxes')]\n            out['boxes'] = clip_pad_boxes(boxes, max_boxes, pad=-2)\n\n            question = ibatch[self.data_names.index('question')]\n            out['question'] = clip_pad_1d(question, max_question_length, pad=0)\n\n            other_names = [data_name for data_name in self.data_names if data_name not in out]\n            for name in other_names:\n                out[name] = torch.as_tensor(ibatch[self.data_names.index(name)])\n\n            batch[i] = tuple(out[data_name] for data_name in self.data_names)\n            if self.append_ind:\n                batch[i] += (torch.tensor(i, dtype=torch.int64),)\n\n        out_tuple = ()\n        for items in zip(*batch):\n            if items[0] is None:\n                out_tuple += (None,)\n            else:\n                out_tuple += (torch.stack(tuple(items), dim=0), )\n\n        return out_tuple\n\n"""
vqa/function/test.py,8,"b""import os\nimport pprint\nimport shutil\n\nimport json\nfrom tqdm import tqdm, trange\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom common.utils.load import smart_load_model_state_dict\nfrom common.trainer import to_cuda\nfrom common.utils.create_logger import create_logger\nfrom vqa.data.build import make_dataloader\nfrom vqa.modules import *\n\n\n@torch.no_grad()\ndef test_net(args, config, ckpt_path=None, save_path=None, save_name=None):\n    print('test net...')\n    pprint.pprint(args)\n    pprint.pprint(config)\n    device_ids = [int(d) for d in config.GPUS.split(',')]\n    # os.environ['CUDA_VISIBLE_DEVICES'] = config.GPUS\n\n    torch.backends.cudnn.enabled = False\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    if ckpt_path is None:\n        _, train_output_path = create_logger(config.OUTPUT_PATH, args.cfg, config.DATASET.TRAIN_IMAGE_SET,\n                                             split='train')\n        model_prefix = os.path.join(train_output_path, config.MODEL_PREFIX)\n        ckpt_path = '{}-best.model'.format(model_prefix)\n        print('Use best checkpoint {}...'.format(ckpt_path))\n    if save_path is None:\n        logger, test_output_path = create_logger(config.OUTPUT_PATH, args.cfg, config.DATASET.TEST_IMAGE_SET,\n                                                 split='test')\n        save_path = test_output_path\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    shutil.copy2(ckpt_path,\n                 os.path.join(save_path, '{}_test_ckpt_{}.model'.format(config.MODEL_PREFIX, config.DATASET.TASK)))\n\n    # get network\n    model = eval(config.MODULE)(config)\n    if len(device_ids) > 1:\n        model = torch.nn.DataParallel(model, device_ids=device_ids).cuda()\n    else:\n        torch.cuda.set_device(device_ids[0])\n        model = model.cuda()\n    checkpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n    smart_load_model_state_dict(model, checkpoint['state_dict'])\n\n    # loader\n    test_loader = make_dataloader(config, mode='test', distributed=False)\n    test_dataset = test_loader.dataset\n    test_database = test_dataset.database\n\n    # test\n    q_ids = []\n    answer_ids = []\n    model.eval()\n    cur_id = 0\n    for nbatch, batch in zip(trange(len(test_loader)), test_loader):\n    # for nbatch, batch in tqdm(enumerate(test_loader)):\n        bs = test_loader.batch_sampler.batch_size if test_loader.batch_sampler is not None else test_loader.batch_size\n        q_ids.extend([test_database[id]['question_id'] for id in range(cur_id, min(cur_id + bs, len(test_database)))])\n        batch = to_cuda(batch)\n        output = model(*batch)\n        answer_ids.extend(output['label_logits'].argmax(dim=1).detach().cpu().tolist())\n        cur_id += bs\n\n    result = [{'question_id': q_id, 'answer': test_dataset.answer_vocab[a_id]} for q_id, a_id in zip(q_ids, answer_ids)]\n\n    cfg_name = os.path.splitext(os.path.basename(args.cfg))[0]\n    result_json_path = os.path.join(save_path, '{}_vqa2_{}.json'.format(cfg_name if save_name is None else save_name,\n                                                                        config.DATASET.TEST_IMAGE_SET))\n    with open(result_json_path, 'w') as f:\n        json.dump(result, f)\n    print('result json saved to {}.'.format(result_json_path))\n    return result_json_path\n"""
vqa/function/train.py,21,"b'import os\nimport pprint\nimport shutil\nimport inspect\n\nfrom tensorboardX import SummaryWriter\nimport numpy as np\nimport torch\nimport torch.nn\nimport torch.optim as optim\nimport torch.distributed as distributed\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nfrom common.utils.create_logger import create_logger\nfrom common.utils.misc import summary_parameters, bn_fp16_half_eval\nfrom common.utils.load import smart_resume, smart_partial_load_model_state_dict\nfrom common.trainer import train\nfrom common.metrics.composite_eval_metric import CompositeEvalMetric\nfrom common.metrics import vqa_metrics\nfrom common.callbacks.batch_end_callbacks.speedometer import Speedometer\nfrom common.callbacks.epoch_end_callbacks.validation_monitor import ValidationMonitor\nfrom common.callbacks.epoch_end_callbacks.checkpoint import Checkpoint\nfrom common.lr_scheduler import WarmupMultiStepLR\nfrom common.nlp.bert.optimization import AdamW, WarmupLinearSchedule\nfrom vqa.data.build import make_dataloader, build_dataset, build_transforms\nfrom vqa.modules import *\nfrom vqa.function.val import do_validation\n\ntry:\n    from apex import amp\n    from apex.parallel import DistributedDataParallel as Apex_DDP\nexcept ImportError:\n    pass\n    #raise ImportError(""Please install apex from https://www.github.com/nvidia/apex if you want to use fp16."")\n\n\ndef train_net(args, config):\n    # setup logger\n    logger, final_output_path = create_logger(config.OUTPUT_PATH, args.cfg, config.DATASET.TRAIN_IMAGE_SET,\n                                              split=\'train\')\n    model_prefix = os.path.join(final_output_path, config.MODEL_PREFIX)\n    if args.log_dir is None:\n        args.log_dir = os.path.join(final_output_path, \'tensorboard_logs\')\n\n    pprint.pprint(args)\n    logger.info(\'training args:{}\\n\'.format(args))\n    pprint.pprint(config)\n    logger.info(\'training config:{}\\n\'.format(pprint.pformat(config)))\n\n    # manually set random seed\n    if config.RNG_SEED > -1:\n        np.random.seed(config.RNG_SEED)\n        torch.random.manual_seed(config.RNG_SEED)\n        torch.cuda.manual_seed_all(config.RNG_SEED)\n\n    # cudnn\n    torch.backends.cudnn.benchmark = False\n    if args.cudnn_off:\n        torch.backends.cudnn.enabled = False\n\n    if args.dist:\n        model = eval(config.MODULE)(config)\n        local_rank = int(os.environ.get(\'LOCAL_RANK\') or 0)\n        config.GPUS = str(local_rank)\n        torch.cuda.set_device(local_rank)\n        master_address = os.environ[\'MASTER_ADDR\']\n        master_port = int(os.environ[\'MASTER_PORT\'] or 23456)\n        world_size = int(os.environ[\'WORLD_SIZE\'] or 1)\n        rank = int(os.environ[\'RANK\'] or 0)\n        if args.slurm:\n            distributed.init_process_group(backend=\'nccl\')\n        else:\n            distributed.init_process_group(\n                backend=\'nccl\',\n                init_method=\'tcp://{}:{}\'.format(master_address, master_port),\n                world_size=world_size,\n                rank=rank,\n                group_name=\'mtorch\')\n        print(f\'native distributed, size: {world_size}, rank: {rank}, local rank: {local_rank}\')\n        torch.cuda.set_device(local_rank)\n        config.GPUS = str(local_rank)\n        model = model.cuda()\n        if not config.TRAIN.FP16:\n            model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n\n        if rank == 0:\n            summary_parameters(model.module if isinstance(model, torch.nn.parallel.DistributedDataParallel) else model,\n                               logger)\n            shutil.copy(args.cfg, final_output_path)\n            shutil.copy(inspect.getfile(eval(config.MODULE)), final_output_path)\n\n        writer = None\n        if args.log_dir is not None:\n            tb_log_dir = os.path.join(args.log_dir, \'rank{}\'.format(rank))\n            if not os.path.exists(tb_log_dir):\n                os.makedirs(tb_log_dir)\n            writer = SummaryWriter(log_dir=tb_log_dir)\n\n        train_loader, train_sampler = make_dataloader(config,\n                                                      mode=\'train\',\n                                                      distributed=True,\n                                                      num_replicas=world_size,\n                                                      rank=rank,\n                                                      expose_sampler=True)\n        val_loader = make_dataloader(config,\n                                     mode=\'val\',\n                                     distributed=True,\n                                     num_replicas=world_size,\n                                     rank=rank)\n\n        batch_size = world_size * (sum(config.TRAIN.BATCH_IMAGES)\n                                   if isinstance(config.TRAIN.BATCH_IMAGES, list)\n                                   else config.TRAIN.BATCH_IMAGES)\n        if config.TRAIN.GRAD_ACCUMULATE_STEPS > 1:\n            batch_size = batch_size * config.TRAIN.GRAD_ACCUMULATE_STEPS\n        base_lr = config.TRAIN.LR * batch_size\n        optimizer_grouped_parameters = [{\'params\': [p for n, p in model.named_parameters() if _k in n],\n                                         \'lr\': base_lr * _lr_mult}\n                                        for _k, _lr_mult in config.TRAIN.LR_MULT]\n        optimizer_grouped_parameters.append({\'params\': [p for n, p in model.named_parameters()\n                                                        if all([_k not in n for _k, _ in config.TRAIN.LR_MULT])]})\n        if config.TRAIN.OPTIMIZER == \'SGD\':\n            optimizer = optim.SGD(optimizer_grouped_parameters,\n                                  lr=config.TRAIN.LR * batch_size,\n                                  momentum=config.TRAIN.MOMENTUM,\n                                  weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'Adam\':\n            optimizer = optim.Adam(optimizer_grouped_parameters,\n                                   lr=config.TRAIN.LR * batch_size,\n                                   weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'AdamW\':\n            optimizer = AdamW(optimizer_grouped_parameters,\n                              lr=config.TRAIN.LR * batch_size,\n                              betas=(0.9, 0.999),\n                              eps=1e-6,\n                              weight_decay=config.TRAIN.WD,\n                              correct_bias=True)\n        else:\n            raise ValueError(\'Not support optimizer {}!\'.format(config.TRAIN.OPTIMIZER))\n        total_gpus = world_size\n\n    else:\n        #os.environ[\'CUDA_VISIBLE_DEVICES\'] = config.GPUS\n        model = eval(config.MODULE)(config)\n        summary_parameters(model, logger)\n        shutil.copy(args.cfg, final_output_path)\n        shutil.copy(inspect.getfile(eval(config.MODULE)), final_output_path)\n        num_gpus = len(config.GPUS.split(\',\'))\n        assert num_gpus <= 1 or (not config.TRAIN.FP16), ""Not support fp16 with torch.nn.DataParallel. "" \\\n                                                         ""Please use amp.parallel.DistributedDataParallel instead.""\n        total_gpus = num_gpus\n        rank = None\n        writer = SummaryWriter(log_dir=args.log_dir) if args.log_dir is not None else None\n\n        # model\n        if num_gpus > 1:\n            model = torch.nn.DataParallel(model, device_ids=[int(d) for d in config.GPUS.split(\',\')]).cuda()\n        else:\n            torch.cuda.set_device(int(config.GPUS))\n            model.cuda()\n\n        # loader\n        train_loader = make_dataloader(config, mode=\'train\', distributed=False)\n        val_loader = make_dataloader(config, mode=\'val\', distributed=False)\n        train_sampler = None\n\n        batch_size = num_gpus * (sum(config.TRAIN.BATCH_IMAGES) if isinstance(config.TRAIN.BATCH_IMAGES, list)\n                                 else config.TRAIN.BATCH_IMAGES)\n        if config.TRAIN.GRAD_ACCUMULATE_STEPS > 1:\n            batch_size = batch_size * config.TRAIN.GRAD_ACCUMULATE_STEPS\n        base_lr = config.TRAIN.LR * batch_size\n        optimizer_grouped_parameters = [{\'params\': [p for n, p in model.named_parameters() if _k in n],\n                                         \'lr\': base_lr * _lr_mult}\n                                        for _k, _lr_mult in config.TRAIN.LR_MULT]\n        optimizer_grouped_parameters.append({\'params\': [p for n, p in model.named_parameters()\n                                                        if all([_k not in n for _k, _ in config.TRAIN.LR_MULT])]})\n\n        if config.TRAIN.OPTIMIZER == \'SGD\':\n            optimizer = optim.SGD(optimizer_grouped_parameters,\n                                  lr=config.TRAIN.LR * batch_size,\n                                  momentum=config.TRAIN.MOMENTUM,\n                                  weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'Adam\':\n            optimizer = optim.Adam(optimizer_grouped_parameters,\n                                   lr=config.TRAIN.LR * batch_size,\n                                   weight_decay=config.TRAIN.WD)\n        elif config.TRAIN.OPTIMIZER == \'AdamW\':\n            optimizer = AdamW(optimizer_grouped_parameters,\n                              lr=config.TRAIN.LR * batch_size,\n                              betas=(0.9, 0.999),\n                              eps=1e-6,\n                              weight_decay=config.TRAIN.WD,\n                              correct_bias=True)\n        else:\n            raise ValueError(\'Not support optimizer {}!\'.format(config.TRAIN.OPTIMIZER))\n\n    # partial load pretrain state dict\n    if config.NETWORK.PARTIAL_PRETRAIN != """":\n        pretrain_state_dict = torch.load(config.NETWORK.PARTIAL_PRETRAIN, map_location=lambda storage, loc: storage)[\'state_dict\']\n        prefix_change = [prefix_change.split(\'->\') for prefix_change in config.NETWORK.PARTIAL_PRETRAIN_PREFIX_CHANGES]\n        if len(prefix_change) > 0:\n            pretrain_state_dict_parsed = {}\n            for k, v in pretrain_state_dict.items():\n                no_match = True\n                for pretrain_prefix, new_prefix in prefix_change:\n                    if k.startswith(pretrain_prefix):\n                        k = new_prefix + k[len(pretrain_prefix):]\n                        pretrain_state_dict_parsed[k] = v\n                        no_match = False\n                        break\n                if no_match:\n                    pretrain_state_dict_parsed[k] = v\n            pretrain_state_dict = pretrain_state_dict_parsed\n        smart_partial_load_model_state_dict(model, pretrain_state_dict)\n\n    # pretrained classifier\n    if config.NETWORK.CLASSIFIER_PRETRAINED:\n        print(\'Initializing classifier weight from pretrained word embeddings...\')\n        answers_word_embed = []\n        for k, v in model.state_dict().items():\n            if \'word_embeddings.weight\' in k:\n                word_embeddings = v.detach().clone()\n                break\n        for answer in train_loader.dataset.answer_vocab:\n            a_tokens = train_loader.dataset.tokenizer.tokenize(answer)\n            a_ids = train_loader.dataset.tokenizer.convert_tokens_to_ids(a_tokens)\n            a_word_embed = (torch.stack([word_embeddings[a_id] for a_id in a_ids], dim=0)).mean(dim=0)\n            answers_word_embed.append(a_word_embed)\n        answers_word_embed_tensor = torch.stack(answers_word_embed, dim=0)\n        for name, module in model.named_modules():\n            if name.endswith(\'final_mlp\'):\n                module[-1].weight.data = answers_word_embed_tensor.to(device=module[-1].weight.data.device)\n\n    # metrics\n    train_metrics_list = [vqa_metrics.SoftAccuracy(allreduce=args.dist,\n                                                   num_replicas=world_size if args.dist else 1)]\n    val_metrics_list = [vqa_metrics.SoftAccuracy(allreduce=args.dist,\n                                                 num_replicas=world_size if args.dist else 1)]\n    for output_name, display_name in config.TRAIN.LOSS_LOGGERS:\n        train_metrics_list.append(\n            vqa_metrics.LossLogger(output_name, display_name=display_name, allreduce=args.dist,\n                                   num_replicas=world_size if args.dist else 1))\n\n    train_metrics = CompositeEvalMetric()\n    val_metrics = CompositeEvalMetric()\n    for child_metric in train_metrics_list:\n        train_metrics.add(child_metric)\n    for child_metric in val_metrics_list:\n        val_metrics.add(child_metric)\n\n    # epoch end callbacks\n    epoch_end_callbacks = []\n    if (rank is None) or (rank == 0):\n        epoch_end_callbacks = [Checkpoint(model_prefix, config.CHECKPOINT_FREQUENT)]\n    validation_monitor = ValidationMonitor(do_validation, val_loader, val_metrics,\n                                           host_metric_name=\'SoftAcc\',\n                                           label_index_in_batch=config.DATASET.LABEL_INDEX_IN_BATCH)\n\n    # optimizer initial lr before\n    for group in optimizer.param_groups:\n        group.setdefault(\'initial_lr\', group[\'lr\'])\n\n    # resume/auto-resume\n    if rank is None or rank == 0:\n        smart_resume(model, optimizer, validation_monitor, config, model_prefix, logger)\n    if args.dist:\n        begin_epoch = torch.tensor(config.TRAIN.BEGIN_EPOCH).cuda()\n        distributed.broadcast(begin_epoch, src=0)\n        config.TRAIN.BEGIN_EPOCH = begin_epoch.item()\n\n    # batch end callbacks\n    batch_size = len(config.GPUS.split(\',\')) * config.TRAIN.BATCH_IMAGES\n    batch_end_callbacks = [Speedometer(batch_size, config.LOG_FREQUENT,\n                                       batches_per_epoch=len(train_loader),\n                                       epochs=config.TRAIN.END_EPOCH - config.TRAIN.BEGIN_EPOCH)]\n\n    # setup lr step and lr scheduler\n    if config.TRAIN.LR_SCHEDULE == \'plateau\':\n        print(""Warning: not support resuming on plateau lr schedule!"")\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                                  mode=\'max\',\n                                                                  factor=config.TRAIN.LR_FACTOR,\n                                                                  patience=1,\n                                                                  verbose=True,\n                                                                  threshold=1e-4,\n                                                                  threshold_mode=\'rel\',\n                                                                  cooldown=2,\n                                                                  min_lr=0,\n                                                                  eps=1e-8)\n    elif config.TRAIN.LR_SCHEDULE == \'triangle\':\n        lr_scheduler = WarmupLinearSchedule(optimizer,\n                                            config.TRAIN.WARMUP_STEPS if config.TRAIN.WARMUP else 0,\n                                            t_total=int(config.TRAIN.END_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS),\n                                            last_epoch=int(config.TRAIN.BEGIN_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS)  - 1)\n    elif config.TRAIN.LR_SCHEDULE == \'step\':\n        lr_iters = [int(epoch * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS) for epoch in config.TRAIN.LR_STEP]\n        lr_scheduler = WarmupMultiStepLR(optimizer, milestones=lr_iters, gamma=config.TRAIN.LR_FACTOR,\n                                         warmup_factor=config.TRAIN.WARMUP_FACTOR,\n                                         warmup_iters=config.TRAIN.WARMUP_STEPS if config.TRAIN.WARMUP else 0,\n                                         warmup_method=config.TRAIN.WARMUP_METHOD,\n                                         last_epoch=int(config.TRAIN.BEGIN_EPOCH * len(train_loader) / config.TRAIN.GRAD_ACCUMULATE_STEPS)  - 1)\n    else:\n        raise ValueError(""Not support lr schedule: {}."".format(config.TRAIN.LR_SCHEDULE))\n\n    # broadcast parameter and optimizer state from rank 0 before training start\n    if args.dist:\n        for v in model.state_dict().values():\n            distributed.broadcast(v, src=0)\n        # for v in optimizer.state_dict().values():\n        #     distributed.broadcast(v, src=0)\n        best_epoch = torch.tensor(validation_monitor.best_epoch).cuda()\n        best_val = torch.tensor(validation_monitor.best_val).cuda()\n        distributed.broadcast(best_epoch, src=0)\n        distributed.broadcast(best_val, src=0)\n        validation_monitor.best_epoch = best_epoch.item()\n        validation_monitor.best_val = best_val.item()\n\n    # apex: amp fp16 mixed-precision training\n    if config.TRAIN.FP16:\n        # model.apply(bn_fp16_half_eval)\n        model, optimizer = amp.initialize(model, optimizer,\n                                          opt_level=\'O2\',\n                                          keep_batchnorm_fp32=False,\n                                          loss_scale=config.TRAIN.FP16_LOSS_SCALE,\n                                          min_loss_scale=32.0)\n        if args.dist:\n            model = Apex_DDP(model, delay_allreduce=True)\n\n    train(model, optimizer, lr_scheduler, train_loader, train_sampler, train_metrics,\n          config.TRAIN.BEGIN_EPOCH, config.TRAIN.END_EPOCH, logger,\n          rank=rank, batch_end_callbacks=batch_end_callbacks, epoch_end_callbacks=epoch_end_callbacks,\n          writer=writer, validation_monitor=validation_monitor, fp16=config.TRAIN.FP16,\n          clip_grad_norm=config.TRAIN.CLIP_GRAD_NORM,\n          gradient_accumulate_steps=config.TRAIN.GRAD_ACCUMULATE_STEPS)\n\n    return rank, model\n'"
vqa/function/val.py,1,"b""from collections import namedtuple\nimport torch\nfrom common.trainer import to_cuda\n\n\n@torch.no_grad()\ndef do_validation(net, val_loader, metrics, label_index_in_batch):\n    net.eval()\n    metrics.reset()\n    for nbatch, batch in enumerate(val_loader):\n        batch = to_cuda(batch)\n        label = batch[label_index_in_batch]\n        datas = [batch[i] for i in range(len(batch)) if i != label_index_in_batch % len(batch)]\n\n        outputs = net(*datas)\n        outputs.update({'label': label})\n        metrics.update(outputs)\n\n"""
vqa/modules/resnet_vlbert_for_vqa.py,26,"b'import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom external.pytorch_pretrained_bert import BertTokenizer\nfrom external.pytorch_pretrained_bert.modeling import BertPredictionHeadTransform\nfrom common.module import Module\nfrom common.fast_rcnn import FastRCNN\nfrom common.visual_linguistic_bert import VisualLinguisticBert\n\nBERT_WEIGHTS_NAME = \'pytorch_model.bin\'\n\n\nclass ResNetVLBERT(Module):\n    def __init__(self, config):\n\n        super(ResNetVLBERT, self).__init__(config)\n\n        self.enable_cnn_reg_loss = config.NETWORK.ENABLE_CNN_REG_LOSS\n        if not config.NETWORK.BLIND:\n            self.image_feature_extractor = FastRCNN(config,\n                                                    average_pool=True,\n                                                    final_dim=config.NETWORK.IMAGE_FINAL_DIM,\n                                                    enable_cnn_reg_loss=self.enable_cnn_reg_loss)\n            if config.NETWORK.VLBERT.object_word_embed_mode == 1:\n                self.object_linguistic_embeddings = nn.Embedding(81, config.NETWORK.VLBERT.hidden_size)\n            elif config.NETWORK.VLBERT.object_word_embed_mode == 2:\n                self.object_linguistic_embeddings = nn.Embedding(1, config.NETWORK.VLBERT.hidden_size)\n            elif config.NETWORK.VLBERT.object_word_embed_mode == 3:\n                self.object_linguistic_embeddings = None\n            else:\n                raise NotImplementedError\n        self.image_feature_bn_eval = config.NETWORK.IMAGE_FROZEN_BN\n\n        self.tokenizer = BertTokenizer.from_pretrained(config.NETWORK.BERT_MODEL_NAME)\n\n        language_pretrained_model_path = None\n        if config.NETWORK.BERT_PRETRAINED != \'\':\n            language_pretrained_model_path = \'{}-{:04d}.model\'.format(config.NETWORK.BERT_PRETRAINED,\n                                                                      config.NETWORK.BERT_PRETRAINED_EPOCH)\n        elif os.path.isdir(config.NETWORK.BERT_MODEL_NAME):\n            weight_path = os.path.join(config.NETWORK.BERT_MODEL_NAME, BERT_WEIGHTS_NAME)\n            if os.path.isfile(weight_path):\n                language_pretrained_model_path = weight_path\n        self.language_pretrained_model_path = language_pretrained_model_path\n        if language_pretrained_model_path is None:\n            print(""Warning: no pretrained language model found, training from scratch!!!"")\n\n        self.vlbert = VisualLinguisticBert(config.NETWORK.VLBERT,\n                                         language_pretrained_model_path=language_pretrained_model_path)\n\n        # self.hm_out = nn.Linear(config.NETWORK.VLBERT.hidden_size, config.NETWORK.VLBERT.hidden_size)\n        # self.hi_out = nn.Linear(config.NETWORK.VLBERT.hidden_size, config.NETWORK.VLBERT.hidden_size)\n\n        dim = config.NETWORK.VLBERT.hidden_size\n        if config.NETWORK.CLASSIFIER_TYPE == ""2fc"":\n            self.final_mlp = torch.nn.Sequential(\n                torch.nn.Dropout(config.NETWORK.CLASSIFIER_DROPOUT, inplace=False),\n                torch.nn.Linear(dim, config.NETWORK.CLASSIFIER_HIDDEN_SIZE),\n                torch.nn.ReLU(inplace=True),\n                torch.nn.Dropout(config.NETWORK.CLASSIFIER_DROPOUT, inplace=False),\n                torch.nn.Linear(config.NETWORK.CLASSIFIER_HIDDEN_SIZE, config.DATASET.ANSWER_VOCAB_SIZE),\n            )\n        elif config.NETWORK.CLASSIFIER_TYPE == ""1fc"":\n            self.final_mlp = torch.nn.Sequential(\n                torch.nn.Dropout(config.NETWORK.CLASSIFIER_DROPOUT, inplace=False),\n                torch.nn.Linear(dim, config.DATASET.ANSWER_VOCAB_SIZE)\n            )\n        elif config.NETWORK.CLASSIFIER_TYPE == \'mlm\':\n            transform = BertPredictionHeadTransform(config.NETWORK.VLBERT)\n            linear = nn.Linear(config.NETWORK.VLBERT.hidden_size, config.DATASET.ANSWER_VOCAB_SIZE)\n            self.final_mlp = nn.Sequential(\n                transform,\n                nn.Dropout(config.NETWORK.CLASSIFIER_DROPOUT, inplace=False),\n                linear\n            )\n        else:\n            raise ValueError(""Not support classifier type: {}!"".format(config.NETWORK.CLASSIFIER_TYPE))\n\n        # init weights\n        self.init_weight()\n\n        self.fix_params()\n\n    def init_weight(self):\n        # self.hm_out.weight.data.normal_(mean=0.0, std=0.02)\n        # self.hm_out.bias.data.zero_()\n        # self.hi_out.weight.data.normal_(mean=0.0, std=0.02)\n        # self.hi_out.bias.data.zero_()\n        self.image_feature_extractor.init_weight()\n        if self.object_linguistic_embeddings is not None:\n            self.object_linguistic_embeddings.weight.data.normal_(mean=0.0, std=0.02)\n        for m in self.final_mlp.modules():\n            if isinstance(m, torch.nn.Linear):\n                torch.nn.init.xavier_uniform_(m.weight)\n                torch.nn.init.constant_(m.bias, 0)\n        if self.config.NETWORK.CLASSIFIER_TYPE == \'mlm\':\n            language_pretrained = torch.load(self.language_pretrained_model_path)\n            mlm_transform_state_dict = {}\n            pretrain_keys = []\n            for k, v in language_pretrained.items():\n                if k.startswith(\'cls.predictions.transform.\'):\n                    pretrain_keys.append(k)\n                    k_ = k[len(\'cls.predictions.transform.\'):]\n                    if \'gamma\' in k_:\n                        k_ = k_.replace(\'gamma\', \'weight\')\n                    if \'beta\' in k_:\n                        k_ = k_.replace(\'beta\', \'bias\')\n                    mlm_transform_state_dict[k_] = v\n            print(""loading pretrained classifier transform keys: {}."".format(pretrain_keys))\n            self.final_mlp[0].load_state_dict(mlm_transform_state_dict)\n\n    def train(self, mode=True):\n        super(ResNetVLBERT, self).train(mode)\n        # turn some frozen layers to eval mode\n        if self.image_feature_bn_eval:\n            self.image_feature_extractor.bn_eval()\n\n    def fix_params(self):\n        pass\n\n    def _collect_obj_reps(self, span_tags, object_reps):\n        """"""\n        Collect span-level object representations\n        :param span_tags: [batch_size, ..leading_dims.., L]\n        :param object_reps: [batch_size, max_num_objs_per_batch, obj_dim]\n        :return:\n        """"""\n\n        span_tags_fixed = torch.clamp(span_tags, min=0)  # In case there were masked values here\n        row_id = span_tags_fixed.new_zeros(span_tags_fixed.shape)\n        row_id_broadcaster = torch.arange(0, row_id.shape[0], step=1, device=row_id.device)[:, None]\n\n        # Add extra diminsions to the row broadcaster so it matches row_id\n        leading_dims = len(span_tags.shape) - 2\n        for i in range(leading_dims):\n            row_id_broadcaster = row_id_broadcaster[..., None]\n        row_id += row_id_broadcaster\n        return object_reps[row_id.view(-1), span_tags_fixed.view(-1)].view(*span_tags_fixed.shape, -1)\n\n    def prepare_text_from_qa(self, question, question_tags, question_mask, answer, answer_tags, answer_mask):\n        batch_size, max_q_len = question.shape\n        _, max_a_len = answer.shape\n        max_len = (question_mask.sum(1) + answer_mask.sum(1)).max() + 3\n        cls_id, sep_id = self.tokenizer.convert_tokens_to_ids([\'[CLS]\', \'[SEP]\'])\n        q_end = 1 + question_mask.sum(1, keepdim=True)\n        a_end = q_end + 1 + answer_mask.sum(1, keepdim=True)\n        input_ids = torch.zeros((batch_size, max_len), dtype=question.dtype, device=question.device)\n        input_mask = torch.ones((batch_size, max_len), dtype=torch.uint8, device=question.device)\n        input_type_ids = torch.zeros((batch_size, max_len), dtype=question.dtype, device=question.device)\n        text_tags = input_type_ids.new_zeros((batch_size, max_len))\n        grid_i, grid_j = torch.meshgrid(torch.arange(batch_size, device=question.device),\n                                        torch.arange(max_len, device=question.device))\n\n        input_mask[grid_j > a_end] = 0\n        input_type_ids[(grid_j > q_end) & (grid_j <= a_end)] = 1\n        q_input_mask = (grid_j > 0) & (grid_j < q_end)\n        a_input_mask = (grid_j > q_end) & (grid_j < a_end)\n        input_ids[:, 0] = cls_id\n        input_ids[grid_j == q_end] = sep_id\n        input_ids[grid_j == a_end] = sep_id\n        input_ids[q_input_mask] = question[question_mask]\n        input_ids[a_input_mask] = answer[answer_mask]\n        text_tags[q_input_mask] = question_tags[question_mask]\n        text_tags[a_input_mask] = answer_tags[answer_mask]\n\n        return input_ids, input_type_ids, text_tags, input_mask, (a_end - 1).squeeze(1)\n\n    def train_forward(self,\n                      image,\n                      boxes,\n                      im_info,\n                      question,\n                      label,\n                      ):\n        ###########################################\n\n        # visual feature extraction\n        images = image\n        box_mask = (boxes[:, :, 0] > - 1.5)\n        max_len = int(box_mask.sum(1).max().item())\n        box_mask = box_mask[:, :max_len]\n        boxes = boxes[:, :max_len]\n\n        obj_reps = self.image_feature_extractor(images=images,\n                                                boxes=boxes,\n                                                box_mask=box_mask,\n                                                im_info=im_info,\n                                                classes=None,\n                                                segms=None)\n\n        question_ids = question\n        question_tags = question.new_zeros(question_ids.shape)\n        question_mask = (question > 0.5)\n\n        answer_ids = question_ids.new_zeros((question_ids.shape[0], 1)).fill_(\n            self.tokenizer.convert_tokens_to_ids([\'[MASK]\'])[0])\n        answer_mask = question_mask.new_zeros(answer_ids.shape).fill_(1)\n        answer_tags = question_tags.new_zeros(answer_ids.shape)\n\n        ############################################\n\n        # prepare text\n        text_input_ids, text_token_type_ids, text_tags, text_mask, ans_pos = self.prepare_text_from_qa(question_ids,\n                                                                                                       question_tags,\n                                                                                                       question_mask,\n                                                                                                       answer_ids,\n                                                                                                       answer_tags,\n                                                                                                       answer_mask)\n        if self.config.NETWORK.NO_GROUNDING:\n            obj_rep_zeroed = obj_reps[\'obj_reps\'].new_zeros(obj_reps[\'obj_reps\'].shape)\n            text_tags.zero_()\n            text_visual_embeddings = self._collect_obj_reps(text_tags, obj_rep_zeroed)\n        else:\n            text_visual_embeddings = self._collect_obj_reps(text_tags, obj_reps[\'obj_reps\'])\n\n        assert self.config.NETWORK.VLBERT.object_word_embed_mode == 2\n        object_linguistic_embeddings = self.object_linguistic_embeddings(\n            boxes.new_zeros((boxes.shape[0], boxes.shape[1])).long()\n        )\n        object_vl_embeddings = torch.cat((obj_reps[\'obj_reps\'], object_linguistic_embeddings), -1)\n\n        ###########################################\n\n        # Visual Linguistic BERT\n\n        hidden_states, hc = self.vlbert(text_input_ids,\n                                      text_token_type_ids,\n                                      text_visual_embeddings,\n                                      text_mask,\n                                      object_vl_embeddings,\n                                      box_mask,\n                                      output_all_encoded_layers=False)\n        _batch_inds = torch.arange(question.shape[0], device=question.device)\n\n        hm = hidden_states[_batch_inds, ans_pos]\n        # hm = F.tanh(self.hm_out(hidden_states[_batch_inds, ans_pos]))\n        # hi = F.tanh(self.hi_out(hidden_states[_batch_inds, ans_pos + 2]))\n\n        ###########################################\n        outputs = {}\n\n        # classifier\n        # logits = self.final_mlp(hc * hm * hi)\n        # logits = self.final_mlp(hc)\n        logits = self.final_mlp(hm)\n\n        # loss\n        ans_loss = F.binary_cross_entropy_with_logits(logits, label) * label.size(1)\n\n        outputs.update({\'label_logits\': logits,\n                        \'label\': label,\n                        \'ans_loss\': ans_loss})\n\n        loss = ans_loss.mean()\n\n        return outputs, loss\n\n    def inference_forward(self,\n                          image,\n                          boxes,\n                          im_info,\n                          question):\n\n        ###########################################\n\n        # visual feature extraction\n        images = image\n        box_mask = (boxes[:, :, 0] > - 1.5)\n        max_len = int(box_mask.sum(1).max().item())\n        box_mask = box_mask[:, :max_len]\n        boxes = boxes[:, :max_len]\n\n        obj_reps = self.image_feature_extractor(images=images,\n                                                boxes=boxes,\n                                                box_mask=box_mask,\n                                                im_info=im_info,\n                                                classes=None,\n                                                segms=None)\n\n        question_ids = question\n        question_tags = question.new_zeros(question_ids.shape)\n        question_mask = (question > 0.5)\n\n        answer_ids = question_ids.new_zeros((question_ids.shape[0], 1)).fill_(\n            self.tokenizer.convert_tokens_to_ids([\'[MASK]\'])[0])\n        answer_mask = question_mask.new_zeros(answer_ids.shape).fill_(1)\n        answer_tags = question_tags.new_zeros(answer_ids.shape)\n\n        ############################################\n\n        # prepare text\n        text_input_ids, text_token_type_ids, text_tags, text_mask, ans_pos = self.prepare_text_from_qa(question_ids,\n                                                                                                       question_tags,\n                                                                                                       question_mask,\n                                                                                                       answer_ids,\n                                                                                                       answer_tags,\n                                                                                                       answer_mask)\n        if self.config.NETWORK.NO_GROUNDING:\n            obj_rep_zeroed = obj_reps[\'obj_reps\'].new_zeros(obj_reps[\'obj_reps\'].shape)\n            text_tags.zero_()\n            text_visual_embeddings = self._collect_obj_reps(text_tags, obj_rep_zeroed)\n        else:\n            text_visual_embeddings = self._collect_obj_reps(text_tags, obj_reps[\'obj_reps\'])\n\n        assert self.config.NETWORK.VLBERT.object_word_embed_mode == 2\n        object_linguistic_embeddings = self.object_linguistic_embeddings(\n            boxes.new_zeros((boxes.shape[0], boxes.shape[1])).long()\n        )\n        object_vl_embeddings = torch.cat((obj_reps[\'obj_reps\'], object_linguistic_embeddings), -1)\n\n        ###########################################\n\n        # Visual Linguistic BERT\n\n        hidden_states, hc = self.vlbert(text_input_ids,\n                                      text_token_type_ids,\n                                      text_visual_embeddings,\n                                      text_mask,\n                                      object_vl_embeddings,\n                                      box_mask,\n                                      output_all_encoded_layers=False)\n        _batch_inds = torch.arange(question.shape[0], device=question.device)\n\n        hm = hidden_states[_batch_inds, ans_pos]\n        # hm = F.tanh(self.hm_out(hidden_states[_batch_inds, ans_pos]))\n        # hi = F.tanh(self.hi_out(hidden_states[_batch_inds, ans_pos + 2]))\n\n        ###########################################\n        outputs = {}\n\n        # classifier\n        # logits = self.final_mlp(hc * hm * hi)\n        # logits = self.final_mlp(hc)\n        logits = self.final_mlp(hm)\n\n        outputs.update({\'label_logits\': logits})\n\n        return outputs\n'"
common/backbone/resnet/resnet.py,12,"b'""""""\nModified from torchvision, but exposes features from different stages\n""""""\n\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport torch\nimport warnings\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\nmodel_layers = {\n    \'resnet18\': [2, 2, 2, 2],\n    \'resnet34\': [3, 4, 6, 3],\n    \'resnet50\': [3, 4, 6, 3],\n    \'resnet101\': [3, 4, 23, 3],\n    \'resnet152\': [3, 8, 36, 3],\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1, padding=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, dilation=dilation,\n                     padding=padding, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, **kwargs):\n        super(BasicBlock, self).__init__()\n        # if dilation == 1:\n        #     self.conv1 = conv3x3(inplanes, planes, stride, dilation)\n        # elif dilation == 2:\n        #     self.conv1 = conv3x3(inplanes, planes, stride, dilation, padding=2)\n        # else:\n        #     raise ValueError(\'dilation must be 1 or 2!\')\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation, padding=dilation)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, stride_in_1x1=False):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1 if not stride_in_1x1 else stride, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        # if dilation == 1:\n        #     self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride if not stride_in_1x1 else 1,\n        #                            dilation=dilation, padding=1, bias=False)\n        # elif dilation == 2:\n        #     self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride if not stride_in_1x1 else 1,\n        #                            dilation=dilation, padding=2, bias=False)\n        # else:\n        #     raise ValueError(\'dilation must be 1 or 2!\')\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride if not stride_in_1x1 else 1,\n                               dilation=dilation, padding=dilation, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        \n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=None, expose_stages=None, dilations=None, stride_in_1x1=False):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        layers_planes = [64, 128, 256, 512]\n        layers_strides = [1, 2, 2, 2]\n        layers_dilations = dilations if dilations is not None else [1, 1, 1, 1]\n        for i, dilation in enumerate(layers_dilations):\n            if dilation == 2:\n                layers_strides[i] = 1\n        layers_planes = layers_planes[:len(layers)]\n        layers_strides = layers_strides[:len(layers)]\n        layers_dilations = layers_dilations[:len(layers)]\n        for i, (planes, blocks, stride, dilation) in enumerate(zip(layers_planes, layers, layers_strides, layers_dilations)):\n            layer = self._make_layer(block, planes, blocks, stride=stride, dilation=dilation, stride_in_1x1=stride_in_1x1)\n            self.__setattr__(\'layer{}\'.format(i + 1), layer)\n        self.num_layers = i + 1\n        self.has_fc_head = 6 in expose_stages\n        self.expose_stages = expose_stages\n        if self.has_fc_head:\n            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n            self.fc = nn.Linear(512 * block.expansion, num_classes)\n            self.expose_stages.remove(6)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, stride_in_1x1=False):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, dilation, stride_in_1x1=stride_in_1x1))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        expose_feats = {}\n        feats = {}\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        feats[\'body1\'] = x\n\n        for i in range(self.num_layers):\n            x = self.__getattr__(""layer{}"".format(i + 1))(x)\n            feats[\'body{}\'.format(i + 2)] = x\n\n        if self.has_fc_head:\n            x = self.avgpool(x)\n            x = x.view(x.size(0), -1)\n            x = self.fc(x)\n            expose_feats[\'cls_score\'] = x\n\n        if self.expose_stages is not None:\n            for expose_stage in self.expose_stages:\n                feat_name = \'body{}\'.format(expose_stage)\n                expose_feats[feat_name] = feats[feat_name]\n\n        return expose_feats\n\n    def load_pretrained_state_dict(self, state_dict):\n        """"""Load state dict of pretrained model\n        Args:\n            state_dict (dict): state dict to load\n        """"""\n        new_state_dict = self.state_dict()\n        miss_keys = []\n        for k in new_state_dict.keys():\n            if k in state_dict.keys():\n                new_state_dict[k] = state_dict[k]\n            else:\n                miss_keys.append(k)\n        if len(miss_keys) > 0:\n            warnings.warn(\'miss keys: {}\'.format(miss_keys))\n        self.load_state_dict(new_state_dict)\n\n    def frozen_parameters(self, frozen_stages=None, frozen_bn=False):\n        if frozen_bn:\n            for module in self.modules():\n                if isinstance(module, nn.BatchNorm2d):\n                    for param in module.parameters():\n                        param.requires_grad = False\n        if frozen_stages is not None:\n            for stage in frozen_stages:\n                assert (stage >= 1) and (stage <= 6)\n                if stage == 1:\n                    for param in self.conv1.parameters():\n                        param.requires_grad = False\n                    for param in self.bn1.parameters():\n                        param.requires_grad = False\n                elif stage < 6:\n                    for param in self.__getattr__(""layer{}"".format(stage - 1)).parameters():\n                        param.requires_grad = False\n                else:\n                    for param in self.fc.parameters():\n                        param.requires_grad = False\n\n    def bn_eval(self):\n        for module in self.modules():\n            if isinstance(module, nn.BatchNorm2d):\n                module.eval()\n\n\ndef resnet18(pretrained=False, pretrained_model_path=None, num_classes=None, expose_stages=None, dilations=None, **kwargs):\n    """"""Constructs a ResNet-18 model\n    Args:\n        pretrained (bool): if True, load pretrained model. Default: False\n        pretrained_model_path (str, optional): only effective when pretrained=True,\n                                            if not specified, use pretrained model from model_zoo.\n        num_classes (int): number of classes for the fc output score.\n        expose_stages (list, optional): list of expose stages, e.g. [4, 5] means expose conv4 and conv5 stage output.\n                                        if not specified, only expose output of end_stage.\n    """"""\n\n    if num_classes is None:\n        assert expose_stages is not None, ""num_class and expose_stages is both None""\n        assert 6 not in expose_stages, ""can\'t expose the 6th stage for num_classes is None""\n\n    if expose_stages is None:\n        expose_stages = [6]\n\n    end_stage = max(expose_stages)\n    assert end_stage <= 6, ""the max expose_stage is out of range""\n\n    layers = model_layers[\'resnet18\'][:end_stage - 1]\n\n    model = ResNet(block=BasicBlock, layers=layers, num_classes=num_classes, expose_stages=expose_stages, dilations=dilations)\n\n    if pretrained:\n        if pretrained_model_path is not None:\n            state_dict = torch.load(pretrained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            state_dict = model_zoo.load_url(model_urls[\'resnet18\'])\n        model.load_pretrained_state_dict(state_dict)\n    return model\n\n\ndef resnet34(pretrained=False, pretrained_model_path=None, num_classes=None, expose_stages=None, dilations=None, **kwargs):\n    """"""Constructs a ResNet-34 model\n    Args:\n        pretrained (bool): if True, load pretrained model. Default: False\n        pretrained_model_path (str, optional): only effective when pretrained=True,\n                                            if not specified, use pretrained model from model_zoo.\n        num_classes (int): number of classes for the fc output score.\n        expose_stages (list, optional): list of expose stages, e.g. [4, 5] means expose conv4 and conv5 stage output.\n                                        if not specified, only expose output of end_stage.\n    """"""\n\n    if num_classes is None:\n        assert expose_stages is not None, ""num_class and expose_stages is both None""\n        assert 6 not in expose_stages, ""can\'t expose the 6th stage for num_classes is None""\n\n    if expose_stages is None:\n        expose_stages = [6]\n\n    end_stage = max(expose_stages)\n    assert end_stage <= 6, ""the max expose_stage is out of range""\n\n    layers = model_layers[\'resnet34\'][:end_stage - 1]\n\n    model = ResNet(block=BasicBlock, layers=layers, num_classes=num_classes, expose_stages=expose_stages,\n                   dilations=dilations)\n\n    if pretrained:\n        if pretrained_model_path is not None:\n            state_dict = torch.load(pretrained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            state_dict = model_zoo.load_url(model_urls[\'resnet34\'])\n        model.load_pretrained_state_dict(state_dict)\n    return model\n\n\ndef resnet50(pretrained=False, pretrained_model_path=None, num_classes=None, expose_stages=None, dilations=None, stride_in_1x1=False):\n    """"""Constructs a ResNet-50 model\n    Args:\n        pretrained (bool): if True, load pretrained model. Default: False\n        pretrained_model_path (str, optional): only effective when pretrained=True,\n                                            if not specified, use pretrained model from model_zoo.\n        num_classes (int): number of classes for the fc output score.\n        expose_stages (list, optional): list of expose stages, e.g. [4, 5] means expose conv4 and conv5 stage output.\n                                        if not specified, only expose output of end_stage.\n    """"""\n\n    if num_classes is None:\n        assert expose_stages is not None, ""num_class and expose_stages is both None""\n        assert 6 not in expose_stages, ""can\'t expose the 6th stage for num_classes is None""\n\n    if expose_stages is None:\n        expose_stages = [6]\n\n    end_stage = max(expose_stages)\n    assert end_stage <= 6, ""the max expose_stage is out of range""\n\n    layers = model_layers[\'resnet50\'][:end_stage - 1]\n\n    model = ResNet(block=Bottleneck, layers=layers, num_classes=num_classes, expose_stages=expose_stages,\n                   dilations=dilations, stride_in_1x1=stride_in_1x1)\n\n    if pretrained:\n        if pretrained_model_path is not None:\n            state_dict = torch.load(pretrained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            state_dict = model_zoo.load_url(model_urls[\'resnet50\'])\n        model.load_pretrained_state_dict(state_dict)\n    return model\n\n\ndef resnet101(pretrained=False, pretrained_model_path=None, num_classes=None, expose_stages=None, dilations=None, stride_in_1x1=False):\n    """"""Constructs a ResNet-101 model\n    Args:\n        pretrained (bool): if True, load pretrained model. Default: False\n        pretrained_model_path (str, optional): only effective when pretrained=True,\n                                            if not specified, use pretrained model from model_zoo.\n        num_classes (int): number of classes for the fc output score.\n        expose_stages (list, optional): list of expose stages, e.g. [4, 5] means expose conv4 and conv5 stage output.\n                                        if not specified, only expose output of end_stage.\n    """"""\n\n    if num_classes is None:\n        assert expose_stages is not None, ""num_class and expose_stages is both None""\n        assert 6 not in expose_stages, ""can\'t expose the 6th stage for num_classes is None""\n\n    if expose_stages is None:\n        expose_stages = [6]\n\n    end_stage = max(expose_stages)\n    assert end_stage <= 6, ""the max expose_stage is out of range""\n\n    layers = model_layers[\'resnet101\'][:end_stage - 1]\n\n    model = ResNet(block=Bottleneck, layers=layers, num_classes=num_classes, expose_stages=expose_stages,\n                   dilations=dilations, stride_in_1x1=stride_in_1x1)\n\n    if pretrained:\n        if pretrained_model_path is not None:\n            state_dict = torch.load(pretrained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            state_dict = model_zoo.load_url(model_urls[\'resnet101\'])\n        model.load_pretrained_state_dict(state_dict)\n    return model\n\n\ndef resnet152(pretrained=False, pretrained_model_path=None, num_classes=None, expose_stages=None, dilations=None, stride_in_1x1=False):\n    """"""Constructs a ResNet-152 model\n    Args:\n        pretrained (bool): if True, load pretrained model. Default: False\n        pretrained_model_path (str, optional): only effective when pretrained=True,\n                                            if not specified, use pretrained model from model_zoo.\n        num_classes (int): number of classes for the fc output score.\n        expose_stages (list, optional): list of expose stages, e.g. [4, 5] means expose conv4 and conv5 stage output.\n                                        if not specified, only expose output of end_stage.\n    """"""\n\n    if num_classes is None:\n        assert expose_stages is not None, ""num_class and expose_stages is both None""\n        assert 6 not in expose_stages, ""can\'t expose the 6th stage for num_classes is None""\n\n    if expose_stages is None:\n        expose_stages = [6]\n\n    end_stage = max(expose_stages)\n    assert end_stage <= 6, ""the max expose_stage is out of range""\n\n    layers = model_layers[\'resnet152\'][:end_stage - 1]\n\n    model = ResNet(block=Bottleneck, layers=layers, num_classes=num_classes, expose_stages=expose_stages,\n                   dilations=dilations, stride_in_1x1=stride_in_1x1)\n\n    if pretrained:\n        if pretrained_model_path is not None:\n            state_dict = torch.load(pretrained_model_path, map_location=lambda storage, loc: storage)\n        else:\n            state_dict = model_zoo.load_url(model_urls[\'resnet152\'])\n        model.load_pretrained_state_dict(state_dict)\n    return model\n'"
common/callbacks/epoch_end_callbacks/checkpoint.py,2,"b""import torch\n\n\nclass Checkpoint(object):\n    def __init__(self, prefix, frequent):\n        super(Checkpoint, self).__init__()\n        self.prefix = prefix\n        self.frequent = frequent\n\n    def __call__(self, epoch_num, net, optimizer, writer, validation_monitor=None):\n        if (epoch_num + 1) % self.frequent == 0:\n            param_name = '{}-{:04d}.model'.format(self.prefix, epoch_num)\n            checkpoint_dict = dict()\n            checkpoint_dict['state_dict'] = net.state_dict()\n            checkpoint_dict['optimizer'] = optimizer.state_dict()\n            save_to_best = False\n            if validation_monitor is not None:\n                checkpoint_dict['validation_monitor'] = validation_monitor.state_dict()\n                if validation_monitor.best_epoch == epoch_num:\n                    save_to_best = True\n            torch.save(checkpoint_dict, param_name)\n            if save_to_best:\n                best_param_name = '{}-best.model'.format(self.prefix)\n                torch.save(checkpoint_dict, best_param_name)\n                print('Save new best model to {}.'.format(best_param_name))\n"""
common/lib/roi_pooling/debug.py,3,"b'import torch\nfrom roi_pool import ROIPool\nfrom roi_align import ROIAlign\n\nalign = ROIAlign(output_size=(3, 3), spatial_scale=1.0, sampling_ratio=1)\npool = ROIPool(output_size=(3, 3), spatial_scale=1.0)\n\ndevice = torch.device(""cuda:0"")\n\nfeature = torch.arange(81*2*3).view((2,3,9,9)).float().to(device)\nrois = torch.Tensor([[0,0,0,9,9],[1,0,0,9,9],[1,0,0,7,7]]).to(device)\n\npooled = pool(feature,rois)\naligned = align(feature,rois)\n\nimport IPython\nIPython.embed()\n'"
common/lib/roi_pooling/roi_align.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom . import C_ROIPooling\n\n\nclass _ROIAlign(Function):\n    @staticmethod\n    def forward(ctx, input, rois, output_size, spatial_scale, sampling_ratio):\n        ctx.save_for_backward(rois)\n        ctx.output_size = _pair(output_size)\n        ctx.spatial_scale = spatial_scale\n        ctx.sampling_ratio = sampling_ratio\n        ctx.input_shape = input.size()\n        output = C_ROIPooling.roi_align_forward(\n            input, rois, spatial_scale, output_size[0], output_size[1], sampling_ratio\n        )\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        rois, = ctx.saved_tensors\n        output_size = ctx.output_size\n        spatial_scale = ctx.spatial_scale\n        sampling_ratio = ctx.sampling_ratio\n        bs, ch, h, w = ctx.input_shape\n        grad_input = C_ROIPooling.roi_align_backward(\n            grad_output,\n            rois,\n            spatial_scale,\n            output_size[0],\n            output_size[1],\n            bs,\n            ch,\n            h,\n            w,\n            sampling_ratio,\n        )\n        return grad_input, None, None, None, None\n\n\nroi_align = _ROIAlign.apply\n\n\nclass ROIAlign(nn.Module):\n    def __init__(self, output_size, spatial_scale, sampling_ratio=1):\n        """"""\n        :param output_size: e.g. (3,3)\n        :param spatial_scale: e.g. 1.0/16\n        :param sampling_ratio: e.g. 1\n        """"""\n        super(ROIAlign, self).__init__()\n        self.output_size = output_size\n        self.spatial_scale = spatial_scale\n        self.sampling_ratio = sampling_ratio\n\n    def forward(self, input, rois):\n        """"""\n        :param input: the input features [B C H W]\n        :param rois: [k, 5]: (im_index, x1, y1, x2, y2)\n        :return: pooled features [K C H W], K = k\n        """"""\n        return roi_align(\n            input.float(), rois.float(), self.output_size, self.spatial_scale, self.sampling_ratio\n        )\n\n    def __repr__(self):\n        tmpstr = self.__class__.__name__ + ""(""\n        tmpstr += ""output_size="" + str(self.output_size)\n        tmpstr += "", spatial_scale="" + str(self.spatial_scale)\n        tmpstr += "", sampling_ratio="" + str(self.sampling_ratio)\n        tmpstr += "")""\n        return tmpstr\n'"
common/lib/roi_pooling/roi_pool.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom . import C_ROIPooling\n\n\nclass _ROIPool(Function):\n    @staticmethod\n    def forward(ctx, input, rois, output_size, spatial_scale):\n        ctx.output_size = _pair(output_size)\n        ctx.spatial_scale = spatial_scale\n        ctx.input_shape = input.size()\n        output, argmax = C_ROIPooling.roi_pool_forward(\n            input, rois, spatial_scale, output_size[0], output_size[1]\n        )\n        ctx.save_for_backward(input, rois, argmax)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        input, rois, argmax = ctx.saved_tensors\n        output_size = ctx.output_size\n        spatial_scale = ctx.spatial_scale\n        bs, ch, h, w = ctx.input_shape\n        grad_input = C_ROIPooling.roi_pool_backward(\n            grad_output,\n            input,\n            rois,\n            argmax,\n            spatial_scale,\n            output_size[0],\n            output_size[1],\n            bs,\n            ch,\n            h,\n            w,\n        )\n        return grad_input, None, None, None\n\n\nroi_pool = _ROIPool.apply\n\n\nclass ROIPool(nn.Module):\n    def __init__(self, output_size, spatial_scale):\n        """"""\n        :param output_size: e.g. (3,3)\n        :param spatial_scale: e.g. 1.0/16\n        """"""\n        super(ROIPool, self).__init__()\n        self.output_size = output_size\n        self.spatial_scale = spatial_scale\n\n    def forward(self, input, rois):\n        """"""\n        :param input: the input features [B C H W]\n        :param rois: [k, 5] : (im_index, x1, y1, x2, y2)\n        :return: pooled features (K C H W), K = k\n        """"""\n        return roi_pool(input.float(), rois.float(), self.output_size, self.spatial_scale)\n\n    def __repr__(self):\n        tmpstr = self.__class__.__name__ + ""(""\n        tmpstr += ""output_size="" + str(self.output_size)\n        tmpstr += "", spatial_scale="" + str(self.spatial_scale)\n        tmpstr += "")""\n        return tmpstr\n'"
common/lib/roi_pooling/setup.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n#!/usr/bin/env python\n\nimport glob\nimport os\n\nimport torch\nfrom setuptools import find_packages\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import CUDA_HOME\nfrom torch.utils.cpp_extension import CppExtension\nfrom torch.utils.cpp_extension import CUDAExtension\n\nrequirements = [""torch"", ""torchvision""]\n\n\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = this_dir\n\n    main_file = glob.glob(os.path.join(extensions_dir, ""*.cpp""))\n    source_cpu = glob.glob(os.path.join(extensions_dir, ""cpu"", ""*.cpp""))\n    source_cuda = glob.glob(os.path.join(extensions_dir, ""cuda"", ""*.cu""))\n\n    sources = main_file + source_cpu\n    extension = CppExtension\n\n    extra_compile_args = {""cxx"": []}\n    define_macros = []\n\n    if torch.cuda.is_available() and CUDA_HOME is not None:\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(""WITH_CUDA"", None)]\n        extra_compile_args[""nvcc""] = [\n            ""-DCUDA_HAS_FP16=1"",\n            ""-D__CUDA_NO_HALF_OPERATORS__"",\n            ""-D__CUDA_NO_HALF_CONVERSIONS__"",\n            ""-D__CUDA_NO_HALF2_OPERATORS__"",\n        ]\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            ""C_ROIPooling"",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n\n    return ext_modules\n\n\nsetup(\n    name=""C_ROIPooling"",\n    version=""0.1"",\n    description=""ROIPooling in C++ or CUDA"",\n    ext_modules=get_extensions(),\n    cmdclass={""build_ext"": torch.utils.cpp_extension.BuildExtension},\n)\n'"
pretrain/data/datasets/coco_captions.py,14,"b'import random\nimport os\nimport time\nimport json\nfrom PIL import Image\nimport base64\nimport numpy as np\nimport logging\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom external.pytorch_pretrained_bert import BertTokenizer\n\nfrom common.utils.zipreader import ZipReader\nfrom common.utils.create_logger import makedirsExist\nfrom pycocotools.coco import COCO\n\n\nclass COCOCaptionsDataset(Dataset):\n    def __init__(self, ann_file, image_set, root_path, data_path, seq_len=64,\n                 with_precomputed_visual_feat=False, mask_raw_pixels=True,\n                 with_rel_task=True, with_mlm_task=True, with_mvrc_task=True,\n                 transform=None, test_mode=False,\n                 zip_mode=False, cache_mode=False, cache_db=False, ignore_db_cache=True,\n                 tokenizer=None, pretrained_model_name=None,\n                 add_image_as_a_box=False,\n                 aspect_grouping=False, **kwargs):\n        """"""\n        Conceptual Captions Dataset\n\n        :param ann_file: annotation jsonl file\n        :param image_set: image folder name, e.g., \'vcr1images\'\n        :param root_path: root path to cache database loaded from annotation file\n        :param data_path: path to vcr dataset\n        :param transform: transform\n        :param test_mode: test mode means no labels available\n        :param zip_mode: reading images and metadata in zip archive\n        :param cache_mode: cache whole dataset to RAM first, then __getitem__ read them from RAM\n        :param ignore_db_cache: ignore previous cached database, reload it from annotation file\n        :param tokenizer: default is BertTokenizer from pytorch_pretrained_bert\n        :param add_image_as_a_box: add whole image as a box\n        :param aspect_grouping: whether to group images via their aspect\n        :param kwargs:\n        """"""\n        super(COCOCaptionsDataset, self).__init__()\n\n        assert not cache_mode, \'currently not support cache mode!\'\n        assert not test_mode\n\n        annot = {\'train\': \'annotations/captions_train2017.json\',\n                 \'val\': \'annotations/captions_val2017.json\'}\n        annot_inst = {\'train\': \'annotations/instances_train2017.json\',\n                      \'val\': \'annotations/instances_val2017.json\'}\n        if zip_mode:\n            self.root = os.path.join(data_path, \'{0}2017.zip@/{0}2017\'.format(image_set))\n        else:\n            self.root = os.path.join(data_path, \'{}2017\'.format(image_set))\n\n        self.seq_len = seq_len\n        self.with_rel_task = with_rel_task\n        self.with_mlm_task = with_mlm_task\n        self.with_mvrc_task = with_mvrc_task\n        self.data_path = data_path\n        self.root_path = root_path\n        self.ann_file = os.path.join(data_path, annot[image_set])\n        self.ann_file_inst = os.path.join(data_path, annot_inst[image_set])\n        self.with_precomputed_visual_feat = with_precomputed_visual_feat\n        self.mask_raw_pixels = mask_raw_pixels\n        self.image_set = image_set\n        self.transform = transform\n        self.test_mode = test_mode\n        self.zip_mode = zip_mode\n        self.cache_mode = cache_mode\n        self.cache_db = cache_db\n        self.ignore_db_cache = ignore_db_cache\n        self.aspect_grouping = aspect_grouping\n        self.cache_dir = os.path.join(root_path, \'cache\')\n        self.add_image_as_a_box = add_image_as_a_box\n        if not os.path.exists(self.cache_dir):\n            makedirsExist(self.cache_dir)\n        self.tokenizer = tokenizer if tokenizer is not None \\\n            else BertTokenizer.from_pretrained(\n            \'bert-base-uncased\' if pretrained_model_name is None else pretrained_model_name,\n            cache_dir=self.cache_dir)\n\n        if self.zip_mode:\n            self.zipreader = ZipReader()\n\n        self.coco = COCO(self.ann_file)\n        self.coco_inst = COCO(self.ann_file_inst)\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        # filter images without detection annotations\n        self.ids = [\n            img_id\n            for img_id in self.ids\n            if len(self.coco_inst.getAnnIds(imgIds=img_id, iscrowd=None)) > 0\n        ]\n\n        self.json_category_id_to_contiguous_id = {\n            v: i + 1 for i, v in enumerate(self.coco_inst.getCatIds())\n        }\n        self.contiguous_category_id_to_json_id = {\n            v: k for k, v in self.json_category_id_to_contiguous_id.items()\n        }\n        self.id_to_img_map = {k: v for k, v in enumerate(self.ids)}\n\n        if self.aspect_grouping:\n            assert False, ""not support aspect grouping currently!""\n            # self.group_ids = self.group_aspect(self.database)\n\n        print(\'mask_raw_pixels: \', self.mask_raw_pixels)\n\n    @property\n    def data_names(self):\n        return [\'image\', \'boxes\', \'im_info\', \'text\',\n                \'relationship_label\', \'mlm_labels\', \'mvrc_ops\', \'mvrc_labels\']\n\n    def __getitem__(self, index):\n        img_id = self.ids[index]\n\n        # image data\n        # frcnn_data = self._load_json(os.path.join(self.data_path, idb[\'frcnn\']))\n        # boxes = np.frombuffer(self.b64_decode(frcnn_data[\'boxes\']),\n        #                       dtype=np.float32).reshape((frcnn_data[\'num_boxes\'], -1))\n        # boxes_cls_scores = np.frombuffer(self.b64_decode(frcnn_data[\'classes\']),\n        #                                  dtype=np.float32).reshape((frcnn_data[\'num_boxes\'], -1))\n        # boxes_max_conf = boxes_cls_scores.max(axis=1)\n        # inds = np.argsort(boxes_max_conf)[::-1]\n        # boxes = boxes[inds]\n        # boxes_cls_scores = boxes_cls_scores[inds]\n        # boxes = torch.as_tensor(boxes)\n        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n        anns = self.coco.loadAnns(ann_ids)\n        ann_ids_inst = self.coco_inst.getAnnIds(imgIds=img_id)\n        anns_inst = self.coco_inst.loadAnns(ann_ids_inst)\n        idb = anns[0]\n        boxes = [ann_[\'bbox\'] for ann_ in anns_inst]\n        boxes = torch.as_tensor(boxes).reshape(-1, 4)\n        TO_REMOVE = 1\n        xmin, ymin, w, h = boxes.split(1, dim=-1)\n        xmax = xmin + (w - TO_REMOVE).clamp(min=0)\n        ymax = ymin + (h - TO_REMOVE).clamp(min=0)\n        boxes = torch.cat((xmin, ymin, xmax, ymax), dim=-1)\n        boxes_cls_scores = boxes.new_zeros((boxes.shape[0], 81))\n        classes = [ann[""category_id""] for ann in anns_inst]\n        classes = [self.json_category_id_to_contiguous_id[c] for c in classes]\n        for i, class_ in enumerate(classes):\n            boxes_cls_scores[i, class_] = 1.0\n\n        if self.with_precomputed_visual_feat:\n            assert False\n            # image = None\n            # w0, h0 = frcnn_data[\'image_w\'], frcnn_data[\'image_h\']\n            # boxes_features = np.frombuffer(self.b64_decode(frcnn_data[\'features\']),\n            #                                dtype=np.float32).reshape((frcnn_data[\'num_boxes\'], -1))\n            # boxes_features = boxes_features[inds]\n            # boxes_features = torch.as_tensor(boxes_features)\n        else:\n            path = self.coco_inst.loadImgs(img_id)[0][\'file_name\']\n            image = self._load_image(os.path.join(self.root, path))\n            w0, h0 = image.size\n\n        if self.add_image_as_a_box:\n            image_box = torch.as_tensor([[0.0, 0.0, w0 - 1.0, h0 - 1.0]])\n            boxes = torch.cat((image_box, boxes), dim=0)\n            if self.with_precomputed_visual_feat:\n                assert False\n                # image_box_feat = boxes_features.mean(dim=0, keepdim=True)\n                # boxes_features = torch.cat((image_box_feat, boxes_features), dim=0)\n\n        # transform\n        im_info = torch.tensor([w0, h0, 1.0, 1.0, index])\n        if self.transform is not None:\n            image, boxes, _, im_info = self.transform(image, boxes, None, im_info)\n\n        if image is None and (not self.with_precomputed_visual_feat):\n            assert False\n            # w = int(im_info[0].item())\n            # h = int(im_info[1].item())\n            # image = im_info.new_zeros((3, h, w), dtype=torch.float)\n\n        # clamp boxes\n        w = im_info[0].item()\n        h = im_info[1].item()\n        boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(min=0, max=w-1)\n        boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(min=0, max=h-1)\n\n        # Task #1: Caption-Image Relationship Prediction\n        _p = random.random()\n        if _p < 0.5 or (not self.with_rel_task):\n            relationship_label = 1\n            caption = idb[\'caption\']\n        else:\n            assert False\n            relationship_label = 0\n            rand_index = random.randrange(0, len(self.database))\n            while rand_index == index:\n                rand_index = random.randrange(0, len(self.database))\n            caption =self.database[rand_index][\'caption\']\n\n        assert isinstance(caption, str)\n\n        # Task #2: Masked Language Modeling\n        if self.with_mlm_task:\n            caption_tokens = self.tokenizer.basic_tokenizer.tokenize(caption)\n            caption_tokens, mlm_labels = self.random_word_wwm(caption_tokens)\n        else:\n            caption_tokens = self.tokenizer.tokenize(caption)\n            mlm_labels = [-1] * len(caption_tokens)\n        text_tokens = [\'[CLS]\'] + caption_tokens + [\'[SEP]\']\n        mlm_labels = [-1] + mlm_labels + [-1]\n\n        # Task #3: Masked Visual Region Classification\n        if self.with_mvrc_task:\n            if self.add_image_as_a_box:\n                mvrc_ops, mvrc_labels = self.random_mask_region(boxes_cls_scores)\n                mvrc_ops = [0] + mvrc_ops\n                mvrc_labels = [np.zeros_like(boxes_cls_scores[0])] + mvrc_labels\n                num_real_boxes = boxes.shape[0] - 1\n                num_masked_boxes = 0\n                if self.with_precomputed_visual_feat:\n                    assert False\n                    # boxes_features[0] *= num_real_boxes\n                    # for mvrc_op, box_feat in zip(mvrc_ops, boxes_features):\n                    #     if mvrc_op == 1:\n                    #         num_masked_boxes += 1\n                    #         boxes_features[0] -= box_feat\n                    # boxes_features[0] /= (num_real_boxes - num_masked_boxes + 1e-5)\n            else:\n                mvrc_ops, mvrc_labels = self.random_mask_region(boxes_cls_scores)\n            assert len(mvrc_ops) == boxes.shape[0], \\\n                ""Error: mvrc_ops have length {}, expected {}!"".format(len(mvrc_ops), boxes.shape[0])\n            assert len(mvrc_labels) == boxes.shape[0], \\\n                ""Error: mvrc_labels have length {}, expected {}!"".format(len(mvrc_labels), boxes.shape[0])\n        else:\n            mvrc_ops = [0] * boxes.shape[0]\n            mvrc_labels = [np.zeros_like(boxes_cls_scores[0])] * boxes.shape[0]\n\n        # zero out pixels of masked RoI\n        if (not self.with_precomputed_visual_feat) and self.mask_raw_pixels:\n            for mvrc_op, box in zip(mvrc_ops, boxes):\n                if mvrc_op == 1:\n                    x1, y1, x2, y2 = box\n                    image[:, int(y1):(int(y2)+1), int(x1):(int(x2)+1)] = 0\n\n        mvrc_labels = np.stack(mvrc_labels, axis=0)\n\n        text = self.tokenizer.convert_tokens_to_ids(text_tokens)\n\n        if self.with_precomputed_visual_feat:\n            assert False\n            # boxes = torch.cat((boxes, boxes_features), dim=1)\n\n        # truncate seq to max len\n        if len(text) + len(boxes) > self.seq_len:\n            text_len_keep = len(text)\n            box_len_keep = len(boxes)\n            while (text_len_keep + box_len_keep) > self.seq_len:\n                if box_len_keep > text_len_keep:\n                    box_len_keep -= 1\n                else:\n                    text_len_keep -= 1\n            boxes = boxes[:box_len_keep]\n            text = text[:text_len_keep]\n            mlm_labels = mlm_labels[:text_len_keep]\n            mvrc_ops = mvrc_ops[:box_len_keep]\n            mvrc_labels = mvrc_labels[:box_len_keep]\n\n        return image, boxes, im_info, text, relationship_label, mlm_labels, mvrc_ops, mvrc_labels\n\n    # def random_word(self, tokens):\n    #     output_label = []\n    #\n    #     for i, token in enumerate(tokens):\n    #         prob = random.random()\n    #         # mask token with 15% probability\n    #         if prob < 0.15:\n    #             prob /= 0.15\n    #\n    #             # 80% randomly change token to mask token\n    #             if prob < 0.8:\n    #                 tokens[i] = ""[MASK]""\n    #\n    #             # 10% randomly change token to random token\n    #             elif prob < 0.9:\n    #                 tokens[i] = random.choice(list(self.tokenizer.vocab.items()))[0]\n    #\n    #             # -> rest 10% randomly keep current token\n    #\n    #             # append current token to output (we will predict these later)\n    #             try:\n    #                 output_label.append(self.tokenizer.vocab[token])\n    #             except KeyError:\n    #                 # For unknown words (should not occur with BPE vocab)\n    #                 output_label.append(self.tokenizer.vocab[""[UNK]""])\n    #                 logging.warning(""Cannot find token \'{}\' in vocab. Using [UNK] insetad"".format(token))\n    #         else:\n    #             # no masking token (will be ignored by loss function later)\n    #             output_label.append(-1)\n    #\n    #     # if no word masked, random choose a word to mask\n    #     if self.force_mask:\n    #         if all([l_ == -1 for l_ in output_label]):\n    #             choosed = random.randrange(0, len(output_label))\n    #             output_label[choosed] = self.tokenizer.vocab[tokens[choosed]]\n    #\n    #     return tokens, output_label\n\n    def random_word_wwm(self, tokens):\n        output_tokens = []\n        output_label = []\n\n        for i, token in enumerate(tokens):\n            sub_tokens = self.tokenizer.wordpiece_tokenizer.tokenize(token)\n            prob = random.random()\n            # mask token with 15% probability\n            if prob < 0.15:\n                prob /= 0.15\n\n                # 80% randomly change token to mask token\n                if prob < 0.8:\n                    for sub_token in sub_tokens:\n                        output_tokens.append(""[MASK]"")\n                # 10% randomly change token to random token\n                elif prob < 0.9:\n                    for sub_token in sub_tokens:\n                        output_tokens.append(random.choice(list(self.tokenizer.vocab.keys())))\n                        # -> rest 10% randomly keep current token\n                else:\n                    for sub_token in sub_tokens:\n                        output_tokens.append(sub_token)\n\n                        # append current token to output (we will predict these later)\n                for sub_token in sub_tokens:\n                    try:\n                        output_label.append(self.tokenizer.vocab[sub_token])\n                    except KeyError:\n                        # For unknown words (should not occur with BPE vocab)\n                        output_label.append(self.tokenizer.vocab[""[UNK]""])\n                        logging.warning(""Cannot find sub_token \'{}\' in vocab. Using [UNK] insetad"".format(sub_token))\n            else:\n                for sub_token in sub_tokens:\n                    # no masking token (will be ignored by loss function later)\n                    output_tokens.append(sub_token)\n                    output_label.append(-1)\n\n        ## if no word masked, random choose a word to mask\n        # if all([l_ == -1 for l_ in output_label]):\n        #    choosed = random.randrange(0, len(output_label))\n        #    output_label[choosed] = self.tokenizer.vocab[tokens[choosed]]\n\n        return output_tokens, output_label\n\n    def random_mask_region(self, regions_cls_scores):\n        num_regions, num_classes = regions_cls_scores.shape\n        output_op = []\n        output_label = []\n        for k, cls_scores in enumerate(regions_cls_scores):\n            prob = random.random()\n            # mask region with 15% probability\n            if prob < 0.15:\n                prob /= 0.15\n\n                if prob < 0.9:\n                    # 90% randomly replace appearance feature by ""MASK""\n                    output_op.append(1)\n                else:\n                    # -> rest 10% randomly keep current appearance feature\n                    output_op.append(0)\n\n                # append class of region to output (we will predict these later)\n                output_label.append(cls_scores)\n            else:\n                # no masking region (will be ignored by loss function later)\n                output_op.append(0)\n                output_label.append(np.zeros_like(cls_scores))\n\n        # # if no region masked, random choose a region to mask\n        # if all([op == 0 for op in output_op]):\n        #     choosed = random.randrange(0, len(output_op))\n        #     output_op[choosed] = 1\n        #     output_label[choosed] = regions_cls_scores[choosed]\n\n        return output_op, output_label\n\n    @staticmethod\n    def b64_decode(string):\n        return base64.decodebytes(string.encode())\n\n    @staticmethod\n    def group_aspect(database):\n        print(\'grouping aspect...\')\n        t = time.time()\n\n        # get shape of all images\n        widths = torch.as_tensor([idb[\'width\'] for idb in database])\n        heights = torch.as_tensor([idb[\'height\'] for idb in database])\n\n        # group\n        group_ids = torch.zeros(len(database))\n        horz = widths >= heights\n        vert = 1 - horz\n        group_ids[horz] = 0\n        group_ids[vert] = 1\n\n        print(\'Done (t={:.2f}s)\'.format(time.time() - t))\n\n        return group_ids\n\n    def __len__(self):\n        return len(self.ids)\n\n    def _load_image(self, path):\n        if \'.zip@\' in path:\n            return self.zipreader.imread(path).convert(\'RGB\')\n        else:\n            return Image.open(path).convert(\'RGB\')\n\n    def _load_json(self, path):\n        if \'.zip@\' in path:\n            f = self.zipreader.read(path)\n            return json.loads(f.decode())\n        else:\n            with open(path, \'r\') as f:\n                return json.load(f)\n\n'"
pretrain/data/datasets/conceptual_captions.py,12,"b'import random\nimport os\nimport time\nimport json\nimport jsonlines\nfrom PIL import Image\nimport base64\nimport numpy as np\nimport logging\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom external.pytorch_pretrained_bert import BertTokenizer\n\nfrom common.utils.zipreader import ZipReader\nfrom common.utils.create_logger import makedirsExist\n\n\nclass ConceptualCaptionsDataset(Dataset):\n    def __init__(self, ann_file, image_set, root_path, data_path, seq_len=64,\n                 with_precomputed_visual_feat=False, mask_raw_pixels=True,\n                 with_rel_task=True, with_mlm_task=True, with_mvrc_task=True,\n                 transform=None, test_mode=False,\n                 zip_mode=False, cache_mode=False, cache_db=False, ignore_db_cache=True,\n                 tokenizer=None, pretrained_model_name=None,\n                 add_image_as_a_box=False,\n                 aspect_grouping=False, **kwargs):\n        """"""\n        Conceptual Captions Dataset\n\n        :param ann_file: annotation jsonl file\n        :param image_set: image folder name, e.g., \'vcr1images\'\n        :param root_path: root path to cache database loaded from annotation file\n        :param data_path: path to vcr dataset\n        :param transform: transform\n        :param test_mode: test mode means no labels available\n        :param zip_mode: reading images and metadata in zip archive\n        :param cache_mode: cache whole dataset to RAM first, then __getitem__ read them from RAM\n        :param ignore_db_cache: ignore previous cached database, reload it from annotation file\n        :param tokenizer: default is BertTokenizer from pytorch_pretrained_bert\n        :param add_image_as_a_box: add whole image as a box\n        :param aspect_grouping: whether to group images via their aspect\n        :param kwargs:\n        """"""\n        super(ConceptualCaptionsDataset, self).__init__()\n\n        assert not cache_mode, \'currently not support cache mode!\'\n        assert not test_mode\n\n        annot = {\'train\': \'train_frcnn.json\',\n                 \'val\': \'val_frcnn.json\'}\n\n        self.seq_len = seq_len\n        self.with_rel_task = with_rel_task\n        self.with_mlm_task = with_mlm_task\n        self.with_mvrc_task = with_mvrc_task\n        self.data_path = data_path\n        self.root_path = root_path\n        self.ann_file = os.path.join(data_path, annot[image_set])\n        self.with_precomputed_visual_feat = with_precomputed_visual_feat\n        self.mask_raw_pixels = mask_raw_pixels\n        self.image_set = image_set\n        self.transform = transform\n        self.test_mode = test_mode\n        self.zip_mode = zip_mode\n        self.cache_mode = cache_mode\n        self.cache_db = cache_db\n        self.ignore_db_cache = ignore_db_cache\n        self.aspect_grouping = aspect_grouping\n        self.cache_dir = os.path.join(root_path, \'cache\')\n        self.add_image_as_a_box = add_image_as_a_box\n        if not os.path.exists(self.cache_dir):\n            makedirsExist(self.cache_dir)\n        self.tokenizer = tokenizer if tokenizer is not None \\\n            else BertTokenizer.from_pretrained(\n            \'bert-base-uncased\' if pretrained_model_name is None else pretrained_model_name,\n            cache_dir=self.cache_dir)\n\n        self.zipreader = ZipReader()\n\n        self.database = list(jsonlines.open(self.ann_file))\n        if not self.zip_mode:\n            for i, idb in enumerate(self.database):\n                self.database[i][\'frcnn\'] = idb[\'frcnn\'].replace(\'.zip@\', \'\')\\\n                    .replace(\'.0\', \'\').replace(\'.1\', \'\').replace(\'.2\', \'\').replace(\'.3\', \'\')\n                self.database[i][\'image\'] = idb[\'image\'].replace(\'.zip@\', \'\')\n\n        if self.aspect_grouping:\n            assert False, ""not support aspect grouping currently!""\n            self.group_ids = self.group_aspect(self.database)\n\n        print(\'mask_raw_pixels: \', self.mask_raw_pixels)\n\n    @property\n    def data_names(self):\n        return [\'image\', \'boxes\', \'im_info\', \'text\',\n                \'relationship_label\', \'mlm_labels\', \'mvrc_ops\', \'mvrc_labels\']\n\n    def __getitem__(self, index):\n        idb = self.database[index]\n\n        # image data\n        frcnn_data = self._load_json(os.path.join(self.data_path, idb[\'frcnn\']))\n        boxes = np.frombuffer(self.b64_decode(frcnn_data[\'boxes\']),\n                              dtype=np.float32).reshape((frcnn_data[\'num_boxes\'], -1))\n        boxes_cls_scores = np.frombuffer(self.b64_decode(frcnn_data[\'classes\']),\n                                         dtype=np.float32).reshape((frcnn_data[\'num_boxes\'], -1))\n        boxes_max_conf = boxes_cls_scores.max(axis=1)\n        inds = np.argsort(boxes_max_conf)[::-1]\n        boxes = boxes[inds]\n        boxes_cls_scores = boxes_cls_scores[inds]\n        boxes = torch.as_tensor(boxes)\n\n        if self.with_precomputed_visual_feat:\n            image = None\n            w0, h0 = frcnn_data[\'image_w\'], frcnn_data[\'image_h\']\n            boxes_features = np.frombuffer(self.b64_decode(frcnn_data[\'features\']),\n                                           dtype=np.float32).reshape((frcnn_data[\'num_boxes\'], -1))\n            boxes_features = boxes_features[inds]\n            boxes_features = torch.as_tensor(boxes_features)\n        else:\n            try:\n                image = self._load_image(os.path.join(self.data_path, idb[\'image\']))\n                w0, h0 = image.size\n            except:\n                print(""Failed to load image {}, use zero image!"".format(idb[\'image\']))\n                image = None\n                w0, h0 = frcnn_data[\'image_w\'], frcnn_data[\'image_h\']\n\n        if self.add_image_as_a_box:\n            image_box = torch.as_tensor([[0.0, 0.0, w0 - 1.0, h0 - 1.0]])\n            boxes = torch.cat((image_box, boxes), dim=0)\n            if self.with_precomputed_visual_feat:\n                image_box_feat = boxes_features.mean(dim=0, keepdim=True)\n                boxes_features = torch.cat((image_box_feat, boxes_features), dim=0)\n\n        # transform\n        im_info = torch.tensor([w0, h0, 1.0, 1.0, index])\n        if self.transform is not None:\n            image, boxes, _, im_info = self.transform(image, boxes, None, im_info)\n\n        if image is None and (not self.with_precomputed_visual_feat):\n            w = int(im_info[0].item())\n            h = int(im_info[1].item())\n            image = im_info.new_zeros((3, h, w), dtype=torch.float)\n\n        # clamp boxes\n        w = im_info[0].item()\n        h = im_info[1].item()\n        boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(min=0, max=w-1)\n        boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(min=0, max=h-1)\n\n        # Task #1: Caption-Image Relationship Prediction\n        _p = random.random()\n        if _p < 0.5 or (not self.with_rel_task):\n            relationship_label = 1\n            caption = idb[\'caption\']\n        else:\n            relationship_label = 0\n            rand_index = random.randrange(0, len(self.database))\n            while rand_index == index:\n                rand_index = random.randrange(0, len(self.database))\n            caption =self.database[rand_index][\'caption\']\n\n        # Task #2: Masked Language Modeling\n\n        if self.with_mlm_task:\n            caption_tokens = self.tokenizer.basic_tokenizer.tokenize(\' \'.join(caption))\n            caption_tokens, mlm_labels = self.random_word_wwm(caption_tokens)\n        else:\n            caption_tokens = self.tokenizer.tokenize(\' \'.join(caption))\n            mlm_labels = [-1] * len(caption_tokens)\n        text_tokens = [\'[CLS]\'] + caption_tokens + [\'[SEP]\']\n        mlm_labels = [-1] + mlm_labels + [-1]\n\n        # Task #3: Masked Visual Region Classification\n        if self.with_mvrc_task:\n            if self.add_image_as_a_box:\n                mvrc_ops, mvrc_labels = self.random_mask_region(boxes_cls_scores)\n                mvrc_ops = [0] + mvrc_ops\n                mvrc_labels = [np.zeros_like(boxes_cls_scores[0])] + mvrc_labels\n                num_real_boxes = boxes.shape[0] - 1\n                num_masked_boxes = 0\n                if self.with_precomputed_visual_feat:\n                    boxes_features[0] *= num_real_boxes\n                    for mvrc_op, box_feat in zip(mvrc_ops, boxes_features):\n                        if mvrc_op == 1:\n                            num_masked_boxes += 1\n                            boxes_features[0] -= box_feat\n                    boxes_features[0] /= (num_real_boxes - num_masked_boxes + 1e-5)\n            else:\n                mvrc_ops, mvrc_labels = self.random_mask_region(boxes_cls_scores)\n            assert len(mvrc_ops) == boxes.shape[0], \\\n                ""Error: mvrc_ops have length {}, expected {}!"".format(len(mvrc_ops), boxes.shape[0])\n            assert len(mvrc_labels) == boxes.shape[0], \\\n                ""Error: mvrc_labels have length {}, expected {}!"".format(len(mvrc_labels), boxes.shape[0])\n        else:\n            mvrc_ops = [0] * boxes.shape[0]\n            mvrc_labels = [np.zeros_like(boxes_cls_scores[0])] * boxes.shape[0]\n\n        # zero out pixels of masked RoI\n        if (not self.with_precomputed_visual_feat) and self.mask_raw_pixels:\n            for mvrc_op, box in zip(mvrc_ops, boxes):\n                if mvrc_op == 1:\n                    x1, y1, x2, y2 = box\n                    image[:, int(y1):(int(y2)+1), int(x1):(int(x2)+1)] = 0\n\n        mvrc_labels = np.stack(mvrc_labels, axis=0)\n\n        text = self.tokenizer.convert_tokens_to_ids(text_tokens)\n\n        if self.with_precomputed_visual_feat:\n            boxes = torch.cat((boxes, boxes_features), dim=1)\n\n        # truncate seq to max len\n        if len(text) + len(boxes) > self.seq_len:\n            text_len_keep = len(text)\n            box_len_keep = len(boxes)\n            while (text_len_keep + box_len_keep) > self.seq_len and (text_len_keep > 0) and (box_len_keep > 0):\n                if box_len_keep > text_len_keep:\n                    box_len_keep -= 1\n                else:\n                    text_len_keep -= 1\n            if text_len_keep < 2:\n                text_len_keep = 2\n            if box_len_keep < 1:\n                box_len_keep = 1\n            boxes = boxes[:box_len_keep]\n            text = text[:(text_len_keep - 1)] + [text[-1]]\n            mlm_labels = mlm_labels[:(text_len_keep - 1)] + [mlm_labels[-1]]\n            mvrc_ops = mvrc_ops[:box_len_keep]\n            mvrc_labels = mvrc_labels[:box_len_keep]\n\n        return image, boxes, im_info, text, relationship_label, mlm_labels, mvrc_ops, mvrc_labels\n\n    # def random_word(self, tokens):\n    #     output_label = []\n    #\n    #     for i, token in enumerate(tokens):\n    #         prob = random.random()\n    #         # mask token with 15% probability\n    #         if prob < 0.15:\n    #             prob /= 0.15\n    #\n    #             # 80% randomly change token to mask token\n    #             if prob < 0.8:\n    #                 tokens[i] = ""[MASK]""\n    #\n    #             # 10% randomly change token to random token\n    #             elif prob < 0.9:\n    #                 tokens[i] = random.choice(list(self.tokenizer.vocab.items()))[0]\n    #\n    #             # -> rest 10% randomly keep current token\n    #\n    #             # append current token to output (we will predict these later)\n    #             try:\n    #                 output_label.append(self.tokenizer.vocab[token])\n    #             except KeyError:\n    #                 # For unknown words (should not occur with BPE vocab)\n    #                 output_label.append(self.tokenizer.vocab[""[UNK]""])\n    #                 logging.warning(""Cannot find token \'{}\' in vocab. Using [UNK] insetad"".format(token))\n    #         else:\n    #             # no masking token (will be ignored by loss function later)\n    #             output_label.append(-1)\n    #\n    #     # if no word masked, random choose a word to mask\n    #     if self.force_mask:\n    #         if all([l_ == -1 for l_ in output_label]):\n    #             choosed = random.randrange(0, len(output_label))\n    #             output_label[choosed] = self.tokenizer.vocab[tokens[choosed]]\n    #\n    #     return tokens, output_label\n\n    def random_word_wwm(self, tokens):\n        output_tokens = []\n        output_label = []\n\n        for i, token in enumerate(tokens):\n            sub_tokens = self.tokenizer.wordpiece_tokenizer.tokenize(token)\n            prob = random.random()\n            # mask token with 15% probability\n            if prob < 0.15:\n                prob /= 0.15\n\n                # 80% randomly change token to mask token\n                if prob < 0.8:\n                    for sub_token in sub_tokens:\n                        output_tokens.append(""[MASK]"")\n                # 10% randomly change token to random token\n                elif prob < 0.9:\n                    for sub_token in sub_tokens:\n                        output_tokens.append(random.choice(list(self.tokenizer.vocab.keys())))\n                        # -> rest 10% randomly keep current token\n                else:\n                    for sub_token in sub_tokens:\n                        output_tokens.append(sub_token)\n\n                        # append current token to output (we will predict these later)\n                for sub_token in sub_tokens:\n                    try:\n                        output_label.append(self.tokenizer.vocab[sub_token])\n                    except KeyError:\n                        # For unknown words (should not occur with BPE vocab)\n                        output_label.append(self.tokenizer.vocab[""[UNK]""])\n                        logging.warning(""Cannot find sub_token \'{}\' in vocab. Using [UNK] insetad"".format(sub_token))\n            else:\n                for sub_token in sub_tokens:\n                    # no masking token (will be ignored by loss function later)\n                    output_tokens.append(sub_token)\n                    output_label.append(-1)\n\n        ## if no word masked, random choose a word to mask\n        # if all([l_ == -1 for l_ in output_label]):\n        #    choosed = random.randrange(0, len(output_label))\n        #    output_label[choosed] = self.tokenizer.vocab[tokens[choosed]]\n\n        return output_tokens, output_label\n\n    def random_mask_region(self, regions_cls_scores):\n        num_regions, num_classes = regions_cls_scores.shape\n        output_op = []\n        output_label = []\n        for k, cls_scores in enumerate(regions_cls_scores):\n            prob = random.random()\n            # mask region with 15% probability\n            if prob < 0.15:\n                prob /= 0.15\n\n                if prob < 0.9:\n                    # 90% randomly replace appearance feature by ""MASK""\n                    output_op.append(1)\n                else:\n                    # -> rest 10% randomly keep current appearance feature\n                    output_op.append(0)\n\n                # append class of region to output (we will predict these later)\n                output_label.append(cls_scores)\n            else:\n                # no masking region (will be ignored by loss function later)\n                output_op.append(0)\n                output_label.append(np.zeros_like(cls_scores))\n\n        # # if no region masked, random choose a region to mask\n        # if all([op == 0 for op in output_op]):\n        #     choosed = random.randrange(0, len(output_op))\n        #     output_op[choosed] = 1\n        #     output_label[choosed] = regions_cls_scores[choosed]\n\n        return output_op, output_label\n\n    @staticmethod\n    def b64_decode(string):\n        return base64.decodebytes(string.encode())\n\n    @staticmethod\n    def group_aspect(database):\n        print(\'grouping aspect...\')\n        t = time.time()\n\n        # get shape of all images\n        widths = torch.as_tensor([idb[\'width\'] for idb in database])\n        heights = torch.as_tensor([idb[\'height\'] for idb in database])\n\n        # group\n        group_ids = torch.zeros(len(database))\n        horz = widths >= heights\n        vert = 1 - horz\n        group_ids[horz] = 0\n        group_ids[vert] = 1\n\n        print(\'Done (t={:.2f}s)\'.format(time.time() - t))\n\n        return group_ids\n\n    def __len__(self):\n        return len(self.database)\n\n    def _load_image(self, path):\n        if \'.zip@\' in path:\n            return self.zipreader.imread(path).convert(\'RGB\')\n        else:\n            return Image.open(path).convert(\'RGB\')\n\n    def _load_json(self, path):\n        if \'.zip@\' in path:\n            f = self.zipreader.read(path)\n            return json.loads(f.decode())\n        else:\n            with open(path, \'r\') as f:\n                return json.load(f)\n\n'"
pretrain/data/datasets/general_corpus.py,1,"b'import random\nfrom torch.utils.data import Dataset\nfrom external.pytorch_pretrained_bert import BertTokenizer\nimport logging\n\n\nclass GeneralCorpus(Dataset):\n    def __init__(self, ann_file, pretrained_model_name, tokenizer=None, seq_len=64, min_seq_len=64,\n                 encoding=""utf-8"", on_memory=True,\n                 **kwargs):\n        assert on_memory, ""only support on_memory mode!""\n\n        self.tokenizer = tokenizer if tokenizer is not None else BertTokenizer.from_pretrained(pretrained_model_name)\n        self.vocab = self.tokenizer.vocab\n        self.seq_len = seq_len\n        self.min_seq_len = min_seq_len\n        self.on_memory = on_memory\n        self.ann_file = ann_file\n        self.encoding = encoding\n        self.test_mode = False\n\n        # load samples into memory\n        if on_memory:\n            self.corpus = self.load_corpus()\n\n    def load_corpus(self):\n        corpus = []\n        for ann_file in self.ann_file.split(\'+\'):\n            with open(ann_file, \'r\', encoding=self.encoding) as f:\n                corpus.extend([l.strip(\'\\n\').strip(\'\\r\').strip(\'\\n\') for l in f.readlines()])\n\n        corpus = [l.strip() for l in corpus if l.strip() != \'\']\n\n        return corpus\n\n    @property\n    def data_names(self):\n        return [\'text\', \'mlm_labels\']\n\n    def __len__(self):\n        return len(self.corpus)\n\n    def __getitem__(self, item):\n        raw = self.corpus[item]\n\n        # tokenize\n        tokens = self.tokenizer.basic_tokenizer.tokenize(raw)\n\n        # add more tokens if len(tokens) < min_len\n        _cur = (item + 1) % len(self.corpus)\n        while len(tokens) < self.min_seq_len:\n            _cur_tokens = self.tokenizer.basic_tokenizer.tokenize(self.corpus[_cur])\n            tokens.extend(_cur_tokens)\n            _cur = (_cur + 1) % len(self.corpus)\n\n        # masked language modeling\n        tokens, mlm_labels = self.random_word_wwm(tokens)\n\n        # convert token to its vocab id\n        ids = self.tokenizer.convert_tokens_to_ids(tokens)\n\n        # truncate\n        if len(ids) > self.seq_len:\n            ids = ids[:self.seq_len]\n            mlm_labels = mlm_labels[:self.seq_len]\n\n        return ids, mlm_labels\n\n    # def random_word(self, tokens):\n    #     output_label = []\n    #\n    #     for i, token in enumerate(tokens):\n    #         prob = random.random()\n    #         # mask token with 15% probability\n    #         if prob < 0.15:\n    #             prob /= 0.15\n    #\n    #             # 80% randomly change token to mask token\n    #             if prob < 0.8:\n    #                 tokens[i] = ""[MASK]""\n    #\n    #             # 10% randomly change token to random token\n    #             elif prob < 0.9:\n    #                 tokens[i] = random.choice(list(self.tokenizer.vocab.items()))[0]\n    #\n    #             # -> rest 10% randomly keep current token\n    #\n    #             # append current token to output (we will predict these later)\n    #             try:\n    #                 output_label.append(self.tokenizer.vocab[token])\n    #             except KeyError:\n    #                 # For unknown words (should not occur with BPE vocab)\n    #                 output_label.append(self.tokenizer.vocab[""[UNK]""])\n    #                 logging.warning(""Cannot find token \'{}\' in vocab. Using [UNK] insetad"".format(token))\n    #         else:\n    #             # no masking token (will be ignored by loss function later)\n    #             output_label.append(-1)\n    #\n    #     # if no word masked, random choose a word to mask\n    #     if self.force_mask:\n    #         if all([l_ == -1 for l_ in output_label]):\n    #             choosed = random.randrange(0, len(output_label))\n    #             output_label[choosed] = self.tokenizer.vocab[tokens[choosed]]\n    #\n    #     return tokens, output_label\n\n    def random_word_wwm(self, tokens):\n        output_tokens = []\n        output_label = []\n\n        for i, token in enumerate(tokens):\n            sub_tokens = self.tokenizer.wordpiece_tokenizer.tokenize(token)\n            prob = random.random()\n            # mask token with 15% probability\n            if prob < 0.15:\n                prob /= 0.15\n\n                # 80% randomly change token to mask token\n                if prob < 0.8:\n                    for sub_token in sub_tokens:\n                        output_tokens.append(""[MASK]"")\n                # 10% randomly change token to random token\n                elif prob < 0.9:\n                    for sub_token in sub_tokens:\n                        output_tokens.append(random.choice(list(self.tokenizer.vocab.keys())))\n                        # -> rest 10% randomly keep current token\n                else:\n                    for sub_token in sub_tokens:\n                        output_tokens.append(sub_token)\n\n                        # append current token to output (we will predict these later)\n                for sub_token in sub_tokens:\n                    try:\n                        output_label.append(self.tokenizer.vocab[sub_token])\n                    except KeyError:\n                        # For unknown words (should not occur with BPE vocab)\n                        output_label.append(self.tokenizer.vocab[""[UNK]""])\n                        logging.warning(""Cannot find sub_token \'{}\' in vocab. Using [UNK] insetad"".format(sub_token))\n            else:\n                for sub_token in sub_tokens:\n                    # no masking token (will be ignored by loss function later)\n                    output_tokens.append(sub_token)\n                    output_label.append(-1)\n\n        ## if no word masked, random choose a word to mask\n        # if all([l_ == -1 for l_ in output_label]):\n        #    choosed = random.randrange(0, len(output_label))\n        #    output_label[choosed] = self.tokenizer.vocab[tokens[choosed]]\n\n        return output_tokens, output_label'"
pretrain/data/samplers/distributed.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# Code is copy-pasted exactly as in torch.utils.data.distributed.\n# FIXME remove this once c10d fixes the bug it has\nimport math\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data.sampler import Sampler\n\n\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset : offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch'"
pretrain/data/samplers/grouped_batch_sampler.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport itertools\n\nimport torch\nfrom torch.utils.data.sampler import BatchSampler\nfrom torch.utils.data.sampler import Sampler\n\n\nclass GroupedBatchSampler(BatchSampler):\n    """"""\n    Wraps another sampler to yield a mini-batch of indices.\n    It enforces that elements from the same group should appear in groups of batch_size.\n    It also tries to provide mini-batches which follows an ordering which is\n    as close as possible to the ordering from the original sampler.\n    Arguments:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_uneven (bool): If ``True``, the sampler will drop the batches whose\n            size is less than ``batch_size``\n    """"""\n\n    def __init__(self, sampler, group_ids, batch_size, drop_uneven=False):\n        if not isinstance(sampler, Sampler):\n            raise ValueError(\n                ""sampler should be an instance of ""\n                ""torch.utils.data.Sampler, but got sampler={}"".format(sampler)\n            )\n        self.sampler = sampler\n        self.group_ids = torch.as_tensor(group_ids)\n        assert self.group_ids.dim() == 1\n        self.batch_size = batch_size\n        self.drop_uneven = drop_uneven\n\n        self.groups = torch.unique(self.group_ids).sort(0)[0]\n\n        self._can_reuse_batches = False\n\n    def _prepare_batches(self):\n        dataset_size = len(self.group_ids)\n        # get the sampled indices from the sampler\n        sampled_ids = torch.as_tensor(list(self.sampler))\n        # potentially not all elements of the dataset were sampled\n        # by the sampler (e.g., DistributedSampler).\n        # construct a tensor which contains -1 if the element was\n        # not sampled, and a non-negative number indicating the\n        # order where the element was sampled.\n        # for example. if sampled_ids = [3, 1] and dataset_size = 5,\n        # the order is [-1, 1, -1, 0, -1]\n        order = torch.full((dataset_size,), -1, dtype=torch.int64)\n        order[sampled_ids] = torch.arange(len(sampled_ids))\n\n        # get a mask with the elements that were sampled\n        mask = order >= 0\n\n        # find the elements that belong to each individual cluster\n        clusters = [(self.group_ids == i) & mask for i in self.groups]\n        # get relative order of the elements inside each cluster\n        # that follows the order from the sampler\n        relative_order = [order[cluster] for cluster in clusters]\n        # with the relative order, find the absolute order in the\n        # sampled space\n        permutation_ids = [s[s.sort()[1]] for s in relative_order]\n        # permute each cluster so that they follow the order from\n        # the sampler\n        permuted_clusters = [sampled_ids[idx] for idx in permutation_ids]\n\n        # splits each cluster in batch_size, and merge as a list of tensors\n        splits = [c.split(self.batch_size) for c in permuted_clusters]\n        merged = tuple(itertools.chain.from_iterable(splits))\n\n        # now each batch internally has the right order, but\n        # they are grouped by clusters. Find the permutation between\n        # different batches that brings them as close as possible to\n        # the order that we have in the sampler. For that, we will consider the\n        # ordering as coming from the first element of each batch, and sort\n        # correspondingly\n        first_element_of_batch = [t[0].item() for t in merged]\n        # get and inverse mapping from sampled indices and the position where\n        # they occur (as returned by the sampler)\n        inv_sampled_ids_map = {v: k for k, v in enumerate(sampled_ids.tolist())}\n        # from the first element in each batch, get a relative ordering\n        first_index_of_batch = torch.as_tensor(\n            [inv_sampled_ids_map[s] for s in first_element_of_batch]\n        )\n\n        # permute the batches so that they approximately follow the order\n        # from the sampler\n        permutation_order = first_index_of_batch.sort(0)[1].tolist()\n        # finally, permute the batches\n        batches = [merged[i].tolist() for i in permutation_order]\n\n        if self.drop_uneven:\n            kept = []\n            for batch in batches:\n                if len(batch) == self.batch_size:\n                    kept.append(batch)\n            batches = kept\n        return batches\n\n    def __iter__(self):\n        if self._can_reuse_batches:\n            batches = self._batches\n            self._can_reuse_batches = False\n        else:\n            batches = self._prepare_batches()\n        self._batches = batches\n        return iter(batches)\n\n    def __len__(self):\n        if not hasattr(self, ""_batches""):\n            self._batches = self._prepare_batches()\n            self._can_reuse_batches = True\n        return len(self._batches)\n\n\n\n'"
pretrain/data/transforms/transforms.py,1,"b'import random\n\nimport numpy as np\n\nimport torch\nimport torchvision\nfrom torchvision.transforms import functional as F\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, boxes, masks, im_info):\n        for t in self.transforms:\n            image, boxes, masks, im_info = t(image, boxes, masks, im_info)\n        return image, boxes, masks, im_info\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + ""(""\n        for t in self.transforms:\n            format_string += ""\\n""\n            format_string += ""    {0}"".format(t)\n        format_string += ""\\n)""\n        return format_string\n\n\nclass Resize(object):\n    def __init__(self, min_size, max_size):\n        self.min_size = min_size\n        self.max_size = max_size\n\n    # modified from torchvision to add support for max size\n    def get_size(self, image_size):\n        w, h = image_size\n        size = self.min_size\n        max_size = self.max_size\n        if max_size is not None:\n            min_original_size = float(min((w, h)))\n            max_original_size = float(max((w, h)))\n            if max_original_size / min_original_size * size > max_size:\n                size = int(max_size * min_original_size / max_original_size)\n\n        if (w <= h and w == size) or (h <= w and h == size):\n            return (w, h)\n\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n        else:\n            oh = size\n            ow = int(size * w / h)\n\n        return (ow, oh)\n\n    def __call__(self, image, boxes, masks, im_info):\n        origin_size = im_info[:2]\n        size = self.get_size(origin_size)\n        if image is not None:\n            image = F.resize(image, (size[1], size[0]))\n\n        ratios = [size[0] * 1.0 / origin_size[0], size[1] * 1.0 / origin_size[1]]\n        if boxes is not None:\n            boxes[:, [0, 2]] *= ratios[0]\n            boxes[:, [1, 3]] *= ratios[1]\n        im_info[0], im_info[1] = size\n        im_info[2], im_info[3] = ratios\n        return image, boxes, masks, im_info\n\n\nclass RandomHorizontalFlip(object):\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, image, boxes, masks, im_info):\n        if random.random() < self.prob:\n            w, h = im_info[:2]\n            if image is not None:\n                image = F.hflip(image)\n            if boxes is not None:\n                boxes[:, [0, 2]] = w - 1 - boxes[:, [2, 0]]\n            if masks is not None:\n                masks = torch.as_tensor(masks.numpy()[:, :, ::-1].tolist())\n        return image, boxes, masks, im_info\n\n\nclass ToTensor(object):\n    def __call__(self, image, boxes, masks, im_info):\n        return F.to_tensor(image) if image is not None else image, boxes, masks, im_info\n\n\nclass Normalize(object):\n    def __init__(self, mean, std, to_bgr255=True):\n        self.mean = mean\n        self.std = std\n        self.to_bgr255 = to_bgr255\n\n    def __call__(self, image, boxes, masks, im_info):\n        if image is not None:\n            if self.to_bgr255:\n                image = image[[2, 1, 0]] * 255\n            image = F.normalize(image, mean=self.mean, std=self.std)\n        return image, boxes, masks, im_info\n\n\nclass FixPadding(object):\n    def __init__(self, min_size, max_size, pad=0):\n        self.min_size = min_size\n        self.max_size = max_size\n        self.pad = pad\n\n    def __call__(self, image, boxes, masks, im_info):\n\n        if image is not None:\n            # padding to fixed size for determinacy\n            c, h, w = image.shape\n            if h <= w:\n                h1 = self.min_size\n                w1 = self.max_size\n            else:\n                h1 = self.max_size\n                w1 = self.min_size\n            padded_image = image.new_zeros((c, h1, w1)).fill_(self.pad)\n            padded_image[:, :h, :w] = image\n            image = padded_image\n\n        return image, boxes, masks, im_info\n'"
refcoco/data/datasets/refcoco.py,16,"b'import os\nimport json\nimport _pickle as cPickle\nfrom PIL import Image\nimport base64\nimport numpy as np\nimport time\nimport logging\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom external.pytorch_pretrained_bert import BertTokenizer\n\nfrom common.utils.zipreader import ZipReader\nfrom common.utils.create_logger import makedirsExist\nfrom common.utils.bbox import bbox_iou_py_vectorized\n\nfrom pycocotools.coco import COCO\nfrom .refer.refer import REFER\n\n\nclass RefCOCO(Dataset):\n    def __init__(self, image_set, root_path, data_path, boxes=\'gt\', proposal_source=\'official\',\n                 transform=None, test_mode=False,\n                 zip_mode=False, cache_mode=False, cache_db=False, ignore_db_cache=True,\n                 tokenizer=None, pretrained_model_name=None,\n                 add_image_as_a_box=False, mask_size=(14, 14),\n                 aspect_grouping=False, **kwargs):\n        """"""\n        RefCOCO+ Dataset\n\n        :param image_set: image folder name\n        :param root_path: root path to cache database loaded from annotation file\n        :param data_path: path to dataset\n        :param boxes: boxes to use, \'gt\' or \'proposal\'\n        :param transform: transform\n        :param test_mode: test mode means no labels available\n        :param zip_mode: reading images and metadata in zip archive\n        :param cache_mode: cache whole dataset to RAM first, then __getitem__ read them from RAM\n        :param ignore_db_cache: ignore previous cached database, reload it from annotation file\n        :param tokenizer: default is BertTokenizer from pytorch_pretrained_bert\n        :param add_image_as_a_box: add whole image as a box\n        :param mask_size: size of instance mask of each object\n        :param aspect_grouping: whether to group images via their aspect\n        :param kwargs:\n        """"""\n        super(RefCOCO, self).__init__()\n\n        assert not cache_mode, \'currently not support cache mode!\'\n\n        categories = [\'__background__\', \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\', \'train\', \'truck\',\n                      \'boat\',\n                      \'trafficlight\', \'firehydrant\', \'stopsign\', \'parkingmeter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\',\n                      \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\',\n                      \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sportsball\', \'kite\', \'baseballbat\', \'baseballglove\',\n                      \'skateboard\', \'surfboard\', \'tennisracket\', \'bottle\', \'wineglass\', \'cup\', \'fork\', \'knife\', \'spoon\',\n                      \'bowl\', \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hotdog\', \'pizza\', \'donut\',\n                      \'cake\', \'chair\', \'couch\', \'pottedplant\', \'bed\', \'diningtable\', \'toilet\', \'tv\', \'laptop\', \'mouse\',\n                      \'remote\', \'keyboard\', \'cellphone\', \'microwave\', \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\',\n                      \'clock\', \'vase\', \'scissors\', \'teddybear\', \'hairdrier\', \'toothbrush\']\n\n        coco_annot_files = {\n            ""train2014"": ""annotations/instances_train2014.json"",\n            ""val2014"": ""annotations/instances_val2014.json"",\n            ""test2015"": ""annotations/image_info_test2015.json"",\n        }\n        proposal_dets = \'refcoco+/proposal/res101_coco_minus_refer_notime_dets.json\'\n        proposal_masks = \'refcoco+/proposal/res101_coco_minus_refer_notime_masks.json\'\n        self.vg_proposal = (""vgbua_res101_precomputed"", ""trainval2014_resnet101_faster_rcnn_genome"")\n        self.proposal_source = proposal_source\n        self.boxes = boxes\n        self.test_mode = test_mode\n        self.category_to_idx = {c: i for i, c in enumerate(categories)}\n        self.data_path = data_path\n        self.root_path = root_path\n        self.transform = transform\n        self.image_sets = [iset.strip() for iset in image_set.split(\'+\')]\n        self.coco = COCO(annotation_file=os.path.join(data_path, coco_annot_files[\'train2014\']))\n        self.refer = REFER(data_path, dataset=\'refcoco+\', splitBy=\'unc\')\n        self.refer_ids = []\n        for iset in self.image_sets:\n            self.refer_ids.extend(self.refer.getRefIds(split=iset))\n        self.refs = self.refer.loadRefs(ref_ids=self.refer_ids)\n        if \'proposal\' in boxes:\n            with open(os.path.join(data_path, proposal_dets), \'r\') as f:\n                proposal_list = json.load(f)\n            self.proposals = {}\n            for proposal in proposal_list:\n                image_id = proposal[\'image_id\']\n                if image_id in self.proposals:\n                    self.proposals[image_id].append(proposal[\'box\'])\n                else:\n                    self.proposals[image_id] = [proposal[\'box\']]\n        self.zip_mode = zip_mode\n        self.cache_mode = cache_mode\n        self.cache_db = cache_db\n        self.ignore_db_cache = ignore_db_cache\n        self.aspect_grouping = aspect_grouping\n        self.cache_dir = os.path.join(root_path, \'cache\')\n        self.add_image_as_a_box = add_image_as_a_box\n        self.mask_size = mask_size\n        if not os.path.exists(self.cache_dir):\n            makedirsExist(self.cache_dir)\n        self.tokenizer = tokenizer if tokenizer is not None \\\n            else BertTokenizer.from_pretrained(\n            \'bert-base-uncased\' if pretrained_model_name is None else pretrained_model_name,\n            cache_dir=self.cache_dir)\n\n        if zip_mode:\n            self.zipreader = ZipReader()\n\n        self.database = self.load_annotations()\n        if self.aspect_grouping:\n            self.group_ids = self.group_aspect(self.database)\n\n    @property\n    def data_names(self):\n        if self.test_mode:\n            return [\'image\', \'boxes\', \'im_info\', \'expression\']\n        else:\n            return [\'image\', \'boxes\', \'im_info\', \'expression\', \'label\']\n\n    def __getitem__(self, index):\n        idb = self.database[index]\n\n        # image related\n        img_id = idb[\'image_id\']\n        image = self._load_image(idb[\'image_fn\'])\n        im_info = torch.as_tensor([idb[\'width\'], idb[\'height\'], 1.0, 1.0])\n        if not self.test_mode:\n            gt_box = torch.as_tensor(idb[\'gt_box\'])\n        flipped = False\n        if self.boxes == \'gt\':\n            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n            anns = self.coco.loadAnns(ann_ids)\n            boxes = []\n            for ann in anns:\n                x_, y_, w_, h_ = ann[\'bbox\']\n                boxes.append([x_, y_, x_ + w_, y_ + h_])\n            boxes = torch.as_tensor(boxes)\n        elif self.boxes == \'proposal\':\n            if self.proposal_source == \'official\':\n                boxes = torch.as_tensor(self.proposals[img_id])\n                boxes[:, [2, 3]] += boxes[:, [0, 1]]\n            elif self.proposal_source == \'vg\':\n                box_file = os.path.join(self.data_path, self.vg_proposal[0], \'{0}.zip@/{0}\'.format(self.vg_proposal[1]))\n                boxes_fn = os.path.join(box_file, \'{}.json\'.format(idb[\'image_id\']))\n                boxes_data = self._load_json(boxes_fn)\n                boxes = torch.as_tensor(\n                    np.frombuffer(self.b64_decode(boxes_data[\'boxes\']), dtype=np.float32).reshape(\n                        (boxes_data[\'num_boxes\'], -1))\n                )\n            else:\n                raise NotImplemented\n        elif self.boxes == \'proposal+gt\' or self.boxes == \'gt+proposal\':\n            if self.proposal_source == \'official\':\n                boxes = torch.as_tensor(self.proposals[img_id])\n                boxes[:, [2, 3]] += boxes[:, [0, 1]]\n            elif self.proposal_source == \'vg\':\n                box_file = os.path.join(self.data_path, self.vg_proposal[0], \'{0}.zip@/{0}\'.format(self.vg_proposal[1]))\n                boxes_fn = os.path.join(box_file, \'{}.json\'.format(idb[\'image_id\']))\n                boxes_data = self._load_json(boxes_fn)\n                boxes = torch.as_tensor(\n                    np.frombuffer(self.b64_decode(boxes_data[\'boxes\']), dtype=np.float32).reshape(\n                        (boxes_data[\'num_boxes\'], -1))\n                )\n            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n            anns = self.coco.loadAnns(ann_ids)\n            gt_boxes = []\n            for ann in anns:\n                x_, y_, w_, h_ = ann[\'bbox\']\n                gt_boxes.append([x_, y_, x_ + w_, y_ + h_])\n            gt_boxes = torch.as_tensor(gt_boxes)\n            boxes = torch.cat((boxes, gt_boxes), 0)\n        else:\n            raise NotImplemented\n\n        if self.add_image_as_a_box:\n            w0, h0 = im_info[0], im_info[1]\n            image_box = torch.as_tensor([[0.0, 0.0, w0 - 1, h0 - 1]])\n            boxes = torch.cat((image_box, boxes), dim=0)\n\n        if self.transform is not None:\n            if not self.test_mode:\n                boxes = torch.cat((gt_box[None], boxes), 0)\n            image, boxes, _, im_info, flipped = self.transform(image, boxes, None, im_info, flipped)\n            if not self.test_mode:\n                gt_box = boxes[0]\n                boxes = boxes[1:]\n\n        # clamp boxes\n        w = im_info[0].item()\n        h = im_info[1].item()\n        boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(min=0, max=w - 1)\n        boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(min=0, max=h - 1)\n        if not self.test_mode:\n            gt_box[[0, 2]] = gt_box[[0, 2]].clamp(min=0, max=w - 1)\n            gt_box[[1, 3]] = gt_box[[1, 3]].clamp(min=0, max=h - 1)\n\n        # assign label to each box by its IoU with gt_box\n        if not self.test_mode:\n            boxes_ious = bbox_iou_py_vectorized(boxes, gt_box[None]).view(-1)\n            label = (boxes_ious > 0.5).float()\n\n        # expression\n        exp_tokens = idb[\'tokens\']\n        exp_retokens = self.tokenizer.tokenize(\' \'.join(exp_tokens))\n        if flipped:\n            exp_retokens = self.flip_tokens(exp_retokens, verbose=True)\n        exp_ids = self.tokenizer.convert_tokens_to_ids(exp_retokens)\n\n        if self.test_mode:\n            return image, boxes, im_info, exp_ids\n        else:\n            return image, boxes, im_info, exp_ids, label\n\n    @staticmethod\n    def flip_tokens(tokens, verbose=True):\n        changed = False\n        tokens_new = [tok for tok in tokens]\n        for i, tok in enumerate(tokens):\n            if tok == \'left\':\n                tokens_new[i] = \'right\'\n                changed = True\n            elif tok == \'right\':\n                tokens_new[i] = \'left\'\n                changed = True\n        if verbose and changed:\n            logging.info(\'[Tokens Flip] {} -> {}\'.format(tokens, tokens_new))\n        return tokens_new\n\n    @staticmethod\n    def b64_decode(string):\n        return base64.decodebytes(string.encode())\n\n    def load_annotations(self):\n        tic = time.time()\n        database = []\n        db_cache_name = \'refcoco+_boxes_{}_{}\'.format(self.boxes, \'+\'.join(self.image_sets))\n        if self.zip_mode:\n            db_cache_name = db_cache_name + \'_zipmode\'\n        if self.test_mode:\n            db_cache_name = db_cache_name + \'_testmode\'\n        db_cache_root = os.path.join(self.root_path, \'cache\')\n        db_cache_path = os.path.join(db_cache_root, \'{}.pkl\'.format(db_cache_name))\n\n        if os.path.exists(db_cache_path):\n            if not self.ignore_db_cache:\n                # reading cached database\n                print(\'cached database found in {}.\'.format(db_cache_path))\n                with open(db_cache_path, \'rb\') as f:\n                    print(\'loading cached database from {}...\'.format(db_cache_path))\n                    tic = time.time()\n                    database = cPickle.load(f)\n                    print(\'Done (t={:.2f}s)\'.format(time.time() - tic))\n                    return database\n            else:\n                print(\'cached database ignored.\')\n\n        # ignore or not find cached database, reload it from annotation file\n        print(\'loading database of split {}...\'.format(\'+\'.join(self.image_sets)))\n        tic = time.time()\n\n        for ref_id, ref in zip(self.refer_ids, self.refs):\n            iset = \'train2014\'\n            if not self.test_mode:\n                gt_x, gt_y, gt_w, gt_h = self.refer.getRefBox(ref_id=ref_id)\n            if self.zip_mode:\n                image_fn = os.path.join(self.data_path, iset + \'.zip@/\' + iset,\n                                        \'COCO_{}_{:012d}.jpg\'.format(iset, ref[\'image_id\']))\n            else:\n                image_fn = os.path.join(self.data_path, iset, \'COCO_{}_{:012d}.jpg\'.format(iset, ref[\'image_id\']))\n            for sent in ref[\'sentences\']:\n                idb = {\n                    \'sent_id\': sent[\'sent_id\'],\n                    \'ann_id\': ref[\'ann_id\'],\n                    \'ref_id\': ref[\'ref_id\'],\n                    \'image_id\': ref[\'image_id\'],\n                    \'image_fn\': image_fn,\n                    \'width\': self.coco.imgs[ref[\'image_id\']][\'width\'],\n                    \'height\': self.coco.imgs[ref[\'image_id\']][\'height\'],\n                    \'raw\': sent[\'raw\'],\n                    \'sent\': sent[\'sent\'],\n                    \'tokens\': sent[\'tokens\'],\n                    \'category_id\': ref[\'category_id\'],\n                    \'gt_box\': [gt_x, gt_y, gt_x + gt_w, gt_y + gt_h] if not self.test_mode else None\n                }\n                database.append(idb)\n\n        print(\'Done (t={:.2f}s)\'.format(time.time() - tic))\n\n        # cache database via cPickle\n        if self.cache_db:\n            print(\'caching database to {}...\'.format(db_cache_path))\n            tic = time.time()\n            if not os.path.exists(db_cache_root):\n                makedirsExist(db_cache_root)\n            with open(db_cache_path, \'wb\') as f:\n                cPickle.dump(database, f)\n            print(\'Done (t={:.2f}s)\'.format(time.time() - tic))\n\n        return database\n\n    @staticmethod\n    def group_aspect(database):\n        print(\'grouping aspect...\')\n        t = time.time()\n\n        # get shape of all images\n        widths = torch.as_tensor([idb[\'width\'] for idb in database])\n        heights = torch.as_tensor([idb[\'height\'] for idb in database])\n\n        # group\n        group_ids = torch.zeros(len(database))\n        horz = widths >= heights\n        vert = 1 - horz\n        group_ids[horz] = 0\n        group_ids[vert] = 1\n\n        print(\'Done (t={:.2f}s)\'.format(time.time() - t))\n\n        return group_ids\n\n    def __len__(self):\n        return len(self.database)\n\n    def _load_image(self, path):\n        if \'.zip@\' in path:\n            return self.zipreader.imread(path).convert(\'RGB\')\n        else:\n            return Image.open(path).convert(\'RGB\')\n\n    def _load_json(self, path):\n        if \'.zip@\' in path:\n            f = self.zipreader.read(path)\n            return json.loads(f.decode())\n        else:\n            with open(path, \'r\') as f:\n                return json.load(f)\n\n'"
refcoco/data/samplers/distributed.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# Code is copy-pasted exactly as in torch.utils.data.distributed.\n# FIXME remove this once c10d fixes the bug it has\nimport math\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data.sampler import Sampler\n\n\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset : offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch'"
refcoco/data/samplers/grouped_batch_sampler.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport itertools\n\nimport torch\nfrom torch.utils.data.sampler import BatchSampler\nfrom torch.utils.data.sampler import Sampler\n\n\nclass GroupedBatchSampler(BatchSampler):\n    """"""\n    Wraps another sampler to yield a mini-batch of indices.\n    It enforces that elements from the same group should appear in groups of batch_size.\n    It also tries to provide mini-batches which follows an ordering which is\n    as close as possible to the ordering from the original sampler.\n    Arguments:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_uneven (bool): If ``True``, the sampler will drop the batches whose\n            size is less than ``batch_size``\n    """"""\n\n    def __init__(self, sampler, group_ids, batch_size, drop_uneven=False):\n        if not isinstance(sampler, Sampler):\n            raise ValueError(\n                ""sampler should be an instance of ""\n                ""torch.utils.data.Sampler, but got sampler={}"".format(sampler)\n            )\n        self.sampler = sampler\n        self.group_ids = torch.as_tensor(group_ids)\n        assert self.group_ids.dim() == 1\n        self.batch_size = batch_size\n        self.drop_uneven = drop_uneven\n\n        self.groups = torch.unique(self.group_ids).sort(0)[0]\n\n        self._can_reuse_batches = False\n\n    def _prepare_batches(self):\n        dataset_size = len(self.group_ids)\n        # get the sampled indices from the sampler\n        sampled_ids = torch.as_tensor(list(self.sampler))\n        # potentially not all elements of the dataset were sampled\n        # by the sampler (e.g., DistributedSampler).\n        # construct a tensor which contains -1 if the element was\n        # not sampled, and a non-negative number indicating the\n        # order where the element was sampled.\n        # for example. if sampled_ids = [3, 1] and dataset_size = 5,\n        # the order is [-1, 1, -1, 0, -1]\n        order = torch.full((dataset_size,), -1, dtype=torch.int64)\n        order[sampled_ids] = torch.arange(len(sampled_ids))\n\n        # get a mask with the elements that were sampled\n        mask = order >= 0\n\n        # find the elements that belong to each individual cluster\n        clusters = [(self.group_ids == i) & mask for i in self.groups]\n        # get relative order of the elements inside each cluster\n        # that follows the order from the sampler\n        relative_order = [order[cluster] for cluster in clusters]\n        # with the relative order, find the absolute order in the\n        # sampled space\n        permutation_ids = [s[s.sort()[1]] for s in relative_order]\n        # permute each cluster so that they follow the order from\n        # the sampler\n        permuted_clusters = [sampled_ids[idx] for idx in permutation_ids]\n\n        # splits each cluster in batch_size, and merge as a list of tensors\n        splits = [c.split(self.batch_size) for c in permuted_clusters]\n        merged = tuple(itertools.chain.from_iterable(splits))\n\n        # now each batch internally has the right order, but\n        # they are grouped by clusters. Find the permutation between\n        # different batches that brings them as close as possible to\n        # the order that we have in the sampler. For that, we will consider the\n        # ordering as coming from the first element of each batch, and sort\n        # correspondingly\n        first_element_of_batch = [t[0].item() for t in merged]\n        # get and inverse mapping from sampled indices and the position where\n        # they occur (as returned by the sampler)\n        inv_sampled_ids_map = {v: k for k, v in enumerate(sampled_ids.tolist())}\n        # from the first element in each batch, get a relative ordering\n        first_index_of_batch = torch.as_tensor(\n            [inv_sampled_ids_map[s] for s in first_element_of_batch]\n        )\n\n        # permute the batches so that they approximately follow the order\n        # from the sampler\n        permutation_order = first_index_of_batch.sort(0)[1].tolist()\n        # finally, permute the batches\n        batches = [merged[i].tolist() for i in permutation_order]\n\n        if self.drop_uneven:\n            kept = []\n            for batch in batches:\n                if len(batch) == self.batch_size:\n                    kept.append(batch)\n            batches = kept\n        return batches\n\n    def __iter__(self):\n        if self._can_reuse_batches:\n            batches = self._batches\n            self._can_reuse_batches = False\n        else:\n            batches = self._prepare_batches()\n        self._batches = batches\n        return iter(batches)\n\n    def __len__(self):\n        if not hasattr(self, ""_batches""):\n            self._batches = self._prepare_batches()\n            self._can_reuse_batches = True\n        return len(self._batches)\n\n\n\n'"
refcoco/data/transforms/transforms.py,1,"b'import random\n\nimport numpy as np\n\nimport torch\nimport torchvision\nfrom torchvision.transforms import functional as F\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, boxes, masks, im_info, flipped):\n        for t in self.transforms:\n            image, boxes, masks, im_info, flipped = t(image, boxes, masks, im_info, flipped)\n        return image, boxes, masks, im_info, flipped\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + ""(""\n        for t in self.transforms:\n            format_string += ""\\n""\n            format_string += ""    {0}"".format(t)\n        format_string += ""\\n)""\n        return format_string\n\n\nclass Resize(object):\n    def __init__(self, min_size, max_size):\n        self.min_size = min_size\n        self.max_size = max_size\n\n    # modified from torchvision to add support for max size\n    def get_size(self, image_size):\n        w, h = image_size\n        size = self.min_size\n        max_size = self.max_size\n        if max_size is not None:\n            min_original_size = float(min((w, h)))\n            max_original_size = float(max((w, h)))\n            if max_original_size / min_original_size * size > max_size:\n                size = int(max_size * min_original_size / max_original_size)\n\n        if (w <= h and w == size) or (h <= w and h == size):\n            return (w, h)\n\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n        else:\n            oh = size\n            ow = int(size * w / h)\n\n        return (ow, oh)\n\n    def __call__(self, image, boxes, masks, im_info, flipped):\n        origin_size = im_info[:2]\n        size = self.get_size(origin_size)\n        if image is not None:\n            image = F.resize(image, (size[1], size[0]))\n\n        ratios = [size[0] * 1.0 / origin_size[0], size[1] * 1.0 / origin_size[1]]\n        if boxes is not None:\n            boxes[:, [0, 2]] *= ratios[0]\n            boxes[:, [1, 3]] *= ratios[1]\n        im_info[0], im_info[1] = size\n        im_info[2], im_info[3] = ratios\n        return image, boxes, masks, im_info, flipped\n\n\nclass RandomHorizontalFlip(object):\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, image, boxes, masks, im_info, flipped):\n        if random.random() < self.prob:\n            w, h = im_info[:2]\n            if image is not None:\n                image = F.hflip(image)\n            if boxes is not None:\n                boxes[:, [0, 2]] = w - 1 - boxes[:, [2, 0]]\n            if masks is not None:\n                masks = torch.as_tensor(masks.numpy()[:, :, ::-1].tolist())\n            flipped = not flipped\n        return image, boxes, masks, im_info, flipped\n\n\nclass ToTensor(object):\n    def __call__(self, image, boxes, masks, im_info, flipped):\n        return F.to_tensor(image) if image is not None else image, boxes, masks, im_info, flipped\n\n\nclass Normalize(object):\n    def __init__(self, mean, std, to_bgr255=True):\n        self.mean = mean\n        self.std = std\n        self.to_bgr255 = to_bgr255\n\n    def __call__(self, image, boxes, masks, im_info, flipped):\n        if image is not None:\n            if self.to_bgr255:\n                image = image[[2, 1, 0]] * 255\n            image = F.normalize(image, mean=self.mean, std=self.std)\n        return image, boxes, masks, im_info, flipped\n\n\nclass FixPadding(object):\n    def __init__(self, min_size, max_size, pad=0):\n        self.min_size = min_size\n        self.max_size = max_size\n        self.pad = pad\n\n    def __call__(self, image, boxes, masks, im_info, flipped):\n\n        if image is not None:\n            # padding to fixed size for determinacy\n            c, h, w = image.shape\n            if h <= w:\n                h1 = self.min_size\n                w1 = self.max_size\n            else:\n                h1 = self.max_size\n                w1 = self.min_size\n            padded_image = image.new_zeros((c, h1, w1)).fill_(self.pad)\n            padded_image[:, :h, :w] = image\n            image = padded_image\n\n        return image, boxes, masks, im_info, flipped\n'"
vcr/data/datasets/vcr.py,17,"b'import os\nimport time\nimport jsonlines\nimport json\nimport _pickle as cPickle\nfrom PIL import Image\nfrom copy import deepcopy\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom external.pytorch_pretrained_bert import BertTokenizer, BasicTokenizer\n\nfrom common.utils.zipreader import ZipReader\nfrom common.utils.create_logger import makedirsExist\nfrom common.utils.mask import generate_instance_mask\nfrom common.nlp.misc import get_align_matrix\nfrom common.utils.misc import block_digonal_matrix\nfrom common.nlp.misc import random_word_with_token_ids\nfrom common.nlp.roberta import RobertaTokenizer\n\nGENDER_NEUTRAL_NAMES = [\'Casey\', \'Riley\', \'Jessie\', \'Jackie\', \'Avery\', \'Jaime\', \'Peyton\', \'Kerry\', \'Jody\', \'Kendall\',\n                        \'Frankie\', \'Pat\', \'Quinn\']\n# GENDER_NEUTRAL_NAMES = [\'person\']\n\n\nclass VCRDataset(Dataset):\n    def __init__(self, ann_file, image_set, root_path, data_path, transform=None, task=\'Q2A\', test_mode=False,\n                 zip_mode=False, cache_mode=False, cache_db=False, ignore_db_cache=True,\n                 basic_tokenizer=None, tokenizer=None, pretrained_model_name=None,\n                 only_use_relevant_dets=False, add_image_as_a_box=False, mask_size=(14, 14),\n                 aspect_grouping=False, basic_align=False, qa2r_noq=False, qa2r_aug=False,\n                 seq_len=64,\n                 **kwargs):\n        """"""\n        Visual Commonsense Reasoning Dataset\n\n        :param ann_file: annotation jsonl file\n        :param image_set: image folder name, e.g., \'vcr1images\'\n        :param root_path: root path to cache database loaded from annotation file\n        :param data_path: path to vcr dataset\n        :param transform: transform\n        :param task: \'Q2A\' means question to answer, \'QA2R\' means question and answer to rationale,\n                     \'Q2AR\' means question to answer and rationale\n        :param test_mode: test mode means no labels available\n        :param zip_mode: reading images and metadata in zip archive\n        :param cache_mode: cache whole dataset to RAM first, then __getitem__ read them from RAM\n        :param ignore_db_cache: ignore previous cached database, reload it from annotation file\n        :param tokenizer: default is BertTokenizer from pytorch_pretrained_bert\n        :param only_use_relevant_dets: filter out detections not used in query and response\n        :param add_image_as_a_box: add whole image as a box\n        :param mask_size: size of instance mask of each object\n        :param aspect_grouping: whether to group images via their aspect\n        :param basic_align: align to tokens retokenized by basic_tokenizer\n        :param qa2r_noq: in QA->R, the query contains only the correct answer, without question\n        :param qa2r_aug: in QA->R, whether to augment choices to include those with wrong answer in query\n        :param kwargs:\n        """"""\n        super(VCRDataset, self).__init__()\n\n        assert not cache_mode, \'currently not support cache mode!\'\n        assert task in [\'Q2A\', \'QA2R\', \'Q2AR\'] , \'not support task {}\'.format(task)\n        assert not qa2r_aug, ""Not implemented!""\n\n        self.qa2r_noq = qa2r_noq\n        self.qa2r_aug = qa2r_aug\n\n        self.seq_len = seq_len\n\n        categories = [\'__background__\', \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\', \'train\', \'truck\', \'boat\',\n                      \'trafficlight\', \'firehydrant\', \'stopsign\', \'parkingmeter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\',\n                      \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\',\n                      \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sportsball\', \'kite\', \'baseballbat\', \'baseballglove\',\n                      \'skateboard\', \'surfboard\', \'tennisracket\', \'bottle\', \'wineglass\', \'cup\', \'fork\', \'knife\', \'spoon\',\n                      \'bowl\', \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hotdog\', \'pizza\', \'donut\',\n                      \'cake\', \'chair\', \'couch\', \'pottedplant\', \'bed\', \'diningtable\', \'toilet\', \'tv\', \'laptop\', \'mouse\',\n                      \'remote\', \'keyboard\', \'cellphone\', \'microwave\', \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\',\n                      \'clock\', \'vase\', \'scissors\', \'teddybear\', \'hairdrier\', \'toothbrush\']\n        self.category_to_idx = {c: i for i, c in enumerate(categories)}\n        self.data_path = data_path\n        self.root_path = root_path\n        self.ann_file = os.path.join(data_path, ann_file)\n        self.image_set = image_set\n        self.transform = transform\n        self.task = task\n        self.test_mode = test_mode\n        self.zip_mode = zip_mode\n        self.cache_mode = cache_mode\n        self.cache_db = cache_db\n        self.ignore_db_cache = ignore_db_cache\n        self.aspect_grouping = aspect_grouping\n        self.basic_align = basic_align\n        print(\'Dataset Basic Align: {}\'.format(self.basic_align))\n        self.cache_dir = os.path.join(root_path, \'cache\')\n        self.only_use_relevant_dets = only_use_relevant_dets\n        self.add_image_as_a_box = add_image_as_a_box\n        self.mask_size = mask_size\n        if not os.path.exists(self.cache_dir):\n            makedirsExist(self.cache_dir)\n        self.basic_tokenizer = basic_tokenizer if basic_tokenizer is not None \\\n            else BasicTokenizer(do_lower_case=True)\n        if tokenizer is None:\n            if pretrained_model_name is None:\n                pretrained_model_name = \'bert-base-uncased\'\n            if \'roberta\' in pretrained_model_name:\n                tokenizer = RobertaTokenizer.from_pretrained(pretrained_model_name, cache_dir=self.cache_dir)\n            else:\n                tokenizer = BertTokenizer.from_pretrained(pretrained_model_name, cache_dir=self.cache_dir)\n        self.tokenizer = tokenizer\n\n        if zip_mode:\n            self.zipreader = ZipReader()\n\n        self.database = self.load_annotations(self.ann_file)\n        if self.aspect_grouping:\n            assert False, ""Not support aspect grouping now!""\n            self.group_ids = self.group_aspect(self.database)\n\n        self.person_name_id = 0\n\n    def load_annotations(self, ann_file):\n        tic = time.time()\n        database = []\n        db_cache_name = \'vcr_nometa_{}_{}_{}\'.format(self.task, self.image_set, os.path.basename(ann_file)[:-len(\'.jsonl\')])\n        if self.only_use_relevant_dets:\n            db_cache_name = db_cache_name + \'_only_relevant_dets\'\n        if self.zip_mode:\n            db_cache_name = db_cache_name + \'_zipped\'\n        db_cache_root = os.path.join(self.root_path, \'cache\')\n        db_cache_path = os.path.join(db_cache_root, \'{}.pkl\'.format(db_cache_name))\n\n        if os.path.exists(db_cache_path):\n            if not self.ignore_db_cache:\n                # reading cached database\n                print(\'cached database found in {}.\'.format(db_cache_path))\n                with open(db_cache_path, \'rb\') as f:\n                    print(\'loading cached database from {}...\'.format(db_cache_path))\n                    tic = time.time()\n                    database = cPickle.load(f)\n                    print(\'Done (t={:.2f}s)\'.format(time.time() - tic))\n                    return database\n            else:\n                print(\'cached database ignored.\')\n\n        # ignore or not find cached database, reload it from annotation file\n        print(\'loading database from {}...\'.format(ann_file))\n        tic = time.time()\n\n        with jsonlines.open(ann_file) as reader:\n            for ann in reader:\n                if self.zip_mode:\n                    img_fn = os.path.join(self.data_path, self.image_set + \'.zip@/\' + self.image_set, ann[\'img_fn\'])\n                    metadata_fn = os.path.join(self.data_path, self.image_set + \'.zip@/\' + self.image_set, ann[\'metadata_fn\'])\n                else:\n                    img_fn = os.path.join(self.data_path, self.image_set, ann[\'img_fn\'])\n                    metadata_fn = os.path.join(self.data_path, self.image_set, ann[\'metadata_fn\'])\n\n                db_i = {\n                    \'annot_id\': ann[\'annot_id\'],\n                    \'objects\': ann[\'objects\'],\n                    \'img_fn\': img_fn,\n                    \'metadata_fn\': metadata_fn,\n                    \'question\': ann[\'question\'],\n                    \'answer_choices\': ann[\'answer_choices\'],\n                    \'answer_label\': ann[\'answer_label\'] if not self.test_mode else None,\n                    \'rationale_choices\': ann[\'rationale_choices\'],\n                    \'rationale_label\': ann[\'rationale_label\'] if not self.test_mode else None,\n                }\n                database.append(db_i)\n        print(\'Done (t={:.2f}s)\'.format(time.time() - tic))\n\n        # cache database via cPickle\n        if self.cache_db:\n            print(\'caching database to {}...\'.format(db_cache_path))\n            tic = time.time()\n            if not os.path.exists(db_cache_root):\n                makedirsExist(db_cache_root)\n            with open(db_cache_path, \'wb\') as f:\n                cPickle.dump(database, f)\n            print(\'Done (t={:.2f}s)\'.format(time.time() - tic))\n\n        return database\n\n    @staticmethod\n    def group_aspect(database):\n        print(\'grouping aspect...\')\n        t = time.time()\n\n        # get shape of all images\n        widths = torch.as_tensor([idb[\'width\'] for idb in database])\n        heights = torch.as_tensor([idb[\'height\'] for idb in database])\n\n        # group\n        group_ids = torch.zeros(len(database))\n        horz = widths >= heights\n        vert = 1 - horz\n        group_ids[horz] = 0\n        group_ids[vert] = 1\n\n        print(\'Done (t={:.2f}s)\'.format(time.time() - t))\n\n        return group_ids\n\n    def retokenize_and_convert_to_ids_with_tag(self, tokens, objects_replace_name, non_obj_tag=-1):\n        parsed_tokens = []\n        tags = []\n        align_ids = []\n        raw = []\n        align_id = 0\n        for mixed_token in tokens:\n            if isinstance(mixed_token, list):\n                tokens = [objects_replace_name[o] for o in mixed_token]\n                retokenized_tokens = self.tokenizer.tokenize(tokens[0])\n                raw.append(tokens[0])\n                tags.extend([mixed_token[0] + non_obj_tag + 1 for _ in retokenized_tokens])\n                align_ids.extend([align_id for _ in retokenized_tokens])\n                align_id += 1\n                for token, o in zip(tokens[1:], mixed_token[1:]):\n                    retokenized_tokens.append(\'and\')\n                    tags.append(non_obj_tag)\n                    align_ids.append(align_id)\n                    align_id += 1\n                    re_tokens = self.tokenizer.tokenize(token)\n                    retokenized_tokens.extend(re_tokens)\n                    tags.extend([o + non_obj_tag + 1 for _ in re_tokens])\n                    align_ids.extend([align_id for _ in re_tokens])\n                    align_id += 1\n                    raw.extend([\'and\', token])\n                parsed_tokens.extend(retokenized_tokens)\n            else:\n                if self.basic_align:\n                    # basic align\n                    basic_tokens = self.basic_tokenizer.tokenize(mixed_token)\n                    raw.extend(basic_tokens)\n                    for t in basic_tokens:\n                        retokenized_tokens = self.tokenizer.tokenize(t)\n                        parsed_tokens.extend(retokenized_tokens)\n                        align_ids.extend([align_id for _ in retokenized_tokens])\n                        tags.extend([non_obj_tag for _ in retokenized_tokens])\n                        align_id += 1\n                else:\n                    # fully align to original tokens\n                    raw.append(mixed_token)\n                    retokenized_tokens = self.tokenizer.tokenize(mixed_token)\n                    parsed_tokens.extend(retokenized_tokens)\n                    align_ids.extend([align_id for _ in retokenized_tokens])\n                    tags.extend([non_obj_tag for _ in retokenized_tokens])\n                    align_id += 1\n        ids = self.tokenizer.convert_tokens_to_ids(parsed_tokens)\n        ids_with_tag = list(zip(ids, tags, align_ids))\n        \n        return ids_with_tag, raw\n\n    @staticmethod\n    def keep_only_relevant_dets(question, answer_choices, rationale_choices):\n        dets_to_use = []\n        for i, tok in enumerate(question):\n            if isinstance(tok, list):\n                for j, o in enumerate(tok):\n                    if o not in dets_to_use:\n                        dets_to_use.append(o)\n                    question[i][j] = dets_to_use.index(o)\n        if answer_choices is not None:\n            for n, answer in enumerate(answer_choices):\n                for i, tok in enumerate(answer):\n                    if isinstance(tok, list):\n                        for j, o in enumerate(tok):\n                            if o not in dets_to_use:\n                                dets_to_use.append(o)\n                            answer_choices[n][i][j] = dets_to_use.index(o)\n        if rationale_choices is not None:\n            for n, rationale in enumerate(rationale_choices):\n                for i, tok in enumerate(rationale):\n                    if isinstance(tok, list):\n                        for j, o in enumerate(tok):\n                            if o not in dets_to_use:\n                                dets_to_use.append(o)\n                            rationale_choices[n][i][j] = dets_to_use.index(o)\n\n        return dets_to_use, question, answer_choices, rationale_choices\n\n    def __getitem__(self, index):\n        # self.person_name_id = 0\n        idb = deepcopy(self.database[index])\n\n        metadata = self._load_json(idb[\'metadata_fn\'])\n        idb[\'boxes\'] = metadata[\'boxes\']\n        idb[\'segms\'] = metadata[\'segms\']\n        # idb[\'width\'] = metadata[\'width\']\n        # idb[\'height\'] = metadata[\'height\']\n        if self.only_use_relevant_dets:\n            dets_to_use, idb[\'question\'], idb[\'answer_choices\'], idb[\'rationale_choices\'] = \\\n                self.keep_only_relevant_dets(idb[\'question\'],\n                                             idb[\'answer_choices\'],\n                                             idb[\'rationale_choices\'] if not self.task == \'Q2A\' else None)\n            idb[\'objects\'] = [idb[\'objects\'][i] for i in dets_to_use]\n            idb[\'boxes\'] = [idb[\'boxes\'][i] for i in dets_to_use]\n            idb[\'segms\'] = [idb[\'segms\'][i] for i in dets_to_use]\n        objects_replace_name = []\n        for o in idb[\'objects\']:\n            if o == \'person\':\n                objects_replace_name.append(GENDER_NEUTRAL_NAMES[self.person_name_id])\n                self.person_name_id = (self.person_name_id + 1) % len(GENDER_NEUTRAL_NAMES)\n            else:\n                objects_replace_name.append(o)\n\n        non_obj_tag = 0 if self.add_image_as_a_box else -1\n        idb[\'question\'] = self.retokenize_and_convert_to_ids_with_tag(idb[\'question\'],\n                                                                      objects_replace_name=objects_replace_name,\n                                                                      non_obj_tag=non_obj_tag)\n\n        idb[\'answer_choices\'] = [self.retokenize_and_convert_to_ids_with_tag(answer,\n                                                                             objects_replace_name=objects_replace_name,\n                                                                             non_obj_tag=non_obj_tag)\n                                 for answer in idb[\'answer_choices\']]\n\n        idb[\'rationale_choices\'] = [self.retokenize_and_convert_to_ids_with_tag(rationale,\n                                                                                objects_replace_name=objects_replace_name,\n                                                                                non_obj_tag=non_obj_tag)\n                                    for rationale in idb[\'rationale_choices\']] if not self.task == \'Q2A\' else None\n\n        # truncate text to seq_len\n        if self.task == \'Q2A\':\n            q = idb[\'question\'][0]\n            for a, a_raw in idb[\'answer_choices\']:\n                while len(q) + len(a) > self.seq_len:\n                    if len(a) > len(q):\n                        a.pop()\n                    else:\n                        q.pop()\n        elif self.task == \'QA2R\':\n            if not self.test_mode:\n                q = idb[\'question\'][0]\n                a = idb[\'answer_choices\'][idb[\'answer_label\']][0]\n                for r, r_raw in idb[\'rationale_choices\']:\n                    while len(q) + len(a) + len(r) > self.seq_len:\n                        if len(r) > (len(q) + len(a)):\n                            r.pop()\n                        elif len(q) > 1:\n                            q.pop()\n                        else:\n                            a.pop()\n        else:\n            raise NotImplemented\n\n        image = self._load_image(idb[\'img_fn\'])\n        w0, h0 = image.size\n        objects = idb[\'objects\']\n\n        # extract bounding boxes and instance masks in metadata\n        boxes = torch.zeros((len(objects), 6))\n        masks = torch.zeros((len(objects), *self.mask_size))\n        if len(objects) > 0:\n            boxes[:, :5] = torch.tensor(idb[\'boxes\'])\n            boxes[:, 5] = torch.tensor([self.category_to_idx[obj] for obj in objects])\n            for i in range(len(objects)):\n                seg_polys = [torch.as_tensor(seg) for seg in idb[\'segms\'][i]]\n                masks[i] = generate_instance_mask(seg_polys, idb[\'boxes\'][i], mask_size=self.mask_size,\n                                                  dtype=torch.float32, copy=False)\n        if self.add_image_as_a_box:\n            image_box = torch.as_tensor([[0, 0, w0 - 1, h0 - 1, 1.0, 0]])\n            image_mask = torch.ones((1, *self.mask_size))\n            boxes = torch.cat((image_box, boxes), dim=0)\n            masks = torch.cat((image_mask, masks), dim=0)\n\n        question, question_raw = idb[\'question\']\n        question_align_matrix = get_align_matrix([w[2] for w in question])\n        answer_choices, answer_choices_raw = zip(*idb[\'answer_choices\'])\n        answer_choices = list(answer_choices)\n        answer_align_matrix = [get_align_matrix([w[2] for w in a]) for a in answer_choices]\n        answer_label = torch.as_tensor(idb[\'answer_label\']) if not self.test_mode else None\n        if not self.task == \'Q2A\':\n            rationale_choices = [r[0] for r in idb[\'rationale_choices\']]\n            rationale_align_matrix = [get_align_matrix([w[2] for w in r]) for r in rationale_choices]\n            rationale_label = torch.as_tensor(idb[\'rationale_label\']) if not self.test_mode else None\n\n        # transform\n        im_info = torch.tensor([w0, h0, 1.0, 1.0, index])\n        if self.transform is not None:\n            image, boxes, masks, im_info = self.transform(image, boxes, masks, im_info)\n\n        # clamp boxes\n        w = im_info[0].item()\n        h = im_info[1].item()\n        boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(min=0, max=w - 1)\n        boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(min=0, max=h - 1)\n\n        if self.task == \'Q2AR\':\n            if not self.test_mode:\n                outputs = (image, boxes, masks,\n                           question, question_align_matrix,\n                           answer_choices, answer_align_matrix, answer_label,\n                           rationale_choices, rationale_align_matrix, rationale_label,\n                           im_info)\n            else:\n                outputs = (image, boxes, masks,\n                           question, question_align_matrix,\n                           answer_choices, answer_align_matrix,\n                           rationale_choices, rationale_align_matrix,\n                           im_info)\n        elif self.task == \'Q2A\':\n            if not self.test_mode:\n                outputs = (image, boxes, masks,\n                           question, question_align_matrix,\n                           answer_choices, answer_align_matrix, answer_label,\n                           im_info)\n            else:\n                outputs = (image, boxes, masks,\n                           question, question_align_matrix,\n                           answer_choices, answer_align_matrix,\n                           im_info)\n        elif self.task == \'QA2R\':\n            if not self.test_mode:\n                outputs = (image, boxes, masks,\n                           ([] if self.qa2r_noq else question) + answer_choices[answer_label],\n                           answer_align_matrix[answer_label] if self.qa2r_noq else block_digonal_matrix(question_align_matrix, answer_align_matrix[answer_label]),\n                           rationale_choices, rationale_align_matrix, rationale_label,\n                           im_info)\n            else:\n                outputs = (image, boxes, masks,\n                           [([] if self.qa2r_noq else question) + a for a in answer_choices],\n                           [m if self.qa2r_noq else block_digonal_matrix(question_align_matrix, m)\n                            for m in answer_align_matrix],\n                           rationale_choices, rationale_align_matrix,\n                           im_info)\n\n        return outputs\n\n    def __len__(self):\n        return len(self.database)\n\n    def _load_image(self, path):\n        if \'.zip@\' in path:\n            return self.zipreader.imread(path)\n        else:\n            return Image.open(path)\n\n    def _load_json(self, path):\n        if \'.zip@\' in path:\n            f = self.zipreader.read(path)\n            return json.loads(f.decode())\n        else:\n            with open(path, \'r\') as f:\n                return json.load(f)\n\n    @property\n    def data_names(self):\n        if not self.test_mode:\n            if self.task == \'Q2A\':\n                data_names = [\'image\', \'boxes\', \'masks\',\n                              \'question\', \'question_align_matrix\',\n                              \'answer_choices\', \'answer_align_matrix\', \'answer_label\',\n                              \'im_info\']\n            elif self.task == \'QA2R\':\n                data_names = [\'image\', \'boxes\', \'masks\',\n                              \'question\', \'question_align_matrix\',\n                              \'rationale_choices\', \'rationale_align_matrix\', \'rationale_label\',\n                              \'im_info\']\n            else:\n                data_names = [\'image\', \'boxes\', \'masks\',\n                              \'question\', \'question_align_matrix\',\n                              \'answer_choices\', \'answer_align_matrix\', \'answer_label\',\n                              \'rationale_choices\', \'rationale_align_matrix\', \'rationale_label\',\n                              \'im_info\']\n        else:\n            if self.task == \'Q2A\':\n                data_names = [\'image\', \'boxes\', \'masks\',\n                              \'question\', \'question_align_matrix\',\n                              \'answer_choices\', \'answer_align_matrix\',\n                              \'im_info\']\n            elif self.task == \'QA2R\':\n                data_names = [\'image\', \'boxes\', \'masks\',\n                              \'question\', \'question_align_matrix\',\n                              \'rationale_choices\', \'rationale_align_matrix\',\n                              \'im_info\']\n            else:\n                data_names = [\'image\', \'boxes\', \'masks\',\n                              \'question\', \'question_align_matrix\',\n                              \'answer_choices\', \'answer_align_matrix\',\n                              \'rationale_choices\', \'rationale_align_matrix\',\n                              \'im_info\']\n\n        return data_names\n'"
vcr/data/samplers/distributed.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# Code is copy-pasted exactly as in torch.utils.data.distributed.\n# FIXME remove this once c10d fixes the bug it has\nimport math\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data.sampler import Sampler\n\n\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset : offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch'"
vcr/data/samplers/grouped_batch_sampler.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport itertools\n\nimport torch\nfrom torch.utils.data.sampler import BatchSampler\nfrom torch.utils.data.sampler import Sampler\n\n\nclass GroupedBatchSampler(BatchSampler):\n    """"""\n    Wraps another sampler to yield a mini-batch of indices.\n    It enforces that elements from the same group should appear in groups of batch_size.\n    It also tries to provide mini-batches which follows an ordering which is\n    as close as possible to the ordering from the original sampler.\n    Arguments:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_uneven (bool): If ``True``, the sampler will drop the batches whose\n            size is less than ``batch_size``\n    """"""\n\n    def __init__(self, sampler, group_ids, batch_size, drop_uneven=False):\n        if not isinstance(sampler, Sampler):\n            raise ValueError(\n                ""sampler should be an instance of ""\n                ""torch.utils.data.Sampler, but got sampler={}"".format(sampler)\n            )\n        self.sampler = sampler\n        self.group_ids = torch.as_tensor(group_ids)\n        assert self.group_ids.dim() == 1\n        self.batch_size = batch_size\n        self.drop_uneven = drop_uneven\n\n        self.groups = torch.unique(self.group_ids).sort(0)[0]\n\n        self._can_reuse_batches = False\n\n    def _prepare_batches(self):\n        dataset_size = len(self.group_ids)\n        # get the sampled indices from the sampler\n        sampled_ids = torch.as_tensor(list(self.sampler))\n        # potentially not all elements of the dataset were sampled\n        # by the sampler (e.g., DistributedSampler).\n        # construct a tensor which contains -1 if the element was\n        # not sampled, and a non-negative number indicating the\n        # order where the element was sampled.\n        # for example. if sampled_ids = [3, 1] and dataset_size = 5,\n        # the order is [-1, 1, -1, 0, -1]\n        order = torch.full((dataset_size,), -1, dtype=torch.int64)\n        order[sampled_ids] = torch.arange(len(sampled_ids))\n\n        # get a mask with the elements that were sampled\n        mask = order >= 0\n\n        # find the elements that belong to each individual cluster\n        clusters = [(self.group_ids == i) & mask for i in self.groups]\n        # get relative order of the elements inside each cluster\n        # that follows the order from the sampler\n        relative_order = [order[cluster] for cluster in clusters]\n        # with the relative order, find the absolute order in the\n        # sampled space\n        permutation_ids = [s[s.sort()[1]] for s in relative_order]\n        # permute each cluster so that they follow the order from\n        # the sampler\n        permuted_clusters = [sampled_ids[idx] for idx in permutation_ids]\n\n        # splits each cluster in batch_size, and merge as a list of tensors\n        splits = [c.split(self.batch_size) for c in permuted_clusters]\n        merged = tuple(itertools.chain.from_iterable(splits))\n\n        # now each batch internally has the right order, but\n        # they are grouped by clusters. Find the permutation between\n        # different batches that brings them as close as possible to\n        # the order that we have in the sampler. For that, we will consider the\n        # ordering as coming from the first element of each batch, and sort\n        # correspondingly\n        first_element_of_batch = [t[0].item() for t in merged]\n        # get and inverse mapping from sampled indices and the position where\n        # they occur (as returned by the sampler)\n        inv_sampled_ids_map = {v: k for k, v in enumerate(sampled_ids.tolist())}\n        # from the first element in each batch, get a relative ordering\n        first_index_of_batch = torch.as_tensor(\n            [inv_sampled_ids_map[s] for s in first_element_of_batch]\n        )\n\n        # permute the batches so that they approximately follow the order\n        # from the sampler\n        permutation_order = first_index_of_batch.sort(0)[1].tolist()\n        # finally, permute the batches\n        batches = [merged[i].tolist() for i in permutation_order]\n\n        if self.drop_uneven:\n            kept = []\n            for batch in batches:\n                if len(batch) == self.batch_size:\n                    kept.append(batch)\n            batches = kept\n        return batches\n\n    def __iter__(self):\n        if self._can_reuse_batches:\n            batches = self._batches\n            self._can_reuse_batches = False\n        else:\n            batches = self._prepare_batches()\n        self._batches = batches\n        return iter(batches)\n\n    def __len__(self):\n        if not hasattr(self, ""_batches""):\n            self._batches = self._prepare_batches()\n            self._can_reuse_batches = True\n        return len(self._batches)\n\n\n\n'"
vcr/data/transforms/transforms.py,1,"b'import random\n\nimport numpy as np\n\nimport torch\nimport torchvision\nfrom torchvision.transforms import functional as F\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, boxes, masks, im_info):\n        for t in self.transforms:\n            image, boxes, masks, im_info = t(image, boxes, masks, im_info)\n        return image, boxes, masks, im_info\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + ""(""\n        for t in self.transforms:\n            format_string += ""\\n""\n            format_string += ""    {0}"".format(t)\n        format_string += ""\\n)""\n        return format_string\n\n\nclass Resize(object):\n    def __init__(self, min_size, max_size):\n        self.min_size = min_size\n        self.max_size = max_size\n\n    # modified from torchvision to add support for max size\n    def get_size(self, image_size):\n        w, h = image_size\n        size = self.min_size\n        max_size = self.max_size\n        if max_size is not None:\n            min_original_size = float(min((w, h)))\n            max_original_size = float(max((w, h)))\n            if max_original_size / min_original_size * size > max_size:\n                size = int(max_size * min_original_size / max_original_size)\n\n        if (w <= h and w == size) or (h <= w and h == size):\n            return (w, h)\n\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n        else:\n            oh = size\n            ow = int(size * w / h)\n\n        return (ow, oh)\n\n    def __call__(self, image, boxes, masks, im_info):\n        origin_size = image.size\n        size = self.get_size(origin_size)\n        image = F.resize(image, (size[1], size[0]))\n\n        ratios = [size[0] * 1.0 / origin_size[0], size[1] * 1.0 / origin_size[1]]\n        boxes[:, [0, 2]] *= ratios[0]\n        boxes[:, [1, 3]] *= ratios[1]\n        im_info[0], im_info[1] = image.size\n        im_info[2], im_info[3] = ratios\n        return image, boxes, masks, im_info\n\n\nclass RandomHorizontalFlip(object):\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, image, boxes, masks, im_info):\n        if random.random() < self.prob:\n            w, h = image.size\n            image = F.hflip(image)\n            boxes[:, [0, 2]] = w - 1 - boxes[:, [2, 0]]\n            masks = torch.as_tensor(masks.numpy()[:, :, ::-1].tolist())\n        return image, boxes, masks, im_info\n\n\nclass ToTensor(object):\n    def __call__(self, image, boxes, masks, im_info):\n        return F.to_tensor(image), boxes, masks, im_info\n\n\nclass Normalize(object):\n    def __init__(self, mean, std, to_bgr255=True):\n        self.mean = mean\n        self.std = std\n        self.to_bgr255 = to_bgr255\n\n    def __call__(self, image, boxes, masks, im_info):\n        if self.to_bgr255:\n            image = image[[2, 1, 0]] * 255\n        image = F.normalize(image, mean=self.mean, std=self.std)\n        return image, boxes, masks, im_info\n\n\nclass FixPadding(object):\n    def __init__(self, min_size, max_size, pad=0):\n        self.min_size = min_size\n        self.max_size = max_size\n        self.pad = pad\n\n    def __call__(self, image, boxes, masks, im_info):\n\n        # padding to fixed size for determinacy\n        c, h, w = image.shape\n        if h <= w:\n            h1 = self.min_size\n            w1 = self.max_size\n        else:\n            h1 = self.max_size\n            w1 = self.min_size\n        padded_image = image.new_zeros((c, h1, w1)).fill_(self.pad)\n        padded_image[:, :h, :w] = image\n        image = padded_image\n\n        return image, boxes, masks, im_info\n'"
vqa/data/datasets/vqa.py,13,"b'import os\nimport json\nimport _pickle as cPickle\nfrom PIL import Image\nimport re\nimport base64\nimport numpy as np\nimport csv\nimport sys\nimport time\nimport pprint\nimport logging\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom external.pytorch_pretrained_bert import BertTokenizer\n\nfrom common.utils.zipreader import ZipReader\nfrom common.utils.create_logger import makedirsExist\n\nfrom pycocotools.coco import COCO\n\ncsv.field_size_limit(sys.maxsize)\nFIELDNAMES = [\'image_id\', \'image_w\', \'image_h\', \'num_boxes\', \'boxes\', \'features\']\n\n\nclass VQA(Dataset):\n    def __init__(self, image_set, root_path, data_path, answer_vocab_file, use_imdb=True,\n                 with_precomputed_visual_feat=False, boxes=""36"",\n                 transform=None, test_mode=False,\n                 zip_mode=False, cache_mode=False, cache_db=True, ignore_db_cache=True,\n                 tokenizer=None, pretrained_model_name=None,\n                 add_image_as_a_box=False, mask_size=(14, 14),\n                 aspect_grouping=False, **kwargs):\n        """"""\n        Visual Question Answering Dataset\n\n        :param image_set: image folder name\n        :param root_path: root path to cache database loaded from annotation file\n        :param data_path: path to vcr dataset\n        :param transform: transform\n        :param test_mode: test mode means no labels available\n        :param zip_mode: reading images and metadata in zip archive\n        :param cache_mode: cache whole dataset to RAM first, then __getitem__ read them from RAM\n        :param ignore_db_cache: ignore previous cached database, reload it from annotation file\n        :param tokenizer: default is BertTokenizer from pytorch_pretrained_bert\n        :param add_image_as_a_box: add whole image as a box\n        :param mask_size: size of instance mask of each object\n        :param aspect_grouping: whether to group images via their aspect\n        :param kwargs:\n        """"""\n        super(VQA, self).__init__()\n\n        assert not cache_mode, \'currently not support cache mode!\'\n\n        categories = [\'__background__\', \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\', \'train\', \'truck\',\n                      \'boat\',\n                      \'trafficlight\', \'firehydrant\', \'stopsign\', \'parkingmeter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\',\n                      \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\',\n                      \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sportsball\', \'kite\', \'baseballbat\', \'baseballglove\',\n                      \'skateboard\', \'surfboard\', \'tennisracket\', \'bottle\', \'wineglass\', \'cup\', \'fork\', \'knife\', \'spoon\',\n                      \'bowl\', \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hotdog\', \'pizza\', \'donut\',\n                      \'cake\', \'chair\', \'couch\', \'pottedplant\', \'bed\', \'diningtable\', \'toilet\', \'tv\', \'laptop\', \'mouse\',\n                      \'remote\', \'keyboard\', \'cellphone\', \'microwave\', \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\',\n                      \'clock\', \'vase\', \'scissors\', \'teddybear\', \'hairdrier\', \'toothbrush\']\n        vqa_question = {\n            ""train2014"": ""vqa/v2_OpenEnded_mscoco_train2014_questions.json"",\n            ""valminusminival2014"": ""vqa/v2_OpenEnded_mscoco_valminusminival2014_questions.json"",\n            ""val2014"": ""vqa/v2_OpenEnded_mscoco_val2014_questions.json"",\n            ""minival2014"": ""vqa/v2_OpenEnded_mscoco_minival2014_questions.json"",\n            ""test-dev2015"": ""vqa/v2_OpenEnded_mscoco_test-dev2015_questions.json"",\n            ""test2015"": ""vqa/v2_OpenEnded_mscoco_test2015_questions.json"",\n        }\n        vqa_annot = {\n            ""train2014"": ""vqa/v2_mscoco_train2014_annotations.json"",\n            ""valminusminival2014"": ""vqa/v2_mscoco_valminusminival2014_annotations.json"",\n            ""val2014"": ""vqa/v2_mscoco_val2014_annotations.json"",\n            ""minival2014"": ""vqa/v2_mscoco_minival2014_annotations.json"",\n        }\n        vqa_imdb = {\n            ""train2014"": ""vqa/vqa_imdb/imdb_train2014.npy"",\n            ""val2014"": ""vqa/vqa_imdb/imdb_val2014.npy"",\n            \'test2015\': ""vqa/vqa_imdb/imdb_test2015.npy"",\n            \'minival2014\': ""vqa/vqa_imdb/imdb_minival2014.npy"",\n        }\n\n        if boxes == ""36"":\n            precomputed_boxes = {\n                \'train2014\': (""vgbua_res101_precomputed"", ""trainval_resnet101_faster_rcnn_genome_36""),\n                ""valminusminival2014"": (""vgbua_res101_precomputed"", ""trainval_resnet101_faster_rcnn_genome_36""),\n                \'val2014\': (""vgbua_res101_precomputed"", ""trainval_resnet101_faster_rcnn_genome_36""),\n                ""minival2014"": (""vgbua_res101_precomputed"", ""trainval_resnet101_faster_rcnn_genome_36""),\n                ""test-dev2015"": (""vgbua_res101_precomputed"", ""test2015_resnet101_faster_rcnn_genome_36""),\n                ""test2015"": (""vgbua_res101_precomputed"", ""test2015_resnet101_faster_rcnn_genome_36""),\n            }\n        elif boxes == ""10-100ada"":\n            precomputed_boxes = {\n                \'train2014\': (""vgbua_res101_precomputed"", ""trainval2014_resnet101_faster_rcnn_genome""),\n                ""valminusminival2014"": (""vgbua_res101_precomputed"", ""trainval2014_resnet101_faster_rcnn_genome""),\n                \'val2014\': (""vgbua_res101_precomputed"", ""trainval2014_resnet101_faster_rcnn_genome""),\n                ""minival2014"": (""vgbua_res101_precomputed"", ""trainval2014_resnet101_faster_rcnn_genome""),\n                ""test-dev2015"": (""vgbua_res101_precomputed"", ""test2015_resnet101_faster_rcnn_genome""),\n                ""test2015"": (""vgbua_res101_precomputed"", ""test2015_resnet101_faster_rcnn_genome""),\n            }\n        else:\n            raise ValueError(""Not support boxes: {}!"".format(boxes))\n        coco_dataset = {\n            ""train2014"": (""train2014"", ""annotations/instances_train2014.json""),\n            ""valminusminival2014"": (""val2014"", ""annotations/instances_val2014.json""),\n            ""val2014"": (""val2014"", ""annotations/instances_val2014.json""),\n            ""minival2014"": (""val2014"", ""annotations/instances_val2014.json""),\n            ""test-dev2015"": (""test2015"", ""annotations/image_info_test2015.json""),\n            ""test2015"": (""test2015"", ""annotations/image_info_test2015.json""),\n        }\n\n        self.periodStrip = re.compile(""(?!<=\\d)(\\.)(?!\\d)"")\n        self.commaStrip = re.compile(""(\\d)(\\,)(\\d)"")\n        self.punct = [\';\', r""/"", \'[\', \']\', \'""\', \'{\', \'}\',\n                      \'(\', \')\', \'=\', \'+\', \'\\\\\', \'_\', \'-\',\n                      \'>\', \'<\', \'@\', \'`\', \',\', \'?\', \'!\']\n\n        self.use_imdb = use_imdb\n        self.boxes = boxes\n        self.test_mode = test_mode\n        self.with_precomputed_visual_feat = with_precomputed_visual_feat\n        self.category_to_idx = {c: i for i, c in enumerate(categories)}\n        self.data_path = data_path\n        self.root_path = root_path\n        with open(answer_vocab_file, \'r\', encoding=\'utf8\') as f:\n            self.answer_vocab = [w.lower().strip().strip(\'\\r\').strip(\'\\n\').strip(\'\\r\') for w in f.readlines()]\n            self.answer_vocab = list(filter(lambda x: x != \'\', self.answer_vocab))\n            if not self.use_imdb:\n                self.answer_vocab = [self.processPunctuation(w) for w in self.answer_vocab]\n        self.image_sets = [iset.strip() for iset in image_set.split(\'+\')]\n        self.ann_files = [os.path.join(data_path, vqa_annot[iset]) for iset in self.image_sets] \\\n            if not self.test_mode else [None for iset in self.image_sets]\n        self.q_files = [os.path.join(data_path, vqa_question[iset]) for iset in self.image_sets]\n        self.imdb_files = [os.path.join(data_path, vqa_imdb[iset]) for iset in self.image_sets]\n        self.precomputed_box_files = [\n            os.path.join(data_path, precomputed_boxes[iset][0],\n                         \'{0}.zip@/{0}\'.format(precomputed_boxes[iset][1])\n                         if zip_mode else precomputed_boxes[iset][1])\n            for iset in self.image_sets]\n        self.box_bank = {}\n        self.coco_datasets = [(os.path.join(data_path,\n                                            coco_dataset[iset][0],\n                                            \'COCO_{}_{{:012d}}.jpg\'.format(coco_dataset[iset][0]))\n                               if not zip_mode else\n                               os.path.join(data_path,\n                                            coco_dataset[iset][0] + \'.zip@/\' + coco_dataset[iset][0],\n                                            \'COCO_{}_{{:012d}}.jpg\'.format(coco_dataset[iset][0])),\n                               os.path.join(data_path, coco_dataset[iset][1]))\n                              for iset in self.image_sets]\n        self.transform = transform\n        self.zip_mode = zip_mode\n        self.cache_mode = cache_mode\n        self.cache_db = cache_db\n        self.ignore_db_cache = ignore_db_cache\n        self.aspect_grouping = aspect_grouping\n        self.cache_dir = os.path.join(root_path, \'cache\')\n        self.add_image_as_a_box = add_image_as_a_box\n        self.mask_size = mask_size\n        if not os.path.exists(self.cache_dir):\n            makedirsExist(self.cache_dir)\n        self.tokenizer = tokenizer if tokenizer is not None \\\n            else BertTokenizer.from_pretrained(\n            \'bert-base-uncased\' if pretrained_model_name is None else pretrained_model_name,\n            cache_dir=self.cache_dir)\n\n        if zip_mode:\n            self.zipreader = ZipReader()\n\n        self.database = self.load_annotations()\n        if self.aspect_grouping:\n            self.group_ids = self.group_aspect(self.database)\n\n    @property\n    def data_names(self):\n        if self.test_mode:\n            return [\'image\', \'boxes\', \'im_info\', \'question\']\n        else:\n            return [\'image\', \'boxes\', \'im_info\', \'question\', \'label\']\n\n    def __getitem__(self, index):\n        idb = self.database[index]\n\n        # image, boxes, im_info\n        boxes_data = self._load_json(idb[\'box_fn\'])\n        if self.with_precomputed_visual_feat:\n            image = None\n            w0, h0 = idb[\'width\'], idb[\'height\']\n\n            boxes_features = torch.as_tensor(\n                np.frombuffer(self.b64_decode(boxes_data[\'features\']), dtype=np.float32).reshape((boxes_data[\'num_boxes\'], -1))\n            )\n        else:\n            image = self._load_image(idb[\'image_fn\'])\n            w0, h0 = image.size\n        boxes = torch.as_tensor(\n            np.frombuffer(self.b64_decode(boxes_data[\'boxes\']), dtype=np.float32).reshape(\n                (boxes_data[\'num_boxes\'], -1))\n        )\n\n        if self.add_image_as_a_box:\n            image_box = torch.as_tensor([[0.0, 0.0, w0 - 1, h0 - 1]])\n            boxes = torch.cat((image_box, boxes), dim=0)\n            if self.with_precomputed_visual_feat:\n                if \'image_box_feature\' in boxes_data:\n                    image_box_feature = torch.as_tensor(\n                        np.frombuffer(\n                            self.b64_decode(boxes_data[\'image_box_feature\']), dtype=np.float32\n                        ).reshape((1, -1))\n                    )\n                else:\n                    image_box_feature = boxes_features.mean(0, keepdim=True)\n                boxes_features = torch.cat((image_box_feature, boxes_features), dim=0)\n        im_info = torch.tensor([w0, h0, 1.0, 1.0])\n        flipped = False\n        if self.transform is not None:\n            image, boxes, _, im_info, flipped = self.transform(image, boxes, None, im_info, flipped)\n\n        # clamp boxes\n        w = im_info[0].item()\n        h = im_info[1].item()\n        boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(min=0, max=w - 1)\n        boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(min=0, max=h - 1)\n\n        # flip: \'left\' -> \'right\', \'right\' -> \'left\'\n        if self.use_imdb:\n            q_tokens = idb[\'question_tokens\']\n        else:\n            q_tokens = self.tokenizer.tokenize(idb[\'question\'])\n        if flipped:\n            q_tokens = self.flip_tokens(q_tokens, verbose=False)\n        if not self.test_mode:\n            answers = idb[\'answers\']\n            if flipped:\n                answers_tokens = [a.split(\' \') for a in answers]\n                answers_tokens = [self.flip_tokens(a_toks, verbose=False) for a_toks in answers_tokens]\n                answers = [\' \'.join(a_toks) for a_toks in answers_tokens]\n            label = self.get_soft_target(answers)\n\n        # question\n        if self.use_imdb:\n            q_str = \' \'.join(q_tokens)\n            q_retokens = self.tokenizer.tokenize(q_str)\n        else:\n            q_retokens = q_tokens\n        q_ids = self.tokenizer.convert_tokens_to_ids(q_retokens)\n\n        # concat box feature to box\n        if self.with_precomputed_visual_feat:\n            boxes = torch.cat((boxes, boxes_features), dim=-1)\n\n        if self.test_mode:\n            return image, boxes, im_info, q_ids\n        else:\n            # print([(self.answer_vocab[i], p.item()) for i, p in enumerate(label) if p.item() != 0])\n            return image, boxes, im_info, q_ids, label\n\n    @staticmethod\n    def flip_tokens(tokens, verbose=True):\n        changed = False\n        tokens_new = [tok for tok in tokens]\n        for i, tok in enumerate(tokens):\n            if tok == \'left\':\n                tokens_new[i] = \'right\'\n                changed = True\n            elif tok == \'right\':\n                tokens_new[i] = \'left\'\n                changed = True\n        if verbose and changed:\n            logging.info(\'[Tokens Flip] {} -> {}\'.format(tokens, tokens_new))\n        return tokens_new\n\n    @staticmethod\n    def b64_decode(string):\n        return base64.decodebytes(string.encode())\n\n    def answer_to_ind(self, answer):\n        if answer in self.answer_vocab:\n            return self.answer_vocab.index(answer)\n        else:\n            return self.answer_vocab.index(\'<unk>\')\n\n    def get_soft_target(self, answers):\n\n        soft_target = torch.zeros(len(self.answer_vocab), dtype=torch.float)\n        answer_indices = [self.answer_to_ind(answer) for answer in answers]\n        gt_answers = list(enumerate(answer_indices))\n        unique_answers = set(answer_indices)\n\n        for answer in unique_answers:\n            accs = []\n            for gt_answer in gt_answers:\n                other_answers = [item for item in gt_answers if item != gt_answer]\n\n                matching_answers = [item for item in other_answers if item[1] == answer]\n                acc = min(1, float(len(matching_answers)) / 3)\n                accs.append(acc)\n            avg_acc = sum(accs) / len(accs)\n\n            if answer != self.answer_vocab.index(\'<unk>\'):\n                soft_target[answer] = avg_acc\n\n        return soft_target\n\n    def processPunctuation(self, inText):\n\n        if inText == \'<unk>\':\n            return inText\n\n        outText = inText\n        for p in self.punct:\n            if (p + \' \' in inText or \' \' + p in inText) or (re.search(self.commaStrip, inText) != None):\n                outText = outText.replace(p, \'\')\n            else:\n                outText = outText.replace(p, \' \')\n        outText = self.periodStrip.sub("""",\n                                       outText,\n                                       re.UNICODE)\n        return outText\n\n    def load_annotations(self):\n        tic = time.time()\n        database = []\n        if self.use_imdb:\n            db_cache_name = \'vqa2_imdb_boxes{}_{}\'.format(self.boxes, \'+\'.join(self.image_sets))\n        else:\n            db_cache_name = \'vqa2_nonimdb_boxes{}_{}\'.format(self.boxes, \'+\'.join(self.image_sets))\n        if self.with_precomputed_visual_feat:\n            db_cache_name += \'visualprecomp\'\n        if self.zip_mode:\n            db_cache_name = db_cache_name + \'_zipmode\'\n        if self.test_mode:\n            db_cache_name = db_cache_name + \'_testmode\'\n        db_cache_root = os.path.join(self.root_path, \'cache\')\n        db_cache_path = os.path.join(db_cache_root, \'{}.pkl\'.format(db_cache_name))\n\n        if os.path.exists(db_cache_path):\n            if not self.ignore_db_cache:\n                # reading cached database\n                print(\'cached database found in {}.\'.format(db_cache_path))\n                with open(db_cache_path, \'rb\') as f:\n                    print(\'loading cached database from {}...\'.format(db_cache_path))\n                    tic = time.time()\n                    database = cPickle.load(f)\n                    print(\'Done (t={:.2f}s)\'.format(time.time() - tic))\n                    return database\n            else:\n                print(\'cached database ignored.\')\n\n        # ignore or not find cached database, reload it from annotation file\n        print(\'loading database of split {}...\'.format(\'+\'.join(self.image_sets)))\n        tic = time.time()\n\n        if self.use_imdb:\n            for imdb_file, (coco_path, coco_annot), box_file \\\n                    in zip(self.imdb_files, self.coco_datasets, self.precomputed_box_files):\n                print(""loading imdb: {}"".format(imdb_file))\n                imdb = np.load(imdb_file, allow_pickle=True)\n                print(""imdb info:"")\n                pprint.pprint(imdb[0])\n\n                coco = COCO(coco_annot)\n                for item in imdb[1:]:\n                    idb = {\'image_id\': item[\'image_id\'],\n                           \'image_fn\': coco_path.format(item[\'image_id\']),\n                           \'width\': coco.imgs[item[\'image_id\']][\'width\'],\n                           \'height\': coco.imgs[item[\'image_id\']][\'height\'],\n                           \'box_fn\': os.path.join(box_file, \'{}.json\'.format(item[\'image_id\'])),\n                           \'question_id\': item[\'question_id\'],\n                           \'question_tokens\': item[\'question_tokens\'],\n                           \'answers\': item[\'answers\'] if not self.test_mode else None,\n                           }\n                    database.append(idb)\n        else:\n            for ann_file, q_file, (coco_path, coco_annot), box_file \\\n                    in zip(self.ann_files, self.q_files, self.coco_datasets, self.precomputed_box_files):\n                qs = self._load_json(q_file)[\'questions\']\n                anns = self._load_json(ann_file)[\'annotations\'] if not self.test_mode else ([None] * len(qs))\n                coco = COCO(coco_annot)\n                for ann, q in zip(anns, qs):\n                    idb = {\'image_id\': q[\'image_id\'],\n                           \'image_fn\': coco_path.format(q[\'image_id\']),\n                           \'width\': coco.imgs[q[\'image_id\']][\'width\'],\n                           \'height\': coco.imgs[q[\'image_id\']][\'height\'],\n                           \'box_fn\': os.path.join(box_file, \'{}.json\'.format(q[\'image_id\'])),\n                           \'question_id\': q[\'question_id\'],\n                           \'question\': q[\'question\'],\n                           \'answers\': [a[\'answer\'] for a in ann[\'answers\']] if not self.test_mode else None,\n                           \'multiple_choice_answer\': ann[\'multiple_choice_answer\'] if not self.test_mode else None,\n                           ""question_type"": ann[\'question_type\'] if not self.test_mode else None,\n                           ""answer_type"": ann[\'answer_type\'] if not self.test_mode else None,\n                           }\n                    database.append(idb)\n\n        print(\'Done (t={:.2f}s)\'.format(time.time() - tic))\n\n        # cache database via cPickle\n        if self.cache_db:\n            print(\'caching database to {}...\'.format(db_cache_path))\n            tic = time.time()\n            if not os.path.exists(db_cache_root):\n                makedirsExist(db_cache_root)\n            with open(db_cache_path, \'wb\') as f:\n                cPickle.dump(database, f)\n            print(\'Done (t={:.2f}s)\'.format(time.time() - tic))\n\n        return database\n\n    @staticmethod\n    def group_aspect(database):\n        print(\'grouping aspect...\')\n        t = time.time()\n\n        # get shape of all images\n        widths = torch.as_tensor([idb[\'width\'] for idb in database])\n        heights = torch.as_tensor([idb[\'height\'] for idb in database])\n\n        # group\n        group_ids = torch.zeros(len(database))\n        horz = widths >= heights\n        vert = 1 - horz\n        group_ids[horz] = 0\n        group_ids[vert] = 1\n\n        print(\'Done (t={:.2f}s)\'.format(time.time() - t))\n\n        return group_ids\n\n    def load_precomputed_boxes(self, box_file):\n        if box_file in self.box_bank:\n            return self.box_bank[box_file]\n        else:\n            in_data = {}\n            with open(box_file, ""r"") as tsv_in_file:\n                reader = csv.DictReader(tsv_in_file, delimiter=\'\\t\', fieldnames=FIELDNAMES)\n                for item in reader:\n                    item[\'image_id\'] = int(item[\'image_id\'])\n                    item[\'image_h\'] = int(item[\'image_h\'])\n                    item[\'image_w\'] = int(item[\'image_w\'])\n                    item[\'num_boxes\'] = int(item[\'num_boxes\'])\n                    for field in ([\'boxes\', \'features\'] if self.with_precomputed_visual_feat else [\'boxes\']):\n                        item[field] = np.frombuffer(base64.decodebytes(item[field].encode()),\n                                                    dtype=np.float32).reshape((item[\'num_boxes\'], -1))\n                    in_data[item[\'image_id\']] = item\n            self.box_bank[box_file] = in_data\n            return in_data\n\n    def __len__(self):\n        return len(self.database)\n\n    def _load_image(self, path):\n        if \'.zip@\' in path:\n            return self.zipreader.imread(path).convert(\'RGB\')\n        else:\n            return Image.open(path).convert(\'RGB\')\n\n    def _load_json(self, path):\n        if \'.zip@\' in path:\n            f = self.zipreader.read(path)\n            return json.loads(f.decode())\n        else:\n            with open(path, \'r\') as f:\n                return json.load(f)\n\n'"
vqa/data/samplers/distributed.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# Code is copy-pasted exactly as in torch.utils.data.distributed.\n# FIXME remove this once c10d fixes the bug it has\nimport math\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data.sampler import Sampler\n\n\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset : offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch'"
vqa/data/samplers/grouped_batch_sampler.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport itertools\n\nimport torch\nfrom torch.utils.data.sampler import BatchSampler\nfrom torch.utils.data.sampler import Sampler\n\n\nclass GroupedBatchSampler(BatchSampler):\n    """"""\n    Wraps another sampler to yield a mini-batch of indices.\n    It enforces that elements from the same group should appear in groups of batch_size.\n    It also tries to provide mini-batches which follows an ordering which is\n    as close as possible to the ordering from the original sampler.\n    Arguments:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_uneven (bool): If ``True``, the sampler will drop the batches whose\n            size is less than ``batch_size``\n    """"""\n\n    def __init__(self, sampler, group_ids, batch_size, drop_uneven=False):\n        if not isinstance(sampler, Sampler):\n            raise ValueError(\n                ""sampler should be an instance of ""\n                ""torch.utils.data.Sampler, but got sampler={}"".format(sampler)\n            )\n        self.sampler = sampler\n        self.group_ids = torch.as_tensor(group_ids)\n        assert self.group_ids.dim() == 1\n        self.batch_size = batch_size\n        self.drop_uneven = drop_uneven\n\n        self.groups = torch.unique(self.group_ids).sort(0)[0]\n\n        self._can_reuse_batches = False\n\n    def _prepare_batches(self):\n        dataset_size = len(self.group_ids)\n        # get the sampled indices from the sampler\n        sampled_ids = torch.as_tensor(list(self.sampler))\n        # potentially not all elements of the dataset were sampled\n        # by the sampler (e.g., DistributedSampler).\n        # construct a tensor which contains -1 if the element was\n        # not sampled, and a non-negative number indicating the\n        # order where the element was sampled.\n        # for example. if sampled_ids = [3, 1] and dataset_size = 5,\n        # the order is [-1, 1, -1, 0, -1]\n        order = torch.full((dataset_size,), -1, dtype=torch.int64)\n        order[sampled_ids] = torch.arange(len(sampled_ids))\n\n        # get a mask with the elements that were sampled\n        mask = order >= 0\n\n        # find the elements that belong to each individual cluster\n        clusters = [(self.group_ids == i) & mask for i in self.groups]\n        # get relative order of the elements inside each cluster\n        # that follows the order from the sampler\n        relative_order = [order[cluster] for cluster in clusters]\n        # with the relative order, find the absolute order in the\n        # sampled space\n        permutation_ids = [s[s.sort()[1]] for s in relative_order]\n        # permute each cluster so that they follow the order from\n        # the sampler\n        permuted_clusters = [sampled_ids[idx] for idx in permutation_ids]\n\n        # splits each cluster in batch_size, and merge as a list of tensors\n        splits = [c.split(self.batch_size) for c in permuted_clusters]\n        merged = tuple(itertools.chain.from_iterable(splits))\n\n        # now each batch internally has the right order, but\n        # they are grouped by clusters. Find the permutation between\n        # different batches that brings them as close as possible to\n        # the order that we have in the sampler. For that, we will consider the\n        # ordering as coming from the first element of each batch, and sort\n        # correspondingly\n        first_element_of_batch = [t[0].item() for t in merged]\n        # get and inverse mapping from sampled indices and the position where\n        # they occur (as returned by the sampler)\n        inv_sampled_ids_map = {v: k for k, v in enumerate(sampled_ids.tolist())}\n        # from the first element in each batch, get a relative ordering\n        first_index_of_batch = torch.as_tensor(\n            [inv_sampled_ids_map[s] for s in first_element_of_batch]\n        )\n\n        # permute the batches so that they approximately follow the order\n        # from the sampler\n        permutation_order = first_index_of_batch.sort(0)[1].tolist()\n        # finally, permute the batches\n        batches = [merged[i].tolist() for i in permutation_order]\n\n        if self.drop_uneven:\n            kept = []\n            for batch in batches:\n                if len(batch) == self.batch_size:\n                    kept.append(batch)\n            batches = kept\n        return batches\n\n    def __iter__(self):\n        if self._can_reuse_batches:\n            batches = self._batches\n            self._can_reuse_batches = False\n        else:\n            batches = self._prepare_batches()\n        self._batches = batches\n        return iter(batches)\n\n    def __len__(self):\n        if not hasattr(self, ""_batches""):\n            self._batches = self._prepare_batches()\n            self._can_reuse_batches = True\n        return len(self._batches)\n\n\n\n'"
vqa/data/transforms/transforms.py,1,"b'import random\n\nimport numpy as np\n\nimport torch\nimport torchvision\nfrom torchvision.transforms import functional as F\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, boxes, masks, im_info, flipped):\n        for t in self.transforms:\n            image, boxes, masks, im_info, flipped = t(image, boxes, masks, im_info, flipped)\n        return image, boxes, masks, im_info, flipped\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + ""(""\n        for t in self.transforms:\n            format_string += ""\\n""\n            format_string += ""    {0}"".format(t)\n        format_string += ""\\n)""\n        return format_string\n\n\nclass Resize(object):\n    def __init__(self, min_size, max_size):\n        self.min_size = min_size\n        self.max_size = max_size\n\n    # modified from torchvision to add support for max size\n    def get_size(self, image_size):\n        w, h = image_size\n        size = self.min_size\n        max_size = self.max_size\n        if max_size is not None:\n            min_original_size = float(min((w, h)))\n            max_original_size = float(max((w, h)))\n            if max_original_size / min_original_size * size > max_size:\n                size = int(max_size * min_original_size / max_original_size)\n\n        if (w <= h and w == size) or (h <= w and h == size):\n            return (w, h)\n\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n        else:\n            oh = size\n            ow = int(size * w / h)\n\n        return (ow, oh)\n\n    def __call__(self, image, boxes, masks, im_info, flipped):\n        origin_size = im_info[:2]\n        size = self.get_size(origin_size)\n        if image is not None:\n            image = F.resize(image, (size[1], size[0]))\n\n        ratios = [size[0] * 1.0 / origin_size[0], size[1] * 1.0 / origin_size[1]]\n        if boxes is not None:\n            boxes[:, [0, 2]] *= ratios[0]\n            boxes[:, [1, 3]] *= ratios[1]\n        im_info[0], im_info[1] = size\n        im_info[2], im_info[3] = ratios\n        return image, boxes, masks, im_info, flipped\n\n\nclass RandomHorizontalFlip(object):\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, image, boxes, masks, im_info, flipped):\n        if random.random() < self.prob:\n            w, h = im_info[:2]\n            if image is not None:\n                image = F.hflip(image)\n            if boxes is not None:\n                boxes[:, [0, 2]] = w - 1 - boxes[:, [2, 0]]\n            if masks is not None:\n                masks = torch.as_tensor(masks.numpy()[:, :, ::-1].tolist())\n            flipped = not flipped\n        return image, boxes, masks, im_info, flipped\n\n\nclass ToTensor(object):\n    def __call__(self, image, boxes, masks, im_info, flipped):\n        return F.to_tensor(image) if image is not None else image, boxes, masks, im_info, flipped\n\n\nclass Normalize(object):\n    def __init__(self, mean, std, to_bgr255=True):\n        self.mean = mean\n        self.std = std\n        self.to_bgr255 = to_bgr255\n\n    def __call__(self, image, boxes, masks, im_info, flipped):\n        if image is not None:\n            if self.to_bgr255:\n                image = image[[2, 1, 0]] * 255\n            image = F.normalize(image, mean=self.mean, std=self.std)\n        return image, boxes, masks, im_info, flipped\n\n\nclass FixPadding(object):\n    def __init__(self, min_size, max_size, pad=0):\n        self.min_size = min_size\n        self.max_size = max_size\n        self.pad = pad\n\n    def __call__(self, image, boxes, masks, im_info, flipped):\n\n        if image is not None:\n            # padding to fixed size for determinacy\n            c, h, w = image.shape\n            if h <= w:\n                h1 = self.min_size\n                w1 = self.max_size\n            else:\n                h1 = self.max_size\n                w1 = self.min_size\n            padded_image = image.new_zeros((c, h1, w1)).fill_(self.pad)\n            padded_image[:, :h, :w] = image\n            image = padded_image\n\n        return image, boxes, masks, im_info, flipped\n'"
