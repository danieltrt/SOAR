file_path,api_count,code
dataset.py,5,"b'import numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport glob\nimport os\n\ndef loader(path, batch_size=32, num_workers=4, pin_memory=True):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    return data.DataLoader(\n        datasets.ImageFolder(path,\n                             transforms.Compose([\n                                 transforms.Scale(256),\n                                 transforms.RandomSizedCrop(224),\n                                 transforms.RandomHorizontalFlip(),\n                                 transforms.ToTensor(),\n                                 normalize,\n                             ])),\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory)\n\ndef test_loader(path, batch_size=32, num_workers=4, pin_memory=True):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    return data.DataLoader(\n        datasets.ImageFolder(path,\n                             transforms.Compose([\n                                 transforms.Scale(256),\n                                 transforms.CenterCrop(224),\n                                 transforms.ToTensor(),\n                                 normalize,\n                             ])),\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory)'"
finetune.py,15,"b'import torch\nfrom torch.autograd import Variable\nfrom torchvision import models\nimport cv2\nimport sys\nimport numpy as np\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport dataset\nfrom prune import *\nimport argparse\nfrom operator import itemgetter\nfrom heapq import nsmallest\nimport time\n\nclass ModifiedVGG16Model(torch.nn.Module):\n    def __init__(self):\n        super(ModifiedVGG16Model, self).__init__()\n\n        model = models.vgg16(pretrained=True)\n        self.features = model.features\n\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(25088, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, 2))\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\nclass FilterPrunner:\n    def __init__(self, model):\n        self.model = model\n        self.reset()\n    \n    def reset(self):\n        self.filter_ranks = {}\n\n    def forward(self, x):\n        self.activations = []\n        self.gradients = []\n        self.grad_index = 0\n        self.activation_to_layer = {}\n\n        activation_index = 0\n        for layer, (name, module) in enumerate(self.model.features._modules.items()):\n            x = module(x)\n            if isinstance(module, torch.nn.modules.conv.Conv2d):\n                x.register_hook(self.compute_rank)\n                self.activations.append(x)\n                self.activation_to_layer[activation_index] = layer\n                activation_index += 1\n\n        return self.model.classifier(x.view(x.size(0), -1))\n\n    def compute_rank(self, grad):\n        activation_index = len(self.activations) - self.grad_index - 1\n        activation = self.activations[activation_index]\n\n        taylor = activation * grad\n        # Get the average value for every filter, \n        # accross all the other dimensions\n        taylor = taylor.mean(dim=(0, 2, 3)).data\n\n\n        if activation_index not in self.filter_ranks:\n            self.filter_ranks[activation_index] = \\\n                torch.FloatTensor(activation.size(1)).zero_()\n\n            if args.use_cuda:\n                self.filter_ranks[activation_index] = self.filter_ranks[activation_index].cuda()\n\n        self.filter_ranks[activation_index] += taylor\n        self.grad_index += 1\n\n    def lowest_ranking_filters(self, num):\n        data = []\n        for i in sorted(self.filter_ranks.keys()):\n            for j in range(self.filter_ranks[i].size(0)):\n                data.append((self.activation_to_layer[i], j, self.filter_ranks[i][j]))\n\n        return nsmallest(num, data, itemgetter(2))\n\n    def normalize_ranks_per_layer(self):\n        for i in self.filter_ranks:\n            v = torch.abs(self.filter_ranks[i])\n            v = v / np.sqrt(torch.sum(v * v))\n            self.filter_ranks[i] = v.cpu()\n\n    def get_prunning_plan(self, num_filters_to_prune):\n        filters_to_prune = self.lowest_ranking_filters(num_filters_to_prune)\n\n        # After each of the k filters are prunned,\n        # the filter index of the next filters change since the model is smaller.\n        filters_to_prune_per_layer = {}\n        for (l, f, _) in filters_to_prune:\n            if l not in filters_to_prune_per_layer:\n                filters_to_prune_per_layer[l] = []\n            filters_to_prune_per_layer[l].append(f)\n\n        for l in filters_to_prune_per_layer:\n            filters_to_prune_per_layer[l] = sorted(filters_to_prune_per_layer[l])\n            for i in range(len(filters_to_prune_per_layer[l])):\n                filters_to_prune_per_layer[l][i] = filters_to_prune_per_layer[l][i] - i\n\n        filters_to_prune = []\n        for l in filters_to_prune_per_layer:\n            for i in filters_to_prune_per_layer[l]:\n                filters_to_prune.append((l, i))\n\n        return filters_to_prune             \n\nclass PrunningFineTuner_VGG16:\n    def __init__(self, train_path, test_path, model):\n        self.train_data_loader = dataset.loader(train_path)\n        self.test_data_loader = dataset.test_loader(test_path)\n\n        self.model = model\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.prunner = FilterPrunner(self.model) \n        self.model.train()\n\n    def test(self):\n        return\n        self.model.eval()\n        correct = 0\n        total = 0\n\n        for i, (batch, label) in enumerate(self.test_data_loader):\n            if args.use_cuda:\n                batch = batch.cuda()\n            output = model(Variable(batch))\n            pred = output.data.max(1)[1]\n            correct += pred.cpu().eq(label).sum()\n            total += label.size(0)\n        \n        print(""Accuracy :"", float(correct) / total)\n        \n        self.model.train()\n\n    def train(self, optimizer = None, epoches=10):\n        if optimizer is None:\n            optimizer = optim.SGD(model.classifier.parameters(), lr=0.0001, momentum=0.9)\n\n        for i in range(epoches):\n            print(""Epoch: "", i)\n            self.train_epoch(optimizer)\n            self.test()\n        print(""Finished fine tuning."")\n        \n\n    def train_batch(self, optimizer, batch, label, rank_filters):\n\n        if args.use_cuda:\n            batch = batch.cuda()\n            label = label.cuda()\n\n        self.model.zero_grad()\n        input = Variable(batch)\n\n        if rank_filters:\n            output = self.prunner.forward(input)\n            self.criterion(output, Variable(label)).backward()\n        else:\n            self.criterion(self.model(input), Variable(label)).backward()\n            optimizer.step()\n\n    def train_epoch(self, optimizer = None, rank_filters = False):\n        for i, (batch, label) in enumerate(self.train_data_loader):\n            self.train_batch(optimizer, batch, label, rank_filters)\n\n    def get_candidates_to_prune(self, num_filters_to_prune):\n        self.prunner.reset()\n        self.train_epoch(rank_filters = True)\n        self.prunner.normalize_ranks_per_layer()\n        return self.prunner.get_prunning_plan(num_filters_to_prune)\n        \n    def total_num_filters(self):\n        filters = 0\n        for name, module in self.model.features._modules.items():\n            if isinstance(module, torch.nn.modules.conv.Conv2d):\n                filters = filters + module.out_channels\n        return filters\n\n    def prune(self):\n        #Get the accuracy before prunning\n        self.test()\n        self.model.train()\n\n        #Make sure all the layers are trainable\n        for param in self.model.features.parameters():\n            param.requires_grad = True\n\n        number_of_filters = self.total_num_filters()\n        num_filters_to_prune_per_iteration = 512\n        iterations = int(float(number_of_filters) / num_filters_to_prune_per_iteration)\n\n        iterations = int(iterations * 2.0 / 3)\n\n        print(""Number of prunning iterations to reduce 67% filters"", iterations)\n\n        for _ in range(iterations):\n            print(""Ranking filters.. "")\n            prune_targets = self.get_candidates_to_prune(num_filters_to_prune_per_iteration)\n            layers_prunned = {}\n            for layer_index, filter_index in prune_targets:\n                if layer_index not in layers_prunned:\n                    layers_prunned[layer_index] = 0\n                layers_prunned[layer_index] = layers_prunned[layer_index] + 1 \n\n            print(""Layers that will be prunned"", layers_prunned)\n            print(""Prunning filters.. "")\n            model = self.model.cpu()\n            for layer_index, filter_index in prune_targets:\n                model = prune_vgg16_conv_layer(model, layer_index, filter_index, use_cuda=args.use_cuda)\n\n            self.model = model\n            if args.use_cuda:\n                self.model = self.model.cuda()\n\n            message = str(100*float(self.total_num_filters()) / number_of_filters) + ""%""\n            print(""Filters prunned"", str(message))\n            self.test()\n            print(""Fine tuning to recover from prunning iteration."")\n            optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n            self.train(optimizer, epoches = 10)\n\n\n        print(""Finished. Going to fine tune the model a bit more"")\n        self.train(optimizer, epoches=15)\n        torch.save(model.state_dict(), ""model_prunned"")\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--train"", dest=""train"", action=""store_true"")\n    parser.add_argument(""--prune"", dest=""prune"", action=""store_true"")\n    parser.add_argument(""--train_path"", type = str, default = ""train"")\n    parser.add_argument(""--test_path"", type = str, default = ""test"")\n    parser.add_argument(\'--use-cuda\', action=\'store_true\', default=False, help=\'Use NVIDIA GPU acceleration\')    \n    parser.set_defaults(train=False)\n    parser.set_defaults(prune=False)\n    args = parser.parse_args()\n    args.use_cuda = args.use_cuda and torch.cuda.is_available()\n\n    return args\n\nif __name__ == \'__main__\':\n    args = get_args()\n\n    if args.train:\n        model = ModifiedVGG16Model()\n    elif args.prune:\n        model = torch.load(""model"", map_location=lambda storage, loc: storage)\n\n    if args.use_cuda:\n        model = model.cuda()\n\n    fine_tuner = PrunningFineTuner_VGG16(args.train_path, args.test_path, model)\n\n    if args.train:\n        fine_tuner.train(epoches=10)\n        torch.save(model, ""model"")\n\n    elif args.prune:\n        fine_tuner.prune()\n'"
prune.py,13,"b'import torch\nfrom torch.autograd import Variable\nfrom torchvision import models\nimport cv2\nimport sys\nimport numpy as np\n \ndef replace_layers(model, i, indexes, layers):\n    if i in indexes:\n        return layers[indexes.index(i)]\n    return model[i]\n\ndef prune_vgg16_conv_layer(model, layer_index, filter_index, use_cuda=False):\n    _, conv = list(model.features._modules.items())[layer_index]\n    next_conv = None\n    offset = 1\n\n    while layer_index + offset <  len(model.features._modules.items()):\n        res =  list(model.features._modules.items())[layer_index+offset]\n        if isinstance(res[1], torch.nn.modules.conv.Conv2d):\n            next_name, next_conv = res\n            break\n        offset = offset + 1\n    \n    new_conv = \\\n        torch.nn.Conv2d(in_channels = conv.in_channels, \\\n            out_channels = conv.out_channels - 1,\n            kernel_size = conv.kernel_size, \\\n            stride = conv.stride,\n            padding = conv.padding,\n            dilation = conv.dilation,\n            groups = conv.groups,\n            bias = (conv.bias is not None))\n\n    old_weights = conv.weight.data.cpu().numpy()\n    new_weights = new_conv.weight.data.cpu().numpy()\n\n    new_weights[: filter_index, :, :, :] = old_weights[: filter_index, :, :, :]\n    new_weights[filter_index : , :, :, :] = old_weights[filter_index + 1 :, :, :, :]\n    new_conv.weight.data = torch.from_numpy(new_weights)\n    if use_cuda:\n        new_conv.weight.data = new_conv.weight.data.cuda()\n\n    bias_numpy = conv.bias.data.cpu().numpy()\n\n    bias = np.zeros(shape = (bias_numpy.shape[0] - 1), dtype = np.float32)\n    bias[:filter_index] = bias_numpy[:filter_index]\n    bias[filter_index : ] = bias_numpy[filter_index + 1 :]\n    new_conv.bias.data = torch.from_numpy(bias)\n    if use_cuda:\n        new_conv.bias.data = new_conv.bias.data.cuda()\n\n    if not next_conv is None:\n        next_new_conv = \\\n            torch.nn.Conv2d(in_channels = next_conv.in_channels - 1,\\\n                out_channels =  next_conv.out_channels, \\\n                kernel_size = next_conv.kernel_size, \\\n                stride = next_conv.stride,\n                padding = next_conv.padding,\n                dilation = next_conv.dilation,\n                groups = next_conv.groups,\n                bias = (next_conv.bias is not None))\n\n        old_weights = next_conv.weight.data.cpu().numpy()\n        new_weights = next_new_conv.weight.data.cpu().numpy()\n\n        new_weights[:, : filter_index, :, :] = old_weights[:, : filter_index, :, :]\n        new_weights[:, filter_index : , :, :] = old_weights[:, filter_index + 1 :, :, :]\n        next_new_conv.weight.data = torch.from_numpy(new_weights)\n        if use_cuda:\n            next_new_conv.weight.data = next_new_conv.weight.data.cuda()\n\n        next_new_conv.bias.data = next_conv.bias.data\n\n    if not next_conv is None:\n        features = torch.nn.Sequential(\n                *(replace_layers(model.features, i, [layer_index, layer_index+offset], \\\n                    [new_conv, next_new_conv]) for i, _ in enumerate(model.features)))\n        del model.features\n        del conv\n\n        model.features = features\n\n    else:\n        #Prunning the last conv layer. This affects the first linear layer of the classifier.\n        model.features = torch.nn.Sequential(\n                *(replace_layers(model.features, i, [layer_index], \\\n                    [new_conv]) for i, _ in enumerate(model.features)))\n        layer_index = 0\n        old_linear_layer = None\n        for _, module in model.classifier._modules.items():\n            if isinstance(module, torch.nn.Linear):\n                old_linear_layer = module\n                break\n            layer_index = layer_index  + 1\n\n        if old_linear_layer is None:\n            raise BaseException(""No linear laye found in classifier"")\n        params_per_input_channel = old_linear_layer.in_features // conv.out_channels\n\n        new_linear_layer = \\\n            torch.nn.Linear(old_linear_layer.in_features - params_per_input_channel, \n                old_linear_layer.out_features)\n        \n        old_weights = old_linear_layer.weight.data.cpu().numpy()\n        new_weights = new_linear_layer.weight.data.cpu().numpy()        \n\n        new_weights[:, : filter_index * params_per_input_channel] = \\\n            old_weights[:, : filter_index * params_per_input_channel]\n        new_weights[:, filter_index * params_per_input_channel :] = \\\n            old_weights[:, (filter_index + 1) * params_per_input_channel :]\n        \n        new_linear_layer.bias.data = old_linear_layer.bias.data\n\n        new_linear_layer.weight.data = torch.from_numpy(new_weights)\n        if use_cuda:\n            new_linear_layer.weight.data = new_linear_layer.weight.data.cuda()\n\n        classifier = torch.nn.Sequential(\n            *(replace_layers(model.classifier, i, [layer_index], \\\n                [new_linear_layer]) for i, _ in enumerate(model.classifier)))\n\n        del model.classifier\n        del next_conv\n        del conv\n        model.classifier = classifier\n\n    return model\n\nif __name__ == \'__main__\':\n    model = models.vgg16(pretrained=True)\n    model.train()\n\n    t0 = time.time()\n    model = prune_conv_layer(model, 28, 10)\n    print(""The prunning took"", time.time() - t0)'"
