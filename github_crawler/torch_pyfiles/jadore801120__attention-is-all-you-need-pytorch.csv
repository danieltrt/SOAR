file_path,api_count,code
apply_bpe.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\n""""""Use operations learned with learn_bpe.py to encode a new text.\nThe text will not be smaller, but use only a fixed vocabulary, with rare words\nencoded as variable-length sequences of subword units.\n\nReference:\nRico Sennrich, Barry Haddow and Alexandra Birch (2015). Neural Machine Translation of Rare Words with Subword Units.\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n""""""\n\nfrom __future__ import unicode_literals, division\n\nimport sys\nimport os\nimport inspect\nimport codecs\nimport io\nimport re\nimport warnings\nimport random\n\n\nclass BPE(object):\n\n    def __init__(self, codes, merges=-1, separator=\'@@\', vocab=None, glossaries=None):\n\n        codes.seek(0)\n        offset=1\n\n        # check version information\n        firstline = codes.readline()\n        if firstline.startswith(\'#version:\'):\n            self.version = tuple([int(x) for x in re.sub(r\'(\\.0+)*$\',\'\', firstline.split()[-1]).split(""."")])\n            offset += 1\n        else:\n            self.version = (0, 1)\n            codes.seek(0)\n\n        self.bpe_codes = [tuple(item.strip(\'\\r\\n \').split(\' \')) for (n, item) in enumerate(codes) if (n < merges or merges == -1)]\n\n        for i, item in enumerate(self.bpe_codes):\n            if len(item) != 2:\n                sys.stderr.write(\'Error: invalid line {0} in BPE codes file: {1}\\n\'.format(i+offset, \' \'.join(item)))\n                sys.stderr.write(\'The line should exist of exactly two subword units, separated by whitespace\\n\')\n                sys.exit(1)\n\n        # some hacking to deal with duplicates (only consider first instance)\n        self.bpe_codes = dict([(code,i) for (i,code) in reversed(list(enumerate(self.bpe_codes)))])\n\n        self.bpe_codes_reverse = dict([(pair[0] + pair[1], pair) for pair,i in self.bpe_codes.items()])\n\n        self.separator = separator\n\n        self.vocab = vocab\n\n        self.glossaries = glossaries if glossaries else []\n\n        self.glossaries_regex = re.compile(\'^({})$\'.format(\'|\'.join(glossaries))) if glossaries else None\n\n        self.cache = {}\n\n    def process_line(self, line, dropout=0):\n        """"""segment line, dealing with leading and trailing whitespace""""""\n\n        out = """"\n\n        leading_whitespace = len(line)-len(line.lstrip(\'\\r\\n \'))\n        if leading_whitespace:\n            out += line[:leading_whitespace]\n\n        out += self.segment(line, dropout)\n\n        trailing_whitespace = len(line)-len(line.rstrip(\'\\r\\n \'))\n        if trailing_whitespace and trailing_whitespace != len(line):\n            out += line[-trailing_whitespace:]\n\n        return out\n\n    def segment(self, sentence, dropout=0):\n        """"""segment single sentence (whitespace-tokenized string) with BPE encoding""""""\n        segments = self.segment_tokens(sentence.strip(\'\\r\\n \').split(\' \'), dropout)\n        return \' \'.join(segments)\n\n    def segment_tokens(self, tokens, dropout=0):\n        """"""segment a sequence of tokens with BPE encoding""""""\n        output = []\n        for word in tokens:\n            # eliminate double spaces\n            if not word:\n                continue\n            new_word = [out for segment in self._isolate_glossaries(word)\n                        for out in encode(segment,\n                                          self.bpe_codes,\n                                          self.bpe_codes_reverse,\n                                          self.vocab,\n                                          self.separator,\n                                          self.version,\n                                          self.cache,\n                                          self.glossaries_regex,\n                                          dropout)]\n\n            for item in new_word[:-1]:\n                output.append(item + self.separator)\n            output.append(new_word[-1])\n\n        return output\n\n    def _isolate_glossaries(self, word):\n        word_segments = [word]\n        for gloss in self.glossaries:\n            word_segments = [out_segments for segment in word_segments\n                                 for out_segments in isolate_glossary(segment, gloss)]\n        return word_segments\n\ndef encode(orig, bpe_codes, bpe_codes_reverse, vocab, separator, version, cache, glossaries_regex=None, dropout=0):\n    """"""Encode word based on list of BPE merge operations, which are applied consecutively\n    """"""\n\n    if not dropout and orig in cache:\n        return cache[orig]\n\n    if glossaries_regex and glossaries_regex.match(orig):\n        cache[orig] = (orig,)\n        return (orig,)\n\n    if len(orig) == 1:\n        return orig\n\n    if version == (0, 1):\n        word = list(orig) + [\'</w>\']\n    elif version == (0, 2): # more consistent handling of word-final segments\n        word = list(orig[:-1]) + [orig[-1] + \'</w>\']\n    else:\n        raise NotImplementedError\n\n    while len(word) > 1:\n\n        # get list of symbol pairs; optionally apply dropout\n        pairs = [(bpe_codes[pair],i,pair) for (i,pair) in enumerate(zip(word, word[1:])) if (not dropout or random.random() > dropout) and pair in bpe_codes]\n\n        if not pairs:\n            break\n\n        #get first merge operation in list of BPE codes\n        bigram = min(pairs)[2]\n\n        # find start position of all pairs that we want to merge\n        positions = [i for (rank,i,pair) in pairs if pair == bigram]\n\n        i = 0\n        new_word = []\n        bigram = \'\'.join(bigram)\n        for j in positions:\n            # merges are invalid if they start before current position. This can happen if there are overlapping pairs: (x x x -> xx x)\n            if j < i:\n                continue\n            new_word.extend(word[i:j]) # all symbols before merged pair\n            new_word.append(bigram) # merged pair\n            i = j+2 # continue after merged pair\n        new_word.extend(word[i:]) # add all symbols until end of word\n        word = new_word\n\n    # don\'t print end-of-word symbols\n    if word[-1] == \'</w>\':\n        word = word[:-1]\n    elif word[-1].endswith(\'</w>\'):\n        word[-1] = word[-1][:-4]\n\n    word = tuple(word)\n    if vocab:\n        word = check_vocab_and_split(word, bpe_codes_reverse, vocab, separator)\n\n    cache[orig] = word\n    return word\n\ndef recursive_split(segment, bpe_codes, vocab, separator, final=False):\n    """"""Recursively split segment into smaller units (by reversing BPE merges)\n    until all units are either in-vocabulary, or cannot be split futher.""""""\n\n    try:\n        if final:\n            left, right = bpe_codes[segment + \'</w>\']\n            right = right[:-4]\n        else:\n            left, right = bpe_codes[segment]\n    except:\n        #sys.stderr.write(\'cannot split {0} further.\\n\'.format(segment))\n        yield segment\n        return\n\n    if left + separator in vocab:\n        yield left\n    else:\n        for item in recursive_split(left, bpe_codes, vocab, separator, False):\n            yield item\n\n    if (final and right in vocab) or (not final and right + separator in vocab):\n        yield right\n    else:\n        for item in recursive_split(right, bpe_codes, vocab, separator, final):\n            yield item\n\ndef check_vocab_and_split(orig, bpe_codes, vocab, separator):\n    """"""Check for each segment in word if it is in-vocabulary,\n    and segment OOV segments into smaller units by reversing the BPE merge operations""""""\n\n    out = []\n\n    for segment in orig[:-1]:\n        if segment + separator in vocab:\n            out.append(segment)\n        else:\n            #sys.stderr.write(\'OOV: {0}\\n\'.format(segment))\n            for item in recursive_split(segment, bpe_codes, vocab, separator, False):\n                out.append(item)\n\n    segment = orig[-1]\n    if segment in vocab:\n        out.append(segment)\n    else:\n        #sys.stderr.write(\'OOV: {0}\\n\'.format(segment))\n        for item in recursive_split(segment, bpe_codes, vocab, separator, True):\n            out.append(item)\n\n    return out\n\n\ndef read_vocabulary(vocab_file, threshold):\n    """"""read vocabulary file produced by get_vocab.py, and filter according to frequency threshold.\n    """"""\n\n    vocabulary = set()\n\n    for line in vocab_file:\n        word, freq = line.strip(\'\\r\\n \').split(\' \')\n        freq = int(freq)\n        if threshold == None or freq >= threshold:\n            vocabulary.add(word)\n\n    return vocabulary\n\ndef isolate_glossary(word, glossary):\n    """"""\n    Isolate a glossary present inside a word.\n\n    Returns a list of subwords. In which all \'glossary\' glossaries are isolated \n\n    For example, if \'USA\' is the glossary and \'1934USABUSA\' the word, the return value is:\n        [\'1934\', \'USA\', \'B\', \'USA\']\n    """"""\n    # regex equivalent of (if word == glossary or glossary not in word)\n    if re.match(\'^\'+glossary+\'$\', word) or not re.search(glossary, word):\n        return [word]\n    else:\n        segments = re.split(r\'({})\'.format(glossary), word)\n        segments, ending = segments[:-1], segments[-1]\n        segments = list(filter(None, segments)) # Remove empty strings in regex group.\n        return segments + [ending.strip(\'\\r\\n \')] if ending != \'\' else segments\n'"
learn_bpe.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\n""""""Use byte pair encoding (BPE) to learn a variable-length encoding of the vocabulary in a text.\nUnlike the original BPE, it does not compress the plain text, but can be used to reduce the vocabulary\nof a text to a configurable number of symbols, with only a small increase in the number of tokens.\n\nReference:\nRico Sennrich, Barry Haddow and Alexandra Birch (2016). Neural Machine Translation of Rare Words with Subword Units.\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n""""""\n\nfrom __future__ import unicode_literals\n\nimport os\nimport sys\nimport inspect\nimport codecs\nimport re\nimport copy\nimport warnings\nfrom collections import defaultdict, Counter\n\n\ndef update_vocabulary(vocab, file_name, is_dict=False):\n    """"""Read text and return dictionary that encodes vocabulary\n    """"""\n\n    #vocab = Counter()\n    with codecs.open(file_name, encoding=\'utf-8\') as fobj:\n        for i, line in enumerate(fobj):\n            if is_dict:\n                try:\n                    word, count = line.strip(\'\\r\\n \').split(\' \')\n                except:\n                    print(\'Failed reading vocabulary file at line {0}: {1}\'.format(i, line))\n                    sys.exit(1)\n                vocab[word] += int(count)\n            else:\n                for word in line.strip(\'\\r\\n \').split(\' \'):\n                    if word:\n                        vocab[word] += 1\n    return vocab\n\n\ndef update_pair_statistics(pair, changed, stats, indices):\n    """"""Minimally update the indices and frequency of symbol pairs\n\n    if we merge a pair of symbols, only pairs that overlap with occurrences\n    of this pair are affected, and need to be updated.\n    """"""\n    stats[pair] = 0\n    indices[pair] = defaultdict(int)\n    first, second = pair\n    new_pair = first+second\n    for j, word, old_word, freq in changed:\n\n        # find all instances of pair, and update frequency/indices around it\n        i = 0\n        while True:\n            # find first symbol\n            try:\n                i = old_word.index(first, i)\n            except ValueError:\n                break\n            # if first symbol is followed by second symbol, we\'ve found an occurrence of pair (old_word[i:i+2])\n            if i < len(old_word)-1 and old_word[i+1] == second:\n                # assuming a symbol sequence ""A B C"", if ""B C"" is merged, reduce the frequency of ""A B""\n                if i:\n                    prev = old_word[i-1:i+1]\n                    stats[prev] -= freq\n                    indices[prev][j] -= 1\n                if i < len(old_word)-2:\n                    # assuming a symbol sequence ""A B C B"", if ""B C"" is merged, reduce the frequency of ""C B"".\n                    # however, skip this if the sequence is A B C B C, because the frequency of ""C B"" will be reduced by the previous code block\n                    if old_word[i+2] != first or i >= len(old_word)-3 or old_word[i+3] != second:\n                        nex = old_word[i+1:i+3]\n                        stats[nex] -= freq\n                        indices[nex][j] -= 1\n                i += 2\n            else:\n                i += 1\n\n        i = 0\n        while True:\n            try:\n                # find new pair\n                i = word.index(new_pair, i)\n            except ValueError:\n                break\n            # assuming a symbol sequence ""A BC D"", if ""B C"" is merged, increase the frequency of ""A BC""\n            if i:\n                prev = word[i-1:i+1]\n                stats[prev] += freq\n                indices[prev][j] += 1\n            # assuming a symbol sequence ""A BC B"", if ""B C"" is merged, increase the frequency of ""BC B""\n            # however, if the sequence is A BC BC, skip this step because the count of ""BC BC"" will be incremented by the previous code block\n            if i < len(word)-1 and word[i+1] != new_pair:\n                nex = word[i:i+2]\n                stats[nex] += freq\n                indices[nex][j] += 1\n            i += 1\n\n\ndef get_pair_statistics(vocab):\n    """"""Count frequency of all symbol pairs, and create index""""""\n\n    # data structure of pair frequencies\n    stats = defaultdict(int)\n\n    #index from pairs to words\n    indices = defaultdict(lambda: defaultdict(int))\n\n    for i, (word, freq) in enumerate(vocab):\n        prev_char = word[0]\n        for char in word[1:]:\n            stats[prev_char, char] += freq\n            indices[prev_char, char][i] += 1\n            prev_char = char\n\n    return stats, indices\n\n\ndef replace_pair(pair, vocab, indices):\n    """"""Replace all occurrences of a symbol pair (\'A\', \'B\') with a new symbol \'AB\'""""""\n    first, second = pair\n    pair_str = \'\'.join(pair)\n    pair_str = pair_str.replace(\'\\\\\',\'\\\\\\\\\')\n    changes = []\n    pattern = re.compile(r\'(?<!\\S)\' + re.escape(first + \' \' + second) + r\'(?!\\S)\')\n    if sys.version_info < (3, 0):\n        iterator = indices[pair].iteritems()\n    else:\n        iterator = indices[pair].items()\n    for j, freq in iterator:\n        if freq < 1:\n            continue\n        word, freq = vocab[j]\n        new_word = \' \'.join(word)\n        new_word = pattern.sub(pair_str, new_word)\n        new_word = tuple(new_word.split(\' \'))\n\n        vocab[j] = (new_word, freq)\n        changes.append((j, new_word, word, freq))\n\n    return changes\n\ndef prune_stats(stats, big_stats, threshold):\n    """"""Prune statistics dict for efficiency of max()\n\n    The frequency of a symbol pair never increases, so pruning is generally safe\n    (until we the most frequent pair is less frequent than a pair we previously pruned)\n    big_stats keeps full statistics for when we need to access pruned items\n    """"""\n    for item,freq in list(stats.items()):\n        if freq < threshold:\n            del stats[item]\n            if freq < 0:\n                big_stats[item] += freq\n            else:\n                big_stats[item] = freq\n\n\ndef learn_bpe(infile_names, outfile_name, num_symbols, min_frequency=2, verbose=False, is_dict=False, total_symbols=False):\n    """"""Learn num_symbols BPE operations from vocabulary, and write to outfile.\n    """"""\n    sys.stderr = codecs.getwriter(\'UTF-8\')(sys.stderr.buffer)\n    sys.stdout = codecs.getwriter(\'UTF-8\')(sys.stdout.buffer)\n    sys.stdin = codecs.getreader(\'UTF-8\')(sys.stdin.buffer)\n\n    #vocab = get_vocabulary(infile, is_dict)\n    vocab = Counter()\n    for f in infile_names:\n        sys.stderr.write(f\'Collecting vocab from {f}\\n\')\n        vocab = update_vocabulary(vocab, f, is_dict)\n\n    vocab = dict([(tuple(x[:-1])+(x[-1]+\'</w>\',) ,y) for (x,y) in vocab.items()])\n    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n\n    stats, indices = get_pair_statistics(sorted_vocab)\n    big_stats = copy.deepcopy(stats)\n\n    if total_symbols:\n        uniq_char_internal = set()\n        uniq_char_final = set()\n        for word in vocab:\n            for char in word[:-1]:\n                uniq_char_internal.add(char)\n            uniq_char_final.add(word[-1])\n        sys.stderr.write(\'Number of word-internal characters: {0}\\n\'.format(len(uniq_char_internal)))\n        sys.stderr.write(\'Number of word-final characters: {0}\\n\'.format(len(uniq_char_final)))\n        sys.stderr.write(\'Reducing number of merge operations by {0}\\n\'.format(len(uniq_char_internal) + len(uniq_char_final)))\n        num_symbols -= len(uniq_char_internal) + len(uniq_char_final)\n\n\n    sys.stderr.write(f\'Write vocab file to {outfile_name}\')\n    with codecs.open(outfile_name, \'w\', encoding=\'utf-8\') as outfile:\n        # version 0.2 changes the handling of the end-of-word token (\'</w>\');\n        # version numbering allows bckward compatibility\n\n        outfile.write(\'#version: 0.2\\n\')\n        # threshold is inspired by Zipfian assumption, but should only affect speed\n        threshold = max(stats.values()) / 10\n        for i in range(num_symbols):\n            if stats:\n                most_frequent = max(stats, key=lambda x: (stats[x], x))\n\n            # we probably missed the best pair because of pruning; go back to full statistics\n            if not stats or (i and stats[most_frequent] < threshold):\n                prune_stats(stats, big_stats, threshold)\n                stats = copy.deepcopy(big_stats)\n                most_frequent = max(stats, key=lambda x: (stats[x], x))\n                # threshold is inspired by Zipfian assumption, but should only affect speed\n                threshold = stats[most_frequent] * i/(i+10000.0)\n                prune_stats(stats, big_stats, threshold)\n\n            if stats[most_frequent] < min_frequency:\n                sys.stderr.write(f\'no pair has frequency >= {min_frequency}. Stopping\\n\')\n                break\n\n            if verbose:\n                sys.stderr.write(\'pair {0}: {1} {2} -> {1}{2} (frequency {3})\\n\'.format(\n                    i, most_frequent[0], most_frequent[1], stats[most_frequent]))\n            outfile.write(\'{0} {1}\\n\'.format(*most_frequent))\n            changes = replace_pair(most_frequent, sorted_vocab, indices)\n            update_pair_statistics(most_frequent, changes, stats, indices)\n            stats[most_frequent] = 0\n            if not i % 100:\n                prune_stats(stats, big_stats, threshold)\n\n'"
preprocess.py,0,"b'\'\'\' Handling the data io \'\'\'\nimport os\nimport argparse\nimport logging\nimport dill as pickle\nimport urllib\nfrom tqdm import tqdm\nimport sys\nimport codecs\nimport spacy\nimport torch\nimport tarfile\nimport torchtext.data\nimport torchtext.datasets\nfrom torchtext.datasets import TranslationDataset\nimport transformer.Constants as Constants\nfrom learn_bpe import learn_bpe\nfrom apply_bpe import BPE\n\n\n__author__ = ""Yu-Hsiang Huang""\n\n\n_TRAIN_DATA_SOURCES = [\n    {""url"": ""http://data.statmt.org/wmt17/translation-task/"" \\\n             ""training-parallel-nc-v12.tgz"",\n     ""trg"": ""news-commentary-v12.de-en.en"",\n     ""src"": ""news-commentary-v12.de-en.de""},\n    #{""url"": ""http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz"",\n    # ""trg"": ""commoncrawl.de-en.en"",\n    # ""src"": ""commoncrawl.de-en.de""},\n    #{""url"": ""http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz"",\n    # ""trg"": ""europarl-v7.de-en.en"",\n    # ""src"": ""europarl-v7.de-en.de""}\n    ]\n\n_VAL_DATA_SOURCES = [\n    {""url"": ""http://data.statmt.org/wmt17/translation-task/dev.tgz"",\n     ""trg"": ""newstest2013.en"",\n     ""src"": ""newstest2013.de""}]\n\n_TEST_DATA_SOURCES = [\n    {""url"": ""https://storage.googleapis.com/tf-perf-public/"" \\\n                ""official_transformer/test_data/newstest2014.tgz"",\n     ""trg"": ""newstest2014.en"",\n     ""src"": ""newstest2014.de""}]\n\n\nclass TqdmUpTo(tqdm):\n    def update_to(self, b=1, bsize=1, tsize=None):\n        if tsize is not None:\n            self.total = tsize\n        self.update(b * bsize - self.n)\n\n\ndef file_exist(dir_name, file_name):\n    for sub_dir, _, files in os.walk(dir_name):\n        if file_name in files:\n            return os.path.join(sub_dir, file_name)\n    return None\n\n\ndef download_and_extract(download_dir, url, src_filename, trg_filename):\n    src_path = file_exist(download_dir, src_filename)\n    trg_path = file_exist(download_dir, trg_filename)\n\n    if src_path and trg_path:\n        sys.stderr.write(f""Already downloaded and extracted {url}.\\n"")\n        return src_path, trg_path\n\n    compressed_file = _download_file(download_dir, url)\n\n    sys.stderr.write(f""Extracting {compressed_file}.\\n"")\n    with tarfile.open(compressed_file, ""r:gz"") as corpus_tar:\n        corpus_tar.extractall(download_dir)\n\n    src_path = file_exist(download_dir, src_filename)\n    trg_path = file_exist(download_dir, trg_filename)\n\n    if src_path and trg_path:\n        return src_path, trg_path\n\n    raise OSError(f""Download/extraction failed for url {url} to path {download_dir}"")\n\n\ndef _download_file(download_dir, url):\n    filename = url.split(""/"")[-1]\n    if file_exist(download_dir, filename):\n        sys.stderr.write(f""Already downloaded: {url} (at {filename}).\\n"")\n    else:\n        sys.stderr.write(f""Downloading from {url} to {filename}.\\n"")\n        with TqdmUpTo(unit=\'B\', unit_scale=True, miniters=1, desc=filename) as t:\n            urllib.request.urlretrieve(url, filename=filename, reporthook=t.update_to)\n    return filename\n\n\ndef get_raw_files(raw_dir, sources):\n    raw_files = { ""src"": [], ""trg"": [], }\n    for d in sources:\n        src_file, trg_file = download_and_extract(raw_dir, d[""url""], d[""src""], d[""trg""])\n        raw_files[""src""].append(src_file)\n        raw_files[""trg""].append(trg_file)\n    return raw_files\n\n\ndef mkdir_if_needed(dir_name):\n    if not os.path.isdir(dir_name):\n        os.makedirs(dir_name)\n\n\ndef compile_files(raw_dir, raw_files, prefix):\n    src_fpath = os.path.join(raw_dir, f""raw-{prefix}.src"")\n    trg_fpath = os.path.join(raw_dir, f""raw-{prefix}.trg"")\n\n    if os.path.isfile(src_fpath) and os.path.isfile(trg_fpath):\n        sys.stderr.write(f""Merged files found, skip the merging process.\\n"")\n        return src_fpath, trg_fpath\n\n    sys.stderr.write(f""Merge files into two files: {src_fpath} and {trg_fpath}.\\n"")\n\n    with open(src_fpath, \'w\') as src_outf, open(trg_fpath, \'w\') as trg_outf:\n        for src_inf, trg_inf in zip(raw_files[\'src\'], raw_files[\'trg\']):\n            sys.stderr.write(f\'  Input files: \\n\'\\\n                    f\'    - SRC: {src_inf}, and\\n\' \\\n                    f\'    - TRG: {trg_inf}.\\n\')\n            with open(src_inf, newline=\'\\n\') as src_inf, open(trg_inf, newline=\'\\n\') as trg_inf:\n                cntr = 0\n                for i, line in enumerate(src_inf):\n                    cntr += 1\n                    src_outf.write(line.replace(\'\\r\', \' \').strip() + \'\\n\')\n                for j, line in enumerate(trg_inf):\n                    cntr -= 1\n                    trg_outf.write(line.replace(\'\\r\', \' \').strip() + \'\\n\')\n                assert cntr == 0, \'Number of lines in two files are inconsistent.\'\n    return src_fpath, trg_fpath\n\n\ndef encode_file(bpe, in_file, out_file):\n    sys.stderr.write(f""Read raw content from {in_file} and \\n""\\\n            f""Write encoded content to {out_file}\\n"")\n    \n    with codecs.open(in_file, encoding=\'utf-8\') as in_f:\n        with codecs.open(out_file, \'w\', encoding=\'utf-8\') as out_f:\n            for line in in_f:\n                out_f.write(bpe.process_line(line))\n\n\ndef encode_files(bpe, src_in_file, trg_in_file, data_dir, prefix):\n    src_out_file = os.path.join(data_dir, f""{prefix}.src"")\n    trg_out_file = os.path.join(data_dir, f""{prefix}.trg"")\n\n    if os.path.isfile(src_out_file) and os.path.isfile(trg_out_file):\n        sys.stderr.write(f""Encoded files found, skip the encoding process ...\\n"")\n\n    encode_file(bpe, src_in_file, src_out_file)\n    encode_file(bpe, trg_in_file, trg_out_file)\n    return src_out_file, trg_out_file\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-raw_dir\', required=True)\n    parser.add_argument(\'-data_dir\', required=True)\n    parser.add_argument(\'-codes\', required=True)\n    parser.add_argument(\'-save_data\', required=True)\n    parser.add_argument(\'-prefix\', required=True)\n    parser.add_argument(\'-max_len\', type=int, default=100)\n    parser.add_argument(\'--symbols\', \'-s\', type=int, default=32000, help=""Vocabulary size"")\n    parser.add_argument(\n        \'--min-frequency\', type=int, default=6, metavar=\'FREQ\',\n        help=\'Stop if no symbol pair has frequency >= FREQ (default: %(default)s))\')\n    parser.add_argument(\'--dict-input\', action=""store_true"",\n        help=""If set, input file is interpreted as a dictionary where each line contains a word-count pair"")\n    parser.add_argument(\n        \'--separator\', type=str, default=\'@@\', metavar=\'STR\',\n        help=""Separator between non-final subword units (default: \'%(default)s\'))"")\n    parser.add_argument(\'--total-symbols\', \'-t\', action=""store_true"")\n    opt = parser.parse_args()\n\n    # Create folder if needed.\n    mkdir_if_needed(opt.raw_dir)\n    mkdir_if_needed(opt.data_dir)\n\n    # Download and extract raw data.\n    raw_train = get_raw_files(opt.raw_dir, _TRAIN_DATA_SOURCES)\n    raw_val = get_raw_files(opt.raw_dir, _VAL_DATA_SOURCES)\n    raw_test = get_raw_files(opt.raw_dir, _TEST_DATA_SOURCES)\n\n    # Merge files into one.\n    train_src, train_trg = compile_files(opt.raw_dir, raw_train, opt.prefix + \'-train\')\n    val_src, val_trg = compile_files(opt.raw_dir, raw_val, opt.prefix + \'-val\')\n    test_src, test_trg = compile_files(opt.raw_dir, raw_test, opt.prefix + \'-test\')\n\n    # Build up the code from training files if not exist\n    opt.codes = os.path.join(opt.data_dir, opt.codes)\n    if not os.path.isfile(opt.codes):\n        sys.stderr.write(f""Collect codes from training data and save to {opt.codes}.\\n"")\n        learn_bpe(raw_train[\'src\'] + raw_train[\'trg\'], opt.codes, opt.symbols, opt.min_frequency, True)\n    sys.stderr.write(f""BPE codes prepared.\\n"")\n\n    sys.stderr.write(f""Build up the tokenizer.\\n"")\n    with codecs.open(opt.codes, encoding=\'utf-8\') as codes: \n        bpe = BPE(codes, separator=opt.separator)\n\n    sys.stderr.write(f""Encoding ...\\n"")\n    encode_files(bpe, train_src, train_trg, opt.data_dir, opt.prefix + \'-train\')\n    encode_files(bpe, val_src, val_trg, opt.data_dir, opt.prefix + \'-val\')\n    encode_files(bpe, test_src, test_trg, opt.data_dir, opt.prefix + \'-test\')\n    sys.stderr.write(f""Done.\\n"")\n\n\n    field = torchtext.data.Field(\n        tokenize=str.split,\n        lower=True,\n        pad_token=Constants.PAD_WORD,\n        init_token=Constants.BOS_WORD,\n        eos_token=Constants.EOS_WORD)\n\n    fields = (field, field)\n\n    MAX_LEN = opt.max_len\n\n    def filter_examples_with_length(x):\n        return len(vars(x)[\'src\']) <= MAX_LEN and len(vars(x)[\'trg\']) <= MAX_LEN\n\n    enc_train_files_prefix = opt.prefix + \'-train\'\n    train = TranslationDataset(\n        fields=fields,\n        path=os.path.join(opt.data_dir, enc_train_files_prefix),\n        exts=(\'.src\', \'.trg\'),\n        filter_pred=filter_examples_with_length)\n\n    from itertools import chain\n    field.build_vocab(chain(train.src, train.trg), min_freq=2)\n\n    data = { \'settings\': opt, \'vocab\': field, }\n    opt.save_data = os.path.join(opt.data_dir, opt.save_data)\n\n    print(\'[Info] Dumping the processed data to pickle file\', opt.save_data)\n    pickle.dump(data, open(opt.save_data, \'wb\'))\n\n\n\ndef main_wo_bpe():\n    \'\'\'\n    Usage: python preprocess.py -lang_src de -lang_trg en -save_data multi30k_de_en.pkl -share_vocab\n    \'\'\'\n\n    spacy_support_langs = [\'de\', \'el\', \'en\', \'es\', \'fr\', \'it\', \'lt\', \'nb\', \'nl\', \'pt\']\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-lang_src\', required=True, choices=spacy_support_langs)\n    parser.add_argument(\'-lang_trg\', required=True, choices=spacy_support_langs)\n    parser.add_argument(\'-save_data\', required=True)\n    parser.add_argument(\'-data_src\', type=str, default=None)\n    parser.add_argument(\'-data_trg\', type=str, default=None)\n\n    parser.add_argument(\'-max_len\', type=int, default=100)\n    parser.add_argument(\'-min_word_count\', type=int, default=3)\n    parser.add_argument(\'-keep_case\', action=\'store_true\')\n    parser.add_argument(\'-share_vocab\', action=\'store_true\')\n    #parser.add_argument(\'-ratio\', \'--train_valid_test_ratio\', type=int, nargs=3, metavar=(8,1,1))\n    #parser.add_argument(\'-vocab\', default=None)\n\n    opt = parser.parse_args()\n    assert not any([opt.data_src, opt.data_trg]), \'Custom data input is not support now.\'\n    assert not any([opt.data_src, opt.data_trg]) or all([opt.data_src, opt.data_trg])\n    print(opt)\n\n    src_lang_model = spacy.load(opt.lang_src)\n    trg_lang_model = spacy.load(opt.lang_trg)\n\n    def tokenize_src(text):\n        return [tok.text for tok in src_lang_model.tokenizer(text)]\n\n    def tokenize_trg(text):\n        return [tok.text for tok in trg_lang_model.tokenizer(text)]\n\n    SRC = torchtext.data.Field(\n        tokenize=tokenize_src, lower=not opt.keep_case,\n        pad_token=Constants.PAD_WORD, init_token=Constants.BOS_WORD, eos_token=Constants.EOS_WORD)\n\n    TRG = torchtext.data.Field(\n        tokenize=tokenize_trg, lower=not opt.keep_case,\n        pad_token=Constants.PAD_WORD, init_token=Constants.BOS_WORD, eos_token=Constants.EOS_WORD)\n\n    MAX_LEN = opt.max_len\n    MIN_FREQ = opt.min_word_count\n\n    if not all([opt.data_src, opt.data_trg]):\n        assert {opt.lang_src, opt.lang_trg} == {\'de\', \'en\'}\n    else:\n        # Pack custom txt file into example datasets\n        raise NotImplementedError\n\n    def filter_examples_with_length(x):\n        return len(vars(x)[\'src\']) <= MAX_LEN and len(vars(x)[\'trg\']) <= MAX_LEN\n\n    train, val, test = torchtext.datasets.Multi30k.splits(\n            exts = (\'.\' + opt.lang_src, \'.\' + opt.lang_trg),\n            fields = (SRC, TRG),\n            filter_pred=filter_examples_with_length)\n\n    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n    print(\'[Info] Get source language vocabulary size:\', len(SRC.vocab))\n    TRG.build_vocab(train.trg, min_freq=MIN_FREQ)\n    print(\'[Info] Get target language vocabulary size:\', len(TRG.vocab))\n\n    if opt.share_vocab:\n        print(\'[Info] Merging two vocabulary ...\')\n        for w, _ in SRC.vocab.stoi.items():\n            # TODO: Also update the `freq`, although it is not likely to be used.\n            if w not in TRG.vocab.stoi:\n                TRG.vocab.stoi[w] = len(TRG.vocab.stoi)\n        TRG.vocab.itos = [None] * len(TRG.vocab.stoi)\n        for w, i in TRG.vocab.stoi.items():\n            TRG.vocab.itos[i] = w\n        SRC.vocab.stoi = TRG.vocab.stoi\n        SRC.vocab.itos = TRG.vocab.itos\n        print(\'[Info] Get merged vocabulary size:\', len(TRG.vocab))\n\n\n    data = {\n        \'settings\': opt,\n        \'vocab\': {\'src\': SRC, \'trg\': TRG},\n        \'train\': train.examples,\n        \'valid\': val.examples,\n        \'test\': test.examples}\n\n    print(\'[Info] Dumping the processed data to pickle file\', opt.save_data)\n    pickle.dump(data, open(opt.save_data, \'wb\'))\n\n\nif __name__ == \'__main__\':\n    main_wo_bpe()\n    #main()\n'"
train.py,7,"b'\'\'\'\nThis script handles the training process.\n\'\'\'\n\nimport argparse\nimport math\nimport time\nimport dill as pickle\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchtext.data import Field, Dataset, BucketIterator\nfrom torchtext.datasets import TranslationDataset\n\nimport transformer.Constants as Constants\nfrom transformer.Models import Transformer\nfrom transformer.Optim import ScheduledOptim\n\n__author__ = ""Yu-Hsiang Huang""\n\ndef cal_performance(pred, gold, trg_pad_idx, smoothing=False):\n    \'\'\' Apply label smoothing if needed \'\'\'\n\n    loss = cal_loss(pred, gold, trg_pad_idx, smoothing=smoothing)\n\n    pred = pred.max(1)[1]\n    gold = gold.contiguous().view(-1)\n    non_pad_mask = gold.ne(trg_pad_idx)\n    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n    n_word = non_pad_mask.sum().item()\n\n    return loss, n_correct, n_word\n\n\ndef cal_loss(pred, gold, trg_pad_idx, smoothing=False):\n    \'\'\' Calculate cross entropy loss, apply label smoothing if needed. \'\'\'\n\n    gold = gold.contiguous().view(-1)\n\n    if smoothing:\n        eps = 0.1\n        n_class = pred.size(1)\n\n        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = F.log_softmax(pred, dim=1)\n\n        non_pad_mask = gold.ne(trg_pad_idx)\n        loss = -(one_hot * log_prb).sum(dim=1)\n        loss = loss.masked_select(non_pad_mask).sum()  # average later\n    else:\n        loss = F.cross_entropy(pred, gold, ignore_index=trg_pad_idx, reduction=\'sum\')\n    return loss\n\n\ndef patch_src(src, pad_idx):\n    src = src.transpose(0, 1)\n    return src\n\n\ndef patch_trg(trg, pad_idx):\n    trg = trg.transpose(0, 1)\n    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n    return trg, gold\n\n\ndef train_epoch(model, training_data, optimizer, opt, device, smoothing):\n    \'\'\' Epoch operation in training phase\'\'\'\n\n    model.train()\n    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n\n    desc = \'  - (Training)   \'\n    for batch in tqdm(training_data, mininterval=2, desc=desc, leave=False):\n\n        # prepare data\n        src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n        trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n\n        # forward\n        optimizer.zero_grad()\n        pred = model(src_seq, trg_seq)\n\n        # backward and update parameters\n        loss, n_correct, n_word = cal_performance(\n            pred, gold, opt.trg_pad_idx, smoothing=smoothing) \n        loss.backward()\n        optimizer.step_and_update_lr()\n\n        # note keeping\n        n_word_total += n_word\n        n_word_correct += n_correct\n        total_loss += loss.item()\n\n    loss_per_word = total_loss/n_word_total\n    accuracy = n_word_correct/n_word_total\n    return loss_per_word, accuracy\n\n\ndef eval_epoch(model, validation_data, device, opt):\n    \'\'\' Epoch operation in evaluation phase \'\'\'\n\n    model.eval()\n    total_loss, n_word_total, n_word_correct = 0, 0, 0\n\n    desc = \'  - (Validation) \'\n    with torch.no_grad():\n        for batch in tqdm(validation_data, mininterval=2, desc=desc, leave=False):\n\n            # prepare data\n            src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n            trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n\n            # forward\n            pred = model(src_seq, trg_seq)\n            loss, n_correct, n_word = cal_performance(\n                pred, gold, opt.trg_pad_idx, smoothing=False)\n\n            # note keeping\n            n_word_total += n_word\n            n_word_correct += n_correct\n            total_loss += loss.item()\n\n    loss_per_word = total_loss/n_word_total\n    accuracy = n_word_correct/n_word_total\n    return loss_per_word, accuracy\n\n\ndef train(model, training_data, validation_data, optimizer, device, opt):\n    \'\'\' Start training \'\'\'\n\n    log_train_file, log_valid_file = None, None\n\n    if opt.log:\n        log_train_file = opt.log + \'.train.log\'\n        log_valid_file = opt.log + \'.valid.log\'\n\n        print(\'[Info] Training performance will be written to file: {} and {}\'.format(\n            log_train_file, log_valid_file))\n\n        with open(log_train_file, \'w\') as log_tf, open(log_valid_file, \'w\') as log_vf:\n            log_tf.write(\'epoch,loss,ppl,accuracy\\n\')\n            log_vf.write(\'epoch,loss,ppl,accuracy\\n\')\n\n    def print_performances(header, loss, accu, start_time):\n        print(\'  - {header:12} ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, \'\\\n              \'elapse: {elapse:3.3f} min\'.format(\n                  header=f""({header})"", ppl=math.exp(min(loss, 100)),\n                  accu=100*accu, elapse=(time.time()-start_time)/60))\n\n    #valid_accus = []\n    valid_losses = []\n    for epoch_i in range(opt.epoch):\n        print(\'[ Epoch\', epoch_i, \']\')\n\n        start = time.time()\n        train_loss, train_accu = train_epoch(\n            model, training_data, optimizer, opt, device, smoothing=opt.label_smoothing)\n        print_performances(\'Training\', train_loss, train_accu, start)\n\n        start = time.time()\n        valid_loss, valid_accu = eval_epoch(model, validation_data, device, opt)\n        print_performances(\'Validation\', valid_loss, valid_accu, start)\n\n        valid_losses += [valid_loss]\n\n        checkpoint = {\'epoch\': epoch_i, \'settings\': opt, \'model\': model.state_dict()}\n\n        if opt.save_model:\n            if opt.save_mode == \'all\':\n                model_name = opt.save_model + \'_accu_{accu:3.3f}.chkpt\'.format(accu=100*valid_accu)\n                torch.save(checkpoint, model_name)\n            elif opt.save_mode == \'best\':\n                model_name = opt.save_model + \'.chkpt\'\n                if valid_loss <= min(valid_losses):\n                    torch.save(checkpoint, model_name)\n                    print(\'    - [Info] The checkpoint file has been updated.\')\n\n        if log_train_file and log_valid_file:\n            with open(log_train_file, \'a\') as log_tf, open(log_valid_file, \'a\') as log_vf:\n                log_tf.write(\'{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n\'.format(\n                    epoch=epoch_i, loss=train_loss,\n                    ppl=math.exp(min(train_loss, 100)), accu=100*train_accu))\n                log_vf.write(\'{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n\'.format(\n                    epoch=epoch_i, loss=valid_loss,\n                    ppl=math.exp(min(valid_loss, 100)), accu=100*valid_accu))\n\ndef main():\n    \'\'\' \n    Usage:\n    python train.py -data_pkl m30k_deen_shr.pkl -log m30k_deen_shr -embs_share_weight -proj_share_weight -label_smoothing -save_model trained -b 256 -warmup 128000\n    \'\'\'\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'-data_pkl\', default=None)     # all-in-1 data pickle or bpe field\n\n    parser.add_argument(\'-train_path\', default=None)   # bpe encoded data\n    parser.add_argument(\'-val_path\', default=None)     # bpe encoded data\n\n    parser.add_argument(\'-epoch\', type=int, default=10)\n    parser.add_argument(\'-b\', \'--batch_size\', type=int, default=2048)\n\n    parser.add_argument(\'-d_model\', type=int, default=512)\n    parser.add_argument(\'-d_inner_hid\', type=int, default=2048)\n    parser.add_argument(\'-d_k\', type=int, default=64)\n    parser.add_argument(\'-d_v\', type=int, default=64)\n\n    parser.add_argument(\'-n_head\', type=int, default=8)\n    parser.add_argument(\'-n_layers\', type=int, default=6)\n    parser.add_argument(\'-warmup\',\'--n_warmup_steps\', type=int, default=4000)\n\n    parser.add_argument(\'-dropout\', type=float, default=0.1)\n    parser.add_argument(\'-embs_share_weight\', action=\'store_true\')\n    parser.add_argument(\'-proj_share_weight\', action=\'store_true\')\n\n    parser.add_argument(\'-log\', default=None)\n    parser.add_argument(\'-save_model\', default=None)\n    parser.add_argument(\'-save_mode\', type=str, choices=[\'all\', \'best\'], default=\'best\')\n\n    parser.add_argument(\'-no_cuda\', action=\'store_true\')\n    parser.add_argument(\'-label_smoothing\', action=\'store_true\')\n\n    opt = parser.parse_args()\n    opt.cuda = not opt.no_cuda\n    opt.d_word_vec = opt.d_model\n\n    if not opt.log and not opt.save_model:\n        print(\'No experiment result will be saved.\')\n        raise\n\n    if opt.batch_size < 2048 and opt.n_warmup_steps <= 4000:\n        print(\'[Warning] The warmup steps may be not enough.\\n\'\\\n              \'(sz_b, warmup) = (2048, 4000) is the official setting.\\n\'\\\n              \'Using smaller batch w/o longer warmup may cause \'\\\n              \'the warmup stage ends with only little data trained.\')\n\n    device = torch.device(\'cuda\' if opt.cuda else \'cpu\')\n\n    #========= Loading Dataset =========#\n\n    if all((opt.train_path, opt.val_path)):\n        training_data, validation_data = prepare_dataloaders_from_bpe_files(opt, device)\n    elif opt.data_pkl:\n        training_data, validation_data = prepare_dataloaders(opt, device)\n    else:\n        raise\n\n    print(opt)\n\n    transformer = Transformer(\n        opt.src_vocab_size,\n        opt.trg_vocab_size,\n        src_pad_idx=opt.src_pad_idx,\n        trg_pad_idx=opt.trg_pad_idx,\n        trg_emb_prj_weight_sharing=opt.proj_share_weight,\n        emb_src_trg_weight_sharing=opt.embs_share_weight,\n        d_k=opt.d_k,\n        d_v=opt.d_v,\n        d_model=opt.d_model,\n        d_word_vec=opt.d_word_vec,\n        d_inner=opt.d_inner_hid,\n        n_layers=opt.n_layers,\n        n_head=opt.n_head,\n        dropout=opt.dropout).to(device)\n\n    optimizer = ScheduledOptim(\n        optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09),\n        2.0, opt.d_model, opt.n_warmup_steps)\n\n    train(transformer, training_data, validation_data, optimizer, device, opt)\n\n\ndef prepare_dataloaders_from_bpe_files(opt, device):\n    batch_size = opt.batch_size\n    MIN_FREQ = 2\n    if not opt.embs_share_weight:\n        raise\n\n    data = pickle.load(open(opt.data_pkl, \'rb\'))\n    MAX_LEN = data[\'settings\'].max_len\n    field = data[\'vocab\']\n    fields = (field, field)\n\n    def filter_examples_with_length(x):\n        return len(vars(x)[\'src\']) <= MAX_LEN and len(vars(x)[\'trg\']) <= MAX_LEN\n\n    train = TranslationDataset(\n        fields=fields,\n        path=opt.train_path, \n        exts=(\'.src\', \'.trg\'),\n        filter_pred=filter_examples_with_length)\n    val = TranslationDataset(\n        fields=fields,\n        path=opt.val_path, \n        exts=(\'.src\', \'.trg\'),\n        filter_pred=filter_examples_with_length)\n\n    opt.max_token_seq_len = MAX_LEN + 2\n    opt.src_pad_idx = opt.trg_pad_idx = field.vocab.stoi[Constants.PAD_WORD]\n    opt.src_vocab_size = opt.trg_vocab_size = len(field.vocab)\n\n    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n    return train_iterator, val_iterator\n\n\ndef prepare_dataloaders(opt, device):\n    batch_size = opt.batch_size\n    data = pickle.load(open(opt.data_pkl, \'rb\'))\n\n    opt.max_token_seq_len = data[\'settings\'].max_len\n    opt.src_pad_idx = data[\'vocab\'][\'src\'].vocab.stoi[Constants.PAD_WORD]\n    opt.trg_pad_idx = data[\'vocab\'][\'trg\'].vocab.stoi[Constants.PAD_WORD]\n\n    opt.src_vocab_size = len(data[\'vocab\'][\'src\'].vocab)\n    opt.trg_vocab_size = len(data[\'vocab\'][\'trg\'].vocab)\n\n    #========= Preparing Model =========#\n    if opt.embs_share_weight:\n        assert data[\'vocab\'][\'src\'].vocab.stoi == data[\'vocab\'][\'trg\'].vocab.stoi, \\\n            \'To sharing word embedding the src/trg word2idx table shall be the same.\'\n\n    fields = {\'src\': data[\'vocab\'][\'src\'], \'trg\':data[\'vocab\'][\'trg\']}\n\n    train = Dataset(examples=data[\'train\'], fields=fields)\n    val = Dataset(examples=data[\'valid\'], fields=fields)\n\n    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n\n    return train_iterator, val_iterator\n\n\nif __name__ == \'__main__\':\n    main()\n'"
translate.py,3,"b'\'\'\' Translate input text with trained model. \'\'\'\n\nimport torch\nimport argparse\nimport dill as pickle\nfrom tqdm import tqdm\n\nimport transformer.Constants as Constants\nfrom torchtext.data import Dataset\nfrom transformer.Models import Transformer\nfrom transformer.Translator import Translator\n\n\ndef load_model(opt, device):\n\n    checkpoint = torch.load(opt.model, map_location=device)\n    model_opt = checkpoint[\'settings\']\n\n    model = Transformer(\n        model_opt.src_vocab_size,\n        model_opt.trg_vocab_size,\n\n        model_opt.src_pad_idx,\n        model_opt.trg_pad_idx,\n\n        trg_emb_prj_weight_sharing=model_opt.proj_share_weight,\n        emb_src_trg_weight_sharing=model_opt.embs_share_weight,\n        d_k=model_opt.d_k,\n        d_v=model_opt.d_v,\n        d_model=model_opt.d_model,\n        d_word_vec=model_opt.d_word_vec,\n        d_inner=model_opt.d_inner_hid,\n        n_layers=model_opt.n_layers,\n        n_head=model_opt.n_head,\n        dropout=model_opt.dropout).to(device)\n\n    model.load_state_dict(checkpoint[\'model\'])\n    print(\'[Info] Trained model state loaded.\')\n    return model \n\n\ndef main():\n    \'\'\'Main Function\'\'\'\n\n    parser = argparse.ArgumentParser(description=\'translate.py\')\n\n    parser.add_argument(\'-model\', required=True,\n                        help=\'Path to model weight file\')\n    parser.add_argument(\'-data_pkl\', required=True,\n                        help=\'Pickle file with both instances and vocabulary.\')\n    parser.add_argument(\'-output\', default=\'pred.txt\',\n                        help=""""""Path to output the predictions (each line will\n                        be the decoded sequence"""""")\n    parser.add_argument(\'-beam_size\', type=int, default=5)\n    parser.add_argument(\'-max_seq_len\', type=int, default=100)\n    parser.add_argument(\'-no_cuda\', action=\'store_true\')\n\n    # TODO: Translate bpe encoded files \n    #parser.add_argument(\'-src\', required=True,\n    #                    help=\'Source sequence to decode (one line per sequence)\')\n    #parser.add_argument(\'-vocab\', required=True,\n    #                    help=\'Source sequence to decode (one line per sequence)\')\n    # TODO: Batch translation\n    #parser.add_argument(\'-batch_size\', type=int, default=30,\n    #                    help=\'Batch size\')\n    #parser.add_argument(\'-n_best\', type=int, default=1,\n    #                    help=""""""If verbose is set, will output the n_best\n    #                    decoded sentences"""""")\n\n    opt = parser.parse_args()\n    opt.cuda = not opt.no_cuda\n\n    data = pickle.load(open(opt.data_pkl, \'rb\'))\n    SRC, TRG = data[\'vocab\'][\'src\'], data[\'vocab\'][\'trg\']\n    opt.src_pad_idx = SRC.vocab.stoi[Constants.PAD_WORD]\n    opt.trg_pad_idx = TRG.vocab.stoi[Constants.PAD_WORD]\n    opt.trg_bos_idx = TRG.vocab.stoi[Constants.BOS_WORD]\n    opt.trg_eos_idx = TRG.vocab.stoi[Constants.EOS_WORD]\n\n    test_loader = Dataset(examples=data[\'test\'], fields={\'src\': SRC, \'trg\': TRG})\n    \n    device = torch.device(\'cuda\' if opt.cuda else \'cpu\')\n    translator = Translator(\n        model=load_model(opt, device),\n        beam_size=opt.beam_size,\n        max_seq_len=opt.max_seq_len,\n        src_pad_idx=opt.src_pad_idx,\n        trg_pad_idx=opt.trg_pad_idx,\n        trg_bos_idx=opt.trg_bos_idx,\n        trg_eos_idx=opt.trg_eos_idx).to(device)\n\n    unk_idx = SRC.vocab.stoi[SRC.unk_token]\n    with open(opt.output, \'w\') as f:\n        for example in tqdm(test_loader, mininterval=2, desc=\'  - (Test)\', leave=False):\n            #print(\' \'.join(example.src))\n            src_seq = [SRC.vocab.stoi.get(word, unk_idx) for word in example.src]\n            pred_seq = translator.translate_sentence(torch.LongTensor([src_seq]).to(device))\n            pred_line = \' \'.join(TRG.vocab.itos[idx] for idx in pred_seq)\n            pred_line = pred_line.replace(Constants.BOS_WORD, \'\').replace(Constants.EOS_WORD, \'\')\n            #print(pred_line)\n            f.write(pred_line.strip() + \'\\n\')\n\n    print(\'[Info] Finished.\')\n\nif __name__ == ""__main__"":\n    \'\'\'\n    Usage: python translate.py -model trained.chkpt -data multi30k.pt -no_cuda\n    \'\'\'\n    main()\n'"
transformer/Constants.py,0,"b""PAD_WORD = '<blank>'\nUNK_WORD = '<unk>'\nBOS_WORD = '<s>'\nEOS_WORD = '</s>'\n"""
transformer/Layers.py,1,"b'\'\'\' Define the Layers \'\'\'\nimport torch.nn as nn\nimport torch\nfrom transformer.SubLayers import MultiHeadAttention, PositionwiseFeedForward\n\n\n__author__ = ""Yu-Hsiang Huang""\n\n\nclass EncoderLayer(nn.Module):\n    \'\'\' Compose with two layers \'\'\'\n\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(self, enc_input, slf_attn_mask=None):\n        enc_output, enc_slf_attn = self.slf_attn(\n            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n        enc_output = self.pos_ffn(enc_output)\n        return enc_output, enc_slf_attn\n\n\nclass DecoderLayer(nn.Module):\n    \'\'\' Compose with three layers \'\'\'\n\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(\n            self, dec_input, enc_output,\n            slf_attn_mask=None, dec_enc_attn_mask=None):\n        dec_output, dec_slf_attn = self.slf_attn(\n            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n        dec_output, dec_enc_attn = self.enc_attn(\n            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n        dec_output = self.pos_ffn(dec_output)\n        return dec_output, dec_slf_attn, dec_enc_attn\n'"
transformer/Models.py,4,"b'\'\'\' Define the Transformer model \'\'\'\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom transformer.Layers import EncoderLayer, DecoderLayer\n\n\n__author__ = ""Yu-Hsiang Huang""\n\n\ndef get_pad_mask(seq, pad_idx):\n    return (seq != pad_idx).unsqueeze(-2)\n\n\ndef get_subsequent_mask(seq):\n    \'\'\' For masking out the subsequent info. \'\'\'\n    sz_b, len_s = seq.size()\n    subsequent_mask = (1 - torch.triu(\n        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n    return subsequent_mask\n\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_hid, n_position=200):\n        super(PositionalEncoding, self).__init__()\n\n        # Not a parameter\n        self.register_buffer(\'pos_table\', self._get_sinusoid_encoding_table(n_position, d_hid))\n\n    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n        \'\'\' Sinusoid position encoding table \'\'\'\n        # TODO: make it with torch instead of numpy\n\n        def get_position_angle_vec(position):\n            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n\n        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n\n    def forward(self, x):\n        return x + self.pos_table[:, :x.size(1)].clone().detach()\n\n\nclass Encoder(nn.Module):\n    \'\'\' A encoder model with self attention mechanism. \'\'\'\n\n    def __init__(\n            self, n_src_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n            d_model, d_inner, pad_idx, dropout=0.1, n_position=200):\n\n        super().__init__()\n\n        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx)\n        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_stack = nn.ModuleList([\n            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n            for _ in range(n_layers)])\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n    def forward(self, src_seq, src_mask, return_attns=False):\n\n        enc_slf_attn_list = []\n\n        # -- Forward\n        \n        enc_output = self.dropout(self.position_enc(self.src_word_emb(src_seq)))\n        enc_output = self.layer_norm(enc_output)\n\n        for enc_layer in self.layer_stack:\n            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)\n            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n\n        if return_attns:\n            return enc_output, enc_slf_attn_list\n        return enc_output,\n\n\nclass Decoder(nn.Module):\n    \'\'\' A decoder model with self attention mechanism. \'\'\'\n\n    def __init__(\n            self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n            d_model, d_inner, pad_idx, n_position=200, dropout=0.1):\n\n        super().__init__()\n\n        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)\n        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_stack = nn.ModuleList([\n            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n            for _ in range(n_layers)])\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n    def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns=False):\n\n        dec_slf_attn_list, dec_enc_attn_list = [], []\n\n        # -- Forward\n        dec_output = self.dropout(self.position_enc(self.trg_word_emb(trg_seq)))\n        dec_output = self.layer_norm(dec_output)\n\n        for dec_layer in self.layer_stack:\n            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n            dec_slf_attn_list += [dec_slf_attn] if return_attns else []\n            dec_enc_attn_list += [dec_enc_attn] if return_attns else []\n\n        if return_attns:\n            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n        return dec_output,\n\n\nclass Transformer(nn.Module):\n    \'\'\' A sequence to sequence model with attention mechanism. \'\'\'\n\n    def __init__(\n            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,\n            d_word_vec=512, d_model=512, d_inner=2048,\n            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1, n_position=200,\n            trg_emb_prj_weight_sharing=True, emb_src_trg_weight_sharing=True):\n\n        super().__init__()\n\n        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n\n        self.encoder = Encoder(\n            n_src_vocab=n_src_vocab, n_position=n_position,\n            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n            pad_idx=src_pad_idx, dropout=dropout)\n\n        self.decoder = Decoder(\n            n_trg_vocab=n_trg_vocab, n_position=n_position,\n            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n            pad_idx=trg_pad_idx, dropout=dropout)\n\n        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False)\n\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p) \n\n        assert d_model == d_word_vec, \\\n        \'To facilitate the residual connections, \\\n         the dimensions of all module outputs shall be the same.\'\n\n        self.x_logit_scale = 1.\n        if trg_emb_prj_weight_sharing:\n            # Share the weight between target word embedding & last dense layer\n            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight\n            self.x_logit_scale = (d_model ** -0.5)\n\n        if emb_src_trg_weight_sharing:\n            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight\n\n\n    def forward(self, src_seq, trg_seq):\n\n        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) & get_subsequent_mask(trg_seq)\n\n        enc_output, *_ = self.encoder(src_seq, src_mask)\n        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)\n        seq_logit = self.trg_word_prj(dec_output) * self.x_logit_scale\n\n        return seq_logit.view(-1, seq_logit.size(2))\n'"
transformer/Modules.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__author__ = ""Yu-Hsiang Huang""\n\nclass ScaledDotProductAttention(nn.Module):\n    \'\'\' Scaled Dot-Product Attention \'\'\'\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, q, k, v, mask=None):\n\n        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n\n        if mask is not None:\n            attn = attn.masked_fill(mask == 0, -1e9)\n\n        attn = self.dropout(F.softmax(attn, dim=-1))\n        output = torch.matmul(attn, v)\n\n        return output, attn\n'"
transformer/Optim.py,0,"b'\'\'\'A wrapper class for scheduled optimizer \'\'\'\nimport numpy as np\n\nclass ScheduledOptim():\n    \'\'\'A simple wrapper class for learning rate scheduling\'\'\'\n\n    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n        self._optimizer = optimizer\n        self.init_lr = init_lr\n        self.d_model = d_model\n        self.n_warmup_steps = n_warmup_steps\n        self.n_steps = 0\n\n\n    def step_and_update_lr(self):\n        ""Step with the inner optimizer""\n        self._update_learning_rate()\n        self._optimizer.step()\n\n\n    def zero_grad(self):\n        ""Zero out the gradients with the inner optimizer""\n        self._optimizer.zero_grad()\n\n\n    def _get_lr_scale(self):\n        d_model = self.d_model\n        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n\n\n    def _update_learning_rate(self):\n        \'\'\' Learning rate scheduling per step \'\'\'\n\n        self.n_steps += 1\n        lr = self.init_lr * self._get_lr_scale()\n\n        for param_group in self._optimizer.param_groups:\n            param_group[\'lr\'] = lr\n\n'"
transformer/SubLayers.py,2,"b'\'\'\' Define the sublayers in encoder/decoder layer \'\'\'\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformer.Modules import ScaledDotProductAttention\n\n__author__ = ""Yu-Hsiang Huang""\n\nclass MultiHeadAttention(nn.Module):\n    \'\'\' Multi-Head Attention module \'\'\'\n\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n\n        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n\n    def forward(self, q, k, v, mask=None):\n\n        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n\n        residual = q\n\n        # Pass through the pre-attention projection: b x lq x (n*dv)\n        # Separate different heads: b x lq x n x dv\n        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n\n        # Transpose for attention dot product: b x n x lq x dv\n        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n\n        if mask is not None:\n            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n\n        q, attn = self.attention(q, k, v, mask=mask)\n\n        # Transpose to move the head dimension back: b x lq x n x dv\n        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n        q = self.dropout(self.fc(q))\n        q += residual\n\n        q = self.layer_norm(q)\n\n        return q, attn\n\n\nclass PositionwiseFeedForward(nn.Module):\n    \'\'\' A two-feed-forward-layer module \'\'\'\n\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n\n        residual = x\n\n        x = self.w_2(F.relu(self.w_1(x)))\n        x = self.dropout(x)\n        x += residual\n\n        x = self.layer_norm(x)\n\n        return x\n'"
transformer/Translator.py,8,"b""''' This module will handle the text generation with beam search. '''\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformer.Models import Transformer, get_pad_mask, get_subsequent_mask\n\n\nclass Translator(nn.Module):\n    ''' Load a trained model and translate in beam search fashion. '''\n\n    def __init__(\n            self, model, beam_size, max_seq_len,\n            src_pad_idx, trg_pad_idx, trg_bos_idx, trg_eos_idx):\n        \n\n        super(Translator, self).__init__()\n\n        self.alpha = 0.7\n        self.beam_size = beam_size\n        self.max_seq_len = max_seq_len\n        self.src_pad_idx = src_pad_idx\n        self.trg_bos_idx = trg_bos_idx\n        self.trg_eos_idx = trg_eos_idx\n\n        self.model = model\n        self.model.eval()\n\n        self.register_buffer('init_seq', torch.LongTensor([[trg_bos_idx]]))\n        self.register_buffer(\n            'blank_seqs', \n            torch.full((beam_size, max_seq_len), trg_pad_idx, dtype=torch.long))\n        self.blank_seqs[:, 0] = self.trg_bos_idx\n        self.register_buffer(\n            'len_map', \n            torch.arange(1, max_seq_len + 1, dtype=torch.long).unsqueeze(0))\n\n\n    def _model_decode(self, trg_seq, enc_output, src_mask):\n        trg_mask = get_subsequent_mask(trg_seq)\n        dec_output, *_ = self.model.decoder(trg_seq, trg_mask, enc_output, src_mask)\n        return F.softmax(self.model.trg_word_prj(dec_output), dim=-1)\n\n\n    def _get_init_state(self, src_seq, src_mask):\n        beam_size = self.beam_size\n\n        enc_output, *_ = self.model.encoder(src_seq, src_mask)\n        dec_output = self._model_decode(self.init_seq, enc_output, src_mask)\n        \n        best_k_probs, best_k_idx = dec_output[:, -1, :].topk(beam_size)\n\n        scores = torch.log(best_k_probs).view(beam_size)\n        gen_seq = self.blank_seqs.clone().detach()\n        gen_seq[:, 1] = best_k_idx[0]\n        enc_output = enc_output.repeat(beam_size, 1, 1)\n        return enc_output, gen_seq, scores\n\n\n    def _get_the_best_score_and_idx(self, gen_seq, dec_output, scores, step):\n        assert len(scores.size()) == 1\n        \n        beam_size = self.beam_size\n\n        # Get k candidates for each beam, k^2 candidates in total.\n        best_k2_probs, best_k2_idx = dec_output[:, -1, :].topk(beam_size)\n\n        # Include the previous scores.\n        scores = torch.log(best_k2_probs).view(beam_size, -1) + scores.view(beam_size, 1)\n\n        # Get the best k candidates from k^2 candidates.\n        scores, best_k_idx_in_k2 = scores.view(-1).topk(beam_size)\n \n        # Get the corresponding positions of the best k candidiates.\n        best_k_r_idxs, best_k_c_idxs = best_k_idx_in_k2 // beam_size, best_k_idx_in_k2 % beam_size\n        best_k_idx = best_k2_idx[best_k_r_idxs, best_k_c_idxs]\n\n        # Copy the corresponding previous tokens.\n        gen_seq[:, :step] = gen_seq[best_k_r_idxs, :step]\n        # Set the best tokens in this beam search step\n        gen_seq[:, step] = best_k_idx\n\n        return gen_seq, scores\n\n\n    def translate_sentence(self, src_seq):\n        # Only accept batch size equals to 1 in this function.\n        # TODO: expand to batch operation.\n        assert src_seq.size(0) == 1\n\n        src_pad_idx, trg_eos_idx = self.src_pad_idx, self.trg_eos_idx \n        max_seq_len, beam_size, alpha = self.max_seq_len, self.beam_size, self.alpha \n\n        with torch.no_grad():\n            src_mask = get_pad_mask(src_seq, src_pad_idx)\n            enc_output, gen_seq, scores = self._get_init_state(src_seq, src_mask)\n\n            ans_idx = 0   # default\n            for step in range(2, max_seq_len):    # decode up to max length\n                dec_output = self._model_decode(gen_seq[:, :step], enc_output, src_mask)\n                gen_seq, scores = self._get_the_best_score_and_idx(gen_seq, dec_output, scores, step)\n\n                # Check if all path finished\n                # -- locate the eos in the generated sequences\n                eos_locs = gen_seq == trg_eos_idx   \n                # -- replace the eos with its position for the length penalty use\n                seq_lens, _ = self.len_map.masked_fill(~eos_locs, max_seq_len).min(1)\n                # -- check if all beams contain eos\n                if (eos_locs.sum(1) > 0).sum(0).item() == beam_size:\n                    # TODO: Try different terminate conditions.\n                    _, ans_idx = scores.div(seq_lens.float() ** alpha).max(0)\n                    ans_idx = ans_idx.item()\n                    break\n        return gen_seq[ans_idx][:seq_lens[ans_idx]].tolist()\n"""
transformer/__init__.py,0,"b'import transformer.Constants\nimport transformer.Modules\nimport transformer.Layers\nimport transformer.SubLayers\nimport transformer.Models\nimport transformer.Translator\nimport transformer.Optim\n\n__all__ = [\n    transformer.Constants, transformer.Modules, transformer.Layers,\n    transformer.SubLayers, transformer.Models, transformer.Optim,\n    transformer.Translator]\n'"
