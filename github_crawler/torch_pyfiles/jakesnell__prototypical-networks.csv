file_path,api_count,code
setup.py,0,"b""from setuptools import setup\n\nsetup(name='protonets',\n      version='0.0.1',\n      author='Jake Snell',\n      author_email='jsnell@cs.toronto.edu',\n      license='MIT',\n      packages=['protonets'],\n      install_requires=[\n          'torch',\n          'tqdm'\n      ])\n"""
protonets/__init__.py,0,b''
protonets/engine.py,0,"b'from tqdm import tqdm\n\nclass Engine(object):\n    def __init__(self):\n        hook_names = [\'on_start\', \'on_start_epoch\', \'on_sample\', \'on_forward\',\n                      \'on_backward\', \'on_end_epoch\', \'on_update\', \'on_end\']\n\n        self.hooks = { }\n        for hook_name in hook_names:\n            self.hooks[hook_name] = lambda state: None\n\n    def train(self, **kwargs):\n        state = {\n            \'model\': kwargs[\'model\'],\n            \'loader\': kwargs[\'loader\'],\n            \'optim_method\': kwargs[\'optim_method\'],\n            \'optim_config\': kwargs[\'optim_config\'],\n            \'max_epoch\': kwargs[\'max_epoch\'],\n            \'epoch\': 0, # epochs done so far\n            \'t\': 0, # samples seen so far\n            \'batch\': 0, # samples seen in current epoch\n            \'stop\': False\n        }\n\n        state[\'optimizer\'] = state[\'optim_method\'](state[\'model\'].parameters(), **state[\'optim_config\'])\n\n        self.hooks[\'on_start\'](state)\n        while state[\'epoch\'] < state[\'max_epoch\'] and not state[\'stop\']:\n            state[\'model\'].train()\n\n            self.hooks[\'on_start_epoch\'](state)\n\n            state[\'epoch_size\'] = len(state[\'loader\'])\n\n            for sample in tqdm(state[\'loader\'], desc=""Epoch {:d} train"".format(state[\'epoch\'] + 1)):\n                state[\'sample\'] = sample\n                self.hooks[\'on_sample\'](state)\n\n                state[\'optimizer\'].zero_grad()\n                loss, state[\'output\'] = state[\'model\'].loss(state[\'sample\'])\n                self.hooks[\'on_forward\'](state)\n\n                loss.backward()\n                self.hooks[\'on_backward\'](state)\n\n                state[\'optimizer\'].step()\n\n                state[\'t\'] += 1\n                state[\'batch\'] += 1\n                self.hooks[\'on_update\'](state)\n\n            state[\'epoch\'] += 1\n            state[\'batch\'] = 0\n            self.hooks[\'on_end_epoch\'](state)\n\n        self.hooks[\'on_end\'](state)\n'"
protonets/data/__init__.py,0,b'from . import omniglot\n'
protonets/data/base.py,2,"b""import torch\n\ndef convert_dict(k, v):\n    return { k: v }\n\nclass CudaTransform(object):\n    def __init__(self):\n        pass\n\n    def __call__(self, data):\n        for k,v in data.items():\n            if hasattr(v, 'cuda'):\n                data[k] = v.cuda()\n\n        return data\n\nclass SequentialBatchSampler(object):\n    def __init__(self, n_classes):\n        self.n_classes = n_classes\n\n    def __len__(self):\n        return self.n_classes\n\n    def __iter__(self):\n        for i in range(self.n_classes):\n            yield torch.LongTensor([i])\n\nclass EpisodicBatchSampler(object):\n    def __init__(self, n_classes, n_way, n_episodes):\n        self.n_classes = n_classes\n        self.n_way = n_way\n        self.n_episodes = n_episodes\n\n    def __len__(self):\n        return self.n_episodes\n\n    def __iter__(self):\n        for i in range(self.n_episodes):\n            yield torch.randperm(self.n_classes)[:self.n_way]\n"""
protonets/data/omniglot.py,4,"b'import os\nimport sys\nimport glob\n\nfrom functools import partial\n\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nfrom torchvision.transforms import ToTensor\n\nfrom torchnet.dataset import ListDataset, TransformDataset\nfrom torchnet.transform import compose\n\nimport protonets\nfrom protonets.data.base import convert_dict, CudaTransform, EpisodicBatchSampler, SequentialBatchSampler\n\nOMNIGLOT_DATA_DIR  = os.path.join(os.path.dirname(__file__), \'../../data/omniglot\')\nOMNIGLOT_CACHE = { }\n\ndef load_image_path(key, out_field, d):\n    d[out_field] = Image.open(d[key])\n    return d\n\ndef convert_tensor(key, d):\n    d[key] = 1.0 - torch.from_numpy(np.array(d[key], np.float32, copy=False)).transpose(0, 1).contiguous().view(1, d[key].size[0], d[key].size[1])\n    return d\n\ndef rotate_image(key, rot, d):\n    d[key] = d[key].rotate(rot)\n    return d\n\ndef scale_image(key, height, width, d):\n    d[key] = d[key].resize((height, width))\n    return d\n\ndef load_class_images(d):\n    if d[\'class\'] not in OMNIGLOT_CACHE:\n        alphabet, character, rot = d[\'class\'].split(\'/\')\n        image_dir = os.path.join(OMNIGLOT_DATA_DIR, \'data\', alphabet, character)\n\n        class_images = sorted(glob.glob(os.path.join(image_dir, \'*.png\')))\n        if len(class_images) == 0:\n            raise Exception(""No images found for omniglot class {} at {}. Did you run download_omniglot.sh first?"".format(d[\'class\'], image_dir))\n\n        image_ds = TransformDataset(ListDataset(class_images),\n                                    compose([partial(convert_dict, \'file_name\'),\n                                             partial(load_image_path, \'file_name\', \'data\'),\n                                             partial(rotate_image, \'data\', float(rot[3:])),\n                                             partial(scale_image, \'data\', 28, 28),\n                                             partial(convert_tensor, \'data\')]))\n\n        loader = torch.utils.data.DataLoader(image_ds, batch_size=len(image_ds), shuffle=False)\n\n        for sample in loader:\n            OMNIGLOT_CACHE[d[\'class\']] = sample[\'data\']\n            break # only need one sample because batch size equal to dataset length\n\n    return { \'class\': d[\'class\'], \'data\': OMNIGLOT_CACHE[d[\'class\']] }\n\ndef extract_episode(n_support, n_query, d):\n    # data: N x C x H x W\n    n_examples = d[\'data\'].size(0)\n\n    if n_query == -1:\n        n_query = n_examples - n_support\n\n    example_inds = torch.randperm(n_examples)[:(n_support+n_query)]\n    support_inds = example_inds[:n_support]\n    query_inds = example_inds[n_support:]\n\n    xs = d[\'data\'][support_inds]\n    xq = d[\'data\'][query_inds]\n\n    return {\n        \'class\': d[\'class\'],\n        \'xs\': xs,\n        \'xq\': xq\n    }\n\ndef load(opt, splits):\n    split_dir = os.path.join(OMNIGLOT_DATA_DIR, \'splits\', opt[\'data.split\'])\n\n    ret = { }\n    for split in splits:\n        if split in [\'val\', \'test\'] and opt[\'data.test_way\'] != 0:\n            n_way = opt[\'data.test_way\']\n        else:\n            n_way = opt[\'data.way\']\n\n        if split in [\'val\', \'test\'] and opt[\'data.test_shot\'] != 0:\n            n_support = opt[\'data.test_shot\']\n        else:\n            n_support = opt[\'data.shot\']\n\n        if split in [\'val\', \'test\'] and opt[\'data.test_query\'] != 0:\n            n_query = opt[\'data.test_query\']\n        else:\n            n_query = opt[\'data.query\']\n\n        if split in [\'val\', \'test\']:\n            n_episodes = opt[\'data.test_episodes\']\n        else:\n            n_episodes = opt[\'data.train_episodes\']\n\n        transforms = [partial(convert_dict, \'class\'),\n                      load_class_images,\n                      partial(extract_episode, n_support, n_query)]\n        if opt[\'data.cuda\']:\n            transforms.append(CudaTransform())\n\n        transforms = compose(transforms)\n\n        class_names = []\n        with open(os.path.join(split_dir, ""{:s}.txt"".format(split)), \'r\') as f:\n            for class_name in f.readlines():\n                class_names.append(class_name.rstrip(\'\\n\'))\n        ds = TransformDataset(ListDataset(class_names), transforms)\n\n        if opt[\'data.sequential\']:\n            sampler = SequentialBatchSampler(len(ds))\n        else:\n            sampler = EpisodicBatchSampler(len(ds), n_way, n_episodes)\n\n        # use num_workers=0, otherwise may receive duplicate episodes\n        ret[split] = torch.utils.data.DataLoader(ds, batch_sampler=sampler, num_workers=0)\n\n    return ret\n'"
protonets/models/__init__.py,0,"b'from protonets.models.factory import get_model, register_model\n\nimport protonets.models.few_shot\n'"
protonets/models/factory.py,0,"b'MODEL_REGISTRY = {}\n\ndef register_model(model_name):\n    def decorator(f):\n        MODEL_REGISTRY[model_name] = f\n        return f\n\n    return decorator\n\ndef get_model(model_name, model_opt):\n    if model_name in MODEL_REGISTRY:\n        return MODEL_REGISTRY[model_name](**model_opt)\n    else:\n        raise ValueError(""Unknown model {:s}"".format(model_name))\n'"
protonets/models/few_shot.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\nfrom protonets.models import register_model\n\nfrom .utils import euclidean_dist\n\nclass Flatten(nn.Module):\n    def __init__(self):\n        super(Flatten, self).__init__()\n\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass Protonet(nn.Module):\n    def __init__(self, encoder):\n        super(Protonet, self).__init__()\n        \n        self.encoder = encoder\n\n    def loss(self, sample):\n        xs = Variable(sample['xs']) # support\n        xq = Variable(sample['xq']) # query\n\n        n_class = xs.size(0)\n        assert xq.size(0) == n_class\n        n_support = xs.size(1)\n        n_query = xq.size(1)\n\n        target_inds = torch.arange(0, n_class).view(n_class, 1, 1).expand(n_class, n_query, 1).long()\n        target_inds = Variable(target_inds, requires_grad=False)\n\n        if xq.is_cuda:\n            target_inds = target_inds.cuda()\n\n        x = torch.cat([xs.view(n_class * n_support, *xs.size()[2:]),\n                       xq.view(n_class * n_query, *xq.size()[2:])], 0)\n\n        z = self.encoder.forward(x)\n        z_dim = z.size(-1)\n\n        z_proto = z[:n_class*n_support].view(n_class, n_support, z_dim).mean(1)\n        zq = z[n_class*n_support:]\n\n        dists = euclidean_dist(zq, z_proto)\n\n        log_p_y = F.log_softmax(-dists, dim=1).view(n_class, n_query, -1)\n\n        loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n\n        _, y_hat = log_p_y.max(2)\n        acc_val = torch.eq(y_hat, target_inds.squeeze()).float().mean()\n\n        return loss_val, {\n            'loss': loss_val.item(),\n            'acc': acc_val.item()\n        }\n\n@register_model('protonet_conv')\ndef load_protonet_conv(**kwargs):\n    x_dim = kwargs['x_dim']\n    hid_dim = kwargs['hid_dim']\n    z_dim = kwargs['z_dim']\n\n    def conv_block(in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n\n    encoder = nn.Sequential(\n        conv_block(x_dim[0], hid_dim),\n        conv_block(hid_dim, hid_dim),\n        conv_block(hid_dim, hid_dim),\n        conv_block(hid_dim, z_dim),\n        Flatten()\n    )\n\n    return Protonet(encoder)\n"""
protonets/models/utils.py,1,"b'import torch\n\ndef euclidean_dist(x, y):\n    # x: N x D\n    # y: M x D\n    n = x.size(0)\n    m = y.size(0)\n    d = x.size(1)\n    assert d == y.size(1)\n\n    x = x.unsqueeze(1).expand(n, m, d)\n    y = y.unsqueeze(0).expand(n, m, d)\n\n    return torch.pow(x - y, 2).sum(2)\n'"
protonets/utils/__init__.py,0,"b'def filter_opt(opt, tag):\n    ret = { }\n\n    for k,v in opt.items():\n        tokens = k.split(\'.\')\n        if tokens[0] == tag:\n            ret[\'.\'.join(tokens[1:])] = v\n\n    return ret\n\ndef format_opts(d):\n    ret = []\n    for k,v in d.items():\n        if isinstance(v, bool) and v == True:\n            ret = ret + [""--"" + k]\n        elif isinstance(v, bool) and v == False:\n            pass\n        else:\n            ret = ret + [""--"" + k, str(v)]\n    return ret\n\ndef merge_dict(x, y):\n    ret = x.copy()\n\n    for k,v in y.items():\n        ret[k] = v\n\n    return ret\n'"
protonets/utils/data.py,0,"b'import protonets.data\n\ndef load(opt, splits):\n    if opt[\'data.dataset\'] == \'omniglot\':\n        ds = protonets.data.omniglot.load(opt, splits)\n    else:\n        raise ValueError(""Unknown dataset: {:s}"".format(opt[\'data.dataset\']))\n\n    return ds\n'"
protonets/utils/log.py,0,"b'import json\n\nimport numpy as np\n\ndef extract_meter_values(meters):\n    ret = { }\n\n    for split in meters.keys():\n        ret[split] = { }\n        for field,meter in meters[split].items():\n            ret[split][field] = meter.value()[0]\n\n    return ret\n\ndef render_meter_values(meter_values):\n    field_info = []\n    for split in meter_values.keys():\n        for field,val in meter_values[split].items():\n            field_info.append(""{:s} {:s} = {:0.6f}"".format(split, field, val))\n\n    return \', \'.join(field_info)\n\ndef convert_array(d):\n    ret = { }\n    for k,v in d.items():\n        if isinstance(v, dict):\n            ret[k] = { }\n            for kk,vv in v.items():\n                ret[k][kk] = np.array(vv)\n        else:\n            ret[k] = np.array(v)\n\n    return ret\n\ndef load_trace(trace_file):\n    ret = { }\n\n    with open(trace_file, \'r\') as f:\n        for i,line in enumerate(f):\n            vals = json.loads(line.rstrip(\'\\n\'))\n\n            if i == 0:\n                for k,v in vals.items():\n                    if isinstance(v, dict):\n                        ret[k] = { }\n                        for kk in v.keys():\n                            ret[k][kk] = []\n                    else:\n                        ret[k] = []\n\n            for k,v in vals.items():\n                if isinstance(v, dict):\n                    for kk,vv in v.items():\n                        ret[k][kk].append(vv)\n                else:\n                    ret[k].append(v)\n\n    return convert_array(ret)\n'"
protonets/utils/model.py,0,"b""from tqdm import tqdm\n\nfrom protonets.utils import filter_opt\nfrom protonets.models import get_model\n\ndef load(opt):\n    model_opt = filter_opt(opt, 'model')\n    model_name = model_opt['model_name']\n\n    del model_opt['model_name']\n\n    return get_model(model_name, model_opt)\n\ndef evaluate(model, data_loader, meters, desc=None):\n    model.eval()\n\n    for field,meter in meters.items():\n        meter.reset()\n\n    if desc is not None:\n        data_loader = tqdm(data_loader, desc=desc)\n\n    for sample in data_loader:\n        _, output = model.loss(sample)\n        for field, meter in meters.items():\n            meter.add(output[field])\n\n    return meters\n"""
scripts/predict/few_shot/eval.py,3,"b'import os\nimport json\nimport math\nfrom tqdm import tqdm\n\nimport torch\nimport torchnet as tnt\n\nfrom protonets.utils import filter_opt, merge_dict\nimport protonets.utils.data as data_utils\nimport protonets.utils.model as model_utils\n\ndef main(opt):\n    # load model\n    model = torch.load(opt[\'model.model_path\'])\n    model.eval()\n\n    # load opts\n    model_opt_file = os.path.join(os.path.dirname(opt[\'model.model_path\']), \'opt.json\')\n    with open(model_opt_file, \'r\') as f:\n        model_opt = json.load(f)\n\n    # Postprocess arguments\n    model_opt[\'model.x_dim\'] = map(int, model_opt[\'model.x_dim\'].split(\',\'))\n    model_opt[\'log.fields\'] = model_opt[\'log.fields\'].split(\',\')\n\n    # construct data\n    data_opt = { \'data.\' + k: v for k,v in filter_opt(model_opt, \'data\').items() }\n\n    episode_fields = {\n        \'data.test_way\': \'data.way\',\n        \'data.test_shot\': \'data.shot\',\n        \'data.test_query\': \'data.query\',\n        \'data.test_episodes\': \'data.train_episodes\'\n    }\n\n    for k,v in episode_fields.items():\n        if opt[k] != 0:\n            data_opt[k] = opt[k]\n        elif model_opt[k] != 0:\n            data_opt[k] = model_opt[k]\n        else:\n            data_opt[k] = model_opt[v]\n\n    print(""Evaluating {:d}-way, {:d}-shot with {:d} query examples/class over {:d} episodes"".format(\n        data_opt[\'data.test_way\'], data_opt[\'data.test_shot\'],\n        data_opt[\'data.test_query\'], data_opt[\'data.test_episodes\']))\n\n    torch.manual_seed(1234)\n    if data_opt[\'data.cuda\']:\n        torch.cuda.manual_seed(1234)\n\n    data = data_utils.load(data_opt, [\'test\'])\n\n    if data_opt[\'data.cuda\']:\n        model.cuda()\n\n    meters = { field: tnt.meter.AverageValueMeter() for field in model_opt[\'log.fields\'] }\n\n    model_utils.evaluate(model, data[\'test\'], meters, desc=""test"")\n\n    for field,meter in meters.items():\n        mean, std = meter.value()\n        print(""test {:s}: {:0.6f} +/- {:0.6f}"".format(field, mean, 1.96 * std / math.sqrt(data_opt[\'data.test_episodes\'])))\n'"
scripts/predict/few_shot/run_eval.py,0,"b'import argparse\n\nfrom eval import main\n\nparser = argparse.ArgumentParser(description=\'Evaluate few-shot prototypical networks\')\n\ndefault_model_path = \'results/best_model.pt\'\nparser.add_argument(\'--model.model_path\', type=str, default=default_model_path, metavar=\'MODELPATH\',\n                    help=""location of pretrained model to evaluate (default: {:s})"".format(default_model_path))\n\nparser.add_argument(\'--data.test_way\', type=int, default=0, metavar=\'TESTWAY\',\n                    help=""number of classes per episode in test. 0 means same as model\'s data.test_way (default: 0)"")\nparser.add_argument(\'--data.test_shot\', type=int, default=0, metavar=\'TESTSHOT\',\n                    help=""number of support examples per class in test. 0 means same as model\'s data.shot (default: 0)"")\nparser.add_argument(\'--data.test_query\', type=int, default=0, metavar=\'TESTQUERY\',\n                    help=""number of query examples per class in test. 0 means same as model\'s data.query (default: 0)"")\nparser.add_argument(\'--data.test_episodes\', type=int, default=1000, metavar=\'NTEST\',\n                    help=""number of test episodes per epoch (default: 1000)"")\n\nargs = vars(parser.parse_args())\n\nmain(args)\n'"
scripts/train/few_shot/run_train.py,0,"b'import argparse\n\nfrom train import main\n\nparser = argparse.ArgumentParser(description=\'Train prototypical networks\')\n\n# data args\ndefault_dataset = \'omniglot\'\nparser.add_argument(\'--data.dataset\', type=str, default=default_dataset, metavar=\'DS\',\n                    help=""data set name (default: {:s})"".format(default_dataset))\ndefault_split = \'vinyals\'\nparser.add_argument(\'--data.split\', type=str, default=default_split, metavar=\'SP\',\n                    help=""split name (default: {:s})"".format(default_split))\nparser.add_argument(\'--data.way\', type=int, default=60, metavar=\'WAY\',\n                    help=""number of classes per episode (default: 60)"")\nparser.add_argument(\'--data.shot\', type=int, default=5, metavar=\'SHOT\',\n                    help=""number of support examples per class (default: 5)"")\nparser.add_argument(\'--data.query\', type=int, default=5, metavar=\'QUERY\',\n                    help=""number of query examples per class (default: 5)"")\nparser.add_argument(\'--data.test_way\', type=int, default=5, metavar=\'TESTWAY\',\n                    help=""number of classes per episode in test. 0 means same as data.way (default: 5)"")\nparser.add_argument(\'--data.test_shot\', type=int, default=0, metavar=\'TESTSHOT\',\n                    help=""number of support examples per class in test. 0 means same as data.shot (default: 0)"")\nparser.add_argument(\'--data.test_query\', type=int, default=15, metavar=\'TESTQUERY\',\n                    help=""number of query examples per class in test. 0 means same as data.query (default: 15)"")\nparser.add_argument(\'--data.train_episodes\', type=int, default=100, metavar=\'NTRAIN\',\n                    help=""number of train episodes per epoch (default: 100)"")\nparser.add_argument(\'--data.test_episodes\', type=int, default=100, metavar=\'NTEST\',\n                    help=""number of test episodes per epoch (default: 100)"")\nparser.add_argument(\'--data.trainval\', action=\'store_true\', help=""run in train+validation mode (default: False)"")\nparser.add_argument(\'--data.sequential\', action=\'store_true\', help=""use sequential sampler instead of episodic (default: False)"")\nparser.add_argument(\'--data.cuda\', action=\'store_true\', help=""run in CUDA mode (default: False)"")\n\n# model args\ndefault_model_name = \'protonet_conv\'\nparser.add_argument(\'--model.model_name\', type=str, default=default_model_name, metavar=\'MODELNAME\',\n                    help=""model name (default: {:s})"".format(default_model_name))\nparser.add_argument(\'--model.x_dim\', type=str, default=\'1,28,28\', metavar=\'XDIM\',\n                    help=""dimensionality of input images (default: \'1,28,28\')"")\nparser.add_argument(\'--model.hid_dim\', type=int, default=64, metavar=\'HIDDIM\',\n                    help=""dimensionality of hidden layers (default: 64)"")\nparser.add_argument(\'--model.z_dim\', type=int, default=64, metavar=\'ZDIM\',\n                    help=""dimensionality of input images (default: 64)"")\n\n# train args\nparser.add_argument(\'--train.epochs\', type=int, default=10000, metavar=\'NEPOCHS\',\n                    help=\'number of epochs to train (default: 10000)\')\nparser.add_argument(\'--train.optim_method\', type=str, default=\'Adam\', metavar=\'OPTIM\',\n                    help=\'optimization method (default: Adam)\')\nparser.add_argument(\'--train.learning_rate\', type=float, default=0.001, metavar=\'LR\',\n                    help=\'learning rate (default: 0.0001)\')\nparser.add_argument(\'--train.decay_every\', type=int, default=20, metavar=\'LRDECAY\',\n                    help=\'number of epochs after which to decay the learning rate\')\ndefault_weight_decay = 0.0\nparser.add_argument(\'--train.weight_decay\', type=float, default=default_weight_decay, metavar=\'WD\',\n                    help=""weight decay (default: {:f})"".format(default_weight_decay))\nparser.add_argument(\'--train.patience\', type=int, default=200, metavar=\'PATIENCE\',\n                    help=\'number of epochs to wait before validation improvement (default: 1000)\')\n\n# log args\ndefault_fields = \'loss,acc\'\nparser.add_argument(\'--log.fields\', type=str, default=default_fields, metavar=\'FIELDS\',\n                    help=""fields to monitor during training (default: {:s})"".format(default_fields))\ndefault_exp_dir = \'results\'\nparser.add_argument(\'--log.exp_dir\', type=str, default=default_exp_dir, metavar=\'EXP_DIR\',\n                    help=""directory where experiments should be saved (default: {:s})"".format(default_exp_dir))\n\nargs = vars(parser.parse_args())\n\nmain(args)\n'"
scripts/train/few_shot/run_trainval.py,0,"b'import argparse\n\nfrom trainval import main\n\nparser = argparse.ArgumentParser(description=\'Re-run prototypical networks training in trainval mode\')\n\nparser.add_argument(\'--model.model_path\', type=str, default=\'results/best_model.pt\', metavar=\'MODELPATH\',\n                    help=""location of pretrained model to retrain in trainval mode"")\n\nargs = vars(parser.parse_args())\n\nmain(args)\n'"
scripts/train/few_shot/train.py,6,"b'import os\nimport json\nfrom functools import partial\nfrom tqdm import tqdm\n\nimport numpy as np\n\nimport torch\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler \nimport torchvision\nimport torchnet as tnt\n\nfrom protonets.engine import Engine\n\nimport protonets.utils.data as data_utils\nimport protonets.utils.model as model_utils\nimport protonets.utils.log as log_utils\n\ndef main(opt):\n    if not os.path.isdir(opt[\'log.exp_dir\']):\n        os.makedirs(opt[\'log.exp_dir\'])\n\n    # save opts\n    with open(os.path.join(opt[\'log.exp_dir\'], \'opt.json\'), \'w\') as f:\n        json.dump(opt, f)\n        f.write(\'\\n\')\n\n    trace_file = os.path.join(opt[\'log.exp_dir\'], \'trace.txt\')\n\n    # Postprocess arguments\n    opt[\'model.x_dim\'] = list(map(int, opt[\'model.x_dim\'].split(\',\')))\n    opt[\'log.fields\'] = opt[\'log.fields\'].split(\',\')\n\n    torch.manual_seed(1234)\n    if opt[\'data.cuda\']:\n        torch.cuda.manual_seed(1234)\n\n    if opt[\'data.trainval\']:\n        data = data_utils.load(opt, [\'trainval\'])\n        train_loader = data[\'trainval\']\n        val_loader = None\n    else:\n        data = data_utils.load(opt, [\'train\', \'val\'])\n        train_loader = data[\'train\']\n        val_loader = data[\'val\']\n\n    model = model_utils.load(opt)\n\n    if opt[\'data.cuda\']:\n        model.cuda()\n\n    engine = Engine()\n\n    meters = { \'train\': { field: tnt.meter.AverageValueMeter() for field in opt[\'log.fields\'] } }\n\n    if val_loader is not None:\n        meters[\'val\'] = { field: tnt.meter.AverageValueMeter() for field in opt[\'log.fields\'] }\n\n    def on_start(state):\n        if os.path.isfile(trace_file):\n            os.remove(trace_file)\n        state[\'scheduler\'] = lr_scheduler.StepLR(state[\'optimizer\'], opt[\'train.decay_every\'], gamma=0.5)\n    engine.hooks[\'on_start\'] = on_start\n\n    def on_start_epoch(state):\n        for split, split_meters in meters.items():\n            for field, meter in split_meters.items():\n                meter.reset()\n        state[\'scheduler\'].step()\n    engine.hooks[\'on_start_epoch\'] = on_start_epoch\n\n    def on_update(state):\n        for field, meter in meters[\'train\'].items():\n            meter.add(state[\'output\'][field])\n    engine.hooks[\'on_update\'] = on_update\n\n    def on_end_epoch(hook_state, state):\n        if val_loader is not None:\n            if \'best_loss\' not in hook_state:\n                hook_state[\'best_loss\'] = np.inf\n            if \'wait\' not in hook_state:\n                hook_state[\'wait\'] = 0\n\n        if val_loader is not None:\n            model_utils.evaluate(state[\'model\'],\n                                 val_loader,\n                                 meters[\'val\'],\n                                 desc=""Epoch {:d} valid"".format(state[\'epoch\']))\n\n        meter_vals = log_utils.extract_meter_values(meters)\n        print(""Epoch {:02d}: {:s}"".format(state[\'epoch\'], log_utils.render_meter_values(meter_vals)))\n        meter_vals[\'epoch\'] = state[\'epoch\']\n        with open(trace_file, \'a\') as f:\n            json.dump(meter_vals, f)\n            f.write(\'\\n\')\n\n        if val_loader is not None:\n            if meter_vals[\'val\'][\'loss\'] < hook_state[\'best_loss\']:\n                hook_state[\'best_loss\'] = meter_vals[\'val\'][\'loss\']\n                print(""==> best model (loss = {:0.6f}), saving model..."".format(hook_state[\'best_loss\']))\n\n                state[\'model\'].cpu()\n                torch.save(state[\'model\'], os.path.join(opt[\'log.exp_dir\'], \'best_model.pt\'))\n                if opt[\'data.cuda\']:\n                    state[\'model\'].cuda()\n\n                hook_state[\'wait\'] = 0\n            else:\n                hook_state[\'wait\'] += 1\n\n                if hook_state[\'wait\'] > opt[\'train.patience\']:\n                    print(""==> patience {:d} exceeded"".format(opt[\'train.patience\']))\n                    state[\'stop\'] = True\n        else:\n            state[\'model\'].cpu()\n            torch.save(state[\'model\'], os.path.join(opt[\'log.exp_dir\'], \'best_model.pt\'))\n            if opt[\'data.cuda\']:\n                state[\'model\'].cuda()\n\n    engine.hooks[\'on_end_epoch\'] = partial(on_end_epoch, { })\n\n    engine.train(\n        model = model,\n        loader = train_loader,\n        optim_method = getattr(optim, opt[\'train.optim_method\']),\n        optim_config = { \'lr\': opt[\'train.learning_rate\'],\n                         \'weight_decay\': opt[\'train.weight_decay\'] },\n        max_epoch = opt[\'train.epochs\']\n    )\n'"
scripts/train/few_shot/trainval.py,0,"b""import os\nimport json\nimport subprocess\n\nfrom protonets.utils import format_opts, merge_dict\nfrom protonets.utils.log import load_trace\n\ndef main(opt):\n    result_dir = os.path.dirname(opt['model.model_path'])\n\n    # get target training loss to exceed\n    trace_file = os.path.join(result_dir, 'trace.txt')\n    trace_vals = load_trace(trace_file)\n    best_epoch = trace_vals['val']['loss'].argmin()\n\n    # load opts\n    model_opt_file = os.path.join(os.path.dirname(opt['model.model_path']), 'opt.json')\n    with open(model_opt_file, 'r') as f:\n        model_opt = json.load(f)\n\n    # override previous training ops\n    model_opt = merge_dict(model_opt, {\n        'log.exp_dir': os.path.join(model_opt['log.exp_dir'], 'trainval'),\n        'data.trainval': True,\n        'train.epochs': best_epoch + model_opt['train.patience'],\n    })\n\n    subprocess.call(['python', os.path.join(os.getcwd(), 'scripts/train/few_shot/run_train.py')] + format_opts(model_opt))\n"""
