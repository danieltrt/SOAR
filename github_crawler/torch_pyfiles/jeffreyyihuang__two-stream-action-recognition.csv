file_path,api_count,code
average_fusion.py,2,"b""from matplotlib import pyplot as plt\nimport pickle\nimport numpy as np\nimport torch\nfrom utils import *\nimport dataloader\n\nif __name__ == '__main__':\n\n    rgb_preds='record/spatial/spatial_video_preds.pickle'\n    opf_preds = 'record/motion/motion_video_preds.pickle'\n\n    with open(rgb_preds,'rb') as f:\n        rgb =pickle.load(f)\n    f.close()\n    with open(opf_preds,'rb') as f:\n        opf =pickle.load(f)\n    f.close()\n\n    dataloader = dataloader.spatial_dataloader(BATCH_SIZE=1, num_workers=1, \n                                    path='/home/ubuntu/data/UCF101/spatial_no_sampled/', \n                                    ucf_list='/home/ubuntu/cvlab/pytorch/ucf101_two_stream/github/UCF_list/',\n                                    ucf_split='01')\n    train_loader,val_loader,test_video = dataloader.run()\n\n    video_level_preds = np.zeros((len(rgb.keys()),101))\n    video_level_labels = np.zeros(len(rgb.keys()))\n    correct=0\n    ii=0\n    for name in sorted(rgb.keys()):   \n        r = rgb[name]\n        o = opf[name]\n\n        label = int(test_video[name])-1\n                    \n        video_level_preds[ii,:] = (r+o)\n        video_level_labels[ii] = label\n        ii+=1         \n        if np.argmax(r+o) == (label):\n            correct+=1\n\n    video_level_labels = torch.from_numpy(video_level_labels).long()\n    video_level_preds = torch.from_numpy(video_level_preds).float()\n        \n    top1,top5 = accuracy(video_level_preds, video_level_labels, topk=(1,5))     \n                                \n    print top1,top5\n"""
motion_cnn.py,9,"b'import numpy as np\nimport pickle\nfrom PIL import Image\nimport time\nimport tqdm\nimport shutil\nfrom random import randint\nimport argparse\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom utils import *\nfrom network import *\nimport dataloader\n\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""1""\n\nparser = argparse.ArgumentParser(description=\'UCF101 motion stream on resnet101\')\nparser.add_argument(\'--epochs\', default=500, type=int, metavar=\'N\', help=\'number of total epochs\')\nparser.add_argument(\'--batch-size\', default=64, type=int, metavar=\'N\', help=\'mini-batch size (default: 64)\')\nparser.add_argument(\'--lr\', default=1e-2, type=float, metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--evaluate\', dest=\'evaluate\', action=\'store_true\', help=\'evaluate model on validation set\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\', help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\', help=\'manual epoch number (useful on restarts)\')\n\ndef main():\n    global arg\n    arg = parser.parse_args()\n    print arg\n\n    #Prepare DataLoader\n    data_loader = dataloader.Motion_DataLoader(\n                        BATCH_SIZE=arg.batch_size,\n                        num_workers=8,\n                        path=\'/home/ubuntu/data/UCF101/tvl1_flow/\',\n                        ucf_list=\'/home/ubuntu/cvlab/pytorch/ucf101_two_stream/github/UCF_list/\',\n                        ucf_split=\'01\',\n                        in_channel=10,\n                        )\n    \n    train_loader,test_loader, test_video = data_loader.run()\n    #Model \n    model = Motion_CNN(\n                        # Data Loader\n                        train_loader=train_loader,\n                        test_loader=test_loader,\n                        # Utility\n                        start_epoch=arg.start_epoch,\n                        resume=arg.resume,\n                        evaluate=arg.evaluate,\n                        # Hyper-parameter\n                        nb_epochs=arg.epochs,\n                        lr=arg.lr,\n                        batch_size=arg.batch_size,\n                        channel = 10*2,\n                        test_video=test_video\n                        )\n    #Training\n    model.run()\n\nclass Motion_CNN():\n    def __init__(self, nb_epochs, lr, batch_size, resume, start_epoch, evaluate, train_loader, test_loader, channel,test_video):\n        self.nb_epochs=nb_epochs\n        self.lr=lr\n        self.batch_size=batch_size\n        self.resume=resume\n        self.start_epoch=start_epoch\n        self.evaluate=evaluate\n        self.train_loader=train_loader\n        self.test_loader=test_loader\n        self.best_prec1=0\n        self.channel=channel\n        self.test_video=test_video\n\n    def build_model(self):\n        print (\'==> Build model and setup loss and optimizer\')\n        #build model\n        self.model = resnet101(pretrained= True, channel=self.channel).cuda()\n        #print self.model\n        #Loss function and optimizer\n        self.criterion = nn.CrossEntropyLoss().cuda()\n        self.optimizer = torch.optim.SGD(self.model.parameters(), self.lr, momentum=0.9)\n        self.scheduler = ReduceLROnPlateau(self.optimizer, \'min\', patience=1,verbose=True)\n\n    def resume_and_evaluate(self):\n        if self.resume:\n            if os.path.isfile(self.resume):\n                print(""==> loading checkpoint \'{}\'"".format(self.resume))\n                checkpoint = torch.load(self.resume)\n                self.start_epoch = checkpoint[\'epoch\']\n                self.best_prec1 = checkpoint[\'best_prec1\']\n                self.model.load_state_dict(checkpoint[\'state_dict\'])\n                self.optimizer.load_state_dict(checkpoint[\'optimizer\'])\n                print(""==> loaded checkpoint \'{}\' (epoch {}) (best_prec1 {})""\n                  .format(self.resume, checkpoint[\'epoch\'], self.best_prec1))\n            else:\n                print(""==> no checkpoint found at \'{}\'"".format(self.resume))\n        if self.evaluate:\n            self.epoch=0\n            prec1, val_loss = self.validate_1epoch()\n            return\n    \n    def run(self):\n        self.build_model()\n        self.resume_and_evaluate()\n        cudnn.benchmark = True\n        \n        for self.epoch in range(self.start_epoch, self.nb_epochs):\n            self.train_1epoch()\n            prec1, val_loss = self.validate_1epoch()\n            is_best = prec1 > self.best_prec1\n            #lr_scheduler\n            self.scheduler.step(val_loss)\n            # save model\n            if is_best:\n                self.best_prec1 = prec1\n                with open(\'record/motion/motion_video_preds.pickle\',\'wb\') as f:\n                    pickle.dump(self.dic_video_level_preds,f)\n                f.close() \n            \n            save_checkpoint({\n                \'epoch\': self.epoch,\n                \'state_dict\': self.model.state_dict(),\n                \'best_prec1\': self.best_prec1,\n                \'optimizer\' : self.optimizer.state_dict()\n            },is_best,\'record/motion/checkpoint.pth.tar\',\'record/motion/model_best.pth.tar\')\n\n    def train_1epoch(self):\n        print(\'==> Epoch:[{0}/{1}][training stage]\'.format(self.epoch, self.nb_epochs))\n\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n        #switch to train mode\n        self.model.train()    \n        end = time.time()\n        # mini-batch training\n        progress = tqdm(self.train_loader)\n        for i, (data,label) in enumerate(progress):\n\n            # measure data loading time\n            data_time.update(time.time() - end)\n            \n            label = label.cuda(async=True)\n            input_var = Variable(data).cuda()\n            target_var = Variable(label).cuda()\n\n            # compute output\n            output = self.model(input_var)\n            loss = self.criterion(output, target_var)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(output.data, label, topk=(1, 5))\n            losses.update(loss.data[0], data.size(0))\n            top1.update(prec1[0], data.size(0))\n            top5.update(prec5[0], data.size(0))\n\n            # compute gradient and do SGD step\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n        \n        info = {\'Epoch\':[self.epoch],\n                \'Batch Time\':[round(batch_time.avg,3)],\n                \'Data Time\':[round(data_time.avg,3)],\n                \'Loss\':[round(losses.avg,5)],\n                \'Prec@1\':[round(top1.avg,4)],\n                \'Prec@5\':[round(top5.avg,4)],\n                \'lr\': self.optimizer.param_groups[0][\'lr\']\n                }\n        record_info(info, \'record/motion/opf_train.csv\',\'train\')\n\n    def validate_1epoch(self):\n        print(\'==> Epoch:[{0}/{1}][validation stage]\'.format(self.epoch, self.nb_epochs))\n\n        batch_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n        # switch to evaluate mode\n        self.model.eval()\n        self.dic_video_level_preds={}\n        end = time.time()\n        progress = tqdm(self.test_loader)\n        for i, (keys,data,label) in enumerate(progress):\n            \n            #data = data.sub_(127.353346189).div_(14.971742063)\n            label = label.cuda(async=True)\n            data_var = Variable(data, volatile=True).cuda(async=True)\n            label_var = Variable(label, volatile=True).cuda(async=True)\n\n            # compute output\n            output = self.model(data_var)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n            #Calculate video level prediction\n            preds = output.data.cpu().numpy()\n            nb_data = preds.shape[0]\n            for j in range(nb_data):\n                videoName = keys[j].split(\'-\',1)[0] # ApplyMakeup_g01_c01\n                if videoName not in self.dic_video_level_preds.keys():\n                    self.dic_video_level_preds[videoName] = preds[j,:]\n                else:\n                    self.dic_video_level_preds[videoName] += preds[j,:]\n                    \n        #Frame to video level accuracy\n        video_top1, video_top5, video_loss = self.frame2_video_level_accuracy()\n        info = {\'Epoch\':[self.epoch],\n                \'Batch Time\':[round(batch_time.avg,3)],\n                \'Loss\':[round(video_loss,5)],\n                \'Prec@1\':[round(video_top1,3)],\n                \'Prec@5\':[round(video_top5,3)]\n                }\n        record_info(info, \'record/motion/opf_test.csv\',\'test\')\n        return video_top1, video_loss\n\n    def frame2_video_level_accuracy(self):\n     \n        correct = 0\n        video_level_preds = np.zeros((len(self.dic_video_level_preds),101))\n        video_level_labels = np.zeros(len(self.dic_video_level_preds))\n        ii=0\n        for key in sorted(self.dic_video_level_preds.keys()):\n            name = key.split(\'-\',1)[0]\n\n            preds = self.dic_video_level_preds[name]\n            label = int(self.test_video[name])-1\n                \n            video_level_preds[ii,:] = preds\n            video_level_labels[ii] = label\n            ii+=1         \n            if np.argmax(preds) == (label):\n                correct+=1\n\n        #top1 top5\n        video_level_labels = torch.from_numpy(video_level_labels).long()\n        video_level_preds = torch.from_numpy(video_level_preds).float()\n\n        loss = self.criterion(Variable(video_level_preds).cuda(), Variable(video_level_labels).cuda())    \n        top1,top5 = accuracy(video_level_preds, video_level_labels, topk=(1,5))     \n                            \n        top1 = float(top1.numpy())\n        top5 = float(top5.numpy())\n            \n        return top1,top5,loss.data.cpu().numpy()\n\nif __name__==\'__main__\':\n    main()'"
network.py,11,"b'import torch.nn as nn\nimport math\nimport numpy as np\nimport torch.utils.model_zoo as model_zoo\nimport torch\nfrom torch.autograd import Variable\n\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, nb_classes=101, channel=20):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1_custom = nn.Conv2d(channel, 64, kernel_size=7, stride=2, padding=3,   \n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc_custom = nn.Linear(512 * block.expansion, nb_classes)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1_custom(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        out = self.fc_custom(x)\n        return out\n\n\ndef resnet18(pretrained=False, channel= 20, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], nb_classes=101, channel=channel, **kwargs)\n    if pretrained:\n       pretrain_dict = model_zoo.load_url(model_urls[\'resnet18\'])                  # modify pretrain code\n       model_dict = model.state_dict()\n       model_dict=weight_transform(model_dict, pretrain_dict, channel)\n       model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet34(pretrained=False, channel= 20, **kwargs):\n\n    model = ResNet(BasicBlock, [3, 4, 6, 3], nb_classes=101, channel=channel, **kwargs)\n    if pretrained:\n       pretrain_dict = model_zoo.load_url(model_urls[\'resnet34\'])                  # modify pretrain code\n       model_dict = model.state_dict()\n       model_dict=weight_transform(model_dict, pretrain_dict, channel)\n       model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet50(pretrained=False, channel= 20, **kwargs):\n\n    model = ResNet(Bottleneck, [3, 4, 6, 3], nb_classes=101, channel=channel, **kwargs)\n    if pretrained:\n       pretrain_dict = model_zoo.load_url(model_urls[\'resnet50\'])                  # modify pretrain code\n       model_dict = model.state_dict()\n       model_dict=weight_transform(model_dict, pretrain_dict, channel)\n       model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet101(pretrained=False, channel= 20, **kwargs):\n\n    model = ResNet(Bottleneck, [3, 4, 23, 3],nb_classes=101, channel=channel, **kwargs)\n    if pretrained:\n       pretrain_dict = model_zoo.load_url(model_urls[\'resnet101\'])                  # modify pretrain code\n       model_dict = model.state_dict()\n       model_dict=weight_transform(model_dict, pretrain_dict, channel)\n       model.load_state_dict(model_dict)\n\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\ndef cross_modality_pretrain(conv1_weight, channel):\n    # transform the original 3 channel weight to ""channel"" channel\n    S=0\n    for i in range(3):\n        S += conv1_weight[:,i,:,:]\n    avg = S/3.\n    new_conv1_weight = torch.FloatTensor(64,channel,7,7)\n    #print type(avg),type(new_conv1_weight)\n    for i in range(channel):\n        new_conv1_weight[:,i,:,:] = avg.data\n    return new_conv1_weight\n\ndef weight_transform(model_dict, pretrain_dict, channel):\n    weight_dict  = {k:v for k, v in pretrain_dict.items() if k in model_dict}\n    #print pretrain_dict.keys()\n    w3 = pretrain_dict[\'conv1.weight\']\n    #print type(w3)\n    if channel == 3:\n        wt = w3\n    else:\n        wt = cross_modality_pretrain(w3,channel)\n\n    weight_dict[\'conv1_custom.weight\'] = wt\n    model_dict.update(weight_dict)\n    return model_dict\n\n#Test network\nif __name__ == \'__main__\':\n    model = resnet34(pretrained= True, channel=10)\n    print model\n     '"
spatial_cnn.py,9,"b'import numpy as np\nimport pickle\nimport os\nfrom PIL import Image\nimport time\nfrom tqdm import tqdm\nimport shutil\nfrom random import randint\nimport argparse\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport dataloader\nfrom utils import *\nfrom network import *\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\nparser = argparse.ArgumentParser(description=\'UCF101 spatial stream on resnet101\')\nparser.add_argument(\'--epochs\', default=500, type=int, metavar=\'N\', help=\'number of total epochs\')\nparser.add_argument(\'--batch-size\', default=25, type=int, metavar=\'N\', help=\'mini-batch size (default: 25)\')\nparser.add_argument(\'--lr\', default=5e-4, type=float, metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--evaluate\', dest=\'evaluate\', action=\'store_true\', help=\'evaluate model on validation set\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\', help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\', help=\'manual epoch number (useful on restarts)\')\n\ndef main():\n    global arg\n    arg = parser.parse_args()\n    print arg\n\n    #Prepare DataLoader\n    data_loader = dataloader.spatial_dataloader(\n                        BATCH_SIZE=arg.batch_size,\n                        num_workers=8,\n                        path=\'/home/ubuntu/data/UCF101/spatial_no_sampled/\',\n                        ucf_list =\'/home/ubuntu/cvlab/pytorch/ucf101_two_stream/github/UCF_list/\',\n                        ucf_split =\'01\', \n                        )\n    \n    train_loader, test_loader, test_video = data_loader.run()\n    #Model \n    model = Spatial_CNN(\n                        nb_epochs=arg.epochs,\n                        lr=arg.lr,\n                        batch_size=arg.batch_size,\n                        resume=arg.resume,\n                        start_epoch=arg.start_epoch,\n                        evaluate=arg.evaluate,\n                        train_loader=train_loader,\n                        test_loader=test_loader,\n                        test_video=test_video\n    )\n    #Training\n    model.run()\n\nclass Spatial_CNN():\n    def __init__(self, nb_epochs, lr, batch_size, resume, start_epoch, evaluate, train_loader, test_loader, test_video):\n        self.nb_epochs=nb_epochs\n        self.lr=lr\n        self.batch_size=batch_size\n        self.resume=resume\n        self.start_epoch=start_epoch\n        self.evaluate=evaluate\n        self.train_loader=train_loader\n        self.test_loader=test_loader\n        self.best_prec1=0\n        self.test_video=test_video\n\n    def build_model(self):\n        print (\'==> Build model and setup loss and optimizer\')\n        #build model\n        self.model = resnet101(pretrained= True, channel=3).cuda()\n        #Loss function and optimizer\n        self.criterion = nn.CrossEntropyLoss().cuda()\n        self.optimizer = torch.optim.SGD(self.model.parameters(), self.lr, momentum=0.9)\n        self.scheduler = ReduceLROnPlateau(self.optimizer, \'min\', patience=1,verbose=True)\n    \n    def resume_and_evaluate(self):\n        if self.resume:\n            if os.path.isfile(self.resume):\n                print(""==> loading checkpoint \'{}\'"".format(self.resume))\n                checkpoint = torch.load(self.resume)\n                self.start_epoch = checkpoint[\'epoch\']\n                self.best_prec1 = checkpoint[\'best_prec1\']\n                self.model.load_state_dict(checkpoint[\'state_dict\'])\n                self.optimizer.load_state_dict(checkpoint[\'optimizer\'])\n                print(""==> loaded checkpoint \'{}\' (epoch {}) (best_prec1 {})""\n                  .format(self.resume, checkpoint[\'epoch\'], self.best_prec1))\n            else:\n                print(""==> no checkpoint found at \'{}\'"".format(self.resume))\n        if self.evaluate:\n            self.epoch = 0\n            prec1, val_loss = self.validate_1epoch()\n            return\n\n    def run(self):\n        self.build_model()\n        self.resume_and_evaluate()\n        cudnn.benchmark = True\n        \n        for self.epoch in range(self.start_epoch, self.nb_epochs):\n            self.train_1epoch()\n            prec1, val_loss = self.validate_1epoch()\n            is_best = prec1 > self.best_prec1\n            #lr_scheduler\n            self.scheduler.step(val_loss)\n            # save model\n            if is_best:\n                self.best_prec1 = prec1\n                with open(\'record/spatial/spatial_video_preds.pickle\',\'wb\') as f:\n                    pickle.dump(self.dic_video_level_preds,f)\n                f.close()\n            \n            save_checkpoint({\n                \'epoch\': self.epoch,\n                \'state_dict\': self.model.state_dict(),\n                \'best_prec1\': self.best_prec1,\n                \'optimizer\' : self.optimizer.state_dict()\n            },is_best,\'record/spatial/checkpoint.pth.tar\',\'record/spatial/model_best.pth.tar\')\n\n    def train_1epoch(self):\n        print(\'==> Epoch:[{0}/{1}][training stage]\'.format(self.epoch, self.nb_epochs))\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n        #switch to train mode\n        self.model.train()    \n        end = time.time()\n        # mini-batch training\n        progress = tqdm(self.train_loader)\n        for i, (data_dict,label) in enumerate(progress):\n\n    \n            # measure data loading time\n            data_time.update(time.time() - end)\n            \n            label = label.cuda(async=True)\n            target_var = Variable(label).cuda()\n\n            # compute output\n            output = Variable(torch.zeros(len(data_dict[\'img1\']),101).float()).cuda()\n            for i in range(len(data_dict)):\n                key = \'img\'+str(i)\n                data = data_dict[key]\n                input_var = Variable(data).cuda()\n                output += self.model(input_var)\n\n            loss = self.criterion(output, target_var)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(output.data, label, topk=(1, 5))\n            losses.update(loss.data[0], data.size(0))\n            top1.update(prec1[0], data.size(0))\n            top5.update(prec5[0], data.size(0))\n\n            # compute gradient and do SGD step\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n        \n        info = {\'Epoch\':[self.epoch],\n                \'Batch Time\':[round(batch_time.avg,3)],\n                \'Data Time\':[round(data_time.avg,3)],\n                \'Loss\':[round(losses.avg,5)],\n                \'Prec@1\':[round(top1.avg,4)],\n                \'Prec@5\':[round(top5.avg,4)],\n                \'lr\': self.optimizer.param_groups[0][\'lr\']\n                }\n        record_info(info, \'record/spatial/rgb_train.csv\',\'train\')\n\n    def validate_1epoch(self):\n        print(\'==> Epoch:[{0}/{1}][validation stage]\'.format(self.epoch, self.nb_epochs))\n        batch_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n        # switch to evaluate mode\n        self.model.eval()\n        self.dic_video_level_preds={}\n        end = time.time()\n        progress = tqdm(self.test_loader)\n        for i, (keys,data,label) in enumerate(progress):\n            \n            label = label.cuda(async=True)\n            data_var = Variable(data, volatile=True).cuda(async=True)\n            label_var = Variable(label, volatile=True).cuda(async=True)\n\n            # compute output\n            output = self.model(data_var)\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n            #Calculate video level prediction\n            preds = output.data.cpu().numpy()\n            nb_data = preds.shape[0]\n            for j in range(nb_data):\n                videoName = keys[j].split(\'/\',1)[0]\n                if videoName not in self.dic_video_level_preds.keys():\n                    self.dic_video_level_preds[videoName] = preds[j,:]\n                else:\n                    self.dic_video_level_preds[videoName] += preds[j,:]\n\n        video_top1, video_top5, video_loss = self.frame2_video_level_accuracy()\n            \n\n        info = {\'Epoch\':[self.epoch],\n                \'Batch Time\':[round(batch_time.avg,3)],\n                \'Loss\':[round(video_loss,5)],\n                \'Prec@1\':[round(video_top1,3)],\n                \'Prec@5\':[round(video_top5,3)]}\n        record_info(info, \'record/spatial/rgb_test.csv\',\'test\')\n        return video_top1, video_loss\n\n    def frame2_video_level_accuracy(self):\n            \n        correct = 0\n        video_level_preds = np.zeros((len(self.dic_video_level_preds),101))\n        video_level_labels = np.zeros(len(self.dic_video_level_preds))\n        ii=0\n        for name in sorted(self.dic_video_level_preds.keys()):\n        \n            preds = self.dic_video_level_preds[name]\n            label = int(self.test_video[name])-1\n                \n            video_level_preds[ii,:] = preds\n            video_level_labels[ii] = label\n            ii+=1         \n            if np.argmax(preds) == (label):\n                correct+=1\n\n        #top1 top5\n        video_level_labels = torch.from_numpy(video_level_labels).long()\n        video_level_preds = torch.from_numpy(video_level_preds).float()\n            \n        top1,top5 = accuracy(video_level_preds, video_level_labels, topk=(1,5))\n        loss = self.criterion(Variable(video_level_preds).cuda(), Variable(video_level_labels).cuda())     \n                            \n        top1 = float(top1.numpy())\n        top5 = float(top5.numpy())\n            \n        #print(\' * Video level Prec@1 {top1:.3f}, Video level Prec@5 {top5:.3f}\'.format(top1=top1, top5=top5))\n        return top1,top5,loss.data.cpu().numpy()\n\n\n\n\n\n\n\nif __name__==\'__main__\':\n    main()'"
utils.py,5,"b'import pickle,os\nfrom PIL import Image\nimport scipy.io\nimport time\nfrom tqdm import tqdm\nimport pandas as pd\nimport shutil\nfrom random import randint\nimport numpy as np\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\n\n# other util\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef save_checkpoint(state, is_best, checkpoint, model_best):\n    torch.save(state, checkpoint)\n    if is_best:\n        shutil.copyfile(checkpoint, model_best)\n\ndef record_info(info,filename,mode):\n\n    if mode ==\'train\':\n\n        result = (\n              \'Time {batch_time} \'\n              \'Data {data_time} \\n\'\n              \'Loss {loss} \'\n              \'Prec@1 {top1} \'\n              \'Prec@5 {top5}\\n\'\n              \'LR {lr}\\n\'.format(batch_time=info[\'Batch Time\'],\n               data_time=info[\'Data Time\'], loss=info[\'Loss\'], top1=info[\'Prec@1\'], top5=info[\'Prec@5\'],lr=info[\'lr\']))      \n        print result\n\n        df = pd.DataFrame.from_dict(info)\n        column_names = [\'Epoch\',\'Batch Time\',\'Data Time\',\'Loss\',\'Prec@1\',\'Prec@5\',\'lr\']\n        \n    if mode ==\'test\':\n        result = (\n              \'Time {batch_time} \\n\'\n              \'Loss {loss} \'\n              \'Prec@1 {top1} \'\n              \'Prec@5 {top5} \\n\'.format( batch_time=info[\'Batch Time\'],\n               loss=info[\'Loss\'], top1=info[\'Prec@1\'], top5=info[\'Prec@5\']))      \n        print result\n        df = pd.DataFrame.from_dict(info)\n        column_names = [\'Epoch\',\'Batch Time\',\'Loss\',\'Prec@1\',\'Prec@5\']\n    \n    if not os.path.isfile(filename):\n        df.to_csv(filename,index=False,columns=column_names)\n    else: # else it exists so append without writing the header\n        df.to_csv(filename,mode = \'a\',header=False,index=False,columns=column_names)   \n\n\n'"
dataloader/__init__.py,0,b'from .motion_dataloader import *\nfrom .spatial_dataloader import *'
dataloader/motion_dataloader.py,6,"b""import numpy as np\nimport pickle\nfrom PIL import Image\nimport time\nimport shutil\nimport random\nimport argparse\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom split_train_test_video import *\n \nclass motion_dataset(Dataset):  \n    def __init__(self, dic, in_channel, root_dir, mode, transform=None):\n        #Generate a 16 Frame clip\n        self.keys=dic.keys()\n        self.values=dic.values()\n        self.root_dir = root_dir\n        self.transform = transform\n        self.mode=mode\n        self.in_channel = in_channel\n        self.img_rows=224\n        self.img_cols=224\n\n    def stackopf(self):\n        name = 'v_'+self.video\n        u = self.root_dir+ 'u/' + name\n        v = self.root_dir+ 'v/'+ name\n        \n        flow = torch.FloatTensor(2*self.in_channel,self.img_rows,self.img_cols)\n        i = int(self.clips_idx)\n\n\n        for j in range(self.in_channel):\n            idx = i + j\n            idx = str(idx)\n            frame_idx = 'frame'+ idx.zfill(6)\n            h_image = u +'/' + frame_idx +'.jpg'\n            v_image = v +'/' + frame_idx +'.jpg'\n            \n            imgH=(Image.open(h_image))\n            imgV=(Image.open(v_image))\n\n            H = self.transform(imgH)\n            V = self.transform(imgV)\n\n            \n            flow[2*(j-1),:,:] = H\n            flow[2*(j-1)+1,:,:] = V      \n            imgH.close()\n            imgV.close()  \n        return flow\n\n    def __len__(self):\n        return len(self.keys)\n\n    def __getitem__(self, idx):\n        #print ('mode:',self.mode,'calling Dataset:__getitem__ @ idx=%d'%idx)\n        \n        if self.mode == 'train':\n            self.video, nb_clips = self.keys[idx].split('-')\n            self.clips_idx = random.randint(1,int(nb_clips))\n        elif self.mode == 'val':\n            self.video,self.clips_idx = self.keys[idx].split('-')\n        else:\n            raise ValueError('There are only train and val mode')\n\n        label = self.values[idx]\n        label = int(label)-1 \n        data = self.stackopf()\n\n        if self.mode == 'train':\n            sample = (data,label)\n        elif self.mode == 'val':\n            sample = (self.video,data,label)\n        else:\n            raise ValueError('There are only train and val mode')\n        return sample\n\n\n\n\n\nclass Motion_DataLoader():\n    def __init__(self, BATCH_SIZE, num_workers, in_channel,  path, ucf_list, ucf_split):\n\n        self.BATCH_SIZE=BATCH_SIZE\n        self.num_workers = num_workers\n        self.frame_count={}\n        self.in_channel = in_channel\n        self.data_path=path\n        # split the training and testing videos\n        splitter = UCF101_splitter(path=ucf_list,split=ucf_split)\n        self.train_video, self.test_video = splitter.split_video()\n        \n    def load_frame_count(self):\n        #print '==> Loading frame number of each video'\n        with open('dic/frame_count.pickle','rb') as file:\n            dic_frame = pickle.load(file)\n        file.close()\n\n        for line in dic_frame :\n            videoname = line.split('_',1)[1].split('.',1)[0]\n            n,g = videoname.split('_',1)\n            if n == 'HandStandPushups':\n                videoname = 'HandstandPushups_'+ g\n            self.frame_count[videoname]=dic_frame[line] \n\n    def run(self):\n        self.load_frame_count()\n        self.get_training_dic()\n        self.val_sample19()\n        train_loader = self.train()\n        val_loader = self.val()\n\n        return train_loader, val_loader, self.test_video\n            \n    def val_sample19(self):\n        self.dic_test_idx = {}\n        #print len(self.test_video)\n        for video in self.test_video:\n            n,g = video.split('_',1)\n\n            sampling_interval = int((self.frame_count[video]-10+1)/19)\n            for index in range(19):\n                clip_idx = index*sampling_interval\n                key = video + '-' + str(clip_idx+1)\n                self.dic_test_idx[key] = self.test_video[video]\n             \n    def get_training_dic(self):\n        self.dic_video_train={}\n        for video in self.train_video:\n            \n            nb_clips = self.frame_count[video]-10+1\n            key = video +'-' + str(nb_clips)\n            self.dic_video_train[key] = self.train_video[video] \n                            \n    def train(self):\n        training_set = motion_dataset(dic=self.dic_video_train, in_channel=self.in_channel, root_dir=self.data_path,\n            mode='train',\n            transform = transforms.Compose([\n            transforms.Scale([224,224]),\n            transforms.ToTensor(),\n            ]))\n        print '==> Training data :',len(training_set),' videos',training_set[1][0].size()\n\n        train_loader = DataLoader(\n            dataset=training_set, \n            batch_size=self.BATCH_SIZE,\n            shuffle=True,\n            num_workers=self.num_workers,\n            pin_memory=True\n            )\n\n        return train_loader\n\n    def val(self):\n        validation_set = motion_dataset(dic= self.dic_test_idx, in_channel=self.in_channel, root_dir=self.data_path ,\n            mode ='val',\n            transform = transforms.Compose([\n            transforms.Scale([224,224]),\n            transforms.ToTensor(),\n            ]))\n        print '==> Validation data :',len(validation_set),' frames',validation_set[1][1].size()\n        #print validation_set[1]\n\n        val_loader = DataLoader(\n            dataset=validation_set, \n            batch_size=self.BATCH_SIZE, \n            shuffle=False,\n            num_workers=self.num_workers)\n\n        return val_loader\n\nif __name__ == '__main__':\n    data_loader =Motion_DataLoader(BATCH_SIZE=1,num_workers=1,in_channel=10,\n                                        path='/home/ubuntu/data/UCF101/tvl1_flow/',\n                                        ucf_list='/home/ubuntu/cvlab/pytorch/ucf101_two_stream/github/UCF_list/',\n                                        ucf_split='01'\n                                        )\n    train_loader,val_loader,test_video = data_loader.run()\n    #print train_loader,val_loader"""
dataloader/spatial_dataloader.py,1,"b""import pickle\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport random\nfrom split_train_test_video import *\nfrom skimage import io, color, exposure\n\nclass spatial_dataset(Dataset):  \n    def __init__(self, dic, root_dir, mode, transform=None):\n \n        self.keys = dic.keys()\n        self.values=dic.values()\n        self.root_dir = root_dir\n        self.mode =mode\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.keys)\n\n    def load_ucf_image(self,video_name, index):\n        if video_name.split('_')[0] == 'HandstandPushups':\n            n,g = video_name.split('_',1)\n            name = 'HandStandPushups_'+g\n            path = self.root_dir + 'HandstandPushups'+'/separated_images/v_'+name+'/v_'+name+'_'\n        else:\n            path = self.root_dir + video_name.split('_')[0]+'/separated_images/v_'+video_name+'/v_'+video_name+'_'\n         \n        img = Image.open(path +str(index)+'.jpg')\n        transformed_img = self.transform(img)\n        img.close()\n\n        return transformed_img\n\n    def __getitem__(self, idx):\n\n        if self.mode == 'train':\n            video_name, nb_clips = self.keys[idx].split(' ')\n            nb_clips = int(nb_clips)\n            clips = []\n            clips.append(random.randint(1, nb_clips/3))\n            clips.append(random.randint(nb_clips/3, nb_clips*2/3))\n            clips.append(random.randint(nb_clips*2/3, nb_clips+1))\n            \n        elif self.mode == 'val':\n            video_name, index = self.keys[idx].split(' ')\n            index =abs(int(index))\n        else:\n            raise ValueError('There are only train and val mode')\n\n        label = self.values[idx]\n        label = int(label)-1\n        \n        if self.mode=='train':\n            data ={}\n            for i in range(len(clips)):\n                key = 'img'+str(i)\n                index = clips[i]\n                data[key] = self.load_ucf_image(video_name, index)\n                    \n            sample = (data, label)\n        elif self.mode=='val':\n            data = self.load_ucf_image(video_name,index)\n            sample = (video_name, data, label)\n        else:\n            raise ValueError('There are only train and val mode')\n           \n        return sample\n\nclass spatial_dataloader():\n    def __init__(self, BATCH_SIZE, num_workers, path, ucf_list, ucf_split):\n\n        self.BATCH_SIZE=BATCH_SIZE\n        self.num_workers=num_workers\n        self.data_path=path\n        self.frame_count ={}\n        # split the training and testing videos\n        splitter = UCF101_splitter(path=ucf_list,split=ucf_split)\n        self.train_video, self.test_video = splitter.split_video()\n\n    def load_frame_count(self):\n        #print '==> Loading frame number of each video'\n        with open('dic/frame_count.pickle','rb') as file:\n            dic_frame = pickle.load(file)\n        file.close()\n\n        for line in dic_frame :\n            videoname = line.split('_',1)[1].split('.',1)[0]\n            n,g = videoname.split('_',1)\n            if n == 'HandStandPushups':\n                videoname = 'HandstandPushups_'+ g\n            self.frame_count[videoname]=dic_frame[line]\n\n    def run(self):\n        self.load_frame_count()\n        self.get_training_dic()\n        self.val_sample20()\n        train_loader = self.train()\n        val_loader = self.validate()\n\n        return train_loader, val_loader, self.test_video\n\n    def get_training_dic(self):\n        #print '==> Generate frame numbers of each training video'\n        self.dic_training={}\n        for video in self.train_video:\n            #print videoname\n            nb_frame = self.frame_count[video]-10+1\n            key = video+' '+ str(nb_frame)\n            self.dic_training[key] = self.train_video[video]\n                    \n    def val_sample20(self):\n        print '==> sampling testing frames'\n        self.dic_testing={}\n        for video in self.test_video:\n            nb_frame = self.frame_count[video]-10+1\n            interval = int(nb_frame/19)\n            for i in range(19):\n                frame = i*interval\n                key = video+ ' '+str(frame+1)\n                self.dic_testing[key] = self.test_video[video]      \n\n    def train(self):\n        training_set = spatial_dataset(dic=self.dic_training, root_dir=self.data_path, mode='train', transform = transforms.Compose([\n                transforms.RandomCrop(224),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n                ]))\n        print '==> Training data :',len(training_set),'frames'\n        print training_set[1][0]['img1'].size()\n\n        train_loader = DataLoader(\n            dataset=training_set, \n            batch_size=self.BATCH_SIZE,\n            shuffle=True,\n            num_workers=self.num_workers)\n        return train_loader\n\n    def validate(self):\n        validation_set = spatial_dataset(dic=self.dic_testing, root_dir=self.data_path, mode='val', transform = transforms.Compose([\n                transforms.Scale([224,224]),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n                ]))\n        \n        print '==> Validation data :',len(validation_set),'frames'\n        print validation_set[1][1].size()\n\n        val_loader = DataLoader(\n            dataset=validation_set, \n            batch_size=self.BATCH_SIZE, \n            shuffle=False,\n            num_workers=self.num_workers)\n        return val_loader\n\n\n\n\n\nif __name__ == '__main__':\n    \n    dataloader = spatial_dataloader(BATCH_SIZE=1, num_workers=1, \n                                path='/home/ubuntu/data/UCF101/spatial_no_sampled/', \n                                ucf_list='/home/ubuntu/cvlab/pytorch/ucf101_two_stream/github/UCF_list/',\n                                ucf_split='01')\n    train_loader,val_loader,test_video = dataloader.run()"""
dataloader/split_train_test_video.py,0,"b""import os, pickle\n\n\nclass UCF101_splitter():\n    def __init__(self, path, split):\n        self.path = path\n        self.split = split\n\n    def get_action_index(self):\n        self.action_label={}\n        with open(self.path+'classInd.txt') as f:\n            content = f.readlines()\n            content = [x.strip('\\r\\n') for x in content]\n        f.close()\n        for line in content:\n            label,action = line.split(' ')\n            #print label,action\n            if action not in self.action_label.keys():\n                self.action_label[action]=label\n\n    def split_video(self):\n        self.get_action_index()\n        for path,subdir,files in os.walk(self.path):\n            for filename in files:\n                if filename.split('.')[0] == 'trainlist'+self.split:\n                    train_video = self.file2_dic(self.path+filename)\n                if filename.split('.')[0] == 'testlist'+self.split:\n                    test_video = self.file2_dic(self.path+filename)\n        print '==> (Training video, Validation video):(', len(train_video),len(test_video),')'\n        self.train_video = self.name_HandstandPushups(train_video)\n        self.test_video = self.name_HandstandPushups(test_video)\n\n        return self.train_video, self.test_video\n\n    def file2_dic(self,fname):\n        with open(fname) as f:\n            content = f.readlines()\n            content = [x.strip('\\r\\n') for x in content]\n        f.close()\n        dic={}\n        for line in content:\n            #print line\n            video = line.split('/',1)[1].split(' ',1)[0]\n            key = video.split('_',1)[1].split('.',1)[0]\n            label = self.action_label[line.split('/')[0]]   \n            dic[key] = int(label)\n            #print key,label\n        return dic\n\n    def name_HandstandPushups(self,dic):\n        dic2 = {}\n        for video in dic:\n            n,g = video.split('_',1)\n            if n == 'HandStandPushups':\n                videoname = 'HandstandPushups_'+ g\n            else:\n                videoname=video\n            dic2[videoname] = dic[video]\n        return dic2\n\n\nif __name__ == '__main__':\n\n    path = '../UCF_list/'\n    split = '01'\n    splitter = UCF101_splitter(path=path,split=split)\n    train_video,test_video = splitter.split_video()\n    print len(train_video),len(test_video)"""
