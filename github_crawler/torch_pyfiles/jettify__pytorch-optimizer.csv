file_path,api_count,code
setup.py,0,"b'import os\nimport re\nimport sys\nfrom setuptools import setup, find_packages\n\n\ninstall_requires = [\'torch>=1.1.0\', \'pytorch_ranger>=0.1.1\']\n\nPY35 = (3, 5, 0)\n\n\nif sys.version_info < PY35:\n    raise RuntimeError(\'torch-optimizer requires Python 3.5.0+\')\n\n\ndef _read(f):\n    with open(os.path.join(os.path.dirname(__file__), f)) as f_:\n        return f_.read().strip()\n\n\ndef _read_version():\n    regexp = re.compile(r""^__version__\\W*=\\W*\'([\\d.abrc]+)\'"")\n    init_py = os.path.join(\n        os.path.dirname(__file__), \'torch_optimizer\', \'__init__.py\'\n    )\n    with open(init_py) as f:\n        for line in f:\n            match = regexp.match(line)\n            if match is not None:\n                return match.group(1)\n        raise RuntimeError(\n            \'Cannot find version in torch_optimizer/__init__.py\'\n        )\n\n\nclassifiers = [\n    \'License :: OSI Approved :: Apache Software License\',\n    \'Intended Audience :: Developers\',\n    \'Programming Language :: Python :: 3\',\n    \'Programming Language :: Python :: 3.5\',\n    \'Programming Language :: Python :: 3.6\',\n    \'Programming Language :: Python :: 3.7\',\n    \'Programming Language :: Python :: 3.8\',\n    \'Operating System :: OS Independent\',\n    \'Development Status :: 3 - Alpha\',\n]\n\nkeywords = [\n    \'torch-optimizer\',\n    \'pytorch\',\n    # optimizers\n    \'accsgd\',\n    \'adabound\',\n    \'adamod\',\n    \'diffgrad\',\n    \'lamb\',\n    \'lookahead\',\n    \'novograd\',\n    \'pid\',\n    \'qhadam\',\n    \'qhm\',\n    \'radam\',\n    \'sgdw\',\n    \'yogi\',\n    \'ranger\',\n]\n\nproject_urls = {\n    \'Website\': \'https://github.com/jettify/pytorch-optimizer\',\n    \'Documentation\': \'https://pytorch-optimizer.readthedocs.io\',\n    \'Issues\': \'https://github.com/jettify/pytorch-optimizer/issues\',\n}\n\n\nsetup(\n    name=\'torch-optimizer\',\n    version=_read_version(),\n    description=(\'pytorch-optimizer\'),\n    long_description=\'\\n\\n\'.join((_read(\'README.rst\'), _read(\'CHANGES.rst\'))),\n    long_description_content_type=\'text/x-rst\',\n    classifiers=classifiers,\n    platforms=[\'POSIX\'],\n    author=\'Nikolay Novik\',\n    author_email=\'nickolainovik@gmail.com\',\n    url=\'https://github.com/jettify/pytorch-optimizer\',\n    download_url=\'https://pypi.org/project/torch-optimizer/\',\n    license=\'Apache 2\',\n    packages=find_packages(),\n    install_requires=install_requires,\n    keywords=keywords,\n    zip_safe=True,\n    include_package_data=True,\n    project_urls=project_urls,\n)\n'"
docs/conf.py,1,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'pytorch-optimizer\'\ncopyright = \'2020, Nikolai Novik\'\nauthor = \'Nikolai Novik\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\n\n# Sphinx extension modules\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.napoleon"",\n    ""sphinx_autodoc_typehints"",\n    ""sphinx.ext.doctest"",\n    ""sphinx.ext.todo"",\n    ""sphinx.ext.coverage"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.ifconfig"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.intersphinx"",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# Configuration for intersphinx: refer to the Python standard library and PyTorch\nintersphinx_mapping = {\n    ""python"": (""https://docs.python.org/3"", None),\n    ""pytorch"": (""https://pytorch.org/docs/stable"", None),\n}\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\ndesc = \'collection of optimizers for PyTorch\'\nhtml_theme_options = {\n    \'description\': desc,\n    \'github_user\': \'jettify\',\n    \'github_repo\': \'pytorch-optimizer\',\n    \'github_button\': True,\n    \'github_type\': \'star\',\n    \'github_banner\': True,\n}\n'"
examples/mnist.py,11,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch_optimizer as optim\n\nfrom torchvision import datasets, transforms, utils\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n\ndef train(conf, model, device, train_loader, optimizer, epoch, writer):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % conf.log_interval == 0:\n            loss = loss.item()\n            idx = batch_idx + epoch * (len(train_loader))\n            writer.add_scalar('Loss/train', loss, idx)\n            print(\n                'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    epoch,\n                    batch_idx * len(data),\n                    len(train_loader.dataset),\n                    100.0 * batch_idx / len(train_loader),\n                    loss,\n                )\n            )\n\n\ndef test(conf, model, device, test_loader, epoch, writer):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # sum up batch loss\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            # get the index of the max log-probability\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    fmt = '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n    print(\n        fmt.format(\n            test_loss,\n            correct,\n            len(test_loader.dataset),\n            100.0 * correct / len(test_loader.dataset),\n        )\n    )\n\n    writer.add_scalar('Accuracy', correct, epoch)\n    writer.add_scalar('Loss/test', test_loss, epoch)\n\n\ndef prepare_loaders(conf, use_cuda=False):\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            '../data',\n            train=True,\n            download=True,\n            transform=transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.1307,), (0.3081,)),\n                ]\n            ),\n        ),\n        batch_size=conf.batch_size,\n        shuffle=True,\n        **kwargs,\n    )\n\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            '../data',\n            train=False,\n            transform=transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.1307,), (0.3081,)),\n                ]\n            ),\n        ),\n        batch_size=conf.test_batch_size,\n        shuffle=True,\n        **kwargs,\n    )\n    return train_loader, test_loader\n\n\nclass Config:\n    def __init__(\n        self,\n        batch_size: int = 64,\n        test_batch_size: int = 1000,\n        epochs: int = 15,\n        lr: float = 0.01,\n        gamma: float = 0.7,\n        no_cuda: bool = True,\n        seed: int = 42,\n        log_interval: int = 10,\n    ):\n        self.batch_size = batch_size\n        self.test_batch_size = test_batch_size\n        self.epochs = epochs\n        self.lr = lr\n        self.gamma = gamma\n        self.no_cuda = no_cuda\n        self.seed = seed\n        self.log_interval = log_interval\n\n\ndef main():\n    conf = Config()\n    log_dir = 'runs/mnist_custom_optim'\n    print('Tensorboard: tensorboard --logdir={}'.format(log_dir))\n\n    with SummaryWriter(log_dir) as writer:\n        use_cuda = not conf.no_cuda and torch.cuda.is_available()\n        torch.manual_seed(conf.seed)\n        device = torch.device('cuda' if use_cuda else 'cpu')\n        train_loader, test_loader = prepare_loaders(conf, use_cuda)\n\n        model = Net().to(device)\n\n        # create grid of images and write to tensorboard\n        images, labels = next(iter(train_loader))\n        img_grid = utils.make_grid(images)\n        writer.add_image('mnist_images', img_grid)\n        # visualize NN computation graph\n        writer.add_graph(model, images)\n\n        # custom optimizer from torch_optimizer package\n        optimizer = optim.DiffGrad(model.parameters(), lr=conf.lr)\n\n        scheduler = StepLR(optimizer, step_size=1, gamma=conf.gamma)\n        for epoch in range(1, conf.epochs + 1):\n            train(conf, model, device, train_loader, optimizer, epoch, writer)\n            test(conf, model, device, test_loader, epoch, writer)\n            scheduler.step()\n            for name, param in model.named_parameters():\n                writer.add_histogram(name, param, epoch)\n                writer.add_histogram('{}.grad'.format(name), param.grad, epoch)\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/viz_optimizers.py,4,"b""import math\nimport numpy as np\nimport torch_optimizer as optim\nimport torch\nfrom hyperopt import fmin, tpe, hp\nimport matplotlib.pyplot as plt\n\n\nplt.style.use('seaborn-white')\n\n\ndef rosenbrock(tensor):\n    # https://en.wikipedia.org/wiki/Test_functions_for_optimization\n    x, y = tensor\n    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n\n\ndef rastrigin(tensor, lib=torch):\n    # https://en.wikipedia.org/wiki/Test_functions_for_optimization\n    x, y = tensor\n    A = 10\n    f = (\n        A * 2\n        + (x ** 2 - A * lib.cos(x * math.pi * 2))\n        + (y ** 2 - A * lib.cos(y * math.pi * 2))\n    )\n    return f\n\n\ndef execute_steps(\n    func, initial_state, optimizer_class, optimizer_config, num_iter=500\n):\n    x = torch.Tensor(initial_state).requires_grad_(True)\n    optimizer = optimizer_class([x], **optimizer_config)\n    steps = []\n    steps = np.zeros((2, num_iter + 1))\n    steps[:, 0] = np.array(initial_state)\n    for i in range(1, num_iter + 1):\n        optimizer.zero_grad()\n        f = func(x)\n        f.backward(retain_graph=True)\n        torch.nn.utils.clip_grad_norm_(x, 1.0)\n        optimizer.step()\n        steps[:, i] = x.detach().numpy()\n    return steps\n\n\ndef objective_rastrigin(params):\n    lr = params['lr']\n    optimizer_class = params['optimizer_class']\n    initial_state = (-2.0, 3.5)\n    minimum = (0, 0)\n    optimizer_config = dict(lr=lr)\n    num_iter = 100\n    steps = execute_steps(\n        rastrigin, initial_state, optimizer_class, optimizer_config, num_iter\n    )\n    return (steps[0][-1] - minimum[0]) ** 2 + (steps[1][-1] - minimum[1]) ** 2\n\n\ndef objective_rosenbrok(params):\n    lr = params['lr']\n    optimizer_class = params['optimizer_class']\n    minimum = (1.0, 1.0)\n    initial_state = (-2.0, 2.0)\n    optimizer_config = dict(lr=lr)\n    num_iter = 100\n    steps = execute_steps(\n        rosenbrock, initial_state, optimizer_class, optimizer_config, num_iter\n    )\n    return (steps[0][-1] - minimum[0]) ** 2 + (steps[1][-1] - minimum[1]) ** 2\n\n\ndef plot_rastrigin(grad_iter, optimizer_name, lr):\n    x = np.linspace(-4.5, 4.5, 250)\n    y = np.linspace(-4.5, 4.5, 250)\n    minimum = (0, 0)\n\n    X, Y = np.meshgrid(x, y)\n    Z = rastrigin([X, Y], lib=np)\n\n    iter_x, iter_y = grad_iter[0, :], grad_iter[1, :]\n\n    fig = plt.figure(figsize=(8, 8))\n\n    ax = fig.add_subplot(1, 1, 1)\n    ax.contour(X, Y, Z, 20, cmap='jet')\n    ax.plot(iter_x, iter_y, color='r', marker='x')\n    ax.set_title(\n        'Rastrigin func: {} with '\n        '{} iterations, lr={:.6}'.format(optimizer_name, len(iter_x), lr)\n    )\n    plt.plot(*minimum, 'gD')\n    plt.plot(iter_x[-1], iter_y[-1], 'rD')\n    plt.savefig('docs/rastrigin_{}.png'.format(optimizer_name))\n\n\ndef plot_rosenbrok(grad_iter, optimizer_name, lr):\n    x = np.linspace(-2, 2, 250)\n    y = np.linspace(-1, 3, 250)\n    minimum = (1.0, 1.0)\n\n    X, Y = np.meshgrid(x, y)\n    Z = rosenbrock([X, Y])\n\n    iter_x, iter_y = grad_iter[0, :], grad_iter[1, :]\n\n    fig = plt.figure(figsize=(8, 8))\n\n    ax = fig.add_subplot(1, 1, 1)\n    ax.contour(X, Y, Z, 90, cmap='jet')\n    ax.plot(iter_x, iter_y, color='r', marker='x')\n\n    ax.set_title(\n        'Rosenbrock func: {} with {} '\n        'iterations, lr={:.6}'.format(optimizer_name, len(iter_x), lr)\n    )\n    plt.plot(*minimum, 'gD')\n    plt.plot(iter_x[-1], iter_y[-1], 'rD')\n    plt.savefig('docs/rosenbrock_{}.png'.format(optimizer_name))\n\n\ndef execute_experiments(\n    optimizers, objective, func, plot_func, initial_state, seed=1\n):\n    seed = seed\n    for item in optimizers:\n        optimizer_class, lr_low, lr_hi = item\n        space = {\n            'optimizer_class': hp.choice('optimizer_class', [optimizer_class]),\n            'lr': hp.loguniform('lr', lr_low, lr_hi),\n        }\n        best = fmin(\n            fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=200,\n            rstate=np.random.RandomState(seed),\n        )\n        print(best['lr'], optimizer_class)\n\n        steps = execute_steps(\n            func,\n            initial_state,\n            optimizer_class,\n            {'lr': best['lr']},\n            num_iter=500,\n        )\n        plot_func(steps, optimizer_class.__name__, best['lr'])\n\n\ndef LookaheadYogi(*a, **kw):\n    base = optim.Yogi(*a, **kw)\n    return optim.Lookahead(base)\n\n\nif __name__ == '__main__':\n    # python examples/viz_optimizers.py\n\n    # Each optimizer has tweaked search space to produce better plots and\n    # help to converge on better lr faster.\n    optimizers = [\n        # baselines\n        (torch.optim.Adam, -8, 0.5),\n        (torch.optim.SGD, -8, -1.0),\n        # Adam based\n        (optim.AdaBound, -8, 0.3),\n        (optim.AdaMod, -8, 0.2),\n        (optim.DiffGrad, -8, 0.4),\n        (optim.Lamb, -8, -2.9),\n        (optim.NovoGrad, -8, -1.7),\n        (optim.RAdam, -8, 0.5),\n        (optim.Yogi, -8, 0.1),\n        # SGD/Momentum based\n        (optim.AccSGD, -8, -1.4),\n        (optim.SGDW, -8, -1.5),\n        (optim.PID, -8, -1.0),\n        (optim.QHM, -6, -0.2),\n        (optim.QHAdam, -8, 0.1),\n        (optim.Ranger, -8, 0.1),\n        (optim.RangerQH, -8, 0.1),\n        (optim.RangerVA, -8, 0.1),\n        (optim.Shampoo, -8, 0.1),\n        (LookaheadYogi, -8, 0.1),\n    ]\n    execute_experiments(\n        optimizers,\n        objective_rastrigin,\n        rastrigin,\n        plot_rastrigin,\n        (-2.0, 3.5),\n    )\n\n    execute_experiments(\n        optimizers,\n        objective_rosenbrok,\n        rosenbrock,\n        plot_rosenbrok,\n        (-2.0, 2.0),\n    )\n"""
tests/conftest.py,0,b''
tests/test_basic.py,3,"b""import torch\nimport pytest\n\nimport torch_optimizer as optim\n\n\ndef rosenbrock(tensor):\n    x, y = tensor\n    return (1 - x) ** 2 + 1 * (y - x ** 2) ** 2\n\n\ndef quadratic(tensor):\n    x, y = tensor\n    a = 1.0\n    b = 1.0\n    return (x ** 2) / a + (y ** 2) / b\n\n\ndef beale(tensor):\n    x, y = tensor\n    f = (\n        (1.5 - x + x * y) ** 2\n        + (2.25 - x + x * y ** 2) ** 2\n        + (2.625 - x + x * y ** 3) ** 2\n    )\n    return f\n\n\ncases = [\n    (rosenbrock, (1.5, 1.5), (1, 1)),\n    (quadratic, (1.5, 1.5), (0, 0)),\n    (beale, (1.5, 1.5), (3, 0.5)),\n]\n\n\ndef ids(v):\n    n = '{} {}'.format(v[0].__name__, v[1:])\n    return n\n\n\ndef build_lookahead(*a, **kw):\n    base = optim.Yogi(*a, **kw)\n    return optim.Lookahead(base)\n\n\noptimizers = [\n    (optim.PID, {'lr': 0.002, 'momentum': 0.8, 'weight_decay': 0.0001}, 900),\n    (optim.QHM, {'lr': 0.02, 'momentum': 0.95, 'nu': 1}, 900),\n    (\n        optim.NovoGrad,\n        {'lr': 2.9, 'betas': (0.9, 0.999), 'grad_averaging': True},\n        900,\n    ),\n    (optim.RAdam, {'lr': 0.01, 'betas': (0.9, 0.95), 'eps': 1e-3}, 800),\n    (optim.SGDW, {'lr': 0.002, 'momentum': 0.91}, 900),\n    (optim.DiffGrad, {'lr': 0.5}, 500),\n    (optim.AdaMod, {'lr': 1.0}, 800),\n    (optim.AdaBound, {'lr': 1.0}, 800),\n    (optim.Yogi, {'lr': 1.0}, 500),\n    (optim.AccSGD, {'lr': 0.015}, 800),\n    (build_lookahead, {'lr': 1.0}, 500),\n    (optim.QHAdam, {'lr': 1.0}, 500),\n]\n\n\n@pytest.mark.parametrize('case', cases, ids=ids)\n@pytest.mark.parametrize('optimizer_config', optimizers, ids=ids)\ndef test_benchmark_function(case, optimizer_config):\n    func, initial_state, min_loc = case\n    optimizer_class, config, iterations = optimizer_config\n\n    x = torch.Tensor(initial_state).requires_grad_(True)\n    x_min = torch.Tensor(min_loc)\n    optimizer = optimizer_class([x], **config)\n    for _ in range(iterations):\n        optimizer.zero_grad()\n        f = func(x)\n        f.backward(retain_graph=True)\n        optimizer.step()\n    assert torch.allclose(x, x_min, atol=0.001)\n\n    name = optimizer.__class__.__name__\n    assert name in optimizer.__repr__()\n"""
tests/test_optimizer.py,20,"b""import functools\nfrom copy import deepcopy\n\nimport torch\nimport torch_optimizer as optim\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau, StepLR\n\nfrom tests.utils import assert_dict_equal\n\n\ndef _build_params_dict(weight, bias, **kwargs):\n    return [{'params': [weight]}, dict(params=[bias], **kwargs)]\n\n\ndef _build_params_dict_single(weight, bias, **kwargs):\n    return [dict(params=bias, **kwargs)]\n\n\ndef make_test_params(optimizer_class):\n    cases = [\n        (lambda weight, bias: optimizer_class([weight, bias], lr=1e-3),),\n        (\n            lambda weight, bias: optimizer_class(\n                _build_params_dict(weight, bias, lr=1e-2), lr=1e-3\n            ),\n        ),\n        (\n            lambda weight, bias: optimizer_class(\n                _build_params_dict_single(weight, bias, lr=1e-2), lr=1e-3\n            ),\n        ),\n        (\n            lambda weight, bias: optimizer_class(\n                _build_params_dict_single(weight, bias, lr=1e-2)\n            ),\n        ),\n        (\n            lambda weight, bias: optimizer_class([weight, bias], lr=1e-3),\n            [lambda opt: StepLR(opt, gamma=0.9, step_size=10)],\n        ),\n        (\n            lambda weight, bias: optimizer_class([weight, bias], lr=1e-3),\n            [\n                lambda opt: StepLR(opt, gamma=0.9, step_size=10),\n                lambda opt: ReduceLROnPlateau(opt),\n            ],\n        ),\n        (\n            lambda weight, bias: optimizer_class([weight, bias], lr=1e-3),\n            [\n                lambda opt: StepLR(opt, gamma=0.99, step_size=10),\n                lambda opt: ExponentialLR(opt, gamma=0.99),\n                lambda opt: ReduceLROnPlateau(opt),\n            ],\n        ),\n    ]\n    ids = ['%s_%s' % (optimizer_class.__name__, i) for i in range(len(cases))]\n    return cases, ids\n\n\ndef build_lookahead(*a, **kw):\n    base = optim.Yogi(*a, **kw)\n    return optim.Lookahead(base)\n\n\noptimizers = [\n    optim.AccSGD,\n    optim.AdaBound,\n    optim.AdaMod,\n    optim.DiffGrad,\n    optim.Lamb,\n    optim.NovoGrad,\n    optim.PID,\n    optim.QHAdam,\n    optim.QHM,\n    optim.RAdam,\n    optim.SGDW,\n    optim.Yogi,\n    build_lookahead,\n    optim.Ranger,\n    optim.RangerQH,\n    optim.RangerVA,\n    optim.Shampoo,\n]\n\n\ndef pytest_generate_tests(metafunc):\n    if 'optimizer_constructor' in metafunc.fixturenames:\n        cases = []\n        ids = []\n        for o in optimizers:\n            c, i = make_test_params(o)\n            cases = cases + c\n            ids = ids + i\n        metafunc.parametrize('optimizer_constructor', cases, ids=ids)\n\n\nclass TestOptim:\n    def _test_basic_cases_template(\n        self, weight, bias, input, constructor, scheduler_constructors\n    ):\n        weight = Variable(weight, requires_grad=True)\n        bias = Variable(bias, requires_grad=True)\n        input = Variable(input)\n        optimizer = constructor(weight, bias)\n        schedulers = []\n        for scheduler_constructor in scheduler_constructors:\n            schedulers.append(scheduler_constructor(optimizer))\n\n        # to check if the optimizer can be printed as a string\n        optimizer.__repr__()\n\n        def fn():\n            optimizer.zero_grad()\n            y = weight.mv(input)\n            if (\n                y.is_cuda\n                and bias.is_cuda\n                and y.get_device() != bias.get_device()\n            ):\n                y = y.cuda(bias.get_device())\n            loss = (y + bias).pow(2).sum()\n            loss.backward()\n            return loss\n\n        initial_value = fn().item()\n\n        optimizer.step(fn)\n\n        for _i in range(200):\n            for scheduler in schedulers:\n                if isinstance(scheduler, ReduceLROnPlateau):\n                    val_loss = fn()\n                    scheduler.step(val_loss)\n                else:\n                    scheduler.step()\n        assert fn().item() < initial_value\n\n    def _test_state_dict(self, weight, bias, input, constructor):\n        weight = Variable(weight, requires_grad=True)\n        bias = Variable(bias, requires_grad=True)\n        input = Variable(input)\n\n        def fn_base(optimizer, weight, bias):\n            optimizer.zero_grad()\n            i = input_cuda if weight.is_cuda else input\n            loss = (weight.mv(i) + bias).pow(2).sum()\n            loss.backward()\n            return loss\n\n        optimizer = constructor(weight, bias)\n        fn = functools.partial(fn_base, optimizer, weight, bias)\n\n        # Prime the optimizer\n        for _i in range(20):\n            optimizer.step(fn)\n        # Clone the weights and construct new optimizer for them\n        weight_c = Variable(weight.data.clone(), requires_grad=True)\n        bias_c = Variable(bias.data.clone(), requires_grad=True)\n        optimizer_c = constructor(weight_c, bias_c)\n        fn_c = functools.partial(fn_base, optimizer_c, weight_c, bias_c)\n        # Load state dict\n        state_dict = deepcopy(optimizer.state_dict())\n        state_dict_c = deepcopy(optimizer.state_dict())\n        optimizer_c.load_state_dict(state_dict_c)\n\n        precision = 0.0001\n        # Run both optimizations in parallel\n        for _i in range(20):\n            optimizer.step(fn)\n            optimizer_c.step(fn_c)\n            assert torch.allclose(weight, weight_c, atol=precision)\n            assert torch.allclose(bias, bias_c, atol=precision)\n\n        # Make sure state dict wasn't modified\n        assert assert_dict_equal(state_dict, state_dict_c)\n\n        # Check that state dict can be loaded even when we cast parameters\n        # to a different type and move to a different device.\n        if not torch.cuda.is_available():\n            return\n\n        input_cuda = Variable(input.data.float().cuda())\n        weight_cuda = Variable(weight.data.float().cuda(), requires_grad=True)\n        bias_cuda = Variable(bias.data.float().cuda(), requires_grad=True)\n        optimizer_cuda = constructor(weight_cuda, bias_cuda)\n        fn_cuda = functools.partial(\n            fn_base, optimizer_cuda, weight_cuda, bias_cuda\n        )\n\n        state_dict = deepcopy(optimizer.state_dict())\n        state_dict_c = deepcopy(optimizer.state_dict())\n        optimizer_cuda.load_state_dict(state_dict_c)\n\n        # Make sure state dict wasn't modified\n        assert assert_dict_equal(state_dict, state_dict_c)\n\n        for _i in range(20):\n            optimizer.step(fn)\n            optimizer_cuda.step(fn_cuda)\n            assert weight == weight_cuda\n            assert bias == bias_cuda\n\n        # validate deepcopy() copies all public attributes\n        def getPublicAttr(obj):\n            return set(k for k in obj.__dict__ if not k.startswith('_'))\n\n        assert getPublicAttr(optimizer) == getPublicAttr(deepcopy(optimizer))\n\n    def _test_basic_cases(\n        self,\n        constructor,\n        scheduler_constructors=None,\n        ignore_multidevice=False,\n    ):\n        if scheduler_constructors is None:\n            scheduler_constructors = []\n        self._test_state_dict(\n            torch.randn(10, 5), torch.randn(10), torch.randn(5), constructor\n        )\n        self._test_basic_cases_template(\n            torch.randn(10, 5),\n            torch.randn(10),\n            torch.randn(5),\n            constructor,\n            scheduler_constructors,\n        )\n        # non-contiguous parameters\n        self._test_basic_cases_template(\n            torch.randn(10, 5, 2)[..., 0],\n            torch.randn(10, 2)[..., 0],\n            torch.randn(5),\n            constructor,\n            scheduler_constructors,\n        )\n        # CUDA\n        if not torch.cuda.is_available():\n            return\n        self._test_basic_cases_template(\n            torch.randn(10, 5).cuda(),\n            torch.randn(10).cuda(),\n            torch.randn(5).cuda(),\n            constructor,\n            scheduler_constructors,\n        )\n        # Multi-GPU\n        if not torch.cuda.device_count() > 1 or ignore_multidevice:\n            return\n        self._test_basic_cases_template(\n            torch.randn(10, 5).cuda(0),\n            torch.randn(10).cuda(1),\n            torch.randn(5).cuda(0),\n            constructor,\n            scheduler_constructors,\n        )\n\n    def test_optimizer(self, optimizer_constructor):\n        self._test_basic_cases(*optimizer_constructor)\n"""
tests/test_optimizer_with_nn.py,5,"b""import numpy as np\nimport pytest\nimport torch\nimport torch_optimizer as optim\n\nfrom torch import nn\n\n\ndef make_dataset(seed=42):\n    rng = np.random.RandomState(seed)\n    N = 100\n    D = 2\n\n    X = rng.randn(N, D) * 2\n\n    # center the first N/2 points at (-2,-2)\n    mid = N // 2\n    X[: mid, :] = X[: mid, :] - 2 * np.ones((mid, D))\n\n    # center the last N/2 points at (2, 2)\n    X[mid:, :] = X[mid:, :] + 2 * np.ones((mid, D))\n\n    # labels: first N/2 are 0, last N/2 are 1\n    Y = np.array([0] * mid + [1] * mid).reshape(100, 1)\n\n    x = torch.Tensor(X)\n    y = torch.Tensor(Y)\n    return x, y\n\n\nclass LogisticRegression(nn.Module):\n    def __init__(self):\n        super(LogisticRegression, self).__init__()\n        self.linear1 = nn.Linear(2, 4)\n        self.linear2 = nn.Linear(4, 1)\n\n    def forward(self, x):\n        output = torch.relu(self.linear1(x))\n        output = self.linear2(output)\n        y_pred = torch.sigmoid(output)\n        return y_pred\n\n\ndef ids(v):\n    return '{} {}'.format(v[0].__name__, v[1:])\n\n\ndef build_lookahead(*a, **kw):\n    base = optim.Yogi(*a, **kw)\n    return optim.Lookahead(base)\n\n\noptimizers = [\n    (optim.NovoGrad, {'lr': 0.01, 'weight_decay': 1e-3}, 200),\n    (optim.PID, {'lr': 0.01, 'weight_decay': 1e-3, 'momentum': 0.1}, 200),\n    (optim.Lamb, {'lr': 0.01, 'weight_decay': 1e-3}, 200),\n    (optim.SGDW, {'lr': 1.0, 'weight_decay': 1e-3}, 200),\n    (optim.DiffGrad, {'lr': 0.5, 'weight_decay': 1e-3}, 200),\n    (optim.AdaMod, {'lr': 2.0, 'weight_decay': 1e-3}, 200),\n    (optim.AdaBound, {'lr': 1.5, 'gamma': 0.1, 'weight_decay': 1e-3}, 200),\n    (optim.Yogi, {'lr': 0.1, 'weight_decay': 1e-3}, 200),\n    (optim.RAdam, {'lr': 1.0, 'weight_decay': 1e-3}, 200),\n    (optim.AccSGD, {'lr': 1.0, 'weight_decay': 1e-3}, 200),\n    (build_lookahead, {'lr': 0.1, 'weight_decay': 1e-3}, 200),\n    (optim.QHM, {'lr': 0.1, 'weight_decay': 1e-5, 'momentum': 0.2}, 200),\n    (optim.QHAdam, {'lr': 0.1, 'weight_decay': 1e-3}, 200),\n    (optim.Ranger, {'lr': 0.1, 'weight_decay': 1e-3}, 200),\n    (optim.RangerQH, {'lr': 0.01, 'weight_decay': 1e-3}, 200),\n    (optim.RangerVA, {'lr': 0.01, 'weight_decay': 1e-3}, 200),\n    (optim.Shampoo, {'lr': 0.1, 'weight_decay': 1e-3, 'momentum': 0.8}, 200),\n]\n\n\n@pytest.mark.parametrize('optimizer_config', optimizers, ids=ids)\ndef test_basic_nn_modeloptimizer_config(optimizer_config):\n    torch.manual_seed(42)\n    x_data, y_data = make_dataset()\n    model = LogisticRegression()\n\n    loss_fn = nn.BCELoss()\n    optimizer_class, config, iterations = optimizer_config\n    optimizer = optimizer_class(model.parameters(), **config)\n    init_loss = None\n    for _ in range(iterations):\n        y_pred = model(x_data)\n        loss = loss_fn(y_pred, y_data)\n        if init_loss is None:\n            init_loss = loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    assert init_loss.item() > 2.0 * loss.item()\n"""
tests/test_param_validation.py,2,"b""import torch\nimport pytest\nimport torch_optimizer as optim\n\n\ndef assert_sparse_not_supported(optimizer_class, err_msg=None):\n    param = torch.randn(1, 1).to_sparse().requires_grad_(True)\n    grad = torch.randn(1, 1).to_sparse()\n    param.grad = grad\n    optimizer = optimizer_class([param])\n    optimizer.zero_grad()\n    with pytest.raises(RuntimeError) as ctx:\n        optimizer.step()\n\n    msg = err_msg or 'does not support sparse gradients'\n    assert msg in str(ctx.value)\n\n\nno_sparse_optimizers = [\n    optim.AdaBound,\n    optim.AdaMod,\n    optim.DiffGrad,\n    optim.Lamb,\n    optim.NovoGrad,\n    optim.RAdam,\n    optim.Yogi,\n]\n\n\n@pytest.mark.parametrize('optimizer_class', no_sparse_optimizers)\ndef test_sparse_not_supported(optimizer_class):\n    assert_sparse_not_supported(optimizer_class)\n\n\noptimizers = [\n    optim.AccSGD,\n    optim.AdaBound,\n    optim.AdaMod,\n    optim.DiffGrad,\n    optim.Lamb,\n    optim.NovoGrad,\n    optim.PID,\n    optim.QHAdam,\n    optim.QHM,\n    optim.RAdam,\n    optim.SGDW,\n    optim.Shampoo,\n    optim.Yogi,\n]\n\n\n@pytest.mark.parametrize('optimizer_class', optimizers)\ndef test_learning_rate(optimizer_class):\n    lr = -0.01\n    with pytest.raises(ValueError) as ctx:\n        optimizer_class(None, lr=-0.01)\n    msg = 'Invalid learning rate: {}'.format(lr)\n    assert msg in str(ctx.value)\n\n\neps_optimizers = [\n    optim.AdaBound,\n    optim.AdaMod,\n    optim.DiffGrad,\n    optim.Lamb,\n    optim.NovoGrad,\n    optim.QHAdam,\n    optim.RAdam,\n    optim.Yogi,\n]\n\n\n@pytest.mark.parametrize('optimizer_class', eps_optimizers)\ndef test_eps_validation(optimizer_class):\n    eps = -0.1\n    with pytest.raises(ValueError) as ctx:\n        optimizer_class(None, lr=0.1, eps=eps)\n    msg = 'Invalid epsilon value: {}'.format(eps)\n    assert msg in str(ctx.value)\n\n\nweight_decay_optimizers = [\n    optim.AccSGD,\n    optim.AdaBound,\n    optim.AdaMod,\n    optim.DiffGrad,\n    optim.Lamb,\n    optim.PID,\n    optim.QHAdam,\n    optim.QHM,\n    optim.RAdam,\n    optim.SGDW,\n    optim.Shampoo,\n    optim.Yogi,\n]\n\n\n@pytest.mark.parametrize('optimizer_class', optimizers)\ndef test_weight_decay_validation(optimizer_class):\n    weight_decay = -0.1\n    with pytest.raises(ValueError) as ctx:\n        optimizer_class(None, lr=0.1, weight_decay=weight_decay)\n    msg = 'Invalid weight_decay value: {}'.format(weight_decay)\n    assert msg in str(ctx.value)\n\n\nbetas_optimizers = [\n    optim.AdaBound,\n    optim.AdaMod,\n    optim.DiffGrad,\n    optim.Lamb,\n    optim.NovoGrad,\n    optim.RAdam,\n    optim.Yogi,\n    optim.QHAdam,\n]\n\n\n@pytest.mark.parametrize('optimizer_class', eps_optimizers)\ndef test_betas_validation(optimizer_class):\n    betas = (-1, 0.999)\n    with pytest.raises(ValueError) as ctx:\n        optimizer_class(None, lr=0.1, betas=(-1, 0.999))\n    msg = 'Invalid beta parameter at index 0: {}'.format(betas[0])\n    assert msg in str(ctx.value)\n\n    betas = (0.9, -0.999)\n    with pytest.raises(ValueError) as ctx:\n        optimizer_class(None, lr=0.1, betas=betas)\n    msg = 'Invalid beta parameter at index 1: {}'.format(betas[1])\n    assert msg in str(ctx.value)\n"""
tests/utils.py,2,"b'import torch\n\n\ndef assert_dict_equal(a, b, precision=0.000001):\n    if isinstance(a, dict) and isinstance(b, dict):\n        assert set(a.keys()) == set(b.keys())\n        for k in a.keys():\n            assert_dict_equal(a[k], b[k], precision)\n    elif isinstance(a, list) and isinstance(b, list):\n        assert len(a) == len(b)\n        for v1, v2 in zip(a, b):\n            assert_dict_equal(v1, v2, precision)\n    elif isinstance(a, torch.Tensor) and isinstance(b, torch.Tensor):\n        assert torch.allclose(a, b, atol=precision)\n    else:\n        assert a == b\n    return True\n'"
torch_optimizer/__init__.py,3,"b'""""""torch-optimizer -- collection of of optimization algorithms for PyTorch.\n\nAPI and usage patterns are the same as `torch.optim`__\n\nExample\n-------\n\n>>> import torch_optimizer as optim\n# model = ...\n>>> optimizer = optim.DiffGrad(model.parameters(), lr=0.001)\n>>> optimizer.step()\n\nSee documentation for full list of supported optimizers.\n\n__ https://pytorch.org/docs/stable/optim.html#module-torch.optim\n""""""\nfrom typing import Type, List, Dict\n\nfrom pytorch_ranger import Ranger, RangerQH, RangerVA\nfrom torch.optim.optimizer import Optimizer\n\nfrom .accsgd import AccSGD\nfrom .adabound import AdaBound\nfrom .adamod import AdaMod\nfrom .diffgrad import DiffGrad\nfrom .lamb import Lamb\nfrom .lookahead import Lookahead\nfrom .novograd import NovoGrad\nfrom .pid import PID\nfrom .qhadam import QHAdam\nfrom .qhm import QHM\nfrom .radam import RAdam\nfrom .sgdw import SGDW\nfrom .shampoo import Shampoo\nfrom .yogi import Yogi\n\n\n__all__ = (\n    \'AccSGD\',\n    \'AdaBound\',\n    \'AdaMod\',\n    \'DiffGrad\',\n    \'Lamb\',\n    \'Lookahead\',\n    \'NovoGrad\',\n    \'PID\',\n    \'QHAdam\',\n    \'QHM\',\n    \'RAdam\',\n    \'Ranger\',\n    \'RangerQH\',\n    \'RangerVA\',\n    \'SGDW\',\n    \'Shampoo\',\n    \'Yogi\',\n    # utils\n    \'get\',\n)\n__version__ = \'0.0.1a13\'\n\n\n_package_opts = [\n    AccSGD,\n    AdaBound,\n    AdaMod,\n    DiffGrad,\n    Lamb,\n    Lookahead,\n    NovoGrad,\n    PID,\n    QHAdam,\n    QHM,\n    RAdam,\n    Ranger,\n    RangerQH,\n    RangerVA,\n    SGDW,\n    Shampoo,\n    Yogi,\n]  # type: List[Type[Optimizer]]\n\n\n_NAME_OPTIM_MAP = {\n    opt.__name__.lower(): opt for opt in _package_opts\n}  # type: Dict[str, Type[Optimizer]]\n\n\ndef get(name: str) -> Type[Optimizer]:\n    r""""""Returns an optimizer class from its name. Case insensitive.\n\n    Args:\n        name: the optimizer name.\n    """"""\n    optimizer_class = _NAME_OPTIM_MAP.get(name.lower())\n    if optimizer_class is None:\n        raise ValueError(\'Optimizer {} not found\'.format(name))\n    return optimizer_class\n'"
torch_optimizer/accsgd.py,1,"b'import copy\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import OptLossClosure, Params, OptFloat\n\n\n__all__ = (\'AccSGD\',)\n\n\nclass AccSGD(Optimizer):\n    r""""""Implements AccSGD algorithm.\n\n    It has been proposed in `On the insufficiency of existing momentum\n    schemes for Stochastic Optimization`__ and  `Accelerating Stochastic\n    Gradient Descent For Least Squares Regression`__\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        kappa: ratio of long to short step (default: 1000)\n        xi: statistical advantage parameter (default: 10)\n        small_const: any value <=1 (default: 0.7)\n        weight_decay: weight decay (L2 penalty) (default: 0)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.AccSGD(model.parameters(), lr=0.1)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n     __ https://arxiv.org/abs/1704.08227\n     __ https://arxiv.org/abs/1803.05591\n\n    Note:\n        Reference code: https://github.com/rahulkidambi/AccSGD\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-3,\n        kappa: float = 1000.0,\n        xi: float = 10.0,\n        small_const: float = 0.7,\n        weight_decay: float = 0,\n    ) -> None:\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if weight_decay < 0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n        defaults = dict(\n            lr=lr,\n            kappa=kappa,\n            xi=xi,\n            small_const=small_const,\n            weight_decay=weight_decay,\n        )\n        super(AccSGD, self).__init__(params, defaults)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        r""""""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            large_lr = (group[\'lr\'] * group[\'kappa\']) / (group[\'small_const\'])\n            alpha = 1.0 - (\n                (group[\'small_const\'] * group[\'small_const\'] * group[\'xi\'])\n                / group[\'kappa\']\n            )\n            beta = 1.0 - alpha\n            zeta = group[\'small_const\'] / (group[\'small_const\'] + beta)\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n                param_state = self.state[p]\n                if \'momentum_buffer\' not in param_state:\n                    param_state[\'momentum_buffer\'] = copy.deepcopy(p.data)\n                buf = param_state[\'momentum_buffer\']\n                buf.mul_((1.0 / beta) - 1.0)\n                buf.add_(-large_lr, d_p)\n                buf.add_(p.data)\n                buf.mul_(beta)\n\n                p.data.add_(-group[\'lr\'], d_p)\n                p.data.mul_(zeta)\n                p.data.add_(1.0 - zeta, buf)\n\n        return loss\n'"
torch_optimizer/adabound.py,6,"b'import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import Betas2, OptLossClosure, Params, State, OptFloat\n\n\n__all__ = (\'AdaBound\',)\n\n\nclass AdaBound(Optimizer):\n    r""""""Implements AdaBound algorithm.\n\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of\n    Learning Rate`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        betas: coefficients used for computing running averages of gradient\n            and its square (default: (0.9, 0.999))\n        final_lr: final (SGD) learning rate (default: 0.1)\n        gamma: convergence speed of the bound functions\n            (default: 1e-3)\n        eps: term added to the denominator to improve numerical stability\n            (default: 1e-8)\n        weight_decay: weight decay (L2 penalty) (default: 0)\n        amsbound: whether to use the AMSBound variant of this algorithm\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.AdaBound(model.parameters(), lr=0.1)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ https://arxiv.org/abs/1902.09843\n\n    Note:\n        Reference code: https://github.com/Luolc/AdaBound\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-3,\n        betas: Betas2 = (0.9, 0.999),\n        final_lr: float = 0.1,\n        gamma: float = 1e-3,\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n        amsbound: bool = False,\n    ) -> None:\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if eps < 0.0:\n            raise ValueError(\'Invalid epsilon value: {}\'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 0: {}\'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 1: {}\'.format(betas[1])\n            )\n        if final_lr < 0.0:\n            raise ValueError(\n                \'Invalid final learning rate: {}\'.format(final_lr)\n            )\n        if not 0.0 <= gamma < 1.0:\n            raise ValueError(\'Invalid gamma parameter: {}\'.format(gamma))\n        if weight_decay < 0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            final_lr=final_lr,\n            gamma=gamma,\n            eps=eps,\n            weight_decay=weight_decay,\n            amsbound=amsbound,\n        )\n        super(AdaBound, self).__init__(params, defaults)\n        self.base_lrs = [group[\'lr\'] for group in self.param_groups]\n\n    def __setstate__(self, state: State) -> None:\n        super(AdaBound, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsbound\', False)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        r""""""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    msg = (\n                        \'AdaBound does not support sparse gradients, \'\n                        \'please consider SparseAdam instead\'\n                    )\n                    raise RuntimeError(msg)\n                amsbound = group[\'amsbound\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p)\n                    if amsbound:\n                        # Maintains max of all exp. moving avg. of\n                        # sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsbound:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsbound:\n                    # Maintains the maximum of all 2nd moment running\n                    # avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = (\n                    group[\'lr\']\n                    * math.sqrt(bias_correction2)\n                    / bias_correction1\n                )\n\n                # Applies bounds on actual learning rate\n                # lr_scheduler cannot affect final_lr, this is a workaround\n                # to apply lr decay\n                final_lr = group[\'final_lr\'] * group[\'lr\'] / base_lr\n                lower_bound = final_lr * (\n                    1 - 1 / (group[\'gamma\'] * state[\'step\'] + 1)\n                )\n                upper_bound = final_lr * (\n                    1 + 1 / (group[\'gamma\'] * state[\'step\'])\n                )\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom).clamp_(lower_bound, upper_bound).mul_(\n                    exp_avg\n                )\n\n                p.data.add_(-step_size)\n        return loss\n'"
torch_optimizer/adamod.py,6,"b'import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import Betas2, OptFloat, OptLossClosure, Params\n\n\n__all__ = (\'AdaMod\',)\n\n\nclass AdaMod(Optimizer):\n    r""""""Implements AdaMod algorithm.\n\n    It has been proposed in `Adaptive and Momental Bounds for Adaptive\n    Learning Rate Methods`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        betas: coefficients used for computing running averages of gradient\n            and its square (default: (0.9, 0.999))\n        beta3: smoothing coefficient for adaptive learning rates\n            (default: 0.9999)\n        eps: term added to the denominator to improve numerical stability\n            (default: 1e-8)\n        weight_decay: weight decay (L2 penalty) (default: 0)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.AdaMod(model.parameters(), lr=0.1)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ https://arxiv.org/abs/1910.12249\n\n    Note:\n        Reference code: https://github.com/lancopku/AdaMod\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-3,\n        betas: Betas2 = (0.9, 0.999),\n        beta3: float = 0.999,\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n    ) -> None:\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if eps < 0.0:\n            raise ValueError(\'Invalid epsilon value: {}\'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 0: {}\'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 1: {}\'.format(betas[1])\n            )\n        if not 0.0 <= beta3 < 1.0:\n            raise ValueError(\'Invalid beta3 parameter: {}\'.format(beta3))\n        if weight_decay < 0.0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n        defaults = dict(\n            lr=lr, betas=betas, beta3=beta3, eps=eps, weight_decay=weight_decay\n        )\n        super(AdaMod, self).__init__(params, defaults)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    msg = \'AdaMod does not support sparse gradients\'\n                    raise RuntimeError(msg)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p)\n                    # Exponential moving average of actual learning rates\n                    state[\'exp_avg_lr\'] = torch.zeros_like(p)\n\n                exp_avg, exp_avg_sq, exp_avg_lr = (\n                    state[\'exp_avg\'],\n                    state[\'exp_avg_sq\'],\n                    state[\'exp_avg_lr\'],\n                )\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = (\n                    group[\'lr\']\n                    * math.sqrt(bias_correction2)\n                    / bias_correction1\n                )\n\n                if group[\'weight_decay\'] != 0:\n                    p.data.add_(-group[\'weight_decay\'] * group[\'lr\'], p.data)\n\n                # Applies momental bounds on actual learning rates\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom)\n                exp_avg_lr.mul_(group[\'beta3\']).add_(\n                    1 - group[\'beta3\'], step_size\n                )\n                step_size = torch.min(step_size, exp_avg_lr)\n                step_size.mul_(exp_avg)\n\n                p.data.add_(-step_size)\n\n        return loss\n'"
torch_optimizer/diffgrad.py,6,"b'import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import Betas2, OptFloat, OptLossClosure, Params\n\n\n__all__ = (\'DiffGrad\',)\n\n\nclass DiffGrad(Optimizer):\n    r""""""Implements DiffGrad algorithm.\n\n    It has been proposed in `DiffGrad: An Optimization Method for\n    Convolutional Neural Networks`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        betas: coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps: term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay: weight decay (L2 penalty) (default: 0)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.DiffGrad(model.parameters(), lr=0.1)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ https://arxiv.org/abs/1909.11015\n\n    Note:\n        Reference code: https://github.com/shivram1987/diffGrad\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-3,\n        betas: Betas2 = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0.0,\n    ) -> None:\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if eps < 0.0:\n            raise ValueError(\'Invalid epsilon value: {}\'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 0: {}\'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 1: {}\'.format(betas[1])\n            )\n        if weight_decay < 0.0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        super(DiffGrad, self).__init__(params, defaults)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        r""""""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            beta1, beta2 = group[\'betas\']\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    msg = (\n                        \'DiffGrad does not support sparse gradients, \'\n                        \'please consider SparseAdam instead\'\n                    )\n                    raise RuntimeError(msg)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p)\n                    # Previous gradient\n                    state[\'previous_grad\'] = torch.zeros_like(p)\n\n                exp_avg, exp_avg_sq, previous_grad = (\n                    state[\'exp_avg\'],\n                    state[\'exp_avg_sq\'],\n                    state[\'previous_grad\'],\n                )\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad.add_(p.data, alpha=group[\'weight_decay\'])\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n\n                # compute diffgrad coefficient (dfc)\n                diff = torch.abs(previous_grad - grad)\n                dfc = torch.div(1.0, (1.0 + torch.exp(-diff)))\n                state[\'previous_grad\'] = grad.clone()\n\n                # update momentum with dfc\n                exp_avg1 = exp_avg * dfc\n\n                step_size = (\n                    group[\'lr\']\n                    * math.sqrt(bias_correction2)\n                    / bias_correction1\n                )\n\n                p.data.addcdiv_(exp_avg1, denom, value=-step_size)\n\n        return loss\n'"
torch_optimizer/lamb.py,5,"b'import torch\nfrom torch.optim.optimizer import Optimizer\nimport math\nfrom .types import Betas2, OptFloat, OptLossClosure, Params\n\n\n__all__ = (\'Lamb\',)\n\n\nclass Lamb(Optimizer):\n    r""""""Implements Lamb algorithm.\n\n    It has been proposed in `Large Batch Optimization for Deep Learning:\n    Training BERT in 76 minutes`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        betas: coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps: term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay: weight decay (L2 penalty) (default: 0)\n        clamp_value: clamp weight_norm in (0,clamp_value) (default: 10)\n            set to a high value to avoid it (e.g 10e3)\n        adam: always use trust ratio = 1, which turns this\n            into Adam. Useful for comparison purposes. (default: False)\n        debias: debias adam by (1 - beta**step) (default: False)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.Lamb(model.parameters(), lr=0.1)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ https://arxiv.org/abs/1904.00962\n\n    Note:\n        Reference code: https://github.com/cybertronai/pytorch-lamb\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-3,\n        betas: Betas2 = (0.9, 0.999),\n        eps: float = 1e-6,\n        weight_decay: float = 0,\n        clamp_value: float = 10,\n        adam: bool = False,\n        debias: bool = False,\n    ) -> None:\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if eps < 0.0:\n            raise ValueError(\'Invalid epsilon value: {}\'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 0: {}\'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 1: {}\'.format(betas[1])\n            )\n        if weight_decay < 0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n        if clamp_value < 0.0:\n            raise ValueError(\'Invalid clamp value: {}\'.format(clamp_value))\n\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.clamp_value = clamp_value\n        self.adam = adam\n        self.debias = debias\n\n        super(Lamb, self).__init__(params, defaults)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        r""""""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    msg = (\n                        \'Lamb does not support sparse gradients, \'\n                        \'please consider SparseAdam instead\'\n                    )\n                    raise RuntimeError(msg)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                # Paper v3 does not use debiasing.\n                if self.debias:\n                    bias_correction = math.sqrt(1 - beta2 ** state[\'step\'])\n                    bias_correction /= 1 - beta1 ** state[\'step\']\n                else:\n                    bias_correction = 1\n\n                # Apply bias to lr to avoid broadcast.\n                step_size = group[\'lr\'] * bias_correction\n\n                weight_norm = torch.norm(p.data).clamp(0, self.clamp_value)\n\n                adam_step = exp_avg / exp_avg_sq.sqrt().add(group[\'eps\'])\n                if group[\'weight_decay\'] != 0:\n                    adam_step.add_(group[\'weight_decay\'], p.data)\n\n                adam_norm = torch.norm(adam_step)\n                if weight_norm == 0 or adam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm / adam_norm\n                state[\'weight_norm\'] = weight_norm\n                state[\'adam_norm\'] = adam_norm\n                state[\'trust_ratio\'] = trust_ratio\n                if self.adam:\n                    trust_ratio = 1\n\n                p.data.add_(-step_size * trust_ratio, adam_step)\n\n        return loss\n'"
torch_optimizer/lookahead.py,4,"b'from collections import defaultdict\nfrom typing import Dict, Any\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import OptLossClosure, OptFloat, State\n\n\n__all__ = (\'Lookahead\',)\n\n\nclass Lookahead(Optimizer):\n    r""""""Implements Lookahead optimization algorithm.\n\n    It has been proposed in `Lookahead Optimizer: k steps forward, 1\n    step back`__\n\n    Arguments:\n        optimizer: base inner optimizer optimize, like Yogi, DiffGrad or Adam.\n        k: number of lookahead steps (default: 5)\n        alpha: linear interpolation factor. 1.0 recovers the inner optimizer.\n            (default: 5)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> yogi = optim.Yogi(model.parameters(), lr=0.1)\n        >>> optimizer = optim.Lookahead(yogi, k=5, alpha=0.5)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ https://arxiv.org/abs/1907.08610\n\n    Note:\n        Reference code: https://github.com/alphadl/lookahead.pytorch\n    """"""\n\n    def __init__(\n        self, optimizer: Optimizer, k: int = 5, alpha: float = 0.5\n    ) -> None:\n        if k < 0.0:\n            raise ValueError(\'Invalid number of lookahead steps: {}\'.format(k))\n        if alpha < 0:\n            raise ValueError(\n                \'Invalid linear interpolation factor: {}\'.format(alpha)\n            )\n\n        self.optimizer = optimizer\n        self.k = k\n        self.alpha = alpha\n        self.param_groups = self.optimizer.param_groups\n        self.state = defaultdict(dict)\n        self.fast_state = self.optimizer.state\n        for group in self.param_groups:\n            group[\'counter\'] = 0\n\n    def _update(self, group: Dict[str, Any]) -> None:\n        for fast in group[\'params\']:\n            param_state = self.state[fast]\n            if \'slow_param\' not in param_state:\n                param_state[\'slow_param\'] = torch.clone(fast.data).detach()\n\n            slow = param_state[\'slow_param\']\n            fast.data.mul_(self.alpha).add_(1.0 - self.alpha, slow)\n            slow.data.copy_(fast)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        r""""""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = self.optimizer.step(closure=closure)\n        for group in self.param_groups:\n            if group[\'counter\'] == 0:\n                self._update(group)\n            group[\'counter\'] = (group[\'counter\'] + 1) % self.k\n        return loss\n\n    def state_dict(self) -> State:\n        r""""""Returns the state of the optimizer as a :class:`dict`.\n\n        It contains two entries:\n        * state - a dict holding current optimization state. Its content\n            differs between optimizer classes.\n        * param_groups - a dict containing all parameter groups\n        """"""\n        fast_state_dict = self.optimizer.state_dict()\n        slow_state = {\n            (id(k) if isinstance(k, torch.Tensor) else k): v\n            for k, v in self.state.items()\n        }\n        fast_state = fast_state_dict[\'state\']\n        param_groups = fast_state_dict[\'param_groups\']\n        return {\n            \'fast_state\': fast_state,\n            \'slow_state\': slow_state,\n            \'param_groups\': param_groups,\n        }\n\n    def load_state_dict(self, state_dict: State) -> None:\n        r""""""Loads the optimizer state.\n\n        Arguments:\n            state_dict: optimizer state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        """"""\n        slow_state_dict = {\n            \'state\': state_dict[\'slow_state\'],\n            \'param_groups\': state_dict[\'param_groups\'],\n        }\n        fast_state_dict = {\n            \'state\': state_dict[\'fast_state\'],\n            \'param_groups\': state_dict[\'param_groups\'],\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.optimizer.load_state_dict(fast_state_dict)\n        self.fast_state = self.optimizer.state\n\n    def zero_grad(self) -> None:\n        r""""""Clears the gradients of all optimized :class:`torch.Tensor` s.""""""\n        self.optimizer.zero_grad()\n\n    def __repr__(self) -> str:\n        base_str = self.optimizer.__repr__()\n        format_string = self.__class__.__name__ + \' (\'\n        format_string += \'\\n\'\n        format_string += \'k: {}\\n\'.format(self.k)\n        format_string += \'alpha: {}\\n\'.format(self.alpha)\n        format_string += base_str\n        format_string += \'\\n\'\n        format_string += \')\'\n        return format_string\n'"
torch_optimizer/novograd.py,6,"b'import torch\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import Betas2, OptFloat, OptLossClosure, Params\n\n\n__all__ = (\'NovoGrad\',)\n\n\nclass NovoGrad(Optimizer):\n    r""""""Implements Novograd optimization algorithm.\n\n    It has been proposed in `Stochastic Gradient Methods with Layer-wise\n    Adaptive Moments for Training of Deep Networks`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        betas: coefficients used for computing\n            running averages of gradient and its square (default: (0.95, 0))\n        eps: term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay: weight decay (L2 penalty) (default: 0)\n        grad_averaging: gradient averaging (default: False)\n        amsgrad: whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`\n            (default: False)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.Yogi(model.parameters(), lr=0.1)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n        >>> optimizer.step()\n        >>> scheduler.step()\n\n    __ https://arxiv.org/abs/1905.11286\n\n    Note:\n        Reference code: https://github.com/NVIDIA/DeepLearningExamples\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-3,\n        betas: Betas2 = (0.95, 0),\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n        grad_averaging: bool = False,\n        amsgrad: bool = False,\n    ):\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if eps < 0.0:\n            raise ValueError(\'Invalid epsilon value: {}\'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 0: {}\'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 1: {}\'.format(betas[1])\n            )\n        if weight_decay < 0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            grad_averaging=grad_averaging,\n            amsgrad=amsgrad,\n        )\n\n        super(NovoGrad, self).__init__(params, defaults)\n\n    def __setstate__(self, state: dict) -> None:\n        super(NovoGrad, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsgrad\', False)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        r""""""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    msg = (\n                        \'NovoGrad does not support sparse gradients, \'\n                        \'please consider SparseAdam instead\'\n                    )\n                    raise RuntimeError(msg)\n                amsgrad = group[\'amsgrad\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros([]).to(\n                        state[\'exp_avg\'].device\n                    )\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq.\n                        # grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros([]).to(\n                            state[\'exp_avg\'].device\n                        )\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsgrad:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                norm = torch.sum(torch.pow(grad, 2))\n\n                if exp_avg_sq == 0:\n                    exp_avg_sq.copy_(norm)\n                else:\n                    exp_avg_sq.mul_(beta2).add_(1 - beta2, norm)\n\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg.\n                    # till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                grad.div_(denom)\n                if group[\'weight_decay\'] != 0:\n                    grad.add_(group[\'weight_decay\'], p.data)\n                if group[\'grad_averaging\']:\n                    grad.mul_(1 - beta1)\n                exp_avg.mul_(beta1).add_(grad)\n\n                p.data.add_(-group[\'lr\'], exp_avg)\n\n        return loss\n'"
torch_optimizer/pid.py,4,"b'import torch\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import OptFloat, OptLossClosure, Params\n\n\nclass PID(Optimizer):\n    r""""""Implements PID optimization algorithm.\n\n    It has been proposed in `A PID Controller Approach for Stochastic\n    Optimization of Deep Networks`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        momentum: momentum factor (default: 0.0)\n        weight_decay: weight decay (L2 penalty) (default: 0.0)\n        dampening: dampening for momentum (default: 0.0)\n        derivative: D part of the PID (default: 10.0)\n        integral: I part of the PID (default: 5.0)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.PID(model.parameters(), lr=0.001, momentum=0.1)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ http://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf\n\n    Note:\n        Reference code: https://github.com/tensorboy/PIDOptimizer\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-3,\n        momentum: float = 0.0,\n        dampening: float = 0,\n        weight_decay: float = 0.0,\n        integral: float = 5.0,\n        derivative: float = 10.0,\n    ) -> None:\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            integral=integral,\n            derivative=derivative,\n        )\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if momentum < 0.0:\n            raise ValueError(\'Invalid momentum value: {}\'.format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n        if integral < 0.0:\n            raise ValueError(\'Invalid PID integral value: {}\'.format(integral))\n        if derivative < 0.0:\n            raise ValueError(\n                \'Invalid PID derivative value: {}\'.format(derivative)\n            )\n\n        super(PID, self).__init__(params, defaults)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        r""""""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            momentum = group[\'momentum\']\n            dampening = group[\'dampening\']\n            integral = group[\'integral\']\n            derivative = group[\'derivative\']\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if \'i_buffer\' not in param_state:\n                        i_buf = param_state[\'i_buffer\'] = torch.zeros_like(p)\n                        i_buf.mul_(momentum).add_(d_p)\n                    else:\n                        i_buf = param_state[\'i_buffer\']\n                        i_buf.mul_(momentum).add_(1 - dampening, d_p)\n                    if \'grad_buffer\' not in param_state:\n                        g_buf = param_state[\'grad_buffer\'] = torch.zeros_like(\n                            p\n                        )\n                        g_buf = d_p\n\n                        d_buf = param_state[\'d_buffer\'] = torch.zeros_like(p)\n                        d_buf.mul_(momentum).add_(d_p - g_buf)\n                    else:\n                        d_buf = param_state[\'d_buffer\']\n                        g_buf = param_state[\'grad_buffer\']\n                        d_buf.mul_(momentum).add_(1 - momentum, d_p - g_buf)\n                        self.state[p][\'grad_buffer\'] = d_p.clone()\n\n                    d_p = d_p.add_(integral, i_buf).add_(derivative, d_buf)\n                p.data.add_(-group[\'lr\'], d_p)\n        return loss\n'"
torch_optimizer/qhadam.py,3,"b'import torch\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import Betas2, OptFloat, OptLossClosure, Params, Nus2\n\n\n__all__ = (\'QHAdam\',)\n\n\nclass QHAdam(Optimizer):\n    r""""""Implements the QHAdam optimization algorithm.\n\n    It has been proposed in `Adaptive methods for Nonconvex Optimization`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        betas: coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        nus: immediate discount factors used to estimate the gradient and its\n            square (default: (1.0, 1.0))\n        eps: term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay: weight decay (L2 penalty) (default: 0)\n        decouple_weight_decay: whether to decouple the weight\n            decay from the gradient-based optimization step (default: False)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.QHAdam(model.parameters(), lr=0.1)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ https://arxiv.org/abs/1810.06801\n\n    Note:\n        Reference code: https://github.com/facebookresearch/qhoptim\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-3,\n        betas: Betas2 = (0.9, 0.999),\n        nus: Nus2 = (1.0, 1.0),\n        weight_decay: float = 0.0,\n        decouple_weight_decay: bool = False,\n        eps: float = 1e-8,\n    ):\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if eps < 0.0:\n            raise ValueError(\'Invalid epsilon value: {}\'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 0: {}\'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 1: {}\'.format(betas[1])\n            )\n        if weight_decay < 0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n\n        defaults = {\n            \'lr\': lr,\n            \'betas\': betas,\n            \'nus\': nus,\n            \'weight_decay\': weight_decay,\n            \'decouple_weight_decay\': decouple_weight_decay,\n            \'eps\': eps,\n        }\n        super(QHAdam, self).__init__(params, defaults)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            lr = group[\'lr\']\n            beta1, beta2 = group[\'betas\']\n            nu1, nu2 = group[\'nus\']\n            weight_decay = group[\'weight_decay\']\n            decouple_weight_decay = group[\'decouple_weight_decay\']\n            eps = group[\'eps\']\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n\n                d_p = p.grad.data\n                if d_p.is_sparse:\n                    raise RuntimeError(\n                        \'QHAdam does not support sparse gradients, \'\n                        \'please consider SparseAdam instead\'\n                    )\n\n                state = self.state[p]\n\n                if weight_decay != 0:\n                    if decouple_weight_decay:\n                        p.data.mul_(1 - lr * weight_decay)\n                    else:\n                        d_p.add_(weight_decay, p.data)\n\n                d_p_sq = d_p.mul(d_p)\n\n                if len(state) == 0:\n                    state[\'beta1_weight\'] = 0.0\n                    state[\'beta2_weight\'] = 0.0\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                state[\'beta1_weight\'] = 1.0 + beta1 * state[\'beta1_weight\']\n                state[\'beta2_weight\'] = 1.0 + beta2 * state[\'beta2_weight\']\n\n                beta1_weight = state[\'beta1_weight\']\n                beta2_weight = state[\'beta2_weight\']\n                exp_avg = state[\'exp_avg\']\n                exp_avg_sq = state[\'exp_avg_sq\']\n\n                beta1_adj = 1.0 - (1.0 / beta1_weight)\n                beta2_adj = 1.0 - (1.0 / beta2_weight)\n                exp_avg.mul_(beta1_adj).add_(1.0 - beta1_adj, d_p)\n                exp_avg_sq.mul_(beta2_adj).add_(1.0 - beta2_adj, d_p_sq)\n\n                avg_grad = exp_avg.mul(nu1)\n                if nu1 != 1.0:\n                    avg_grad.add_(1.0 - nu1, d_p)\n\n                avg_grad_rms = exp_avg_sq.mul(nu2)\n                if nu2 != 1.0:\n                    avg_grad_rms.add_(1.0 - nu2, d_p_sq)\n                avg_grad_rms.sqrt_()\n                if eps != 0.0:\n                    avg_grad_rms.add_(eps)\n\n                p.data.addcdiv_(-lr, avg_grad, avg_grad_rms)\n\n        return loss\n'"
torch_optimizer/qhm.py,3,"b'import torch\nfrom torch.optim.optimizer import Optimizer\nfrom .types import OptLossClosure, Params, OptFloat\n\n\n__all__ = (\'QHM\',)\n\n\nclass QHM(Optimizer):\n\n    GRAD = \'grad\'\n    DIRECT = \'direct\'\n\n    r""""""Implements quasi-hyperbolic momentum (QHM)  optimization algorithm.\n\n    It has been proposed in `Quasi-hyperbolic momentum and Adam for deep\n    learning`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        momentum: momentum factor (:math:`\\beta` from the paper)\n        nu: immediate discount factor (:math:`\\nu` from the paper)\n        weight_decay: weight decay (L2 regularization coefficient, times two)\n            (default: 0.0)\n        weight_decay_type: method of applying the weight decay:\n            ``""grad""`` for accumulation in the gradient\n            (same as :class:`torch.optim.SGD`) or\n            ``""direct""`` for direct application to the parameters\n            (default: ``""grad""``)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.QHM(model.parameters(), lr=0.1, momentum=0.9)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n\n    __ https://arxiv.org/abs/1810.06801\n\n    Note:\n        Reference code: https://github.com/facebookresearch/qhoptim\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-3,\n        momentum: float = 0.0,\n        nu: float = 0.7,\n        weight_decay: float = 0.0,\n        weight_decay_type: str = \'grad\',\n    ) -> None:\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if momentum < 0.0:\n            raise ValueError(\'Invalid momentum value: {}\'.format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n        if weight_decay_type not in (self.GRAD, self.DIRECT):\n            _type = weight_decay_type\n            msg = \'Invalid weight_decay_type value: {}\'.format(_type)\n            raise ValueError(msg)\n\n        defaults = {\n            \'lr\': lr,\n            \'momentum\': momentum,\n            \'nu\': nu,\n            \'weight_decay\': weight_decay,\n            \'weight_decay_type\': weight_decay_type,\n        }\n        super(QHM, self).__init__(params, defaults)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            lr, nu, momentum = group[\'lr\'], group[\'nu\'], group[\'momentum\']\n            weight_decay, weight_decay_type = (\n                group[\'weight_decay\'],\n                group[\'weight_decay_type\'],\n            )\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                param_state = self.state[p]\n\n                if weight_decay != 0:\n                    if weight_decay_type == self.GRAD:\n                        d_p.add_(weight_decay, p.data)\n                    else:\n                        p.data.mul_(1.0 - lr * weight_decay)\n\n                if len(param_state) == 0:\n                    param_state[\'momentum_buffer\'] = torch.zeros_like(p.data)\n\n                momentum_buffer = param_state[\'momentum_buffer\']\n                momentum_buffer.mul_(momentum).add_(1.0 - momentum, d_p)\n\n                p.data.add_(-lr * nu, momentum_buffer)\n                p.data.add_(-lr * (1.0 - nu), d_p)\n\n        return loss\n'"
torch_optimizer/radam.py,3,"b'import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import Betas2, OptFloat, OptLossClosure, Params\n\n\n__all__ = (\'RAdam\',)\n\n\nclass RAdam(Optimizer):\n    r""""""Implements RAdam optimization algorithm.\n\n    It has been proposed in `On the Variance of the Adaptive Learning\n    Rate and Beyond`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        betas: coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps: term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay: weight decay (L2 penalty) (default: 0)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.RAdam(model.parameters(), lr=0.1)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ https://arxiv.org/abs/1908.03265\n\n    Note:\n        Reference code: https://github.com/LiyuanLucasLiu/RAdam\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-3,\n        betas: Betas2 = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n    ) -> None:\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if eps < 0.0:\n            raise ValueError(\'Invalid epsilon value: {}\'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 0: {}\'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 1: {}\'.format(betas[1])\n            )\n        if weight_decay < 0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n\n        if (\n            isinstance(params, (list, tuple))\n            and len(params) > 0\n            and isinstance(params[0], dict)\n        ):\n            for param in params:\n                if \'betas\' in param and (\n                    param[\'betas\'][0] != betas[0]\n                    or param[\'betas\'][1] != betas[1]\n                ):\n                    param[\'buffer\'] = [[None, None, None] for _ in range(10)]\n\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            buffer=[[None, None, None] for _ in range(10)],\n        )\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        r""""""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            lr = group[\'lr\']\n            weight_decay = group[\'weight_decay\']\n            beta1, beta2 = group[\'betas\']\n            eps = group[\'eps\']\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    msg = (\n                        \'RAdam does not support sparse gradients, \'\n                        \'please consider SparseAdam instead\'\n                    )\n                    raise RuntimeError(msg)\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(\n                        p_data_fp32\n                    )\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[\'step\'] += 1\n                buffered = group[\'buffer\'][int(state[\'step\'] % 10)]\n                if state[\'step\'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\'step\']\n                    beta2_t = beta2 ** state[\'step\']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (\n                        1 - beta2_t\n                    )\n                    buffered[1] = N_sma\n\n                    # more conservative since it\'s an approximated value\n                    if N_sma >= 5:\n                        step_size = (\n                            lr\n                            * math.sqrt(\n                                (1 - beta2_t)\n                                * (N_sma - 4)\n                                / (N_sma_max - 4)\n                                * (N_sma - 2)\n                                / N_sma\n                                * N_sma_max\n                                / (N_sma_max - 2)\n                            )\n                            / (1 - beta1 ** state[\'step\'])\n                        )\n                    else:\n                        step_size = lr / (1 - beta1 ** state[\'step\'])\n                    buffered[2] = step_size\n\n                if weight_decay != 0:\n                    p_data_fp32.add_(-weight_decay * lr, p_data_fp32)\n\n                # more conservative since it\'s an approximated value\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(eps)\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n'"
torch_optimizer/sgdw.py,2,"b'import torch\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import OptFloat, OptLossClosure, Params, State\n\n\n__all__ = (\'SGDW\',)\n\n\nclass SGDW(Optimizer):\n    r""""""Implements SGDW algorithm.\n\n    It has been proposed in `Decoupled Weight Decay Regularization`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        momentum: momentum factor (default: 0)\n        weight_decay: weight decay (L2 penalty) (default: 0)\n        dampening: dampening for momentum (default: 0)\n        nesterov: enables Nesterov momentum (default: False)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.SGDW(model.parameters(), lr=0.1, momentum=0.9)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ https://arxiv.org/abs/1711.05101\n\n    Note:\n        Reference code: https://github.com/pytorch/pytorch/pull/22466\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-3,\n        momentum: float = 0.0,\n        dampening: float = 0.0,\n        weight_decay: float = 0.0,\n        nesterov: bool = False,\n    ) -> None:\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if momentum < 0.0:\n            raise ValueError(\'Invalid momentum value: {}\'.format(momentum))\n        if dampening < 0.0:\n            raise ValueError(\'Invalid dampening value: {}\'.format(dampening))\n        if weight_decay < 0.0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n        )\n        if nesterov and (momentum <= 0 or dampening != 0):\n            raise ValueError(\n                \'Nesterov momentum requires a momentum and zero dampening\'\n            )\n        super(SGDW, self).__init__(params, defaults)\n\n    def __setstate__(self, state: State) -> None:\n        super(SGDW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'nesterov\', False)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            momentum = group[\'momentum\']\n            dampening = group[\'dampening\']\n            nesterov = group[\'nesterov\']\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n\n                if p.grad.is_sparse:\n                    msg = (\n                        \'SGDW does not support sparse gradients, \'\n                        \'please consider SparseAdam instead\'\n                    )\n                    raise RuntimeError(msg)\n\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if \'momentum_buffer\' not in param_state:\n                        buf = param_state[\'momentum_buffer\'] = torch.clone(\n                            d_p\n                        ).detach()\n                    else:\n                        buf = param_state[\'momentum_buffer\']\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\n                    if nesterov:\n                        d_p = d_p.add(momentum, buf)\n                    else:\n                        d_p = buf\n\n                # Apply momentum\n                p.data.add_(-group[\'lr\'], d_p)\n\n                # Apply weight decay\n                if weight_decay != 0:\n                    p.data.add_(-group[\'lr\'], weight_decay)\n        return loss\n'"
torch_optimizer/shampoo.py,4,"b'import torch\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import OptFloat, OptLossClosure, Params\n\n\ndef _matrix_power(matrix: torch.Tensor, power: float) -> torch.Tensor:\n    # use CPU for svd for speed up\n    device = matrix.device\n    matrix = matrix.cpu()\n    u, s, v = torch.svd(matrix)\n    return (u @ s.pow_(power).diag() @ v.t()).to(device)\n\n\nclass Shampoo(Optimizer):\n    r""""""Implements Shampoo Optimizer Algorithm.\n\n    It has been proposed in `Shampoo: Preconditioned Stochastic Tensor\n    Optimization`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-3)\n        momentum: momentum factor (default: 0)\n        weight_decay: weight decay (L2 penalty) (default: 0)\n        epsilon: epsilon added to each mat_gbar_j for numerical stability\n            (default: 1e-4)\n        update_freq: update frequency to compute inverse (default: 1)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.Shampoo(model.parameters(), lr=0.01)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ https://arxiv.org/abs/1802.09568\n\n    Note:\n        Reference code: https://github.com/moskomule/shampoo.pytorch\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-1,\n        momentum: float = 0.0,\n        weight_decay: float = 0.0,\n        epsilon: float = 1e-4,\n        update_freq: int = 1,\n    ):\n\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if momentum < 0.0:\n            raise ValueError(\'Invalid momentum value: {}\'.format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n        if epsilon < 0.0:\n            raise ValueError(\'Invalid momentum value: {}\'.format(momentum))\n        if update_freq < 1:\n            raise ValueError(\'Invalid momentum value: {}\'.format(momentum))\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            epsilon=epsilon,\n            update_freq=update_freq,\n        )\n        super(Shampoo, self).__init__(params, defaults)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                order = grad.ndimension()\n                original_size = grad.size()\n                state = self.state[p]\n                momentum = group[\'momentum\']\n                weight_decay = group[\'weight_decay\']\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    if momentum > 0:\n                        state[\'momentum_buffer\'] = grad.clone()\n                    for dim_id, dim in enumerate(grad.size()):\n                        # precondition matrices\n                        state[\'precond_{}\'.format(dim_id)] = group[\n                            \'epsilon\'\n                        ] * torch.eye(dim, out=grad.new(dim, dim))\n                        state[\n                            \'inv_precond_{dim_id}\'.format(dim_id=dim_id)\n                        ] = grad.new(dim, dim).zero_()\n\n                if momentum > 0:\n                    grad.mul_(1 - momentum).add_(\n                        momentum, state[\'momentum_buffer\']\n                    )\n\n                if weight_decay > 0:\n                    grad.add_(group[\'weight_decay\'], p.data)\n\n                # See Algorithm 2 for detail\n                for dim_id, dim in enumerate(grad.size()):\n                    precond = state[\'precond_{}\'.format(dim_id)]\n                    inv_precond = state[\'inv_precond_{}\'.format(dim_id)]\n\n                    # mat_{dim_id}(grad)\n                    grad = grad.transpose_(0, dim_id).contiguous()\n                    transposed_size = grad.size()\n                    grad = grad.view(dim, -1)\n\n                    grad_t = grad.t()\n                    precond.add_(grad @ grad_t)\n                    if state[\'step\'] % group[\'update_freq\'] == 0:\n                        inv_precond.copy_(_matrix_power(precond, -1 / order))\n\n                    if dim_id == order - 1:\n                        # finally\n                        grad = grad_t @ inv_precond\n                        # grad: (-1, last_dim)\n                        grad = grad.view(original_size)\n                    else:\n                        # if not final\n                        grad = inv_precond @ grad\n                        # grad (dim, -1)\n                        grad = grad.view(transposed_size)\n\n                state[\'step\'] += 1\n                state[\'momentum_buffer\'] = grad\n                p.data.add_(-group[\'lr\'], grad)\n\n        return loss\n'"
torch_optimizer/types.py,0,"b'from typing import Iterable, Union, Callable, Dict, Optional, Tuple, Any\nfrom torch import Tensor\n\nParams = Union[Iterable[Tensor], Iterable[Dict[str, Any]]]\n\nLossClosure = Callable[[], float]\nOptLossClosure = Optional[LossClosure]\nBetas2 = Tuple[float, float]\nState = Dict[str, Any]\nOptFloat = Optional[float]\nNus2 = Tuple[float, float]\n'"
torch_optimizer/yogi.py,7,"b'import math\nimport torch\nimport torch.nn as nn\nfrom torch.optim.optimizer import Optimizer\n\nfrom .types import Betas2, OptFloat, OptLossClosure, Params\n\n\n__all__ = (\'Yogi\',)\n\n\nclass Yogi(Optimizer):\n    r""""""Implements Yogi Optimizer Algorithm.\n    It has been proposed in `Adaptive methods for Nonconvex Optimization`__.\n\n    Arguments:\n        params: iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr: learning rate (default: 1e-2)\n        betas: coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps: term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        initial_accumulator: initial values for first and\n            second moments (default: 1e-6)\n        weight_decay: weight decay (L2 penalty) (default: 0)\n\n    Example:\n        >>> import torch_optimizer as optim\n        >>> optimizer = optim.Yogi(model.parameters(), lr=0.01)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization  # noqa\n\n    Note:\n        Reference code: https://github.com/4rtemi5/Yogi-Optimizer_Keras\n    """"""\n\n    def __init__(\n        self,\n        params: Params,\n        lr: float = 1e-2,\n        betas: Betas2 = (0.9, 0.999),\n        eps: float = 1e-3,\n        initial_accumulator: float = 1e-6,\n        weight_decay: float = 0,\n    ) -> None:\n        if lr <= 0.0:\n            raise ValueError(\'Invalid learning rate: {}\'.format(lr))\n        if eps < 0.0:\n            raise ValueError(\'Invalid epsilon value: {}\'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 0: {}\'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                \'Invalid beta parameter at index 1: {}\'.format(betas[1])\n            )\n        if weight_decay < 0:\n            raise ValueError(\n                \'Invalid weight_decay value: {}\'.format(weight_decay)\n            )\n\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            initial_accumulator=initial_accumulator,\n            weight_decay=weight_decay,\n        )\n        super(Yogi, self).__init__(params, defaults)\n\n    def step(self, closure: OptLossClosure = None) -> OptFloat:\n        r""""""Performs a single optimization step.\n\n        Arguments:\n            closure: A closure that reevaluates the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'Yogi does not support sparse gradients, \'\n                        \'please consider SparseAdam instead\'\n                    )\n\n                state = self.state[p]\n\n                # State initialization\n                # Followed from official implementation in tensorflow addons:\n                # https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/yogi.py#L118 # noqa\n                # For more details refer to the discussion:\n                # https://github.com/jettify/pytorch-optimizer/issues/77\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = nn.init.constant_(\n                        torch.empty_like(\n                            p.data, memory_format=torch.preserve_format\n                        ),\n                        group[\'initial_accumulator\'],\n                    )\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = nn.init.constant_(\n                        torch.empty_like(\n                            p.data, memory_format=torch.preserve_format\n                        ),\n                        group[\'initial_accumulator\'],\n                    )\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                grad_squared = grad.mul(grad)\n\n                exp_avg_sq.addcmul_(\n                    -(1 - beta2),\n                    torch.sign(exp_avg_sq - grad_squared),\n                    grad_squared,\n                )\n\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(\n                    group[\'eps\']\n                )\n                step_size = group[\'lr\'] / bias_correction1\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n        return loss\n'"
