file_path,api_count,code
demo.py,5,"b'import cv2\nimport numpy as np\nimport os\nimport torch\n\n\nfrom torch.nn.functional import upsample\nfrom dataloaders import utils\nimport networks.deeplab_resnet as resnet\n\nfrom glob import glob\nfrom copy import deepcopy\n\n\ndrawing = False\nstart = False\n\n# TODO: import test image path\nimage_list = glob(os.path.join(\'ims/\', \'*.\'+\'jpg\'))\nimage = cv2.imread(image_list[0])\nimg_shape = (450, 450)\nimage = utils.fixed_resize(image, img_shape).astype(np.uint8)\n\nw = img_shape[0]\nh = img_shape[1]\noutput = np.zeros((w, h, 3), np.uint8)\nthres = 0.8\n\nleft = 0xFFF\nright = 0\nup = 0xFFF\ndown = 0\n\ngpu_id = 0\ndevice = torch.device(""cuda:""+str(gpu_id) if torch.cuda.is_available() else ""cpu"")\n\n#  Create the network and load the weights\nnet = resnet.resnet101(1, nInputChannels=4, classifier=\'psp\')\nprint(""Initializing weights from: {}"".format(os.path.join(\'models/\', \'deepgc_pascal_epoch-99.pth\')))\nstate_dict_checkpoint = torch.load(os.path.join(\'models/\', \'deepgc_pascal_epoch-99.pth\'),\n                                   map_location=lambda storage, loc: storage)\n\nnet.load_state_dict(state_dict_checkpoint)\nnet.eval()\nnet.to(device)\n\n\ndef interactive_drawing(event, x, y, flag, params):\n    global xs, ys, ix, iy, drawing, image, output, left, right, up, down\n\n    if event == cv2.EVENT_LBUTTONDOWN:\n        drawing = True\n        ix, iy = x, y\n        xs, ys = x, y\n        left = min(left, x)\n        right = max(right, x)\n        up = min(up, y)\n        down = max(down, y)\n\n    elif event == cv2.EVENT_MOUSEMOVE:\n        if drawing is True:\n            cv2.line(image, (ix, iy), (x, y), (0, 0, 255), 2)\n            cv2.line(output, (ix, iy), (x, y), (255, 255, 255), 1)\n            ix = x\n            iy = y\n            left = min(left, x)\n            right = max(right, x)\n            up = min(up, y)\n            down = max(down, y)\n\n    elif event == cv2.EVENT_LBUTTONUP:\n        drawing = False\n        cv2.line(image, (ix, iy), (x, y), (0, 0, 255), 2)\n        cv2.line(output, (ix, iy), (x, y), (255, 255, 255), 1)\n        ix = x\n        iy = y\n        cv2.line(image, (ix, iy), (xs, ys), (0, 0, 255), 2)\n        cv2.line(output, (ix, iy), (xs, ys), (255, 255, 255), 1)\n    return x, y\n\ndef main():\n    global image, output\n    cv2.namedWindow(\'draw\', flags=cv2.WINDOW_NORMAL)\n    cv2.setMouseCallback(\'draw\', interactive_drawing)\n\n    image_idx = 0\n    while(1):\n        cv2.imshow(\'draw\', image)\n        k = cv2.waitKey(1) & 0xFF\n        if k != 255:\n            # print(k)\n            pass\n        if k == 100: # D\n            cv2.imwrite(\'./\' + str(image_idx) + \'out.png\', image)\n        if k == 115:\n            global left, right, up, down\n            left = 0xFFF\n            right = 0\n            up = 0xFFF\n            down = 0\n            drawing = False  # true if mouse is pressed\n            image = cv2.imread(image_list[image_idx])\n            image = utils.fixed_resize(image, (450, 450)).astype(np.uint8)\n            sp = image.shape\n            w = sp[0]\n            h = sp[1]\n            output = np.zeros((w, h, 3), np.uint8)\n            while (1):\n                cv2.imshow(\'draw\', image)\n                k = cv2.waitKey(1) & 0xFF\n                if k == 32:\n                    break\n                if k == 27:\n                    image = cv2.imread(image_list[image_idx])\n                    image = utils.fixed_resize(image, (450, 450)).astype(np.uint8)\n                    output = np.zeros((w, h, 3), np.uint8)\n\n            tmp = (output[:, :, 0] > 0).astype(np.uint8)\n            tmp_ = deepcopy(tmp)\n            fill_mask = np.ones((tmp.shape[0] + 2, tmp.shape[1] + 2))\n            fill_mask[1:-1, 1:-1] = tmp_\n            fill_mask = fill_mask.astype(np.uint8)\n            cv2.floodFill(tmp_, fill_mask, (int((left + right) / 2), int((up + down) / 2)), 5)\n            tmp_ = tmp_.astype(np.int8)\n\n            output = cv2.resize(output, img_shape)\n\n            tmp_ = tmp_.astype(np.int8)\n            tmp_[tmp_ == 5] = -1  # pixel inside bounding box\n            tmp_[tmp_ == 0] = 1  # pixel on and outside bounding box\n\n            tmp = (tmp == 0).astype(np.uint8)\n\n            dismap = cv2.distanceTransform(tmp, cv2.DIST_L2, cv2.DIST_MASK_PRECISE)  # compute distance inside and outside bounding box\n            dismap = tmp_ * dismap + 128\n\n            dismap[dismap > 255] = 255\n            dismap[dismap < 0] = 0\n            dismap = dismap\n\n            dismap = utils.fixed_resize(dismap, (450, 450)).astype(np.uint8)\n\n            dismap = np.expand_dims(dismap, axis=-1)\n\n            image = image[:, :, ::-1] # change to rgb\n            merge_input = np.concatenate((image, dismap), axis=2).astype(np.float32)\n            inputs = torch.from_numpy(merge_input.transpose((2, 0, 1))[np.newaxis, ...])\n\n            # Run a forward pass\n            inputs = inputs.to(device)\n            outputs = net.forward(inputs)\n            outputs = upsample(outputs, size=(450, 450), mode=\'bilinear\', align_corners=True)\n            outputs = outputs.to(torch.device(\'cpu\'))\n\n            prediction = np.transpose(outputs.data.numpy()[0, ...], (1, 2, 0))\n            prediction = 1 / (1 + np.exp(-prediction))\n            prediction = np.squeeze(prediction)\n            prediction[prediction>thres] = 255\n            prediction[prediction<=thres] = 0\n\n            prediction = np.expand_dims(prediction, axis=-1).astype(np.uint8)\n            image = image[:, :, ::-1] # change to bgr\n            display_mask = np.concatenate([np.zeros_like(prediction), np.zeros_like(prediction), prediction], axis=-1)\n            image = cv2.addWeighted(image, 0.9, display_mask, 0.5, 0.1)\n\n        if k == 99:\n            break\n\n        if k == 110:\n            image_idx += 1\n            if image_idx >= len(image_list):\n                print(\'Already the last image. Starting from the beginning.\')\n                image_idx = 0\n            image = cv2.imread(image_list[image_idx])\n            image = utils.fixed_resize(image, (450, 450)).astype(np.uint8)\n            sp = image.shape\n            w = sp[0]\n            h = sp[1]\n            output = np.zeros((w, h, 3), np.uint8)\n\n        if k == 112:\n            image_idx -= 1\n            if image_idx < 0:\n                print(\'Reached the first image. Starting from the end.\')\n                image_idx = len(image_list)-1\n            image = cv2.imread(image_list[image_idx])\n            image = utils.fixed_resize(image, (450, 450)).astype(np.uint8)\n            sp = image.shape\n            w = sp[0]\n            h = sp[1]\n            output = np.zeros((w, h, 3), np.uint8)\n\n\n    cv2.destroyAllWindows()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
mypath.py,0,"b""class Path(object):\n    @staticmethod\n    def db_root_dir(database):\n        if database == 'pascal':\n            return '/path/to/Segmentation/VOCdevkit/VOC2012'   # folder that contains VOCdevkit/.\n        elif database == 'sbd':\n            return '/path/to/Segmentation/benchmark_RELEASE'  # folder that contains dataset/.\n        elif database == 'coco':\n            return '/path/to/coco'  # folder that contains annotations/.\n        else:\n            print('Database {} not available.'.format(database))\n            raise NotImplementedError\n\n    @staticmethod\n    def models_dir():\n        return '/path/to/Models/'\n\n"""
train.py,8,"b'import socket\nimport timeit\nfrom datetime import datetime\nimport os\nimport glob\nfrom collections import OrderedDict\n\n\n# PyTorch includes\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.nn.functional import upsample\n\n# Tensorboard include\nfrom tensorboardX import SummaryWriter\n\n# Custom includes\nfrom dataloaders.combine_dbs import CombineDBs as combine_dbs\nfrom dataloaders import pascal, sbd\nfrom networks import deeplab_resnet as resnet\nfrom layers.loss import class_balanced_cross_entropy_loss\nfrom dataloaders import custom_transforms as tr\nfrom dataloaders.utils import generate_param_report\n     \n\ngpu_id = 0\nprint(\'Using GPU: {} \'.format(gpu_id))\n# Setting parameters\nuse_sbd = True\nnEpochs = 200  # Number of epochs for training\nresume_epoch = 0  # Default is 0, change if want to resume\n\np = OrderedDict()  # Parameters to include in report\nclassifier = \'psp\'  # Head classifier to use\np[\'trainBatch\'] = 4  # Training batch size\ntestBatch = 4  # Testing batch size\nuseTest = True  # See evolution of the test set when training\nnTestInterval = 10  # Run on test set every nTestInterval epochs\nsnapshot = 10  # Store a model every snapshot epochs\nnInputChannels = 4  # Number of input channels (RGB + Distance Map of bounding box)\nzero_pad_crop = True  # Insert zero padding when cropping the image\np[\'nAveGrad\'] = 1  # Average the gradient of several iterations\np[\'lr\'] = 1e-4  # Learning rate\np[\'wd\'] = 5e-4  # Weight decay\np[\'momentum\'] = 0.9  # Momentum\n\nsave_dir_root = os.path.join(os.path.dirname(os.path.abspath(__file__)))\nexp_name = os.path.dirname(os.path.abspath(__file__)).split(\'/\')[-1]\n\nif resume_epoch != 0:\n    runs = sorted(glob.glob(os.path.join(save_dir_root, \'run\', \'run_*\')))\n    run_id = int(runs[-1].split(\'_\')[-1]) if runs else 0\nelse:\n    runs = sorted(glob.glob(os.path.join(save_dir_root, \'run\', \'run_*\')))\n    run_id = int(runs[-1].split(\'_\')[-1]) + 1 if runs else 0\n\nsave_dir = os.path.join(save_dir_root, \'run\', \'run_\' + str(run_id))\nif not os.path.exists(os.path.join(save_dir, \'models\')):\n    os.makedirs(os.path.join(save_dir, \'models\'))\n\n# Network definition\nmodelName = \'deepgc_pascal\'\nnet = resnet.resnet101(1, pretrained=True, nInputChannels=nInputChannels, classifier=classifier)\n\nif resume_epoch == 0:\n    print(""Initializing from pretrained Deeplab-v2 model"")\nelse:\n    print(""Initializing weights from: {}"".format(\n        os.path.join(save_dir, \'models\', modelName + \'_epoch-\' + str(resume_epoch - 1) + \'.pth\')))\n    net.load_state_dict(\n        torch.load(os.path.join(save_dir, \'models\', modelName + \'_epoch-\' + str(resume_epoch - 1) + \'.pth\'),\n                   map_location=lambda storage, loc: storage)) # Load all tensors onto the CPU\n\ntrain_params = [{\'params\': resnet.get_1x_lr_params(net), \'lr\': p[\'lr\']},\n                {\'params\': resnet.get_10x_lr_params(net), \'lr\': p[\'lr\'] * 10}]\nif gpu_id >= 0:\n    torch.cuda.set_device(device=gpu_id)\n    net.cuda()\n\nif resume_epoch != nEpochs:\n    # Logging into Tensorboard\n    log_dir = os.path.join(save_dir, \'models\', datetime.now().strftime(\'%b%d_%H-%M-%S\') + \'_\' + socket.gethostname())\n    writer = SummaryWriter(log_dir=log_dir)\n\n    # Use the following optimizer\n    optimizer = optim.SGD(train_params, lr=p[\'lr\'], momentum=p[\'momentum\'], weight_decay=p[\'wd\'])\n    p[\'optimizer\'] = str(optimizer)\n\n    composed_transforms_tr = transforms.Compose([\n        tr.RandomHorizontalFlip(),\n        tr.ScaleNRotate(rots=(-15, 15), scales=(.75, 1.25)),\n        tr.FixedResize(resolutions={\'image\': (450, 450), \'gt\': (450, 450)}),\n        tr.DistanceMap(v=0.15, elem=\'gt\'),\n        tr.ConcatInputs(elems=(\'image\', \'distance_map\')),\n        tr.ToTensor()])\n\n    composed_transforms_ts = transforms.Compose([\n        tr.FixedResize(resolutions={\'image\': (450, 450), \'gt\': (450, 450)}),\n        tr.DistanceMap(v=0.15, elem=\'gt\'),\n        tr.ConcatInputs(elems=(\'image\', \'distance_map\')),\n        tr.ToTensor()])\n\n    voc_train = pascal.VOCSegmentation(split=\'train\', transform=composed_transforms_tr)\n    voc_val = pascal.VOCSegmentation(split=\'val\', transform=composed_transforms_ts)\n\n    if use_sbd:\n        sbd_train = sbd.SBDSegmentation(split=[\'train\', \'val\'], transform=composed_transforms_tr, retname=True)\n        db_train = combine_dbs([voc_train, sbd_train], excluded=[voc_val])\n    else:\n        db_train = voc_train\n\n    trainloader = DataLoader(db_train, batch_size=p[\'trainBatch\'], shuffle=True, num_workers=2)\n    testloader = DataLoader(voc_val, batch_size=testBatch, shuffle=False, num_workers=2)\n\n    generate_param_report(os.path.join(save_dir, exp_name + \'.txt\'), p)\n\n    num_img_tr = len(trainloader)\n    num_img_ts = len(testloader)\n    running_loss_tr = 0.0\n    running_loss_ts = 0.0\n    aveGrad = 0\n    print(""Training Network"")\n\n    # Main Training and Testing Loop\n    for epoch in range(resume_epoch, nEpochs):\n        start_time = timeit.default_timer()\n\n        net.train()\n        for ii, sample_batched in enumerate(trainloader):\n\n            inputs, gts = sample_batched[\'concat\'], sample_batched[\'gt\']\n\n            # Forward-Backward of the mini-batch\n            inputs, gts = Variable(inputs, requires_grad=True), Variable(gts)\n            if gpu_id >= 0:\n                inputs, gts = inputs.cuda(), gts.cuda()\n\n            output = net.forward(inputs)\n            output = upsample(output, size=(450, 450), mode=\'bilinear\', align_corners=True)\n\n\n            # Compute the losses, side outputs and fuse\n            loss = class_balanced_cross_entropy_loss(output, gts, size_average=True, batch_average=True)\n            running_loss_tr += loss.item()\n\n            # Print stuff\n            if ii % num_img_tr == num_img_tr - 1:\n                running_loss_tr = running_loss_tr / num_img_tr\n                writer.add_scalar(\'data/total_loss_epoch\', running_loss_tr, epoch)\n                print(\'[Epoch: %d, numImages: %5d]\' % (epoch, ii * p[\'trainBatch\'] + inputs.data.shape[0]))\n                print(\'Loss: %f\' % running_loss_tr)\n                running_loss_tr = 0\n                stop_time = timeit.default_timer()\n                print(""Execution time: "" + str(stop_time - start_time) + ""\\n"")\n\n            # Backward the averaged gradient\n            loss /= p[\'nAveGrad\']\n            loss.backward()\n            aveGrad += 1\n\n            # Update the weights once in p[\'nAveGrad\'] forward passes\n            if aveGrad % p[\'nAveGrad\'] == 0:\n                writer.add_scalar(\'data/total_loss_iter\', loss.item(), ii + num_img_tr * epoch)\n                optimizer.step()\n                optimizer.zero_grad()\n                aveGrad = 0\n\n        # Save the model\n        if (epoch % snapshot) == snapshot - 1 and epoch != 0:\n            torch.save(net.state_dict(), os.path.join(save_dir, \'models\', modelName + \'_epoch-\' + str(epoch) + \'.pth\'))\n\n        # One testing epoch\n        if useTest and epoch % nTestInterval == (nTestInterval - 1):\n            net.eval()\n            for ii, sample_batched in enumerate(testloader):\n                inputs, gts = sample_batched[\'concat\'], sample_batched[\'gt\']\n\n                # Forward pass of the mini-batch\n                inputs, gts = Variable(inputs, requires_grad=True), Variable(gts)\n                if gpu_id >= 0:\n                    inputs, gts = inputs.cuda(), gts.cuda()\n\n                with torch.no_grad():\n                    output = net.forward(inputs)\n                output = upsample(output, size=(450, 450), mode=\'bilinear\', align_corners=True)\n\n                # Compute the losses, side outputs and fuse\n                loss = class_balanced_cross_entropy_loss(output, gts, size_average=True)\n                running_loss_ts += loss.item()\n\n                # Print stuff\n                if ii % num_img_ts == num_img_ts - 1:\n                    running_loss_ts = running_loss_ts / num_img_ts\n                    print(\'[Epoch: %d, numImages: %5d]\' % (epoch, ii * testBatch + inputs.data.shape[0]))\n                    writer.add_scalar(\'data/test_loss_epoch\', running_loss_ts, epoch)\n                    print(\'Loss: %f\' % running_loss_ts)\n                    running_loss_ts = 0\n\n    writer.close()\n'"
train_coco.py,8,"b'import socket\nimport timeit\nfrom datetime import datetime\nimport os\nimport glob\nfrom collections import OrderedDict\n\n# PyTorch includes\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.nn.functional import upsample\n\n# Tensorboard include\nfrom tensorboardX import SummaryWriter\n\n# Custom includes\nfrom dataloaders import coco\nfrom networks import deeplab_resnet as resnet\nfrom layers.loss import class_balanced_cross_entropy_loss\nfrom dataloaders import custom_transforms as tr\nfrom dataloaders.utils import generate_param_report\n\ngpu_id = 0\nprint(\'Using GPU: {} \'.format(gpu_id))\n# Setting parameters\nnEpochs = 200  # Number of epochs for training\nresume_epoch = 0  # Default is 0, change if want to resume\n\np = OrderedDict()  # Parameters to include in report\nclassifier = \'psp\'  # Head classifier to use\np[\'trainBatch\'] = 4  # Training batch size\ntestBatch = 4  # Testing batch size\nuseTest = True  # See evolution of the test set when training\nnTestInterval = 10  # Run on test set every nTestInterval epochs\nsnapshot = 1  # Store a model every snapshot epochs\nnInputChannels = 4  # Number of input channels (RGB + Distance Map of bounding box)\nzero_pad_crop = True  # Insert zero padding when cropping the image\np[\'nAveGrad\'] = 1  # Average the gradient of several iterations\np[\'lr\'] = 1e-4  # Learning rate\np[\'wd\'] = 5e-4  # Weight decay\np[\'momentum\'] = 0.9  # Momentum\n\nsave_dir_root = os.path.join(os.path.dirname(os.path.abspath(__file__)))\nexp_name = os.path.dirname(os.path.abspath(__file__)).split(\'/\')[-1]\n\nif resume_epoch != 0:\n    runs = sorted(glob.glob(os.path.join(save_dir_root, \'run\', \'run_*\')))\n    run_id = int(runs[-1].split(\'_\')[-1]) if runs else 0\nelse:\n    runs = sorted(glob.glob(os.path.join(save_dir_root, \'run\', \'run_*\')))\n    run_id = int(runs[-1].split(\'_\')[-1]) + 1 if runs else 0\n\nsave_dir = os.path.join(save_dir_root, \'run\', \'run_\' + str(run_id))\nif not os.path.exists(os.path.join(save_dir, \'models\')):\n    os.makedirs(os.path.join(save_dir, \'models\'))\n\n# Network definition\nmodelName = \'deepgc_pascal\'\nnet = resnet.resnet101(1, pretrained=True, nInputChannels=nInputChannels, classifier=classifier)\n\nif resume_epoch == 0:\n    print(""Initializing from pretrained Deeplab-v2 model"")\nelse:\n    print(""Initializing weights from: {}"".format(\n        os.path.join(save_dir, \'models\', modelName + \'_epoch-\' + str(resume_epoch - 1) + \'.pth\')))\n    net.load_state_dict(\n        torch.load(os.path.join(save_dir, \'models\', modelName + \'_epoch-\' + str(resume_epoch - 1) + \'.pth\'),\n                   map_location=lambda storage, loc: storage))  # Load all tensors onto the CPU\n\n# TODO: Load model trained on VOC and SBD datasets\n\ntrain_params = [{\'params\': resnet.get_1x_lr_params(net), \'lr\': p[\'lr\']},\n                {\'params\': resnet.get_10x_lr_params(net), \'lr\': p[\'lr\'] * 10}]\n\nif gpu_id >= 0:\n    torch.cuda.set_device(device=gpu_id)\n    net.cuda()\n\nif resume_epoch != nEpochs:\n    # Logging into Tensorboard\n    log_dir = os.path.join(save_dir, \'models\', datetime.now().strftime(\'%b%d_%H-%M-%S\') + \'_\' + socket.gethostname())\n    writer = SummaryWriter(log_dir=log_dir)\n\n    # Use the following optimizer\n    optimizer = optim.SGD(train_params, lr=p[\'lr\'], momentum=p[\'momentum\'], weight_decay=p[\'wd\'])\n    p[\'optimizer\'] = str(optimizer)\n\n    composed_transforms_tr = transforms.Compose([\n        tr.RandomHorizontalFlip(),\n        tr.ScaleNRotate(rots=(-15, 15), scales=(.75, 1.25)),\n        tr.FixedResize(resolutions={\'image\': (450, 450), \'gt\': (450, 450)}),\n        tr.DistanceMap(v=0.15, elem=\'gt\'),\n        tr.ConcatInputs(elems=(\'image\', \'distance_map\')),\n        tr.ToTensor()])\n\n    composed_transforms_ts = transforms.Compose([\n        tr.FixedResize(resolutions={\'image\': (450, 450), \'gt\': (450, 450)}),\n        tr.DistanceMap(v=0.15, elem=\'gt\'),\n        tr.ConcatInputs(elems=(\'image\', \'distance_map\')),\n        tr.ToTensor()])\n\n    coco_train = coco.COCOSegmentation(split=\'train\', transform=composed_transforms_tr)\n    coco_val = coco.COCOSegmentation(split=\'val\', transform=composed_transforms_ts)\n\n    trainloader = DataLoader(coco_train, batch_size=p[\'trainBatch\'], shuffle=True, num_workers=4)\n    testloader = DataLoader(coco_val, batch_size=testBatch, shuffle=False, num_workers=4)\n\n    generate_param_report(os.path.join(save_dir, exp_name + \'.txt\'), p)\n\n    num_img_tr = len(trainloader)\n    num_img_ts = len(testloader)\n    running_loss_tr = 0.0\n    running_loss_ts = 0.0\n    aveGrad = 0\n    print(""Training Network"")\n\n    # Main Training and Testing Loop\n    for epoch in range(resume_epoch, nEpochs):\n        start_time = timeit.default_timer()\n\n        net.train()\n        for ii, sample_batched in enumerate(trainloader):\n\n            inputs, gts = sample_batched[\'concat\'], sample_batched[\'gt\']\n\n            # Forward-Backward of the mini-batch\n            inputs, gts = Variable(inputs, requires_grad=True), Variable(gts)\n            if gpu_id >= 0:\n                inputs, gts = inputs.cuda(), gts.cuda()\n\n            output = net.forward(inputs)\n            output = upsample(output, size=(450, 450), mode=\'bilinear\', align_corners=True)\n\n            # Compute the losses, side outputs and fuse\n            loss = class_balanced_cross_entropy_loss(output, gts, size_average=True, batch_average=True)\n            running_loss_tr += loss.item()\n\n            # Print stuff\n            if ii % num_img_tr == num_img_tr - 1:\n                running_loss_tr = running_loss_tr / num_img_tr\n                writer.add_scalar(\'train/total_loss_epoch\', running_loss_tr, epoch)\n                print(\'[Epoch: %d, numImages: %5d]\' % (epoch, ii * p[\'trainBatch\'] + inputs.data.shape[0]))\n                print(\'Loss: %f\' % running_loss_tr)\n                running_loss_tr = 0\n                stop_time = timeit.default_timer()\n                print(""Execution time: "" + str(stop_time - start_time) + ""\\n"")\n\n            # Backward the averaged gradient\n            loss /= p[\'nAveGrad\']\n            loss.backward()\n            aveGrad += 1\n\n            # Update the weights once in p[\'nAveGrad\'] forward passes\n            if aveGrad % p[\'nAveGrad\'] == 0:\n                writer.add_scalar(\'train/total_loss_iter\', loss.item(), ii + num_img_tr * epoch)\n                optimizer.step()\n                optimizer.zero_grad()\n                aveGrad = 0\n\n        # Save the model\n        if (epoch % snapshot) == snapshot - 1 and epoch != 0:\n            torch.save(net.state_dict(), os.path.join(save_dir, \'models\', modelName + \'_epoch-\' + str(epoch) + \'.pth\'))\n\n        # One testing epoch\n        if useTest and epoch % nTestInterval == (nTestInterval - 1):\n            net.eval()\n            for ii, sample_batched in enumerate(testloader):\n                inputs, gts = sample_batched[\'concat\'], sample_batched[\'gt\']\n\n                # Forward pass of the mini-batch\n                inputs, gts = Variable(inputs, requires_grad=True), Variable(gts)\n                if gpu_id >= 0:\n                    inputs, gts = inputs.cuda(), gts.cuda()\n\n                with torch.no_grad():\n                    output = net.forward(inputs)\n                output = upsample(output, size=(450, 450), mode=\'bilinear\', align_corners=True)\n\n                # Compute the losses, side outputs and fuse\n                loss = class_balanced_cross_entropy_loss(output, gts, size_average=True)\n                running_loss_ts += loss.item()\n\n\n                # Print stuff\n                if ii % num_img_ts == num_img_ts - 1:\n                    running_loss_ts = running_loss_ts / num_img_ts\n                    writer.add_scalar(\'val/total_loss_epoch\', running_loss_ts, epoch)\n                    print(\'Validation:\')\n                    print(\'[Epoch: %d, numImages: %5d]\' % (epoch, ii * testBatch + inputs.data.shape[0]))\n                    writer.add_scalar(\'data/test_loss_epoch\', running_loss_ts, epoch)\n                    print(\'Loss: %f\' % running_loss_ts)\n                    running_loss_ts = 0\n\n    writer.close()\n'"
dataloaders/__init__.py,0,b''
dataloaders/coco.py,2,"b'import os\nimport numpy as np\nfrom pycocotools import mask as maskUtils\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import Dataset\nfrom mypath import Path\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nclass COCOSegmentation(Dataset):\n    """"""\n    PascalVoc dataset\n    """"""\n\n    def __init__(self,\n                 base_dir=Path.db_root_dir(\'coco\'),\n                 split=\'train\',\n                 year=\'2014\',\n                 transform=None,\n                ):\n        """"""\n        :param base_dir: path to COCO dataset directory\n        :param split: train/val\n        :param transform: transform to apply\n        """"""\n        super().__init__()\n        self._base_dir = base_dir\n        self._image_dir = os.path.join(self._base_dir, \'images\', \'{}{}\'.format(split, year))\n        self._annot_dir = os.path.join(self._base_dir, \'annotations\', \'instances_{}{}.json\'.format(split, year))\n\n        self.year = year\n        if isinstance(split, str):\n            self.split = [split]\n        else:\n            split.sort()\n            self.split = split\n\n        self.transform = transform\n\n\n        self.im_ids = []\n        self.image_info = []\n        self.objects = []\n\n        coco = COCO(self._annot_dir)\n        class_ids = sorted(coco.getCatIds())\n\n        for i in class_ids:\n            self.im_ids.extend(list(coco.getImgIds(catIds=[i])))\n        # Remove duplicates\n        self.im_ids = list(set(self.im_ids))\n\n        for i in self.im_ids:\n            image_info = {\n                ""id"": i,\n                ""path"": os.path.join(self._image_dir, coco.imgs[i][\'file_name\']),\n                ""width"": coco.imgs[i][""width""],\n                ""height"": coco.imgs[i][""height""],\n                ""annotations"": coco.loadAnns(coco.getAnnIds(\n                imgIds=[i], catIds=class_ids, iscrowd=None))\n            }\n            self.image_info.append(image_info)\n\n        for image_info in self.image_info:\n            path = image_info[\'path\']\n            width = image_info[\'width\']\n            height = image_info[\'height\']\n            annotations = image_info[\'annotations\']\n            for annotation in annotations:\n                objects = {\n                    \'image_path\': path,\n                    \'mask_annotation\': annotation,\n                    \'height\': height,\n                    \'width\': width\n                }\n                self.objects.append(objects)\n\n        # Display stats\n        print(\'Number of images: {:d}\\nNumber of objects: {:d}\'.format(len(self.image_info),\n                                                                        len(self.objects)))\n\n    def __len__(self):\n        return len(self.objects)\n\n    def _make_img_gt_point_pair(self, index):\n        object = self.objects[index]\n        image_path = object[\'image_path\']\n        annotation = object[\'mask_annotation\']\n        height = object[\'height\']\n        width = object[\'width\']\n\n        _target = self.annToMask(annotation, height, width)\n\n        if annotation[\'iscrowd\']:\n            # For crowd masks, annToMask() sometimes returns a mask\n            # smaller than the given dimensions. If so, resize it.\n            if _target.shape[0] != height or _target.shape[1] != width:\n                _target = np.ones([height, width], dtype=bool)\n\n        # Read Image\n        _img = np.array(Image.open(image_path).convert(\'RGB\')).astype(np.float32)\n\n        return _img, _target\n\n    def annToRLE(self, ann, height, width):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        segm = ann[\'segmentation\']\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, height, width)\n            rle = maskUtils.merge(rles)\n        elif isinstance(segm[\'counts\'], list):\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, height, width)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann, height, width):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann, height, width)\n        m = maskUtils.decode(rle)\n        return m\n\n    def __getitem__(self, index):\n        _img, _target = self._make_img_gt_point_pair(index)\n        sample = {\'image\': _img, \'gt\': _target}\n\n        if self.transform is not None:\n            sample = self.transform(sample)\n\n        return sample\n\n\n\n    def __str__(self):\n        return \'COCO\' + str(self.year) + \'(split=\' + str(self.split)\n\n\nif __name__ == \'__main__\':\n    from dataloaders import custom_transforms as tr\n    from torch.utils.data import DataLoader\n    from torchvision import transforms\n    import matplotlib.pyplot as plt\n\n    composed_transforms_tr = transforms.Compose([\n        tr.RandomHorizontalFlip(),\n        tr.ScaleNRotate(rots=(-15, 15), scales=(.75, 1.25)),\n        tr.FixedResize(resolutions={\'image\': (450, 450), \'gt\': (450, 450)}),\n        tr.DistanceMap(v=0.15, elem=\'gt\'),\n        tr.ConcatInputs(elems=(\'image\', \'distance_map\')),\n        tr.ToTensor()])\n\n    voc_train = COCOSegmentation(split=\'val\', transform=composed_transforms_tr)\n\n    dataloader = DataLoader(voc_train, batch_size=5, shuffle=True, num_workers=4)\n\n    for ii, sample in enumerate(dataloader):\n        for jj in range(sample[""image""].size()[0]):\n            dismap = sample[\'distance_map\'][jj].numpy()\n            gt = sample[\'gt\'][jj].numpy()\n            gt[gt > 0] = 255\n            gt = np.array(gt[0]).astype(np.uint8)\n            dismap = np.array(dismap[0]).astype(np.uint8)\n            display = 0.9 * gt + 0.4 * dismap\n            display = display.astype(np.uint8)\n            plt.figure()\n            plt.title(\'display\')\n            plt.imshow(display, cmap=\'gray\')\n\n        if ii == 1:\n            break\n            \n    plt.show(block=True)'"
dataloaders/combine_dbs.py,2,"b'import torch.utils.data as data\n\n\nclass CombineDBs(data.Dataset):\n    def __init__(self, dataloaders, excluded=None):\n        self.dataloaders = dataloaders\n        self.excluded = excluded\n        self.im_ids = []\n\n        # Combine object lists\n        for dl in dataloaders:\n            for elem in dl.im_ids:\n                if elem not in self.im_ids:\n                    self.im_ids.append(elem)\n\n        # Exclude\n        if excluded:\n            for dl in excluded:\n                for elem in dl.im_ids:\n                    if elem in self.im_ids:\n                        self.im_ids.remove(elem)\n\n        # Get object pointers\n        self.obj_list = []\n        self.im_list = []\n        new_im_ids = []\n        obj_counter = 0\n        num_images = 0\n        for ii, dl in enumerate(dataloaders):\n            for jj, curr_im_id in enumerate(dl.im_ids):\n                if (curr_im_id in self.im_ids) and (curr_im_id not in new_im_ids):\n                    flag = False\n                    new_im_ids.append(curr_im_id)\n                    for kk in range(len(dl.obj_dict[curr_im_id])):\n                        if dl.obj_dict[curr_im_id][kk] != -1:\n                            self.obj_list.append({\'db_ii\': ii, \'obj_ii\': dl.obj_list.index([jj, kk])})\n                            flag = True\n                        obj_counter += 1\n                    self.im_list.append({\'db_ii\': ii, \'im_ii\': jj})\n                    if flag:\n                        num_images += 1\n\n        self.im_ids = new_im_ids\n        print(\'Combined number of images: {:d}\\nCombined number of objects: {:d}\'.format(num_images, len(self.obj_list)))\n\n    def __getitem__(self, index):\n\n        _db_ii = self.obj_list[index][""db_ii""]\n        _obj_ii = self.obj_list[index][\'obj_ii\']\n        sample = self.dataloaders[_db_ii].__getitem__(_obj_ii)\n\n        if \'meta\' in sample.keys():\n            sample[\'meta\'][\'db\'] = str(self.dataloaders[_db_ii])\n\n        return sample\n\n    def __len__(self):\n        return len(self.obj_list)\n\n    def __str__(self):\n        include_db = [str(db) for db in self.dataloaders]\n        exclude_db = [str(db) for db in self.excluded]\n        return \'Included datasets:\'+str(include_db)+\'\\n\'+\'Excluded datasets:\'+str(exclude_db)\n\n\nif __name__ == ""__main__"":\n    import matplotlib.pyplot as plt\n    from dataloaders import pascal\n    from dataloaders import sbd\n    import torch\n    import numpy as np\n    import dataset.custom_transforms as tr\n    from torchvision import transforms\n\n    composed_transforms_tr = transforms.Compose([\n        tr.RandomHorizontalFlip(),\n        tr.ScaleNRotate(rots=(-15, 15), scales=(.75, 1.25)),\n        tr.FixedResize(resolutions={\'image\': (450, 450), \'gt\': (450, 450)}),\n        tr.DistanceMap(v=0.15, elem=\'gt\'),\n        tr.ConcatInputs(elems=(\'image\', \'distance_map\')),\n        tr.ToTensor()])\n\n    composed_transforms_ts = transforms.Compose([\n        tr.FixedResize(resolutions={\'image\': (450, 450), \'gt\': (450, 450)}),\n        tr.DistanceMap(v=0.15, elem=\'gt\'),\n        tr.ConcatInputs(elems=(\'image\', \'distance_map\')),\n        tr.ToTensor()])\n\n    pascal_voc_val = pascal.VOCSegmentation(split=\'val\', transform=composed_transforms_ts, retname=True)\n    sbd = sbd.SBDSegmentation(split=[\'train\', \'val\'], transform=composed_transforms_tr, retname=True)\n    pascal_voc_train = pascal.VOCSegmentation(split=\'train\', transform=composed_transforms_tr, retname=True)\n\n    dataset = CombineDBs([pascal_voc_train, sbd], excluded=[pascal_voc_val])\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n\n    for ii, sample in enumerate(dataloader):\n        for jj in range(sample[""image""].size()[0]):\n            dismap = sample[\'distance_map\'][jj].numpy()\n            gt = sample[\'gt\'][jj].numpy()\n            gt[gt > 0] = 255\n            gt = np.array(gt[0]).astype(np.uint8)\n            dismap = np.array(dismap[0]).astype(np.uint8)\n            display = 0.9 * gt + 0.4 * dismap\n            display = display.astype(np.uint8)\n            plt.figure()\n            plt.title(\'display\')\n            plt.imshow(display, cmap=\'gray\')\n\n        if ii == 1:\n            break\n    plt.show(block=True)'"
dataloaders/custom_transforms.py,2,"b'import torch, cv2\n\nimport numpy.random as random\nimport numpy as np\nfrom dataloaders import utils\n\nclass ScaleNRotate(object):\n    """"""Scale (zoom-in, zoom-out) and Rotate the image and the ground truth.\n    Args:\n        two possibilities:\n        1.  rots (tuple): (minimum, maximum) rotation angle\n            scales (tuple): (minimum, maximum) scale\n        2.  rots [list]: list of fixed possible rotation angles\n            scales [list]: list of fixed possible scales\n    """"""\n    def __init__(self, rots=(-30, 30), scales=(.75, 1.25), semseg=False):\n        assert (isinstance(rots, type(scales)))\n        self.rots = rots\n        self.scales = scales\n        self.semseg = semseg\n\n    def __call__(self, sample):\n\n        if type(self.rots) == tuple:\n            # Continuous range of scales and rotations\n            rot = (self.rots[1] - self.rots[0]) * random.random() - \\\n                  (self.rots[1] - self.rots[0])/2\n\n            sc = (self.scales[1] - self.scales[0]) * random.random() - \\\n                 (self.scales[1] - self.scales[0]) / 2 + 1\n        elif type(self.rots) == list:\n            # Fixed range of scales and rotations\n            rot = self.rots[random.randint(0, len(self.rots))]\n            sc = self.scales[random.randint(0, len(self.scales))]\n\n        for elem in sample.keys():\n            if \'meta\' in elem:\n                continue\n\n            tmp = sample[elem]\n\n            h, w = tmp.shape[:2]\n            center = (w / 2, h / 2)\n            assert(center != 0)  # Strange behaviour warpAffine\n            M = cv2.getRotationMatrix2D(center, rot, sc)\n\n            if ((tmp == 0) | (tmp == 1)).all():\n                flagval = cv2.INTER_NEAREST\n            elif \'gt\' in elem and self.semseg:\n                flagval = cv2.INTER_NEAREST\n            else:\n                flagval = cv2.INTER_CUBIC\n            tmp = cv2.warpAffine(tmp, M, (w, h), flags=flagval)\n\n            sample[elem] = tmp\n\n        return sample\n\n    def __str__(self):\n        return \'ScaleNRotate:(rot=\'+str(self.rots)+\',scale=\'+str(self.scales)+\')\'\n\n\nclass FixedResize(object):\n    """"""Resize the image and the ground truth to specified resolution.\n    Args:\n        resolutions (dict): the list of resolutions\n    """"""\n    def __init__(self, resolutions=None, flagvals=None):\n        self.resolutions = resolutions\n        self.flagvals = flagvals\n        if self.flagvals is not None:\n            assert(len(self.resolutions) == len(self.flagvals))\n\n    def __call__(self, sample):\n\n        # Fixed range of scales\n        if self.resolutions is None:\n            return sample\n\n        elems = list(sample.keys())\n\n        for elem in elems:\n\n            if \'meta\' in elem or \'bbox\' in elem or (\'extreme_points_coord\' in elem and elem not in self.resolutions):\n                continue\n\n            if elem in self.resolutions:\n                if self.resolutions[elem] is None:\n                    continue\n                if isinstance(sample[elem], list):\n                    if sample[elem][0].ndim == 3:\n                        output_size = np.append(self.resolutions[elem], [3, len(sample[elem])])\n                    else:\n                        output_size = np.append(self.resolutions[elem], len(sample[elem]))\n                    tmp = sample[elem]\n                    sample[elem] = np.zeros(output_size, dtype=np.float32)\n                    for ii, crop in enumerate(tmp):\n                        if self.flagvals is None:\n                            sample[elem][..., ii] = utils.fixed_resize(crop, self.resolutions[elem])\n                        else:\n                            sample[elem][..., ii] = utils.fixed_resize(crop, self.resolutions[elem], flagval=self.flagvals[elem])\n                else:\n                    if self.flagvals is None:\n                        sample[elem] = utils.fixed_resize(sample[elem], self.resolutions[elem])\n                    else:\n                        sample[elem] = utils.fixed_resize(sample[elem], self.resolutions[elem], flagval=self.flagvals[elem])\n\n        return sample\n\n    def __str__(self):\n        return \'FixedResize:\'+str(self.resolutions)\n\n\nclass RandomHorizontalFlip(object):\n    """"""Horizontally flip the given image and ground truth randomly with a probability of 0.5.""""""\n\n    def __call__(self, sample):\n\n        if random.random() < 0.5:\n            for elem in sample.keys():\n                if \'meta\' in elem:\n                    continue\n                tmp = sample[elem]\n                tmp = cv2.flip(tmp, flipCode=1)\n                sample[elem] = tmp\n\n        return sample\n\n    def __str__(self):\n        return \'RandomHorizontalFlip\'\n\n\nclass DistanceMap(object):\n    """"""\n    Returns the distance map in a given binary mask\n    v: controls the degree of rectangle variation\n    elem: which element of the sample to choose as the binary mask\n    """"""\n    def __init__(self, v=0.15, elem=\'gt\'):\n        self.v = v\n        self.elem = elem\n\n    def __call__(self, sample):\n        if sample[self.elem].ndim == 3:\n            raise ValueError(\'DistanceMap not implemented for multiple object per image.\')\n        _target = sample[self.elem]\n        if np.max(_target) == 0:\n            # TODO: if mask do no have any object, distance=255\n            sample[\'distance_map\'] = np.zeros(_target.shape, dtype=_target.dtype) + 255\n        else:\n            sample[\'distance_map\'] = utils.distance_map(_target, self.v)\n\n        return sample\n\n    def __str__(self):\n        return \'DistanceMap:(v=\'+str(self.v)+\', elem=\'+str(self.elem)+\')\'\n\n\nclass ConcatInputs(object):\n\n    def __init__(self, elems=(\'image\', \'distance_map\')):\n        self.elems = elems\n\n    def __call__(self, sample):\n\n        res = sample[self.elems[0]]\n\n        for elem in self.elems[1:]:\n            assert(sample[self.elems[0]].shape[:2] == sample[elem].shape[:2])\n\n            # Check if third dimension is missing\n            tmp = sample[elem]\n            if tmp.ndim == 2:\n                tmp = tmp[:, :, np.newaxis]\n\n            res = np.concatenate((res, tmp), axis=2)\n\n        sample[\'concat\'] = res\n\n        return sample\n\n    def __str__(self):\n        return \'ConcatInputs:\'+str(self.elems)\n\n\nclass ToTensor(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample):\n\n        for elem in sample.keys():\n            if \'meta\' in elem:\n                continue\n            elif \'bbox\' in elem:\n                tmp = sample[elem]\n                sample[elem] = torch.from_numpy(tmp)\n                continue\n\n            tmp = sample[elem].astype(np.float32)\n\n            if tmp.ndim == 2:\n                tmp = tmp[:, :, np.newaxis]\n\n            # swap color axis because\n            # numpy image: H x W x C\n            # torch image: C X H X W\n            tmp = tmp.transpose((2, 0, 1))\n            sample[elem] = torch.from_numpy(tmp).float()\n\n        return sample\n\n    def __str__(self):\n        return \'ToTensor\'\n'"
dataloaders/pascal.py,2,"b'from __future__ import print_function, division\nimport os\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom mypath import Path\nimport json\n\nclass VOCSegmentation(Dataset):\n    """"""\n    PascalVoc dataset\n    """"""\n\n    def __init__(self,\n                 base_dir=Path.db_root_dir(\'pascal\'),\n                 split=\'train\',\n                 transform=None,\n                 area_thres=0,\n                 preprocess=False,\n                 default=False,\n                 retname=True):\n        """"""\n        :param base_dir: path to VOC dataset directory\n        :param split: train/val\n        :param transform: transform to apply\n        """"""\n        super().__init__()\n        self._base_dir = base_dir\n        self._image_dir = os.path.join(self._base_dir, \'JPEGImages\')\n        self._mask_dir = os.path.join(self._base_dir, \'SegmentationObject\')\n        self._cat_dir = os.path.join(self._base_dir, \'SegmentationClass\')\n\n        self.area_thres = area_thres\n        self.default = default\n        self.retname = retname\n\n        if isinstance(split, str):\n            self.split = [split]\n        else:\n            split.sort()\n            self.split = split\n\n        # Build the ids file\n        area_th_str = """"\n        if self.area_thres != 0:\n            area_th_str = \'_area_thres-\' + str(area_thres)\n\n        self.obj_list_file = os.path.join(self._base_dir, \'ImageSets\', \'Segmentation\',\n                                             \'_\'.join(self.split) + \'_instances\' + area_th_str + \'.txt\')\n        self.transform = transform\n\n        _splits_dir = os.path.join(self._base_dir, \'ImageSets\', \'Segmentation\')\n\n        self.im_ids = []\n        self.images = []\n        self.categories = []\n        self.masks = []\n\n        for splt in self.split:\n            with open(os.path.join(os.path.join(_splits_dir, splt + \'.txt\')), ""r"") as f:\n                lines = f.read().splitlines()\n\n            for ii, line in enumerate(lines):\n                _image = os.path.join(self._image_dir, line + "".jpg"")\n                _cat = os.path.join(self._cat_dir, line + "".png"")\n                _mask = os.path.join(self._mask_dir, line + "".png"")\n                assert os.path.isfile(_image)\n                assert os.path.isfile(_cat)\n                assert os.path.isfile(_mask)\n                self.im_ids.append(line.rstrip(\'\\n\'))\n                self.images.append(_image)\n                self.categories.append(_cat)\n                self.masks.append(_mask)\n\n        assert (len(self.images) == len(self.masks))\n        assert (len(self.images) == len(self.categories))\n\n        # Precompute the list of objects and their categories for each image\n        if (not self._check_preprocess()) or preprocess:\n            print(\'Preprocessing of PASCAL VOC dataset, this will take long, but it will be done only once.\')\n            self._preprocess()\n\n        # Build the list of objects\n        self.obj_list = []\n        num_images = 0\n        for ii in range(len(self.im_ids)):\n            flag = False\n            for jj in range(len(self.obj_dict[self.im_ids[ii]])):\n                if self.obj_dict[self.im_ids[ii]][jj] != -1:\n                    self.obj_list.append([ii, jj])\n                    flag = True\n            if flag:\n                num_images += 1\n\n        # Display stats\n        print(\'Number of images: {:d}\\nNumber of objects: {:d}\'.format(num_images, len(self.obj_list)))\n\n\n    def __len__(self):\n        return len(self.obj_list)\n\n\n    def __getitem__(self, index):\n        _img, _target, _, _, _, _ = self._make_img_gt_point_pair(index)\n        sample = {\'image\': _img, \'gt\': _target}\n\n        if self.retname: # return meta information\n            _im_ii = self.obj_list[index][0]\n            _obj_ii = self.obj_list[index][1]\n            sample[\'meta\'] = {\'image\': str(self.im_ids[_im_ii]),\n                              \'object\': str(_obj_ii),\n                              \'category\': self.obj_dict[self.im_ids[_im_ii]][_obj_ii],\n                              \'im_size\': (_img.shape[0], _img.shape[1])}\n\n        if self.transform is not None:\n            sample = self.transform(sample)\n\n        return sample\n\n    def _check_preprocess(self):\n        _obj_list_file = self.obj_list_file\n        if not os.path.isfile(_obj_list_file):\n            return False\n        else:\n            self.obj_dict = json.load(open(_obj_list_file, \'r\'))\n\n            return list(np.sort([str(x) for x in self.obj_dict.keys()])) == list(np.sort(self.im_ids))\n\n    def _preprocess(self):\n        self.obj_dict = {}\n        obj_counter = 0\n        for ii in range(len(self.im_ids)):\n            # Read object masks and get number of objects\n            _mask = np.array(Image.open(self.masks[ii]))\n            _mask_ids = np.unique(_mask)\n            if _mask_ids[-1] == 255:\n                n_obj = _mask_ids[-2]\n            else:\n                n_obj = _mask_ids[-1]\n\n            # Get the categories from these objects\n            _cats = np.array(Image.open(self.categories[ii]))\n            _cat_ids = []\n            for jj in range(n_obj):\n                tmp = np.where(_mask == jj + 1)\n                obj_area = len(tmp[0])\n                if obj_area > self.area_thres:\n                    _cat_ids.append(int(_cats[tmp[0][0], tmp[1][0]]))\n                else:\n                    _cat_ids.append(-1)\n                obj_counter += 1\n\n            self.obj_dict[self.im_ids[ii]] = _cat_ids\n\n        with open(self.obj_list_file, \'w\') as outfile:\n            outfile.write(\'{{\\n\\t""{:s}"": {:s}\'.format(self.im_ids[0], json.dumps(self.obj_dict[self.im_ids[0]])))\n            for ii in range(1, len(self.im_ids)):\n                outfile.write(\',\\n\\t""{:s}"": {:s}\'.format(self.im_ids[ii], json.dumps(self.obj_dict[self.im_ids[ii]])))\n            outfile.write(\'\\n}\\n\')\n\n        print(\'Preprocessing finished\')\n\n    def _make_img_gt_point_pair(self, index):\n        _im_ii = self.obj_list[index][0]\n        _obj_ii = self.obj_list[index][1]\n\n        # Read Image\n        _img = np.array(Image.open(self.images[_im_ii]).convert(\'RGB\')).astype(np.float32)\n\n        # Read Target object\n        _tmp = (np.array(Image.open(self.masks[_im_ii]))).astype(np.float32)\n        _void_pixels = (_tmp == 255) # ignore label == 255, it is boundary pixel\n        _tmp[_void_pixels] = 0\n\n        _other_same_class = np.zeros(_tmp.shape)\n        _other_classes = np.zeros(_tmp.shape)\n\n        if self.default:\n            _target = _tmp\n            _background = np.logical_and(_tmp == 0, ~_void_pixels) # background is where label == 0 except boundary pixel\n        else:\n            _target = (_tmp == (_obj_ii + 1)).astype(np.float32) # mask a certain object, other pixel is zero\n            _background = np.logical_and(_tmp == 0, ~_void_pixels) # background is where label == 0 except boundary pixel\n            obj_cat = self.obj_dict[self.im_ids[_im_ii]][_obj_ii] # object label\n            for ii in range(1, np.max(_tmp).astype(np.int)+1): # 1, ..., num(instances)\n                ii_cat = self.obj_dict[self.im_ids[_im_ii]][ii-1] # instance\'s category\n                if obj_cat == ii_cat and ii != _obj_ii+1:\n                    _other_same_class = np.logical_or(_other_same_class, _tmp == ii)\n                elif ii != _obj_ii+1:\n                    _other_classes = np.logical_or(_other_classes, _tmp == ii)\n\n        return _img, _target, _void_pixels.astype(np.float32), \\\n               _other_classes.astype(np.float32), _other_same_class.astype(np.float32), \\\n               _background.astype(np.float32)\n\n    def __str__(self):\n        return \'VOC2012(split=\' + str(self.split) + \',area_thres=\' + str(self.area_thres) + \')\'\n\n\nif __name__ == \'__main__\':\n    from dataloaders import custom_transforms as tr\n    from torch.utils.data import DataLoader\n    from torchvision import transforms\n    import matplotlib.pyplot as plt\n\n    composed_transforms_tr = transforms.Compose([\n        tr.RandomHorizontalFlip(),\n        tr.ScaleNRotate(rots=(-15, 15), scales=(.75, 1.25)),\n        tr.FixedResize(resolutions={\'image\': (450, 450), \'gt\': (450, 450)}),\n        tr.DistanceMap(v=0.15, elem=\'gt\'),\n        tr.ConcatInputs(elems=(\'image\', \'distance_map\')),\n        tr.ToTensor()])\n\n    voc_train = VOCSegmentation(split=\'train\', retname=False,\n                                transform=composed_transforms_tr)\n\n    dataloader = DataLoader(voc_train, batch_size=2, shuffle=True, num_workers=2)\n\n    for ii, sample in enumerate(dataloader):\n        for jj in range(sample[""image""].size()[0]):\n            dismap = sample[\'distance_map\'][jj].numpy()\n            gt = sample[\'gt\'][jj].numpy()\n            gt[gt > 0] = 255\n            gt = np.array(gt[0]).astype(np.uint8)\n            dismap = np.array(dismap[0]).astype(np.uint8)\n            display = 0.9 * gt + 0.4 * dismap\n            display = display.astype(np.uint8)\n            plt.figure()\n            plt.title(\'display\')\n            plt.imshow(display, cmap=\'gray\')\n\n        if ii == 1:\n            break\n    plt.show(block=True)'"
dataloaders/sbd.py,2,"b'from __future__ import print_function, division\nimport json\nimport os\n\nimport numpy as np\nimport scipy.io\nimport torch.utils.data as data\nfrom PIL import Image\nfrom mypath import Path\n\n\nclass SBDSegmentation(data.Dataset):\n\n    def __init__(self,\n                 base_dir=Path.db_root_dir(\'sbd\'),\n                 split=\'val\',\n                 transform=None,\n                 preprocess=False,\n                 area_thres=0,\n                 retname=True):\n        """"""\n        :param base_dir: path to VOC dataset directory\n        :param split: train/val\n        :param transform: transform to apply\n        """"""\n        super().__init__()\n        self._base_dir = base_dir\n        self._dataset_dir = os.path.join(self._base_dir, \'dataset\')\n        self._mask_dir = os.path.join(self._dataset_dir, \'inst\')\n        self._image_dir = os.path.join(self._dataset_dir, \'img\')\n\n        self.transform = transform\n        if isinstance(split, str):\n            self.split = [split]\n        else:\n            split.sort()\n            self.split = split\n        self.area_thres = area_thres\n        self.retname = retname\n\n        if self.area_thres != 0:\n            self.obj_list_file = os.path.join(self._dataset_dir, \'_\'.join(self.split) + \'_instances_area_thres-\' +\n                                              str(area_thres) + \'.txt\')\n        else:\n            self.obj_list_file = os.path.join(self._dataset_dir, \'_\'.join(self.split) + \'_instances\' + \'.txt\')\n\n\n        # Get list of all images from the split and check that the files exist\n        self.im_ids = []\n        self.images = []\n        self.masks = []\n        for splt in self.split:\n            with open(os.path.join(self._dataset_dir, splt + \'.txt\'), ""r"") as f:\n                lines = f.read().splitlines()\n\n            for line in lines:\n                _image = os.path.join(self._image_dir, line + "".jpg"")\n                _mask = os.path.join(self._mask_dir, line + "".mat"")\n                assert os.path.isfile(_image)\n                assert os.path.isfile(_mask)\n                self.im_ids.append(line)\n                self.images.append(_image)\n                self.masks.append(_mask)\n\n        assert (len(self.images) == len(self.masks))\n\n        # Precompute the list of objects and their categories for each image\n        if (not self._check_preprocess()) or preprocess:\n            print(\'Preprocessing SBD dataset, this will take long, but it will be done only once.\')\n            self._preprocess()\n\n        # Build the list of objects\n        self.obj_list = []\n        num_images = 0\n        for ii in range(len(self.im_ids)):\n            if self.im_ids[ii] in self.obj_dict.keys():\n                flag = False\n                for jj in range(len(self.obj_dict[self.im_ids[ii]])):\n                    if self.obj_dict[self.im_ids[ii]][jj] != -1:\n                        self.obj_list.append([ii, jj])\n                        flag = True\n                if flag:\n                    num_images += 1\n\n        # Display stats\n        print(\'Number of images: {:d}\\nNumber of objects: {:d}\'.format(num_images, len(self.obj_list)))\n\n\n    def __getitem__(self, index):\n\n        _img, _target = self._make_img_gt_point_pair(index)\n\n        sample = {\'image\': _img, \'gt\': _target}\n\n        if self.retname:\n            _im_ii = self.obj_list[index][0]\n            _obj_ii = self.obj_list[index][1]\n            sample[\'meta\'] = {\'image\': str(self.im_ids[_im_ii]),\n                              \'object\': str(_obj_ii),\n                              \'im_size\': (_img.shape[0], _img.shape[1]),\n                              \'category\': self.obj_dict[self.im_ids[_im_ii]][_obj_ii]}\n\n        if self.transform is not None:\n            sample = self.transform(sample)\n\n        return sample\n\n    def __len__(self):\n        return len(self.obj_list)\n\n\n    def _check_preprocess(self):\n        # Check that the file with categories is there and with correct size\n        _obj_list_file = self.obj_list_file\n        if not os.path.isfile(_obj_list_file):\n            return False\n        else:\n            self.obj_dict = json.load(open(_obj_list_file, \'r\'))\n            return list(np.sort([str(x) for x in self.obj_dict.keys()])) == list(np.sort(self.im_ids))\n\n\n    def _preprocess(self):\n        # Get all object instances and their category\n        self.obj_dict = {}\n        obj_counter = 0\n        for ii in range(len(self.im_ids)):\n            # Read object masks and get number of objects\n            tmp = scipy.io.loadmat(self.masks[ii])\n            _mask = tmp[""GTinst""][0][""Segmentation""][0]\n            _cat_ids = tmp[""GTinst""][0][""Categories""][0].astype(int)\n\n            _mask_ids = np.unique(_mask)\n            n_obj = _mask_ids[-1]\n            assert (n_obj == len(_cat_ids))\n\n            for jj in range(n_obj):\n                temp = np.where(_mask == jj + 1)\n                obj_area = len(temp[0])\n                if obj_area < self.area_thres:\n                    _cat_ids[jj] = -1\n                obj_counter += 1\n\n            self.obj_dict[self.im_ids[ii]] = np.squeeze(_cat_ids, 1).tolist()\n\n        # Save it to file for future reference\n        with open(self.obj_list_file, \'w\') as outfile:\n            outfile.write(\'{{\\n\\t""{:s}"": {:s}\'.format(self.im_ids[0], json.dumps(self.obj_dict[self.im_ids[0]])))\n            for ii in range(1, len(self.im_ids)):\n                outfile.write(\',\\n\\t""{:s}"": {:s}\'.format(self.im_ids[ii], json.dumps(self.obj_dict[self.im_ids[ii]])))\n            outfile.write(\'\\n}\\n\')\n\n        print(\'Pre-processing finished\')\n\n\n    def _make_img_gt_point_pair(self, index):\n        _im_ii = self.obj_list[index][0]\n        _obj_ii = self.obj_list[index][1]\n\n        # Read Image\n        _img = np.array(Image.open(self.images[_im_ii]).convert(\'RGB\')).astype(np.float32)\n\n        # Read Taret object\n        _tmp = scipy.io.loadmat(self.masks[_im_ii])[""GTinst""][0][""Segmentation""][0]\n        _target = (_tmp == (_obj_ii + 1)).astype(np.float32)\n\n        return _img, _target\n\n\n    def __str__(self):\n        return \'SBDSegmentation(split=\' + str(self.split) + \', area_thres=\' + str(self.area_thres) + \')\'\n\n\nif __name__ == \'__main__\':\n    from dataloaders import custom_transforms as tr\n    from torch.utils.data import DataLoader\n    from torchvision import transforms\n    import matplotlib.pyplot as plt\n\n    composed_transforms_tr = transforms.Compose([\n        tr.RandomHorizontalFlip(),\n        tr.ScaleNRotate(rots=(-15, 15), scales=(.75, 1.25)),\n        tr.FixedResize(resolutions={\'image\': (450, 450), \'gt\': (450, 450)}),\n        tr.DistanceMap(v=0.15, elem=\'gt\'),\n        tr.ConcatInputs(elems=(\'image\', \'distance_map\')),\n        tr.ToTensor()])\n\n    sbd_train = SBDSegmentation(split=\'train\', retname=False,\n                                transform=composed_transforms_tr)\n\n    dataloader = DataLoader(sbd_train, batch_size=2, shuffle=True, num_workers=2)\n\n    for ii, sample in enumerate(dataloader):\n        for jj in range(sample[""image""].size()[0]):\n            dismap = sample[\'distance_map\'][jj].numpy()\n            gt = sample[\'gt\'][jj].numpy()\n            gt[gt > 0] = 255\n            gt = np.array(gt[0]).astype(np.uint8)\n            dismap = np.array(dismap[0]).astype(np.uint8)\n            display = 0.9 * gt + 0.4 * dismap\n            display = display.astype(np.uint8)\n            plt.figure()\n            plt.title(\'display\')\n            plt.imshow(display, cmap=\'gray\')\n\n        if ii == 1:\n            break\n    plt.show(block=True)'"
dataloaders/utils.py,0,"b""import os\n\nimport torch, cv2\nimport random\nimport numpy as np\nfrom copy import deepcopy\n\n\ndef distance_map(mask, pad=10, v=0.15, relax=False):\n    bounds = (0, 0, mask.shape[1] - 1, mask.shape[0] - 1)\n\n    bbox = get_bbox(mask, pad=pad, relax=relax)  # get image bounding box\n    if bbox is not None:\n        bbox = add_turbulence(bbox, v=v)\n    else:\n        dismap = np.zeros_like(mask) + 255\n        return dismap\n\n    x_min = max(bbox[0], bounds[0])\n    y_min = max(bbox[1], bounds[1])\n    x_max = min(bbox[2], bounds[2])\n    y_max = min(bbox[3], bounds[3])\n\n    bbox = [x_min, y_min, x_max, y_max]\n\n    dismap = np.zeros((mask.shape[0], mask.shape[1]))\n    dismap = compute_dismap(dismap, bbox)\n    return dismap\n\n\ndef compute_dismap(dismap, bbox):\n    x_min, y_min, x_max, y_max = bbox[:]\n\n    # draw bounding box\n    cv2.line(dismap, (x_min, y_min), (x_max, y_min), color=1, thickness=1)\n    cv2.line(dismap, (x_min, y_min), (x_min, y_max), color=1, thickness=1)\n    cv2.line(dismap, (x_max, y_max), (x_max, y_min), color=1, thickness=1)\n    cv2.line(dismap, (x_max, y_max), (x_min, y_max), color=1, thickness=1)\n\n    tmp = (dismap > 0).astype(np.uint8)  # mark boundary\n    tmp_ = deepcopy(tmp)\n\n    fill_mask = np.ones((tmp.shape[0] + 2, tmp.shape[1] + 2)).astype(np.uint8)\n    fill_mask[1:-1, 1:-1] = tmp_\n    cv2.floodFill(tmp_, fill_mask, (int((x_min + x_max) / 2), int((y_min + y_max) / 2)), 5) # fill pixel inside bounding box\n\n    tmp_ = tmp_.astype(np.int8)\n    tmp_[tmp_ == 5] = -1  # pixel inside bounding box\n    tmp_[tmp_ == 0] = 1  # pixel on and outside bounding box\n\n    tmp = (tmp == 0).astype(np.uint8)\n\n    dismap = cv2.distanceTransform(tmp, cv2.DIST_L2, cv2.DIST_MASK_PRECISE)  # compute distance inside and outside bounding box\n    dismap = tmp_ * dismap + 128\n\n    dismap[dismap > 255] = 255\n    dismap[dismap < 0] = 0\n\n    dismap = dismap.astype(np.uint8)\n\n    return dismap\n\n\ndef add_turbulence(bbox, v=0.15):\n    x_min, y_min, x_max, y_max = bbox[:]\n    x_min_new = int(x_min + v * np.random.normal(0, 1) * (x_max - x_min))\n    x_max_new = int(x_max + v * np.random.normal(0, 1) * (x_max - x_min))\n    y_min_new = int(y_min + v * np.random.normal(0, 1) * (y_max - y_min))\n    y_max_new = int(y_max + v * np.random.normal(0, 1) * (y_max - y_min))\n\n    return [x_min_new, y_min_new, x_max_new, y_max_new]\n\n\ndef get_bbox(mask, points=None, pad=0, relax=False):\n    if points is not None:\n        inds = np.flip(points.transpose(), axis=0)\n    else:\n        inds = np.where(mask > 0)\n\n    if inds[0].shape[0] == 0:\n        return None\n\n    if relax:\n        pad = 0\n\n    x_min_bound = 0\n    y_min_bound = 0\n    x_max_bound = mask.shape[1] - 1\n    y_max_bound = mask.shape[0] - 1\n\n    x_min = max(inds[1].min() - pad, x_min_bound)\n    y_min = max(inds[0].min() - pad, y_min_bound)\n    x_max = min(inds[1].max() + pad, x_max_bound)\n    y_max = min(inds[0].max() + pad, y_max_bound)\n\n    return [x_min, y_min, x_max, y_max]\n\n\ndef fixed_resize(sample, resolution, flagval=None):\n    if flagval is None:\n        if ((sample == 0) | (sample == 1)).all():\n            flagval = cv2.INTER_NEAREST\n        else:\n            flagval = cv2.INTER_CUBIC\n\n    if isinstance(resolution, int):\n        tmp = [resolution, resolution]\n        tmp[np.argmax(sample.shape[:2])] = int(\n            round(float(resolution) / np.min(sample.shape[:2]) * np.max(sample.shape[:2])))\n        resolution = tuple(tmp)\n\n    if sample.ndim == 2 or (sample.ndim == 3 and sample.shape[2] == 3):\n        sample = cv2.resize(sample, resolution[::-1], interpolation=flagval)\n    else:\n        tmp = sample\n        sample = np.zeros(np.append(resolution, tmp.shape[2]), dtype=np.float32)\n        for ii in range(sample.shape[2]):\n            sample[:, :, ii] = cv2.resize(tmp[:, :, ii], resolution[::-1], interpolation=flagval)\n\n    return sample\n\n\ndef generate_param_report(logfile, param):\n    log_file = open(logfile, 'w')\n    for key, val in param.items():\n        log_file.write(key + ':' + str(val) + '\\n')\n    log_file.close()\n"""
layers/__init__.py,0,b''
layers/loss.py,15,"b'from __future__ import division\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\n\n\ndef class_balanced_cross_entropy_loss(output, label, size_average=True, batch_average=True, void_pixels=None):\n    """"""Define the class balanced cross entropy loss to train the network\n    Args:\n    output: Output of the network\n    label: Ground truth label\n    size_average: return per-element (pixel) average loss\n    batch_average: return per-batch average loss\n    void_pixels: pixels to ignore from the loss\n    Returns:\n    Tensor that evaluates the loss\n    """"""\n    assert(output.size() == label.size())\n\n    labels = torch.ge(label, 0.5).float()\n\n    num_labels_pos = torch.sum(labels)\n    num_labels_neg = torch.sum(1.0 - labels)\n    num_total = num_labels_pos + num_labels_neg\n\n    output_gt_zero = torch.ge(output, 0).float()\n    loss_val = torch.mul(output, (labels - output_gt_zero)) - torch.log(\n        1 + torch.exp(output - 2 * torch.mul(output, output_gt_zero)))\n\n    loss_pos_pix = -torch.mul(labels, loss_val)\n    loss_neg_pix = -torch.mul(1.0 - labels, loss_val)\n\n    if void_pixels is not None:\n        w_void = torch.le(void_pixels, 0.5).float()\n        loss_pos_pix = torch.mul(w_void, loss_pos_pix)\n        loss_neg_pix = torch.mul(w_void, loss_neg_pix)\n        num_total = num_total - torch.ge(void_pixels, 0.5).float().sum()\n\n    loss_pos = torch.sum(loss_pos_pix)\n    loss_neg = torch.sum(loss_neg_pix)\n\n    final_loss = num_labels_neg / num_total * loss_pos + num_labels_pos / num_total * loss_neg\n\n    if size_average:\n        final_loss /= np.prod(label.size())\n    elif batch_average:\n        final_loss /= label.size()[0]\n\n    return final_loss\n'"
networks/__init__.py,0,b''
networks/deeplab_resnet.py,7,"b'import torch.nn as nn\nimport torchvision.models.resnet as resnet\nimport torch\nimport numpy as np\nfrom copy import deepcopy\nimport os\nfrom torch.nn import functional as F\nfrom mypath import Path\n\naffine_par = True\n\n\ndef outS(i):\n    i = int(i)\n    i = (i+1)/2\n    i = int(np.ceil((i+1)/2.0))\n    i = (i+1)/2\n    return i\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation_=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) # change\n        self.bn1 = nn.BatchNorm2d(planes, affine=affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        padding = 1\n        if dilation_ == 2:\n            padding = 2\n        elif dilation_ == 4:\n            padding = 4\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, # change\n                               padding=padding, bias=False, dilation=dilation_)\n        self.bn2 = nn.BatchNorm2d(planes, affine=affine_par)\n        for i in self.bn2.parameters():\n            i.requires_grad = False\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion, affine=affine_par)\n        for i in self.bn3.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ClassifierModule(nn.Module):\n\n    def __init__(self, dilation_series, padding_series, n_classes):\n        super(ClassifierModule, self).__init__()\n        self.conv2d_list = nn.ModuleList()\n        for dilation, padding in zip(dilation_series, padding_series):\n            self.conv2d_list.append(nn.Conv2d(2048, n_classes, kernel_size=3, stride=1, padding=padding, dilation=dilation, bias=True))\n\n        for m in self.conv2d_list:\n            m.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.conv2d_list[0](x)\n        for i in range(len(self.conv2d_list)-1):\n            out += self.conv2d_list[i+1](x)\n        return out\n\n\nclass PSPModule(nn.Module):\n    """"""\n    Pyramid Scene Parsing moduleD\n    """"""\n    def __init__(self, in_features=2048, out_features=512, sizes=(1, 2, 3, 6), n_classes=1):\n        super(PSPModule, self).__init__()\n        self.stages = []\n        self.stages = nn.ModuleList([self._make_stage_1(in_features, size) for size in sizes])\n        self.bottleneck = self._make_stage_2(in_features * (len(sizes)//4 + 1), out_features)\n        self.relu = nn.ReLU()\n        self.final = nn.Conv2d(out_features, n_classes, kernel_size=1)\n\n    def _make_stage_1(self, in_features, size):\n        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n        conv = nn.Conv2d(in_features, in_features//4, kernel_size=1, bias=False)\n        bn = nn.BatchNorm2d(in_features//4, affine=affine_par)\n        relu = nn.ReLU(inplace=True)\n\n        return nn.Sequential(prior, conv, bn, relu)\n\n    def _make_stage_2(self, in_features, out_features):\n        conv = nn.Conv2d(in_features, out_features, kernel_size=1, bias=False)\n        bn = nn.BatchNorm2d(out_features, affine=affine_par)\n        relu = nn.ReLU(inplace=True)\n\n        return nn.Sequential(conv, bn, relu)\n\n    def forward(self, feats):\n        h, w = feats.size(2), feats.size(3)\n        priors = [F.upsample(input=stage(feats), size=(h, w), mode=\'bilinear\', align_corners=True) for stage in self.stages]\n        priors.append(feats)\n        bottle = self.relu(self.bottleneck(torch.cat(priors, 1)))\n        out = self.final(bottle)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, n_classes, nInputChannels=3, classifier=""atrous"",\n                 dilations=(2, 4), strides=(2, 2, 2, 1, 1), _print=False):\n        if _print:\n            print(""Constructing ResNet model..."")\n            print(""Dilations: {}"".format(dilations))\n            print(""Number of classes: {}"".format(n_classes))\n            print(""Number of Input Channels: {}"".format(nInputChannels))\n        self.inplanes = 64\n        self.classifier = classifier\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(nInputChannels, 64, kernel_size=7, stride=strides[0], padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)\n        for i in self.bn1.parameters():\n            i.requires_grad = False\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=strides[1], padding=1, ceil_mode=False)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=strides[2])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=strides[3], dilation__=dilations[0])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=strides[4], dilation__=dilations[1])\n\n        if classifier == ""atrous"":\n            if _print:\n                print(\'Initializing classifier: A-trous pyramid\')\n            self.layer5 = self._make_pred_layer(ClassifierModule, [6, 12, 18, 24], [6, 12, 18, 24], n_classes=n_classes)\n        elif classifier == ""psp"":\n            if _print:\n                print(\'Initializing classifier: PSP\')\n            self.layer5 = PSPModule(in_features=2048, out_features=512, sizes=(1, 2, 3, 6), n_classes=n_classes)\n        else:\n            self.layer5 = None\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0, 0.01)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation__=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion or dilation__ == 2 or dilation__ == 4:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, affine=affine_par),\n            )\n        for i in downsample._modules[\'1\'].parameters():\n            i.requires_grad = False\n        layers = [block(self.inplanes, planes, stride, dilation_=dilation__, downsample=downsample)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation_=dilation__))\n\n        return nn.Sequential(*layers)\n\n    def _make_pred_layer(self, block, dilation_series, padding_series, n_classes):\n        return block(dilation_series, padding_series, n_classes)\n\n    def forward(self, x, bbox=None):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        if self.layer5 is not None:\n            x = self.layer5(x)\n        return x\n\n    def load_pretrained_ms(self, base_network, nInputChannels=3):\n        flag = 0\n        for module, module_ori in zip(self.modules(), base_network.Scale.modules()):\n            if isinstance(module, nn.Conv2d) and isinstance(module_ori, nn.Conv2d):\n                # 3 channel -> 4 channel\n                if not flag and nInputChannels != 3:\n                    module.weight[:, :3, :, :].data = deepcopy(module_ori.weight.data)\n                    module.bias = deepcopy(module_ori.bias)\n                    for i in range(3, int(module.weight.data.shape[1])):\n                        module.weight[:, i, :, :].data = deepcopy(module_ori.weight[:, -1, :, :][:, np.newaxis, :, :].data)\n                    flag = 1\n                elif module.weight.data.shape == module_ori.weight.data.shape:\n                    module.weight = torch.nn.Parameter(deepcopy(module_ori.weight)) # add nn.Parameter\n                    module.bias = deepcopy(module_ori.bias)\n                else:\n                    print(\'Skipping Conv layer with size: {} and target size: {}\'\n                          .format(module.weight.data.shape, module_ori.weight.data.shape))\n            elif isinstance(module, nn.BatchNorm2d) and isinstance(module_ori, nn.BatchNorm2d) \\\n                    and module.weight.data.shape == module_ori.weight.data.shape:\n                module.weight.data = deepcopy(module_ori.weight.data)\n                module.bias.data = deepcopy(module_ori.bias.data)\n\n\ndef resnet101(n_classes, pretrained=False, nInputChannels=3, classifier=""atrous"",\n              dilations=(2, 4), strides=(2, 2, 2, 1, 1)):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], n_classes, nInputChannels=nInputChannels,\n                   classifier=classifier, dilations=dilations, strides=strides, _print=True)\n    if pretrained:\n        model_full = Res_Deeplab(n_classes, pretrained=pretrained)\n        model.load_pretrained_ms(model_full, nInputChannels=nInputChannels)\n    return model\n\n\nclass MS_Deeplab(nn.Module):\n    def __init__(self, block, NoLabels, nInputChannels=3):\n        super(MS_Deeplab, self).__init__()\n        self.Scale = ResNet(block, [3, 4, 23, 3], NoLabels, nInputChannels=nInputChannels)\n\n    def forward(self, x):\n        input_size = x.size()[2]\n        self.interp1 = nn.Upsample(size=(int(input_size*0.75)+1, int(input_size*0.75)+1), mode=\'bilinear\', align_corners=True)\n        self.interp2 = nn.Upsample(size=(int(input_size*0.5)+1, int(input_size*0.5)+1), mode=\'bilinear\', align_corners=True)\n        self.interp3 = nn.Upsample(size=(outS(input_size), outS(input_size)), mode=\'bilinear\', align_corners=True)\n        out = []\n        x2 = self.interp1(x)\n        x3 = self.interp2(x)\n        out.append(self.Scale(x))  # for original scale\n        out.append(self.interp3(self.Scale(x2)))  # for 0.75x scale\n        out.append(self.Scale(x3))  # for 0.5x scale\n\n        x2Out_interp = out[1]\n        x3Out_interp = self.interp3(out[2])\n        temp1 = torch.max(out[0], x2Out_interp)\n        out.append(torch.max(temp1, x3Out_interp))\n        return out[-1]\n\n\ndef Res_Deeplab(n_classes=21, pretrained=False):\n    model = MS_Deeplab(Bottleneck, n_classes)\n    if pretrained:\n        pth_model = \'MS_DeepLab_resnet_trained_VOC.pth\'\n        saved_state_dict = torch.load(os.path.join(Path.models_dir(), pth_model),\n                                      map_location=lambda storage, loc: storage)\n        if n_classes != 21:\n            for i in saved_state_dict:\n                i_parts = i.split(\'.\')\n                if i_parts[1] == \'layer5\':\n                    saved_state_dict[i] = model.state_dict()[i]\n        model.load_state_dict(saved_state_dict)\n    return model\n\n\ndef get_lr_params(model):\n    """"""\n    This generator returns all the parameters of the net except for\n    the last classification layer. Note that for each batchnorm layer,\n    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n    any batchnorm parameter\n    """"""\n    b = [model.conv1, model.bn1, model.layer1, model.layer2, model.layer3, model.layer4, model.layer5]\n    for i in range(len(b)):\n        for k in b[i].parameters():\n            if k.requires_grad:\n                yield k\n\n\ndef get_1x_lr_params(model):\n    """"""\n    This generator returns all the parameters of the net except for\n    the last classification layer. Note that for each batchnorm layer,\n    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n    any batchnorm parameter\n    """"""\n    b = [model.conv1, model.bn1, model.layer1, model.layer2, model.layer3, model.layer4]\n    for i in range(len(b)):\n        for k in b[i].parameters():\n            if k.requires_grad:\n                yield k\n\n\ndef get_10x_lr_params(model):\n    """"""\n    This generator returns all the parameters for the last layer of the net,\n    which does the classification of pixel into classes\n    """"""\n    b = [model.layer5]\n    for j in range(len(b)):\n        for k in b[j].parameters():\n            if k.requires_grad:\n                yield k\n\n\ndef lr_poly(base_lr, iter_, max_iter=100, power=0.9):\n    return base_lr*((1-float(iter_)/max_iter)**power)\n'"
