file_path,api_count,code
mypath.py,0,"b""class Path(object):\n    @staticmethod\n    def db_root_dir(dataset):\n        if dataset == 'pascal':\n            return '/path/to/datasets/VOCdevkit/VOC2012/'  # folder that contains VOCdevkit/.\n        elif dataset == 'sbd':\n            return '/path/to/datasets/benchmark_RELEASE/'  # folder that contains dataset/.\n        elif dataset == 'cityscapes':\n            return '/path/to/datasets/cityscapes/'     # foler that contains leftImg8bit/\n        elif dataset == 'coco':\n            return '/path/to/datasets/coco/'\n        else:\n            print('Dataset {} not available.'.format(dataset))\n            raise NotImplementedError\n"""
train.py,7,"b'import argparse\nimport os\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom mypath import Path\nfrom dataloaders import make_data_loader\nfrom modeling.sync_batchnorm.replicate import patch_replication_callback\nfrom modeling.deeplab import *\nfrom utils.loss import SegmentationLosses\nfrom utils.calculate_weights import calculate_weigths_labels\nfrom utils.lr_scheduler import LR_Scheduler\nfrom utils.saver import Saver\nfrom utils.summaries import TensorboardSummary\nfrom utils.metrics import Evaluator\n\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args\n\n        # Define Saver\n        self.saver = Saver(args)\n        self.saver.save_experiment_config()\n        # Define Tensorboard Summary\n        self.summary = TensorboardSummary(self.saver.experiment_dir)\n        self.writer = self.summary.create_summary()\n        \n        # Define Dataloader\n        kwargs = {\'num_workers\': args.workers, \'pin_memory\': True}\n        self.train_loader, self.val_loader, self.test_loader, self.nclass = make_data_loader(args, **kwargs)\n\n        # Define network\n        model = DeepLab(num_classes=self.nclass,\n                        backbone=args.backbone,\n                        output_stride=args.out_stride,\n                        sync_bn=args.sync_bn,\n                        freeze_bn=args.freeze_bn)\n\n        train_params = [{\'params\': model.get_1x_lr_params(), \'lr\': args.lr},\n                        {\'params\': model.get_10x_lr_params(), \'lr\': args.lr * 10}]\n\n        # Define Optimizer\n        optimizer = torch.optim.SGD(train_params, momentum=args.momentum,\n                                    weight_decay=args.weight_decay, nesterov=args.nesterov)\n\n        # Define Criterion\n        # whether to use class balanced weights\n        if args.use_balanced_weights:\n            classes_weights_path = os.path.join(Path.db_root_dir(args.dataset), args.dataset+\'_classes_weights.npy\')\n            if os.path.isfile(classes_weights_path):\n                weight = np.load(classes_weights_path)\n            else:\n                weight = calculate_weigths_labels(args.dataset, self.train_loader, self.nclass)\n            weight = torch.from_numpy(weight.astype(np.float32))\n        else:\n            weight = None\n        self.criterion = SegmentationLosses(weight=weight, cuda=args.cuda).build_loss(mode=args.loss_type)\n        self.model, self.optimizer = model, optimizer\n        \n        # Define Evaluator\n        self.evaluator = Evaluator(self.nclass)\n        # Define lr scheduler\n        self.scheduler = LR_Scheduler(args.lr_scheduler, args.lr,\n                                            args.epochs, len(self.train_loader))\n\n        # Using cuda\n        if args.cuda:\n            self.model = torch.nn.DataParallel(self.model, device_ids=self.args.gpu_ids)\n            patch_replication_callback(self.model)\n            self.model = self.model.cuda()\n\n        # Resuming checkpoint\n        self.best_pred = 0.0\n        if args.resume is not None:\n            if not os.path.isfile(args.resume):\n                raise RuntimeError(""=> no checkpoint found at \'{}\'"" .format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            if args.cuda:\n                self.model.module.load_state_dict(checkpoint[\'state_dict\'])\n            else:\n                self.model.load_state_dict(checkpoint[\'state_dict\'])\n            if not args.ft:\n                self.optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            self.best_pred = checkpoint[\'best_pred\']\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n\n        # Clear start epoch if fine-tuning\n        if args.ft:\n            args.start_epoch = 0\n\n    def training(self, epoch):\n        train_loss = 0.0\n        self.model.train()\n        tbar = tqdm(self.train_loader)\n        num_img_tr = len(self.train_loader)\n        for i, sample in enumerate(tbar):\n            image, target = sample[\'image\'], sample[\'label\']\n            if self.args.cuda:\n                image, target = image.cuda(), target.cuda()\n            self.scheduler(self.optimizer, i, epoch, self.best_pred)\n            self.optimizer.zero_grad()\n            output = self.model(image)\n            loss = self.criterion(output, target)\n            loss.backward()\n            self.optimizer.step()\n            train_loss += loss.item()\n            tbar.set_description(\'Train loss: %.3f\' % (train_loss / (i + 1)))\n            self.writer.add_scalar(\'train/total_loss_iter\', loss.item(), i + num_img_tr * epoch)\n\n            # Show 10 * 3 inference results each epoch\n            if i % (num_img_tr // 10) == 0:\n                global_step = i + num_img_tr * epoch\n                self.summary.visualize_image(self.writer, self.args.dataset, image, target, output, global_step)\n\n        self.writer.add_scalar(\'train/total_loss_epoch\', train_loss, epoch)\n        print(\'[Epoch: %d, numImages: %5d]\' % (epoch, i * self.args.batch_size + image.data.shape[0]))\n        print(\'Loss: %.3f\' % train_loss)\n\n        if self.args.no_val:\n            # save checkpoint every epoch\n            is_best = False\n            self.saver.save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'state_dict\': self.model.module.state_dict(),\n                \'optimizer\': self.optimizer.state_dict(),\n                \'best_pred\': self.best_pred,\n            }, is_best)\n\n\n    def validation(self, epoch):\n        self.model.eval()\n        self.evaluator.reset()\n        tbar = tqdm(self.val_loader, desc=\'\\r\')\n        test_loss = 0.0\n        for i, sample in enumerate(tbar):\n            image, target = sample[\'image\'], sample[\'label\']\n            if self.args.cuda:\n                image, target = image.cuda(), target.cuda()\n            with torch.no_grad():\n                output = self.model(image)\n            loss = self.criterion(output, target)\n            test_loss += loss.item()\n            tbar.set_description(\'Test loss: %.3f\' % (test_loss / (i + 1)))\n            pred = output.data.cpu().numpy()\n            target = target.cpu().numpy()\n            pred = np.argmax(pred, axis=1)\n            # Add batch sample into evaluator\n            self.evaluator.add_batch(target, pred)\n\n        # Fast test during the training\n        Acc = self.evaluator.Pixel_Accuracy()\n        Acc_class = self.evaluator.Pixel_Accuracy_Class()\n        mIoU = self.evaluator.Mean_Intersection_over_Union()\n        FWIoU = self.evaluator.Frequency_Weighted_Intersection_over_Union()\n        self.writer.add_scalar(\'val/total_loss_epoch\', test_loss, epoch)\n        self.writer.add_scalar(\'val/mIoU\', mIoU, epoch)\n        self.writer.add_scalar(\'val/Acc\', Acc, epoch)\n        self.writer.add_scalar(\'val/Acc_class\', Acc_class, epoch)\n        self.writer.add_scalar(\'val/fwIoU\', FWIoU, epoch)\n        print(\'Validation:\')\n        print(\'[Epoch: %d, numImages: %5d]\' % (epoch, i * self.args.batch_size + image.data.shape[0]))\n        print(""Acc:{}, Acc_class:{}, mIoU:{}, fwIoU: {}"".format(Acc, Acc_class, mIoU, FWIoU))\n        print(\'Loss: %.3f\' % test_loss)\n\n        new_pred = mIoU\n        if new_pred > self.best_pred:\n            is_best = True\n            self.best_pred = new_pred\n            self.saver.save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'state_dict\': self.model.module.state_dict(),\n                \'optimizer\': self.optimizer.state_dict(),\n                \'best_pred\': self.best_pred,\n            }, is_best)\n\ndef main():\n    parser = argparse.ArgumentParser(description=""PyTorch DeeplabV3Plus Training"")\n    parser.add_argument(\'--backbone\', type=str, default=\'resnet\',\n                        choices=[\'resnet\', \'xception\', \'drn\', \'mobilenet\'],\n                        help=\'backbone name (default: resnet)\')\n    parser.add_argument(\'--out-stride\', type=int, default=16,\n                        help=\'network output stride (default: 8)\')\n    parser.add_argument(\'--dataset\', type=str, default=\'pascal\',\n                        choices=[\'pascal\', \'coco\', \'cityscapes\'],\n                        help=\'dataset name (default: pascal)\')\n    parser.add_argument(\'--use-sbd\', action=\'store_true\', default=True,\n                        help=\'whether to use SBD dataset (default: True)\')\n    parser.add_argument(\'--workers\', type=int, default=4,\n                        metavar=\'N\', help=\'dataloader threads\')\n    parser.add_argument(\'--base-size\', type=int, default=513,\n                        help=\'base image size\')\n    parser.add_argument(\'--crop-size\', type=int, default=513,\n                        help=\'crop image size\')\n    parser.add_argument(\'--sync-bn\', type=bool, default=None,\n                        help=\'whether to use sync bn (default: auto)\')\n    parser.add_argument(\'--freeze-bn\', type=bool, default=False,\n                        help=\'whether to freeze bn parameters (default: False)\')\n    parser.add_argument(\'--loss-type\', type=str, default=\'ce\',\n                        choices=[\'ce\', \'focal\'],\n                        help=\'loss func type (default: ce)\')\n    # training hyper params\n    parser.add_argument(\'--epochs\', type=int, default=None, metavar=\'N\',\n                        help=\'number of epochs to train (default: auto)\')\n    parser.add_argument(\'--start_epoch\', type=int, default=0,\n                        metavar=\'N\', help=\'start epochs (default:0)\')\n    parser.add_argument(\'--batch-size\', type=int, default=None,\n                        metavar=\'N\', help=\'input batch size for \\\n                                training (default: auto)\')\n    parser.add_argument(\'--test-batch-size\', type=int, default=None,\n                        metavar=\'N\', help=\'input batch size for \\\n                                testing (default: auto)\')\n    parser.add_argument(\'--use-balanced-weights\', action=\'store_true\', default=False,\n                        help=\'whether to use balanced weights (default: False)\')\n    # optimizer params\n    parser.add_argument(\'--lr\', type=float, default=None, metavar=\'LR\',\n                        help=\'learning rate (default: auto)\')\n    parser.add_argument(\'--lr-scheduler\', type=str, default=\'poly\',\n                        choices=[\'poly\', \'step\', \'cos\'],\n                        help=\'lr scheduler mode: (default: poly)\')\n    parser.add_argument(\'--momentum\', type=float, default=0.9,\n                        metavar=\'M\', help=\'momentum (default: 0.9)\')\n    parser.add_argument(\'--weight-decay\', type=float, default=5e-4,\n                        metavar=\'M\', help=\'w-decay (default: 5e-4)\')\n    parser.add_argument(\'--nesterov\', action=\'store_true\', default=False,\n                        help=\'whether use nesterov (default: False)\')\n    # cuda, seed and logging\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=\n                        False, help=\'disables CUDA training\')\n    parser.add_argument(\'--gpu-ids\', type=str, default=\'0\',\n                        help=\'use which gpu to train, must be a \\\n                        comma-separated list of integers only (default=0)\')\n    parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                        help=\'random seed (default: 1)\')\n    # checking point\n    parser.add_argument(\'--resume\', type=str, default=None,\n                        help=\'put the path to resuming file if needed\')\n    parser.add_argument(\'--checkname\', type=str, default=None,\n                        help=\'set the checkpoint name\')\n    # finetuning pre-trained models\n    parser.add_argument(\'--ft\', action=\'store_true\', default=False,\n                        help=\'finetuning on a different dataset\')\n    # evaluation option\n    parser.add_argument(\'--eval-interval\', type=int, default=1,\n                        help=\'evaluuation interval (default: 1)\')\n    parser.add_argument(\'--no-val\', action=\'store_true\', default=False,\n                        help=\'skip validation during training\')\n\n    args = parser.parse_args()\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    if args.cuda:\n        try:\n            args.gpu_ids = [int(s) for s in args.gpu_ids.split(\',\')]\n        except ValueError:\n            raise ValueError(\'Argument --gpu_ids must be a comma-separated list of integers only\')\n\n    if args.sync_bn is None:\n        if args.cuda and len(args.gpu_ids) > 1:\n            args.sync_bn = True\n        else:\n            args.sync_bn = False\n\n    # default settings for epochs, batch_size and lr\n    if args.epochs is None:\n        epoches = {\n            \'coco\': 30,\n            \'cityscapes\': 200,\n            \'pascal\': 50,\n        }\n        args.epochs = epoches[args.dataset.lower()]\n\n    if args.batch_size is None:\n        args.batch_size = 4 * len(args.gpu_ids)\n\n    if args.test_batch_size is None:\n        args.test_batch_size = args.batch_size\n\n    if args.lr is None:\n        lrs = {\n            \'coco\': 0.1,\n            \'cityscapes\': 0.01,\n            \'pascal\': 0.007,\n        }\n        args.lr = lrs[args.dataset.lower()] / (4 * len(args.gpu_ids)) * args.batch_size\n\n\n    if args.checkname is None:\n        args.checkname = \'deeplab-\'+str(args.backbone)\n    print(args)\n    torch.manual_seed(args.seed)\n    trainer = Trainer(args)\n    print(\'Starting Epoch:\', trainer.args.start_epoch)\n    print(\'Total Epoches:\', trainer.args.epochs)\n    for epoch in range(trainer.args.start_epoch, trainer.args.epochs):\n        trainer.training(epoch)\n        if not trainer.args.no_val and epoch % args.eval_interval == (args.eval_interval - 1):\n            trainer.validation(epoch)\n\n    trainer.writer.close()\n\nif __name__ == ""__main__"":\n   main()\n'"
dataloaders/__init__.py,1,"b""from dataloaders.datasets import cityscapes, coco, combine_dbs, pascal, sbd\nfrom torch.utils.data import DataLoader\n\ndef make_data_loader(args, **kwargs):\n\n    if args.dataset == 'pascal':\n        train_set = pascal.VOCSegmentation(args, split='train')\n        val_set = pascal.VOCSegmentation(args, split='val')\n        if args.use_sbd:\n            sbd_train = sbd.SBDSegmentation(args, split=['train', 'val'])\n            train_set = combine_dbs.CombineDBs([train_set, sbd_train], excluded=[val_set])\n\n        num_class = train_set.NUM_CLASSES\n        train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, **kwargs)\n        val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, **kwargs)\n        test_loader = None\n\n        return train_loader, val_loader, test_loader, num_class\n\n    elif args.dataset == 'cityscapes':\n        train_set = cityscapes.CityscapesSegmentation(args, split='train')\n        val_set = cityscapes.CityscapesSegmentation(args, split='val')\n        test_set = cityscapes.CityscapesSegmentation(args, split='test')\n        num_class = train_set.NUM_CLASSES\n        train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, **kwargs)\n        val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, **kwargs)\n        test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, **kwargs)\n\n        return train_loader, val_loader, test_loader, num_class\n\n    elif args.dataset == 'coco':\n        train_set = coco.COCOSegmentation(args, split='train')\n        val_set = coco.COCOSegmentation(args, split='val')\n        num_class = train_set.NUM_CLASSES\n        train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, **kwargs)\n        val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, **kwargs)\n        test_loader = None\n        return train_loader, val_loader, test_loader, num_class\n\n    else:\n        raise NotImplementedError\n\n"""
dataloaders/custom_transforms.py,2,"b'import torch\nimport random\nimport numpy as np\n\nfrom PIL import Image, ImageOps, ImageFilter\n\nclass Normalize(object):\n    """"""Normalize a tensor image with mean and standard deviation.\n    Args:\n        mean (tuple): means for each channel.\n        std (tuple): standard deviations for each channel.\n    """"""\n    def __init__(self, mean=(0., 0., 0.), std=(1., 1., 1.)):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, sample):\n        img = sample[\'image\']\n        mask = sample[\'label\']\n        img = np.array(img).astype(np.float32)\n        mask = np.array(mask).astype(np.float32)\n        img /= 255.0\n        img -= self.mean\n        img /= self.std\n\n        return {\'image\': img,\n                \'label\': mask}\n\n\nclass ToTensor(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample):\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        img = sample[\'image\']\n        mask = sample[\'label\']\n        img = np.array(img).astype(np.float32).transpose((2, 0, 1))\n        mask = np.array(mask).astype(np.float32)\n\n        img = torch.from_numpy(img).float()\n        mask = torch.from_numpy(mask).float()\n\n        return {\'image\': img,\n                \'label\': mask}\n\n\nclass RandomHorizontalFlip(object):\n    def __call__(self, sample):\n        img = sample[\'image\']\n        mask = sample[\'label\']\n        if random.random() < 0.5:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n\n        return {\'image\': img,\n                \'label\': mask}\n\n\nclass RandomRotate(object):\n    def __init__(self, degree):\n        self.degree = degree\n\n    def __call__(self, sample):\n        img = sample[\'image\']\n        mask = sample[\'label\']\n        rotate_degree = random.uniform(-1*self.degree, self.degree)\n        img = img.rotate(rotate_degree, Image.BILINEAR)\n        mask = mask.rotate(rotate_degree, Image.NEAREST)\n\n        return {\'image\': img,\n                \'label\': mask}\n\n\nclass RandomGaussianBlur(object):\n    def __call__(self, sample):\n        img = sample[\'image\']\n        mask = sample[\'label\']\n        if random.random() < 0.5:\n            img = img.filter(ImageFilter.GaussianBlur(\n                radius=random.random()))\n\n        return {\'image\': img,\n                \'label\': mask}\n\n\nclass RandomScaleCrop(object):\n    def __init__(self, base_size, crop_size, fill=0):\n        self.base_size = base_size\n        self.crop_size = crop_size\n        self.fill = fill\n\n    def __call__(self, sample):\n        img = sample[\'image\']\n        mask = sample[\'label\']\n        # random scale (short edge)\n        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n        w, h = img.size\n        if h > w:\n            ow = short_size\n            oh = int(1.0 * h * ow / w)\n        else:\n            oh = short_size\n            ow = int(1.0 * w * oh / h)\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((ow, oh), Image.NEAREST)\n        # pad crop\n        if short_size < self.crop_size:\n            padh = self.crop_size - oh if oh < self.crop_size else 0\n            padw = self.crop_size - ow if ow < self.crop_size else 0\n            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=self.fill)\n        # random crop crop_size\n        w, h = img.size\n        x1 = random.randint(0, w - self.crop_size)\n        y1 = random.randint(0, h - self.crop_size)\n        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n\n        return {\'image\': img,\n                \'label\': mask}\n\n\nclass FixScaleCrop(object):\n    def __init__(self, crop_size):\n        self.crop_size = crop_size\n\n    def __call__(self, sample):\n        img = sample[\'image\']\n        mask = sample[\'label\']\n        w, h = img.size\n        if w > h:\n            oh = self.crop_size\n            ow = int(1.0 * w * oh / h)\n        else:\n            ow = self.crop_size\n            oh = int(1.0 * h * ow / w)\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((ow, oh), Image.NEAREST)\n        # center crop\n        w, h = img.size\n        x1 = int(round((w - self.crop_size) / 2.))\n        y1 = int(round((h - self.crop_size) / 2.))\n        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n\n        return {\'image\': img,\n                \'label\': mask}\n\nclass FixedResize(object):\n    def __init__(self, size):\n        self.size = (size, size)  # size: (h, w)\n\n    def __call__(self, sample):\n        img = sample[\'image\']\n        mask = sample[\'label\']\n\n        assert img.size == mask.size\n\n        img = img.resize(self.size, Image.BILINEAR)\n        mask = mask.resize(self.size, Image.NEAREST)\n\n        return {\'image\': img,\n                \'label\': mask}'"
dataloaders/utils.py,1,"b'import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\ndef decode_seg_map_sequence(label_masks, dataset=\'pascal\'):\n    rgb_masks = []\n    for label_mask in label_masks:\n        rgb_mask = decode_segmap(label_mask, dataset)\n        rgb_masks.append(rgb_mask)\n    rgb_masks = torch.from_numpy(np.array(rgb_masks).transpose([0, 3, 1, 2]))\n    return rgb_masks\n\n\ndef decode_segmap(label_mask, dataset, plot=False):\n    """"""Decode segmentation class labels into a color image\n    Args:\n        label_mask (np.ndarray): an (M,N) array of integer values denoting\n          the class label at each spatial location.\n        plot (bool, optional): whether to show the resulting color image\n          in a figure.\n    Returns:\n        (np.ndarray, optional): the resulting decoded color image.\n    """"""\n    if dataset == \'pascal\' or dataset == \'coco\':\n        n_classes = 21\n        label_colours = get_pascal_labels()\n    elif dataset == \'cityscapes\':\n        n_classes = 19\n        label_colours = get_cityscapes_labels()\n    else:\n        raise NotImplementedError\n\n    r = label_mask.copy()\n    g = label_mask.copy()\n    b = label_mask.copy()\n    for ll in range(0, n_classes):\n        r[label_mask == ll] = label_colours[ll, 0]\n        g[label_mask == ll] = label_colours[ll, 1]\n        b[label_mask == ll] = label_colours[ll, 2]\n    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n    rgb[:, :, 0] = r / 255.0\n    rgb[:, :, 1] = g / 255.0\n    rgb[:, :, 2] = b / 255.0\n    if plot:\n        plt.imshow(rgb)\n        plt.show()\n    else:\n        return rgb\n\n\ndef encode_segmap(mask):\n    """"""Encode segmentation label images as pascal classes\n    Args:\n        mask (np.ndarray): raw segmentation label image of dimension\n          (M, N, 3), in which the Pascal classes are encoded as colours.\n    Returns:\n        (np.ndarray): class map with dimensions (M,N), where the value at\n        a given location is the integer denoting the class index.\n    """"""\n    mask = mask.astype(int)\n    label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)\n    for ii, label in enumerate(get_pascal_labels()):\n        label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii\n    label_mask = label_mask.astype(int)\n    return label_mask\n\n\ndef get_cityscapes_labels():\n    return np.array([\n        [128, 64, 128],\n        [244, 35, 232],\n        [70, 70, 70],\n        [102, 102, 156],\n        [190, 153, 153],\n        [153, 153, 153],\n        [250, 170, 30],\n        [220, 220, 0],\n        [107, 142, 35],\n        [152, 251, 152],\n        [0, 130, 180],\n        [220, 20, 60],\n        [255, 0, 0],\n        [0, 0, 142],\n        [0, 0, 70],\n        [0, 60, 100],\n        [0, 80, 100],\n        [0, 0, 230],\n        [119, 11, 32]])\n\n\ndef get_pascal_labels():\n    """"""Load the mapping that associates pascal classes with label colors\n    Returns:\n        np.ndarray with dimensions (21, 3)\n    """"""\n    return np.asarray([[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n                       [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n                       [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n                       [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n                       [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n                       [0, 64, 128]])'"
doc/deeplab_resnet.py,8,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\n\nBatchNorm2d = SynchronizedBatchNorm2d\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               dilation=dilation, padding=dilation, bias=False)\n        self.bn2 = BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n\n    def __init__(self, nInputChannels, block, layers, os=16, pretrained=False):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        if os == 16:\n            strides = [1, 2, 2, 1]\n            dilations = [1, 1, 1, 2]\n            blocks = [1, 2, 4]\n        elif os == 8:\n            strides = [1, 2, 1, 1]\n            dilations = [1, 1, 2, 2]\n            blocks = [1, 2, 1]\n        else:\n            raise NotImplementedError\n\n        # Modules\n        self.conv1 = nn.Conv2d(nInputChannels, 64, kernel_size=7, stride=2, padding=3,\n                                bias=False)\n        self.bn1 = BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=strides[0], dilation=dilations[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=strides[1], dilation=dilations[1])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=strides[2], dilation=dilations[2])\n        self.layer4 = self._make_MG_unit(block, 512, blocks=blocks, stride=strides[3], dilation=dilations[3])\n\n        self._init_weight()\n\n        if pretrained:\n            self._load_pretrained_model()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, dilation, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_MG_unit(self, block, planes, blocks=[1, 2, 4], stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, dilation=blocks[0]*dilation, downsample=downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, len(blocks)):\n            layers.append(block(self.inplanes, planes, stride=1, dilation=blocks[i]*dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        low_level_feat = x\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x, low_level_feat\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _load_pretrained_model(self):\n        pretrain_dict = model_zoo.load_url(\'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\')\n        model_dict = {}\n        state_dict = self.state_dict()\n        for k, v in pretrain_dict.items():\n            if k in state_dict:\n                model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\ndef ResNet101(nInputChannels=3, os=16, pretrained=False):\n    model = ResNet(nInputChannels, Bottleneck, [3, 4, 23, 3], os, pretrained=pretrained)\n    return model\n\n\nclass ASPP_module(nn.Module):\n    def __init__(self, inplanes, planes, dilation):\n        super(ASPP_module, self).__init__()\n        if dilation == 1:\n            kernel_size = 1\n            padding = 0\n        else:\n            kernel_size = 3\n            padding = dilation\n        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n                                            stride=1, padding=padding, dilation=dilation, bias=False)\n        self.bn = BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n\n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\nclass DeepLabv3_plus(nn.Module):\n    def __init__(self, nInputChannels=3, n_classes=21, os=16, pretrained=False, freeze_bn=False, _print=True):\n        if _print:\n            print(""Constructing DeepLabv3+ model..."")\n            print(""Backbone: Resnet-101"")\n            print(""Number of classes: {}"".format(n_classes))\n            print(""Output stride: {}"".format(os))\n            print(""Number of Input Channels: {}"".format(nInputChannels))\n        super(DeepLabv3_plus, self).__init__()\n\n        # Atrous Conv\n        self.resnet_features = ResNet101(nInputChannels, os, pretrained=pretrained)\n\n        # ASPP\n        if os == 16:\n            dilations = [1, 6, 12, 18]\n        elif os == 8:\n            dilations = [1, 12, 24, 36]\n        else:\n            raise NotImplementedError\n\n        self.aspp1 = ASPP_module(2048, 256, dilation=dilations[0])\n        self.aspp2 = ASPP_module(2048, 256, dilation=dilations[1])\n        self.aspp3 = ASPP_module(2048, 256, dilation=dilations[2])\n        self.aspp4 = ASPP_module(2048, 256, dilation=dilations[3])\n\n        self.relu = nn.ReLU()\n\n        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(2048, 256, 1, stride=1, bias=False),\n                                             BatchNorm2d(256),\n                                             nn.ReLU())\n\n        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n        self.bn1 = BatchNorm2d(256)\n\n        # adopt [1x1, 48] for channel reduction.\n        self.conv2 = nn.Conv2d(256, 48, 1, bias=False)\n        self.bn2 = BatchNorm2d(48)\n\n        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                       BatchNorm2d(256),\n                                       nn.ReLU(),\n                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                       BatchNorm2d(256),\n                                       nn.ReLU(),\n                                       nn.Conv2d(256, n_classes, kernel_size=1, stride=1))\n        if freeze_bn:\n            self._freeze_bn()\n\n    def forward(self, input):\n        x, low_level_features = self.resnet_features(input)\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.upsample(x5, size=x4.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = F.upsample(x, size=(int(math.ceil(input.size()[-2]/4)),\n                                int(math.ceil(input.size()[-1]/4))), mode=\'bilinear\', align_corners=True)\n\n        low_level_features = self.conv2(low_level_features)\n        low_level_features = self.bn2(low_level_features)\n        low_level_features = self.relu(low_level_features)\n\n\n        x = torch.cat((x, low_level_features), dim=1)\n        x = self.last_conv(x)\n        x = F.interpolate(x, size=input.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        return x\n\n    def _freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, BatchNorm2d):\n                m.eval()\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\ndef get_1x_lr_params(model):\n    """"""\n    This generator returns all the parameters of the net except for\n    the last classification layer. Note that for each batchnorm layer,\n    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n    any batchnorm parameter\n    """"""\n    b = [model.resnet_features]\n    for i in range(len(b)):\n        for k in b[i].parameters():\n            if k.requires_grad:\n                yield k\n\n\ndef get_10x_lr_params(model):\n    """"""\n    This generator returns all the parameters for the last layer of the net,\n    which does the classification of pixel into classes\n    """"""\n    b = [model.aspp1, model.aspp2, model.aspp3, model.aspp4, model.conv1, model.conv2, model.last_conv]\n    for j in range(len(b)):\n        for k in b[j].parameters():\n            if k.requires_grad:\n                yield k\n\n\nif __name__ == ""__main__"":\n    model = DeepLabv3_plus(nInputChannels=3, n_classes=21, os=16, pretrained=True, _print=True)\n    model.eval()\n    image = torch.randn(1, 3, 512, 512)\n    with torch.no_grad():\n        output = model.forward(image)\n    print(output.size())\n\n\n\n\n\n\n'"
doc/deeplab_xception.py,7,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\n\nBatchNorm2d = SynchronizedBatchNorm2d\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=0, dilation=1, bias=False):\n        super(SeparableConv2d, self)._init_()\n\n        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, padding, dilation,\n                               groups=inplanes, bias=bias)\n        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\ndef fixed_padding(inputs, kernel_size, dilation):\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n    return padded_inputs\n\n\nclass SeparableConv2d_same(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, bias=False):\n        super(SeparableConv2d_same, self).__init__()\n\n        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, 0, dilation,\n                               groups=inplanes, bias=bias)\n        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = fixed_padding(x, self.conv1.kernel_size[0], dilation=self.conv1.dilation[0])\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, inplanes, planes, reps, stride=1, dilation=1, start_with_relu=True, grow_first=True, is_last=False):\n        super(Block, self).__init__()\n\n        if planes != inplanes or stride != 1:\n            self.skip = nn.Conv2d(inplanes, planes, 1, stride=stride, bias=False)\n            self.skipbn = BatchNorm2d(planes)\n        else:\n            self.skip = None\n\n        self.relu = nn.ReLU(inplace=True)\n        rep = []\n\n        filters = inplanes\n        if grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d_same(inplanes, planes, 3, stride=1, dilation=dilation))\n            rep.append(BatchNorm2d(planes))\n            filters = planes\n\n        for i in range(reps - 1):\n            rep.append(self.relu)\n            rep.append(SeparableConv2d_same(filters, filters, 3, stride=1, dilation=dilation))\n            rep.append(BatchNorm2d(filters))\n\n        if not grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d_same(inplanes, planes, 3, stride=1, dilation=dilation))\n            rep.append(BatchNorm2d(planes))\n\n        if not start_with_relu:\n            rep = rep[1:]\n\n        if stride != 1:\n            rep.append(SeparableConv2d_same(planes, planes, 3, stride=2))\n\n        if stride == 1 and is_last:\n            rep.append(SeparableConv2d_same(planes, planes, 3, stride=1))\n\n\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x += skip\n\n        return x\n\n\nclass Xception(nn.Module):\n    """"""\n    Modified Alighed Xception\n    """"""\n    def __init__(self, inplanes=3, os=16, pretrained=False):\n        super(Xception, self).__init__()\n\n        if os == 16:\n            entry_block3_stride = 2\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 2)\n        elif os == 8:\n            entry_block3_stride = 1\n            middle_block_dilation = 2\n            exit_block_dilations = (2, 4)\n        else:\n            raise NotImplementedError\n\n\n        # Entry flow\n        self.conv1 = nn.Conv2d(inplanes, 32, 3, stride=2, padding=1, bias=False)\n        self.bn1 = BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)\n        self.bn2 = BatchNorm2d(64)\n\n        self.block1 = Block(64, 128, reps=2, stride=2, start_with_relu=False)\n        self.block2 = Block(128, 256, reps=2, stride=2, start_with_relu=True, grow_first=True)\n        self.block3 = Block(256, 728, reps=2, stride=entry_block3_stride, start_with_relu=True, grow_first=True,\n                            is_last=True)\n\n        # Middle flow\n        self.block4  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block5  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block6  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block7  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block8  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block9  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block10 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block11 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block12 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block13 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block14 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block15 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block16 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block17 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block18 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n        self.block19 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation, start_with_relu=True, grow_first=True)\n\n        # Exit flow\n        self.block20 = Block(728, 1024, reps=2, stride=1, dilation=exit_block_dilations[0],\n                             start_with_relu=True, grow_first=False, is_last=True)\n\n        self.conv3 = SeparableConv2d_same(1024, 1536, 3, stride=1, dilation=exit_block_dilations[1])\n        self.bn3 = BatchNorm2d(1536)\n\n        self.conv4 = SeparableConv2d_same(1536, 1536, 3, stride=1, dilation=exit_block_dilations[1])\n        self.bn4 = BatchNorm2d(1536)\n\n        self.conv5 = SeparableConv2d_same(1536, 2048, 3, stride=1, dilation=exit_block_dilations[1])\n        self.bn5 = BatchNorm2d(2048)\n\n        # Init weights\n        self._init_weight()\n\n        # Load pretrained model\n        if pretrained:\n            self._load_xception_pretrained()\n\n    def forward(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        low_level_feat = x\n        x = self.block2(x)\n        x = self.block3(x)\n\n        # Middle flow\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n        x = self.block13(x)\n        x = self.block14(x)\n        x = self.block15(x)\n        x = self.block16(x)\n        x = self.block17(x)\n        x = self.block18(x)\n        x = self.block19(x)\n\n        # Exit flow\n        x = self.block20(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu(x)\n\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu(x)\n\n        return x, low_level_feat\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _load_xception_pretrained(self):\n        pretrain_dict = model_zoo.load_url(\'http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth\')\n        model_dict = {}\n        state_dict = self.state_dict()\n\n        for k, v in pretrain_dict.items():\n            if k in model_dict:\n                if \'pointwise\' in k:\n                    v = v.unsqueeze(-1).unsqueeze(-1)\n                if k.startswith(\'block11\'):\n                    model_dict[k] = v\n                    model_dict[k.replace(\'block11\', \'block12\')] = v\n                    model_dict[k.replace(\'block11\', \'block13\')] = v\n                    model_dict[k.replace(\'block11\', \'block14\')] = v\n                    model_dict[k.replace(\'block11\', \'block15\')] = v\n                    model_dict[k.replace(\'block11\', \'block16\')] = v\n                    model_dict[k.replace(\'block11\', \'block17\')] = v\n                    model_dict[k.replace(\'block11\', \'block18\')] = v\n                    model_dict[k.replace(\'block11\', \'block19\')] = v\n                elif k.startswith(\'block12\'):\n                    model_dict[k.replace(\'block12\', \'block20\')] = v\n                elif k.startswith(\'bn3\'):\n                    model_dict[k] = v\n                    model_dict[k.replace(\'bn3\', \'bn4\')] = v\n                elif k.startswith(\'conv4\'):\n                    model_dict[k.replace(\'conv4\', \'conv5\')] = v\n                elif k.startswith(\'bn4\'):\n                    model_dict[k.replace(\'bn4\', \'bn5\')] = v\n                else:\n                    model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\nclass ASPP_module(nn.Module):\n    def __init__(self, inplanes, planes, dilation):\n        super(ASPP_module, self).__init__()\n        if dilation == 1:\n            kernel_size = 1\n            padding = 0\n        else:\n            kernel_size = 3\n            padding = dilation\n        self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n                                            stride=1, padding=padding, dilation=dilation, bias=False)\n        self.bn = BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n\n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_convolution(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\nclass DeepLabv3_plus(nn.Module):\n    def __init__(self, nInputChannels=3, n_classes=21, os=16, pretrained=False, freeze_bn=False, _print=True):\n        if _print:\n            print(""Constructing DeepLabv3+ model..."")\n            print(""Backbone: Xception"")\n            print(""Number of classes: {}"".format(n_classes))\n            print(""Output stride: {}"".format(os))\n            print(""Number of Input Channels: {}"".format(nInputChannels))\n        super(DeepLabv3_plus, self).__init__()\n\n        # Atrous Conv\n        self.xception_features = Xception(nInputChannels, os, pretrained)\n\n        # ASPP\n        if os == 16:\n            dilations = [1, 6, 12, 18]\n        elif os == 8:\n            dilations = [1, 12, 24, 36]\n        else:\n            raise NotImplementedError\n\n        self.aspp1 = ASPP_module(2048, 256, dilation=dilations[0])\n        self.aspp2 = ASPP_module(2048, 256, dilation=dilations[1])\n        self.aspp3 = ASPP_module(2048, 256, dilation=dilations[2])\n        self.aspp4 = ASPP_module(2048, 256, dilation=dilations[3])\n\n        self.relu = nn.ReLU()\n\n        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(2048, 256, 1, stride=1, bias=False),\n                                             BatchNorm2d(256),\n                                             nn.ReLU())\n\n        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n        self.bn1 = BatchNorm2d(256)\n\n        # adopt [1x1, 48] for channel reduction.\n        self.conv2 = nn.Conv2d(128, 48, 1, bias=False)\n        self.bn2 = BatchNorm2d(48)\n\n        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                       BatchNorm2d(256),\n                                       nn.ReLU(),\n                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                       BatchNorm2d(256),\n                                       nn.ReLU(),\n                                       nn.Conv2d(256, n_classes, kernel_size=1, stride=1))\n        if freeze_bn:\n            self._freeze_bn()\n\n    def forward(self, input):\n        x, low_level_features = self.xception_features(input)\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = F.interpolate(x, size=(int(math.ceil(input.size()[-2]/4)),\n                                int(math.ceil(input.size()[-1]/4))), mode=\'bilinear\', align_corners=True)\n\n        low_level_features = self.conv2(low_level_features)\n        low_level_features = self.bn2(low_level_features)\n        low_level_features = self.relu(low_level_features)\n\n\n        x = torch.cat((x, low_level_features), dim=1)\n        x = self.last_conv(x)\n        x = F.interpolate(x, size=input.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        return x\n\n    def _freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, BatchNorm2d):\n                m.eval()\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\ndef get_1x_lr_params(model):\n    """"""\n    This generator returns all the parameters of the net except for\n    the last classification layer. Note that for each batchnorm layer,\n    requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n    any batchnorm parameter\n    """"""\n    b = [model.xception_features]\n    for i in range(len(b)):\n        for k in b[i].parameters():\n            if k.requires_grad:\n                yield k\n\n\ndef get_10x_lr_params(model):\n    """"""\n    This generator returns all the parameters for the last layer of the net,\n    which does the classification of pixel into classes\n    """"""\n    b = [model.aspp1, model.aspp2, model.aspp3, model.aspp4, model.conv1, model.conv2, model.last_conv]\n    for j in range(len(b)):\n        for k in b[j].parameters():\n            if k.requires_grad:\n                yield k\n\n\nif __name__ == ""__main__"":\n    model = DeepLabv3_plus(nInputChannels=3, n_classes=21, os=16, pretrained=True, _print=True)\n    model.eval()\n    image = torch.randn(1, 3, 512, 512)\n    with torch.no_grad():\n        output = model.forward(image)\n    print(output.size())\n\n\n\n'"
modeling/__init__.py,0,b''
modeling/aspp.py,5,"b""import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\n\nclass _ASPPModule(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size, padding, dilation, BatchNorm):\n        super(_ASPPModule, self).__init__()\n        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n                                            stride=1, padding=padding, dilation=dilation, bias=False)\n        self.bn = BatchNorm(planes)\n        self.relu = nn.ReLU()\n\n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_conv(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\nclass ASPP(nn.Module):\n    def __init__(self, backbone, output_stride, BatchNorm):\n        super(ASPP, self).__init__()\n        if backbone == 'drn':\n            inplanes = 512\n        elif backbone == 'mobilenet':\n            inplanes = 320\n        else:\n            inplanes = 2048\n        if output_stride == 16:\n            dilations = [1, 6, 12, 18]\n        elif output_stride == 8:\n            dilations = [1, 12, 24, 36]\n        else:\n            raise NotImplementedError\n\n        self.aspp1 = _ASPPModule(inplanes, 256, 1, padding=0, dilation=dilations[0], BatchNorm=BatchNorm)\n        self.aspp2 = _ASPPModule(inplanes, 256, 3, padding=dilations[1], dilation=dilations[1], BatchNorm=BatchNorm)\n        self.aspp3 = _ASPPModule(inplanes, 256, 3, padding=dilations[2], dilation=dilations[2], BatchNorm=BatchNorm)\n        self.aspp4 = _ASPPModule(inplanes, 256, 3, padding=dilations[3], dilation=dilations[3], BatchNorm=BatchNorm)\n\n        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(inplanes, 256, 1, stride=1, bias=False),\n                                             BatchNorm(256),\n                                             nn.ReLU())\n        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n        self.bn1 = BatchNorm(256)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self._init_weight()\n\n    def forward(self, x):\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        return self.dropout(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\ndef build_aspp(backbone, output_stride, BatchNorm):\n    return ASPP(backbone, output_stride, BatchNorm)"""
modeling/decoder.py,4,"b""import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\n\nclass Decoder(nn.Module):\n    def __init__(self, num_classes, backbone, BatchNorm):\n        super(Decoder, self).__init__()\n        if backbone == 'resnet' or backbone == 'drn':\n            low_level_inplanes = 256\n        elif backbone == 'xception':\n            low_level_inplanes = 128\n        elif backbone == 'mobilenet':\n            low_level_inplanes = 24\n        else:\n            raise NotImplementedError\n\n        self.conv1 = nn.Conv2d(low_level_inplanes, 48, 1, bias=False)\n        self.bn1 = BatchNorm(48)\n        self.relu = nn.ReLU()\n        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                       BatchNorm(256),\n                                       nn.ReLU(),\n                                       nn.Dropout(0.5),\n                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n                                       BatchNorm(256),\n                                       nn.ReLU(),\n                                       nn.Dropout(0.1),\n                                       nn.Conv2d(256, num_classes, kernel_size=1, stride=1))\n        self._init_weight()\n\n\n    def forward(self, x, low_level_feat):\n        low_level_feat = self.conv1(low_level_feat)\n        low_level_feat = self.bn1(low_level_feat)\n        low_level_feat = self.relu(low_level_feat)\n\n        x = F.interpolate(x, size=low_level_feat.size()[2:], mode='bilinear', align_corners=True)\n        x = torch.cat((x, low_level_feat), dim=1)\n        x = self.last_conv(x)\n\n        return x\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\ndef build_decoder(num_classes, backbone, BatchNorm):\n    return Decoder(num_classes, backbone, BatchNorm)"""
modeling/deeplab.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\nfrom modeling.aspp import build_aspp\nfrom modeling.decoder import build_decoder\nfrom modeling.backbone import build_backbone\n\nclass DeepLab(nn.Module):\n    def __init__(self, backbone=\'resnet\', output_stride=16, num_classes=21,\n                 sync_bn=True, freeze_bn=False):\n        super(DeepLab, self).__init__()\n        if backbone == \'drn\':\n            output_stride = 8\n\n        if sync_bn == True:\n            BatchNorm = SynchronizedBatchNorm2d\n        else:\n            BatchNorm = nn.BatchNorm2d\n\n        self.backbone = build_backbone(backbone, output_stride, BatchNorm)\n        self.aspp = build_aspp(backbone, output_stride, BatchNorm)\n        self.decoder = build_decoder(num_classes, backbone, BatchNorm)\n\n        self.freeze_bn = freeze_bn\n\n    def forward(self, input):\n        x, low_level_feat = self.backbone(input)\n        x = self.aspp(x)\n        x = self.decoder(x, low_level_feat)\n        x = F.interpolate(x, size=input.size()[2:], mode=\'bilinear\', align_corners=True)\n\n        return x\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, SynchronizedBatchNorm2d):\n                m.eval()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def get_1x_lr_params(self):\n        modules = [self.backbone]\n        for i in range(len(modules)):\n            for m in modules[i].named_modules():\n                if self.freeze_bn:\n                    if isinstance(m[1], nn.Conv2d):\n                        for p in m[1].parameters():\n                            if p.requires_grad:\n                                yield p\n                else:\n                    if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \\\n                            or isinstance(m[1], nn.BatchNorm2d):\n                        for p in m[1].parameters():\n                            if p.requires_grad:\n                                yield p\n\n    def get_10x_lr_params(self):\n        modules = [self.aspp, self.decoder]\n        for i in range(len(modules)):\n            for m in modules[i].named_modules():\n                if self.freeze_bn:\n                    if isinstance(m[1], nn.Conv2d):\n                        for p in m[1].parameters():\n                            if p.requires_grad:\n                                yield p\n                else:\n                    if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \\\n                            or isinstance(m[1], nn.BatchNorm2d):\n                        for p in m[1].parameters():\n                            if p.requires_grad:\n                                yield p\n\nif __name__ == ""__main__"":\n    model = DeepLab(backbone=\'mobilenet\', output_stride=16)\n    model.eval()\n    input = torch.rand(1, 3, 513, 513)\n    output = model(input)\n    print(output.size())\n\n\n'"
utils/calculate_weights.py,0,"b""import os\nfrom tqdm import tqdm\nimport numpy as np\nfrom mypath import Path\n\ndef calculate_weigths_labels(dataset, dataloader, num_classes):\n    # Create an instance from the data loader\n    z = np.zeros((num_classes,))\n    # Initialize tqdm\n    tqdm_batch = tqdm(dataloader)\n    print('Calculating classes weights')\n    for sample in tqdm_batch:\n        y = sample['label']\n        y = y.detach().cpu().numpy()\n        mask = (y >= 0) & (y < num_classes)\n        labels = y[mask].astype(np.uint8)\n        count_l = np.bincount(labels, minlength=num_classes)\n        z += count_l\n    tqdm_batch.close()\n    total_frequency = np.sum(z)\n    class_weights = []\n    for frequency in z:\n        class_weight = 1 / (np.log(1.02 + (frequency / total_frequency)))\n        class_weights.append(class_weight)\n    ret = np.array(class_weights)\n    classes_weights_path = os.path.join(Path.db_root_dir(dataset), dataset+'_classes_weights.npy')\n    np.save(classes_weights_path, ret)\n\n    return ret"""
utils/loss.py,4,"b'import torch\nimport torch.nn as nn\n\nclass SegmentationLosses(object):\n    def __init__(self, weight=None, size_average=True, batch_average=True, ignore_index=255, cuda=False):\n        self.ignore_index = ignore_index\n        self.weight = weight\n        self.size_average = size_average\n        self.batch_average = batch_average\n        self.cuda = cuda\n\n    def build_loss(self, mode=\'ce\'):\n        """"""Choices: [\'ce\' or \'focal\']""""""\n        if mode == \'ce\':\n            return self.CrossEntropyLoss\n        elif mode == \'focal\':\n            return self.FocalLoss\n        else:\n            raise NotImplementedError\n\n    def CrossEntropyLoss(self, logit, target):\n        n, c, h, w = logit.size()\n        criterion = nn.CrossEntropyLoss(weight=self.weight, ignore_index=self.ignore_index,\n                                        size_average=self.size_average)\n        if self.cuda:\n            criterion = criterion.cuda()\n\n        loss = criterion(logit, target.long())\n\n        if self.batch_average:\n            loss /= n\n\n        return loss\n\n    def FocalLoss(self, logit, target, gamma=2, alpha=0.5):\n        n, c, h, w = logit.size()\n        criterion = nn.CrossEntropyLoss(weight=self.weight, ignore_index=self.ignore_index,\n                                        size_average=self.size_average)\n        if self.cuda:\n            criterion = criterion.cuda()\n\n        logpt = -criterion(logit, target.long())\n        pt = torch.exp(logpt)\n        if alpha is not None:\n            logpt *= alpha\n        loss = -((1 - pt) ** gamma) * logpt\n\n        if self.batch_average:\n            loss /= n\n\n        return loss\n\nif __name__ == ""__main__"":\n    loss = SegmentationLosses(cuda=True)\n    a = torch.rand(1, 3, 7, 7).cuda()\n    b = torch.rand(1, 7, 7).cuda()\n    print(loss.CrossEntropyLoss(a, b).item())\n    print(loss.FocalLoss(a, b, gamma=0, alpha=None).item())\n    print(loss.FocalLoss(a, b, gamma=2, alpha=0.5).item())\n\n\n\n\n'"
utils/lr_scheduler.py,0,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## ECE Department, Rutgers University\n## Email: zhang.hang@rutgers.edu\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nimport math\n\nclass LR_Scheduler(object):\n    """"""Learning Rate Scheduler\n\n    Step mode: ``lr = baselr * 0.1 ^ {floor(epoch-1 / lr_step)}``\n\n    Cosine mode: ``lr = baselr * 0.5 * (1 + cos(iter/maxiter))``\n\n    Poly mode: ``lr = baselr * (1 - iter/maxiter) ^ 0.9``\n\n    Args:\n        args:\n          :attr:`args.lr_scheduler` lr scheduler mode (`cos`, `poly`),\n          :attr:`args.lr` base learning rate, :attr:`args.epochs` number of epochs,\n          :attr:`args.lr_step`\n\n        iters_per_epoch: number of iterations per epoch\n    """"""\n    def __init__(self, mode, base_lr, num_epochs, iters_per_epoch=0,\n                 lr_step=0, warmup_epochs=0):\n        self.mode = mode\n        print(\'Using {} LR Scheduler!\'.format(self.mode))\n        self.lr = base_lr\n        if mode == \'step\':\n            assert lr_step\n        self.lr_step = lr_step\n        self.iters_per_epoch = iters_per_epoch\n        self.N = num_epochs * iters_per_epoch\n        self.epoch = -1\n        self.warmup_iters = warmup_epochs * iters_per_epoch\n\n    def __call__(self, optimizer, i, epoch, best_pred):\n        T = epoch * self.iters_per_epoch + i\n        if self.mode == \'cos\':\n            lr = 0.5 * self.lr * (1 + math.cos(1.0 * T / self.N * math.pi))\n        elif self.mode == \'poly\':\n            lr = self.lr * pow((1 - 1.0 * T / self.N), 0.9)\n        elif self.mode == \'step\':\n            lr = self.lr * (0.1 ** (epoch // self.lr_step))\n        else:\n            raise NotImplemented\n        # warm up lr schedule\n        if self.warmup_iters > 0 and T < self.warmup_iters:\n            lr = lr * 1.0 * T / self.warmup_iters\n        if epoch > self.epoch:\n            print(\'\\n=>Epoches %i, learning rate = %.4f, \\\n                previous best = %.4f\' % (epoch, lr, best_pred))\n            self.epoch = epoch\n        assert lr >= 0\n        self._adjust_learning_rate(optimizer, lr)\n\n    def _adjust_learning_rate(self, optimizer, lr):\n        if len(optimizer.param_groups) == 1:\n            optimizer.param_groups[0][\'lr\'] = lr\n        else:\n            # enlarge the lr at the head\n            optimizer.param_groups[0][\'lr\'] = lr\n            for i in range(1, len(optimizer.param_groups)):\n                optimizer.param_groups[i][\'lr\'] = lr * 10\n'"
utils/metrics.py,0,"b""import numpy as np\n\n\nclass Evaluator(object):\n    def __init__(self, num_class):\n        self.num_class = num_class\n        self.confusion_matrix = np.zeros((self.num_class,)*2)\n\n    def Pixel_Accuracy(self):\n        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n        return Acc\n\n    def Pixel_Accuracy_Class(self):\n        Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n        Acc = np.nanmean(Acc)\n        return Acc\n\n    def Mean_Intersection_over_Union(self):\n        MIoU = np.diag(self.confusion_matrix) / (\n                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n                    np.diag(self.confusion_matrix))\n        MIoU = np.nanmean(MIoU)\n        return MIoU\n\n    def Frequency_Weighted_Intersection_over_Union(self):\n        freq = np.sum(self.confusion_matrix, axis=1) / np.sum(self.confusion_matrix)\n        iu = np.diag(self.confusion_matrix) / (\n                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n                    np.diag(self.confusion_matrix))\n\n        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n        return FWIoU\n\n    def _generate_matrix(self, gt_image, pre_image):\n        mask = (gt_image >= 0) & (gt_image < self.num_class)\n        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n        count = np.bincount(label, minlength=self.num_class**2)\n        confusion_matrix = count.reshape(self.num_class, self.num_class)\n        return confusion_matrix\n\n    def add_batch(self, gt_image, pre_image):\n        assert gt_image.shape == pre_image.shape\n        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.num_class,) * 2)\n\n\n\n\n"""
utils/saver.py,1,"b'import os\nimport shutil\nimport torch\nfrom collections import OrderedDict\nimport glob\n\nclass Saver(object):\n\n    def __init__(self, args):\n        self.args = args\n        self.directory = os.path.join(\'run\', args.dataset, args.checkname)\n        self.runs = sorted(glob.glob(os.path.join(self.directory, \'experiment_*\')))\n        run_id = int(self.runs[-1].split(\'_\')[-1]) + 1 if self.runs else 0\n\n        self.experiment_dir = os.path.join(self.directory, \'experiment_{}\'.format(str(run_id)))\n        if not os.path.exists(self.experiment_dir):\n            os.makedirs(self.experiment_dir)\n\n    def save_checkpoint(self, state, is_best, filename=\'checkpoint.pth.tar\'):\n        """"""Saves checkpoint to disk""""""\n        filename = os.path.join(self.experiment_dir, filename)\n        torch.save(state, filename)\n        if is_best:\n            best_pred = state[\'best_pred\']\n            with open(os.path.join(self.experiment_dir, \'best_pred.txt\'), \'w\') as f:\n                f.write(str(best_pred))\n            if self.runs:\n                previous_miou = [0.0]\n                for run in self.runs:\n                    run_id = run.split(\'_\')[-1]\n                    path = os.path.join(self.directory, \'experiment_{}\'.format(str(run_id)), \'best_pred.txt\')\n                    if os.path.exists(path):\n                        with open(path, \'r\') as f:\n                            miou = float(f.readline())\n                            previous_miou.append(miou)\n                    else:\n                        continue\n                max_miou = max(previous_miou)\n                if best_pred > max_miou:\n                    shutil.copyfile(filename, os.path.join(self.directory, \'model_best.pth.tar\'))\n            else:\n                shutil.copyfile(filename, os.path.join(self.directory, \'model_best.pth.tar\'))\n\n    def save_experiment_config(self):\n        logfile = os.path.join(self.experiment_dir, \'parameters.txt\')\n        log_file = open(logfile, \'w\')\n        p = OrderedDict()\n        p[\'datset\'] = self.args.dataset\n        p[\'backbone\'] = self.args.backbone\n        p[\'out_stride\'] = self.args.out_stride\n        p[\'lr\'] = self.args.lr\n        p[\'lr_scheduler\'] = self.args.lr_scheduler\n        p[\'loss_type\'] = self.args.loss_type\n        p[\'epoch\'] = self.args.epochs\n        p[\'base_size\'] = self.args.base_size\n        p[\'crop_size\'] = self.args.crop_size\n\n        for key, val in p.items():\n            log_file.write(key + \':\' + str(val) + \'\\n\')\n        log_file.close()'"
utils/summaries.py,2,"b""import os\nimport torch\nfrom torchvision.utils import make_grid\nfrom tensorboardX import SummaryWriter\nfrom dataloaders.utils import decode_seg_map_sequence\n\nclass TensorboardSummary(object):\n    def __init__(self, directory):\n        self.directory = directory\n\n    def create_summary(self):\n        writer = SummaryWriter(log_dir=os.path.join(self.directory))\n        return writer\n\n    def visualize_image(self, writer, dataset, image, target, output, global_step):\n        grid_image = make_grid(image[:3].clone().cpu().data, 3, normalize=True)\n        writer.add_image('Image', grid_image, global_step)\n        grid_image = make_grid(decode_seg_map_sequence(torch.max(output[:3], 1)[1].detach().cpu().numpy(),\n                                                       dataset=dataset), 3, normalize=False, range=(0, 255))\n        writer.add_image('Predicted label', grid_image, global_step)\n        grid_image = make_grid(decode_seg_map_sequence(torch.squeeze(target[:3], 1).detach().cpu().numpy(),\n                                                       dataset=dataset), 3, normalize=False, range=(0, 255))\n        writer.add_image('Groundtruth label', grid_image, global_step)"""
dataloaders/datasets/__init__.py,0,b''
dataloaders/datasets/cityscapes.py,2,"b'import os\nimport numpy as np\nimport scipy.misc as m\nfrom PIL import Image\nfrom torch.utils import data\nfrom mypath import Path\nfrom torchvision import transforms\nfrom dataloaders import custom_transforms as tr\n\nclass CityscapesSegmentation(data.Dataset):\n    NUM_CLASSES = 19\n\n    def __init__(self, args, root=Path.db_root_dir(\'cityscapes\'), split=""train""):\n\n        self.root = root\n        self.split = split\n        self.args = args\n        self.files = {}\n\n        self.images_base = os.path.join(self.root, \'leftImg8bit\', self.split)\n        self.annotations_base = os.path.join(self.root, \'gtFine_trainvaltest\', \'gtFine\', self.split)\n\n        self.files[split] = self.recursive_glob(rootdir=self.images_base, suffix=\'.png\')\n\n        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n        self.class_names = [\'unlabelled\', \'road\', \'sidewalk\', \'building\', \'wall\', \'fence\', \\\n                            \'pole\', \'traffic_light\', \'traffic_sign\', \'vegetation\', \'terrain\', \\\n                            \'sky\', \'person\', \'rider\', \'car\', \'truck\', \'bus\', \'train\', \\\n                            \'motorcycle\', \'bicycle\']\n\n        self.ignore_index = 255\n        self.class_map = dict(zip(self.valid_classes, range(self.NUM_CLASSES)))\n\n        if not self.files[split]:\n            raise Exception(""No files for split=[%s] found in %s"" % (split, self.images_base))\n\n        print(""Found %d %s images"" % (len(self.files[split]), split))\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n\n        img_path = self.files[self.split][index].rstrip()\n        lbl_path = os.path.join(self.annotations_base,\n                                img_path.split(os.sep)[-2],\n                                os.path.basename(img_path)[:-15] + \'gtFine_labelIds.png\')\n\n        _img = Image.open(img_path).convert(\'RGB\')\n        _tmp = np.array(Image.open(lbl_path), dtype=np.uint8)\n        _tmp = self.encode_segmap(_tmp)\n        _target = Image.fromarray(_tmp)\n\n        sample = {\'image\': _img, \'label\': _target}\n\n        if self.split == \'train\':\n            return self.transform_tr(sample)\n        elif self.split == \'val\':\n            return self.transform_val(sample)\n        elif self.split == \'test\':\n            return self.transform_ts(sample)\n\n    def encode_segmap(self, mask):\n        # Put all void classes to zero\n        for _voidc in self.void_classes:\n            mask[mask == _voidc] = self.ignore_index\n        for _validc in self.valid_classes:\n            mask[mask == _validc] = self.class_map[_validc]\n        return mask\n\n    def recursive_glob(self, rootdir=\'.\', suffix=\'\'):\n        """"""Performs recursive glob with given suffix and rootdir\n            :param rootdir is the root directory\n            :param suffix is the suffix to be searched\n        """"""\n        return [os.path.join(looproot, filename)\n                for looproot, _, filenames in os.walk(rootdir)\n                for filename in filenames if filename.endswith(suffix)]\n\n    def transform_tr(self, sample):\n        composed_transforms = transforms.Compose([\n            tr.RandomHorizontalFlip(),\n            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size, fill=255),\n            tr.RandomGaussianBlur(),\n            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            tr.ToTensor()])\n\n        return composed_transforms(sample)\n\n    def transform_val(self, sample):\n\n        composed_transforms = transforms.Compose([\n            tr.FixScaleCrop(crop_size=self.args.crop_size),\n            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            tr.ToTensor()])\n\n        return composed_transforms(sample)\n\n    def transform_ts(self, sample):\n\n        composed_transforms = transforms.Compose([\n            tr.FixedResize(size=self.args.crop_size),\n            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            tr.ToTensor()])\n\n        return composed_transforms(sample)\n\nif __name__ == \'__main__\':\n    from dataloaders.utils import decode_segmap\n    from torch.utils.data import DataLoader\n    import matplotlib.pyplot as plt\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    args = parser.parse_args()\n    args.base_size = 513\n    args.crop_size = 513\n\n    cityscapes_train = CityscapesSegmentation(args, split=\'train\')\n\n    dataloader = DataLoader(cityscapes_train, batch_size=2, shuffle=True, num_workers=2)\n\n    for ii, sample in enumerate(dataloader):\n        for jj in range(sample[""image""].size()[0]):\n            img = sample[\'image\'].numpy()\n            gt = sample[\'label\'].numpy()\n            tmp = np.array(gt[jj]).astype(np.uint8)\n            segmap = decode_segmap(tmp, dataset=\'cityscapes\')\n            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])\n            img_tmp *= (0.229, 0.224, 0.225)\n            img_tmp += (0.485, 0.456, 0.406)\n            img_tmp *= 255.0\n            img_tmp = img_tmp.astype(np.uint8)\n            plt.figure()\n            plt.title(\'display\')\n            plt.subplot(211)\n            plt.imshow(img_tmp)\n            plt.subplot(212)\n            plt.imshow(segmap)\n\n        if ii == 1:\n            break\n\n    plt.show(block=True)\n\n'"
dataloaders/datasets/coco.py,4,"b'import numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom mypath import Path\nfrom tqdm import trange\nimport os\nfrom pycocotools.coco import COCO\nfrom pycocotools import mask\nfrom torchvision import transforms\nfrom dataloaders import custom_transforms as tr\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nclass COCOSegmentation(Dataset):\n    NUM_CLASSES = 21\n    CAT_LIST = [0, 5, 2, 16, 9, 44, 6, 3, 17, 62, 21, 67, 18, 19, 4,\n        1, 64, 20, 63, 7, 72]\n\n    def __init__(self,\n                 args,\n                 base_dir=Path.db_root_dir(\'coco\'),\n                 split=\'train\',\n                 year=\'2017\'):\n        super().__init__()\n        ann_file = os.path.join(base_dir, \'annotations/instances_{}{}.json\'.format(split, year))\n        ids_file = os.path.join(base_dir, \'annotations/{}_ids_{}.pth\'.format(split, year))\n        self.img_dir = os.path.join(base_dir, \'images/{}{}\'.format(split, year))\n        self.split = split\n        self.coco = COCO(ann_file)\n        self.coco_mask = mask\n        if os.path.exists(ids_file):\n            self.ids = torch.load(ids_file)\n        else:\n            ids = list(self.coco.imgs.keys())\n            self.ids = self._preprocess(ids, ids_file)\n        self.args = args\n\n    def __getitem__(self, index):\n        _img, _target = self._make_img_gt_point_pair(index)\n        sample = {\'image\': _img, \'label\': _target}\n\n        if self.split == ""train"":\n            return self.transform_tr(sample)\n        elif self.split == \'val\':\n            return self.transform_val(sample)\n\n    def _make_img_gt_point_pair(self, index):\n        coco = self.coco\n        img_id = self.ids[index]\n        img_metadata = coco.loadImgs(img_id)[0]\n        path = img_metadata[\'file_name\']\n        _img = Image.open(os.path.join(self.img_dir, path)).convert(\'RGB\')\n        cocotarget = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n        _target = Image.fromarray(self._gen_seg_mask(\n            cocotarget, img_metadata[\'height\'], img_metadata[\'width\']))\n\n        return _img, _target\n\n    def _preprocess(self, ids, ids_file):\n        print(""Preprocessing mask, this will take a while. "" + \\\n              ""But don\'t worry, it only run once for each split."")\n        tbar = trange(len(ids))\n        new_ids = []\n        for i in tbar:\n            img_id = ids[i]\n            cocotarget = self.coco.loadAnns(self.coco.getAnnIds(imgIds=img_id))\n            img_metadata = self.coco.loadImgs(img_id)[0]\n            mask = self._gen_seg_mask(cocotarget, img_metadata[\'height\'],\n                                      img_metadata[\'width\'])\n            # more than 1k pixels\n            if (mask > 0).sum() > 1000:\n                new_ids.append(img_id)\n            tbar.set_description(\'Doing: {}/{}, got {} qualified images\'. \\\n                                 format(i, len(ids), len(new_ids)))\n        print(\'Found number of qualified images: \', len(new_ids))\n        torch.save(new_ids, ids_file)\n        return new_ids\n\n    def _gen_seg_mask(self, target, h, w):\n        mask = np.zeros((h, w), dtype=np.uint8)\n        coco_mask = self.coco_mask\n        for instance in target:\n            rle = coco_mask.frPyObjects(instance[\'segmentation\'], h, w)\n            m = coco_mask.decode(rle)\n            cat = instance[\'category_id\']\n            if cat in self.CAT_LIST:\n                c = self.CAT_LIST.index(cat)\n            else:\n                continue\n            if len(m.shape) < 3:\n                mask[:, :] += (mask == 0) * (m * c)\n            else:\n                mask[:, :] += (mask == 0) * (((np.sum(m, axis=2)) > 0) * c).astype(np.uint8)\n        return mask\n\n    def transform_tr(self, sample):\n        composed_transforms = transforms.Compose([\n            tr.RandomHorizontalFlip(),\n            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),\n            tr.RandomGaussianBlur(),\n            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            tr.ToTensor()])\n\n        return composed_transforms(sample)\n\n    def transform_val(self, sample):\n\n        composed_transforms = transforms.Compose([\n            tr.FixScaleCrop(crop_size=self.args.crop_size),\n            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            tr.ToTensor()])\n\n        return composed_transforms(sample)\n\n\n    def __len__(self):\n        return len(self.ids)\n\n\n\nif __name__ == ""__main__"":\n    from dataloaders import custom_transforms as tr\n    from dataloaders.utils import decode_segmap\n    from torch.utils.data import DataLoader\n    from torchvision import transforms\n    import matplotlib.pyplot as plt\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    args = parser.parse_args()\n    args.base_size = 513\n    args.crop_size = 513\n\n    coco_val = COCOSegmentation(args, split=\'val\', year=\'2017\')\n\n    dataloader = DataLoader(coco_val, batch_size=4, shuffle=True, num_workers=0)\n\n    for ii, sample in enumerate(dataloader):\n        for jj in range(sample[""image""].size()[0]):\n            img = sample[\'image\'].numpy()\n            gt = sample[\'label\'].numpy()\n            tmp = np.array(gt[jj]).astype(np.uint8)\n            segmap = decode_segmap(tmp, dataset=\'coco\')\n            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])\n            img_tmp *= (0.229, 0.224, 0.225)\n            img_tmp += (0.485, 0.456, 0.406)\n            img_tmp *= 255.0\n            img_tmp = img_tmp.astype(np.uint8)\n            plt.figure()\n            plt.title(\'display\')\n            plt.subplot(211)\n            plt.imshow(img_tmp)\n            plt.subplot(212)\n            plt.imshow(segmap)\n\n        if ii == 1:\n            break\n\n    plt.show(block=True)'"
dataloaders/datasets/combine_dbs.py,2,"b'import torch.utils.data as data\n\n\nclass CombineDBs(data.Dataset):\n    NUM_CLASSES = 21\n    def __init__(self, dataloaders, excluded=None):\n        self.dataloaders = dataloaders\n        self.excluded = excluded\n        self.im_ids = []\n\n        # Combine object lists\n        for dl in dataloaders:\n            for elem in dl.im_ids:\n                if elem not in self.im_ids:\n                    self.im_ids.append(elem)\n\n        # Exclude\n        if excluded:\n            for dl in excluded:\n                for elem in dl.im_ids:\n                    if elem in self.im_ids:\n                        self.im_ids.remove(elem)\n\n        # Get object pointers\n        self.cat_list = []\n        self.im_list = []\n        new_im_ids = []\n        num_images = 0\n        for ii, dl in enumerate(dataloaders):\n            for jj, curr_im_id in enumerate(dl.im_ids):\n                if (curr_im_id in self.im_ids) and (curr_im_id not in new_im_ids):\n                    num_images += 1\n                    new_im_ids.append(curr_im_id)\n                    self.cat_list.append({\'db_ii\': ii, \'cat_ii\': jj})\n\n        self.im_ids = new_im_ids\n        print(\'Combined number of images: {:d}\'.format(num_images))\n\n    def __getitem__(self, index):\n\n        _db_ii = self.cat_list[index][""db_ii""]\n        _cat_ii = self.cat_list[index][\'cat_ii\']\n        sample = self.dataloaders[_db_ii].__getitem__(_cat_ii)\n\n        if \'meta\' in sample.keys():\n            sample[\'meta\'][\'db\'] = str(self.dataloaders[_db_ii])\n\n        return sample\n\n    def __len__(self):\n        return len(self.cat_list)\n\n    def __str__(self):\n        include_db = [str(db) for db in self.dataloaders]\n        exclude_db = [str(db) for db in self.excluded]\n        return \'Included datasets:\'+str(include_db)+\'\\n\'+\'Excluded datasets:\'+str(exclude_db)\n\n\nif __name__ == ""__main__"":\n    import matplotlib.pyplot as plt\n    from dataloaders.datasets import pascal, sbd\n    from dataloaders import sbd\n    import torch\n    import numpy as np\n    from dataloaders.utils import decode_segmap\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    args = parser.parse_args()\n    args.base_size = 513\n    args.crop_size = 513\n\n    pascal_voc_val = pascal.VOCSegmentation(args, split=\'val\')\n    sbd = sbd.SBDSegmentation(args, split=[\'train\', \'val\'])\n    pascal_voc_train = pascal.VOCSegmentation(args, split=\'train\')\n\n    dataset = CombineDBs([pascal_voc_train, sbd], excluded=[pascal_voc_val])\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)\n\n    for ii, sample in enumerate(dataloader):\n        for jj in range(sample[""image""].size()[0]):\n            img = sample[\'image\'].numpy()\n            gt = sample[\'label\'].numpy()\n            tmp = np.array(gt[jj]).astype(np.uint8)\n            segmap = decode_segmap(tmp, dataset=\'pascal\')\n            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])\n            img_tmp *= (0.229, 0.224, 0.225)\n            img_tmp += (0.485, 0.456, 0.406)\n            img_tmp *= 255.0\n            img_tmp = img_tmp.astype(np.uint8)\n            plt.figure()\n            plt.title(\'display\')\n            plt.subplot(211)\n            plt.imshow(img_tmp)\n            plt.subplot(212)\n            plt.imshow(segmap)\n\n        if ii == 1:\n            break\n    plt.show(block=True)'"
dataloaders/datasets/pascal.py,2,"b'from __future__ import print_function, division\nimport os\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom mypath import Path\nfrom torchvision import transforms\nfrom dataloaders import custom_transforms as tr\n\nclass VOCSegmentation(Dataset):\n    """"""\n    PascalVoc dataset\n    """"""\n    NUM_CLASSES = 21\n\n    def __init__(self,\n                 args,\n                 base_dir=Path.db_root_dir(\'pascal\'),\n                 split=\'train\',\n                 ):\n        """"""\n        :param base_dir: path to VOC dataset directory\n        :param split: train/val\n        :param transform: transform to apply\n        """"""\n        super().__init__()\n        self._base_dir = base_dir\n        self._image_dir = os.path.join(self._base_dir, \'JPEGImages\')\n        self._cat_dir = os.path.join(self._base_dir, \'SegmentationClass\')\n\n        if isinstance(split, str):\n            self.split = [split]\n        else:\n            split.sort()\n            self.split = split\n\n        self.args = args\n\n        _splits_dir = os.path.join(self._base_dir, \'ImageSets\', \'Segmentation\')\n\n        self.im_ids = []\n        self.images = []\n        self.categories = []\n\n        for splt in self.split:\n            with open(os.path.join(os.path.join(_splits_dir, splt + \'.txt\')), ""r"") as f:\n                lines = f.read().splitlines()\n\n            for ii, line in enumerate(lines):\n                _image = os.path.join(self._image_dir, line + "".jpg"")\n                _cat = os.path.join(self._cat_dir, line + "".png"")\n                assert os.path.isfile(_image)\n                assert os.path.isfile(_cat)\n                self.im_ids.append(line)\n                self.images.append(_image)\n                self.categories.append(_cat)\n\n        assert (len(self.images) == len(self.categories))\n\n        # Display stats\n        print(\'Number of images in {}: {:d}\'.format(split, len(self.images)))\n\n    def __len__(self):\n        return len(self.images)\n\n\n    def __getitem__(self, index):\n        _img, _target = self._make_img_gt_point_pair(index)\n        sample = {\'image\': _img, \'label\': _target}\n\n        for split in self.split:\n            if split == ""train"":\n                return self.transform_tr(sample)\n            elif split == \'val\':\n                return self.transform_val(sample)\n\n\n    def _make_img_gt_point_pair(self, index):\n        _img = Image.open(self.images[index]).convert(\'RGB\')\n        _target = Image.open(self.categories[index])\n\n        return _img, _target\n\n    def transform_tr(self, sample):\n        composed_transforms = transforms.Compose([\n            tr.RandomHorizontalFlip(),\n            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),\n            tr.RandomGaussianBlur(),\n            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            tr.ToTensor()])\n\n        return composed_transforms(sample)\n\n    def transform_val(self, sample):\n\n        composed_transforms = transforms.Compose([\n            tr.FixScaleCrop(crop_size=self.args.crop_size),\n            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            tr.ToTensor()])\n\n        return composed_transforms(sample)\n\n    def __str__(self):\n        return \'VOC2012(split=\' + str(self.split) + \')\'\n\n\nif __name__ == \'__main__\':\n    from dataloaders.utils import decode_segmap\n    from torch.utils.data import DataLoader\n    import matplotlib.pyplot as plt\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    args = parser.parse_args()\n    args.base_size = 513\n    args.crop_size = 513\n\n    voc_train = VOCSegmentation(args, split=\'train\')\n\n    dataloader = DataLoader(voc_train, batch_size=5, shuffle=True, num_workers=0)\n\n    for ii, sample in enumerate(dataloader):\n        for jj in range(sample[""image""].size()[0]):\n            img = sample[\'image\'].numpy()\n            gt = sample[\'label\'].numpy()\n            tmp = np.array(gt[jj]).astype(np.uint8)\n            segmap = decode_segmap(tmp, dataset=\'pascal\')\n            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])\n            img_tmp *= (0.229, 0.224, 0.225)\n            img_tmp += (0.485, 0.456, 0.406)\n            img_tmp *= 255.0\n            img_tmp = img_tmp.astype(np.uint8)\n            plt.figure()\n            plt.title(\'display\')\n            plt.subplot(211)\n            plt.imshow(img_tmp)\n            plt.subplot(212)\n            plt.imshow(segmap)\n\n        if ii == 1:\n            break\n\n    plt.show(block=True)\n\n\n'"
dataloaders/datasets/sbd.py,2,"b'from __future__ import print_function, division\nimport os\n\nimport numpy as np\nimport scipy.io\nimport torch.utils.data as data\nfrom PIL import Image\nfrom mypath import Path\n\nfrom torchvision import transforms\nfrom dataloaders import custom_transforms as tr\n\nclass SBDSegmentation(data.Dataset):\n    NUM_CLASSES = 21\n\n    def __init__(self,\n                 args,\n                 base_dir=Path.db_root_dir(\'sbd\'),\n                 split=\'train\',\n                 ):\n        """"""\n        :param base_dir: path to VOC dataset directory\n        :param split: train/val\n        :param transform: transform to apply\n        """"""\n        super().__init__()\n        self._base_dir = base_dir\n        self._dataset_dir = os.path.join(self._base_dir, \'dataset\')\n        self._image_dir = os.path.join(self._dataset_dir, \'img\')\n        self._cat_dir = os.path.join(self._dataset_dir, \'cls\')\n\n\n        if isinstance(split, str):\n            self.split = [split]\n        else:\n            split.sort()\n            self.split = split\n\n        self.args = args\n\n        # Get list of all images from the split and check that the files exist\n        self.im_ids = []\n        self.images = []\n        self.categories = []\n        for splt in self.split:\n            with open(os.path.join(self._dataset_dir, splt + \'.txt\'), ""r"") as f:\n                lines = f.read().splitlines()\n\n            for line in lines:\n                _image = os.path.join(self._image_dir, line + "".jpg"")\n                _categ= os.path.join(self._cat_dir, line + "".mat"")\n                assert os.path.isfile(_image)\n                assert os.path.isfile(_categ)\n                self.im_ids.append(line)\n                self.images.append(_image)\n                self.categories.append(_categ)\n\n        assert (len(self.images) == len(self.categories))\n\n        # Display stats\n        print(\'Number of images: {:d}\'.format(len(self.images)))\n\n\n    def __getitem__(self, index):\n        _img, _target = self._make_img_gt_point_pair(index)\n        sample = {\'image\': _img, \'label\': _target}\n\n        return self.transform(sample)\n\n    def __len__(self):\n        return len(self.images)\n\n    def _make_img_gt_point_pair(self, index):\n        _img = Image.open(self.images[index]).convert(\'RGB\')\n        _target = Image.fromarray(scipy.io.loadmat(self.categories[index])[""GTcls""][0][\'Segmentation\'][0])\n\n        return _img, _target\n\n    def transform(self, sample):\n        composed_transforms = transforms.Compose([\n            tr.RandomHorizontalFlip(),\n            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),\n            tr.RandomGaussianBlur(),\n            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            tr.ToTensor()])\n\n        return composed_transforms(sample)\n\n\n    def __str__(self):\n        return \'SBDSegmentation(split=\' + str(self.split) + \')\'\n\n\nif __name__ == \'__main__\':\n    from dataloaders.utils import decode_segmap\n    from torch.utils.data import DataLoader\n    import matplotlib.pyplot as plt\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    args = parser.parse_args()\n    args.base_size = 513\n    args.crop_size = 513\n\n    sbd_train = SBDSegmentation(args, split=\'train\')\n    dataloader = DataLoader(sbd_train, batch_size=2, shuffle=True, num_workers=2)\n\n    for ii, sample in enumerate(dataloader):\n        for jj in range(sample[""image""].size()[0]):\n            img = sample[\'image\'].numpy()\n            gt = sample[\'label\'].numpy()\n            tmp = np.array(gt[jj]).astype(np.uint8)\n            segmap = decode_segmap(tmp, dataset=\'pascal\')\n            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])\n            img_tmp *= (0.229, 0.224, 0.225)\n            img_tmp += (0.485, 0.456, 0.406)\n            img_tmp *= 255.0\n            img_tmp = img_tmp.astype(np.uint8)\n            plt.figure()\n            plt.title(\'display\')\n            plt.subplot(211)\n            plt.imshow(img_tmp)\n            plt.subplot(212)\n            plt.imshow(segmap)\n\n        if ii == 1:\n            break\n\n    plt.show(block=True)'"
modeling/backbone/__init__.py,0,"b""from modeling.backbone import resnet, xception, drn, mobilenet\n\ndef build_backbone(backbone, output_stride, BatchNorm):\n    if backbone == 'resnet':\n        return resnet.ResNet101(output_stride, BatchNorm)\n    elif backbone == 'xception':\n        return xception.AlignedXception(output_stride, BatchNorm)\n    elif backbone == 'drn':\n        return drn.drn_d_54(BatchNorm)\n    elif backbone == 'mobilenet':\n        return mobilenet.MobileNetV2(output_stride, BatchNorm)\n    else:\n        raise NotImplementedError\n"""
modeling/backbone/drn.py,4,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nfrom modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\n\nwebroot = \'http://dl.yf.io/drn/\'\n\nmodel_urls = {\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'drn-c-26\': webroot + \'drn_c_26-ddedf421.pth\',\n    \'drn-c-42\': webroot + \'drn_c_42-9d336e8c.pth\',\n    \'drn-c-58\': webroot + \'drn_c_58-0a53a92c.pth\',\n    \'drn-d-22\': webroot + \'drn_d_22-4bd2f8ea.pth\',\n    \'drn-d-38\': webroot + \'drn_d_38-eebb45f0.pth\',\n    \'drn-d-54\': webroot + \'drn_d_54-0e0534ff.pth\',\n    \'drn-d-105\': webroot + \'drn_d_105-12b40979.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=padding, bias=False, dilation=dilation)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True, BatchNorm=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride,\n                             padding=dilation[0], dilation=dilation[0])\n        self.bn1 = BatchNorm(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes,\n                             padding=dilation[1], dilation=dilation[1])\n        self.bn2 = BatchNorm(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True, BatchNorm=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=dilation[1], bias=False,\n                               dilation=dilation[1])\n        self.bn2 = BatchNorm(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass DRN(nn.Module):\n\n    def __init__(self, block, layers, arch=\'D\',\n                 channels=(16, 32, 64, 128, 256, 512, 512, 512),\n                 BatchNorm=None):\n        super(DRN, self).__init__()\n        self.inplanes = channels[0]\n        self.out_dim = channels[-1]\n        self.arch = arch\n\n        if arch == \'C\':\n            self.conv1 = nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n                                   padding=3, bias=False)\n            self.bn1 = BatchNorm(channels[0])\n            self.relu = nn.ReLU(inplace=True)\n\n            self.layer1 = self._make_layer(\n                BasicBlock, channels[0], layers[0], stride=1, BatchNorm=BatchNorm)\n            self.layer2 = self._make_layer(\n                BasicBlock, channels[1], layers[1], stride=2, BatchNorm=BatchNorm)\n\n        elif arch == \'D\':\n            self.layer0 = nn.Sequential(\n                nn.Conv2d(3, channels[0], kernel_size=7, stride=1, padding=3,\n                          bias=False),\n                BatchNorm(channels[0]),\n                nn.ReLU(inplace=True)\n            )\n\n            self.layer1 = self._make_conv_layers(\n                channels[0], layers[0], stride=1, BatchNorm=BatchNorm)\n            self.layer2 = self._make_conv_layers(\n                channels[1], layers[1], stride=2, BatchNorm=BatchNorm)\n\n        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=2, BatchNorm=BatchNorm)\n        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=2, BatchNorm=BatchNorm)\n        self.layer5 = self._make_layer(block, channels[4], layers[4],\n                                       dilation=2, new_level=False, BatchNorm=BatchNorm)\n        self.layer6 = None if layers[5] == 0 else \\\n            self._make_layer(block, channels[5], layers[5], dilation=4,\n                             new_level=False, BatchNorm=BatchNorm)\n\n        if arch == \'C\':\n            self.layer7 = None if layers[6] == 0 else \\\n                self._make_layer(BasicBlock, channels[6], layers[6], dilation=2,\n                                 new_level=False, residual=False, BatchNorm=BatchNorm)\n            self.layer8 = None if layers[7] == 0 else \\\n                self._make_layer(BasicBlock, channels[7], layers[7], dilation=1,\n                                 new_level=False, residual=False, BatchNorm=BatchNorm)\n        elif arch == \'D\':\n            self.layer7 = None if layers[6] == 0 else \\\n                self._make_conv_layers(channels[6], layers[6], dilation=2, BatchNorm=BatchNorm)\n            self.layer8 = None if layers[7] == 0 else \\\n                self._make_conv_layers(channels[7], layers[7], dilation=1, BatchNorm=BatchNorm)\n\n        self._init_weight()\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1,\n                    new_level=True, residual=True, BatchNorm=None):\n        assert dilation == 1 or dilation % 2 == 0\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = list()\n        layers.append(block(\n            self.inplanes, planes, stride, downsample,\n            dilation=(1, 1) if dilation == 1 else (\n                dilation // 2 if new_level else dilation, dilation),\n            residual=residual, BatchNorm=BatchNorm))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, residual=residual,\n                                dilation=(dilation, dilation), BatchNorm=BatchNorm))\n\n        return nn.Sequential(*layers)\n\n    def _make_conv_layers(self, channels, convs, stride=1, dilation=1, BatchNorm=None):\n        modules = []\n        for i in range(convs):\n            modules.extend([\n                nn.Conv2d(self.inplanes, channels, kernel_size=3,\n                          stride=stride if i == 0 else 1,\n                          padding=dilation, bias=False, dilation=dilation),\n                BatchNorm(channels),\n                nn.ReLU(inplace=True)])\n            self.inplanes = channels\n        return nn.Sequential(*modules)\n\n    def forward(self, x):\n        if self.arch == \'C\':\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.relu(x)\n        elif self.arch == \'D\':\n            x = self.layer0(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n\n        x = self.layer3(x)\n        low_level_feat = x\n\n        x = self.layer4(x)\n        x = self.layer5(x)\n\n        if self.layer6 is not None:\n            x = self.layer6(x)\n\n        if self.layer7 is not None:\n            x = self.layer7(x)\n\n        if self.layer8 is not None:\n            x = self.layer8(x)\n\n        return x, low_level_feat\n\n\nclass DRN_A(nn.Module):\n\n    def __init__(self, block, layers, BatchNorm=None):\n        self.inplanes = 64\n        super(DRN_A, self).__init__()\n        self.out_dim = 512 * block.expansion\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = BatchNorm(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], BatchNorm=BatchNorm)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, BatchNorm=BatchNorm)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                       dilation=2, BatchNorm=BatchNorm)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                       dilation=4, BatchNorm=BatchNorm)\n\n        self._init_weight()\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, BatchNorm=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, BatchNorm=BatchNorm))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,\n                                dilation=(dilation, dilation, ), BatchNorm=BatchNorm))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\ndef drn_a_50(BatchNorm, pretrained=True):\n    model = DRN_A(Bottleneck, [3, 4, 6, 3], BatchNorm=BatchNorm)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef drn_c_26(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch=\'C\', BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls[\'drn-c-26\'])\n        del pretrained[\'fc.weight\']\n        del pretrained[\'fc.bias\']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_c_42(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 1, 1], arch=\'C\', BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls[\'drn-c-42\'])\n        del pretrained[\'fc.weight\']\n        del pretrained[\'fc.bias\']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_c_58(BatchNorm, pretrained=True):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 1, 1], arch=\'C\', BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls[\'drn-c-58\'])\n        del pretrained[\'fc.weight\']\n        del pretrained[\'fc.bias\']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_22(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch=\'D\', BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls[\'drn-d-22\'])\n        del pretrained[\'fc.weight\']\n        del pretrained[\'fc.bias\']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_24(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 2, 2], arch=\'D\', BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls[\'drn-d-24\'])\n        del pretrained[\'fc.weight\']\n        del pretrained[\'fc.bias\']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_38(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 1, 1], arch=\'D\', BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls[\'drn-d-38\'])\n        del pretrained[\'fc.weight\']\n        del pretrained[\'fc.bias\']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_40(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 2, 2], arch=\'D\', BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls[\'drn-d-40\'])\n        del pretrained[\'fc.weight\']\n        del pretrained[\'fc.bias\']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_54(BatchNorm, pretrained=True):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 1, 1], arch=\'D\', BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls[\'drn-d-54\'])\n        del pretrained[\'fc.weight\']\n        del pretrained[\'fc.bias\']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_105(BatchNorm, pretrained=True):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 23, 3, 1, 1], arch=\'D\', BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls[\'drn-d-105\'])\n        del pretrained[\'fc.weight\']\n        del pretrained[\'fc.bias\']\n        model.load_state_dict(pretrained)\n    return model\n\nif __name__ == ""__main__"":\n    import torch\n    model = drn_a_50(BatchNorm=nn.BatchNorm2d, pretrained=True)\n    input = torch.rand(1, 3, 512, 512)\n    output, low_level_feat = model(input)\n    print(output.size())\n    print(low_level_feat.size())\n'"
modeling/backbone/mobilenet.py,5,"b'import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport math\nfrom modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\nimport torch.utils.model_zoo as model_zoo\n\ndef conv_bn(inp, oup, stride, BatchNorm):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        BatchNorm(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef fixed_padding(inputs, kernel_size, dilation):\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n    return padded_inputs\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, dilation, expand_ratio, BatchNorm):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n        self.kernel_size = 3\n        self.dilation = dilation\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 0, dilation, groups=hidden_dim, bias=False),\n                BatchNorm(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, 1, 1, bias=False),\n                BatchNorm(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, 1, bias=False),\n                BatchNorm(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 0, dilation, groups=hidden_dim, bias=False),\n                BatchNorm(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, 1, bias=False),\n                BatchNorm(oup),\n            )\n\n    def forward(self, x):\n        x_pad = fixed_padding(x, self.kernel_size, dilation=self.dilation)\n        if self.use_res_connect:\n            x = x + self.conv(x_pad)\n        else:\n            x = self.conv(x_pad)\n        return x\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, output_stride=8, BatchNorm=None, width_mult=1., pretrained=True):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        current_stride = 1\n        rate = 1\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        input_channel = int(input_channel * width_mult)\n        self.features = [conv_bn(3, input_channel, 2, BatchNorm)]\n        current_stride *= 2\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            if current_stride == output_stride:\n                stride = 1\n                dilation = rate\n                rate *= s\n            else:\n                stride = s\n                dilation = 1\n                current_stride *= s\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, stride, dilation, t, BatchNorm))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, dilation, t, BatchNorm))\n                input_channel = output_channel\n        self.features = nn.Sequential(*self.features)\n        self._initialize_weights()\n\n        if pretrained:\n            self._load_pretrained_model()\n\n        self.low_level_features = self.features[0:4]\n        self.high_level_features = self.features[4:]\n\n    def forward(self, x):\n        low_level_feat = self.low_level_features(x)\n        x = self.high_level_features(low_level_feat)\n        return x, low_level_feat\n\n    def _load_pretrained_model(self):\n        pretrain_dict = model_zoo.load_url(\'http://jeff95.me/models/mobilenet_v2-6a65762b.pth\')\n        model_dict = {}\n        state_dict = self.state_dict()\n        for k, v in pretrain_dict.items():\n            if k in state_dict:\n                model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\nif __name__ == ""__main__"":\n    input = torch.rand(1, 3, 512, 512)\n    model = MobileNetV2(output_stride=16, BatchNorm=nn.BatchNorm2d)\n    output, low_level_feat = model(input)\n    print(output.size())\n    print(low_level_feat.size())\n'"
modeling/backbone/resnet.py,4,"b'import math\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nfrom modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, BatchNorm=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               dilation=dilation, padding=dilation, bias=False)\n        self.bn2 = BatchNorm(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, output_stride, BatchNorm, pretrained=True):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        blocks = [1, 2, 4]\n        if output_stride == 16:\n            strides = [1, 2, 2, 1]\n            dilations = [1, 1, 1, 2]\n        elif output_stride == 8:\n            strides = [1, 2, 1, 1]\n            dilations = [1, 1, 2, 4]\n        else:\n            raise NotImplementedError\n\n        # Modules\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                                bias=False)\n        self.bn1 = BatchNorm(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=strides[0], dilation=dilations[0], BatchNorm=BatchNorm)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=strides[1], dilation=dilations[1], BatchNorm=BatchNorm)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=strides[2], dilation=dilations[2], BatchNorm=BatchNorm)\n        self.layer4 = self._make_MG_unit(block, 512, blocks=blocks, stride=strides[3], dilation=dilations[3], BatchNorm=BatchNorm)\n        # self.layer4 = self._make_layer(block, 512, layers[3], stride=strides[3], dilation=dilations[3], BatchNorm=BatchNorm)\n        self._init_weight()\n\n        if pretrained:\n            self._load_pretrained_model()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, BatchNorm=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, dilation, downsample, BatchNorm))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation, BatchNorm=BatchNorm))\n\n        return nn.Sequential(*layers)\n\n    def _make_MG_unit(self, block, planes, blocks, stride=1, dilation=1, BatchNorm=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, dilation=blocks[0]*dilation,\n                            downsample=downsample, BatchNorm=BatchNorm))\n        self.inplanes = planes * block.expansion\n        for i in range(1, len(blocks)):\n            layers.append(block(self.inplanes, planes, stride=1,\n                                dilation=blocks[i]*dilation, BatchNorm=BatchNorm))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        low_level_feat = x\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x, low_level_feat\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _load_pretrained_model(self):\n        pretrain_dict = model_zoo.load_url(\'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\')\n        model_dict = {}\n        state_dict = self.state_dict()\n        for k, v in pretrain_dict.items():\n            if k in state_dict:\n                model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\ndef ResNet101(output_stride, BatchNorm, pretrained=True):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], output_stride, BatchNorm, pretrained=pretrained)\n    return model\n\nif __name__ == ""__main__"":\n    import torch\n    model = ResNet101(BatchNorm=nn.BatchNorm2d, pretrained=True, output_stride=8)\n    input = torch.rand(1, 3, 512, 512)\n    output, low_level_feat = model(input)\n    print(output.size())\n    print(low_level_feat.size())'"
modeling/backbone/xception.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\n\ndef fixed_padding(inputs, kernel_size, dilation):\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n    return padded_inputs\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, bias=False, BatchNorm=None):\n        super(SeparableConv2d, self).__init__()\n\n        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, 0, dilation,\n                               groups=inplanes, bias=bias)\n        self.bn = BatchNorm(inplanes)\n        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = fixed_padding(x, self.conv1.kernel_size[0], dilation=self.conv1.dilation[0])\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, inplanes, planes, reps, stride=1, dilation=1, BatchNorm=None,\n                 start_with_relu=True, grow_first=True, is_last=False):\n        super(Block, self).__init__()\n\n        if planes != inplanes or stride != 1:\n            self.skip = nn.Conv2d(inplanes, planes, 1, stride=stride, bias=False)\n            self.skipbn = BatchNorm(planes)\n        else:\n            self.skip = None\n\n        self.relu = nn.ReLU(inplace=True)\n        rep = []\n\n        filters = inplanes\n        if grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(inplanes, planes, 3, 1, dilation, BatchNorm=BatchNorm))\n            rep.append(BatchNorm(planes))\n            filters = planes\n\n        for i in range(reps - 1):\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(filters, filters, 3, 1, dilation, BatchNorm=BatchNorm))\n            rep.append(BatchNorm(filters))\n\n        if not grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(inplanes, planes, 3, 1, dilation, BatchNorm=BatchNorm))\n            rep.append(BatchNorm(planes))\n\n        if stride != 1:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(planes, planes, 3, 2, BatchNorm=BatchNorm))\n            rep.append(BatchNorm(planes))\n\n        if stride == 1 and is_last:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(planes, planes, 3, 1, BatchNorm=BatchNorm))\n            rep.append(BatchNorm(planes))\n\n        if not start_with_relu:\n            rep = rep[1:]\n\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x = x + skip\n\n        return x\n\n\nclass AlignedXception(nn.Module):\n    """"""\n    Modified Alighed Xception\n    """"""\n    def __init__(self, output_stride, BatchNorm,\n                 pretrained=True):\n        super(AlignedXception, self).__init__()\n\n        if output_stride == 16:\n            entry_block3_stride = 2\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 2)\n        elif output_stride == 8:\n            entry_block3_stride = 1\n            middle_block_dilation = 2\n            exit_block_dilations = (2, 4)\n        else:\n            raise NotImplementedError\n\n\n        # Entry flow\n        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1, bias=False)\n        self.bn1 = BatchNorm(32)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)\n        self.bn2 = BatchNorm(64)\n\n        self.block1 = Block(64, 128, reps=2, stride=2, BatchNorm=BatchNorm, start_with_relu=False)\n        self.block2 = Block(128, 256, reps=2, stride=2, BatchNorm=BatchNorm, start_with_relu=False,\n                            grow_first=True)\n        self.block3 = Block(256, 728, reps=2, stride=entry_block3_stride, BatchNorm=BatchNorm,\n                            start_with_relu=True, grow_first=True, is_last=True)\n\n        # Middle flow\n        self.block4  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block5  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block6  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block7  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block8  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block9  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block10 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block11 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block12 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block13 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block14 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block15 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block16 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block17 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block18 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n        self.block19 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n\n        # Exit flow\n        self.block20 = Block(728, 1024, reps=2, stride=1, dilation=exit_block_dilations[0],\n                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=False, is_last=True)\n\n        self.conv3 = SeparableConv2d(1024, 1536, 3, stride=1, dilation=exit_block_dilations[1], BatchNorm=BatchNorm)\n        self.bn3 = BatchNorm(1536)\n\n        self.conv4 = SeparableConv2d(1536, 1536, 3, stride=1, dilation=exit_block_dilations[1], BatchNorm=BatchNorm)\n        self.bn4 = BatchNorm(1536)\n\n        self.conv5 = SeparableConv2d(1536, 2048, 3, stride=1, dilation=exit_block_dilations[1], BatchNorm=BatchNorm)\n        self.bn5 = BatchNorm(2048)\n\n        # Init weights\n        self._init_weight()\n\n        # Load pretrained model\n        if pretrained:\n            self._load_pretrained_model()\n\n    def forward(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        # add relu here\n        x = self.relu(x)\n        low_level_feat = x\n        x = self.block2(x)\n        x = self.block3(x)\n\n        # Middle flow\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n        x = self.block13(x)\n        x = self.block14(x)\n        x = self.block15(x)\n        x = self.block16(x)\n        x = self.block17(x)\n        x = self.block18(x)\n        x = self.block19(x)\n\n        # Exit flow\n        x = self.block20(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu(x)\n\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu(x)\n\n        return x, low_level_feat\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, SynchronizedBatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\n    def _load_pretrained_model(self):\n        pretrain_dict = model_zoo.load_url(\'http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth\')\n        model_dict = {}\n        state_dict = self.state_dict()\n\n        for k, v in pretrain_dict.items():\n            if k in state_dict:\n                if \'pointwise\' in k:\n                    v = v.unsqueeze(-1).unsqueeze(-1)\n                if k.startswith(\'block11\'):\n                    model_dict[k] = v\n                    model_dict[k.replace(\'block11\', \'block12\')] = v\n                    model_dict[k.replace(\'block11\', \'block13\')] = v\n                    model_dict[k.replace(\'block11\', \'block14\')] = v\n                    model_dict[k.replace(\'block11\', \'block15\')] = v\n                    model_dict[k.replace(\'block11\', \'block16\')] = v\n                    model_dict[k.replace(\'block11\', \'block17\')] = v\n                    model_dict[k.replace(\'block11\', \'block18\')] = v\n                    model_dict[k.replace(\'block11\', \'block19\')] = v\n                elif k.startswith(\'block12\'):\n                    model_dict[k.replace(\'block12\', \'block20\')] = v\n                elif k.startswith(\'bn3\'):\n                    model_dict[k] = v\n                    model_dict[k.replace(\'bn3\', \'bn4\')] = v\n                elif k.startswith(\'conv4\'):\n                    model_dict[k.replace(\'conv4\', \'conv5\')] = v\n                elif k.startswith(\'bn4\'):\n                    model_dict[k.replace(\'bn4\', \'bn5\')] = v\n                else:\n                    model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\n\n\nif __name__ == ""__main__"":\n    import torch\n    model = AlignedXception(BatchNorm=nn.BatchNorm2d, pretrained=True, output_stride=16)\n    input = torch.rand(1, 3, 512, 512)\n    output, low_level_feat = model(input)\n    print(output.size())\n    print(low_level_feat.size())\n'"
modeling/sync_batchnorm/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# File   : __init__.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nfrom .batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d\nfrom .replicate import DataParallelWithCallback, patch_replication_callback'"
modeling/sync_batchnorm/batchnorm.py,6,"b'# -*- coding: utf-8 -*-\n# File   : batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport collections\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n\nfrom .comm import SyncMaster\n\n__all__ = [\'SynchronizedBatchNorm1d\', \'SynchronizedBatchNorm2d\', \'SynchronizedBatchNorm3d\']\n\n\ndef _sum_ft(tensor):\n    """"""sum over the first and last dimention""""""\n    return tensor.sum(dim=0).sum(dim=-1)\n\n\ndef _unsqueeze_ft(tensor):\n    """"""add new dementions at the front and the tail""""""\n    return tensor.unsqueeze(0).unsqueeze(-1)\n\n\n_ChildMessage = collections.namedtuple(\'_ChildMessage\', [\'sum\', \'ssum\', \'sum_size\'])\n_MasterMessage = collections.namedtuple(\'_MasterMessage\', [\'sum\', \'inv_std\'])\n\n\nclass _SynchronizedBatchNorm(_BatchNorm):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n\n        self._sync_master = SyncMaster(self._data_parallel_master)\n\n        self._is_parallel = False\n        self._parallel_id = None\n        self._slave_pipe = None\n\n    def forward(self, input):\n        # If it is not parallel computation or is in evaluation mode, use PyTorch\'s implementation.\n        if not (self._is_parallel and self.training):\n            return F.batch_norm(\n                input, self.running_mean, self.running_var, self.weight, self.bias,\n                self.training, self.momentum, self.eps)\n\n        # Resize the input to (B, C, -1).\n        input_shape = input.size()\n        input = input.view(input.size(0), self.num_features, -1)\n\n        # Compute the sum and square-sum.\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n        # Reduce-and-broadcast the statistics.\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n        # Compute the output.\n        if self.affine:\n            # MJY:: Fuse the multiplication for speed.\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n        # Reshape it.\n        return output.view(input_shape)\n\n    def __data_parallel_replicate__(self, ctx, copy_id):\n        self._is_parallel = True\n        self._parallel_id = copy_id\n\n        # parallel_id == 0 means master device.\n        if self._parallel_id == 0:\n            ctx.sync_master = self._sync_master\n        else:\n            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n\n    def _data_parallel_master(self, intermediates):\n        """"""Reduce the sum and square-sum, compute the statistics, and broadcast it.""""""\n\n        # Always using same ""device order"" makes the ReduceAdd operation faster.\n        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\n        to_reduce = [i[1][:2] for i in intermediates]\n        to_reduce = [j for i in to_reduce for j in i]  # flatten\n        target_gpus = [i[1].sum.get_device() for i in intermediates]\n\n        sum_size = sum([i[1].sum_size for i in intermediates])\n        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\n        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\n        outputs = []\n        for i, rec in enumerate(intermediates):\n            outputs.append((rec[0], _MasterMessage(*broadcasted[i * 2:i * 2 + 2])))\n\n        return outputs\n\n    def _compute_mean_std(self, sum_, ssum, size):\n        """"""Compute the mean and standard-deviation with sum and square-sum. This method\n        also maintains the moving average on the master device.""""""\n        assert size > 1, \'BatchNorm computes unbiased standard-deviation, which requires size > 1.\'\n        mean = sum_ / size\n        sumvar = ssum - sum_ * mean\n        unbias_var = sumvar / (size - 1)\n        bias_var = sumvar / size\n\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n\n        return mean, bias_var.clamp(self.eps) ** -0.5\n\n\nclass SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n    r""""""Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n    mini-batch.\n    .. math::\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n    standard-deviation are reduced across all devices during training.\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n    During evaluation, this running mean/variance is used for normalization.\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, L)` slices, it\'s common terminology to call this Temporal BatchNorm\n    Args:\n        num_features: num_features from an expected input of size\n            `batch_size x num_features [x width]`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n    Shape:\n        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(\'expected 2D or 3D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 4d input that is seen as a mini-batch\n    of 3d inputs\n    .. math::\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n    standard-deviation are reduced across all devices during training.\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n    During evaluation, this running mean/variance is used for normalization.\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, H, W)` slices, it\'s common terminology to call this Spatial BatchNorm\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\'expected 4D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 5d input that is seen as a mini-batch\n    of 4d inputs\n    .. math::\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n    standard-deviation are reduced across all devices during training.\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n    During evaluation, this running mean/variance is used for normalization.\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, D, H, W)` slices, it\'s common terminology to call this Volumetric BatchNorm\n    or Spatio-temporal BatchNorm\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x depth x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n    Shape:\n        - Input: :math:`(N, C, D, H, W)`\n        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError(\'expected 5D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)'"
modeling/sync_batchnorm/comm.py,0,"b'# -*- coding: utf-8 -*-\n# File   : comm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport queue\nimport collections\nimport threading\n\n__all__ = [\'FutureResult\', \'SlavePipe\', \'SyncMaster\']\n\n\nclass FutureResult(object):\n    """"""A thread-safe future implementation. Used only as one-to-one pipe.""""""\n\n    def __init__(self):\n        self._result = None\n        self._lock = threading.Lock()\n        self._cond = threading.Condition(self._lock)\n\n    def put(self, result):\n        with self._lock:\n            assert self._result is None, \'Previous result has\\\'t been fetched.\'\n            self._result = result\n            self._cond.notify()\n\n    def get(self):\n        with self._lock:\n            if self._result is None:\n                self._cond.wait()\n\n            res = self._result\n            self._result = None\n            return res\n\n\n_MasterRegistry = collections.namedtuple(\'MasterRegistry\', [\'result\'])\n_SlavePipeBase = collections.namedtuple(\'_SlavePipeBase\', [\'identifier\', \'queue\', \'result\'])\n\n\nclass SlavePipe(_SlavePipeBase):\n    """"""Pipe for master-slave communication.""""""\n\n    def run_slave(self, msg):\n        self.queue.put((self.identifier, msg))\n        ret = self.result.get()\n        self.queue.put(True)\n        return ret\n\n\nclass SyncMaster(object):\n    """"""An abstract `SyncMaster` object.\n    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n    and passed to a registered callback.\n    - After receiving the messages, the master device should gather the information and determine to message passed\n    back to each slave devices.\n    """"""\n\n    def __init__(self, master_callback):\n        """"""\n        Args:\n            master_callback: a callback to be invoked after having collected messages from slave devices.\n        """"""\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def __getstate__(self):\n        return {\'master_callback\': self._master_callback}\n\n    def __setstate__(self, state):\n        self.__init__(state[\'master_callback\'])\n\n    def register_slave(self, identifier):\n        """"""\n        Register an slave device.\n        Args:\n            identifier: an identifier, usually is the device id.\n        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n        """"""\n        if self._activated:\n            assert self._queue.empty(), \'Queue is not clean before next initialization.\'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n        """"""\n        Main entry for the master device in each forward pass.\n        The messages were first collected from each devices (including the master device), and then\n        an callback will be invoked to compute the message to be sent back to each devices\n        (including the master device).\n        Args:\n            master_msg: the message that the master want to send to itself. This will be placed as the first\n            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n        Returns: the message to be sent back to the master device.\n        """"""\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, \'The first result should belongs to the master.\'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)\n'"
modeling/sync_batchnorm/replicate.py,1,"b'# -*- coding: utf-8 -*-\n# File   : replicate.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport functools\n\nfrom torch.nn.parallel.data_parallel import DataParallel\n\n__all__ = [\n    \'CallbackContext\',\n    \'execute_replication_callbacks\',\n    \'DataParallelWithCallback\',\n    \'patch_replication_callback\'\n]\n\n\nclass CallbackContext(object):\n    pass\n\n\ndef execute_replication_callbacks(modules):\n    """"""\n    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\n    (shared among multiple copies of this module on different devices).\n    Through this context, different copies can share some information.\n    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n    of any slave copies.\n    """"""\n    master_copy = modules[0]\n    nr_modules = len(list(master_copy.modules()))\n    ctxs = [CallbackContext() for _ in range(nr_modules)]\n\n    for i, module in enumerate(modules):\n        for j, m in enumerate(module.modules()):\n            if hasattr(m, \'__data_parallel_replicate__\'):\n                m.__data_parallel_replicate__(ctxs[j], i)\n\n\nclass DataParallelWithCallback(DataParallel):\n    """"""\n    Data Parallel with a replication callback.\n    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n    original `replicate` function.\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n        # sync_bn.__data_parallel_replicate__ will be invoked.\n    """"""\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n\ndef patch_replication_callback(data_parallel):\n    """"""\n    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n    Useful when you have customized `DataParallel` implementation.\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n        > patch_replication_callback(sync_bn)\n        # this is equivalent to\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n    """"""\n\n    assert isinstance(data_parallel, DataParallel)\n\n    old_replicate = data_parallel.replicate\n\n    @functools.wraps(old_replicate)\n    def new_replicate(module, device_ids):\n        modules = old_replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n    data_parallel.replicate = new_replicate'"
modeling/sync_batchnorm/unittest.py,1,"b""# -*- coding: utf-8 -*-\n# File   : unittest.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport unittest\n\nimport numpy as np\nfrom torch.autograd import Variable\n\n\ndef as_numpy(v):\n    if isinstance(v, Variable):\n        v = v.data\n    return v.cpu().numpy()\n\n\nclass TorchTestCase(unittest.TestCase):\n    def assertTensorClose(self, a, b, atol=1e-3, rtol=1e-3):\n        npa, npb = as_numpy(a), as_numpy(b)\n        self.assertTrue(\n                np.allclose(npa, npb, atol=atol),\n                'Tensor close check failed\\n{}\\n{}\\nadiff={}, rdiff={}'.format(a, b, np.abs(npa - npb).max(), np.abs((npa - npb) / np.fmax(npa, 1e-5)).max())\n        )\n"""
