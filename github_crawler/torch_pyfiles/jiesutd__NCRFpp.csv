file_path,api_count,code
main.py,35,"b'# -*- coding: utf-8 -*-\n# @Author: Jie\n# @Date:   2017-06-15 14:11:08\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2019-02-13 12:41:44\n\nfrom __future__ import print_function\nimport time\nimport sys\nimport argparse\nimport random\nimport torch\nimport gc\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom utils.metric import get_ner_fmeasure\nfrom model.seqlabel import SeqLabel\nfrom model.sentclassifier import SentClassifier\nfrom utils.data import Data\n\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\n\nseed_num = 42\nrandom.seed(seed_num)\ntorch.manual_seed(seed_num)\nnp.random.seed(seed_num)\n\n\ndef data_initialization(data):\n    data.initial_feature_alphabets()\n    data.build_alphabet(data.train_dir)\n    data.build_alphabet(data.dev_dir)\n    data.build_alphabet(data.test_dir)\n    data.fix_alphabet()\n\n\ndef predict_check(pred_variable, gold_variable, mask_variable, sentence_classification=False):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len): pred tag result, in numpy format\n            gold_variable (batch_size, sent_len): gold result variable\n            mask_variable (batch_size, sent_len): mask variable\n    """"""\n    pred = pred_variable.cpu().data.numpy()\n    gold = gold_variable.cpu().data.numpy()\n    mask = mask_variable.cpu().data.numpy()\n    overlaped = (pred == gold)\n    if sentence_classification:\n        # print(overlaped)\n        # print(overlaped*pred)\n        right_token = np.sum(overlaped)\n        total_token = overlaped.shape[0] ## =batch_size\n    else:\n        right_token = np.sum(overlaped * mask)\n        total_token = mask.sum()\n    # print(""right: %s, total: %s""%(right_token, total_token))\n    return right_token, total_token\n\n\ndef recover_label(pred_variable, gold_variable, mask_variable, label_alphabet, word_recover, sentence_classification=False):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len): pred tag result\n            gold_variable (batch_size, sent_len): gold result variable\n            mask_variable (batch_size, sent_len): mask variable\n    """"""\n    pred_variable = pred_variable[word_recover]\n    gold_variable = gold_variable[word_recover]\n    mask_variable = mask_variable[word_recover]\n    batch_size = gold_variable.size(0)\n    if sentence_classification:\n        pred_tag = pred_variable.cpu().data.numpy().tolist()\n        gold_tag = gold_variable.cpu().data.numpy().tolist()\n        pred_label = [label_alphabet.get_instance(pred) for pred in pred_tag]\n        gold_label = [label_alphabet.get_instance(gold) for gold in gold_tag]\n    else:\n        seq_len = gold_variable.size(1)\n        mask = mask_variable.cpu().data.numpy()\n        pred_tag = pred_variable.cpu().data.numpy()\n        gold_tag = gold_variable.cpu().data.numpy()\n        batch_size = mask.shape[0]\n        pred_label = []\n        gold_label = []\n        for idx in range(batch_size):\n            pred = [label_alphabet.get_instance(pred_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n            gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n            assert(len(pred)==len(gold))\n            pred_label.append(pred)\n            gold_label.append(gold)\n    return pred_label, gold_label\n\n\ndef recover_nbest_label(pred_variable, mask_variable, label_alphabet, word_recover):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len, nbest): pred tag result\n            mask_variable (batch_size, sent_len): mask variable\n            word_recover (batch_size)\n        output:\n            nbest_pred_label list: [batch_size, nbest, each_seq_len]\n    """"""\n    # exit(0)\n    pred_variable = pred_variable[word_recover]\n    mask_variable = mask_variable[word_recover]\n    batch_size = pred_variable.size(0)\n    seq_len = pred_variable.size(1)\n    nbest = pred_variable.size(2)\n    mask = mask_variable.cpu().data.numpy()\n    pred_tag = pred_variable.cpu().data.numpy()\n    batch_size = mask.shape[0]\n    pred_label = []\n    for idx in range(batch_size):\n        pred = []\n        for idz in range(nbest):\n            each_pred = [label_alphabet.get_instance(pred_tag[idx][idy][idz]) for idy in range(seq_len) if mask[idx][idy] != 0]\n            pred.append(each_pred)\n        pred_label.append(pred)\n    return pred_label\n\n\ndef lr_decay(optimizer, epoch, decay_rate, init_lr):\n    lr = init_lr/(1+decay_rate*epoch)\n    print("" Learning rate is set as:"", lr)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return optimizer\n\n\n\ndef evaluate(data, model, name, nbest=None):\n    if name == ""train"":\n        instances = data.train_Ids\n    elif name == ""dev"":\n        instances = data.dev_Ids\n    elif name == \'test\':\n        instances = data.test_Ids\n    elif name == \'raw\':\n        instances = data.raw_Ids\n    else:\n        print(""Error: wrong evaluate name,"", name)\n        exit(1)\n    right_token = 0\n    whole_token = 0\n    nbest_pred_results = []\n    pred_scores = []\n    pred_results = []\n    gold_results = []\n    ## set model in eval model\n    model.eval()\n    batch_size = data.HP_batch_size\n    start_time = time.time()\n    train_num = len(instances)\n    total_batch = train_num//batch_size+1\n    for batch_id in range(total_batch):\n        start = batch_id*batch_size\n        end = (batch_id+1)*batch_size\n        if end > train_num:\n            end =  train_num\n        instance = instances[start:end]\n        if not instance:\n            continue\n        batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, False, data.sentence_classification)\n        if nbest and not data.sentence_classification:\n            scores, nbest_tag_seq = model.decode_nbest(batch_word,batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask, nbest)\n            nbest_pred_result = recover_nbest_label(nbest_tag_seq, mask, data.label_alphabet, batch_wordrecover)\n            nbest_pred_results += nbest_pred_result\n            pred_scores += scores[batch_wordrecover].cpu().data.numpy().tolist()\n            ## select the best sequence to evalurate\n            tag_seq = nbest_tag_seq[:,:,0]\n        else:\n            tag_seq = model(batch_word, batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask)\n        # print(""tag:"",tag_seq)\n        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover, data.sentence_classification)\n        pred_results += pred_label\n        gold_results += gold_label\n    decode_time = time.time() - start_time\n    speed = len(instances)/decode_time\n    acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)\n    if nbest and not data.sentence_classification:\n        return speed, acc, p, r, f, nbest_pred_results, pred_scores\n    return speed, acc, p, r, f, pred_results, pred_scores\n\n\ndef batchify_with_label(input_batch_list, gpu, if_train=True, sentence_classification=False):\n    if sentence_classification:\n        return batchify_sentence_classification_with_label(input_batch_list, gpu, if_train)\n    else:\n        return batchify_sequence_labeling_with_label(input_batch_list, gpu, if_train)\n\n\ndef batchify_sequence_labeling_with_label(input_batch_list, gpu, if_train=True):\n    """"""\n        input: list of words, chars and labels, various length. [[words, features, chars, labels],[words, features, chars,labels],...]\n            words: word ids for one sentence. (batch_size, sent_len)\n            features: features ids for one sentence. (batch_size, sent_len, feature_num)\n            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)\n            labels: label ids for one sentence. (batch_size, sent_len)\n\n        output:\n            zero padding for word and char, with their batch length\n            word_seq_tensor: (batch_size, max_sent_len) Variable\n            feature_seq_tensors: [(batch_size, max_sent_len),...] list of Variable\n            word_seq_lengths: (batch_size,1) Tensor\n            char_seq_tensor: (batch_size*max_sent_len, max_word_len) Variable\n            char_seq_lengths: (batch_size*max_sent_len,1) Tensor\n            char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order\n            label_seq_tensor: (batch_size, max_sent_len)\n            mask: (batch_size, max_sent_len)\n    """"""\n    batch_size = len(input_batch_list)\n    words = [sent[0] for sent in input_batch_list]\n    features = [np.asarray(sent[1]) for sent in input_batch_list]\n    feature_num = len(features[0][0])\n    chars = [sent[2] for sent in input_batch_list]\n    labels = [sent[3] for sent in input_batch_list]\n    word_seq_lengths = torch.LongTensor(list(map(len, words)))\n    max_seq_len = word_seq_lengths.max().item()\n    word_seq_tensor = torch.zeros((batch_size, max_seq_len), requires_grad =  if_train).long()\n    label_seq_tensor = torch.zeros((batch_size, max_seq_len), requires_grad =  if_train).long()\n    feature_seq_tensors = []\n    for idx in range(feature_num):\n        feature_seq_tensors.append(torch.zeros((batch_size, max_seq_len),requires_grad =  if_train).long())\n    mask = torch.zeros((batch_size, max_seq_len), requires_grad =  if_train).byte()\n    for idx, (seq, label, seqlen) in enumerate(zip(words, labels, word_seq_lengths)):\n        seqlen = seqlen.item()\n        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n        label_seq_tensor[idx, :seqlen] = torch.LongTensor(label)\n        mask[idx, :seqlen] = torch.Tensor([1]*seqlen)\n        for idy in range(feature_num):\n            feature_seq_tensors[idy][idx,:seqlen] = torch.LongTensor(features[idx][:,idy])\n    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n    word_seq_tensor = word_seq_tensor[word_perm_idx]\n    for idx in range(feature_num):\n        feature_seq_tensors[idx] = feature_seq_tensors[idx][word_perm_idx]\n\n    label_seq_tensor = label_seq_tensor[word_perm_idx]\n    mask = mask[word_perm_idx]\n    ### deal with char\n    # pad_chars (batch_size, max_seq_len)\n    pad_chars = [chars[idx] + [[0]] * (max_seq_len-len(chars[idx])) for idx in range(len(chars))]\n    length_list = [list(map(len, pad_char)) for pad_char in pad_chars]\n    max_word_len = max(map(max, length_list))\n    char_seq_tensor = torch.zeros((batch_size, max_seq_len, max_word_len), requires_grad =  if_train).long()\n    char_seq_lengths = torch.LongTensor(length_list)\n    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n            # print len(word), wordlen\n            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n\n    char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size*max_seq_len,-1)\n    char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size*max_seq_len,)\n    char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)\n    char_seq_tensor = char_seq_tensor[char_perm_idx]\n    _, char_seq_recover = char_perm_idx.sort(0, descending=False)\n    _, word_seq_recover = word_perm_idx.sort(0, descending=False)\n    if gpu:\n        word_seq_tensor = word_seq_tensor.cuda()\n        for idx in range(feature_num):\n            feature_seq_tensors[idx] = feature_seq_tensors[idx].cuda()\n        word_seq_lengths = word_seq_lengths.cuda()\n        word_seq_recover = word_seq_recover.cuda()\n        label_seq_tensor = label_seq_tensor.cuda()\n        char_seq_tensor = char_seq_tensor.cuda()\n        char_seq_recover = char_seq_recover.cuda()\n        mask = mask.cuda()\n    return word_seq_tensor,feature_seq_tensors, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask\n\n\ndef batchify_sentence_classification_with_label(input_batch_list, gpu, if_train=True):\n    """"""\n        input: list of words, chars and labels, various length. [[words, features, chars, labels],[words, features, chars,labels],...]\n            words: word ids for one sentence. (batch_size, sent_len)\n            features: features ids for one sentence. (batch_size, feature_num), each sentence has one set of feature\n            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)\n            labels: label ids for one sentence. (batch_size,), each sentence has one set of feature\n\n        output:\n            zero padding for word and char, with their batch length\n            word_seq_tensor: (batch_size, max_sent_len) Variable\n            feature_seq_tensors: [(batch_size,), ... ] list of Variable\n            word_seq_lengths: (batch_size,1) Tensor\n            char_seq_tensor: (batch_size*max_sent_len, max_word_len) Variable\n            char_seq_lengths: (batch_size*max_sent_len,1) Tensor\n            char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order\n            label_seq_tensor: (batch_size, )\n            mask: (batch_size, max_sent_len)\n    """"""\n\n    batch_size = len(input_batch_list)\n    words = [sent[0] for sent in input_batch_list]\n    features = [np.asarray(sent[1]) for sent in input_batch_list]    \n    feature_num = len(features[0])\n    chars = [sent[2] for sent in input_batch_list]\n    labels = [sent[3] for sent in input_batch_list]\n    word_seq_lengths = torch.LongTensor(list(map(len, words)))\n    max_seq_len = word_seq_lengths.max().item()\n    word_seq_tensor = torch.zeros((batch_size, max_seq_len), requires_grad =  if_train).long()\n    label_seq_tensor = torch.zeros((batch_size, ), requires_grad =  if_train).long()\n    feature_seq_tensors = []\n    for idx in range(feature_num):\n        feature_seq_tensors.append(torch.zeros((batch_size, max_seq_len),requires_grad =  if_train).long())\n    mask = torch.zeros((batch_size, max_seq_len), requires_grad =  if_train).byte()\n    label_seq_tensor = torch.LongTensor(labels)\n    # exit(0)\n    for idx, (seq,  seqlen) in enumerate(zip(words,  word_seq_lengths)):\n        seqlen = seqlen.item()\n        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n        mask[idx, :seqlen] = torch.Tensor([1]*seqlen)\n        for idy in range(feature_num):\n            feature_seq_tensors[idy][idx,:seqlen] = torch.LongTensor(features[idx][:,idy])\n    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n    word_seq_tensor = word_seq_tensor[word_perm_idx]\n    for idx in range(feature_num):\n        feature_seq_tensors[idx] = feature_seq_tensors[idx][word_perm_idx]\n    label_seq_tensor = label_seq_tensor[word_perm_idx]\n    mask = mask[word_perm_idx]\n    ### deal with char\n    # pad_chars (batch_size, max_seq_len)\n    pad_chars = [chars[idx] + [[0]] * (max_seq_len-len(chars[idx])) for idx in range(len(chars))]\n    length_list = [list(map(len, pad_char)) for pad_char in pad_chars]\n    max_word_len = max(map(max, length_list))\n    char_seq_tensor = torch.zeros((batch_size, max_seq_len, max_word_len), requires_grad =  if_train).long()\n    char_seq_lengths = torch.LongTensor(length_list)\n    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n            # print len(word), wordlen\n            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n\n    char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size*max_seq_len,-1)\n    char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size*max_seq_len,)\n    char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)\n    char_seq_tensor = char_seq_tensor[char_perm_idx]\n    _, char_seq_recover = char_perm_idx.sort(0, descending=False)\n    _, word_seq_recover = word_perm_idx.sort(0, descending=False)\n    if gpu:\n        word_seq_tensor = word_seq_tensor.cuda()\n        for idx in range(feature_num):\n            feature_seq_tensors[idx] = feature_seq_tensors[idx].cuda()\n        word_seq_lengths = word_seq_lengths.cuda()\n        word_seq_recover = word_seq_recover.cuda()\n        label_seq_tensor = label_seq_tensor.cuda()\n        char_seq_tensor = char_seq_tensor.cuda()\n        char_seq_recover = char_seq_recover.cuda()\n        mask = mask.cuda()\n    return word_seq_tensor,feature_seq_tensors, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask\n\n\n\n\ndef train(data):\n    print(""Training model..."")\n    data.show_data_summary()\n    save_data_name = data.model_dir +"".dset""\n    data.save(save_data_name)\n    if data.sentence_classification:\n        model = SentClassifier(data)\n    else:\n        model = SeqLabel(data)\n\n    if data.optimizer.lower() == ""sgd"":\n        optimizer = optim.SGD(model.parameters(), lr=data.HP_lr, momentum=data.HP_momentum,weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adagrad"":\n        optimizer = optim.Adagrad(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adadelta"":\n        optimizer = optim.Adadelta(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""rmsprop"":\n        optimizer = optim.RMSprop(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adam"":\n        optimizer = optim.Adam(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    else:\n        print(""Optimizer illegal: %s""%(data.optimizer))\n        exit(1)\n    best_dev = -10\n    # data.HP_iteration = 1\n    ## start training\n    for idx in range(data.HP_iteration):\n        epoch_start = time.time()\n        temp_start = epoch_start\n        print(""Epoch: %s/%s"" %(idx,data.HP_iteration))\n        if data.optimizer == ""SGD"":\n            optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)\n        instance_count = 0\n        sample_id = 0\n        sample_loss = 0\n        total_loss = 0\n        right_token = 0\n        whole_token = 0\n        random.shuffle(data.train_Ids)\n        print(""Shuffle: first input word list:"", data.train_Ids[0][0])\n        ## set model in train model\n        model.train()\n        model.zero_grad()\n        batch_size = data.HP_batch_size\n        batch_id = 0\n        train_num = len(data.train_Ids)\n        total_batch = train_num//batch_size+1\n        for batch_id in range(total_batch):\n            start = batch_id*batch_size\n            end = (batch_id+1)*batch_size\n            if end >train_num:\n                end = train_num\n            instance = data.train_Ids[start:end]\n            if not instance:\n                continue\n            batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, True, data.sentence_classification)\n            instance_count += 1\n            loss, tag_seq = model.calculate_loss(batch_word, batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_label, mask)\n            right, whole = predict_check(tag_seq, batch_label, mask, data.sentence_classification)\n            right_token += right\n            whole_token += whole\n            # print(""loss:"",loss.item())\n            sample_loss += loss.item()\n            total_loss += loss.item()\n            if end%500 == 0:\n                temp_time = time.time()\n                temp_cost = temp_time - temp_start\n                temp_start = temp_time\n                print(""     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f""%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))\n                if sample_loss > 1e8 or str(sample_loss) == ""nan"":\n                    print(""ERROR: LOSS EXPLOSION (>1e8) ! PLEASE SET PROPER PARAMETERS AND STRUCTURE! EXIT...."")\n                    exit(1)\n                sys.stdout.flush()\n                sample_loss = 0\n            loss.backward()\n            optimizer.step()\n            model.zero_grad()\n        temp_time = time.time()\n        temp_cost = temp_time - temp_start\n        print(""     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f""%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))\n\n        epoch_finish = time.time()\n        epoch_cost = epoch_finish - epoch_start\n        print(""Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s""%(idx, epoch_cost, train_num/epoch_cost, total_loss))\n        print(""totalloss:"", total_loss)\n        if total_loss > 1e8 or str(total_loss) == ""nan"":\n            print(""ERROR: LOSS EXPLOSION (>1e8) ! PLEASE SET PROPER PARAMETERS AND STRUCTURE! EXIT...."")\n            exit(1)\n        # continue\n        speed, acc, p, r, f, _,_ = evaluate(data, model, ""dev"")\n        dev_finish = time.time()\n        dev_cost = dev_finish - epoch_finish\n\n        if data.seg:\n            current_score = f\n            print(""Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f""%(dev_cost, speed, acc, p, r, f))\n        else:\n            current_score = acc\n            print(""Dev: time: %.2fs speed: %.2fst/s; acc: %.4f""%(dev_cost, speed, acc))\n\n        if current_score > best_dev:\n            if data.seg:\n                print(""Exceed previous best f score:"", best_dev)\n            else:\n                print(""Exceed previous best acc score:"", best_dev)\n            model_name = data.model_dir +\'.\'+ str(idx) + "".model""\n            print(""Save current best model in file:"", model_name)\n            torch.save(model.state_dict(), model_name)\n            best_dev = current_score\n        # ## decode test\n        speed, acc, p, r, f, _,_ = evaluate(data, model, ""test"")\n        test_finish = time.time()\n        test_cost = test_finish - dev_finish\n        if data.seg:\n            print(""Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f""%(test_cost, speed, acc, p, r, f))\n        else:\n            print(""Test: time: %.2fs, speed: %.2fst/s; acc: %.4f""%(test_cost, speed, acc))\n        gc.collect()\n\n\ndef load_model_decode(data, name):\n    print(""Load Model from file: "", data.model_dir)\n    if data.sentence_classification:\n        model = SentClassifier(data)\n    else:\n        model = SeqLabel(data)\n    # model = SeqModel(data)\n    ## load model need consider if the model trained in GPU and load in CPU, or vice versa\n    # if not gpu:\n    #     model.load_state_dict(torch.load(model_dir))\n    #     # model.load_state_dict(torch.load(model_dir), map_location=lambda storage, loc: storage)\n    #     # model = torch.load(model_dir, map_location=lambda storage, loc: storage)\n    # else:\n    #     model.load_state_dict(torch.load(model_dir))\n    #     # model = torch.load(model_dir)\n    model.load_state_dict(torch.load(data.load_model_dir))\n\n    print(""Decode %s data, nbest: %s ...""%(name, data.nbest))\n    start_time = time.time()\n    speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, name, data.nbest)\n    end_time = time.time()\n    time_cost = end_time - start_time\n    if data.seg:\n        print(""%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f""%(name, time_cost, speed, acc, p, r, f))\n    else:\n        print(""%s: time:%.2fs, speed:%.2fst/s; acc: %.4f""%(name, time_cost, speed, acc))\n    return pred_results, pred_scores\n\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Tuning with NCRF++\')\n    # parser.add_argument(\'--status\', choices=[\'train\', \'decode\'], help=\'update algorithm\', default=\'train\')\n    parser.add_argument(\'--config\',  help=\'Configuration File\', default=\'None\')\n    parser.add_argument(\'--wordemb\',  help=\'Embedding for words\', default=\'None\')\n    parser.add_argument(\'--charemb\',  help=\'Embedding for chars\', default=\'None\')\n    parser.add_argument(\'--status\', choices=[\'train\', \'decode\'], help=\'update algorithm\', default=\'train\')\n    parser.add_argument(\'--savemodel\', default=""data/model/saved_model.lstmcrf."")\n    parser.add_argument(\'--savedset\', help=\'Dir of saved data setting\')\n    parser.add_argument(\'--train\', default=""data/conll03/train.bmes"") \n    parser.add_argument(\'--dev\', default=""data/conll03/dev.bmes"" )  \n    parser.add_argument(\'--test\', default=""data/conll03/test.bmes"") \n    parser.add_argument(\'--seg\', default=""True"") \n    parser.add_argument(\'--raw\') \n    parser.add_argument(\'--loadmodel\')\n    parser.add_argument(\'--output\') \n\n    args = parser.parse_args()\n    data = Data()\n    data.HP_gpu = torch.cuda.is_available()\n    if args.config == \'None\':\n        data.train_dir = args.train \n        data.dev_dir = args.dev \n        data.test_dir = args.test\n        data.model_dir = args.savemodel\n        data.dset_dir = args.savedset\n        print(""Save dset directory:"",data.dset_dir)\n        save_model_dir = args.savemodel\n        data.word_emb_dir = args.wordemb\n        data.char_emb_dir = args.charemb\n        if args.seg.lower() == \'true\':\n            data.seg = True\n        else:\n            data.seg = False\n        print(""Seed num:"",seed_num)\n    else:\n        data.read_config(args.config)\n    # data.show_data_summary()\n    status = data.status.lower()\n    print(""Seed num:"",seed_num)\n\n    if status == \'train\':\n        print(""MODEL: train"")\n        data_initialization(data)\n        data.generate_instance(\'train\')\n        data.generate_instance(\'dev\')\n        data.generate_instance(\'test\')\n        data.build_pretrain_emb()\n        train(data)\n    elif status == \'decode\':\n        print(""MODEL: decode"")\n        data.load(data.dset_dir)\n        data.read_config(args.config)\n        print(data.raw_dir)\n        # exit(0)\n        data.show_data_summary()\n        data.generate_instance(\'raw\')\n        print(""nbest: %s""%(data.nbest))\n        decode_results, pred_scores = load_model_decode(data, \'raw\')\n        if data.nbest and not data.sentence_classification:\n            data.write_nbest_decoded_results(decode_results, pred_scores, \'raw\')\n        else:\n            data.write_decoded_results(decode_results, \'raw\')\n    else:\n        print(""Invalid argument! Please use valid arguments! (train/test/decode)"")\n\n'"
main_parse.py,25,"b'# -*- coding: utf-8 -*-\n# @Author: Jie\n# @Date:   2017-06-15 14:11:08\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2019-02-13 10:58:43\n\nfrom __future__ import print_function\nimport time\nimport sys\nimport argparse\nimport random\nimport copy\nimport torch\nimport gc\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nfrom utils.metric import get_ner_fmeasure\nfrom model.seqlabel import SeqLabel\nfrom utils.data import Data\n\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle as pickle\n\nseed_num = 42\nrandom.seed(seed_num)\ntorch.manual_seed(seed_num)\nnp.random.seed(seed_num)\n\n\ndef data_initialization(data):\n    data.initial_feature_alphabets()\n    data.build_alphabet(data.train_dir)\n    data.build_alphabet(data.dev_dir)\n    data.build_alphabet(data.test_dir)\n    data.fix_alphabet()\n\n\ndef predict_check(pred_variable, gold_variable, mask_variable):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len): pred tag result, in numpy format\n            gold_variable (batch_size, sent_len): gold result variable\n            mask_variable (batch_size, sent_len): mask variable\n    """"""\n    pred = pred_variable.cpu().data.numpy()\n    gold = gold_variable.cpu().data.numpy()\n    mask = mask_variable.cpu().data.numpy()\n    overlaped = (pred == gold)\n    right_token = np.sum(overlaped * mask)\n    total_token = mask.sum()\n    # print(""right: %s, total: %s""%(right_token, total_token))\n    return right_token, total_token\n\n\ndef recover_label(pred_variable, gold_variable, mask_variable, label_alphabet, word_recover):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len): pred tag result\n            gold_variable (batch_size, sent_len): gold result variable\n            mask_variable (batch_size, sent_len): mask variable\n    """"""\n    \n    pred_variable = pred_variable[word_recover]\n    gold_variable = gold_variable[word_recover]\n    mask_variable = mask_variable[word_recover]\n    batch_size = gold_variable.size(0)\n    seq_len = gold_variable.size(1)\n    mask = mask_variable.cpu().data.numpy()\n    pred_tag = pred_variable.cpu().data.numpy()\n    gold_tag = gold_variable.cpu().data.numpy()\n    batch_size = mask.shape[0]\n    pred_label = []\n    gold_label = []\n    for idx in range(batch_size):\n        pred = [label_alphabet.get_instance(pred_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n        gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n        # print(""p:"",pred, pred_tag.tolist())\n        # print(""g:"", gold, gold_tag.tolist())\n        assert(len(pred)==len(gold))\n        pred_label.append(pred)\n        gold_label.append(gold)\n    return pred_label, gold_label\n\n\ndef recover_nbest_label(pred_variable, mask_variable, label_alphabet, word_recover):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len, nbest): pred tag result\n            mask_variable (batch_size, sent_len): mask variable\n            word_recover (batch_size)\n        output:\n            nbest_pred_label list: [batch_size, nbest, each_seq_len]\n    """"""\n    # print(""word recover:"", word_recover.size())\n    # exit(0)\n    pred_variable = pred_variable[word_recover]\n    mask_variable = mask_variable[word_recover]\n    batch_size = pred_variable.size(0)\n    seq_len = pred_variable.size(1)\n    print(pred_variable.size())\n    nbest = pred_variable.size(2)\n    mask = mask_variable.cpu().data.numpy()\n    pred_tag = pred_variable.cpu().data.numpy()\n    batch_size = mask.shape[0]\n    pred_label = []\n    for idx in range(batch_size):\n        pred = []\n        for idz in range(nbest):\n            each_pred = [label_alphabet.get_instance(pred_tag[idx][idy][idz]) for idy in range(seq_len) if mask[idx][idy] != 0]\n            pred.append(each_pred)\n        pred_label.append(pred)\n    return pred_label\n\n\n\n# def save_data_setting(data, save_file):\n#     new_data = copy.deepcopy(data)\n#     ## remove input instances\n#     new_data.train_texts = []\n#     new_data.dev_texts = []\n#     new_data.test_texts = []\n#     new_data.raw_texts = []\n\n#     new_data.train_Ids = []\n#     new_data.dev_Ids = []\n#     new_data.test_Ids = []\n#     new_data.raw_Ids = []\n#     ## save data settings\n#     with open(save_file, \'w\') as fp:\n#         pickle.dump(new_data, fp)\n#     print(""Data setting saved to file: "", save_file)\n\n\n# def load_data_setting(save_file):\n#     with open(save_file, \'r\') as fp:\n#         data = pickle.load(fp)\n#     print(""Data setting loaded from file: "", save_file)\n#     data.show_data_summary()\n#     return data\n\ndef lr_decay(optimizer, epoch, decay_rate, init_lr):\n    lr = init_lr/(1+decay_rate*epoch)\n    print("" Learning rate is set as:"", lr)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return optimizer\n\n\n\ndef evaluate(data, model, name, nbest=None):\n    if name == ""train"":\n        instances = data.train_Ids\n    elif name == ""dev"":\n        instances = data.dev_Ids\n    elif name == \'test\':\n        instances = data.test_Ids\n    elif name == \'raw\':\n        instances = data.raw_Ids\n    else:\n        print(""Error: wrong evaluate name,"", name)\n    right_token = 0\n    whole_token = 0\n    nbest_pred_results = []\n    pred_scores = []\n    pred_results = []\n    gold_results = []\n    ## set model in eval model\n    model.eval()\n    batch_size = data.HP_batch_size\n    start_time = time.time()\n    train_num = len(instances)\n    total_batch = train_num//batch_size+1\n    for batch_id in range(total_batch):\n        start = batch_id*batch_size\n        end = (batch_id+1)*batch_size \n        if end > train_num:\n            end =  train_num\n        instance = instances[start:end]\n        if not instance:\n            continue\n        batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu, True)\n        if nbest:\n            scores, nbest_tag_seq = model.decode_nbest(batch_word,batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask, nbest)\n            nbest_pred_result = recover_nbest_label(nbest_tag_seq, mask, data.label_alphabet, batch_wordrecover)\n            nbest_pred_results += nbest_pred_result \n            pred_scores += scores[batch_wordrecover].cpu().data.numpy().tolist()\n            ## select the best sequence to evalurate\n            tag_seq = nbest_tag_seq[:,:,0]\n        else:\n            tag_seq = model(batch_word, batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask)\n        # print(""tag:"",tag_seq)\n        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover)\n        pred_results += pred_label\n        gold_results += gold_label\n    decode_time = time.time() - start_time\n    speed = len(instances)/decode_time\n    acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)\n    if nbest:\n        return speed, acc, p, r, f, nbest_pred_results, pred_scores\n    return speed, acc, p, r, f, pred_results, pred_scores\n\n\ndef batchify_with_label(input_batch_list, gpu, volatile_flag=False):\n    """"""\n        input: list of words, chars and labels, various length. [[words,chars, labels],[words,chars,labels],...]\n            words: word ids for one sentence. (batch_size, sent_len) \n            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)\n        output:\n            zero padding for word and char, with their batch length\n            word_seq_tensor: (batch_size, max_sent_len) Variable\n            word_seq_lengths: (batch_size,1) Tensor\n            char_seq_tensor: (batch_size*max_sent_len, max_word_len) Variable\n            char_seq_lengths: (batch_size*max_sent_len,1) Tensor\n            char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order \n            label_seq_tensor: (batch_size, max_sent_len)\n            mask: (batch_size, max_sent_len) \n    """"""\n    batch_size = len(input_batch_list)\n    words = [sent[0] for sent in input_batch_list]\n    features = [np.asarray(sent[1]) for sent in input_batch_list]\n    feature_num = len(features[0][0])\n    chars = [sent[2] for sent in input_batch_list]\n    labels = [sent[3] for sent in input_batch_list]\n    word_seq_lengths = torch.LongTensor(map(len, words))\n    max_seq_len = word_seq_lengths.max()\n    word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile =  volatile_flag).long()\n    label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).long()\n    feature_seq_tensors = []\n    for idx in range(feature_num):\n        feature_seq_tensors.append(autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).long())\n    mask = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).byte()\n    for idx, (seq, label, seqlen) in enumerate(zip(words, labels, word_seq_lengths)):\n        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n        label_seq_tensor[idx, :seqlen] = torch.LongTensor(label)\n        mask[idx, :seqlen] = torch.Tensor([1]*seqlen)\n        for idy in range(feature_num):\n            feature_seq_tensors[idy][idx,:seqlen] = torch.LongTensor(features[idx][:,idy])\n    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n    word_seq_tensor = word_seq_tensor[word_perm_idx]\n    for idx in range(feature_num):\n        feature_seq_tensors[idx] = feature_seq_tensors[idx][word_perm_idx]\n\n    label_seq_tensor = label_seq_tensor[word_perm_idx]\n    mask = mask[word_perm_idx]\n    ### deal with char\n    # pad_chars (batch_size, max_seq_len)\n    pad_chars = [chars[idx] + [[0]] * (max_seq_len-len(chars[idx])) for idx in range(len(chars))]\n    length_list = [map(len, pad_char) for pad_char in pad_chars]\n    max_word_len = max(map(max, length_list))\n    char_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len, max_word_len)), volatile =  volatile_flag).long()\n    char_seq_lengths = torch.LongTensor(length_list)\n    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n            # print len(word), wordlen\n            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n    \n    char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size*max_seq_len,-1)\n    char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size*max_seq_len,)\n    char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)\n    char_seq_tensor = char_seq_tensor[char_perm_idx]\n    _, char_seq_recover = char_perm_idx.sort(0, descending=False)\n    _, word_seq_recover = word_perm_idx.sort(0, descending=False)\n    if gpu:\n        word_seq_tensor = word_seq_tensor.cuda()\n        for idx in range(feature_num):\n            feature_seq_tensors[idx] = feature_seq_tensors[idx].cuda()\n        word_seq_lengths = word_seq_lengths.cuda()\n        word_seq_recover = word_seq_recover.cuda()\n        label_seq_tensor = label_seq_tensor.cuda()\n        char_seq_tensor = char_seq_tensor.cuda()\n        char_seq_recover = char_seq_recover.cuda()\n        mask = mask.cuda()\n    return word_seq_tensor,feature_seq_tensors, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask\n\n\ndef train(data):\n    print(""Training model..."")\n    data.show_data_summary()\n    save_data_name = data.model_dir +"".dset""\n    data.save(save_data_name)\n    model = SeqLabel(data)\n    loss_function = nn.NLLLoss()\n    if data.optimizer.lower() == ""sgd"":\n        optimizer = optim.SGD(model.parameters(), lr=data.HP_lr, momentum=data.HP_momentum,weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adagrad"":\n        optimizer = optim.Adagrad(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adadelta"":\n        optimizer = optim.Adadelta(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""rmsprop"":\n        optimizer = optim.RMSprop(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adam"":\n        optimizer = optim.Adam(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    else:\n        print(""Optimizer illegal: %s""%(data.optimizer))\n        exit(0)\n    best_dev = -10\n    # data.HP_iteration = 1\n    ## start training\n    for idx in range(data.HP_iteration):\n        epoch_start = time.time()\n        temp_start = epoch_start\n        print(""Epoch: %s/%s"" %(idx,data.HP_iteration))\n        if data.optimizer == ""SGD"":\n            optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)\n        instance_count = 0\n        sample_id = 0\n        sample_loss = 0\n        total_loss = 0\n        right_token = 0\n        whole_token = 0\n        random.shuffle(data.train_Ids)\n        ## set model in train model\n        model.train()\n        model.zero_grad()\n        batch_size = data.HP_batch_size\n        batch_id = 0\n        train_num = len(data.train_Ids)\n        total_batch = train_num//batch_size+1\n        for batch_id in range(total_batch):\n            start = batch_id*batch_size\n            end = (batch_id+1)*batch_size \n            if end >train_num:\n                end = train_num\n            instance = data.train_Ids[start:end]\n            if not instance:\n                continue\n            batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu)\n            instance_count += 1\n            loss, tag_seq = model.neg_log_likelihood_loss(batch_word,batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_label, mask)\n            right, whole = predict_check(tag_seq, batch_label, mask)\n            right_token += right\n            whole_token += whole\n            sample_loss += loss.data[0]\n            total_loss += loss.data[0]\n            if end%500 == 0:\n                temp_time = time.time()\n                temp_cost = temp_time - temp_start\n                temp_start = temp_time\n                print(""     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f""%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))\n                sys.stdout.flush()\n                sample_loss = 0\n            loss.backward()\n            optimizer.step()\n            model.zero_grad()\n        temp_time = time.time()\n        temp_cost = temp_time - temp_start\n        print(""     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f""%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))       \n        epoch_finish = time.time()\n        epoch_cost = epoch_finish - epoch_start\n        print(""Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s""%(idx, epoch_cost, train_num/epoch_cost, total_loss))\n        # continue\n        speed, acc, p, r, f, _,_ = evaluate(data, model, ""dev"")\n        dev_finish = time.time()\n        dev_cost = dev_finish - epoch_finish\n\n        if data.seg:\n            current_score = f\n            print(""Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f""%(dev_cost, speed, acc, p, r, f))\n        else:\n            current_score = acc\n            print(""Dev: time: %.2fs speed: %.2fst/s; acc: %.4f""%(dev_cost, speed, acc))\n\n        if current_score > best_dev:\n            if data.seg:\n                print(""Exceed previous best f score:"", best_dev)\n            else:\n                print(""Exceed previous best acc score:"", best_dev)\n            model_name = data.model_dir +\'.\'+ str(idx) + "".model""\n            print(""Save current best model in file:"", model_name)\n            torch.save(model.state_dict(), model_name)\n            best_dev = current_score \n        # ## decode test\n        speed, acc, p, r, f, _,_ = evaluate(data, model, ""test"")\n        test_finish = time.time()\n        test_cost = test_finish - dev_finish\n        if data.seg:\n            print(""Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f""%(test_cost, speed, acc, p, r, f))\n        else:\n            print(""Test: time: %.2fs, speed: %.2fst/s; acc: %.4f""%(test_cost, speed, acc))\n        gc.collect() \n\n\ndef load_model_decode(data, name):\n    print(""Load Model from file: "", data.model_dir)\n    model = SeqLabel(data)\n    ## load model need consider if the model trained in GPU and load in CPU, or vice versa\n    # if not gpu:\n    #     model.load_state_dict(torch.load(model_dir))\n    #     # model.load_state_dict(torch.load(model_dir), map_location=lambda storage, loc: storage)\n    #     # model = torch.load(model_dir, map_location=lambda storage, loc: storage)\n    # else:\n    #     model.load_state_dict(torch.load(model_dir))\n    #     # model = torch.load(model_dir)\n    model.load_state_dict(torch.load(data.load_model_dir))\n\n    print(""Decode %s data, nbest: %s ...""%(name, data.nbest))\n    start_time = time.time()\n    speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, name, data.nbest)\n    end_time = time.time()\n    time_cost = end_time - start_time\n    if data.seg:\n        print(""%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f""%(name, time_cost, speed, acc, p, r, f))\n    else:\n        print(""%s: time:%.2fs, speed:%.2fst/s; acc: %.4f""%(name, time_cost, speed, acc))\n    return pred_results, pred_scores\n\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Tuning with NCRF++\')\n    parser.add_argument(\'--wordemb\',  help=\'Embedding for words\', default=\'None\')\n    parser.add_argument(\'--charemb\',  help=\'Embedding for chars\', default=\'None\')\n    parser.add_argument(\'--status\', choices=[\'train\', \'decode\'], help=\'update algorithm\', default=\'train\')\n    parser.add_argument(\'--savemodel\', default=""data/model/saved_model.lstmcrf."")\n    parser.add_argument(\'--savedset\', help=\'Dir of saved data setting\')\n    parser.add_argument(\'--train\', default=""data/conll03/train.bmes"") \n    parser.add_argument(\'--dev\', default=""data/conll03/dev.bmes"" )  \n    parser.add_argument(\'--test\', default=""data/conll03/test.bmes"") \n    parser.add_argument(\'--seg\', default=""True"") \n    parser.add_argument(\'--raw\') \n    parser.add_argument(\'--loadmodel\')\n    parser.add_argument(\'--output\') \n    args = parser.parse_args()\n    data = Data()\n    \n    data.train_dir = args.train \n    data.dev_dir = args.dev \n    data.test_dir = args.test\n    data.model_dir = args.savemodel\n    data.dset_dir = args.savedset\n    print(""dset directory:"",data.dset_dir)\n    status = args.status.lower()\n    save_model_dir = args.savemodel\n    data.HP_gpu = torch.cuda.is_available()\n    print(""Seed num:"",seed_num)\n    data.number_normalized = True\n    data.word_emb_dir = ""../data/glove.6B.100d.txt""\n    \n    if status == \'train\':\n        print(""MODEL: train"")\n        data_initialization(data)\n        data.use_char = True\n        data.HP_batch_size = 10\n        data.HP_lr = 0.015\n        data.char_seq_feature = ""CNN""\n        data.generate_instance(\'train\')\n        data.generate_instance(\'dev\')\n        data.generate_instance(\'test\')\n        data.build_pretrain_emb()\n        train(data)\n    elif status == \'decode\':   \n        print(""MODEL: decode"")\n        data.load(data.dset_dir)    \n        data.raw_dir = args.raw\n        data.decode_dir = args.output\n        data.load_model_dir = args.loadmodel\n        data.show_data_summary()\n        data.generate_instance(\'raw\')\n        print(""nbest: %s""%(data.nbest))\n        decode_results, pred_scores = load_model_decode(data, \'raw\')\n        if data.nbest:\n            data.write_nbest_decoded_results(decode_results, pred_scores, \'raw\')\n        else:\n            data.write_decoded_results(decode_results, \'raw\')\n    else:\n        print(""Invalid argument! Please use valid arguments! (train/test/decode)"")\n\n\n\n\n'"
model/__init__.py,0,"b""__author__ = 'max'\n"""
model/charbigru.py,4,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-10-17 16:47:32\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-10-18 11:12:13\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport numpy as np\n\nclass CharBiGRU(nn.Module):\n    def __init__(self, alphabet_size, pretrain_char_embedding, embedding_dim, hidden_dim, dropout, gpu, bidirect_flag = True):\n        super(CharBiGRU, self).__init__()\n        print(""build char sequence feature extractor: GRU ..."")\n        self.gpu = gpu\n        self.hidden_dim = hidden_dim\n        if bidirect_flag:\n            self.hidden_dim = hidden_dim // 2\n        self.char_drop = nn.Dropout(dropout)\n        self.char_embeddings = nn.Embedding(alphabet_size, embedding_dim)\n        if pretrain_char_embedding is not None:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(pretrain_char_embedding))\n        else:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(self.random_embedding(alphabet_size, embedding_dim)))\n        self.char_lstm = nn.GRU(embedding_dim, self.hidden_dim, num_layers=1, batch_first=True, bidirectional=bidirect_flag)\n        if self.gpu:\n            self.char_drop = self.char_drop.cuda()\n            self.char_embeddings = self.char_embeddings.cuda()\n            self.char_lstm = self.char_lstm.cuda()\n\n\n    def random_embedding(self, vocab_size, embedding_dim):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        scale = np.sqrt(3.0 / embedding_dim)\n        for index in range(vocab_size):\n            pretrain_emb[index,:] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        return pretrain_emb\n\n\n    def get_last_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size, word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_hidden = None\n        pack_input = pack_padded_sequence(char_embeds, seq_lengths, True)\n        char_rnn_out, char_hidden = self.char_lstm(pack_input, char_hidden)\n        # char_rnn_out, _ = pad_packed_sequence(char_rnn_out)\n        return char_hidden.transpose(1,0).contiguous().view(batch_size,-1)\n\n    def get_all_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size,  word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, word_length, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_hidden = None\n        pack_input = pack_padded_sequence(char_embeds, seq_lengths, True)\n        char_rnn_out, char_hidden = self.char_lstm(pack_input, char_hidden)\n        char_rnn_out, _ = pad_packed_sequence(char_rnn_out)\n        return char_rnn_out.transpose(1,0)\n\n\n    def forward(self, input, seq_lengths):\n        return self.get_all_hiddens(input, seq_lengths)\n'"
model/charbilstm.py,4,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-10-17 16:47:32\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-10-18 11:19:37\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport numpy as np\n\nclass CharBiLSTM(nn.Module):\n    def __init__(self, alphabet_size, pretrain_char_embedding, embedding_dim, hidden_dim, dropout, gpu, bidirect_flag = True):\n        super(CharBiLSTM, self).__init__()\n        print(""build char sequence feature extractor: LSTM ..."")\n        self.gpu = gpu\n        self.hidden_dim = hidden_dim\n        if bidirect_flag:\n            self.hidden_dim = hidden_dim // 2\n        self.char_drop = nn.Dropout(dropout)\n        self.char_embeddings = nn.Embedding(alphabet_size, embedding_dim)\n        if pretrain_char_embedding is not None:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(pretrain_char_embedding))\n        else:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(self.random_embedding(alphabet_size, embedding_dim)))\n        self.char_lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=1, batch_first=True, bidirectional=bidirect_flag)\n        if self.gpu:\n            self.char_drop = self.char_drop.cuda()\n            self.char_embeddings = self.char_embeddings.cuda()\n            self.char_lstm = self.char_lstm.cuda()\n\n\n    def random_embedding(self, vocab_size, embedding_dim):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        scale = np.sqrt(3.0 / embedding_dim)\n        for index in range(vocab_size):\n            pretrain_emb[index,:] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        return pretrain_emb\n\n\n    def get_last_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size, word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_hidden = None\n        pack_input = pack_padded_sequence(char_embeds, seq_lengths, True)\n        char_rnn_out, char_hidden = self.char_lstm(pack_input, char_hidden)\n        ## char_hidden = (h_t, c_t)\n        #  char_hidden[0] = h_t = (2, batch_size, lstm_dimension)\n        # char_rnn_out, _ = pad_packed_sequence(char_rnn_out)\n        return char_hidden[0].transpose(1,0).contiguous().view(batch_size,-1)\n\n    def get_all_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size,  word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, word_length, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_hidden = None\n        pack_input = pack_padded_sequence(char_embeds, seq_lengths, True)\n        char_rnn_out, char_hidden = self.char_lstm(pack_input, char_hidden)\n        char_rnn_out, _ = pad_packed_sequence(char_rnn_out)\n        return char_rnn_out.transpose(1,0)\n\n\n    def forward(self, input, seq_lengths):\n        return self.get_all_hiddens(input, seq_lengths)\n'"
model/charcnn.py,4,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-10-17 16:47:32\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2019-01-18 21:06:06\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass CharCNN(nn.Module):\n    def __init__(self, alphabet_size, pretrain_char_embedding, embedding_dim, hidden_dim, dropout, gpu):\n        super(CharCNN, self).__init__()\n        print(""build char sequence feature extractor: CNN ..."")\n        self.gpu = gpu\n        self.hidden_dim = hidden_dim\n        self.char_drop = nn.Dropout(dropout)\n        self.char_embeddings = nn.Embedding(alphabet_size, embedding_dim)\n        if pretrain_char_embedding is not None:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(pretrain_char_embedding))\n        else:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(self.random_embedding(alphabet_size, embedding_dim)))\n        self.char_cnn = nn.Conv1d(embedding_dim, self.hidden_dim, kernel_size=3, padding=1)\n        if self.gpu:\n            self.char_drop = self.char_drop.cuda()\n            self.char_embeddings = self.char_embeddings.cuda()\n            self.char_cnn = self.char_cnn.cuda()\n\n\n    def random_embedding(self, vocab_size, embedding_dim):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        scale = np.sqrt(3.0 / embedding_dim)\n        for index in range(vocab_size):\n            pretrain_emb[index,:] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        return pretrain_emb\n\n\n    def get_last_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size, word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_embeds = char_embeds.transpose(2,1).contiguous()\n        char_cnn_out = self.char_cnn(char_embeds)\n        char_cnn_out = F.max_pool1d(char_cnn_out, char_cnn_out.size(2)).view(batch_size, -1)\n        return char_cnn_out\n\n    def get_all_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size,  word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, word_length, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_embeds = char_embeds.transpose(2,1).contiguous()\n        char_cnn_out = self.char_cnn(char_embeds).transpose(2,1).contiguous()\n        return char_cnn_out\n\n\n\n    def forward(self, input, seq_lengths):\n        return self.get_all_hiddens(input, seq_lengths)\n'"
model/crf.py,34,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-12-04 23:19:38\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-12-16 22:15:56\nfrom __future__ import print_function\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nSTART_TAG = -2\nSTOP_TAG = -1\n\n\n# Compute log sum exp in a numerically stable way for the forward algorithm\ndef log_sum_exp(vec, m_size):\n    """"""\n    calculate log of exp sum\n    args:\n        vec (batch_size, vanishing_dim, hidden_dim) : input tensor\n        m_size : hidden_dim\n    return:\n        batch_size, hidden_dim\n    """"""\n    _, idx = torch.max(vec, 1)  # B * 1 * M\n    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\n    return max_score.view(-1, m_size) + torch.log(torch.sum(torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)  # B * M\n\nclass CRF(nn.Module):\n\n    def __init__(self, tagset_size, gpu):\n        super(CRF, self).__init__()\n        print(""build CRF..."")\n        self.gpu = gpu\n        # Matrix of transition parameters.  Entry i,j is the score of transitioning from i to j.\n        self.tagset_size = tagset_size\n        # # We add 2 here, because of START_TAG and STOP_TAG\n        # # transitions (f_tag_size, t_tag_size), transition value from f_tag to t_tag\n        init_transitions = torch.zeros(self.tagset_size+2, self.tagset_size+2)\n        init_transitions[:,START_TAG] = -10000.0\n        init_transitions[STOP_TAG,:] = -10000.0\n        init_transitions[:,0] = -10000.0\n        init_transitions[0,:] = -10000.0\n        if self.gpu:\n            init_transitions = init_transitions.cuda()\n        self.transitions = nn.Parameter(init_transitions)\n\n        # self.transitions = nn.Parameter(torch.Tensor(self.tagset_size+2, self.tagset_size+2))\n        # self.transitions.data.zero_()\n\n    def _calculate_PZ(self, feats, mask):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                masks: (batch, seq_len)\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        # print feats.view(seq_len, tag_size)\n        assert(tag_size == self.tagset_size+2)\n        mask = mask.transpose(1,0).contiguous()\n        ins_num = seq_len * batch_size\n        ## be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1,0).contiguous().view(ins_num,1, tag_size).expand(ins_num, tag_size, tag_size)\n        ## need to consider start\n        scores = feats + self.transitions.view(1,tag_size,tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n        # build iter\n        seq_iter = enumerate(scores)\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, START_TAG, :].clone().view(batch_size, tag_size, 1)  # bat_size * to_target_size\n\n        ## add start score (from start to all tag, duplicate to batch_size)\n        # partition = partition + self.transitions[START_TAG,:].view(1, tag_size, 1).expand(batch_size, tag_size, 1)\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            # previous to_target is current from_target\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            # cur_values: bat_size * from_target * to_target\n\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            cur_partition = log_sum_exp(cur_values, tag_size)\n            # print cur_partition.data\n\n                # (bat_size * from_target * to_target) -> (bat_size * to_target)\n            # partition = utils.switch(partition, cur_partition, mask[idx].view(bat_size, 1).expand(bat_size, self.tagset_size)).view(bat_size, -1)\n            mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n\n            ## effective updated partition part, only keep the partition value of mask value = 1\n            masked_cur_partition = cur_partition.masked_select(mask_idx)\n            ## let mask_idx broadcastable, to disable warning\n            mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n\n            ## replace the partition where the maskvalue=1, other partition value keeps the same\n            partition.masked_scatter_(mask_idx, masked_cur_partition)\n        # until the last state, add transition score for all partition (and do log_sum_exp) then select the value in STOP_TAG\n        cur_values = self.transitions.view(1,tag_size, tag_size).expand(batch_size, tag_size, tag_size) + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n        cur_partition = log_sum_exp(cur_values, tag_size)\n        final_partition = cur_partition[:, STOP_TAG]\n        return final_partition.sum(), scores\n\n\n    def _viterbi_decode(self, feats, mask):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                mask: (batch, seq_len)\n            output:\n                decode_idx: (batch, seq_len) decoded sequence\n                path_score: (batch, 1) corresponding score for each sequence (to be implementated)\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        assert(tag_size == self.tagset_size+2)\n        ## calculate sentence length for each sentence\n        length_mask = torch.sum(mask.long(), dim = 1).view(batch_size,1).long()\n        ## mask to (seq_len, batch_size)\n        mask = mask.transpose(1,0).contiguous()\n        ins_num = seq_len * batch_size\n        ## be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1,0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        ## need to consider start\n        scores = feats + self.transitions.view(1,tag_size,tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\n        # build iter\n        seq_iter = enumerate(scores)\n        ## record the position of best score\n        back_points = list()\n        partition_history = list()\n        ##  reverse mask (bug for mask = 1- mask, use this as alternative choice)\n        # mask = 1 + (-1)*mask\n        mask =  (1 - mask.long()).byte()\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, START_TAG, :].clone().view(batch_size, tag_size)  # bat_size * to_target_size\n        # print ""init part:"",partition.size()\n        partition_history.append(partition)\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            # previous to_target is current from_target\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            # cur_values: batch_size * from_target * to_target\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            ## forscores, cur_bp = torch.max(cur_values[:,:-2,:], 1) # do not consider START_TAG/STOP_TAG\n            # print ""cur value:"", cur_values.size()\n            partition, cur_bp = torch.max(cur_values, 1)\n            # print ""partsize:"",partition.size()\n            # exit(0)\n            # print partition\n            # print cur_bp\n            # print ""one best, "",idx\n            partition_history.append(partition)\n            ## cur_bp: (batch_size, tag_size) max source score position in current tag\n            ## set padded label as 0, which will be filtered in post processing\n            cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n            back_points.append(cur_bp)\n        # exit(0)\n        ### add score to final STOP_TAG\n        partition_history = torch.cat(partition_history, 0).view(seq_len, batch_size, -1).transpose(1,0).contiguous() ## (batch_size, seq_len. tag_size)\n        ### get the last position for each setences, and select the last partitions using gather()\n        last_position = length_mask.view(batch_size,1,1).expand(batch_size, 1, tag_size) -1\n        last_partition = torch.gather(partition_history, 1, last_position).view(batch_size,tag_size,1)\n        ### calculate the score from last partition to end state (and then select the STOP_TAG from it)\n        last_values = last_partition.expand(batch_size, tag_size, tag_size) + self.transitions.view(1,tag_size, tag_size).expand(batch_size, tag_size, tag_size)\n        _, last_bp = torch.max(last_values, 1)\n        pad_zero = autograd.Variable(torch.zeros(batch_size, tag_size)).long()\n        if self.gpu:\n            pad_zero = pad_zero.cuda()\n        back_points.append(pad_zero)\n        back_points  =  torch.cat(back_points).view(seq_len, batch_size, tag_size)\n\n        ## select end ids in STOP_TAG\n        pointer = last_bp[:, STOP_TAG]\n        insert_last = pointer.contiguous().view(batch_size,1,1).expand(batch_size,1, tag_size)\n        back_points = back_points.transpose(1,0).contiguous()\n        ## move the end ids(expand to tag_size) to the corresponding position of back_points to replace the 0 values\n        # print ""lp:"",last_position\n        # print ""il:"",insert_last\n        back_points.scatter_(1, last_position, insert_last)\n        # print ""bp:"",back_points\n        # exit(0)\n        back_points = back_points.transpose(1,0).contiguous()\n        ## decode from the end, padded position ids are 0, which will be filtered if following evaluation\n        decode_idx = autograd.Variable(torch.LongTensor(seq_len, batch_size))\n        if self.gpu:\n            decode_idx = decode_idx.cuda()\n        decode_idx[-1] = pointer.detach()\n        for idx in range(len(back_points)-2, -1, -1):\n            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n            decode_idx[idx] = pointer.detach().view(batch_size)\n        path_score = None\n        decode_idx = decode_idx.transpose(1,0)\n        return path_score, decode_idx\n\n\n\n    def forward(self, feats):\n    \tpath_score, best_path = self._viterbi_decode(feats)\n    \treturn path_score, best_path\n\n\n    def _score_sentence(self, scores, mask, tags):\n        """"""\n            input:\n                scores: variable (seq_len, batch, tag_size, tag_size)\n                mask: (batch, seq_len)\n                tags: tensor  (batch, seq_len)\n            output:\n                score: sum of score for gold sequences within whole batch\n        """"""\n        # Gives the score of a provided tag sequence\n        batch_size = scores.size(1)\n        seq_len = scores.size(0)\n        tag_size = scores.size(2)\n        ## convert tag value into a new format, recorded label bigram information to index\n        new_tags = autograd.Variable(torch.LongTensor(batch_size, seq_len))\n        if self.gpu:\n            new_tags = new_tags.cuda()\n        for idx in range(seq_len):\n            if idx == 0:\n                ## start -> first score\n                new_tags[:,0] =  (tag_size - 2)*tag_size + tags[:,0]\n\n            else:\n                new_tags[:,idx] =  tags[:,idx-1]*tag_size + tags[:,idx]\n\n        ## transition for label to STOP_TAG\n        end_transition = self.transitions[:,STOP_TAG].contiguous().view(1, tag_size).expand(batch_size, tag_size)\n        ## length for batch,  last word position = length - 1\n        length_mask = torch.sum(mask.long(), dim = 1).view(batch_size,1).long()\n        ## index the label id of last word\n        end_ids = torch.gather(tags, 1, length_mask - 1)\n\n        ## index the transition score for end_id to STOP_TAG\n        end_energy = torch.gather(end_transition, 1, end_ids)\n\n        ## convert tag as (seq_len, batch_size, 1)\n        new_tags = new_tags.transpose(1,0).contiguous().view(seq_len, batch_size, 1)\n        ### need convert tags id to search from 400 positions of scores\n        tg_energy = torch.gather(scores.view(seq_len, batch_size, -1), 2, new_tags).view(seq_len, batch_size)  # seq_len * bat_size\n        ## mask transpose to (seq_len, batch_size)\n        tg_energy = tg_energy.masked_select(mask.transpose(1,0))\n\n        # ## calculate the score from START_TAG to first label\n        # start_transition = self.transitions[START_TAG,:].view(1, tag_size).expand(batch_size, tag_size)\n        # start_energy = torch.gather(start_transition, 1, tags[0,:])\n\n        ## add all score together\n        # gold_score = start_energy.sum() + tg_energy.sum() + end_energy.sum()\n        gold_score = tg_energy.sum() + end_energy.sum()\n        return gold_score\n\n    def neg_log_likelihood_loss(self, feats, mask, tags):\n        # nonegative log likelihood\n        batch_size = feats.size(0)\n        forward_score, scores = self._calculate_PZ(feats, mask)\n        gold_score = self._score_sentence(scores, mask, tags)\n        # print ""batch, f:"", forward_score.data[0], "" g:"", gold_score.data[0], "" dis:"", forward_score.data[0] - gold_score.data[0]\n        # exit(0)\n        return forward_score - gold_score\n\n\n\n    def _viterbi_decode_nbest(self, feats, mask, nbest):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                mask: (batch, seq_len)\n            output:\n                decode_idx: (batch, nbest, seq_len) decoded sequence\n                path_score: (batch, nbest) corresponding score for each sequence (to be implementated)\n                nbest decode for sentence with one token is not well supported, to be optimized\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        assert(tag_size == self.tagset_size+2)\n        ## calculate sentence length for each sentence\n        length_mask = torch.sum(mask.long(), dim = 1).view(batch_size,1).long()\n        ## mask to (seq_len, batch_size)\n        mask = mask.transpose(1,0).contiguous()\n        ins_num = seq_len * batch_size\n        ## be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1,0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        ## need to consider start\n        scores = feats + self.transitions.view(1,tag_size,tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\n        # build iter\n        seq_iter = enumerate(scores)\n        ## record the position of best score\n        back_points = list()\n        partition_history = list()\n        ##  reverse mask (bug for mask = 1- mask, use this as alternative choice)\n        # mask = 1 + (-1)*mask\n        mask =  (1 - mask.long()).byte()\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, START_TAG, :].clone()  # bat_size * to_target_size\n        ## initial partition [batch_size, tag_size]\n        partition_history.append(partition.view(batch_size, tag_size, 1).expand(batch_size, tag_size, nbest))\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            if idx == 1:\n                cur_values = cur_values.view(batch_size, tag_size, tag_size) + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            else:\n                # previous to_target is current from_target\n                # partition: previous results log(exp(from_target)), #(batch_size * nbest * from_target)\n                # cur_values: batch_size * from_target * to_target\n                cur_values = cur_values.view(batch_size, tag_size, 1, tag_size).expand(batch_size, tag_size, nbest, tag_size) + partition.contiguous().view(batch_size, tag_size, nbest, 1).expand(batch_size, tag_size, nbest, tag_size)\n                ## compare all nbest and all from target\n                cur_values = cur_values.view(batch_size, tag_size*nbest, tag_size)\n                # print ""cur size:"",cur_values.size()\n            partition, cur_bp = torch.topk(cur_values, nbest, 1)\n            ## cur_bp/partition: [batch_size, nbest, tag_size], id should be normize through nbest in following backtrace step\n            # print partition[:,0,:]\n            # print cur_bp[:,0,:]\n            # print ""nbest, "",idx\n            if idx == 1:\n                cur_bp = cur_bp*nbest\n            partition = partition.transpose(2,1)\n            cur_bp = cur_bp.transpose(2,1)\n\n            # print partition\n            # exit(0)\n            #partition: (batch_size * to_target * nbest)\n            #cur_bp: (batch_size * to_target * nbest) Notice the cur_bp number is the whole position of tag_size*nbest, need to convert when decode\n            partition_history.append(partition)\n            ## cur_bp: (batch_size,nbest, tag_size) topn source score position in current tag\n            ## set padded label as 0, which will be filtered in post processing\n            ## mask[idx] ? mask[idx-1]\n            cur_bp.masked_fill_(mask[idx].view(batch_size, 1, 1).expand(batch_size, tag_size, nbest), 0)\n            # print cur_bp[0]\n            back_points.append(cur_bp)\n        ### add score to final STOP_TAG\n        partition_history = torch.cat(partition_history,0).view(seq_len, batch_size, tag_size, nbest).transpose(1,0).contiguous() ## (batch_size, seq_len, nbest, tag_size)\n        ### get the last position for each setences, and select the last partitions using gather()\n        last_position = length_mask.view(batch_size,1,1,1).expand(batch_size, 1, tag_size, nbest) - 1\n        last_partition = torch.gather(partition_history, 1, last_position).view(batch_size, tag_size, nbest, 1)\n        ### calculate the score from last partition to end state (and then select the STOP_TAG from it)\n        last_values = last_partition.expand(batch_size, tag_size, nbest, tag_size) + self.transitions.view(1, tag_size, 1, tag_size).expand(batch_size, tag_size, nbest, tag_size)\n        last_values = last_values.view(batch_size, tag_size*nbest, tag_size)\n        end_partition, end_bp = torch.topk(last_values, nbest, 1)\n        ## end_partition: (batch, nbest, tag_size)\n        end_bp = end_bp.transpose(2,1)\n        # end_bp: (batch, tag_size, nbest)\n        pad_zero = autograd.Variable(torch.zeros(batch_size, tag_size, nbest)).long()\n        if self.gpu:\n            pad_zero = pad_zero.cuda()\n        back_points.append(pad_zero)\n        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size, nbest)\n\n        ## select end ids in STOP_TAG\n        pointer = end_bp[:, STOP_TAG, :] ## (batch_size, nbest)\n        insert_last = pointer.contiguous().view(batch_size, 1, 1, nbest).expand(batch_size, 1, tag_size, nbest)\n        back_points = back_points.transpose(1,0).contiguous()\n        ## move the end ids(expand to tag_size) to the corresponding position of back_points to replace the 0 values\n        # print ""lp:"",last_position\n        # print ""il:"",insert_last[0]\n        # exit(0)\n        ## copy the ids of last position:insert_last to back_points, though the last_position index\n        ## last_position includes the length of batch sentences\n        # print ""old:"", back_points[9,0,:,:]\n        back_points.scatter_(1, last_position, insert_last)\n        ## back_points: [batch_size, seq_length, tag_size, nbest]\n        # print ""new:"", back_points[9,0,:,:]\n        # exit(0)\n        # print pointer[2]\n        \'\'\'\n        back_points: in simple demonstratration\n        x,x,x,x,x,x,x,x,x,7\n        x,x,x,x,x,4,0,0,0,0\n        x,x,6,0,0,0,0,0,0,0\n        \'\'\'\n\n        back_points = back_points.transpose(1,0).contiguous()\n        # print back_points[0]\n        ## back_points: (seq_len, batch, tag_size, nbest)\n        ## decode from the end, padded position ids are 0, which will be filtered in following evaluation\n        decode_idx = autograd.Variable(torch.LongTensor(seq_len, batch_size, nbest))\n        if self.gpu:\n            decode_idx = decode_idx.cuda()\n        decode_idx[-1] = pointer.data/nbest\n        # print ""pointer-1:"",pointer[2]\n        # exit(0)\n        # use old mask, let 0 means has token\n        for idx in range(len(back_points)-2, -1, -1):\n            # print ""pointer: "",idx,  pointer[3]\n            # print ""back:"",back_points[idx][3]\n            # print ""mask:"",mask[idx+1,3]\n            new_pointer = torch.gather(back_points[idx].view(batch_size, tag_size*nbest), 1, pointer.contiguous().view(batch_size,nbest))\n            decode_idx[idx] = new_pointer.data/nbest\n            # # use new pointer to remember the last end nbest ids for non longest\n            pointer = new_pointer + pointer.contiguous().view(batch_size,nbest)*mask[idx].view(batch_size,1).expand(batch_size, nbest).long()\n\n        # exit(0)\n        path_score = None\n        decode_idx = decode_idx.transpose(1,0)\n        ## decode_idx: [batch, seq_len, nbest]\n        # print decode_idx[:,:,0]\n        # print ""nbest:"",nbest\n        # print ""diff:"", decode_idx[:,:,0]- decode_idx[:,:,4]\n        # print decode_idx[:,0,:]\n        # exit(0)\n\n        ### calculate probability for each sequence\n        scores = end_partition[:, :, STOP_TAG]\n        ## scores: [batch_size, nbest]\n        max_scores,_ = torch.max(scores, 1)\n        minus_scores = scores - max_scores.view(batch_size,1).expand(batch_size, nbest)\n        path_score = F.softmax(minus_scores, 1)\n        ## path_score: [batch_size, nbest]\n        # exit(0)\n        return path_score, decode_idx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
model/sentclassifier.py,4,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2019-01-01 21:11:50\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2019-02-13 12:30:56\n\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .wordsequence import WordSequence\n\nclass SentClassifier(nn.Module):\n    def __init__(self, data):\n        super(SentClassifier, self).__init__()\n        print(""build sentence classification network..."")\n        print(""use_char: "", data.use_char)\n        if data.use_char:\n            print(""char feature extractor: "", data.char_feature_extractor)\n        print(""word feature extractor: "", data.word_feature_extractor)\n\n        self.gpu = data.HP_gpu\n        self.average_batch = data.average_batch_loss\n        label_size = data.label_alphabet_size\n        self.word_hidden = WordSequence(data)\n\n\n\n    def calculate_loss(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover, batch_label, mask):\n        outs = self.word_hidden.sentence_representation(word_inputs,feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover)\n        batch_size = word_inputs.size(0)\n        # loss_function = nn.CrossEntropyLoss(ignore_index=0, reduction=\'sum\')\n        outs = outs.view(batch_size, -1)\n        # print(""a"",outs)\n        # score = F.log_softmax(outs, 1)\n        # print(score.size(), batch_label.view(batch_size).size())\n        # print(score)\n        # print(batch_label)\n        # exit(0)\n        total_loss = F.cross_entropy(outs, batch_label.view(batch_size))\n        # total_loss = loss_function(score, batch_label.view(batch_size))\n        \n        _, tag_seq  = torch.max(outs, 1)\n        if self.average_batch:\n            total_loss = total_loss / batch_size\n        return total_loss, tag_seq\n\n\n    def forward(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover, mask):\n        outs = self.word_hidden.sentence_representation(word_inputs,feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover)\n        batch_size = word_inputs.size(0)\n        outs = outs.view(batch_size, -1)\n        _, tag_seq  = torch.max(outs, 1)\n        # if a == 0:\n        #     print(tag_seq)\n        return tag_seq\n\n\n'"
model/seqlabel.py,4,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-10-17 16:47:32\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2019-02-13 11:49:38\n\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .wordsequence import WordSequence\nfrom .crf import CRF\n\nclass SeqLabel(nn.Module):\n    def __init__(self, data):\n        super(SeqLabel, self).__init__()\n        self.use_crf = data.use_crf\n        print(""build sequence labeling network..."")\n        print(""use_char: "", data.use_char)\n        if data.use_char:\n            print(""char feature extractor: "", data.char_feature_extractor)\n        print(""word feature extractor: "", data.word_feature_extractor)\n        print(""use crf: "", self.use_crf)\n\n        self.gpu = data.HP_gpu\n        self.average_batch = data.average_batch_loss\n        ## add two more label for downlayer lstm, use original label size for CRF\n        label_size = data.label_alphabet_size\n        data.label_alphabet_size += 2\n        self.word_hidden = WordSequence(data)\n        if self.use_crf:\n            self.crf = CRF(label_size, self.gpu)\n\n\n    def calculate_loss(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover, batch_label, mask):\n        outs = self.word_hidden(word_inputs,feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover)\n        batch_size = word_inputs.size(0)\n        seq_len = word_inputs.size(1)\n        if self.use_crf:\n            total_loss = self.crf.neg_log_likelihood_loss(outs, mask, batch_label)\n            scores, tag_seq = self.crf._viterbi_decode(outs, mask)\n        else:\n            loss_function = nn.NLLLoss(ignore_index=0, size_average=False)\n            outs = outs.view(batch_size * seq_len, -1)\n            score = F.log_softmax(outs, 1)\n            total_loss = loss_function(score, batch_label.view(batch_size * seq_len))\n            _, tag_seq  = torch.max(score, 1)\n            tag_seq = tag_seq.view(batch_size, seq_len)\n        if self.average_batch:\n            total_loss = total_loss / batch_size\n        return total_loss, tag_seq\n\n\n    def forward(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover, mask):\n        outs = self.word_hidden(word_inputs,feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover)\n        batch_size = word_inputs.size(0)\n        seq_len = word_inputs.size(1)\n        if self.use_crf:\n            scores, tag_seq = self.crf._viterbi_decode(outs, mask)\n        else:\n            outs = outs.view(batch_size * seq_len, -1)\n            _, tag_seq  = torch.max(outs, 1)\n            tag_seq = tag_seq.view(batch_size, seq_len)\n            ## filter padded position with zero\n            tag_seq = mask.long() * tag_seq\n        return tag_seq\n\n\n    # def get_lstm_features(self, word_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover):\n    #     return self.word_hidden(word_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover)\n\n\n    def decode_nbest(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover, mask, nbest):\n        if not self.use_crf:\n            print(""Nbest output is currently supported only for CRF! Exit..."")\n            exit(0)\n        outs = self.word_hidden(word_inputs,feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover)\n        batch_size = word_inputs.size(0)\n        seq_len = word_inputs.size(1)\n        scores, tag_seq = self.crf._viterbi_decode_nbest(outs, mask, nbest)\n        return scores, tag_seq\n\n'"
model/wordrep.py,7,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-10-17 16:47:32\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2019-02-01 15:52:01\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom .charbilstm import CharBiLSTM\nfrom .charbigru import CharBiGRU\nfrom .charcnn import CharCNN\n\nclass WordRep(nn.Module):\n    def __init__(self, data):\n        super(WordRep, self).__init__()\n        print(""build word representation..."")\n        self.gpu = data.HP_gpu\n        self.use_char = data.use_char\n        self.batch_size = data.HP_batch_size\n        self.char_hidden_dim = 0\n        self.char_all_feature = False\n        self.sentence_classification = data.sentence_classification\n        if self.use_char:\n            self.char_hidden_dim = data.HP_char_hidden_dim\n            self.char_embedding_dim = data.char_emb_dim\n            if data.char_feature_extractor == ""CNN"":\n                self.char_feature = CharCNN(data.char_alphabet.size(), data.pretrain_char_embedding, self.char_embedding_dim, self.char_hidden_dim, data.HP_dropout, self.gpu)\n            elif data.char_feature_extractor == ""LSTM"":\n                self.char_feature = CharBiLSTM(data.char_alphabet.size(), data.pretrain_char_embedding, self.char_embedding_dim, self.char_hidden_dim, data.HP_dropout, self.gpu)\n            elif data.char_feature_extractor == ""GRU"":\n                self.char_feature = CharBiGRU(data.char_alphabet.size(), data.pretrain_char_embedding, self.char_embedding_dim, self.char_hidden_dim, data.HP_dropout, self.gpu)\n            elif data.char_feature_extractor == ""ALL"":\n                self.char_all_feature = True\n                self.char_feature = CharCNN(data.char_alphabet.size(), data.pretrain_char_embedding, self.char_embedding_dim, self.char_hidden_dim, data.HP_dropout, self.gpu)\n                self.char_feature_extra = CharBiLSTM(data.char_alphabet.size(), data.pretrain_char_embedding, self.char_embedding_dim, self.char_hidden_dim, data.HP_dropout, self.gpu)\n            else:\n                print(""Error char feature selection, please check parameter data.char_feature_extractor (CNN/LSTM/GRU/ALL)."")\n                exit(0)\n        self.embedding_dim = data.word_emb_dim\n        self.drop = nn.Dropout(data.HP_dropout)\n        self.word_embedding = nn.Embedding(data.word_alphabet.size(), self.embedding_dim)\n        if data.pretrain_word_embedding is not None:\n            self.word_embedding.weight.data.copy_(torch.from_numpy(data.pretrain_word_embedding))\n        else:\n            self.word_embedding.weight.data.copy_(torch.from_numpy(self.random_embedding(data.word_alphabet.size(), self.embedding_dim)))\n\n        self.feature_num = data.feature_num\n        self.feature_embedding_dims = data.feature_emb_dims\n        self.feature_embeddings = nn.ModuleList()\n        for idx in range(self.feature_num):\n            self.feature_embeddings.append(nn.Embedding(data.feature_alphabets[idx].size(), self.feature_embedding_dims[idx]))\n        for idx in range(self.feature_num):\n            if data.pretrain_feature_embeddings[idx] is not None:\n                self.feature_embeddings[idx].weight.data.copy_(torch.from_numpy(data.pretrain_feature_embeddings[idx]))\n            else:\n                self.feature_embeddings[idx].weight.data.copy_(torch.from_numpy(self.random_embedding(data.feature_alphabets[idx].size(), self.feature_embedding_dims[idx])))\n\n        if self.gpu:\n            self.drop = self.drop.cuda()\n            self.word_embedding = self.word_embedding.cuda()\n            for idx in range(self.feature_num):\n                self.feature_embeddings[idx] = self.feature_embeddings[idx].cuda()\n\n\n\n    def random_embedding(self, vocab_size, embedding_dim):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        scale = np.sqrt(3.0 / embedding_dim)\n        for index in range(vocab_size):\n            pretrain_emb[index,:] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        return pretrain_emb\n\n\n    def forward(self, word_inputs,feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover):\n        """"""\n            input:\n                word_inputs: (batch_size, sent_len)\n                features: list [(batch_size, sent_len), (batch_len, sent_len),...]\n                word_seq_lengths: list of batch_size, (batch_size,1)\n                char_inputs: (batch_size*sent_len, word_length)\n                char_seq_lengths: list of whole batch_size for char, (batch_size*sent_len, 1)\n                char_seq_recover: variable which records the char order information, used to recover char order\n            output:\n                Variable(batch_size, sent_len, hidden_dim)\n        """"""\n        batch_size = word_inputs.size(0)\n        sent_len = word_inputs.size(1)\n\n        word_embs =  self.word_embedding(word_inputs)\n\n        word_list = [word_embs]\n        if not self.sentence_classification:\n            for idx in range(self.feature_num):\n                word_list.append(self.feature_embeddings[idx](feature_inputs[idx]))\n        if self.use_char:\n            ## calculate char lstm last hidden\n            # print(""charinput:"", char_inputs)\n            # exit(0)\n            char_features = self.char_feature.get_last_hiddens(char_inputs, char_seq_lengths.cpu().numpy())\n            char_features = char_features[char_seq_recover]\n            char_features = char_features.view(batch_size,sent_len,-1)\n            ## concat word and char together\n            word_list.append(char_features)\n            word_embs = torch.cat([word_embs, char_features], 2)\n            if self.char_all_feature:\n                char_features_extra = self.char_feature_extra.get_last_hiddens(char_inputs, char_seq_lengths.cpu().numpy())\n                char_features_extra = char_features_extra[char_seq_recover]\n                char_features_extra = char_features_extra.view(batch_size,sent_len,-1)\n                ## concat word and char together\n                word_list.append(char_features_extra)    \n        word_embs = torch.cat(word_list, 2)\n        # if a == 0:\n        #     print(""inputs"", word_inputs)\n        #     print(""embeddings:"", word_embs)\n        word_represent = self.drop(word_embs)\n        return word_represent\n'"
model/wordsequence.py,6,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-10-17 16:47:32\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2019-02-01 15:59:26\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom .wordrep import WordRep\n\nclass WordSequence(nn.Module):\n    def __init__(self, data):\n        super(WordSequence, self).__init__()\n        print(""build word sequence feature extractor: %s...""%(data.word_feature_extractor))\n        self.gpu = data.HP_gpu\n        self.use_char = data.use_char\n        # self.batch_size = data.HP_batch_size\n        # self.hidden_dim = data.HP_hidden_dim\n        self.droplstm = nn.Dropout(data.HP_dropout)\n        self.bilstm_flag = data.HP_bilstm\n        self.lstm_layer = data.HP_lstm_layer\n        self.wordrep = WordRep(data)\n        self.input_size = data.word_emb_dim\n        self.feature_num = data.feature_num\n        if self.use_char:\n            self.input_size += data.HP_char_hidden_dim\n            if data.char_feature_extractor == ""ALL"":\n                self.input_size += data.HP_char_hidden_dim\n        for idx in range(self.feature_num):\n            self.input_size += data.feature_emb_dims[idx]\n        # The LSTM takes word embeddings as inputs, and outputs hidden states\n        # with dimensionality hidden_dim.\n        if self.bilstm_flag:\n            lstm_hidden = data.HP_hidden_dim // 2\n        else:\n            lstm_hidden = data.HP_hidden_dim\n\n        self.word_feature_extractor = data.word_feature_extractor\n        if self.word_feature_extractor == ""GRU"":\n            self.lstm = nn.GRU(self.input_size, lstm_hidden, num_layers=self.lstm_layer, batch_first=True, bidirectional=self.bilstm_flag)\n        elif self.word_feature_extractor == ""LSTM"":\n            self.lstm = nn.LSTM(self.input_size, lstm_hidden, num_layers=self.lstm_layer, batch_first=True, bidirectional=self.bilstm_flag)\n        elif self.word_feature_extractor == ""CNN"":\n            # cnn_hidden = data.HP_hidden_dim\n            self.word2cnn = nn.Linear(self.input_size, data.HP_hidden_dim)\n            self.cnn_layer = data.HP_cnn_layer\n            print(""CNN layer: "", self.cnn_layer)\n            self.cnn_list = nn.ModuleList()\n            self.cnn_drop_list = nn.ModuleList()\n            self.cnn_batchnorm_list = nn.ModuleList()\n            kernel = 3\n            pad_size = int((kernel-1)/2)\n            for idx in range(self.cnn_layer):\n                self.cnn_list.append(nn.Conv1d(data.HP_hidden_dim, data.HP_hidden_dim, kernel_size=kernel, padding=pad_size))\n                self.cnn_drop_list.append(nn.Dropout(data.HP_dropout))\n                self.cnn_batchnorm_list.append(nn.BatchNorm1d(data.HP_hidden_dim))\n        # The linear layer that maps from hidden state space to tag space\n        self.hidden2tag = nn.Linear(data.HP_hidden_dim, data.label_alphabet_size)\n\n        if self.gpu:\n            self.droplstm = self.droplstm.cuda()\n            self.hidden2tag = self.hidden2tag.cuda()\n            if self.word_feature_extractor == ""CNN"":\n                self.word2cnn = self.word2cnn.cuda()\n                for idx in range(self.cnn_layer):\n                    self.cnn_list[idx] = self.cnn_list[idx].cuda()\n                    self.cnn_drop_list[idx] = self.cnn_drop_list[idx].cuda()\n                    self.cnn_batchnorm_list[idx] = self.cnn_batchnorm_list[idx].cuda()\n            else:\n                self.lstm = self.lstm.cuda()\n\n\n    def forward(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover):\n        """"""\n            input:\n                word_inputs: (batch_size, sent_len)\n                feature_inputs: [(batch_size, sent_len), ...] list of variables\n                word_seq_lengths: list of batch_size, (batch_size,1)\n                char_inputs: (batch_size*sent_len, word_length)\n                char_seq_lengths: list of whole batch_size for char, (batch_size*sent_len, 1)\n                char_seq_recover: variable which records the char order information, used to recover char order\n            output:\n                Variable(batch_size, sent_len, hidden_dim)\n        """"""\n        \n        word_represent = self.wordrep(word_inputs,feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover)\n        ## word_embs (batch_size, seq_len, embed_size)\n        if self.word_feature_extractor == ""CNN"":\n            batch_size = word_inputs.size(0)\n            word_in = torch.tanh(self.word2cnn(word_represent)).transpose(2,1).contiguous()\n            for idx in range(self.cnn_layer):\n                if idx == 0:\n                    cnn_feature = F.relu(self.cnn_list[idx](word_in))\n                else:\n                    cnn_feature = F.relu(self.cnn_list[idx](cnn_feature))\n                cnn_feature = self.cnn_drop_list[idx](cnn_feature)\n                if batch_size > 1:\n                    cnn_feature = self.cnn_batchnorm_list[idx](cnn_feature)\n            feature_out = cnn_feature.transpose(2,1).contiguous()\n        else:\n            packed_words = pack_padded_sequence(word_represent, word_seq_lengths.cpu().numpy(), True)\n            hidden = None\n            lstm_out, hidden = self.lstm(packed_words, hidden)\n            lstm_out, _ = pad_packed_sequence(lstm_out)\n            ## lstm_out (seq_len, seq_len, hidden_size)\n            feature_out = self.droplstm(lstm_out.transpose(1,0))\n        ## feature_out (batch_size, seq_len, hidden_size)\n        outputs = self.hidden2tag(feature_out)\n        return outputs\n\n    def sentence_representation(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover):\n        """"""\n            input:\n                word_inputs: (batch_size, sent_len)\n                feature_inputs: [(batch_size, ), ...] list of variables\n                word_seq_lengths: list of batch_size, (batch_size,1)\n                char_inputs: (batch_size*sent_len, word_length)\n                char_seq_lengths: list of whole batch_size for char, (batch_size*sent_len, 1)\n                char_seq_recover: variable which records the char order information, used to recover char order\n            output:\n                Variable(batch_size, sent_len, hidden_dim)\n        """"""\n\n        word_represent = self.wordrep(word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover)\n        ## word_embs (batch_size, seq_len, embed_size)\n        batch_size = word_inputs.size(0)\n        if self.word_feature_extractor == ""CNN"":\n            word_in = torch.tanh(self.word2cnn(word_represent)).transpose(2,1).contiguous()\n            for idx in range(self.cnn_layer):\n                if idx == 0:\n                    cnn_feature = F.relu(self.cnn_list[idx](word_in))\n                else:\n                    cnn_feature = F.relu(self.cnn_list[idx](cnn_feature))\n                cnn_feature = self.cnn_drop_list[idx](cnn_feature)\n                if batch_size > 1:\n                    cnn_feature = self.cnn_batchnorm_list[idx](cnn_feature)\n            feature_out = F.max_pool1d(cnn_feature, cnn_feature.size(2)).view(batch_size, -1)\n        else:\n            packed_words = pack_padded_sequence(word_represent, word_seq_lengths.cpu().numpy(), True)\n            hidden = None\n            lstm_out, hidden = self.lstm(packed_words, hidden)\n            ## lstm_out (seq_len, seq_len, hidden_size)\n            ## feature_out (batch_size, hidden_size)\n            feature_out = hidden[0].transpose(1,0).contiguous().view(batch_size,-1)\n            \n        feature_list = [feature_out]\n        for idx in range(self.feature_num):\n            feature_list.append(self.feature_embeddings[idx](feature_inputs[idx]))\n        final_feature = torch.cat(feature_list, 1)\n        outputs = self.hidden2tag(self.droplstm(final_feature))\n        ## outputs: (batch_size, label_alphabet_size)\n        return outputs\n'"
utils/__init__.py,0,"b""__author__ = 'max'\n"""
utils/alphabet.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Max\n# @Date:   2018-01-19 11:33:37\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-04-26 13:56:03\n\n\n""""""\nAlphabet maps objects to integer ids. It provides two way mapping from the index to the objects.\n""""""\nfrom __future__ import print_function\nimport json\nimport os\nimport sys\n\n\nclass Alphabet:\n    def __init__(self, name, label=False, keep_growing=True):\n        self.name = name\n        self.UNKNOWN = ""</unk>""\n        self.label = label\n        self.instance2index = {}\n        self.instances = []\n        self.keep_growing = keep_growing\n\n        # Index 0 is occupied by default, all else following.\n        self.default_index = 0\n        self.next_index = 1\n        if not self.label:\n            self.add(self.UNKNOWN)\n\n    def clear(self, keep_growing=True):\n        self.instance2index = {}\n        self.instances = []\n        self.keep_growing = keep_growing\n\n        # Index 0 is occupied by default, all else following.\n        self.default_index = 0\n        self.next_index = 1\n\n    def add(self, instance):\n        if instance not in self.instance2index:\n            self.instances.append(instance)\n            self.instance2index[instance] = self.next_index\n            self.next_index += 1\n\n    def get_index(self, instance):\n        try:\n            return self.instance2index[instance]\n        except KeyError:\n            if self.keep_growing:\n                index = self.next_index\n                self.add(instance)\n                return index\n            else:\n                return self.instance2index[self.UNKNOWN]\n\n    def get_instance(self, index):\n        if index == 0:\n            if self.label:\n                return self.instances[0]\n            # First index is occupied by the wildcard element.\n            return None\n        try:\n            return self.instances[index - 1]\n        except IndexError:\n            print(\'WARNING:Alphabet get_instance ,unknown instance, return the first label.\')\n            return self.instances[0]\n\n    def size(self):\n        # if self.label:\n        #     return len(self.instances)\n        # else:\n        return len(self.instances) + 1\n\n    def iteritems(self):\n        if sys.version_info[0] < 3:  # If using python3, dict item access uses different syntax\n            return self.instance2index.iteritems()\n        else:\n            return self.instance2index.items()\n\n    def enumerate_items(self, start=1):\n        if start < 1 or start >= self.size():\n            raise IndexError(""Enumerate is allowed between [1 : size of the alphabet)"")\n        return zip(range(start, len(self.instances) + 1), self.instances[start - 1:])\n\n    def close(self):\n        self.keep_growing = False\n\n    def open(self):\n        self.keep_growing = True\n\n    def get_content(self):\n        return {\'instance2index\': self.instance2index, \'instances\': self.instances}\n\n    def from_json(self, data):\n        self.instances = data[""instances""]\n        self.instance2index = data[""instance2index""]\n\n    def save(self, output_directory, name=None):\n        """"""\n        Save both alhpabet records to the given directory.\n        :param output_directory: Directory to save model and weights.\n        :param name: The alphabet saving name, optional.\n        :return:\n        """"""\n        saving_name = name if name else self.__name\n        try:\n            json.dump(self.get_content(), open(os.path.join(output_directory, saving_name + "".json""), \'w\'))\n        except Exception as e:\n            print(""Exception: Alphabet is not saved: "" % repr(e))\n\n    def load(self, input_directory, name=None):\n        """"""\n        Load model architecture and weights from the give directory. This allow we use old models even the structure\n        changes.\n        :param input_directory: Directory to save model and weights\n        :return:\n        """"""\n        loading_name = name if name else self.__name\n        self.from_json(json.load(open(os.path.join(input_directory, loading_name + "".json""))))\n'"
utils/data.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Jie\n# @Date:   2017-06-14 17:34:32\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2019-01-25 20:25:59\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport sys\nfrom .alphabet import Alphabet\nfrom .functions import *\n\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle as pickle\n\n\nSTART = ""</s>""\nUNKNOWN = ""</unk>""\nPADDING = ""</pad>""\n\nclass Data:\n    def __init__(self):\n        self.sentence_classification = False\n        self.MAX_SENTENCE_LENGTH = 250\n        self.MAX_WORD_LENGTH = -1\n        self.number_normalized = True\n        self.norm_word_emb = False\n        self.norm_char_emb = False\n        self.word_alphabet = Alphabet(\'word\')\n        self.char_alphabet = Alphabet(\'character\')\n\n        self.feature_name = []\n        self.feature_alphabets = []\n        self.feature_num = len(self.feature_alphabets)\n        self.feat_config = None\n\n\n        self.label_alphabet = Alphabet(\'label\',True)\n        self.tagScheme = ""NoSeg"" ## BMES/BIO\n        self.split_token = \' ||| \'\n        self.seg = True\n\n        ### I/O\n        self.train_dir = None\n        self.dev_dir = None\n        self.test_dir = None\n        self.raw_dir = None\n\n        self.decode_dir = None\n        self.dset_dir = None ## data vocabulary related file\n        self.model_dir = None ## model save  file\n        self.load_model_dir = None ## model load file\n\n        self.word_emb_dir = None\n        self.char_emb_dir = None\n        self.feature_emb_dirs = []\n\n        self.train_texts = []\n        self.dev_texts = []\n        self.test_texts = []\n        self.raw_texts = []\n\n        self.train_Ids = []\n        self.dev_Ids = []\n        self.test_Ids = []\n        self.raw_Ids = []\n\n        self.pretrain_word_embedding = None\n        self.pretrain_char_embedding = None\n        self.pretrain_feature_embeddings = []\n\n        self.label_size = 0\n        self.word_alphabet_size = 0\n        self.char_alphabet_size = 0\n        self.label_alphabet_size = 0\n        self.feature_alphabet_sizes = []\n        self.feature_emb_dims = []\n        self.norm_feature_embs = []\n        self.word_emb_dim = 50\n        self.char_emb_dim = 30\n\n        ###Networks\n        self.word_feature_extractor = ""LSTM"" ## ""LSTM""/""CNN""/""GRU""/\n        self.use_char = True\n        self.char_feature_extractor = ""CNN"" ## ""LSTM""/""CNN""/""GRU""/None\n        self.use_crf = True\n        self.nbest = None\n\n        ## Training\n        self.average_batch_loss = False\n        self.optimizer = ""SGD"" ## ""SGD""/""AdaGrad""/""AdaDelta""/""RMSProp""/""Adam""\n        self.status = ""train""\n        ### Hyperparameters\n        self.HP_cnn_layer = 4\n        self.HP_iteration = 100\n        self.HP_batch_size = 10\n        self.HP_char_hidden_dim = 50\n        self.HP_hidden_dim = 200\n        self.HP_dropout = 0.5\n        self.HP_lstm_layer = 1\n        self.HP_bilstm = True\n\n        self.HP_gpu = False\n        self.HP_lr = 0.015\n        self.HP_lr_decay = 0.05\n        self.HP_clip = None\n        self.HP_momentum = 0\n        self.HP_l2 = 1e-8\n\n    def show_data_summary(self):\n        \n        print(""++""*50)\n        print(""DATA SUMMARY START:"")\n        print("" I/O:"")\n        if self.sentence_classification:\n            print(""     Start Sentence Classification task..."")\n        else:\n            print(""     Start   Sequence   Laebling   task..."")\n        print(""     Tag          scheme: %s""%(self.tagScheme))\n        print(""     Split         token: %s""%(self.split_token))\n        print(""     MAX SENTENCE LENGTH: %s""%(self.MAX_SENTENCE_LENGTH))\n        print(""     MAX   WORD   LENGTH: %s""%(self.MAX_WORD_LENGTH))\n        print(""     Number   normalized: %s""%(self.number_normalized))\n        print(""     Word  alphabet size: %s""%(self.word_alphabet_size))\n        print(""     Char  alphabet size: %s""%(self.char_alphabet_size))\n        print(""     Label alphabet size: %s""%(self.label_alphabet_size))\n        print(""     Word embedding  dir: %s""%(self.word_emb_dir))\n        print(""     Char embedding  dir: %s""%(self.char_emb_dir))\n        print(""     Word embedding size: %s""%(self.word_emb_dim))\n        print(""     Char embedding size: %s""%(self.char_emb_dim))\n        print(""     Norm   word     emb: %s""%(self.norm_word_emb))\n        print(""     Norm   char     emb: %s""%(self.norm_char_emb))\n        print(""     Train  file directory: %s""%(self.train_dir))\n        print(""     Dev    file directory: %s""%(self.dev_dir))\n        print(""     Test   file directory: %s""%(self.test_dir))\n        print(""     Raw    file directory: %s""%(self.raw_dir))\n        print(""     Dset   file directory: %s""%(self.dset_dir))\n        print(""     Model  file directory: %s""%(self.model_dir))\n        print(""     Loadmodel   directory: %s""%(self.load_model_dir))\n        print(""     Decode file directory: %s""%(self.decode_dir))\n        print(""     Train instance number: %s""%(len(self.train_texts)))\n        print(""     Dev   instance number: %s""%(len(self.dev_texts)))\n        print(""     Test  instance number: %s""%(len(self.test_texts)))\n        print(""     Raw   instance number: %s""%(len(self.raw_texts)))\n        print(""     FEATURE num: %s""%(self.feature_num))\n        for idx in range(self.feature_num):\n            print(""         Fe: %s  alphabet  size: %s""%(self.feature_alphabets[idx].name, self.feature_alphabet_sizes[idx]))\n            print(""         Fe: %s  embedding  dir: %s""%(self.feature_alphabets[idx].name, self.feature_emb_dirs[idx]))\n            print(""         Fe: %s  embedding size: %s""%(self.feature_alphabets[idx].name, self.feature_emb_dims[idx]))\n            print(""         Fe: %s  norm       emb: %s""%(self.feature_alphabets[idx].name, self.norm_feature_embs[idx]))\n        print("" ""+""++""*20)\n        print("" Model Network:"")\n        print(""     Model        use_crf: %s""%(self.use_crf))\n        print(""     Model word extractor: %s""%(self.word_feature_extractor))\n        print(""     Model       use_char: %s""%(self.use_char))\n        if self.use_char:\n            print(""     Model char extractor: %s""%(self.char_feature_extractor))\n            print(""     Model char_hidden_dim: %s""%(self.HP_char_hidden_dim))\n        print("" ""+""++""*20)\n        print("" Training:"")\n        print(""     Optimizer: %s""%(self.optimizer))\n        print(""     Iteration: %s""%(self.HP_iteration))\n        print(""     BatchSize: %s""%(self.HP_batch_size))\n        print(""     Average  batch   loss: %s""%(self.average_batch_loss))\n\n        print("" ""+""++""*20)\n        print("" Hyperparameters:"")\n\n        print(""     Hyper              lr: %s""%(self.HP_lr))\n        print(""     Hyper        lr_decay: %s""%(self.HP_lr_decay))\n        print(""     Hyper         HP_clip: %s""%(self.HP_clip))\n        print(""     Hyper        momentum: %s""%(self.HP_momentum))\n        print(""     Hyper              l2: %s""%(self.HP_l2))\n        print(""     Hyper      hidden_dim: %s""%(self.HP_hidden_dim))\n        print(""     Hyper         dropout: %s""%(self.HP_dropout))\n        print(""     Hyper      lstm_layer: %s""%(self.HP_lstm_layer))\n        print(""     Hyper          bilstm: %s""%(self.HP_bilstm))\n        print(""     Hyper             GPU: %s""%(self.HP_gpu))\n        print(""DATA SUMMARY END."")\n        print(""++""*50)\n        sys.stdout.flush()\n\n\n    def initial_feature_alphabets(self):\n        if self.sentence_classification:\n            ## if sentence classification data format, splited by \'\\t\'\n            items = open(self.train_dir,\'r\').readline().strip(\'\\n\').split(\'\\t\')\n        else:\n            ## if sequence labeling data format i.e. CoNLL 2003, split by \' \'\n            items = open(self.train_dir,\'r\').readline().strip(\'\\n\').split()\n        total_column = len(items)\n        if total_column > 2:\n            for idx in range(1, total_column-1):\n                feature_prefix = items[idx].split(\']\',1)[0]+""]""\n                self.feature_alphabets.append(Alphabet(feature_prefix))\n                self.feature_name.append(feature_prefix)\n                print(""Find feature: "", feature_prefix)\n        self.feature_num = len(self.feature_alphabets)\n        self.pretrain_feature_embeddings = [None]*self.feature_num\n        self.feature_emb_dims = [20]*self.feature_num\n        self.feature_emb_dirs = [None]*self.feature_num\n        self.norm_feature_embs = [False]*self.feature_num\n        self.feature_alphabet_sizes = [0]*self.feature_num\n        if self.feat_config:\n            for idx in range(self.feature_num):\n                if self.feature_name[idx] in self.feat_config:\n                    self.feature_emb_dims[idx] = self.feat_config[self.feature_name[idx]][\'emb_size\']\n                    self.feature_emb_dirs[idx] = self.feat_config[self.feature_name[idx]][\'emb_dir\']\n                    self.norm_feature_embs[idx] = self.feat_config[self.feature_name[idx]][\'emb_norm\']\n        # exit(0)\n\n\n    def build_alphabet(self, input_file):\n        in_lines = open(input_file,\'r\').readlines()\n        for line in in_lines:\n            if len(line) > 2:\n                ## if sentence classification data format, splited by \\t\n                if self.sentence_classification:\n                    pairs = line.strip().split(self.split_token)\n                    sent = pairs[0]\n                    if sys.version_info[0] < 3:\n                        sent = sent.decode(\'utf-8\')\n                    words = sent.split()\n                    for word in words:\n                        if self.number_normalized:\n                            word = normalize_word(word)\n                        self.word_alphabet.add(word)\n                        for char in word:\n                            self.char_alphabet.add(char)\n                    label = pairs[-1]\n                    self.label_alphabet.add(label)\n                    ## build feature alphabet\n                    for idx in range(self.feature_num):\n                        feat_idx = pairs[idx+1].split(\']\',1)[-1]\n                        self.feature_alphabets[idx].add(feat_idx)\n\n                ## if sequence labeling data format i.e. CoNLL 2003\n                else:\n                    pairs = line.strip().split()\n                    word = pairs[0]\n                    if sys.version_info[0] < 3:\n                        word = word.decode(\'utf-8\')\n                    if self.number_normalized:\n                        word = normalize_word(word)\n                    label = pairs[-1]\n                    self.label_alphabet.add(label)\n                    self.word_alphabet.add(word)\n                    ## build feature alphabet\n                    for idx in range(self.feature_num):\n                        feat_idx = pairs[idx+1].split(\']\',1)[-1]\n                        self.feature_alphabets[idx].add(feat_idx)\n                    for char in word:\n                        self.char_alphabet.add(char)\n        self.word_alphabet_size = self.word_alphabet.size()\n        self.char_alphabet_size = self.char_alphabet.size()\n        self.label_alphabet_size = self.label_alphabet.size()\n        for idx in range(self.feature_num):\n            self.feature_alphabet_sizes[idx] = self.feature_alphabets[idx].size()\n        startS = False\n        startB = False\n        for label,_ in self.label_alphabet.iteritems():\n            if ""S-"" in label.upper():\n                startS = True\n            elif ""B-"" in label.upper():\n                startB = True\n        if startB:\n            if startS:\n                self.tagScheme = ""BMES""\n            else:\n                self.tagScheme = ""BIO""\n        if self.sentence_classification:\n            self.tagScheme = ""Not sequence labeling task""\n\n\n    def fix_alphabet(self):\n        self.word_alphabet.close()\n        self.char_alphabet.close()\n        self.label_alphabet.close()\n        for idx in range(self.feature_num):\n            self.feature_alphabets[idx].close()\n\n\n    def build_pretrain_emb(self):\n        if self.word_emb_dir:\n            print(""Load pretrained word embedding, norm: %s, dir: %s""%(self.norm_word_emb, self.word_emb_dir))\n            self.pretrain_word_embedding, self.word_emb_dim = build_pretrain_embedding(self.word_emb_dir, self.word_alphabet, self.word_emb_dim, self.norm_word_emb)\n        if self.char_emb_dir:\n            print(""Load pretrained char embedding, norm: %s, dir: %s""%(self.norm_char_emb, self.char_emb_dir))\n            self.pretrain_char_embedding, self.char_emb_dim = build_pretrain_embedding(self.char_emb_dir, self.char_alphabet, self.char_emb_dim, self.norm_char_emb)\n        for idx in range(self.feature_num):\n            if self.feature_emb_dirs[idx]:\n                print(""Load pretrained feature %s embedding:, norm: %s, dir: %s""%(self.feature_name[idx], self.norm_feature_embs[idx], self.feature_emb_dirs[idx]))\n                self.pretrain_feature_embeddings[idx], self.feature_emb_dims[idx] = build_pretrain_embedding(self.feature_emb_dirs[idx], self.feature_alphabets[idx], self.feature_emb_dims[idx], self.norm_feature_embs[idx])\n\n\n    def generate_instance(self, name):\n        self.fix_alphabet()\n        if name == ""train"":\n            self.train_texts, self.train_Ids = read_instance(self.train_dir, self.word_alphabet, self.char_alphabet, self.feature_alphabets, self.label_alphabet, self.number_normalized, self.MAX_SENTENCE_LENGTH, self.sentence_classification, self.split_token)\n        elif name == ""dev"":\n            self.dev_texts, self.dev_Ids = read_instance(self.dev_dir, self.word_alphabet, self.char_alphabet, self.feature_alphabets, self.label_alphabet, self.number_normalized, self.MAX_SENTENCE_LENGTH, self.sentence_classification, self.split_token)\n        elif name == ""test"":\n            self.test_texts, self.test_Ids = read_instance(self.test_dir, self.word_alphabet, self.char_alphabet, self.feature_alphabets, self.label_alphabet, self.number_normalized, self.MAX_SENTENCE_LENGTH, self.sentence_classification, self.split_token)\n        elif name == ""raw"":\n            self.raw_texts, self.raw_Ids = read_instance(self.raw_dir, self.word_alphabet, self.char_alphabet, self.feature_alphabets, self.label_alphabet, self.number_normalized, self.MAX_SENTENCE_LENGTH, self.sentence_classification, self.split_token)\n        else:\n            print(""Error: you can only generate train/dev/test instance! Illegal input:%s""%(name))\n\n\n    def write_decoded_results(self, predict_results, name):\n        \n        sent_num = len(predict_results)\n        content_list = []\n        if name == \'raw\':\n           content_list = self.raw_texts\n        elif name == \'test\':\n            content_list = self.test_texts\n        elif name == \'dev\':\n            content_list = self.dev_texts\n        elif name == \'train\':\n            content_list = self.train_texts\n        else:\n            print(""Error: illegal name during writing predict result, name should be within train/dev/test/raw !"")\n        assert(sent_num == len(content_list))\n        fout = open(self.decode_dir,\'w\')\n        for idx in range(sent_num):\n            if self.sentence_classification:\n                fout.write("" "".join(content_list[idx][0])+""\\t""+predict_results[idx]+ \'\\n\')\n            else:\n                sent_length = len(predict_results[idx])\n                for idy in range(sent_length):\n                    ## content_list[idx] is a list with [word, char, label]\n                    fout.write(content_list[idx][0][idy].encode(\'utf-8\') + "" "" + predict_results[idx][idy] + \'\\n\')\n                fout.write(\'\\n\')\n        fout.close()\n        print(""Predict %s result has been written into file. %s""%(name, self.decode_dir))\n\n\n    def load(self,data_file):\n        f = open(data_file, \'rb\')\n        tmp_dict = pickle.load(f)\n        f.close()\n        self.__dict__.update(tmp_dict)\n\n    def save(self,save_file):\n        f = open(save_file, \'wb\')\n        pickle.dump(self.__dict__, f, 2)\n        f.close()\n\n\n\n    def write_nbest_decoded_results(self, predict_results, pred_scores, name):\n        ## predict_results : [whole_sent_num, nbest, each_sent_length]\n        ## pred_scores: [whole_sent_num, nbest]\n        fout = open(self.decode_dir,\'w\')\n        sent_num = len(predict_results)\n        content_list = []\n        if name == \'raw\':\n           content_list = self.raw_texts\n        elif name == \'test\':\n            content_list = self.test_texts\n        elif name == \'dev\':\n            content_list = self.dev_texts\n        elif name == \'train\':\n            content_list = self.train_texts\n        else:\n            print(""Error: illegal name during writing predict result, name should be within train/dev/test/raw !"")\n        assert(sent_num == len(content_list))\n        assert(sent_num == len(pred_scores))\n        for idx in range(sent_num):\n            sent_length = len(predict_results[idx][0])\n            nbest = len(predict_results[idx])\n            score_string = ""# ""\n            for idz in range(nbest):\n                score_string += format(pred_scores[idx][idz], \'.4f\')+"" ""\n            fout.write(score_string.strip() + ""\\n"")\n\n            for idy in range(sent_length):\n                try:  # Will fail with python3\n                    label_string = content_list[idx][0][idy].encode(\'utf-8\') + "" ""\n                except:\n                    label_string = content_list[idx][0][idy] + "" ""\n                for idz in range(nbest):\n                    label_string += predict_results[idx][idz][idy]+"" ""\n                label_string = label_string.strip() + ""\\n""\n                fout.write(label_string)\n            fout.write(\'\\n\')\n        fout.close()\n        print(""Predict %s %s-best result has been written into file. %s""%(name,nbest, self.decode_dir))\n\n\n    def read_config(self,config_file):\n        config = config_file_to_dict(config_file)\n        ## read data:\n        the_item = \'train_dir\'\n        if the_item in config:\n            self.train_dir = config[the_item]\n        the_item = \'dev_dir\'\n        if the_item in config:\n            self.dev_dir = config[the_item]\n        the_item = \'test_dir\'\n        if the_item in config:\n            self.test_dir = config[the_item]\n        the_item = \'raw_dir\'\n        if the_item in config:\n            self.raw_dir = config[the_item]\n        the_item = \'decode_dir\'\n        if the_item in config:\n            self.decode_dir = config[the_item]\n        the_item = \'dset_dir\'\n        if the_item in config:\n            self.dset_dir = config[the_item]\n        the_item = \'model_dir\'\n        if the_item in config:\n            self.model_dir = config[the_item]\n        the_item = \'load_model_dir\'\n        if the_item in config:\n            self.load_model_dir = config[the_item]\n\n        the_item = \'word_emb_dir\'\n        if the_item in config:\n            self.word_emb_dir = config[the_item]\n        the_item = \'char_emb_dir\'\n        if the_item in config:\n            self.char_emb_dir = config[the_item]\n\n\n        the_item = \'MAX_SENTENCE_LENGTH\'\n        if the_item in config:\n            self.MAX_SENTENCE_LENGTH = int(config[the_item])\n        the_item = \'MAX_WORD_LENGTH\'\n        if the_item in config:\n            self.MAX_WORD_LENGTH = int(config[the_item])\n\n        the_item = \'norm_word_emb\'\n        if the_item in config:\n            self.norm_word_emb = str2bool(config[the_item])\n        the_item = \'norm_char_emb\'\n        if the_item in config:\n            self.norm_char_emb = str2bool(config[the_item])\n        the_item = \'number_normalized\'\n        if the_item in config:\n            self.number_normalized = str2bool(config[the_item])\n\n        the_item = \'sentence_classification\'\n        if the_item in config:\n            self.sentence_classification = str2bool(config[the_item])\n        the_item = \'seg\'\n        if the_item in config:\n            self.seg = str2bool(config[the_item])\n        the_item = \'word_emb_dim\'\n        if the_item in config:\n            self.word_emb_dim = int(config[the_item])\n        the_item = \'char_emb_dim\'\n        if the_item in config:\n            self.char_emb_dim = int(config[the_item])\n\n        ## read network:\n        the_item = \'use_crf\'\n        if the_item in config:\n            self.use_crf = str2bool(config[the_item])\n        the_item = \'use_char\'\n        if the_item in config:\n            self.use_char = str2bool(config[the_item])\n        the_item = \'word_seq_feature\'\n        if the_item in config:\n            self.word_feature_extractor = config[the_item]\n        the_item = \'char_seq_feature\'\n        if the_item in config:\n            self.char_feature_extractor = config[the_item]\n        the_item = \'nbest\'\n        if the_item in config:\n            self.nbest = int(config[the_item])\n\n        the_item = \'feature\'\n        if the_item in config:\n            self.feat_config = config[the_item] ## feat_config is a dict\n\n\n        ## read training setting:\n        the_item = \'optimizer\'\n        if the_item in config:\n            self.optimizer = config[the_item]\n        the_item = \'ave_batch_loss\'\n        if the_item in config:\n            self.average_batch_loss = str2bool(config[the_item])\n        the_item = \'status\'\n        if the_item in config:\n            self.status = config[the_item]\n\n        ## read Hyperparameters:\n        the_item = \'cnn_layer\'\n        if the_item in config:\n            self.HP_cnn_layer = int(config[the_item])\n        the_item = \'iteration\'\n        if the_item in config:\n            self.HP_iteration = int(config[the_item])\n        the_item = \'batch_size\'\n        if the_item in config:\n            self.HP_batch_size = int(config[the_item])\n\n        the_item = \'char_hidden_dim\'\n        if the_item in config:\n            self.HP_char_hidden_dim = int(config[the_item])\n        the_item = \'hidden_dim\'\n        if the_item in config:\n            self.HP_hidden_dim = int(config[the_item])\n        the_item = \'dropout\'\n        if the_item in config:\n            self.HP_dropout = float(config[the_item])\n        the_item = \'lstm_layer\'\n        if the_item in config:\n            self.HP_lstm_layer = int(config[the_item])\n        the_item = \'bilstm\'\n        if the_item in config:\n            self.HP_bilstm = str2bool(config[the_item])\n\n        the_item = \'gpu\'\n        if the_item in config:\n            self.HP_gpu = str2bool(config[the_item])\n        the_item = \'learning_rate\'\n        if the_item in config:\n            self.HP_lr = float(config[the_item])\n        the_item = \'lr_decay\'\n        if the_item in config:\n            self.HP_lr_decay = float(config[the_item])\n        the_item = \'clip\'\n        if the_item in config:\n            self.HP_clip = float(config[the_item])\n        the_item = \'momentum\'\n        if the_item in config:\n            self.HP_momentum = float(config[the_item])\n        the_item = \'l2\'\n        if the_item in config:\n            self.HP_l2 = float(config[the_item])\n        ## no seg for sentence classification\n        if self.sentence_classification:\n            self.seg = False\n            self.use_crf = False\n\n\ndef config_file_to_dict(input_file):\n    config = {}\n    fins = open(input_file,\'r\').readlines()\n    for line in fins:\n        if len(line) > 0 and line[0] == ""#"":\n            continue\n        if ""="" in line:\n            pair = line.strip().split(\'#\',1)[0].split(\'=\',1)\n            item = pair[0]\n            if item==""feature"":\n                if item not in config:\n                    feat_dict = {}\n                    config[item]= feat_dict\n                feat_dict = config[item]\n                new_pair = pair[-1].split()\n                feat_name = new_pair[0]\n                one_dict = {}\n                one_dict[""emb_dir""] = None\n                one_dict[""emb_size""] = 10\n                one_dict[""emb_norm""] = False\n                if len(new_pair) > 1:\n                    for idx in range(1,len(new_pair)):\n                        conf_pair = new_pair[idx].split(\'=\')\n                        if conf_pair[0] == ""emb_dir"":\n                            one_dict[""emb_dir""]=conf_pair[-1]\n                        elif conf_pair[0] == ""emb_size"":\n                            one_dict[""emb_size""]=int(conf_pair[-1])\n                        elif conf_pair[0] == ""emb_norm"":\n                            one_dict[""emb_norm""]=str2bool(conf_pair[-1])\n                feat_dict[feat_name] = one_dict\n                # print ""feat"",feat_dict\n            else:\n                if item in config:\n                    print(""Warning: duplicated config item found: %s, updated.""%(pair[0]))\n                config[item] = pair[-1]\n\n\n    return config\n\n\ndef str2bool(string):\n    if string == ""True"" or string == ""true"" or string == ""TRUE"":\n        return True\n    else:\n        return False\n'"
utils/functions.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Jie\n# @Date:   2017-06-15 14:23:06\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2019-02-14 12:23:52\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport sys\nimport numpy as np\n\ndef normalize_word(word):\n    new_word = """"\n    for char in word:\n        if char.isdigit():\n            new_word += \'0\'\n        else:\n            new_word += char\n    return new_word\n\n\ndef read_instance(input_file, word_alphabet, char_alphabet, feature_alphabets, label_alphabet, number_normalized, max_sent_length, sentence_classification=False, split_token=\'\\t\', char_padding_size=-1, char_padding_symbol = \'</pad>\'):\n    feature_num = len(feature_alphabets)\n    in_lines = open(input_file,\'r\', encoding=""utf8"").readlines()\n    instence_texts = []\n    instence_Ids = []\n    words = []\n    features = []\n    chars = []\n    labels = []\n    word_Ids = []\n    feature_Ids = []\n    char_Ids = []\n    label_Ids = []\n\n    ## if sentence classification data format, splited by \\t\n    if sentence_classification:\n        for line in in_lines:\n            if len(line) > 2:\n                pairs = line.strip().split(split_token)\n                sent = pairs[0]\n                if sys.version_info[0] < 3:\n                    sent = sent.decode(\'utf-8\')\n                original_words = sent.split()\n                for word in original_words:\n                    words.append(word)\n                    if number_normalized:\n                        word = normalize_word(word)\n                    word_Ids.append(word_alphabet.get_index(word))\n                    ## get char\n                    char_list = []\n                    char_Id = []\n                    for char in word:\n                        char_list.append(char)\n                    if char_padding_size > 0:\n                        char_number = len(char_list)\n                        if char_number < char_padding_size:\n                            char_list = char_list + [char_padding_symbol]*(char_padding_size-char_number)\n                        assert(len(char_list) == char_padding_size)\n                    for char in char_list:\n                        char_Id.append(char_alphabet.get_index(char))\n                    chars.append(char_list)\n                    char_Ids.append(char_Id)\n\n                label = pairs[-1]\n                label_Id = label_alphabet.get_index(label)\n                ## get features\n                feat_list = []\n                feat_Id = []\n                for idx in range(feature_num):\n                    feat_idx = pairs[idx+1].split(\']\',1)[-1]\n                    feat_list.append(feat_idx)\n                    feat_Id.append(feature_alphabets[idx].get_index(feat_idx))\n                ## combine together and return, notice the feature/label as different format with sequence labeling task\n                if (len(words) > 0) and ((max_sent_length < 0) or (len(words) < max_sent_length)):\n                    instence_texts.append([words, feat_list, chars, label])\n                    instence_Ids.append([word_Ids, feat_Id, char_Ids,label_Id])\n                words = []\n                features = []\n                chars = []\n                char_Ids = []\n                word_Ids = []\n                feature_Ids = []\n                label_Ids = []\n        if (len(words) > 0) and ((max_sent_length < 0) or (len(words) < max_sent_length)) :\n            instence_texts.append([words, feat_list, chars, label])\n            instence_Ids.append([word_Ids, feat_Id, char_Ids,label_Id])\n            words = []\n            features = []\n            chars = []\n            char_Ids = []\n            word_Ids = []\n            feature_Ids = []\n            label_Ids = []\n\n    else:\n    ### for sequence labeling data format i.e. CoNLL 2003\n        for line in in_lines:\n            if len(line) > 2:\n                pairs = line.strip().split()\n                word = pairs[0]\n                if sys.version_info[0] < 3:\n                    word = word.decode(\'utf-8\')\n                words.append(word)\n                if number_normalized:\n                    word = normalize_word(word)\n                label = pairs[-1]\n                labels.append(label)\n                word_Ids.append(word_alphabet.get_index(word))\n                label_Ids.append(label_alphabet.get_index(label))\n                ## get features\n                feat_list = []\n                feat_Id = []\n                for idx in range(feature_num):\n                    feat_idx = pairs[idx+1].split(\']\',1)[-1]\n                    feat_list.append(feat_idx)\n                    feat_Id.append(feature_alphabets[idx].get_index(feat_idx))\n                features.append(feat_list)\n                feature_Ids.append(feat_Id)\n                ## get char\n                char_list = []\n                char_Id = []\n                for char in word:\n                    char_list.append(char)\n                if char_padding_size > 0:\n                    char_number = len(char_list)\n                    if char_number < char_padding_size:\n                        char_list = char_list + [char_padding_symbol]*(char_padding_size-char_number)\n                    assert(len(char_list) == char_padding_size)\n                else:\n                    ### not padding\n                    pass\n                for char in char_list:\n                    char_Id.append(char_alphabet.get_index(char))\n                chars.append(char_list)\n                char_Ids.append(char_Id)\n            else:\n                if (len(words) > 0) and ((max_sent_length < 0) or (len(words) < max_sent_length)) :\n                    instence_texts.append([words, features, chars, labels])\n                    instence_Ids.append([word_Ids, feature_Ids, char_Ids,label_Ids])\n                words = []\n                features = []\n                chars = []\n                labels = []\n                word_Ids = []\n                feature_Ids = []\n                char_Ids = []\n                label_Ids = []\n        if (len(words) > 0) and ((max_sent_length < 0) or (len(words) < max_sent_length)) :\n            instence_texts.append([words, features, chars, labels])\n            instence_Ids.append([word_Ids, feature_Ids, char_Ids,label_Ids])\n            words = []\n            features = []\n            chars = []\n            labels = []\n            word_Ids = []\n            feature_Ids = []\n            char_Ids = []\n            label_Ids = []\n    return instence_texts, instence_Ids\n\n\ndef build_pretrain_embedding(embedding_path, word_alphabet, embedd_dim=100, norm=True):\n    embedd_dict = dict()\n    if embedding_path != None:\n        embedd_dict, embedd_dim = load_pretrain_emb(embedding_path)\n    alphabet_size = word_alphabet.size()\n    scale = np.sqrt(3.0 / embedd_dim)\n    pretrain_emb = np.empty([word_alphabet.size(), embedd_dim])\n    perfect_match = 0\n    case_match = 0\n    not_match = 0\n    for word, index in word_alphabet.iteritems():\n        if word in embedd_dict:\n            if norm:\n                pretrain_emb[index,:] = norm2one(embedd_dict[word])\n            else:\n                pretrain_emb[index,:] = embedd_dict[word]\n            perfect_match += 1\n        elif word.lower() in embedd_dict:\n            if norm:\n                pretrain_emb[index,:] = norm2one(embedd_dict[word.lower()])\n            else:\n                pretrain_emb[index,:] = embedd_dict[word.lower()]\n            case_match += 1\n        else:\n            pretrain_emb[index,:] = np.random.uniform(-scale, scale, [1, embedd_dim])\n            not_match += 1\n    pretrained_size = len(embedd_dict)\n    print(""Embedding:\\n     pretrain word:%s, prefect match:%s, case_match:%s, oov:%s, oov%%:%s""%(pretrained_size, perfect_match, case_match, not_match, (not_match+0.)/alphabet_size))\n    return pretrain_emb, embedd_dim\n\ndef norm2one(vec):\n    root_sum_square = np.sqrt(np.sum(np.square(vec)))\n    return vec/root_sum_square\n\ndef load_pretrain_emb(embedding_path):\n    embedd_dim = -1\n    embedd_dict = dict()\n    with open(embedding_path, \'r\', encoding=""utf8"") as file:\n        for line in file:\n            line = line.strip()\n            if len(line) == 0:\n                continue\n            tokens = line.split()\n            if embedd_dim < 0:\n                embedd_dim = len(tokens) - 1\n            elif embedd_dim + 1 != len(tokens):\n                ## ignore illegal embedding line\n                continue\n                # assert (embedd_dim + 1 == len(tokens))\n            embedd = np.empty([1, embedd_dim])\n            embedd[:] = tokens[1:]\n            if sys.version_info[0] < 3:\n                first_col = tokens[0].decode(\'utf-8\')\n            else:\n                first_col = tokens[0]\n            embedd_dict[first_col] = embedd\n    return embedd_dict, embedd_dim\n\nif __name__ == \'__main__\':\n    a = np.arange(9.0)\n    print(a)\n    print(norm2one(a))\n'"
utils/metric.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Jie\n# @Date:   2017-02-16 09:53:19\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2019-02-17 22:46:59\n\n# from operator import add\n#\nfrom __future__ import print_function\nimport sys\n\n\n\n## input as sentence level labels\ndef get_ner_fmeasure(golden_lists, predict_lists, label_type=""BMES""):\n    sent_num = len(golden_lists)\n    golden_full = []\n    predict_full = []\n    right_full = []\n    right_tag = 0\n    all_tag = 0\n    for idx in range(0,sent_num):\n        # word_list = sentence_lists[idx]\n        golden_list = golden_lists[idx]\n        predict_list = predict_lists[idx]\n        for idy in range(len(golden_list)):\n            if golden_list[idy] == predict_list[idy]:\n                right_tag += 1\n        all_tag += len(golden_list)\n        if label_type == ""BMES"" or label_type == ""BIOES"":\n            gold_matrix = get_ner_BMES(golden_list)\n            pred_matrix = get_ner_BMES(predict_list)\n        else:\n            gold_matrix = get_ner_BIO(golden_list)\n            pred_matrix = get_ner_BIO(predict_list)\n        # print ""gold"", gold_matrix\n        # print ""pred"", pred_matrix\n        right_ner = list(set(gold_matrix).intersection(set(pred_matrix)))\n        golden_full += gold_matrix\n        predict_full += pred_matrix\n        right_full += right_ner\n    right_num = len(right_full)\n    golden_num = len(golden_full)\n    predict_num = len(predict_full)\n    if predict_num == 0:\n        precision = -1\n    else:\n        precision =  (right_num+0.0)/predict_num\n    if golden_num == 0:\n        recall = -1\n    else:\n        recall = (right_num+0.0)/golden_num\n    if (precision == -1) or (recall == -1) or (precision+recall) <= 0.:\n        f_measure = -1\n    else:\n        f_measure = 2*precision*recall/(precision+recall)\n    accuracy = (right_tag+0.0)/all_tag\n    # print ""Accuracy: "", right_tag,""/"",all_tag,""="",accuracy\n    if  label_type.upper().startswith(""B-""):\n        print(""gold_num = "", golden_num, "" pred_num = "", predict_num, "" right_num = "", right_num)\n    else:\n        print(""Right token = "", right_tag, "" All token = "", all_tag, "" acc = "", accuracy)\n    return accuracy, precision, recall, f_measure\n\n\ndef reverse_style(input_string):\n    target_position = input_string.index(\'[\')\n    input_len = len(input_string)\n    output_string = input_string[target_position:input_len] + input_string[0:target_position]\n    return output_string\n\n\ndef get_ner_BMES(label_list):\n    # list_len = len(word_list)\n    # assert(list_len == len(label_list)), ""word list size unmatch with label list""\n    list_len = len(label_list)\n    begin_label = \'B-\'\n    end_label = \'E-\'\n    single_label = \'S-\'\n    whole_tag = \'\'\n    index_tag = \'\'\n    tag_list = []\n    stand_matrix = []\n    for i in range(0, list_len):\n        # wordlabel = word_list[i]\n        current_label = label_list[i].upper()\n        if begin_label in current_label:\n            if index_tag != \'\':\n                tag_list.append(whole_tag + \',\' + str(i-1))\n            whole_tag = current_label.replace(begin_label,"""",1) +\'[\' +str(i)\n            index_tag = current_label.replace(begin_label,"""",1)\n\n        elif single_label in current_label:\n            if index_tag != \'\':\n                tag_list.append(whole_tag + \',\' + str(i-1))\n            whole_tag = current_label.replace(single_label,"""",1) +\'[\' +str(i)\n            tag_list.append(whole_tag)\n            whole_tag = """"\n            index_tag = """"\n        elif end_label in current_label:\n            if index_tag != \'\':\n                tag_list.append(whole_tag +\',\' + str(i))\n            whole_tag = \'\'\n            index_tag = \'\'\n        else:\n            continue\n    if (whole_tag != \'\')&(index_tag != \'\'):\n        tag_list.append(whole_tag)\n    tag_list_len = len(tag_list)\n\n    for i in range(0, tag_list_len):\n        if  len(tag_list[i]) > 0:\n            tag_list[i] = tag_list[i]+ \']\'\n            insert_list = reverse_style(tag_list[i])\n            stand_matrix.append(insert_list)\n    # print stand_matrix\n    return stand_matrix\n\n\ndef get_ner_BIO(label_list):\n    # list_len = len(word_list)\n    # assert(list_len == len(label_list)), ""word list size unmatch with label list""\n    list_len = len(label_list)\n    begin_label = \'B-\'\n    inside_label = \'I-\'\n    whole_tag = \'\'\n    index_tag = \'\'\n    tag_list = []\n    stand_matrix = []\n    for i in range(0, list_len):\n        # wordlabel = word_list[i]\n        current_label = label_list[i].upper()\n        if begin_label in current_label:\n            if index_tag == \'\':\n                whole_tag = current_label.replace(begin_label,"""",1) +\'[\' +str(i)\n                index_tag = current_label.replace(begin_label,"""",1)\n            else:\n                tag_list.append(whole_tag + \',\' + str(i-1))\n                whole_tag = current_label.replace(begin_label,"""",1)  + \'[\' + str(i)\n                index_tag = current_label.replace(begin_label,"""",1)\n\n        elif inside_label in current_label:\n            if current_label.replace(inside_label,"""",1) == index_tag:\n                whole_tag = whole_tag\n            else:\n                if (whole_tag != \'\')&(index_tag != \'\'):\n                    tag_list.append(whole_tag +\',\' + str(i-1))\n                whole_tag = \'\'\n                index_tag = \'\'\n        else:\n            if (whole_tag != \'\')&(index_tag != \'\'):\n                tag_list.append(whole_tag +\',\' + str(i-1))\n            whole_tag = \'\'\n            index_tag = \'\'\n\n    if (whole_tag != \'\')&(index_tag != \'\'):\n        tag_list.append(whole_tag)\n    tag_list_len = len(tag_list)\n\n    for i in range(0, tag_list_len):\n        if  len(tag_list[i]) > 0:\n            tag_list[i] = tag_list[i]+ \']\'\n            insert_list = reverse_style(tag_list[i])\n            stand_matrix.append(insert_list)\n    return stand_matrix\n\n\n\ndef readSentence(input_file):\n    in_lines = open(input_file,\'r\').readlines()\n    sentences = []\n    labels = []\n    sentence = []\n    label = []\n    for line in in_lines:\n        if len(line) < 2:\n            sentences.append(sentence)\n            labels.append(label)\n            sentence = []\n            label = []\n        else:\n            pair = line.strip(\'\\n\').split(\' \')\n            sentence.append(pair[0])\n            label.append(pair[-1])\n    return sentences,labels\n\n\ndef readTwoLabelSentence(input_file, pred_col=-1):\n    in_lines = open(input_file,\'r\').readlines()\n    sentences = []\n    predict_labels = []\n    golden_labels = []\n    sentence = []\n    predict_label = []\n    golden_label = []\n    for line in in_lines:\n        if ""##score##"" in line:\n            continue\n        if len(line) < 2:\n            sentences.append(sentence)\n            golden_labels.append(golden_label)\n            predict_labels.append(predict_label)\n            sentence = []\n            golden_label = []\n            predict_label = []\n        else:\n            pair = line.strip(\'\\n\').split(\' \')\n            sentence.append(pair[0])\n            golden_label.append(pair[1])\n            predict_label.append(pair[pred_col])\n\n    return sentences,golden_labels,predict_labels\n\n\ndef fmeasure_from_file(golden_file, predict_file, label_type=""BMES""):\n    print(""Get f measure from file:"", golden_file, predict_file)\n    print(""Label format:"",label_type)\n    golden_sent,golden_labels = readSentence(golden_file)\n    predict_sent,predict_labels = readSentence(predict_file)\n    P,R,F = get_ner_fmeasure(golden_labels, predict_labels, label_type)\n    print (""P:%sm R:%s, F:%s""%(P,R,F))\n\n\n\ndef fmeasure_from_singlefile(twolabel_file, label_type=""BMES"", pred_col=-1):\n    sent,golden_labels,predict_labels = readTwoLabelSentence(twolabel_file, pred_col)\n    P,R,F = get_ner_fmeasure(golden_labels, predict_labels, label_type)\n    print (""P:%s, R:%s, F:%s""%(P,R,F))\n\n\n\nif __name__ == \'__main__\':\n    # print ""sys:"",len(sys.argv)\n    if len(sys.argv) == 3:\n        fmeasure_from_singlefile(sys.argv[1],""BMES"",int(sys.argv[2]))\n    else:\n        fmeasure_from_singlefile(sys.argv[1],""BMES"")\n\n'"
utils/tagSchemeConverter.py,0,"b'# -*- coding: utf-8 -*-\r\n# @Author: Jie Yang\r\n# @Date:   2017-11-27 16:53:36\r\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\r\n# @Last Modified time: 2019-01-09 21:39:10\r\n\r\n\r\n""""""\r\n    convert NER/Chunking tag schemes, i.e. BIO->BIOES, BIOES->BIO, IOB->BIO, IOB->BIOES\r\n""""""\r\nfrom __future__ import print_function\r\n\r\nimport sys\r\n\r\n\r\ndef BIO2BIOES(input_file, output_file):\r\n    print(""Convert BIO -> BIOES for file:"", input_file)\r\n    with open(input_file,\'r\') as in_file:\r\n        fins = in_file.readlines()\r\n    fout = open(output_file,\'w\')\r\n    words = []\r\n    labels = []\r\n    for line in fins:\r\n        if len(line) < 3:\r\n            sent_len = len(words)\r\n            for idx in range(sent_len):\r\n                if ""-"" not in labels[idx]:\r\n                    fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\r\n                else:\r\n                    label_type = labels[idx].split(\'-\')[-1]\r\n                    if ""B-"" in labels[idx]:\r\n                        if (idx == sent_len - 1) or (""I-"" not in labels[idx+1]):\r\n                            fout.write(words[idx]+"" S-""+label_type+""\\n"")\r\n                        else:\r\n                            fout.write(words[idx]+"" B-""+label_type+""\\n"")\r\n                    elif ""I-"" in labels[idx]:\r\n                        if (idx == sent_len - 1) or (""I-"" not in labels[idx+1]):\r\n                            fout.write(words[idx]+"" E-""+label_type+""\\n"")\r\n                        else:\r\n                            fout.write(words[idx]+"" I-""+label_type+""\\n"")\r\n            fout.write(\'\\n\')\r\n            words = []\r\n            labels = []\r\n        else:\r\n            pair = line.strip(\'\\n\').split()\r\n            words.append(pair[0])\r\n            labels.append(pair[-1].upper())\r\n    fout.close()\r\n    print(""BIOES file generated:"", output_file)\r\n\r\n\r\n\r\ndef BIOES2BIO(input_file, output_file):\r\n    print(""Convert BIOES -> BIO for file:"", input_file)\r\n    with open(input_file,\'r\') as in_file:\r\n        fins = in_file.readlines()\r\n    fout = open(output_file,\'w\')\r\n    words = []\r\n    labels = []\r\n    for line in fins:\r\n        if len(line) < 3:\r\n            sent_len = len(words)\r\n            for idx in range(sent_len):\r\n                if ""-"" not in labels[idx]:\r\n                    fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\r\n                else:\r\n                    label_type = labels[idx].split(\'-\')[-1]\r\n                    if ""E-"" in labels[idx]:\r\n                        fout.write(words[idx]+"" I-""+label_type+""\\n"")\r\n                    elif ""S-"" in labels[idx]:\r\n                        fout.write(words[idx]+"" B-""+label_type+""\\n"")\r\n                    else:\r\n                        fout.write(words[idx]+"" ""+labels[idx]+""\\n"")     \r\n            fout.write(\'\\n\')\r\n            words = []\r\n            labels = []\r\n        else:\r\n            pair = line.strip(\'\\n\').split()\r\n            words.append(pair[0])\r\n            labels.append(pair[-1].upper())\r\n    fout.close()\r\n    print(""BIO file generated:"", output_file)\r\n\r\n\r\ndef IOB2BIO(input_file, output_file):\r\n    print(""Convert IOB -> BIO for file:"", input_file)\r\n    with open(input_file,\'r\') as in_file:\r\n        fins = in_file.readlines()\r\n    fout = open(output_file,\'w\')\r\n    words = []\r\n    labels = []\r\n    for line in fins:\r\n        if len(line) < 3:\r\n            sent_len = len(words)\r\n            for idx in range(sent_len):\r\n                if ""I-"" in labels[idx]:\r\n                    label_type = labels[idx].split(\'-\')[-1]\r\n                    if (idx == 0) or (labels[idx-1] == ""O"") or (label_type != labels[idx-1].split(\'-\')[-1]):\r\n                        fout.write(words[idx]+"" B-""+label_type+""\\n"")\r\n                    else:\r\n                        fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\r\n                else:\r\n                    fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\r\n            fout.write(\'\\n\')\r\n            words = []\r\n            labels = []\r\n        else:\r\n            pair = line.strip(\'\\n\').split()\r\n            words.append(pair[0])\r\n            labels.append(pair[-1].upper())\r\n    fout.close()\r\n    print(""BIO file generated:"", output_file)\r\n\r\n\r\ndef choose_label(input_file, output_file):\r\n    with open(input_file,\'r\') as in_file:\r\n        fins = in_file.readlines()\r\n    with open(output_file,\'w\') as fout:\r\n        for line in fins:\r\n            if len(line) < 3:\r\n                fout.write(line)\r\n            else:\r\n                pairs = line.strip(\'\\n\').split(\' \')\r\n                fout.write(pairs[0]+"" ""+ pairs[-1]+""\\n"")\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    \'\'\'Convert NER tag schemes among IOB/BIO/BIOES.\r\n        For example: if you want to convert the IOB tag scheme to BIO, then you run as following:\r\n            python tagSchemeConverter.py IOB2BIO input_iob_file output_bio_file\r\n        Input data format is the standard CoNLL 2003 data format.\r\n    \'\'\'\r\n    if sys.argv[1].upper() == ""IOB2BIO"":\r\n        IOB2BIO(sys.argv[2],sys.argv[3])\r\n    elif sys.argv[1].upper() == ""BIO2BIOES"":\r\n        BIO2BIOES(sys.argv[2],sys.argv[3])\r\n    elif sys.argv[1].upper() == ""BIOES2BIO"":\r\n        BIOES2BIO(sys.argv[2],sys.argv[3])\r\n    elif sys.argv[1].upper() == ""IOB2BIOES"":\r\n        IOB2BIO(sys.argv[2],""temp"")\r\n        BIO2BIOES(""temp"",sys.argv[3])\r\n    else:\r\n        print(""Argument error: sys.argv[1] should belongs to \\""IOB2BIO/BIO2BIOES/BIOES2BIO/IOB2BIOES\\"""")\r\n'"
