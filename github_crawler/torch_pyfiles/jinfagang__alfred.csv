file_path,api_count,code
setup.py,6,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\ninstall alfred into local bin dir.\n""""""\nfrom setuptools import setup, find_packages\nfrom setuptools import setup, Extension\nimport io\nfrom os import path\n\nthis_directory = path.abspath(path.dirname(__file__))\nwith io.open(path.join(this_directory, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(name=\'alfred-py\',\n      version=\'2.7.7\',\n      keywords=[\'deep learning\', \'script helper\', \'tools\'],\n      description=\'\'\'\n      Alfred is a DeepLearning utility library.\n      \'\'\',\n      long_description=long_description,\n      long_description_content_type=\'text/markdown\',\n      license=\'Apache 2.0\',\n      packages=[\n          \'alfred\',\n          \'alfred.dl\',\n          \'alfred.dl.inference\',\n          \'alfred.dl.data\',\n          \'alfred.dl.data.common\',\n          \'alfred.dl.data.meta\',\n          \'alfred.dl.torch\',\n          \'alfred.dl.torch.train\',\n          \'alfred.dl.torch.distribute\',\n          \'alfred.dl.torch.runner\',\n          \'alfred.dl.torch.nn\',\n          \'alfred.dl.torch.nn.modules\',\n          \'alfred.dl.torch.ops\',\n          \'alfred.dl.tf\',\n          \'alfred.vis\',\n          \'alfred.modules\',\n          \'alfred.modules.scrap\',\n          \'alfred.modules.text\',\n          \'alfred.modules.vision\',\n          \'alfred.modules.data\',\n          \'alfred.modules.cabinet\',\n          \'alfred.modules\',\n          \'alfred.fusion\',\n          \'alfred.vis.image\',\n          \'alfred.vis.pointcloud\',\n          \'alfred.utils\',\n          \'alfred.protos\'\n      ],\n      # package_dir={\'alfred\': \'alfred\'},\n      entry_points={\n          \'console_scripts\': [\n              \'alfred = alfred.alfred:main\'\n          ]\n      },\n      include_package_data=True,\n      author=""Lucas Jin"",\n      author_email=""jinfagang19@163.com"",\n      url=\'https://github.com/jinfagang/alfred\',\n      platforms=\'any\',\n      install_requires=[\'colorama\', \'requests\', \'regex\',\n                        \'future\', \'deprecated\', \'loguru\', \'pyquaternion\', \'lxml\']\n      )\n'"
alfred/__init__.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""Bring in all of the public Alfred interface into this module.""""""\nimport importlib\n# pylint: disable=g-bad-import-order\n\nfrom .modules import *\nfrom .vis import *\nfrom .fusion import *\nglobals().update(importlib.import_module(\'alfred\').__dict__)'"
alfred/alfred.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nmain entrance of Alfred\n""""""\nimport os\nimport sys\nimport argparse\nfrom colorama import Fore, Back, Style\nimport traceback\n\nfrom .modules.vision.video_extractor import VideoExtractor\nfrom .modules.scrap.image_scraper import ImageScraper\nfrom .modules.vision.to_video import VideoCombiner\nfrom .modules.vision.video_reducer import VideoReducer\n\nfrom .modules.data.view_voc import vis_voc\nfrom .modules.data.view_coco import vis_coco\nfrom .modules.data.view_txt import vis_det_txt\nfrom .modules.data.gather_voclabels import gather_labels\nfrom .modules.data.voc2coco import convert\nfrom .modules.data.eval_voc import eval_voc\n\nfrom .modules.cabinet.count_file import count_file\nfrom .modules.cabinet.split_txt import split_txt_file\nfrom .modules.cabinet.license import apply_license\nfrom .modules.cabinet.stack_imgs import stack_imgs\n\nfrom alfred.utils.log import logger as logging\n\n__VERSION__ = \'2.7.1\'\n__AUTHOR__ = \'Lucas Jin\'\n__DATE__ = \'20202.10.01, since 2019.11.11\'\n__LOC__ = \'Shenzhen, China\'\n__git__ = \'http://github.com/jinfagang/alfred\'\n\n\ndef arg_parse():\n    """"""\n    parse arguments\n    :return:\n    """"""\n    parser = argparse.ArgumentParser(prog=""alfred"")\n    parser.add_argument(\'--version\', \'-v\', action=""store_true"", help=\'show version info.\')\n\n    # vision, text, scrap\n    main_sub_parser = parser.add_subparsers()\n\n    # =============== vision part ================\n    vision_parser = main_sub_parser.add_parser(\'vision\', help=\'vision related commands.\')\n    vision_sub_parser = vision_parser.add_subparsers()\n\n    vision_extract_parser = vision_sub_parser.add_parser(\'extract\', help=\'extract image from video: alfred vision \'\n                                                                         \'extract -v tt.mp4\')\n    vision_extract_parser.set_defaults(which=\'vision-extract\')\n    vision_extract_parser.add_argument(\'--video\', \'-v\', help=\'video to extract\')\n    vision_extract_parser.add_argument(\'--jumps\', \'-j\', help=\'jump frames for wide extract\')\n\n    vision_reduce_parser = vision_sub_parser.add_parser(\'reduce\', help=\'reduce video by drop frames\'\n                                                                       \'\\nalfred vision reduce -v a.mp4 -j 10\')\n    vision_reduce_parser.set_defaults(which=\'vision-reduce\')\n    vision_reduce_parser.add_argument(\'--video\', \'-v\', help=\'video to extract\')\n    vision_reduce_parser.add_argument(\'--jumps\', \'-j\', help=\'jump frames for wide extract\')\n\n    vision_2video_parser = vision_sub_parser.add_parser(\'2video\', help=\'combine into a video: alfred vision \'\n                                                                       \'2video  -d ./images\')\n    vision_2video_parser.set_defaults(which=\'vision-2video\')\n    vision_2video_parser.add_argument(\'--dir\', \'-d\', help=\'dir contains image sequences.\')\n\n    vision_clean_parser = vision_sub_parser.add_parser(\'clean\', help=\'clean images in a dir.\')\n    vision_clean_parser.set_defaults(which=\'vision-clean\')\n    vision_clean_parser.add_argument(\'--dir\', \'-d\', help=\'dir contains images.\')\n\n    vision_getface_parser = vision_sub_parser.add_parser(\'getface\', help=\'get all faces inside an image and save it.\')\n    vision_getface_parser.set_defaults(which=\'vision-getface\')\n    vision_getface_parser.add_argument(\'--dir\', \'-d\', help=\'dir contains images to extract faces.\')\n\n    # =============== text part ================\n    text_parser = main_sub_parser.add_parser(\'text\', help=\'text related commands.\')\n    text_sub_parser = text_parser.add_subparsers()\n\n    text_clean_parser = text_sub_parser.add_parser(\'clean\', help=\'clean text.\')\n    text_clean_parser.set_defaults(which=\'text-clean\')\n    text_clean_parser.add_argument(\'--file\', \'-f\', help=\'file to clean\')\n\n    text_translate_parser = text_sub_parser.add_parser(\'translate\', help=\'translate\')\n    text_translate_parser.set_defaults(which=\'text-translate\')\n    text_translate_parser.add_argument(\'--file\', \'-f\', help=\'translate a words to target language\')\n\n    # =============== scrap part ================\n    scrap_parser = main_sub_parser.add_parser(\'scrap\', help=\'scrap related commands.\')\n    scrap_sub_parser = scrap_parser.add_subparsers()\n\n    scrap_image_parser = scrap_sub_parser.add_parser(\'image\', help=\'scrap images.\')\n    scrap_image_parser.set_defaults(which=\'scrap-image\')\n    scrap_image_parser.add_argument(\'--query\', \'-q\', help=\'query words.\')\n\n    # =============== cabinet part ================\n    cabinet_parser = main_sub_parser.add_parser(\'cab\', help=\'cabinet related commands.\')\n    cabinet_sub_parser = cabinet_parser.add_subparsers()\n\n    count_file_parser = cabinet_sub_parser.add_parser(\'count\', help=\'scrap images.\')\n    count_file_parser.set_defaults(which=\'cab-count\')\n    count_file_parser.add_argument(\'--dir\', \'-d\', default=\'./\', help=\'dir to count.\')\n    count_file_parser.add_argument(\'--type\', \'-t\', help=\'dir to count.\')\n\n    split_txt_parser = cabinet_sub_parser.add_parser(\'split\', help=\'split txt file.\')\n    split_txt_parser.set_defaults(which=\'cab-split\')\n    split_txt_parser.add_argument(\'--file\', \'-f\', required=True, help=\'file to split.\')\n    split_txt_parser.add_argument(\'--ratios\', \'-r\', help=\'ratios.\')\n    split_txt_parser.add_argument(\'--names\', \'-n\', help=\'names.\')\n\n    stackimgs_parser = cabinet_sub_parser.add_parser(\'stackimgs\', help=\'stack images into one\')\n    stackimgs_parser.set_defaults(which=\'cab-stackimgs\')\n    stackimgs_parser.add_argument(\'--imgs\', \'-i\', required=True, nargs=\'+\', help=\'images list.\')\n    stackimgs_parser.add_argument(\'--dim\', \'-d\', help=\'dims like 2x3.\')\n\n    apply_license_parser = cabinet_sub_parser.add_parser(\'license\', help=\'automatically add/update license.\')\n    apply_license_parser.set_defaults(which=\'cab-license\')\n    apply_license_parser.add_argument(\'--owner\', \'-o\', required=True, help=\'owner of license.\')\n    apply_license_parser.add_argument(\'--name\', \'-n\', help=\'project name.\')\n    apply_license_parser.add_argument(\'--year\', \'-y\', help=\'project year: 2016-2020.\')\n    apply_license_parser.add_argument(\'--url\', \'-u\', default=\'manaai.cn\', help=\'your website url.\')\n    apply_license_parser.add_argument(\'--dir\', \'-d\', default=\'./\', help=\'to apply license dir.\')\n    apply_license_parser.add_argument(\'--except\', \'-e\', help=\'except extensions: xml,cc,h\')\n\n    # =============== data part ================\n    data_parser = main_sub_parser.add_parser(\'data\', help=\'data related commands.\')\n    data_sub_parser = data_parser.add_subparsers()\n\n    view_voc_parser = data_sub_parser.add_parser(\'vocview\', help=\'view voc.\')\n    view_voc_parser.set_defaults(which=\'data-vocview\')\n    view_voc_parser.add_argument(\'--image_dir\', \'-i\', help=\'Root path of VOC image.\')\n    view_voc_parser.add_argument(\'--label_dir\', \'-l\', help=\'Root path of VOC label.\')\n\n    view_txt_parser = data_sub_parser.add_parser(\'txtview\', help=\'view voc.\')\n    view_txt_parser.set_defaults(which=\'data-txtview\')\n    view_txt_parser.add_argument(\'--image_dir\', \'-i\', help=\'Root path of VOC image.\')\n    view_txt_parser.add_argument(\'--label_dir\', \'-l\', help=\'Root path of VOC label.\')\n\n    view_coco_parser = data_sub_parser.add_parser(\'cocoview\', help=\'view voc.\')\n    view_coco_parser.set_defaults(which=\'data-cocoview\')\n    view_coco_parser.add_argument(\'--image_dir\', \'-i\', help=\'Root path of COCO images.\')\n    view_coco_parser.add_argument(\'--json\', \'-j\', help=\'Root path of COCO annotations.json .\')\n\n    voc_label_parser = data_sub_parser.add_parser(\'voclabel\', help=\'gather labels from annotations dir.\')\n    voc_label_parser.set_defaults(which=\'data-voclabel\')\n    voc_label_parser.add_argument(\'--anno_dir\', \'-d\', help=\'dir to annotations.\')\n\n    split_voc_parser = data_sub_parser.add_parser(\'splitvoc\', help=\'split VOC to train and val.\')\n    split_voc_parser.set_defaults(which=\'data-splitvoc\')\n    split_voc_parser.add_argument(\'--image_dir\', \'-i\', help=\'Root path of VOC image.\')\n    split_voc_parser.add_argument(\'--label_dir\', \'-l\', help=\'Root path of VOC label.\')\n\n    labelone2voc_parser = data_sub_parser.add_parser(\'labelone2voc\', help=\'convert labelone to VOC.\')\n    labelone2voc_parser.set_defaults(which=\'data-labelone2voc\')\n    labelone2voc_parser.add_argument(\'--json_dir\', \'-j\', help=\'Root of labelone json dir.\')\n\n    voc2coco_parser = data_sub_parser.add_parser(\'voc2coco\', help=\'convert VOC to coco.\')\n    voc2coco_parser.set_defaults(which=\'data-voc2coco\')\n    voc2coco_parser.add_argument(\'--xml_dir\', \'-d\', help=\'Root of xmls dir (Annotations/).\')\n\n    evalvoc_parser = data_sub_parser.add_parser(\'evalvoc\', help=\'evaluation on VOC.\')\n    evalvoc_parser.set_defaults(which=\'data-evalvoc\')\n    evalvoc_parser.add_argument(\'-g\', \'--gt_dir\', type=str, required=True, help=""Ground truth path (can be xml dir or txt dir, coco json will support soon)"")\n    evalvoc_parser.add_argument(\'-d\', \'--det_dir\', type=str, required=True, help=""Detection result (should saved into txt format)"")\n    evalvoc_parser.add_argument(\'-im\', \'--images_dir\', type=str, default=\'images\', help=""Raw images dir for animation."")\n    evalvoc_parser.add_argument(\'-na\', \'--no-animation\', help=""no animation is shown."", action=""store_true"")\n    evalvoc_parser.add_argument(\'-np\', \'--no-plot\', help=""no plot is shown."", action=""store_true"")\n    evalvoc_parser.add_argument(\'-q\', \'--quiet\', help=""minimalistic console output."", action=""store_true"")\n    evalvoc_parser.add_argument(\'--min_overlap\', type=float, default=0.5, help=""min overlap, default is 0.5"")\n    evalvoc_parser.add_argument(\'-i\', \'--ignore\', nargs=\'+\', type=str, help=""ignore a list of classes."")\n    evalvoc_parser.add_argument(\'--set-class-iou\', nargs=\'+\', type=str, help=""set IoU for a specific class."")\n\n    return parser.parse_args()\n\n\ndef print_welcome_msg():\n    print(Fore.BLUE + Style.BRIGHT + \'Alfred \' + Style.RESET_ALL +\n          Fore.WHITE + \'- Valet of Artificial Intelligence.\' + Style.RESET_ALL)\n    print(\'Author: \' + Fore.RED + Style.BRIGHT + __AUTHOR__ + Style.RESET_ALL)\n    print(\'At    : \' + Fore.RED + Style.BRIGHT + __DATE__ + Style.RESET_ALL)\n    print(\'Loc   : \' + Fore.RED + Style.BRIGHT + __LOC__ + Style.RESET_ALL)\n    print(\'Star  : \' + Fore.RED + Style.BRIGHT + __git__ + Style.RESET_ALL)\n    print(\'Ver.  : \' + Fore.RED + Style.BRIGHT + __VERSION__ + Style.RESET_ALL)\n\n\ndef main(args=None):\n    args = arg_parse()\n    if args.version:\n        print(print_welcome_msg())\n        exit(0)\n    else:\n        args_dict = vars(args)\n        print_welcome_msg()\n        try:\n            module = args_dict[\'which\'].split(\'-\')[0]\n            action = args_dict[\'which\'].split(\'-\')[1]\n            print(Fore.GREEN + Style.BRIGHT)\n            print(\'=> Module: \' + Fore.WHITE + Style.BRIGHT + module + Fore.GREEN + Style.BRIGHT)\n            print(\'=> Action: \' + Fore.WHITE + Style.BRIGHT + action)\n            if module == \'vision\':\n                if action == \'extract\':\n                    v_f = args_dict[\'video\']\n                    j = args_dict[\'jumps\']\n                    print(Fore.BLUE + Style.BRIGHT + \'Extracting from {}\'.format(v_f))\n                    video_extractor = VideoExtractor(jump_frames=j)\n                    video_extractor.extract(v_f)\n                elif action == \'reduce\':\n                    v_f = args_dict[\'video\']\n                    j = args_dict[\'jumps\']\n                    print(Fore.BLUE + Style.BRIGHT + \'Reduce from {}, jumps: {}\'.format(v_f, j))\n                    video_reducer = VideoReducer(jump_frames=j)\n                    video_reducer.act(v_f)\n                elif action == \'2video\':\n                    d = args_dict[\'dir\']\n                    combiner = VideoCombiner(img_dir=d)\n                    print(Fore.BLUE + Style.BRIGHT + \'Combine video from {}\'.format(d))\n                    print(Fore.BLUE + Style.BRIGHT + \'What the hell.. {}\'.format(d))\n                    combiner.combine()\n\n                elif action == \'clean\':\n                    d = args_dict[\'dir\']\n                    print(Fore.BLUE + Style.BRIGHT + \'Cleaning from {}\'.format(d))\n\n                elif action == \'getface\':\n                    try:\n                        from .modules.vision.face_extractor import FaceExtractor\n                        import dlib\n\n                        d = args_dict[\'dir\']\n                        print(Fore.BLUE + Style.BRIGHT + \'Extract faces from {}\'.format(d))\n\n                        face_extractor = FaceExtractor()\n                        face_extractor.get_faces(d)\n                    except ImportError:\n                        print(\'This action needs to install dlib first. http://dlib.net\')\n\n            elif module == \'text\':\n                if action == \'clean\':\n                    f = args_dict[\'file\']\n                    print(Fore.BLUE + Style.BRIGHT + \'Cleaning from {}\'.format(f))\n                elif action == \'translate\':\n                    f = args.v\n                    print(Fore.BLUE + Style.BRIGHT + \'Translate from {}\'.format(f))\n            elif module == \'scrap\':\n                if action == \'image\':\n                    q = args_dict[\'query\']\n                    q_list = q.split(\',\')\n                    q_list = [i.replace(\' \', \'\') for i in q_list]\n                    image_scraper = ImageScraper()\n                    image_scraper.scrap(q_list)\n            elif module == \'cab\':\n                if action == \'count\':\n                    d = args_dict[\'dir\']\n                    t = args_dict[\'type\']\n                    logging.info(\'dir: {}, types: {}\'.format(d, t))\n                    count_file(d, t)\n                elif action == \'split\':\n                    f = args_dict[\'file\']\n                    r = args_dict[\'ratios\']\n                    n = args_dict[\'names\']\n                    logging.info(\'files: {}, ratios: {}, names: {}\'.format(f, r, n))\n                    split_txt_file(f, r, n)\n                elif action == \'stackimgs\':\n                    f = args_dict[\'imgs\']\n                    r = args_dict[\'dim\']\n                    logging.info(\'files: {}, dim: {}\'.format(f, r))\n                    stack_imgs(f, r)\n                elif action == \'license\':\n                    owner = args_dict[\'owner\']\n                    project_name = args_dict[\'name\']\n                    year = args_dict[\'year\']\n                    url = args_dict[\'url\']\n                    d = args_dict[\'dir\']\n                    apply_license(owner, project_name, year, url, d)\n            elif module == \'data\':\n                if action == \'vocview\':\n                    image_dir = args_dict[\'image_dir\']\n                    label_dir = args_dict[\'label_dir\']\n                    vis_voc(img_root=image_dir, label_root=label_dir)\n                elif action == \'cocoview\':\n                    img_d = args_dict[\'image_dir\']\n                    json_f = args_dict[\'json\']\n                    vis_coco(img_d, json_f)\n                elif action == \'txtview\':\n                    image_dir = args_dict[\'image_dir\']\n                    label_dir = args_dict[\'label_dir\']\n                    vis_det_txt(img_root=image_dir, label_root=label_dir)\n                elif action == \'voclabel\':\n                    anno_dir = args_dict[\'anno_dir\']\n                    gather_labels(anno_dir)\n                elif action == \'splitvoc\':\n                    logging.info(\'split VOC to train and val not implement yet.\')\n                    pass\n                elif action == \'labelone2voc\':\n                    logging.info(\'labelone2voc not implement yet.\')\n                    pass\n                elif action == \'voc2coco\':\n                    logging.info(\'start convert VOC to coco... Annotations root: {}\'.format(args_dict[\'xml_dir\']))\n                    convert(args_dict[\'xml_dir\'])\n                elif action == \'evalvoc\':\n                    logging.info(\'start eval on VOC dataset..\')\n                    eval_voc(args)\n\n        except Exception as e:\n            traceback.print_exc()\n            print(Fore.RED, \'parse args error, type -h to see help. msg: {}\'.format(e))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
alfred/tests.py,2,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nfrom utils.mana import welcome\n\nfrom utils.log import logger as logging\nfrom vis.image.det import visualize_det_cv2\nimport cv2\nimport numpy as np\nfrom vis.image.get_dataset_label_map import coco_label_map_list\nfrom vis.image.common import draw_rect_with_style\nimport torch\nfrom dl.torch.common import print_tensor\n\nfrom varname import varname\n\ndef a_func(num):\n    print(varname() + \': \' + str(num))\n\n\n\nif __name__ == \'__main__\':\n    v = a_func(1098)\n\n    # welcome(\'\')\n    # logging.info(\'hi hiu\')\n    # logging.error(\'ops\')\n\n    # a = cv2.imread(\'/home/jintian/Pictures/1.jpeg\')\n\n    # dets = [\n    #     [1, 0.9, 4, 124, 333, 256],\n    #     [2, 0.7, 155, 336, 367, 485],\n    # ]\n    # dets = np.array(dets)\n    # print(type(a))\n\n    # draw_rect_with_style(a, (78, 478), (478, 223), (0, 255, 255), style=\'dashed\')\n    # visualize_det_cv2(a, dets, coco_label_map_list, is_show=True)\n\n    aaa = torch.randn([1, 23, 45])\n    print_tensor(aaa)'"
examples/alfred_show_box_gt.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport os\n\nimport sys\nimport numpy as np\nfrom alfred.vis.image.common import get_unique_color_by_id\nfrom alfred.fusion.kitti_fusion import LidarCamCalibData, \\\n    load_pc_from_file, cam3d_to_pixel, lidar_pt_to_cam0_frame\nfrom alfred.fusion.common import draw_3d_box, compute_3d_box_cam_coords, center_to_corner_3d\nimport cv2\n\n\nimg_f = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'./data/000011.png\')\nv_f = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'./data/000011.bin\')\ncalib_f = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'./data/000011.txt\')\nframe_calib = LidarCamCalibData(calib_f=calib_f)\n\nres = [[5.06, 1.43, 12.42, 1.90, 0.42, 1.04,  0.68],\n       [-5.12, 1.85, 4.13, 1.50, 1.46, 3.70,  1.56],\n       [-4.95, 1.83, 26.64, 1.86, 1.57, 3.83,  1.55]]\n\nimg = cv2.imread(img_f)\n\nfor p in res:\n    xyz = np.array([p[: 3]])\n\n    c2d = cam3d_to_pixel(xyz, frame_calib)\n    if c2d is not None:\n        cv2.circle(img, (int(c2d[0]), int(c2d[1])), 8, (0, 0, 255), -1)\n\n    # hwl -> lwh\n    lwh = np.array([p[3: 6]])[:, [2, 1, 0]]\n    r_y = p[6]\n    pts3d = compute_3d_box_cam_coords(xyz[0], lwh[0], r_y)\n\n    pts2d = []\n    for pt in pts3d:\n        coords = cam3d_to_pixel(pt, frame_calib)\n        if coords is not None:\n            pts2d.append(coords[:2])\n    pts2d = np.array(pts2d)\n    draw_3d_box(pts2d, img)\n\ncv2.imshow(\'rr\', img)\ncv2.imwrite(\'result.png\', img)\ncv2.waitKey(0)\n'"
examples/draw_3d_box_on_image.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport os\n\nimport sys\nimport numpy as np\nfrom alfred.vis.image.common import get_unique_color_by_id\nfrom alfred.fusion.kitti_fusion import LidarCamCalibData, \\\n    load_pc_from_file, lidar_pts_to_cam0_frame, lidar_pt_to_cam0_frame\nfrom alfred.fusion.common import draw_3d_box, compute_3d_box_lidar_coords\nimport cv2\n\n# from 2011_09_26/2011_09_26_drive_0051_sync\n\nimg_f = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'./data/000011.png\')\nv_f = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'./data/000011.bin\')\ncalib_f = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'./data/000011.txt\')\n\nframe_calib = LidarCamCalibData(calib_f=calib_f)\n\nres = [[4.481686, 5.147319, -1.0229858, 1.5728549, 3.646751, 1.5121397, 1.5486346],\n       [-2.5172017, 5.0262384, -1.0679419, 1.6241353, 4.0445814, 1.4938312, 1.620804],\n       [1.1783253, -2.9209857, -0.9852259, 1.5852798, 3.7360613, 1.4671413, 1.5811548],\n       [12.925569, -4.9808474, -0.71562666, 0.5328532, 0.89768076, 1.7436955, 0.7869441],\n       [-9.657954, -2.9310253, -0.9663244, 1.6315838, 4.0691543, 1.4506648, 4.7061768],\n       [-7.734651, 4.928315, -1.3513744, 1.7096852, 4.41021, 1.4849466, 1.5580404],\n       [-21.06287, -6.378005, -0.6494193, 0.58654386, 0.67096156, 1.7274126, 1.5062331],\n       [-12.977588, 4.7324443, -1.2884868, 1.6366509, 3.993301, 1.4792416, 1.5961027],\n       [27.237848, 4.973592, -0.63590205, 1.6796488, 4.1773257, 1.8397285, 1.5534456],\n       [-15.21727, -3.3323386, -1.1841949, 1.5691711, 3.7851675, 1.4302691, 1.4623685],\n       [-8.560741, -15.309304, -0.40493315, 1.5614295, 3.6039133, 1.4802926, 3.685232],\n       [-28.535696, 1.8784677, -1.349385, 1.8589652, 4.6122866, 2.0191495, 4.708105],\n       [22.139666, -19.737762, -0.74519694, 0.52543664, 1.7905389, 1.684143, -0.26117292],\n       [-4.4033785, -2.856424, -0.95746094, 1.7221596, 4.5044794, 1.6574095, 1.5402203],\n       [7.085311, -12.124656, -0.7908472, 1.605196, 4.036379, 1.4904786, 3.1525888],\n       [-17.75546, 4.869718, -1.4353731, 1.625128, 4.0645328, 1.4669982, 1.5843123],\n       [22.015368, -16.157223, -0.97120696, 0.70649695, 1.8466028, 1.6473441, 3.46424],\n       [34.445316, -2.0812414, -0.5032885, 0.6895117, 0.8842125, 1.7723539, -1.4539356],\n       [-32.120346, 7.0260167, -1.6048443, 0.59323585, 0.7810404, 1.7134606, 0.9840808],\n       [11.191077, -20.68808, -0.3166721, 2.1275487, 6.112693, 2.4575462, 4.6473494],\n       [-0.18853411, -11.496099, -0.723109, 1.6154484, 3.9286208, 1.5749075, 3.0955489],\n       [7.4211736, -7.1129866, -1.355744, 1.5750822, 3.9536934, 1.4568869, -0.6677291],\n       [16.404984, 7.875185, -0.9816911, 0.64251673, 0.63132536, 1.7938845, 1.0830851],\n       [20.704462, -21.648046, -0.99220616, 1.5985962, 3.830404, 1.521529, 3.0288131],\n       [-34.060417, -1.6139596, -1.1061747, 0.73393285, 0.8841753, 1.7669718, 4.5250244],\n       [-9.143257, -8.996165, -0.9218217, 1.5279316, 3.592435, 1.4721779, 0.85066897],\n       [-31.856539, -2.953291, -1.4160485, 0.67631316, 0.86612713, 1.7683575, 3.113426],\n       [-29.955063, -4.6513176, -1.2724423, 1.5479406, 3.5412807, 1.463421, 0.11858773],\n       [10.639572, 11.339079, -0.35397023, 0.6703583, 0.57711476, 1.7787935, 4.486712],\n       [-11.947865, -21.075172, -0.32996762, 1.5983682, 3.945621, 1.4992962, 1.6880405],\n       [-17.38843, -6.5131726, -0.07191068, 0.6577756, 0.7161297, 1.8168749, 1.8645211],\n       [2.0013125, -16.632671, -0.54558295, 0.54916567, 1.8482145, 1.7980447, 5.3003416]]\n\npcs = load_pc_from_file(v_f)\nimg = cv2.imread(img_f)\n\n\nfor p in res:\n    xyz = np.array([p[: 3]])\n\n    c2d = lidar_pt_to_cam0_frame(xyz, frame_calib)\n    if c2d is not None:\n        cv2.circle(img, (int(c2d[0]), int(c2d[1])), 3, (0, 255, 255), -1)\n\n    hwl = np.array([p[3: 6]])\n    r_y = [p[6]]\n    pts3d = compute_3d_box_lidar_coords(xyz, hwl, angles=r_y, origin=(0.5, 0.5, 0.5), axis=2)\n\n    pts2d = []\n    for pt in pts3d[0]:\n        coords = lidar_pt_to_cam0_frame(pt, frame_calib)\n        if coords is not None:\n            pts2d.append(coords[:2])\n    pts2d = np.array(pts2d)\n    draw_3d_box(pts2d, img)\n\ncv2.imshow(\'rr\', img)\ncv2.imwrite(\'result.png\', img)\ncv2.waitKey(0)\n'"
examples/draw_3d_pointcloud.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nfrom alfred.vis.pointcloud.pointcloud_vis import draw_pcs_open3d\nimport open3d as o3d\nimport numpy as np\nfrom alfred.fusion.common import draw_3d_box, compute_3d_box_lidar_coords\nfrom alfred.fusion.kitti_fusion import load_pc_from_file\nimport os\n\n\nv_f = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'./data/000011.bin\')\npcs = load_pc_from_file(v_f)\n\ngeometries = []\npcs = np.array(pcs[:,:3])\npcobj = o3d.geometry.PointCloud()\npcobj.points = o3d.utility.Vector3dVector(pcs)\ngeometries.append(pcobj)\n# try getting 3d boxes coordinates\n\nres = [[4.481686, 5.147319, -1.0229858, 1.5728549, 3.646751, 1.5121397, 1.5486346],\n       [-2.5172017, 5.0262384, -1.0679419, 1.6241353, 4.0445814, 1.4938312, 1.620804],\n       [1.1783253, -2.9209857, -0.9852259, 1.5852798, 3.7360613, 1.4671413, 1.5811548],\n       [12.925569, -4.9808474, -0.71562666, 0.5328532, 0.89768076, 1.7436955, 0.7869441],\n       [-9.657954, -2.9310253, -0.9663244, 1.6315838, 4.0691543, 1.4506648, 4.7061768],\n       [-7.734651, 4.928315, -1.3513744, 1.7096852, 4.41021, 1.4849466, 1.5580404],\n       [-21.06287, -6.378005, -0.6494193, 0.58654386, 0.67096156, 1.7274126, 1.5062331],\n       [-12.977588, 4.7324443, -1.2884868, 1.6366509, 3.993301, 1.4792416, 1.5961027],\n       [27.237848, 4.973592, -0.63590205, 1.6796488, 4.1773257, 1.8397285, 1.5534456],\n       [-15.21727, -3.3323386, -1.1841949, 1.5691711, 3.7851675, 1.4302691, 1.4623685],\n       [-8.560741, -15.309304, -0.40493315, 1.5614295, 3.6039133, 1.4802926, 3.685232],\n       [-28.535696, 1.8784677, -1.349385, 1.8589652, 4.6122866, 2.0191495, 4.708105],\n       [22.139666, -19.737762, -0.74519694, 0.52543664, 1.7905389, 1.684143, -0.26117292],\n       [-4.4033785, -2.856424, -0.95746094, 1.7221596, 4.5044794, 1.6574095, 1.5402203],\n       [7.085311, -12.124656, -0.7908472, 1.605196, 4.036379, 1.4904786, 3.1525888],\n       [-17.75546, 4.869718, -1.4353731, 1.625128, 4.0645328, 1.4669982, 1.5843123],\n       [22.015368, -16.157223, -0.97120696, 0.70649695, 1.8466028, 1.6473441, 3.46424],\n       [34.445316, -2.0812414, -0.5032885, 0.6895117, 0.8842125, 1.7723539, -1.4539356],\n       [-32.120346, 7.0260167, -1.6048443, 0.59323585, 0.7810404, 1.7134606, 0.9840808],\n       [11.191077, -20.68808, -0.3166721, 2.1275487, 6.112693, 2.4575462, 4.6473494],\n       [-0.18853411, -11.496099, -0.723109, 1.6154484, 3.9286208, 1.5749075, 3.0955489],\n       [7.4211736, -7.1129866, -1.355744, 1.5750822, 3.9536934, 1.4568869, -0.6677291],\n       [16.404984, 7.875185, -0.9816911, 0.64251673, 0.63132536, 1.7938845, 1.0830851],\n       [20.704462, -21.648046, -0.99220616, 1.5985962, 3.830404, 1.521529, 3.0288131],\n       [-34.060417, -1.6139596, -1.1061747, 0.73393285, 0.8841753, 1.7669718, 4.5250244],\n       [-9.143257, -8.996165, -0.9218217, 1.5279316, 3.592435, 1.4721779, 0.85066897],\n       [-31.856539, -2.953291, -1.4160485, 0.67631316, 0.86612713, 1.7683575, 3.113426],\n       [-29.955063, -4.6513176, -1.2724423, 1.5479406, 3.5412807, 1.463421, 0.11858773],\n       [10.639572, 11.339079, -0.35397023, 0.6703583, 0.57711476, 1.7787935, 4.486712],\n       [-11.947865, -21.075172, -0.32996762, 1.5983682, 3.945621, 1.4992962, 1.6880405],\n       [-17.38843, -6.5131726, -0.07191068, 0.6577756, 0.7161297, 1.8168749, 1.8645211],\n       [2.0013125, -16.632671, -0.54558295, 0.54916567, 1.8482145, 1.7980447, 5.3003416]]\n\nfor p in res:\n    xyz = np.array([p[: 3]])\n    hwl = np.array([p[3: 6]])\n    r_y = [p[6]]\n    pts3d = compute_3d_box_lidar_coords(xyz, hwl, angles=r_y, origin=(0.5, 0.5, 0.5), axis=2)\n\n    print(\'points 3d box: {}\'.format(pts3d))\n    lines = [[0,1],[1,2],[2,3],[3,0],\n             [4,5],[5,6],[6,7],[7,4],\n             [0,4],[1,5],[2,6],[3,7]]\n    colors = [[1, 0, 1] for i in range(len(lines))]\n    line_set = o3d.geometry.LineSet()\n    line_set.points = o3d.utility.Vector3dVector(pts3d[0])\n    line_set.lines = o3d.utility.Vector2iVector(lines)\n    line_set.colors = o3d.utility.Vector3dVector(colors)\n    geometries.append(line_set)\n    draw_pcs_open3d(geometries)'"
examples/pykitti_test.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport pykitti\nimport cv2\nimport numpy as np\nfrom alfred.vis.pointcloud.pointcloud_vis import draw_pcs_open3d\n\n\nbase_dir = \'/media/jintain/sg/permanent/datasets/KITTI/videos\'\ndate = \'2011_09_26\'\ndrive = \'0051\'\n\ndata = pykitti.raw(base_dir, date, drive)\nprint(data.cam2_files)\ncam2_img = data.get_rgb(3)[1]\nprint(cam2_img)\ncv2.imshow(\'rr\', np.array(cam2_img))\n\n# data.calib.T_cam0_velo.dot(point_velo)\nres = [[12.189727, 4.65575, -1.0090133, 1.6713146, 3.9860756, 1.4752198, 1.4311914],\n       [7.0290184, 18.43234, -1.0616484, 1.5949062, 3.7942128, 1.4587526, 0.03434156],\n       [9.716782, 18.663864, -1.081424, 1.6270422, 4.0220504, 1.428338, 0.010275014],\n       [12.390503, 18.554394, -1.0709403, 1.5716408, 3.8583813, 1.4068353, 0.092568964],\n       [9.162392, -3.2395134, -0.9900443, 0.48879692, 1.7805163, 1.780584, 4.7180395],\n       [1.5449369, 19.820513, -1.1250883, 1.61444, 4.0291963, 1.4679328, 0.20142984],\n       [15.010401, 17.861265, -0.61177015, 1.8016329, 4.52904, 1.9179995, -0.0009133518],\n       [0.2915942, 14.302571, -1.6358033, 0.6031256, 1.7338636, 1.693197, 2.0567284],\n       [32.58985, 16.622143, -0.9154575, 1.56024, 3.6420622, 1.4507264, 1.5841204],\n       [10.96289, 33.31957, -1.8625767, 1.6718575, 4.1056437, 1.5355072, -0.5065325],\n       [-20.711775, 12.870968, -1.3916719, 0.6494945, 0.6588189, 1.7635618, 2.878424],\n       [-14.706663, 14.144306, -1.4347086, 0.5646943, 1.7102921, 1.7303042, 1.6427889],\n       [-34.937218, -32.419926, -1.9705622, 2.0217955, 6.3850527, 2.5362377, 0.9260524],\n       [-25.85193, 13.433075, -1.6172849, 0.5029159, 1.7657202, 1.6948656, 1.8433876],\n       [-8.7119255, 15.603356, -0.861634, 0.61332655, 1.7866454, 1.7575798, -0.15929039],\n       [0.44268692, -31.126797, -1.4658432, 0.6214817, 1.778398, 1.6685283, 2.7185097],\n       [-1.3864591, 43.80352, -1.6687126, 1.990596, 5.726587, 2.5764484, 0.53529406],\n       [-46.30665, -24.680546, -1.5553175, 0.54056036, 1.8155692, 1.7282323, 1.4364488],\n       [-25.206638, 14.19597, -1.6388608, 0.60298264, 0.6539766, 1.7206633, 2.6259918],\n       [42.099804, 16.609531, -0.95861834, 1.6101078, 3.805344, 1.5348499, 1.4423454]]\n\nfor p in res:\n    pts3d_c = p[:3]\n    cam0_xyz = data.calib.Tr\n\ncv2.waitKey(0)'"
examples/vis_coco.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nfrom alfred.vis.image.get_dataset_label_map import coco_label_map_list\n\n\na = coco_label_map_list\nprint(a)'"
alfred/dl/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/fusion/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/fusion/common.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport cv2\nimport numpy as np\n\n\ndef draw_3d_box(pts, img, color=(255, 0, 255), thickness=1):\n    """"""\n    Given 8 points of a 3D Bounding Box, draw it on image\n            1 -------- 0\n           /|         /|\n          2 -------- 3 .\n          | |        | |\n          . 5 -------- 4\n          |/         |/\n          6 -------- 7\n\n    the points assume above order\n\n    :param pts:\n    :param img:\n    :param color:\n    :param thickness:\n    :return:\n    """"""\n    # currently, we skip box which not has 8 points\n    pts = np.array(pts, dtype=np.int)\n    if pts.shape != (8, 2):\n        return\n        # clockwise face idx\n    face_idx = [[0, 1, 5, 4],\n                [1, 2, 6, 5],\n                [2, 3, 7, 6],\n                [3, 0, 4, 7]]\n    for ind_f in range(3, -1, -1):\n        # 3, 2, 1, 0\n        f = face_idx[ind_f]\n\n        # indicates the direction\n        for j in range(4):\n            if ind_f == 0 and j == 0:\n                cv2.line(img, (pts[f[0], 0], pts[f[0], 1]),\n                         (pts[f[1], 0], pts[f[1], 1]), (255, 255, 0),\n                         thickness, lineType=cv2.LINE_AA)\n            cv2.line(img, (pts[f[j], 0], pts[f[j], 1]),\n                     (pts[f[(j + 1) % 4], 0], pts[f[(j + 1) % 4], 1]), color,\n                     thickness, lineType=cv2.LINE_AA)\n\n\ndef compute_3d_box_cam_coords(xyz, lwh, rotation_y):\n    """"""\n    KITTI camera coordinates using -y as up\n    this only works on camera coordinates xyz\n    center\n    dim\n    rotation\n\n    Algorithm: supports a 3d box at center (0, 0, 0), using r_y we can get a Rotate matrix\n    calculate the new 3d box after rotate by Rotate operation.\n    :param xyz:\n    :param lwh:\n    :param rotation_y:\n    :return:\n    """"""\n    location = xyz\n    dim = lwh\n    c, s = np.cos(rotation_y), np.sin(rotation_y)\n    R = np.array([[c, 0, s], [0, 1, 0], [-s, 0, c]], dtype=np.float32)\n    l, w, h = dim[0], dim[1], dim[2]\n    x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2]\n    y_corners = [0, 0, 0, 0, -h, -h, -h, -h]\n    # y_corners = [0, 0, 0, 0, h, h, h, h]\n    z_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2]\n\n    corners = np.array([x_corners, y_corners, z_corners], dtype=np.float32)\n    corners_3d = np.dot(R, corners)\n    corners_3d = corners_3d + np.array(location, dtype=np.float32).reshape(3, 1)\n    return corners_3d.transpose(1, 0)\n\n\ndef compute_3d_box_lidar_coords(centers,\n                                dims,\n                                angles=None,\n                                origin=(0.5, 0.5, 0.5),\n                                axis=2):\n    corners = _corners_nd(dims, origin=origin)\n    # corners: [N, 8, 3]\n    if angles is not None:\n        corners = _rotation_3d_in_axis(corners, angles, axis=axis)\n    corners += centers.reshape([-1, 1, 3])\n    return corners\n\n\n# ------------------ Should not calling directly\ndef _rotation_3d_in_axis(points, angles, axis=0):\n    # points: [N, point_size, 3]\n    rot_sin = np.sin(angles)\n    rot_cos = np.cos(angles)\n    ones = np.ones_like(rot_cos)\n    zeros = np.zeros_like(rot_cos)\n    if axis == 1:\n        rot_mat_T = np.stack([[rot_cos, zeros, -rot_sin], [zeros, ones, zeros],\n                              [rot_sin, zeros, rot_cos]])\n    elif axis == 2 or axis == -1:\n        rot_mat_T = np.stack([[rot_cos, -rot_sin, zeros],\n                              [rot_sin, rot_cos, zeros], [zeros, zeros, ones]])\n    elif axis == 0:\n        rot_mat_T = np.stack([[zeros, rot_cos, -rot_sin],\n                              [zeros, rot_sin, rot_cos], [ones, zeros, zeros]])\n    else:\n        raise ValueError(""axis should in range"")\n    return np.einsum(\'aij,jka->aik\', points, rot_mat_T)\n\n\ndef _corners_nd(dims, origin=0.5):\n    ndim = int(dims.shape[1])\n    corners_norm = np.stack(\n        np.unravel_index(np.arange(2 ** ndim), [2] * ndim),\n        axis=1).astype(dims.dtype)\n    # now corners_norm has format: (2d) x0y0, x0y1, x1y0, x1y1\n    # (3d) x0y0z0, x0y0z1, x0y1z0, x0y1z1, x1y0z0, x1y0z1, x1y1z0, x1y1z1\n    # so need to convert to a format which is convenient to do other computing.\n    # for 2d boxes, format is clockwise start with minimum point\n    # for 3d boxes, please draw lines by your hand.\n    if ndim == 2:\n        # generate clockwise box corners\n        corners_norm = corners_norm[[0, 1, 3, 2]]\n    elif ndim == 3:\n        corners_norm = corners_norm[[0, 1, 3, 2, 4, 5, 7, 6]]\n    corners_norm = corners_norm - np.array(origin, dtype=dims.dtype)\n    corners = dims.reshape([-1, 1, ndim]) * corners_norm.reshape(\n        [1, 2 ** ndim, ndim])\n    return corners\n'"
alfred/fusion/geometry.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\n\nConvert quaternion to euler angles\n\n\n""""""\nimport numpy as np\nimport math\n\n\ndef euler_to_quaternion(yaw, pitch, roll):\n    """"""\n    return a list of [qx, qy, qz, qw]\n    """"""\n    qx = np.sin(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) - \\\n        np.cos(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)\n    qy = np.cos(roll/2) * np.sin(pitch/2) * np.cos(yaw/2) + \\\n        np.sin(roll/2) * np.cos(pitch/2) * np.sin(yaw/2)\n    qz = np.cos(roll/2) * np.cos(pitch/2) * np.sin(yaw/2) - \\\n        np.sin(roll/2) * np.sin(pitch/2) * np.cos(yaw/2)\n    qw = np.cos(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) + \\\n        np.sin(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)\n    return [qx, qy, qz, qw]\n\n\ndef quaternion_to_euler(x, y, z, w):\n    """"""\n    return a list of euler angles [yaw, pitch, roll]\n    """"""\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    roll = math.atan2(t0, t1)\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    pitch = math.asin(t2)\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    yaw = math.atan2(t3, t4)\n    return [yaw, pitch, roll]\n'"
alfred/fusion/kitti_fusion.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\nimport numpy as np\n\n\nclass LidarCamCalibData(object):\n    """"""\n\n    Load from raw:\n\n    P0: 7.215377000000e+02 0.000000000000e+00 6.095593000000e+02 0.000000000000e+00 0.000000000000e+00 7.215377000000e+02 1.728540000000e+02 0.000000000000e+00 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 0.000000000000e+00\nP1: 7.215377000000e+02 0.000000000000e+00 6.095593000000e+02 -3.875744000000e+02 0.000000000000e+00 7.215377000000e+02 1.728540000000e+02 0.000000000000e+00 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 0.000000000000e+00\nP2: 7.215377000000e+02 0.000000000000e+00 6.095593000000e+02 4.485728000000e+01 0.000000000000e+00 7.215377000000e+02 1.728540000000e+02 2.163791000000e-01 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 2.745884000000e-03\nP3: 7.215377000000e+02 0.000000000000e+00 6.095593000000e+02 -3.395242000000e+02 0.000000000000e+00 7.215377000000e+02 1.728540000000e+02 2.199936000000e+00 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 2.729905000000e-03\nR0_rect: 9.999239000000e-01 9.837760000000e-03 -7.445048000000e-03 -9.869795000000e-03 9.999421000000e-01 -4.278459000000e-03 7.402527000000e-03 4.351614000000e-03 9.999631000000e-01\nTr_velo_to_cam: 7.533745000000e-03 -9.999714000000e-01 -6.166020000000e-04 -4.069766000000e-03 1.480249000000e-02 7.280733000000e-04 -9.998902000000e-01 -7.631618000000e-02 9.998621000000e-01 7.523790000000e-03 1.480755000000e-02 -2.717806000000e-01\nTr_imu_to_velo: 9.999976000000e-01 7.553071000000e-04 -2.035826000000e-03 -8.086759000000e-01 -7.854027000000e-04 9.998898000000e-01 -1.482298000000e-02 3.195559000000e-01 2.024406000000e-03 1.482454000000e-02 9.998881000000e-01 -7.997231000000e-01\n\n    Suppose we have 1 lidar and 4 cameras,\n    more sensors can add more params too.\n\n    tr: 3x1, transform vector\n    r: 3x3, rotation vector\n\n    """"""\n    def __init__(self, calib_f=None):\n        # transformation between lidar and 4 cameras\n        self.T_lidar_to_cam_0 = []\n        self.T_lidar_to_cam_1 = []\n        self.T_lidar_to_cam_2 = []\n        self.T_lidar_to_cam_3 = []\n\n        # rotation between lidar and 4 cameras\n        self.R_lidar_to_cam_0 = []\n        self.R_lidar_to_cam_1 = []\n        self.R_lidar_to_cam_2 = []\n        self.R_lidar_to_cam_3 = []\n\n        # combined transform and rectify\n        self.TR_lidar_to_cam_0 = []\n        self.TR_lidar_to_cam_1 = []\n        self.TR_lidar_to_cam_2 = []\n        self.TR_lidar_to_cam_3 = []\n\n        # this params only works on KITTI, rectify and project matrix\n        self.P_cam_0 = []\n        self.P_cam_1 = []\n        self.P_cam_2 = []\n        self.P_cam_3 = []\n\n        # rectify for cam N to cam0\n        self.Rect_cam_0 = []\n        self.Rect_cam_1 = []\n        self.Rect_cam_2 = []\n        self.Rect_cam_3 = []\n\n        # normal calibration\n        self.K_cam_0 = []\n        self.K_cam_1 = []\n        self.K_cam_2 = []\n        self.K_cam_3 = []\n\n        self.d_cam_0 = []\n        self.d_cam_1 = []\n        self.d_cam_2 = []\n        self.d_cam_3 = []\n\n        self.checked = False\n\n        self.calib_f = calib_f\n        if self.calib_f is not None:\n            self._read_kitti_calib_from_txt()\n            self.bootstrap()\n\n    def _read_kitti_calib_from_txt(self, is_video=False):\n        if not is_video:\n            data = {}\n            with open(self.calib_f, \'r\') as f:\n                for line in f.readlines():\n                    line = line.strip()\n                    if line != \'\':\n                        key, value = line.split(\': \')\n                        # The only non-float values in these files are dates, which\n                        # we don\'t care about anyway\n                        try:\n                            data[key] = [float(x) for x in value.split()]\n                        except ValueError:\n                            pass\n            self.TR_lidar_to_cam_0 = data[\'Tr_velo_to_cam\']\n            self.P_cam_0 = data[\'P2\']\n            self.Rect_cam_0 = data[\'R0_rect\']\n\n    def bootstrap(self):\n        if len(self.TR_lidar_to_cam_0) > 0:\n            self.TR_lidar_to_cam_0 = np.reshape(self.TR_lidar_to_cam_0, (3, 4))\n        else:\n            self.TR_lidar_to_cam_0 = np.concatenate(\n                (np.reshape(self.R_lidar_to_cam_0, (3, 3)),\n                 np.array([self.T_lidar_to_cam_0]).T), axis=1\n            )\n        self.TR_lidar_to_cam_0 = np.pad(self.TR_lidar_to_cam_0, ((0, 1), (0, 0)), \'constant\')\n\n        if isinstance(self.Rect_cam_0, list):\n            self.Rect_cam_0 = np.reshape(self.Rect_cam_0, (3, 3))\n        self.Rect_cam_0 = np.pad(self.Rect_cam_0, ((0, 1), (0, 1)), \'constant\')\n        if isinstance(self.P_cam_0, list):\n            self.P_cam_0 = np.reshape(self.P_cam_0, (3, 4))\n\n        assert self.TR_lidar_to_cam_0.shape == (4, 4), \'TR_lidar_to_cam_0 is R|T, which is 3x4, but received wrong\'\n        assert self.Rect_cam_0.shape == (4, 4), \'Rect_cam_0 is 3x3, but solve failed (a 9 length list is also OK)\'\n        assert self.P_cam_0.shape == (3, 4), \'P_cam {} vs {} failed\'.format(\'(3, 4)\', self.P_cam_0.shape)\n        self.checked = True\n\n    def __str__(self):\n        return \'TR_lidar_to_cam_0: {}\\nP_cam_0: {}\\nRect_cam_0: {}\\nK_cam_0: {}\'.format(\n            self.TR_lidar_to_cam_0, self.P_cam_0, self.Rect_cam_0, self.K_cam_0\n        )\n\n\ndef lidar_pts_to_cam0_frame(pts3d, calib, filter_intensity=False):\n    """"""\n    Directly convert all lidar points to camera frame\n    :param pts3d:\n    :param calib:\n    :param filter_intensity\n    :return:\n    """"""\n    # filter out intensity <= 0\n    if filter_intensity:\n        pts3d = pts3d[pts3d[:, 3] > 0, :]\n    pts3d[:, 3] = 1\n    pts3d = np.transpose(pts3d)\n    # Pad the r0_rect matrix to a 4x4\n    if isinstance(calib, LidarCamCalibData):\n        if not calib.checked:\n            ValueError(\'calib not bootstraped, did you called calib_data.bootstrap()?\')\n        else:\n            cam0_xyz = np.dot(calib.TR_lidar_to_cam_0, pts3d)\n\n            ret_xyz = np.dot(calib.Rect_cam_0, cam0_xyz)\n            idx = (ret_xyz[2, :] >= 0)\n            pts2d_cam = np.dot(calib.P_cam_0, ret_xyz[:, idx])\n            return pts3d[:, idx], pts2d_cam / pts2d_cam[2, :]\n    else:\n        ValueError(\'frame_calib must be an LidarCamCalibData type\')\n\n\ndef lidar_pt_to_cam0_frame(pt3d, calib):\n    """"""\n    Convert a single point of lidar\n    :param pt3d:\n    :param calib:\n    :return:\n    """"""\n    # padding the 4th element to 1\n    pt3d = np.append(pt3d, 1)\n    # Pad the r0_rect matrix to a 4x4\n    if isinstance(calib, LidarCamCalibData):\n        if not calib.checked:\n            raise ValueError(\'calib not bootstrap, did you called calib_data.bootstrap()?\')\n        else:\n            # 1. Get xyz1 on cam0\n            cam0_xyz = np.dot(calib.TR_lidar_to_cam_0, pt3d)\n\n            # 2. Get cam0 after rectify\n            ret_xyz = np.dot(calib.Rect_cam_0, cam0_xyz)\n\n            # 3. if points not on image, then return None\n            if ret_xyz[2] >= 0:\n                # 6. Get projected coords\n                pts2d_cam = np.dot(calib.P_cam_0, ret_xyz)\n                return pts2d_cam / pts2d_cam[2]\n            else:\n                return None\n    else:\n        raise ValueError(\'frame_calib must be an LidarCamCalibData type\')\n\n\ndef cam3d_to_pixel(cam3d, calib):\n    """"""\n    Convert a single point of lidar\n    :param cam3d:\n    :param calib:\n    :return:\n    """"""\n    # padding the 4th element to 1\n    pt3d = np.append(cam3d, 1)\n    # Pad the r0_rect matrix to a 4x4\n    if isinstance(calib, LidarCamCalibData):\n        if not calib.checked:\n            raise ValueError(\'calib not bootstrap, did you called calib_data.bootstrap()?\')\n        else:\n            # 2. Get cam0 after rectify\n            ret_xyz = np.dot(calib.Rect_cam_0, pt3d)\n\n            # 3. if points not on image, then return None\n            if ret_xyz[2] >= 0:\n                # 6. Get projected coords\n                pts2d_cam = np.dot(calib.P_cam_0, ret_xyz)\n                return pts2d_cam / pts2d_cam[2]\n            else:\n                return None\n    else:\n        raise ValueError(\'frame_calib must be an LidarCamCalibData type\')\n\n\ndef load_pc_from_file(v_f):\n    return np.fromfile(v_f, dtype=np.float32, count=-1).reshape([-1, 4])\n# ------------------------------ Drawing utilities ------------------------------------------\n\n'"
alfred/fusion/nuscenes_fusion.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\n\nFusion for nuScenes dataset\n\n""""""\nimport numpy as np\nfrom pyquaternion import Quaternion\n\n\ndef project_cam_coords_to_pixel(pts3d_cam, intrinsic):\n    """"""\n    project pts3d_cam on image\n    such as:\n\n    [[-1.9, 0.12, 37]] -> [[222, 345]]\n    """"""\n    intrinsic = np.array(intrinsic)\n    pts3d_cam = np.array(pts3d_cam)\n    if pts3d_cam.shape[0] != 3:\n        # if not hstack, try to transpose it\n        pts3d_cam = pts3d_cam.T\n    assert pts3d_cam.shape[0] == 3, \'pts3d_cam must be 3 rows.\'\n    assert intrinsic.shape == (3, 3), \'intrinsic should be 3x3.\'\n    a = np.dot(intrinsic, pts3d_cam)\n    a = a/a[-1, :]\n    return a.T[:, :2]\n\n# def compute_3d_box_cam_coords_nuscenes(xyz, lwh, quarternion):\n#     """"""\n#         nuScenes camera coordinates using -y as up\n#         using quarternion represents rotation of box\n#     """"""\n#     # we get rotation_y from quarternion first\n#     quarternion = Quaternion(axis=quarternion[1:], angle=quarternion[0])\n#     trans_m = transform_matrix(xyz, quarternion)\n#     l, w, h = lwh[0], lwh[1], lwh[2]\n#     x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2]\n#     y_corners = [0, 0, 0, 0, -h, -h, -h, -h]\n#     # y_corners = [0, 0, 0, 0, h, h, h, h]\n#     z_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2]\n\n#     corners = np.array([x_corners, y_corners, z_corners], dtype=np.float32)\n#     corners4d = np.ones((corners.shape[0]+1, corners.shape[1]))\n#     corners4d[:-1, :] = corners\n#     print(\'transformation matrix: \', trans_m)\n#     print(\'corners4d: \', corners4d)\n#     corners4d_trans = np.dot(trans_m, corners4d)\n#     print(\'corners4d_trans: \', corners4d_trans)\n#     return corners4d_trans\n\n\ndef compute_3d_box_cam_coords_nuscenes(xyz, lwh, quarternion):\n    """"""\n        nuScenes camera coordinates using -y as up\n        using quarternion represents rotation of box\n\n        only calculate rotation_y?: arcsin(2(wy-zx))\n    """"""\n    # we get rotation_y from quarternion first\n    if isinstance(quarternion, Quaternion):\n        l, w, h = lwh[0], lwh[1], lwh[2]\n        x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2]\n        # y_corners = [0, 0, 0, 0, -h, -h, -h, -h]\n        y_corners = [-h/2, -h/2, -h/2, -h/2, h/2, h/2, h/2, h/2]\n        z_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2]\n        corners = np.array([x_corners, y_corners, z_corners], dtype=np.float32)\n\n        rotation_y = np.pi/2 - quarternion.radians\n        c, s = np.cos(rotation_y), np.sin(rotation_y)\n        R = np.array([[c, 0, s], [0, 1, 0], [-s, 0, c]], dtype=np.float32)\n\n        corners_trans = np.dot(R, corners)\n        corners_trans += [[i] for i in xyz]\n        return corners_trans\n    else:\n        raise ValueError(\'quarternion must be a Quaternion object, make sure \'\\\n            \'you using pyquarternion.\')\n\n\ndef load_pc_from_file(pc_f):\n    # nuScenes lidar is 5 digits one line (last one the ring index)\n    return np.fromfile(pc_f, dtype=np.float32, count=-1).reshape([-1, 5])'"
alfred/modules/__init__.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/protos/labelmap_pb2.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: labelmap.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'labelmap.proto\',\n  package=\'\',\n  syntax=\'proto2\',\n  serialized_options=None,\n  serialized_pb=_b(\'\\n\\x0elabelmap.proto\\""M\\n\\x0cLabelMapItem\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05label\\x18\\x02 \\x01(\\x05\\x12\\n\\n\\x02id\\x18\\x04 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x03 \\x01(\\t\\""\\\'\\n\\x08LabelMap\\x12\\x1b\\n\\x04item\\x18\\x01 \\x03(\\x0b\\x32\\r.LabelMapItem\')\n)\n\n\n\n\n_LABELMAPITEM = _descriptor.Descriptor(\n  name=\'LabelMapItem\',\n  full_name=\'LabelMapItem\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'LabelMapItem.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'label\', full_name=\'LabelMapItem.label\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'LabelMapItem.id\', index=2,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'display_name\', full_name=\'LabelMapItem.display_name\', index=3,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=18,\n  serialized_end=95,\n)\n\n\n_LABELMAP = _descriptor.Descriptor(\n  name=\'LabelMap\',\n  full_name=\'LabelMap\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'item\', full_name=\'LabelMap.item\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=97,\n  serialized_end=136,\n)\n\n_LABELMAP.fields_by_name[\'item\'].message_type = _LABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'LabelMapItem\'] = _LABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'LabelMap\'] = _LABELMAP\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nLabelMapItem = _reflection.GeneratedProtocolMessageType(\'LabelMapItem\', (_message.Message,), {\n  \'DESCRIPTOR\' : _LABELMAPITEM,\n  \'__module__\' : \'labelmap_pb2\'\n  # @@protoc_insertion_point(class_scope:LabelMapItem)\n  })\n_sym_db.RegisterMessage(LabelMapItem)\n\nLabelMap = _reflection.GeneratedProtocolMessageType(\'LabelMap\', (_message.Message,), {\n  \'DESCRIPTOR\' : _LABELMAP,\n  \'__module__\' : \'labelmap_pb2\'\n  # @@protoc_insertion_point(class_scope:LabelMap)\n  })\n_sym_db.RegisterMessage(LabelMap)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
alfred/tests/cv_box_fancy.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport cv2\nimport numpy as np\n\n# ============================================================================\n\n\ndef draw_border(img, pt1, pt2, color, thickness, r, d):\n    x1, y1 = pt1\n    x2, y2 = pt2\n\n    # Top left\n    cv2.line(img, (x1 + r, y1), (x1 + r + d, y1), color, thickness)\n    cv2.line(img, (x1, y1 + r), (x1, y1 + r + d), color, thickness)\n    cv2.ellipse(img, (x1 + r, y1 + r), (r, r), 180, 0, 90, color, thickness)\n\n    # Top right\n    cv2.line(img, (x2 - r, y1), (x2 - r - d, y1), color, thickness)\n    cv2.line(img, (x2, y1 + r), (x2, y1 + r + d), color, thickness)\n    cv2.ellipse(img, (x2 - r, y1 + r), (r, r), 270, 0, 90, color, thickness)\n\n    # Bottom left\n    cv2.line(img, (x1 + r, y2), (x1 + r + d, y2), color, thickness)\n    cv2.line(img, (x1, y2 - r), (x1, y2 - r - d), color, thickness)\n    cv2.ellipse(img, (x1 + r, y2 - r), (r, r), 90, 0, 90, color, thickness)\n\n    # Bottom right\n    cv2.line(img, (x2 - r, y2), (x2 - r - d, y2), color, thickness)\n    cv2.line(img, (x2, y2 - r), (x2, y2 - r - d), color, thickness)\n    cv2.ellipse(img, (x2 - r, y2 - r), (r, r), 0, 0, 90, color, thickness)\n\n# ============================================================================\n\n\nimg = np.zeros((512, 512, 3), dtype=np.uint8)\n\ndraw_border(img, (10, 10), (100, 100), (127, 255, 255), 2, 10, 20)\ndraw_border(img, (128, 128), (240, 160), (255, 255, 127), 2, 10, 5)\ndraw_border(img, (68, 68), (123, 289), (0, 255, 0), 1, 2, 5)\n\ncv2.imshow(\'round_rect.png\', img)\ncv2.waitKey(0)\n'"
alfred/utils/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/utils/cv_wrapper.py,0,"b'""""""\n\nsome opencv wrappers to make video inference\nmore simple\n\n""""""\n'"
alfred/utils/file_io.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\nimport errno\nimport logging\nimport os\nimport shutil\nfrom collections import OrderedDict\nfrom typing import IO, Any, Dict, List, MutableMapping, Optional\nfrom urllib.parse import urlparse\nimport portalocker\nimport logging\nimport os\nimport shutil\nfrom typing import Callable, Optional\nfrom urllib import request\n\n\n\n__all__ = [""PathManager"", ""get_cache_dir"", ""file_lock""]\n\n\n\n\ndef download(\n    url: str, dir: str, *, filename: Optional[str] = None, progress: bool = True\n):\n    """"""\n    Download a file from a given URL to a directory. If file exists, will not\n        overwrite the existing file.\n\n    Args:\n        url (str):\n        dir (str): the directory to download the file\n        filename (str or None): the basename to save the file.\n            Will use the name in the URL if not given.\n        progress (bool): whether to use tqdm to draw a progress bar.\n\n    Returns:\n        str: the path to the downloaded file or the existing one.\n    """"""\n    os.makedirs(dir, exist_ok=True)\n    if filename is None:\n        filename = url.split(""/"")[-1]\n        assert len(filename), ""Cannot obtain filename from url {}"".format(url)\n    fpath = os.path.join(dir, filename)\n    logger = logging.getLogger(__name__)\n\n    if os.path.isfile(fpath):\n        logger.info(""File {} exists! Skipping download."".format(filename))\n        return fpath\n\n    tmp = fpath + "".tmp""  # download to a tmp file first, to be more atomic.\n    try:\n        logger.info(""Downloading from {} ..."".format(url))\n        if progress:\n            import tqdm\n            def hook(t: tqdm.tqdm) -> Callable[[int, int, Optional[int]], None]:\n                last_b = [0]\n\n                def inner(\n                    b: int, bsize: int, tsize: Optional[int] = None\n                ) -> None:\n                    if tsize is not None:\n                        t.total = tsize\n                    t.update((b - last_b[0]) * bsize)  # type: ignore\n                    last_b[0] = b\n\n                return inner\n\n            with tqdm.tqdm(  # type: ignore\n                unit=""B"", unit_scale=True, miniters=1, desc=filename, leave=True\n            ) as t:\n                tmp, _ = request.urlretrieve(\n                    url, filename=tmp, reporthook=hook(t)\n                )\n\n        else:\n            tmp, _ = request.urlretrieve(url, filename=tmp)\n        statinfo = os.stat(tmp)\n        size = statinfo.st_size\n        if size == 0:\n            raise IOError(""Downloaded an empty file from {}!"".format(url))\n        # download to tmp first and move to fpath, to make this function more\n        # atomic.\n        shutil.move(tmp, fpath)\n    except IOError:\n        logger.error(""Failed to download {}"".format(url))\n        raise\n    finally:\n        try:\n            os.unlink(tmp)\n        except IOError:\n            pass\n\n    logger.info(\n        ""Successfully downloaded "" + fpath + "". "" + str(size) + "" bytes.""\n    )\n    return fpath\n\n\ndef get_cache_dir(cache_dir: Optional[str] = None) -> str:\n    """"""\n    Returns a default directory to cache static files\n    (usually downloaded from Internet), if None is provided.\n\n    Args:\n        cache_dir (None or str): if not None, will be returned as is.\n            If None, returns the default cache directory as:\n\n        1) $DL_LIB_CACHE, if set\n        2) otherwise ~/.torch/dl_lib_cache\n    """"""\n    if cache_dir is None:\n        cache_dir = os.path.expanduser(\n            os.getenv(""DL_LIB_CACHE"", ""~/.torch/dl_lib_cache"")\n        )\n    return cache_dir\n\n\ndef file_lock(path: str):  # type: ignore\n    """"""\n    A file lock. Once entered, it is guaranteed that no one else holds the\n    same lock. Others trying to enter the lock will block for 30 minutes and\n    raise an exception.\n\n    This is useful to make sure workers don\'t cache files to the same location.\n\n    Args:\n        path (str): a path to be locked. This function will create a lock named\n            `path + "".lock""`\n\n    Examples:\n\n        filename = ""/path/to/file""\n        with file_lock(filename):\n            if not os.path.isfile(filename):\n                do_create_file()\n    """"""\n    dirname = os.path.dirname(path)\n    try:\n        os.makedirs(dirname, exist_ok=True)\n    except OSError:\n        # makedir is not atomic. Exceptions can happen when multiple workers try\n        # to create the same dir, despite exist_ok=True.\n        # When this happens, we assume the dir is created and proceed to creating\n        # the lock. If failed to create the directory, the next line will raise\n        # exceptions.\n        pass\n    return portalocker.Lock(path + "".lock"", timeout=1800)  # type: ignore\n\n\nclass PathHandler:\n    """"""\n    PathHandler is a base class that defines common I/O functionality for a URI\n    protocol. It routes I/O for a generic URI which may look like ""protocol://*""\n    or a canonical filepath ""/foo/bar/baz"".\n    """"""\n\n    def _get_supported_prefixes(self) -> List[str]:\n        """"""\n        Returns:\n            List[str]: the list of URI prefixes this PathHandler can support\n        """"""\n        raise NotImplementedError()\n\n    def _get_local_path(self, path: str) -> str:\n        """"""\n        Get a filepath which is compatible with native Python I/O such as `open`\n        and `os.path`.\n\n        If URI points to a remote resource, this function may download and cache\n        the resource to local disk. In this case, this function is meant to be\n        used with read-only resources.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            local_path (str): a file path which exists on the local file system\n        """"""\n        raise NotImplementedError()\n\n    def _open(self, path: str, mode: str = ""r"") -> IO[Any]:\n        """"""\n        Open a stream to a URI, similar to the built-in `open`.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n            mode (str): Specifies the mode in which the file is opened. It defaults\n                to \'r\'.\n\n        Returns:\n            file: a file-like object.\n        """"""\n        raise NotImplementedError()\n\n    def _copy(\n        self, src_path: str, dst_path: str, overwrite: bool = False\n    ) -> bool:\n        """"""\n        Copies a source path to a destination path.\n\n        Args:\n            src_path (str): A URI supported by this PathHandler\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing file\n\n        Returns:\n            status (bool): True on success\n        """"""\n        raise NotImplementedError()\n\n    def _exists(self, path: str) -> bool:\n        """"""\n        Checks if there is a resource at the given URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path exists\n        """"""\n        raise NotImplementedError()\n\n    def _isfile(self, path: str) -> bool:\n        """"""\n        Checks if the resource at the given URI is a file.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path is a file\n        """"""\n        raise NotImplementedError()\n\n    def _isdir(self, path: str) -> bool:\n        """"""\n        Checks if the resource at the given URI is a directory.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path is a directory\n        """"""\n        raise NotImplementedError()\n\n    def _ls(self, path: str) -> List[str]:\n        """"""\n        List the contents of the directory at the provided URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            List[str]: list of contents in given path\n        """"""\n        raise NotImplementedError()\n\n    def _mkdirs(self, path: str) -> None:\n        """"""\n        Recursive directory creation function. Like mkdir(), but makes all\n        intermediate-level directories needed to contain the leaf directory.\n        Similar to the native `os.makedirs`.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n        """"""\n        raise NotImplementedError()\n\n    def _rm(self, path: str) -> None:\n        """"""\n        Remove the file (not directory) at the provided URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n        """"""\n        raise NotImplementedError()\n\n\nclass NativePathHandler(PathHandler):\n    """"""\n    Handles paths that can be accessed using Python native system calls. This\n    handler uses `open()` and `os.*` calls on the given path.\n    """"""\n\n    def _get_local_path(self, path: str) -> str:\n        return path\n\n    def _open(self, path: str, mode: str = ""r"") -> IO[Any]:\n        return open(path, mode)\n\n    def _copy(\n        self, src_path: str, dst_path: str, overwrite: bool = False\n    ) -> bool:\n        """"""\n        Copies a source path to a destination path.\n\n        Args:\n            src_path (str): A URI supported by this PathHandler\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing file\n\n        Returns:\n            status (bool): True on success\n        """"""\n        if os.path.exists(dst_path) and not overwrite:\n            logger = logging.getLogger(__name__)\n            logger.error(""Destination file {} already exists."".format(dst_path))\n            return False\n\n        try:\n            shutil.copyfile(src_path, dst_path)\n            return True\n        except Exception as e:\n            logger = logging.getLogger(__name__)\n            logger.error(""Error in file copy - {}"".format(str(e)))\n            return False\n\n    def _exists(self, path: str) -> bool:\n        return os.path.exists(path)\n\n    def _isfile(self, path: str) -> bool:\n        return os.path.isfile(path)\n\n    def _isdir(self, path: str) -> bool:\n        return os.path.isdir(path)\n\n    def _ls(self, path: str) -> List[str]:\n        return os.listdir(path)\n\n    def _mkdirs(self, path: str) -> None:\n        try:\n            os.makedirs(path, exist_ok=True)\n        except OSError as e:\n            # EEXIST it can still happen if multiple processes are creating the dir\n            if e.errno != errno.EEXIST:\n                raise\n\n    def _rm(self, path: str) -> None:\n        os.remove(path)\n\n\nclass HTTPURLHandler(PathHandler):\n    """"""\n    Download URLs and cache them to disk.\n    """"""\n\n    def __init__(self) -> None:\n        self.cache_map: Dict[str, str] = {}\n\n    def _get_supported_prefixes(self) -> List[str]:\n        return [""http://"", ""https://"", ""ftp://""]\n\n    def _get_local_path(self, path: str) -> str:\n        """"""\n        This implementation downloads the remote resource and caches it locally.\n        The resource will only be downloaded if not previously requested.\n        """"""\n        if path not in self.cache_map or not os.path.exists(\n            self.cache_map[path]\n        ):\n            logger = logging.getLogger(__name__)\n            parsed_url = urlparse(path)\n            dirname = os.path.join(\n                get_cache_dir(), os.path.dirname(parsed_url.path.lstrip(""/""))\n            )\n            filename = path.split(""/"")[-1]\n            cached = os.path.join(dirname, filename)\n            with file_lock(cached):\n                if not os.path.isfile(cached):\n                    logger.info(""Downloading {} ..."".format(path))\n                    cached = download(path, dirname, filename=filename)\n            logger.info(""URL {} cached in {}"".format(path, cached))\n            self.cache_map[path] = cached\n        return self.cache_map[path]\n\n    def _open(self, path: str, mode: str = ""r"") -> IO[Any]:\n        assert mode in (\n            ""r"",\n            ""rb"",\n        ), ""{} does not support open with {} mode"".format(\n            self.__class__.__name__, mode\n        )\n        local_path = self._get_local_path(path)\n        return open(local_path, mode)\n\n\nclass PathManager:\n    """"""\n    A class for users to open generic paths or translate generic paths to file names.\n    """"""\n\n    _PATH_HANDLERS: MutableMapping[str, PathHandler] = OrderedDict()\n    _NATIVE_PATH_HANDLER = NativePathHandler()\n\n    @staticmethod\n    def __get_path_handler(path: str) -> PathHandler:\n        """"""\n        Finds a PathHandler that supports the given path. Falls back to the native\n        PathHandler if no other handler is found.\n\n        Args:\n            path (str): URI path to resource\n\n        Returns:\n            handler (PathHandler)\n        """"""\n        for p in PathManager._PATH_HANDLERS.keys():\n            if path.startswith(p):\n                return PathManager._PATH_HANDLERS[p]\n        return PathManager._NATIVE_PATH_HANDLER\n\n    @staticmethod\n    def open(path: str, mode: str = ""r"") -> IO[Any]:\n        """"""\n        Open a stream to a URI, similar to the built-in `open`.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            file: a file-like object.\n        """"""\n        return PathManager.__get_path_handler(path)._open(path, mode)\n\n    @staticmethod\n    def copy(src_path: str, dst_path: str, overwrite: bool = False) -> bool:\n        """"""\n        Copies a source path to a destination path.\n\n        Args:\n            src_path (str): A URI supported by this PathHandler\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing file\n\n        Returns:\n            status (bool): True on success\n        """"""\n\n        # Copying across handlers is not supported.\n        assert PathManager.__get_path_handler(\n            src_path\n        ) == PathManager.__get_path_handler(dst_path)\n        return PathManager.__get_path_handler(src_path)._copy(\n            src_path, dst_path, overwrite\n        )\n\n    @staticmethod\n    def get_local_path(path: str) -> str:\n        """"""\n        Get a filepath which is compatible with native Python I/O such as `open`\n        and `os.path`.\n\n        If URI points to a remote resource, this function may download and cache\n        the resource to local disk.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            local_path (str): a file path which exists on the local file system\n        """"""\n        return PathManager.__get_path_handler(path)._get_local_path(path)\n\n    @staticmethod\n    def exists(path: str) -> bool:\n        """"""\n        Checks if there is a resource at the given URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path exists\n        """"""\n        return PathManager.__get_path_handler(path)._exists(path)\n\n    @staticmethod\n    def isfile(path: str) -> bool:\n        """"""\n        Checks if there the resource at the given URI is a file.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path is a file\n        """"""\n        return PathManager.__get_path_handler(path)._isfile(path)\n\n    @staticmethod\n    def isdir(path: str) -> bool:\n        """"""\n        Checks if the resource at the given URI is a directory.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path is a directory\n        """"""\n        return PathManager.__get_path_handler(path)._isdir(path)\n\n    @staticmethod\n    def ls(path: str) -> List[str]:\n        """"""\n        List the contents of the directory at the provided URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            List[str]: list of contents in given path\n        """"""\n        return PathManager.__get_path_handler(path)._ls(path)\n\n    @staticmethod\n    def mkdirs(path: str) -> None:\n        """"""\n        Recursive directory creation function. Like mkdir(), but makes all\n        intermediate-level directories needed to contain the leaf directory.\n        Similar to the native `os.makedirs`.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n        """"""\n        return PathManager.__get_path_handler(path)._mkdirs(path)\n\n    @staticmethod\n    def rm(path: str) -> None:\n        """"""\n        Remove the file (not directory) at the provided URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n        """"""\n        return PathManager.__get_path_handler(path)._rm(path)\n\n    @staticmethod\n    def register_handler(handler: PathHandler) -> None:\n        """"""\n        Register a path handler associated with `handler._get_supported_prefixes`\n        URI prefixes.\n\n        Args:\n            handler (PathHandler)\n        """"""\n        assert isinstance(handler, PathHandler), handler\n        for prefix in handler._get_supported_prefixes():\n            assert prefix not in PathManager._PATH_HANDLERS\n            PathManager._PATH_HANDLERS[prefix] = handler\n\n        # Sort path handlers in reverse order so longer prefixes take priority,\n        # eg: http://foo/bar before http://foo\n        PathManager._PATH_HANDLERS = OrderedDict(\n            sorted(\n                PathManager._PATH_HANDLERS.items(),\n                key=lambda t: t[0],\n                reverse=True,\n            )\n        )\n\n\nPathManager.register_handler(HTTPURLHandler())\n'"
alfred/utils/image_convertor.py,0,"b'""""""\n\nimage format convert in popular libs\n\nPIL.Image -> OpenCV\nOpenCV -> PIL.Image\nMatplotlib.PLT -> OpenCV\n\n""""""\nfrom PIL import Image\nimport numpy as np\n\n\ndef cv2pil(image):\n    new_image = image.copy()\n    if new_image.ndim == 2:\n        pass\n    elif new_image.shape[2] == 3:\n        new_image = new_image[:, :, ::-1]\n    elif new_image.shape[2] == 4:\n        new_image = new_image[:, :, [2, 1, 0, 3]]\n    new_image = Image.fromarray(new_image)\n    return new_image\n\n\ndef pil2cv(image):\n    new_image = np.array(image, dtype=np.uint8)\n    if new_image.ndim == 2:\n        pass\n    elif new_image.shape[2] == 3:\n        new_image = new_image[:, :, ::-1]\n    elif new_image.shape[2] == 4:\n        new_image = new_image[:, :, [2, 1, 0, 3]]\n    return new_image'"
alfred/utils/log.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nfrom loguru import logger\nimport sys\n\n\ndef init_logger():\n    logger.remove()  # Remove the pre-configured handler\n    logger.start(sys.stderr, format=""<lvl>{level}</lvl> {time:MM-DD HH:mm:ss} {file}:{line} - {message}"")\n\nlogger.remove()  # Remove the pre-configured handler\nlogger.start(sys.stderr, format=""<green>{level}</green> <green>{time:MM.DD HH:mm:ss}</green> <blue>{file}:{line}:</blue> <green>{message}</green>"")\n\n'"
alfred/utils/mana.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nUtils using in MANA universe\n\nsuch as print welcome message\n""""""\nfrom colorama import Fore, Back, Style\n\n\nwelcome_msg = \'\'\'\n    __  ______    _   _____    ___    ____\n   /  |/  /   |  / | / /   |  /   |  /  _/\n  / /|_/ / /| | /  |/ / /| | / /| |  / /  \n / /  / / ___ |/ /|  / ___ |/ ___ |_/ /   \n/_/  /_/_/  |_/_/ |_/_/  |_/_/  |_/___/    http://manaai.cn\n\'\'\'\n\ndef welcome(ori_git_url):\n    print(Fore.YELLOW + Style.BRIGHT + \'Welcome to MANA AI platform!\' + Style.RESET_ALL)\n    print(Fore.BLUE + Style.BRIGHT + welcome_msg + Style.RESET_ALL)\n    print(Style.BRIGHT + ""once you saw this msg, indicates you were back supported by our team!"" + Style.RESET_ALL)\n    print(\'the latest updates of our codes always at: {} or {}\'.format(ori_git_url, \'http://manaai.cn\'))\n    print(\'NOTE: Our codes distributed from anywhere else were not supported!\')\n'"
alfred/utils/timer.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# -*- coding: utf-8 -*-\n\nfrom time import perf_counter\nfrom typing import Optional\n\n\nclass Timer:\n    """"""\n    A timer which computes the time elapsed since the start/reset of the timer.\n    """"""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        """"""\n        Reset the timer.\n        """"""\n        self._start = perf_counter()\n        self._paused: Optional[float] = None\n        self._total_paused = 0\n\n    def pause(self):\n        """"""\n        Pause the timer.\n        """"""\n        if self._paused is not None:\n            raise ValueError(""Trying to pause a Timer that is already paused!"")\n        self._paused = perf_counter()\n\n    def is_paused(self) -> bool:\n        """"""\n        Returns:\n            bool: whether the timer is currently paused\n        """"""\n        return self._paused is not None\n\n    def resume(self):\n        """"""\n        Resume the timer.\n        """"""\n        if self._paused is None:\n            raise ValueError(""Trying to resume a Timer that is not paused!"")\n        self._total_paused += perf_counter() - self._paused\n        self._paused = None\n\n    def seconds(self) -> float:\n        """"""\n        Returns:\n            (float): the total number of seconds since the start/reset of the\n                timer, excluding the time when the timer is paused.\n        """"""\n        if self._paused is not None:\n            end_time: float = self._paused  # type: ignore\n        else:\n            end_time = perf_counter()\n        return end_time - self._start - self._total_paused\n'"
alfred/vis/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/dl/inference/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/dl/inference/image_inference.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\n\nSimple demo\n\nmother method of all demos\n\n\n""""""\nimport cv2\nimport os\nimport sys\nimport time\nfrom deprecated import deprecated\nfrom alfred.utils.log import logger as logging\n\n\nclass ImageInferEngine(object):\n\n    def __init__(self, f, is_show=False, record=False):\n        """"""\n        run on\n\n        1. single file\n        2. video file\n        3. webcam\n        :param f:\n        """"""\n        self.img_ext = [\'png\', \'jpg\', \'jpeg\']\n        self.video_ext = [\'avi\', \'mp4\']\n        self.f = f\n        self.is_show = is_show\n        self.record = record\n        self.crt_cost = 0\n\n        if not f:\n            self.mode = \'webcam\'\n        elif os.path.basename(f).split(\'.\')[-1] in self.img_ext:\n            self.mode = \'image\'\n        elif os.path.basename(f).split(\'.\')[-1] in self.video_ext:\n            self.mode = \'video\'\n        logging.info(\'[Demo] in {} mode.\'.format(self.mode))\n\n    def read_image_file(self, img_f):\n        """"""\n        this method should return image read result\n\n        :param img_f:\n        :return:\n        """"""\n        if self.mode == \'image\':\n            raise NotImplementedError(\'read_image_file must be implemented when using image mode \')\n        else:\n            pass\n\n    @deprecated(reason=""This method has been deprecated, using solve_one_image instead"")\n    def solve_a_image(self, img):\n        """"""\n        this method must be implemented to solve a single image\n\n        img must be a numpy array\n        then return the detection or segmentation out\n        :param img:\n        :return:\n        """"""\n        raise NotImplementedError(\'solve_a_image method must be implemented\')\n\n    def solve_one_image(self, img):\n        """"""\n        this method must be implemented to solve a single image\n\n        img must be a numpy array\n        then return the detection or segmentation out\n        :param img:\n        :return:\n        """"""\n        raise NotImplementedError(\'solve_a_image method must be implemented\')\n\n    def vis_result(self, img, net_out):\n        """"""\n        this method must be implement to visualize result on image\n\n        :param img\n        :param net_out:\n        :return:\n        """"""\n        raise NotImplementedError(\'Visualize network output on image\')\n\n    def run(self):\n        if self.mode == \'image\':\n            img = self.read_image_file(self.f)\n            res = self.solve_a_image(img)\n            res_img = self.vis_result(img, res)\n            if self.is_show:\n                cv2.imshow(\'result\', res_img)\n                cv2.waitKey(0)\n        elif self.mode == \'video\' or self.mode == \'webcam\':\n            cap = cv2.VideoCapture(self.f)\n            if self.record and self.mode == \'video\':\n                fps = cap.get(cv2.CAP_PROP_FPS)\n                size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n                        int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n                save_f = os.path.join(os.path.dirname(self.f), \'result_\' + os.path.basename(self.f))\n                logging.info(\'Result video will record into {}\'.format(save_f))\n                video_writer = cv2.VideoWriter(save_f, cv2.VideoWriter_fourcc(*\'DIVX\'), fps, size)\n\n            while cap.isOpened():\n                ok, frame = cap.read()\n                if ok:\n                    tic = time.time()\n                    res = self.solve_a_image(frame)\n                    if self.is_show:\n                        logging.info(\'fps: {}\'.format(1 / (time.time() - tic)))\n                    self.crt_cost = time.time() - tic\n                    res_img = self.vis_result(frame, res)\n                    if self.record and self.mode == \'video\':\n                        video_writer.write(res_img)\n\n                    if self.is_show:\n                        cv2.imshow(\'result\', res_img)\n                        cv2.waitKey(1)\n                else:\n                    logging.info(\'Done\')\n                    exit(0)\n\n\n\n\n\n\n'"
alfred/dl/tf/common.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport os\n\n\ndef mute_tf():\n    """"""\n    this function will mute tensorflow\n    disable tensorflow logging information\n    call this before you import tensorflow\n    """"""\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\''"
alfred/dl/torch/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/dl/torch/common.py,2,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nCommon utility of pytorch\n\nthis contains code that frequently used while writing torch applications\n\n""""""\nimport torch\nfrom colorama import Fore, Back, Style\n\n\ndevice = torch.device(\'cuda\') if torch.cuda.is_available() else torch.device(\'cpu\')\n\n\ndef print_tensor(t, label=None):\n    if isinstance(t, torch.Tensor):\n        if label:\n            print(Fore.YELLOW + Style.BRIGHT + ""tensor: {}"".format(label) + Style.RESET_ALL)\n        else:\n            print(Fore.YELLOW + Style.BRIGHT + ""tensor: "" + Style.RESET_ALL)\n        print(\'value: {}\\nshape: {}\\ndtype: {}\\n\'.format(\n            t, t.shape, t.dtype\n        ))\n    else:\n        print(\'{} is not a tensor.\'.format(t))'"
alfred/dl/torch/env.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nimport importlib\nimport importlib.util\nimport logging\nimport os\nimport random\nimport sys\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\n\n__all__ = [""seed_all_rng""]\n\n\ndef seed_all_rng(seed=None):\n    """"""\n    Set the random seed for the RNG in torch, numpy and python.\n\n    Args:\n        seed (int): if None, will use a strong random seed.\n    """"""\n    if seed is None:\n        seed = (\n            os.getpid()\n            + int(datetime.now().strftime(""%S%f""))\n            + int.from_bytes(os.urandom(2), ""big"")\n        )\n        logger = logging.getLogger(__name__)\n        logger.info(""Using a generated random seed {}"".format(seed))\n    np.random.seed(seed)\n    torch.set_rng_state(torch.manual_seed(seed).get_state())\n    random.seed(seed)\n\n\n# from https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\ndef _import_file(module_name, file_path, make_importable=False):\n    spec = importlib.util.spec_from_file_location(module_name, file_path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    if make_importable:\n        sys.modules[module_name] = module\n    return module\n\n\ndef _configure_libraries():\n    """"""\n    Configurations for some libraries.\n    """"""\n    # An environment option to disable `import cv2` globally,\n    # in case it leads to negative performance impact\n    disable_cv2 = int(os.environ.get(""dl_lib_DISABLE_CV2"", False))\n    if disable_cv2:\n        sys.modules[""cv2""] = None\n    else:\n        # Disable opencl in opencv since its interaction with cuda often has negative effects\n        # This envvar is supported after OpenCV 3.4.0\n        os.environ[""OPENCV_OPENCL_RUNTIME""] = ""disabled""\n        try:\n            import cv2\n\n            if int(cv2.__version__.split(""."")[0]) >= 3:\n                cv2.ocl.setUseOpenCL(False)\n        except ImportError:\n            pass\n\n\n_ENV_SETUP_DONE = False\n\n\ndef setup_environment():\n    """"""Perform environment setup work. The default setup is a no-op, but this\n    function allows the user to specify a Python source file or a module in\n    the $dl_lib_ENV_MODULE environment variable, that performs\n    custom setup work that may be necessary to their computing environment.\n    """"""\n    global _ENV_SETUP_DONE\n    if _ENV_SETUP_DONE:\n        return\n    _ENV_SETUP_DONE = True\n\n    _configure_libraries()\n\n    custom_module_path = os.environ.get(""dl_lib_ENV_MODULE"")\n\n    if custom_module_path:\n        setup_custom_environment(custom_module_path)\n    else:\n        # The default setup is a no-op\n        pass\n\n\ndef setup_custom_environment(custom_module):\n    """"""\n    Load custom environment setup by importing a Python source file or a\n    module, and run the setup function.\n    """"""\n    if custom_module.endswith("".py""):\n        module = _import_file(""dl_lib.utils.env.custom_module"", custom_module)\n    else:\n        module = importlib.import_module(custom_module)\n    assert hasattr(module, ""setup_environment"") and callable(module.setup_environment), (\n        ""Custom environment module defined in {} does not have the ""\n        ""required callable attribute \'setup_environment\'.""\n    ).format(custom_module)\n    module.setup_environment()\n'"
alfred/dl/torch/metrics.py,33,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass Scalar(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\'total\', torch.FloatTensor([0.0]))\n        self.register_buffer(\'count\', torch.FloatTensor([0.0]))\n\n    def forward(self, scalar):\n        if not scalar.eq(0.0):\n            self.count += 1\n            self.total += scalar.data.float()\n        return self.value.cpu()\n\n    @property\n    def value(self):\n        return self.total / self.count\n\n    def clear(self):\n        self.total.zero_()\n        self.count.zero_()\n\nclass Accuracy(nn.Module):\n    def __init__(self,\n                 dim=1,\n                 ignore_idx=-1,\n                 threshold=0.5,\n                 encode_background_as_zeros=True):\n        super().__init__()\n        self.register_buffer(\'total\', torch.FloatTensor([0.0]))\n        self.register_buffer(\'count\', torch.FloatTensor([0.0]))\n        self._ignore_idx = ignore_idx\n        self._dim = dim\n        self._threshold = threshold\n        self._encode_background_as_zeros = encode_background_as_zeros\n\n    def forward(self, labels, preds, weights=None):\n        # labels: [N, ...]\n        # preds: [N, C, ...]\n        if self._encode_background_as_zeros:\n            scores = torch.sigmoid(preds)\n            labels_pred = torch.max(preds, dim=self._dim)[1] + 1\n            pred_labels = torch.where((scores > self._threshold).any(self._dim),\n                                      labels_pred,\n                                      torch.tensor(0).type_as(labels_pred))\n        else:\n            pred_labels = torch.max(preds, dim=self._dim)[1]\n        N, *Ds = labels.shape\n        labels = labels.view(N, int(np.prod(Ds)))\n        pred_labels = pred_labels.view(N, int(np.prod(Ds)))\n        if weights is None:\n            weights = (labels != self._ignore_idx).float()\n        else:\n            weights = weights.float()\n\n        num_examples = torch.sum(weights)\n        num_examples = torch.clamp(num_examples, min=1.0).float()\n        total = torch.sum((pred_labels == labels.long()).float())\n        self.count += num_examples\n        self.total += total\n        return self.value.cpu()\n        # return (total /  num_examples.data).cpu()\n    @property\n    def value(self):\n        return self.total / self.count\n\n    def clear(self):\n        self.total.zero_()\n        self.count.zero_()\n\n\nclass Precision(nn.Module):\n    def __init__(self, dim=1, ignore_idx=-1, threshold=0.5):\n        super().__init__()\n        self.register_buffer(\'total\', torch.FloatTensor([0.0]))\n        self.register_buffer(\'count\', torch.FloatTensor([0.0]))\n        self._ignore_idx = ignore_idx\n        self._dim = dim\n        self._threshold = threshold\n\n    def forward(self, labels, preds, weights=None):\n        # labels: [N, ...]\n        # preds: [N, C, ...]\n        if preds.shape[self._dim] == 1:  # BCE\n            pred_labels = (torch.sigmoid(preds) >\n                           self._threshold).long().squeeze(self._dim)\n        else:\n            assert preds.shape[\n                self._dim] == 2, ""precision only support 2 class""\n            pred_labels = torch.max(preds, dim=self._dim)[1]\n        N, *Ds = labels.shape\n        labels = labels.view(N, int(np.prod(Ds)))\n        pred_labels = pred_labels.view(N, int(np.prod(Ds)))\n        if weights is None:\n            weights = (labels != self._ignore_idx).float()\n        else:\n            weights = weights.float()\n\n        pred_trues = pred_labels > 0\n        pred_falses = pred_labels == 0\n        trues = labels > 0\n        falses = labels == 0\n        true_positives = (weights * (trues & pred_trues).float()).sum()\n        true_negatives = (weights * (falses & pred_falses).float()).sum()\n        false_positives = (weights * (falses & pred_trues).float()).sum()\n        false_negatives = (weights * (trues & pred_falses).float()).sum()\n        count = true_positives + false_positives\n        # print(count, true_positives)\n        if count > 0:\n            self.count += count\n            self.total += true_positives\n        return self.value.cpu()\n        # return (total /  num_examples.data).cpu()\n    @property\n    def value(self):\n        return self.total / self.count\n    def clear(self):\n        self.total.zero_()\n        self.count.zero_()\n\n\nclass Recall(nn.Module):\n    def __init__(self, dim=1, ignore_idx=-1, threshold=0.5):\n        super().__init__()\n        self.register_buffer(\'total\', torch.FloatTensor([0.0]))\n        self.register_buffer(\'count\', torch.FloatTensor([0.0]))\n        self._ignore_idx = ignore_idx\n        self._dim = dim\n        self._threshold = threshold\n\n    def forward(self, labels, preds, weights=None):\n        # labels: [N, ...]\n        # preds: [N, C, ...]\n        if preds.shape[self._dim] == 1:  # BCE\n            pred_labels = (torch.sigmoid(preds) >\n                           self._threshold).long().squeeze(self._dim)\n        else:\n            assert preds.shape[\n                self._dim] == 2, ""precision only support 2 class""\n            pred_labels = torch.max(preds, dim=self._dim)[1]\n        N, *Ds = labels.shape\n        labels = labels.view(N, int(np.prod(Ds)))\n        pred_labels = pred_labels.view(N, int(np.prod(Ds)))\n        if weights is None:\n            weights = (labels != self._ignore_idx).float()\n        else:\n            weights = weights.float()\n        pred_trues = pred_labels == 1\n        pred_falses = pred_labels == 0\n        trues = labels == 1\n        falses = labels == 0\n        true_positives = (weights * (trues & pred_trues).float()).sum()\n        true_negatives = (weights * (falses & pred_falses).float()).sum()\n        false_positives = (weights * (falses & pred_trues).float()).sum()\n        false_negatives = (weights * (trues & pred_falses).float()).sum()\n        count = true_positives + false_negatives\n        if count > 0:\n            self.count += count\n            self.total += true_positives\n        return self.value.cpu()\n        # return (total /  num_examples.data).cpu()\n    @property\n    def value(self):\n        return self.total / self.count\n    def clear(self):\n        self.total.zero_()\n        self.count.zero_()\n\n\ndef _calc_binary_metrics(labels,\n                         scores,\n                         weights=None,\n                         ignore_idx=-1,\n                         threshold=0.5):\n\n    pred_labels = (scores > threshold).long()\n    N, *Ds = labels.shape\n    labels = labels.view(N, int(np.prod(Ds)))\n    pred_labels = pred_labels.view(N, int(np.prod(Ds)))\n    pred_trues = pred_labels > 0\n    pred_falses = pred_labels == 0\n    trues = labels > 0\n    falses = labels == 0\n    true_positives = (weights * (trues & pred_trues).float()).sum()\n    true_negatives = (weights * (falses & pred_falses).float()).sum()\n    false_positives = (weights * (falses & pred_trues).float()).sum()\n    false_negatives = (weights * (trues & pred_falses).float()).sum()\n    return true_positives, true_negatives, false_positives, false_negatives\n\n\nclass PrecisionRecall(nn.Module):\n    def __init__(self,\n                 dim=1,\n                 ignore_idx=-1,\n                 thresholds=0.5,\n                 use_sigmoid_score=False,\n                 encode_background_as_zeros=True):\n        super().__init__()\n        if not isinstance(thresholds, (list, tuple)):\n            thresholds = [thresholds]\n\n        self.register_buffer(\'prec_total\',\n                             torch.FloatTensor(len(thresholds)).zero_())\n        self.register_buffer(\'prec_count\',\n                             torch.FloatTensor(len(thresholds)).zero_())\n        self.register_buffer(\'rec_total\',\n                             torch.FloatTensor(len(thresholds)).zero_())\n        self.register_buffer(\'rec_count\',\n                             torch.FloatTensor(len(thresholds)).zero_())\n\n        self._ignore_idx = ignore_idx\n        self._dim = dim\n        self._thresholds = thresholds\n        self._use_sigmoid_score = use_sigmoid_score\n        self._encode_background_as_zeros = encode_background_as_zeros\n\n    def forward(self, labels, preds, weights=None):\n        # labels: [N, ...]\n        # preds: [N, ..., C]\n        if self._encode_background_as_zeros:\n            # this don\'t support softmax\n            assert self._use_sigmoid_score is True\n            total_scores = torch.sigmoid(preds)\n            # scores, label_preds = torch.max(total_scores, dim=1)\n        else:\n            if self._use_sigmoid_score:\n                total_scores = torch.sigmoid(preds)[..., 1:]\n            else:\n                total_scores = F.softmax(preds, dim=-1)[..., 1:]\n        """"""\n        if preds.shape[self._dim] == 1:  # BCE\n            scores = torch.sigmoid(preds)\n        else:\n            # assert preds.shape[\n            #     self._dim] == 2, ""precision only support 2 class""\n            # TODO: add support for [N, C, ...] format.\n            # TODO: add multiclass support\n            if self._use_sigmoid_score:\n                scores = torch.sigmoid(preds)[:, ..., 1:].sum(-1)\n            else:\n                scores = F.softmax(preds, dim=self._dim)[:, ..., 1:].sum(-1)\n        """"""\n        scores = torch.max(total_scores, dim=-1)[0]\n        if weights is None:\n            weights = (labels != self._ignore_idx).float()\n        else:\n            weights = weights.float()\n        for i, thresh in enumerate(self._thresholds):\n            tp, tn, fp, fn = _calc_binary_metrics(labels, scores, weights,\n                                                  self._ignore_idx, thresh)\n            rec_count = tp + fn\n            prec_count = tp + fp\n            if rec_count > 0:\n                self.rec_count[i] += rec_count\n                self.rec_total[i] += tp\n            if prec_count > 0:\n                self.prec_count[i] += prec_count\n                self.prec_total[i] += tp\n\n        return self.value\n        # return (total /  num_examples.data).cpu()\n    @property\n    def value(self):\n        prec_count = torch.clamp(self.prec_count, min=1.0)\n        rec_count = torch.clamp(self.rec_count, min=1.0)\n        return ((self.prec_total / prec_count).cpu(),\n                (self.rec_total / rec_count).cpu())\n\n    @property\n    def thresholds(self):\n        return self._thresholds\n\n    def clear(self):\n        self.rec_count.zero_()\n        self.prec_count.zero_()\n        self.prec_total.zero_()\n        self.rec_total.zero_()\n'"
alfred/dl/torch/model_summary.py,9,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\ncodes token from\nhttps://github.com/sksq96/pytorch-summary\n\nI edit something here, credits belongs to author\n""""""\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom collections import OrderedDict\nimport numpy as np\n\n\ndef summary(model, input_size, batch_size=-1, device=""cuda""):\n    def register_hook(module):\n        def hook(module, input, output):\n            class_name = str(module.__class__).split(""."")[-1].split(""\'"")[0]\n            module_idx = len(summary)\n\n            m_key = ""%s-%i"" % (class_name, module_idx + 1)\n            summary[m_key] = OrderedDict()\n            try:\n                summary[m_key][""input_shape""] = list(input[0].size())\n            except Exception as e:\n                print(\'error when fetching input shape: \', e)\n                summary[m_key][""input_shape""] = [2, -1, -1]\n            summary[m_key][""input_shape""][0] = batch_size\n            if isinstance(output, (list, tuple)):\n                summary[m_key][""output_shape""] = [\n                    [-1] + list(o.size())[1:] for o in output\n                ]\n            else:\n                summary[m_key][""output_shape""] = list(output.size())\n                summary[m_key][""output_shape""][0] = batch_size\n\n            params = 0\n            if hasattr(module, ""weight"") and hasattr(module.weight, ""size""):\n                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n                summary[m_key][""trainable""] = module.weight.requires_grad\n            if hasattr(module, ""bias"") and hasattr(module.bias, ""size""):\n                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n            summary[m_key][""nb_params""] = params\n\n        if (\n                not isinstance(module, nn.Sequential)\n                and not isinstance(module, nn.ModuleList)\n                and not (module == model)\n        ):\n            hooks.append(module.register_forward_hook(hook))\n\n    device = device.lower()\n    assert device in [\n        ""cuda"",\n        ""cpu"",\n    ], ""Input device is not valid, please specify \'cuda\' or \'cpu\'""\n\n    if device == ""cuda"" and torch.cuda.is_available():\n        dtype = torch.cuda.FloatTensor\n    else:\n        dtype = torch.FloatTensor\n\n    # multiple inputs to the network\n    if isinstance(input_size, tuple) or isinstance(input_size, list) and input_size[0] <= 3:\n        # batch_size of 2 for batchnorm\n        x = torch.rand(2, *input_size).type(dtype)\n    else:\n        x = torch.rand(2, *input_size).type(dtype)\n        print(\'[WARNING] channel more than 3, this may cause some errors. or you does not put channel as first dim.\')\n        # print(\'Wrong! you should send input size specific without batch size, etc: (3, 64, 64), channel first.\')\n        # exit(0)\n    # create properties\n    summary = OrderedDict()\n    hooks = []\n\n    # register hook\n    model.apply(register_hook)\n\n    # make a forward pass\n    try:\n        print(\'[INFO] fake data input: \', x.size())\n        model(x)\n    except Exception as e:\n        print(\'[ERROR] summary failed. error: {}\'.format(e))\n        print(\'[TIPS] make sure your called model.to(device) \')\n        print(\'also possibly error is input size should specific without batch, and using channel first, etc: (3, 128, 128)\')\n        print(\'full trace back: \', e.with_traceback(0))\n        exit(0)\n\n    # remove these hooks\n    for h in hooks:\n        h.remove()\n\n    print(""----------------------------------------------------------------"")\n    line_new = ""{:>20}  {:>25} {:>15}"".format(""Layer (type)"", ""Output Shape"", ""Param #"")\n    print(line_new)\n    print(""================================================================"")\n    total_params = 0\n    total_output = 0\n    trainable_params = 0\n    for layer in summary:\n        # input_shape, output_shape, trainable, nb_params\n        line_new = ""{:>20}  {:>25} {:>15}"".format(\n            layer,\n            str(summary[layer][""output_shape""]),\n            ""{0:,}"".format(summary[layer][""nb_params""]),\n        )\n        total_params += summary[layer][""nb_params""]\n        total_output += np.prod(summary[layer][""output_shape""])\n        if ""trainable"" in summary[layer]:\n            if summary[layer][""trainable""] == True:\n                trainable_params += summary[layer][""nb_params""]\n        print(line_new)\n\n    # assume 4 bytes/number (float on cuda).\n    total_input_size = abs(np.prod(input_size) * batch_size * 4. / (1024 ** 2.))\n    total_output_size = abs(2. * total_output * 4. / (1024 ** 2.))  # x2 for gradients\n    total_params_size = abs(total_params.numpy() * 4. / (1024 ** 2.))\n    total_size = total_params_size + total_output_size + total_input_size\n\n    print(""================================================================"")\n    print(""Total params: {0:,}"".format(total_params))\n    print(""Trainable params: {0:,}"".format(trainable_params))\n    print(""Non-trainable params: {0:,}"".format(total_params - trainable_params))\n    print(""----------------------------------------------------------------"")\n    print(""Input size (MB): %0.2f"" % total_input_size)\n    print(""Forward/backward pass size (MB): %0.2f"" % total_output_size)\n    print(""Params size (MB): %0.2f"" % total_params_size)\n    print(""Estimated Total Size (MB): %0.2f"" % total_size)\n    print(""----------------------------------------------------------------"")\n'"
alfred/dl/torch/tools.py,6,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport functools\nimport inspect\nimport sys\nfrom collections import OrderedDict\n\nimport numba\nimport numpy as np\nimport torch\n\n\ndef get_pos_to_kw_map(func):\n    pos_to_kw = {}\n    fsig = inspect.signature(func)\n    pos = 0\n    for name, info in fsig.parameters.items():\n        if info.kind is info.POSITIONAL_OR_KEYWORD:\n            pos_to_kw[pos] = name\n        pos += 1\n    return pos_to_kw\n\n\ndef get_kw_to_default_map(func):\n    kw_to_default = {}\n    fsig = inspect.signature(func)\n    for name, info in fsig.parameters.items():\n        if info.kind is info.POSITIONAL_OR_KEYWORD:\n            if info.default is not info.empty:\n                kw_to_default[name] = info.default\n    return kw_to_default\n\n\ndef change_default_args(**kwargs):\n    def layer_wrapper(layer_class):\n        class DefaultArgLayer(layer_class):\n            def __init__(self, *args, **kw):\n                pos_to_kw = get_pos_to_kw_map(layer_class.__init__)\n                kw_to_pos = {kw: pos for pos, kw in pos_to_kw.items()}\n                for key, val in kwargs.items():\n                    if key not in kw and kw_to_pos[key] > len(args):\n                        kw[key] = val\n                super().__init__(*args, **kw)\n\n        return DefaultArgLayer\n\n    return layer_wrapper\n\ndef torch_to_np_dtype(ttype):\n    type_map = {\n        torch.float16: np.dtype(np.float16),\n        torch.float32: np.dtype(np.float32),\n        torch.float16: np.dtype(np.float64),\n        torch.int32: np.dtype(np.int32),\n        torch.int64: np.dtype(np.int64),\n        torch.uint8: np.dtype(np.uint8),\n    }\n    return type_map[ttype]'"
alfred/modules/cabinet/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/modules/cabinet/count_file.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\n\ncount how many certain files under dir\n\n""""""\nimport os\nimport glob\nfrom alfred.utils.log import logger as logging\n\n\n\ndef count_file(d, f_type):\n    assert os.path.exists(d), \'{} not exist.\'.format(d)\n    # f_type can be jpg,png,pdf etc, connected by comma\n    all_types = f_type.split(\',\')\n    logging.info(\'count all file types: {} under: {}\'.format(all_types, d))\n    all_files = []\n    for t in all_types:\n        t = t.replace(\'.\', \'\')\n        one = glob.glob(os.path.join(d, \'*.{}\'.format(t)))\n        one = [i for i in one if os.path.isfile(i)]\n        logging.info(\'{} num: {}\'.format(t, len(one)))\n        all_files.extend(one)\n    logging.info(\'file types: {}, total num: {}\'.format(all_types, len(all_files)))\n'"
alfred/modules/cabinet/license.py,0,"b'#!/usr/bin/env python\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n# encoding: utf-8\n\nimport argparse\nimport fnmatch\nimport logging\nimport os\nimport sys\nfrom shutil import copyfile\nfrom string import Template\nfrom datetime import datetime\nfrom alfred.utils.log import logger\nimport regex as re\n\nLOGGER = logger\n\ntype_settings = {\n    ""java"": {\n        ""extensions"": ["".java"", "".scala"", "".groovy"", "".jape"", "".js""],\n        ""keepFirst"": None,\n        ""blockCommentStartPattern"": re.compile(r\'^\\s*/\\*\'),\n        ""blockCommentEndPattern"": re.compile(r\'\\*/\\s*$\'),\n        ""lineCommentStartPattern"": re.compile(r\'\\s*//\'),\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": ""/*\\n"",\n        ""headerEndLine"": "" */\\n"",\n        ""headerLinePrefix"": "" * "",\n        ""headerLineSuffix"": None,\n    },\n    ""script"": {\n        ""extensions"": ["".sh"", "".csh"", "".py"", "".pl""],\n        ""keepFirst"": re.compile(r\'^#!|^# -\\*-\'),\n        ""blockCommentStartPattern"": None,\n        ""blockCommentEndPattern"": None,\n        ""lineCommentStartPattern"": re.compile(r\'\\s*#\'),\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": ""##\\n"",\n        ""headerEndLine"": ""##\\n"",\n        ""headerLinePrefix"": ""## "",\n        ""headerLineSuffix"": None\n    },\n    ""perl"": {\n        ""extensions"": ["".pl""],\n        ""keepFirst"": re.compile(r\'^#!|^# -\\*-\'),\n        ""blockCommentStartPattern"": None,\n        ""blockCommentEndPattern"": None,\n        ""lineCommentStartPattern"": re.compile(r\'\\s*#\'),\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": ""##\\n"",\n        ""headerEndLine"": ""##\\n"",\n        ""headerLinePrefix"": ""## "",\n        ""headerLineSuffix"": None\n    },\n    ""python"": {\n        ""extensions"": ["".py""],\n        ""keepFirst"": re.compile(r\'^#!|^# +pylint|^# +-\\*-|^# +coding|^# +encoding\'),\n        ""blockCommentStartPattern"": None,\n        ""blockCommentEndPattern"": None,\n        ""lineCommentStartPattern"": re.compile(r\'\\s*#\'),\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": ""#\\n"",\n        ""headerEndLine"": ""#\\n"",\n        ""headerLinePrefix"": ""# "",\n        ""headerLineSuffix"": None\n    },\n    ""robot"": {\n        ""extensions"": ["".robot""],\n        ""keepFirst"": re.compile(r\'^#!|^# +pylint|^# +-\\*-|^# +coding|^# +encoding\'),\n        ""blockCommentStartPattern"": None,\n        ""blockCommentEndPattern"": None,\n        ""lineCommentStartPattern"": re.compile(r\'\\s*#\'),\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": None,\n        ""headerEndLine"": None,\n        ""headerLinePrefix"": ""# "",\n        ""headerLineSuffix"": None\n    },\n    ""xml"": {\n        ""extensions"": ["".xml""],\n        ""keepFirst"": re.compile(r\'^\\s*<\\?xml.*\\?>\'),\n        ""blockCommentStartPattern"": re.compile(r\'^\\s*<!--\'),\n        ""blockCommentEndPattern"": re.compile(r\'-->\\s*$\'),\n        ""lineCommentStartPattern"": None,\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": ""<!--\\n"",\n        ""headerEndLine"": ""  -->\\n"",\n        ""headerLinePrefix"": ""-- "",\n        ""headerLineSuffix"": None\n    },\n    ""sql"": {\n        ""extensions"": ["".sql""],\n        ""keepFirst"": None,\n        ""blockCommentStartPattern"": None,  # re.compile(\'^\\s*/\\*\'),\n        ""blockCommentEndPattern"": None,  # re.compile(r\'\\*/\\s*$\'),\n        ""lineCommentStartPattern"": re.compile(r\'\\s*--\'),\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": ""--\\n"",\n        ""headerEndLine"": ""--\\n"",\n        ""headerLinePrefix"": ""-- "",\n        ""headerLineSuffix"": None\n    },\n    ""c"": {\n        ""extensions"": ["".c"", "".cc"", "".cpp"", ""c++"", "".h"", "".hpp""],\n        ""keepFirst"": None,\n        ""blockCommentStartPattern"": re.compile(r\'^\\s*/\\*\'),\n        ""blockCommentEndPattern"": re.compile(r\'\\*/\\s*$\'),\n        ""lineCommentStartPattern"": re.compile(r\'\\s*//\'),\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": ""/*\\n"",\n        ""headerEndLine"": "" */\\n"",\n        ""headerLinePrefix"": "" * "",\n        ""headerLineSuffix"": None\n    },\n    ""ruby"": {\n        ""extensions"": ["".rb""],\n        ""keepFirst"": ""^#!"",\n        ""blockCommentStartPattern"": re.compile(\'^=begin\'),\n        ""blockCommentEndPattern"": re.compile(r\'^=end\'),\n        ""lineCommentStartPattern"": re.compile(r\'\\s*#\'),\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": ""##\\n"",\n        ""headerEndLine"": ""##\\n"",\n        ""headerLinePrefix"": ""## "",\n        ""headerLineSuffix"": None\n    },\n    ""csharp"": {\n        ""extensions"": ["".cs""],\n        ""keepFirst"": None,\n        ""blockCommentStartPattern"": None,\n        ""blockCommentEndPattern"": None,\n        ""lineCommentStartPattern"": re.compile(r\'\\s*//\'),\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": None,\n        ""headerEndLine"": None,\n        ""headerLinePrefix"": ""// "",\n        ""headerLineSuffix"": None\n    },\n    ""vb"": {\n        ""extensions"": ["".vb""],\n        ""keepFirst"": None,\n        ""blockCommentStartPattern"": None,\n        ""blockCommentEndPattern"": None,\n        ""lineCommentStartPattern"": re.compile(r""^\\s*\\\'""),\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": None,\n        ""headerEndLine"": None,\n        ""headerLinePrefix"": ""\' "",\n        ""headerLineSuffix"": None\n    },\n    ""erlang"": {\n        ""extensions"": ["".erl"", "".src"", "".config"", "".schema""],\n        ""keepFirst"": None,\n        ""blockCommentStartPattern"": None,\n        ""blockCommentEndPattern"": None,\n        ""lineCommentStartPattern"": None,\n        ""lineCommentEndPattern"": None,\n        ""headerStartLine"": ""%% -*- erlang -*-\\n%% %CopyrightBegin%\\n%%\\n"",\n        ""headerEndLine"": ""%%\\n%% %CopyrightEnd%\\n\\n"",\n        ""headerLinePrefix"": ""%% "",\n        ""headerLineSuffix"": None,\n    }\n}\n\nyears_pattern = re.compile(\n    r""(?<=Copyright\\s*(?:\\(\\s*[Cc\xc2\xa9]\\s*\\)\\s*))?([0-9][0-9][0-9][0-9](?:-[0-9][0-9]?[0-9]?[0-9]?)?)"",\n    re.IGNORECASE)\nlicensePattern = re.compile(r""license"", re.IGNORECASE)\nemptyPattern = re.compile(r\'^\\s*$\')\n\n# maps each extension to its processing type. Filled from tpeSettings during initialization\next2type = {}\npatterns = []\n\n\n# class for dict args. Use --argname key1=val1,val2 key2=val3 key3=val4, val5\nclass DictArgs(argparse.Action):\n    def __call__(self, parser, namespace, values, option_string=None):\n        dict_args = {}\n        if not isinstance(values, (list,)):\n            values = (values,)\n        for value in values:\n            n, v = value.split(""="")\n            if n not in type_settings:\n                LOGGER.error(""No valid language \'%s\' to add additional file extensions for"" % n)\n            if v and "","" in str(v):\n                dict_args[n] = v.split("","")\n            else:\n                dict_args[n] = list()\n                dict_args[n].append(str(v).strip())\n        setattr(namespace, self.dest, dict_args)\n\n\ndef get_paths(fnpatterns, start_dir="".""):\n    """"""\n    Retrieve files that match any of the glob patterns from the start_dir and below.\n    :param fnpatterns: the file name patterns\n    :param start_dir: directory where to start searching\n    :return: generator that returns one path after the other\n    """"""\n    seen = set()\n    for root, dirs, files in os.walk(start_dir):\n        names = []\n        for pattern in fnpatterns:\n            names += fnmatch.filter(files, pattern)\n        for name in names:\n            path = os.path.join(root, name)\n            if path in seen:\n                continue\n            seen.add(path)\n            yield path\n\n\ndef read_template(template_file, vardict, safe_subst=False):\n    """"""\n    Read a template file replace variables from the dict and return the lines.\n    Throws exception if a variable cannot be replaced.\n    :param template_file: template file with variables\n    :param vardict: dictionary to replace variables with values\n    :param safe_subst:\n    :return: lines of the template, with variables replaced\n    """"""\n    with open(template_file, \'r\') as f:\n        lines = f.readlines()\n    if safe_subst:\n        lines = [Template(line).safe_substitute(vardict) for line in lines]\n    else:\n        lines = [Template(line).substitute(vardict) for line in lines]\n    return lines\n\n\ndef for_type(templatelines, ftype):\n    """"""\n    Format the template lines for the given ftype.\n    :param templatelines: the lines of the template text\n    :param ftype: file type\n    :return: header lines\n    """"""\n    lines = []\n    settings = type_settings[ftype]\n    header_start_line = settings[""headerStartLine""]\n    header_end_line = settings[""headerEndLine""]\n    header_line_prefix = settings[""headerLinePrefix""]\n    header_line_suffix = settings[""headerLineSuffix""]\n    if header_start_line is not None:\n        lines.append(header_start_line)\n    for line in templatelines:\n        tmp = line\n        if header_line_prefix is not None and line == \'\\n\':\n            tmp = header_line_prefix.rstrip() + tmp\n        elif header_line_prefix is not None:\n            tmp = header_line_prefix + tmp\n        if header_line_suffix is not None:\n            tmp = tmp + header_line_suffix\n        lines.append(tmp)\n    if header_end_line is not None:\n        lines.append(header_end_line)\n    return lines\n\n\n##\ndef read_file(file, encoding=\'utf-8\'):\n    """"""\n    Read a file and return a dictionary with the following elements:\n    :param file: the file to read\n    :param encoding: the options specified by the user\n    :return: a dictionary with the following entries or None if the file is not supported:\n      - skip: number of lines at the beginning to skip (always keep them when replacing or adding something)\n       can also be seen as the index of the first line not to skip\n      - headStart: index of first line of detected header, or None if non header detected\n      - headEnd: index of last line of detected header, or None\n      - yearsLine: index of line which contains the copyright years, or None\n      - haveLicense: found a line that matches a pattern that indicates this could be a license header\n      - settings: the type settings\n    """"""\n    skip = 0\n    head_start = None\n    head_end = None\n    years_line = None\n    have_license = False\n    extension = os.path.splitext(file)[1]\n    LOGGER.debug(""File extension is %s"", extension)\n    # if we have no entry in the mapping from extensions to processing type, return None\n    ftype = ext2type.get(extension)\n    logging.debug(""Type for this file is %s"", ftype)\n    if not ftype:\n        return None\n    settings = type_settings.get(ftype)\n    with open(file, \'r\', encoding=encoding) as f:\n        lines = f.readlines()\n    # now iterate throw the lines and try to determine the various indies\n    # first try to find the start of the header: skip over shebang or empty lines\n    keep_first = settings.get(""keepFirst"")\n    block_comment_start_pattern = settings.get(""blockCommentStartPattern"")\n    block_comment_end_pattern = settings.get(""blockCommentEndPattern"")\n    line_comment_start_pattern = settings.get(""lineCommentStartPattern"")\n    i = 0\n    LOGGER.info(""Processing file {} as {}"".format(file, ftype))\n    for line in lines:\n        if i == 0 and keep_first and keep_first.findall(line):\n            skip = i + 1\n        elif emptyPattern.findall(line):\n            pass\n        elif block_comment_start_pattern and block_comment_start_pattern.findall(line):\n            head_start = i\n            break\n        elif line_comment_start_pattern and line_comment_start_pattern.findall(line):\n            head_start = i\n            break\n        elif not block_comment_start_pattern and \\\n                line_comment_start_pattern and \\\n                line_comment_start_pattern.findall(line):\n            head_start = i\n            break\n        else:\n            # we have reached something else, so no header in this file\n            # logging.debug(""Did not find the start giving up at line %s, line is >%s<"",i,line)\n            return {""type"": ftype,\n                    ""lines"": lines,\n                    ""skip"": skip,\n                    ""headStart"": None,\n                    ""headEnd"": None,\n                    ""yearsLine"": None,\n                    ""settings"": settings,\n                    ""haveLicense"": have_license\n                    }\n        i = i + 1\n    LOGGER.debug(""Found preliminary start at {}, i={}, lines={}"".format(head_start, i, len(lines)))\n    # now we have either reached the end, or we are at a line where a block start or line comment occurred\n    # if we have reached the end, return default dictionary without info\n    if i == len(lines):\n        LOGGER.debug(""We have reached the end, did not find anything really"")\n        return {""type"": ftype,\n                ""lines"": lines,\n                ""skip"": skip,\n                ""headStart"": head_start,\n                ""headEnd"": head_end,\n                ""yearsLine"": years_line,\n                ""settings"": settings,\n                ""haveLicense"": have_license\n                }\n    # otherwise process the comment block until it ends\n    if block_comment_start_pattern:\n        LOGGER.debug(""Found comment start, process until end"")\n        for j in range(i, len(lines)):\n            LOGGER.debug(""Checking line {}"".format(j))\n            if licensePattern.findall(lines[j]):\n                have_license = True\n            elif block_comment_end_pattern.findall(lines[j]):\n                return {""type"": ftype,\n                        ""lines"": lines,\n                        ""skip"": skip,\n                        ""headStart"": head_start,\n                        ""headEnd"": j,\n                        ""yearsLine"": years_line,\n                        ""settings"": settings,\n                        ""haveLicense"": have_license\n                        }\n            elif years_pattern.findall(lines[j]):\n                have_license = True\n                years_line = j\n        # if we went through all the lines without finding an end, maybe we have some syntax error or some other\n        # unusual situation, so lets return no header\n        LOGGER.debug(""Did not find the end of a block comment, returning no header"")\n        return {""type"": ftype,\n                ""lines"": lines,\n                ""skip"": skip,\n                ""headStart"": None,\n                ""headEnd"": None,\n                ""yearsLine"": None,\n                ""settings"": settings,\n                ""haveLicense"": have_license\n                }\n    else:\n        LOGGER.debug(""ELSE1"")\n        for j in range(i, len(lines)):\n            if line_comment_start_pattern.findall(lines[j]) and licensePattern.findall(lines[j]):\n                have_license = True\n            elif not line_comment_start_pattern.findall(lines[j]):\n                LOGGER.debug(""ELSE2"")\n                return {""type"": ftype,\n                        ""lines"": lines,\n                        ""skip"": skip,\n                        ""headStart"": i,\n                        ""headEnd"": j - 1,\n                        ""yearsLine"": years_line,\n                        ""settings"": settings,\n                        ""haveLicense"": have_license\n                        }\n            elif years_pattern.findall(lines[j]):\n                have_license = True\n                years_line = j\n        # if we went through all the lines without finding the end of the block, it could be that the whole\n        # file only consisted of the header, so lets return the last line index\n        LOGGER.debug(""RETURN"")\n        return {""type"": ftype,\n                ""lines"": lines,\n                ""skip"": skip,\n                ""headStart"": i,\n                ""headEnd"": len(lines) - 1,\n                ""yearsLine"": years_line,\n                ""settings"": settings,\n                ""haveLicense"": have_license\n                }\n\n\ndef make_backup(file, arguments):\n    """"""\n    Backup file by copying it to a file with the extension .bak appended to the name.\n    :param file: file to back up\n    :param arguments: program args, only backs up, if required by an option\n    :return:\n    """"""\n    if arguments.b:\n        LOGGER.info(""Backing up file {} to {}"".format(file, file + "".bak""))\n        copyfile(file, file + "".bak"")\n\n\ndef apply_license(owner, proj_n, year=None, url=\'google.com\', files_dir=\'./\', tmpl=\'apache-2\', rm_old=True):\n    """"""\n    Apply new license to all files under target dir\n    :param year:\n    :param owner:\n    :param proj_n:\n    :param url:\n    :param files_dir:\n    :param tmpl:\n    :param rm_old:\n    :return:\n    """"""\n    if not year:\n        year = datetime.now().year\n        logger.info(\'year not specific, using year: {}\'.format(year))\n    logger.info(\'owner: {}\'.format(owner))\n    logger.info(\'project name: {}\'.format(proj_n))\n    logger.info(\'start apply license under: {}\'.format(files_dir))\n    additional_extensions = []\n    for t in type_settings:\n        settings = type_settings[t]\n        exts = settings[""extensions""]\n        # if additional file extensions are provided by the user, they are ""merged"" here:\n        if additional_extensions and t in additional_extensions:\n            for aext in additional_extensions[t]:\n                LOGGER.debug(""Enable custom file extension \'%s\' for language \'%s\'"" % (aext, t))\n                exts.append(aext)\n\n        for ext in exts:\n            ext2type[ext] = t\n            patterns.append(""*"" + ext)\n\n    LOGGER.debug(""Allowed file patterns %s"" % patterns)\n    try:\n        error = False\n        template_lines = None\n        start_dir = files_dir\n        settings = dict()\n        settings[""years""] = year\n        settings[""owner""] = owner\n        settings[""projectname""] = proj_n\n        settings[""projecturl""] = url\n\n        # if we have a template name specified, try to get or load the template\n        opt_tmpl = tmpl\n        LOGGER.info(\'using license TEMPLATE: {}\'.format(tmpl))\n        # first get all the names of our own templates\n        # for this get first the path of this file\n        templates_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""templates"")\n        if not os.path.exists(templates_dir):\n            LOGGER.error(""can not found templates dir! {}"".format(templates_dir))\n            exit(-1)\n        LOGGER.info(""File path: {}"".format(os.path.abspath(__file__)))\n        # get all the templates in the templates directory\n        templates = [f for f in get_paths(""*.tmpl"", templates_dir)]\n        templates = [(os.path.splitext(os.path.basename(t))[0], t) for t in templates]\n        # filter by trying to match the name against what was specified\n        tmpls = [t for t in templates if opt_tmpl in t[0]]\n        # check if one of the matching template names is identical to the parameter, then take that one\n        tmpls_eq = [t for t in tmpls if opt_tmpl == t[0]]\n        if len(tmpls_eq) > 0:\n            tmpls = tmpls_eq\n        if len(tmpls) == 1:\n            tmpl_name = tmpls[0][0]\n            tmpl_file = tmpls[0][1]\n            LOGGER.info(""Using template {}"".format(tmpl_name))\n            template_lines = read_template(tmpl_file, settings)\n        else:\n            if len(tmpls) == 0:\n                # check if we can interpret the option as file\n                if os.path.isfile(opt_tmpl):\n                    LOGGER.info(""Using file {}"".format(os.path.abspath(opt_tmpl)))\n                    template_lines = read_template(os.path.abspath(opt_tmpl), settings)\n                else:\n                    LOGGER.error(""Not a built-in template and not a file, cannot proceed: {}"".format(opt_tmpl))\n                    LOGGER.error(""Built in templates: {}"".format("", "".join([t[0] for t in templates])))\n                    error = True\n            else:\n                LOGGER.error(""There are multiple matching template names: {}"".format([t[0] for t in tmpls]))\n                error = True\n\n        if not error:\n            # logging.debug(""Got template lines: %s"",templateLines)\n            # now do the actual processing: if we did not get some error, we have a template loaded or\n            # no template at all\n            # if we have no template, then we will have the years.\n            # now process all the files and either replace the years or replace/add the header\n            LOGGER.info(""Processing directory: {}"".format(start_dir))\n            LOGGER.info(""Patterns: {}"".format(patterns))\n            paths = get_paths(patterns, start_dir)\n            for file in paths:\n                LOGGER.debug(""Processing file: %s"", file)\n                finfo = read_file(file)\n                if not finfo:\n                    LOGGER.debug(""File not supported %s"", file)\n                    continue\n                # logging.debug(""FINFO for the file: %s"", finfo)\n                lines = finfo[""lines""]\n                LOGGER.debug(\n                    ""Info for the file: headStart=%s, headEnd=%s, haveLicense=%s, skip=%s, len=%s, yearsline=%s"",\n                    finfo[""headStart""], finfo[""headEnd""], finfo[""haveLicense""], finfo[""skip""], len(lines),\n                    finfo[""yearsLine""])\n                # if we have a template: replace or add\n                if template_lines:\n                    # make_backup(file, arguments)\n                    with open(file, \'w\', encoding=\'utf-8\') as fw:\n                        # if we found a header, replace it\n                        # otherwise, add it after the lines to skip\n                        head_start = finfo[""headStart""]\n                        head_end = finfo[""headEnd""]\n                        have_license = finfo[""haveLicense""]\n                        ftype = finfo[""type""]\n                        skip = finfo[""skip""]\n                        if head_start is not None and head_end is not None and have_license:\n                            LOGGER.debug(""Replacing header in file {}"".format(file))\n                            # first write the lines before the header\n                            fw.writelines(lines[0:head_start])\n                            #  now write the new header from the template lines\n                            fw.writelines(for_type(template_lines, ftype))\n                            #  now write the rest of the lines\n                            fw.writelines(lines[head_end + 1:])\n                        else:\n                            LOGGER.debug(""Adding header to file {}, skip={}"".format(file, skip))\n                            fw.writelines(lines[0:skip])\n                            fw.writelines(for_type(template_lines, ftype))\n                            fw.writelines(lines[skip:])\n                    # TODO: optionally remove backup if all worked well?\n                else:\n                    # no template lines, just update the line with the year, if we found a year\n                    years_line = finfo[""yearsLine""]\n                    if years_line is not None:\n                        # make_backup(file, arguments)\n                        with open(file, \'w\', encoding=\'utf-8\') as fw:\n                            LOGGER.debug(""Updating years in file {} in line {}"".format(file, years_line))\n                            fw.writelines(lines[0:years_line])\n                            fw.write(years_pattern.sub(year, lines[years_line]))\n                            fw.writelines(lines[years_line + 1:])\n    finally:\n        logging.shutdown()\n'"
alfred/modules/cabinet/split_txt.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\n\nsplit txt file with ratios\n\nalfred cab split -f all.txt -r 0.1,0.8,0.1 -n train,val,test\n\n""""""\nimport os\nimport glob\nfrom alfred.utils.log import logger as logging\nimport numpy as np\n\n\ndef split_txt_file(f, ratios, names):\n    assert os.path.exists(f), \'{} not exist.\'.format(f)\n    if not ratios:\n        ratios = [0.2, 0.8]\n    else:\n        ratios = [float(i) for i in ratios.split(\',\')]\n    logging.info(\'split ratios: {}\'.format(ratios))\n\n    if not names:\n        names = [\'part_{}\'.format(i) for i in range(len(ratios))]\n    else:\n        names = names.split(\',\')\n    names = [i+\'.txt\' for i in names]\n    logging.info(\'split save to names: {}\'.format(names))\n\n    a = sum(ratios)\n    if a != 1.:\n        logging.info(\n            \'ratios: {} does not sum to 1. you must change it first.\'.format(ratios))\n        exit(1)\n\n    # read txt file\n    with open(f, \'r\') as f:\n        lines = f.readlines()\n        lines_no_empty = [i for i in lines if i != \'\' and i != \'\\n\']\n        logging.info(\'to split file have all {} lines. droped {} empty lines.\'.format(len(lines),\n                                                                                      len(lines) - len(lines_no_empty)))\n        lines = lines_no_empty\n        # split with ratios\n        last_lines = 0\n        for i, r in enumerate(ratios):\n            one = lines[last_lines: last_lines+int(r * len(lines))]\n            with open(names[i], \'w\') as ff:\n                ff.writelines(one)\n                logging.info(\'Part {} saved into: {}. portion: {}/{}={}\'.format(\n                    i, names[i], len(one), len(lines), len(one)/(len(lines))))\n            last_lines += len(one)\n    logging.info(\'split done.\')\n'"
alfred/modules/cabinet/stack_imgs.py,0,"b'""""""\n\nstack images in matrix style\n\n""""""\nimport cv2\nimport numpy as np\nfrom alfred.utils.log import logger as logging\n\n\n\ndef check_shape_resize_if_possible(imgs):\n    shapes = [i.shape for i in imgs]\n    if len(set(shapes))==1:\n        return imgs\n    else:\n        logging.info(\'detected images shape not equal, resize to the first shape...\')\n        imgs = [cv2.resize(i, (shapes[0][1], shapes[0][0])) for i in imgs]\n        return imgs\n\n\n\ndef stack_imgs(imgs_list, dim2d):\n    """"""\n    send a list of images\n    then using dim2d to stack it\n\n    for example:\n        a.png\n        b.png\n        c.png\n        d.png\n    \n    dim2d:\n        2x2\n    """"""\n    a = int(dim2d.split(\'x\')[0])\n    b = int(dim2d.split(\'x\')[1])\n    if len(imgs_list) % a != 0 or len(imgs_list) % b:\n        logging.info(\'dim2d {} is not applicable for {} images.\'.format(dim2d, len(imgs_list)))\n        exit(0)\n    elif len(imgs_list) != a*b:\n        logging.error(\'len imgs not equal to: axb={}\'.format(a*b))\n        exit(0)\n    else:\n        imgs_list = [cv2.imread(i) for i in imgs_list]\n        all_raws = []\n        # 2x1 bug?\n        for ri in range(a):\n            one_raw = []\n            for ci in range(b):\n                one_raw.append(imgs_list[ri*b + ci])\n                logging.info(\'stacking row: {}, with len: {}\'.format(ri, len(one_raw)))\n            imgs = check_shape_resize_if_possible(one_raw)\n            img_a = np.hstack(imgs)\n            all_raws.append(img_a)\n        all_raws = check_shape_resize_if_possible(all_raws)\n        final_img = np.vstack(all_raws)\n        logging.info(\'final combined img shape: {}\'.format(final_img.shape))\n        cv2.imwrite(\'stacked_img.jpg\', final_img)\n        logging.info(\'done.\')\n\n'"
alfred/modules/data/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/modules/data/coco2voc.py,0,b''
alfred/modules/data/convert_csv2voc.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nConvert a CSV labeling dataset to VOC format\n\nCARDS_COURTYARD_B_T_frame_0011.jpg,1280,720,yourleft,647,453,824,551\nCARDS_COURTYARD_B_T_frame_0011.jpg,1280,720,yourright,515,431,622,543\n\nassuming images and csv under same folder.\n\n""""""\nimport os\nimport sys\nimport glob\nimport numpy as np\nfrom PIL import Image\nfrom lxml.etree import Element, SubElement, tostring, ElementTree, tostring\n\n\n\n\ndef convert_one_csv_to_xml(csv_f, img_f):\n    if os.path.exists(csv_f):\n        csv_anno = np.loadtxt(csv_f)\n        if len(csv_anno.shape) < 2 and csv_anno.shape[0] != 0:\n            csv_anno = np.expand_dims(csv_anno, axis=0)\n        target_path = os.path.join(os.path.dirname(csv_f), os.path.basename(csv_f).split(\'.\')[0]+\'.xml\')\n        # convert xml \n        if os.path.exists(img_f):\n            im = Image.open(img_f)\n            width = im.size[0]\n            height = im.size[1]\n            node_root = Element(\'annotation\')\n            node_folder = SubElement(node_root, \'folder\')\n            node_folder.text = \'images\'\n            node_filename = SubElement(node_root, \'filename\')\n            node_filename.text = os.path.basename(img_f)\n            node_size = SubElement(node_root, \'size\')\n            node_width = SubElement(node_size, \'width\')\n            node_width.text = str(width)\n            node_height = SubElement(node_size, \'height\')\n            node_height.text = str(height)\n            node_depth = SubElement(node_size, \'depth\')\n            node_depth.text = \'3\'\n            \n            for item in csv_anno:\n                node_object = SubElement(node_root, \'object\')\n                node_name = SubElement(node_object, \'name\')\n                node_name.text = label_map[item[0]]\n                node_difficult = SubElement(node_object, \'difficult\')\n                node_difficult.text = \'0\'\n                node_bndbox = SubElement(node_object, \'bndbox\')\n                node_xmin = SubElement(node_bndbox, \'xmin\')\n                node_xmin.text = str(int(item[1]*width))\n                node_ymin = SubElement(node_bndbox, \'ymin\')\n                node_ymin.text = str(int(item[1]*height))\n                node_xmax = SubElement(node_bndbox, \'xmax\')\n                node_xmax.text = str(int(item[2]*width))\n                node_ymax = SubElement(node_bndbox, \'ymax\')\n                node_ymax.text = str(int(item[3]*height))\n            f = open(target_path, \'wb\')\n            f.write(tostring(node_root, pretty_print=True))\n            f.close()\n        else:\n            print(\'image: {} not exist.\'.format(img_f))\n    else:\n        print(\'!! {} not exist.\'.format(csv_f))'"
alfred/modules/data/convert_cvat2voc.py,0,"b'#!/usr/bin/env python\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nGiven a CVAT XML and a directory with the image dataset, this script reads the\nCVAT XML and writes the annotations in PASCAL VOC format into a given\ndirectory.\n\nThis implementation supports both interpolation tracks from video and \nannotated images.  If it encounters any tracks or annotations that are \nnot bounding boxes, it ignores them.\n""""""\n\nimport os\nimport argparse\nfrom alfred.utils.log import logger as log\nfrom lxml import etree\nfrom pascal_voc_writer import Writer\nfrom collections import OrderedDict\n\n\ndef parse_args():\n    """"""Parse arguments of command line""""""\n    parser = argparse.ArgumentParser(\n        description=\'Convert CVAT XML annotations to PASCAL VOC format\'\n    )\n\n    parser.add_argument(\n        \'--cvat-xml\', metavar=\'FILE\', required=True,\n        help=\'input file with CVAT annotation in xml format\'\n    )\n\n    parser.add_argument(\n        \'--image-dir\', metavar=\'DIRECTORY\', required=True,\n        help=\'directory which contains original images\'\n    )\n\n    parser.add_argument(\n        \'--output-dir\', metavar=\'DIRECTORY\', required=True,\n        help=\'directory for output annotations in PASCAL VOC format\'\n    )\n\n    return parser.parse_args()\n\n\ndef process_cvat_xml(xml_file, image_dir, output_dir):\n    """"""\n    Transforms a single XML in CVAT format to multiple PASCAL VOC format\n    XMls.\n\n    :param xml_file: CVAT format XML\n    :param image_dir: image directory of the dataset\n    :param output_dir: directory of annotations with PASCAL VOC format\n    :return:\n    """"""\n    KNOWN_TAGS = {\'box\', \'image\', \'attribute\'}\n    os.makedirs(output_dir, exist_ok=True)\n    cvat_xml = etree.parse(xml_file)\n\n    basename = os.path.splitext( os.path.basename( xml_file ) )[0]\n\n    tracks= cvat_xml.findall( \'.//track\' )\n    log.info(\'tracks: {}\'.format(tracks))\n\n    if (tracks is not None) and (len(tracks) > 0):\n        frames = {}\n\n        for track in tracks:\n            trackid = int(track.get(""id""))\n            label = track.get(""label"")\n            boxes = track.findall( \'./box\' )\n            for box in boxes:\n                frameid  = int(box.get(\'frame\'))\n                outside  = int(box.get(\'outside\'))\n                #occluded = int(box.get(\'occluded\'))  #currently unused\n                #keyframe = int(box.get(\'keyframe\'))  #currently unused\n                xtl      = float(box.get(\'xtl\'))\n                ytl      = float(box.get(\'ytl\'))\n                xbr      = float(box.get(\'xbr\'))\n                ybr      = float(box.get(\'ybr\'))\n                \n                frame = frames.get( frameid, {} )\n                \n                if outside == 0:\n                    frame[ trackid ] = { \'xtl\': xtl, \'ytl\': ytl, \'xbr\': xbr, \'ybr\': ybr, \'label\': label }\n\n                frames[ frameid ] = frame\n\n        width = int(cvat_xml.find(\'.//original_size/width\').text)\n        height  = int(cvat_xml.find(\'.//original_size/height\').text)\n\n        # Spit out a list of each object for each frame\n        for frameid in sorted(frames.keys()):\n            #print( frameid )\n\n            image_name = ""%s_%08d.jpg"" % (basename, frameid)\n            image_path = os.path.join(image_dir, image_name)\n            if not os.path.exists(image_path):\n                log.info(\'{} image cannot be found. Is `{}` image directory correct?\'.\n                    format(image_path, image_dir))\n            writer = Writer(image_path, width, height)\n\n            frame = frames[frameid]\n\n            objids = sorted(frame.keys())\n\n            for objid in objids:\n\n                box = frame[objid]\n\n                label = box.get(\'label\')\n                xmin = float(box.get(\'xtl\'))\n                ymin = float(box.get(\'ytl\'))\n                xmax = float(box.get(\'xbr\'))\n                ymax = float(box.get(\'ybr\'))\n\n                writer.addObject(label, xmin, ymin, xmax, ymax)\n\n            anno_name = os.path.basename(os.path.splitext(image_name)[0] + \'.xml\')\n            anno_dir = os.path.dirname(os.path.join(output_dir, image_name))\n            os.makedirs(anno_dir, exist_ok=True)\n            writer.save(os.path.join(anno_dir, anno_name))\n\n    else:\n        for img_tag in cvat_xml.findall(\'image\'):\n            image_name = img_tag.get(\'name\')\n            width = img_tag.get(\'width\')\n            height = img_tag.get(\'height\')\n            depth = img_tag.get(\'depth\', 3)\n            image_path = os.path.join(image_dir, image_name)\n            if not os.path.exists(image_path):\n                log.info(\'{} image cannot be found. Is `{}` image directory correct?\'.\n                    format(image_path, image_dir))\n            writer = Writer(image_path, width, height, depth=depth)\n\n            unknown_tags = {x.tag for x in img_tag.iter()}.difference(KNOWN_TAGS)\n            if unknown_tags:\n                log.info(\'Ignoring tags for image {}: {}\'.format(image_path, unknown_tags))\n\n            for box in img_tag.findall(\'box\'):\n                label = box.get(\'label\')\n                # concat label with attributes\n                # todo: check if exist or not\n                all_attributes = box.findall(\'attribute\')\n                attr_dict = OrderedDict()\n                for attr in all_attributes:\n                    attr_dict[attr.get(\'name\')] = attr.text\n                lst = sorted(attr_dict.items(), key=lambda item: item[0])\n                attr_dict = OrderedDict(lst)\n                # label += \'_\' + \'_\'.join(attr_dict.values())\n                # we only take color for now\n                label = label.replace(\'_\', \'\')\n                label += \'_\' + list(attr_dict.values())[0]\n                # log.info(label)\n                xmin = float(box.get(\'xtl\'))\n                ymin = float(box.get(\'ytl\'))\n                xmax = float(box.get(\'xbr\'))\n                ymax = float(box.get(\'ybr\'))\n\n                writer.addObject(label, xmin, ymin, xmax, ymax)\n\n            anno_name = os.path.basename(os.path.splitext(image_name)[0] + \'.xml\')\n            anno_dir = os.path.dirname(os.path.join(output_dir, image_name))\n            os.makedirs(anno_dir, exist_ok=True)\n            writer.save(os.path.join(anno_dir, anno_name))\n\n\ndef main():\n    args = parse_args()\n    process_cvat_xml(args.cvat_xml, args.image_dir, args.output_dir)\n\n\nif __name__ == ""__main__"":\n    main()'"
alfred/modules/data/convert_labelone2voc.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nconvert labelone to voc format\n\n<annotation>\n  <folder>VOC2007</folder>\n  <filename>000002.jpg</filename>\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0//\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xc2\xa0\xc2\xa0\n  <size>\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0//\xe5\x9b\xbe\xe5\x83\x8f\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x88\xe9\x95\xbf\xe5\xae\xbd\xe4\xbb\xa5\xe5\x8f\x8a\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    <width>335</width>\n    <height>500</height>\n    <depth>3</depth>\n  </size>\n  <object>\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0//\xe6\xa3\x80\xe6\xb5\x8b\xe5\x88\xb0\xe7\x9a\x84\xe7\x89\xa9\xe4\xbd\x93\n    <name>cat</name>\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0//\xe7\x89\xa9\xe4\xbd\x93\xe7\xb1\xbb\xe5\x88\xab\n    <pose>Unspecified</pose>\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0//\xe6\x8b\x8d\xe6\x91\x84\xe8\xa7\x92\xe5\xba\xa6\n    <truncated>0</truncated>\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0//\xe6\x98\xaf\xe5\x90\xa6\xe8\xa2\xab\xe6\x88\xaa\xe6\x96\xad\xef\xbc\x880\xe8\xa1\xa8\xe7\xa4\xba\xe5\xae\x8c\xe6\x95\xb4\n    <difficult>0</difficult>\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0//\xe7\x9b\xae\xe6\xa0\x87\xe6\x98\xaf\xe5\x90\xa6\xe9\x9a\xbe\xe4\xbb\xa5\xe8\xaf\x86\xe5\x88\xab\xef\xbc\x880\xe8\xa1\xa8\xe7\xa4\xba\xe5\xae\xb9\xe6\x98\x93\xe8\xaf\x86\xe5\x88\xab\xef\xbc\x89\n    <bndbox>\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0//bounding-box\xef\xbc\x88\xe5\x8c\x85\xe5\x90\xab\xe5\xb7\xa6\xe4\xb8\x8b\xe8\xa7\x92\xe5\x92\x8c\xe5\x8f\xb3\xe4\xb8\x8a\xe8\xa7\x92xy\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x89\n      <xmin>139</xmin>\n      <ymin>200</ymin>\n      <xmax>207</xmax>\n      <ymax>301</ymax>\n    </bndbox>\n  </object>\n</annotation>\n\n\n""""""\nimport os\nimport json\nimport glob\nimport sys\nfrom lxml.etree import Element, SubElement, tostring, ElementTree, tostring\nfrom lxml import etree\nfrom PIL import Image\n\n\ndef convert_one(a):\n    os.makedirs(os.path.dirname(a)+\'_voc\', exist_ok=True)\n    d = json.load(open(a))\n    print(d)\n\n    target_path = os.path.join(os.path.dirname(a)+\'_voc\', os.path.basename(a).split(\'.\')[0]+\'.xml\')\n    img_path = os.path.join(\'images\', d[\'imagePath\'])\n    # convert xml \n    if os.path.exists(img_path):\n        im = Image.open(img_path)\n        width = im.size[0]\n        height = im.size[1]\n        node_root = Element(\'annotation\')\n        node_folder = SubElement(node_root, \'folder\')\n        node_folder.text = \'images\'\n        node_filename = SubElement(node_root, \'filename\')\n        node_filename.text = d[\'imagePath\']\n        node_size = SubElement(node_root, \'size\')\n        node_width = SubElement(node_size, \'width\')\n        node_width.text = str(width)\n        node_height = SubElement(node_size, \'height\')\n        node_height.text = str(height)\n        node_depth = SubElement(node_size, \'depth\')\n        node_depth.text = \'3\'\n        \n        for item in d[\'shapes\']:\n            node_object = SubElement(node_root, \'object\')\n            node_name = SubElement(node_object, \'name\')\n            node_name.text = item[\'label\']\n            node_difficult = SubElement(node_object, \'difficult\')\n            node_difficult.text = \'0\'\n            node_bndbox = SubElement(node_object, \'bndbox\')\n            node_xmin = SubElement(node_bndbox, \'xmin\')\n            node_xmin.text = str(item[\'points\'][1][0])\n            node_ymin = SubElement(node_bndbox, \'ymin\')\n            node_ymin.text = str(item[\'points\'][1][1])\n            node_xmax = SubElement(node_bndbox, \'xmax\')\n            node_xmax.text = str(item[\'points\'][3][0])\n            node_ymax = SubElement(node_bndbox, \'ymax\')\n            node_ymax.text = str(item[\'points\'][3][1])\n        f = open(target_path, \'wb\')\n        f.write(etree.tostring(node_root, pretty_print=True))\n        f.close()\n    else:\n        print(\'xxx {} annotations according image: {} not exist.\'.format(a, img_path))\n\n\n\ndef run():\n    all_json_files = glob.glob(os.path.join(sys.argv[1], \'*.json\'))\n    print(len(all_json_files))\n    for i in all_json_files:\n        convert_one(i)\n    print(\'done!\')\n\n\n\nif __name__ == ""__main__"":\n    run()'"
alfred/modules/data/eval_coco.py,0,"b'""""""\n\nsend 2 json files contains \ninstances_gt.json and instances_generated.json\n\nthis will calculate a mAP of coco\nthen output the final result\n\n""""""'"
alfred/modules/data/eval_voc.py,0,"b'""""""\nsend 2 files\n\nground truth\nbottle 6 234 45 362\nperson 1 156 103 336\nperson 36 111 198 416\nperson 91 42 338 500\n\ndetections:\n\nbottle 0.14981 80 1 295 500  \nbus 0.12601 36 13 404 316  \nhorse 0.12526 430 117 500 307  \npottedplant 0.14585 212 78 292 118  \ntvmonitor 0.070565 388 89 500 196 \n\n\nit will eval on txt format files\n\n""""""\nimport glob\nimport json\nimport os\nimport shutil\nimport operator\nimport sys\nimport argparse\nimport math\nimport sys\nimport os\nimport glob\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport cv2\nfrom alfred.utils.log import logger as logging\nimport matplotlib.pyplot as plt\n\n\n\n\ndef log_average_miss_rate(precision, fp_cumsum, num_images):\n    """"""\n        log-average miss rate:\n            Calculated by averaging miss rates at 9 evenly spaced FPPI points\n            between 10e-2 and 10e0, in log-space.\n\n        output:\n                lamr | log-average miss rate\n                mr | miss rate\n                fppi | false positives per image\n\n        references:\n            [1] Dollar, Piotr, et al. ""Pedestrian Detection: An Evaluation of the\n               State of the Art."" Pattern Analysis and Machine Intelligence, IEEE\n               Transactions on 34.4 (2012): 743 - 761.\n    """"""\n    # if there were no detections of that class\n    if precision.size == 0:\n        lamr = 0\n        mr = 1\n        fppi = 0\n        return lamr, mr, fppi\n    fppi = fp_cumsum / float(num_images)\n    mr = (1 - precision)\n\n    fppi_tmp = np.insert(fppi, 0, -1.0)\n    mr_tmp = np.insert(mr, 0, 1.0)\n    # Use 9 evenly spaced reference points in log-space\n    ref = np.logspace(-2.0, 0.0, num=9)\n    for i, ref_i in enumerate(ref):\n        # np.where() will always find at least 1 index, since min(ref) = 0.01 and min(fppi_tmp) = -1.0\n        j = np.where(fppi_tmp <= ref_i)[-1][-1]\n        ref[i] = mr_tmp[j]\n\n    # log(0) is undefined, so we use the np.maximum(1e-10, ref)\n    lamr = math.exp(np.mean(np.log(np.maximum(1e-10, ref))))\n    return lamr, mr, fppi\n\n\ndef error(msg):\n    print(msg)\n    sys.exit(0)\n\n\ndef is_float_between_0_and_1(value):\n    try:\n        val = float(value)\n        if val > 0.0 and val < 1.0:\n            return True\n        else:\n            return False\n    except ValueError:\n        return False\n\n\n""""""\n Calculate the AP given the recall and precision array\n    1st) We compute a version of the measured precision/recall curve with\n         precision monotonically decreasing\n    2nd) We compute the AP as the area under this curve by numerical integration.\n""""""\n\n\ndef voc_ap(rec, prec):\n    """"""\n    --- Official matlab code VOC2012---\n    mrec=[0 ; rec ; 1];\n    mpre=[0 ; prec ; 0];\n    for i=numel(mpre)-1:-1:1\n            mpre(i)=max(mpre(i),mpre(i+1));\n    end\n    i=find(mrec(2:end)~=mrec(1:end-1))+1;\n    ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n    """"""\n    rec.insert(0, 0.0)  # insert 0.0 at begining of list\n    rec.append(1.0)  # insert 1.0 at end of list\n    mrec = rec[:]\n    prec.insert(0, 0.0)  # insert 0.0 at begining of list\n    prec.append(0.0)  # insert 0.0 at end of list\n    mpre = prec[:]\n    """"""\n     This part makes the precision monotonically decreasing\n        (goes from the end to the beginning)\n        matlab: for i=numel(mpre)-1:-1:1\n                    mpre(i)=max(mpre(i),mpre(i+1));\n    """"""\n    # matlab indexes start in 1 but python in 0, so I have to do:\n    #     range(start=(len(mpre) - 2), end=0, step=-1)\n    # also the python function range excludes the end, resulting in:\n    #     range(start=(len(mpre) - 2), end=-1, step=-1)\n    for i in range(len(mpre)-2, -1, -1):\n        mpre[i] = max(mpre[i], mpre[i+1])\n    """"""\n     This part creates a list of indexes where the recall changes\n        matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;\n    """"""\n    i_list = []\n    for i in range(1, len(mrec)):\n        if mrec[i] != mrec[i-1]:\n            i_list.append(i)  # if it was matlab would be i + 1\n    """"""\n     The Average Precision (AP) is the area under the curve\n        (numerical integration)\n        matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n    """"""\n    ap = 0.0\n    for i in i_list:\n        ap += ((mrec[i]-mrec[i-1])*mpre[i])\n    return ap, mrec, mpre\n\n\ndef draw_text_in_image(img, text, pos, color, line_width):\n    font = cv2.FONT_HERSHEY_PLAIN\n    fontScale = 1\n    lineType = 1\n    bottomLeftCornerOfText = pos\n    cv2.putText(img, text,\n                bottomLeftCornerOfText,\n                font,\n                fontScale,\n                color,\n                lineType)\n    text_width, _ = cv2.getTextSize(text, font, fontScale, lineType)[0]\n    return img, (line_width + text_width)\n\n\ndef adjust_axes(r, t, fig, axes):\n    # get text width for re-scaling\n    bb = t.get_window_extent(renderer=r)\n    text_width_inches = bb.width / fig.dpi\n    # get axis width in inches\n    current_fig_width = fig.get_figwidth()\n    new_fig_width = current_fig_width + text_width_inches\n    propotion = new_fig_width / current_fig_width\n    # get axis limit\n    x_lim = axes.get_xlim()\n    axes.set_xlim([x_lim[0], x_lim[1]*propotion])\n\n\ndef draw_plot_func(dictionary, n_classes, window_title, plot_title, x_label, output_path, to_show, plot_color, true_p_bar):\n    # sort the dictionary by decreasing value, into a list of tuples\n    sorted_dic_by_value = sorted(\n        dictionary.items(), key=operator.itemgetter(1))\n    # unpacking the list of tuples into two lists\n    sorted_keys, sorted_values = zip(*sorted_dic_by_value)\n    #\n    if true_p_bar != """":\n        """"""\n         Special case to draw in:\n            - green -> TP: True Positives (object detected and matches ground-truth)\n            - red -> FP: False Positives (object detected but does not match ground-truth)\n            - orange -> FN: False Negatives (object not detected but present in the ground-truth)\n        """"""\n        fp_sorted = []\n        tp_sorted = []\n        for key in sorted_keys:\n            fp_sorted.append(dictionary[key] - true_p_bar[key])\n            tp_sorted.append(true_p_bar[key])\n        plt.barh(range(n_classes), fp_sorted, align=\'center\',\n                 color=\'crimson\', label=\'False Positive\')\n        plt.barh(range(n_classes), tp_sorted, align=\'center\',\n                 color=\'forestgreen\', label=\'True Positive\', left=fp_sorted)\n        # add legend\n        plt.legend(loc=\'lower right\')\n        """"""\n         Write number on side of bar\n        """"""\n        fig = plt.gcf()  # gcf - get current figure\n        axes = plt.gca()\n        r = fig.canvas.get_renderer()\n        for i, val in enumerate(sorted_values):\n            fp_val = fp_sorted[i]\n            tp_val = tp_sorted[i]\n            fp_str_val = "" "" + str(fp_val)\n            tp_str_val = fp_str_val + "" "" + str(tp_val)\n            # trick to paint multicolor with offset:\n            # first paint everything and then repaint the first number\n            t = plt.text(val, i, tp_str_val, color=\'forestgreen\',\n                         va=\'center\', fontweight=\'bold\')\n            plt.text(val, i, fp_str_val, color=\'crimson\',\n                     va=\'center\', fontweight=\'bold\')\n            if i == (len(sorted_values)-1):  # largest bar\n                adjust_axes(r, t, fig, axes)\n    else:\n        plt.barh(range(n_classes), sorted_values, color=plot_color)\n        """"""\n         Write number on side of bar\n        """"""\n        fig = plt.gcf()  # gcf - get current figure\n        axes = plt.gca()\n        r = fig.canvas.get_renderer()\n        for i, val in enumerate(sorted_values):\n            str_val = "" "" + str(val)  # add a space before\n            if val < 1.0:\n                str_val = "" {0:.2f}"".format(val)\n            t = plt.text(val, i, str_val, color=plot_color,\n                         va=\'center\', fontweight=\'bold\')\n            # re-set axes to show number inside the figure\n            if i == (len(sorted_values)-1):  # largest bar\n                adjust_axes(r, t, fig, axes)\n    # set window title\n    fig.canvas.set_window_title(window_title)\n    # write classes in y axis\n    tick_font_size = 12\n    plt.yticks(range(n_classes), sorted_keys, fontsize=tick_font_size)\n    """"""\n     Re-scale height accordingly\n    """"""\n    init_height = fig.get_figheight()\n    # comput the matrix height in points and inches\n    dpi = fig.dpi\n    height_pt = n_classes * (tick_font_size * 1.4)  # 1.4 (some spacing)\n    height_in = height_pt / dpi\n    # compute the required figure height\n    top_margin = 0.15  # in percentage of the figure height\n    bottom_margin = 0.05  # in percentage of the figure height\n    figure_height = height_in / (1 - top_margin - bottom_margin)\n    # set new height\n    if figure_height > init_height:\n        fig.set_figheight(figure_height)\n\n    # set plot title\n    plt.title(plot_title, fontsize=14)\n    # set axis titles\n    # plt.xlabel(\'classes\')\n    plt.xlabel(x_label, fontsize=\'large\')\n    # adjust size of window\n    fig.tight_layout()\n    # save the plot\n    fig.savefig(output_path)\n    # show image\n    if to_show:\n        plt.show()\n    # close the plot\n    plt.close()\n\n\ndef load_txt_or_xml_format(t_f):\n    if t_f.endswith(\'txt\'):\n        # open txt file lines to a list\n        with open(t_f) as f:\n            content = f.readlines()\n        # remove whitespace characters like `\\n` at the end of each line\n        content = [x.strip() for x in content]\n        return content\n    elif t_f.endswith(\'xml\'):\n        root = ET.parse(t_f).getroot()\n        all_gts = []\n        for obj in root.findall(\'object\'):\n            obj_name = obj.find(\'name\').text\n            bndbox = obj.find(\'bndbox\')\n            left = bndbox.find(\'xmin\').text\n            top = bndbox.find(\'ymin\').text\n            right = bndbox.find(\'xmax\').text\n            bottom = bndbox.find(\'ymax\').text\n            all_gts.append(\' \'.join([obj_name, left, top, right, bottom]))\n        return all_gts\n    else:\n        logging.error(\'unsupported gt file format.\')\n        exit(0)\n\n\ndef eval_voc(args):\n    if args.ignore is None:\n        args.ignore = []\n\n    specific_iou_flagged = False\n    if args.set_class_iou is not None:\n        specific_iou_flagged = True\n\n    GT_PATH = args.gt_dir\n    DR_PATH = args.det_dir\n    IMG_PATH = args.images_dir\n    MINOVERLAP = args.min_overlap\n\n    logging.info(\'Ground truth dir: {}\'.format(GT_PATH))\n    logging.info(\'Detection result dir: {}\'.format(DR_PATH))\n    logging.info(\'Images dir: {}\'.format(IMG_PATH))\n    logging.info(\'Min overlap: {}\'.format(MINOVERLAP))\n\n    if os.path.exists(IMG_PATH):\n        for dirpath, dirnames, files in os.walk(IMG_PATH):\n            if not files:\n                # no image files found\n                args.no_animation = True\n    else:\n        args.no_animation = True\n    show_animation = False\n    if not args.no_animation:\n        try:\n            import cv2\n            show_animation = True\n        except ImportError:\n            print(""\\""opencv-python\\"" not found, please install to visualize the results."")\n            args.no_animation = True\n    draw_plot = False\n    if not args.no_plot:\n        try:\n            import matplotlib.pyplot as plt\n            draw_plot = True\n        except ImportError:\n            print(\n                ""\\""matplotlib\\"" not found, please install it to get the resulting plots."")\n            args.no_plot = True\n\n    TEMP_FILES_PATH = os.path.join(os.path.dirname(GT_PATH), ""temp_files"")\n    logging.info(\'creating a temp path: {}\'.format(os.path.abspath(TEMP_FILES_PATH)))\n    if not os.path.exists(TEMP_FILES_PATH):  # if it doesn\'t exist already\n        os.makedirs(TEMP_FILES_PATH)\n    results_files_path = ""./results_{}"".format(MINOVERLAP)\n    if os.path.exists(results_files_path):  # if it exist already\n        # reset the results directory\n        shutil.rmtree(results_files_path)\n\n    os.makedirs(results_files_path)\n    if draw_plot:\n        os.makedirs(os.path.join(results_files_path, ""classes""))\n    if show_animation:\n        os.makedirs(os.path.join(results_files_path,\n                                 ""images"", ""detections_one_by_one""))\n\n    # get a list with the ground-truth files\n    # Make can solve both txt ground truth and xml ground truth\n    if os.path.isfile(GT_PATH):\n        logging.info(\'{} is a file, eval on coco not support now.\'.format(GT_PATH))\n        exit(0)\n    else:\n        all_files_gt = os.listdir(GT_PATH)\n        ground_truth_files_list = []\n        gt_format = \'txt\'\n        if all_files_gt[0].endswith(\'txt\'):\n            logging.info(\'detected your ground truth were txt format, start eval....\')\n            ground_truth_files_list = glob.glob(os.path.join(GT_PATH, \'*.txt\'))\n        elif all_files_gt[0].endswith(\'xml\'):\n            logging.info(\'detected your ground truth were xml format, start eval....\')\n            ground_truth_files_list = glob.glob(os.path.join(GT_PATH, \'*.xml\'))\n            gt_format = \'xml\'\n        else:\n            logging.error(\'unsupported ground truth format, pls using xml or txt as ground truth format.\')\n            exit(0)\n\n    if len(ground_truth_files_list) == 0:\n        logging.error(""Error: No ground-truth files found!"")\n        exit(0)\n    ground_truth_files_list.sort()\n    # dictionary with counter per class\n    gt_counter_per_class = {}\n    counter_images_per_class = {}\n\n    # todo: Ground truth can be txt or xml both can be converted\n    for gt_file in ground_truth_files_list:\n        file_id = os.path.basename(gt_file).split(\'.\')[0]\n        # check if there is a correspondent detection-results file\n        temp_path = os.path.join(DR_PATH, (file_id + "".txt""))\n        if not os.path.exists(temp_path):\n            error_msg = ""Error. File not found: {}\\n"".format(temp_path)\n            error_msg += ""(You can avoid this error message by running extra/intersect-gt-and-dr.py)""\n            logging.error(error_msg)\n        lines_list = load_txt_or_xml_format(gt_file)\n        # create ground-truth dictionary\n        bounding_boxes = []\n        is_difficult = False\n        already_seen_classes = []\n        for line in lines_list:\n            try:\n                if ""difficult"" in line:\n                    class_name, left, top, right, bottom, _difficult = line.split()\n                    is_difficult = True\n                else:\n                    class_name, left, top, right, bottom = line.split()\n            except ValueError:\n                error_msg = ""Error: File "" + gt_file + "" in the wrong format.\\n""\n                error_msg += "" Expected: <class_name> <left> <top> <right> <bottom> [\'difficult\']\\n""\n                error_msg += "" Received: "" + line\n                error_msg += ""\\n\\nIf you have a <class_name> with spaces between words you should remove them\\n""\n                error_msg += ""by running the script \\""remove_space.py\\"" or \\""rename_class.py\\"" in the \\""extra/\\"" folder.""\n                logging.error(error_msg)\n            # check if class is in the ignore list, if yes skip\n            if class_name in args.ignore:\n                continue\n            bbox = left + "" "" + top + "" "" + right + "" "" + bottom\n            if is_difficult:\n                bounding_boxes.append(\n                    {""class_name"": class_name, ""bbox"": bbox, ""used"": False, ""difficult"": True})\n                is_difficult = False\n            else:\n                bounding_boxes.append(\n                    {""class_name"": class_name, ""bbox"": bbox, ""used"": False})\n                # count that object\n                if class_name in gt_counter_per_class:\n                    gt_counter_per_class[class_name] += 1\n                else:\n                    # if class didn\'t exist yet\n                    gt_counter_per_class[class_name] = 1\n\n                if class_name not in already_seen_classes:\n                    if class_name in counter_images_per_class:\n                        counter_images_per_class[class_name] += 1\n                    else:\n                        # if class didn\'t exist yet\n                        counter_images_per_class[class_name] = 1\n                    already_seen_classes.append(class_name)\n        # dump bounding_boxes into a "".json"" file\n        with open(TEMP_FILES_PATH + ""/"" + file_id + ""_ground_truth.json"", \'w\') as outfile:\n            json.dump(bounding_boxes, outfile)\n\n    gt_classes = list(gt_counter_per_class.keys())\n    logging.info(\'gt_classes gathered: {}\'.format(gt_classes))\n    # let\'s sort the classes alphabetically\n    gt_classes = sorted(gt_classes)\n    n_classes = len(gt_classes)\n    # print(gt_classes)\n    # print(gt_counter_per_class)\n\n    if specific_iou_flagged:\n        n_args = len(args.set_class_iou)\n        error_msg = \\\n            \'\\n --set-class-iou [class_1] [IoU_1] [class_2] [IoU_2] [...]\'\n        if n_args % 2 != 0:\n            logging.error(\'Error, missing arguments. Flag usage:\' + error_msg)\n        # [class_1] [IoU_1] [class_2] [IoU_2]\n        # specific_iou_classes = [\'class_1\', \'class_2\']\n        specific_iou_classes = args.set_class_iou[::2]  # even\n        # iou_list = [\'IoU_1\', \'IoU_2\']\n        iou_list = args.set_class_iou[1::2]  # odd\n        if len(specific_iou_classes) != len(iou_list):\n            logging.error(\'Error, missing arguments. Flag usage:\' + error_msg)\n        for tmp_class in specific_iou_classes:\n            if tmp_class not in gt_classes:\n                logging.error(\'Error, unknown class \\""\' + tmp_class +\n                      \'\\"". Flag usage:\' + error_msg)\n        for num in iou_list:\n            if not is_float_between_0_and_1(num):\n                logging.error(\'Error, IoU must be between 0.0 and 1.0. Flag usage:\' + error_msg)\n\n    # get a list with the detection-results files\n    dr_files_list = glob.glob(os.path.join(DR_PATH, \'*.txt\'))\n    logging.info(\'detection files detected: {}, vs ground truth: {}\'.format(len(dr_files_list), len(ground_truth_files_list)))\n    dr_files_list.sort()\n\n    for class_index, class_name in enumerate(gt_classes):\n        bounding_boxes = []\n        for txt_file in dr_files_list:\n            # print(txt_file)\n            # the first time it checks if all the corresponding ground-truth files exist\n            file_id = os.path.basename(txt_file).split(""."")[0]\n            temp_path = \'\'\n            if gt_format == \'txt\':\n                temp_path = os.path.join(GT_PATH, (file_id + "".txt""))\n            elif gt_format == \'xml\':\n                temp_path = os.path.join(GT_PATH, (file_id + "".xml""))\n\n            if class_index == 0:\n                if not os.path.exists(temp_path):\n                    error_msg = ""Error. according ground truth File not found: {}\\n"".format(temp_path)\n                    error_msg += ""(You can avoid this error message by running extra/intersect-gt-and-dr.py)""\n                    logging.error(error_msg)\n            lines = load_txt_or_xml_format(txt_file)\n            for line in lines:\n                try:\n                    tmp_class_name, confidence, left, top, right, bottom = line.split()\n                except ValueError:\n                    error_msg = ""Error: File "" + txt_file + "" in the wrong format.\\n""\n                    error_msg += "" Expected: <class_name> <confidence> <left> <top> <right> <bottom>\\n""\n                    error_msg += "" Received: "" + line\n                    logging.error(error_msg)\n                if tmp_class_name == class_name:\n                    # print(""match"")\n                    bbox = left + "" "" + top + "" "" + right + "" "" + bottom\n                    bounding_boxes.append(\n                        {""confidence"": confidence, ""file_id"": file_id, ""bbox"": bbox})\n                    # print(bounding_boxes)\n        # sort detection-results by decreasing confidence\n        bounding_boxes.sort(key=lambda x: float(x[\'confidence\']), reverse=True)\n        with open(TEMP_FILES_PATH + ""/"" + class_name + ""_dr.json"", \'w\') as outfile:\n            json.dump(bounding_boxes, outfile)\n    logging.info(\'ground truth and det files solved, start calculating mAP...\')\n    sum_AP = 0.0\n    ap_dictionary = {}\n    lamr_dictionary = {}\n    # open file to store the results\n    with open(results_files_path + ""/results.txt"", \'w\') as results_file:\n        results_file.write(""# AP and precision/recall per class\\n"")\n        count_true_positives = {}\n        for class_index, class_name in enumerate(gt_classes):\n            count_true_positives[class_name] = 0\n            """"""\n            Load detection-results of that class\n            """"""\n            dr_file = TEMP_FILES_PATH + ""/"" + class_name + ""_dr.json""\n            dr_data = json.load(open(dr_file))\n\n            """"""\n            Assign detection-results to ground-truth objects\n            """"""\n            nd = len(dr_data)\n            tp = [0] * nd  # creates an array of zeros of size nd\n            fp = [0] * nd\n            for idx, detection in enumerate(dr_data):\n                file_id = detection[""file_id""]\n                if show_animation:\n                    # find ground truth image\n                    ground_truth_img = glob.glob1(IMG_PATH, file_id + "".*"")\n                    #tifCounter = len(glob.glob1(myPath,""*.tif""))\n                    if len(ground_truth_img) == 0:\n                        logging.error(""Error. Image not found with id: "" + file_id)\n                    elif len(ground_truth_img) > 1:\n                        logging.error(""Error. Multiple image with id: "" + file_id)\n                    else:  # found image\n                        #print(IMG_PATH + ""/"" + ground_truth_img[0])\n                        # Load image\n                        img = cv2.imread(IMG_PATH + ""/"" + ground_truth_img[0])\n                        # load image with draws of multiple detections\n                        img_cumulative_path = results_files_path + \\\n                            ""/images/"" + ground_truth_img[0]\n                        if os.path.isfile(img_cumulative_path):\n                            img_cumulative = cv2.imread(img_cumulative_path)\n                        else:\n                            img_cumulative = img.copy()\n                        # Add bottom border to image\n                        bottom_border = 60\n                        BLACK = [0, 0, 0]\n                        img = cv2.copyMakeBorder(\n                            img, 0, bottom_border, 0, 0, cv2.BORDER_CONSTANT, value=BLACK)\n                # assign detection-results to ground truth object if any\n                # open ground-truth with that file_id\n                gt_file = TEMP_FILES_PATH + ""/"" + file_id + ""_ground_truth.json""\n                ground_truth_data = json.load(open(gt_file))\n                ovmax = -1\n                gt_match = -1\n                # load detected object bounding-box\n                bb = [float(x) for x in detection[""bbox""].split()]\n                for obj in ground_truth_data:\n                    # look for a class_name match\n                    if obj[""class_name""] == class_name:\n                        bbgt = [float(x) for x in obj[""bbox""].split()]\n                        bi = [max(bb[0], bbgt[0]), max(bb[1], bbgt[1]),\n                              min(bb[2], bbgt[2]), min(bb[3], bbgt[3])]\n                        iw = bi[2] - bi[0] + 1\n                        ih = bi[3] - bi[1] + 1\n                        if iw > 0 and ih > 0:\n                            # compute overlap (IoU) = area of intersection / area of union\n                            ua = (bb[2] - bb[0] + 1) * (bb[3] - bb[1] + 1) + (bbgt[2] - bbgt[0]\n                                                                              + 1) * (bbgt[3] - bbgt[1] + 1) - iw * ih\n                            ov = iw * ih / ua\n                            if ov > ovmax:\n                                ovmax = ov\n                                gt_match = obj\n\n                # assign detection as true positive/don\'t care/false positive\n                if show_animation:\n                    status = ""NO MATCH FOUND!""  # status is only used in the animation\n                # set minimum overlap\n                min_overlap = MINOVERLAP\n                if specific_iou_flagged:\n                    if class_name in specific_iou_classes:\n                        index = specific_iou_classes.index(class_name)\n                        min_overlap = float(iou_list[index])\n                if ovmax >= min_overlap:\n                    if ""difficult"" not in gt_match:\n                        if not bool(gt_match[""used""]):\n                            # true positive\n                            tp[idx] = 1\n                            gt_match[""used""] = True\n                            count_true_positives[class_name] += 1\n                            # update the "".json"" file\n                            with open(gt_file, \'w\') as f:\n                                f.write(json.dumps(ground_truth_data))\n                            if show_animation:\n                                status = ""MATCH!""\n                        else:\n                            # false positive (multiple detection)\n                            fp[idx] = 1\n                            if show_animation:\n                                status = ""REPEATED MATCH!""\n                else:\n                    # false positive\n                    fp[idx] = 1\n                    if ovmax > 0:\n                        status = ""INSUFFICIENT OVERLAP""\n\n                """"""\n                Draw image to show animation\n                """"""\n                if show_animation:\n                    height, widht = img.shape[:2]\n                    # colors (OpenCV works with BGR)\n                    white = (255, 255, 255)\n                    light_blue = (255, 200, 100)\n                    green = (0, 255, 0)\n                    light_red = (30, 30, 255)\n                    # 1st line\n                    margin = 10\n                    v_pos = int(height - margin - (bottom_border / 2.0))\n                    text = ""Image: "" + ground_truth_img[0] + "" ""\n                    img, line_width = draw_text_in_image(\n                        img, text, (margin, v_pos), white, 0)\n                    text = ""Class ["" + str(class_index) + ""/"" + \\\n                        str(n_classes) + ""]: "" + class_name + "" ""\n                    img, line_width = draw_text_in_image(\n                        img, text, (margin + line_width, v_pos), light_blue, line_width)\n                    if ovmax != -1:\n                        color = light_red\n                        if status == ""INSUFFICIENT OVERLAP"":\n                            text = ""IoU: {0:.2f}% "".format(\n                                ovmax*100) + ""< {0:.2f}% "".format(min_overlap*100)\n                        else:\n                            text = ""IoU: {0:.2f}% "".format(\n                                ovmax*100) + "">= {0:.2f}% "".format(min_overlap*100)\n                            color = green\n                        img, _ = draw_text_in_image(\n                            img, text, (margin + line_width, v_pos), color, line_width)\n                    # 2nd line\n                    v_pos += int(bottom_border / 2.0)\n                    rank_pos = str(idx+1)  # rank position (idx starts at 0)\n                    text = ""Detection #rank: "" + rank_pos + \\\n                        "" confidence: {0:.2f}% "".format(\n                            float(detection[""confidence""])*100)\n                    img, line_width = draw_text_in_image(\n                        img, text, (margin, v_pos), white, 0)\n                    color = light_red\n                    if status == ""MATCH!"":\n                        color = green\n                    text = ""Result: "" + status + "" ""\n                    img, line_width = draw_text_in_image(\n                        img, text, (margin + line_width, v_pos), color, line_width)\n\n                    font = cv2.FONT_HERSHEY_SIMPLEX\n                    if ovmax > 0:  # if there is intersections between the bounding-boxes\n                        bbgt = [int(round(float(x)))\n                                for x in gt_match[""bbox""].split()]\n                        cv2.rectangle(img, (bbgt[0], bbgt[1]),\n                                      (bbgt[2], bbgt[3]), light_blue, 2)\n                        cv2.rectangle(\n                            img_cumulative, (bbgt[0], bbgt[1]), (bbgt[2], bbgt[3]), light_blue, 2)\n                        cv2.putText(img_cumulative, class_name,\n                                    (bbgt[0], bbgt[1] - 5), font, 0.6, light_blue, 1, cv2.LINE_AA)\n                    bb = [int(i) for i in bb]\n                    cv2.rectangle(img, (bb[0], bb[1]),\n                                  (bb[2], bb[3]), color, 2)\n                    cv2.rectangle(img_cumulative,\n                                  (bb[0], bb[1]), (bb[2], bb[3]), color, 2)\n                    cv2.putText(img_cumulative, class_name,\n                                (bb[0], bb[1] - 5), font, 0.6, color, 1, cv2.LINE_AA)\n                    # show image\n                    cv2.imshow(""Animation"", img)\n                    cv2.waitKey(20)  # show for 20 ms\n                    # save image to results\n                    output_img_path = results_files_path + ""/images/detections_one_by_one/"" + \\\n                        class_name + ""_detection"" + str(idx) + "".jpg""\n                    cv2.imwrite(output_img_path, img)\n                    # save the image with all the objects drawn to it\n                    cv2.imwrite(img_cumulative_path, img_cumulative)\n\n            # print(tp)\n            # compute precision/recall\n            cumsum = 0\n            for idx, val in enumerate(fp):\n                fp[idx] += cumsum\n                cumsum += val\n            cumsum = 0\n            for idx, val in enumerate(tp):\n                tp[idx] += cumsum\n                cumsum += val\n            # print(tp)\n            rec = tp[:]\n            for idx, val in enumerate(tp):\n                rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]\n            # print(rec)\n            prec = tp[:]\n            for idx, val in enumerate(tp):\n                prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n            # print(prec)\n\n            ap, mrec, mprec = voc_ap(rec[:], prec[:])\n            sum_AP += ap\n            # class_name + "" AP = {0:.2f}%"".format(ap*100)\n            text = ""{0:.2f}%"".format(ap*100) + "" = "" + class_name + "" AP ""\n            """"""\n            Write to results.txt\n            """"""\n            rounded_prec = [\'%.2f\' % elem for elem in prec]\n            rounded_rec = [\'%.2f\' % elem for elem in rec]\n            results_file.write(text + ""\\n Precision: "" + str(rounded_prec) +\n                               ""\\n Recall :"" + str(rounded_rec) + ""\\n\\n"")\n            if not args.quiet:\n                print(text)\n            ap_dictionary[class_name] = ap\n\n            n_images = counter_images_per_class[class_name]\n            lamr, mr, fppi = log_average_miss_rate(\n                np.array(rec), np.array(fp), n_images)\n            lamr_dictionary[class_name] = lamr\n\n            """"""\n            Draw plot\n            """"""\n            if draw_plot:\n                plt.plot(rec, prec, \'-o\')\n                # add a new penultimate point to the list (mrec[-2], 0.0)\n                # since the last line segment (and respective area) do not affect the AP value\n                area_under_curve_x = mrec[:-1] + [mrec[-2]] + [mrec[-1]]\n                area_under_curve_y = mprec[:-1] + [0.0] + [mprec[-1]]\n                plt.fill_between(area_under_curve_x, 0,\n                                 area_under_curve_y, alpha=0.2, edgecolor=\'r\')\n                # set window title\n                fig = plt.gcf()  # gcf - get current figure\n                fig.canvas.set_window_title(\'AP \' + class_name)\n                # set plot title\n                plt.title(\'class: \' + text)\n                #plt.suptitle(\'This is a somewhat long figure title\', fontsize=16)\n                # set axis titles\n                plt.xlabel(\'Recall\')\n                plt.ylabel(\'Precision\')\n                # optional - set axes\n                axes = plt.gca()  # gca - get current axes\n                axes.set_xlim([0.0, 1.0])\n                axes.set_ylim([0.0, 1.05])  # .05 to give some extra space\n                # Alternative option -> wait for button to be pressed\n                # while not plt.waitforbuttonpress(): pass # wait for key display\n                # Alternative option -> normal display\n                # plt.show()\n                # save the plot\n                fig.savefig(results_files_path +\n                            ""/classes/"" + class_name + "".png"")\n                plt.cla()  # clear axes for next plot\n\n        if show_animation:\n            cv2.destroyAllWindows()\n\n        results_file.write(""\\n# mAP of all classes\\n"")\n        mAP = sum_AP / n_classes\n        text = ""mAP = {0:.2f}%"".format(mAP*100)\n        results_file.write(text + ""\\n"")\n        print(text)\n\n    # remove the temp_files directory\n    shutil.rmtree(TEMP_FILES_PATH)\n\n    det_counter_per_class = {}\n    for txt_file in dr_files_list:\n        # get lines to list\n        lines_list = load_txt_or_xml_format(txt_file)\n        for line in lines_list:\n            class_name = line.split()[0]\n            # check if class is in the ignore list, if yes skip\n            if class_name in args.ignore:\n                continue\n            # count that object\n            if class_name in det_counter_per_class:\n                det_counter_per_class[class_name] += 1\n            else:\n                # if class didn\'t exist yet\n                det_counter_per_class[class_name] = 1\n    # print(det_counter_per_class)\n    dr_classes = list(det_counter_per_class.keys())\n\n    if draw_plot:\n        window_title = ""ground-truth-info""\n        plot_title = ""ground-truth\\n""\n        plot_title += ""("" + str(len(ground_truth_files_list)) + \\\n            "" files and "" + str(n_classes) + "" classes)""\n        x_label = ""Number of objects per class""\n        output_path = results_files_path + ""/ground-truth-info.png""\n        to_show = False\n        plot_color = \'forestgreen\'\n        draw_plot_func(\n            gt_counter_per_class,\n            n_classes,\n            window_title,\n            plot_title,\n            x_label,\n            output_path,\n            to_show,\n            plot_color,\n            \'\',\n        )\n\n    with open(results_files_path + ""/results.txt"", \'a\') as results_file:\n        results_file.write(""\\n# Number of ground-truth objects per class\\n"")\n        for class_name in sorted(gt_counter_per_class):\n            results_file.write(class_name + "": "" +\n                               str(gt_counter_per_class[class_name]) + ""\\n"")\n\n    for class_name in dr_classes:\n        # if class exists in detection-result but not in ground-truth then there are no true positives in that class\n        if class_name not in gt_classes:\n            count_true_positives[class_name] = 0\n    # print(count_true_positives)\n\n    # Saving results and ploting\n    if draw_plot:\n        window_title = ""detection-results-info""\n        # Plot title\n        plot_title = ""detection-results\\n""\n        plot_title += ""("" + str(len(dr_files_list)) + "" files and ""\n        count_non_zero_values_in_dictionary = sum(\n            int(x) > 0 for x in list(det_counter_per_class.values()))\n        plot_title += str(count_non_zero_values_in_dictionary) + \\\n            "" detected classes)""\n        # end Plot title\n        x_label = ""Number of objects per class""\n        output_path = results_files_path + ""/detection-results-info.png""\n        to_show = False\n        plot_color = \'forestgreen\'\n        true_p_bar = count_true_positives\n        draw_plot_func(\n            det_counter_per_class,\n            len(det_counter_per_class),\n            window_title,\n            plot_title,\n            x_label,\n            output_path,\n            to_show,\n            plot_color,\n            true_p_bar\n        )\n\n    with open(results_files_path + ""/results.txt"", \'a\') as results_file:\n        results_file.write(""\\n# Number of detected objects per class\\n"")\n        for class_name in sorted(dr_classes):\n            n_det = det_counter_per_class[class_name]\n            text = class_name + "": "" + str(n_det)\n            text += "" (tp:"" + str(count_true_positives[class_name]) + """"\n            text += "", fp:"" + \\\n                str(n_det - count_true_positives[class_name]) + "")\\n""\n            results_file.write(text)\n\n    if draw_plot:\n        window_title = ""lamr""\n        plot_title = ""log-average miss rate""\n        x_label = ""log-average miss rate""\n        output_path = results_files_path + ""/lamr.png""\n        to_show = False\n        plot_color = \'royalblue\'\n        draw_plot_func(\n            lamr_dictionary,\n            n_classes,\n            window_title,\n            plot_title,\n            x_label,\n            output_path,\n            to_show,\n            plot_color,\n            """"\n        )\n\n    if draw_plot:\n        window_title = ""mAP""\n        plot_title = ""mAP = {0:.2f}%"".format(mAP*100)\n        x_label = ""Average Precision""\n        output_path = results_files_path + ""/mAP.png""\n        to_show = True\n        plot_color = \'royalblue\'\n        draw_plot_func(\n            ap_dictionary,\n            n_classes,\n            window_title,\n            plot_title,\n            x_label,\n            output_path,\n            to_show,\n            plot_color,\n            """"\n        )\n\n    '"
alfred/modules/data/extract_voc.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\n\nextract VOC image patchs from VOC annotations\n\n""""""\n""""""\nread VOC format\nannotations and extract image patchs out\n\n""""""\nimport os\nimport json\nimport sys\nimport xml.etree.ElementTree as ET\nfrom alfred.utils.log import logger as logging\nimport cv2\nimport argparse\n\n\ndef get(root, name):\n    vars = root.findall(name)\n    return vars\n\n\ndef get_and_check(root, name, length):\n    vars = root.findall(name)\n    if len(vars) == 0:\n        raise NotImplementedError(\'Can not find %s in %s.\'%(name, root.tag))\n    if length > 0 and len(vars) != length:\n        raise NotImplementedError(\'The size of %s is supposed to be %d, but is %d.\'%(name, length, len(vars)))\n    if length == 1:\n        vars = vars[0]\n    return vars\n\n\ndef get_filename_as_int(filename):\n    try:\n        filename = os.path.splitext(filename)[0]\n        return int(filename)\n    except:\n        raise NotImplementedError(\'Filename %s is supposed to be an integer.\'%(filename))\n\n\ndef convert(xml_dir, output_dir, img_dir, xml_list=None):\n    assert os.path.join(\'image dir {} not exist\'.format(img_dir))\n    os.makedirs(output_dir, exist_ok=True)\n    if xml_list:\n        list_fp = open(xml_list, \'r\')\n    else:\n        list_fp = os.listdir(xml_dir)\n    logging.info(\'we got all xml files: {}\'.format(len(list_fp)))\n    json_dict = {""images"":[], ""type"": ""instances"", ""annotations"": [],\n                 ""categories"": []}\n\n    i = 0\n    for line in list_fp:\n        line = line.strip()\n        print(""Processing %s""%(line))\n        xml_f = os.path.join(xml_dir, line)\n        tree = ET.parse(xml_f)\n        root = tree.getroot()\n        path = get(root, \'path\')\n        if len(path) == 1:\n            filename = os.path.basename(path[0].text)\n        elif len(path) == 0:\n            filename = get_and_check(root, \'filename\', 1).text\n        else:\n            raise NotImplementedError(\'%d paths found in %s\'%(len(path), line))\n        # compare filename with xml filename\n        if os.path.basename(xml_f).split(\'.\')[0] != filename.split(\'.\')[0]:\n            # if not equal, we replace filename with xml file name\n            # print(\'{} != {}\'.format(os.path.basename(xml_f).split(\'.\')[0], filename.split(\'.\')[0]))\n            filename = os.path.basename(xml_f).split(\'.\')[0] + \'.\' + filename.split(\'.\')[-1]\n            print(\'revise filename to: {}\'.format(filename))\n        ## The filename must be a number\n        # image_id = get_filename_as_int(filename)\n        image_id = i\n        size = get_and_check(root, \'size\', 1)\n        width = int(get_and_check(size, \'width\', 1).text)\n        height = int(get_and_check(size, \'height\', 1).text)\n        image = {\'file_name\': filename, \'height\': height, \'width\': width,\n                 \'id\':image_id}\n        json_dict[\'images\'].append(image)\n        ## Cruuently we do not support segmentation\n        #  segmented = get_and_check(root, \'segmented\', 1).text\n        #  assert segmented == \'0\'\n        \n        img_root = img_dir\n        img_f = os.path.join(img_root, os.path.basename(xml_f).replace(\'xml\', \'jpg\'))\n        assert img_f, \'{} not exist\'.format(img_f)\n        logging.info(\'reading image from: {}\'.format(img_f))\n        ori_img = cv2.imread(img_f)\n        bx_id = 0\n        for obj in get(root, \'object\'):\n            category = get_and_check(obj, \'name\', 1).text\n            bndbox = get_and_check(obj, \'bndbox\', 1)\n            xmin = float(get_and_check(bndbox, \'xmin\', 1).text)\n            ymin = float(get_and_check(bndbox, \'ymin\', 1).text)\n            xmax = float(get_and_check(bndbox, \'xmax\', 1).text)\n            ymax = float(get_and_check(bndbox, \'ymax\', 1).text)\n            assert(xmax > xmin)\n            assert(ymax > ymin)\n            \n            # we got an image patch and will save it\n            # print(\'{} - {}, {} - {}\'.format(int(xmin), int(xmax), int(ymin), int(ymax)))\n            img_patch = ori_img[ int(ymin):int(ymax), int(xmin):int(xmax)]\n            # cv2.imshow(\'rr\', img_patch)\n            # cv2.waitKey(0)\n            # print(category)\n            os.makedirs(os.path.join(output_dir, category), exist_ok=True)\n            to_save_f = os.path.join(output_dir, category, \'{}_{}.jpg\'.format(i, bx_id))\n            cv2.imwrite(to_save_f, img_patch)\n            bx_id += 1\n        # image_id plus 1\n        i += 1\n    logging.info(\'done.\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Extract image patchs from VOC annotations.\')\n    parser.add_argument(\'--xml_dir\', \'-x\', type=str, help=\'xml dir\')\n    parser.add_argument(\'--image_dir\', \'-i\', type=str, default=\'JPEGImages\', help=\'xml dir\')\n    parser.add_argument(\'--output_dir\', \'-o\', type=str, default=\'extracted_out\', help=\'xml dir\')\n    args = parser.parse_args()\n\n    xml = args.xml_dir\n    img = args.image_dir\n    output = args.output_dir\n\n    convert(xml, output, img)'"
alfred/modules/data/gather_voclabels.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\ngather all voc labels from Annotations root folder\nwhich contains all xml annotations\n\n""""""\n\n""""""\n\ngather the label from Annotations\n""""""\nimport os\nimport pickle\nimport os.path\nimport sys\nimport numpy as np\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\nimport glob\n\n\n\ndef gather_labels(anno_dir):\n    all_labels = glob.glob(os.path.join(anno_dir, \'*.xml\'))\n    all_names = []\n    for label in all_labels:\n        print(\'parsing {}\'.format(label))\n        root = ET.parse(label).getroot()\n        for obj in root.iter(\'object\'):\n            name = obj.find(\'name\').text\n            if name not in all_names:\n                all_names.append(name)\n    print(\'Done. summary...\')\n    print(\'all {} classes.\'.format(len(all_names)))\n    print(all_names)\n\n\n'"
alfred/modules/data/labelone_view.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/modules/data/split.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport os\nimport glob\nimport numpy as np\n\nsave_dir = \'./ImageSets/Main\'\nos.makedirs(save_dir, exist_ok=True)\n\nall_imgs = [os.path.basename(i).split(\'.\')[0]+\'\\n\' for i in glob.glob(\'./JPEGImages/*.jpg\')]\n\nratio = 0.9\nprint(\'Found {} images, spliting ratio is 0.9\'.format(len(all_imgs)))\n\nnp.random.shuffle(all_imgs)\nsplit = int(len(all_imgs) * ratio)\ntrain_ids = all_imgs[: split]\nval_ids = all_imgs[split: ]\nprint(\'{} for train, {} for validation.\'.format(len(train_ids), len(val_ids)))\n\nprint(\'saving split..\')\nwith open(os.path.join(save_dir, \'train.txt\'), \'w\') as f:\n    f.writelines(train_ids)\nwith open(os.path.join(save_dir, \'val.txt\'), \'w\') as f:\n    f.writelines(val_ids)\nprint(\'Done.\')'"
alfred/modules/data/txt2voc.py,0,"b'# Script to convert yolo annotations to voc format\n\n# Sample format\n# <annotation>\n#     <folder>_image_fashion</folder>\n#     <filename>brooke-cagle-39574.jpg</filename>\n#     <size>\n#         <width>1200</width>\n#         <height>800</height>\n#         <depth>3</depth>\n#     </size>\n#     <segmented>0</segmented>\n#     <object>\n#         <name>head</name>\n#         <pose>Unspecified</pose>\n#         <truncated>0</truncated>\n#         <difficult>0</difficult>\n#         <bndbox>\n#             <xmin>549</xmin>\n#             <ymin>251</ymin>\n#             <xmax>625</xmax>\n#             <ymax>335</ymax>\n#         </bndbox>\n#     </object>\n# <annotation>\nimport os\nimport xml.etree.cElementTree as ET\nfrom PIL import Image\nimport sys\nimport glob\n\n\nCLASS_MAPPING = {\n    \'0\': \'name\'\n    # Add your remaining classes here.\n}\n\n\ndef create_root(file_prefix, width, height):\n    root = ET.Element(""annotations"")\n    ET.SubElement(root, ""filename"").text = ""{}.jpg"".format(file_prefix)\n    ET.SubElement(root, ""folder"").text = ""images""\n    size = ET.SubElement(root, ""size"")\n    ET.SubElement(size, ""width"").text = str(width)\n    ET.SubElement(size, ""height"").text = str(height)\n    ET.SubElement(size, ""depth"").text = ""3""\n    return root\n\n\ndef create_object_annotation(root, voc_labels):\n    for voc_label in voc_labels:\n        obj = ET.SubElement(root, ""object"")\n        ET.SubElement(obj, ""name"").text = voc_label[0]\n        ET.SubElement(obj, ""pose"").text = ""Unspecified""\n        ET.SubElement(obj, ""truncated"").text = str(0)\n        ET.SubElement(obj, ""difficult"").text = str(0)\n        bbox = ET.SubElement(obj, ""bndbox"")\n        ET.SubElement(bbox, ""xmin"").text = str(voc_label[1])\n        ET.SubElement(bbox, ""ymin"").text = str(voc_label[2])\n        ET.SubElement(bbox, ""xmax"").text = str(voc_label[3])\n        ET.SubElement(bbox, ""ymax"").text = str(voc_label[4])\n    return root\n\n\ndef create_file(file_prefix, width, height, voc_labels, des_dir):\n    root = create_root(file_prefix, width, height)\n    root = create_object_annotation(root, voc_labels)\n    tree = ET.ElementTree(root)\n    tree.write(""{}/{}.xml"".format(des_dir, file_prefix))\n\n\ndef read_file(file_path, des_dir):\n    file_prefix = os.path.basename(file_path).split("".txt"")[0]\n    image_file_name = ""{}.jpg"".format(file_prefix)\n    img = Image.open(""{}/{}"".format(""images"", image_file_name))\n    w, h = img.size\n    with open(file_path, \'r\') as file:\n        lines = file.readlines()\n        voc_labels = []\n        for line in lines:\n            voc = []\n            line = line.strip()\n            data = line.split()\n            # voc.append(CLASS_MAPPING.get(data[0]))\n            voc.append(data[0])\n            bbox_width = float(data[3]) * w\n            bbox_height = float(data[4]) * h\n            center_x = float(data[1]) * w\n            center_y = float(data[2]) * h\n            voc.append(center_x - (bbox_width / 2))\n            voc.append(center_y - (bbox_height / 2))\n            voc.append(center_x + (bbox_width / 2))\n            voc.append(center_y + (bbox_height / 2))\n            voc_labels.append(voc)\n        create_file(file_prefix, w, h, voc_labels, des_dir)\n    print(""Processing complete for file: {}"".format(file_path))\n\n\ndef start(dir_name):\n    des_d = \'output_xmls\'\n    os.makedirs(des_d, exist_ok=True)\n    txts = glob.glob(os.path.join(dir_name, \'*.txt\'))\n    for filename in txts:\n        read_file(filename, des_d)\n\n\nif __name__ == ""__main__"":\n    start(sys.argv[1])\n'"
alfred/modules/data/view_coco.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\n\nthis script will using pycoco API\ndraw our converted annotation to check\nif result is right or not\n\n""""""\ntry:\n    from pycocotools.coco import COCO\nexcept ImportError as e:\n    print(\'Got import error: {}\'.format(e))\n    print(\'you are not either install pycocotools or its dependencies. pls install first.\')\n    exit(-1)\nimport os\nimport sys\nimport cv2\nfrom pycocotools import mask as maskUtils\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport skimage.io as io\nfrom alfred.utils.log import logger as logging\nimport cv2\nfrom alfred.vis.image.det import visualize_det_cv2_part\nfrom alfred.vis.image.common import get_unique_color_by_id\n\n\n# USED_CATEGORIES_IDS = [i for i in range(1, 16)]\n\n\ndef vis_coco(coco_img_root, ann_f):\n    data_dir = coco_img_root\n    coco = COCO(ann_f)\n\n    cats = coco.loadCats(coco.getCatIds())\n    logging.info(\'cats: {}\'.format(cats))\n    img_ids = coco.getImgIds()\n    logging.info(\'all images we got: {}\'.format(len(img_ids)))\n\n    # draw instances\n    for img_id in img_ids:\n        img = coco.loadImgs(img_id)[0]\n        print(\'checking img: {}, id: {}\'.format(img, img_id))\n\n        img_f = os.path.join(data_dir, img[\'file_name\'])\n        anno_ids = coco.getAnnIds(imgIds=img[\'id\'])\n        annos = coco.loadAnns(anno_ids)\n\n        logging.info(\'showing anno: {}\'.format(annos))\n        if len(annos[0][\'segmentation\']) == 0:\n            logging.info(\'no segmentation found, using opencv vis.\')\n            img = cv2.imread(img_f)\n\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            font_scale = 0.36\n            font_thickness = 1\n            line_thickness = 1\n\n            for ann in annos:\n                b = ann[\'bbox\']\n                x1 = int(b[0])\n                y1 = int(b[1])\n                x2 = int(x1 + b[2])\n                y2 = int(y1 + b[3])\n                cls_id = ann[\'category_id\']\n                unique_color = get_unique_color_by_id(cls_id)\n                cv2.rectangle(img, (x1, y1), (x2, y2),\n                              unique_color, line_thickness, cv2.LINE_AA)\n                text_label = \'{}\'.format(cls_id)\n                (ret_val, _) = cv2.getTextSize(\n                    text_label, font, font_scale, font_thickness)\n                txt_bottom_left = (x1+4, y1-4)\n                cv2.rectangle(img, (txt_bottom_left[0]-4, txt_bottom_left[1] - ret_val[1]-2),\n                              (txt_bottom_left[0] + ret_val[0] +\n                               2, txt_bottom_left[1]+4),\n                              (0, 0, 0), -1)\n                cv2.putText(img, text_label, txt_bottom_left, font,\n                            font_scale, (237, 237, 237), font_thickness, cv2.LINE_AA)\n            cv2.imshow(\'rr\', img)\n            cv2.waitKey(0)\n        else:\n            I = io.imread(img_f)\n            plt.imshow(I)\n            plt.axis(\'off\')\n            coco.showAnns(annos)\n            plt.show()\n\n\n'"
alfred/modules/data/view_txt.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\n\nview txt labeled detection data\n\n\n""""""\nimport os\nimport sys\nimport cv2\nfrom glob import glob\nimport os\nimport sys\nimport cv2\nfrom alfred.utils.log import logger as logging\n\n\ndef vis_det_txt(img_root, label_root):\n    logging.info(\'img root: {}, label root: {}\'.format(img_root, label_root))\n    # auto detection .jpg or .png images\n    txt_files = glob(os.path.join(label_root, \'*.txt\'))\n    for txt_f in txt_files:\n        img_f = os.path.join(img_root, os.path.basename(txt_f).split(\'.\')[0] + \'.jpg\')\n        if os.path.exists(img_f):\n            img = cv2.imread(img_f)\n            if os.path.exists(txt_f):\n                with open(txt_f) as f:\n                    annos = f.readlines()\n                    for ann in annos:\n                        ann = ann.strip().split(\' \')\n                        if len(ann) == 5:\n                            # not include prob\n                            category = ann[0]\n                            xmin = int(float(ann[1]))\n                            ymin = int(float(ann[2]))\n                            xmax = int(float(ann[3]))\n                            ymax = int(float(ann[4]))\n                            cv2.putText(img, category, (xmin, ymin), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255))\n                            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2, 1)\n                        elif len(ann) == 6:\n                            # include prob\n                            category = ann[0]\n                            prob = float(ann[1])\n                            xmin = int(float(ann[2]))\n                            ymin = int(float(ann[3]))\n                            xmax = int(float(ann[4]))\n                            ymax = int(float(ann[5]))\n                            cv2.putText(img, \'{} {}\'.format(category, prob), (xmin, ymin), \n                            cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255))\n                            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2, 1)\n                cv2.imshow(\'txt check\', img)\n                cv2.waitKey(0)\n            else:\n                logging.warning(\'xxxx image: {} not found.\'.format(img_f))\n\n\nif __name__ == ""__main__"":\n    vis_det_txt(sys.argv[1], sys.argv[2])\n'"
alfred/modules/data/view_voc.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\n\nthis tool helps viewing VOC format data\n\n\n""""""\nimport os\nimport sys\nimport cv2\nimport xml.etree.ElementTree as ET\nfrom glob import glob\nimport os\nimport sys\nimport cv2\nfrom alfred.utils.log import logger as logging\n\n\ndef get_and_check(root, name, length):\n    vars = root.findall(name)\n    if len(vars) == 0:\n        raise NotImplementedError(\'Can not find %s in %s.\' % (name, root.tag))\n    if length > 0 and len(vars) != length:\n        raise NotImplementedError(\'The size of %s is supposed to be %d, but is %d.\' % (name, length, len(vars)))\n    if length == 1:\n        vars = vars[0]\n    return vars\n\n\ndef get(root, name):\n    vars = root.findall(name)\n    return vars\n\n\ndef vis_voc(img_root, label_root, label_major=True):\n    logging.info(\'img root: {}, label root: {}\'.format(img_root, label_root))\n    # auto detection .jpg or .png images\n    if label_major:\n        logging.info(\'label major will using xmls to found images... it might cause no image found\')\n        xml_files = glob(os.path.join(label_root, \'*.xml\'))\n        for xml in xml_files:\n            if os.path.exists(xml):\n                img_f = os.path.join(img_root, os.path.basename(xml).split(\'.\')[0] + \'.jpg\')\n                if not os.path.exists(img_f):\n                    logging.info(\'switch to png format\')\n                    img_f = os.path.join(img_root, os.path.basename(xml).split(\'.\')[0] + \'.png\')\n                img = cv2.imread(img_f)\n                if os.path.exists(img_f):\n                    tree = ET.parse(xml)\n                    root = tree.getroot()\n                    for obj in get(root, \'object\'):\n                        category = get_and_check(obj, \'name\', 1).text\n                        bndbox = get_and_check(obj, \'bndbox\', 1)\n                        xmin = int(float(get_and_check(bndbox, \'xmin\', 1).text))\n                        ymin = int(float(get_and_check(bndbox, \'ymin\', 1).text))\n                        xmax = int(float(get_and_check(bndbox, \'xmax\', 1).text))\n                        ymax = int(float(get_and_check(bndbox, \'ymax\', 1).text))\n\n                        cv2.putText(img, category, (xmin, ymin), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255))\n                        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2, 1)\n                    cv2.imshow(\'voc check\', img)\n                    cv2.waitKey(0)\n                else:\n                    logging.warning(\'xxxx image: {} for label: {} not found.\'.format(img_f, xml))\n    else:\n        img_files = glob(os.path.join(img_root, \'*.[jp][pn]g\'))\n        for img_f in img_files:\n            if os.path.exists(img_f):\n                img = cv2.imread(img_f)\n                label_path = os.path.join(label_root, os.path.basename(img_f).split(\'.\')[0] + \'.xml\')\n                if os.path.exists(label_path):\n                    #\n                    tree = ET.parse(label_path)\n                    root = tree.getroot()\n                    for obj in get(root, \'object\'):\n                        category = get_and_check(obj, \'name\', 1).text\n                        bndbox = get_and_check(obj, \'bndbox\', 1)\n                        xmin = int(float(get_and_check(bndbox, \'xmin\', 1).text))\n                        ymin = int(float(get_and_check(bndbox, \'ymin\', 1).text))\n                        xmax = int(float(get_and_check(bndbox, \'xmax\', 1).text))\n                        ymax = int(float(get_and_check(bndbox, \'ymax\', 1).text))\n\n                        cv2.putText(img, category, (xmin, ymin), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 255, 255))\n                        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2, 1)\n                    cv2.imshow(\'voc check\', img)\n                    cv2.waitKey(0)\n                else:\n                    logging.warning(\'xxxx image: {} according label: {} not found.\'.format(img_f, label_path))\n\n\nif __name__ == ""__main__"":\n    vis_voc(sys.argv[1], sys.argv[2])\n'"
alfred/modules/data/voc2coco.py,0,"b'#!/usr/bin/python\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n# pip install lxml\n\nimport sys\nimport os\nimport json\nimport xml.etree.ElementTree as ET\nfrom alfred.utils.log import logger as logging\n\n\nSTART_BOUNDING_BOX_ID = 1\nPRE_DEFINE_CATEGORIES = {}\n# If necessary, pre-define category and its id\n#  PRE_DEFINE_CATEGORIES = {""aeroplane"": 1, ""bicycle"": 2, ""bird"": 3, ""boat"": 4,\n                         #  ""bottle"":5, ""bus"": 6, ""car"": 7, ""cat"": 8, ""chair"": 9,\n                         #  ""cow"": 10, ""diningtable"": 11, ""dog"": 12, ""horse"": 13,\n                         #  ""motorbike"": 14, ""person"": 15, ""pottedplant"": 16,\n                         #  ""sheep"": 17, ""sofa"": 18, ""train"": 19, ""tvmonitor"": 20}\n\n\ndef get(root, name):\n    vars = root.findall(name)\n    return vars\n\n\ndef get_and_check(root, name, length):\n    vars = root.findall(name)\n    if len(vars) == 0:\n        raise NotImplementedError(\'Can not find %s in %s.\'%(name, root.tag))\n    if length > 0 and len(vars) != length:\n        raise NotImplementedError(\'The size of %s is supposed to be %d, but is %d.\'%(name, length, len(vars)))\n    if length == 1:\n        vars = vars[0]\n    return vars\n\n\ndef get_filename_as_int(filename):\n    try:\n        filename = os.path.splitext(filename)[0]\n        return int(filename)\n    except:\n        raise NotImplementedError(\'Filename %s is supposed to be an integer.\'%(filename))\n    \n\n""""""\nxml_list is optional, we at least need xml_dir and json_file\n""""""\ndef convert(xml_dir, json_file=None, xml_list=None):\n    if xml_list:\n        list_fp = open(xml_list, \'r\')\n    else:\n        list_fp = os.listdir(xml_dir)\n    logging.info(\'we got all xml files: {}\'.format(len(list_fp)))\n    json_dict = {""images"":[], ""type"": ""instances"", ""annotations"": [],\n                 ""categories"": []}\n    categories = PRE_DEFINE_CATEGORIES\n    bnd_id = START_BOUNDING_BOX_ID\n\n    i = 0\n    for line in list_fp:\n        line = line.strip()\n        print(""Processing %s""%(line))\n        xml_f = os.path.join(xml_dir, line)\n        tree = ET.parse(xml_f)\n        root = tree.getroot()\n        path = get(root, \'path\')\n        if len(path) == 1:\n            filename = os.path.basename(path[0].text)\n        elif len(path) == 0:\n            filename = get_and_check(root, \'filename\', 1).text\n        else:\n            raise NotImplementedError(\'%d paths found in %s\'%(len(path), line))\n        # compare filename with xml filename\n        if os.path.basename(xml_f).split(\'.\')[0] != filename.split(\'.\')[0]:\n            # if not equal, we replace filename with xml file name\n            # print(\'{} != {}\'.format(os.path.basename(xml_f).split(\'.\')[0], filename.split(\'.\')[0]))\n            filename = os.path.basename(xml_f).split(\'.\')[0] + \'.\' + filename.split(\'.\')[-1]\n            # filename could be wrong sufix\n            print(\'revise filename to: {}\'.format(filename))\n            if not os.path.exists(os.path.join(os.path.dirname(xml_f), filename)):\n                logging.info(\'revise filename wrong, try change sufix (but also could be wrong, check your VOC format pls.)\')\n                filename = filename.split(\'.\')[0] + \'.jpg\'\n            \n        ## The filename must be a number\n        # image_id = get_filename_as_int(filename)\n        image_id = i\n        size = get_and_check(root, \'size\', 1)\n        width = int(get_and_check(size, \'width\', 1).text)\n        height = int(get_and_check(size, \'height\', 1).text)\n        image = {\'file_name\': filename, \'height\': height, \'width\': width,\n                 \'id\':image_id}\n        json_dict[\'images\'].append(image)\n        ## Cruuently we do not support segmentation\n        #  segmented = get_and_check(root, \'segmented\', 1).text\n        #  assert segmented == \'0\'\n        for obj in get(root, \'object\'):\n            category = get_and_check(obj, \'name\', 1).text\n            if category not in categories:\n                new_id = len(categories)\n                categories[category] = new_id\n            category_id = categories[category]\n            bndbox = get_and_check(obj, \'bndbox\', 1)\n            xmin = float(get_and_check(bndbox, \'xmin\', 1).text)\n            ymin = float(get_and_check(bndbox, \'ymin\', 1).text)\n            xmax = float(get_and_check(bndbox, \'xmax\', 1).text)\n            ymax = float(get_and_check(bndbox, \'ymax\', 1).text)\n            assert(xmax > xmin)\n            assert(ymax > ymin)\n            o_width = abs(xmax - xmin)\n            o_height = abs(ymax - ymin)\n            ann = {\'area\': o_width*o_height, \'iscrowd\': 0, \'image_id\':\n                   image_id, \'bbox\':[xmin, ymin, o_width, o_height],\n                   \'category_id\': category_id, \'id\': bnd_id, \'ignore\': 0,\n                   \'segmentation\': []}\n            json_dict[\'annotations\'].append(ann)\n            bnd_id = bnd_id + 1\n        # image_id plus 1\n        i += 1\n\n    for cate, cid in categories.items():\n        cat = {\'supercategory\': \'none\', \'id\': cid, \'name\': cate}\n        json_dict[\'categories\'].append(cat)\n    if not json_file:\n        json_file = \'annotations_coco.json\'\n        logging.info(\'converted coco format will saved into: {}\'.format(json_file))\n    json_fp = open(json_file, \'w\')\n    json_str = json.dumps(json_dict)\n    json_fp.write(json_str)\n    json_fp.close()\n    logging.info(\'done.\')\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 3:\n        print(\'at least 2 auguments are need.\')\n        print(\'Usage: %s XML_LIST.txt(optional) XML_DIR OUTPU_JSON.json\'%(sys.argv[0]))\n        exit(1)\n    if len(sys.argv) == 3:\n        # xml_dir, output_json\n        convert(sys.argv[1], sys.argv[2])\n    elif len(sys.argv) == 4:\n        # xml_list, xml_dir, output_json\n        convert(sys.argv[1], sys.argv[2], sys.argv[3])'"
alfred/modules/data/voc2yolo.py,0,b''
alfred/modules/scrap/__init__.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/modules/scrap/image_scraper.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport re\nimport requests\nimport os\nimport argparse\nimport random\nfrom colorama import Fore, Back, Style\n\n\nclass ImageScraper(object):\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def save_image(image_data, save_prefix, save_dir, index):\n        if not os.path.exists(save_dir):\n            os.mkdir(save_dir)\n        if save_prefix:\n            if index:\n                save_file = os.path.join(save_dir, save_prefix + \'_\' + str(index) + \'.jpg\')\n            else:\n                file_name = \'\'.join(random.sample(\'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz0123456789\', 16))\n                save_file = os.path.join(save_dir, save_prefix + \'_\' + file_name + \'.jpg\')\n        else:\n            if index:\n                save_file = os.path.join(save_dir, \'_\' + str(index) + \'.jpg\')\n            else:\n                file_name = \'\'.join(random.sample(\'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz0123456789\', 16))\n                save_file = os.path.join(save_dir, \'_\' + file_name + \'.jpg\')\n        with open(save_file, \'wb\') as f:\n            print(\'-- image saved into {}\'.format(os.path.basename(save_file)))\n            f.write(image_data)\n\n    @staticmethod\n    def decode_url(url):\n        url = url.replace(""_z2C$q"", "":"")\n        url = url.replace(""_z&e3B"", ""."")\n        url = url.replace(""AzdH3F"", ""/"")\n        in_table = ""wkv1ju2it3hs4g5rq6fp7eo8dn9cm0bla""\n        out_table = ""abcdefghijklmnopqrstuvw1234567890""\n        trans_table = str.maketrans(in_table, out_table)\n        url = url.translate(trans_table)\n        return url\n\n    def scrap(self, query_words, save_dir=None, max_count=5000):\n        print(Fore.BLUE)\n        print(\'-- scrap images of: \' + Fore.YELLOW + Style.BRIGHT + \' \'.join(query_words) + Style.RESET_ALL)\n        if save_dir:\n            root_path = save_dir\n        else:\n            root_path = os.getcwd()\n        for k, query_word in enumerate(query_words):\n            url_pattern = ""https://image.baidu.com/search/acjson?tn=resultjson_com&ipn=rj&"" \\\n                          ""ct=201326592&fp=result&queryWord={word}&cl=2&lm=-1&ie=utf-8&oe=utf-8&st=-1&ic=0"" \\\n                          ""&word={word}&face=0&istype=2nc=1&pn={pn}&rn=60""\n            urls = (url_pattern.format(word=query_word, pn=p) for p in range(0, max_count, 30))\n            for i_u, url in enumerate(urls):\n                try:\n                    html = requests.get(url).text\n                    image_urls = re.findall(\'""objURL"":""(.*?)"",\', html, re.S)\n                    for i, image_url in enumerate(image_urls):\n                        try:\n                            image_url = self.decode_url(image_url)\n                            print(\'-- decoding url.. : {}\'.format(image_url))\n                            print(\'-- solving %d image\' % i)\n                            image = requests.get(image_url, stream=False, timeout=10).content\n                            save_dir = os.path.join(os.path.abspath(root_path), query_words[k])\n                            self.save_image(image, query_word.replace(\' \', \'\'), save_dir, str(i_u) + str(i))\n                        except requests.exceptions.ConnectionError:\n                            print(\'-- url: %s can not found image.\' % image_url)\n                            continue\n                except Exception as e:\n                    print(e)\n                    print(\'-- pass this url.\')\n                    pass'"
alfred/modules/scrap/scraper_images.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nscraper scrap images from Baidu image\nfor using just:\npython3 scraper_images.py -q [query_word1, query_word2,...] -d ./data -m 800000\nand all image will save into relative path you set, and saved into different folders\nunder this dir\n""""""\nimport re\nimport requests\nimport os\nimport argparse\nimport random\n\n\ndef save_image(image_data, save_prefix, save_dir, index):\n    if not os.path.exists(save_dir):\n        os.mkdir(save_dir)\n    if save_prefix:\n        if index:\n            save_file = os.path.join(save_dir, save_prefix + \'_\' + str(index) + \'.jpg\')\n        else:\n            file_name = \'\'.join(random.sample(\'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz0123456789\', 16))\n            save_file = os.path.join(save_dir, save_prefix + \'_\' + file_name + \'.jpg\')\n    else:\n        if index:\n            save_file = os.path.join(save_dir, \'_\' + str(index) + \'.jpg\')\n        else:\n            file_name = \'\'.join(random.sample(\'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz0123456789\', 16))\n            save_file = os.path.join(save_dir, \'_\' + file_name + \'.jpg\')\n    with open(save_file, \'wb\') as f:\n        print(\'\\n--- image saved into {}\'.format(os.path.basename(save_file)))\n        f.write(image_data)\n\n\ndef decode_url(url):\n    url = url.replace(""_z2C$q"", "":"")\n    url = url.replace(""_z&e3B"", ""."")\n    url = url.replace(""AzdH3F"", ""/"")\n    in_table = ""wkv1ju2it3hs4g5rq6fp7eo8dn9cm0bla""\n    out_table = ""abcdefghijklmnopqrstuvw1234567890""\n    trans_table = str.maketrans(in_table, out_table)\n    url = url.translate(trans_table)\n    return url\n\n\ndef get_images_and_save(query_words, root_path, max_number):\n    print(query_words)\n    print(root_path)\n    for k, query_word in enumerate(query_words):\n        url_pattern = ""https://image.baidu.com/search/acjson?tn=resultjson_com&ipn=rj&"" \\\n                      ""ct=201326592&fp=result&queryWord={word}&cl=2&lm=-1&ie=utf-8&oe=utf-8&st=-1&ic=0"" \\\n                      ""&word={word}&face=0&istype=2nc=1&pn={pn}&rn=60""\n        urls = (url_pattern.format(word=query_word, pn=p) for p in range(0, max_number, 30))\n        for i_u, url in enumerate(urls):\n            try:\n                print(url)\n                html = requests.get(url).text\n                image_urls = re.findall(\'""objURL"":""(.*?)"",\', html, re.S)\n                for i, image_url in enumerate(image_urls):\n                    try:\n                        print(\'[INFO] decoding url..\')\n                        image_url = decode_url(image_url)\n                        print(image_url)\n                        print(\'[INFO] solving %d image\' % i)\n                        image = requests.get(image_url, stream=False, timeout=10).content\n                        save_dir = os.path.join(os.path.abspath(root_path), query_words[k])\n                        save_image(image, query_word.replace(\' \', \'\'), save_dir, str(i_u)+str(i))\n                    except requests.exceptions.ConnectionError:\n                        print(\'[INFO] url: %s can not found image.\' % image_url)\n                        continue\n            except Exception as e:\n                print(e)\n                print(\'[ERROR] pass this url.\')\n                pass\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'scraper images, scrap any image from Baidu.\')\n\n    help_ = \'set your query words in a list\'\n    parser.add_argument(\'-q\', \'--query\', nargs=\'+\', help=help_)\n\n    help_ = \'set save root dir\'\n    parser.add_argument(\'-d\', \'--dir\', default=\'./\', help=help_)\n\n    help_ = \'max download image number, default is 80000\'\n    parser.add_argument(\'-m\', \'--max\', type=int, default=80000, help=help_)\n\n    args_ = parser.parse_args()\n    return args_\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    queries = args.query\n    save_root_dir = args.dir\n    max_num = args.max\n    if not os.path.exists(os.path.abspath(save_root_dir)):\n        os.mkdir(os.path.abspath(save_root_dir))\n    get_images_and_save(queries, save_root_dir, max_num)\n\n'"
alfred/modules/text/__init__.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/modules/vision/__init__.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/modules/vision/face_extractor.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nThis file using for extracting faces of all images\n\n""""""\nimport glob\ntry:\n    import dlib\nexcept ImportError:\n    print(\'You have not installed dlib, install from https://github.com/davisking/dlib\')\n    print(\'see you later.\')\n    exit(0)\nimport os\nimport cv2\nimport numpy as np\nfrom loguru import logger\n\n\nclass FaceExtractor(object):\n\n    def __init__(self, predictor_path=\'shape_predictor_68_face_landmarks.dat\'):\n        self.detector = dlib.get_frontal_face_detector()\n        self.predictor_path = predictor_path\n        self.predictor = dlib.shape_predictor(predictor_path)\n\n\n    def get_faces_list(self, img, landmark=False):\n        """"""\n        get faces and locations\n        """"""\n        assert isinstance(img, np.ndarray), \'img should be numpy array (cv2 frame)\'\n        if landmark:\n            if os.path.exists(self.predictor_path):\n                predictor = dlib.shape_predictor(self.predictor_path)\n            else:\n                logger.error(\'Could not found model file {}, you should download \'\n                \'dlib landmark model: http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\'.format(self.predictor_path))\n                exit(0)\n        dets = self.detector(img, 1)\n        all_faces = []\n        locations = []\n        landmarks = []\n        for i, d in enumerate(dets):\n            # get the face crop\n            x = int(d.left())\n            y = int(d.top())\n            w = int(d.width())\n            h = int(d.height())\n\n            face_patch = np.array(img)[y: y + h, x: x + w, 0:3]\n\n            if landmark:\n                shape = predictor(img, d)\n                landmarks.append(shape)\n            locations.append([x, y, w, h])\n            all_faces.append(face_patch)\n        if landmark:\n            return all_faces, locations, landmarks\n        else:\n            return all_faces, locations\n\n    def get_faces(self, img_d):\n        """"""\n        get all faces from img_d\n        :param img_d:\n        :return:\n        """"""\n\n        all_images = []\n        for e in [\'png\', \'jpg\', \'jpeg\']:\n            all_images.extend(glob.glob(os.path.join(img_d, \'*.{}\'.format(e))))\n        print(\'Found all {} images under {}\'.format(len(all_images), img_d))\n\n        s_d = os.path.dirname(img_d) + ""_faces""\n        if not os.path.exists(s_d):\n            os.makedirs(s_d)\n        for img_f in all_images:\n            img = cv2.imread(img_f, cv2.COLOR_BGR2RGB)\n\n            dets = self.detector(img, 1)\n            print(\'=> get {} faces in {}\'.format(len(dets), img_f))\n            print(\'=> saving faces...\')\n            for i, d in enumerate(dets):\n                save_face_f = os.path.join(s_d, os.path.basename(img_f).split(\'.\')[0]\n                                           + \'_face_{}.png\'.format(i))\n\n                # get the face crop\n                x = int(d.left())\n                y = int(d.top())\n                w = int(d.width())\n                h = int(d.height())\n\n                face_patch = np.array(img)[y: y + h, x: x + w, 0:3]\n                # print(face_patch.shape)\n                img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n\n                # cv2.imshow(\'tt\', img)\n                # cv2.waitKey(0)\n                cv2.imwrite(save_face_f, face_patch)\n        print(\'Done!\')\n        # cv2.waitKey(0)\n\n'"
alfred/modules/vision/to_video.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nthis part using for combine image sequences into a single video\n\nas previously version, the sequence are not well ordered so that video were not\nfrequent, we solve that problem now\n\n""""""\nimport os\nimport cv2\nfrom colorama import Fore, Back, Style\nimport numpy as np\nimport sys\n\n\nclass VideoCombiner(object):\n    def __init__(self, img_dir):\n        self.img_dir = img_dir\n\n        if not os.path.exists(self.img_dir):\n            print(Fore.RED + \'=> Error: \' + \'{} not exist.\'.format(self.img_dir))\n            exit(0)\n\n        self._get_video_shape()\n\n    def _get_video_shape(self):\n        self.all_images = [os.path.join(self.img_dir, i) for i in os.listdir(self.img_dir)]\n\n        # this sorted method seems has problem\n        self.all_images.sort(key=lambda f: int(\'\'.join(filter(str.isdigit, f))))\n        for item in self.all_images[:int(len(self.all_images) // 2)]:\n            print(item)\n        # order the images order.\n\n        sample_img = np.random.choice(self.all_images)\n        if os.path.exists(sample_img):\n            img = cv2.imread(sample_img)\n            self.video_shape = img.shape\n        else:\n            print(Fore.RED + \'=> Error: \' + \'{} not found or open failed, try again.\'.format(sample_img))\n            exit(0)\n\n    def combine(self, target_file=\'combined.mp4\'):\n        size = (self.video_shape[1], self.video_shape[0])\n        print(\'=> target video frame size: \', size)\n        print(\'=> all {} frames to solve.\'.format(len(self.all_images)))\n        target_f = \'combined_{}.mp4\'.format(os.path.basename(self.img_dir))\n        video_writer = cv2.VideoWriter(target_f, cv2.VideoWriter_fourcc(*\'DIVX\'), 24, size)\n        i = 0\n        print(\'=> Solving, be patient.\')\n        for img in self.all_images:\n            img = cv2.imread(img, cv2.COLOR_BGR2RGB)\n            i += 1\n            # print(\'=> Solving: \', i)\n            video_writer.write(img)\n        video_writer.release()\n        print(\'Done!\')\n\n\n# d = sys.argv[1]\n# combiner = VideoCombiner(d)\n# combiner.combine()'"
alfred/modules/vision/video_extractor.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport os\nimport sys\ntry:\n    import cv2\nexcept ImportError:\n    print(\'you are not install opencv-python, using pip install opencv-python install it.\')\nfrom colorama import Fore, Back, Style\n\n\nclass VideoExtractor(object):\n\n    def __init__(self, jump_frames=6, save_format=\'frame_%06d.jpg\'):\n        """"""\n        we set frames to jump, etc, using jump_frames=6\n        will save one frame per 6 frames jumped\n        :param jump_frames:\n        :param save_format: this is the frames save format\n        users can decide what\'s the format is: frame_0000004.jpg\n        """"""\n        self.current_frame = 0\n        self.current_save_frame = 0\n        if jump_frames:\n            self.jump_frames = int(jump_frames)\n        else:\n            self.jump_frames = 6\n        self.save_format = save_format\n\n    def extract(self, video_f):\n        if os.path.exists(video_f) and os.path.isfile(video_f):\n            cap = cv2.VideoCapture(video_f)\n\n            save_dir = os.path.join(os.path.dirname(video_f), os.path.basename(video_f).split(\'.\')[0])\n            if not os.path.exists(save_dir):\n                os.makedirs(save_dir)\n\n            res = True\n            while res:\n                res, image = cap.read()\n                self.current_frame += 1\n                if self.current_frame % self.jump_frames == 0:\n                    print(\'Read frame: {} jump frames: {}\'.format(self.current_frame, self.jump_frames))\n                    cv2.imwrite(os.path.join(save_dir, self.save_format % self.current_save_frame), image)\n                    self.current_save_frame += 1\n\n            print(Fore.GREEN + Style.BRIGHT)\n            print(\'Success!\')\n        else:\n            print(Fore.RED + Style.BRIGHT)\n            print(\'Error! \' + Style.RESET_ALL + \'{} not exist.\'.format(video_f))\n'"
alfred/modules/vision/video_reducer.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport os\nimport sys\nimport cv2\nfrom colorama import Fore, Back, Style\nfrom alfred.utils.log import logger as logging\n\n\nclass VideoReducer(object):\n\n    def __init__(self, jump_frames=6):\n        """"""\n        we set frames to jump, etc, using jump_frames=6\n        will save one frame per 6 frames jumped\n        :param jump_frames:\n        :param save_format: this is the frames save format\n        users can decide what\'s the format is: frame_0000004.jpg\n        """"""\n        self.current_frame = 0\n        self.current_save_frame = 0\n        if jump_frames:\n            self.jump_frames = int(jump_frames)\n        else:\n            self.jump_frames = 6\n\n    def act(self, video_f):\n        """"""\n        reduce the video frame by drop frames \n        \n        """"""\n        if os.path.exists(video_f) and os.path.isfile(video_f):\n            logging.info(\'start to reduce file: {}\'.format(video_f))\n            cap = cv2.VideoCapture(video_f)\n            target_f = os.path.join(os.path.dirname(video_f), os.path.basename(video_f).split(\'.\')[0] + \'_reduced.mp4\')\n            size = (int(cap.get(3)), int(cap.get(4)))\n            logging.info(\'video size: {}, will support reduce size in the future.\'.format(size))\n            video_writer = cv2.VideoWriter(target_f, cv2.VideoWriter_fourcc(*\'DIVX\'), 24, size)\n            res = True\n            while res:\n                res, image = cap.read()\n                self.current_frame += 1\n                if (self.current_frame % self.jump_frames == 0) or self.current_frame < 15:\n                    print(\'Read frame: {} jump frames: {}\'.format(self.current_frame, self.jump_frames))\n                    self.current_save_frame += 1\n                    video_writer.write(image)\n            video_writer.release()\n            logging.info(\'reduced video file has been saved into: {}\'.format(target_f))\n        else:\n            print(Fore.RED + Style.BRIGHT)\n            print(\'Error! \' + Style.RESET_ALL + \'{} not exist.\'.format(video_f))\n'"
alfred/modules/vision/vis_kit.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nthis file contains a tool kit for computer vision kit\nwe can using this powerful kit to display detection or\nsegmentation skillfully\n""""""\nimport numpy as np\nimport colorsys\nimport cv2\nimport time\nimport os\nimport tensorflow as tf\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.patheffects as patheffects\n\n\ndef draw_box_without_score(img, boxes, classes=None, is_show=False):\n    """"""\n    Draw boxes on image, the box mostly are annotations, not the model predict box\n    """"""\n    assert isinstance(boxes,\n                      np.ndarray), \'boxes must nump array, with shape of (None, 5)\\nevery element contains (x1,y1,x2,y2, label)\'\n    if classes:\n        pass\n    else:\n        height = img.shape[0]\n        width = img.shape[1]\n\n        font = cv2.QT_FONT_NORMAL\n        font_scale = 0.4\n        font_thickness = 1\n        line_thickness = 1\n\n        all_cls = []\n        for i in range(boxes.shape[0]):\n            cls = boxes[i, -1]\n            all_cls.append(cls)\n            all_cls = set(all_cls)\n            unique_color = _create_unique_color_uchar(all_cls.index(cls))\n\n            y1 = int(boxes[i, 2])\n            x1 = int(boxes[i, 3])\n            y2 = int(boxes[i, 4])\n            x2 = int(boxes[i, 5])\n\n            cv2.rectangle(img, (x1, y1), (x2, y2), unique_color, line_thickness)\n\n            text_label = \'{}\'.format(cls)\n            (ret_val, base_line) = cv2.getTextSize(text_label, font, font_scale, font_thickness)\n            text_org = (x1, y1 - 0)\n\n            cv2.rectangle(img, (text_org[0] - 5, text_org[1] + base_line + 2),\n                          (text_org[0] + ret_val[0] + 5, text_org[1] - ret_val[1] - 2), unique_color, line_thickness)\n            # this rectangle for fill text rect\n            cv2.rectangle(img, (text_org[0] - 5, text_org[1] + base_line + 2),\n                          (text_org[0] + ret_val[0] + 4, text_org[1] - ret_val[1] - 2),\n                          unique_color, -1)\n            cv2.putText(img, text_label, text_org, font, font_scale, (255, 255, 255), font_thickness)\n        if is_show:\n            cv2.imshow(\'image\', img)\n            cv2.waitKey(0)\n        return img\n\n\ndef visualize_det_cv2(img, detections, classes=None, thresh=0.6, is_show=False, background_id=-1):\n    """"""\n    visualize detection on image using cv2, this is the standard way to visualize detections\n    :param img:\n    :param detections: ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n            each row is one object\n    :param classes:\n    :param thresh:\n    :param is_show:\n    :param background_id: -1\n    :return:\n    """"""\n    assert classes, \'from visualize_det_cv2, classes must be provided, each class in a list with\' \\\n                    \'certain order.\'\n    assert isinstance(img, np.ndarray), \'from visualize_det_cv2, img must be a numpy array object.\'\n\n    height = img.shape[0]\n    width = img.shape[1]\n\n    font = cv2.QT_FONT_NORMAL\n    font_scale = 0.4\n    font_thickness = 1\n    line_thickness = 1\n\n    for i in range(detections.shape[0]):\n        cls_id = int(detections[i, 0])\n        if cls_id != background_id:\n            score = detections[i, 1]\n            if score > thresh:\n                unique_color = _create_unique_color_uchar(cls_id)\n\n                # if detection coordinates normalized, then do this step, otherwise not\n                # x1 = int(detections[i, 2] * width)\n                # y1 = int(detections[i, 3] * height)\n                # x2 = int(detections[i, 4] * width)\n                # y2 = int(detections[i, 5] * height)\n\n                y1 = int(detections[i, 2])\n                x1 = int(detections[i, 3])\n                y2 = int(detections[i, 4])\n                x2 = int(detections[i, 5])\n\n                cv2.rectangle(img, (x1, y1), (x2, y2), unique_color, line_thickness)\n\n                text_label = \'{} {:.2f}\'.format(classes[cls_id], score)\n                (ret_val, base_line) = cv2.getTextSize(text_label, font, font_scale, font_thickness)\n                text_org = (x1, y1 - 0)\n\n                cv2.rectangle(img, (text_org[0] - 5, text_org[1] + base_line + 2),\n                              (text_org[0] + ret_val[0] + 5, text_org[1] - ret_val[1] - 2), unique_color,\n                              line_thickness)\n                # this rectangle for fill text rect\n                cv2.rectangle(img, (text_org[0] - 5, text_org[1] + base_line + 2),\n                              (text_org[0] + ret_val[0] + 4, text_org[1] - ret_val[1] - 2),\n                              unique_color, -1)\n                cv2.putText(img, text_label, text_org, font, font_scale, (255, 255, 255), font_thickness)\n    if is_show:\n        cv2.imshow(\'image\', img)\n        cv2.waitKey(0)\n    return img\n\n\ndef visualize_det_mask_cv2(img, detections, masks, classes=None, is_show=False, background_id=-1, is_video=False):\n    """"""\n    this method using for display detections and masks on image\n    :param img:\n    :param detections: numpy.array([[id, score, x1, y1, x2, y2]...])\n            each row is one object. contains id and score in the first 2 rows\n    :param masks: numpy.array([[mask_width, mask_height], ...], every element is an\n    one chanel mask of on object\n    :param classes: classes names in a list with certain order\n    :param is_show: to show if it is video\n    :param background_id\n    :param is_video\n    :return:\n    """"""\n    assert isinstance(img, np.ndarray) and isinstance(detections, np.ndarray) and isinstance(masks, np.ndarray), \\\n        \'images and detections and masks must be numpy array\'\n    assert detections.shape[0] == masks.shape[-1], \'detections nums and masks nums are not equal\'\n    assert is_show != is_video, \'you can not set is_show and is_video at the same time.\'\n    # draw detections first\n    img = visualize_det_cv2(img, detections, classes=classes, is_show=False)\n\n    masked_image = img\n    print(\'masked image shape: \', masked_image.shape)\n    num_instances = detections.shape[0]\n    for i in range(num_instances):\n        cls_id = int(detections[i, 0])\n        if cls_id != background_id:\n            unique_color = _create_unique_color_uchar(cls_id)\n            mask = masks[:, :, i]\n            masked_image = _apply_mask2(masked_image, mask, unique_color)\n    # masked_image = masked_image.astype(int)\n    if is_video:\n        cv2.imshow(\'image\', masked_image)\n        cv2.waitKey(1)\n    elif is_show:\n        cv2.imshow(\'image\', masked_image)\n        cv2.waitKey(0)\n    return masked_image\n\n\ndef draw_masks(img, masks, cls_color_list, is_show=False, background_id=-1, is_video=False, convert_bgr=False):\n    """"""\n    draw masks pure on an image, the mask format is something like this:\n    [[[1], [1], [1], .., [2]],\n     [[1], [1], [1], .., [2]],\n     [[1], [1], [1], .., [2]]]\n    every pixel in image is a class\n\n    the color list better using RGBA channel\n    cls_color_list = [(223,  224, 225, 0.4), (12, 23, 23, 0.4), ...] a list of colors\n\n    Note: suppose the img in BGR format, you should convert to RGB once img returned\n    :param img:\n    :param masks:\n    :param cls_color_list:\n    :param is_show:\n    :param background_id:\n    :param is_video:\n    :return:\n    """"""\n    n, h, w, c = masks.shape\n\n    mask_flatten = masks[0].flatten()\n    mask_color = np.array(list(map(lambda i: cls_color_list[i], mask_flatten)))\n    # reshape to normal image shape,\n    mask_color = np.reshape(mask_color, (h, w, 3)).astype(\'float32\')\n\n    # add this mask on img\n    # img = cv2.add(img, mask_color)\n    if convert_bgr:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.addWeighted(img, 0.6, mask_color, 0.4, 0)\n    if is_show:\n        cv2.imshow(\'img\', img)\n        cv2.imwrite(\'test_res.jpg\', img)\n        cv2.waitKey(0)\n    return img\n\n\ndef _apply_mask2(image, mask, color, alpha=0.5):\n    for c in range(3):\n        image[:, :, c] = np.where(mask == 1, image[:, :, c] * (1 - alpha) + alpha * color[c],\n                                  image[:, :, c])\n    return image\n\n\ndef create_unique_color_float(tag, hue_step=0.41, alpha=0.7):\n    h, v = (tag * hue_step) % 1, 1. - (int(tag * hue_step) % 4) / 5.\n    r, g, b = colorsys.hsv_to_rgb(h, 1., v)\n    return r, g, b, alpha\n\n\ndef create_unique_color_uchar(tag, hue_step=0.41, alpha=0.7):\n    r, g, b, a = create_unique_color_float(tag, hue_step, alpha)\n    return int(255 * r), int(255 * g), int(255 * b), int(255 * a)\n\n\n# ----------------------- 3D drawing functionality ----------------------\n\n'"
alfred/vis/image/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/vis/image/common.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\ncommon functionality in visualization kit\n""""""\nimport numpy as np\nimport cv2\nimport colorsys\n\n\ndef create_unique_color_float(tag, hue_step=0.41, alpha=0.7):\n    h, v = (tag * hue_step) % 1, 1. - (int(tag * hue_step) % 4) / 5.\n    r, g, b = colorsys.hsv_to_rgb(h, 1., v)\n    return r, g, b, alpha\n\n\ndef create_unique_color_uchar(tag, hue_step=0.41, alpha=0.7):\n    r, g, b, a = create_unique_color_float(tag, hue_step, alpha)\n    return int(255 * r), int(255 * g), int(255 * b), int(255 * a)\n\n\ndef get_unique_color_by_id(idx, alpha=0.7):\n    """"""\n    this method can be using when get unique color from id\n    or something else\n    :param idx:\n    :param alpha:\n    :return:\n    """"""\n    return create_unique_color_uchar(idx, alpha)\n\n\n""""""\nwe need some help functions to draw doted rectangle in opencv\n""""""\n\n\ndef _drawline(img, pt1, pt2, color, thickness=1, style=\'dotted\', gap=20):\n    dist = ((pt1[0]-pt2[0])**2+(pt1[1]-pt2[1])**2)**.5\n    pts = []\n    for i in np.arange(0, dist, gap):\n        r = i/dist\n        x = int((pt1[0]*(1-r)+pt2[0]*r)+.5)\n        y = int((pt1[1]*(1-r)+pt2[1]*r)+.5)\n        p = (x, y)\n        pts.append(p)\n\n    if style == \'dotted\':\n        for p in pts:\n            cv2.circle(img, p, thickness, color, -1)\n    elif style == \'dashed\':\n        s = pts[0]\n        e = pts[0]\n        i = 0\n        for p in pts:\n            s = e\n            e = p\n            if i % 2 == 1:\n                cv2.line(img, s, e, color, thickness)\n            i += 1\n    else:\n        ValueError(\'style can only be dotted or dashed for now!\')\n\n\ndef _drawpoly(img, pts, color, thickness=1, style=\'dotted\',):\n    s = pts[0]\n    e = pts[0]\n    pts.append(pts.pop(0))\n    for p in pts:\n        s = e\n        e = p\n        _drawline(img, s, e, color, thickness, style, gap=6)\n\n\ndef draw_rect_with_style(img, pt1, pt2, color, thickness=1, style=\'dotted\'):\n    pts = [pt1, (pt2[0], pt1[1]), pt2, (pt1[0], pt2[1])]\n    _drawpoly(img, pts, color, thickness, style)\n    return img\n\n\ndef put_txt_with_newline(image, multi_line_txt, pt, font, font_scale, color, thickness, line_type):\n    text_size, _ = cv2.getTextSize(multi_line_txt, font, font_scale, thickness)\n    line_height = text_size[1] + 5\n    x, y0 = pt\n    for i, line in enumerate(multi_line_txt.split(""\\n"")):\n        y = y0 + i * line_height\n        cv2.putText(image,\n                    line,\n                    (x, y),\n                    font,\n                    font_scale,\n                    color,\n                    thickness,\n                    line_type)\n\n\nif __name__ == \'__main__\':\n    c = create_unique_color_uchar(1)\n    print(c)\n'"
alfred/vis/image/det.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\ndraw detection result base on various format\n\nafter detection\n\nalso include draw 3d box on image\n""""""\nimport numpy as np\nimport cv2\nimport os\n\nfrom .common import create_unique_color_uchar\nfrom .common import draw_rect_with_style\nimport warnings\nfrom collections import Counter, OrderedDict\nfrom .common import put_txt_with_newline\n\n\ndef _draw_round_dot_border(img, pt1, pt2, color, thickness, r=2, d=5):\n    x1, y1 = pt1\n    x2, y2 = pt2\n\n    # Top left\n    cv2.line(img, (x1 + r, y1), (x1 + r + d, y1), color, thickness)\n    cv2.line(img, (x1, y1 + r), (x1, y1 + r + d), color, thickness)\n    cv2.ellipse(img, (x1 + r, y1 + r), (r, r), 180, 0, 90, color, thickness)\n\n    # Top right\n    cv2.line(img, (x2 - r, y1), (x2 - r - d, y1), color, thickness)\n    cv2.line(img, (x2, y1 + r), (x2, y1 + r + d), color, thickness)\n    cv2.ellipse(img, (x2 - r, y1 + r), (r, r), 270, 0, 90, color, thickness)\n\n    # Bottom left\n    cv2.line(img, (x1 + r, y2), (x1 + r + d, y2), color, thickness)\n    cv2.line(img, (x1, y2 - r), (x1, y2 - r - d), color, thickness)\n    cv2.ellipse(img, (x1 + r, y2 - r), (r, r), 90, 0, 90, color, thickness)\n\n    # Bottom right\n    cv2.line(img, (x2 - r, y2), (x2 - r - d, y2), color, thickness)\n    cv2.line(img, (x2, y2 - r), (x2, y2 - r - d), color, thickness)\n    cv2.ellipse(img, (x2 - r, y2 - r), (r, r), 0, 0, 90, color, thickness)\n    return img\n\n\ndef draw_one_bbox(image, box, unique_color, thickness):\n    x1 = int(box[0])\n    y1 = int(box[1])\n    x2 = int(box[2])\n    y2 = int(box[3])\n    cv2.rectangle(image, (x1, y1), (x2, y2), unique_color, thickness)\n    return image\n\n\n# ==================== Below are deprecation API =================\ndef draw_box_without_score(img, boxes, classes=None, is_show=False):\n    """"""\n    Draw boxes on image, the box mostly are annotations, not the model predict box\n    """"""\n    warnings.warn(\n        \'this method is deprecated, using visiualize_det_cv2 instead\', DeprecationWarning)\n    assert isinstance(boxes,\n                      np.ndarray), \'boxes must nump array, with shape of (None, 5)\\nevery element contains (x1,y1,x2,y2, label)\'\n    if classes:\n        pass\n    else:\n        height = img.shape[0]\n        width = img.shape[1]\n\n        font = cv2.QT_FONT_NORMAL\n        font_scale = 0.4\n        font_thickness = 1\n        line_thickness = 1\n\n        all_cls = []\n        for i in range(boxes.shape[0]):\n            cls = boxes[i, -1]\n            all_cls.append(cls)\n            all_cls = set(all_cls)\n            unique_color = create_unique_color_uchar(all_cls.index(cls))\n\n            y1 = int(boxes[i, 2])\n            x1 = int(boxes[i, 3])\n            y2 = int(boxes[i, 4])\n            x2 = int(boxes[i, 5])\n\n            cv2.rectangle(img, (x1, y1), (x2, y2),\n                          unique_color, line_thickness)\n\n            text_label = \'{}\'.format(cls)\n            (ret_val, base_line) = cv2.getTextSize(\n                text_label, font, font_scale, font_thickness)\n            text_org = (x1, y1 - 0)\n\n            cv2.rectangle(img, (text_org[0] - 5, text_org[1] + base_line + 2),\n                          (text_org[0] + ret_val[0] + 5, text_org[1] - ret_val[1] - 2), unique_color, line_thickness)\n            # this rectangle for fill text rect\n            cv2.rectangle(img, (text_org[0] - 5, text_org[1] + base_line + 2),\n                          (text_org[0] + ret_val[0] + 4,\n                           text_org[1] - ret_val[1] - 2),\n                          unique_color, -1)\n            cv2.putText(img, text_label, text_org, font,\n                        font_scale, (255, 255, 255), font_thickness)\n        if is_show:\n            cv2.imshow(\'image\', img)\n            cv2.waitKey(0)\n        return img\n\n\ndef visualize_det_cv2(img, detections, classes=None, thresh=0.6, is_show=False, background_id=-1, mode=\'xyxy\'):\n    """"""\n    visualize detection on image using cv2, this is the standard way to visualize detections\n\n    new add mode option\n    mode can be one of \'xyxy\' and \'xywh\', \'xyxy\' as default\n\n    :param img:\n    :param detections: ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n            each row is one object\n    :param classes:\n    :param thresh:\n    :param is_show:\n    :param background_id: -1\n    :param mode:\n    :return:\n    """"""\n    assert classes, \'from visualize_det_cv2, classes must be provided, each class in a list with\' \\\n                    \'certain order.\'\n    assert isinstance(\n        img, np.ndarray), \'from visualize_det_cv2, img must be a numpy array object.\'\n\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = 0.36\n\n    font_thickness = 1\n    line_thickness = 1\n\n    for i in range(detections.shape[0]):\n        cls_id = int(detections[i, 0])\n        if cls_id != background_id:\n            score = detections[i, 1]\n            if score > thresh:\n                unique_color = create_unique_color_uchar(cls_id)\n                x1, y1, x2, y2 = 0, 0, 0, 0\n                if mode == \'xyxy\':\n                    x1 = int(detections[i, 2])\n                    y1 = int(detections[i, 3])\n                    x2 = int(detections[i, 4])\n                    y2 = int(detections[i, 5])\n                else:\n                    x1 = int(detections[i, 2])\n                    y1 = int(detections[i, 3])\n                    x2 = x1 + int(detections[i, 4])\n                    y2 = y1 + int(detections[i, 5])\n\n                cv2.rectangle(img, (x1, y1), (x2, y2),\n                              unique_color, line_thickness, cv2.LINE_AA)\n                text_label = \'{} {:.2f}\'.format(classes[cls_id], score)\n                (ret_val, _) = cv2.getTextSize(\n                    text_label, font, font_scale, font_thickness)\n                txt_bottom_left = (x1+4, y1-4)\n                cv2.rectangle(img, (txt_bottom_left[0]-4, txt_bottom_left[1] - ret_val[1]-2),\n                              (txt_bottom_left[0] + ret_val[0] +\n                               2, txt_bottom_left[1]+4),\n                              (0, 0, 0), -1)\n                cv2.putText(img, text_label, txt_bottom_left, font,\n                            font_scale, (237, 237, 237), font_thickness, cv2.LINE_AA)\n    if is_show:\n        cv2.imshow(\'image\', img)\n        cv2.waitKey(0)\n    return img\n\n\ndef visualize_det_cv2_style0(img, detections, classes=None, cls_colors=None, thresh=0.6, suit_color=False, is_show=False, background_id=-1, mode=\'xyxy\', line_thickness=1,\n                             font_scale=0.38, counter_on=False, counter_pos=(30, 150)):\n    """"""\n    visualize detection on image using cv2, this is the standard way to visualize detections\n\n    new add mode option\n    mode can be one of \'xyxy\' and \'xywh\', \'xyxy\' as default\n\n    :param img:\n    :param detections: ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n            each row is one object\n    :param classes:\n    :param cls_colors:\n    :param thresh:\n    :param is_show:\n    :param background_id: -1\n    :param mode:\n    :return:\n    """"""\n    assert classes, \'from visualize_det_cv2, classes must be provided, each class in a list with\' \\\n                    \'certain order.\'\n    assert isinstance(\n        img, np.ndarray), \'from visualize_det_cv2, img must be a numpy array object.\'\n    if cls_colors:\n        assert len(cls_colors) == len(\n            classes), \'cls_colors must be same with classes length if you specific cls_colors.\'\n\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_thickness = 1\n\n    cls_counter = []\n    for i in range(detections.shape[0]):\n        cls_id = int(detections[i, 0])\n        if cls_id != background_id:\n            score = detections[i, 1]\n            if score > thresh:\n                if cls_colors:\n                    unique_color = cls_colors[cls_id]\n                else:\n                    unique_color = create_unique_color_uchar(cls_id)\n                x1, y1, x2, y2 = 0, 0, 0, 0\n                if mode == \'xyxy\':\n                    x1 = int(detections[i, 2])\n                    y1 = int(detections[i, 3])\n                    x2 = int(detections[i, 4])\n                    y2 = int(detections[i, 5])\n                else:\n                    x1 = int(detections[i, 2])\n                    y1 = int(detections[i, 3])\n                    x2 = x1 + int(detections[i, 4])\n                    y2 = y1 + int(detections[i, 5])\n\n                cv2.rectangle(img, (x1, y1), (x2, y2),\n                              unique_color, line_thickness)\n                text_label = \'{} {:.1f}%\'.format(classes[cls_id], score*100)\n                if counter_on:\n                    cls_counter.append(classes[cls_id])\n\n                ((txt_w, txt_h), _) = cv2.getTextSize(\n                    text_label, font, font_scale, 1)\n                # Place text background.\n                back_tl = x1, y1 - int(1.5 * txt_h)\n                back_br = x1 + txt_w, y1+2\n                if suit_color:\n                    cv2.rectangle(img, back_tl, back_br, unique_color, -1)\n                else:\n                    cv2.rectangle(img, back_tl, back_br, (0, 0, 0), -1)\n\n                txt_tl = x1, y1 - int(0.3 * txt_h)\n                cv2.putText(img, text_label, txt_tl, font, font_scale,\n                            (255, 255, 255), font_thickness, lineType=cv2.LINE_AA)\n    if counter_on:\n        cc = Counter(cls_counter)\n        cc = OrderedDict(sorted(cc.items()))\n        # drw counter result on image\n        txt = \'\'\n        for k, v in cc.items():\n            txt += \'{}: {}\\n\'.format(k, v)\n        put_txt_with_newline(img, txt, counter_pos, font, 1.9, (0, 255, 0), 2, cv2.LINE_AA)\n\n    if is_show:\n        cv2.imshow(\'image\', img)\n        cv2.waitKey(0)\n    return img\n\n\ndef visualize_det_cv2_fancy(img, detections, classes=None, thresh=0.6, is_show=False, background_id=-1, mode=\'xyxy\', r=4, d=6):\n    """"""\n    visualize detections with a more fancy way\n\n    new add mode option\n    mode can be one of \'xyxy\' and \'xywh\', \'xyxy\' as default\n\n    :param img:\n    :param detections: ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n            each row is one object\n    :param classes:\n    :param thresh:\n    :param is_show:\n    :param background_id: -1\n    :param mode:\n    :return:\n    """"""\n    assert classes, \'from visualize_det_cv2, classes must be provided, each class in a list with\' \\\n                    \'certain order.\'\n    assert isinstance(\n        img, np.ndarray), \'from visualize_det_cv2, img must be a numpy array object.\'\n\n    height = img.shape[0]\n    width = img.shape[1]\n\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = 0.35\n    font_thickness = 1\n    line_thickness = 2\n\n    for i in range(detections.shape[0]):\n        cls_id = int(detections[i, 0])\n        if cls_id != background_id:\n            score = detections[i, 1]\n            if score > thresh:\n                unique_color = create_unique_color_uchar(cls_id)\n                x1, y1, x2, y2 = 0, 0, 0, 0\n                if mode == \'xyxy\':\n                    x1 = int(detections[i, 2])\n                    y1 = int(detections[i, 3])\n                    x2 = int(detections[i, 4])\n                    y2 = int(detections[i, 5])\n                else:\n                    x1 = int(detections[i, 2])\n                    y1 = int(detections[i, 3])\n                    x2 = x1 + int(detections[i, 4])\n                    y2 = y1 + int(detections[i, 5])\n\n                _draw_round_dot_border(\n                    img, (x1, y1), (x2, y2), unique_color, line_thickness, r, d)\n                text_label = \'{} {:.2f}\'.format(classes[cls_id], score)\n                (txt_size, line_h) = cv2.getTextSize(\n                    text_label, font, font_scale, font_thickness)\n                txt_org = (int((x1+x2)/2 - txt_size[0]/2), int(y1+line_h+2))\n                cv2.putText(img, text_label, txt_org, font, font_scale,\n                            (255, 255, 255), font_thickness, lineType=cv2.LINE_AA)\n    if is_show:\n        cv2.imshow(\'image\', img)\n        cv2.waitKey(0)\n    return img\n\n\ndef visualize_det_cv2_part(img, confs, cls_ids, locs, class_names=None, thresh=0.6,\n                           is_show=False, background_id=-1, mode=\'xyxy\', style=\'none\', \n                           force_color=None, line_thickness=1, wait_t=0):\n    """"""\n    visualize detection on image using cv2, this is the standard way to visualize detections\n\n    new add mode option\n    mode can be one of \'xyxy\' and \'xywh\', \'xyxy\' as default\n\n    :param img:\n    :param detections: ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n            each row is one object\n    :param classes:\n    :param thresh:\n    :param is_show:\n    :param background_id: -1\n    :param mode:\n    :return:\n    """"""\n    assert class_names, \'from visualize_det_cv2, classes must be provided, each class in a list with\' \\\n        \'certain order.\'\n    assert isinstance(\n        img, np.ndarray), \'from visualize_det_cv2, img must be a numpy array object.\'\n    if force_color:\n        assert isinstance(force_color, list) or isinstance(\n            force_color, np.ndarray), \'force_color must be list or numpy array\'\n\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = 0.36\n    font_thickness = 1\n    line_thickness = line_thickness\n\n    n_boxes = 0\n    if isinstance(locs, np.ndarray):\n        n_boxes = locs.shape[0]\n    elif isinstance(locs, list):\n        n_boxes = len(locs)\n    else:\n        print(\'locs with unsupported type, boxes must be ndarray or list.\')\n\n    for i in range(n_boxes):\n        cls_id = int(cls_ids[i])\n        if cls_id != background_id:\n            score = confs[i]\n            if score > thresh:\n                if force_color:\n                    unique_color = force_color[cls_id]\n                else:\n                    unique_color = create_unique_color_uchar(cls_id)\n                x1, y1, x2, y2 = 0, 0, 0, 0\n                if mode == \'xyxy\':\n                    x1 = int(locs[i, 0])\n                    y1 = int(locs[i, 1])\n                    x2 = int(locs[i, 2])\n                    y2 = int(locs[i, 3])\n                else:\n                    x1 = int(locs[i, 0])\n                    y1 = int(locs[i, 1])\n                    x2 = x1 + int(locs[i, 2])\n                    y2 = y1 + int(locs[i, 3])\n\n                if style in [\'dashed\', \'dotted\']:\n                    draw_rect_with_style(\n                        img, (x1, y1), (x2, y2), unique_color, line_thickness, style=style)\n                else:\n                    cv2.rectangle(img, (x1, y1), (x2, y2),\n                                  unique_color, line_thickness, cv2.LINE_AA)\n                text_label = \'{} {:.2f}\'.format(class_names[cls_id], score)\n                (ret_val, _) = cv2.getTextSize(\n                    text_label, font, font_scale, font_thickness)\n                txt_bottom_left = (x1+4, y1-4)\n                cv2.rectangle(img, (txt_bottom_left[0]-4, txt_bottom_left[1] - ret_val[1]-2),\n                              (txt_bottom_left[0] + ret_val[0] +\n                               2, txt_bottom_left[1]+4),\n                              (0, 0, 0), -1)\n                cv2.putText(img, text_label, txt_bottom_left, font,\n                            font_scale, (237, 237, 237), font_thickness, cv2.LINE_AA)\n    if is_show:\n        cv2.imshow(\'image\', img)\n        cv2.waitKey(wait_t)\n    return img\n\n\ndef visualize_det_mask_cv2(img, detections, masks, classes=None, is_show=False, background_id=-1, is_video=False):\n    """"""\n    this method using for display detections and masks on image\n    :param img:\n    :param detections: numpy.array([[id, score, x1, y1, x2, y2]...])\n            each row is one object. contains id and score in the first 2 rows\n    :param masks: numpy.array([[mask_width, mask_height], ...], every element is an\n    one chanel mask of on object\n    :param classes: classes names in a list with certain order\n    :param is_show: to show if it is video\n    :param background_id\n    :param is_video\n    :return:\n    """"""\n    assert isinstance(img, np.ndarray) and isinstance(detections, np.ndarray) and isinstance(masks, np.ndarray), \\\n        \'images and detections and masks must be numpy array\'\n    assert detections.shape[0] == masks.shape[-1], \'detections nums and masks nums are not equal\'\n    assert is_show != is_video, \'you can not set is_show and is_video at the same time.\'\n    # draw detections first\n    img = visualize_det_cv2(img, detections, classes=classes, is_show=False)\n\n    masked_image = img\n    print(\'masked image shape: \', masked_image.shape)\n    num_instances = detections.shape[0]\n    for i in range(num_instances):\n        cls_id = int(detections[i, 0])\n        if cls_id != background_id:\n            unique_color = create_unique_color_uchar(cls_id)\n            mask = masks[:, :, i]\n            masked_image = _apply_mask2(masked_image, mask, unique_color)\n    # masked_image = masked_image.astype(int)\n    if is_video:\n        cv2.imshow(\'image\', masked_image)\n        cv2.waitKey(1)\n    elif is_show:\n        cv2.imshow(\'image\', masked_image)\n        cv2.waitKey(0)\n    return masked_image\n\n\ndef _apply_mask2(image, mask, color, alpha=0.5):\n    for c in range(3):\n        image[:, :, c] = np.where(mask == 1, image[:, :, c] * (1 - alpha) + alpha * color[c],\n                                  image[:, :, c])\n    return image\n\n\n# --------------- Drawing 3d box on image parts --------------\ndef draw_one_3d_box_cv2(img, box_3d, obj_id_name_map, score, tlwhy_format=False, calib_cam_to_img_p2=None,\n                        force_color=None):\n    """"""\n    provide a obj id name map like: {1, \'car\'}\n    id to distinguish with previous object type\n\n    tlwhy means input box are in format: [x, y, z, l, w, h, ry]\n    that means we should convert it first.\n    :param img:\n    :param box_3d:\n    :param obj_id_name_map:\n    :param score:\n    :param tlwhy_format:\n    :param calib_cam_to_img_p2:\n    :param force_color:\n    :return:\n    """"""\n    assert isinstance(obj_id_name_map, dict), \'obj_id_name_map must be dict\'\n    # color = None\n    if force_color:\n        color = force_color\n    else:\n        color = create_unique_color_uchar(list(obj_id_name_map.keys())[0])\n    if tlwhy_format:\n        # transform [x, y, z, l, w, h, ry] to normal box\n        assert calib_cam_to_img_p2, \'You should provide calibration matrix, convert camera to image coordinate.\'\n        center = box_3d[0: 3]\n        dims = box_3d[3: 6]\n        rot_y = -box_3d[6] / 180 * np.pi\n        # alpha / 180 * np.pi + np.arctan(center[0] / center[2])\n\n        converted_box_3d = []\n        for i in [1, -1]:\n            for j in [1, -1]:\n                for k in [0, 1]:\n                    point = np.copy(center)\n                    point[0] = center[0] + i * dims[1] / 2 * np.cos(-rot_y + np.pi / 2) + \\\n                        (j * i) * dims[2] / 2 * np.cos(-rot_y)\n                    point[2] = center[2] + i * dims[1] / 2 * np.sin(-rot_y + np.pi / 2) + \\\n                        (j * i) * dims[2] / 2 * np.sin(-rot_y)\n                    point[1] = center[1] - k * dims[0]\n\n                    point = np.append(point, 1)\n                    point = np.dot(calib_cam_to_img_p2, point)\n                    point = point[:2] / point[2]\n                    point = point.astype(np.int16)\n                    converted_box_3d.append(point)\n        print(\'final box: \', converted_box_3d)\n        # box_3d = np.asarray(converted_box_3d)\n        box_3d = converted_box_3d\n        # print(box_3d.shape)\n        for i in range(4):\n            point_1_ = box_3d[2 * i]\n            point_2_ = box_3d[2 * i + 1]\n            cv2.line(img, (point_1_[0], point_1_[1]),\n                     (point_2_[0], point_2_[1]), color, 1)\n\n        for i in range(8):\n            point_1_ = box_3d[i]\n            point_2_ = box_3d[(i + 2) % 8]\n            cv2.line(img, (point_1_[0], point_1_[1]),\n                     (point_2_[0], point_2_[1]), color, 1)\n        return img\n    else:\n        # assert len(box_3d) == 8, \'every box 3d should have 8 points. if you got 7, you may want tlwhy=True\'\n        face_idx = np.array([0, 1, 5, 4,  # front face\n                             1, 2, 6, 5,  # left face\n                             2, 3, 7, 6,  # back face\n                             3, 0, 4, 7]).reshape((4, 4))\n        # print(\'start draw...\')\n        for i in range(4):\n            x = np.append(box_3d[0, face_idx[i, ]],\n                          box_3d[0, face_idx[i, 0]])\n            y = np.append(box_3d[1, face_idx[i, ]],\n                          box_3d[1, face_idx[i, 0]])\n            # print(\'x: \', x)\n            # print(\'y: \', y)\n            # cv2.line(img, (point_1_, point_1_), (point_2_, point_2_), color, 1)\n            pts = np.vstack((x, y)).T\n            # filter negative values\n            pts = (pts + abs(pts)) / 2\n            pts = np.array([pts], dtype=int)\n            # print(pts)\n            cv2.polylines(img, pts, isClosed=True, color=color, thickness=1)\n            if i == 3:\n                # add text\n                ori_txt = pts[0][1]\n\n        return img\n'"
alfred/vis/image/get_dataset_color_map.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""Visualizes the segmentation results via specified color map.\n\nVisualizes the semantic segmentation results by the color map\ndefined by the different datasets. Supported colormaps are:\n\n* ADE20K (http://groups.csail.mit.edu/vision/datasets/ADE20K/).\n\n* Cityscapes dataset (https://www.cityscapes-dataset.com).\n\n* Mapillary Vistas (https://research.mapillary.com).\n\n* PASCAL VOC 2012 (http://host.robots.ox.ac.uk/pascal/VOC/).\n""""""\n\nimport numpy as np\n\n# Dataset names.\n_ADE20K = \'ade20k\'\n_CITYSCAPES = \'cityscapes\'\n_MAPILLARY_VISTAS = \'mapillary_vistas\'\n_PASCAL = \'pascal\'\n\n# Max number of entries in the colormap for each dataset.\n_DATASET_MAX_ENTRIES = {\n    _ADE20K: 151,\n    _CITYSCAPES: 19,\n    _MAPILLARY_VISTAS: 66,\n    _PASCAL: 256,\n}\n\n\ndef create_ade20k_label_colormap():\n    """"""Creates a label colormap used in ADE20K segmentation benchmark.\n\n    Returns:\n      A colormap for visualizing segmentation results.\n    """"""\n    return np.asarray([\n        [0, 0, 0],\n        [120, 120, 120],\n        [180, 120, 120],\n        [6, 230, 230],\n        [80, 50, 50],\n        [4, 200, 3],\n        [120, 120, 80],\n        [140, 140, 140],\n        [204, 5, 255],\n        [230, 230, 230],\n        [4, 250, 7],\n        [224, 5, 255],\n        [235, 255, 7],\n        [150, 5, 61],\n        [120, 120, 70],\n        [8, 255, 51],\n        [255, 6, 82],\n        [143, 255, 140],\n        [204, 255, 4],\n        [255, 51, 7],\n        [204, 70, 3],\n        [0, 102, 200],\n        [61, 230, 250],\n        [255, 6, 51],\n        [11, 102, 255],\n        [255, 7, 71],\n        [255, 9, 224],\n        [9, 7, 230],\n        [220, 220, 220],\n        [255, 9, 92],\n        [112, 9, 255],\n        [8, 255, 214],\n        [7, 255, 224],\n        [255, 184, 6],\n        [10, 255, 71],\n        [255, 41, 10],\n        [7, 255, 255],\n        [224, 255, 8],\n        [102, 8, 255],\n        [255, 61, 6],\n        [255, 194, 7],\n        [255, 122, 8],\n        [0, 255, 20],\n        [255, 8, 41],\n        [255, 5, 153],\n        [6, 51, 255],\n        [235, 12, 255],\n        [160, 150, 20],\n        [0, 163, 255],\n        [140, 140, 140],\n        [250, 10, 15],\n        [20, 255, 0],\n        [31, 255, 0],\n        [255, 31, 0],\n        [255, 224, 0],\n        [153, 255, 0],\n        [0, 0, 255],\n        [255, 71, 0],\n        [0, 235, 255],\n        [0, 173, 255],\n        [31, 0, 255],\n        [11, 200, 200],\n        [255, 82, 0],\n        [0, 255, 245],\n        [0, 61, 255],\n        [0, 255, 112],\n        [0, 255, 133],\n        [255, 0, 0],\n        [255, 163, 0],\n        [255, 102, 0],\n        [194, 255, 0],\n        [0, 143, 255],\n        [51, 255, 0],\n        [0, 82, 255],\n        [0, 255, 41],\n        [0, 255, 173],\n        [10, 0, 255],\n        [173, 255, 0],\n        [0, 255, 153],\n        [255, 92, 0],\n        [255, 0, 255],\n        [255, 0, 245],\n        [255, 0, 102],\n        [255, 173, 0],\n        [255, 0, 20],\n        [255, 184, 184],\n        [0, 31, 255],\n        [0, 255, 61],\n        [0, 71, 255],\n        [255, 0, 204],\n        [0, 255, 194],\n        [0, 255, 82],\n        [0, 10, 255],\n        [0, 112, 255],\n        [51, 0, 255],\n        [0, 194, 255],\n        [0, 122, 255],\n        [0, 255, 163],\n        [255, 153, 0],\n        [0, 255, 10],\n        [255, 112, 0],\n        [143, 255, 0],\n        [82, 0, 255],\n        [163, 255, 0],\n        [255, 235, 0],\n        [8, 184, 170],\n        [133, 0, 255],\n        [0, 255, 92],\n        [184, 0, 255],\n        [255, 0, 31],\n        [0, 184, 255],\n        [0, 214, 255],\n        [255, 0, 112],\n        [92, 255, 0],\n        [0, 224, 255],\n        [112, 224, 255],\n        [70, 184, 160],\n        [163, 0, 255],\n        [153, 0, 255],\n        [71, 255, 0],\n        [255, 0, 163],\n        [255, 204, 0],\n        [255, 0, 143],\n        [0, 255, 235],\n        [133, 255, 0],\n        [255, 0, 235],\n        [245, 0, 255],\n        [255, 0, 122],\n        [255, 245, 0],\n        [10, 190, 212],\n        [214, 255, 0],\n        [0, 204, 255],\n        [20, 0, 255],\n        [255, 255, 0],\n        [0, 153, 255],\n        [0, 41, 255],\n        [0, 255, 204],\n        [41, 0, 255],\n        [41, 255, 0],\n        [173, 0, 255],\n        [0, 245, 255],\n        [71, 0, 255],\n        [122, 0, 255],\n        [0, 255, 184],\n        [0, 92, 255],\n        [184, 255, 0],\n        [0, 133, 255],\n        [255, 214, 0],\n        [25, 194, 194],\n        [102, 255, 0],\n        [92, 0, 255],\n    ])\n\n\ndef create_cityscapes_label_colormap():\n    """"""Creates a label colormap used in CITYSCAPES segmentation benchmark.\n\n    Returns:\n      A colormap for visualizing segmentation results.\n    """"""\n    return np.asarray([\n        [128, 64, 128],\n        [244, 35, 232],\n        [70, 70, 70],\n        [102, 102, 156],\n        [190, 153, 153],\n        [153, 153, 153],\n        [250, 170, 30],\n        [220, 220, 0],\n        [107, 142, 35],\n        [152, 251, 152],\n        [70, 130, 180],\n        [220, 20, 60],\n        [255, 0, 0],\n        [0, 0, 142],\n        [0, 0, 70],\n        [0, 60, 100],\n        [0, 80, 100],\n        [0, 0, 230],\n        [119, 11, 32],\n    ])\n\n\ndef create_mapillary_vistas_label_colormap():\n    """"""Creates a label colormap used in Mapillary Vistas segmentation benchmark.\n\n    Returns:\n      A colormap for visualizing segmentation results.\n    """"""\n    return np.asarray([\n        [165, 42, 42],\n        [0, 192, 0],\n        [196, 196, 196],\n        [190, 153, 153],\n        [180, 165, 180],\n        [102, 102, 156],\n        [102, 102, 156],\n        [128, 64, 255],\n        [140, 140, 200],\n        [170, 170, 170],\n        [250, 170, 160],\n        [96, 96, 96],\n        [230, 150, 140],\n        [128, 64, 128],\n        [110, 110, 110],\n        [244, 35, 232],\n        [150, 100, 100],\n        [70, 70, 70],\n        [150, 120, 90],\n        [220, 20, 60],\n        [255, 0, 0],\n        [255, 0, 0],\n        [255, 0, 0],\n        [200, 128, 128],\n        [255, 255, 255],\n        [64, 170, 64],\n        [128, 64, 64],\n        [70, 130, 180],\n        [255, 255, 255],\n        [152, 251, 152],\n        [107, 142, 35],\n        [0, 170, 30],\n        [255, 255, 128],\n        [250, 0, 30],\n        [0, 0, 0],\n        [220, 220, 220],\n        [170, 170, 170],\n        [222, 40, 40],\n        [100, 170, 30],\n        [40, 40, 40],\n        [33, 33, 33],\n        [170, 170, 170],\n        [0, 0, 142],\n        [170, 170, 170],\n        [210, 170, 100],\n        [153, 153, 153],\n        [128, 128, 128],\n        [0, 0, 142],\n        [250, 170, 30],\n        [192, 192, 192],\n        [220, 220, 0],\n        [180, 165, 180],\n        [119, 11, 32],\n        [0, 0, 142],\n        [0, 60, 100],\n        [0, 0, 142],\n        [0, 0, 90],\n        [0, 0, 230],\n        [0, 80, 100],\n        [128, 64, 64],\n        [0, 0, 110],\n        [0, 0, 70],\n        [0, 0, 192],\n        [32, 32, 32],\n        [0, 0, 0],\n        [0, 0, 0],\n    ])\n\n\ndef create_pascal_label_colormap():\n    """"""Creates a label colormap used in PASCAL VOC segmentation benchmark.\n\n    Returns:\n      A colormap for visualizing segmentation results.\n    """"""\n    colormap = np.zeros((_DATASET_MAX_ENTRIES[_PASCAL], 3), dtype=int)\n    ind = np.arange(_DATASET_MAX_ENTRIES[_PASCAL], dtype=int)\n\n    for shift in reversed(range(8)):\n        for channel in range(3):\n            colormap[:, channel] |= bit_get(ind, channel) << shift\n        ind >>= 3\n\n    return colormap\n\n\ndef get_ade20k_name():\n    return _ADE20K\n\n\ndef get_cityscapes_name():\n    return _CITYSCAPES\n\n\ndef get_mapillary_vistas_name():\n    return _MAPILLARY_VISTAS\n\n\ndef get_pascal_name():\n    return _PASCAL\n\n\ndef bit_get(val, idx):\n    """"""Gets the bit value.\n\n    Args:\n      val: Input value, int or numpy int array.\n      idx: Which bit of the input val.\n\n    Returns:\n      The ""idx""-th bit of input val.\n    """"""\n    return (val >> idx) & 1\n\n\ndef create_label_colormap(dataset=_PASCAL):\n    """"""Creates a label colormap for the specified dataset.\n\n    Args:\n      dataset: The colormap used in the dataset.\n\n    Returns:\n      A numpy array of the dataset colormap.\n\n    Raises:\n      ValueError: If the dataset is not supported.\n    """"""\n    if dataset == _ADE20K:\n        return create_ade20k_label_colormap()\n    elif dataset == _CITYSCAPES:\n        return create_cityscapes_label_colormap()\n    elif dataset == _MAPILLARY_VISTAS:\n        return create_mapillary_vistas_label_colormap()\n    elif dataset == _PASCAL:\n        return create_pascal_label_colormap()\n    else:\n        raise ValueError(\'Unsupported dataset.\')\n\n\ndef label_to_color_image(label, dataset=_PASCAL):\n    """"""Adds color defined by the dataset colormap to the label.\n\n    Args:\n      label: A 2D array with integer type, storing the segmentation label.\n      dataset: The colormap used in the dataset.\n\n    Returns:\n      result: A 2D array with floating type. The element of the array\n        is the color indexed by the corresponding element in the input label\n        to the dataset color map.\n\n    Raises:\n      ValueError: If label is not of rank 2 or its value is larger than color\n        map maximum entry.\n    """"""\n    if label.ndim != 2:\n        raise ValueError(\'Expect 2-D input label\')\n\n    if np.max(label) >= _DATASET_MAX_ENTRIES[dataset]:\n        raise ValueError(\'label value too large.\')\n\n    colormap = create_label_colormap(dataset)\n    return colormap[label]\n'"
alfred/vis/image/get_dataset_label_map.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nThis file contains some useful common label map\nsuch as COCO, VOC, Cityscapes ...\n\nthis ids are same with official one.\n\n""""""\n\n# NOTE: coco has some skip category, such as skipped 83...\n# so we have to using map\n\ncoco_label_map = {\n    0: \'__background__\',\n    1: \'person\',\n    2: \'bicycle\',\n    3: \'car\',\n    4: \'motorcycle\',\n    5: \'airplane\',\n    6: \'bus\',\n    7: \'train\',\n    8: \'truck\',\n    9: \'boat\',\n    10: \'traffic light\',\n    11: \'fire hydrant\',\n    13: \'stop sign\',\n    14: \'parking meter\',\n    15: \'bench\',\n    16: \'bird\',\n    17: \'cat\',\n    18: \'dog\',\n    19: \'horse\',\n    20: \'sheep\',\n    21: \'cow\',\n    22: \'elephant\',\n    23: \'bear\',\n    24: \'zebra\',\n    25: \'giraffe\',\n    27: \'backpack\',\n    28: \'umbrella\',\n    31: \'handbag\',\n    32: \'tie\',\n    33: \'suitcase\',\n    34: \'frisbee\',\n    35: \'skis\',\n    36: \'snowboard\',\n    37: \'sports ball\',\n    38: \'kite\',\n    39: \'baseball bat\',\n    40: \'baseball glove\',\n    41: \'skateboard\',\n    42: \'surfboard\',\n    43: \'tennis racket\',\n    44: \'bottle\',\n    46: \'wine glass\',\n    47: \'cup\',\n    48: \'fork\',\n    49: \'knife\',\n    50: \'spoon\',\n    51: \'bowl\',\n    52: \'banana\',\n    53: \'apple\',\n    54: \'sandwich\',\n    55: \'orange\',\n    56: \'broccoli\',\n    57: \'carrot\',\n    58: \'hot dog\',\n    59: \'pizza\',\n    60: \'donut\',\n    61: \'cake\',\n    62: \'chair\',\n    63: \'couch\',\n    64: \'potted plant\',\n    65: \'bed\',\n    67: \'dining table\',\n    70: \'toilet\',\n    72: \'tv\',\n    73: \'laptop\',\n    74: \'mouse\',\n    75: \'remote\',\n    76: \'keyboard\',\n    77: \'cell phone\',\n    78: \'microwave\',\n    79: \'oven\',\n    80: \'toaster\',\n    81: \'sink\',\n    82: \'refrigerator\',\n    84: \'book\',\n    85: \'clock\',\n    86: \'vase\',\n    87: \'scissors\',\n    88: \'teddy bear\',\n    89: \'hair drier\',\n    90: \'toothbrush\'\n}\ncoco_label_map_list = list(coco_label_map.values())\n\n# map with mostly model prediction which is 0-80\ncoco_label_map_continuous = {i: coco_label_map_list[i] for i in range(0, len(coco_label_map_list))}\n\n\nvoc_label_map = {\n    0: \'_background_\',\n    1: \'aeroplane\',\n    2: \'bicycle\',\n    3: \'bird\',\n    4: \'boat\',\n    5: \'bottle\',\n    6: \'bus\',\n    7: \'car\',\n    8: \'cat\',\n    9: \'chair\',\n    10: \'cow\',\n    11: \'diningtable\',\n    12: \'dog\',\n    13: \'horse\',\n    14: \'motorbike\',\n    15: \'person\',\n    16: \'pottedplant\',\n    17: \'sheep\',\n    18: \'sofa\',\n    19: \'train\',\n    20: \'tvmonitor\',\n}\n\nvoc_label_map_list = list(voc_label_map.values())\n\ncityscapes_label_map = {\n    0: \'unlabeled\',\n    1: \'ego vehicle\',\n    2: \'rectification border\',\n    3: \'out of roi\',\n    4: \'static\',\n    5: \'dynamic\',\n    6: \'ground\',\n    7: \'road\',\n    8: \'sidewalk\',\n    9: \'parking\',\n    10: \'rail track\',\n    11: \'building\',\n    12: \'wall\',\n    13: \'fence\',\n    14: \'guard rail\',\n    15: \'bridge\',\n    16: \'tunnel\',\n    17: \'pole\',\n    18: \'polegroup\',\n    19: \'traffic light\',\n    20: \'traffic sign\',\n    21: \'vegetation\',\n    22: \'terrain\',\n    23: \'sky\',\n    24: \'person\',\n    25: \'rider\',\n    26: \'car\',\n    27: \'truck\',\n    28: \'bus\',\n    29: \'caravan\',\n    30: \'trailer\',\n    31: \'train\',\n    32: \'motorcycle\',\n    33: \'bicycle\',\n    -1: \'license plate\',\n}\n\nimagenet_label_map = {0: \'tench, Tinca tinca\',\n                      1: \'goldfish, Carassius auratus\',\n                      2: \'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\',\n                      3: \'tiger shark, Galeocerdo cuvieri\',\n                      4: \'hammerhead, hammerhead shark\',\n                      5: \'electric ray, crampfish, numbfish, torpedo\',\n                      6: \'stingray\',\n                      7: \'cock\',\n                      8: \'hen\',\n                      9: \'ostrich, Struthio camelus\',\n                      10: \'brambling, Fringilla montifringilla\',\n                      11: \'goldfinch, Carduelis carduelis\',\n                      12: \'house finch, linnet, Carpodacus mexicanus\',\n                      13: \'junco, snowbird\',\n                      14: \'indigo bunting, indigo finch, indigo bird, Passerina cyanea\',\n                      15: \'robin, American robin, Turdus migratorius\',\n                      16: \'bulbul\',\n                      17: \'jay\',\n                      18: \'magpie\',\n                      19: \'chickadee\',\n                      20: \'water ouzel, dipper\',\n                      21: \'kite\',\n                      22: \'bald eagle, American eagle, Haliaeetus leucocephalus\',\n                      23: \'vulture\',\n                      24: \'great grey owl, great gray owl, Strix nebulosa\',\n                      25: \'European fire salamander, Salamandra salamandra\',\n                      26: \'common newt, Triturus vulgaris\',\n                      27: \'eft\',\n                      28: \'spotted salamander, Ambystoma maculatum\',\n                      29: \'axolotl, mud puppy, Ambystoma mexicanum\',\n                      30: \'bullfrog, Rana catesbeiana\',\n                      31: \'tree frog, tree-frog\',\n                      32: \'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\',\n                      33: \'loggerhead, loggerhead turtle, Caretta caretta\',\n                      34: \'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea\',\n                      35: \'mud turtle\',\n                      36: \'terrapin\',\n                      37: \'box turtle, box tortoise\',\n                      38: \'banded gecko\',\n                      39: \'common iguana, iguana, Iguana iguana\',\n                      40: \'American chameleon, anole, Anolis carolinensis\',\n                      41: \'whiptail, whiptail lizard\',\n                      42: \'agama\',\n                      43: \'frilled lizard, Chlamydosaurus kingi\',\n                      44: \'alligator lizard\',\n                      45: \'Gila monster, Heloderma suspectum\',\n                      46: \'green lizard, Lacerta viridis\',\n                      47: \'African chameleon, Chamaeleo chamaeleon\',\n                      48: \'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis\',\n                      49: \'African crocodile, Nile crocodile, Crocodylus niloticus\',\n                      50: \'American alligator, Alligator mississipiensis\',\n                      51: \'triceratops\',\n                      52: \'thunder snake, worm snake, Carphophis amoenus\',\n                      53: \'ringneck snake, ring-necked snake, ring snake\',\n                      54: \'hognose snake, puff adder, sand viper\',\n                      55: \'green snake, grass snake\',\n                      56: \'king snake, kingsnake\',\n                      57: \'garter snake, grass snake\',\n                      58: \'water snake\',\n                      59: \'vine snake\',\n                      60: \'night snake, Hypsiglena torquata\',\n                      61: \'boa constrictor, Constrictor constrictor\',\n                      62: \'rock python, rock snake, Python sebae\',\n                      63: \'Indian cobra, Naja naja\',\n                      64: \'green mamba\',\n                      65: \'sea snake\',\n                      66: \'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus\',\n                      67: \'diamondback, diamondback rattlesnake, Crotalus adamanteus\',\n                      68: \'sidewinder, horned rattlesnake, Crotalus cerastes\',\n                      69: \'trilobite\',\n                      70: \'harvestman, daddy longlegs, Phalangium opilio\',\n                      71: \'scorpion\',\n                      72: \'black and gold garden spider, Argiope aurantia\',\n                      73: \'barn spider, Araneus cavaticus\',\n                      74: \'garden spider, Aranea diademata\',\n                      75: \'black widow, Latrodectus mactans\',\n                      76: \'tarantula\',\n                      77: \'wolf spider, hunting spider\',\n                      78: \'tick\',\n                      79: \'centipede\',\n                      80: \'black grouse\',\n                      81: \'ptarmigan\',\n                      82: \'ruffed grouse, partridge, Bonasa umbellus\',\n                      83: \'prairie chicken, prairie grouse, prairie fowl\',\n                      84: \'peacock\',\n                      85: \'quail\',\n                      86: \'partridge\',\n                      87: \'African grey, African gray, Psittacus erithacus\',\n                      88: \'macaw\',\n                      89: \'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\',\n                      90: \'lorikeet\',\n                      91: \'coucal\',\n                      92: \'bee eater\',\n                      93: \'hornbill\',\n                      94: \'hummingbird\',\n                      95: \'jacamar\',\n                      96: \'toucan\',\n                      97: \'drake\',\n                      98: \'red-breasted merganser, Mergus serrator\',\n                      99: \'goose\',\n                      100: \'black swan, Cygnus atratus\',\n                      101: \'tusker\',\n                      102: \'echidna, spiny anteater, anteater\',\n                      103: \'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus\',\n                      104: \'wallaby, brush kangaroo\',\n                      105: \'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\',\n                      106: \'wombat\',\n                      107: \'jellyfish\',\n                      108: \'sea anemone, anemone\',\n                      109: \'brain coral\',\n                      110: \'flatworm, platyhelminth\',\n                      111: \'nematode, nematode worm, roundworm\',\n                      112: \'conch\',\n                      113: \'snail\',\n                      114: \'slug\',\n                      115: \'sea slug, nudibranch\',\n                      116: \'chiton, coat-of-mail shell, sea cradle, polyplacophore\',\n                      117: \'chambered nautilus, pearly nautilus, nautilus\',\n                      118: \'Dungeness crab, Cancer magister\',\n                      119: \'rock crab, Cancer irroratus\',\n                      120: \'fiddler crab\',\n                      121: \'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica\',\n                      122: \'American lobster, Northern lobster, Maine lobster, Homarus americanus\',\n                      123: \'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\',\n                      124: \'crayfish, crawfish, crawdad, crawdaddy\',\n                      125: \'hermit crab\',\n                      126: \'isopod\',\n                      127: \'white stork, Ciconia ciconia\',\n                      128: \'black stork, Ciconia nigra\',\n                      129: \'spoonbill\',\n                      130: \'flamingo\',\n                      131: \'little blue heron, Egretta caerulea\',\n                      132: \'American egret, great white heron, Egretta albus\',\n                      133: \'bittern\',\n                      134: \'crane\',\n                      135: \'limpkin, Aramus pictus\',\n                      136: \'European gallinule, Porphyrio porphyrio\',\n                      137: \'American coot, marsh hen, mud hen, water hen, Fulica americana\',\n                      138: \'bustard\',\n                      139: \'ruddy turnstone, Arenaria interpres\',\n                      140: \'red-backed sandpiper, dunlin, Erolia alpina\',\n                      141: \'redshank, Tringa totanus\',\n                      142: \'dowitcher\',\n                      143: \'oystercatcher, oyster catcher\',\n                      144: \'pelican\',\n                      145: \'king penguin, Aptenodytes patagonica\',\n                      146: \'albatross, mollymawk\',\n                      147: \'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus\',\n                      148: \'killer whale, killer, orca, grampus, sea wolf, Orcinus orca\',\n                      149: \'dugong, Dugong dugon\',\n                      150: \'sea lion\',\n                      151: \'Chihuahua\',\n                      152: \'Japanese spaniel\',\n                      153: \'Maltese dog, Maltese terrier, Maltese\',\n                      154: \'Pekinese, Pekingese, Peke\',\n                      155: \'Shih-Tzu\',\n                      156: \'Blenheim spaniel\',\n                      157: \'papillon\',\n                      158: \'toy terrier\',\n                      159: \'Rhodesian ridgeback\',\n                      160: \'Afghan hound, Afghan\',\n                      161: \'basset, basset hound\',\n                      162: \'beagle\',\n                      163: \'bloodhound, sleuthhound\',\n                      164: \'bluetick\',\n                      165: \'black-and-tan coonhound\',\n                      166: \'Walker hound, Walker foxhound\',\n                      167: \'English foxhound\',\n                      168: \'redbone\',\n                      169: \'borzoi, Russian wolfhound\',\n                      170: \'Irish wolfhound\',\n                      171: \'Italian greyhound\',\n                      172: \'whippet\',\n                      173: \'Ibizan hound, Ibizan Podenco\',\n                      174: \'Norwegian elkhound, elkhound\',\n                      175: \'otterhound, otter hound\',\n                      176: \'Saluki, gazelle hound\',\n                      177: \'Scottish deerhound, deerhound\',\n                      178: \'Weimaraner\',\n                      179: \'Staffordshire bullterrier, Staffordshire bull terrier\',\n                      180: \'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier\',\n                      181: \'Bedlington terrier\',\n                      182: \'Border terrier\',\n                      183: \'Kerry blue terrier\',\n                      184: \'Irish terrier\',\n                      185: \'Norfolk terrier\',\n                      186: \'Norwich terrier\',\n                      187: \'Yorkshire terrier\',\n                      188: \'wire-haired fox terrier\',\n                      189: \'Lakeland terrier\',\n                      190: \'Sealyham terrier, Sealyham\',\n                      191: \'Airedale, Airedale terrier\',\n                      192: \'cairn, cairn terrier\',\n                      193: \'Australian terrier\',\n                      194: \'Dandie Dinmont, Dandie Dinmont terrier\',\n                      195: \'Boston bull, Boston terrier\',\n                      196: \'miniature schnauzer\',\n                      197: \'giant schnauzer\',\n                      198: \'standard schnauzer\',\n                      199: \'Scotch terrier, Scottish terrier, Scottie\',\n                      200: \'Tibetan terrier, chrysanthemum dog\',\n                      201: \'silky terrier, Sydney silky\',\n                      202: \'soft-coated wheaten terrier\',\n                      203: \'West Highland white terrier\',\n                      204: \'Lhasa, Lhasa apso\',\n                      205: \'flat-coated retriever\',\n                      206: \'curly-coated retriever\',\n                      207: \'golden retriever\',\n                      208: \'Labrador retriever\',\n                      209: \'Chesapeake Bay retriever\',\n                      210: \'German short-haired pointer\',\n                      211: \'vizsla, Hungarian pointer\',\n                      212: \'English setter\',\n                      213: \'Irish setter, red setter\',\n                      214: \'Gordon setter\',\n                      215: \'Brittany spaniel\',\n                      216: \'clumber, clumber spaniel\',\n                      217: \'English springer, English springer spaniel\',\n                      218: \'Welsh springer spaniel\',\n                      219: \'cocker spaniel, English cocker spaniel, cocker\',\n                      220: \'Sussex spaniel\',\n                      221: \'Irish water spaniel\',\n                      222: \'kuvasz\',\n                      223: \'schipperke\',\n                      224: \'groenendael\',\n                      225: \'malinois\',\n                      226: \'briard\',\n                      227: \'kelpie\',\n                      228: \'komondor\',\n                      229: \'Old English sheepdog, bobtail\',\n                      230: \'Shetland sheepdog, Shetland sheep dog, Shetland\',\n                      231: \'collie\',\n                      232: \'Border collie\',\n                      233: \'Bouvier des Flandres, Bouviers des Flandres\',\n                      234: \'Rottweiler\',\n                      235: \'German shepherd, German shepherd dog, German police dog, alsatian\',\n                      236: \'Doberman, Doberman pinscher\',\n                      237: \'miniature pinscher\',\n                      238: \'Greater Swiss Mountain dog\',\n                      239: \'Bernese mountain dog\',\n                      240: \'Appenzeller\',\n                      241: \'EntleBucher\',\n                      242: \'boxer\',\n                      243: \'bull mastiff\',\n                      244: \'Tibetan mastiff\',\n                      245: \'French bulldog\',\n                      246: \'Great Dane\',\n                      247: \'Saint Bernard, St Bernard\',\n                      248: \'Eskimo dog, husky\',\n                      249: \'malamute, malemute, Alaskan malamute\',\n                      250: \'Siberian husky\',\n                      251: \'dalmatian, coach dog, carriage dog\',\n                      252: \'affenpinscher, monkey pinscher, monkey dog\',\n                      253: \'basenji\',\n                      254: \'pug, pug-dog\',\n                      255: \'Leonberg\',\n                      256: \'Newfoundland, Newfoundland dog\',\n                      257: \'Great Pyrenees\',\n                      258: \'Samoyed, Samoyede\',\n                      259: \'Pomeranian\',\n                      260: \'chow, chow chow\',\n                      261: \'keeshond\',\n                      262: \'Brabancon griffon\',\n                      263: \'Pembroke, Pembroke Welsh corgi\',\n                      264: \'Cardigan, Cardigan Welsh corgi\',\n                      265: \'toy poodle\',\n                      266: \'miniature poodle\',\n                      267: \'standard poodle\',\n                      268: \'Mexican hairless\',\n                      269: \'timber wolf, grey wolf, gray wolf, Canis lupus\',\n                      270: \'white wolf, Arctic wolf, Canis lupus tundrarum\',\n                      271: \'red wolf, maned wolf, Canis rufus, Canis niger\',\n                      272: \'coyote, prairie wolf, brush wolf, Canis latrans\',\n                      273: \'dingo, warrigal, warragal, Canis dingo\',\n                      274: \'dhole, Cuon alpinus\',\n                      275: \'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\',\n                      276: \'hyena, hyaena\',\n                      277: \'red fox, Vulpes vulpes\',\n                      278: \'kit fox, Vulpes macrotis\',\n                      279: \'Arctic fox, white fox, Alopex lagopus\',\n                      280: \'grey fox, gray fox, Urocyon cinereoargenteus\',\n                      281: \'tabby, tabby cat\',\n                      282: \'tiger cat\',\n                      283: \'Persian cat\',\n                      284: \'Siamese cat, Siamese\',\n                      285: \'Egyptian cat\',\n                      286: \'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\',\n                      287: \'lynx, catamount\',\n                      288: \'leopard, Panthera pardus\',\n                      289: \'snow leopard, ounce, Panthera uncia\',\n                      290: \'jaguar, panther, Panthera onca, Felis onca\',\n                      291: \'lion, king of beasts, Panthera leo\',\n                      292: \'tiger, Panthera tigris\',\n                      293: \'cheetah, chetah, Acinonyx jubatus\',\n                      294: \'brown bear, bruin, Ursus arctos\',\n                      295: \'American black bear, black bear, Ursus americanus, Euarctos americanus\',\n                      296: \'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\',\n                      297: \'sloth bear, Melursus ursinus, Ursus ursinus\',\n                      298: \'mongoose\',\n                      299: \'meerkat, mierkat\',\n                      300: \'tiger beetle\',\n                      301: \'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle\',\n                      302: \'ground beetle, carabid beetle\',\n                      303: \'long-horned beetle, longicorn, longicorn beetle\',\n                      304: \'leaf beetle, chrysomelid\',\n                      305: \'dung beetle\',\n                      306: \'rhinoceros beetle\',\n                      307: \'weevil\',\n                      308: \'fly\',\n                      309: \'bee\',\n                      310: \'ant, emmet, pismire\',\n                      311: \'grasshopper, hopper\',\n                      312: \'cricket\',\n                      313: \'walking stick, walkingstick, stick insect\',\n                      314: \'cockroach, roach\',\n                      315: \'mantis, mantid\',\n                      316: \'cicada, cicala\',\n                      317: \'leafhopper\',\n                      318: \'lacewing, lacewing fly\',\n                      319: ""dragonfly, darning needle, devil\'s darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk"",\n                      320: \'damselfly\',\n                      321: \'admiral\',\n                      322: \'ringlet, ringlet butterfly\',\n                      323: \'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus\',\n                      324: \'cabbage butterfly\',\n                      325: \'sulphur butterfly, sulfur butterfly\',\n                      326: \'lycaenid, lycaenid butterfly\',\n                      327: \'starfish, sea star\',\n                      328: \'sea urchin\',\n                      329: \'sea cucumber, holothurian\',\n                      330: \'wood rabbit, cottontail, cottontail rabbit\',\n                      331: \'hare\',\n                      332: \'Angora, Angora rabbit\',\n                      333: \'hamster\',\n                      334: \'porcupine, hedgehog\',\n                      335: \'fox squirrel, eastern fox squirrel, Sciurus niger\',\n                      336: \'marmot\',\n                      337: \'beaver\',\n                      338: \'guinea pig, Cavia cobaya\',\n                      339: \'sorrel\',\n                      340: \'zebra\',\n                      341: \'hog, pig, grunter, squealer, Sus scrofa\',\n                      342: \'wild boar, boar, Sus scrofa\',\n                      343: \'warthog\',\n                      344: \'hippopotamus, hippo, river horse, Hippopotamus amphibius\',\n                      345: \'ox\',\n                      346: \'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis\',\n                      347: \'bison\',\n                      348: \'ram, tup\',\n                      349: \'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis\',\n                      350: \'ibex, Capra ibex\',\n                      351: \'hartebeest\',\n                      352: \'impala, Aepyceros melampus\',\n                      353: \'gazelle\',\n                      354: \'Arabian camel, dromedary, Camelus dromedarius\',\n                      355: \'llama\',\n                      356: \'weasel\',\n                      357: \'mink\',\n                      358: \'polecat, fitch, foulmart, foumart, Mustela putorius\',\n                      359: \'black-footed ferret, ferret, Mustela nigripes\',\n                      360: \'otter\',\n                      361: \'skunk, polecat, wood pussy\',\n                      362: \'badger\',\n                      363: \'armadillo\',\n                      364: \'three-toed sloth, ai, Bradypus tridactylus\',\n                      365: \'orangutan, orang, orangutang, Pongo pygmaeus\',\n                      366: \'gorilla, Gorilla gorilla\',\n                      367: \'chimpanzee, chimp, Pan troglodytes\',\n                      368: \'gibbon, Hylobates lar\',\n                      369: \'siamang, Hylobates syndactylus, Symphalangus syndactylus\',\n                      370: \'guenon, guenon monkey\',\n                      371: \'patas, hussar monkey, Erythrocebus patas\',\n                      372: \'baboon\',\n                      373: \'macaque\',\n                      374: \'langur\',\n                      375: \'colobus, colobus monkey\',\n                      376: \'proboscis monkey, Nasalis larvatus\',\n                      377: \'marmoset\',\n                      378: \'capuchin, ringtail, Cebus capucinus\',\n                      379: \'howler monkey, howler\',\n                      380: \'titi, titi monkey\',\n                      381: \'spider monkey, Ateles geoffroyi\',\n                      382: \'squirrel monkey, Saimiri sciureus\',\n                      383: \'Madagascar cat, ring-tailed lemur, Lemur catta\',\n                      384: \'indri, indris, Indri indri, Indri brevicaudatus\',\n                      385: \'Indian elephant, Elephas maximus\',\n                      386: \'African elephant, Loxodonta africana\',\n                      387: \'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens\',\n                      388: \'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca\',\n                      389: \'barracouta, snoek\',\n                      390: \'eel\',\n                      391: \'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch\',\n                      392: \'rock beauty, Holocanthus tricolor\',\n                      393: \'anemone fish\',\n                      394: \'sturgeon\',\n                      395: \'gar, garfish, garpike, billfish, Lepisosteus osseus\',\n                      396: \'lionfish\',\n                      397: \'puffer, pufferfish, blowfish, globefish\',\n                      398: \'abacus\',\n                      399: \'abaya\',\n                      400: ""academic gown, academic robe, judge\'s robe"",\n                      401: \'accordion, piano accordion, squeeze box\',\n                      402: \'acoustic guitar\',\n                      403: \'aircraft carrier, carrier, flattop, attack aircraft carrier\',\n                      404: \'airliner\',\n                      405: \'airship, dirigible\',\n                      406: \'altar\',\n                      407: \'ambulance\',\n                      408: \'amphibian, amphibious vehicle\',\n                      409: \'analog clock\',\n                      410: \'apiary, bee house\',\n                      411: \'apron\',\n                      412: \'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin\',\n                      413: \'assault rifle, assault gun\',\n                      414: \'backpack, back pack, knapsack, packsack, rucksack, haversack\',\n                      415: \'bakery, bakeshop, bakehouse\',\n                      416: \'balance beam, beam\',\n                      417: \'balloon\',\n                      418: \'ballpoint, ballpoint pen, ballpen, Biro\',\n                      419: \'Band Aid\',\n                      420: \'banjo\',\n                      421: \'bannister, banister, balustrade, balusters, handrail\',\n                      422: \'barbell\',\n                      423: \'barber chair\',\n                      424: \'barbershop\',\n                      425: \'barn\',\n                      426: \'barometer\',\n                      427: \'barrel, cask\',\n                      428: \'barrow, garden cart, lawn cart, wheelbarrow\',\n                      429: \'baseball\',\n                      430: \'basketball\',\n                      431: \'bassinet\',\n                      432: \'bassoon\',\n                      433: \'bathing cap, swimming cap\',\n                      434: \'bath towel\',\n                      435: \'bathtub, bathing tub, bath, tub\',\n                      436: \'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\',\n                      437: \'beacon, lighthouse, beacon light, pharos\',\n                      438: \'beaker\',\n                      439: \'bearskin, busby, shako\',\n                      440: \'beer bottle\',\n                      441: \'beer glass\',\n                      442: \'bell cote, bell cot\',\n                      443: \'bib\',\n                      444: \'bicycle-built-for-two, tandem bicycle, tandem\',\n                      445: \'bikini, two-piece\',\n                      446: \'binder, ring-binder\',\n                      447: \'binoculars, field glasses, opera glasses\',\n                      448: \'birdhouse\',\n                      449: \'boathouse\',\n                      450: \'bobsled, bobsleigh, bob\',\n                      451: \'bolo tie, bolo, bola tie, bola\',\n                      452: \'bonnet, poke bonnet\',\n                      453: \'bookcase\',\n                      454: \'bookshop, bookstore, bookstall\',\n                      455: \'bottlecap\',\n                      456: \'bow\',\n                      457: \'bow tie, bow-tie, bowtie\',\n                      458: \'brass, memorial tablet, plaque\',\n                      459: \'brassiere, bra, bandeau\',\n                      460: \'breakwater, groin, groyne, mole, bulwark, seawall, jetty\',\n                      461: \'breastplate, aegis, egis\',\n                      462: \'broom\',\n                      463: \'bucket, pail\',\n                      464: \'buckle\',\n                      465: \'bulletproof vest\',\n                      466: \'bullet train, bullet\',\n                      467: \'butcher shop, meat market\',\n                      468: \'cab, hack, taxi, taxicab\',\n                      469: \'caldron, cauldron\',\n                      470: \'candle, taper, wax light\',\n                      471: \'cannon\',\n                      472: \'canoe\',\n                      473: \'can opener, tin opener\',\n                      474: \'cardigan\',\n                      475: \'car mirror\',\n                      476: \'carousel, carrousel, merry-go-round, roundabout, whirligig\',\n                      477: ""carpenter\'s kit, tool kit"",\n                      478: \'carton\',\n                      479: \'car wheel\',\n                      480: \'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM\',\n                      481: \'cassette\',\n                      482: \'cassette player\',\n                      483: \'castle\',\n                      484: \'catamaran\',\n                      485: \'CD player\',\n                      486: \'cello, violoncello\',\n                      487: \'cellular telephone, cellular phone, cellphone, cell, mobile phone\',\n                      488: \'chain\',\n                      489: \'chainlink fence\',\n                      490: \'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour\',\n                      491: \'chain saw, chainsaw\',\n                      492: \'chest\',\n                      493: \'chiffonier, commode\',\n                      494: \'chime, bell, gong\',\n                      495: \'china cabinet, china closet\',\n                      496: \'Christmas stocking\',\n                      497: \'church, church building\',\n                      498: \'cinema, movie theater, movie theatre, movie house, picture palace\',\n                      499: \'cleaver, meat cleaver, chopper\',\n                      500: \'cliff dwelling\',\n                      501: \'cloak\',\n                      502: \'clog, geta, patten, sabot\',\n                      503: \'cocktail shaker\',\n                      504: \'coffee mug\',\n                      505: \'coffeepot\',\n                      506: \'coil, spiral, volute, whorl, helix\',\n                      507: \'combination lock\',\n                      508: \'computer keyboard, keypad\',\n                      509: \'confectionery, confectionary, candy store\',\n                      510: \'container ship, containership, container vessel\',\n                      511: \'convertible\',\n                      512: \'corkscrew, bottle screw\',\n                      513: \'cornet, horn, trumpet, trump\',\n                      514: \'cowboy boot\',\n                      515: \'cowboy hat, ten-gallon hat\',\n                      516: \'cradle\',\n                      517: \'crane\',\n                      518: \'crash helmet\',\n                      519: \'crate\',\n                      520: \'crib, cot\',\n                      521: \'Crock Pot\',\n                      522: \'croquet ball\',\n                      523: \'crutch\',\n                      524: \'cuirass\',\n                      525: \'dam, dike, dyke\',\n                      526: \'desk\',\n                      527: \'desktop computer\',\n                      528: \'dial telephone, dial phone\',\n                      529: \'diaper, nappy, napkin\',\n                      530: \'digital clock\',\n                      531: \'digital watch\',\n                      532: \'dining table, board\',\n                      533: \'dishrag, dishcloth\',\n                      534: \'dishwasher, dish washer, dishwashing machine\',\n                      535: \'disk brake, disc brake\',\n                      536: \'dock, dockage, docking facility\',\n                      537: \'dogsled, dog sled, dog sleigh\',\n                      538: \'dome\',\n                      539: \'doormat, welcome mat\',\n                      540: \'drilling platform, offshore rig\',\n                      541: \'drum, membranophone, tympan\',\n                      542: \'drumstick\',\n                      543: \'dumbbell\',\n                      544: \'Dutch oven\',\n                      545: \'electric fan, blower\',\n                      546: \'electric guitar\',\n                      547: \'electric locomotive\',\n                      548: \'entertainment center\',\n                      549: \'envelope\',\n                      550: \'espresso maker\',\n                      551: \'face powder\',\n                      552: \'feather boa, boa\',\n                      553: \'file, file cabinet, filing cabinet\',\n                      554: \'fireboat\',\n                      555: \'fire engine, fire truck\',\n                      556: \'fire screen, fireguard\',\n                      557: \'flagpole, flagstaff\',\n                      558: \'flute, transverse flute\',\n                      559: \'folding chair\',\n                      560: \'football helmet\',\n                      561: \'forklift\',\n                      562: \'fountain\',\n                      563: \'fountain pen\',\n                      564: \'four-poster\',\n                      565: \'freight car\',\n                      566: \'French horn, horn\',\n                      567: \'frying pan, frypan, skillet\',\n                      568: \'fur coat\',\n                      569: \'garbage truck, dustcart\',\n                      570: \'gasmask, respirator, gas helmet\',\n                      571: \'gas pump, gasoline pump, petrol pump, island dispenser\',\n                      572: \'goblet\',\n                      573: \'go-kart\',\n                      574: \'golf ball\',\n                      575: \'golfcart, golf cart\',\n                      576: \'gondola\',\n                      577: \'gong, tam-tam\',\n                      578: \'gown\',\n                      579: \'grand piano, grand\',\n                      580: \'greenhouse, nursery, glasshouse\',\n                      581: \'grille, radiator grille\',\n                      582: \'grocery store, grocery, food market, market\',\n                      583: \'guillotine\',\n                      584: \'hair slide\',\n                      585: \'hair spray\',\n                      586: \'half track\',\n                      587: \'hammer\',\n                      588: \'hamper\',\n                      589: \'hand blower, blow dryer, blow drier, hair dryer, hair drier\',\n                      590: \'hand-held computer, hand-held microcomputer\',\n                      591: \'handkerchief, hankie, hanky, hankey\',\n                      592: \'hard disc, hard disk, fixed disk\',\n                      593: \'harmonica, mouth organ, harp, mouth harp\',\n                      594: \'harp\',\n                      595: \'harvester, reaper\',\n                      596: \'hatchet\',\n                      597: \'holster\',\n                      598: \'home theater, home theatre\',\n                      599: \'honeycomb\',\n                      600: \'hook, claw\',\n                      601: \'hoopskirt, crinoline\',\n                      602: \'horizontal bar, high bar\',\n                      603: \'horse cart, horse-cart\',\n                      604: \'hourglass\',\n                      605: \'iPod\',\n                      606: \'iron, smoothing iron\',\n                      607: ""jack-o\'-lantern"",\n                      608: \'jean, blue jean, denim\',\n                      609: \'jeep, landrover\',\n                      610: \'jersey, T-shirt, tee shirt\',\n                      611: \'jigsaw puzzle\',\n                      612: \'jinrikisha, ricksha, rickshaw\',\n                      613: \'joystick\',\n                      614: \'kimono\',\n                      615: \'knee pad\',\n                      616: \'knot\',\n                      617: \'lab coat, laboratory coat\',\n                      618: \'ladle\',\n                      619: \'lampshade, lamp shade\',\n                      620: \'laptop, laptop computer\',\n                      621: \'lawn mower, mower\',\n                      622: \'lens cap, lens cover\',\n                      623: \'letter opener, paper knife, paperknife\',\n                      624: \'library\',\n                      625: \'lifeboat\',\n                      626: \'lighter, light, igniter, ignitor\',\n                      627: \'limousine, limo\',\n                      628: \'liner, ocean liner\',\n                      629: \'lipstick, lip rouge\',\n                      630: \'Loafer\',\n                      631: \'lotion\',\n                      632: \'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system\',\n                      633: ""loupe, jeweler\'s loupe"",\n                      634: \'lumbermill, sawmill\',\n                      635: \'magnetic compass\',\n                      636: \'mailbag, postbag\',\n                      637: \'mailbox, letter box\',\n                      638: \'maillot\',\n                      639: \'maillot, tank suit\',\n                      640: \'manhole cover\',\n                      641: \'maraca\',\n                      642: \'marimba, xylophone\',\n                      643: \'mask\',\n                      644: \'matchstick\',\n                      645: \'maypole\',\n                      646: \'maze, labyrinth\',\n                      647: \'measuring cup\',\n                      648: \'medicine chest, medicine cabinet\',\n                      649: \'megalith, megalithic structure\',\n                      650: \'microphone, mike\',\n                      651: \'microwave, microwave oven\',\n                      652: \'military uniform\',\n                      653: \'milk can\',\n                      654: \'minibus\',\n                      655: \'miniskirt, mini\',\n                      656: \'minivan\',\n                      657: \'missile\',\n                      658: \'mitten\',\n                      659: \'mixing bowl\',\n                      660: \'mobile home, manufactured home\',\n                      661: \'Model T\',\n                      662: \'modem\',\n                      663: \'monastery\',\n                      664: \'monitor\',\n                      665: \'moped\',\n                      666: \'mortar\',\n                      667: \'mortarboard\',\n                      668: \'mosque\',\n                      669: \'mosquito net\',\n                      670: \'motor scooter, scooter\',\n                      671: \'mountain bike, all-terrain bike, off-roader\',\n                      672: \'mountain tent\',\n                      673: \'mouse, computer mouse\',\n                      674: \'mousetrap\',\n                      675: \'moving van\',\n                      676: \'muzzle\',\n                      677: \'nail\',\n                      678: \'neck brace\',\n                      679: \'necklace\',\n                      680: \'nipple\',\n                      681: \'notebook, notebook computer\',\n                      682: \'obelisk\',\n                      683: \'oboe, hautboy, hautbois\',\n                      684: \'ocarina, sweet potato\',\n                      685: \'odometer, hodometer, mileometer, milometer\',\n                      686: \'oil filter\',\n                      687: \'organ, pipe organ\',\n                      688: \'oscilloscope, scope, cathode-ray oscilloscope, CRO\',\n                      689: \'overskirt\',\n                      690: \'oxcart\',\n                      691: \'oxygen mask\',\n                      692: \'packet\',\n                      693: \'paddle, boat paddle\',\n                      694: \'paddlewheel, paddle wheel\',\n                      695: \'padlock\',\n                      696: \'paintbrush\',\n                      697: ""pajama, pyjama, pj\'s, jammies"",\n                      698: \'palace\',\n                      699: \'panpipe, pandean pipe, syrinx\',\n                      700: \'paper towel\',\n                      701: \'parachute, chute\',\n                      702: \'parallel bars, bars\',\n                      703: \'park bench\',\n                      704: \'parking meter\',\n                      705: \'passenger car, coach, carriage\',\n                      706: \'patio, terrace\',\n                      707: \'pay-phone, pay-station\',\n                      708: \'pedestal, plinth, footstall\',\n                      709: \'pencil box, pencil case\',\n                      710: \'pencil sharpener\',\n                      711: \'perfume, essence\',\n                      712: \'Petri dish\',\n                      713: \'photocopier\',\n                      714: \'pick, plectrum, plectron\',\n                      715: \'pickelhaube\',\n                      716: \'picket fence, paling\',\n                      717: \'pickup, pickup truck\',\n                      718: \'pier\',\n                      719: \'piggy bank, penny bank\',\n                      720: \'pill bottle\',\n                      721: \'pillow\',\n                      722: \'ping-pong ball\',\n                      723: \'pinwheel\',\n                      724: \'pirate, pirate ship\',\n                      725: \'pitcher, ewer\',\n                      726: ""plane, carpenter\'s plane, woodworking plane"",\n                      727: \'planetarium\',\n                      728: \'plastic bag\',\n                      729: \'plate rack\',\n                      730: \'plow, plough\',\n                      731: ""plunger, plumber\'s helper"",\n                      732: \'Polaroid camera, Polaroid Land camera\',\n                      733: \'pole\',\n                      734: \'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria\',\n                      735: \'poncho\',\n                      736: \'pool table, billiard table, snooker table\',\n                      737: \'pop bottle, soda bottle\',\n                      738: \'pot, flowerpot\',\n                      739: ""potter\'s wheel"",\n                      740: \'power drill\',\n                      741: \'prayer rug, prayer mat\',\n                      742: \'printer\',\n                      743: \'prison, prison house\',\n                      744: \'projectile, missile\',\n                      745: \'projector\',\n                      746: \'puck, hockey puck\',\n                      747: \'punching bag, punch bag, punching ball, punchball\',\n                      748: \'purse\',\n                      749: \'quill, quill pen\',\n                      750: \'quilt, comforter, comfort, puff\',\n                      751: \'racer, race car, racing car\',\n                      752: \'racket, racquet\',\n                      753: \'radiator\',\n                      754: \'radio, wireless\',\n                      755: \'radio telescope, radio reflector\',\n                      756: \'rain barrel\',\n                      757: \'recreational vehicle, RV, R.V.\',\n                      758: \'reel\',\n                      759: \'reflex camera\',\n                      760: \'refrigerator, icebox\',\n                      761: \'remote control, remote\',\n                      762: \'restaurant, eating house, eating place, eatery\',\n                      763: \'revolver, six-gun, six-shooter\',\n                      764: \'rifle\',\n                      765: \'rocking chair, rocker\',\n                      766: \'rotisserie\',\n                      767: \'rubber eraser, rubber, pencil eraser\',\n                      768: \'rugby ball\',\n                      769: \'rule, ruler\',\n                      770: \'running shoe\',\n                      771: \'safe\',\n                      772: \'safety pin\',\n                      773: \'saltshaker, salt shaker\',\n                      774: \'sandal\',\n                      775: \'sarong\',\n                      776: \'sax, saxophone\',\n                      777: \'scabbard\',\n                      778: \'scale, weighing machine\',\n                      779: \'school bus\',\n                      780: \'schooner\',\n                      781: \'scoreboard\',\n                      782: \'screen, CRT screen\',\n                      783: \'screw\',\n                      784: \'screwdriver\',\n                      785: \'seat belt, seatbelt\',\n                      786: \'sewing machine\',\n                      787: \'shield, buckler\',\n                      788: \'shoe shop, shoe-shop, shoe store\',\n                      789: \'shoji\',\n                      790: \'shopping basket\',\n                      791: \'shopping cart\',\n                      792: \'shovel\',\n                      793: \'shower cap\',\n                      794: \'shower curtain\',\n                      795: \'ski\',\n                      796: \'ski mask\',\n                      797: \'sleeping bag\',\n                      798: \'slide rule, slipstick\',\n                      799: \'sliding door\',\n                      800: \'slot, one-armed bandit\',\n                      801: \'snorkel\',\n                      802: \'snowmobile\',\n                      803: \'snowplow, snowplough\',\n                      804: \'soap dispenser\',\n                      805: \'soccer ball\',\n                      806: \'sock\',\n                      807: \'solar dish, solar collector, solar furnace\',\n                      808: \'sombrero\',\n                      809: \'soup bowl\',\n                      810: \'space bar\',\n                      811: \'space heater\',\n                      812: \'space shuttle\',\n                      813: \'spatula\',\n                      814: \'speedboat\',\n                      815: ""spider web, spider\'s web"",\n                      816: \'spindle\',\n                      817: \'sports car, sport car\',\n                      818: \'spotlight, spot\',\n                      819: \'stage\',\n                      820: \'steam locomotive\',\n                      821: \'steel arch bridge\',\n                      822: \'steel drum\',\n                      823: \'stethoscope\',\n                      824: \'stole\',\n                      825: \'stone wall\',\n                      826: \'stopwatch, stop watch\',\n                      827: \'stove\',\n                      828: \'strainer\',\n                      829: \'streetcar, tram, tramcar, trolley, trolley car\',\n                      830: \'stretcher\',\n                      831: \'studio couch, day bed\',\n                      832: \'stupa, tope\',\n                      833: \'submarine, pigboat, sub, U-boat\',\n                      834: \'suit, suit of clothes\',\n                      835: \'sundial\',\n                      836: \'sunglass\',\n                      837: \'sunglasses, dark glasses, shades\',\n                      838: \'sunscreen, sunblock, sun blocker\',\n                      839: \'suspension bridge\',\n                      840: \'swab, swob, mop\',\n                      841: \'sweatshirt\',\n                      842: \'swimming trunks, bathing trunks\',\n                      843: \'swing\',\n                      844: \'switch, electric switch, electrical switch\',\n                      845: \'syringe\',\n                      846: \'table lamp\',\n                      847: \'tank, army tank, armored combat vehicle, armoured combat vehicle\',\n                      848: \'tape player\',\n                      849: \'teapot\',\n                      850: \'teddy, teddy bear\',\n                      851: \'television, television system\',\n                      852: \'tennis ball\',\n                      853: \'thatch, thatched roof\',\n                      854: \'theater curtain, theatre curtain\',\n                      855: \'thimble\',\n                      856: \'thresher, thrasher, threshing machine\',\n                      857: \'throne\',\n                      858: \'tile roof\',\n                      859: \'toaster\',\n                      860: \'tobacco shop, tobacconist shop, tobacconist\',\n                      861: \'toilet seat\',\n                      862: \'torch\',\n                      863: \'totem pole\',\n                      864: \'tow truck, tow car, wrecker\',\n                      865: \'toyshop\',\n                      866: \'tractor\',\n                      867: \'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi\',\n                      868: \'tray\',\n                      869: \'trench coat\',\n                      870: \'tricycle, trike, velocipede\',\n                      871: \'trimaran\',\n                      872: \'tripod\',\n                      873: \'triumphal arch\',\n                      874: \'trolleybus, trolley coach, trackless trolley\',\n                      875: \'trombone\',\n                      876: \'tub, vat\',\n                      877: \'turnstile\',\n                      878: \'typewriter keyboard\',\n                      879: \'umbrella\',\n                      880: \'unicycle, monocycle\',\n                      881: \'upright, upright piano\',\n                      882: \'vacuum, vacuum cleaner\',\n                      883: \'vase\',\n                      884: \'vault\',\n                      885: \'velvet\',\n                      886: \'vending machine\',\n                      887: \'vestment\',\n                      888: \'viaduct\',\n                      889: \'violin, fiddle\',\n                      890: \'volleyball\',\n                      891: \'waffle iron\',\n                      892: \'wall clock\',\n                      893: \'wallet, billfold, notecase, pocketbook\',\n                      894: \'wardrobe, closet, press\',\n                      895: \'warplane, military plane\',\n                      896: \'washbasin, handbasin, washbowl, lavabo, wash-hand basin\',\n                      897: \'washer, automatic washer, washing machine\',\n                      898: \'water bottle\',\n                      899: \'water jug\',\n                      900: \'water tower\',\n                      901: \'whiskey jug\',\n                      902: \'whistle\',\n                      903: \'wig\',\n                      904: \'window screen\',\n                      905: \'window shade\',\n                      906: \'Windsor tie\',\n                      907: \'wine bottle\',\n                      908: \'wing\',\n                      909: \'wok\',\n                      910: \'wooden spoon\',\n                      911: \'wool, woolen, woollen\',\n                      912: \'worm fence, snake fence, snake-rail fence, Virginia fence\',\n                      913: \'wreck\',\n                      914: \'yawl\',\n                      915: \'yurt\',\n                      916: \'web site, website, internet site, site\',\n                      917: \'comic book\',\n                      918: \'crossword puzzle, crossword\',\n                      919: \'street sign\',\n                      920: \'traffic light, traffic signal, stoplight\',\n                      921: \'book jacket, dust cover, dust jacket, dust wrapper\',\n                      922: \'menu\',\n                      923: \'plate\',\n                      924: \'guacamole\',\n                      925: \'consomme\',\n                      926: \'hot pot, hotpot\',\n                      927: \'trifle\',\n                      928: \'ice cream, icecream\',\n                      929: \'ice lolly, lolly, lollipop, popsicle\',\n                      930: \'French loaf\',\n                      931: \'bagel, beigel\',\n                      932: \'pretzel\',\n                      933: \'cheeseburger\',\n                      934: \'hotdog, hot dog, red hot\',\n                      935: \'mashed potato\',\n                      936: \'head cabbage\',\n                      937: \'broccoli\',\n                      938: \'cauliflower\',\n                      939: \'zucchini, courgette\',\n                      940: \'spaghetti squash\',\n                      941: \'acorn squash\',\n                      942: \'butternut squash\',\n                      943: \'cucumber, cuke\',\n                      944: \'artichoke, globe artichoke\',\n                      945: \'bell pepper\',\n                      946: \'cardoon\',\n                      947: \'mushroom\',\n                      948: \'Granny Smith\',\n                      949: \'strawberry\',\n                      950: \'orange\',\n                      951: \'lemon\',\n                      952: \'fig\',\n                      953: \'pineapple, ananas\',\n                      954: \'banana\',\n                      955: \'jackfruit, jak, jack\',\n                      956: \'custard apple\',\n                      957: \'pomegranate\',\n                      958: \'hay\',\n                      959: \'carbonara\',\n                      960: \'chocolate sauce, chocolate syrup\',\n                      961: \'dough\',\n                      962: \'meat loaf, meatloaf\',\n                      963: \'pizza, pizza pie\',\n                      964: \'potpie\',\n                      965: \'burrito\',\n                      966: \'red wine\',\n                      967: \'espresso\',\n                      968: \'cup\',\n                      969: \'eggnog\',\n                      970: \'alp\',\n                      971: \'bubble\',\n                      972: \'cliff, drop, drop-off\',\n                      973: \'coral reef\',\n                      974: \'geyser\',\n                      975: \'lakeside, lakeshore\',\n                      976: \'promontory, headland, head, foreland\',\n                      977: \'sandbar, sand bar\',\n                      978: \'seashore, coast, seacoast, sea-coast\',\n                      979: \'valley, vale\',\n                      980: \'volcano\',\n                      981: \'ballplayer, baseball player\',\n                      982: \'groom, bridegroom\',\n                      983: \'scuba diver\',\n                      984: \'rapeseed\',\n                      985: \'daisy\',\n                      986: ""yellow lady\'s slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum"",\n                      987: \'corn\',\n                      988: \'acorn\',\n                      989: \'hip, rose hip, rosehip\',\n                      990: \'buckeye, horse chestnut, conker\',\n                      991: \'coral fungus\',\n                      992: \'agaric\',\n                      993: \'gyromitra\',\n                      994: \'stinkhorn, carrion fungus\',\n                      995: \'earthstar\',\n                      996: \'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa\',\n                      997: \'bolete\',\n                      998: \'ear, spike, capitulum\',\n                      999: \'toilet tissue, toilet paper, bathroom tissue\'}\n'"
alfred/vis/image/mask.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nDraw masks on image,\nevery mask will has single id, and color are not same\nalso this will give options to draw detection or not\n\n\n""""""\nimport cv2\nimport numpy as np\nfrom .common import get_unique_color_by_id\nfrom .det import draw_one_bbox\nfrom PIL import Image\n\n\ndef draw_masks_maskrcnn(image, boxes, scores, labels, masks, human_label_list=None,\n                        score_thresh=0.6, draw_box=True):\n    """"""\n    Standared mask drawing function\n\n    boxes: a list of boxes, or numpy array\n    scores: a list of scores or numpy array\n    labels: same as scores\n    masks: resize to same width and height as box masks\n\n    NOTE: if masks not same with box, then it will resize inside this function\n\n    this function has speed issue be kind of slow.\n    and it overlays a box on another mask rather than mask by mask, this is not best\n    way.\n\n    :param image:\n    :param boxes:\n    :param scores:\n    :param labels:\n    :param masks:\n    :param human_label_list\n    :param score_thresh\n    :param draw_box:\n    :return:\n    """"""\n    n_instances = 0\n    if isinstance(boxes, list):\n        n_instances = len(boxes)\n    else:\n        n_instances = boxes.shape[0]\n    # black image with same size as original image\n    empty_image = np.zeros(image.shape, dtype=np.uint8)\n    for i in range(n_instances):\n        box = boxes[i]\n        score = scores[i]\n        label = labels[i]\n        mask = masks[i]\n\n        cls_color = get_unique_color_by_id(label)\n        # only get RGB\n        instance_color = get_unique_color_by_id(i)[:-1]\n        # now adding masks to image, and colorize it\n        if score >= score_thresh:\n\n            x1 = int(box[0])\n            y1 = int(box[1])\n            x2 = int(box[2])\n            y2 = int(box[3])\n\n            if draw_box:\n                image = draw_one_bbox(image, box, cls_color, 1)\n                if human_label_list:\n                    # draw text on image\n                    font = cv2.QT_FONT_NORMAL\n                    font_scale = 0.4\n                    font_thickness = 1\n                    line_thickness = 1\n\n                    txt = \'{} {:.2f}\'.format(human_label_list[label], score)\n                    cv2.putText(image, txt, (x1, y1), font, font_scale, cls_color, font_thickness)\n\n            # colorize mask\n            m_w = int(x2-x1)\n            m_h = int(y2-y1)\n            mask = Image.fromarray(mask).resize((m_w, m_h), Image.LINEAR)\n            mask = np.array(mask)\n            # cv2.imshow(\'rr2\', mask)\n            # cv2.waitKey(0)\n\n            mask_flatten = mask.flatten()\n            # if pixel value less than 0.5, that\'s background, min: 0.0009, max: 0.9\n            mask_flatten_color = np.array(list(map(lambda it: instance_color if it > 0.5 else [0, 0, 0],\n                                                   mask_flatten)), dtype=np.uint8)\n\n            mask_color = np.resize(mask_flatten_color, (m_h, m_w, 3))\n            empty_image[y1: y2, x1: x2, :] = mask_color\n    # combine image and masks\n    # now we got mask\n    combined = cv2.addWeighted(image, 0.5, empty_image, 0.6, 0)\n    return combined\n\n\ndef draw_masks_maskrcnn_v2(image, boxes, scores, labels, masks, human_label_list=None,\n                        score_thresh=0.6, draw_box=True):\n    """"""\n    We change way to draw masks on image\n\n    :param image:\n    :param boxes:\n    :param scores:\n    :param labels:\n    :param masks:\n    :param human_label_list\n    :param score_thresh\n    :param draw_box:\n    :return:\n    """"""\n    n_instances = 0\n    if isinstance(boxes, list):\n        n_instances = len(boxes)\n    else:\n        n_instances = boxes.shape[0]\n    # black image with same size as original image\n    empty_image = np.zeros(image.shape, dtype=np.uint8)\n    for i in range(n_instances):\n        box = boxes[i]\n        score = scores[i]\n        label = labels[i]\n        mask = masks[i]\n\n        cls_color = get_unique_color_by_id(label)\n        # only get RGB\n        instance_color = get_unique_color_by_id(i)[:-1]\n        # now adding masks to image, and colorize it\n        if score >= score_thresh:\n\n            x1 = int(box[0])\n            y1 = int(box[1])\n            x2 = int(box[2])\n            y2 = int(box[3])\n\n            if draw_box:\n                image = draw_one_bbox(image, box, cls_color, 1)\n                if human_label_list:\n                    # draw text on image\n                    font = cv2.QT_FONT_NORMAL\n                    font_scale = 0.4\n                    font_thickness = 1\n                    line_thickness = 1\n\n                    txt = \'{} {:.2f}\'.format(human_label_list[label], score)\n                    cv2.putText(image, txt, (x1, y1), font, font_scale, cls_color, font_thickness)\n\n            # colorize mask\n            m_w = int(x2-x1)\n            m_h = int(y2-y1)\n            mask = Image.fromarray(mask).resize((m_w, m_h), Image.LINEAR)\n            mask = np.array(mask)\n            mask_flatten = mask.flatten()\n            # if pixel value less than 0.5, that\'s background, min: 0.0009, max: 0.9\n            mask_flatten_color = np.array(list(map(lambda it: instance_color if it > 0.5 else [0, 0, 0],\n                                                   mask_flatten)), dtype=np.uint8)\n\n            mask_color = np.resize(mask_flatten_color, (m_h, m_w, 3))\n            empty_image[y1: y2, x1: x2, :] = mask_color\n    # combine image and masks\n    # now we got mask\n    combined = cv2.addWeighted(image, 0.5, empty_image, 0.6, 0)\n    return combined\n\n\n# more fast mask drawing here'"
alfred/vis/image/process.py,0,"b'""""""\n\nProcess on image\n\n""""""\n\n\nimport cv2\n\n\ndef darken_image(ori_img, dark_factor=0.6):\n    """"""\n    this will darken origin image and return darken one\n    """"""\n    hsv_img = cv2.cvtColor(ori_img, cv2.COLOR_BGR2HSV)\n    hsv_img[...,2] = hsv_img[...,2]*dark_factor\n    originimg = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2BGR)\n    return originimg'"
alfred/vis/image/seg.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\ndraw segmentation result\n\neven instance segmentation result\n\n""""""\nimport numpy as np\nimport cv2\nfrom .get_dataset_colormap import label_to_color_image\nfrom .get_dataset_colormap import _ADE20K, _CITYSCAPES, _MAPILLARY_VISTAS, _PASCAL\n\n\ndef draw_seg_by_dataset(img, seg, dataset, alpha=0.7, is_show=False, bgr_in=False):\n    assert dataset in [_PASCAL, _CITYSCAPES, _MAPILLARY_VISTAS, _ADE20K], \'dataset not support yet.\'\n    img = np.asarray(img, dtype=np.uint8)\n    if bgr_in:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    mask_color = np.asarray(label_to_color_image(seg, dataset), dtype=np.uint8)\n    img_shape = img.shape\n    mask_shape = mask_color.shape\n    if img_shape != mask_shape:\n        # resize mask to img shape\n        mask_color = cv2.resize(mask_color, (img.shape[1], img.shape[0]))\n\n    res = cv2.addWeighted(img, 0.3, mask_color, alpha, 0.6)\n    if is_show:\n        cv2.imshow(\'result\', res)\n        cv2.waitKey(0)\n    return res, mask_color\n'"
alfred/vis/pointcloud/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/vis/pointcloud/pointcloud_vis.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""\nshowing 3d point cloud using open3d\n""""""\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ntry:\n    from open3d import *\n    import open3d as o3d\nexcept ImportError:\n    print(\'importing 3d_vis in alfred-py need open3d installed.\')\n    exit(0)\n\n\ndef draw_pcs_open3d(geometries):\n    """"""\n    drawing the points using open3d\n    it can draw points and linesets\n    ```\n    point_cloud = PointCloud()\n    point_cloud.points = Vector3dVector(pcs)\n\n\n    points = [[0,0,0],[1,0,0],[0,1,0],[1,1,0],\n                [0,0,1],[1,0,1],[0,1,1],[1,1,1]]\n    lines = [[0,1],[0,2],[1,3],[2,3],\n                [4,5],[4,6],[5,7],[6,7],\n                [0,4],[1,5],[2,6],[3,7]]\n    colors = [[1, 0, 0] for i in range(len(lines))]\n    line_set = LineSet()\n    line_set.points = Vector3dVector(points)\n    line_set.lines = Vector2iVector(lines)\n    line_set.colors = Vector3dVector(colors)\n    draw_pcs_open3d([point_cloud, line_set])\n    ```\n    """"""\n    def capture_depth(vis):\n        depth = vis.capture_depth_float_buffer()\n        plt.imshow(np.asarray(depth))\n        plt.show()\n        return False\n\n    def capture_image(vis):\n        image = vis.capture_screen_float_buffer()\n        plt.imshow(np.asarray(image))\n        plt.show()\n        return False\n    vis = o3d.visualization.Visualizer()\n    vis.create_window()\n    for g in geometries:\n        vis.add_geometry(g)\n    opt = vis.get_render_option()\n    opt.background_color = np.asarray([0, 0, 0])\n    opt.point_size = 1\n    # opt.show_coordinate_frame = True\n    vis.run()\n    vis.destroy_window()\n'"
alfred/dl/data/common/__init__.py,0,b''
alfred/dl/data/common/coco_dataset.py,0,"b""from collections import defaultdict\nimport json\nimport numpy as np\nimport os\nimport PIL.Image\nimport PIL.ImageDraw\n\nfrom ..meta.getter_dataset import GetterDataset\nimport cv2\n\ntry:\n    from pycocotools import mask as coco_mask\nexcept ImportError:\n    pass\n\n\nclass COCOInstancesBaseDataset(GetterDataset):\n\n    def __init__(self, data_dir='auto', split='train', year='2017',\n                 use_crowded=False, read_img_with='pil'):\n        if year == '2017' and split in ['minival', 'valminusminival']:\n            raise ValueError(\n                'coco2017 dataset does not support given split: {}'\n                .format(split))\n\n        super(COCOInstancesBaseDataset, self).__init__()\n        self.use_crowded = use_crowded\n        self.read_img_with = read_img_with\n\n        if split in ['val', 'minival', 'valminusminival']:\n            img_split = 'val'\n        else:\n            img_split = 'train'\n\n        self.img_root = os.path.join(\n            data_dir, '{}{}'.format(img_split, year))\n        tmp = self.img_root\n        if not os.path.exists(self.img_root):\n            self.img_root = os.path.join(data_dir, 'images')\n        assert os.path.exists(\n            self.img_root), 'we have tried img_root: {} and {}, either found, did u make sure it exists?'.format(tmp, self.img_root)\n        anno_path = os.path.join(\n            data_dir, 'annotations', 'instances_{}{}.json'.format(split, year))\n\n        self.data_dir = data_dir\n        annos = json.load(open(anno_path, 'r'))\n\n        self.id_to_prop = {}\n        for prop in annos['images']:\n            self.id_to_prop[prop['id']] = prop\n        self.ids = sorted(list(self.id_to_prop.keys()))\n\n        self.cat_ids = [cat['id'] for cat in annos['categories']]\n\n        self.id_to_anno = defaultdict(list)\n        for anno in annos['annotations']:\n            self.id_to_anno[anno['image_id']].append(anno)\n\n        self.add_getter('img', self._get_image)\n        self.add_getter('mask', self._get_mask)\n        self.add_getter(\n            ['bbox', 'label', 'area', 'crowded'],\n            self._get_annotations)\n\n    def __len__(self):\n        return len(self.ids)\n\n    def _get_image(self, i):\n        img_path = os.path.join(\n            self.img_root, self.id_to_prop[self.ids[i]]['file_name'])\n        if self.read_img_with == 'cv2':\n            img = cv2.imread(img_path)\n        else:\n            img = PIL.Image.open(img_path)\n        return img\n\n    def _get_mask(self, i):\n        # List[{'segmentation', 'area', 'iscrowd',\n        #       'image_id', 'bbox', 'category_id', 'id'}]\n        annotation = self.id_to_anno[self.ids[i]]\n        H = self.id_to_prop[self.ids[i]]['height']\n        W = self.id_to_prop[self.ids[i]]['width']\n\n        mask = []\n        crowded = []\n        for anno in annotation:\n            msk = self._segm_to_mask(anno['segmentation'], (H, W))\n            # FIXME: some of minival annotations are malformed.\n            if msk.shape != (H, W):\n                continue\n            mask.append(msk)\n            crowded.append(anno['iscrowd'])\n        mask = np.array(mask, dtype=np.bool)\n        crowded = np.array(crowded, dtype=np.bool)\n        if len(mask) == 0:\n            mask = np.zeros((0, H, W), dtype=np.bool)\n\n        if not self.use_crowded:\n            not_crowded = np.logical_not(crowded)\n            mask = mask[not_crowded]\n        return mask\n\n    def _get_annotations(self, i):\n        # List[{'segmentation', 'area', 'iscrowd',\n        #       'image_id', 'bbox', 'category_id', 'id'}]\n        annotation = self.id_to_anno[self.ids[i]]\n        bbox = np.array([ann['bbox'] for ann in annotation],\n                        dtype=np.float32)\n        if len(bbox) == 0:\n            bbox = np.zeros((0, 4), dtype=np.float32)\n        # (x, y, width, height)  -> (x_min, y_min, x_max, y_max)\n        bbox[:, 2] = bbox[:, 0] + bbox[:, 2]\n        bbox[:, 3] = bbox[:, 1] + bbox[:, 3]\n        # (x_min, y_min, x_max, y_max) -> (y_min, x_min, y_max, x_max)\n        bbox = bbox[:, [1, 0, 3, 2]]\n\n        label = np.array([self.cat_ids.index(ann['category_id'])\n                          for ann in annotation], dtype=np.int32)\n\n        area = np.array([ann['area']\n                         for ann in annotation], dtype=np.float32)\n\n        crowded = np.array([ann['iscrowd']\n                            for ann in annotation], dtype=np.bool)\n\n        # Remove invalid boxes\n        bbox_area = np.prod(bbox[:, 2:] - bbox[:, :2], axis=1)\n        keep_mask = np.logical_and(bbox[:, 0] <= bbox[:, 2],\n                                   bbox[:, 1] <= bbox[:, 3])\n        keep_mask = np.logical_and(keep_mask, bbox_area > 0)\n\n        if not self.use_crowded:\n            keep_mask = np.logical_and(keep_mask, np.logical_not(crowded))\n\n        bbox = bbox[keep_mask]\n        label = label[keep_mask]\n        area = area[keep_mask]\n        crowded = crowded[keep_mask]\n        return bbox, label, area, crowded\n\n    def _segm_to_mask(self, segm, size):\n        # Copied from pycocotools.coco.COCO.annToMask\n        H, W = size\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            mask = np.zeros((H, W), dtype=np.uint8)\n            mask = PIL.Image.fromarray(mask)\n            for sgm in segm:\n                xy = np.array(sgm).reshape((-1, 2))\n                xy = [tuple(xy_i) for xy_i in xy]\n                PIL.ImageDraw.Draw(mask).polygon(xy=xy, outline=1, fill=1)\n            mask = np.asarray(mask)\n        elif isinstance(segm['counts'], list):\n            rle = coco_mask.frPyObjects(segm, H, W)\n            mask = coco_mask.decode(rle)\n        else:\n            mask = coco_mask.decode(segm)\n        return mask.astype(np.bool)\n\n\nclass COCOBboxDataset(COCOInstancesBaseDataset):\n\n    def __init__(self, data_dir='auto', split='train', year='2017',\n                 use_crowded=False, read_img_with='pil', return_area=False, return_crowded=False):\n        super(COCOBboxDataset, self).__init__(\n            data_dir, split, year, use_crowded, read_img_with)\n        keys = ('img', 'bbox', 'label')\n        if return_area:\n            keys += ('area',)\n        if return_crowded:\n            keys += ('crowded',)\n        self.keys = keys\n"""
alfred/dl/data/meta/__init__.py,0,b''
alfred/dl/data/meta/concatenated_dataset.py,0,"b'from .sliceable_dataset import SliceableDataset\n\n\nclass ConcatenatedDataset(SliceableDataset):\n    """"""A sliceable version of :class:`chainer.datasets.ConcatenatedDataset`.\n\n    Here is an example.\n\n    >>> dataset_a = TupleDataset([0, 1, 2], [0, 1, 4])\n    >>> dataset_b = TupleDataset([3, 4, 5], [9, 16, 25])\n    >>>\n    >>> dataset = ConcatenatedDataset(dataset_a, dataset_b)\n    >>> dataset.slice[:, 0][:]  # [0, 1, 2, 3, 4, 5]\n\n    Args:\n        datasets: The underlying datasets.\n            Each dataset should inherit\n            :class:`~chainercv.chainer_experimental.datasets.sliceable.Sliceabledataset`\n            and should have the same keys.\n    """"""\n\n    def __init__(self, *datasets):\n        if len(datasets) == 0:\n            raise ValueError(\'At least one dataset is required\')\n        self._datasets = datasets\n        self._keys = datasets[0].keys\n        for dataset in datasets[1:]:\n            if not dataset.keys == self._keys:\n                raise ValueError(\'All datasets should have the same keys\')\n\n    def __len__(self):\n        return sum(len(dataset) for dataset in self._datasets)\n\n    @property\n    def keys(self):\n        return self._keys\n\n    def get_example_by_keys(self, index, key_indices):\n        if index < 0:\n            raise IndexError\n        for dataset in self._datasets:\n            if index < len(dataset):\n                return dataset.get_example_by_keys(index, key_indices)\n            index -= len(dataset)\n        raise IndexError'"
alfred/dl/data/meta/dataset_mixin.py,0,"b'import numpy\nimport six\n\n# below code from Chianer\n\nclass DatasetMixin(object):\n\n    """"""Default implementation of dataset indexing.\n\n    DatasetMixin provides the :meth:`__getitem__` operator. The default\n    implementation uses :meth:`get_example` to extract each example, and\n    combines the results into a list. This mixin makes it easy to implement a\n    new dataset that does not support efficient slicing.\n\n    Dataset implementation using DatasetMixin still has to provide the\n    :meth:`__len__` operator explicitly.\n\n    """"""\n\n    def __getitem__(self, index):\n        """"""Returns an example or a sequence of examples.\n\n        It implements the standard Python indexing and one-dimensional integer\n        array indexing. It uses the :meth:`get_example` method by default, but\n        it may be overridden by the implementation to, for example, improve the\n        slicing performance.\n\n        Args:\n            index (int, slice, list or numpy.ndarray): An index of an example\n                or indexes of examples.\n\n        Returns:\n            If index is int, returns an example created by `get_example`.\n            If index is either slice or one-dimensional list or numpy.ndarray,\n            returns a list of examples created by `get_example`.\n\n        .. admonition:: Example\n\n           >>> import numpy\n           >>> from chainer import dataset\n           >>> class SimpleDataset(dataset.DatasetMixin):\n           ...     def __init__(self, values):\n           ...         self.values = values\n           ...     def __len__(self):\n           ...         return len(self.values)\n           ...     def get_example(self, i):\n           ...         return self.values[i]\n           ...\n           >>> ds = SimpleDataset([0, 1, 2, 3, 4, 5])\n           >>> ds[1]   # Access by int\n           1\n           >>> ds[1:3]  # Access by slice\n           [1, 2]\n           >>> ds[[4, 0]]  # Access by one-dimensional integer list\n           [4, 0]\n           >>> index = numpy.arange(3)\n           >>> ds[index]  # Access by one-dimensional integer numpy.ndarray\n           [0, 1, 2]\n\n        """"""\n        if isinstance(index, slice):\n            current, stop, step = index.indices(len(self))\n            return [self.get_example(i) for i in\n                    six.moves.range(current, stop, step)]\n        elif isinstance(index, list) or isinstance(index, numpy.ndarray):\n            return [self.get_example(i) for i in index]\n        else:\n            return self.get_example(index)\n\n    def __len__(self):\n        """"""Returns the number of data points.""""""\n        raise NotImplementedError\n\n    def get_example(self, i):\n        """"""Returns the i-th example.\n\n        Implementations should override it. It should raise :class:`IndexError`\n        if the index is invalid.\n\n        Args:\n            i (int): The index of the example.\n\n        Returns:\n            The i-th example.\n\n        """"""\n        raise NotImplementedError\n'"
alfred/dl/data/meta/getter_dataset.py,0,"b'from .sliceable_dataset import _as_key_indices\nfrom .sliceable_dataset import _is_iterable\nfrom .sliceable_dataset import SliceableDataset\n\n\nclass GetterDataset(SliceableDataset):\n    """"""A sliceable dataset class that is defined with getters.\n\n    This is a dataset class with getters.\n    Please refer to the tutorial for more detailed explanation.\n\n    Here is an example.\n\n    >>> class SliceableLabeledImageDataset(GetterDataset):\n    >>>     def __init__(self, pairs, root=\'.\'):\n    >>>         super(SliceableLabeledImageDataset, self).__init__()\n    >>>         with open(pairs) as f:\n    >>>             self._pairs = [l.split() for l in f]\n    >>>         self._root = root\n    >>>\n    >>>         self.add_getter(\'img\', self.get_image)\n    >>>         self.add_getter(\'label\', self.get_label)\n    >>>\n    >>>     def __len__(self):\n    >>>         return len(self._pairs)\n    >>>\n    >>>     def get_image(self, i):\n    >>>         path, _ = self._pairs[i]\n    >>>         return read_image(os.path.join(self._root, path))\n    >>>\n    >>>     def get_label(self, i):\n    >>>         _, label = self._pairs[i]\n    >>>         return np.int32(label)\n    >>>\n    >>> dataset = SliceableLabeledImageDataset(\'list.txt\')\n    >>>\n    >>> # get a subset with label = 0, 1, 2\n    >>> # no images are loaded\n    >>> indices = [i for i, label in\n    ...            enumerate(dataset.slice[:, \'label\']) if label in {0, 1, 2}]\n    >>> dataset_012 = dataset.slice[indices]\n    """"""\n\n    def __init__(self):\n        self._keys = []\n        self._getters = []\n        self._return_tuple = True\n\n    def __len__(self):\n        raise NotImplementedError\n\n    @property\n    def keys(self):\n        if self._return_tuple:\n            return tuple(key for key, _, _ in self._keys)\n        else:\n            return self._keys[0][0]\n\n    @keys.setter\n    def keys(self, keys):\n        self._keys = [self._keys[key_index]\n                      for key_index in _as_key_indices(keys, self.keys)]\n        self._return_tuple = _is_iterable(keys)\n\n    def add_getter(self, keys, getter):\n        """"""Register a getter function\n\n        Args:\n            keys (string or tuple of strings): The name(s) of data\n                that the getter function returns.\n            getter (callable): A getter function that takes an index and\n                returns data of the corresponding example.\n        """"""\n        self._getters.append(getter)\n        if _is_iterable(keys):\n            for key_index, key in enumerate(keys):\n                self._keys.append((key, len(self._getters) - 1, key_index))\n        else:\n            self._keys.append((keys, len(self._getters) - 1, None))\n\n    def get_example_by_keys(self, index, key_indices):\n        example = []\n        cache = {}\n        for key_index in key_indices:\n            _, getter_index, key_index = self._keys[key_index]\n            if getter_index not in cache:\n                cache[getter_index] = self._getters[getter_index](index)\n            if key_index is None:\n                example.append(cache[getter_index])\n            else:\n                example.append(cache[getter_index][key_index])\n        return tuple(example)\n'"
alfred/dl/data/meta/sliceable_dataset.py,0,"b'import numbers\nimport numpy as np\nimport six\n\nfrom .dataset_mixin import DatasetMixin\n\n\ndef _is_iterable(x):\n    if isinstance(x, str):\n        return False\n    return hasattr(x, \'__iter__\')\n\n\ndef _as_tuple(t):\n    if _is_iterable(t):\n        return tuple(t)\n    else:\n        return t,\n\n\ndef _bool_to_indices(indices, len_):\n    true_indices = []\n    for i, index in enumerate(indices):\n        if isinstance(index, (bool, np.bool_)):\n            if index:\n                true_indices.append(i)\n        else:\n            return indices\n\n    if not len(indices) == len_:\n        raise ValueError(\n            \'The number of booleans is different from the length of dataset\')\n    return true_indices\n\n\ndef _as_key_indices(keys, key_names):\n    key_names = _as_tuple(key_names)\n    keys = _bool_to_indices(_as_tuple(keys), len(key_names))\n\n    for key in keys:\n        if isinstance(key, numbers.Integral):\n            key_index = key\n            if key_index < 0:\n                key_index += len(key_names)\n            if key_index not in range(0, len(key_names)):\n                raise IndexError(\n                    \'index {} is out of bounds for keys with size {}\'.format(\n                        key, len(key_names)))\n        else:\n            try:\n                key_index = key_names.index(key)\n            except ValueError:\n                raise KeyError(\'{} does not exists\'.format(key))\n        yield key_index\n\n\nclass SliceableDataset(DatasetMixin):\n    """"""An abstract dataset class that supports slicing.\n\n    This ia a dataset class that supports slicing.\n    A dataset class inheriting this class should implement\n    three methods: :meth:`__len__`, :meth:`keys`, and\n    :meth:`get_example_by_keys`.\n\n    Users can easily create sliceable datasets using\n    :class:`~chainercv.chainer_experimental.datasets.sliceable.GetterDataset`\n    or\n    :class:`~chainercv.chainer_experimental.datasets.sliceable.TupleDataset`.\n    """"""\n\n    def __len__(self):\n        raise NotImplementedError\n\n    @property\n    def keys(self):\n        """"""Return names of all keys\n\n        Returns:\n            string or tuple of strings\n        """"""\n        raise NotImplementedError\n\n    def get_example_by_keys(self, index, key_indices):\n        """"""Return data of an example by keys\n\n        Args:\n            index (int): An index of an example.\n            key_indices (tuple of ints): A tuple of indices of requested keys.\n\n        Returns:\n            tuple of data\n        """"""\n        raise NotImplementedError\n\n    def get_example(self, index):\n        if isinstance(self.keys, tuple):\n            return self.get_example_by_keys(\n                index, tuple(range(len(self.keys))))\n        else:\n            return self.get_example_by_keys(index, (0,))[0]\n\n    @property\n    def slice(self):\n        return SliceHelper(self)\n\n    def __iter__(self):\n        return (self.get_example(i) for i in six.moves.range(len(self)))\n\n\nclass SliceHelper(object):\n    """"""A helper class for :class:`SliceableDataset`.""""""\n\n    def __init__(self, dataset):\n        self._dataset = dataset\n\n    def __getitem__(self, args):\n        if isinstance(args, tuple):\n            indices, keys = args\n        else:\n            indices = args\n            keys = self._dataset.keys\n\n        if not isinstance(indices, slice):\n            indices = _bool_to_indices(indices, len(self._dataset))\n        key_indices = tuple(_as_key_indices(keys, self._dataset.keys))\n        return_tuple = _is_iterable(keys)\n\n        return SlicedDataset(\n            self._dataset, indices,\n            tuple(key_indices) if return_tuple else key_indices[0])\n\n\nclass SlicedDataset(SliceableDataset):\n    """"""A sliced view for :class:`SliceableDataset`.""""""\n\n    def __init__(self, dataset, indices, key_indices):\n        self._dataset = dataset\n        self._indices = indices\n        self._key_indices = key_indices\n\n    def __len__(self):\n        if isinstance(self._indices, slice):\n            start, end, step = self._indices.indices(len(self._dataset))\n            return len(range(start, end, step))\n        else:\n            return len(self._indices)\n\n    @property\n    def keys(self):\n        keys = _as_tuple(self._dataset.keys)\n        if isinstance(self._key_indices, tuple):\n            return tuple(keys[key_index] for key_index in self._key_indices)\n        else:\n            return keys[self._key_indices]\n\n    def get_example_by_keys(self, index, key_indices):\n        if isinstance(key_indices, tuple):\n            key_indices = tuple(\n                _as_tuple(self._key_indices)[key_index]\n                for key_index in key_indices)\n        else:\n            key_indices = _as_tuple(self._key_indices)[key_indices]\n\n        if isinstance(self._indices, slice):\n            start, _, step = self._indices.indices(len(self._dataset))\n            return self._dataset.get_example_by_keys(\n                start + index * step, key_indices)\n        else:\n            return self._dataset.get_example_by_keys(\n                self._indices[index], key_indices)\n'"
alfred/dl/torch/distribute/utils.py,13,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n""""""\nThis file contains primitives for multi-gpu communication.\nThis is useful when doing distributed training.\n""""""\n\nimport functools\nimport logging\nimport pickle\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\n\n_LOCAL_PROCESS_GROUP = None\n""""""\nA torch process group which only includes processes that on the same machine as the current process.\nThis variable is set when processes are spawned by `launch()` in ""engine/launch.py"".\n""""""\n\n# function ported from mmcv\ndef get_dist_info():\n    if torch.__version__ < \'1.0\':\n        initialized = dist._initialized\n    else:\n        if dist.is_available():\n            initialized = dist.is_initialized()\n        else:\n            initialized = False\n    if initialized:\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n    else:\n        rank = 0\n        world_size = 1\n    return rank, world_size\n\n\ndef get_world_size() -> int:\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank() -> int:\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef get_local_rank() -> int:\n    """"""\n    Returns:\n        The rank of the current process within the local (per-machine) process group.\n    """"""\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    assert _LOCAL_PROCESS_GROUP is not None\n    return dist.get_rank(group=_LOCAL_PROCESS_GROUP)\n\n\ndef get_local_size() -> int:\n    """"""\n    Returns:\n        The size of the per-machine process group,\n        i.e. the number of processes per machine.\n    """"""\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size(group=_LOCAL_PROCESS_GROUP)\n\n\ndef is_main_process() -> bool:\n    return get_rank() == 0\n\n\ndef synchronize():\n    """"""\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    """"""\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()\n    if world_size == 1:\n        return\n    dist.barrier()\n\n\n@functools.lru_cache()\ndef _get_global_gloo_group():\n    """"""\n    Return a process group based on gloo backend, containing all the ranks\n    The result is cached.\n    """"""\n    if dist.get_backend() == ""nccl"":\n        return dist.new_group(backend=""gloo"")\n    else:\n        return dist.group.WORLD\n\n\ndef _serialize_to_tensor(data, group):\n    backend = dist.get_backend(group)\n    assert backend in [""gloo"", ""nccl""]\n    device = torch.device(""cpu"" if backend == ""gloo"" else ""cuda"")\n\n    buffer = pickle.dumps(data)\n    if len(buffer) > 1024 ** 3:\n        logger = logging.getLogger(__name__)\n        logger.warning(\n            ""Rank {} trying to all-gather {:.2f} GB of data on device {}"".format(\n                get_rank(), len(buffer) / (1024 ** 3), device\n            )\n        )\n    storage = torch.ByteStorage.from_buffer(buffer)\n    tensor = torch.ByteTensor(storage).to(device=device)\n    return tensor\n\n\ndef _pad_to_largest_tensor(tensor, group):\n    """"""\n    Returns:\n        list[int]: size of the tensor, on each rank\n        Tensor: padded tensor that has the max size\n    """"""\n    world_size = dist.get_world_size(group=group)\n    assert (\n        world_size >= 1\n    ), ""comm.gather/all_gather must be called from ranks within the given group!""\n    local_size = torch.tensor([tensor.numel()], dtype=torch.int64, device=tensor.device)\n    size_list = [\n        torch.zeros([1], dtype=torch.int64, device=tensor.device) for _ in range(world_size)\n    ]\n    dist.all_gather(size_list, local_size, group=group)\n    size_list = [int(size.item()) for size in size_list]\n\n    max_size = max(size_list)\n\n    # we pad the tensor because torch all_gather does not support\n    # gathering tensors of different shapes\n    if local_size != max_size:\n        padding = torch.zeros((max_size - local_size,), dtype=torch.uint8, device=tensor.device)\n        tensor = torch.cat((tensor, padding), dim=0)\n    return size_list, tensor\n\n\ndef all_gather(data, group=None):\n    """"""\n    Run all_gather on arbitrary picklable data (not necessarily tensors).\n\n    Args:\n        data: any picklable object\n        group: a torch process group. By default, will use a group which\n            contains all ranks on gloo backend.\n\n    Returns:\n        list[data]: list of data gathered from each rank\n    """"""\n    if get_world_size() == 1:\n        return [data]\n    if group is None:\n        group = _get_global_gloo_group()\n    if dist.get_world_size(group) == 1:\n        return [data]\n\n    tensor = _serialize_to_tensor(data, group)\n\n    size_list, tensor = _pad_to_largest_tensor(tensor, group)\n    max_size = max(size_list)\n\n    # receiving Tensor from all ranks\n    tensor_list = [\n        torch.empty((max_size,), dtype=torch.uint8, device=tensor.device) for _ in size_list\n    ]\n    dist.all_gather(tensor_list, tensor, group=group)\n\n    data_list = []\n    for size, tensor in zip(size_list, tensor_list):\n        buffer = tensor.cpu().numpy().tobytes()[:size]\n        data_list.append(pickle.loads(buffer))\n\n    return data_list\n\n\ndef gather(data, dst=0, group=None):\n    """"""\n    Run gather on arbitrary picklable data (not necessarily tensors).\n\n    Args:\n        data: any picklable object\n        dst (int): destination rank\n        group: a torch process group. By default, will use a group which\n            contains all ranks on gloo backend.\n\n    Returns:\n        list[data]: on dst, a list of data gathered from each rank. Otherwise,\n            an empty list.\n    """"""\n    if get_world_size() == 1:\n        return [data]\n    if group is None:\n        group = _get_global_gloo_group()\n    if dist.get_world_size(group=group) == 1:\n        return [data]\n    rank = dist.get_rank(group=group)\n\n    tensor = _serialize_to_tensor(data, group)\n    size_list, tensor = _pad_to_largest_tensor(tensor, group)\n\n    # receiving Tensor from all ranks\n    if rank == dst:\n        max_size = max(size_list)\n        tensor_list = [\n            torch.empty((max_size,), dtype=torch.uint8, device=tensor.device) for _ in size_list\n        ]\n        dist.gather(tensor, tensor_list, dst=dst, group=group)\n\n        data_list = []\n        for size, tensor in zip(size_list, tensor_list):\n            buffer = tensor.cpu().numpy().tobytes()[:size]\n            data_list.append(pickle.loads(buffer))\n        return data_list\n    else:\n        dist.gather(tensor, [], dst=dst, group=group)\n        return []\n\n\ndef shared_random_seed():\n    """"""\n    Returns:\n        int: a random number that is the same across all workers.\n            If workers need a shared RNG, they can use this shared seed to\n            create one.\n\n    All workers must call this function, otherwise it will deadlock.\n    """"""\n    ints = np.random.randint(2 ** 31)\n    all_ints = all_gather(ints)\n    return all_ints[0]\n\n\ndef reduce_dict(input_dict, average=True):\n    """"""\n    Reduce the values in the dictionary from all processes so that process with rank\n    0 has the reduced results.\n\n    Args:\n        input_dict (dict): inputs to be reduced. All the values must be scalar CUDA Tensor.\n        average (bool): whether to do average or sum\n\n    Returns:\n        a dict with the same keys as input_dict, after reduction.\n    """"""\n    world_size = get_world_size()\n    if world_size < 2:\n        return input_dict\n    with torch.no_grad():\n        names = []\n        values = []\n        # sort the keys so that they are consistent across processes\n        for k in sorted(input_dict.keys()):\n            names.append(k)\n            values.append(input_dict[k])\n        values = torch.stack(values, dim=0)\n        dist.reduce(values, dst=0)\n        if dist.get_rank() == 0 and average:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            values /= world_size\n        reduced_dict = {k: v for k, v in zip(names, values)}\n    return reduced_dict\n'"
alfred/dl/torch/nn/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nfrom .functional import one_hot\nfrom .modules.common import Empty, Sequential\nfrom .modules.normalization import GroupNorm\n'"
alfred/dl/torch/nn/functional.py,2,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport torch\n\ndef one_hot(tensor, depth, dim=-1, on_value=1.0, dtype=torch.float32):\n    tensor_onehot = torch.zeros(\n        *list(tensor.shape), depth, dtype=dtype, device=tensor.device)\n    tensor_onehot.scatter_(dim, tensor.unsqueeze(dim).long(), on_value)\n    return tensor_onehot\n'"
alfred/dl/torch/nn/weights_init.py,1,"b""# Copyright (c) Open-MMLab. All rights reserved.\nimport torch.nn as nn\n\n\ndef constant_init(module, val, bias=0):\n    if hasattr(module, 'weight') and module.weight is not None:\n        nn.init.constant_(module.weight, val)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef xavier_init(module, gain=1, bias=0, distribution='normal'):\n    assert distribution in ['uniform', 'normal']\n    if distribution == 'uniform':\n        nn.init.xavier_uniform_(module.weight, gain=gain)\n    else:\n        nn.init.xavier_normal_(module.weight, gain=gain)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef normal_init(module, mean=0, std=1, bias=0):\n    nn.init.normal_(module.weight, mean, std)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef uniform_init(module, a=0, b=1, bias=0):\n    nn.init.uniform_(module.weight, a, b)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef kaiming_init(module,\n                 a=0,\n                 mode='fan_out',\n                 nonlinearity='relu',\n                 bias=0,\n                 distribution='normal'):\n    assert distribution in ['uniform', 'normal']\n    if distribution == 'uniform':\n        nn.init.kaiming_uniform_(\n            module.weight, a=a, mode=mode, nonlinearity=nonlinearity)\n    else:\n        nn.init.kaiming_normal_(\n            module.weight, a=a, mode=mode, nonlinearity=nonlinearity)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef caffe2_xavier_init(module, bias=0):\n    # `XavierFill` in Caffe2 corresponds to `kaiming_uniform_` in PyTorch\n    # Acknowledgment to FAIR's internal code\n    kaiming_init(\n        module,\n        a=1,\n        mode='fan_in',\n        nonlinearity='leaky_relu',\n        distribution='uniform')\n"""
alfred/dl/torch/ops/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/dl/torch/ops/array_ops.py,1,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport ctypes\nimport math\nimport time\nimport torch\n\n\ndef scatter_nd(indices, updates, shape):\n    """"""pytorch edition of tensorflow scatter_nd.\n    this function don\'t contain except handle code. so use this carefully\n    when indice repeats, don\'t support repeat add which is supported\n    in tensorflow.\n    """"""\n    ret = torch.zeros(*shape, dtype=updates.dtype, device=updates.device)\n    ndim = indices.shape[-1]\n    output_shape = list(indices.shape[:-1]) + shape[indices.shape[-1]:]\n    flatted_indices = indices.view(-1, ndim)\n    slices = [flatted_indices[:, i] for i in range(ndim)]\n    slices += [Ellipsis]\n    ret[slices] = updates.view(*output_shape)\n    return ret\n\n\ndef gather_nd(params, indices):\n    # this function has a limit that MAX_ADVINDEX_CALC_DIMS=5\n    ndim = indices.shape[-1]\n    output_shape = list(indices.shape[:-1]) + list(params.shape[indices.shape[-1]:])\n    flatted_indices = indices.view(-1, ndim)\n    slices = [flatted_indices[:, i] for i in range(ndim)]\n    slices += [Ellipsis]\n    return params[slices].view(*output_shape)\n'"
alfred/dl/torch/runner/checkpoint.py,7,"b'# Copyright (c) Open-MMLab. All rights reserved.\nimport os\nimport os.path as osp\nimport pkgutil\nimport time\nimport warnings\nfrom collections import OrderedDict\nfrom importlib import import_module\n\nimport torch\nimport torchvision\nfrom torch.utils import model_zoo\n\nfrom ..distribute.utils import get_dist_info\n\n\nopen_mmlab_model_urls = {\n    \'vgg16_caffe\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/vgg16_caffe-292e1171.pth\',  # noqa: E501\n    \'resnet50_caffe\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet50_caffe-788b5fa3.pth\',  # noqa: E501\n    \'resnet101_caffe\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet101_caffe-3ad79236.pth\',  # noqa: E501\n    \'resnext50_32x4d\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext50-32x4d-0ab1a123.pth\',  # noqa: E501\n    \'resnext101_32x4d\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext101_32x4d-a5af3160.pth\',  # noqa: E501\n    \'resnext101_64x4d\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext101_64x4d-ee2c6f71.pth\',  # noqa: E501\n    \'contrib/resnet50_gn\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet50_gn_thangvubk-ad1730dd.pth\',  # noqa: E501\n    \'detectron/resnet50_gn\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet50_gn-9186a21c.pth\',  # noqa: E501\n    \'detectron/resnet101_gn\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet101_gn-cac0ab98.pth\',  # noqa: E501\n    \'jhu/resnet50_gn_ws\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet50_gn_ws-15beedd8.pth\',  # noqa: E501\n    \'jhu/resnet101_gn_ws\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet101_gn_ws-3e3c308c.pth\',  # noqa: E501\n    \'jhu/resnext50_32x4d_gn_ws\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext50_32x4d_gn_ws-0d87ac85.pth\',  # noqa: E501\n    \'jhu/resnext101_32x4d_gn_ws\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext101_32x4d_gn_ws-34ac1a9e.pth\',  # noqa: E501\n    \'jhu/resnext50_32x4d_gn\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext50_32x4d_gn-c7e8b754.pth\',  # noqa: E501\n    \'jhu/resnext101_32x4d_gn\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext101_32x4d_gn-ac3bb84e.pth\',  # noqa: E501\n    \'msra/hrnetv2_w18\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/hrnetv2_w18-00eb2006.pth\',  # noqa: E501\n    \'msra/hrnetv2_w32\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/hrnetv2_w32-dc9eeb4f.pth\',  # noqa: E501\n    \'msra/hrnetv2_w40\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/hrnetv2_w40-ed0b031c.pth\',  # noqa: E501\n    \'bninception_caffe\': \'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/pretrain/third_party/bn_inception_caffe-ed2e8665.pth\',  # noqa: E501\n    \'kin400/i3d_r50_f32s2_k400\': \'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/pretrain/third_party/i3d_r50_f32s2_k400-2c57e077.pth\',  # noqa: E501\n    \'kin400/nl3d_r50_f32s2_k400\': \'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/pretrain/third_party/nl3d_r50_f32s2_k400-fa7e7caa.pth\',  # noqa: E501\n}  # yapf: disable\n\n\ndef load_state_dict(module, state_dict, strict=False, logger=None):\n    """"""Load state_dict to a module.\n\n    This method is modified from :meth:`torch.nn.Module.load_state_dict`.\n    Default value for ``strict`` is set to ``False`` and the message for\n    param mismatch will be shown even if strict is False.\n\n    Args:\n        module (Module): Module that receives the state_dict.\n        state_dict (OrderedDict): Weights.\n        strict (bool): whether to strictly enforce that the keys\n            in :attr:`state_dict` match the keys returned by this module\'s\n            :meth:`~torch.nn.Module.state_dict` function. Default: ``False``.\n        logger (:obj:`logging.Logger`, optional): Logger to log the error\n            message. If not specified, print function will be used.\n    """"""\n    unexpected_keys = []\n    all_missing_keys = []\n    err_msg = []\n\n    metadata = getattr(state_dict, \'_metadata\', None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    # use _load_from_state_dict to enable checkpoint version control\n    def load(module, prefix=\'\'):\n        local_metadata = {} if metadata is None else metadata.get(\n            prefix[:-1], {})\n        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n                                     all_missing_keys, unexpected_keys,\n                                     err_msg)\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + \'.\')\n\n    load(module)\n    load = None  # break load->load reference cycle\n\n    # ignore ""num_batches_tracked"" of BN layers\n    missing_keys = [\n        key for key in all_missing_keys if \'num_batches_tracked\' not in key\n    ]\n\n    if unexpected_keys:\n        err_msg.append(\'unexpected key in source state_dict: {}\\n\'.format(\n            \', \'.join(unexpected_keys)))\n    if missing_keys:\n        err_msg.append(\'missing keys in source state_dict: {}\\n\'.format(\n            \', \'.join(missing_keys)))\n\n    rank, _ = get_dist_info()\n    if len(err_msg) > 0 and rank == 0:\n        err_msg.insert(\n            0, \'The model and loaded state dict do not match exactly\\n\')\n        err_msg = \'\\n\'.join(err_msg)\n        if strict:\n            raise RuntimeError(err_msg)\n        elif logger is not None:\n            logger.warning(err_msg)\n        else:\n            print(err_msg)\n\n\ndef load_url_dist(url):\n    """""" In distributed setting, this function only download checkpoint at\n    local rank 0 """"""\n    rank, world_size = get_dist_info()\n    rank = int(os.environ.get(\'LOCAL_RANK\', rank))\n    if rank == 0:\n        checkpoint = model_zoo.load_url(url)\n    if world_size > 1:\n        torch.distributed.barrier()\n        if rank > 0:\n            checkpoint = model_zoo.load_url(url)\n    return checkpoint\n\n\ndef get_torchvision_models():\n    model_urls = dict()\n    for _, name, ispkg in pkgutil.walk_packages(torchvision.models.__path__):\n        if ispkg:\n            continue\n        _zoo = import_module(\'torchvision.models.{}\'.format(name))\n        if hasattr(_zoo, \'model_urls\'):\n            _urls = getattr(_zoo, \'model_urls\')\n            model_urls.update(_urls)\n    return model_urls\n\n\ndef _load_checkpoint(filename, map_location=None):\n    """"""Load checkpoint from somewhere (modelzoo, file, url).\n\n    Args:\n        filename (str): Either a filepath or URI.\n        map_location (str | None): Same as :func:`torch.load`. Default: None.\n\n    Returns:\n        dict | OrderedDict: The loaded checkpoint. It can be either an\n            OrderedDict storing model weights or a dict containing other\n            information, which depends on the checkpoint.\n    """"""\n    if filename.startswith(\'modelzoo://\'):\n        warnings.warn(\'The URL scheme of ""modelzoo://"" is deprecated, please \'\n                      \'use ""torchvision://"" instead\')\n        model_urls = get_torchvision_models()\n        model_name = filename[11:]\n        checkpoint = load_url_dist(model_urls[model_name])\n    elif filename.startswith(\'torchvision://\'):\n        model_urls = get_torchvision_models()\n        model_name = filename[14:]\n        checkpoint = load_url_dist(model_urls[model_name])\n    elif filename.startswith(\'open-mmlab://\'):\n        model_name = filename[13:]\n        checkpoint = load_url_dist(open_mmlab_model_urls[model_name])\n    elif filename.startswith((\'http://\', \'https://\')):\n        checkpoint = load_url_dist(filename)\n    else:\n        if not osp.isfile(filename):\n            raise IOError(\'{} is not a checkpoint file\'.format(filename))\n        checkpoint = torch.load(filename, map_location=map_location)\n    return checkpoint\n\n\ndef load_checkpoint(model,\n                    filename,\n                    map_location=None,\n                    strict=False,\n                    logger=None):\n    """"""Load checkpoint from a file or URI.\n\n    Args:\n        model (Module): Module to load checkpoint.\n        filename (str): Either a filepath or URL or modelzoo://xxxxxxx.\n        map_location (str): Same as :func:`torch.load`.\n        strict (bool): Whether to allow different params for the model and\n            checkpoint.\n        logger (:mod:`logging.Logger` or None): The logger for error message.\n\n    Returns:\n        dict or OrderedDict: The loaded checkpoint.\n    """"""\n    checkpoint = _load_checkpoint(filename, map_location)\n    # get state_dict from checkpoint\n    if isinstance(checkpoint, OrderedDict):\n        state_dict = checkpoint\n    elif isinstance(checkpoint, dict) and \'state_dict\' in checkpoint:\n        state_dict = checkpoint[\'state_dict\']\n    else:\n        raise RuntimeError(\n            \'No state_dict found in checkpoint file {}\'.format(filename))\n    # strip prefix of state_dict\n    if list(state_dict.keys())[0].startswith(\'module.\'):\n        state_dict = {k[7:]: v for k, v in checkpoint[\'state_dict\'].items()}\n    # load state_dict\n    if hasattr(model, \'module\'):\n        load_state_dict(model.module, state_dict, strict, logger)\n    else:\n        load_state_dict(model, state_dict, strict, logger)\n    return checkpoint\n\n\ndef weights_to_cpu(state_dict):\n    """"""Copy a model state_dict to cpu.\n\n    Args:\n        state_dict (OrderedDict): Model weights on GPU.\n\n    Returns:\n        OrderedDict: Model weights on GPU.\n    """"""\n    state_dict_cpu = OrderedDict()\n    for key, val in state_dict.items():\n        state_dict_cpu[key] = val.cpu()\n    return state_dict_cpu'"
alfred/dl/torch/train/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nfrom .checkpoint import (latest_checkpoint, restore,\n                         restore_latest_checkpoints,\n                         restore_models, save, save_models,\n                         try_restore_latest_checkpoints)\nfrom .common import create_folder\nfrom .optim import MixedPrecisionWrapper\n'"
alfred/dl/torch/train/checkpoint.py,3,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport json\nimport logging\nimport os\nimport signal\nfrom pathlib import Path\n\nimport torch\n\n\nclass DelayedKeyboardInterrupt(object):\n    def __enter__(self):\n        self.signal_received = False\n        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n\n    def handler(self, sig, frame):\n        self.signal_received = (sig, frame)\n        logging.debug(\'SIGINT received. Delaying KeyboardInterrupt.\')\n\n    def __exit__(self, type, value, traceback):\n        signal.signal(signal.SIGINT, self.old_handler)\n        if self.signal_received:\n            self.old_handler(*self.signal_received)\n\n\ndef latest_checkpoint(model_dir, model_name):\n    """"""return path of latest checkpoint in a model_dir\n    Args:\n        model_dir: string, indicate your model dir(save ckpts, summarys,\n            logs, etc).\n        model_name: name of your model. we find ckpts by name\n    Returns:\n        path: None if isn\'t exist or latest checkpoint path.\n    """"""\n    ckpt_info_path = Path(model_dir) / ""checkpoints.json""\n    if not ckpt_info_path.is_file():\n        return None\n    with open(ckpt_info_path, \'r\') as f:\n        ckpt_dict = json.loads(f.read())\n    if model_name not in ckpt_dict[\'latest_ckpt\']:\n        return None\n    latest_ckpt = ckpt_dict[\'latest_ckpt\'][model_name]\n    ckpt_file_name = Path(model_dir) / latest_ckpt\n    if not ckpt_file_name.is_file():\n        return None\n    \n    return str(ckpt_file_name)\n\ndef _ordered_unique(seq):\n    seen = set()\n    return [x for x in seq if not (x in seen or seen.add(x))]\n\ndef save(model_dir,\n         model,\n         model_name,\n         global_step,\n         max_to_keep=8,\n         keep_latest=True):\n    """"""save a model into model_dir.\n    Args:\n        model_dir: string, indicate your model dir(save ckpts, summarys,\n            logs, etc).\n        model: torch.nn.Module instance.\n        model_name: name of your model. we find ckpts by name\n        global_step: int, indicate current global step.\n        max_to_keep: int, maximum checkpoints to keep.\n        keep_latest: bool, if True and there are too much ckpts, \n            will delete oldest ckpt. else will delete ckpt which has\n            smallest global step.\n    Returns:\n        path: None if isn\'t exist or latest checkpoint path.\n    """"""\n\n    # prevent save incomplete checkpoint due to key interrupt\n    with DelayedKeyboardInterrupt():\n        ckpt_info_path = Path(model_dir) / ""checkpoints.json""\n        ckpt_filename = ""{}-{}.tckpt"".format(model_name, global_step)\n        ckpt_path = Path(model_dir) / ckpt_filename\n        if not ckpt_info_path.is_file():\n            ckpt_info_dict = {\'latest_ckpt\': {}, \'all_ckpts\': {}}\n        else:\n            with open(ckpt_info_path, \'r\') as f:\n                ckpt_info_dict = json.loads(f.read())\n        ckpt_info_dict[\'latest_ckpt\'][model_name] = ckpt_filename\n        if model_name in ckpt_info_dict[\'all_ckpts\']:\n            ckpt_info_dict[\'all_ckpts\'][model_name].append(ckpt_filename)\n        else:\n            ckpt_info_dict[\'all_ckpts\'][model_name] = [ckpt_filename]\n        all_ckpts = ckpt_info_dict[\'all_ckpts\'][model_name]\n\n        torch.save(model.state_dict(), ckpt_path)\n        # check ckpt in all_ckpts is exist, if not, delete it from all_ckpts\n        all_ckpts_checked = []\n        for ckpt in all_ckpts:\n            ckpt_path_uncheck = Path(model_dir) / ckpt\n            if ckpt_path_uncheck.is_file():\n                all_ckpts_checked.append(str(ckpt_path_uncheck))\n        all_ckpts = all_ckpts_checked\n        if len(all_ckpts) > max_to_keep:\n            if keep_latest:\n                ckpt_to_delete = all_ckpts.pop(0)\n            else:\n                # delete smallest step\n                get_step = lambda name: int(name.split(\'.\')[0].split(\'-\')[1])\n                min_step = min([get_step(name) for name in all_ckpts])\n                ckpt_to_delete = ""{}-{}.tckpt"".format(model_name, min_step)\n                all_ckpts.remove(ckpt_to_delete)\n            os.remove(str(Path(model_dir) / ckpt_to_delete))\n        all_ckpts_filename = _ordered_unique([Path(f).name for f in all_ckpts])\n        ckpt_info_dict[\'all_ckpts\'][model_name] = all_ckpts_filename\n        with open(ckpt_info_path, \'w\') as f:\n            f.write(json.dumps(ckpt_info_dict, indent=2))\n\n\ndef restore(ckpt_path, model):\n    if not Path(ckpt_path).is_file():\n        raise ValueError(""checkpoint {} not exist."".format(ckpt_path))\n    model.load_state_dict(torch.load(ckpt_path))\n    print(""Restoring parameters from {}"".format(ckpt_path))\n\n\ndef _check_model_names(models):\n    model_names = []\n    for model in models:\n        if not hasattr(model, ""name""):\n            raise ValueError(""models must have name attr"")\n        model_names.append(model.name)\n    if len(model_names) != len(set(model_names)):\n        raise ValueError(""models must have unique name: {}"".format(\n            "", "".join(model_names)))\n\n\ndef _get_name_to_model_map(models):\n    if isinstance(models, dict):\n        name_to_model = {name: m for name, m in models.items()}\n    else:\n        _check_model_names(models)\n        name_to_model = {m.name: m for m in models}\n    return name_to_model\n\n\ndef try_restore_latest_checkpoints(model_dir, models):\n    name_to_model = _get_name_to_model_map(models)\n    for name, model in name_to_model.items():\n        latest_ckpt = latest_checkpoint(model_dir, name)\n        if latest_ckpt is not None:\n            restore(latest_ckpt, model)\n\ndef restore_latest_checkpoints(model_dir, models):\n    name_to_model = _get_name_to_model_map(models)\n    for name, model in name_to_model.items():\n        latest_ckpt = latest_checkpoint(model_dir, name)\n        if latest_ckpt is not None:\n            restore(latest_ckpt, model)\n        else:\n            raise ValueError(""model {}\\\'s ckpt isn\'t exist"".format(name))\n\ndef restore_models(model_dir, models, global_step):\n    name_to_model = _get_name_to_model_map(models)\n    for name, model in name_to_model.items():\n        ckpt_filename = ""{}-{}.tckpt"".format(name, global_step)\n        ckpt_path = model_dir + ""/"" + ckpt_filename\n        restore(ckpt_path, model)\n\n\ndef save_models(model_dir,\n                models,\n                global_step,\n                max_to_keep=15,\n                keep_latest=True):\n    with DelayedKeyboardInterrupt():\n        name_to_model = _get_name_to_model_map(models)\n        for name, model in name_to_model.items():\n            save(model_dir, model, name, global_step, max_to_keep, keep_latest)\n'"
alfred/dl/torch/train/common.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport datetime\nimport os\nimport shutil\n\ndef create_folder(prefix, add_time=True, add_str=None, delete=False):\n    additional_str = \'\'\n    if delete is True:\n        if os.path.exists(prefix):\n            shutil.rmtree(prefix)\n        os.makedirs(prefix)\n    folder = prefix\n    if add_time is True:\n        # additional_str has a form such as \'170903_220351\'\n        additional_str += datetime.datetime.now().strftime(""%y%m%d_%H%M%S"")\n        if add_str is not None:\n            folder += \'/\' + additional_str + \'_\' + add_str\n        else:\n            folder += \'/\' + additional_str\n    if delete is True:\n        if os.path.exists(folder):\n            shutil.rmtree(folder)\n    os.makedirs(folder)\n    return folder'"
alfred/dl/torch/train/fastai_optim.py,5,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nfrom collections import Iterable, defaultdict\nfrom copy import deepcopy\nfrom itertools import chain\n\nimport torch\nfrom torch import nn\nfrom torch._utils import _unflatten_dense_tensors\nfrom torch.autograd import Variable\nfrom torch.nn.utils import parameters_to_vector\n\nbn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n\n\ndef split_bn_bias(layer_groups):\n    ""Split the layers in `layer_groups` into batchnorm (`bn_types`) and non-batchnorm groups.""\n    split_groups = []\n    for l in layer_groups:\n        l1, l2 = [], []\n        for c in l.children():\n            if isinstance(c, bn_types): l2.append(c)\n            else: l1.append(c)\n        split_groups += [nn.Sequential(*l1), nn.Sequential(*l2)]\n    return split_groups\n\n\ndef get_master(layer_groups, flat_master: bool = False):\n    ""Return two lists, one for the model parameters in FP16 and one for the master parameters in FP32.""\n    split_groups = split_bn_bias(layer_groups)\n    model_params = [[\n        param for param in lg.parameters() if param.requires_grad\n    ] for lg in split_groups]\n    if flat_master:\n        master_params = []\n        for lg in model_params:\n            if len(lg) != 0:\n                mp = parameters_to_vector([param.data.float() for param in lg])\n                mp = torch.nn.Parameter(mp, requires_grad=True)\n                if mp.grad is None: mp.grad = mp.new(*mp.size())\n                master_params.append([mp])\n            else:\n                master_params.append([])\n        return model_params, master_params\n    else:\n        master_params = [[param.clone().float().detach() for param in lg]\n                         for lg in model_params]\n        for mp in master_params:\n            for param in mp:\n                param.requires_grad = True\n        return model_params, master_params\n\n\ndef model_g2master_g(model_params, master_params,\n                     flat_master: bool = False) -> None:\n    ""Copy the `model_params` gradients to `master_params` for the optimizer step.""\n    if flat_master:\n        for model_group, master_group in zip(model_params, master_params):\n            if len(master_group) != 0:\n                master_group[0].grad.data.copy_(\n                    parameters_to_vector(\n                        [p.grad.data.float() for p in model_group]))\n    else:\n        for model_group, master_group in zip(model_params, master_params):\n            for model, master in zip(model_group, master_group):\n                if model.grad is not None:\n                    if master.grad is None:\n                        master.grad = master.data.new(*master.data.size())\n                    master.grad.data.copy_(model.grad.data)\n                else:\n                    master.grad = None\n\n\ndef master2model(model_params, master_params,\n                 flat_master: bool = False) -> None:\n    ""Copy `master_params` to `model_params`.""\n    if flat_master:\n        for model_group, master_group in zip(model_params, master_params):\n            if len(model_group) != 0:\n                for model, master in zip(\n                        model_group,\n                        _unflatten_dense_tensors(master_group[0].data,\n                                                 model_group)):\n                    model.data.copy_(master)\n    else:\n        for model_group, master_group in zip(model_params, master_params):\n            for model, master in zip(model_group, master_group):\n                model.data.copy_(master.data)\n\n\ndef listify(p=None, q=None):\n    ""Make `p` listy and the same length as `q`.""\n    if p is None: p = []\n    elif isinstance(p, str): p = [p]\n    elif not isinstance(p, Iterable): p = [p]\n    n = q if type(q) == int else len(p) if q is None else len(q)\n    if len(p) == 1: p = p * n\n    assert len(p) == n, f\'List len mismatch ({len(p)} vs {n})\'\n    return list(p)\n\n\ndef trainable_params(m: nn.Module):\n    ""Return list of trainable params in `m`.""\n    res = filter(lambda p: p.requires_grad, m.parameters())\n    return res\n\n\ndef is_tuple(x) -> bool:\n    return isinstance(x, tuple)\n\n\n# copy from fastai.\nclass OptimWrapper(torch.optim.Optimizer):\n    ""Basic wrapper around `opt` to simplify hyper-parameters changes.""\n\n    def __init__(self, opt, wd, true_wd: bool = False, bn_wd: bool = True):\n        # super().__init__(opt.param_groups, dict())\n        self.opt, self.true_wd, self.bn_wd = opt, true_wd, bn_wd\n        self.opt_keys = list(self.opt.param_groups[0].keys())\n        self.opt_keys.remove(\'params\')\n        self.read_defaults()\n        self.wd = wd\n\n    @classmethod\n    def create(cls, opt_func, lr, layer_groups, **kwargs):\n        ""Create an `optim.Optimizer` from `opt_func` with `lr`. Set lr on `layer_groups`.""\n        split_groups = split_bn_bias(layer_groups)\n        opt = opt_func([{\n            \'params\': trainable_params(l),\n            \'lr\': 0\n        } for l in split_groups])\n        opt = cls(opt, **kwargs)\n        opt.lr, opt.opt_func = listify(lr, layer_groups), opt_func\n        return opt\n\n    def new(self, layer_groups):\n        ""Create a new `OptimWrapper` from `self` with another `layer_groups` but the same hyper-parameters.""\n        opt_func = getattr(self, \'opt_func\', self.opt.__class__)\n        split_groups = split_bn_bias(layer_groups)\n        opt = opt_func([{\n            \'params\': trainable_params(l),\n            \'lr\': 0\n        } for l in split_groups])\n        return self.create(\n            opt_func,\n            self.lr,\n            layer_groups,\n            wd=self.wd,\n            true_wd=self.true_wd,\n            bn_wd=self.bn_wd)\n\n    def __repr__(self) -> str:\n        return f\'OptimWrapper over {repr(self.opt)}.\\nTrue weight decay: {self.true_wd}\'\n\n    #Pytorch optimizer methods\n    def step(self) -> None:\n        ""Set weight decay and step optimizer.""\n        # weight decay outside of optimizer step (AdamW)\n        if self.true_wd:\n            for lr, wd, pg1, pg2 in zip(self._lr, self._wd,\n                                        self.opt.param_groups[::2],\n                                        self.opt.param_groups[1::2]):\n                for p in pg1[\'params\']:\n                    p.data.mul_(1 - wd * lr)\n                if self.bn_wd:\n                    for p in pg2[\'params\']:\n                        p.data.mul_(1 - wd * lr)\n            self.set_val(\'weight_decay\', listify(0, self._wd))\n        self.opt.step()\n\n    def zero_grad(self) -> None:\n        ""Clear optimizer gradients.""\n        self.opt.zero_grad()\n\n    #Passthrough to the inner opt.\n    def __getstate__(self):\n        return self.opt.__getstate__()\n\n    def __setstate__(self, state):\n        return self.opt.__setstate__(state)\n\n    def state_dict(self):\n        return self.opt.state_dict()\n\n    def load_state_dict(self, state_dict):\n        return self.opt.load_state_dict(state_dict)\n\n    def add_param_group(self, param_group):\n        return self.opt.add_param_group(param_group)\n\n    def clear(self):\n        ""Reset the state of the inner optimizer.""\n        sd = self.state_dict()\n        sd[\'state\'] = {}\n        self.load_state_dict(sd)\n\n    @property\n    def param_groups(self):\n        return self.opt.param_groups\n\n    @property\n    def defaults(self):\n        return self.opt.defaults\n\n    @property\n    def state(self):\n        return self.opt.state\n\n\n    #Hyperparameters as properties\n    @property\n    def lr(self) -> float:\n        return self._lr[-1]\n\n    @lr.setter\n    def lr(self, val: float) -> None:\n        self._lr = self.set_val(\'lr\', listify(val, self._lr))\n\n    @property\n    def mom(self) -> float:\n        return self._mom[-1]\n\n    @mom.setter\n    def mom(self, val: float) -> None:\n        if \'momentum\' in self.opt_keys:\n            self.set_val(\'momentum\', listify(val, self._mom))\n        elif \'betas\' in self.opt_keys:\n            self.set_val(\'betas\', (listify(val, self._mom), self._beta))\n        self._mom = listify(val, self._mom)\n\n    @property\n    def beta(self) -> float:\n        return None if self._beta is None else self._beta[-1]\n\n    @beta.setter\n    def beta(self, val: float) -> None:\n        ""Set beta (or alpha as makes sense for given optimizer).""\n        if val is None: return\n        if \'betas\' in self.opt_keys:\n            self.set_val(\'betas\', (self._mom, listify(val, self._beta)))\n        elif \'alpha\' in self.opt_keys:\n            self.set_val(\'alpha\', listify(val, self._beta))\n        self._beta = listify(val, self._beta)\n\n    @property\n    def wd(self) -> float:\n        return self._wd[-1]\n\n    @wd.setter\n    def wd(self, val: float) -> None:\n        ""Set weight decay.""\n        if not self.true_wd:\n            self.set_val(\n                \'weight_decay\', listify(val, self._wd), bn_groups=self.bn_wd)\n        self._wd = listify(val, self._wd)\n\n    #Helper functions\n    def read_defaults(self) -> None:\n        ""Read the values inside the optimizer for the hyper-parameters.""\n        self._beta = None\n        if \'lr\' in self.opt_keys: self._lr = self.read_val(\'lr\')\n        if \'momentum\' in self.opt_keys: self._mom = self.read_val(\'momentum\')\n        if \'alpha\' in self.opt_keys: self._beta = self.read_val(\'alpha\')\n        if \'betas\' in self.opt_keys:\n            self._mom, self._beta = self.read_val(\'betas\')\n        if \'weight_decay\' in self.opt_keys:\n            self._wd = self.read_val(\'weight_decay\')\n\n    def set_val(self, key: str, val, bn_groups: bool = True):\n        ""Set `val` inside the optimizer dictionary at `key`.""\n        if is_tuple(val): val = [(v1, v2) for v1, v2 in zip(*val)]\n        for v, pg1, pg2 in zip(val, self.opt.param_groups[::2],\n                               self.opt.param_groups[1::2]):\n            pg1[key] = v\n            if bn_groups: pg2[key] = v\n        return val\n\n    def read_val(self, key: str):\n        ""Read a hyperparameter `key` in the optimizer dictionary.""\n        val = [pg[key] for pg in self.opt.param_groups[::2]]\n        if is_tuple(val[0]): val = [o[0] for o in val], [o[1] for o in val]\n        return val\n\n\nclass FastAIMixedOptim(OptimWrapper):\n    @classmethod\n    def create(cls,\n               opt_func,\n               lr,\n               layer_groups,\n               model,\n               flat_master=False,\n               loss_scale=512.0,\n               **kwargs):\n        ""Create an `optim.Optimizer` from `opt_func` with `lr`. Set lr on `layer_groups`.""\n        opt = OptimWrapper.create(opt_func, lr, layer_groups, **kwargs)\n        opt.model_params, opt.master_params = get_master(\n            layer_groups, flat_master)\n        opt.flat_master = flat_master\n        opt.loss_scale = loss_scale\n        opt.model = model\n        #Changes the optimizer so that the optimization step is done in FP32.\n        # opt = self.learn.opt\n        mom, wd, beta = opt.mom, opt.wd, opt.beta\n        lrs = [lr for lr in opt._lr for _ in range(2)]\n        opt_params = [{\n            \'params\': mp,\n            \'lr\': lr\n        } for mp, lr in zip(opt.master_params, lrs)]\n        opt.opt = opt_func(opt_params)\n        opt.mom, opt.wd, opt.beta = mom, wd, beta\n        return opt\n\n    def step(self):\n        model_g2master_g(self.model_params, self.master_params,\n                         self.flat_master)\n        for group in self.master_params:\n            for param in group:\n                param.grad.div_(self.loss_scale)\n        super(FastAIMixedOptim, self).step()\n        self.model.zero_grad()\n        #Update the params from master to model.\n        master2model(self.model_params, self.master_params, self.flat_master)\n'"
alfred/dl/torch/train/learning_schedules.py,1,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n""""""PyTorch edition of TensorFlow learning schedule in tensorflow object\ndetection API. \n""""""\nimport numpy as np\nfrom torch.optim.optimizer import Optimizer\nclass _LRSchedulerStep(object):\n    def __init__(self, optimizer, last_step=-1):\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n        if last_step == -1:\n            for group in optimizer.param_groups:\n                group.setdefault(\'initial_lr\', group[\'lr\'])\n        else:\n            for i, group in enumerate(optimizer.param_groups):\n                if \'initial_lr\' not in group:\n                    raise KeyError(\n                        ""param \'initial_lr\' is not specified ""\n                        ""in param_groups[{}] when resuming an optimizer"".\n                        format(i))\n        self.base_lrs = list(\n            map(lambda group: group[\'initial_lr\'], optimizer.param_groups))\n        self.step(last_step + 1)\n        self.last_step = last_step\n\n    """"""\n    def get_lr(self):\n        raise NotImplementedError\n    """"""\n\n    def get_lr(self):\n        ret = [self._get_lr_per_group(base_lr) for base_lr in self.base_lrs]\n        return ret\n\n    def _get_lr_per_group(self, base_lr):\n        raise NotImplementedError\n\n    def step(self, step=None):\n        if step is None:\n            step = self.last_step + 1\n        self.last_step = step\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n\n\nclass Constant(_LRSchedulerStep):\n    def __init__(self, optimizer, last_step=-1):\n        super().__init__(optimizer, last_step)\n\n    def _get_lr_per_group(self, base_lr):\n        return base_lr\n\n\nclass ManualStepping(_LRSchedulerStep):\n    """"""Pytorch edition of manual_stepping in tensorflow.\n    DON\'T SUPPORT PARAM GROUPS.\n    """"""\n\n    def __init__(self, optimizer, boundaries, rates, last_step=-1):\n        self._boundaries = boundaries\n        self._num_boundaries = len(boundaries)\n        self._learning_rates = rates\n\n        if any([b < 0 for b in boundaries]) or any(\n            [not isinstance(b, int) for b in boundaries]):\n            raise ValueError(\'boundaries must be a list of positive integers\')\n        if any(\n            [bnext <= b for bnext, b in zip(boundaries[1:], boundaries[:-1])]):\n            raise ValueError(\n                \'Entries in boundaries must be strictly increasing.\')\n        if any([not isinstance(r, float) for r in rates]):\n            raise ValueError(\'Learning rates must be floats\')\n        if len(rates) != len(boundaries) + 1:\n            raise ValueError(\'Number of provided learning rates must exceed \'\n                             \'number of boundary points by exactly 1.\')\n        super().__init__(optimizer, last_step)\n\n    def _get_lr_per_group(self, base_lr):\n        step = self.last_step\n        ret = None\n        for i, bound in enumerate(self._boundaries):\n            if step > bound:\n                ret = self._learning_rates[i + 1]\n        if ret is not None:\n            return ret\n        return self._learning_rates[0]\n\n\nclass ExponentialDecayWithBurnin(_LRSchedulerStep):\n    """"""Pytorch edition of manual_stepping in tensorflow.\n    """"""\n\n    def __init__(self,\n                 optimizer,\n                 learning_rate_decay_steps,\n                 learning_rate_decay_factor,\n                 burnin_learning_rate,\n                 burnin_steps,\n                 last_step=-1):\n        self._decay_steps = learning_rate_decay_steps\n        self._decay_factor = learning_rate_decay_factor\n        self._burnin_learning_rate = burnin_learning_rate\n        self._burnin_steps = burnin_steps\n\n        super().__init__(optimizer, last_step)\n\n    def _get_lr_per_group(self, base_lr):\n        if self._burnin_learning_rate == 0:\n            burnin_learning_rate = base_lr\n        step = self.last_step\n        post_burnin_learning_rate = (base_lr * self._decay_factor ^\n                                     (step // self._decay_steps))\n        if step < self._burnin_steps:\n            return burnin_learning_rate\n        else:\n            return post_burnin_learning_rate\n\n\nclass ExponentialDecay(_LRSchedulerStep):\n    def __init__(self,\n                 optimizer,\n                 learning_rate_decay_steps,\n                 learning_rate_decay_factor,\n                 staircase=True,\n                 last_step=-1):\n        self._decay_steps = learning_rate_decay_steps\n        self._decay_factor = learning_rate_decay_factor\n        self._staircase = staircase\n\n        super().__init__(optimizer, last_step)\n\n    def _get_lr_per_group(self, base_lr):\n        step = self.last_step\n        if self._staircase:\n            post_burnin_learning_rate = base_lr * pow(self._decay_factor,\n                                         (step // self._decay_steps))\n        else:\n            post_burnin_learning_rate = base_lr * pow(self._decay_factor,\n                                         (step / self._decay_steps))\n\n        return post_burnin_learning_rate\n\n\nclass CosineDecayWithWarmup(_LRSchedulerStep):\n    def __init__(self,\n                 optimizer,\n                 total_steps,\n                 warmup_learning_rate,\n                 warmup_steps,\n                 last_step=-1):\n        if total_steps < warmup_steps:\n            raise ValueError(\'total_steps must be larger or equal to \'\n                             \'warmup_steps.\')\n        self._total_steps = total_steps\n        self._warmup_learning_rate = warmup_learning_rate\n        self._warmup_steps = warmup_steps\n\n        super().__init__(optimizer, last_step)\n\n    def _get_lr_per_group(self, base_lr):\n        if base_lr < self._warmup_learning_rate:\n            raise ValueError(\'learning_rate_base must be larger \'\n                             \'or equal to warmup_learning_rate.\')\n\n        step = self.last_step\n        learning_rate = 0.5 * base_lr * (\n            1 + np.cos(np.pi *\n                       (float(step) - self._warmup_steps\n                        ) / float(self._total_steps - self._warmup_steps)))\n        if self._warmup_steps > 0:\n            slope = (base_lr - self._warmup_learning_rate) / self._warmup_steps\n            pre_cosine_learning_rate = slope * float(\n                step) + self._warmup_learning_rate\n            if step < self._warmup_steps:\n                return pre_cosine_learning_rate\n            else:\n                return learning_rate\n'"
alfred/dl/torch/train/learning_schedules_fastai.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport numpy as np\nimport math\nfrom functools import partial\n\nclass LRSchedulerStep(object):\n    def __init__(self, fai_optimizer, total_step, lr_phases,\n                 mom_phases):\n        self.optimizer = fai_optimizer\n        self.total_step = total_step\n        self.lr_phases = []\n        \n        for i, (start, lambda_func) in enumerate(lr_phases):\n            if len(self.lr_phases) != 0:\n                assert self.lr_phases[-1][0] < int(start * total_step)\n            if isinstance(lambda_func, str):\n                lambda_func = eval(lambda_func)\n            if i < len(lr_phases) - 1:\n                self.lr_phases.append((int(start * total_step), int(lr_phases[i + 1][0] * total_step), lambda_func))\n            else:\n                self.lr_phases.append((int(start * total_step), total_step, lambda_func))\n        assert self.lr_phases[0][0] == 0\n        self.mom_phases = []\n        \n        for i, (start, lambda_func) in enumerate(mom_phases):\n            if len(self.mom_phases) != 0:\n                assert self.mom_phases[-1][0] < int(start * total_step)\n            if isinstance(lambda_func, str):\n                lambda_func = eval(lambda_func)\n            if i < len(mom_phases) - 1:\n                self.mom_phases.append((int(start * total_step), int(mom_phases[i + 1][0] * total_step), lambda_func))\n            else:\n                self.mom_phases.append((int(start * total_step), total_step, lambda_func))\n        if len(mom_phases) > 0:\n            assert self.mom_phases[0][0] == 0\n\n    def step(self, step):\n        lrs = []\n        moms = []\n        for start, end, func in self.lr_phases:\n            if step >= start:\n                lrs.append(func((step - start) / (end - start)))\n        if len(lrs) > 0:\n            self.optimizer.lr = lrs[-1]\n        for start, end, func in self.mom_phases:\n            if step >= start:\n                moms.append(func((step - start) / (end - start)))\n                self.optimizer.mom = func((step - start) / (end - start))\n        if len(moms) > 0:\n            self.optimizer.mom = moms[-1]\n\ndef annealing_cos(start, end, pct):\n    # print(pct, start, end)\n    ""Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.""\n    cos_out = np.cos(np.pi * pct) + 1\n    return end + (start - end) / 2 * cos_out\n\nclass OneCycle(LRSchedulerStep):\n    def __init__(self, fai_optimizer, total_step, lr_max, moms, div_factor,\n                 pct_start):\n        self.lr_max = lr_max\n        self.moms = moms\n        self.div_factor = div_factor\n        self.pct_start = pct_start\n        a1 = int(total_step * self.pct_start)\n        a2 = total_step - a1\n        low_lr = self.lr_max / self.div_factor\n        lr_phases = ((0, partial(annealing_cos, low_lr, self.lr_max)),\n                     (self.pct_start,\n                      partial(annealing_cos, self.lr_max, low_lr / 1e4)))\n        mom_phases = ((0, partial(annealing_cos, *self.moms)),\n                      (self.pct_start, partial(annealing_cos,\n                                               *self.moms[::-1])))\n        fai_optimizer.lr, fai_optimizer.mom = low_lr, self.moms[0]\n        super().__init__(fai_optimizer, total_step, lr_phases, mom_phases)\n\nclass ExponentialDecay(LRSchedulerStep):\n    def __init__(self,\n                 fai_optimizer, \n                 total_step,\n                 initial_learning_rate,\n                 decay_length, \n                 decay_factor,\n                 staircase=True):\n        """"""\n        Args:\n            decay_length: must in (0, 1)\n        """"""\n        assert decay_length > 0\n        assert decay_length < 1\n        self._decay_steps_unified = decay_length\n        self._decay_factor = decay_factor\n        self._staircase = staircase\n        step = 0\n        stage = 1\n        lr_phases = []\n        if staircase:\n            while step <= total_step:\n                func = lambda p, _d=initial_learning_rate * stage: _d\n                lr_phases.append((step / total_step, func))\n                stage *= decay_factor\n                step += int(decay_length * total_step)\n        else:\n            func = lambda p: pow(decay_factor, (p / decay_length))\n            lr_phases.append((0, func))\n        super().__init__(fai_optimizer, total_step, lr_phases, [])\n\nclass ManualStepping(LRSchedulerStep):\n    def __init__(self,\n                 fai_optimizer, \n                 total_step,\n                 boundaries,\n                 rates):\n        assert all([b > 0 and b < 1 for b in boundaries])\n        assert len(boundaries) + 1 == len(rates)\n        boundaries.insert(0, 0.0)\n        lr_phases = []\n        for start, rate in zip(boundaries, rates):\n            func = lambda p, _d=rate: _d\n            lr_phases.append((start, func))\n        super().__init__(fai_optimizer, total_step, lr_phases, [])\n\n\nclass FakeOptim:\n    def __init__(self):\n        self.lr = 0\n        self.mom = 0\n\nif __name__ == ""__main__"":\n    import matplotlib.pyplot as plt\n    opt = FakeOptim()# 3e-3, wd=0.4, div_factor=10\n    # schd = OneCycle(opt, 100, 3e-3, (0.95, 0.85), 10.0, 0.4)\n    schd = ExponentialDecay(opt, 100, 3e-4, 0.1, 0.8, staircase=True)\n    schd = ManualStepping(opt, 100, [0.8, 0.9], [0.001, 0.0001, 0.00005])\n    lrs = []\n    moms = []\n    for i in range(100):\n        schd.step(i)\n        lrs.append(opt.lr)\n        moms.append(opt.mom)\n    \n    plt.plot(lrs)\n    # plt.plot(moms)\n    # plt.show()\n    # plt.plot(moms)\n    plt.show()\n'"
alfred/dl/torch/train/optim.py,8,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nfrom collections import defaultdict, Iterable\n\nimport torch\nfrom copy import deepcopy\nfrom itertools import chain\nfrom torch.autograd import Variable\n\nrequired = object()\n\ndef param_fp32_copy(params):\n    param_copy = [\n        param.clone().type(torch.cuda.FloatTensor).detach() for param in params\n    ]\n    for param in param_copy:\n        param.requires_grad = True\n    return param_copy\n\ndef set_grad(params, params_with_grad, scale=1.0):\n    for param, param_w_grad in zip(params, params_with_grad):\n        if param.grad is None:\n            param.grad = torch.nn.Parameter(\n                param.data.new().resize_(*param.data.size()))\n        grad = param_w_grad.grad.data\n        if scale is not None:\n            grad /= scale\n        if torch.isnan(grad).any() or torch.isinf(grad).any():\n            return True # invalid grad\n        param.grad.data.copy_(grad)\n    return False\n\nclass MixedPrecisionWrapper(object):\n    """"""mixed precision optimizer wrapper.\n    Arguments:\n        optimizer (torch.optim.Optimizer): an instance of \n            :class:`torch.optim.Optimizer`\n        scale: (float): a scalar for grad scale.\n        auto_scale: (bool): whether enable auto scale.\n            The algorihm of auto scale is discribled in \n            http://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n    """"""\n\n    def __init__(self,\n                 optimizer,\n                 scale=None,\n                 auto_scale=True,\n                 inc_factor=2.0,\n                 dec_factor=0.5,\n                 num_iters_be_stable=500):\n        if not isinstance(optimizer, torch.optim.Optimizer):\n            raise ValueError(""must provide a torch.optim.Optimizer"")\n        self.optimizer = optimizer\n        if hasattr(self.optimizer, \'name\'):\n            self.name = self.optimizer.name  # for ckpt system\n        param_groups_copy = []\n        for i, group in enumerate(optimizer.param_groups):\n            group_copy = {n: v for n, v in group.items() if n != \'params\'}\n            group_copy[\'params\'] = param_fp32_copy(group[\'params\'])\n            param_groups_copy.append(group_copy)\n\n        # switch param_groups, may be dangerous\n        self.param_groups = optimizer.param_groups\n        optimizer.param_groups = param_groups_copy\n        self.grad_scale = scale\n        self.auto_scale = auto_scale\n        self.inc_factor = inc_factor\n        self.dec_factor = dec_factor\n        self.stable_iter_count = 0\n        self.num_iters_be_stable = num_iters_be_stable\n\n    def __getstate__(self):\n        return self.optimizer.__getstate__()\n\n    def __setstate__(self, state):\n        return self.optimizer.__setstate__(state)\n\n    def __repr__(self):\n        return self.optimizer.__repr__()\n\n    def state_dict(self):\n        return self.optimizer.state_dict()\n\n    def load_state_dict(self, state_dict):\n        return self.optimizer.load_state_dict(state_dict)\n\n    def zero_grad(self):\n        return self.optimizer.zero_grad()\n\n    def step(self, closure=None):\n        for g, g_copy in zip(self.param_groups, self.optimizer.param_groups):\n            invalid = set_grad(g_copy[\'params\'], g[\'params\'], self.grad_scale)\n            if invalid:\n                if self.grad_scale is None or self.auto_scale is False:\n                    raise ValueError(""nan/inf detected but auto_scale disabled."")\n                self.grad_scale *= self.dec_factor\n                print(\'scale decay to {}\'.format(self.grad_scale))\n                return\n        if self.auto_scale is True:\n            self.stable_iter_count += 1\n            if self.stable_iter_count > self.num_iters_be_stable:\n                if self.grad_scale is not None:\n                    self.grad_scale *= self.inc_factor\n                self.stable_iter_count = 0\n\n        if closure is None:\n            self.optimizer.step()\n        else:\n            self.optimizer.step(closure)\n        for g, g_copy in zip(self.param_groups, self.optimizer.param_groups):\n            for p_copy, p in zip(g_copy[\'params\'], g[\'params\']):\n                p.data.copy_(p_copy.data)\n\n'"
alfred/dl/torch/nn/modules/__init__.py,0,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n'"
alfred/dl/torch/nn/modules/common.py,3,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport sys\nfrom collections import OrderedDict\n\nimport torch\nfrom torch.nn import functional as F\n\nclass Empty(torch.nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(Empty, self).__init__()\n\n    def forward(self, *args, **kwargs):\n        if len(args) == 1:\n            return args[0]\n        elif len(args) == 0:\n            return None\n        return args\n\nclass Sequential(torch.nn.Module):\n    r""""""A sequential container.\n    Modules will be added to it in the order they are passed in the constructor.\n    Alternatively, an ordered dict of modules can also be passed in.\n\n    To make it easier to understand, given is a small example::\n\n        # Example of using Sequential\n        model = Sequential(\n                  nn.Conv2d(1,20,5),\n                  nn.ReLU(),\n                  nn.Conv2d(20,64,5),\n                  nn.ReLU()\n                )\n\n        # Example of using Sequential with OrderedDict\n        model = Sequential(OrderedDict([\n                  (\'conv1\', nn.Conv2d(1,20,5)),\n                  (\'relu1\', nn.ReLU()),\n                  (\'conv2\', nn.Conv2d(20,64,5)),\n                  (\'relu2\', nn.ReLU())\n                ]))\n        \n        # Example of using Sequential with kwargs(python 3.6+)\n        model = Sequential(\n                  conv1=nn.Conv2d(1,20,5),\n                  relu1=nn.ReLU(),\n                  conv2=nn.Conv2d(20,64,5),\n                  relu2=nn.ReLU()\n                )\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(Sequential, self).__init__()\n        if len(args) == 1 and isinstance(args[0], OrderedDict):\n            for key, module in args[0].items():\n                self.add_module(key, module)\n        else:\n            for idx, module in enumerate(args):\n                self.add_module(str(idx), module)\n        for name, module in kwargs.items():\n            if sys.version_info < (3, 6):\n                raise ValueError(""kwargs only supported in py36+"")\n            if name in self._modules:\n                raise ValueError(""name exists."")\n            self.add_module(name, module)\n\n    def __getitem__(self, idx):\n        if not (-len(self) <= idx < len(self)):\n            raise IndexError(\'index {} is out of range\'.format(idx))\n        if idx < 0:\n            idx += len(self)\n        it = iter(self._modules.values())\n        for i in range(idx):\n            next(it)\n        return next(it)\n\n    def __len__(self):\n        return len(self._modules)\n\n    def add(self, module, name=None):\n        if name is None:\n            name = str(len(self._modules))\n            if name in self._modules:\n                raise KeyError(""name exists"")\n        self.add_module(name, module)\n\n    def forward(self, input):\n        # i = 0\n        for module in self._modules.values():\n            # print(i)\n            input = module(input)\n            # i += 1\n        return input'"
alfred/dl/torch/nn/modules/normalization.py,1,"b'#\n# Copyright (c) 2020 JinTian.\n#\n# This file is part of alfred\n# (see http://jinfagang.github.io).\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# ""License""); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport torch\n\n\nclass GroupNorm(torch.nn.GroupNorm):\n    def __init__(self, num_channels, num_groups, eps=1e-5, affine=True):\n        super().__init__(\n            num_groups=num_groups,\n            num_channels=num_channels,\n            eps=eps,\n            affine=affine)\n'"
