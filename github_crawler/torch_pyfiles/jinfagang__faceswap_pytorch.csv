file_path,api_count,code
predict_64x64.py,5,"b'""""""\nconvert  a face to another person\n\n""""""\nfrom models.swapnet import SwapNet\nimport torch\nfrom alfred.dl.torch.common import device\nimport cv2\nimport numpy as np\nfrom dataset.training_data import random_warp\nfrom utils.umeyama import umeyama\n\nmean_value = np.array([0.03321508, 0.05035182, 0.02038819])\n\n\ndef process_img(ori_img):\n    img = cv2.resize(ori_img, (256, 256))\n    range_ = np.linspace( 128-80, 128+80, 5 )\n    mapx = np.broadcast_to( range_, (5,5) )\n    mapy = mapx.T\n\n    # warp image like in the training\n    mapx = mapx + np.random.normal( size=(5,5), scale=5 )\n    mapy = mapy + np.random.normal( size=(5,5), scale=5 )\n    interp_mapx = cv2.resize(mapx, (80, 80))[8:72, 8:72].astype(\'float32\')\n    interp_mapy = cv2.resize(mapy, (80, 80))[8:72, 8:72].astype(\'float32\')\n    warped_image = cv2.remap(img, interp_mapx, interp_mapy, cv2.INTER_LINEAR)\n    return warped_image\n\n\ndef load_img():\n    a = \'images/34600_test_A_target.png\'\n    img = cv2.imread(a) / 255.\n    return img\n\n\ndef predict():\n    # convert trump to cage\n    # img_f = \'data/trump/51834796.jpg\'\n    # img_f = \'data/trump/494045244.jpg\'\n    # NOTE: using face extracted (not original image)\n    img_f = \'data/trump/464669134_face_0.png\'\n\n    ori_img = cv2.imread(img_f)\n    img = cv2.resize(ori_img, (64, 64)) / 255.\n    img = np.rot90(img)\n    # img = load_img()\n    in_img = np.array(img, dtype=np.float).transpose(2, 1, 0)\n\n    # normalize img\n    in_img = torch.Tensor(in_img).to(device).unsqueeze(0)\n    model = SwapNet().to(device)\n    if torch.cuda.is_available():\n        checkpoint = torch.load(\'checkpoint/faceswap_trump_cage_64x64.pth\')\n    else:\n        checkpoint = torch.load(\'checkpoint/faceswap_trump_cage_64x64.pth\', map_location={\'cuda:0\': \'cpu\'})\n    model.load_state_dict(checkpoint[\'state\'])\n    model.eval()\n    print(\'model loaded.\')\n\n    out = model.forward(in_img, select=\'B\')\n    out = np.clip(out.detach().cpu().numpy()[0]*255, 0, 255).astype(\'uint8\').transpose(2, 1, 0)\n\n    cv2.imshow(\'original image\', ori_img)\n    cv2.imshow(\'network input image\', img)\n    cv2.imshow(\'result image\', np.rot90(out, axes=(1, 0)))\n    cv2.waitKey(0)\n\n\nif __name__ == \'__main__\':\n    predict()\n'"
tests.py,2,"b""from models.swapnet import SwapNet\nfrom models.swapnet_128 import SwapNet128\nfrom utils.model_summary import summary\nfrom alfred.dl.torch.common import device\nfrom dataset.face_pair_dataset import random_warp_128\nfrom dataset.training_data import  random_transform, random_transform_args\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport torch\nfrom utils.umeyama import umeyama\n\n# model = SwapNet().to(device)\n# summary(model, input_size=(3, 64, 64))\n\n# def random_warp(image):\n#     assert image.shape == (256, 256, 3)\n#     range_ = np.linspace(128 - 120, 128 + 120, 5)\n#     mapx = np.broadcast_to(range_, (5, 5))\n#     mapy = mapx.T\n#     mapx = mapx + np.random.normal(size=(5, 5), scale=5)\n#     mapy = mapy + np.random.normal(size=(5, 5), scale=5)\n\n#     interp_mapx = cv2.resize(mapx, (80, 80))[8:72, 8:72].astype('float32')\n#     interp_mapy = cv2.resize(mapy, (80, 80))[8:72, 8:72].astype('float32')\n\n#     # just crop the image, remove the top left bottom right 8 pixels (in order to get the pure face)\n#     warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)\n\n#     src_points = np.stack([mapx.ravel(), mapy.ravel()], axis=-1)\n#     dst_points = np.mgrid[0:65:16, 0:65:16].T.reshape(-1, 2)\n#     mat = umeyama(src_points, dst_points, True)[0:2]\n#     target_image = cv2.warpAffine(image, mat, (64, 64))\n#     return warped_image, target_image\n\n# model = SwapNet128().to(device)\n# summary(model, input_size=(3, 128, 128))\n\n# a = Image.open('data/trump_cage/cage/2455911_face_0.png')\n# a = a.resize((256, 256), Image.ANTIALIAS)\n# a = random_transform(np.array(a), **random_transform_args)\n# warped_img, target_img = random_warp_128(np.array(a))\n\n# t = torch.from_numpy(target_img.transpose(2, 0, 1) / 255.).to(device)\n# b = t.detach().cpu().numpy().transpose((2, 1, 0))*255\n# print(b.shape)\n\n# cv2.imshow('rr', np.array(a))\n# cv2.imshow('warped image', np.array(warped_img))\n# cv2.imshow('target image', np.array(target_img))\n# cv2.imshow('bbbbbbbbb', b)\n# cv2.waitKey(0)"""
train_fbb_gal_128x128.py,11,"b'""""""\r\nCopyright StrangeAI Authors @2019\r\n\r\n\r\nAs the network without linear connect layer\r\nthe feature are not compressed, so the encoder are weak\r\nit consist to many informations, and decoder can not using the abstract \r\ninformation to construct a new image\r\n\r\n""""""\r\nfrom __future__ import print_function\r\nimport argparse\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nimport torch.utils.data\r\nfrom torch import nn, optim\r\nfrom torch.autograd import Variable\r\nfrom torch.nn import functional as F\r\nimport torch.backends.cudnn as cudnn\r\nfrom utils.util import get_image_paths, load_images, stack_images\r\nfrom dataset.training_data import get_training_data\r\nfrom alfred.dl.torch.common import device\r\nfrom shutil import copyfile\r\ntry:\r\n    from models.swapnet_128 import SwapNet128, toTensor, var_to_np\r\nexcept Exception:\r\n    print(\'can not import swapnet128, if you need high resolution face swap, \'\r\n    \'you can download from http://luoli.ai (you can afford a VIP membership to get all other codes)\')\r\nfrom loguru import logger\r\nfrom dataset.face_pair_dataset import FacePairDataset128x128\r\nfrom torchvision import transforms\r\nfrom torch.utils.data import DataLoader\r\nfrom alfred.utils.log import init_logger\r\n\r\ninit_logger()\r\n\r\nbatch_size = 32\r\nepochs = 100000\r\nsave_per_epoch = 300\r\n\r\na_dir = \'./data/galgadot_fbb/fanbingbing_faces\'\r\nb_dir = \'./data/galgadot_fbb/galgadot_faces\'\r\n# we start to train on bigger size\r\ndataset_name = \'galgadot_fbb\'\r\ntarget_size = 128\r\nlog_img_dir = \'./checkpoint/results_{}_{}x{}\'.format(dataset_name, target_size, target_size)\r\nlog_model_dir = \'./checkpoint/{}_{}x{}\'.format(dataset_name,\r\n    target_size, target_size)\r\ncheck_point_save_path = os.path.join(\r\n    log_model_dir, \'faceswap_{}_{}x{}.pth\'.format(dataset_name, target_size, target_size))\r\n\r\n\r\ndef main():\r\n    os.makedirs(log_img_dir, exist_ok=True)\r\n    os.makedirs(log_model_dir, exist_ok=True)\r\n    logger.info(""loading datasets"")\r\n\r\n    transform = transforms.Compose([\r\n        # transforms.Resize((target_size, target_size)),\r\n        transforms.RandomHorizontalFlip(),\r\n        # transforms.RandomVerticalFlip(),\r\n        # transforms.ToTensor(),\r\n    ])\r\n    ds = FacePairDataset128x128(a_dir=a_dir, b_dir=b_dir,\r\n                         target_size=target_size, transform=transform)\r\n    dataloader = DataLoader(ds, batch_size, shuffle=True)\r\n\r\n    model = SwapNet128()\r\n    model.to(device)\r\n    start_epoch = 0\r\n    logger.info(\'try resume from checkpoint\')\r\n    try:\r\n        if torch.cuda.is_available():\r\n            checkpoint = torch.load(check_point_save_path)\r\n        else:\r\n            checkpoint = torch.load(\r\n                check_point_save_path, map_location={\'cuda:0\': \'cpu\'})\r\n        model.load_state_dict(checkpoint[\'state\'])\r\n        start_epoch = checkpoint[\'epoch\']\r\n        logger.info(\'checkpoint loaded.\')\r\n    except FileNotFoundError:\r\n        print(\'Can\\\'t found {}\'.format(check_point_save_path))\r\n\r\n    criterion = nn.L1Loss()\r\n    optimizer_1 = optim.Adam([{\'params\': model.encoder.parameters()},\r\n                              {\'params\': model.decoder_a.parameters()}], lr=5e-5, betas=(0.5, 0.999))\r\n    optimizer_2 = optim.Adam([{\'params\': model.encoder.parameters()},\r\n                              {\'params\': model.decoder_b.parameters()}], lr=5e-5, betas=(0.5, 0.999))\r\n\r\n    logger.info(\'Start training, from epoch {} \'.format(start_epoch))\r\n    try:\r\n        for epoch in range(start_epoch, epochs):\r\n            iter = 0\r\n            for data in dataloader:\r\n                iter += 1\r\n                img_a_target, img_a_input, img_b_target, img_b_input = data\r\n                img_a_target = img_a_target.to(device)\r\n                img_a_input = img_a_input.to(device)\r\n                img_b_target = img_b_target.to(device)\r\n                img_b_input = img_b_input.to(device)\r\n                # print(img_a.size())\r\n                # print(img_b.size())\r\n\r\n                optimizer_1.zero_grad()\r\n                optimizer_2.zero_grad()\r\n                predict_a = model(img_a_input, to=\'a\')\r\n                predict_b = model(img_b_input, to=\'b\')\r\n                loss1 = criterion(predict_a, img_a_target)\r\n                loss2 = criterion(predict_b, img_b_target)\r\n                loss1.backward()\r\n                loss2.backward()\r\n                optimizer_1.step()\r\n                optimizer_2.step()\r\n                logger.info(\'Epoch: {}, iter: {}, lossA: {}, lossB: {}\'.format(\r\n                    epoch, iter, loss1.item(), loss2.item()))\r\n                if epoch % save_per_epoch == 0 and epoch != 0:\r\n                    logger.info(\'Saving models...\')\r\n                    state = {\r\n                        \'state\': model.state_dict(),\r\n                        \'epoch\': epoch\r\n                    }\r\n                    torch.save(state, os.path.join(os.path.dirname(\r\n                        check_point_save_path), \'faceswap_{}_128x128_{}.pth\'.format(dataset_name, epoch)))\r\n                    copyfile(os.path.join(os.path.dirname(check_point_save_path), \'faceswap_{}_128x128_{}.pth\'.format(dataset_name, epoch)),\r\n                                    check_point_save_path)\r\n                if epoch % 10 == 0 and epoch != 0 and iter == 1:\r\n                    img_a_original = np.array(img_a_target.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    img_b_original = np.array(img_b_target.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    a_predict_a = np.array(predict_a.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    b_predict_b = np.array(predict_b.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n\r\n                    a_predict_b = model(img_a_input, to=\'b\')\r\n                    b_predict_a = model(img_b_input, to=\'a\')\r\n                    a_predict_b = np.array(a_predict_b.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    b_predict_a = np.array(b_predict_a.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_0.png\'.format(epoch)), cv2.cvtColor(img_a_original, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_3.png\'.format(epoch)), cv2.cvtColor(img_b_original, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_1.png\'.format(epoch)), cv2.cvtColor(a_predict_a, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_4.png\'.format(epoch)), cv2.cvtColor(b_predict_b, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_2.png\'.format(epoch)), cv2.cvtColor(a_predict_b, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_5.png\'.format(epoch)), cv2.cvtColor(b_predict_a, cv2.COLOR_BGR2RGB))\r\n                    logger.info(\'Record a result\')\r\n    except KeyboardInterrupt:\r\n        logger.info(\'try saving models...\')\r\n        state = {\r\n            \'state\': model.state_dict(),\r\n            \'epoch\': epoch\r\n        }\r\n        torch.save(state, os.path.join(os.path.dirname(check_point_save_path), \'faceswap_{}_128x128_{}.pth\'.format(dataset_name, epoch)))\r\n        copyfile(os.path.join(os.path.dirname(check_point_save_path), \'faceswap_{}_128x128_{}.pth\'.format(dataset_name, epoch)),\r\n                        check_point_save_path)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
train_fbb_gal_64x64.py,11,"b'""""""\r\nCopyright StrangeAI Authors @2019\r\n\r\noriginal forked from deepfakes repo\r\nedit and promoted by StrangeAI authors\r\n\r\n""""""\r\n\r\nfrom __future__ import print_function\r\nimport argparse\r\nimport os\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\n\r\nimport torch.utils.data\r\nfrom torch import nn, optim\r\nfrom torch.autograd import Variable\r\nfrom torch.nn import functional as F\r\nimport torch.backends.cudnn as cudnn\r\nfrom torch.utils.data import DataLoader\r\nfrom models.swapnet import SwapNet, toTensor, var_to_np\r\nfrom utils.util import get_image_paths, load_images, stack_images\r\nfrom dataset.training_data import get_training_data\r\nfrom alfred.dl.torch.common import device\r\nfrom shutil import copyfile\r\nfrom loguru import logger\r\nfrom dataset.face_pair_dataset import FacePairDataset, FacePairDataset64x64\r\nfrom torchvision import transforms\r\nimport sys\r\n\r\nlogger.remove()  # Remove the pre-configured handler\r\nlogger.start(sys.stderr, format=""<lvl>{level}</lvl> {time:MM-DD HH:mm:ss} {file}:{line} - {message}"")\r\n\r\nbatch_size = 64\r\nepochs = 100000\r\nsave_per_epoch = 300\r\n\r\na_dir = \'./data/galgadot_fbb/fanbingbing_faces\'\r\nb_dir = \'./data/galgadot_fbb/galgadot_faces\'\r\n# we start to train on bigger size\r\ntarget_size = 64\r\ndataset_name = \'galgadot_fbb\'\r\nlog_img_dir = \'./checkpoint/results_{}_{}x{}\'.format(dataset_name, target_size, target_size)\r\nlog_model_dir = \'./checkpoint/{}_{}x{}\'.format(dataset_name,\r\n    target_size, target_size)\r\ncheck_point_save_path = os.path.join(\r\n    log_model_dir, \'faceswap_{}_{}x{}.pth\'.format(dataset_name, target_size, target_size))\r\n\r\n\r\ndef main():\r\n    os.makedirs(log_img_dir, exist_ok=True)\r\n    os.makedirs(log_model_dir, exist_ok=True)\r\n\r\n    transform = transforms.Compose([\r\n        # transforms.Resize((target_size, target_size)),\r\n        transforms.RandomHorizontalFlip(),\r\n        # transforms.RandomVerticalFlip(),\r\n        # transforms.ToTensor(),\r\n    ])\r\n    ds = FacePairDataset64x64(a_dir=a_dir, b_dir=b_dir,\r\n                         target_size=target_size, transform=transform)\r\n    dataloader = DataLoader(ds, batch_size, shuffle=True)\r\n\r\n    model = SwapNet()\r\n    model.to(device)\r\n    start_epoch = 0\r\n    logger.info(\'try resume from checkpoint\')\r\n    if os.path.isdir(\'checkpoint\'):\r\n        try:\r\n            if torch.cuda.is_available():\r\n                checkpoint = torch.load(check_point_save_path)\r\n            else:\r\n                checkpoint = torch.load(\r\n                    check_point_save_path, map_location={\'cuda:0\': \'cpu\'})\r\n            model.load_state_dict(checkpoint[\'state\'])\r\n            start_epoch = checkpoint[\'epoch\']\r\n            logger.info(\'checkpoint loaded.\')\r\n        except FileNotFoundError:\r\n            print(\'Can\\\'t found faceswap_trump_cage.pth\')\r\n\r\n    criterion = nn.L1Loss()\r\n    optimizer_1 = optim.Adam([{\'params\': model.encoder.parameters()},\r\n                              {\'params\': model.decoder_A.parameters()}], lr=5e-5, betas=(0.5, 0.999))\r\n    optimizer_2 = optim.Adam([{\'params\': model.encoder.parameters()},\r\n                              {\'params\': model.decoder_B.parameters()}], lr=5e-5, betas=(0.5, 0.999))\r\n\r\n    logger.info(\'Start training, from epoch {} \'.format(start_epoch))\r\n    try:\r\n        for epoch in range(start_epoch, epochs):\r\n            iter = 0\r\n            for data in dataloader:\r\n                iter += 1\r\n                img_a_target, img_a_input, img_b_target, img_b_input = data\r\n                img_a_target = img_a_target.to(device)\r\n                img_a_input = img_a_input.to(device)\r\n                img_b_target = img_b_target.to(device)\r\n                img_b_input = img_b_input.to(device)\r\n                # print(img_a.size())\r\n                # print(img_b.size())\r\n\r\n                optimizer_1.zero_grad()\r\n                optimizer_2.zero_grad()\r\n                predict_a = model(img_a_input, select=\'A\')\r\n                predict_b = model(img_b_input, select=\'B\')\r\n                loss1 = criterion(predict_a, img_a_target)\r\n                loss2 = criterion(predict_b, img_b_target)\r\n                loss1.backward()\r\n                loss2.backward()\r\n                optimizer_1.step()\r\n                optimizer_2.step()\r\n                logger.info(\'Epoch: {}, iter: {}, lossA: {}, lossB: {}\'.format(\r\n                    epoch, iter, loss1.item(), loss2.item()))\r\n                if epoch % save_per_epoch == 0 and epoch != 0:\r\n                    logger.info(\'Saving models...\')\r\n                    state = {\r\n                        \'state\': model.state_dict(),\r\n                        \'epoch\': epoch\r\n                    }\r\n                    torch.save(state, os.path.join(os.path.dirname(\r\n                        check_point_save_path), \'faceswap_trump_cage_128x128_{}.pth\'.format(epoch)))\r\n                    copyfile(os.path.join(os.path.dirname(check_point_save_path), \'faceswap_trump_cage_128x128_{}.pth\'.format(epoch)),\r\n                            check_point_save_path)\r\n                if epoch % 10 == 0 and epoch != 0 and iter == 1:\r\n                    img_a_original = np.array(img_a_target.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    img_b_original = np.array(img_b_target.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    a_predict_a = np.array(predict_a.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    b_predict_b = np.array(predict_b.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n\r\n                    a_predict_b = model(img_a_input, select=\'B\')\r\n                    b_predict_a = model(img_b_input, select=\'A\')\r\n                    a_predict_b = np.array(a_predict_b.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    b_predict_a = np.array(b_predict_a.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_0.png\'.format(epoch)), cv2.cvtColor(img_a_original, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_3.png\'.format(epoch)), cv2.cvtColor(img_b_original, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_1.png\'.format(epoch)), cv2.cvtColor(a_predict_a, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_4.png\'.format(epoch)), cv2.cvtColor(b_predict_b, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_2.png\'.format(epoch)), cv2.cvtColor(a_predict_b, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_5.png\'.format(epoch)), cv2.cvtColor(b_predict_a, cv2.COLOR_BGR2RGB))\r\n                    logger.info(\'Record a result\')\r\n    except KeyboardInterrupt:\r\n        logger.warning(\'try saving models...do not interrupt\')\r\n        state = {\r\n            \'state\': model.state_dict(),\r\n            \'epoch\': epoch\r\n        }\r\n        torch.save(state, os.path.join(os.path.dirname(\r\n            check_point_save_path), \'faceswap_trump_cage_256x256_{}.pth\'.format(epoch)))\r\n        copyfile(os.path.join(os.path.dirname(check_point_save_path), \'faceswap_trump_cage_256x256_{}.pth\'.format(epoch)),\r\n                    check_point_save_path)\r\n\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
train_trump_cage_128x128.py,11,"b'""""""\r\nCopyright StrangeAI Authors @2019\r\n\r\n\r\nAs the network without linear connect layer\r\nthe feature are not compressed, so the encoder are weak\r\nit consist to many informations, and decoder can not using the abstract \r\ninformation to construct a new image\r\n\r\n""""""\r\nfrom __future__ import print_function\r\nimport argparse\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nimport torch.utils.data\r\nfrom torch import nn, optim\r\nfrom torch.autograd import Variable\r\nfrom torch.nn import functional as F\r\nimport torch.backends.cudnn as cudnn\r\nfrom utils.util import get_image_paths, load_images, stack_images\r\nfrom dataset.training_data import get_training_data\r\nfrom alfred.dl.torch.common import device\r\nfrom shutil import copyfile\r\ntry:\r\n    from models.swapnet_128 import SwapNet128, toTensor, var_to_np\r\nexcept Exception:\r\n    print(\'can not import swapnet128, if you need high resolution face swap, \'\r\n    \'you can download from http://luoli.ai (you can afford a VIP membership to get all other codes)\')\r\nfrom loguru import logger\r\nfrom dataset.face_pair_dataset import FacePairDataset128x128\r\nfrom torchvision import transforms\r\nfrom torch.utils.data import DataLoader\r\nfrom alfred.utils.log import init_logger\r\n\r\ninit_logger()\r\n\r\nbatch_size = 32\r\nepochs = 100000\r\nsave_per_epoch = 300\r\n\r\na_dir = \'./data/trump_cage/trump\'\r\nb_dir = \'./data/trump_cage/cage\'\r\ndataset_name = \'trump_cage\'\r\n# we start to train on bigger size\r\ntarget_size = 128\r\nlog_img_dir = \'./checkpoint/results_{}_{}x{}\'.format(dataset_name, target_size, target_size)\r\nlog_model_dir = \'./checkpoint/{}_{}x{}\'.format(dataset_name,\r\n    target_size, target_size)\r\ncheck_point_save_path = os.path.join(\r\n    log_model_dir, \'faceswap_{}_{}x{}.pth\'.format(dataset_name, target_size, target_size))\r\n\r\n\r\ndef main():\r\n    os.makedirs(log_img_dir, exist_ok=True)\r\n    os.makedirs(log_model_dir, exist_ok=True)\r\n    logger.info(""loading datasets"")\r\n\r\n    transform = transforms.Compose([\r\n        # transforms.Resize((target_size, target_size)),\r\n        transforms.RandomHorizontalFlip(),\r\n        # transforms.RandomVerticalFlip(),\r\n        # transforms.ToTensor(),\r\n    ])\r\n    ds = FacePairDataset128x128(a_dir=a_dir, b_dir=b_dir,\r\n                         target_size=target_size, transform=transform)\r\n    dataloader = DataLoader(ds, batch_size, shuffle=True)\r\n\r\n    model = SwapNet128()\r\n    model.to(device)\r\n    start_epoch = 0\r\n    logger.info(\'try resume from checkpoint\')\r\n    try:\r\n        if torch.cuda.is_available():\r\n            checkpoint = torch.load(check_point_save_path)\r\n        else:\r\n            checkpoint = torch.load(\r\n                check_point_save_path, map_location={\'cuda:0\': \'cpu\'})\r\n        model.load_state_dict(checkpoint[\'state\'])\r\n        start_epoch = checkpoint[\'epoch\']\r\n        logger.info(\'checkpoint loaded.\')\r\n    except FileNotFoundError:\r\n        print(\'Can\\\'t found {}\'.format(check_point_save_path))\r\n\r\n    criterion = nn.L1Loss()\r\n    optimizer_1 = optim.Adam([{\'params\': model.encoder.parameters()},\r\n                              {\'params\': model.decoder_a.parameters()}], lr=5e-5, betas=(0.5, 0.999))\r\n    optimizer_2 = optim.Adam([{\'params\': model.encoder.parameters()},\r\n                              {\'params\': model.decoder_b.parameters()}], lr=5e-5, betas=(0.5, 0.999))\r\n\r\n    logger.info(\'Start training, from epoch {} \'.format(start_epoch))\r\n    try:\r\n        for epoch in range(start_epoch, epochs):\r\n            iter = 0\r\n            for data in dataloader:\r\n                iter += 1\r\n                img_a_target, img_a_input, img_b_target, img_b_input = data\r\n                img_a_target = img_a_target.to(device)\r\n                img_a_input = img_a_input.to(device)\r\n                img_b_target = img_b_target.to(device)\r\n                img_b_input = img_b_input.to(device)\r\n                # print(img_a.size())\r\n                # print(img_b.size())\r\n\r\n                optimizer_1.zero_grad()\r\n                optimizer_2.zero_grad()\r\n                predict_a = model(img_a_input, to=\'a\')\r\n                predict_b = model(img_b_input, to=\'b\')\r\n                loss1 = criterion(predict_a, img_a_target)\r\n                loss2 = criterion(predict_b, img_b_target)\r\n                loss1.backward()\r\n                loss2.backward()\r\n                optimizer_1.step()\r\n                optimizer_2.step()\r\n                logger.info(\'Epoch: {}, iter: {}, lossA: {}, lossB: {}\'.format(\r\n                    epoch, iter, loss1.item(), loss2.item()))\r\n                if epoch % save_per_epoch == 0 and epoch != 0 and iter == 1:\r\n                    logger.info(\'Saving models...\')\r\n                    state = {\r\n                        \'state\': model.state_dict(),\r\n                        \'epoch\': epoch\r\n                    }\r\n                    torch.save(state, os.path.join(os.path.dirname(\r\n                        check_point_save_path), \'faceswap_{}_128x128_{}.pth\'.format(dataset_name, epoch)))\r\n                    copyfile(os.path.join(os.path.dirname(check_point_save_path), \'faceswap_{}_128x128_{}.pth\'.format(dataset_name, epoch)),\r\n                                    check_point_save_path)\r\n                if epoch % 10 == 0 and epoch != 0 and iter == 1:\r\n                    img_a_original = np.array(img_a_target.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    img_b_original = np.array(img_b_target.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    a_predict_a = np.array(predict_a.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    b_predict_b = np.array(predict_b.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n\r\n                    a_predict_b = model(img_a_input, to=\'b\')\r\n                    b_predict_a = model(img_b_input, to=\'a\')\r\n                    a_predict_b = np.array(a_predict_b.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n                    b_predict_a = np.array(b_predict_a.detach().cpu().numpy()[0].transpose(2, 1, 0)*255, dtype=np.uint8)\r\n\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_0.png\'.format(epoch)), cv2.cvtColor(img_a_original, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_3.png\'.format(epoch)), cv2.cvtColor(img_b_original, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_1.png\'.format(epoch)), cv2.cvtColor(a_predict_a, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_4.png\'.format(epoch)), cv2.cvtColor(b_predict_b, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_2.png\'.format(epoch)), cv2.cvtColor(a_predict_b, cv2.COLOR_BGR2RGB))\r\n                    cv2.imwrite(os.path.join(log_img_dir, \'{}_5.png\'.format(epoch)), cv2.cvtColor(b_predict_a, cv2.COLOR_BGR2RGB))\r\n                    logger.info(\'Record a result\')\r\n    except KeyboardInterrupt:\r\n        logger.info(\'try saving models...\')\r\n        state = {\r\n            \'state\': model.state_dict(),\r\n            \'epoch\': epoch\r\n        }\r\n        torch.save(state, os.path.join(os.path.dirname(check_point_save_path), \'faceswap_{}_128x128_{}.pth\'.format(dataset_name, epoch)))\r\n        copyfile(os.path.join(os.path.dirname(check_point_save_path), \'faceswap_{}_128x128_{}.pth\'.format(dataset_name, epoch)),\r\n                        check_point_save_path)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
train_trump_cage_64x64.py,9,"b'""""""\r\nCopyright StrangeAI Authors @2019\r\n\r\n\r\noriginal forked from deepfakes repo\r\n\r\nedit and promoted by StrangeAI authors\r\n\r\n""""""\r\n\r\nfrom __future__ import print_function\r\nimport argparse\r\nimport os\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\n\r\nimport torch.utils.data\r\nfrom torch import nn, optim\r\nfrom torch.autograd import Variable\r\nfrom torch.nn import functional as F\r\nimport torch.backends.cudnn as cudnn\r\n\r\nfrom models.swapnet import SwapNet, toTensor, var_to_np\r\nfrom utils.util import get_image_paths, load_images, stack_images\r\nfrom dataset.training_data import get_training_data\r\nfrom alfred.dl.torch.common import device\r\nfrom shutil import copyfile\r\nfrom loguru import logger\r\n\r\nbatch_size = 64\r\nepochs = 100000\r\nsave_per_epoch = 300\r\n\r\na_dir = \'./data/trump_cage/trump\'\r\nb_dir = \'./data/trump_cage/cage\'\r\n# we start to train on bigger size\r\ntarget_size = 64\r\ndataset_name = \'trump_cage\'\r\nlog_img_dir = \'./checkpoint/results_{}_{}x{}\'.format(dataset_name, target_size, target_size)\r\nlog_model_dir = \'./checkpoint/{}_{}x{}\'.format(dataset_name,\r\n    target_size, target_size)\r\ncheck_point_save_path = os.path.join(\r\n    log_model_dir, \'faceswap_{}_{}x{}.pth\'.format(dataset_name, target_size, target_size))\r\n\r\n\r\ndef main():\r\n    os.makedirs(log_img_dir, exist_ok=True)\r\n    os.makedirs(log_model_dir, exist_ok=True)\r\n    \r\n    logger.info(""loading datasets"")\r\n    images_A = get_image_paths(a_dir)\r\n    images_B = get_image_paths(b_dir)\r\n    images_A = load_images(images_A) / 255.0\r\n    images_B = load_images(images_B) / 255.0\r\n\r\n    print(\'mean value to remember: \', images_B.mean(\r\n        axis=(0, 1, 2)) - images_A.mean(axis=(0, 1, 2)))\r\n    images_A += images_B.mean(axis=(0, 1, 2)) - images_A.mean(axis=(0, 1, 2))\r\n\r\n    model = SwapNet()\r\n    model.to(device)\r\n    start_epoch = 0\r\n    logger.info(\'try resume from checkpoint\')\r\n    if os.path.isdir(\'checkpoint\'):\r\n        try:\r\n            if torch.cuda.is_available():\r\n                checkpoint = torch.load(\'./checkpoint/faceswap_trump_cage_64x64.pth\')\r\n            else:\r\n                checkpoint = torch.load(\r\n                    \'./checkpoint/faceswap_trump_cage_64x64.pth\', map_location={\'cuda:0\': \'cpu\'})\r\n            model.load_state_dict(checkpoint[\'state\'])\r\n            start_epoch = checkpoint[\'epoch\']\r\n            logger.info(\'checkpoint loaded.\')\r\n        except FileNotFoundError:\r\n            print(\'Can\\\'t found faceswap_trump_cage.pth\')\r\n\r\n    criterion = nn.L1Loss()\r\n    optimizer_1 = optim.Adam([{\'params\': model.encoder.parameters()},\r\n                              {\'params\': model.decoder_A.parameters()}], lr=5e-5, betas=(0.5, 0.999))\r\n    optimizer_2 = optim.Adam([{\'params\': model.encoder.parameters()},\r\n                              {\'params\': model.decoder_B.parameters()}], lr=5e-5, betas=(0.5, 0.999))\r\n\r\n    logger.info(\'Start training, from epoch {} \'.format(start_epoch))\r\n\r\n    for epoch in range(start_epoch, epochs):\r\n        warped_A, target_A = get_training_data(images_A, batch_size)\r\n        # print(warped_A.shape)\r\n        # t_a = np.array(warped_A[0] * 255, dtype=np.uint8)\r\n        # print(t_a)\r\n        # print(t_a.shape)\r\n        # cv2.imshow(\'rr\', t_a)\r\n        # cv2.waitKey(0)\r\n        # warped a and target a are not rotated, where did rotate?\r\n\r\n        warped_B, target_B = get_training_data(images_B, batch_size)\r\n        warped_A, target_A = toTensor(warped_A), toTensor(target_A)\r\n        warped_B, target_B = toTensor(warped_B), toTensor(target_B)\r\n        # warp_a = np.array(warped_A[0].detach().cpu().numpy().transpose(2, 1, 0)*255, dtype=np.uint8)\r\n        # cv2.imshow(\'rr\', warp_a)\r\n        # cv2.waitKey(0)\r\n        warped_A, target_A, warped_B, target_B = Variable(warped_A.float()), Variable(target_A.float()), \\\r\n            Variable(warped_B.float()), Variable(target_B.float())\r\n        optimizer_1.zero_grad()\r\n        optimizer_2.zero_grad()\r\n        warped_A_out = model(warped_A, \'A\')\r\n        warped_B_out = model(warped_B, \'B\')\r\n        loss1 = criterion(warped_A_out, target_A)\r\n        loss2 = criterion(warped_B_out, target_B)\r\n        loss1.backward()\r\n        loss2.backward()\r\n        optimizer_1.step()\r\n        optimizer_2.step()\r\n        logger.info(\'epoch: {}, lossA: {}, lossB: {}\'.format(epoch, loss1.item(), loss2.item()))\r\n        if epoch % save_per_epoch == 0 and iter == 0:\r\n            logger.info(\'Saving models...\')\r\n            state = {\r\n                \'state\': model.state_dict(),\r\n                \'epoch\': epoch\r\n            }\r\n            torch.save(state, os.path.join(os.path.dirname(\r\n                        check_point_save_path), \'faceswap_{}_64x64_{}.pth\'.format(dataset_name, epoch)))\r\n            copyfile(os.path.join(os.path.dirname(check_point_save_path), \'faceswap_{}_64x64_{}.pth\'.format(dataset_name, epoch)),\r\n                            check_point_save_path)\r\n        if epoch % 100 == 0:\r\n            test_A_ = warped_A[0:2]   \r\n            a_predict_a = var_to_np(model(test_A_, \'A\'))[0]*255\r\n            # warped a out\r\n            # print(test_A_[0].detach().cpu().numpy().shape)\r\n            a_predict_b = var_to_np(model(test_A_, \'B\'))[0]*255\r\n\r\n            warp_a = test_A_[0].detach().cpu().numpy()*255\r\n            target_a = target_A[0].detach().cpu().numpy()*255\r\n\r\n            cv2.imwrite(os.path.join(log_img_dir, ""{}_res_a_to_a.png"".format(epoch)), np.array(a_predict_a.transpose(2, 1, 0)).astype(\'uint8\'))\r\n            cv2.imwrite(os.path.join(log_img_dir, ""{}_res_a_to_b.png"".format(epoch)), np.array(a_predict_b.transpose(2, 1, 0)).astype(\'uint8\'))\r\n            cv2.imwrite(os.path.join(log_img_dir, ""{}_test_A_warped.png"".format(epoch)), np.array(warp_a.transpose(2, 1, 0)).astype(\'uint8\'))\r\n            cv2.imwrite(os.path.join(log_img_dir, ""{}_test_A_target.png"".format(epoch)), np.array(target_a.transpose(2, 1, 0)).astype(\'uint8\'))\r\n            logger.info(\'Record a result\')\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
dataset/face_pair_dataset.py,14,"b'""""""\nCopyright StrangeAI authors @2019\n\nassume you have to directly which you want\nconvert A to B, just put all faces of A person to A,\nfaces of B person to B\n\n""""""\nimport torch\nfrom torch.utils.data import Dataset\nimport glob\nimport os\nfrom alfred.dl.torch.common import device\nimport cv2\nfrom PIL import Image\nfrom torchvision import transforms\nimport numpy as np\nfrom utils.umeyama import umeyama\nimport cv2\n\nrandom_transform_args = {\n    \'rotation_range\': 10,\n    \'zoom_range\': 0.05,\n    \'shift_range\': 0.05,\n    \'random_flip\': 0.4,\n}\n\n\ndef random_transform(image, rotation_range, zoom_range, shift_range, random_flip):\n    h, w = image.shape[0:2]\n    rotation = np.random.uniform(-rotation_range, rotation_range)\n    scale = np.random.uniform(1 - zoom_range, 1 + zoom_range)\n    tx = np.random.uniform(-shift_range, shift_range) * w\n    ty = np.random.uniform(-shift_range, shift_range) * h\n    mat = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n    mat[:, 2] += (tx, ty)\n    result = cv2.warpAffine(image, mat, (w, h), borderMode=cv2.BORDER_REPLICATE)\n    if np.random.random() < random_flip:\n        result = result[:, ::-1]\n    return result\n\n\ndef random_warp_128(image):\n    assert image.shape == (256, 256, 3), \'resize image to 256 256 first\'\n    range_ = np.linspace(128 - 120, 128 + 120, 9)\n    mapx = np.broadcast_to(range_, (9, 9))\n    mapy = mapx.T\n    mapx = mapx + np.random.normal(size=(9, 9), scale=5)\n    mapy = mapy + np.random.normal(size=(9, 9), scale=5)\n    interp_mapx = cv2.resize(mapx, (144, 144))[8:136, 8:136].astype(\'float32\')\n    interp_mapy = cv2.resize(mapy, (144, 144))[8:136, 8:136].astype(\'float32\')\n    warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)\n    src_points = np.stack([mapx.ravel(), mapy.ravel()], axis=-1)\n    dst_points = np.mgrid[0:129:16, 0:129:16].T.reshape(-1, 2)\n    mat = umeyama(src_points, dst_points, True)[0:2]\n    target_image = cv2.warpAffine(image, mat, (128, 128))\n    return warped_image, target_image\n\n\ndef random_warp_64(image):\n    assert image.shape == (256, 256, 3)\n    range_ = np.linspace(128 - 120, 128 + 120, 5)\n    mapx = np.broadcast_to(range_, (5, 5))\n    mapy = mapx.T\n    mapx = mapx + np.random.normal(size=(5, 5), scale=5)\n    mapy = mapy + np.random.normal(size=(5, 5), scale=5)\n    interp_mapx = cv2.resize(mapx, (80, 80))[8:72, 8:72].astype(\'float32\')\n    interp_mapy = cv2.resize(mapy, (80, 80))[8:72, 8:72].astype(\'float32\')\n    warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)\n    src_points = np.stack([mapx.ravel(), mapy.ravel()], axis=-1)\n    dst_points = np.mgrid[0:65:16, 0:65:16].T.reshape(-1, 2)\n    mat = umeyama(src_points, dst_points, True)[0:2]\n    target_image = cv2.warpAffine(image, mat, (64, 64))\n    return warped_image, target_image\n\n\nclass FacePairDataset(Dataset):\n\n    def __init__(self, a_dir, b_dir, target_size, transform):\n        super(FacePairDataset, self).__init__\n        self.a_dir = a_dir\n        self.b_dir = b_dir\n        self.target_size = target_size\n\n        self.transform = transform\n        # extension can be changed here to png or others\n        self.a_images_list = glob.glob(os.path.join(a_dir, \'*.png\'))\n        self.b_images_list = glob.glob(os.path.join(b_dir, \'*.png\'))\n\n    def __getitem__(self, index):\n        # return 2 image pair, A and B\n        img_a = Image.open(self.a_images_list[index])\n        img_b = Image.open(self.b_images_list[index])\n        \n        # align the face first\n        img_a = img_a.resize((self.target_size, self.target_size), Image.ANTIALIAS)\n        img_b = img_b.resize((self.target_size, self.target_size), Image.ANTIALIAS)\n       \n        # transform\n        if self.transform:\n            img_a = self.transform(img_a)\n            img_b = self.transform(img_b)\n  \n        # already resized, warp it\n        img_a = random_transform(np.array(img_a), **random_transform_args)\n        img_b = random_transform(np.array(img_b), **random_transform_args)\n        img_a_input, img_a = random_warp(np.array(img_a), 256)\n        img_b_input, img_b = random_warp(np.array(img_b), 256)\n        img_a_tensor = torch.Tensor(img_a.transpose(2, 0, 1)/255.).float()\n        img_a_input_tensor = torch.Tensor(img_a_input.transpose(2, 0, 1)/255.).float()\n        img_b_tensor = torch.Tensor(img_b.transpose(2, 0, 1)/255.).float()\n        img_b_input_tensor = torch.Tensor(img_b_input.transpose(2, 0, 1)/255.).float()\n        return img_a_tensor, img_a_input_tensor, img_b_tensor, img_b_input_tensor\n\n    def __len__(self):\n        return min(len(self.a_images_list), len(self.b_images_list))\n\n\n\nclass FacePairDataset64x64(Dataset):\n\n    def __init__(self, a_dir, b_dir, target_size, transform):\n        super(FacePairDataset64x64, self).__init__\n        self.a_dir = a_dir\n        self.b_dir = b_dir\n        self.target_size = target_size\n\n        self.transform = transform\n        # extension can be changed here to png or others\n        self.a_images_list = glob.glob(os.path.join(a_dir, \'*.png\'))\n        self.b_images_list = glob.glob(os.path.join(b_dir, \'*.png\'))\n\n    def __getitem__(self, index):\n        # return 2 image pair, A and B\n        img_a = Image.open(self.a_images_list[index])\n        img_b = Image.open(self.b_images_list[index])\n        \n        # align the face first\n        img_a = img_a.resize((256, 256), Image.ANTIALIAS)\n        img_b = img_b.resize((256, 256), Image.ANTIALIAS)\n       \n        # transform\n        if self.transform:\n            img_a = self.transform(img_a)\n            img_b = self.transform(img_b)\n  \n        # # already resized, warp it\n        img_a = random_transform(np.array(img_a), **random_transform_args)\n        img_b = random_transform(np.array(img_b), **random_transform_args)\n        img_a_input, img_a = random_warp_64(np.array(img_a))\n        img_b_input, img_b = random_warp_64(np.array(img_b))\n\n        img_a = np.array(img_a)\n        img_b = np.array(img_b)\n\n        img_a_tensor = torch.Tensor(img_a.transpose(2, 0, 1)/255.).float()\n        img_a_input_tensor = torch.Tensor(img_a_input.transpose(2, 0, 1)/255.).float()\n        img_b_tensor = torch.Tensor(img_b.transpose(2, 0, 1)/255.).float()\n        img_b_input_tensor = torch.Tensor(img_b_input.transpose(2, 0, 1)/255.).float()\n        return img_a_tensor, img_a_input_tensor, img_b_tensor, img_b_input_tensor\n\n    def __len__(self):\n        return min(len(self.a_images_list), len(self.b_images_list))\n\n\n\nclass FacePairDataset128x128(Dataset):\n\n    def __init__(self, a_dir, b_dir, target_size, transform):\n        super(FacePairDataset128x128, self).__init__\n        self.a_dir = a_dir\n        self.b_dir = b_dir\n        self.target_size = target_size\n\n        self.transform = transform\n        self.a_images_list = glob.glob(os.path.join(a_dir, \'*.png\'))\n        self.b_images_list = glob.glob(os.path.join(b_dir, \'*.png\'))\n\n    def __getitem__(self, index):\n        # return 2 image pair, A and B\n        img_a = Image.open(self.a_images_list[index])\n        img_b = Image.open(self.b_images_list[index])\n        \n        # align the face first\n        img_a = img_a.resize((256, 256), Image.ANTIALIAS)\n        img_b = img_b.resize((256, 256), Image.ANTIALIAS)\n       \n        # transform\n        if self.transform:\n            img_a = self.transform(img_a)\n            img_b = self.transform(img_b)\n  \n        img_a = random_transform(np.array(img_a), **random_transform_args)\n        img_b = random_transform(np.array(img_b), **random_transform_args)\n        img_a_input, img_a = random_warp_128(np.array(img_a))\n        img_b_input, img_b = random_warp_128(np.array(img_b))\n        img_a_tensor = torch.Tensor(img_a.transpose(2, 0, 1)/255.).float()\n        img_a_input_tensor = torch.Tensor(img_a_input.transpose(2, 0, 1)/255.).float()\n        img_b_tensor = torch.Tensor(img_b.transpose(2, 0, 1)/255.).float()\n        img_b_input_tensor = torch.Tensor(img_b_input.transpose(2, 0, 1)/255.).float()\n        return img_a_tensor, img_a_input_tensor, img_b_tensor, img_b_input_tensor\n\n    def __len__(self):\n        return min(len(self.a_images_list), len(self.b_images_list))'"
dataset/training_data.py,0,"b""import numpy\r\nfrom utils.umeyama import umeyama\r\nimport cv2\r\n\r\nrandom_transform_args = {\r\n    'rotation_range': 10,\r\n    'zoom_range': 0.05,\r\n    'shift_range': 0.05,\r\n    'random_flip': 0.4,\r\n}\r\n\r\n\r\ndef random_transform(image, rotation_range, zoom_range, shift_range, random_flip):\r\n    h, w = image.shape[0:2]\r\n    rotation = numpy.random.uniform(-rotation_range, rotation_range)\r\n    scale = numpy.random.uniform(1 - zoom_range, 1 + zoom_range)\r\n    tx = numpy.random.uniform(-shift_range, shift_range) * w\r\n    ty = numpy.random.uniform(-shift_range, shift_range) * h\r\n    mat = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\r\n    mat[:, 2] += (tx, ty)\r\n    result = cv2.warpAffine(image, mat, (w, h), borderMode=cv2.BORDER_REPLICATE)\r\n    if numpy.random.random() < random_flip:\r\n        result = result[:, ::-1]\r\n    return result\r\n\r\n\r\n# get pair of random warped images from aligened face image\r\ndef random_warp(image):\r\n    assert image.shape == (256, 256, 3)\r\n    range_ = numpy.linspace(128 - 80, 128 + 80, 5)\r\n    mapx = numpy.broadcast_to(range_, (5, 5))\r\n    mapy = mapx.T\r\n\r\n    mapx = mapx + numpy.random.normal(size=(5, 5), scale=5)\r\n    mapy = mapy + numpy.random.normal(size=(5, 5), scale=5)\r\n\r\n    interp_mapx = cv2.resize(mapx, (80, 80))[8:72, 8:72].astype('float32')\r\n    interp_mapy = cv2.resize(mapy, (80, 80))[8:72, 8:72].astype('float32')\r\n\r\n    # just crop the image, remove the top left bottom right 8 pixels (in order to get the pure face)\r\n    warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)\r\n\r\n    src_points = numpy.stack([mapx.ravel(), mapy.ravel()], axis=-1)\r\n    dst_points = numpy.mgrid[0:65:16, 0:65:16].T.reshape(-1, 2)\r\n    mat = umeyama(src_points, dst_points, True)[0:2]\r\n    target_image = cv2.warpAffine(image, mat, (64, 64))\r\n    return warped_image, target_image\r\n\r\n\r\ndef get_training_data(images, batch_size):\r\n    indices = numpy.random.randint(len(images), size=batch_size)\r\n    for i, index in enumerate(indices):\r\n        image = images[index]\r\n        image = random_transform(image, **random_transform_args)\r\n        warped_img, target_img = random_warp(image)\r\n        if i == 0:\r\n            warped_images = numpy.empty((batch_size,) + warped_img.shape, warped_img.dtype)\r\n            target_images = numpy.empty((batch_size,) + target_img.shape, warped_img.dtype)\r\n        warped_images[i] = warped_img\r\n        target_images[i] = target_img\r\n    return warped_images, target_images\r\n"""
images/grid_res.py,0,"b'""""""\ngrid a final image from result images\n\n""""""\nimport cv2\nimport numpy as np\nimport os\nimport sys\nimport glob\nfrom PIL import Image\n\n\nd = sys.argv[1]\nprint(\'from \', d)\n\nall_img_files = glob.glob(os.path.join(d, \'*.png\'))\nassert len(all_img_files) % 6 == 0, \'images divided by 6\'\nall_img_files = sorted(all_img_files)\nrows = len(all_img_files) // 6\nprint(rows)\nprint(len(all_img_files))\n\n\nres_img = Image.new(\'RGB\',(128*6, 128*(len(all_img_files)//6)), (255, 255, 255))\n\nfor i in range(len(all_img_files)//6):\n    for j in range(6):\n        # print(\'now: \', all_img_files[6*i + j])\n        img = Image.open(all_img_files[6*i + j])\n        res_img.paste(img, (j*128, i*128))\nres_img.save(\'res_grid.png\')\nprint(np.array(res_img).shape)\ncv2.imshow(\'rr\', np.array(res_img))\ncv2.waitKey(0)'"
models/padding_same_conv.py,9,"b'# modify con2d function to use same padding\n# code referd to @famssa in \'https://github.com/pytorch/pytorch/issues/3867\'\n# and tensorflow source code\n\nimport torch.utils.data\nfrom torch.nn import functional as F\n\nimport math\nimport torch\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.functional import pad\nfrom torch.nn.modules import Module\nfrom torch.nn.modules.utils import _single, _pair, _triple\n\n\nclass _ConvNd(Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, transposed, output_padding, groups, bias):\n        super(_ConvNd, self).__init__()\n        if in_channels % groups != 0:\n            raise ValueError(\'in_channels must be divisible by groups\')\n        if out_channels % groups != 0:\n            raise ValueError(\'out_channels must be divisible by groups\')\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.transposed = transposed\n        self.output_padding = output_padding\n        self.groups = groups\n        if transposed:\n            self.weight = Parameter(torch.Tensor(\n                in_channels, out_channels // groups, *kernel_size))\n        else:\n            self.weight = Parameter(torch.Tensor(\n                out_channels, in_channels // groups, *kernel_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def __repr__(self):\n        s = (\'{name}({in_channels}, {out_channels}, kernel_size={kernel_size}\'\n             \', stride={stride}\')\n        if self.padding != (0,) * len(self.padding):\n            s += \', padding={padding}\'\n        if self.dilation != (1,) * len(self.dilation):\n            s += \', dilation={dilation}\'\n        if self.output_padding != (0,) * len(self.output_padding):\n            s += \', output_padding={output_padding}\'\n        if self.groups != 1:\n            s += \', groups={groups}\'\n        if self.bias is None:\n            s += \', bias=False\'\n        s += \')\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass Conv2d(_ConvNd):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n        super(Conv2d, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            False, _pair(0), groups, bias)\n\n    def forward(self, input):\n        return conv2d_same_padding(input, self.weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n\nclass Conv2dPaddingSame(_ConvNd):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n        super(Conv2dPaddingSame, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            False, _pair(0), groups, bias)\n\n    def forward(self, input):\n        return conv2d_same_padding(input, self.weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n\n# custom con2d, because pytorch don\'t have ""padding=\'same\'"" option.\ndef conv2d_same_padding(input, weight, bias=None, stride=1, padding=1, dilation=1, groups=1):\n\n    input_rows = input.size(2)\n    filter_rows = weight.size(2)\n    effective_filter_size_rows = (filter_rows - 1) * dilation[0] + 1\n    out_rows = (input_rows + stride[0] - 1) // stride[0]\n    padding_needed = max(0, (out_rows - 1) * stride[0] + effective_filter_size_rows -\n                  input_rows)\n    padding_rows = max(0, (out_rows - 1) * stride[0] +\n                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n    rows_odd = (padding_rows % 2 != 0)\n    padding_cols = max(0, (out_rows - 1) * stride[0] +\n                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n    cols_odd = (padding_rows % 2 != 0)\n\n    if rows_odd or cols_odd:\n        input = pad(input, [0, int(cols_odd), 0, int(rows_odd)])\n\n    return F.conv2d(input, weight, bias, stride,\n                  padding=(padding_rows // 2, padding_cols // 2),\n                  dilation=dilation, groups=groups)\n'"
models/swapnet.py,3,"b'""""""\r\nCopyright StrangeAI Authors @2019\r\n\r\n""""""\r\nimport torch\r\nimport torch.utils.data\r\nfrom torch import nn, optim\r\nfrom .padding_same_conv import Conv2d\r\nfrom alfred.dl.torch.common import device\r\n\r\n\r\ndef toTensor(img):\r\n    img = torch.from_numpy(img.transpose((0, 3, 1, 2))).to(device)\r\n    return img\r\n\r\n\r\ndef var_to_np(img_var):\r\n    return img_var.data.cpu().numpy()\r\n\r\n\r\nclass _ConvLayer(nn.Sequential):\r\n    def __init__(self, input_features, output_features):\r\n        super(_ConvLayer, self).__init__()\r\n        self.add_module(\'conv2\', Conv2d(input_features, output_features,\r\n                                        kernel_size=5, stride=2))\r\n        self.add_module(\'leakyrelu\', nn.LeakyReLU(0.1, inplace=True))\r\n\r\n\r\nclass _UpScale(nn.Sequential):\r\n    def __init__(self, input_features, output_features):\r\n        super(_UpScale, self).__init__()\r\n        self.add_module(\'conv2_\', Conv2d(input_features, output_features * 4,\r\n                                         kernel_size=3))\r\n        self.add_module(\'leakyrelu\', nn.LeakyReLU(0.1, inplace=True))\r\n        self.add_module(\'pixelshuffler\', _PixelShuffler())\r\n\r\n\r\nclass Flatten(nn.Module):\r\n\r\n    def forward(self, input):\r\n        output = input.view(input.size(0), -1)\r\n        return output\r\n\r\n\r\nclass Reshape(nn.Module):\r\n\r\n    def forward(self, input):\r\n        output = input.view(-1, 1024, 4, 4)  # channel * 4 * 4\r\n\r\n        return output\r\n\r\n\r\nclass _PixelShuffler(nn.Module):\r\n    def forward(self, input):\r\n        batch_size, c, h, w = input.size()\r\n        rh, rw = (2, 2)\r\n        oh, ow = h * rh, w * rw\r\n        oc = c // (rh * rw)\r\n        out = input.view(batch_size, rh, rw, oc, h, w)\r\n        out = out.permute(0, 3, 4, 1, 5, 2).contiguous()\r\n        out = out.view(batch_size, oc, oh, ow)  # channel first\r\n\r\n        return out\r\n\r\n\r\nclass SwapNet(nn.Module):\r\n    def __init__(self):\r\n        super(SwapNet, self).__init__()\r\n\r\n        self.encoder = nn.Sequential(\r\n            _ConvLayer(3, 128),\r\n            _ConvLayer(128, 256),\r\n            _ConvLayer(256, 512),\r\n            _ConvLayer(512, 1024),\r\n            Flatten(),\r\n            nn.Linear(1024 * 4 * 4, 1024),\r\n            nn.Linear(1024, 1024 * 4 * 4),\r\n            Reshape(),\r\n            _UpScale(1024, 512),\r\n        )\r\n\r\n        self.decoder_A = nn.Sequential(\r\n            _UpScale(512, 256),\r\n            _UpScale(256, 128),\r\n            _UpScale(128, 64),\r\n            Conv2d(64, 3, kernel_size=5, padding=1),\r\n            nn.Sigmoid(),\r\n        )\r\n\r\n        self.decoder_B = nn.Sequential(\r\n            _UpScale(512, 256),\r\n            _UpScale(256, 128),\r\n            _UpScale(128, 64),\r\n            Conv2d(64, 3, kernel_size=5, padding=1),\r\n            nn.Sigmoid(),\r\n        )\r\n\r\n    def forward(self, x, select=\'A\'):\r\n        if select == \'A\':\r\n            out = self.encoder(x)\r\n            out = self.decoder_A(out)\r\n        else:\r\n            out = self.encoder(x)\r\n            out = self.decoder_B(out)\r\n        return out\r\n'"
utils/face_extractor.py,0,"b'""""""\nThis file using for extracting faces of all images\n\n""""""\nimport glob\ntry:\n    import dlib\nexcept ImportError:\n    print(\'You have not installed dlib, install from https://github.com/davisking/dlib\')\n    print(\'see you later.\')\n    exit(0)\nimport os\nimport cv2\nimport numpy as np\nfrom loguru import logger\n\n\nclass FaceExtractor(object):\n\n    def __init__(self):\n        self.detector = dlib.get_frontal_face_detector()\n        # self.predictor = dlib.shape_predictor(""shape_predictor_68_face_landmarks.dat"")\n\n        self.predictor_path = os.path.expanduser(\'~/shape_predictor_68_face_landmarks.dat\')\n    \n    def get_faces_list(self, img, landmark=False):\n        """"""\n        get faces and locations \n        """"""\n        assert isinstance(img, np.ndarray), \'img should be numpy array (cv2 frame)\'\n        if landmark:\n            if os.path.exists(self.predictor_path):\n                predictor = dlib.shape_predictor(self.predictor_path)\n            else:\n                logger.error(\'can not call this method, you should download \'\n                \'dlib landmark model: http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\')\n                exit(0)\n        dets = self.detector(img, 1)\n        all_faces = []\n        locations = []\n        landmarks = []\n        for i, d in enumerate(dets):\n            # get the face crop\n            x = int(d.left())\n            y = int(d.top())\n            w = int(d.width())\n            h = int(d.height())\n\n            face_patch = np.array(img)[y: y + h, x: x + w, 0:3]\n            \n            if landmark:\n                shape = predictor(img, d)\n                landmarks.append(shape)\n            locations.append([x, y, w, h])\n            all_faces.append(face_patch)\n        if landmark:\n            return all_faces, locations, landmarks\n        else:\n            return all_faces, locations\n    \n    def get_faces(self, img_d):\n        """"""\n        get all faces from img_d\n        :param img_d:\n        :return:\n        """"""\n\n        all_images = []\n        for e in [\'png\', \'jpg\', \'jpeg\']:\n            all_images.extend(glob.glob(os.path.join(img_d, \'*.{}\'.format(e))))\n        print(\'Found all {} images under {}\'.format(len(all_images), img_d))\n\n        s_d = os.path.dirname(img_d) + ""_faces""\n        if not os.path.exists(s_d):\n            os.makedirs(s_d)\n        for img_f in all_images:\n            img = cv2.imread(img_f, cv2.COLOR_BGR2RGB)\n\n            dets = self.detector(img, 1)\n            print(\'=> get {} faces in {}\'.format(len(dets), img_f))\n            print(\'=> saving faces...\')\n            for i, d in enumerate(dets):\n                save_face_f = os.path.join(s_d, os.path.basename(img_f).split(\'.\')[0]\n                                           + \'_face_{}.png\'.format(i))\n\n                # get the face crop\n                x = int(d.left())\n                y = int(d.top())\n                w = int(d.width())\n                h = int(d.height())\n\n                face_patch = np.array(img)[y: y + h, x: x + w, 0:3]\n                # print(face_patch.shape)\n                img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n\n                # cv2.imshow(\'tt\', img)\n                # cv2.waitKey(0)\n                cv2.imwrite(save_face_f, face_patch)\n        print(\'Done!\')\n        # cv2.waitKey(0)\n\n'"
utils/model_summary.py,8,"b'# -----------------------\n#\n# Copyright Jin Fagang @2018\n# \n# 1/25/19\n# torch_summary\n# -----------------------\n""""""\ncodes token from\nhttps://github.com/sksq96/pytorch-summary\n\nI edit something here, credits belongs to author\n""""""\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom collections import OrderedDict\nimport numpy as np\n\n\ndef summary(model, input_size, batch_size=-1, device=""cuda""):\n    def register_hook(module):\n        def hook(module, input, output):\n            class_name = str(module.__class__).split(""."")[-1].split(""\'"")[0]\n            module_idx = len(summary)\n\n            m_key = ""%s-%i"" % (class_name, module_idx + 1)\n            summary[m_key] = OrderedDict()\n            summary[m_key][""input_shape""] = list(input[0].size())\n            summary[m_key][""input_shape""][0] = batch_size\n            if isinstance(output, (list, tuple)):\n                summary[m_key][""output_shape""] = [\n                    [-1] + list(o.size())[1:] for o in output\n                ]\n            else:\n                summary[m_key][""output_shape""] = list(output.size())\n                summary[m_key][""output_shape""][0] = batch_size\n\n            params = 0\n            if hasattr(module, ""weight"") and hasattr(module.weight, ""size""):\n                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n                summary[m_key][""trainable""] = module.weight.requires_grad\n            if hasattr(module, ""bias"") and hasattr(module.bias, ""size""):\n                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n            summary[m_key][""nb_params""] = params\n\n        if (\n                not isinstance(module, nn.Sequential)\n                and not isinstance(module, nn.ModuleList)\n                and not (module == model)\n        ):\n            hooks.append(module.register_forward_hook(hook))\n\n    device = device.lower()\n    assert device in [\n        ""cuda"",\n        ""cpu"",\n    ], ""Input device is not valid, please specify \'cuda\' or \'cpu\'""\n\n    if device == ""cuda"" and torch.cuda.is_available():\n        dtype = torch.cuda.FloatTensor\n    else:\n        dtype = torch.FloatTensor\n\n    # multiple inputs to the network\n    if isinstance(input_size, tuple) and input_size[0] <= 3:\n        # batch_size of 2 for batchnorm\n        x = torch.rand(2, *input_size).type(dtype)\n    else:\n        print(\'Wrong! you should send input size specific without batch size, etc: (3, 64, 64), channel first.\')\n        exit(0)\n    # create properties\n    summary = OrderedDict()\n    hooks = []\n\n    # register hook\n    model.apply(register_hook)\n\n    # make a forward pass\n    try:\n        print(\'fake data input: \', x.size())\n        model(x)\n    except Exception as e:\n        print(\'summary failed. error: {}\'.format(e))\n        print(\'make sure your called model.to(device) \')\n        exit(0)\n\n    # remove these hooks\n    for h in hooks:\n        h.remove()\n\n    print(""----------------------------------------------------------------"")\n    line_new = ""{:>20}  {:>25} {:>15}"".format(""Layer (type)"", ""Output Shape"", ""Param #"")\n    print(line_new)\n    print(""================================================================"")\n    total_params = 0\n    total_output = 0\n    trainable_params = 0\n    for layer in summary:\n        # input_shape, output_shape, trainable, nb_params\n        line_new = ""{:>20}  {:>25} {:>15}"".format(\n            layer,\n            str(summary[layer][""output_shape""]),\n            ""{0:,}"".format(summary[layer][""nb_params""]),\n        )\n        total_params += summary[layer][""nb_params""]\n        total_output += np.prod(summary[layer][""output_shape""])\n        if ""trainable"" in summary[layer]:\n            if summary[layer][""trainable""] == True:\n                trainable_params += summary[layer][""nb_params""]\n        print(line_new)\n\n    # assume 4 bytes/number (float on cuda).\n    total_input_size = abs(np.prod(input_size) * batch_size * 4. / (1024 ** 2.))\n    total_output_size = abs(2. * total_output * 4. / (1024 ** 2.))  # x2 for gradients\n    total_params_size = abs(total_params.numpy() * 4. / (1024 ** 2.))\n    total_size = total_params_size + total_output_size + total_input_size\n\n    print(""================================================================"")\n    print(""Total params: {0:,}"".format(total_params))\n    print(""Trainable params: {0:,}"".format(trainable_params))\n    print(""Non-trainable params: {0:,}"".format(total_params - trainable_params))\n    print(""----------------------------------------------------------------"")\n    print(""Input size (MB): %0.2f"" % total_input_size)\n    print(""Forward/backward pass size (MB): %0.2f"" % total_output_size)\n    print(""Params size (MB): %0.2f"" % total_params_size)\n    print(""Estimated Total Size (MB): %0.2f"" % total_size)\n    print(""----------------------------------------------------------------"")\n'"
utils/umeyama.py,0,"b'# # License (Modified BSD) # Copyright (C) 2011, the scikit-image team All rights reserved. # # Redistribution and\r\n# use in source and binary forms, with or without modification, are permitted provided that the following conditions\r\n# are met: # # Redistributions of source code must retain the above copyright notice, this list of conditions and the\r\n#  following disclaimer. # Redistributions in binary form must reproduce the above copyright notice, this list of\r\n# conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\r\n#  Neither the name of skimage nor the names of its contributors may be used to endorse or promote products derived\r\n# from this software without specific prior written permission. # THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS\'\'\r\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND\r\n#  FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,\r\n# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE\r\n# GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\r\n# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\r\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\r\n\r\n# umeyama function from scikit-image/skimage/transform/_geometric.py\r\n\r\nimport numpy as np\r\n\r\n\r\ndef umeyama(src, dst, estimate_scale):\r\n    """"""Estimate N-D similarity transformation with or without scaling.\r\n    Parameters\r\n    ----------\r\n    src : (M, N) array\r\n        Source coordinates.\r\n    dst : (M, N) array\r\n        Destination coordinates.\r\n    estimate_scale : bool\r\n        Whether to estimate scaling factor.\r\n    Returns\r\n    -------\r\n    T : (N + 1, N + 1)\r\n        The homogeneous similarity transformation matrix. The matrix contains\r\n        NaN values only if the problem is not well-conditioned.\r\n    References\r\n    ----------\r\n    .. [1] ""Least-squares estimation of transformation parameters between two\r\n            point patterns"", Shinji Umeyama, PAMI 1991, DOI: 10.1109/34.88573\r\n    """"""\r\n\r\n    num = src.shape[0]\r\n    dim = src.shape[1]\r\n\r\n    # Compute mean of src and dst.\r\n    src_mean = src.mean(axis=0)\r\n    dst_mean = dst.mean(axis=0)\r\n\r\n    # Subtract mean from src and dst.\r\n    src_demean = src - src_mean\r\n    dst_demean = dst - dst_mean\r\n\r\n    # Eq. (38).\r\n    A = np.dot(dst_demean.T, src_demean) / num\r\n\r\n    # Eq. (39).\r\n    d = np.ones((dim,), dtype=np.double)\r\n    if np.linalg.det(A) < 0:\r\n        d[dim - 1] = -1\r\n\r\n    T = np.eye(dim + 1, dtype=np.double)\r\n\r\n    U, S, V = np.linalg.svd(A)\r\n\r\n    # Eq. (40) and (43).\r\n    rank = np.linalg.matrix_rank(A)\r\n    if rank == 0:\r\n        return np.nan * T\r\n    elif rank == dim - 1:\r\n        if np.linalg.det(U) * np.linalg.det(V) > 0:\r\n            T[:dim, :dim] = np.dot(U, V)\r\n        else:\r\n            s = d[dim - 1]\r\n            d[dim - 1] = -1\r\n            T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V))\r\n            d[dim - 1] = s\r\n    else:\r\n        T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V.T))\r\n\r\n    if estimate_scale:\r\n        # Eq. (41) and (42).\r\n        scale = 1.0 / src_demean.var(axis=0).sum() * np.dot(S, d)\r\n    else:\r\n        scale = 1.0\r\n\r\n    T[:dim, dim] = dst_mean - scale * np.dot(T[:dim, :dim], src_mean.T)\r\n    T[:dim, :dim] *= scale\r\n\r\n    return T\r\n'"
utils/util.py,0,"b'import cv2\r\nimport numpy\r\nimport os\r\n\r\n\r\ndef get_image_paths(directory):\r\n    # return [x.path for x in os.scandir(directory) if x.name.endswith("".jpg"") or x.name.endswith("".png"")]\r\n    return [x.path for x in os.scandir(directory) if x.name.endswith("".png"")]\r\n\r\n\r\ndef load_images(image_paths, convert=None):\r\n    iter_all_images = (cv2.resize(cv2.imread(fn), (256, 256)) for fn in image_paths)\r\n    if convert:\r\n        iter_all_images = (convert(img) for img in iter_all_images)\r\n    for i, image in enumerate(iter_all_images):\r\n        if i == 0:\r\n            all_images = numpy.empty((len(image_paths),) + image.shape, dtype=image.dtype)\r\n        all_images[i] = image\r\n    return all_images\r\n\r\n\r\ndef get_transpose_axes(n):\r\n    if n % 2 == 0:\r\n        y_axes = list(range(1, n - 1, 2))\r\n        x_axes = list(range(0, n - 1, 2))\r\n    else:\r\n        y_axes = list(range(0, n - 1, 2))\r\n        x_axes = list(range(1, n - 1, 2))\r\n    return y_axes, x_axes, [n - 1]\r\n\r\n\r\ndef stack_images(images):\r\n    images_shape = numpy.array(images.shape)\r\n    new_axes = get_transpose_axes(len(images_shape))\r\n    new_shape = [numpy.prod(images_shape[x]) for x in new_axes]\r\n    return numpy.transpose(\r\n        images,\r\n        axes=numpy.concatenate(new_axes)\r\n    ).reshape(new_shape)\r\n\r\n'"
