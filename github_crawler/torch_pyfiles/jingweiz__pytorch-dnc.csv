file_path,api_count,code
main.py,0,"b'import numpy as np\n\n# custom modules\nfrom utils.options import Options\nfrom utils.factory import EnvDict, CircuitDict, AgentDict\n\n# 0. setting up\nopt = Options()\nnp.random.seed(opt.seed)\n\n# 1. env     (prototype)\nenv_prototype     = EnvDict[opt.env_type]\n# 2. circuit (prototype)\ncircuit_prototype = CircuitDict[opt.circuit_type]\n# 3. agent\nagent = AgentDict[opt.agent_type](opt.agent_params,\n                                  env_prototype     = env_prototype,\n                                  circuit_prototype = circuit_prototype)\n# 5. fit model\nif opt.mode == 1:   # train\n    agent.fit_model()\nelif opt.mode == 2: # test opt.model_file\n    agent.test_model()\n'"
core/__init__.py,0,b''
core/accessor.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.nn as nn\n\nclass Accessor(nn.Module):\n    def __init__(self, args):\n        super(Accessor, self).__init__()\n        # logging\n        self.logger = args.logger\n        # params\n        self.use_cuda = args.use_cuda\n        self.dtype = args.dtype\n\n        # params\n        self.batch_size = args.batch_size\n        self.hidden_dim = args.hidden_dim\n        self.num_write_heads = args.num_write_heads\n        self.num_read_heads = args.num_read_heads\n        self.mem_hei = args.mem_hei\n        self.mem_wid = args.mem_wid\n        self.clip_value = args.clip_value\n\n        # functional components\n        self.write_head_params = args.write_head_params\n        self.read_head_params = args.read_head_params\n        self.memory_params = args.memory_params\n\n        # fill in the missing values\n        # write_heads\n        self.write_head_params.num_heads = self.num_write_heads\n        self.write_head_params.batch_size = self.batch_size\n        self.write_head_params.hidden_dim = self.hidden_dim\n        self.write_head_params.mem_hei = self.mem_hei\n        self.write_head_params.mem_wid = self.mem_wid\n        # read_heads\n        self.read_head_params.num_heads = self.num_read_heads\n        self.read_head_params.batch_size = self.batch_size\n        self.read_head_params.hidden_dim = self.hidden_dim\n        self.read_head_params.mem_hei = self.mem_hei\n        self.read_head_params.mem_wid = self.mem_wid\n        # memory\n        self.memory_params.batch_size = self.batch_size\n        self.memory_params.clip_value = self.clip_value\n        self.memory_params.mem_hei = self.mem_hei\n        self.memory_params.mem_wid = self.mem_wid\n\n    def _init_weights(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def _reset_states(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def _reset(self):           # NOTE: should be called at each child\'s __init__\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def visual(self):\n        self.write_heads.visual()\n        self.read_heads.visual()\n        self.memory.visual()\n\n    def forward(self, lstm_hidden_vb):\n        raise NotImplementedError(""not implemented in base calss"")\n'"
core/agent.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.optim as optim\n\nfrom utils.helpers import Experience\n\nclass Agent(object):\n    def __init__(self, args, env_prototype, circuit_prototype):\n        # logging\n        self.mode = args.mode                       # NOTE: when mode==2 we visualize accessor states\n        self.logger = args.logger\n\n        # prototypes for env & model & memory\n        self.env_prototype = env_prototype          # NOTE: instantiated in inherited Agents\n        self.env_params = args.env_params\n        self.circuit_prototype = circuit_prototype  # NOTE: instantiated in inherited Agents\n        self.circuit_params = args.circuit_params\n\n        # TODO: let\'s decide what to save later\n        # params\n        self.model_name = args.model_name           # NOTE: will save the current model to model_name\n        self.model_file = args.model_file           # NOTE: will load pretrained model_file if not None\n\n        self.render = args.render\n        self.visualize = args.visualize\n        if self.visualize:\n            self.vis = args.vis\n            self.refs = args.refs\n\n        self.save_best = args.save_best\n        if self.save_best:\n            self.best_step   = None                 # NOTE: achieves best_reward at this step\n            self.best_reward = None                 # NOTE: only save a new model if achieves higher reward\n\n        self.use_cuda = args.use_cuda\n        self.dtype = args.dtype\n\n        # agent_params\n        # criteria and optimizer\n        self.criteria = args.criteria\n        self.optim = args.optim\n        # hyperparameters\n        self.steps = args.steps\n        self.batch_size = args.batch_size\n        self.early_stop = args.early_stop\n        self.clip_grad = args.clip_grad\n        # self.clip_value = args.clip_value\n        self.lr = args.lr\n        self.optim_eps = args.optim_eps\n        self.optim_alpha = args.optim_alpha\n        self.eval_freq = args.eval_freq\n        self.eval_steps = args.eval_steps\n        self.prog_freq = args.prog_freq\n        self.test_nepisodes = args.test_nepisodes\n\n    def _reset_experience(self):\n        self.experience = Experience(state0 = None,\n                                     action = None,\n                                     reward = None,\n                                     state1 = None,\n                                     terminal1 = False)\n\n    def _load_model(self, model_file):\n        if model_file:\n            self.logger.warning(""Loading Model: "" + self.model_file + "" ..."")\n            self.circuit.load_state_dict(torch.load(model_file))\n            self.logger.warning(""Loaded  Model: "" + self.model_file + "" ..."")\n        else:\n            self.logger.warning(""No Pretrained Model. Will Train From Scratch."")\n\n    def _save_model(self, step, curr_reward=0.):\n        self.logger.warning(""Saving Model    @ Step: "" + str(step) + "": "" + self.model_name + "" ..."")\n        if self.save_best:\n            if self.best_step is None:\n                self.best_step   = step\n                self.best_reward = curr_reward\n            if curr_reward >= self.best_reward:\n                self.best_step   = step\n                self.best_reward = curr_reward\n                torch.save(self.circuit.state_dict(), self.model_name)\n            self.logger.warning(""Saved  Model    @ Step: "" + str(step) + "": "" + self.model_name + "". {Best Step: "" + str(self.best_step) + "" | Best Reward: "" + str(self.best_reward) + ""}"")\n        else:\n            torch.save(self.circuit.state_dict(), self.model_name)\n            self.logger.warning(""Saved  Model    @ Step: "" + str(step) + "": "" + self.model_name + ""."")\n\n    def _forward(self, observation):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def _backward(self, reward, terminal):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def fit_model(self):    # training\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def _eval_model(self):  # evaluation during training\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def test_model(self):   # testing pre-trained models\n        raise NotImplementedError(""not implemented in base calss"")\n'"
core/circuit.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass Circuit(nn.Module):   # NOTE: basically this whole module is treated as a custom rnn cell\n    def __init__(self, args):\n        super(Circuit, self).__init__()\n        # logging\n        self.logger = args.logger\n        # params\n        self.use_cuda = args.use_cuda\n        self.dtype = args.dtype\n        # params\n        self.batch_size = args.batch_size\n        self.input_dim = args.input_dim\n        self.output_dim = args.output_dim\n        self.hidden_dim = args.hidden_dim\n        self.num_write_heads = args.num_write_heads\n        self.num_read_heads = args.num_read_heads\n        self.mem_hei = args.mem_hei\n        self.mem_wid = args.mem_wid\n        self.clip_value = args.clip_value\n\n        # functional components\n        self.controller_params = args.controller_params\n        self.accessor_params = args.accessor_params\n\n        # now we fill in the missing values for each module\n        self.read_vec_dim = self.num_read_heads * self.mem_wid\n        # controller\n        self.controller_params.batch_size = self.batch_size\n        self.controller_params.input_dim = self.input_dim\n        self.controller_params.read_vec_dim = self.read_vec_dim\n        self.controller_params.output_dim = self.output_dim\n        self.controller_params.hidden_dim = self.hidden_dim\n        self.controller_params.mem_hei = self.mem_hei\n        self.controller_params.mem_wid = self.mem_wid\n        self.controller_params.clip_value = self.clip_value\n        # accessor: {write_heads, read_heads, memory}\n        self.accessor_params.batch_size = self.batch_size\n        self.accessor_params.hidden_dim = self.hidden_dim\n        self.accessor_params.num_write_heads = self.num_write_heads\n        self.accessor_params.num_read_heads = self.num_read_heads\n        self.accessor_params.mem_hei = self.mem_hei\n        self.accessor_params.mem_wid = self.mem_wid\n        self.accessor_params.clip_value = self.clip_value\n\n        self.logger.warning(""<-----------------------------======> Circuit:    {Controller, Accessor}"")\n\n    def _init_weights(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def print_model(self):\n        self.logger.warning(""<-----------------------------======> Circuit:    {Overall Architecture}"")\n        self.logger.warning(self)\n\n    def _reset_states(self): # should be called at the beginning of forwarding a new input sequence\n        # we first reset the previous read vector\n        self.read_vec_vb = Variable(self.read_vec_ts).type(self.dtype)\n        # we then reset the controller\'s hidden state\n        self.controller._reset_states()\n        # we then reset the write/read weights of heads\n        self.accessor._reset_states()\n\n    def _reset(self):\n        self._init_weights()\n        self.type(self.dtype)\n        self.print_model()\n        # reset internal states\n        self.read_vec_ts = torch.zeros(self.batch_size, self.read_vec_dim).fill_(1e-6)\n        self._reset_states()\n\n    def forward(self, input_vb):\n        # NOTE: the operation order must be the following: control, access{write, read}, output\n\n        # 1. first feed {input, read_vec_{t-1}} to controller\n        hidden_vb = self.controller.forward(input_vb, self.read_vec_vb)\n        # 2. then we write to memory_{t-1} to get memory_{t}; then read from memory_{t} to get read_vec_{t}\n        self.read_vec_vb = self.accessor.forward(hidden_vb)\n        # 3. finally we concat the output from the controller and the current read_vec_{t} to get the final output\n        output_vb = self.hid_to_out(torch.cat((hidden_vb.view(-1, self.hidden_dim),\n                                               self.read_vec_vb.view(-1, self.read_vec_dim)), 1))\n\n        # we clip the output values here\n        return F.sigmoid(torch.clamp(output_vb, min=-self.clip_value, max=self.clip_value)).view(1, self.batch_size, self.output_dim)\n'"
core/controller.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils.init_weights import init_weights, normalized_columns_initializer\n\nclass Controller(nn.Module):\n    def __init__(self, args):\n        super(Controller, self).__init__()\n        # logging\n        self.logger = args.logger\n        # general params\n        self.use_cuda = args.use_cuda\n        self.dtype = args.dtype\n\n        # params\n        self.batch_size = args.batch_size\n        self.input_dim = args.input_dim\n        self.read_vec_dim = args.read_vec_dim\n        self.output_dim = args.output_dim\n        self.hidden_dim = args.hidden_dim\n        self.mem_hei = args.mem_hei\n        self.mem_wid = args.mem_wid\n        self.clip_value = args.clip_value\n\n    def _init_weights(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def print_model(self):\n        self.logger.warning(""<--------------------------------===> Controller:"")\n        self.logger.warning(self)\n\n    def _reset_states(self):\n        # we reset controller\'s hidden state\n        self.lstm_hidden_vb = (Variable(self.lstm_hidden_ts[0]).type(self.dtype),\n                               Variable(self.lstm_hidden_ts[1]).type(self.dtype))\n\n    def _reset(self):           # NOTE: should be called at each child\'s __init__\n        self._init_weights()\n        self.type(self.dtype)   # put on gpu if possible\n        self.print_model()\n        # reset internal states\n        self.lstm_hidden_ts = []\n        self.lstm_hidden_ts.append(torch.zeros(self.batch_size, self.hidden_dim))\n        self.lstm_hidden_ts.append(torch.zeros(self.batch_size, self.hidden_dim))\n        self._reset_states()\n\n    def forward(self, input_vb):\n        raise NotImplementedError(""not implemented in base calss"")\n'"
core/env.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom copy import deepcopy\n\nfrom utils.helpers import Experience            # NOTE: here state0 is always ""None""\n\nclass Env(object):\n    def __init__(self, args, env_ind=0):\n        self.logger     = args.logger\n        self.ind        = env_ind               # NOTE: for creating multiple environment instances\n        # general setup\n        self.mode       = args.mode             # NOTE: save frames when mode=2\n        if self.mode == 2:\n            try:\n                import scipy.misc\n                self.imsave = scipy.misc.imsave\n            except ImportError as e: self.logger.warning(""WARNING: scipy.misc not found"")\n            self.img_dir = args.root_dir + ""/imgs/""\n            self.frame_ind = 0\n        self.seed       = args.seed + self.ind  # NOTE: so to give a different seed to each instance\n        self.visualize  = args.visualize\n        if self.visualize:\n            self.vis        = args.vis\n            self.refs       = args.refs\n            self.win_state1 = ""win_state1""\n\n        self.env_type   = args.env_type\n        self.game       = args.game\n        self._reset_experience()\n\n        self.logger.warning(""<-----------------------------======> Env:"")\n        self.logger.warning(""Creating {"" + self.env_type + "" | "" + self.game + ""} w/ Seed: "" + str(self.seed))\n\n    def _reset_experience(self):\n        self.exp_state0 = None  # NOTE: always None in this module\n        self.exp_action = None\n        self.exp_reward = None\n        self.exp_state1 = None\n        self.exp_terminal1 = None\n\n    def _get_experience(self):\n        return Experience(state0 = self.exp_state0, # NOTE: here state0 is always None\n                          action = self.exp_action,\n                          reward = self.exp_reward,\n                          state1 = self._preprocessState(self.exp_state1),\n                          terminal1 = self.exp_terminal1)\n\n    def _preprocessState(self, state):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    @property\n    def state_shape(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    @property\n    def action_dim(self):\n        return self.env.action_space.n\n\n    def render(self):       # render using the original gl window\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def visual(self):       # visualize onto visdom\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def reset(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def step(self, action):\n        raise NotImplementedError(""not implemented in base calss"")\n\nclass GymEnv(Env):  # low dimensional observations\n    def __init__(self, args, env_ind=0):\n        super(GymEnv, self).__init__(args, env_ind)\n\n        assert self.env_type == ""gym""\n        try: import gym\n        except ImportError as e: self.logger.warning(""WARNING: gym not found"")\n\n        self.env = gym.make(self.game)\n        self.env.seed(self.seed)    # NOTE: so each env would be different\n\n        # action space setup\n        self.actions     = range(self.action_dim)\n        self.logger.warning(""Action Space: %s"", self.actions)\n\n        # state space setup\n        self.logger.warning(""State  Space: %s"", self.state_shape)\n\n    def _preprocessState(self, state):    # NOTE: here no preprecessing is needed\n        return state\n\n    @property\n    def state_shape(self):\n        return self.env.observation_space.shape[0]\n\n    def render(self):\n        if self.mode == 2:\n            frame = self.env.render(mode=\'rgb_array\')\n            frame_name = self.img_dir + ""frame_%04d.jpg"" % self.frame_ind\n            self.imsave(frame_name, frame)\n            self.logger.warning(""Saved  Frame    @ Step: "" + str(self.frame_ind) + "" To: "" + frame_name)\n            self.frame_ind += 1\n            return frame\n        else:\n            return self.env.render()\n\n\n    def visual(self):\n        pass\n\n    def sample_random_action(self):\n        return self.env.action_space.sample()\n\n    def reset(self):\n        self._reset_experience()\n        self.exp_state1 = self.env.reset()\n        return self._get_experience()\n\n    def step(self, action_index):\n        self.exp_action = action_index\n        self.exp_state1, self.exp_reward, self.exp_terminal1, _ = self.env.step(self.actions[self.exp_action])\n        return self._get_experience()\n'"
core/head.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass Head(nn.Module):\n    def __init__(self, args):\n        super(Head, self).__init__()\n        # logging\n        self.logger = args.logger\n        # params\n        self.visualize = args.visualize\n        if self.visualize:\n            self.vis      = args.vis\n            self.refs     = args.refs\n            self.win_head = None\n        self.use_cuda = args.use_cuda\n        self.dtype = args.dtype\n        # params\n        self.num_heads = args.num_heads\n        self.batch_size = args.batch_size\n        self.hidden_dim = args.hidden_dim\n        self.mem_hei = args.mem_hei\n        self.mem_wid = args.mem_wid\n        self.num_allowed_shifts = args.num_allowed_shifts\n\n    def _reset_states(self):\n        self.wl_prev_vb = Variable(self.wl_prev_ts).type(self.dtype) # batch_size x num_heads x mem_hei\n\n    def _reset(self):           # NOTE: should be called at each child\'s __init__\n        # self._init_weights()\n        self.type(self.dtype)   # put on gpu if possible\n        # TODO: see how this one is reset\n        # reset internal states\n        self.wl_prev_ts = torch.eye(1, self.mem_hei).unsqueeze(0).expand(self.batch_size, self.num_heads, self.mem_hei)\n        self._reset_states()\n\n    def _visual(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def _access(self, memory_vb):\n        # NOTE: called at the end of forward, to use the weight to read/write from/to memory\n        raise NotImplementedError(""not implemented in base calss"")\n'"
core/memory.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\nclass ExternalMemory(object):\n    def __init__(self, args):\n        # logging\n        self.logger = args.logger\n        # params\n        self.visualize = args.visualize\n        if self.visualize:\n            self.vis        = args.vis\n            self.refs       = args.refs\n            self.win_memory = ""win_memory""\n        self.use_cuda = args.use_cuda\n        self.dtype = args.dtype\n\n        self.batch_size = args.batch_size\n        self.mem_hei = args.mem_hei\n        self.mem_wid = args.mem_wid\n\n        self.logger.warning(""<-----------------------------------> Memory:     {"" + str(self.batch_size) + ""(batch_size) x "" + str(self.mem_hei) + ""(mem_hei) x "" + str(self.mem_wid) + ""(mem_wid)}"")\n\n    def _save_memory(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def _load_memory(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def _reset_states(self):\n        self.memory_vb = Variable(self.memory_ts).type(self.dtype)\n\n    def _reset(self):           # NOTE: should be called at each child\'s __init__\n        self.memory_ts = torch.zeros(self.batch_size, self.mem_hei, self.mem_wid).fill_(1e-6)\n        self._reset_states()\n\n    def visual(self):\n        if self.visualize:      # here we visualize the memory of batch0\n            self.win_memory = self.vis.heatmap(self.memory_vb.data[0].clone().cpu().numpy(), env=self.refs, win=self.win_memory, opts=dict(title=""memory""))\n\nclass External2DMemory(ExternalMemory):\n    def __init__(self, args):\n        super(External2DMemory, self).__init__(args)\n        self._reset()\n\nclass External3DMemory(ExternalMemory):\n    def __init__(self, args):\n        super(External3DMemory, self).__init__(args)\n        self._reset()\n'"
utils/__init__.py,0,b''
utils/factory.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom core.envs.copy_env import CopyEnv\nfrom core.envs.repeat_copy_env import RepeatCopyEnv\nEnvDict = {""copy"":        CopyEnv,\n           ""repeat-copy"": RepeatCopyEnv}\n\nfrom core.circuits.ntm_circuit import NTMCircuit\nfrom core.circuits.dnc_circuit import DNCCircuit\nCircuitDict = {""none"": None,\n               ""ntm"":  NTMCircuit,  # lstm_controller + static_accessor\n               ""dnc"":  DNCCircuit}  # lstm_controller + dynamic_accessor\n\nfrom core.agents.empty_agent import EmptyAgent\nfrom core.agents.sl_agent    import SLAgent\nAgentDict = {""empty"": EmptyAgent,   # to test integration of new envs, contains only the most basic control loop\n             ""sl"":    SLAgent}      # for supervised learning tasks\n'"
utils/fake_ops.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nfrom torch.autograd import Variable\n\n# NOTE: since currently cumprod does not support autograd\n# NOTE: we implement this op using exisiting torch ops\n# TODO: replace fake_cumprod w/ cumprod once this PR is ready:\n# TODO: https://github.com/pytorch/pytorch/pull/1439\n# NOTE: https://discuss.pytorch.org/t/cumprod-exclusive-true-equivalences/2614/8https://discuss.pytorch.org/t/cumprod-exclusive-true-equivalences/2614/8\ndef fake_cumprod(vb):\n    """"""\n    args:\n        vb:  [hei x wid]\n          -> NOTE: we are lazy here so now it only supports cumprod along wid\n    """"""\n    # real_cumprod = torch.cumprod(vb.data, 1)\n    vb = vb.unsqueeze(0)\n    mul_mask_vb = Variable(torch.zeros(vb.size(2), vb.size(1), vb.size(2))).type_as(vb)\n    for i in range(vb.size(2)):\n       mul_mask_vb[i, :, :i+1] = 1\n    add_mask_vb = 1 - mul_mask_vb\n    vb = vb.expand_as(mul_mask_vb) * mul_mask_vb + add_mask_vb\n    # vb = torch.prod(vb, 2).transpose(0, 2)                # 0.1.12\n    vb = torch.prod(vb, 2, keepdim=True).transpose(0, 2)    # 0.2.0\n    # print(real_cumprod - vb.data) # NOTE: checked, ==0\n    return vb\n'"
utils/helpers.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport logging\nimport numpy as np\nfrom collections import namedtuple\n\ndef loggerConfig(log_file, verbose=2):\n   logger      = logging.getLogger()\n   formatter   = logging.Formatter('[%(levelname)-8s] (%(processName)-11s) %(message)s')\n   fileHandler = logging.FileHandler(log_file, 'w')\n   fileHandler.setFormatter(formatter)\n   logger.addHandler(fileHandler)\n   if verbose >= 2:\n       logger.setLevel(logging.DEBUG)\n   elif verbose >= 1:\n       logger.setLevel(logging.INFO)\n   else:\n       # NOTE: we currently use this level to log to get rid of visdom's info printouts\n       logger.setLevel(logging.WARNING)\n   return logger\n\n# This is to be understood as a transition: Given `state0`, performing `action`\n# yields `reward` and results in `state1`, which might be `terminal`.\n# NOTE: used as the return format for Env(), and for format to push into replay memory for off-policy methods\n# NOTE: when return from Env(), state0 is always None\nExperience  = namedtuple('Experience',  'state0, action, reward, state1, terminal1')\n"""
utils/init_weights.py,2,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport torch\n\ndef normalized_columns_initializer(weights, std=1.0):\n    out = torch.randn(weights.size())\n    out *= std / torch.sqrt(out.pow(2).sum(1).expand_as(out))\n    return out\n\ndef init_weights(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = np.prod(weight_shape[1:4])\n        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n    elif classname.find('Linear') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = weight_shape[1]\n        fan_out = weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n"""
utils/options.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport visdom\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom utils.helpers import loggerConfig\n\nCONFIGS = [\n# agent_type, env_type,      game, circuit_type\n[ ""empty"",    ""repeat-copy"", """",   ""none""      ],  # 0\n[ ""sl"",       ""copy"",        """",   ""ntm""       ],  # 1\n[ ""sl"",       ""repeat-copy"", """",   ""dnc""       ]   # 2\n]\n\nclass Params(object):   # NOTE: shared across all modules\n    def __init__(self):\n        self.verbose     = 0            # 0(warning) | 1(info) | 2(debug)\n\n        # training signature\n        self.machine     = ""daim""       # ""machine_id""\n        self.timestamp   = ""17080800""   # ""yymmdd##""\n        # training configuration\n        self.mode        = 1            # 1(train) | 2(test model_file)\n        self.config      = 1 \n\n        self.seed        = 1\n        self.render      = False        # whether render the window from the original envs or not\n        self.visualize   = True         # whether do online plotting and stuff or not\n        self.save_best   = False        # save model w/ highest reward if True, otherwise always save the latest model\n\n        self.agent_type, self.env_type, self.game, self.circuit_type = CONFIGS[self.config]\n\n        self.use_cuda    = torch.cuda.is_available()\n        self.dtype       = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n\n        # prefix for model/log/visdom\n        self.refs        = self.machine + ""_"" + self.timestamp # NOTE: using this as env for visdom\n        self.root_dir    = os.getcwd()\n\n        # model files\n        # NOTE: will save the current model to model_name\n        self.model_name  = self.root_dir + ""/models/"" + self.refs + "".pth""\n        # NOTE: will load pretrained model_file if not None\n        self.model_file  = None#self.root_dir + ""/models/{TODO:FILL_IN_PRETAINED_MODEL_FILE}.pth""\n        if self.mode == 2:\n            self.model_file  = self.model_name  # NOTE: so only need to change self.mode to 2 to test the current training\n            assert self.model_file is not None, ""Pre-Trained model is None, Testing aborted!!!""\n            self.refs = self.refs + ""_test""     # NOTE: using this as env for visdom for testing, to avoid accidentally redraw on the training plots\n\n        # logging configs\n        self.log_name    = self.root_dir + ""/logs/"" + self.refs + "".log""\n        self.logger      = loggerConfig(self.log_name, self.verbose)\n        self.logger.warning(""<===================================>"")\n\n        if self.visualize:\n            self.vis = visdom.Visdom()\n            self.logger.warning(""bash$: python -m visdom.server"")           # activate visdom server on bash\n            self.logger.warning(""http://localhost:8097/env/"" + self.refs)   # open this address on browser\n\nclass EnvParams(Params):    # settings for network architecture\n    def __init__(self):\n        super(EnvParams, self).__init__()\n\n        self.batch_size = None\n        if self.env_type == ""copy"":\n            self.len_word  = 8\n            self.min_num_words = 5\n            self.max_num_words = 10\n        elif self.env_type == ""repeat-copy"":\n            self.len_word  = 4\n            self.min_num_words = 1\n            self.max_num_words = 2\n            self.min_repeats   = 1\n            self.max_repeats   = 2\n            self.max_repeats_norm = 10.\n\nclass ControllerParams(Params):\n    def __init__(self):\n        super(ControllerParams, self).__init__()\n\n        self.batch_size     = None\n        self.input_dim      = None  # set after env\n        self.read_vec_dim   = None  # num_read_heads x mem_wid\n        self.output_dim     = None  # set after env\n        self.hidden_dim     = None  #\n        self.mem_hei        = None  # set after memory\n        self.mem_wid        = None  # set after memory\n\nclass HeadParams(Params):\n    def __init__(self):\n        super(HeadParams, self).__init__()\n\n        self.num_heads = None\n        self.batch_size = None\n        self.hidden_dim = None\n        self.mem_hei = None\n        self.mem_wid = None\n        self.num_allowed_shifts = 3\n\nclass WriteHeadParams(HeadParams):\n    def __init__(self):\n        super(WriteHeadParams, self).__init__()\n\nclass ReadHeadParams(HeadParams):\n    def __init__(self):\n        super(ReadHeadParams, self).__init__()\n        if self.circuit_type == ""dnc"":\n            self.num_read_modes = None\n\nclass MemoryParams(Params):\n    def __init__(self):\n        super(MemoryParams, self).__init__()\n\n        self.batch_size = None\n        self.mem_hei = None\n        self.mem_wid = None\n\nclass AccessorParams(Params):\n    def __init__(self):\n        super(AccessorParams, self).__init__()\n\n        self.batch_size = None\n        self.hidden_dim = None\n        self.num_write_heads = None\n        self.num_read_heads = None\n        self.mem_hei = None\n        self.mem_wid = None\n        self.clip_value = None\n        self.write_head_params = WriteHeadParams()\n        self.read_head_params  = ReadHeadParams()\n        self.memory_params     = MemoryParams()\n\nclass CircuitParams(Params):# settings for network architecture\n    def __init__(self):\n        super(CircuitParams, self).__init__()\n\n        self.batch_size     = None\n        self.input_dim      = None  # set after env\n        self.read_vec_dim   = None  # num_read_heads x mem_wid\n        self.output_dim     = None  # set after env\n\n        if self.circuit_type == ""ntm"":\n            self.hidden_dim      = 100\n            self.num_write_heads = 1\n            self.num_read_heads  = 1\n            self.mem_hei         = 128\n            self.mem_wid         = 20\n            self.clip_value      = 20.   # clips controller and circuit output values to in between\n        elif self.circuit_type == ""dnc"":\n            self.hidden_dim      = 64\n            self.num_write_heads = 1\n            self.num_read_heads  = 4\n            self.mem_hei         = 16\n            self.mem_wid         = 16\n            self.clip_value      = 20.   # clips controller and circuit output values to in between\n\n        self.controller_params = ControllerParams()\n        self.accessor_params   = AccessorParams()\n\nclass AgentParams(Params):  # hyperparameters for drl agents\n    def __init__(self):\n        super(AgentParams, self).__init__()\n\n        if self.agent_type == ""sl"":\n            if self.circuit_type == ""ntm"":\n                self.criteria       = nn.BCELoss()\n                self.optim          = optim.RMSprop\n\n                self.steps          = 100000    # max #iterations\n                self.batch_size     = 16\n                self.early_stop     = None      # max #steps per episode\n                self.clip_grad      = 50.\n                self.lr             = 1e-4\n                self.optim_eps      = 1e-10     # NOTE: we use this setting to be equivalent w/ the default settings in tensorflow\n                self.optim_alpha    = 0.9       # NOTE: only for rmsprop, alpha is the decay in tensorflow, whose default is 0.9\n                self.eval_freq      = 500\n                self.eval_steps     = 50\n                self.prog_freq      = self.eval_freq\n                self.test_nepisodes = 5\n            elif self.circuit_type == ""dnc"":\n                self.criteria       = nn.BCELoss()\n                self.optim          = optim.RMSprop\n\n                self.steps          = 100000    # max #iterations\n                self.batch_size     = 16\n                self.early_stop     = None      # max #steps per episode\n                self.clip_grad      = 50.\n                self.lr             = 1e-4\n                self.optim_eps      = 1e-10     # NOTE: we use this setting to be equivalent w/ the default settings in tensorflow\n                self.optim_alpha    = 0.9       # NOTE: only for rmsprop, alpha is the decay in tensorflow, whose default is 0.9\n                self.eval_freq      = 500\n                self.eval_steps     = 50\n                self.prog_freq      = self.eval_freq\n                self.test_nepisodes = 5\n        elif self.agent_type == ""empty"":\n            self.criteria       = nn.BCELoss()\n            self.optim          = optim.RMSprop\n\n            self.steps          = 100000    # max #iterations\n            self.batch_size     = 16\n            self.early_stop     = None      # max #steps per episode\n            self.clip_grad      = 50.\n            self.lr             = 1e-4\n            self.optim_eps      = 1e-10     # NOTE: we use this setting to be equivalent w/ the default settings in tensorflow\n            self.optim_alpha    = 0.9       # NOTE: only for rmsprop, alpha is the decay in tensorflow, whose default is 0.9\n            self.eval_freq      = 500\n            self.eval_steps     = 50\n            self.prog_freq      = self.eval_freq\n            self.test_nepisodes = 5\n\n        self.env_params     = EnvParams()\n        self.circuit_params = CircuitParams()\n\nclass Options(Params):\n    agent_params  = AgentParams()\n'"
utils/similarities.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nfrom torch.autograd import Variable\n\ndef batch_cosine_sim(u, v, epsilon=1e-6):\n    """"""\n    u: content_key: [batch_size x num_heads x mem_wid]\n    v: memory:      [batch_size x mem_hei   x mem_wid]\n    k: similarity:  [batch_size x num_heads x mem_hei]\n    """"""\n    assert u.dim() == 3 and v.dim() == 3\n    numerator = torch.bmm(u, v.transpose(1, 2))\n    # denominator = torch.sqrt(torch.bmm(u.norm(2, 2).pow(2) + epsilon, v.norm(2, 2).pow(2).transpose(1, 2) + epsilon))                             # 0.1.12\n    denominator = torch.sqrt(torch.bmm(u.norm(2, 2, keepdim=True).pow(2) + epsilon, v.norm(2, 2, keepdim=True).pow(2).transpose(1, 2) + epsilon))   # 0.2.0\n    k = numerator / (denominator + epsilon)\n    return k\n\n# batch_size = 3\n# num_heads = 2\n# mem_hei = 5\n# mem_wid = 7\n# u = torch.ones(batch_size, num_heads, mem_wid)\n# u[0][0][4] = 0\n# u[1][0][4] = 10\n# u[1][1][6] = 10\n#\n# v = torch.ones(batch_size, mem_hei, mem_wid)\n# v[0] = v[0] * 2\n# v[1][0][0] = 0\n# v[1][0][1] = 1\n# v[1][0][2] = 2\n# v[1][0][3] = 3\n# v[1][0][4] = 4\n# v[1][1][4] = 0\n# print(u)\n# print(v)\n#\n# batch_cosine_sim(Variable(u), Variable(v))\n'"
core/accessors/__init__.py,0,b''
core/accessors/dynamic_accessor.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom core.accessor import Accessor\nfrom core.heads.dynamic_write_head import DynamicWriteHead as WriteHead\nfrom core.heads.dynamic_read_head import DynamicReadHead as ReadHead\nfrom core.memory import External2DMemory as ExternalMemory\n\nclass DynamicAccessor(Accessor):\n    def __init__(self, args):\n        super(DynamicAccessor, self).__init__(args)\n        # logging\n        self.logger = args.logger\n        # params\n        self.use_cuda = args.use_cuda\n        self.dtype = args.dtype\n        # dynamic-accessor-specific params\n        self.read_head_params.num_read_modes = self.write_head_params.num_heads * 2 + 1\n\n        self.logger.warning(""<--------------------------------===> Accessor:   {WriteHead, ReadHead, Memory}"")\n\n        # functional components\n        self.usage_vb = None    # for dynamic allocation, init in _reset\n        self.link_vb = None     # for temporal link, init in _reset\n        self.preced_vb = None   # for temporal link, init in _reset\n        self.write_heads = WriteHead(self.write_head_params)\n        self.read_heads = ReadHead(self.read_head_params)\n        self.memory = ExternalMemory(self.memory_params)\n\n        self._reset()\n\n    def _init_weights(self):\n        pass\n\n    def _reset_states(self):\n        # reset the usage (for dynamic allocation) & link (for temporal link)\n        self.usage_vb  = Variable(self.usage_ts).type(self.dtype)\n        self.link_vb   = Variable(self.link_ts).type(self.dtype)\n        self.preced_vb = Variable(self.preced_ts).type(self.dtype)\n        # we reset the write/read weights of heads\n        self.write_heads._reset_states()\n        self.read_heads._reset_states()\n        # we also reset the memory to bias value\n        self.memory._reset_states()\n\n    def _reset(self):           # NOTE: should be called at __init__\n        self._init_weights()\n        self.type(self.dtype)   # put on gpu if possible\n        # reset internal states\n        self.usage_ts  = torch.zeros(self.batch_size, self.mem_hei)\n        self.link_ts   = torch.zeros(self.batch_size, self.write_head_params.num_heads, self.mem_hei, self.mem_hei)\n        self.preced_ts = torch.zeros(self.batch_size, self.write_head_params.num_heads, self.mem_hei)\n        self._reset_states()\n\n    def forward(self, hidden_vb):\n        # 1. first we update the usage using the read/write weights from {t-1}\n        self.usage_vb = self.write_heads._update_usage(self.usage_vb)\n        self.usage_vb = self.read_heads._update_usage(hidden_vb, self.usage_vb)\n        # 2. then write to memory_{t-1} to get memory_{t}\n        self.memory.memory_vb = self.write_heads.forward(hidden_vb, self.memory.memory_vb, self.usage_vb)\n        # 3. then we update the temporal link\n        self.link_vb, self.preced_vb = self.write_heads._temporal_link(self.link_vb, self.preced_vb)\n        # 4. then read from memory_{t} to get read_vec_{t}\n        read_vec_vb = self.read_heads.forward(hidden_vb, self.memory.memory_vb, self.link_vb, self.write_head_params.num_heads)\n        return read_vec_vb\n'"
core/accessors/static_accessor.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom core.accessor import Accessor\nfrom core.heads.static_write_head import StaticWriteHead as WriteHead\nfrom core.heads.static_read_head import StaticReadHead as ReadHead\nfrom core.memory import External2DMemory as ExternalMemory\n\nclass StaticAccessor(Accessor):\n    def __init__(self, args):\n        super(StaticAccessor, self).__init__(args)\n        # logging\n        self.logger = args.logger\n        # params\n        self.use_cuda = args.use_cuda\n        self.dtype = args.dtype\n\n        self.logger.warning(""<--------------------------------===> Accessor:   {WriteHead, ReadHead, Memory}"")\n\n        # functional components\n        self.write_heads = WriteHead(self.write_head_params)\n        self.read_heads = ReadHead(self.read_head_params)\n        self.memory = ExternalMemory(self.memory_params)\n\n        self._reset()\n\n    def _init_weights(self):\n        pass\n\n    def _reset_states(self):\n        # we reset the write/read weights of heads\n        self.write_heads._reset_states()\n        self.read_heads._reset_states()\n        # we also reset the memory to bias value\n        self.memory._reset_states()\n\n    def _reset(self):           # NOTE: should be called at __init__\n        self._init_weights()\n        self.type(self.dtype)   # put on gpu if possible\n        # reset internal states\n        self._reset_states()\n\n    def forward(self, hidden_vb):\n        # 1. first write to memory_{t-1} to get memory_{t}\n        self.memory.memory_vb = self.write_heads.forward(hidden_vb, self.memory.memory_vb)\n        # 2. then read from memory_{t} to get read_vec_{t}\n        read_vec_vb = self.read_heads.forward(hidden_vb, self.memory.memory_vb)\n        return read_vec_vb\n'"
core/agents/__init__.py,0,b''
core/agents/empty_agent.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport random\nimport torch\nfrom torch.autograd import Variable\n\nfrom utils.helpers import Experience\nfrom core.agent import Agent\n\nclass EmptyAgent(Agent):\n    def __init__(self, args, env_prototype, circuit_prototype):\n        super(EmptyAgent, self).__init__(args, env_prototype, circuit_prototype)\n        self.logger.warning(""<===================================> Empty"")\n\n        # env\n        self.env_params.batch_size = args.batch_size\n        self.env = self.env_prototype(self.env_params)\n        self.state_shape = self.env.state_shape\n        self.action_dim  = self.env.action_dim\n\n        self._reset_experience()\n\n    def _preprocessState(self, state):\n        state_ts = torch.from_numpy(state).type(self.dtype)\n        return state_ts\n\n    def _forward(self, observation):\n        # NOTE: we update the output_vb and target_vb here\n        input_ts = self._preprocessState(observation[0])\n        self.target_vb = Variable(self._preprocessState(observation[1]))\n        self.mask_ts   = self._preprocessState(observation[2]).expand_as(self.target_vb)\n\n        self.env.visual(input_ts, self.target_vb.data, self.mask_ts)\n\n    def _backward(self, reward, terminal):\n        pass\n\n    def _eval_model(self):\n        pass\n\n    def fit_model(self):    # the most basic control loop, to ease integration of new envs\n        self.step = 0\n        should_start_new = True\n        while self.step < self.steps:\n            if should_start_new:\n                self._reset_experience()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                should_start_new = False\n            # action = random.randrange(self.action_dim)      # thus we only randomly sample actions here, since the model hasn\'t been updated at all till now\n            action = self._forward(self.experience.state1)\n            self.experience = self.env.step(action)\n            if self.experience.terminal1 or self.early_stop and (episode_steps + 1) >= self.early_stop:\n                should_start_new = True\n\n            self.step += 1\n            raw_input()\n\n    def test_model(self):\n        pass\n'"
core/agents/sl_agent.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport random\nimport time\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nfrom utils.helpers import Experience\nfrom core.agent import Agent\n\nclass SLAgent(Agent):   # for supervised learning tasks\n    def __init__(self, args, env_prototype, circuit_prototype):\n        super(SLAgent, self).__init__(args, env_prototype, circuit_prototype)\n        self.logger.warning(""<===================================> Agent:"")\n\n        # env\n        self.env_params.batch_size = args.batch_size\n        self.env = self.env_prototype(self.env_params)\n        self.state_shape = self.env.state_shape\n        self.action_dim  = self.env.action_dim\n\n        # circuit\n        self.circuit_params = args.circuit_params\n        self.circuit_params.batch_size = args.batch_size\n        self.circuit_params.input_dim = self.state_shape\n        self.circuit_params.output_dim = self.action_dim\n        self.circuit = self.circuit_prototype(self.circuit_params)\n        self._load_model(self.model_file)   # load pretrained circuit if provided\n\n        self._reset_experience()\n\n    def _reset_training_loggings(self):\n        self._reset_testing_loggings()\n        self.training_loss_avg_log = []\n\n    def _reset_testing_loggings(self):\n        # setup logging for testing/evaluation stats\n        self.loss_avg_log = []\n        # placeholders for windows for online curve plotting\n        if self.visualize:\n            self.win_loss_avg = ""win_loss_avg""\n\n    def _preprocessState(self, state):\n        state_ts = torch.from_numpy(state).type(self.dtype)\n        return state_ts\n\n    def _forward(self, observation):\n        # first we need to reset all the necessary states\n        self.circuit._reset_states()\n        # NOTE: we update the output_vb and target_vb here\n        input_ts = self._preprocessState(observation[0])\n        self.target_vb = Variable(self._preprocessState(observation[1]))\n        self.mask_ts   = self._preprocessState(observation[2]).expand_as(self.target_vb)\n        self.output_vb = None\n\n        for i in range(input_ts.size(0)):\n            # feed in one row of the sequence per time\n            output_vb = self.circuit.forward(Variable(input_ts[i]))\n            if self.output_vb is None:\n                self.output_vb = output_vb\n            else:\n                self.output_vb = torch.cat((self.output_vb, output_vb), 0)\n\n            # NOTE: this part is for examine the heads\' weights and memory usage\n            # NOTE: only used during testing, cos visualization takes time\n            if self.mode == 2 and self.visualize:\n                self.env.visual(input_ts[i,0,:].unsqueeze(0).unsqueeze(1),\n                                self.target_vb.data[i,0,:].unsqueeze(0).unsqueeze(1),\n                                self.mask_ts[i,0,:].unsqueeze(0).unsqueeze(1),\n                                self.output_vb.data[i,0,:].unsqueeze(0).unsqueeze(1))\n                self.circuit.accessor.visual()\n                raw_input()\n\n        if not self.training and self.visualize:\n            self.env.visual(input_ts, self.target_vb.data, self.mask_ts, self.output_vb.data)\n\n        return 0    # for all the supervised tasks we just return a 0 to keep the same format as rl\n\n    def _backward(self):\n        # TODO: we need to have a custom loss function to take mask into account\n        # TODO: pass in this way might be too unefficient, but it\'s ok for now\n        if self.training:\n            self.optimizer.zero_grad()\n        loss_vb = F.binary_cross_entropy(input=self.output_vb.transpose(0, 1).contiguous().view(1, -1),\n                                         target=self.target_vb.transpose(0, 1).contiguous().view(1, -1),\n                                         weight=self.mask_ts.transpose(0, 1).contiguous().view(1, -1))\n        loss_vb /= self.batch_size\n        if self.training:\n            loss_vb.backward()\n            self.optimizer.step()\n\n        return loss_vb.data[0]\n\n    def fit_model(self):    # the most basic control loop, to ease integration of new envs\n        # self.optimizer = self.optim(self.circuit.parameters(), lr=self.lr)              # adam\n        self.optimizer = self.optim(self.circuit.parameters(), lr=self.lr, eps=self.optim_eps, alpha=self.optim_alpha)   # rmsprop\n\n        self.logger.warning(""<===================================> Training ..."")\n        self.training = True\n        self._reset_training_loggings()\n\n        self.start_time = time.time()\n        self.step = 0\n\n        should_start_new = True\n        while self.step < self.steps:\n            if should_start_new:\n                self._reset_experience()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                should_start_new = False\n            action = self._forward(self.experience.state1)\n            self.experience = self.env.step(action)\n            if self.experience.terminal1 or self.early_stop and (episode_steps + 1) >= self.early_stop:\n                should_start_new = True\n\n            # calculate loss\n            loss = self._backward()\n            self.training_loss_avg_log.append([loss])\n\n            self.step += 1\n\n            # report training stats\n            if self.step % self.prog_freq == 0:\n                self.logger.warning(""Reporting       @ Step: "" + str(self.step) + "" | Elapsed Time: "" + str(time.time() - self.start_time))\n                self.logger.warning(""Training Stats:   avg_loss:         {}"".format(np.mean(np.asarray(self.training_loss_avg_log))))\n\n            # evaluation & checkpointing\n            if self.step % self.eval_freq == 0:\n                # Set states for evaluation\n                self.training = False\n                self.logger.warning(""Evaluating      @ Step: "" + str(self.step))\n                self._eval_model()\n\n                # Set states for resume training\n                self.training = True\n                self.logger.warning(""Resume Training @ Step: "" + str(self.step))\n                should_start_new = True\n\n        print(""total time:"", time.time() - self.start_time)\n\n    def _eval_model(self):\n        self.training = False\n\n        eval_start_time = time.time()\n        eval_step = 0\n\n        eval_loss_avg_log = []\n        eval_should_start_new = True\n        while eval_step < self.eval_steps:\n            if eval_should_start_new:\n                self._reset_experience()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                # if self.visualize: self.env.visual()\n                # if self.render: self.env.render()\n                eval_should_start_new = False\n            eval_action = self._forward(self.experience.state1)\n            self.experience = self.env.step(eval_action)\n            if self.experience.terminal1:# or self.early_stop and (episode_steps + 1) >= self.early_stop:\n                eval_should_start_new = True\n\n            # calculate loss\n            eval_loss = self._backward()\n            eval_loss_avg_log.append([eval_loss])\n\n            eval_step += 1\n\n        # Logging for this evaluation phase\n        self.loss_avg_log.append([self.step, np.mean(np.asarray(eval_loss_avg_log))]); del eval_loss_avg_log\n        # plotting\n        if self.visualize:\n            self.win_loss_avg = self.vis.scatter(X=np.array(self.loss_avg_log), env=self.refs, win=self.win_loss_avg, opts=dict(title=""loss_avg""))\n        # logging\n        self.logger.warning(""Evaluation        Took: "" + str(time.time() - eval_start_time))\n        self.logger.warning(""Iteration: {}; loss_avg: {}"".format(self.step, self.loss_avg_log[-1][1]))\n\n        # save model\n        self._save_model(self.step, 0.) # TODO: here should pass in the negative loss\n\n    def test_model(self):\n        self.logger.warning(""<===================================> Testing ..."")\n        self.training = False\n        self._reset_testing_loggings()\n\n        self.start_time = time.time()\n        self.step = 0\n\n        test_loss_avg_log = []\n        test_should_start_new = True\n        while self.step < self.test_nepisodes:\n            if test_should_start_new:\n                self._reset_experience()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                test_should_start_new = False\n            test_action = self._forward(self.experience.state1)\n            self.experience = self.env.step(test_action)\n            if self.experience.terminal1:# or self.early_stop and (episode_steps + 1) >= self.early_stop:\n                test_should_start_new = True\n\n            # calculate loss\n            test_loss = self._backward()\n            test_loss_avg_log.append([test_loss])\n\n            self.step += 1\n\n        # Logging for this evaluation phase\n        self.loss_avg_log.append([self.step, np.mean(np.asarray(test_loss_avg_log))]); del test_loss_avg_log\n        # plotting\n        if self.visualize:\n            self.win_loss_avg = self.vis.scatter(X=np.array(self.loss_avg_log), env=self.refs, win=self.win_loss_avg, opts=dict(title=""loss_avg""))\n        # logging\n        self.logger.warning(""Testing  Took: "" + str(time.time() - self.start_time))\n        self.logger.warning(""Iteration: {}; loss_avg: {}"".format(self.step, self.loss_avg_log[-1][1]))\n'"
core/circuits/__init__.py,0,b''
core/circuits/dnc_circuit.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.nn as nn\n\nfrom core.circuit import Circuit\nfrom core.controllers.lstm_controller import LSTMController as Controller\nfrom core.accessors.dynamic_accessor import DynamicAccessor as Accessor\n\nclass DNCCircuit(Circuit):\n    def __init__(self, args):\n        super(DNCCircuit, self).__init__(args)\n\n        # functional components\n        self.controller = Controller(self.controller_params)\n        self.accessor = Accessor(self.accessor_params)\n\n        # build model\n        self.hid_to_out = nn.Linear(self.hidden_dim + self.read_vec_dim, self.output_dim)\n\n        self._reset()\n\n    def _init_weights(self):\n        pass\n'"
core/circuits/ntm_circuit.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.nn as nn\n\nfrom core.circuit import Circuit\nfrom core.controllers.lstm_controller import LSTMController as Controller\nfrom core.accessors.static_accessor import StaticAccessor as Accessor\n\nclass NTMCircuit(Circuit):\n    def __init__(self, args):\n        super(NTMCircuit, self).__init__(args)\n\n        # functional components\n        self.controller = Controller(self.controller_params)\n        self.accessor = Accessor(self.accessor_params)\n\n        # build model\n        self.hid_to_out = nn.Linear(self.hidden_dim + self.read_vec_dim, self.output_dim)\n\n        self._reset()\n\n    def _init_weights(self):\n        pass\n'"
core/controllers/__init__.py,0,b''
core/controllers/lstm_controller.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.controller import Controller\n\nclass LSTMController(Controller):\n    def __init__(self, args):\n        super(LSTMController, self).__init__(args)\n\n        # build model\n        self.in_2_hid = nn.LSTMCell(self.input_dim + self.read_vec_dim, self.hidden_dim, 1)\n\n        self._reset()\n\n    def _init_weights(self):\n        pass\n\n    def forward(self, input_vb, read_vec_vb):\n        self.lstm_hidden_vb = self.in_2_hid(torch.cat((input_vb.contiguous().view(-1, self.input_dim),\n                                                       read_vec_vb.contiguous().view(-1, self.read_vec_dim)), 1),\n                                            self.lstm_hidden_vb)\n\n        # we clip the controller hidden states here\n        self.lstm_hidden_vb = [self.lstm_hidden_vb[0].clamp(min=-self.clip_value, max=self.clip_value),\n                               self.lstm_hidden_vb[1]]\n\n        return self.lstm_hidden_vb[0]\n'"
core/envs/__init__.py,0,b''
core/envs/copy_env.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom random import randint\nimport torch\n\nfrom core.env import Env\n\nclass CopyEnv(Env):\n    def __init__(self, args, env_ind=0):\n        super(CopyEnv, self).__init__(args, env_ind)\n\n        # state space setup\n        self.batch_size = args.batch_size\n        self.len_word = args.len_word\n        self.min_num_words = args.min_num_words\n        self.max_num_words = args.max_num_words\n        self.logger.warning(""Word     {length}:   {%s}"", self.len_word)\n        self.logger.warning(""Words #  {min, max}: {%s, %s}"", self.min_num_words, self.max_num_words)\n\n    def _preprocessState(self, state):\n        # NOTE: state input in size: batch_size x num_words  x len_word\n        # NOTE: we return as:        num_words  x batch_size x len_word\n        # NOTE: to ease feeding in one row from all batches per forward pass\n        for i in range(len(state)):\n            state[i] = np.transpose(state[i], (1, 0, 2))\n        return state\n\n    @property\n    def state_shape(self):\n        # NOTE: we use this as the input_dim to be consistent with the sl & rl tasks\n        return self.len_word + 2\n\n    @property\n    def action_dim(self):\n        # NOTE: we use this as the output_dim to be consistent with the sl & rl tasks\n        return self.len_word\n\n    def render(self):\n        pass\n\n    def _readable(self, datum):\n        return \'+\' + \' \'.join([\'-\' if x == 0 else \'%d\' % x for x in datum]) + \'+\'\n\n    def visual(self, input_ts, target_ts, mask_ts, output_ts=None):\n        """"""\n        input_ts:  [(num_wordsx2+2) x batch_size x (len_word+2)]\n        target_ts: [(num_wordsx2+2) x batch_size x (len_word)]\n        mask_ts:   [(num_wordsx2+2) x batch_size x (len_word)]\n        output_ts: [(num_wordsx2+2) x batch_size x (len_word)]\n        """"""\n        output_ts = torch.round(output_ts * mask_ts) if output_ts is not None else None\n        input_strings  = [self._readable(input_ts[:, 0, i])  for i in range(input_ts.size(2))]\n        target_strings = [self._readable(target_ts[:, 0, i]) for i in range(target_ts.size(2))]\n        mask_strings   = [self._readable(mask_ts[:, 0, 0])]\n        output_strings = [self._readable(output_ts[:, 0, i]) for i in range(output_ts.size(2))] if output_ts is not None else None\n        input_strings  = \'Input:\\n\'  + \'\\n\'.join(input_strings)\n        target_strings = \'Target:\\n\' + \'\\n\'.join(target_strings)\n        mask_strings   = \'Mask:\\n\'   + \'\\n\'.join(mask_strings)\n        output_strings = \'Output:\\n\' + \'\\n\'.join(output_strings) if output_ts is not None else None\n        # strings = [input_strings, target_strings, mask_strings, output_strings]\n        # self.logger.warning(input_strings)\n        # self.logger.warning(target_strings)\n        # self.logger.warning(mask_strings)\n        # self.logger.warning(output_strings)\n        print(input_strings)\n        print(target_strings)\n        print(mask_strings)\n        print(output_strings) if output_ts is not None else None\n\n    def sample_random_action(self):\n        pass\n\n    def _generate_sequence(self):\n        """"""\n        generates [batch_size x num_words x len_word] data and\n        prepare input & target & mask\n\n        Returns:\n        exp_state1[0] (input) : starts w/ a start bit, then the seq to be copied\n                              : then an end bit, then 0\'s\n            [0 ... 0, 1, 0;   # start bit\n             data   , 0, 0;   # data with padded 0\'s\n             0 ... 0, 0, 1;   # end bit\n             0 ......... 0]   # num_words   rows of 0\'s\n        exp_state1[1] (target): 0\'s until after inputs has the end bit, then the\n                              : seq to be copied, but w/o the extra channels for\n                              : tart and end bits\n            [0 ... 0;         # num_words+2 rows of 0\'s\n             data   ]         # data\n        exp_state1[2] (mask)  : 1\'s for all row corresponding to the target\n                              : 0\'s otherwise}\n            [0;               # num_words+2 rows of 0\'s\n             1];              # num_words rows of 1\'s\n        NOTE: we pad extra rows of 0\'s to the end of those batches with smaller\n        NOTE: length to make sure each sample in one batch has the same length\n        """"""\n        self.exp_state1 = []\n        # we prepare input, target, mask for each batch\n        batch_num_words     = np.random.randint(self.min_num_words, self.max_num_words+1, size=(self.batch_size))\n        max_batch_num_words = np.max(batch_num_words)\n\n        self.exp_state1.append(np.zeros((self.batch_size, max_batch_num_words * 2 + 2, self.len_word + 2))) # input\n        self.exp_state1.append(np.zeros((self.batch_size, max_batch_num_words * 2 + 2, self.len_word)))     # target\n        self.exp_state1.append(np.zeros((self.batch_size, max_batch_num_words * 2 + 2, 1)))                 # mask\n        for batch_ind in range(self.batch_size):\n            num_words = batch_num_words[batch_ind]\n            data      = np.random.randint(2, size=(num_words, self.len_word))\n            # prepare input  for this sample\n            self.exp_state1[0][batch_ind][0][-2] = 1            # set start bit\n            self.exp_state1[0][batch_ind][1:num_words+1, 0:-2] = data\n            self.exp_state1[0][batch_ind][num_words+1][-1] = 1  # set end bit\n            # prepare target for this sample\n            self.exp_state1[1][batch_ind][num_words+2:num_words*2+2, :] = data\n            # prepare mask   for this sample\n            self.exp_state1[2][batch_ind][num_words+2:num_words*2+2, :] = 1\n\n    def reset(self):\n        self._reset_experience()\n        self._generate_sequence()\n        return self._get_experience()\n\n    def step(self, action_index):\n        self.exp_action = action_index\n        self._generate_sequence()\n        return self._get_experience()\n'"
core/envs/repeat_copy_env.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom random import randint\nimport torch\n\nfrom core.env import Env\n\nclass RepeatCopyEnv(Env):\n    def __init__(self, args, env_ind=0):\n        super(RepeatCopyEnv, self).__init__(args, env_ind)\n\n        # state space setup\n        self.batch_size = args.batch_size\n        self.len_word = args.len_word\n        self.min_num_words = args.min_num_words\n        self.max_num_words = args.max_num_words\n        self.min_repeats = args.min_repeats\n        self.max_repeats = args.max_repeats\n        self.max_repeats_norm = args.max_repeats_norm   # to normalize the repeat flag to make it easier for the network\n        self.logger.warning(""Word     {length}:   {%s}"", self.len_word)\n        self.logger.warning(""Words #  {min, max}: {%s, %s}"", self.min_num_words, self.max_num_words)\n        self.logger.warning(""Repeats  {min, max}: {%s, %s}"", self.min_repeats, self.max_repeats)\n\n    def _preprocessState(self, state):\n        # NOTE: state input in size: batch_size x num_words  x len_word\n        # NOTE: we return as:        num_words  x batch_size x len_word\n        # NOTE: to ease feeding in one row from all batches per forward pass\n        for i in range(len(state)):\n            state[i] = np.transpose(state[i], (1, 0, 2))\n        return state\n\n    @property\n    def state_shape(self):\n        # NOTE: we use this as the input_dim to be consistent with the sl & rl tasks\n        return self.len_word + 2\n\n    @property\n    def action_dim(self):\n        # NOTE: we use this as the output_dim to be consistent with the sl & rl tasks\n        # NOTE: this is different from copy, cos it also has to output an end bit\n        return self.len_word + 1\n\n    def render(self):\n        pass\n\n    def _readable(self, datum):\n        return \'+\' + \' \'.join([\'-\' if x == 0 else \'%d\' % x for x in datum]) + \'+\'\n\n    def visual(self, input_ts, target_ts, mask_ts, output_ts=None):\n        """"""\n        input_ts:  [(num_wordsx(repeats+1)+3) x batch_size x (len_word+2)]\n        target_ts: [(num_wordsx(repeats+1)+3) x batch_size x (len_word+1)]\n        mask_ts:   [(num_wordsx(repeats+1)+3) x batch_size x (len_word+1)]\n        output_ts: [(num_wordsx(repeats+1)+3) x batch_size x (len_word+1)]\n        """"""\n        input_ts  = self._unnormalize_repeats(input_ts)\n        output_ts = torch.round(output_ts * mask_ts) if output_ts is not None else None\n        input_strings  = [self._readable(input_ts[:, 0, i])  for i in range(input_ts.size(2))]\n        target_strings = [self._readable(target_ts[:, 0, i]) for i in range(target_ts.size(2))]\n        mask_strings   = [self._readable(mask_ts[:, 0, 0])]\n        output_strings = [self._readable(output_ts[:, 0, i]) for i in range(output_ts.size(2))] if output_ts is not None else None\n        input_strings  = \'Input:\\n\'  + \'\\n\'.join(input_strings)\n        target_strings = \'Target:\\n\' + \'\\n\'.join(target_strings)\n        mask_strings   = \'Mask:\\n\'   + \'\\n\'.join(mask_strings)\n        output_strings = \'Output:\\n\' + \'\\n\'.join(output_strings) if output_ts is not None else None\n        # strings = [input_strings, target_strings, mask_strings, output_strings]\n        # self.logger.warning(input_strings)\n        # self.logger.warning(target_strings)\n        # self.logger.warning(mask_strings)\n        # self.logger.warning(output_strings)\n        print(input_strings)\n        print(target_strings)\n        print(mask_strings)\n        print(output_strings) if output_ts is not None else None\n\n    def sample_random_action(self):\n        pass\n\n    def _normalize_repeats(self, repeats):\n        return repeats / self.max_repeats_norm\n\n    def _unnormalize_repeats(self, input_ts):\n        if input_ts.size(1) == 1:\n            return input_ts\n        else:\n            return input_ts.cpu() * self.unnormalize_ts.transpose(0, 1)\n\n    def _generate_sequence(self):\n        """"""\n        generates [batch_size x num_words x len_word] data and\n        prepare input & target & mask\n\n        Returns:\n        exp_state1[0] (input) : starts w/ a start bit, then the seq to be copied\n                              : then an repeat flag, then 0\'s\n            [0 ... 0, 1, 0;   # start bit\n             data   , 0, 0;   # data with padded 0\'s\n             0 ... 0, 0, 3;   # repeat flag (would also be normaized)\n             0 ......... 0]   # num_wordsxrepeats+1 rows of 0\'s\n        exp_state1[1] (target): 0\'s until after inputs has the repeat flag, then\n                              : the seq to be copied, then an end bit\n            [0 ... 0, 0;      # num_words+2 rows of 0\'s\n             data   , 0;      # data\n             data   , 0;      # data\n             data   , 0;      # data\n             0 ... 0, 1;]     # end bit\n        exp_state1[2] (mask)  : 1\'s for all row corresponding to the target\n                              : 0\'s otherwise}\n            [0;               # num_words+2 rows of 0\'s\n             1];              # num_wordsxrepeats+1 rows of 1\'s\n        NOTE: we pad extra rows of 0\'s to the end of those batches with smaller\n        NOTE: length to make sure each sample in one batch has the same length\n        """"""\n        self.exp_state1 = []\n        # we prepare input, target, mask for each batch\n        batch_num_words     = np.random.randint(self.min_num_words, self.max_num_words+1, size=(self.batch_size))\n        batch_repeats       = np.random.randint(self.min_repeats, self.max_repeats+1, size=(self.batch_size))\n        max_batch_num_words = np.max(batch_num_words)\n        max_batch_repeats   = np.max(batch_repeats)\n\n        self.exp_state1.append(np.zeros((self.batch_size, max_batch_num_words * (max_batch_repeats + 1) + 3, self.len_word + 2))) # input\n        self.exp_state1.append(np.zeros((self.batch_size, max_batch_num_words * (max_batch_repeats + 1) + 3, self.len_word + 1))) # target\n        self.exp_state1.append(np.zeros((self.batch_size, max_batch_num_words * (max_batch_repeats + 1) + 3, 1)))                 # mask\n        self.unnormalize_ts = torch.ones(self.batch_size, max_batch_num_words * (max_batch_repeats + 1) + 3, self.len_word + 2)\n        for batch_ind in range(self.batch_size):\n            num_words = batch_num_words[batch_ind]\n            repeats   = batch_repeats[batch_ind]\n            data      = np.random.randint(2, size=(num_words, self.len_word))\n            data_rep  = np.tile(data, (repeats, 1))\n            # prepare input  for this sample\n            self.exp_state1[0][batch_ind][0][-2] = 1                        # set start bit\n            self.exp_state1[0][batch_ind][1:num_words+1, 0:self.len_word] = data\n            self.exp_state1[0][batch_ind][num_words+1][-1] = self._normalize_repeats(repeats)   # normalize the repeat flag\n            self.unnormalize_ts[batch_ind][num_words+1][-1] = self.max_repeats_norm             # to ease visualization w/ unnormalized repeat flag\n            # prepare target for this sample\n            self.exp_state1[1][batch_ind][num_words+2:num_words*(repeats+1)+2, 0:self.len_word] = data_rep\n            self.exp_state1[1][batch_ind][num_words*(repeats+1)+2][-1] = 1  # set end bit\n            # prepare mask   for this sample\n            self.exp_state1[2][batch_ind][num_words+2:num_words*(repeats+1)+3, :] = 1\n\n    def reset(self):\n        self._reset_experience()\n        self._generate_sequence()\n        return self._get_experience()\n\n    def step(self, action_index):\n        self.exp_action = action_index\n        self._generate_sequence()\n        return self._get_experience()\n'"
core/heads/__init__.py,0,b''
core/heads/dynamic_head.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom core.head import Head\nfrom utils.similarities import batch_cosine_sim\n\nclass DynamicHead(Head):\n    def __init__(self, args):\n        super(DynamicHead, self).__init__(args)\n\n        # build model\n        # for content focus\n        self.hid_2_key  = nn.Linear(self.hidden_dim, self.num_heads * self.mem_wid)\n        self.hid_2_beta = nn.Linear(self.hidden_dim, self.num_heads * 1)\n\n    def _update_usage(self, prev_usage_vb):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def _content_focus(self, memory_vb):\n        """"""\n        variables needed:\n            key_vb:    [batch_size x num_heads x mem_wid]\n                    -> similarity key vector, to compare to each row in memory\n                    -> by cosine similarity\n            beta_vb:   [batch_size x num_heads x 1]\n                    -> NOTE: refer here: https://github.com/deepmind/dnc/issues/9\n                    -> \\in (1, +inf) after oneplus(); similarity key strength\n                    -> amplify or attenuate the pecision of the focus\n            memory_vb: [batch_size x mem_hei   x mem_wid]\n        returns:\n            wc_vb:     [batch_size x num_heads x mem_hei]\n                    -> the attention weight by content focus\n        """"""\n        K_vb = batch_cosine_sim(self.key_vb, memory_vb)  # [batch_size x num_heads x mem_hei]\n        self.wc_vb = K_vb * self.beta_vb.expand_as(K_vb) # [batch_size x num_heads x mem_hei]\n        self.wc_vb = F.softmax(self.wc_vb.transpose(0, 2)).transpose(0, 2)\n\n    def _location_focus(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def forward(self, hidden_vb, memory_vb):\n        # outputs for computing addressing for heads\n        # NOTE: to be consistent w/ the dnc paper, we use\n        # NOTE: sigmoid to constrain to [0, 1]\n        # NOTE: oneplus to constrain to [1, +inf]\n        self.key_vb   = F.tanh(self.hid_2_key(hidden_vb)).view(-1, self.num_heads, self.mem_wid)    # TODO: relu to bias the memory to store positive values ??? check again\n        self.beta_vb  = F.softplus(self.hid_2_beta(hidden_vb)).view(-1, self.num_heads, 1)          # beta >=1: https://github.com/deepmind/dnc/issues/9\n\n        # now we compute the addressing mechanism\n        self._content_focus(memory_vb)\n'"
core/heads/dynamic_read_head.py,9,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.heads.dynamic_head import DynamicHead\n\nclass DynamicReadHead(DynamicHead):\n    def __init__(self, args):\n        super(DynamicReadHead, self).__init__(args)\n        # params\n        if self.visualize:\n            self.win_head = ""win_read_head""\n        # dynamic-accessor-specific params\n        # NOTE: mixing between backwards and forwards position (for each write head)\n        # NOTE: and content-based lookup, for each read head\n        self.num_read_modes = args.num_read_modes\n\n        # build model: add additional outs for read\n        # for locatoin focus: temporal linkage\n        self.hid_2_free_gate = nn.Linear(self.hidden_dim, self.num_heads * 1)\n        self.hid_2_read_mode = nn.Linear(self.hidden_dim, self.num_heads * self.num_read_modes)\n\n        # logging\n        self.logger.warning(""<-----------------------------------> ReadHeads:  {"" + str(self.num_heads) + "" heads}"")\n        self.logger.warning(self)\n\n        # reset\n        self._reset()\n\n    def visual(self):\n        if self.visualize:      # here we visualize the wl_curr of the first batch\n            self.win_head = self.vis.heatmap(self.wl_curr_vb.data[0].clone().cpu().transpose(0, 1).numpy(), env=self.refs, win=self.win_head, opts=dict(title=""read_head""))\n\n    def _update_usage(self, hidden_vb, prev_usage_vb):\n        """"""\n        calculates the new usage after reading and freeing from memory\n        variables needed:\n            hidden_vb:     [batch_size x hidden_dim]\n            prev_usage_vb: [batch_size x mem_hei]\n            free_gate_vb:  [batch_size x num_heads x 1]\n            wl_prev_vb:    [batch_size x num_heads x mem_hei]\n        returns:\n            usage_vb:      [batch_size x mem_hei]\n        """"""\n        self.free_gate_vb = F.sigmoid(self.hid_2_free_gate(hidden_vb)).view(-1, self.num_heads, 1)\n        free_read_weights_vb = self.free_gate_vb.expand_as(self.wl_prev_vb) * self.wl_prev_vb\n        psi_vb = torch.prod(1. - free_read_weights_vb, 1)\n        return prev_usage_vb * psi_vb\n\n    def _directional_read_weights(self, link_vb, num_write_heads, forward):\n        """"""\n        calculates the forward or the backward read weights\n        for each read head (at a given address), there are `num_writes` link\n        graphs to follow. thus this function computes a read address for each of\n        the `num_reads * num_writes` pairs of read and write heads.\n        we calculate the forward and backward directions for each pair of read\n        and write heads; hence we need to tile the read weights and do a sort of\n        ""outer product"" to get this.\n        variables needed:\n            link_vb:    [batch_size x num_read_heads x mem_hei x mem_hei]\n                     -> {L_t}, current link graph\n            wl_prev_vb: [batch_size x num_read_heads x mem_hei]\n                     -> containing the previous read weights w_{t-1}^r.\n            num_write_heads: NOTE: self.num_heads here is num_read_heads\n            forward:    boolean\n                     -> indicating whether to follow the ""future"" (True)\n                     -> direction in the link graph or the ""past"" (False)\n                     -> direction\n        returns:\n            directional_weights_vb: [batch_size x num_read_heads x num_write_heads x mem_hei]\n        """"""\n        expanded_read_weights_vb = self.wl_prev_vb.unsqueeze(1).expand_as(torch.Tensor(self.batch_size, num_write_heads, self.num_heads, self.mem_hei)).contiguous()\n        if forward:\n            directional_weights_vb = torch.bmm(expanded_read_weights_vb.view(-1, self.num_heads, self.mem_hei), link_vb.view(-1, self.mem_hei, self.mem_hei).transpose(1, 2))\n        else:\n            directional_weights_vb = torch.bmm(expanded_read_weights_vb.view(-1, self.num_heads, self.mem_hei), link_vb.view(-1, self.mem_hei, self.mem_hei))\n        return directional_weights_vb.view(-1, num_write_heads, self.num_heads, self.mem_hei).transpose(1, 2)\n\n    def _location_focus(self, link_vb, num_write_heads): # temporal linkage\n        """"""\n        calculates the read weights after location focus\n        variables needed:\n            link_vb:      [batch_size x num_heads x mem_hei x mem_hei]\n                       -> {L_t}, current link graph\n            num_write_heads: NOTE: self.num_heads here is num_read_heads\n            wc_vb:        [batch_size x num_heads x mem_hei]\n                       -> containing the focus by content of {t}\n            read_mode_vb: [batch_size x num_heads x num_read_modes]\n        returns:\n            wl_curr_vb:   [batch_size x num_read_heads x num_write_heads x mem_hei]\n                       -> focus by content of {t}\n        """"""\n        # calculates f_t^i & b_t^i\n        forward_weights_vb  = self._directional_read_weights(link_vb, num_write_heads, True)\n        backward_weights_vb = self._directional_read_weights(link_vb, num_write_heads, False)\n        backward_mode_vb = self.read_mode_vb[:, :, :num_write_heads]\n        forward_mode_vb  = self.read_mode_vb[:, :, num_write_heads:2*num_write_heads]\n        content_mode_vb  = self.read_mode_vb[:, :, 2*num_write_heads:]\n        self.wl_curr_vb = content_mode_vb.expand_as(self.wc_vb) * self.wc_vb\\\n                        + torch.sum(forward_mode_vb.unsqueeze(3).expand_as(forward_weights_vb) * forward_weights_vb, 2) \\\n                        + torch.sum(backward_mode_vb.unsqueeze(3).expand_as(backward_weights_vb) * backward_weights_vb, 2)\n\n    def _access(self, memory_vb): # read\n        """"""\n        variables needed:\n            wl_curr_vb:   [batch_size x num_heads x mem_hei]\n                       -> location focus of {t}\n            memory_vb:    [batch_size x mem_hei   x mem_wid]\n        returns:\n            read_vec_vb:  [batch_size x num_heads x mem_wid]\n                       -> read vector of {t}\n        """"""\n        return torch.bmm(self.wl_curr_vb, memory_vb)\n\n    def forward(self, hidden_vb, memory_vb, link_vb, num_write_heads):\n        # content focus\n        super(DynamicReadHead, self).forward(hidden_vb, memory_vb)\n        # location focus\n        self.read_mode_vb = F.softmax(self.hid_2_read_mode(hidden_vb).view(-1, self.num_heads, self.num_read_modes).transpose(0, 2)).transpose(0, 2)\n        self._location_focus(link_vb, num_write_heads)\n        self.wl_prev_vb = self.wl_curr_vb\n        # access\n        return self._access(memory_vb)\n'"
core/heads/dynamic_write_head.py,16,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils.fake_ops import fake_cumprod\nfrom core.heads.dynamic_head import DynamicHead\n\nclass DynamicWriteHead(DynamicHead):\n    def __init__(self, args):\n        super(DynamicWriteHead, self).__init__(args)\n        # params\n        if self.visualize:\n            self.win_head = ""win_write_head""\n\n        # buid model: add additional outs for write\n        # for location focus: dynamic allocation\n        self.hid_2_alloc_gate = nn.Linear(self.hidden_dim, self.num_heads * 1)\n        self.hid_2_write_gate = nn.Linear(self.hidden_dim, self.num_heads * 1)\n        # for access\n        self.hid_2_erase = nn.Linear(self.hidden_dim, self.num_heads * self.mem_wid)\n        self.hid_2_add   = nn.Linear(self.hidden_dim, self.num_heads * self.mem_wid) # the write vector in dnc, called ""add"" in ntm\n\n        # logging\n        self.logger.warning(""<-----------------------------------> WriteHeads: {"" + str(self.num_heads) + "" heads}"")\n        self.logger.warning(self)\n\n        # reset\n        self._reset()\n\n    def visual(self):\n        if self.visualize:      # here we visualize the wl_curr of the first batch\n            self.win_head = self.vis.heatmap(self.wl_curr_vb.data[0].clone().cpu().transpose(0, 1).numpy(), env=self.refs, win=self.win_head, opts=dict(title=""write_head""))\n\n    def _update_usage(self, prev_usage_vb):\n        """"""\n        calculates the new usage after writing to memory\n        variables needed:\n            prev_usage_vb: [batch_size x mem_hei]\n            wl_prev_vb:    [batch_size x num_write_heads x mem_hei]\n        returns:\n            usage_vb:      [batch_size x mem_hei]\n        """"""\n        # calculate the aggregated effect of all write heads\n        # NOTE: how multiple write heads are delt w/ is not discussed in the paper\n        # NOTE: this part is only shown in the source code\n        write_weights_vb = 1. - torch.prod(1. - self.wl_prev_vb, 1)\n        return prev_usage_vb + (1. - prev_usage_vb) * write_weights_vb\n\n    def _allocation(self, usage_vb, epsilon=1e-6):\n        """"""\n        computes allocation by sorting usage, a = a_t[\\phi_t[j]]\n        variables needed:\n            usage_vb: [batch_size x mem_hei]\n                   -> indicating current memory usage, this is equal to u_t in\n                      the paper when we only have one write head, but for\n                      multiple write heads, one should update the usage while\n                      iterating through the write heads to take into account the\n                      allocation returned by this function\n        returns:\n            alloc_vb: [batch_size x num_write_heads x mem_hei]\n        """"""\n        # ensure values are not too small prior to cumprod\n        usage_vb = epsilon + (1 - epsilon) * usage_vb\n        # NOTE: we sort usage in ascending order\n        sorted_usage_vb, indices_vb = torch.topk(usage_vb, k=self.mem_hei, dim=1, largest=False)\n        # to imitate tf.cumrprod(exclusive=True) https://discuss.pytorch.org/t/cumprod-exclusive-true-equivalences/2614/8\n        cat_sorted_usage_vb = torch.cat((Variable(torch.ones(self.batch_size, 1)).type(self.dtype), sorted_usage_vb), 1)[:, :-1]\n        # TODO: seems we have to wait for this PR: https://github.com/pytorch/pytorch/pull/1439\n        prod_sorted_usage_vb = fake_cumprod(cat_sorted_usage_vb)\n        # prod_sorted_usage_vb = torch.cumprod(cat_sorted_usage_vb, dim=1) # TODO: use this once the PR is ready\n        # alloc_weight_vb = (1 - sorted_usage_vb) * prod_sorted_usage_vb  # equ. (1)            # 0.1.12\n        alloc_weight_vb = (1 - sorted_usage_vb) * prod_sorted_usage_vb.squeeze()  # equ. (1)    # 0.2.0\n        _, indices_vb = torch.topk(indices_vb, k=self.mem_hei, dim=1, largest=False)\n        alloc_weight_vb = alloc_weight_vb.gather(1, indices_vb)\n        return alloc_weight_vb\n\n    def _location_focus(self, usage_vb):\n        """"""\n        Calculates freeness-based locations for writing to.\n        This finds unused memory by ranking the memory locations by usage, for\n        each write head. (For more than one write head, we use a ""simulated new\n        usage"" which takes into account the fact that the previous write head\n        will increase the usage in that area of the memory.)\n        variables needed:\n            usage_vb:         [batch_size x mem_hei]\n                           -> representing current memory usage\n            write_gate_vb:    [batch_size x num_write_heads x 1]\n                           -> /in [0, 1] indicating how much each write head\n                              does writing based on the address returned here\n                              (and hence how much usage increases)\n        returns:\n            alloc_weights_vb: [batch_size x num_write_heads x mem_hei]\n                            -> containing the freeness-based write locations\n                               Note that this isn\'t scaled by `write_gate`;\n                               this scaling must be applied externally.\n        """"""\n        alloc_weights_vb = []\n        for i in range(self.num_heads):\n            alloc_weights_vb.append(self._allocation(usage_vb))\n            # update usage to take into account writing to this new allocation\n            # NOTE: checked: if not operate directly on _vb.data, then the _vb\n            # NOTE: outside of this func will not change\n            usage_vb += (1 - usage_vb) * self.write_gate_vb[:, i, :].expand_as(usage_vb) * alloc_weights_vb[i]\n        # pack the allocation weights for write heads into one tensor\n        alloc_weight_vb = torch.stack(alloc_weights_vb, dim=1)\n        self.wl_curr_vb = self.write_gate_vb.expand_as(alloc_weight_vb) * (self.alloc_gate_vb.expand_as(self.wc_vb) * alloc_weight_vb + \\\n                                                                           (1. - self.alloc_gate_vb.expand_as(self.wc_vb)) * self.wc_vb)\n\n    def _access(self, memory_vb): # write\n        """"""\n        variables needed:\n            wl_curr_vb: [batch_size x num_heads x mem_hei]\n            erase_vb:   [batch_size x num_heads x mem_wid]\n                     -> /in (0, 1)\n            add_vb:     [batch_size x num_heads x mem_wid]\n                     -> w/ no restrictions in range\n            memory_vb:  [batch_size x mem_hei x mem_wid]\n        returns:\n            memory_vb:  [batch_size x mem_hei x mem_wid]\n        NOTE: IMPORTANT: https://github.com/deepmind/dnc/issues/10\n        """"""\n\n        # first let\'s do erasion\n        weighted_erase_vb = torch.bmm(self.wl_curr_vb.contiguous().view(-1, self.mem_hei, 1),\n                                      self.erase_vb.contiguous().view(-1, 1, self.mem_wid)).view(-1, self.num_heads, self.mem_hei, self.mem_wid)\n        keep_vb = torch.prod(1. - weighted_erase_vb, dim=1)\n        memory_vb = memory_vb * keep_vb\n        # finally let\'s write (do addition)\n        return memory_vb + torch.bmm(self.wl_curr_vb.transpose(1, 2), self.add_vb)\n\n    def forward(self, hidden_vb, memory_vb, usage_vb):\n        # content focus\n        super(DynamicWriteHead, self).forward(hidden_vb, memory_vb)\n        # location focus\n        self.alloc_gate_vb = F.sigmoid(self.hid_2_alloc_gate(hidden_vb)).view(-1, self.num_heads, 1)\n        self.write_gate_vb = F.sigmoid(self.hid_2_write_gate(hidden_vb)).view(-1, self.num_heads, 1)\n        self._location_focus(usage_vb)\n        self.wl_prev_vb = self.wl_curr_vb\n        # access\n        self.erase_vb = F.sigmoid(self.hid_2_erase(hidden_vb)).view(-1, self.num_heads, self.mem_wid)\n        self.add_vb   = F.tanh(self.hid_2_add(hidden_vb)).view(-1, self.num_heads, self.mem_wid)\n        return self._access(memory_vb)\n\n    def _update_link(self, prev_link_vb, prev_preced_vb):\n        """"""\n        calculates the new link graphs\n        For each write head, the link is a directed graph (represented by a\n        matrix with entries in range [0, 1]) whose vertices are the memory\n        locations, and an edge indicates temporal ordering of writes.\n        variables needed:\n            prev_link_vb:   [batch_size x num_heads x mem_hei x mem_wid]\n                         -> {L_t-1}, previous link graphs\n            prev_preced_vb: [batch_size x num_heads x mem_hei]\n                         -> {p_t}, the previous aggregated precedence\n                         -> weights for each write head\n            wl_curr_vb:     [batch_size x num_heads x mem_hei]\n                         -> location focus of {t}\n        returns:\n            link_vb:        [batch_size x num_heads x mem_hei x mem_hei]\n                         -> {L_t}, current link graph\n        """"""\n        write_weights_i_vb = self.wl_curr_vb.unsqueeze(3).expand_as(prev_link_vb)\n        write_weights_j_vb = self.wl_curr_vb.unsqueeze(2).expand_as(prev_link_vb)\n        prev_preced_j_vb = prev_preced_vb.unsqueeze(2).expand_as(prev_link_vb)\n        prev_link_scale_vb = 1 - write_weights_i_vb - write_weights_j_vb\n        new_link_vb = write_weights_i_vb * prev_preced_j_vb\n        link_vb = prev_link_scale_vb * prev_link_vb + new_link_vb\n        # Return the link with the diagonal set to zero, to remove self-looping edges.\n        # TODO: set diag as 0 if there\'s a specific method to do that w/ later releases\n        diag_mask_vb = Variable(1 - torch.eye(self.mem_hei).unsqueeze(0).unsqueeze(0).expand_as(link_vb)).type(self.dtype)\n        link_vb = link_vb * diag_mask_vb\n        return link_vb\n\n    def _update_precedence_weights(self, prev_preced_vb):\n        """"""\n        calculates the new precedence weights given the current write weights\n        the precedence weights are the ""aggregated write weights"" for each write\n        head, where write weights with sum close to zero will leave the\n        precedence weights unchanged, but with sum close to one will replace the\n        precedence weights.\n        variables needed:\n            prev_preced_vb: [batch_size x num_write_heads x mem_hei]\n            wl_curr_vb:     [batch_size x num_write_heads x mem_hei]\n        returns:\n            preced_vb:      [batch_size x num_write_heads x mem_hei]\n        """"""\n        # write_sum_vb = torch.sum(self.wl_curr_vb, 2)              # 0.1.12\n        write_sum_vb = torch.sum(self.wl_curr_vb, 2, keepdim=True)  # 0.2.0\n        return (1 - write_sum_vb).expand_as(prev_preced_vb) * prev_preced_vb + self.wl_curr_vb\n\n    def _temporal_link(self, prev_link_vb, prev_preced_vb):\n        link_vb = self._update_link(prev_link_vb, prev_preced_vb)\n        preced_vb = self._update_precedence_weights(prev_preced_vb)\n        return link_vb, preced_vb\n'"
core/heads/static_head.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom core.head import Head\nfrom utils.similarities import batch_cosine_sim\n\nclass StaticHead(Head):\n    def __init__(self, args):\n        super(StaticHead, self).__init__(args)\n\n        # build model\n        # for content focus\n        self.hid_2_key   = nn.Linear(self.hidden_dim, self.num_heads * self.mem_wid)\n        self.hid_2_beta  = nn.Linear(self.hidden_dim, self.num_heads * 1)\n        # for location focus\n        self.hid_2_gate  = nn.Linear(self.hidden_dim, self.num_heads * 1)\n        self.hid_2_shift = nn.Linear(self.hidden_dim, self.num_heads * self.num_allowed_shifts)\n        self.hid_2_gamma = nn.Linear(self.hidden_dim, self.num_heads * 1)\n\n    def _content_focus(self, memory_vb):\n        """"""\n        variables needed:\n            key_vb:    [batch_size x num_heads x mem_wid]\n                    -> similarity key vector, to compare to each row in memory\n                    -> by cosine similarity\n            beta_vb:   [batch_size x num_heads x 1]\n                    -> NOTE: refer here: https://github.com/deepmind/dnc/issues/9\n                    -> \\in (1, +inf) after oneplus(); similarity key strength\n                    -> amplify or attenuate the pecision of the focus\n            memory_vb: [batch_size x mem_hei   x mem_wid]\n        returns:\n            wc_vb:     [batch_size x num_heads x mem_hei]\n                    -> the attention weight by content focus\n        """"""\n        K_vb = batch_cosine_sim(self.key_vb, memory_vb)  # [batch_size x num_heads x mem_hei]\n        self.wc_vb = K_vb * self.beta_vb.expand_as(K_vb) # [batch_size x num_heads x mem_hei]\n        self.wc_vb = F.softmax(self.wc_vb.transpose(0, 2)).transpose(0, 2)\n\n    def _shift(self, wg_vb, shift_vb):\n        """"""\n        variables needed:\n            wg_vb:    [batch_size x num_heads x mem_hei]\n            shift_vb: [batch_size x num_heads x num_allowed_shifts]\n                   -> sum=1; the shift weight vector\n        returns:\n            ws_vb:    [batch_size x num_heads x mem_hei]\n                   -> the attention weight by location focus\n        """"""\n        batch_size = wg_vb.size(0)\n        input_dim = wg_vb.size(2); assert input_dim == self.mem_hei\n        filter_dim = shift_vb.size(2); assert filter_dim == self.num_allowed_shifts\n\n        ws_vb = None\n        for i in range(batch_size): # for each head in each batch, the kernel is different ... seems there\'s no other way by doing the loop here\n            for j in range(self.num_heads):\n                ws_tmp_vb = F.conv1d(wg_vb[i][j].unsqueeze(0).unsqueeze(0).repeat(1, 1, 3),\n                                     shift_vb[i][j].unsqueeze(0).unsqueeze(0).contiguous(),\n                                     padding=filter_dim//2)[:, :, input_dim:(2*input_dim)]\n                if ws_vb is None:\n                    ws_vb = ws_tmp_vb\n                else:\n                    ws_vb = torch.cat((ws_vb, ws_tmp_vb), 0)\n        ws_vb = ws_vb.view(-1, self.num_heads, self.mem_hei)\n        return ws_vb\n\n    def _location_focus(self):\n        """"""\n        variables needed:\n            wl_prev_vb: [batch_size x num_heads x mem_hei]\n            wc_vb:      [batch_size x num_heads x mem_hei]\n            gate_vb:    [batch_size x num_heads x 1]\n                     -> \\in (0, 1); the interpolation gate\n            shift_vb:   [batch_size x num_heads x num_allowed_shifts]\n                     -> sum=1; the shift weight vector\n            gamma_vb:   [batch_size x num_heads x 1]\n                     -> >=1; the sharpening vector\n        returns:\n            wl_curr_vb: [batch_size x num_heads x mem_hei]\n                     -> the attention weight by location focus\n        """"""\n        self.gate_vb = self.gate_vb.expand_as(self.wc_vb)\n        wg_vb = self.wc_vb * self.gate_vb + self.wl_prev_vb * (1. - self.gate_vb)\n        ws_vb = self._shift(wg_vb, self.shift_vb)\n        wp_vb = ws_vb.pow(self.gamma_vb.expand_as(ws_vb))\n        # self.wl_curr_vb = wp_vb / wp_vb.sum(2).expand_as(wp_vb)               # 0.1.12\n        self.wl_curr_vb = wp_vb / wp_vb.sum(2, keepdim=True).expand_as(wp_vb)   # 0.2.0\n\n    def forward(self, hidden_vb, memory_vb):\n        # outputs for computing addressing for heads\n        # NOTE: to be consistent w/ the dnc paper, we use\n        # NOTE: sigmoid to constrain to [0, 1]\n        # NOTE: oneplus to constrain to [1, +inf]\n        self.key_vb   = F.tanh(self.hid_2_key(hidden_vb)).view(-1, self.num_heads, self.mem_wid)    # TODO: relu to bias the memory to store positive values ??? check again\n        self.beta_vb  = F.softplus(self.hid_2_beta(hidden_vb)).view(-1, self.num_heads, 1)          # beta >=1: https://github.com/deepmind/dnc/issues/9\n        self.gate_vb  = F.sigmoid(self.hid_2_gate(hidden_vb)).view(-1, self.num_heads, 1)           # gate /in (0, 1): interpolation gate, blend wl_{t-1} & wc\n        self.shift_vb = F.softmax(self.hid_2_shift(hidden_vb).view(-1, self.num_heads, self.num_allowed_shifts).transpose(0, 2)).transpose(0, 2)    # shift: /sum=1\n        self.gamma_vb = (1. + F.softplus(self.hid_2_gamma(hidden_vb))).view(-1, self.num_heads, 1)  # gamma >= 1: sharpen the final weights\n\n        # now we compute the addressing mechanism\n        self._content_focus(memory_vb)\n        self._location_focus()\n'"
core/heads/static_read_head.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\n\nfrom core.heads.static_head import StaticHead\n\nclass StaticReadHead(StaticHead):\n    def __init__(self, args):\n        super(StaticReadHead, self).__init__(args)\n        # params\n        if self.visualize:\n            self.win_head = ""win_read_head""\n        # logging\n        self.logger.warning(""<-----------------------------------> ReadHeads:  {"" + str(self.num_heads) + "" heads}"")\n        self.logger.warning(self)\n\n        # reset\n        self._reset()\n\n    def visual(self):\n        if self.visualize:      # here we visualize the wl_curr of the first batch\n            self.win_head = self.vis.heatmap(self.wl_curr_vb.data[0].clone().cpu().transpose(0, 1).numpy(), env=self.refs, win=self.win_head, opts=dict(title=""read_head""))\n\n    def forward(self, hidden_vb, memory_vb):\n        # content & location focus\n        super(StaticReadHead, self).forward(hidden_vb, memory_vb)\n        self.wl_prev_vb = self.wl_curr_vb\n        # access\n        return self._access(memory_vb)\n\n    def _access(self, memory_vb): # read\n        """"""\n        variables needed:\n            wl_curr_vb:   [batch_size x num_heads x mem_hei]\n                       -> location focus of {t}\n            memory_vb:    [batch_size x mem_hei   x mem_wid]\n        returns:\n            read_vec_vb:  [batch_size x num_heads x mem_wid]\n                       -> read vector of {t}\n        """"""\n        return torch.bmm(self.wl_curr_vb, memory_vb)\n'"
core/heads/static_write_head.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.heads.static_head import StaticHead\n\nclass StaticWriteHead(StaticHead):\n    def __init__(self, args):\n        super(StaticWriteHead, self).__init__(args)\n        # params\n        if self.visualize:\n            self.win_head = ""win_write_head""\n\n        # buid model: add additional outs for write\n        # for access\n        self.hid_2_erase = nn.Linear(self.hidden_dim, self.num_heads * self.mem_wid)\n        self.hid_2_add   = nn.Linear(self.hidden_dim, self.num_heads * self.mem_wid)\n\n        # logging\n        self.logger.warning(""<-----------------------------------> WriteHeads: {"" + str(self.num_heads) + "" heads}"")\n        self.logger.warning(self)\n\n        # reset\n        self._reset()\n\n    def visual(self):\n        if self.visualize:      # here we visualize the wl_curr of the first batch\n            self.win_head = self.vis.heatmap(self.wl_curr_vb.data[0].clone().cpu().transpose(0, 1).numpy(), env=self.refs, win=self.win_head, opts=dict(title=""write_head""))\n\n    def forward(self, hidden_vb, memory_vb):\n        # content & location focus\n        super(StaticWriteHead, self).forward(hidden_vb, memory_vb)\n        self.wl_prev_vb = self.wl_curr_vb\n        # access\n        self.erase_vb = F.sigmoid(self.hid_2_erase(hidden_vb)).view(-1, self.num_heads, self.mem_wid)\n        self.add_vb   = F.tanh(self.hid_2_add(hidden_vb)).view(-1, self.num_heads, self.mem_wid)\n        return self._access(memory_vb)\n\n    def _access(self, memory_vb): # write\n        """"""\n        variables needed:\n            wl_curr_vb: [batch_size x num_heads x mem_hei]\n            erase_vb:   [batch_size x num_heads x mem_wid]\n                     -> /in (0, 1)\n            add_vb:     [batch_size x num_heads x mem_wid]\n                     -> w/ no restrictions in range\n            memory_vb:  [batch_size x mem_hei x mem_wid]\n        returns:\n            memory_vb:  [batch_size x mem_hei x mem_wid]\n        NOTE: IMPORTANT: https://github.com/deepmind/dnc/issues/10\n        """"""\n\n        # first let\'s do erasion\n        weighted_erase_vb = torch.bmm(self.wl_curr_vb.contiguous().view(-1, self.mem_hei, 1),\n                                      self.erase_vb.contiguous().view(-1, 1, self.mem_wid)).view(-1, self.num_heads, self.mem_hei, self.mem_wid)\n        keep_vb = torch.prod(1. - weighted_erase_vb, dim=1)\n        memory_vb = memory_vb * keep_vb\n        # finally let\'s write (do addition)\n        return memory_vb + torch.bmm(self.wl_curr_vb.transpose(1, 2), self.add_vb)\n'"
