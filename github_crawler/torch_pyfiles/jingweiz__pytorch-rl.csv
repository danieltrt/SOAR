file_path,api_count,code
main.py,0,"b'import numpy as np\n\n# custom modules\nfrom utils.options import Options\nfrom utils.factory import EnvDict, ModelDict, MemoryDict, AgentDict\n\n# 0. setting up\nopt = Options()\nnp.random.seed(opt.seed)\n\n# 1. env    (prototype)\nenv_prototype    = EnvDict[opt.env_type]\n# 2. model  (prototype)\nmodel_prototype  = ModelDict[opt.model_type]\n# 3. memory (prototype)\nmemory_prototype = MemoryDict[opt.memory_type]\n# 4. agent\nagent = AgentDict[opt.agent_type](opt.agent_params,\n                                  env_prototype    = env_prototype,\n                                  model_prototype  = model_prototype,\n                                  memory_prototype = memory_prototype)\n# 5. fit model\nif opt.mode == 1:   # train\n    agent.fit_model()\nelif opt.mode == 2: # test opt.model_file\n    agent.test_model()\n'"
core/__init__.py,0,b''
core/agent.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch\nimport torch.optim as optim\n\nfrom utils.helpers import Experience\n\nclass Agent(object):\n    def __init__(self, args, env_prototype, model_prototype, memory_prototype=None):\n        # logging\n        self.logger = args.logger\n\n        # prototypes for env & model & memory\n        self.env_prototype = env_prototype          # NOTE: instantiated in fit_model() of inherited Agents\n        self.env_params = args.env_params\n        self.model_prototype = model_prototype      # NOTE: instantiated in fit_model() of inherited Agents\n        self.model_params = args.model_params\n        self.memory_prototype = memory_prototype    # NOTE: instantiated in __init__()  of inherited Agents (dqn needs, a3c doesn\'t so only pass in None)\n        self.memory_params = args.memory_params\n\n        # params\n        self.model_name = args.model_name           # NOTE: will save the current model to model_name\n        self.model_file = args.model_file           # NOTE: will load pretrained model_file if not None\n\n        self.render = args.render\n        self.visualize = args.visualize\n        if self.visualize:\n            self.vis = args.vis\n            self.refs = args.refs\n\n        self.save_best = args.save_best\n        if self.save_best:\n            self.best_step   = None                 # NOTE: achieves best_reward at this step\n            self.best_reward = None                 # NOTE: only save a new model if achieves higher reward\n\n        self.hist_len = args.hist_len\n        self.hidden_dim = args.hidden_dim\n\n        self.use_cuda = args.use_cuda\n        self.dtype = args.dtype\n\n        # agent_params\n        # criteria and optimizer\n        self.value_criteria = args.value_criteria\n        self.optim = args.optim\n        # hyperparameters\n        self.steps = args.steps\n        self.early_stop = args.early_stop\n        self.gamma = args.gamma\n        self.clip_grad = args.clip_grad\n        self.lr = args.lr\n        self.lr_decay = args.lr_decay\n        self.weight_decay = args.weight_decay\n        self.eval_freq = args.eval_freq\n        self.eval_steps = args.eval_steps\n        self.prog_freq = args.prog_freq\n        self.test_nepisodes = args.test_nepisodes\n        if args.agent_type == ""dqn"":\n            self.enable_double_dqn  = args.enable_double_dqn\n            self.enable_dueling = args.enable_dueling\n            self.dueling_type = args.dueling_type\n\n            self.learn_start = args.learn_start\n            self.batch_size = args.batch_size\n            self.valid_size = args.valid_size\n            self.eps_start = args.eps_start\n            self.eps_end = args.eps_end\n            self.eps_eval = args.eps_eval\n            self.eps_decay = args.eps_decay\n            self.target_model_update = args.target_model_update\n            self.action_repetition = args.action_repetition\n            self.memory_interval = args.memory_interval\n            self.train_interval = args.train_interval\n        elif args.agent_type == ""a3c"":\n            self.enable_log_at_train_step = args.enable_log_at_train_step\n\n            self.enable_lstm = args.enable_lstm\n            self.enable_continuous = args.enable_continuous\n            self.num_processes = args.num_processes\n\n            self.rollout_steps = args.rollout_steps\n            self.tau = args.tau\n            self.beta = args.beta\n        elif args.agent_type == ""acer"":\n            self.enable_bias_correction = args.enable_bias_correction\n            self.enable_1st_order_trpo = args.enable_1st_order_trpo\n            self.enable_log_at_train_step = args.enable_log_at_train_step\n\n            self.enable_lstm = args.enable_lstm\n            self.enable_continuous = args.enable_continuous\n            self.num_processes = args.num_processes\n\n            self.replay_ratio = args.replay_ratio\n            self.replay_start = args.replay_start\n            self.batch_size = args.batch_size\n            self.valid_size = args.valid_size\n            self.clip_trace = args.clip_trace\n            self.clip_1st_order_trpo = args.clip_1st_order_trpo\n            self.avg_model_decay = args.avg_model_decay\n\n            self.rollout_steps = args.rollout_steps\n            self.tau = args.tau\n            self.beta = args.beta\n\n    def _reset_experience(self):\n        self.experience = Experience(state0 = None,\n                                     action = None,\n                                     reward = None,\n                                     state1 = None,\n                                     terminal1 = False)\n\n    def _load_model(self, model_file):\n        if model_file:\n            self.logger.warning(""Loading Model: "" + self.model_file + "" ..."")\n            self.model.load_state_dict(torch.load(model_file))\n            self.logger.warning(""Loaded  Model: "" + self.model_file + "" ..."")\n        else:\n            self.logger.warning(""No Pretrained Model. Will Train From Scratch."")\n\n    def _save_model(self, step, curr_reward):\n        self.logger.warning(""Saving Model    @ Step: "" + str(step) + "": "" + self.model_name + "" ..."")\n        if self.save_best:\n            if self.best_step is None:\n                self.best_step   = step\n                self.best_reward = curr_reward\n            if curr_reward >= self.best_reward:\n                self.best_step   = step\n                self.best_reward = curr_reward\n                torch.save(self.model.state_dict(), self.model_name)\n            self.logger.warning(""Saved  Model    @ Step: "" + str(step) + "": "" + self.model_name + "". {Best Step: "" + str(self.best_step) + "" | Best Reward: "" + str(self.best_reward) + ""}"")\n        else:\n            torch.save(self.model.state_dict(), self.model_name)\n            self.logger.warning(""Saved  Model    @ Step: "" + str(step) + "": "" + self.model_name + ""."")\n\n    def _forward(self, observation):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def _backward(self, reward, terminal):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def _eval_model(self):  # evaluation during training\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def fit_model(self):    # training\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def test_model(self):   # testing pre-trained models\n        raise NotImplementedError(""not implemented in base calss"")\n'"
core/agent_single_process.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport random\nimport time\nimport math\nimport torch\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.multiprocessing as mp\n\nfrom utils.helpers import Experience, one_hot\n\nclass AgentSingleProcess(mp.Process):\n    def __init__(self, master, process_id=0):\n        super(AgentSingleProcess, self).__init__(name = ""Process-%d"" % process_id)\n        # NOTE: self.master.* refers to parameters shared across all processes\n        # NOTE: self.*        refers to process-specific properties\n        # NOTE: we are not copying self.master.* to self.* to keep the code clean\n\n        self.master = master\n        self.process_id = process_id\n\n        # env\n        self.env = self.master.env_prototype(self.master.env_params, self.process_id)\n        # model\n        self.model = self.master.model_prototype(self.master.model_params)\n        self._sync_local_with_global()\n\n        # experience\n        self._reset_experience()\n\n    def _reset_experience(self):    # for getting one set of observation from env for every action taken\n        self.experience = Experience(state0 = None,\n                                     action = None,\n                                     reward = None,\n                                     state1 = None,\n                                     terminal1 = False) # TODO: should check this again\n\n    def _sync_local_with_global(self):  # grab the current global model for local learning/evaluating\n        self.model.load_state_dict(self.master.model.state_dict())\n\n    # NOTE: since no backward passes has ever been run on the global model\n    # NOTE: its grad has never been initialized, here we ensure proper initialization\n    # NOTE: reference: https://discuss.pytorch.org/t/problem-on-variable-grad-data/957\n    def _ensure_global_grads(self):\n        for global_param, local_param in zip(self.master.model.parameters(),\n                                             self.model.parameters()):\n            if global_param.grad is not None:\n                return\n            global_param._grad = local_param.grad\n\n    def _forward(self, observation):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def _backward(self, reward, terminal):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def run(self):\n        raise NotImplementedError(""not implemented in base calss"")\n'"
core/env.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom copy import deepcopy\nfrom gym.spaces.box import Box\nimport inspect\n\nfrom utils.helpers import Experience            # NOTE: here state0 is always ""None""\nfrom utils.helpers import preprocessAtari, rgb2gray, rgb2y, scale\n\nclass Env(object):\n    def __init__(self, args, env_ind=0):\n        self.logger     = args.logger\n        self.ind        = env_ind               # NOTE: for creating multiple environment instances\n        # general setup\n        self.mode       = args.mode             # NOTE: save frames when mode=2\n        if self.mode == 2:\n            try:\n                import scipy.misc\n                self.imsave = scipy.misc.imsave\n            except ImportError as e: self.logger.warning(""WARNING: scipy.misc not found"")\n            self.img_dir = args.root_dir + ""/imgs/""\n            self.frame_ind = 0\n        self.seed       = args.seed + self.ind  # NOTE: so to give a different seed to each instance\n        self.visualize  = args.visualize\n        if self.visualize:\n            self.vis        = args.vis\n            self.refs       = args.refs\n            self.win_state1 = ""win_state1""\n\n        self.env_type   = args.env_type\n        self.game       = args.game\n        self._reset_experience()\n\n        self.logger.warning(""<-----------------------------------> Env"")\n        self.logger.warning(""Creating {"" + self.env_type + "" | "" + self.game + ""} w/ Seed: "" + str(self.seed))\n\n    def _reset_experience(self):\n        self.exp_state0 = None  # NOTE: always None in this module\n        self.exp_action = None\n        self.exp_reward = None\n        self.exp_state1 = None\n        self.exp_terminal1 = None\n\n    def _get_experience(self):\n        return Experience(state0 = self.exp_state0, # NOTE: here state0 is always None\n                          action = self.exp_action,\n                          reward = self.exp_reward,\n                          state1 = self._preprocessState(self.exp_state1),\n                          terminal1 = self.exp_terminal1)\n\n    def _preprocessState(self, state):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    @property\n    def state_shape(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    @property\n    def action_dim(self):\n        if isinstance(self.env.action_space, Box):\n            return self.env.action_space.shape[0]\n        else:\n            return self.env.action_space.n\n\n    def render(self):       # render using the original gl window\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def visual(self):       # visualize onto visdom\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def reset(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def step(self, action):\n        raise NotImplementedError(""not implemented in base calss"")\n'"
core/memory.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom collections import deque, namedtuple\nimport warnings\nimport random\n\nfrom utils.helpers import Experience\n\ndef sample_batch_indexes(low, high, size):\n    if high - low >= size:\n        # We have enough data. Draw without replacement, that is each index is unique in the\n        # batch. We cannot use `np.random.choice` here because it is horribly inefficient as\n        # the memory grows. See https://github.com/numpy/numpy/issues/2764 for a discussion.\n        # `random.sample` does the same thing (drawing without replacement) and is way faster.\n        try:\n            r = xrange(low, high)\n        except NameError:\n            r = range(low, high)\n        batch_idxs = random.sample(r, size)\n    else:\n        # Not enough data. Help ourselves with sampling from the range, but the same index\n        # can occur multiple times. This is not good and should be avoided by picking a\n        # large enough warm-up phase.\n        warnings.warn(\'Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\')\n        batch_idxs = np.random.random_integers(low, high - 1, size=size)\n    assert len(batch_idxs) == size\n    return batch_idxs\n\ndef zeroed_observation(observation):\n    if hasattr(observation, \'shape\'):\n        return np.zeros(observation.shape)\n    elif hasattr(observation, \'__iter__\'):\n        out = []\n        for x in observation:\n            out.append(zeroed_observation(x))\n        return out\n    else:\n        return 0.\n\nclass RingBuffer(object):\n    def __init__(self, maxlen):\n        self.maxlen = maxlen\n        self.start = 0\n        self.length = 0\n        self.data = [None for _ in range(maxlen)]\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        if idx < 0 or idx >= self.length:\n            raise KeyError()\n        return self.data[(self.start + idx) % self.maxlen]\n\n    def append(self, v):\n        if self.length < self.maxlen:\n            # We have space, simply increase the length.\n            self.length += 1\n        elif self.length == self.maxlen:\n            # No space, ""remove"" the first item.\n            self.start = (self.start + 1) % self.maxlen\n        else:\n            # This should never happen.\n            raise RuntimeError()\n        self.data[(self.start + self.length - 1) % self.maxlen] = v\n\nclass Memory(object):\n    def __init__(self, window_length, ignore_episode_boundaries=False):\n        self.window_length = window_length\n        self.ignore_episode_boundaries = ignore_episode_boundaries\n\n        self.recent_observations = deque(maxlen=window_length)\n        self.recent_terminals = deque(maxlen=window_length)\n\n    def sample(self, batch_size, batch_idxs=None):\n        raise NotImplementedError()\n\n    def append(self, observation, action, reward, terminal, training=True):\n        self.recent_observations.append(observation)\n        self.recent_terminals.append(terminal)\n\n    def get_recent_state(self, current_observation):\n        # This code is slightly complicated by the fact that subsequent observations might be\n        # from different episodes. We ensure that an experience never spans multiple episodes.\n        # This is probably not that important in practice but it seems cleaner.\n        state = [current_observation]\n        idx = len(self.recent_observations) - 1\n        for offset in range(0, self.window_length - 1):\n            current_idx = idx - offset\n            current_terminal = self.recent_terminals[current_idx - 1] if current_idx - 1 >= 0 else False\n            if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n                # The previously handled observation was terminal, don\'t add the current one.\n                # Otherwise we would leak into a different episode.\n                break\n            state.insert(0, self.recent_observations[current_idx])\n        while len(state) < self.window_length:\n            state.insert(0, zeroed_observation(state[0]))\n        return state\n\n    def get_config(self):\n        config = {\n            \'window_length\': self.window_length,\n            \'ignore_episode_boundaries\': self.ignore_episode_boundaries,\n        }\n        return config\n'"
core/model.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils.init_weights import init_weights, normalized_columns_initializer\n\nclass Model(nn.Module):\n    def __init__(self, args):\n        super(Model, self).__init__()\n        # logging\n        self.logger = args.logger\n        # params\n        self.hidden_dim = args.hidden_dim\n        self.use_cuda = args.use_cuda\n        self.dtype = args.dtype\n        # model_params\n        if hasattr(args, ""enable_dueling""):     # only set for ""dqn""\n            self.enable_dueling = args.enable_dueling\n            self.dueling_type   = args.dueling_type\n        if hasattr(args, ""enable_lstm""):        # only set for ""dqn""\n            self.enable_lstm    = args.enable_lstm\n\n        self.input_dims     = {}\n        self.input_dims[0]  = args.hist_len # from params\n        self.input_dims[1]  = args.state_shape\n        self.output_dims    = args.action_dim\n\n    def _init_weights(self):\n        raise NotImplementedError(""not implemented in base calss"")\n\n    def print_model(self):\n        self.logger.warning(""<-----------------------------------> Model"")\n        self.logger.warning(self)\n\n    def _reset(self):           # NOTE: should be called at each child\'s __init__\n        self._init_weights()\n        self.type(self.dtype)   # put on gpu if possible\n        self.print_model()\n\n    def forward(self, input):\n        raise NotImplementedError(""not implemented in base calss"")\n'"
optims/__init__.py,0,b''
optims/helpers.py,1,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# NOTE: refer to: https://discuss.pytorch.org/t/adaptive-learning-rate/320/31\ndef adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n"""
optims/sharedAdam.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport math\nimport torch\nimport torch.optim as optim\n\nclass SharedAdam(optim.Adam):\n    """"""Implements Adam algorithm with shared states.\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0):\n        super(SharedAdam, self).__init__(params, lr, betas, eps, weight_decay)\n\n        # State initialisation (must be done before step, else will not be shared between threads)\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'] = torch.zeros(1)\n                state[\'exp_avg\'] = p.data.new().resize_as_(p.data).zero_()\n                state[\'exp_avg_sq\'] = p.data.new().resize_as_(p.data).zero_()\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'].share_memory_()\n                state[\'exp_avg\'].share_memory_()\n                state[\'exp_avg_sq\'].share_memory_()\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\'][0]\n                bias_correction2 = 1 - beta2 ** state[\'step\'][0]\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n        return loss\n'"
optims/sharedRMSprop.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom torch import optim\n\n# Non-centered RMSprop update with shared statistics (without momentum)\nclass SharedRMSprop(optim.RMSprop):\n    """"""Implements RMSprop algorithm with shared states.\n    """"""\n\n    def __init__(self, params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0):\n        super(SharedRMSprop, self).__init__(params, lr=lr, alpha=alpha, eps=eps, weight_decay=weight_decay, momentum=0, centered=False)\n\n        # State initialisation (must be done before step, else will not be shared between threads)\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'] = p.data.new().resize_(1).zero_()\n                state[\'square_avg\'] = p.data.new().resize_as_(p.data).zero_()\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'].share_memory_()\n                state[\'square_avg\'].share_memory_()\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                square_avg = state[\'square_avg\']\n                alpha = group[\'alpha\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # g = \xce\xb1g + (1 - \xce\xb1)\xce\x94\xce\xb8^2\n                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n                # \xce\xb8 \xe2\x86\x90 \xce\xb8 - \xce\xb7\xce\x94\xce\xb8/\xe2\x88\x9a(g + \xce\xb5)\n                avg = square_avg.sqrt().add_(group[\'eps\'])\n                p.data.addcdiv_(-group[\'lr\'], grad, avg)\n\n        return loss\n'"
utils/__init__.py,0,b''
utils/distributions.py,8,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport math\nimport random\n\n# Knuth\'s algorithm for generating Poisson samples\ndef sample_poisson(lmbd):\n    L, k, p = math.exp(-lmbd), 0, 1\n    while p > L:\n        k += 1\n        p *= random.uniform(0, 1)\n    return max(k - 1, 0)\n\n# KL divergence k = DKL[ ref_distribution || input_distribution]\ndef categorical_kl_div(input_vb, ref_vb):\n    """"""\n    kl_div = \\sum ref * (log(ref) - log(input))\n    variables needed:\n            input_vb: [batch_size x state_dim]\n              ref_vb: [batch_size x state_dim]\n    returns:\n           kl_div_vb: [batch_size x 1]\n    """"""\n    return (ref_vb * (ref_vb.log() - input_vb.log())).sum(1, keepdim=True)\n\n# import torch\n# from torch.autograd import Variable\n# import torch.nn.functional as F\n# # input_vb = Variable(torch.Tensor([0.2, 0.8])).view(1, 2)\n# # ref_vb   = Variable(torch.Tensor([0.3, 0.7])).view(1, 2)\n# input_vb = Variable(torch.Tensor([0.0002, 0.9998])).view(1, 2)\n# ref_vb   = Variable(torch.Tensor([0.3, 0.7])).view(1, 2)\n# input_vb = Variable(torch.Tensor([0.2, 0.8, 0.5, 0.5, 0.7, 0.3])).view(3, 2)\n# ref_vb   = Variable(torch.Tensor([0.3, 0.7, 0.5, 0.5, 0.1, 0.9])).view(3, 2)\n# print(F.kl_div(input_vb.log(), ref_vb, size_average=False))\n# print(kl_div(input_vb, ref_vb))\n'"
utils/factory.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom core.envs.gym import GymEnv\nfrom core.envs.atari_ram import AtariRamEnv\nfrom core.envs.atari import AtariEnv\nfrom core.envs.lab import LabEnv\nEnvDict = {""gym"":       GymEnv,                 # classic control games from openai w/ low-level   input\n           ""atari-ram"": AtariRamEnv,            # atari integrations from openai, with low-level   input\n           ""atari"":     AtariEnv,               # atari integrations from openai, with pixel-level input\n           ""lab"":       LabEnv}\n\nfrom core.models.empty import EmptyModel\nfrom core.models.dqn_mlp import DQNMlpModel\nfrom core.models.dqn_cnn import DQNCnnModel\nfrom core.models.a3c_mlp_con import A3CMlpConModel\nfrom core.models.a3c_cnn_dis import A3CCnnDisModel\nfrom core.models.acer_mlp_dis import ACERMlpDisModel\nfrom core.models.acer_cnn_dis import ACERCnnDisModel\nModelDict = {""empty"":        EmptyModel,        # contains nothing, only should be used w/ EmptyAgent\n             ""dqn-mlp"":      DQNMlpModel,       # for dqn low-level    input\n             ""dqn-cnn"":      DQNCnnModel,       # for dqn pixel-level  input\n             ""a3c-mlp-con"":  A3CMlpConModel,    # for a3c low-level    input (NOTE: continuous must end in ""-con"")\n             ""a3c-cnn-dis"":  A3CCnnDisModel,    # for a3c pixel-level  input\n             ""acer-mlp-dis"": ACERMlpDisModel,   # for acer low-level   input\n             ""acer-cnn-dis"": ACERCnnDisModel,   # for acer pixel-level input\n             ""none"":         None}\n\nfrom core.memories.sequential import SequentialMemory\nfrom core.memories.episode_parameter import EpisodeParameterMemory\nfrom core.memories.episodic import EpisodicMemory\nMemoryDict = {""sequential"":        SequentialMemory,        # off-policy\n              ""episode-parameter"": EpisodeParameterMemory,  # not in use right now\n              ""episodic"":          EpisodicMemory,          # on/off-policy\n              ""none"":              None}                    # on-policy\n\nfrom core.agents.empty import EmptyAgent\nfrom core.agents.dqn   import DQNAgent\nfrom core.agents.a3c   import A3CAgent\nfrom core.agents.acer  import ACERAgent\nAgentDict = {""empty"": EmptyAgent,               # to test integration of new envs, contains only the most basic control loop\n             ""dqn"":   DQNAgent,                 # dqn  (w/ double dqn & dueling as options)\n             ""a3c"":   A3CAgent,                 # a3c  (multi-process, pure cpu version)\n             ""acer"":  ACERAgent}                # acer (multi-process, pure cpu version)\n'"
utils/helpers.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport logging\nimport numpy as np\nimport cv2\nfrom collections import namedtuple\n\ndef loggerConfig(log_file, verbose=2):\n   logger      = logging.getLogger()\n   formatter   = logging.Formatter('[%(levelname)-8s] (%(processName)-11s) %(message)s')\n   fileHandler = logging.FileHandler(log_file, 'w')\n   fileHandler.setFormatter(formatter)\n   logger.addHandler(fileHandler)\n   if verbose >= 2:\n       logger.setLevel(logging.DEBUG)\n   elif verbose >= 1:\n       logger.setLevel(logging.INFO)\n   else:\n       # NOTE: we currently use this level to log to get rid of visdom's info printouts\n       logger.setLevel(logging.WARNING)\n   return logger\n\n# This is to be understood as a transition: Given `state0`, performing `action`\n# yields `reward` and results in `state1`, which might be `terminal`.\n# NOTE: used as the return format for Env(), and as the format to push into replay memory for off-policy methods (DQN)\n# NOTE: when return from Env(), state0 is always None\nExperience                 = namedtuple('Experience',                 'state0, action, reward, state1, terminal1')\n# NOTE: used for on-policy methods for collect experiences over a rollout of an episode\n# NOTE: policy_vb & value0_vb for storing output Variables along a rollout # NOTE: they should not be detached from the graph!\nA3C_Experience             = namedtuple('A3C_Experience',             'state0, action, reward, state1, terminal1, policy_vb, sigmoid_vb, value0_vb')\nACER_On_Policy_Experience  = namedtuple('ACER_On_Policy_Experience',  'state0, action, reward, state1, terminal1, policy_vb, q0_vb,      value0_vb, detached_avg_policy_vb, detached_old_policy_vb')\n# # NOTE: used as the format to push into the replay memory for ACER; when sampled, used to get ACER_On_Policy_Experience\nACER_Off_Policy_Experience = namedtuple('ACER_Off_Policy_Experience', 'state0, action, reward,                                                                              detached_old_policy_vb')\n\ndef preprocessAtari(frame):\n    frame = frame[34:34 + 160, :160]\n    frame = cv2.resize(frame, (80, 80))\n    frame = cv2.resize(frame, (42, 42))\n    frame = frame.mean(2)\n    frame = frame.astype(np.float32)\n    frame*= (1. / 255.)\n    return frame\n\n# TODO: check the order rgb to confirm\ndef rgb2gray(rgb):\n    gray_image     = 0.2126 * rgb[..., 0]\n    gray_image[:] += 0.0722 * rgb[..., 1]\n    gray_image[:] += 0.7152 * rgb[..., 2]\n    return gray_image\n\n# TODO: check the order rgb to confirm\ndef rgb2y(rgb):\n    y_image     = 0.299 * rgb[..., 0]\n    y_image[:] += 0.587 * rgb[..., 1]\n    y_image[:] += 0.114 * rgb[..., 2]\n    return y_image\n\ndef scale(image, hei_image, wid_image):\n    return cv2.resize(image, (wid_image, hei_image),\n                      interpolation=cv2.INTER_LINEAR)\n\ndef one_hot(n_classes, labels):\n    one_hot_labels = np.zeros(labels.shape + (n_classes,))\n    for c in range(n_classes):\n        one_hot_labels[labels == c, c] = 1\n    return one_hot_labels\n"""
utils/init_weights.py,3,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport torch\n\ndef normalized_columns_initializer(weights, std=1.0):\n    out = torch.randn(weights.size())\n    # out *= std / torch.sqrt(out.pow(2).sum(1).expand_as(out))             # 0.1.12\n    out *= std / torch.sqrt(out.pow(2).sum(1, keepdim=True).expand_as(out)) # 0.2.0\n    return out\n\ndef init_weights(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = np.prod(weight_shape[1:4])\n        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n    elif classname.find('Linear') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = weight_shape[1]\n        fan_out = weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n"""
utils/options.py,9,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport os\nimport visdom\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom utils.helpers import loggerConfig\nfrom optims.sharedAdam import SharedAdam\nfrom optims.sharedRMSprop import SharedRMSprop\n\nCONFIGS = [\n# agent_type, env_type,    game,                       model_type,     memory_type\n[ ""empty"",    ""gym"",       ""CartPole-v0"",              ""empty"",        ""none""      ],  # 0\n[ ""dqn"",      ""gym"",       ""CartPole-v0"",              ""dqn-mlp"",      ""sequential""],  # 1\n[ ""dqn"",      ""atari-ram"", ""Pong-ram-v0"",              ""dqn-mlp"",      ""sequential""],  # 2\n[ ""dqn"",      ""atari"",     ""PongDeterministic-v4"",     ""dqn-cnn"",      ""sequential""],  # 3\n[ ""dqn"",      ""atari"",     ""BreakoutDeterministic-v4"", ""dqn-cnn"",      ""sequential""],  # 4\n[ ""a3c"",      ""atari"",     ""PongDeterministic-v4"",     ""a3c-cnn-dis"",  ""none""      ],  # 5\n[ ""a3c"",      ""gym"",       ""InvertedPendulum-v1"",      ""a3c-mlp-con"",  ""none""      ],  # 6\n[ ""acer"",     ""gym"",       ""CartPole-v0"",              ""acer-mlp-dis"", ""episodic""  ],  # 7  # NOTE: acer under testing\n[ ""acer"",     ""atari"",     ""Boxing-v0"",                ""acer-cnn-dis"", ""episodic""  ]   # 8  # NOTE: acer under testing\n]\n\nclass Params(object):   # NOTE: shared across all modules\n    def __init__(self):\n        self.verbose     = 0            # 0(warning) | 1(info) | 2(debug)\n\n        # training signature\n        self.machine     = ""aisgpu8""    # ""machine_id""\n        self.timestamp   = ""17082701""   # ""yymmdd##""\n        # training configuration\n        self.mode        = 1            # 1(train) | 2(test model_file)\n        self.config      = 7\n\n        self.seed        = 123\n        self.render      = False        # whether render the window from the original envs or not\n        self.visualize   = True         # whether do online plotting and stuff or not\n        self.save_best   = False        # save model w/ highest reward if True, otherwise always save the latest model\n\n        self.agent_type, self.env_type, self.game, self.model_type, self.memory_type = CONFIGS[self.config]\n\n        if self.agent_type == ""dqn"":\n            self.enable_double_dqn  = False\n            self.enable_dueling     = False\n            self.dueling_type       = \'avg\' # avg | max | naive\n\n            if self.env_type == ""gym"":\n                self.hist_len       = 1\n                self.hidden_dim     = 16\n            else:\n                self.hist_len       = 4\n                self.hidden_dim     = 256\n\n            self.use_cuda           = torch.cuda.is_available()\n            self.dtype              = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n        elif self.agent_type == ""a3c"":\n            self.enable_log_at_train_step = True # when False, x-axis would be frame_step instead of train_step\n\n            self.enable_lstm        = True\n            if ""-con"" in self.model_type:\n                self.enable_continuous  = True\n            else:\n                self.enable_continuous  = False\n            self.num_processes      = 16\n\n            self.hist_len           = 1\n            self.hidden_dim         = 128\n\n            self.use_cuda           = False\n            self.dtype              = torch.FloatTensor\n        elif self.agent_type == ""acer"":\n            self.enable_bias_correction   = True\n            self.enable_1st_order_trpo    = True\n            self.enable_log_at_train_step = True # when False, x-axis would be frame_step instead of train_step\n\n            self.enable_lstm        = True\n            if ""-con"" in self.model_type:\n                self.enable_continuous  = True\n            else:\n                self.enable_continuous  = False\n            self.num_processes      = 16\n\n            self.hist_len           = 1\n            self.hidden_dim         = 32\n\n            self.use_cuda           = False\n            self.dtype              = torch.FloatTensor\n        else:\n            self.hist_len           = 1\n            self.hidden_dim         = 256\n\n            self.use_cuda           = torch.cuda.is_available()\n            self.dtype              = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n\n        # prefix for model/log/visdom\n        self.refs        = self.machine + ""_"" + self.timestamp # NOTE: using this as env for visdom\n        self.root_dir    = os.getcwd()\n\n        # model files\n        # NOTE: will save the current model to model_name\n        self.model_name  = self.root_dir + ""/models/"" + self.refs + "".pth""\n        # NOTE: will load pretrained model_file if not None\n        self.model_file  = None#self.root_dir + ""/models/{TODO:FILL_IN_PRETAINED_MODEL_FILE}.pth""\n        if self.mode == 2:\n            self.model_file  = self.model_name  # NOTE: so only need to change self.mode to 2 to test the current training\n            assert self.model_file is not None, ""Pre-Trained model is None, Testing aborted!!!""\n            self.refs = self.refs + ""_test""     # NOTE: using this as env for visdom for testing, to avoid accidentally redraw on the training plots\n\n        # logging configs\n        self.log_name    = self.root_dir + ""/logs/"" + self.refs + "".log""\n        self.logger      = loggerConfig(self.log_name, self.verbose)\n        self.logger.warning(""<===================================>"")\n\n        if self.visualize:\n            self.vis = visdom.Visdom()\n            self.logger.warning(""bash$: python -m visdom.server"")           # activate visdom server on bash\n            self.logger.warning(""http://localhost:8097/env/"" + self.refs)   # open this address on browser\n\nclass EnvParams(Params):    # settings for simulation environment\n    def __init__(self):\n        super(EnvParams, self).__init__()\n\n        if self.env_type == ""gym"":\n            pass\n        elif self.env_type == ""atari-ram"":\n            pass\n        elif self.env_type == ""atari"":\n            self.hei_state = 42\n            self.wid_state = 42\n            self.preprocess_mode = 3    # 0(nothing) | 1(rgb2gray) | 2(rgb2y) | 3(crop&resize)\n        elif self.env_type == ""lab"":\n            pass\n        elif self.env_type == ""gazebo"":\n            self.hei_state = 60\n            self.wid_state = 80\n            self.preprocess_mode = 3  # 0(nothing) | 1(rgb2gray) | 2(rgb2y) | 3(crop&resize depth)\n            self.img_encoding_type = ""passthrough""\n        else:\n            assert False, ""env_type must be: gym | atari-ram | atari | lab""\n\nclass ModelParams(Params):  # settings for network architecture\n    def __init__(self):\n        super(ModelParams, self).__init__()\n\n        self.state_shape = None # NOTE: set in fit_model of inherited Agents\n        self.action_dim  = None # NOTE: set in fit_model of inherited Agents\n\nclass MemoryParams(Params):     # settings for replay memory\n    def __init__(self):\n        super(MemoryParams, self).__init__()\n\n        # NOTE: for multiprocess agents. this memory_size is the total number\n        # NOTE: across all processes\n        if self.agent_type == ""dqn"" and self.env_type == ""gym"":\n            self.memory_size = 50000\n        else:\n            self.memory_size = 1000000\n\nclass AgentParams(Params):  # hyperparameters for drl agents\n    def __init__(self):\n        super(AgentParams, self).__init__()\n\n        # criteria and optimizer\n        if self.agent_type == ""dqn"":\n            self.value_criteria = F.smooth_l1_loss\n            self.optim          = optim.Adam\n            # self.optim          = optim.RMSprop\n        elif self.agent_type == ""a3c"":\n            self.value_criteria = nn.MSELoss()\n            self.optim          = SharedAdam    # share momentum across learners\n        elif self.agent_type == ""acer"":\n            self.value_criteria = nn.MSELoss()\n            self.optim          = SharedRMSprop # share momentum across learners\n        else:\n            self.value_criteria = F.smooth_l1_loss\n            self.optim          = optim.Adam\n        # hyperparameters\n        if self.agent_type == ""dqn"" and self.env_type == ""gym"":\n            self.steps               = 100000   # max #iterations\n            self.early_stop          = None     # max #steps per episode\n            self.gamma               = 0.99\n            self.clip_grad           = 1.#np.inf\n            self.lr                  = 0.0001\n            self.lr_decay            = False\n            self.weight_decay        = 0.\n            self.eval_freq           = 2500     # NOTE: here means every this many steps\n            self.eval_steps          = 1000\n            self.prog_freq           = self.eval_freq\n            self.test_nepisodes      = 1\n\n            self.learn_start         = 500      # start update params after this many steps\n            self.batch_size          = 32\n            self.valid_size          = 250\n            self.eps_start           = 1\n            self.eps_end             = 0.3\n            self.eps_eval            = 0.#0.05\n            self.eps_decay           = 50000\n            self.target_model_update = 1000#0.0001\n            self.action_repetition   = 1\n            self.memory_interval     = 1\n            self.train_interval      = 1\n        elif self.agent_type == ""dqn"" and self.env_type == ""atari-ram"" or \\\n             self.agent_type == ""dqn"" and self.env_type == ""atari"":\n            self.steps               = 50000000 # max #iterations\n            self.early_stop          = None     # max #steps per episode\n            self.gamma               = 0.99\n            self.clip_grad           = 40.#np.inf\n            self.lr                  = 0.00025\n            self.lr_decay            = False\n            self.weight_decay        = 0.\n            self.eval_freq           = 250000#12500    # NOTE: here means every this many steps\n            self.eval_steps          = 125000#2500\n            self.prog_freq           = 10000#self.eval_freq\n            self.test_nepisodes      = 1\n\n            self.learn_start         = 50000    # start update params after this many steps\n            self.batch_size          = 32\n            self.valid_size          = 500\n            self.eps_start           = 1\n            self.eps_end             = 0.1\n            self.eps_eval            = 0.#0.05\n            self.eps_decay           = 1000000\n            self.target_model_update = 10000\n            self.action_repetition   = 4\n            self.memory_interval     = 1\n            self.train_interval      = 4\n        elif self.agent_type == ""a3c"":\n            self.steps               = 20000000 # max #iterations\n            self.early_stop          = None     # max #steps per episode\n            self.gamma               = 0.99\n            self.clip_grad           = 40.\n            self.lr                  = 0.0001\n            self.lr_decay            = False\n            self.weight_decay        = 1e-4 if self.enable_continuous else 0.\n            self.eval_freq           = 60       # NOTE: here means every this many seconds\n            self.eval_steps          = 3000\n            self.prog_freq           = self.eval_freq\n            self.test_nepisodes      = 10\n\n            self.rollout_steps       = 20       # max look-ahead steps in a single rollout\n            self.tau                 = 1.\n            self.beta                = 0.01     # coefficient for entropy penalty\n        elif self.agent_type == ""acer"":\n            self.steps               = 20000000 # max #iterations\n            self.early_stop          = 200      # max #steps per episode\n            self.gamma               = 0.99\n            self.clip_grad           = 40.\n            self.lr                  = 0.0001\n            self.lr_decay            = False\n            self.weight_decay        = 1e-4\n            self.eval_freq           = 60       # NOTE: here means every this many seconds\n            self.eval_steps          = 3000\n            self.prog_freq           = self.eval_freq\n            self.test_nepisodes      = 10\n\n            self.replay_ratio        = 4        # NOTE: 0: purely on-policy; otherwise mix with off-policy\n            self.replay_start        = 20000    # start off-policy learning after this many steps\n            self.batch_size          = 16\n            self.valid_size          = 500      # TODO: should do the same thing as in dqn\n            self.clip_trace          = 10#np.inf# c in retrace\n            self.clip_1st_order_trpo = 1\n            self.avg_model_decay     = 0.99\n\n            self.rollout_steps       = 20       # max look-ahead steps in a single rollout\n            self.tau                 = 1.\n            self.beta                = 1e-2     # coefficient for entropy penalty\n        else:\n            self.steps               = 1000000  # max #iterations\n            self.early_stop          = None     # max #steps per episode\n            self.gamma               = 0.99\n            self.clip_grad           = 1.#np.inf\n            self.lr                  = 0.001\n            self.lr_decay            = False\n            self.weight_decay        = 0.\n            self.eval_freq           = 2500     # NOTE: here means every this many steps\n            self.eval_steps          = 1000\n            self.prog_freq           = self.eval_freq\n            self.test_nepisodes      = 10\n\n            self.learn_start         = 25000    # start update params after this many steps\n            self.batch_size          = 32\n            self.valid_size          = 500\n            self.eps_start           = 1\n            self.eps_end             = 0.1\n            self.eps_eval            = 0.#0.05\n            self.eps_decay           = 50000\n            self.target_model_update = 1000\n            self.action_repetition   = 1\n            self.memory_interval     = 1\n            self.train_interval      = 4\n\n            self.rollout_steps       = 20       # max look-ahead steps in a single rollout\n            self.tau                 = 1.\n\n        if self.memory_type == ""episodic"": assert self.early_stop is not None\n\n        self.env_params    = EnvParams()\n        self.model_params  = ModelParams()\n        self.memory_params = MemoryParams()\n\nclass Options(Params):\n    agent_params  = AgentParams()\n'"
core/agents/__init__.py,0,b''
core/agents/a3c.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.multiprocessing as mp\n\nfrom core.agent import Agent\nfrom core.agents.a3c_single_process import A3CLearner, A3CEvaluator, A3CTester\n\nclass A3CAgent(Agent):\n    def __init__(self, args, env_prototype, model_prototype, memory_prototype):\n        super(A3CAgent, self).__init__(args, env_prototype, model_prototype, memory_prototype)\n        self.logger.warning(""<===================================> A3C-Master {Env(dummy) & Model}"")\n\n        # dummy_env just to get state_shape & action_dim\n        self.dummy_env   = self.env_prototype(self.env_params, self.num_processes)\n        self.state_shape = self.dummy_env.state_shape\n        self.action_dim  = self.dummy_env.action_dim\n        del self.dummy_env\n\n        # global shared model\n        self.model_params.state_shape = self.state_shape\n        self.model_params.action_dim  = self.action_dim\n        self.model = self.model_prototype(self.model_params)\n        self._load_model(self.model_file)   # load pretrained model if provided\n        self.model.share_memory()           # NOTE\n\n        # learning algorithm\n        self.optimizer    = self.optim(self.model.parameters(), lr = self.lr)\n        self.optimizer.share_memory()       # NOTE\n        self.lr_adjusted  = mp.Value(\'d\', self.lr) # adjusted lr\n\n        # global counters\n        self.frame_step   = mp.Value(\'l\', 0) # global frame step counter\n        self.train_step   = mp.Value(\'l\', 0) # global train step counter\n        # global training stats\n        self.p_loss_avg   = mp.Value(\'d\', 0.) # global policy loss\n        self.v_loss_avg   = mp.Value(\'d\', 0.) # global value loss\n        self.loss_avg     = mp.Value(\'d\', 0.) # global loss\n        self.loss_counter = mp.Value(\'l\', 0)  # storing this many losses\n        self._reset_training_loggings()\n\n    def _reset_training_loggings(self):\n        self.p_loss_avg.value   = 0.\n        self.v_loss_avg.value   = 0.\n        self.loss_avg.value     = 0.\n        self.loss_counter.value = 0\n\n    def fit_model(self):\n        self.jobs = []\n        for process_id in range(self.num_processes):\n            self.jobs.append(A3CLearner(self, process_id))\n        self.jobs.append(A3CEvaluator(self, self.num_processes))\n\n        self.logger.warning(""<===================================> Training ..."")\n        for job in self.jobs:\n            job.start()\n        for job in self.jobs:\n            job.join()\n\n    def test_model(self):\n        self.jobs = []\n        self.jobs.append(A3CTester(self))\n\n        self.logger.warning(""<===================================> Testing ..."")\n        for job in self.jobs:\n            job.start()\n        for job in self.jobs:\n            job.join()\n'"
core/agents/a3c_single_process.py,17,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport random\nimport time\nimport math\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nfrom utils.helpers import A3C_Experience\nfrom core.agent_single_process import AgentSingleProcess\n\nclass A3CSingleProcess(AgentSingleProcess):\n    def __init__(self, master, process_id=0):\n        super(A3CSingleProcess, self).__init__(master, process_id)\n\n        # lstm hidden states\n        if self.master.enable_lstm:\n            self._reset_lstm_hidden_vb_episode() # clear up hidden state\n            self._reset_lstm_hidden_vb_rollout() # detach the previous variable from the computation graph\n\n        # NOTE global variable pi\n        if self.master.enable_continuous:\n            self.pi_vb = Variable(torch.Tensor([math.pi]).type(self.master.dtype))\n\n        self.master.logger.warning(""Registered A3C-SingleProcess-Agent #"" + str(self.process_id) + "" w/ Env (seed:"" + str(self.env.seed) + "")."")\n\n    # NOTE: to be called at the beginning of each new episode, clear up the hidden state\n    def _reset_lstm_hidden_vb_episode(self, training=True): # seq_len, batch_size, hidden_dim\n        not_training = not training\n        if self.master.enable_continuous:\n            self.lstm_hidden_vb = (Variable(torch.zeros(2, self.master.hidden_dim).type(self.master.dtype), volatile=not_training),\n                                   Variable(torch.zeros(2, self.master.hidden_dim).type(self.master.dtype), volatile=not_training))\n        else:\n            self.lstm_hidden_vb = (Variable(torch.zeros(1, self.master.hidden_dim).type(self.master.dtype), volatile=not_training),\n                                   Variable(torch.zeros(1, self.master.hidden_dim).type(self.master.dtype), volatile=not_training))\n\n    # NOTE: to be called at the beginning of each rollout, detach the previous variable from the graph\n    def _reset_lstm_hidden_vb_rollout(self):\n        self.lstm_hidden_vb = (Variable(self.lstm_hidden_vb[0].data),\n                               Variable(self.lstm_hidden_vb[1].data))\n\n    def _preprocessState(self, state, is_valotile=False):\n        if isinstance(state, list):\n            state_vb = []\n            for i in range(len(state)):\n                state_vb.append(Variable(torch.from_numpy(state[i]).unsqueeze(0).type(self.master.dtype), volatile=is_valotile))\n        else:\n            state_vb = Variable(torch.from_numpy(state).unsqueeze(0).type(self.master.dtype), volatile=is_valotile)\n        return state_vb\n\n    def _forward(self, state_vb):\n        if self.master.enable_continuous: # NOTE continous control p_vb here is the mu_vb of continous action dist\n            if self.master.enable_lstm:\n                p_vb, sig_vb, v_vb, self.lstm_hidden_vb = self.model(state_vb, self.lstm_hidden_vb)\n            else:\n                p_vb, sig_vb, v_vb = self.model(state_vb)\n            if self.training:\n                _eps = torch.randn(p_vb.size())\n                action = (p_vb + sig_vb.sqrt()*Variable(_eps)).data.numpy()\n            else:\n                action = p_vb.data.numpy()\n            return action, p_vb, sig_vb, v_vb\n        else:\n            if self.master.enable_lstm:\n                p_vb, v_vb, self.lstm_hidden_vb = self.model(state_vb, self.lstm_hidden_vb)\n            else:\n                p_vb, v_vb = self.model(state_vb)\n            if self.training:\n                action = p_vb.multinomial().data[0][0]\n            else:\n                action = p_vb.max(1)[1].data.squeeze().numpy()[0]\n            return action, p_vb, v_vb\n\n    def _normal(self, x, mu, sigma_sq):\n        a = (-1 * (x - mu).pow(2) / (2 * sigma_sq)).exp()\n        b = 1 / (2 * sigma_sq * self.pi_vb.expand_as(sigma_sq)).sqrt()\n        return (a * b).log()\n\nclass A3CLearner(A3CSingleProcess):\n    def __init__(self, master, process_id=0):\n        master.logger.warning(""<===================================> A3C-Learner #"" + str(process_id) + "" {Env & Model}"")\n        super(A3CLearner, self).__init__(master, process_id)\n\n        self._reset_rollout()\n\n        self.training = True    # choose actions by polinomial\n        self.model.train(self.training)\n        # local counters\n        self.frame_step   = 0   # local frame step counter\n        self.train_step   = 0   # local train step counter\n        # local training stats\n        self.p_loss_avg   = 0.  # global policy loss\n        self.v_loss_avg   = 0.  # global value loss\n        self.loss_avg     = 0.  # global value loss\n        self.loss_counter = 0   # storing this many losses\n        self._reset_training_loggings()\n\n        # copy local training stats to global every prog_freq\n        self.last_prog = time.time()\n\n    def _reset_training_loggings(self):\n        self.p_loss_avg   = 0.\n        self.v_loss_avg   = 0.\n        self.loss_avg     = 0.\n        self.loss_counter = 0\n\n    def _reset_rollout(self):       # for storing the experiences collected through one rollout\n        self.rollout = A3C_Experience(state0 = [],\n                                      action = [],\n                                      reward = [],\n                                      state1 = [],\n                                      terminal1 = [],\n                                      policy_vb = [],\n                                      sigmoid_vb = [],\n                                      value0_vb = [])\n\n    def _get_valueT_vb(self):\n        if self.rollout.terminal1[-1]:  # for terminal sT\n            valueT_vb = Variable(torch.zeros(1, 1))\n        else:                           # for non-terminal sT\n            sT_vb = self._preprocessState(self.rollout.state1[-1], True)        # bootstrap from last state\n            if self.master.enable_continuous:\n                if self.master.enable_lstm:\n                    _, _, valueT_vb, _ = self.model(sT_vb, self.lstm_hidden_vb) # NOTE: only doing inference here\n                else:\n                    _, _, valueT_vb = self.model(sT_vb)                         # NOTE: only doing inference here\n            else:\n                if self.master.enable_lstm:\n                    _, valueT_vb, _ = self.model(sT_vb, self.lstm_hidden_vb)    # NOTE: only doing inference here\n                else:\n                    _, valueT_vb = self.model(sT_vb)                            # NOTE: only doing inference here\n            # NOTE: here valueT_vb.volatile=True since sT_vb.volatile=True\n            # NOTE: if we use detach() here, it would remain volatile\n            # NOTE: then all the follow-up computations would only give volatile loss variables\n            valueT_vb = Variable(valueT_vb.data)\n\n        return valueT_vb\n\n    def _backward(self):\n        # preparation\n        rollout_steps = len(self.rollout.reward)\n        policy_vb = self.rollout.policy_vb\n        if self.master.enable_continuous:\n            action_batch_vb = Variable(torch.from_numpy(np.array(self.rollout.action)))\n            if self.master.use_cuda:\n                action_batch_vb = action_batch_vb.cuda()\n            sigma_vb = self.rollout.sigmoid_vb\n        else:\n            action_batch_vb = Variable(torch.from_numpy(np.array(self.rollout.action)).long())\n            if self.master.use_cuda:\n                action_batch_vb = action_batch_vb.cuda()\n            policy_log_vb = [torch.log(policy_vb[i]) for i in range(rollout_steps)]\n            entropy_vb    = [- (policy_log_vb[i] * policy_vb[i]).sum(1) for i in range(rollout_steps)]\n            policy_log_vb = [policy_log_vb[i].gather(1, action_batch_vb[i].unsqueeze(0)) for i in range(rollout_steps) ]\n        valueT_vb     = self._get_valueT_vb()\n        self.rollout.value0_vb.append(Variable(valueT_vb.data)) # NOTE: only this last entry is Volatile, all others are still in the graph\n        gae_ts        = torch.zeros(1, 1)\n\n        # compute loss\n        policy_loss_vb = 0.\n        value_loss_vb  = 0.\n        for i in reversed(range(rollout_steps)):\n            valueT_vb     = self.master.gamma * valueT_vb + self.rollout.reward[i]\n            advantage_vb  = valueT_vb - self.rollout.value0_vb[i]\n            value_loss_vb = value_loss_vb + 0.5 * advantage_vb.pow(2)\n\n            # Generalized Advantage Estimation\n            tderr_ts = self.rollout.reward[i] + self.master.gamma * self.rollout.value0_vb[i + 1].data - self.rollout.value0_vb[i].data\n            gae_ts   = self.master.gamma * gae_ts * self.master.tau + tderr_ts\n            if self.master.enable_continuous:\n                _log_prob = self._normal(action_batch_vb[i], policy_vb[i], sigma_vb[i])\n                _entropy = 0.5 * ((sigma_vb[i] * 2 * self.pi_vb.expand_as(sigma_vb[i])).log() + 1)\n                policy_loss_vb -= (_log_prob * Variable(gae_ts).expand_as(_log_prob)).sum() + self.master.beta * _entropy.sum()\n            else:\n                policy_loss_vb -= policy_log_vb[i] * Variable(gae_ts) + self.master.beta * entropy_vb[i]\n\n        loss_vb = policy_loss_vb + 0.5 * value_loss_vb\n        loss_vb.backward()\n        torch.nn.utils.clip_grad_norm(self.model.parameters(), self.master.clip_grad)\n\n        self._ensure_global_grads()\n        self.master.optimizer.step()\n        self.train_step += 1\n        self.master.train_step.value += 1\n\n        # adjust learning rate if enabled\n        if self.master.lr_decay:\n            self.master.lr_adjusted.value = max(self.master.lr * (self.master.steps - self.master.train_step.value) / self.master.steps, 1e-32)\n            adjust_learning_rate(self.master.optimizer, self.master.lr_adjusted.value)\n\n        # log training stats\n        self.p_loss_avg   += policy_loss_vb.data.numpy()\n        self.v_loss_avg   += value_loss_vb.data.numpy()\n        self.loss_avg     += loss_vb.data.numpy()\n        self.loss_counter += 1\n\n    def _rollout(self, episode_steps, episode_reward):\n        # reset rollout experiences\n        self._reset_rollout()\n\n        t_start = self.frame_step\n        # continue to rollout only if:\n        # 1. not running out of max steps of this current rollout, and\n        # 2. not terminal, and\n        # 3. not exceeding max steps of this current episode\n        # 4. master not exceeding max train steps\n        while (self.frame_step - t_start) < self.master.rollout_steps \\\n              and not self.experience.terminal1 \\\n              and (self.master.early_stop is None or episode_steps < self.master.early_stop):\n            # NOTE: here first store the last frame: experience.state1 as rollout.state0\n            self.rollout.state0.append(self.experience.state1)\n            # then get the action to take from rollout.state0 (experience.state1)\n            if self.master.enable_continuous:\n                action, p_vb, sig_vb, v_vb = self._forward(self._preprocessState(self.experience.state1))\n                self.rollout.sigmoid_vb.append(sig_vb)\n            else:\n                action, p_vb, v_vb = self._forward(self._preprocessState(self.experience.state1))\n            # then execute action in env to get a new experience.state1 -> rollout.state1\n            self.experience = self.env.step(action)\n            # push experience into rollout\n            self.rollout.action.append(action)\n            self.rollout.reward.append(self.experience.reward)\n            self.rollout.state1.append(self.experience.state1)\n            self.rollout.terminal1.append(self.experience.terminal1)\n            self.rollout.policy_vb.append(p_vb)\n            self.rollout.value0_vb.append(v_vb)\n\n            episode_steps += 1\n            episode_reward += self.experience.reward\n            self.frame_step += 1\n            self.master.frame_step.value += 1\n\n            # NOTE: we put this condition in the end to make sure this current rollout won\'t be empty\n            if self.master.train_step.value >= self.master.steps:\n                break\n\n        return episode_steps, episode_reward\n\n    def run(self):\n        # make sure processes are not completely synced by sleeping a bit\n        time.sleep(int(np.random.rand() * (self.process_id + 5)))\n\n        nepisodes = 0\n        nepisodes_solved = 0\n        episode_steps = None\n        episode_reward = None\n        should_start_new = True\n        while self.master.train_step.value < self.master.steps:\n            # sync in every step\n            self._sync_local_with_global()\n            self.model.zero_grad()\n\n            # start of a new episode\n            if should_start_new:\n                episode_steps = 0\n                episode_reward = 0.\n                # reset lstm_hidden_vb for new episode\n                if self.master.enable_lstm:\n                    # NOTE: clear hidden state at the beginning of each episode\n                    self._reset_lstm_hidden_vb_episode()\n                # Obtain the initial observation by resetting the environment\n                self._reset_experience()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                # reset flag\n                should_start_new = False\n            if self.master.enable_lstm:\n                # NOTE: detach the previous hidden variable from the graph at the beginning of each rollout\n                self._reset_lstm_hidden_vb_rollout()\n            # Run a rollout for rollout_steps or until terminal\n            episode_steps, episode_reward = self._rollout(episode_steps, episode_reward)\n\n            if self.experience.terminal1 or \\\n               self.master.early_stop and episode_steps >= self.master.early_stop:\n                nepisodes += 1\n                should_start_new = True\n                if self.experience.terminal1:\n                    nepisodes_solved += 1\n\n            # calculate loss\n            self._backward()\n\n            # copy local training stats to global at prog_freq, and clear up local stats\n            if time.time() - self.last_prog >= self.master.prog_freq:\n                self.master.p_loss_avg.value   += self.p_loss_avg\n                self.master.v_loss_avg.value   += self.v_loss_avg\n                self.master.loss_avg.value     += self.loss_avg\n                self.master.loss_counter.value += self.loss_counter\n                self._reset_training_loggings()\n                self.last_prog = time.time()\n\nclass A3CEvaluator(A3CSingleProcess):\n    def __init__(self, master, process_id=0):\n        master.logger.warning(""<===================================> A3C-Evaluator {Env & Model}"")\n        super(A3CEvaluator, self).__init__(master, process_id)\n\n        self.training = False   # choose actions w/ max probability\n        self.model.train(self.training)\n        self._reset_loggings()\n\n        self.start_time = time.time()\n        self.last_eval = time.time()\n\n    def _reset_loggings(self):\n        # training stats across all processes\n        self.p_loss_avg_log = []\n        self.v_loss_avg_log = []\n        self.loss_avg_log = []\n        # evaluation stats\n        self.entropy_avg_log = []\n        self.v_avg_log = []\n        self.steps_avg_log = []\n        self.steps_std_log = []\n        self.reward_avg_log = []\n        self.reward_std_log = []\n        self.nepisodes_log = []\n        self.nepisodes_solved_log = []\n        self.repisodes_solved_log = []\n        # placeholders for windows for online curve plotting\n        if self.master.visualize:\n            # training stats across all processes\n            self.win_p_loss_avg = ""win_p_loss_avg""\n            self.win_v_loss_avg = ""win_v_loss_avg""\n            self.win_loss_avg = ""win_loss_avg""\n            # evaluation stats\n            self.win_entropy_avg = ""win_entropy_avg""\n            self.win_v_avg = ""win_v_avg""\n            self.win_steps_avg = ""win_steps_avg""\n            self.win_steps_std = ""win_steps_std""\n            self.win_reward_avg = ""win_reward_avg""\n            self.win_reward_std = ""win_reward_std""\n            self.win_nepisodes = ""win_nepisodes""\n            self.win_nepisodes_solved = ""win_nepisodes_solved""\n            self.win_repisodes_solved = ""win_repisodes_solved""\n\n    def _eval_model(self):\n        self.last_eval = time.time()\n        eval_at_train_step = self.master.train_step.value\n        eval_at_frame_step = self.master.frame_step.value\n        # first grab the latest global model to do the evaluation\n        self._sync_local_with_global()\n\n        # evaluate\n        eval_step = 0\n\n        eval_entropy_log = []\n        eval_v_log = []\n        eval_nepisodes = 0\n        eval_nepisodes_solved = 0\n        eval_episode_steps = None\n        eval_episode_steps_log = []\n        eval_episode_reward = None\n        eval_episode_reward_log = []\n        eval_should_start_new = True\n        while eval_step < self.master.eval_steps:\n            if eval_should_start_new:   # start of a new episode\n                eval_episode_steps = 0\n                eval_episode_reward = 0.\n                # reset lstm_hidden_vb for new episode\n                if self.master.enable_lstm:\n                    # NOTE: clear hidden state at the beginning of each episode\n                    self._reset_lstm_hidden_vb_episode(self.training)\n                # Obtain the initial observation by resetting the environment\n                self._reset_experience()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                if not self.training:\n                    if self.master.visualize: self.env.visual()\n                    if self.master.render: self.env.render()\n                # reset flag\n                eval_should_start_new = False\n            if self.master.enable_lstm:\n                # NOTE: detach the previous hidden variable from the graph at the beginning of each step\n                # NOTE: not necessary here in evaluation but we do it anyways\n                self._reset_lstm_hidden_vb_rollout()\n            # Run a single step\n            if self.master.enable_continuous:\n                eval_action, p_vb, sig_vb, v_vb = self._forward(self._preprocessState(self.experience.state1, True))\n            else:\n                eval_action, p_vb, v_vb = self._forward(self._preprocessState(self.experience.state1, True))\n            self.experience = self.env.step(eval_action)\n            if not self.training:\n                if self.master.visualize: self.env.visual()\n                if self.master.render: self.env.render()\n            if self.experience.terminal1 or \\\n               self.master.early_stop and (eval_episode_steps + 1) == self.master.early_stop or \\\n               (eval_step + 1) == self.master.eval_steps:\n                eval_should_start_new = True\n\n            eval_episode_steps += 1\n            eval_episode_reward += self.experience.reward\n            eval_step += 1\n\n            if eval_should_start_new:\n                eval_nepisodes += 1\n                if self.experience.terminal1:\n                    eval_nepisodes_solved += 1\n\n                # This episode is finished, report and reset\n                # NOTE make no sense for continuous\n                if self.master.enable_continuous:\n                    eval_entropy_log.append([0.5 * ((sig_vb * 2 * self.pi_vb.expand_as(sig_vb)).log() + 1).data.numpy()])\n                else:\n                    eval_entropy_log.append([np.mean((-torch.log(p_vb.data.squeeze()) * p_vb.data.squeeze()).numpy())])\n                eval_v_log.append([v_vb.data.numpy()])\n                eval_episode_steps_log.append([eval_episode_steps])\n                eval_episode_reward_log.append([eval_episode_reward])\n                self._reset_experience()\n                eval_episode_steps = None\n                eval_episode_reward = None\n\n        # Logging for this evaluation phase\n        loss_counter = self.master.loss_counter.value\n        p_loss_avg = self.master.p_loss_avg.value / loss_counter if loss_counter > 0 else 0.\n        v_loss_avg = self.master.v_loss_avg.value / loss_counter if loss_counter > 0 else 0.\n        loss_avg = self.master.loss_avg.value / loss_counter if loss_counter > 0 else 0.\n        self.master._reset_training_loggings()\n        def _log_at_step(eval_at_step):\n            self.p_loss_avg_log.append([eval_at_step, p_loss_avg])\n            self.v_loss_avg_log.append([eval_at_step, v_loss_avg])\n            self.loss_avg_log.append([eval_at_step, loss_avg])\n            self.entropy_avg_log.append([eval_at_step, np.mean(np.asarray(eval_entropy_log))])\n            self.v_avg_log.append([eval_at_step, np.mean(np.asarray(eval_v_log))])\n            self.steps_avg_log.append([eval_at_step, np.mean(np.asarray(eval_episode_steps_log))])\n            self.steps_std_log.append([eval_at_step, np.std(np.asarray(eval_episode_steps_log))])\n            self.reward_avg_log.append([eval_at_step, np.mean(np.asarray(eval_episode_reward_log))])\n            self.reward_std_log.append([eval_at_step, np.std(np.asarray(eval_episode_reward_log))])\n            self.nepisodes_log.append([eval_at_step, eval_nepisodes])\n            self.nepisodes_solved_log.append([eval_at_step, eval_nepisodes_solved])\n            self.repisodes_solved_log.append([eval_at_step, (eval_nepisodes_solved/eval_nepisodes) if eval_nepisodes > 0 else 0.])\n            # logging\n            self.master.logger.warning(""Reporting       @ Step: "" + str(eval_at_step) + "" | Elapsed Time: "" + str(time.time() - self.start_time))\n            self.master.logger.warning(""Iteration: {}; lr: {}"".format(eval_at_step, self.master.lr_adjusted.value))\n            self.master.logger.warning(""Iteration: {}; p_loss_avg: {}"".format(eval_at_step, self.p_loss_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; v_loss_avg: {}"".format(eval_at_step, self.v_loss_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; loss_avg: {}"".format(eval_at_step, self.loss_avg_log[-1][1]))\n            self.master._reset_training_loggings()\n            self.master.logger.warning(""Evaluating      @ Step: "" + str(eval_at_train_step) + "" | ("" + str(eval_at_frame_step) + "" frames)..."")\n            self.master.logger.warning(""Evaluation        Took: "" + str(time.time() - self.last_eval))\n            self.master.logger.warning(""Iteration: {}; entropy_avg: {}"".format(eval_at_step, self.entropy_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; v_avg: {}"".format(eval_at_step, self.v_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; steps_avg: {}"".format(eval_at_step, self.steps_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; steps_std: {}"".format(eval_at_step, self.steps_std_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; reward_avg: {}"".format(eval_at_step, self.reward_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; reward_std: {}"".format(eval_at_step, self.reward_std_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; nepisodes: {}"".format(eval_at_step, self.nepisodes_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; nepisodes_solved: {}"".format(eval_at_step, self.nepisodes_solved_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; repisodes_solved: {}"".format(eval_at_step, self.repisodes_solved_log[-1][1]))\n        if self.master.enable_log_at_train_step:\n            _log_at_step(eval_at_train_step)\n        else:\n            _log_at_step(eval_at_frame_step)\n\n        # plotting\n        if self.master.visualize:\n            self.win_p_loss_avg = self.master.vis.scatter(X=np.array(self.p_loss_avg_log), env=self.master.refs, win=self.win_p_loss_avg, opts=dict(title=""p_loss_avg""))\n            self.win_v_loss_avg = self.master.vis.scatter(X=np.array(self.v_loss_avg_log), env=self.master.refs, win=self.win_v_loss_avg, opts=dict(title=""v_loss_avg""))\n            self.win_loss_avg = self.master.vis.scatter(X=np.array(self.loss_avg_log), env=self.master.refs, win=self.win_loss_avg, opts=dict(title=""loss_avg""))\n            self.win_entropy_avg = self.master.vis.scatter(X=np.array(self.entropy_avg_log), env=self.master.refs, win=self.win_entropy_avg, opts=dict(title=""entropy_avg""))\n            self.win_v_avg = self.master.vis.scatter(X=np.array(self.v_avg_log), env=self.master.refs, win=self.win_v_avg, opts=dict(title=""v_avg""))\n            self.win_steps_avg = self.master.vis.scatter(X=np.array(self.steps_avg_log), env=self.master.refs, win=self.win_steps_avg, opts=dict(title=""steps_avg""))\n            # self.win_steps_std = self.master.vis.scatter(X=np.array(self.steps_std_log), env=self.master.refs, win=self.win_steps_std, opts=dict(title=""steps_std""))\n            self.win_reward_avg = self.master.vis.scatter(X=np.array(self.reward_avg_log), env=self.master.refs, win=self.win_reward_avg, opts=dict(title=""reward_avg""))\n            # self.win_reward_std = self.master.vis.scatter(X=np.array(self.reward_std_log), env=self.master.refs, win=self.win_reward_std, opts=dict(title=""reward_std""))\n            self.win_nepisodes = self.master.vis.scatter(X=np.array(self.nepisodes_log), env=self.master.refs, win=self.win_nepisodes, opts=dict(title=""nepisodes""))\n            self.win_nepisodes_solved = self.master.vis.scatter(X=np.array(self.nepisodes_solved_log), env=self.master.refs, win=self.win_nepisodes_solved, opts=dict(title=""nepisodes_solved""))\n            self.win_repisodes_solved = self.master.vis.scatter(X=np.array(self.repisodes_solved_log), env=self.master.refs, win=self.win_repisodes_solved, opts=dict(title=""repisodes_solved""))\n        self.last_eval = time.time()\n\n        # save model\n        self.master._save_model(eval_at_train_step, self.reward_avg_log[-1][1])\n\n    def run(self):\n        while self.master.train_step.value < self.master.steps:\n            if time.time() - self.last_eval > self.master.eval_freq:\n                self._eval_model()\n        # we also do a final evaluation after training is done\n        self._eval_model()\n\nclass A3CTester(A3CSingleProcess):\n    def __init__(self, master, process_id=0):\n        master.logger.warning(""<===================================> A3C-Tester {Env & Model}"")\n        super(A3CTester, self).__init__(master, process_id)\n\n        self.training = False   # choose actions w/ max probability\n        self.model.train(self.training)\n        self._reset_loggings()\n\n        self.start_time = time.time()\n\n    def _reset_loggings(self):\n        # testing stats\n        self.steps_avg_log = []\n        self.steps_std_log = []\n        self.reward_avg_log = []\n        self.reward_std_log = []\n        self.nepisodes_log = []\n        self.nepisodes_solved_log = []\n        self.repisodes_solved_log = []\n        # placeholders for windows for online curve plotting\n        if self.master.visualize:\n            # evaluation stats\n            self.win_steps_avg = ""win_steps_avg""\n            self.win_steps_std = ""win_steps_std""\n            self.win_reward_avg = ""win_reward_avg""\n            self.win_reward_std = ""win_reward_std""\n            self.win_nepisodes = ""win_nepisodes""\n            self.win_nepisodes_solved = ""win_nepisodes_solved""\n            self.win_repisodes_solved = ""win_repisodes_solved""\n\n    def run(self):\n        test_step = 0\n        test_nepisodes = 0\n        test_nepisodes_solved = 0\n        test_episode_steps = None\n        test_episode_steps_log = []\n        test_episode_reward = None\n        test_episode_reward_log = []\n        test_should_start_new = True\n        while test_nepisodes < self.master.test_nepisodes:\n            if test_should_start_new:   # start of a new episode\n                test_episode_steps = 0\n                test_episode_reward = 0.\n                # reset lstm_hidden_vb for new episode\n                if self.master.enable_lstm:\n                    # NOTE: clear hidden state at the beginning of each episode\n                    self._reset_lstm_hidden_vb_episode(self.training)\n                # Obtain the initial observation by resetting the environment\n                self._reset_experience()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                if not self.training:\n                    if self.master.visualize: self.env.visual()\n                    if self.master.render: self.env.render()\n                # reset flag\n                test_should_start_new = False\n            if self.master.enable_lstm:\n                # NOTE: detach the previous hidden variable from the graph at the beginning of each step\n                # NOTE: not necessary here in testing but we do it anyways\n                self._reset_lstm_hidden_vb_rollout()\n            # Run a single step\n            if self.master.enable_continuous:\n                test_action, p_vb, sig_vb, v_vb = self._forward(self._preprocessState(self.experience.state1, True))\n            else:\n                test_action, p_vb, v_vb = self._forward(self._preprocessState(self.experience.state1, True))\n            self.experience = self.env.step(test_action)\n            if not self.training:\n                if self.master.visualize: self.env.visual()\n                if self.master.render: self.env.render()\n            if self.experience.terminal1 or \\\n               self.master.early_stop and (test_episode_steps + 1) == self.master.early_stop:\n                test_should_start_new = True\n\n            test_episode_steps += 1\n            test_episode_reward += self.experience.reward\n            test_step += 1\n\n            if test_should_start_new:\n                test_nepisodes += 1\n                if self.experience.terminal1:\n                    test_nepisodes_solved += 1\n\n                # This episode is finished, report and reset\n                test_episode_steps_log.append([test_episode_steps])\n                test_episode_reward_log.append([test_episode_reward])\n                self._reset_experience()\n                test_episode_steps = None\n                test_episode_reward = None\n\n        self.steps_avg_log.append([test_nepisodes, np.mean(np.asarray(test_episode_steps_log))])\n        self.steps_std_log.append([test_nepisodes, np.std(np.asarray(test_episode_steps_log))]); del test_episode_steps_log\n        self.reward_avg_log.append([test_nepisodes, np.mean(np.asarray(test_episode_reward_log))])\n        self.reward_std_log.append([test_nepisodes, np.std(np.asarray(test_episode_reward_log))]); del test_episode_reward_log\n        self.nepisodes_log.append([test_nepisodes, test_nepisodes])\n        self.nepisodes_solved_log.append([test_nepisodes, test_nepisodes_solved])\n        self.repisodes_solved_log.append([test_nepisodes, (test_nepisodes_solved/test_nepisodes) if test_nepisodes > 0 else 0.])\n        # plotting\n        if self.master.visualize:\n            self.win_steps_avg = self.master.vis.scatter(X=np.array(self.steps_avg_log), env=self.master.refs, win=self.win_steps_avg, opts=dict(title=""steps_avg""))\n            # self.win_steps_std = self.master.vis.scatter(X=np.array(self.steps_std_log), env=self.master.refs, win=self.win_steps_std, opts=dict(title=""steps_std""))\n            self.win_reward_avg = self.master.vis.scatter(X=np.array(self.reward_avg_log), env=self.master.refs, win=self.win_reward_avg, opts=dict(title=""reward_avg""))\n            # self.win_reward_std = self.master.vis.scatter(X=np.array(self.reward_std_log), env=self.master.refs, win=self.win_reward_std, opts=dict(title=""reward_std""))\n            self.win_nepisodes = self.master.vis.scatter(X=np.array(self.nepisodes_log), env=self.master.refs, win=self.win_nepisodes, opts=dict(title=""nepisodes""))\n            self.win_nepisodes_solved = self.master.vis.scatter(X=np.array(self.nepisodes_solved_log), env=self.master.refs, win=self.win_nepisodes_solved, opts=dict(title=""nepisodes_solved""))\n            self.win_repisodes_solved = self.master.vis.scatter(X=np.array(self.repisodes_solved_log), env=self.master.refs, win=self.win_repisodes_solved, opts=dict(title=""repisodes_solved""))\n        # logging\n        self.master.logger.warning(""Testing  Took: "" + str(time.time() - self.start_time))\n        self.master.logger.warning(""Testing: steps_avg: {}"".format(self.steps_avg_log[-1][1]))\n        self.master.logger.warning(""Testing: steps_std: {}"".format(self.steps_std_log[-1][1]))\n        self.master.logger.warning(""Testing: reward_avg: {}"".format(self.reward_avg_log[-1][1]))\n        self.master.logger.warning(""Testing: reward_std: {}"".format(self.reward_std_log[-1][1]))\n        self.master.logger.warning(""Testing: nepisodes: {}"".format(self.nepisodes_log[-1][1]))\n        self.master.logger.warning(""Testing: nepisodes_solved: {}"".format(self.nepisodes_solved_log[-1][1]))\n        self.master.logger.warning(""Testing: repisodes_solved: {}"".format(self.repisodes_solved_log[-1][1]))\n'"
core/agents/acer.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.multiprocessing as mp\n\nfrom core.agent import Agent\nfrom core.agents.acer_single_process import ACERLearner, ACEREvaluator, ACERTester\n\nclass ACERAgent(Agent):\n    def __init__(self, args, env_prototype, model_prototype, memory_prototype):\n        super(ACERAgent, self).__init__(args, env_prototype, model_prototype, memory_prototype)\n        self.logger.warning(""<===================================> ACER-Master {Env(dummy) & Model}"")\n\n        # dummy_env just to get state_shape & action_dim\n        self.dummy_env   = self.env_prototype(self.env_params, self.num_processes)\n        self.state_shape = self.dummy_env.state_shape\n        self.action_dim  = self.dummy_env.action_dim\n        del self.dummy_env\n\n        # global shared model\n        self.model_params.state_shape = self.state_shape\n        self.model_params.action_dim  = self.action_dim\n        self.model = self.model_prototype(self.model_params)\n        self._load_model(self.model_file)   # load pretrained model if provided\n        self.model.share_memory()           # NOTE\n\n        # learning algorithm # TODO: could also linearly anneal learning rate\n        self.optimizer    = self.optim(self.model.parameters(), lr = self.lr)\n        self.optimizer.share_memory()       # NOTE\n        self.lr_adjusted  = mp.Value(\'d\', self.lr) # adjusted lr\n\n        # global shared average model: for 1st order trpo policy update\n        self.avg_model    = self.model_prototype(self.model_params)\n        self.avg_model.load_state_dict(self.model.state_dict())\n        self.avg_model.share_memory()       # NOTE\n        for param in self.avg_model.parameters(): param.requires_grad = False\n\n        # global counters\n        self.frame_step   = mp.Value(\'l\', 0) # global frame step counter\n        self.train_step   = mp.Value(\'l\', 0) # global train step counter\n        self.on_policy_train_step  = mp.Value(\'l\', 0) # global on-policy  train step counter\n        self.off_policy_train_step = mp.Value(\'l\', 0) # global off-policy train step counter\n        # global training stats\n        self.p_loss_avg       = mp.Value(\'d\', 0.) # global policy loss\n        self.v_loss_avg       = mp.Value(\'d\', 0.) # global value loss\n        self.entropy_loss_avg = mp.Value(\'d\', 0.) # global value loss\n        self.loss_counter     = mp.Value(\'l\', 0)  # storing this many losses\n        self._reset_training_loggings()\n\n    def _reset_training_loggings(self):\n        self.p_loss_avg.value       = 0.\n        self.v_loss_avg.value       = 0.\n        self.entropy_loss_avg.value = 0.\n        self.loss_counter.value     = 0\n\n    def fit_model(self):\n        self.jobs = []\n        for process_id in range(self.num_processes):\n            self.jobs.append(ACERLearner(self, process_id))\n        self.jobs.append(ACEREvaluator(self, self.num_processes))\n\n        self.logger.warning(""<===================================> Training ..."")\n        for job in self.jobs:\n            job.start()\n        for job in self.jobs:\n            job.join()\n\n    def test_model(self):\n        self.jobs = []\n        self.jobs.append(ACERTester(self))\n\n        self.logger.warning(""<===================================> Testing ..."")\n        for job in self.jobs:\n            job.start()\n        for job in self.jobs:\n            job.join()\n'"
core/agents/acer_single_process.py,31,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport random\nimport time\nimport math\nimport torch\nfrom torch.autograd import Variable, grad, backward\nimport torch.nn.functional as F\n\nfrom utils.helpers import ACER_On_Policy_Experience\nfrom utils.distributions import sample_poisson, categorical_kl_div\nfrom optims.helpers import adjust_learning_rate\nfrom core.agent_single_process import AgentSingleProcess\n\nclass ACERSingleProcess(AgentSingleProcess):\n    def __init__(self, master, process_id=0):\n        super(ACERSingleProcess, self).__init__(master, process_id)\n\n        # lstm hidden states\n        if self.master.enable_lstm:\n            self._reset_on_policy_lstm_hidden_vb_episode() # clear up hidden state\n            self._reset_on_policy_lstm_hidden_vb_rollout() # detach the previous variable from the computation graph\n            self._reset_off_policy_lstm_hidden_vb()        # clear up hidden state, since sampled batches won\'t be connected from previous batches\n\n        # # NOTE global variable pi\n        # if self.master.enable_continuous:\n        #     self.pi_vb = Variable(torch.Tensor([math.pi]).type(self.master.dtype))\n\n        self.master.logger.warning(""Registered ACER-SingleProcess-Agent #"" + str(self.process_id) + "" w/ Env (seed:"" + str(self.env.seed) + "")."")\n\n    # NOTE: to be called at the beginning of each new episode, clear up the hidden state\n    def _reset_on_policy_lstm_hidden_vb_episode(self, training=True): # seq_len, batch_size, hidden_dim\n        not_training = not training\n        if self.master.enable_continuous:\n            # self.on_policy_lstm_hidden_vb = (Variable(torch.zeros(2, self.master.hidden_dim).type(self.master.dtype), volatile=not_training),\n            #                                  Variable(torch.zeros(2, self.master.hidden_dim).type(self.master.dtype), volatile=not_training))\n            pass\n        else:\n            # for self.model\n            self.on_policy_lstm_hidden_vb = (Variable(torch.zeros(1, self.master.hidden_dim).type(self.master.dtype), volatile=not_training),\n                                             Variable(torch.zeros(1, self.master.hidden_dim).type(self.master.dtype), volatile=not_training))\n            # for self.master.avg_model # NOTE: no grads are needed to compute on this model, so always volatile\n            self.on_policy_avg_lstm_hidden_vb = (Variable(torch.zeros(1, self.master.hidden_dim).type(self.master.dtype), volatile=True),\n                                                 Variable(torch.zeros(1, self.master.hidden_dim).type(self.master.dtype), volatile=True))\n\n    # NOTE: to be called at the beginning of each rollout, detach the previous variable from the graph\n    def _reset_on_policy_lstm_hidden_vb_rollout(self):\n        # for self.model\n        self.on_policy_lstm_hidden_vb = (Variable(self.on_policy_lstm_hidden_vb[0].data),\n                                         Variable(self.on_policy_lstm_hidden_vb[1].data))\n        # for self.master.avg_model\n        self.on_policy_avg_lstm_hidden_vb = (Variable(self.on_policy_avg_lstm_hidden_vb[0].data),\n                                             Variable(self.on_policy_avg_lstm_hidden_vb[1].data))\n\n    # NOTE: to be called before each off-policy learning phase\n    # NOTE: keeping it separate so as not to mess up the on_policy_lstm_hidden_vb if the current on-policy episode has not finished after the last rollout\n    def _reset_off_policy_lstm_hidden_vb(self, training=True):\n        not_training = not training\n        if self.master.enable_continuous:\n            pass\n        else:\n            # for self.model\n            self.off_policy_lstm_hidden_vb = (Variable(torch.zeros(self.master.batch_size, self.master.hidden_dim).type(self.master.dtype), volatile=not_training),\n                                              Variable(torch.zeros(self.master.batch_size, self.master.hidden_dim).type(self.master.dtype), volatile=not_training))\n            # for self.master.avg_model # NOTE: no grads are needed to be computed on this model\n            self.off_policy_avg_lstm_hidden_vb = (Variable(torch.zeros(self.master.batch_size, self.master.hidden_dim).type(self.master.dtype)),\n                                                  Variable(torch.zeros(self.master.batch_size, self.master.hidden_dim).type(self.master.dtype)))\n\n    def _preprocessState(self, state, on_policy, is_valotile=False):\n        if isinstance(state, list):\n            state_vb = []\n            for i in range(len(state)):\n                if on_policy:\n                    state_vb.append(Variable(torch.from_numpy(state[i]).unsqueeze(0).type(self.master.dtype), volatile=is_valotile))\n                else:\n                    state_vb.append(Variable(torch.from_numpy(state[i]).view(-1, self.master.state_shape).type(self.master.dtype), volatile=is_valotile))\n        else:\n            if on_policy:\n                state_vb = Variable(torch.from_numpy(state).unsqueeze(0).type(self.master.dtype), volatile=is_valotile)\n            else:\n                state_vb = Variable(torch.from_numpy(state).view(-1, self.master.state_shape).type(self.master.dtype), volatile=is_valotile)\n        return state_vb\n\n    def _forward(self, state_vb, on_policy=True):\n        if self.master.enable_continuous:\n            pass\n        else:\n            if self.master.enable_lstm:\n                if on_policy:   # learn from the current experience\n                    p_vb, q_vb, v_vb, self.on_policy_lstm_hidden_vb    = self.model(state_vb, self.on_policy_lstm_hidden_vb)\n                    avg_p_vb, _, _, self.on_policy_avg_lstm_hidden_vb  = self.master.avg_model(state_vb, self.on_policy_avg_lstm_hidden_vb)\n                    # then we also need to get an action for the next time step\n                    if self.training:\n                        action = p_vb.multinomial().data[0][0]\n                    else:\n                        action = p_vb.max(1)[1].data.squeeze().numpy()[0]\n                    return action, p_vb, q_vb, v_vb, avg_p_vb\n                else:           # learn from the sampled replays\n                    p_vb, q_vb, v_vb, self.off_policy_lstm_hidden_vb   = self.model(state_vb, self.off_policy_lstm_hidden_vb)\n                    avg_p_vb, _, _, self.off_policy_avg_lstm_hidden_vb = self.master.avg_model(state_vb, self.off_policy_avg_lstm_hidden_vb)\n                    return _, p_vb, q_vb, v_vb, avg_p_vb\n            else:\n                pass\n\nclass ACERLearner(ACERSingleProcess):\n    def __init__(self, master, process_id=0):\n        master.logger.warning(""<===================================> ACER-Learner #"" + str(process_id) + "" {Env & Model & Memory}"")\n        super(ACERLearner, self).__init__(master, process_id)\n\n        # NOTE: diff from pure on-policy methods like a3c, acer is capable of\n        # NOTE: off-policy learning and can make use of replay buffer\n        self.memory = self.master.memory_prototype(capacity = self.master.memory_params.memory_size // self.master.num_processes,\n                                                   max_episode_length = self.master.early_stop)\n\n        self._reset_rollout()\n\n        self.training = True    # choose actions by polinomial\n        self.model.train(self.training)\n        # local counters\n        self.frame_step   = 0   # local frame step counter\n        self.train_step   = 0   # local train step counter\n        self.on_policy_train_step   = 0   # local on-policy  train step counter\n        self.off_policy_train_step  = 0   # local off-policy train step counter\n        # local training stats\n        self.p_loss_avg       = 0.  # global policy loss\n        self.v_loss_avg       = 0.  # global value loss\n        self.entropy_loss_avg = 0.  # global entropy loss\n        self.loss_counter     = 0   # storing this many losses\n        self._reset_training_loggings()\n\n        # copy local training stats to global every prog_freq\n        self.last_prog = time.time()\n\n    def _reset_training_loggings(self):\n        self.p_loss_avg       = 0.\n        self.v_loss_avg       = 0.\n        self.entropy_loss_avg = 0.\n        self.loss_counter     = 0\n\n    def _reset_rollout(self):       # for storing the experiences collected through one rollout\n        self.rollout = ACER_On_Policy_Experience(state0 = [],\n                                                 action = [],\n                                                 reward = [],\n                                                 state1 = [],\n                                                 terminal1 = [],\n                                                 policy_vb = [],\n                                                 q0_vb = [],\n                                                 value0_vb = [],\n                                                 detached_avg_policy_vb = [],\n                                                 detached_old_policy_vb = [])\n\n    def _get_QretT_vb(self, on_policy=True):\n        if on_policy:\n            if self.rollout.terminal1[-1]:              # for terminal sT: Q_ret = 0\n                QretT_vb = Variable(torch.zeros(1, 1))\n            else:                                       # for non-terminal sT: Qret = V(s_i; /theta)\n                sT_vb = self._preprocessState(self.rollout.state1[-1], on_policy, True) # bootstrap from last state\n                if self.master.enable_lstm:\n                    _, _, QretT_vb, _ = self.model(sT_vb, self.on_policy_lstm_hidden_vb)# NOTE: only doing inference here\n                else:\n                    _, _, QretT_vb = self.model(sT_vb)                                  # NOTE: only doing inference here\n                # # NOTE: here QretT_vb.volatile=True since sT_vb.volatile=True\n                # # NOTE: if we use detach() here, it would remain volatile\n                # # NOTE: then all the follow-up computations would only give volatile loss variables\n                # QretT_vb = Variable(QretT_vb.data)\n        else:\n            sT_vb = self._preprocessState(self.rollout.state1[-1], on_policy, True)     # bootstrap from last state\n            if self.master.enable_lstm:\n                _, _, QretT_vb, _ = self.model(sT_vb, self.off_policy_lstm_hidden_vb)   # NOTE: only doing inference here\n            else:\n                _, _, QretT_vb = self.model(sT_vb)                                      # NOTE: only doing inference here\n            # now we have to also set QretT_vb to 0 for terminal sT\'s\n            QretT_vb = ((1 - Variable(torch.from_numpy(np.array(self.rollout.terminal1[-1])).float())) * QretT_vb)\n\n        # NOTE: here QretT_vb.volatile=True since sT_vb.volatile=True\n        # NOTE: if we use detach() here, it would remain volatile\n        # NOTE: then all the follow-up computations would only give volatile loss variables\n        return Variable(QretT_vb.data)\n\n    def _1st_order_trpo(self, detached_policy_loss_vb, detached_policy_vb, detached_avg_policy_vb, detached_splitted_policy_vb=None):\n        on_policy = detached_splitted_policy_vb is None\n        # KL divergence k = \\delta_{\\phi_{\\theta}} DKL[ \\pi(|\\phi_{\\theta_a}) || \\pi{|\\phi_{\\theta}}]\n        # kl_div_vb = F.kl_div(detached_policy_vb.log(), detached_avg_policy_vb, size_average=False) # NOTE: the built-in one does not work on batch\n        kl_div_vb = categorical_kl_div(detached_policy_vb, detached_avg_policy_vb)\n        # NOTE: k & g are wll w.r.t. the network output, which is detached_policy_vb\n        # NOTE: gradient from this part will not flow back into the model\n        # NOTE: that\'s why we are only using detached policy variables here\n        if on_policy:\n            k_vb = grad(outputs=kl_div_vb,               inputs=detached_policy_vb, retain_graph=False, only_inputs=True)[0]\n            g_vb = grad(outputs=detached_policy_loss_vb, inputs=detached_policy_vb, retain_graph=False, only_inputs=True)[0]\n        else:\n            # NOTE NOTE NOTE !!!\n            # NOTE: here is why we cannot simply detach then split the policy_vb, but must split before detach\n            # NOTE: cos if we do that then the split cannot backtrace the grads computed in this later part of the graph\n            # NOTE: it would have no way to connect to the graphs in the model\n            k_vb = grad(outputs=(kl_div_vb.split(1, 0)),               inputs=(detached_splitted_policy_vb), retain_graph=False, only_inputs=True)\n            g_vb = grad(outputs=(detached_policy_loss_vb.split(1, 0)), inputs=(detached_splitted_policy_vb), retain_graph=False, only_inputs=True)\n            k_vb = torch.cat(k_vb, 0)\n            g_vb = torch.cat(g_vb, 0)\n\n        kg_dot_vb = (k_vb * g_vb).sum(1, keepdim=True)\n        kk_dot_vb = (k_vb * k_vb).sum(1, keepdim=True)\n        z_star_vb = g_vb - ((kg_dot_vb - self.master.clip_1st_order_trpo) / kk_dot_vb).clamp(min=0) * k_vb\n\n        return z_star_vb\n\n    def _update_global_avg_model(self):\n        for global_param, global_avg_param in zip(self.master.model.parameters(),\n                                                  self.master.avg_model.parameters()):\n            global_avg_param = self.master.avg_model_decay       * global_avg_param + \\\n                               (1 - self.master.avg_model_decay) * global_param\n\n    def _backward(self, unsplitted_policy_vb=None):\n        on_policy = unsplitted_policy_vb is None\n        # preparation\n        rollout_steps = len(self.rollout.reward)\n        if self.master.enable_continuous:\n            pass\n        else:\n            action_batch_vb = Variable(torch.from_numpy(np.array(self.rollout.action)).view(rollout_steps, -1, 1).long())       # [rollout_steps x batch_size x 1]\n            if self.master.use_cuda:\n                action_batch_vb = action_batch_vb.cuda()\n            if not on_policy:   # we save this transformation for on-policy\n                reward_batch_vb = Variable(torch.from_numpy(np.array(self.rollout.reward)).view(rollout_steps, -1, 1).float())  # [rollout_steps x batch_size x 1]\n            # NOTE: here we use the detached policies, cos when using 1st order trpo,\n            # NOTE: the policy losses are not directly backproped into the model\n            # NOTE: but only backproped up to the output of the network\n            # NOTE: and to make the code consistent, we also decouple the backprop\n            # NOTE: into two parts when not using trpo policy update\n            # NOTE: requires_grad of detached_policy_vb must be True, otherwise grad will not be able to\n            # NOTE: flow between the two stagets of backprop\n            if on_policy:\n                policy_vb                   = self.rollout.policy_vb\n                detached_splitted_policy_vb = None\n                detached_policy_vb          = [Variable(self.rollout.policy_vb[i].data, requires_grad=True) for i in range(rollout_steps)] # [rollout_steps x batch_size x action_dim]\n            else: # NOTE: here rollout.policy_vb is already split by trajectories, we can safely detach and not causing trouble for feed in tuples into grad later\n                # NOTE:           rollout.policy_vb: undetached, splitted -> what we stored during the fake _off_policy_rollout\n                # NOTE:                   policy_vb: undetached, batch    -> 1. entropy, cos grad from entropy need to flow back through the whole graph 2. the backward of 2nd stage should be computed on this\n                # NOTE: detached_splitted_policy_vb:   detached, splitted -> used as inputs in grad in _1st_order_trpo, cos this part of grad is not backproped into the model\n                # NOTE:          detached_policy_vb:   detached, batch    -> to ease batch computation on the detached_policy_vb\n                policy_vb                   = unsplitted_policy_vb\n                detached_splitted_policy_vb = [[Variable(self.rollout.policy_vb[i][j].data, requires_grad=True) for j in range(self.master.batch_size)] for i in range(rollout_steps)] # (rollout_steps x (batch_size x [1 x action_dim]))\n                detached_policy_vb          = [torch.cat(detached_splitted_policy_vb[i]) for i in range(rollout_steps)] # detached   # we cat the splitted tuples for each timestep across trajectories to ease batch computation\n            detached_policy_log_vb = [torch.log(detached_policy_vb[i]) for i in range(rollout_steps)]\n            detached_policy_log_vb = [detached_policy_log_vb[i].gather(1, action_batch_vb[i]) for i in range(rollout_steps) ]\n            # NOTE: entropy is using the undetached policies here, cos we\n            # NOTE: backprop entropy_loss the same way as value_loss at once in the end\n            # NOTE: not decoupled into two stages as the other parts of the policy gradient\n            entropy_vb = [- (policy_vb[i].log() * policy_vb[i]).sum(1, keepdim=True).mean(0) for i in range(rollout_steps)]\n            if self.master.enable_1st_order_trpo:\n                z_star_vb = []\n            else:\n                policy_grad_vb = []\n        QretT_vb = self._get_QretT_vb(on_policy)\n\n        # compute loss\n        entropy_loss_vb = 0.\n        value_loss_vb   = 0.\n        for i in reversed(range(rollout_steps)):\n            # 1. policy loss\n            if on_policy:\n                # importance sampling weights: always 1 for on-policy\n                rho_vb = Variable(torch.ones(1, self.master.action_dim))\n                # Q_ret = r_i + /gamma * Q_ret\n                QretT_vb = self.master.gamma * QretT_vb + self.rollout.reward[i]\n            else:\n                # importance sampling weights: /rho = /pi(|s_i) / /mu(|s_i)\n                rho_vb = detached_policy_vb[i].detach() / self.rollout.detached_old_policy_vb[i] # TODO: check if this detach is necessary\n                # Q_ret = r_i + /gamma * Q_ret\n                QretT_vb = self.master.gamma * QretT_vb + reward_batch_vb[i]\n\n            # A = Q_ret - V(s_i; /theta)\n            advantage_vb = QretT_vb - self.rollout.value0_vb[i]\n            # g = min(c, /rho_a_i) * /delta_theta * log(/pi(a_i|s_i; /theta)) * A\n            detached_policy_loss_vb = - (rho_vb.gather(1, action_batch_vb[i]).clamp(max=self.master.clip_trace) * detached_policy_log_vb[i] * advantage_vb.detach()).mean(0)\n\n            if self.master.enable_bias_correction:# and not on_policy:   # NOTE: have to perform bais correction when off-policy\n                # g = g + /sum_a [1 - c / /rho_a]_+ /pi(a|s_i; /theta) * /delta_theta * log(/pi(a|s_i; /theta)) * (Q(s_i, a; /theta) - V(s_i; /theta)\n                bias_correction_coefficient_vb = (1 - self.master.clip_trace / rho_vb).clamp(min=0) * detached_policy_vb[i]\n                detached_policy_loss_vb -= (bias_correction_coefficient_vb * detached_policy_vb[i].log() * (self.rollout.q0_vb[i].detach() - self.rollout.value0_vb[i].detach())).sum(1, keepdim=True).mean(0)\n\n            # 1.1 backprop policy loss up to the network output\n            if self.master.enable_1st_order_trpo:\n                if on_policy:\n                    z_star_vb.append(self._1st_order_trpo(detached_policy_loss_vb, detached_policy_vb[i], self.rollout.detached_avg_policy_vb[i]))\n                else:\n                    z_star_vb.append(self._1st_order_trpo(detached_policy_loss_vb, detached_policy_vb[i], self.rollout.detached_avg_policy_vb[i], detached_splitted_policy_vb[i]))\n            else:\n                policy_grad_vb.append(grad(outputs=detached_policy_loss_vb, inputs=detached_policy_vb[i], retain_graph=False, only_inputs=True)[0])\n\n            # entropy loss\n            entropy_loss_vb -= entropy_vb[i]\n\n            # 2. value loss\n            Q_vb = self.rollout.q0_vb[i].gather(1, action_batch_vb[i])\n            value_loss_vb += ((QretT_vb - Q_vb) ** 2 / 2).mean(0)\n            # we also need to update QretT_vb here\n            truncated_rho_vb = rho_vb.gather(1, action_batch_vb[i]).clamp(max=1)\n            QretT_vb = truncated_rho_vb * (QretT_vb - Q_vb.detach()) + self.rollout.value0_vb[i].detach()\n\n        # now we have all the losses ready, we backprop\n        self.model.zero_grad()\n        # 1.2 backprop the policy loss from the network output to the whole model\n        if self.master.enable_1st_order_trpo:\n            # NOTE: here need to use the undetached policy_vb, cos we need to backprop to the whole model\n            backward(variables=policy_vb, grad_variables=z_star_vb, retain_graph=True)\n        else:\n            # NOTE: here we can backprop both losses at once, but to make consistent\n            # NOTE: and avoid the need to keep track of another set of undetached policy loss\n            # NOTE: we also decouple the backprop of the policy loss into two stages\n            backward(variables=policy_vb, grad_variables=policy_grad_vb, retain_graph=True)\n        # 2. backprop the value loss and entropy loss\n        (value_loss_vb + self.master.beta * entropy_loss_vb).backward()\n        torch.nn.utils.clip_grad_norm(self.model.parameters(), self.master.clip_grad)\n\n        self._ensure_global_grads()\n        self.master.optimizer.step()\n        self.train_step += 1\n        self.master.train_step.value += 1\n\n        # update master.avg_model\n        self._update_global_avg_model()\n\n        # adjust learning rate if enabled\n        if self.master.lr_decay:\n            self.master.lr_adjusted.value = max(self.master.lr * (self.master.steps - self.master.train_step.value) / self.master.steps, 1e-32)\n            adjust_learning_rate(self.master.optimizer, self.master.lr_adjusted.value)\n\n        # log training stats\n        if self.master.enable_1st_order_trpo:\n            self.p_loss_avg   += torch.cat(z_star_vb, 0).data.mean()\n        else:\n            self.p_loss_avg   += torch.cat(policy_grad_vb, 0).data.mean()\n        self.v_loss_avg       += value_loss_vb.data.numpy()\n        self.entropy_loss_avg += entropy_loss_vb.data.numpy()\n        self.loss_counter += 1\n\n    # NOTE: get action from current model, execute in env\n    # NOTE: then get ACER_On_Policy_Experience to calculate stats for backward\n    # NOTE: push them into replay buffer in the format of {s,a,r,s1,t1,p}\n    def _on_policy_rollout(self, episode_steps, episode_reward):\n        # reset rollout experiences\n        self._reset_rollout()\n\n        t_start = self.frame_step\n        # continue to rollout only if:\n        # 1. not running out of max steps of this current rollout, and\n        # 2. not terminal, and\n        # 3. not exceeding max steps of this current episode\n        # 4. master not exceeding max train steps\n        while (self.frame_step - t_start) < self.master.rollout_steps \\\n              and not self.experience.terminal1 \\\n              and (self.master.early_stop is None or episode_steps < self.master.early_stop):\n            # NOTE: here first store the last frame: experience.state1 as rollout.state0\n            self.rollout.state0.append(self.experience.state1)\n            # then get the action to take from rollout.state0 (experience.state1)\n            if self.master.enable_continuous:\n                pass\n            else:\n                action, p_vb, q_vb, v_vb, avg_p_vb = self._forward(self._preprocessState(self.experience.state1, on_policy=True), on_policy=True)\n            # then execute action in env to get a new experience.state1 -> rollout.state1\n            self.experience = self.env.step(action)\n            # push experience into rollout\n            self.rollout.action.append(action)\n            self.rollout.reward.append(self.experience.reward)\n            self.rollout.state1.append(self.experience.state1)\n            self.rollout.terminal1.append(self.experience.terminal1)\n            self.rollout.policy_vb.append(p_vb)\n            self.rollout.q0_vb.append(q_vb)\n            self.rollout.value0_vb.append(v_vb)\n            self.rollout.detached_avg_policy_vb.append(avg_p_vb.detach()) # NOTE\n            # also push into replay buffer if off-policy learning is enabled\n            if self.master.replay_ratio > 0:\n                if self.rollout.terminal1[-1]:\n                    self.memory.append(self.rollout.state0[-1],\n                                       None,\n                                       None,\n                                       None)\n                else:\n                    self.memory.append(self.rollout.state0[-1],\n                                       self.rollout.action[-1],\n                                       self.rollout.reward[-1],\n                                       self.rollout.policy_vb[-1].detach()) # NOTE: no graphs needed\n\n            episode_steps += 1\n            episode_reward += self.experience.reward\n            self.frame_step += 1\n            self.master.frame_step.value += 1\n\n            # NOTE: we put this condition in the end to make sure this current rollout won\'t be empty\n            if self.master.train_step.value >= self.master.steps:\n                break\n\n        return episode_steps, episode_reward\n\n    # NOTE: sample from replay buffer for a bunch of trajectories\n    # NOTE: then fake rollout on them to get ACER_On_Policy_Experience to get stats for backward\n    def _off_policy_rollout(self):\n        # reset rollout experiences\n        self._reset_rollout()\n\n        # first sample trajectories\n        trajectories = self.memory.sample_batch(self.master.batch_size, maxlen=self.master.rollout_steps)\n        # NOTE: we also store another set of undetached unsplitted policy_vb here to prepare for backward\n        unsplitted_policy_vb = []\n\n        # then fake the on-policy forward\n        for t in range(len(trajectories) - 1):\n            # we first get the data out of the sampled experience\n            state0 = np.stack((trajectory.state0 for trajectory in trajectories[t]))\n            action = np.expand_dims(np.stack((trajectory.action for trajectory in trajectories[t])), axis=1)\n            reward = np.expand_dims(np.stack((trajectory.reward for trajectory in trajectories[t])), axis=1)\n            state1 = np.stack((trajectory.state0 for trajectory in trajectories[t+1]))\n            terminal1 = np.expand_dims(np.stack((1 if trajectory.action is None else 0 for trajectory in trajectories[t+1])), axis=1) # NOTE: here is 0/1, in on-policy is False/True\n            detached_old_policy_vb = torch.cat([trajectory.detached_old_policy_vb for trajectory in trajectories[t]], 0)\n\n            # NOTE: here first store the last frame: experience.state1 as rollout.state0\n            self.rollout.state0.append(state0)\n            # then get its corresponding output variables to fake the on policy experience\n            if self.master.enable_continuous:\n                pass\n            else:\n                _, p_vb, q_vb, v_vb, avg_p_vb = self._forward(self._preprocessState(self.rollout.state0[-1], on_policy=False), on_policy=False)\n            # push experience into rollout\n            self.rollout.action.append(action)\n            self.rollout.reward.append(reward)\n            self.rollout.state1.append(state1)\n            self.rollout.terminal1.append(terminal1)\n            self.rollout.policy_vb.append(p_vb.split(1, 0)) # NOTE: must split before detach !!! otherwise graph is cut\n            self.rollout.q0_vb.append(q_vb)\n            self.rollout.value0_vb.append(v_vb)\n            self.rollout.detached_avg_policy_vb.append(avg_p_vb.detach()) # NOTE\n            self.rollout.detached_old_policy_vb.append(detached_old_policy_vb)\n            unsplitted_policy_vb.append(p_vb)\n\n        # also need to log some training stats here maybe\n\n        return unsplitted_policy_vb\n\n    def run(self):\n        # make sure processes are not completely synced by sleeping a bit\n        time.sleep(int(np.random.rand() * (self.process_id + 5)))\n\n        nepisodes = 0\n        nepisodes_solved = 0\n        episode_steps = None\n        episode_reward = None\n        should_start_new = True\n        while self.master.train_step.value < self.master.steps:\n            # NOTE: on-policy learning  # NOTE: procedure same as a3c, outs differ a bit\n            # sync in every step\n            self._sync_local_with_global()\n            self.model.zero_grad()\n\n            # start of a new episode\n            if should_start_new:\n                episode_steps = 0\n                episode_reward = 0.\n                # reset on_policy_lstm_hidden_vb for new episode\n                if self.master.enable_lstm:\n                    # NOTE: clear hidden state at the beginning of each episode\n                    self._reset_on_policy_lstm_hidden_vb_episode()\n                # Obtain the initial observation by resetting the environment\n                self._reset_experience()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                # reset flag\n                should_start_new = False\n            if self.master.enable_lstm:\n                # NOTE: detach the previous hidden variable from the graph at the beginning of each rollout\n                self._reset_on_policy_lstm_hidden_vb_rollout()\n            # Run a rollout for rollout_steps or until terminal\n            episode_steps, episode_reward = self._on_policy_rollout(episode_steps, episode_reward)\n\n            if self.experience.terminal1 or \\\n               self.master.early_stop and episode_steps >= self.master.early_stop:\n                nepisodes += 1\n                should_start_new = True\n                if self.experience.terminal1:\n                    nepisodes_solved += 1\n\n            # calculate loss\n            self._backward() # NOTE: only train_step will increment inside _backward\n            self.on_policy_train_step += 1\n            self.master.on_policy_train_step.value += 1\n\n            # NOTE: off-policy learning\n            # perfrom some off-policy training once got enough experience\n            if self.master.replay_ratio > 0 and len(self.memory) >= self.master.replay_start:\n                # sample a number of off-policy episodes based on the replay ratio\n                for _ in range(sample_poisson(self.master.replay_ratio)):\n                    # sync in every step\n                    self._sync_local_with_global()  # TODO: don\'t know if this is necessary here\n                    self.model.zero_grad()\n\n                    # reset on_policy_lstm_hidden_vb for new episode\n                    if self.master.enable_lstm:\n                        # NOTE: clear hidden state at the beginning of each episode\n                        self._reset_off_policy_lstm_hidden_vb()\n                    unsplitted_policy_vb = self._off_policy_rollout() # fake rollout, just to collect net outs from sampled trajectories\n                    # calculate loss\n                    self._backward(unsplitted_policy_vb) # NOTE: only train_step will increment inside _backward\n                    self.off_policy_train_step += 1\n                    self.master.off_policy_train_step.value += 1\n\n            # copy local training stats to global at prog_freq, and clear up local stats\n            if time.time() - self.last_prog >= self.master.prog_freq:\n                self.master.p_loss_avg.value       += self.p_loss_avg\n                self.master.v_loss_avg.value       += self.v_loss_avg\n                self.master.entropy_loss_avg.value += self.entropy_loss_avg\n                self.master.loss_counter.value     += self.loss_counter\n                self._reset_training_loggings()\n                self.last_prog = time.time()\n\nclass ACEREvaluator(ACERSingleProcess):\n    def __init__(self, master, process_id=0):\n        master.logger.warning(""<===================================> ACER-Evaluator {Env & Model}"")\n        super(ACEREvaluator, self).__init__(master, process_id)\n\n        self.training = False   # choose actions w/ max probability\n        self.model.train(self.training)\n        self._reset_loggings()\n\n        self.start_time = time.time()\n        self.last_eval = time.time()\n\n    def _reset_loggings(self):\n        # training stats across all processes\n        self.p_loss_avg_log = []\n        self.v_loss_avg_log = []\n        self.entropy_loss_avg_log = []\n        # evaluation stats\n        self.entropy_avg_log = []\n        self.v_avg_log = []\n        self.steps_avg_log = []\n        self.steps_std_log = []\n        self.reward_avg_log = []\n        self.reward_std_log = []\n        self.nepisodes_log = []\n        self.nepisodes_solved_log = []\n        self.repisodes_solved_log = []\n        # placeholders for windows for online curve plotting\n        if self.master.visualize:\n            # training stats across all processes\n            self.win_p_loss_avg = ""win_p_loss_avg""\n            self.win_v_loss_avg = ""win_v_loss_avg""\n            self.win_entropy_loss_avg = ""win_entropy_loss_avg""\n            # evaluation stats\n            self.win_entropy_avg = ""win_entropy_avg""\n            self.win_v_avg = ""win_v_avg""\n            self.win_steps_avg = ""win_steps_avg""\n            self.win_steps_std = ""win_steps_std""\n            self.win_reward_avg = ""win_reward_avg""\n            self.win_reward_std = ""win_reward_std""\n            self.win_nepisodes = ""win_nepisodes""\n            self.win_nepisodes_solved = ""win_nepisodes_solved""\n            self.win_repisodes_solved = ""win_repisodes_solved""\n\n    def _eval_model(self):\n        self.last_eval = time.time()\n        eval_at_train_step = self.master.train_step.value\n        eval_at_frame_step = self.master.frame_step.value\n        eval_at_on_policy_train_step  = self.master.on_policy_train_step.value\n        eval_at_off_policy_train_step = self.master.off_policy_train_step.value\n        # first grab the latest global model to do the evaluation\n        self._sync_local_with_global()\n\n        # evaluate\n        eval_step = 0\n\n        eval_entropy_log = []\n        eval_v_log = []\n        eval_nepisodes = 0\n        eval_nepisodes_solved = 0\n        eval_episode_steps = None\n        eval_episode_steps_log = []\n        eval_episode_reward = None\n        eval_episode_reward_log = []\n        eval_should_start_new = True\n        while eval_step < self.master.eval_steps:\n            if eval_should_start_new:   # start of a new episode\n                eval_episode_steps = 0\n                eval_episode_reward = 0.\n                # reset lstm_hidden_vb for new episode\n                if self.master.enable_lstm:\n                    # NOTE: clear hidden state at the beginning of each episode\n                    self._reset_on_policy_lstm_hidden_vb_episode(self.training)\n                # Obtain the initial observation by resetting the environment\n                self._reset_experience()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                if not self.training:\n                    if self.master.visualize: self.env.visual()\n                    if self.master.render: self.env.render()\n                # reset flag\n                eval_should_start_new = False\n            if self.master.enable_lstm:\n                # NOTE: detach the previous hidden variable from the graph at the beginning of each step\n                # NOTE: not necessary here in evaluation but we do it anyways\n                self._reset_on_policy_lstm_hidden_vb_rollout()\n            # Run a single step\n            if self.master.enable_continuous:\n                pass\n            else:\n                eval_action, p_vb, _, v_vb, _ = self._forward(self._preprocessState(self.experience.state1, True, True), on_policy=True)\n            self.experience = self.env.step(eval_action)\n            if not self.training:\n                if self.master.visualize: self.env.visual()\n                if self.master.render: self.env.render()\n            if self.experience.terminal1 or \\\n               self.master.early_stop and (eval_episode_steps + 1) == self.master.early_stop or \\\n               (eval_step + 1) == self.master.eval_steps:\n                eval_should_start_new = True\n\n            eval_episode_steps += 1\n            eval_episode_reward += self.experience.reward\n            eval_step += 1\n\n            if eval_should_start_new:\n                eval_nepisodes += 1\n                if self.experience.terminal1:\n                    eval_nepisodes_solved += 1\n\n                # This episode is finished, report and reset\n                # NOTE make no sense for continuous\n                if self.master.enable_continuous:\n                    eval_entropy_log.append([0.5 * ((sig_vb * 2 * self.pi_vb.expand_as(sig_vb)).log() + 1).data.numpy()])\n                else:\n                    eval_entropy_log.append([np.mean((-torch.log(p_vb.data.squeeze()) * p_vb.data.squeeze()).numpy())])\n                eval_v_log.append([v_vb.data.numpy()])\n                eval_episode_steps_log.append([eval_episode_steps])\n                eval_episode_reward_log.append([eval_episode_reward])\n                self._reset_experience()\n                eval_episode_steps = None\n                eval_episode_reward = None\n\n        # Logging for this evaluation phase\n        loss_counter = self.master.loss_counter.value\n        p_loss_avg = self.master.p_loss_avg.value / loss_counter if loss_counter > 0 else 0.\n        v_loss_avg = self.master.v_loss_avg.value / loss_counter if loss_counter > 0 else 0.\n        entropy_loss_avg = self.master.entropy_loss_avg.value / loss_counter if loss_counter > 0 else 0.\n        self.master._reset_training_loggings()\n        def _log_at_step(eval_at_step):\n            self.p_loss_avg_log.append([eval_at_step, p_loss_avg])\n            self.v_loss_avg_log.append([eval_at_step, v_loss_avg])\n            self.entropy_loss_avg_log.append([eval_at_step, entropy_loss_avg])\n            self.entropy_avg_log.append([eval_at_step, np.mean(np.asarray(eval_entropy_log))])\n            self.v_avg_log.append([eval_at_step, np.mean(np.asarray(eval_v_log))])\n            self.steps_avg_log.append([eval_at_step, np.mean(np.asarray(eval_episode_steps_log))])\n            self.steps_std_log.append([eval_at_step, np.std(np.asarray(eval_episode_steps_log))])\n            self.reward_avg_log.append([eval_at_step, np.mean(np.asarray(eval_episode_reward_log))])\n            self.reward_std_log.append([eval_at_step, np.std(np.asarray(eval_episode_reward_log))])\n            self.nepisodes_log.append([eval_at_step, eval_nepisodes])\n            self.nepisodes_solved_log.append([eval_at_step, eval_nepisodes_solved])\n            self.repisodes_solved_log.append([eval_at_step, (eval_nepisodes_solved/eval_nepisodes) if eval_nepisodes > 0 else 0.])\n            # logging\n            self.master.logger.warning(""Reporting       @ Step: "" + str(eval_at_step) + "" | Elapsed Time: "" + str(time.time() - self.start_time))\n            self.master.logger.warning(""Iteration: {}; lr: {}"".format(eval_at_step, self.master.lr_adjusted.value))\n            self.master.logger.warning(""Iteration: {}; on_policy_steps: {}"".format(eval_at_step, eval_at_on_policy_train_step))\n            self.master.logger.warning(""Iteration: {}; off_policy_steps: {}"".format(eval_at_step, eval_at_off_policy_train_step))\n            self.master.logger.warning(""Iteration: {}; p_loss_avg: {}"".format(eval_at_step, self.p_loss_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; v_loss_avg: {}"".format(eval_at_step, self.v_loss_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; entropy_loss_avg: {}"".format(eval_at_step, self.entropy_loss_avg_log[-1][1]))\n            self.master._reset_training_loggings()\n            self.master.logger.warning(""Evaluating      @ Step: "" + str(eval_at_train_step) + "" | ("" + str(eval_at_frame_step) + "" frames)..."")\n            self.master.logger.warning(""Evaluation        Took: "" + str(time.time() - self.last_eval))\n            self.master.logger.warning(""Iteration: {}; entropy_avg: {}"".format(eval_at_step, self.entropy_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; v_avg: {}"".format(eval_at_step, self.v_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; steps_avg: {}"".format(eval_at_step, self.steps_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; steps_std: {}"".format(eval_at_step, self.steps_std_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; reward_avg: {}"".format(eval_at_step, self.reward_avg_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; reward_std: {}"".format(eval_at_step, self.reward_std_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; nepisodes: {}"".format(eval_at_step, self.nepisodes_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; nepisodes_solved: {}"".format(eval_at_step, self.nepisodes_solved_log[-1][1]))\n            self.master.logger.warning(""Iteration: {}; repisodes_solved: {}"".format(eval_at_step, self.repisodes_solved_log[-1][1]))\n        if self.master.enable_log_at_train_step:\n            _log_at_step(eval_at_train_step)\n        else:\n            _log_at_step(eval_at_frame_step)\n\n        # plotting\n        if self.master.visualize:\n            self.win_p_loss_avg = self.master.vis.scatter(X=np.array(self.p_loss_avg_log), env=self.master.refs, win=self.win_p_loss_avg, opts=dict(title=""p_loss_avg""))\n            self.win_v_loss_avg = self.master.vis.scatter(X=np.array(self.v_loss_avg_log), env=self.master.refs, win=self.win_v_loss_avg, opts=dict(title=""v_loss_avg""))\n            self.win_entropy_loss_avg = self.master.vis.scatter(X=np.array(self.entropy_loss_avg_log), env=self.master.refs, win=self.win_entropy_loss_avg, opts=dict(title=""entropy_loss_avg""))\n            self.win_entropy_avg = self.master.vis.scatter(X=np.array(self.entropy_avg_log), env=self.master.refs, win=self.win_entropy_avg, opts=dict(title=""entropy_avg""))\n            self.win_v_avg = self.master.vis.scatter(X=np.array(self.v_avg_log), env=self.master.refs, win=self.win_v_avg, opts=dict(title=""v_avg""))\n            self.win_steps_avg = self.master.vis.scatter(X=np.array(self.steps_avg_log), env=self.master.refs, win=self.win_steps_avg, opts=dict(title=""steps_avg""))\n            # self.win_steps_std = self.master.vis.scatter(X=np.array(self.steps_std_log), env=self.master.refs, win=self.win_steps_std, opts=dict(title=""steps_std""))\n            self.win_reward_avg = self.master.vis.scatter(X=np.array(self.reward_avg_log), env=self.master.refs, win=self.win_reward_avg, opts=dict(title=""reward_avg""))\n            # self.win_reward_std = self.master.vis.scatter(X=np.array(self.reward_std_log), env=self.master.refs, win=self.win_reward_std, opts=dict(title=""reward_std""))\n            self.win_nepisodes = self.master.vis.scatter(X=np.array(self.nepisodes_log), env=self.master.refs, win=self.win_nepisodes, opts=dict(title=""nepisodes""))\n            self.win_nepisodes_solved = self.master.vis.scatter(X=np.array(self.nepisodes_solved_log), env=self.master.refs, win=self.win_nepisodes_solved, opts=dict(title=""nepisodes_solved""))\n            self.win_repisodes_solved = self.master.vis.scatter(X=np.array(self.repisodes_solved_log), env=self.master.refs, win=self.win_repisodes_solved, opts=dict(title=""repisodes_solved""))\n        self.last_eval = time.time()\n\n        # save model\n        self.master._save_model(eval_at_train_step, self.reward_avg_log[-1][1])\n\n    def run(self):\n        while self.master.train_step.value < self.master.steps:\n            if time.time() - self.last_eval > self.master.eval_freq:\n                self._eval_model()\n        # we also do a final evaluation after training is done\n        self._eval_model()\n\nclass ACERTester(ACERSingleProcess):\n    def __init__(self, master, process_id=0):\n        master.logger.warning(""<===================================> ACER-Tester {Env & Model}"")\n        super(ACERTester, self).__init__(master, process_id)\n\n        self.training = False   # choose actions w/ max probability\n        self.model.train(self.training)\n        self._reset_loggings()\n\n        self.start_time = time.time()\n\n    def _reset_loggings(self):\n        # testing stats\n        self.steps_avg_log = []\n        self.steps_std_log = []\n        self.reward_avg_log = []\n        self.reward_std_log = []\n        self.nepisodes_log = []\n        self.nepisodes_solved_log = []\n        self.repisodes_solved_log = []\n        # placeholders for windows for online curve plotting\n        if self.master.visualize:\n            # evaluation stats\n            self.win_steps_avg = ""win_steps_avg""\n            self.win_steps_std = ""win_steps_std""\n            self.win_reward_avg = ""win_reward_avg""\n            self.win_reward_std = ""win_reward_std""\n            self.win_nepisodes = ""win_nepisodes""\n            self.win_nepisodes_solved = ""win_nepisodes_solved""\n            self.win_repisodes_solved = ""win_repisodes_solved""\n\n    def run(self):\n        test_step = 0\n        test_nepisodes = 0\n        test_nepisodes_solved = 0\n        test_episode_steps = None\n        test_episode_steps_log = []\n        test_episode_reward = None\n        test_episode_reward_log = []\n        test_should_start_new = True\n        while test_nepisodes < self.master.test_nepisodes:\n            if test_should_start_new:   # start of a new episode\n                test_episode_steps = 0\n                test_episode_reward = 0.\n                # reset lstm_hidden_vb for new episode\n                if self.master.enable_lstm:\n                    # NOTE: clear hidden state at the beginning of each episode\n                    self._reset_on_policy_lstm_hidden_vb_episode(self.training)\n                # Obtain the initial observation by resetting the environment\n                self._reset_experience()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                if not self.training:\n                    if self.master.visualize: self.env.visual()\n                    if self.master.render: self.env.render()\n                # reset flag\n                test_should_start_new = False\n            if self.master.enable_lstm:\n                # NOTE: detach the previous hidden variable from the graph at the beginning of each step\n                # NOTE: not necessary here in testing but we do it anyways\n                self._reset_on_policy_lstm_hidden_vb_rollout()\n            # Run a single step\n            if self.master.enable_continuous:\n                pass\n            else:\n                test_action, p_vb, _, v_vb, _ = self._forward(self._preprocessState(self.experience.state1, True, True), on_policy=True)\n            self.experience = self.env.step(test_action)\n            if not self.training:\n                if self.master.visualize: self.env.visual()\n                if self.master.render: self.env.render()\n            if self.experience.terminal1 or \\\n               self.master.early_stop and (test_episode_steps + 1) == self.master.early_stop:\n                test_should_start_new = True\n\n            test_episode_steps += 1\n            test_episode_reward += self.experience.reward\n            test_step += 1\n\n            if test_should_start_new:\n                test_nepisodes += 1\n                if self.experience.terminal1:\n                    test_nepisodes_solved += 1\n\n                # This episode is finished, report and reset\n                test_episode_steps_log.append([test_episode_steps])\n                test_episode_reward_log.append([test_episode_reward])\n                self._reset_experience()\n                test_episode_steps = None\n                test_episode_reward = None\n\n        self.steps_avg_log.append([test_nepisodes, np.mean(np.asarray(test_episode_steps_log))])\n        self.steps_std_log.append([test_nepisodes, np.std(np.asarray(test_episode_steps_log))]); del test_episode_steps_log\n        self.reward_avg_log.append([test_nepisodes, np.mean(np.asarray(test_episode_reward_log))])\n        self.reward_std_log.append([test_nepisodes, np.std(np.asarray(test_episode_reward_log))]); del test_episode_reward_log\n        self.nepisodes_log.append([test_nepisodes, test_nepisodes])\n        self.nepisodes_solved_log.append([test_nepisodes, test_nepisodes_solved])\n        self.repisodes_solved_log.append([test_nepisodes, (test_nepisodes_solved/test_nepisodes) if test_nepisodes > 0 else 0.])\n        # plotting\n        if self.master.visualize:\n            self.win_steps_avg = self.master.vis.scatter(X=np.array(self.steps_avg_log), env=self.master.refs, win=self.win_steps_avg, opts=dict(title=""steps_avg""))\n            # self.win_steps_std = self.master.vis.scatter(X=np.array(self.steps_std_log), env=self.master.refs, win=self.win_steps_std, opts=dict(title=""steps_std""))\n            self.win_reward_avg = self.master.vis.scatter(X=np.array(self.reward_avg_log), env=self.master.refs, win=self.win_reward_avg, opts=dict(title=""reward_avg""))\n            # self.win_reward_std = self.master.vis.scatter(X=np.array(self.reward_std_log), env=self.master.refs, win=self.win_reward_std, opts=dict(title=""reward_std""))\n            self.win_nepisodes = self.master.vis.scatter(X=np.array(self.nepisodes_log), env=self.master.refs, win=self.win_nepisodes, opts=dict(title=""nepisodes""))\n            self.win_nepisodes_solved = self.master.vis.scatter(X=np.array(self.nepisodes_solved_log), env=self.master.refs, win=self.win_nepisodes_solved, opts=dict(title=""nepisodes_solved""))\n            self.win_repisodes_solved = self.master.vis.scatter(X=np.array(self.repisodes_solved_log), env=self.master.refs, win=self.win_repisodes_solved, opts=dict(title=""repisodes_solved""))\n        # logging\n        self.master.logger.warning(""Testing  Took: "" + str(time.time() - self.start_time))\n        self.master.logger.warning(""Testing: steps_avg: {}"".format(self.steps_avg_log[-1][1]))\n        self.master.logger.warning(""Testing: steps_std: {}"".format(self.steps_std_log[-1][1]))\n        self.master.logger.warning(""Testing: reward_avg: {}"".format(self.reward_avg_log[-1][1]))\n        self.master.logger.warning(""Testing: reward_std: {}"".format(self.reward_std_log[-1][1]))\n        self.master.logger.warning(""Testing: nepisodes: {}"".format(self.nepisodes_log[-1][1]))\n        self.master.logger.warning(""Testing: nepisodes_solved: {}"".format(self.nepisodes_solved_log[-1][1]))\n        self.master.logger.warning(""Testing: repisodes_solved: {}"".format(self.repisodes_solved_log[-1][1]))\n'"
core/agents/dqn.py,8,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport random\nimport time\nimport torch\nfrom torch.autograd import Variable\n\nfrom optims.helpers import adjust_learning_rate\nfrom core.agent import Agent\n\nclass DQNAgent(Agent):\n    def __init__(self, args, env_prototype, model_prototype, memory_prototype):\n        super(DQNAgent, self).__init__(args, env_prototype, model_prototype, memory_prototype)\n        self.logger.warning(""<===================================> DQN"")\n\n        # env\n        self.env = self.env_prototype(self.env_params)\n        self.state_shape = self.env.state_shape\n        self.action_dim  = self.env.action_dim\n\n        # model\n        self.model_params.state_shape = self.state_shape\n        self.model_params.action_dim = self.action_dim\n        self.model = self.model_prototype(self.model_params)\n        self._load_model(self.model_file)   # load pretrained model if provided\n        # target_model\n        self.target_model = self.model_prototype(self.model_params)\n        self._update_target_model_hard()\n\n        # memory\n        # NOTE: we instantiate memory objects only inside fit_model/test_model\n        # NOTE: since in fit_model we need both replay memory and recent memory\n        # NOTE: while in test_model we only need recent memory, in which case memory_size=0\n        self.memory_params = args.memory_params\n\n        # experience & states\n        self._reset_states()\n\n    def _reset_training_loggings(self):\n        self._reset_testing_loggings()\n        # during the evaluation in training, we additionally log for\n        # the predicted Q-values and TD-errors on validation data\n        self.v_avg_log = []\n        self.tderr_avg_log = []\n        # placeholders for windows for online curve plotting\n        if self.visualize:\n            self.win_v_avg = ""win_v_avg""\n            self.win_tderr_avg = ""win_tderr_avg""\n\n    def _reset_testing_loggings(self):\n        # setup logging for testing/evaluation stats\n        self.steps_avg_log = []\n        self.steps_std_log = []\n        self.reward_avg_log = []\n        self.reward_std_log = []\n        self.nepisodes_log = []\n        self.nepisodes_solved_log = []\n        self.repisodes_solved_log = []\n        # placeholders for windows for online curve plotting\n        if self.visualize:\n            self.win_steps_avg = ""win_steps_avg""\n            self.win_steps_std = ""win_steps_std""\n            self.win_reward_avg = ""win_reward_avg""\n            self.win_reward_std = ""win_reward_std""\n            self.win_nepisodes = ""win_nepisodes""\n            self.win_nepisodes_solved = ""win_nepisodes_solved""\n            self.win_repisodes_solved = ""win_repisodes_solved""\n\n    def _reset_states(self):\n        self._reset_experience()\n        self.recent_action = None\n        self.recent_observation = None\n\n    # Hard update every `target_model_update` steps.\n    def _update_target_model_hard(self):\n        self.target_model.load_state_dict(self.model.state_dict())\n\n    # Soft update with `(1 - target_model_update) * old + target_model_update * new`.\n    def _update_target_model_soft(self):\n        for i, (key, target_weights) in enumerate(self.target_model.state_dict().iteritems()):\n            target_weights += self.target_model_update * self.model.state_dict()[key]\n\n    def _sample_validation_data(self):\n        self.logger.warning(""Validation Data @ Step: "" + str(self.step))\n        self.valid_data = self.memory.sample(self.valid_size)\n\n    def _compute_validation_stats(self):\n        return self._get_q_update(self.valid_data)\n\n    def _get_q_update(self, experiences): # compute temporal difference error for a batch\n        # Start by extracting the necessary parameters (we use a vectorized implementation).\n        state0_batch_vb    = Variable(torch.from_numpy(np.array(tuple(experiences[i].state0 for i in range(len(experiences))))).type(self.dtype))\n        action_batch_vb    = Variable(torch.from_numpy(np.array(tuple(experiences[i].action for i in range(len(experiences))))).long())\n        reward_batch_vb    = Variable(torch.from_numpy(np.array(tuple(experiences[i].reward for i in range(len(experiences)))))).type(self.dtype)\n        state1_batch_vb    = Variable(torch.from_numpy(np.array(tuple(experiences[i].state1 for i in range(len(experiences))))).type(self.dtype))\n        terminal1_batch_vb = Variable(torch.from_numpy(np.array(tuple(0. if experiences[i].terminal1 else 1. for i in range(len(experiences)))))).type(self.dtype)\n\n        if self.use_cuda:\n            action_batch_vb = action_batch_vb.cuda()\n\n        # Compute target Q values for mini-batch update.\n        if self.enable_double_dqn:\n            # According to the paper ""Deep Reinforcement Learning with Double Q-learning""\n            # (van Hasselt et al., 2015), in Double DQN, the online network predicts the actions\n            # while the target network is used to estimate the Q value.\n            q_values_vb = self.model(state1_batch_vb)\n            # Detach this variable from the current graph since we don\'t want gradients to propagate\n            q_values_vb = Variable(q_values_vb.data)\n            # _, q_max_actions_vb = q_values_vb.max(dim=1)              # 0.1.12\n            _, q_max_actions_vb = q_values_vb.max(dim=1, keepdim=True)  # 0.2.0\n            # Now, estimate Q values using the target network but select the values with the\n            # highest Q value wrt to the online model (as computed above).\n            next_max_q_values_vb = self.target_model(state1_batch_vb)\n            # Detach this variable from the current graph since we don\'t want gradients to propagate\n            next_max_q_values_vb = Variable(next_max_q_values_vb.data)\n            next_max_q_values_vb = next_max_q_values_vb.gather(1, q_max_actions_vb)\n        else:\n            # Compute the q_values given state1, and extract the maximum for each sample in the batch.\n            # We perform this prediction on the target_model instead of the model for reasons\n            # outlined in Mnih (2015). In short: it makes the algorithm more stable.\n            next_max_q_values_vb = self.target_model(state1_batch_vb)\n            # Detach this variable from the current graph since we don\'t want gradients to propagate\n            next_max_q_values_vb = Variable(next_max_q_values_vb.data)\n            # next_max_q_values_vb, _ = next_max_q_values_vb.max(dim = 1)               # 0.1.12\n            next_max_q_values_vb, _ = next_max_q_values_vb.max(dim = 1, keepdim=True)   # 0.2.0\n\n        # Compute r_t + gamma * max_a Q(s_t+1, a) and update the targets accordingly\n        # but only for the affected output units (as given by action_batch).\n        current_q_values_vb = self.model(state0_batch_vb).gather(1, action_batch_vb.unsqueeze(1)).squeeze()\n        # Set discounted reward to zero for all states that were terminal.\n        next_max_q_values_vb = next_max_q_values_vb * terminal1_batch_vb.unsqueeze(1)\n        # expected_q_values_vb = reward_batch_vb + self.gamma * next_max_q_values_vb            # 0.1.12\n        expected_q_values_vb = reward_batch_vb + self.gamma * next_max_q_values_vb.squeeze()    # 0.2.0\n        # Compute temporal difference error, use huber loss to mitigate outlier impact\n        # TODO: can optionally use huber loss from here: https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n        td_error_vb = self.value_criteria(current_q_values_vb, expected_q_values_vb)\n\n        # return v_avg, tderr_avg_vb\n        if not self.training:   # then is being called from _compute_validation_stats, which is just doing inference\n            td_error_vb = Variable(td_error_vb.data) # detach it from the graph\n        return next_max_q_values_vb.data.clone().mean(), td_error_vb\n\n    def _epsilon_greedy(self, q_values_ts):\n        # calculate epsilon\n        if self.training:   # linearly anneal epsilon\n            self.eps = self.eps_end + max(0, (self.eps_start - self.eps_end) * (self.eps_decay - max(0, self.step - self.learn_start)) / self.eps_decay)\n        else:\n            self.eps = self.eps_eval\n        # choose action\n        if np.random.uniform() < self.eps:  # then we choose a random action\n            action = random.randrange(self.action_dim)\n        else:                               # then we choose the greedy action\n            if self.use_cuda:\n                action = np.argmax(q_values_ts.cpu().numpy())\n            else:\n                action = np.argmax(q_values_ts.numpy())\n        return action\n\n    def _forward(self, observation):\n        # Select an action.\n        state = self.memory.get_recent_state(observation)\n        state_ts = torch.from_numpy(np.array(state)).unsqueeze(0).type(self.dtype)\n        q_values_ts = self.model(Variable(state_ts, volatile=True)).data # NOTE: only doing inference here, so volatile=True\n        if self.training and self.step < self.learn_start:  # then we don\'t do any learning, just accumulate experiences into replay memory\n            action = random.randrange(self.action_dim)      # thus we only randomly sample actions here, since the model hasn\'t been updated at all till now\n        else:\n            action = self._epsilon_greedy(q_values_ts)\n\n        # Book keeping\n        self.recent_observation = observation\n        self.recent_action = action\n\n        return action\n\n    def _backward(self, reward, terminal):\n        # Store most recent experience in memory.\n        if self.step % self.memory_interval == 0:\n            # NOTE: so the tuples stored in memory corresponds to:\n            # NOTE: in recent_observation(state0), take recent_action(action), get reward(reward), ends up in terminal(terminal1)\n            self.memory.append(self.recent_observation, self.recent_action, reward, terminal,\n                               training = self.training)\n\n        if not self.training:\n            # We\'re done here. No need to update the replay memory since we only use the\n            # recent memory to obtain the state over the most recent observations.\n            return\n\n        # sample validation data right before training started\n        # NOTE: here validation data is not entirely clean since the agent might see those data during training\n        # NOTE: but it\'s ok as is also the case in the original dqn code, cos those data are not used to judge performance like in supervised learning\n        # NOTE: but rather to inspect the whole learning procedure; of course we can separate those entirely from the training data but it\'s not worth the effort\n        if self.step == self.learn_start + 1:\n            self._sample_validation_data()\n            self.logger.warning(""Start  Training @ Step: "" + str(self.step))\n\n        # Train the network on a single stochastic batch.\n        if self.step > self.learn_start and self.step % self.train_interval == 0:\n            experiences = self.memory.sample(self.batch_size)\n            # Compute temporal difference error\n            _, td_error_vb = self._get_q_update(experiences)\n            # Construct optimizer and clear old gradients\n            # TODO: can linearly anneal the lr here thus we would have to create a new optimizer here\n            # TODO: we leave the lr constant here for now and wait for update threads maybe from: https://discuss.pytorch.org/t/adaptive-learning-rate/320/11\n            self.optimizer.zero_grad()\n            # run backward pass and clip gradient\n            td_error_vb.backward()\n            for param in self.model.parameters():\n                param.grad.data.clamp_(-self.clip_grad, self.clip_grad)\n            # Perform the update\n            self.optimizer.step()\n\n        # adjust learning rate if enabled\n        if self.lr_decay:\n            self.lr_adjusted = max(self.lr * (self.steps - self.step) / self.steps, 1e-32)\n            adjust_learning_rate(self.optimizer, self.lr_adjusted)\n\n        if self.target_model_update >= 1 and self.step % self.target_model_update == 0:\n            self._update_target_model_hard()    # Hard update every `target_model_update` steps.\n        if self.target_model_update < 1.:       # TODO: have not tested\n            self._update_target_model_soft()    # Soft update with `(1 - target_model_update) * old + target_model_update * new`.\n\n        return\n\n    def fit_model(self):\n        # memory\n        self.memory = self.memory_prototype(limit = self.memory_params.memory_size,\n                                            window_length = self.memory_params.hist_len)\n        self.eps = self.eps_start\n        # self.optimizer = self.optim(self.model.parameters(), lr=self.lr, alpha=0.95, eps=0.01, weight_decay=self.weight_decay)  # RMSprop\n        self.optimizer = self.optim(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)    # Adam\n        self.lr_adjusted = self.lr\n\n        self.logger.warning(""<===================================> Training ..."")\n        self.training = True\n        self._reset_training_loggings()\n\n        self.start_time = time.time()\n        self.step = 0\n\n        nepisodes = 0\n        nepisodes_solved = 0\n        episode_steps = None\n        episode_reward = None\n        total_reward = 0.\n        should_start_new = True\n        while self.step < self.steps:\n            if should_start_new:    # start of a new episode\n                episode_steps = 0\n                episode_reward = 0.\n                # Obtain the initial observation by resetting the environment\n                self._reset_states()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                if not self.training:\n                    if self.visualize: self.env.visual()\n                    if self.render: self.env.render()\n                # reset flag\n                should_start_new = False\n            # Run a single step\n            # This is where all of the work happens. We first perceive and compute the action\n            # (forward step) and then use the reward to improve (backward step)\n            action = self._forward(self.experience.state1)\n            reward = 0.\n            for _ in range(self.action_repetition):\n                self.experience = self.env.step(action)\n                if not self.training:\n                    if self.visualize: self.env.visual()\n                    if self.render: self.env.render()\n                reward += self.experience.reward\n                if self.experience.terminal1:\n                    should_start_new = True\n                    break\n            if self.early_stop and (episode_steps + 1) >= self.early_stop or (self.step + 1) % self.eval_freq == 0:\n                # to make sure the historic observations for the first hist_len-1 steps in (the next episode / eval) would be clean\n                should_start_new = True\n            if should_start_new:\n                self._backward(reward, True)\n            else:\n                self._backward(reward, self.experience.terminal1)\n\n            episode_steps += 1\n            episode_reward += reward\n            self.step += 1\n\n            if should_start_new:\n                # We are in a terminal state but the agent hasn\'t yet seen it. We therefore\n                # perform one more forward-backward call and simply ignore the action before\n                # resetting the environment. We need to pass in ""terminal=False"" here since\n                # the *next* state, that is the state of the newly reset environment, is\n                # always non-terminal by convention.\n                self._forward(self.experience.state1)   # recent_observation & recent_action get updated\n                self._backward(0., False)               # recent experience gets pushed into memory\n                                                        # NOTE: the append happened inside here is just trying to save s1, none of a,r,t are used for this terminal s1 when sample\n                total_reward += episode_reward\n                nepisodes += 1\n                if self.experience.terminal1:\n                    nepisodes_solved += 1\n                self._reset_states()\n                episode_steps = None\n                episode_reward = None\n\n            # report training stats\n            if self.step % self.prog_freq == 0:\n                self.logger.warning(""Reporting       @ Step: "" + str(self.step) + "" | Elapsed Time: "" + str(time.time() - self.start_time))\n                self.logger.warning(""Training Stats:   lr:               {}"".format(self.lr_adjusted))\n                self.logger.warning(""Training Stats:   epsilon:          {}"".format(self.eps))\n                self.logger.warning(""Training Stats:   total_reward:     {}"".format(total_reward))\n                self.logger.warning(""Training Stats:   avg_reward:       {}"".format(total_reward/nepisodes if nepisodes > 0 else 0.))\n                self.logger.warning(""Training Stats:   nepisodes:        {}"".format(nepisodes))\n                self.logger.warning(""Training Stats:   nepisodes_solved: {}"".format(nepisodes_solved))\n                self.logger.warning(""Training Stats:   repisodes_solved: {}"".format(nepisodes_solved/nepisodes if nepisodes > 0 else 0.))\n\n            # evaluation & checkpointing\n            if self.step > self.learn_start and self.step % self.eval_freq == 0:\n                # Set states for evaluation\n                self.training = False\n                self.logger.warning(""Evaluating      @ Step: "" + str(self.step))\n                self._eval_model()\n\n                # Set states for resume training\n                self.training = True\n                self.logger.warning(""Resume Training @ Step: "" + str(self.step))\n                should_start_new = True\n\n    def _eval_model(self):\n        self.training = False\n        eval_step = 0\n\n        eval_nepisodes = 0\n        eval_nepisodes_solved = 0\n        eval_episode_steps = None\n        eval_episode_steps_log = []\n        eval_episode_reward = None\n        eval_episode_reward_log = []\n        eval_should_start_new = True\n        while eval_step < self.eval_steps:\n            if eval_should_start_new:   # start of a new episode\n                eval_episode_steps = 0\n                eval_episode_reward = 0.\n                # Obtain the initial observation by resetting the environment\n                self._reset_states()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                if not self.training:\n                    if self.visualize: self.env.visual()\n                    if self.render: self.env.render()\n                # reset flag\n                eval_should_start_new = False\n            # Run a single step\n            eval_action = self._forward(self.experience.state1)\n            eval_reward = 0.\n            for _ in range(self.action_repetition):\n                self.experience = self.env.step(eval_action)\n                if not self.training:\n                    if self.visualize: self.env.visual()\n                    if self.render: self.env.render()\n                eval_reward += self.experience.reward\n                if self.experience.terminal1:\n                    eval_should_start_new = True\n                    break\n            if self.early_stop and (eval_episode_steps + 1) >= self.early_stop or (eval_step + 1) == self.eval_steps:\n                # to make sure the historic observations for the first hist_len-1 steps in (the next episode / resume training) would be clean\n                eval_should_start_new = True\n            # NOTE: here NOT doing backprop, only adding into recent memory\n            if eval_should_start_new:\n                self._backward(eval_reward, True)\n            else:\n                self._backward(eval_reward, self.experience.terminal1)\n\n            eval_episode_steps += 1\n            eval_episode_reward += eval_reward\n            eval_step += 1\n\n            if eval_should_start_new:\n                # We are in a terminal state but the agent hasn\'t yet seen it. We therefore\n                # perform one more forward-backward call and simply ignore the action before\n                # resetting the environment. We need to pass in ""terminal=False"" here since\n                # the *next* state, that is the state of the newly reset environment, is\n                # always non-terminal by convention.\n                self._forward(self.experience.state1) # recent_observation & recent_action get updated\n                self._backward(0., False)             # NOTE: here NOT doing backprop, only adding into recent memory\n\n                eval_nepisodes += 1\n                if self.experience.terminal1:\n                    eval_nepisodes_solved += 1\n\n                # This episode is finished, report and reset\n                eval_episode_steps_log.append([eval_episode_steps])\n                eval_episode_reward_log.append([eval_episode_reward])\n                self._reset_states()\n                eval_episode_steps = None\n                eval_episode_reward = None\n\n        # Computing validation stats\n        v_avg, tderr_avg_vb = self._compute_validation_stats()\n        # Logging for this evaluation phase\n        self.v_avg_log.append([self.step, v_avg])\n        self.tderr_avg_log.append([self.step, tderr_avg_vb.data.clone().mean()])\n        self.steps_avg_log.append([self.step, np.mean(np.asarray(eval_episode_steps_log))])\n        self.steps_std_log.append([self.step, np.std(np.asarray(eval_episode_steps_log))]); del eval_episode_steps_log\n        self.reward_avg_log.append([self.step, np.mean(np.asarray(eval_episode_reward_log))])\n        self.reward_std_log.append([self.step, np.std(np.asarray(eval_episode_reward_log))]); del eval_episode_reward_log\n        self.nepisodes_log.append([self.step, eval_nepisodes])\n        self.nepisodes_solved_log.append([self.step, eval_nepisodes_solved])\n        self.repisodes_solved_log.append([self.step, (eval_nepisodes_solved/eval_nepisodes) if eval_nepisodes > 0 else 0])\n        # plotting\n        if self.visualize:\n            self.win_v_avg = self.vis.scatter(X=np.array(self.v_avg_log), env=self.refs, win=self.win_v_avg, opts=dict(title=""v_avg""))\n            self.win_tderr_avg = self.vis.scatter(X=np.array(self.tderr_avg_log), env=self.refs, win=self.win_tderr_avg, opts=dict(title=""tderr_avg""))\n            self.win_steps_avg = self.vis.scatter(X=np.array(self.steps_avg_log), env=self.refs, win=self.win_steps_avg, opts=dict(title=""steps_avg""))\n            # self.win_steps_std = self.vis.scatter(X=np.array(self.steps_std_log), env=self.refs, win=self.win_steps_std, opts=dict(title=""steps_std""))\n            self.win_reward_avg = self.vis.scatter(X=np.array(self.reward_avg_log), env=self.refs, win=self.win_reward_avg, opts=dict(title=""reward_avg""))\n            # self.win_reward_std = self.vis.scatter(X=np.array(self.reward_std_log), env=self.refs, win=self.win_reward_std, opts=dict(title=""reward_std""))\n            self.win_nepisodes = self.vis.scatter(X=np.array(self.nepisodes_log), env=self.refs, win=self.win_nepisodes, opts=dict(title=""nepisodes""))\n            self.win_nepisodes_solved = self.vis.scatter(X=np.array(self.nepisodes_solved_log), env=self.refs, win=self.win_nepisodes_solved, opts=dict(title=""nepisodes_solved""))\n            self.win_repisodes_solved = self.vis.scatter(X=np.array(self.repisodes_solved_log), env=self.refs, win=self.win_repisodes_solved, opts=dict(title=""repisodes_solved""))\n        # logging\n        self.logger.warning(""Iteration: {}; v_avg: {}"".format(self.step, self.v_avg_log[-1][1]))\n        self.logger.warning(""Iteration: {}; tderr_avg: {}"".format(self.step, self.tderr_avg_log[-1][1]))\n        self.logger.warning(""Iteration: {}; steps_avg: {}"".format(self.step, self.steps_avg_log[-1][1]))\n        self.logger.warning(""Iteration: {}; steps_std: {}"".format(self.step, self.steps_std_log[-1][1]))\n        self.logger.warning(""Iteration: {}; reward_avg: {}"".format(self.step, self.reward_avg_log[-1][1]))\n        self.logger.warning(""Iteration: {}; reward_std: {}"".format(self.step, self.reward_std_log[-1][1]))\n        self.logger.warning(""Iteration: {}; nepisodes: {}"".format(self.step, self.nepisodes_log[-1][1]))\n        self.logger.warning(""Iteration: {}; nepisodes_solved: {}"".format(self.step, self.nepisodes_solved_log[-1][1]))\n        self.logger.warning(""Iteration: {}; repisodes_solved: {}"".format(self.step, self.repisodes_solved_log[-1][1]))\n\n        # save model\n        self._save_model(self.step, self.reward_avg_log[-1][1])\n\n    def test_model(self):\n        # memory    # NOTE: here we don\'t need a replay memory, just a recent memory\n        self.memory = self.memory_prototype(limit = 0,\n                                            window_length = self.memory_params.hist_len)\n        self.eps = self.eps_eval\n\n        self.logger.warning(""<===================================> Testing ..."")\n        self.training = False\n        self._reset_testing_loggings()\n\n        self.start_time = time.time()\n        self.step = 0\n\n        test_nepisodes = 0\n        test_nepisodes_solved = 0\n        test_episode_steps = None\n        test_episode_steps_log = []\n        test_episode_reward = None\n        test_episode_reward_log = []\n        test_should_start_new = True\n        while test_nepisodes < self.test_nepisodes:\n            if test_should_start_new:   # start of a new episode\n                test_episode_steps = 0\n                test_episode_reward = 0.\n                # Obtain the initial observation by resetting the environment\n                self._reset_states()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                if not self.training:\n                    if self.visualize: self.env.visual()\n                    if self.render: self.env.render()\n                # reset flag\n                test_should_start_new = False\n            # Run a single step\n            test_action = self._forward(self.experience.state1)\n            test_reward = 0.\n            for _ in range(self.action_repetition):\n                self.experience = self.env.step(test_action)\n                if not self.training:\n                    if self.visualize: self.env.visual()\n                    if self.render: self.env.render()\n                test_reward += self.experience.reward\n                if self.experience.terminal1:\n                    test_should_start_new = True\n                    break\n            if self.early_stop and (test_episode_steps + 1) >= self.early_stop:\n                # to make sure the historic observations for the first hist_len-1 steps in (the next episode / resume training) would be clean\n                test_should_start_new = True\n            # NOTE: here NOT doing backprop, only adding into recent memory\n            if test_should_start_new:\n                self._backward(test_reward, True)\n            else:\n                self._backward(test_reward, self.experience.terminal1)\n\n            test_episode_steps += 1\n            test_episode_reward += test_reward\n            self.step += 1\n\n            if test_should_start_new:\n                # We are in a terminal state but the agent hasn\'t yet seen it. We therefore\n                # perform one more forward-backward call and simply ignore the action before\n                # resetting the environment. We need to pass in ""terminal=False"" here since\n                # the *next* state, that is the state of the newly reset environment, is\n                # always non-terminal by convention.\n                self._forward(self.experience.state1) # recent_observation & recent_action get updated\n                self._backward(0., False)             # NOTE: here NOT doing backprop, only adding into recent memory\n\n                test_nepisodes += 1\n                if self.experience.terminal1:\n                    test_nepisodes_solved += 1\n\n                # This episode is finished, report and reset\n                test_episode_steps_log.append([test_episode_steps])\n                test_episode_reward_log.append([test_episode_reward])\n                self._reset_states()\n                test_episode_steps = None\n                test_episode_reward = None\n\n        # Logging for this testing phase\n        self.steps_avg_log.append([self.step, np.mean(np.asarray(test_episode_steps_log))])\n        self.steps_std_log.append([self.step, np.std(np.asarray(test_episode_steps_log))]); del test_episode_steps_log\n        self.reward_avg_log.append([self.step, np.mean(np.asarray(test_episode_reward_log))])\n        self.reward_std_log.append([self.step, np.std(np.asarray(test_episode_reward_log))]); del test_episode_reward_log\n        self.nepisodes_log.append([self.step, test_nepisodes])\n        self.nepisodes_solved_log.append([self.step, test_nepisodes_solved])\n        self.repisodes_solved_log.append([self.step, (test_nepisodes_solved/test_nepisodes) if test_nepisodes > 0 else 0.])\n        # plotting\n        if self.visualize:\n            self.win_steps_avg = self.vis.scatter(X=np.array(self.steps_avg_log), env=self.refs, win=self.win_steps_avg, opts=dict(title=""steps_avg""))\n            # self.win_steps_std = self.vis.scatter(X=np.array(self.steps_std_log), env=self.refs, win=self.win_steps_std, opts=dict(title=""steps_std""))\n            self.win_reward_avg = self.vis.scatter(X=np.array(self.reward_avg_log), env=self.refs, win=self.win_reward_avg, opts=dict(title=""reward_avg""))\n            # self.win_reward_std = self.vis.scatter(X=np.array(self.reward_std_log), env=self.refs, win=self.win_reward_std, opts=dict(title=""reward_std""))\n            self.win_nepisodes = self.vis.scatter(X=np.array(self.nepisodes_log), env=self.refs, win=self.win_nepisodes, opts=dict(title=""nepisodes""))\n            self.win_nepisodes_solved = self.vis.scatter(X=np.array(self.nepisodes_solved_log), env=self.refs, win=self.win_nepisodes_solved, opts=dict(title=""nepisodes_solved""))\n            self.win_repisodes_solved = self.vis.scatter(X=np.array(self.repisodes_solved_log), env=self.refs, win=self.win_repisodes_solved, opts=dict(title=""repisodes_solved""))\n        # logging\n        self.logger.warning(""Testing  Took: "" + str(time.time() - self.start_time))\n        self.logger.warning(""Testing: steps_avg: {}"".format(self.steps_avg_log[-1][1]))\n        self.logger.warning(""Testing: steps_std: {}"".format(self.steps_std_log[-1][1]))\n        self.logger.warning(""Testing: reward_avg: {}"".format(self.reward_avg_log[-1][1]))\n        self.logger.warning(""Testing: reward_std: {}"".format(self.reward_std_log[-1][1]))\n        self.logger.warning(""Testing: nepisodes: {}"".format(self.nepisodes_log[-1][1]))\n        self.logger.warning(""Testing: nepisodes_solved: {}"".format(self.nepisodes_solved_log[-1][1]))\n        self.logger.warning(""Testing: repisodes_solved: {}"".format(self.repisodes_solved_log[-1][1]))\n'"
core/agents/empty.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport random\n\nfrom utils.helpers import Experience\nfrom core.agent import Agent\n\nclass EmptyAgent(Agent):\n    def __init__(self, args, env_prototype, model_prototype, memory_prototype):\n        super(EmptyAgent, self).__init__(args, env_prototype, model_prototype, memory_prototype)\n        self.logger.warning(""<===================================> Empty"")\n\n        # env\n        self.env = self.env_prototype(self.env_params)\n        self.state_shape = self.env.state_shape\n        self.action_dim  = self.env.action_dim\n\n        self._reset_experience()\n\n    def _forward(self, state):\n        pass\n\n    def _backward(self, reward, terminal):\n        pass\n\n    def _eval_model(self):\n        pass\n\n    def fit_model(self):    # the most basic control loop, to ease integration of new envs\n        self.step = 0\n        should_start_new = True\n        while self.step < self.steps:\n            if should_start_new:\n                self._reset_experience()\n                self.experience = self.env.reset()\n                assert self.experience.state1 is not None\n                if self.visualize: self.env.visual()\n                if self.render: self.env.render()\n                should_start_new = False\n            action = random.randrange(self.action_dim)      # thus we only randomly sample actions here, since the model hasn\'t been updated at all till now\n            self.experience = self.env.step(action)\n            if self.visualize: self.env.visual()\n            if self.render: self.env.render()\n            if self.experience.terminal1 or self.early_stop and (episode_steps + 1) >= self.early_stop:\n                should_start_new = True\n\n            self.step += 1\n\n    def test_model(self):\n        pass\n'"
core/envs/__init__.py,0,b''
core/envs/atari.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom copy import deepcopy\nfrom gym.spaces.box import Box\nimport inspect\n\nfrom utils.helpers import Experience            # NOTE: here state0 is always ""None""\nfrom utils.helpers import preprocessAtari, rgb2gray, rgb2y, scale\nfrom core.env import Env\n\nclass AtariEnv(Env):  # pixel-level inputs\n    def __init__(self, args, env_ind=0):\n        super(AtariEnv, self).__init__(args, env_ind)\n\n        assert self.env_type == ""atari""\n        try: import gym\n        except ImportError as e: self.logger.warning(""WARNING: gym not found"")\n\n        self.env = gym.make(self.game)\n        self.env.seed(self.seed)    # NOTE: so each env would be different\n\n        # action space setup\n        self.actions     = range(self.action_dim)\n        self.logger.warning(""Action Space: %s"", self.actions)\n        # state space setup\n        self.hei_state = args.hei_state\n        self.wid_state = args.wid_state\n        self.preprocess_mode = args.preprocess_mode if not None else 0 # 0(crop&resize) | 1(rgb2gray) | 2(rgb2y)\n        assert self.hei_state == self.wid_state\n        self.logger.warning(""State  Space: ("" + str(self.state_shape) + "" * "" + str(self.state_shape) + "")"")\n\n    def _preprocessState(self, state):\n        if self.preprocess_mode == 3:   # crop then resize\n            state = preprocessAtari(state)\n        if self.preprocess_mode == 2:   # rgb2y\n            state = scale(rgb2y(state), self.hei_state, self.wid_state) / 255.\n        elif self.preprocess_mode == 1: # rgb2gray\n            state = scale(rgb2gray(state), self.hei_state, self.wid_state) / 255.\n        elif self.preprocess_mode == 0: # do nothing\n            pass\n        return state.reshape(self.hei_state * self.wid_state)\n\n    @property\n    def state_shape(self):\n        return self.hei_state\n\n    def render(self):\n        return self.env.render()\n\n    def visual(self):\n        if self.visualize:\n            self.win_state1 = self.vis.image(np.transpose(self.exp_state1, (2, 0, 1)), env=self.refs, win=self.win_state1, opts=dict(title=""state1""))\n        if self.mode == 2:\n            frame_name = self.img_dir + ""frame_%04d.jpg"" % self.frame_ind\n            self.imsave(frame_name, self.exp_state1)\n            self.logger.warning(""Saved  Frame    @ Step: "" + str(self.frame_ind) + "" To: "" + frame_name)\n            self.frame_ind += 1\n\n    def sample_random_action(self):\n        return self.env.action_space.sample()\n\n    def reset(self):\n        # TODO: could add random start here, since random start only make sense for atari games\n        self._reset_experience()\n        self.exp_state1 = self.env.reset()\n        return self._get_experience()\n\n    def step(self, action_index):\n        self.exp_action = action_index\n        self.exp_state1, self.exp_reward, self.exp_terminal1, _ = self.env.step(self.actions[self.exp_action])\n        return self._get_experience()\n'"
core/envs/atari_ram.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom copy import deepcopy\nfrom gym.spaces.box import Box\nimport inspect\n\nfrom utils.helpers import Experience            # NOTE: here state0 is always ""None""\nfrom utils.helpers import preprocessAtari, rgb2gray, rgb2y, scale\nfrom core.env import Env\n\nclass AtariRamEnv(Env):  # atari games w/ ram states as input\n    def __init__(self, args, env_ind=0):\n        super(AtariRamEnv, self).__init__(args, env_ind)\n\n        assert self.env_type == ""atari-ram""\n        try: import gym\n        except ImportError as e: self.logger.warning(""WARNING: gym not found"")\n\n        self.env = gym.make(self.game)\n        self.env.seed(self.seed)    # NOTE: so each env would be different\n\n        # action space setup\n        self.actions     = range(self.action_dim)\n        self.logger.warning(""Action Space: %s"", self.actions)\n\n        # state space setup\n        self.logger.warning(""State  Space: %s"", self.state_shape)\n\n    def _preprocessState(self, state):    # NOTE: here the input is [0, 255], so we normalize\n        return state/255.                 # TODO: check again the range, also syntax w/ python3\n\n    @property\n    def state_shape(self):\n        return self.env.observation_space.shape[0]\n\n    def render(self):\n        return self.env.render()\n\n    def visual(self):   # TODO: try to grab also the pixel-level outputs and visualize\n        pass\n\n    def sample_random_action(self):\n        return self.env.action_space.sample()\n\n    def reset(self):\n        self._reset_experience()\n        self.exp_state1 = self.env.reset()\n        return self._get_experience()\n\n    def step(self, action_index):\n        self.exp_action = action_index\n        self.exp_state1, self.exp_reward, self.exp_terminal1, _ = self.env.step(self.actions[self.exp_action])\n        return self._get_experience()\n'"
core/envs/gym.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom copy import deepcopy\nfrom gym.spaces.box import Box\nimport inspect\n\nfrom utils.helpers import Experience            # NOTE: here state0 is always ""None""\nfrom utils.helpers import preprocessAtari, rgb2gray, rgb2y, scale\nfrom core.env import Env\n\nclass GymEnv(Env):  # low dimensional observations\n    def __init__(self, args, env_ind=0):\n        super(GymEnv, self).__init__(args, env_ind)\n\n        assert self.env_type == ""gym""\n        try: import gym\n        except ImportError as e: self.logger.warning(""WARNING: gym not found"")\n\n        self.env = gym.make(self.game)\n        self.env.seed(self.seed)    # NOTE: so each env would be different\n\n        # action space setup\n        self.actions     = range(self.action_dim)\n        self.logger.warning(""Action Space: %s"", self.actions)\n\n        # state space setup\n        self.logger.warning(""State  Space: %s"", self.state_shape)\n\n        # continuous space\n        if args.agent_type == ""a3c"":\n            self.enable_continuous = args.enable_continuous\n        else:\n            self.enable_continuous = False\n\n    def _preprocessState(self, state):    # NOTE: here no preprecessing is needed\n        return state\n\n    @property\n    def state_shape(self):\n        return self.env.observation_space.shape[0]\n\n    def render(self):\n        if self.mode == 2:\n            frame = self.env.render(mode=\'rgb_array\')\n            frame_name = self.img_dir + ""frame_%04d.jpg"" % self.frame_ind\n            self.imsave(frame_name, frame)\n            self.logger.warning(""Saved  Frame    @ Step: "" + str(self.frame_ind) + "" To: "" + frame_name)\n            self.frame_ind += 1\n            return frame\n        else:\n            return self.env.render()\n\n\n    def visual(self):\n        pass\n\n    def sample_random_action(self):\n        return self.env.action_space.sample()\n\n    def reset(self):\n        self._reset_experience()\n        self.exp_state1 = self.env.reset()\n        return self._get_experience()\n\n    def step(self, action_index):\n        self.exp_action = action_index\n        if self.enable_continuous:\n            self.exp_state1, self.exp_reward, self.exp_terminal1, _ = self.env.step(self.exp_action)\n        else:\n            self.exp_state1, self.exp_reward, self.exp_terminal1, _ = self.env.step(self.actions[self.exp_action])\n        return self._get_experience()\n'"
core/envs/lab.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom copy import deepcopy\nfrom gym.spaces.box import Box\nimport inspect\n\nfrom utils.helpers import Experience            # NOTE: here state0 is always ""None""\nfrom utils.helpers import preprocessAtari, rgb2gray, rgb2y, scale\nfrom core.env import Env\n\nclass LabEnv(Env):\n    def __init__(self, args, env_ind=0):\n        super(LabEnv, self).__init__(args, env_ind)\n\n        assert self.env_type == ""lab""\n'"
core/memories/__init__.py,0,b''
core/memories/episode_parameter.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom collections import deque, namedtuple\nimport warnings\nimport random\n\nfrom utils.helpers import Experience\nfrom core.memory import sample_batch_indexes, RingBuffer, Memory\n\nclass EpisodeParameterMemory(Memory):\n    def __init__(self, limit, **kwargs):\n        super(EpisodeParameterMemory, self).__init__(**kwargs)\n        self.limit = limit\n\n        self.params = RingBuffer(limit)\n        self.intermediate_rewards = []\n        self.total_rewards = RingBuffer(limit)\n\n    def sample(self, batch_size, batch_idxs=None):\n        if batch_idxs is None:\n            batch_idxs = sample_batch_indexes(0, self.nb_entries, size=batch_size)\n        assert len(batch_idxs) == batch_size\n\n        batch_params = []\n        batch_total_rewards = []\n        for idx in batch_idxs:\n            batch_params.append(self.params[idx])\n            batch_total_rewards.append(self.total_rewards[idx])\n        return batch_params, batch_total_rewards\n\n    def append(self, observation, action, reward, terminal, training=True):\n        super(EpisodeParameterMemory, self).append(observation, action, reward, terminal, training=training)\n        if training:\n            self.intermediate_rewards.append(reward)\n\n    def finalize_episode(self, params):\n        total_reward = sum(self.intermediate_rewards)\n        self.total_rewards.append(total_reward)\n        self.params.append(params)\n        self.intermediate_rewards = []\n\n    @property\n    def nb_entries(self):\n        return len(self.total_rewards)\n\n    def get_config(self):\n        config = super(SequentialMemory, self).get_config()\n        config['limit'] = self.limit\n        return config\n"""
core/memories/episodic.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport random\nfrom collections import deque, namedtuple\n\nfrom utils.helpers import ACER_Off_Policy_Experience\n\n# TODO: should inherite from Memory to make it consistent\nclass EpisodicMemory():\n    def __init__(self, capacity, max_episode_length):\n        # Max number of transitions possible will be the memory capacity, could be much less\n        self.num_episodes = capacity // max_episode_length\n        self.memory = deque(maxlen=self.num_episodes)\n        self.memory.append([])  # List for first episode\n        self.position = 0\n\n    def append(self, state0, action, reward, detached_old_policy_vb):\n        self.memory[self.position].append(ACER_Off_Policy_Experience(state0, action, reward, detached_old_policy_vb))  # Save s_i, a_i, r_i+1, /mu(|s_i)\n        # Terminal states are saved with actions as None, so switch to next episode\n        if action is None:\n            self.memory.append([])\n            self.position = min(self.position + 1, self.num_episodes - 1)\n\n    # Samples random trajectory\n    def sample(self, maxlen=0):\n        while True:\n            e = random.randrange(len(self.memory))\n            mem = self.memory[e]\n            T = len(mem)\n            if T > 0:\n                # Take a random subset of trajectory if maxlen specified, otherwise return full trajectory\n                if maxlen > 0 and T > maxlen + 1:\n                    t = random.randrange(T - maxlen - 1)  # Include next state after final ""maxlen"" state\n                    return mem[t:t + maxlen + 1]\n                else:\n                    return mem\n\n    # Samples batch of trajectories, truncating them to the same length\n    def sample_batch(self, batch_size, maxlen=0):\n        batch = [self.sample(maxlen=maxlen) for _ in range(batch_size)]\n        minimum_size = min(len(trajectory) for trajectory in batch)\n        batch = [trajectory[:minimum_size] for trajectory in batch]  # Truncate trajectories\n        return list(map(list, zip(*batch)))  # Transpose so that timesteps are packed together\n\n    def __len__(self):\n        return sum(len(episode) for episode in self.memory)\n'"
core/memories/sequential.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nfrom collections import deque, namedtuple\nimport warnings\nimport random\n\nfrom utils.helpers import Experience\nfrom core.memory import sample_batch_indexes, zeroed_observation, RingBuffer, Memory\n\nclass SequentialMemory(Memory):\n    def __init__(self, limit, **kwargs):\n        super(SequentialMemory, self).__init__(**kwargs)\n\n        self.limit = limit\n\n        # Do not use deque to implement the memory. This data structure may seem convenient but\n        # it is way too slow on random access. Instead, we use our own ring buffer implementation.\n        self.actions = RingBuffer(limit)\n        self.rewards = RingBuffer(limit)\n        self.terminals = RingBuffer(limit)\n        self.observations = RingBuffer(limit)\n\n    def sample(self, batch_size, batch_idxs=None):\n        if batch_idxs is None:\n            # Draw random indexes such that we have at least a single entry before each\n            # index.\n            batch_idxs = sample_batch_indexes(0, self.nb_entries - 1, size=batch_size)\n        batch_idxs = np.array(batch_idxs) + 1\n        assert np.min(batch_idxs) >= 1\n        assert np.max(batch_idxs) < self.nb_entries\n        assert len(batch_idxs) == batch_size\n\n        # Create experiences\n        experiences = []\n        for idx in batch_idxs:\n            terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n            while terminal0:\n                # Skip this transition because the environment was reset here. Select a new, random\n                # transition and use this instead. This may cause the batch to contain the same\n                # transition twice.\n                idx = sample_batch_indexes(1, self.nb_entries, size=1)[0]\n                terminal0 = self.terminals[idx - 2] if idx >= 2 else False\n            assert 1 <= idx < self.nb_entries\n\n            # This code is slightly complicated by the fact that subsequent observations might be\n            # from different episodes. We ensure that an experience never spans multiple episodes.\n            # This is probably not that important in practice but it seems cleaner.\n            state0 = [self.observations[idx - 1]]\n            for offset in range(0, self.window_length - 1):\n                current_idx = idx - 2 - offset\n                current_terminal = self.terminals[current_idx - 1] if current_idx - 1 > 0 else False\n                if current_idx < 0 or (not self.ignore_episode_boundaries and current_terminal):\n                    # The previously handled observation was terminal, don't add the current one.\n                    # Otherwise we would leak into a different episode.\n                    break\n                state0.insert(0, self.observations[current_idx])\n            while len(state0) < self.window_length:\n                state0.insert(0, zeroed_observation(state0[0]))\n            action = self.actions[idx - 1]\n            reward = self.rewards[idx - 1]\n            terminal1 = self.terminals[idx - 1]\n\n            # Okay, now we need to create the follow-up state. This is state0 shifted on timestep\n            # to the right. Again, we need to be careful to not include an observation from the next\n            # episode if the last state is terminal.\n            state1 = [np.copy(x) for x in state0[1:]]\n            state1.append(self.observations[idx])\n\n            assert len(state0) == self.window_length\n            assert len(state1) == len(state0)\n            experiences.append(Experience(state0=state0, action=action, reward=reward,\n                                          state1=state1, terminal1=terminal1))\n        assert len(experiences) == batch_size\n        return experiences\n\n    def append(self, observation, action, reward, terminal, training=True):\n        super(SequentialMemory, self).append(observation, action, reward, terminal, training=training)\n\n        # This needs to be understood as follows: in `observation`, take `action`, obtain `reward`\n        # and weather the next state is `terminal` or not.\n        if training:\n            self.observations.append(observation)\n            self.actions.append(action)\n            self.rewards.append(reward)\n            self.terminals.append(terminal)\n\n    @property\n    def nb_entries(self):\n        return len(self.observations)\n\n    def get_config(self):\n        config = super(SequentialMemory, self).get_config()\n        config['limit'] = self.limit\n        return config\n"""
core/models/__init__.py,0,b''
core/models/a3c_cnn_dis.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils.init_weights import init_weights, normalized_columns_initializer\nfrom core.model import Model\n\nclass A3CCnnDisModel(Model):\n    def __init__(self, args):\n        super(A3CCnnDisModel, self).__init__(args)\n        # build model\n        # 0. feature layers\n        self.conv1 = nn.Conv2d(self.input_dims[0], 32, kernel_size=3, stride=2) # NOTE: for pkg=""atari""\n        self.rl1   = nn.ReLU()\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n        self.rl2   = nn.ReLU()\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n        self.rl3   = nn.ReLU()\n        self.conv4 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n        self.rl4   = nn.ReLU()\n        if self.enable_lstm:\n            self.lstm  = nn.LSTMCell(3*3*32, self.hidden_dim)\n        # 1. policy output\n        self.policy_5 = nn.Linear(self.hidden_dim, self.output_dims)\n        self.policy_6 = nn.Softmax()\n        # 2. value output\n        self.value_5  = nn.Linear(self.hidden_dim, 1)\n\n        self._reset()\n\n    def _init_weights(self):\n        self.apply(init_weights)\n        self.policy_5.weight.data = normalized_columns_initializer(self.policy_5.weight.data, 0.01)\n        self.policy_5.bias.data.fill_(0)\n        self.value_5.weight.data = normalized_columns_initializer(self.value_5.weight.data, 1.0)\n        self.value_5.bias.data.fill_(0)\n\n        self.lstm.bias_ih.data.fill_(0)\n        self.lstm.bias_hh.data.fill_(0)\n\n    def forward(self, x, lstm_hidden_vb=None):\n        x = x.view(x.size(0), self.input_dims[0], self.input_dims[1], self.input_dims[1])\n        x = self.rl1(self.conv1(x))\n        x = self.rl2(self.conv2(x))\n        x = self.rl3(self.conv3(x))\n        x = self.rl4(self.conv4(x))\n        x = x.view(-1, 3*3*32)\n        if self.enable_lstm:\n            x, c = self.lstm(x, lstm_hidden_vb)\n        p = self.policy_5(x)\n        p = self.policy_6(p)\n        v = self.value_5(x)\n        if self.enable_lstm:\n            return p, v, (x, c)\n        else:\n            return p, v\n'"
core/models/a3c_mlp_con.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils.init_weights import init_weights, normalized_columns_initializer\nfrom core.model import Model\n\nclass A3CMlpConModel(Model):\n    def __init__(self, args):\n        super(A3CMlpConModel, self).__init__(args)\n        # build model\n        # 0. feature layers\n        self.fc1 = nn.Linear(self.input_dims[0] * self.input_dims[1], self.hidden_dim) # NOTE: for pkg=""gym""\n        self.rl1 = nn.ReLU()\n        self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.rl2 = nn.ReLU()\n        self.fc3 = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.rl3 = nn.ReLU()\n        self.fc4 = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.rl4 = nn.ReLU()\n\n        self.fc1_v = nn.Linear(self.input_dims[0] * self.input_dims[1], self.hidden_dim) # NOTE: for pkg=""gym""\n        self.rl1_v = nn.ReLU()\n        self.fc2_v = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.rl2_v = nn.ReLU()\n        self.fc3_v = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.rl3_v = nn.ReLU()\n        self.fc4_v = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.rl4_v = nn.ReLU()\n\n        # lstm\n        if self.enable_lstm:\n            self.lstm  = nn.LSTMCell(self.hidden_dim, self.hidden_dim)\n            self.lstm_v  = nn.LSTMCell(self.hidden_dim, self.hidden_dim)\n\n        # 1. policy output\n        self.policy_5   = nn.Linear(self.hidden_dim, self.output_dims)\n        self.policy_sig = nn.Linear(self.hidden_dim, self.output_dims)\n        self.softplus   = nn.Softplus()\n        # 2. value output\n        self.value_5    = nn.Linear(self.hidden_dim, 1)\n\n        self._reset()\n\n    def _init_weights(self):\n        self.apply(init_weights)\n        self.fc1.weight.data = normalized_columns_initializer(self.fc1.weight.data, 0.01)\n        self.fc1.bias.data.fill_(0)\n        self.fc2.weight.data = normalized_columns_initializer(self.fc2.weight.data, 0.01)\n        self.fc2.bias.data.fill_(0)\n        self.fc3.weight.data = normalized_columns_initializer(self.fc3.weight.data, 0.01)\n        self.fc3.bias.data.fill_(0)\n        self.fc4.weight.data = normalized_columns_initializer(self.fc4.weight.data, 0.01)\n        self.fc4.bias.data.fill_(0)\n        self.policy_5.weight.data = normalized_columns_initializer(self.policy_5.weight.data, 0.01)\n        self.policy_5.bias.data.fill_(0)\n        self.value_5.weight.data = normalized_columns_initializer(self.value_5.weight.data, 1.0)\n        self.value_5.bias.data.fill_(0)\n\n        self.lstm.bias_ih.data.fill_(0)\n        self.lstm.bias_hh.data.fill_(0)\n\n        self.lstm_v.bias_ih.data.fill_(0)\n        self.lstm_v.bias_hh.data.fill_(0)\n\n    def forward(self, x, lstm_hidden_vb=None):\n        p = x.view(x.size(0), self.input_dims[0] * self.input_dims[1])\n        p = self.rl1(self.fc1(p))\n        p = self.rl2(self.fc2(p))\n        p = self.rl3(self.fc3(p))\n        p = self.rl4(self.fc4(p))\n        p = p.view(-1, self.hidden_dim)\n        if self.enable_lstm:\n            p_, v_ = torch.split(lstm_hidden_vb[0],1)\n            c_p, c_v = torch.split(lstm_hidden_vb[1],1)\n            p, c_p = self.lstm(p, (p_, c_p))\n        p_out = self.policy_5(p)\n        sig = self.policy_sig(p)\n        sig = self.softplus(sig)\n\n        v = x.view(x.size(0), self.input_dims[0] * self.input_dims[1])\n        v = self.rl1_v(self.fc1_v(v))\n        v = self.rl2_v(self.fc2_v(v))\n        v = self.rl3_v(self.fc3_v(v))\n        v = self.rl4_v(self.fc4_v(v))\n        v = v.view(-1, self.hidden_dim)\n        if self.enable_lstm:\n            v, c_v = self.lstm_v(v, (v_, c_v))\n        v_out = self.value_5(v)\n\n        if self.enable_lstm:\n            return p_out, sig, v_out, (torch.cat((p,v),0), torch.cat((c_p, c_v),0))\n        else:\n            return p_out, sig, v_out\n'"
core/models/acer_cnn_dis.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils.init_weights import init_weights, normalized_columns_initializer\nfrom core.model import Model\n\nclass ACERCnnDisModel(Model):\n    def __init__(self, args):\n        super(ACERCnnDisModel, self).__init__(args)\n        # build model\n        # 0. feature layers\n        self.conv1 = nn.Conv2d(self.input_dims[0], 32, kernel_size=3, stride=2) # NOTE: for pkg=""atari""\n        self.rl1   = nn.ReLU()\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n        self.rl2   = nn.ReLU()\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n        self.rl3   = nn.ReLU()\n        self.conv4 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n        self.rl4   = nn.ReLU()\n        if self.enable_lstm:\n            self.lstm  = nn.LSTMCell(3*3*32, self.hidden_dim)\n\n        # 1. actor:  /pi_{/theta}(a_t | x_t)\n        self.actor_5 = nn.Linear(self.hidden_dim, self.output_dims)\n        self.actor_6 = nn.Softmax()\n        # 2. critic: Q_{/theta_v}(x_t, a_t)\n        self.critic_5 = nn.Linear(self.hidden_dim, self.output_dims)\n\n        self._reset()\n\n    def _init_weights(self):\n        self.apply(init_weights)\n        self.actor_5.weight.data = normalized_columns_initializer(self.actor_5.weight.data, 0.01)\n        self.actor_5.bias.data.fill_(0)\n        self.critic_5.weight.data = normalized_columns_initializer(self.critic_5.weight.data, 1.0)\n        self.critic_5.bias.data.fill_(0)\n\n        self.lstm.bias_ih.data.fill_(0)\n        self.lstm.bias_hh.data.fill_(0)\n\n    def forward(self, x, lstm_hidden_vb=None):\n        x = x.view(x.size(0), self.input_dims[0], self.input_dims[1], self.input_dims[1])\n        x = self.rl1(self.conv1(x))\n        x = self.rl2(self.conv2(x))\n        x = self.rl3(self.conv3(x))\n        x = self.rl4(self.conv4(x))\n        x = x.view(-1, 3*3*32)\n        if self.enable_lstm:\n            x, c = self.lstm(x, lstm_hidden_vb)\n        policy = self.actor_6(self.actor_5(x)).clamp(max=1-1e-6, min=1e-6) # TODO: max might not be necessary\n        q = self.critic_5(x)\n        v = (q * policy).sum(1, keepdim=True)   # expectation of Q under /pi\n        if self.enable_lstm:\n            return policy, q, v, (x, c)\n        else:\n            return policy, q, v\n'"
core/models/acer_mlp_dis.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils.init_weights import init_weights, normalized_columns_initializer\nfrom core.model import Model\n\nclass ACERMlpDisModel(Model):\n    def __init__(self, args):\n        super(ACERMlpDisModel, self).__init__(args)\n        # build model\n        # 0. feature layers\n        self.fc1 = nn.Linear(self.input_dims[0] * self.input_dims[1], self.hidden_dim)\n        self.rl1 = nn.ReLU()\n\n        # lstm\n        if self.enable_lstm:\n            self.lstm  = nn.LSTMCell(self.hidden_dim, self.hidden_dim)\n\n        # 1. actor:  /pi_{/theta}(a_t | x_t)\n        self.actor_2 = nn.Linear(self.hidden_dim, self.output_dims)\n        self.actor_3 = nn.Softmax()\n        # 2. critic: Q_{/theta_v}(x_t, a_t)\n        self.critic_2 = nn.Linear(self.hidden_dim, self.output_dims)\n\n        self._reset()\n\n    def _init_weights(self):\n        self.apply(init_weights)\n        self.actor_2.weight.data = normalized_columns_initializer(self.actor_2.weight.data, 0.01)\n        self.actor_2.bias.data.fill_(0)\n        self.critic_2.weight.data = normalized_columns_initializer(self.critic_2.weight.data, 1.0)\n        self.critic_2.bias.data.fill_(0)\n\n        self.lstm.bias_ih.data.fill_(0)\n        self.lstm.bias_hh.data.fill_(0)\n\n    def forward(self, x, lstm_hidden_vb=None):\n        x = x.view(x.size(0), self.input_dims[0] * self.input_dims[1])\n        x = self.rl1(self.fc1(x))\n        # x = x.view(-1, 3*3*32)\n        if self.enable_lstm:\n            x, c = self.lstm(x, lstm_hidden_vb)\n        policy = self.actor_3(self.actor_2(x)).clamp(max=1-1e-6, min=1e-6) # TODO: max might not be necessary\n        q = self.critic_2(x)\n        v = (q * policy).sum(1, keepdim=True)   # expectation of Q under /pi\n        if self.enable_lstm:\n            return policy, q, v, (x, c)\n        else:\n            return policy, q, v\n'"
core/models/dqn_cnn.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils.init_weights import init_weights, normalized_columns_initializer\nfrom core.model import Model\n\nclass DQNCnnModel(Model):\n    def __init__(self, args):\n        super(DQNCnnModel, self).__init__(args)\n        # 84x84\n        # self.conv1 = nn.Conv2d(self.input_dims[0], 32, kernel_size=8, stride=4)\n        # self.rl1   = nn.ReLU()\n        # self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        # self.rl2   = nn.ReLU()\n        # self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n        # self.rl3   = nn.ReLU()\n        # self.fc4   = nn.Linear(64*7*7, self.hidden_dim)\n        # self.rl4   = nn.ReLU()\n        # 42x42\n        self.conv1 = nn.Conv2d(self.input_dims[0], 32, kernel_size=3, stride=2)\n        self.rl1   = nn.ReLU()\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n        self.rl2   = nn.ReLU()\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n        self.rl3   = nn.ReLU()\n        self.fc4   = nn.Linear(32*5*5, self.hidden_dim)\n        self.rl4   = nn.ReLU()\n        if self.enable_dueling: # [0]: V(s); [1,:]: A(s, a)\n            self.fc5 = nn.Linear(self.hidden_dim, self.output_dims + 1)\n            self.v_ind = torch.LongTensor(self.output_dims).fill_(0).unsqueeze(0)\n            self.a_ind = torch.LongTensor(np.arange(1, self.output_dims + 1)).unsqueeze(0)\n        else: # one q value output for each action\n            self.fc5 = nn.Linear(self.hidden_dim, self.output_dims)\n\n        self._reset()\n\n    def _init_weights(self):\n        self.apply(init_weights)\n        self.fc4.weight.data = normalized_columns_initializer(self.fc4.weight.data, 0.0001)\n        self.fc4.bias.data.fill_(0)\n        self.fc5.weight.data = normalized_columns_initializer(self.fc5.weight.data, 0.0001)\n        self.fc5.bias.data.fill_(0)\n\n    def forward(self, x):\n        x = x.view(x.size(0), self.input_dims[0], self.input_dims[1], self.input_dims[1])\n        x = self.rl1(self.conv1(x))\n        x = self.rl2(self.conv2(x))\n        x = self.rl3(self.conv3(x))\n        x = self.rl4(self.fc4(x.view(x.size(0), -1)))\n        if self.enable_dueling:\n            x = self.fc5(x)\n            v_ind_vb = Variable(self.v_ind)\n            a_ind_vb = Variable(self.a_ind)\n            if self.use_cuda:\n                v_ind_vb = v_ind_vb.cuda()\n                a_ind_vb = a_ind_vb.cuda()\n            v = x.gather(1, v_ind_vb.expand(x.size(0), self.output_dims))\n            a = x.gather(1, a_ind_vb.expand(x.size(0), self.output_dims))\n            # now calculate Q(s, a)\n            if self.dueling_type == ""avg"":      # Q(s,a)=V(s)+(A(s,a)-avg_a(A(s,a)))\n                x = v + (a - a.mean(1).expand(x.size(0), self.output_dims))\n            elif self.dueling_type == ""max"":    # Q(s,a)=V(s)+(A(s,a)-max_a(A(s,a)))\n                x = v + (a - a.max(1)[0].expand(x.size(0), self.output_dims))\n            elif self.dueling_type == ""naive"":  # Q(s,a)=V(s)+ A(s,a)\n                x = v + a\n            else:\n                assert False, ""dueling_type must be one of {\'avg\', \'max\', \'naive\'}""\n            del v_ind_vb, a_ind_vb, v, a\n            return x\n        else:\n            return self.fc5(x.view(x.size(0), -1))\n'"
core/models/dqn_mlp.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils.init_weights import init_weights, normalized_columns_initializer\nfrom core.model import Model\n\nclass DQNMlpModel(Model):\n    def __init__(self, args):\n        super(DQNMlpModel, self).__init__(args)\n        # build model\n        self.fc1 = nn.Linear(self.input_dims[0] * self.input_dims[1], self.hidden_dim)\n        self.rl1 = nn.ReLU()\n        self.fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.rl2 = nn.ReLU()\n        self.fc3 = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.rl3 = nn.ReLU()\n        if self.enable_dueling: # [0]: V(s); [1,:]: A(s, a)\n            self.fc4 = nn.Linear(self.hidden_dim, self.output_dims + 1)\n            self.v_ind = torch.LongTensor(self.output_dims).fill_(0).unsqueeze(0)\n            self.a_ind = torch.LongTensor(np.arange(1, self.output_dims + 1)).unsqueeze(0)\n        else: # one q value output for each action\n            self.fc4 = nn.Linear(self.hidden_dim, self.output_dims)\n\n        self._reset()\n\n    def _init_weights(self):\n        # self.apply(init_weights)\n        # self.fc1.weight.data = normalized_columns_initializer(self.fc1.weight.data, 0.01)\n        # self.fc1.bias.data.fill_(0)\n        # self.fc2.weight.data = normalized_columns_initializer(self.fc2.weight.data, 0.01)\n        # self.fc2.bias.data.fill_(0)\n        # self.fc3.weight.data = normalized_columns_initializer(self.fc3.weight.data, 0.01)\n        # self.fc3.bias.data.fill_(0)\n        # self.fc4.weight.data = normalized_columns_initializer(self.fc4.weight.data, 0.01)\n        # self.fc4.bias.data.fill_(0)\n        pass\n\n    def forward(self, x):\n        x = x.view(x.size(0), self.input_dims[0] * self.input_dims[1])\n        x = self.rl1(self.fc1(x))\n        x = self.rl2(self.fc2(x))\n        x = self.rl3(self.fc3(x))\n        if self.enable_dueling:\n            x = self.fc4(x.view(x.size(0), -1))\n            v_ind_vb = Variable(self.v_ind)\n            a_ind_vb = Variable(self.a_ind)\n            if self.use_cuda:\n                v_ind_vb = v_ind_vb.cuda()\n                a_ind_vb = a_ind_vb.cuda()\n            v = x.gather(1, v_ind_vb.expand(x.size(0), self.output_dims))\n            a = x.gather(1, a_ind_vb.expand(x.size(0), self.output_dims))\n            # now calculate Q(s, a)\n            if self.dueling_type == ""avg"":      # Q(s,a)=V(s)+(A(s,a)-avg_a(A(s,a)))\n                # x = v + (a - a.mean(1)).expand(x.size(0), self.output_dims)   # 0.1.12\n                x = v + (a - a.mean(1, keepdim=True))                           # 0.2.0\n            elif self.dueling_type == ""max"":    # Q(s,a)=V(s)+(A(s,a)-max_a(A(s,a)))\n                # x = v + (a - a.max(1)[0]).expand(x.size(0), self.output_dims) # 0.1.12\n                x = v + (a - a.max(1, keepdim=True)[0])                         # 0.2.0\n            elif self.dueling_type == ""naive"":  # Q(s,a)=V(s)+ A(s,a)\n                x = v + a\n            else:\n                assert False, ""dueling_type must be one of {\'avg\', \'max\', \'naive\'}""\n            del v_ind_vb, a_ind_vb, v, a\n            return x\n        else:\n            return self.fc4(x.view(x.size(0), -1))\n'"
core/models/empty.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils.init_weights import init_weights, normalized_columns_initializer\nfrom core.model import Model\n\nclass EmptyModel(Model):\n    def __init__(self, args):\n        super(EmptyModel, self).__init__(args)\n\n        self._reset()\n\n    def _init_weights(self):\n        pass\n\n    def print_model(self):\n        self.logger.warning(""<-----------------------------------> Model"")\n        self.logger.warning(self)\n\n    def _reset(self):           # NOTE: should be called at each child\'s __init__\n        self._init_weights()\n        self.type(self.dtype)   # put on gpu if possible\n        self.print_model()\n\n    def forward(self, input):\n        pass\n'"
