file_path,api_count,code
run_sample.py,0,"b'import argparse\nimport os\n\nfrom misc import pyutils\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n\n    # Environment\n    parser.add_argument(""--num_workers"", default=os.cpu_count()//2, type=int)\n    parser.add_argument(""--voc12_root"", required=True, type=str,\n                        help=""Path to VOC 2012 Devkit, must contain ./JPEGImages as subdirectory."")\n\n    # Dataset\n    parser.add_argument(""--train_list"", default=""voc12/train_aug.txt"", type=str)\n    parser.add_argument(""--val_list"", default=""voc12/val.txt"", type=str)\n    parser.add_argument(""--infer_list"", default=""voc12/train.txt"", type=str,\n                        help=""voc12/train_aug.txt to train a fully supervised model, ""\n                             ""voc12/train.txt or voc12/val.txt to quickly check the quality of the labels."")\n    parser.add_argument(""--chainer_eval_set"", default=""train"", type=str)\n\n    # Class Activation Map\n    parser.add_argument(""--cam_network"", default=""net.resnet50_cam"", type=str)\n    parser.add_argument(""--cam_crop_size"", default=512, type=int)\n    parser.add_argument(""--cam_batch_size"", default=16, type=int)\n    parser.add_argument(""--cam_num_epoches"", default=5, type=int)\n    parser.add_argument(""--cam_learning_rate"", default=0.1, type=float)\n    parser.add_argument(""--cam_weight_decay"", default=1e-4, type=float)\n    parser.add_argument(""--cam_eval_thres"", default=0.15, type=float)\n    parser.add_argument(""--cam_scales"", default=(1.0, 0.5, 1.5, 2.0),\n                        help=""Multi-scale inferences"")\n\n    # Mining Inter-pixel Relations\n    parser.add_argument(""--conf_fg_thres"", default=0.30, type=float)\n    parser.add_argument(""--conf_bg_thres"", default=0.05, type=float)\n\n    # Inter-pixel Relation Network (IRNet)\n    parser.add_argument(""--irn_network"", default=""net.resnet50_irn"", type=str)\n    parser.add_argument(""--irn_crop_size"", default=512, type=int)\n    parser.add_argument(""--irn_batch_size"", default=32, type=int)\n    parser.add_argument(""--irn_num_epoches"", default=3, type=int)\n    parser.add_argument(""--irn_learning_rate"", default=0.1, type=float)\n    parser.add_argument(""--irn_weight_decay"", default=1e-4, type=float)\n\n    # Random Walk Params\n    parser.add_argument(""--beta"", default=10)\n    parser.add_argument(""--exp_times"", default=8,\n                        help=""Hyper-parameter that controls the number of random walk iterations,""\n                             ""The random walk is performed 2^{exp_times}."")\n    parser.add_argument(""--ins_seg_bg_thres"", default=0.25)\n    parser.add_argument(""--sem_seg_bg_thres"", default=0.25)\n\n    # Output Path\n    parser.add_argument(""--log_name"", default=""sample_train_eval"", type=str)\n    parser.add_argument(""--cam_weights_name"", default=""sess/res50_cam.pth"", type=str)\n    parser.add_argument(""--irn_weights_name"", default=""sess/res50_irn.pth"", type=str)\n    parser.add_argument(""--cam_out_dir"", default=""result/cam"", type=str)\n    parser.add_argument(""--ir_label_out_dir"", default=""result/ir_label"", type=str)\n    parser.add_argument(""--sem_seg_out_dir"", default=""result/sem_seg"", type=str)\n    parser.add_argument(""--ins_seg_out_dir"", default=""result/ins_seg"", type=str)\n\n    # Step\n    parser.add_argument(""--train_cam_pass"", default=True)\n    parser.add_argument(""--make_cam_pass"", default=True)\n    parser.add_argument(""--eval_cam_pass"", default=True)\n    parser.add_argument(""--cam_to_ir_label_pass"", default=True)\n    parser.add_argument(""--train_irn_pass"", default=True)\n    parser.add_argument(""--make_ins_seg_pass"", default=True)\n    parser.add_argument(""--eval_ins_seg_pass"", default=True)\n    parser.add_argument(""--make_sem_seg_pass"", default=True)\n    parser.add_argument(""--eval_sem_seg_pass"", default=True)\n\n    args = parser.parse_args()\n\n    os.makedirs(""sess"", exist_ok=True)\n    os.makedirs(args.cam_out_dir, exist_ok=True)\n    os.makedirs(args.ir_label_out_dir, exist_ok=True)\n    os.makedirs(args.sem_seg_out_dir, exist_ok=True)\n    os.makedirs(args.ins_seg_out_dir, exist_ok=True)\n\n    pyutils.Logger(args.log_name + \'.log\')\n    print(vars(args))\n\n    if args.train_cam_pass is True:\n        import step.train_cam\n\n        timer = pyutils.Timer(\'step.train_cam:\')\n        step.train_cam.run(args)\n\n    if args.make_cam_pass is True:\n        import step.make_cam\n\n        timer = pyutils.Timer(\'step.make_cam:\')\n        step.make_cam.run(args)\n\n    if args.eval_cam_pass is True:\n        import step.eval_cam\n\n        timer = pyutils.Timer(\'step.eval_cam:\')\n        step.eval_cam.run(args)\n\n    if args.cam_to_ir_label_pass is True:\n        import step.cam_to_ir_label\n\n        timer = pyutils.Timer(\'step.cam_to_ir_label:\')\n        step.cam_to_ir_label.run(args)\n\n    if args.train_irn_pass is True:\n        import step.train_irn\n\n        timer = pyutils.Timer(\'step.train_irn:\')\n        step.train_irn.run(args)\n\n    if args.make_ins_seg_pass is True:\n        import step.make_ins_seg_labels\n\n        timer = pyutils.Timer(\'step.make_ins_seg_labels:\')\n        step.make_ins_seg_labels.run(args)\n\n    if args.eval_ins_seg_pass is True:\n        import step.eval_ins_seg\n\n        timer = pyutils.Timer(\'step.eval_ins_seg:\')\n        step.eval_ins_seg.run(args)\n\n    if args.make_sem_seg_pass is True:\n        import step.make_sem_seg_labels\n\n        timer = pyutils.Timer(\'step.make_sem_seg_labels:\')\n        step.make_sem_seg_labels.run(args)\n\n    if args.eval_sem_seg_pass is True:\n        import step.eval_sem_seg\n\n        timer = pyutils.Timer(\'step.eval_sem_seg:\')\n        step.eval_sem_seg.run(args)\n\n'"
misc/imutils.py,0,"b""import random\nimport numpy as np\n\nimport pydensecrf.densecrf as dcrf\nfrom pydensecrf.utils import unary_from_labels\nfrom PIL import Image\n\ndef pil_resize(img, size, order):\n    if size[0] == img.shape[0] and size[1] == img.shape[1]:\n        return img\n\n    if order == 3:\n        resample = Image.BICUBIC\n    elif order == 0:\n        resample = Image.NEAREST\n\n    return np.asarray(Image.fromarray(img).resize(size[::-1], resample))\n\ndef pil_rescale(img, scale, order):\n    height, width = img.shape[:2]\n    target_size = (int(np.round(height*scale)), int(np.round(width*scale)))\n    return pil_resize(img, target_size, order)\n\n\ndef random_resize_long(img, min_long, max_long):\n    target_long = random.randint(min_long, max_long)\n    h, w = img.shape[:2]\n\n    if w < h:\n        scale = target_long / h\n    else:\n        scale = target_long / w\n\n    return pil_rescale(img, scale, 3)\n\ndef random_scale(img, scale_range, order):\n\n    target_scale = scale_range[0] + random.random() * (scale_range[1] - scale_range[0])\n\n    if isinstance(img, tuple):\n        return (pil_rescale(img[0], target_scale, order[0]), pil_rescale(img[1], target_scale, order[1]))\n    else:\n        return pil_rescale(img[0], target_scale, order)\n\ndef random_lr_flip(img):\n\n    if bool(random.getrandbits(1)):\n        if isinstance(img, tuple):\n            return [np.fliplr(m) for m in img]\n        else:\n            return np.fliplr(img)\n    else:\n        return img\n\ndef get_random_crop_box(imgsize, cropsize):\n    h, w = imgsize\n\n    ch = min(cropsize, h)\n    cw = min(cropsize, w)\n\n    w_space = w - cropsize\n    h_space = h - cropsize\n\n    if w_space > 0:\n        cont_left = 0\n        img_left = random.randrange(w_space + 1)\n    else:\n        cont_left = random.randrange(-w_space + 1)\n        img_left = 0\n\n    if h_space > 0:\n        cont_top = 0\n        img_top = random.randrange(h_space + 1)\n    else:\n        cont_top = random.randrange(-h_space + 1)\n        img_top = 0\n\n    return cont_top, cont_top+ch, cont_left, cont_left+cw, img_top, img_top+ch, img_left, img_left+cw\n\ndef random_crop(images, cropsize, default_values):\n\n    if isinstance(images, np.ndarray): images = (images,)\n    if isinstance(default_values, int): default_values = (default_values,)\n\n    imgsize = images[0].shape[:2]\n    box = get_random_crop_box(imgsize, cropsize)\n\n    new_images = []\n    for img, f in zip(images, default_values):\n\n        if len(img.shape) == 3:\n            cont = np.ones((cropsize, cropsize, img.shape[2]), img.dtype)*f\n        else:\n            cont = np.ones((cropsize, cropsize), img.dtype)*f\n        cont[box[0]:box[1], box[2]:box[3]] = img[box[4]:box[5], box[6]:box[7]]\n        new_images.append(cont)\n\n    if len(new_images) == 1:\n        new_images = new_images[0]\n\n    return new_images\n\ndef top_left_crop(img, cropsize, default_value):\n\n    h, w = img.shape[:2]\n\n    ch = min(cropsize, h)\n    cw = min(cropsize, w)\n\n    if len(img.shape) == 2:\n        container = np.ones((cropsize, cropsize), img.dtype)*default_value\n    else:\n        container = np.ones((cropsize, cropsize, img.shape[2]), img.dtype)*default_value\n\n    container[:ch, :cw] = img[:ch, :cw]\n\n    return container\n\ndef center_crop(img, cropsize, default_value=0):\n\n    h, w = img.shape[:2]\n\n    ch = min(cropsize, h)\n    cw = min(cropsize, w)\n\n    sh = h - cropsize\n    sw = w - cropsize\n\n    if sw > 0:\n        cont_left = 0\n        img_left = int(round(sw / 2))\n    else:\n        cont_left = int(round(-sw / 2))\n        img_left = 0\n\n    if sh > 0:\n        cont_top = 0\n        img_top = int(round(sh / 2))\n    else:\n        cont_top = int(round(-sh / 2))\n        img_top = 0\n\n    if len(img.shape) == 2:\n        container = np.ones((cropsize, cropsize), img.dtype)*default_value\n    else:\n        container = np.ones((cropsize, cropsize, img.shape[2]), img.dtype)*default_value\n\n    container[cont_top:cont_top+ch, cont_left:cont_left+cw] = \\\n        img[img_top:img_top+ch, img_left:img_left+cw]\n\n    return container\n\ndef HWC_to_CHW(img):\n    return np.transpose(img, (2, 0, 1))\n\ndef crf_inference_label(img, labels, t=10, n_labels=21, gt_prob=0.7):\n\n    h, w = img.shape[:2]\n\n    d = dcrf.DenseCRF2D(w, h, n_labels)\n\n    unary = unary_from_labels(labels, n_labels, gt_prob=gt_prob, zero_unsure=False)\n\n    d.setUnaryEnergy(unary)\n    d.addPairwiseGaussian(sxy=3, compat=3)\n    d.addPairwiseBilateral(sxy=50, srgb=5, rgbim=np.ascontiguousarray(np.copy(img)), compat=10)\n\n    q = d.inference(t)\n\n    return np.argmax(np.array(q).reshape((n_labels, h, w)), axis=0)\n\n\ndef get_strided_size(orig_size, stride):\n    return ((orig_size[0]-1)//stride+1, (orig_size[1]-1)//stride+1)\n\n\ndef get_strided_up_size(orig_size, stride):\n    strided_size = get_strided_size(orig_size, stride)\n    return strided_size[0]*stride, strided_size[1]*stride\n\n\ndef compress_range(arr):\n    uniques = np.unique(arr)\n    maximum = np.max(uniques)\n\n    d = np.zeros(maximum+1, np.int32)\n    d[uniques] = np.arange(uniques.shape[0])\n\n    out = d[arr]\n    return out - np.min(out)\n\n\ndef colorize_score(score_map, exclude_zero=False, normalize=True, by_hue=False):\n    import matplotlib.colors\n    if by_hue:\n        aranged = np.arange(score_map.shape[0]) / (score_map.shape[0])\n        hsv_color = np.stack((aranged, np.ones_like(aranged), np.ones_like(aranged)), axis=-1)\n        rgb_color = matplotlib.colors.hsv_to_rgb(hsv_color)\n\n        test = rgb_color[np.argmax(score_map, axis=0)]\n        test = np.expand_dims(np.max(score_map, axis=0), axis=-1) * test\n\n        if normalize:\n            return test / (np.max(test) + 1e-5)\n        else:\n            return test\n\n    else:\n        VOC_color = np.array([(0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n                     (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0), (192, 128, 0),\n                     (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128), (0, 64, 0), (128, 64, 0),\n                     (0, 192, 0), (128, 192, 0), (0, 64, 128), (255, 255, 255)], np.float32)\n\n        if exclude_zero:\n            VOC_color = VOC_color[1:]\n\n        test = VOC_color[np.argmax(score_map, axis=0)%22]\n        test = np.expand_dims(np.max(score_map, axis=0), axis=-1) * test\n        if normalize:\n            test /= np.max(test) + 1e-5\n\n        return test\n\n\ndef colorize_displacement(disp):\n\n    import matplotlib.colors\n    import math\n\n    a = (np.arctan2(-disp[0], -disp[1]) / math.pi + 1) / 2\n\n    r = np.sqrt(disp[0] ** 2 + disp[1] ** 2)\n    s = r / np.max(r)\n    hsv_color = np.stack((a, s, np.ones_like(a)), axis=-1)\n    rgb_color = matplotlib.colors.hsv_to_rgb(hsv_color)\n\n    return rgb_color\n\n\ndef colorize_label(label_map, normalize=True, by_hue=True, exclude_zero=False, outline=False):\n\n    label_map = label_map.astype(np.uint8)\n\n    if by_hue:\n        import matplotlib.colors\n        sz = np.max(label_map)\n        aranged = np.arange(sz) / sz\n        hsv_color = np.stack((aranged, np.ones_like(aranged), np.ones_like(aranged)), axis=-1)\n        rgb_color = matplotlib.colors.hsv_to_rgb(hsv_color)\n        rgb_color = np.concatenate([np.zeros((1, 3)), rgb_color], axis=0)\n\n        test = rgb_color[label_map]\n    else:\n        VOC_color = np.array([(0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n                              (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0), (192, 128, 0),\n                              (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128), (0, 64, 0), (128, 64, 0),\n                              (0, 192, 0), (128, 192, 0), (0, 64, 128), (255, 255, 255)], np.float32)\n\n        if exclude_zero:\n            VOC_color = VOC_color[1:]\n        test = VOC_color[label_map]\n        if normalize:\n            test /= np.max(test)\n\n    if outline:\n        edge = np.greater(np.sum(np.abs(test[:-1, :-1] - test[1:, :-1]), axis=-1) + np.sum(np.abs(test[:-1, :-1] - test[:-1, 1:]), axis=-1), 0)\n        edge1 = np.pad(edge, ((0, 1), (0, 1)), mode='constant', constant_values=0)\n        edge2 = np.pad(edge, ((1, 0), (1, 0)), mode='constant', constant_values=0)\n        edge = np.repeat(np.expand_dims(np.maximum(edge1, edge2), -1), 3, axis=-1)\n\n        test = np.maximum(test, edge)\n    return test\n"""
misc/indexing.py,17,"b""import torch\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass PathIndex:\n\n    def __init__(self, radius, default_size):\n        self.radius = radius\n        self.radius_floor = int(np.ceil(radius) - 1)\n\n        self.search_paths, self.search_dst = self.get_search_paths_dst(self.radius)\n\n        self.path_indices, self.src_indices, self.dst_indices = self.get_path_indices(default_size)\n\n        return\n\n    def get_search_paths_dst(self, max_radius=5):\n\n        coord_indices_by_length = [[] for _ in range(max_radius * 4)]\n\n        search_dirs = []\n\n        for x in range(1, max_radius):\n            search_dirs.append((0, x))\n\n        for y in range(1, max_radius):\n            for x in range(-max_radius + 1, max_radius):\n                if x * x + y * y < max_radius ** 2:\n                    search_dirs.append((y, x))\n\n        for dir in search_dirs:\n\n            length_sq = dir[0] ** 2 + dir[1] ** 2\n            path_coords = []\n\n            min_y, max_y = sorted((0, dir[0]))\n            min_x, max_x = sorted((0, dir[1]))\n\n            for y in range(min_y, max_y + 1):\n                for x in range(min_x, max_x + 1):\n\n                    dist_sq = (dir[0] * x - dir[1] * y) ** 2 / length_sq\n\n                    if dist_sq < 1:\n                        path_coords.append([y, x])\n\n            path_coords.sort(key=lambda x: -abs(x[0]) - abs(x[1]))\n            path_length = len(path_coords)\n\n            coord_indices_by_length[path_length].append(path_coords)\n\n        path_list_by_length = [np.asarray(v) for v in coord_indices_by_length if v]\n        path_destinations = np.concatenate([p[:, 0] for p in path_list_by_length], axis=0)\n\n        return path_list_by_length, path_destinations\n\n    def get_path_indices(self, size):\n\n        full_indices = np.reshape(np.arange(0, size[0] * size[1], dtype=np.int64), (size[0], size[1]))\n\n        cropped_height = size[0] - self.radius_floor\n        cropped_width = size[1] - 2 * self.radius_floor\n\n        path_indices = []\n\n        for paths in self.search_paths:\n\n            path_indices_list = []\n            for p in paths:\n\n                coord_indices_list = []\n\n                for dy, dx in p:\n                    coord_indices = full_indices[dy:dy + cropped_height,\n                                    self.radius_floor + dx:self.radius_floor + dx + cropped_width]\n                    coord_indices = np.reshape(coord_indices, [-1])\n\n                    coord_indices_list.append(coord_indices)\n\n                path_indices_list.append(coord_indices_list)\n\n            path_indices.append(np.array(path_indices_list))\n\n        src_indices = np.reshape(full_indices[:cropped_height, self.radius_floor:self.radius_floor + cropped_width], -1)\n        dst_indices = np.concatenate([p[:,0] for p in path_indices], axis=0)\n\n        return path_indices, src_indices, dst_indices\n\n\ndef edge_to_affinity(edge, paths_indices):\n\n    aff_list = []\n    edge = edge.view(edge.size(0), -1)\n\n    for i in range(len(paths_indices)):\n        if isinstance(paths_indices[i], np.ndarray):\n            paths_indices[i] = torch.from_numpy(paths_indices[i])\n        paths_indices[i] = paths_indices[i].cuda(non_blocking=True)\n\n    for ind in paths_indices:\n        ind_flat = ind.view(-1)\n        dist = torch.index_select(edge, dim=-1, index=ind_flat)\n        dist = dist.view(dist.size(0), ind.size(0), ind.size(1), ind.size(2))\n        aff = torch.squeeze(1 - F.max_pool2d(dist, (dist.size(2), 1)), dim=2)\n        aff_list.append(aff)\n    aff_cat = torch.cat(aff_list, dim=1)\n\n    return aff_cat\n\n\ndef affinity_sparse2dense(affinity_sparse, ind_from, ind_to, n_vertices):\n\n    ind_from = torch.from_numpy(ind_from)\n    ind_to = torch.from_numpy(ind_to)\n\n    affinity_sparse = affinity_sparse.view(-1).cpu()\n    ind_from = ind_from.repeat(ind_to.size(0)).view(-1)\n    ind_to = ind_to.view(-1)\n\n    indices = torch.stack([ind_from, ind_to])\n    indices_tp = torch.stack([ind_to, ind_from])\n\n    indices_id = torch.stack([torch.arange(0, n_vertices).long(), torch.arange(0, n_vertices).long()])\n\n    affinity_dense = torch.sparse.FloatTensor(torch.cat([indices, indices_id, indices_tp], dim=1),\n                                       torch.cat([affinity_sparse, torch.ones([n_vertices]), affinity_sparse])).to_dense().cuda()\n\n    return affinity_dense\n\n\ndef to_transition_matrix(affinity_dense, beta, times):\n    scaled_affinity = torch.pow(affinity_dense, beta)\n\n    trans_mat = scaled_affinity / torch.sum(scaled_affinity, dim=0, keepdim=True)\n    for _ in range(times):\n        trans_mat = torch.matmul(trans_mat, trans_mat)\n\n    return trans_mat\n\ndef propagate_to_edge(x, edge, radius=5, beta=10, exp_times=8):\n\n    height, width = x.shape[-2:]\n\n    hor_padded = width+radius*2\n    ver_padded = height+radius\n\n    path_index = PathIndex(radius=radius, default_size=(ver_padded, hor_padded))\n\n    edge_padded = F.pad(edge, (radius, radius, 0, radius), mode='constant', value=1.0)\n    sparse_aff = edge_to_affinity(torch.unsqueeze(edge_padded, 0),\n                                  path_index.path_indices)\n\n    dense_aff = affinity_sparse2dense(sparse_aff, path_index.src_indices,\n                                      path_index.dst_indices, ver_padded * hor_padded)\n    dense_aff = dense_aff.view(ver_padded, hor_padded, ver_padded, hor_padded)\n    dense_aff = dense_aff[:-radius, radius:-radius, :-radius, radius:-radius]\n    dense_aff = dense_aff.reshape(height * width, height * width)\n\n    trans_mat = to_transition_matrix(dense_aff, beta=beta, times=exp_times)\n\n    x = x.view(-1, height, width) * (1 - edge)\n\n    rw = torch.matmul(x.view(-1, height * width), trans_mat)\n    rw = rw.view(rw.size(0), 1, height, width)\n\n    return rw"""
misc/pyutils.py,0,"b'\nimport numpy as np\nimport time\nimport sys\n\nclass Logger(object):\n    def __init__(self, outfile):\n        self.terminal = sys.stdout\n        self.log = open(outfile, ""w"")\n        sys.stdout = self\n\n    def write(self, message):\n        self.terminal.write(message)\n        self.log.write(message)\n\n    def flush(self):\n        self.terminal.flush()\n\n\nclass AverageMeter:\n    def __init__(self, *keys):\n        self.__data = dict()\n        for k in keys:\n            self.__data[k] = [0.0, 0]\n\n    def add(self, dict):\n        for k, v in dict.items():\n            if k not in self.__data:\n                self.__data[k] = [0.0, 0]\n            self.__data[k][0] += v\n            self.__data[k][1] += 1\n\n    def get(self, *keys):\n        if len(keys) == 1:\n            return self.__data[keys[0]][0] / self.__data[keys[0]][1]\n        else:\n            v_list = [self.__data[k][0] / self.__data[k][1] for k in keys]\n            return tuple(v_list)\n\n    def pop(self, key=None):\n        if key is None:\n            for k in self.__data.keys():\n                self.__data[k] = [0.0, 0]\n        else:\n            v = self.get(key)\n            self.__data[key] = [0.0, 0]\n            return v\n\n\nclass Timer:\n    def __init__(self, starting_msg = None):\n        self.start = time.time()\n        self.stage_start = self.start\n\n        if starting_msg is not None:\n            print(starting_msg, time.ctime(time.time()))\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        return\n\n    def update_progress(self, progress):\n        self.elapsed = time.time() - self.start\n        self.est_total = self.elapsed / progress\n        self.est_remaining = self.est_total - self.elapsed\n        self.est_finish = int(self.start + self.est_total)\n\n\n    def str_estimated_complete(self):\n        return str(time.ctime(self.est_finish))\n\n    def get_stage_elapsed(self):\n        return time.time() - self.stage_start\n\n    def reset_stage(self):\n        self.stage_start = time.time()\n\n    def lapse(self):\n        out = time.time() - self.stage_start\n        self.stage_start = time.time()\n        return out\n\n\ndef to_one_hot(sparse_integers, maximum_val=None, dtype=np.bool):\n\n    if maximum_val is None:\n        maximum_val = np.max(sparse_integers) + 1\n\n    src_shape = sparse_integers.shape\n\n    flat_src = np.reshape(sparse_integers, [-1])\n    src_size = flat_src.shape[0]\n\n    one_hot = np.zeros((maximum_val, src_size), dtype)\n    one_hot[flat_src, np.arange(src_size)] = 1\n\n    one_hot = np.reshape(one_hot, [maximum_val] + list(src_shape))\n\n    return one_hot\n'"
misc/torchutils.py,4,"b""\nimport torch\n\nfrom torch.utils.data import Subset\nimport numpy as np\nimport math\n\n\nclass PolyOptimizer(torch.optim.SGD):\n\n    def __init__(self, params, lr, weight_decay, max_step, momentum=0.9):\n        super().__init__(params, lr, weight_decay)\n\n        self.global_step = 0\n        self.max_step = max_step\n        self.momentum = momentum\n\n        self.__initial_lr = [group['lr'] for group in self.param_groups]\n\n\n    def step(self, closure=None):\n\n        if self.global_step < self.max_step:\n            lr_mult = (1 - self.global_step / self.max_step) ** self.momentum\n\n            for i in range(len(self.param_groups)):\n                self.param_groups[i]['lr'] = self.__initial_lr[i] * lr_mult\n\n        super().step(closure)\n\n        self.global_step += 1\n\nclass SGDROptimizer(torch.optim.SGD):\n\n    def __init__(self, params, steps_per_epoch, lr=0, weight_decay=0, epoch_start=1, restart_mult=2):\n        super().__init__(params, lr, weight_decay)\n\n        self.global_step = 0\n        self.local_step = 0\n        self.total_restart = 0\n\n        self.max_step = steps_per_epoch * epoch_start\n        self.restart_mult = restart_mult\n\n        self.__initial_lr = [group['lr'] for group in self.param_groups]\n\n\n    def step(self, closure=None):\n\n        if self.local_step >= self.max_step:\n            self.local_step = 0\n            self.max_step *= self.restart_mult\n            self.total_restart += 1\n\n        lr_mult = (1 + math.cos(math.pi * self.local_step / self.max_step))/2 / (self.total_restart + 1)\n\n        for i in range(len(self.param_groups)):\n            self.param_groups[i]['lr'] = self.__initial_lr[i] * lr_mult\n\n        super().step(closure)\n\n        self.local_step += 1\n        self.global_step += 1\n\n\ndef split_dataset(dataset, n_splits):\n\n    return [Subset(dataset, np.arange(i, len(dataset), n_splits)) for i in range(n_splits)]\n\n\ndef gap2d(x, keepdims=False):\n    out = torch.mean(x.view(x.size(0), x.size(1), -1), -1)\n    if keepdims:\n        out = out.view(out.size(0), out.size(1), 1, 1)\n\n    return out\n"""
net/resnet50.py,4,"b""import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n\nmodel_urls = {\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth'\n}\n\n\nclass FixedBatchNorm(nn.BatchNorm2d):\n    def forward(self, input):\n        return F.batch_norm(input, self.running_mean, self.running_var, self.weight, self.bias,\n                            training=False, eps=self.eps)\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = FixedBatchNorm(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=dilation, bias=False, dilation=dilation)\n        self.bn2 = FixedBatchNorm(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = FixedBatchNorm(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, strides=(2, 2, 2, 2), dilations=(1, 1, 1, 1)):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=strides[0], padding=3,\n                               bias=False)\n        self.bn1 = FixedBatchNorm(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1, dilation=dilations[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=strides[1], dilation=dilations[1])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=strides[2], dilation=dilations[2])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=strides[3], dilation=dilations[3])\n        self.inplanes = 1024\n\n        #self.avgpool = nn.AvgPool2d(7, stride=1)\n        #self.fc = nn.Linear(512 * block.expansion, 1000)\n\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                FixedBatchNorm(planes * block.expansion),\n            )\n\n        layers = [block(self.inplanes, planes, stride, downsample, dilation=1)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet50(pretrained=True, **kwargs):\n\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        state_dict = model_zoo.load_url(model_urls['resnet50'])\n        state_dict.pop('fc.weight')\n        state_dict.pop('fc.bias')\n        model.load_state_dict(state_dict)\n    return model"""
net/resnet50_cam.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom misc import torchutils\nfrom net import resnet50\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.resnet50 = resnet50.resnet50(pretrained=True, strides=(2, 2, 2, 1))\n\n        self.stage1 = nn.Sequential(self.resnet50.conv1, self.resnet50.bn1, self.resnet50.relu, self.resnet50.maxpool,\n                                    self.resnet50.layer1)\n        self.stage2 = nn.Sequential(self.resnet50.layer2)\n        self.stage3 = nn.Sequential(self.resnet50.layer3)\n        self.stage4 = nn.Sequential(self.resnet50.layer4)\n\n        self.classifier = nn.Conv2d(2048, 20, 1, bias=False)\n\n        self.backbone = nn.ModuleList([self.stage1, self.stage2, self.stage3, self.stage4])\n        self.newly_added = nn.ModuleList([self.classifier])\n\n    def forward(self, x):\n\n        x = self.stage1(x)\n        x = self.stage2(x).detach()\n\n        x = self.stage3(x)\n        x = self.stage4(x)\n\n        x = torchutils.gap2d(x, keepdims=True)\n        x = self.classifier(x)\n        x = x.view(-1, 20)\n\n        return x\n\n    def train(self, mode=True):\n        for p in self.resnet50.conv1.parameters():\n            p.requires_grad = False\n        for p in self.resnet50.bn1.parameters():\n            p.requires_grad = False\n\n    def trainable_parameters(self):\n\n        return (list(self.backbone.parameters()), list(self.newly_added.parameters()))\n\n\nclass CAM(Net):\n\n    def __init__(self):\n        super(CAM, self).__init__()\n\n    def forward(self, x):\n\n        x = self.stage1(x)\n\n        x = self.stage2(x)\n\n        x = self.stage3(x)\n\n        x = self.stage4(x)\n\n        x = F.conv2d(x, self.classifier.weight)\n        x = F.relu(x)\n\n        x = x[0] + x[1].flip(-1)\n\n        return x\n'"
net/resnet50_irn.py,19,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom net import resnet50\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n\n        # backbone\n        self.resnet50 = resnet50.resnet50(pretrained=True, strides=[2, 2, 2, 1])\n\n        self.stage1 = nn.Sequential(self.resnet50.conv1, self.resnet50.bn1, self.resnet50.relu, self.resnet50.maxpool)\n        self.stage2 = nn.Sequential(self.resnet50.layer1)\n        self.stage3 = nn.Sequential(self.resnet50.layer2)\n        self.stage4 = nn.Sequential(self.resnet50.layer3)\n        self.stage5 = nn.Sequential(self.resnet50.layer4)\n        self.mean_shift = Net.MeanShift(2)\n\n        # branch: class boundary detection\n        self.fc_edge1 = nn.Sequential(\n            nn.Conv2d(64, 32, 1, bias=False),\n            nn.GroupNorm(4, 32),\n            nn.ReLU(inplace=True),\n        )\n        self.fc_edge2 = nn.Sequential(\n            nn.Conv2d(256, 32, 1, bias=False),\n            nn.GroupNorm(4, 32),\n            nn.ReLU(inplace=True),\n        )\n        self.fc_edge3 = nn.Sequential(\n            nn.Conv2d(512, 32, 1, bias=False),\n            nn.GroupNorm(4, 32),\n            nn.Upsample(scale_factor=2, mode=\'bilinear\', align_corners=False),\n            nn.ReLU(inplace=True),\n        )\n        self.fc_edge4 = nn.Sequential(\n            nn.Conv2d(1024, 32, 1, bias=False),\n            nn.GroupNorm(4, 32),\n            nn.Upsample(scale_factor=4, mode=\'bilinear\', align_corners=False),\n            nn.ReLU(inplace=True),\n        )\n        self.fc_edge5 = nn.Sequential(\n            nn.Conv2d(2048, 32, 1, bias=False),\n            nn.GroupNorm(4, 32),\n            nn.Upsample(scale_factor=4, mode=\'bilinear\', align_corners=False),\n            nn.ReLU(inplace=True),\n        )\n        self.fc_edge6 = nn.Conv2d(160, 1, 1, bias=True)\n\n        # branch: displacement field\n        self.fc_dp1 = nn.Sequential(\n            nn.Conv2d(64, 64, 1, bias=False),\n            nn.GroupNorm(8, 64),\n            nn.ReLU(inplace=True),\n        )\n        self.fc_dp2 = nn.Sequential(\n            nn.Conv2d(256, 128, 1, bias=False),\n            nn.GroupNorm(16, 128),\n            nn.ReLU(inplace=True),\n        )\n        self.fc_dp3 = nn.Sequential(\n            nn.Conv2d(512, 256, 1, bias=False),\n            nn.GroupNorm(16, 256),\n            nn.ReLU(inplace=True),\n        )\n        self.fc_dp4 = nn.Sequential(\n            nn.Conv2d(1024, 256, 1, bias=False),\n            nn.GroupNorm(16, 256),\n            nn.Upsample(scale_factor=2, mode=\'bilinear\', align_corners=False),\n            nn.ReLU(inplace=True),\n        )\n        self.fc_dp5 = nn.Sequential(\n            nn.Conv2d(2048, 256, 1, bias=False),\n            nn.GroupNorm(16, 256),\n            nn.Upsample(scale_factor=2, mode=\'bilinear\', align_corners=False),\n            nn.ReLU(inplace=True),\n        )\n        self.fc_dp6 = nn.Sequential(\n            nn.Conv2d(768, 256, 1, bias=False),\n            nn.GroupNorm(16, 256),\n            nn.Upsample(scale_factor=2, mode=\'bilinear\', align_corners=False),\n            nn.ReLU(inplace=True),\n        )\n        self.fc_dp7 = nn.Sequential(\n            nn.Conv2d(448, 256, 1, bias=False),\n            nn.GroupNorm(16, 256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 2, 1, bias=False),\n            self.mean_shift\n        )\n\n        self.backbone = nn.ModuleList([self.stage1, self.stage2, self.stage3, self.stage4, self.stage5])\n        self.edge_layers = nn.ModuleList([self.fc_edge1, self.fc_edge2, self.fc_edge3, self.fc_edge4, self.fc_edge5, self.fc_edge6])\n        self.dp_layers = nn.ModuleList([self.fc_dp1, self.fc_dp2, self.fc_dp3, self.fc_dp4, self.fc_dp5, self.fc_dp6, self.fc_dp7])\n\n    class MeanShift(nn.Module):\n\n        def __init__(self, num_features):\n            super(Net.MeanShift, self).__init__()\n            self.register_buffer(\'running_mean\', torch.zeros(num_features))\n\n        def forward(self, input):\n            if self.training:\n                return input\n            return input - self.running_mean.view(1, 2, 1, 1)\n\n    def forward(self, x):\n        x1 = self.stage1(x).detach()\n        x2 = self.stage2(x1).detach()\n        x3 = self.stage3(x2).detach()\n        x4 = self.stage4(x3).detach()\n        x5 = self.stage5(x4).detach()\n\n        edge1 = self.fc_edge1(x1)\n        edge2 = self.fc_edge2(x2)\n        edge3 = self.fc_edge3(x3)[..., :edge2.size(2), :edge2.size(3)]\n        edge4 = self.fc_edge4(x4)[..., :edge2.size(2), :edge2.size(3)]\n        edge5 = self.fc_edge5(x5)[..., :edge2.size(2), :edge2.size(3)]\n        edge_out = self.fc_edge6(torch.cat([edge1, edge2, edge3, edge4, edge5], dim=1))\n\n        dp1 = self.fc_dp1(x1)\n        dp2 = self.fc_dp2(x2)\n        dp3 = self.fc_dp3(x3)\n        dp4 = self.fc_dp4(x4)[..., :dp3.size(2), :dp3.size(3)]\n        dp5 = self.fc_dp5(x5)[..., :dp3.size(2), :dp3.size(3)]\n\n        dp_up3 = self.fc_dp6(torch.cat([dp3, dp4, dp5], dim=1))[..., :dp2.size(2), :dp2.size(3)]\n        dp_out = self.fc_dp7(torch.cat([dp1, dp2, dp_up3], dim=1))\n\n        return edge_out, dp_out\n\n    def trainable_parameters(self):\n        return (tuple(self.edge_layers.parameters()),\n                tuple(self.dp_layers.parameters()))\n\n    def train(self, mode=True):\n        super().train(mode)\n        self.backbone.eval()\n\n\nclass AffinityDisplacementLoss(Net):\n\n    path_indices_prefix = ""path_indices""\n\n    def __init__(self, path_index):\n\n        super(AffinityDisplacementLoss, self).__init__()\n\n        self.path_index = path_index\n\n        self.n_path_lengths = len(path_index.path_indices)\n        for i, pi in enumerate(path_index.path_indices):\n            self.register_buffer(AffinityDisplacementLoss.path_indices_prefix + str(i), torch.from_numpy(pi))\n\n        self.register_buffer(\n            \'disp_target\',\n            torch.unsqueeze(torch.unsqueeze(torch.from_numpy(path_index.search_dst).transpose(1, 0), 0), -1).float())\n\n    def to_affinity(self, edge):\n        aff_list = []\n        edge = edge.view(edge.size(0), -1)\n\n        for i in range(self.n_path_lengths):\n            ind = self._buffers[AffinityDisplacementLoss.path_indices_prefix + str(i)]\n            ind_flat = ind.view(-1)\n            dist = torch.index_select(edge, dim=-1, index=ind_flat)\n            dist = dist.view(dist.size(0), ind.size(0), ind.size(1), ind.size(2))\n            aff = torch.squeeze(1 - F.max_pool2d(dist, (dist.size(2), 1)), dim=2)\n            aff_list.append(aff)\n        aff_cat = torch.cat(aff_list, dim=1)\n\n        return aff_cat\n\n    def to_pair_displacement(self, disp):\n        height, width = disp.size(2), disp.size(3)\n        radius_floor = self.path_index.radius_floor\n\n        cropped_height = height - radius_floor\n        cropped_width = width - 2 * radius_floor\n\n        disp_src = disp[:, :, :cropped_height, radius_floor:radius_floor + cropped_width]\n\n        disp_dst = [disp[:, :, dy:dy + cropped_height, radius_floor + dx:radius_floor + dx + cropped_width]\n                       for dy, dx in self.path_index.search_dst]\n        disp_dst = torch.stack(disp_dst, 2)\n\n        pair_disp = torch.unsqueeze(disp_src, 2) - disp_dst\n        pair_disp = pair_disp.view(pair_disp.size(0), pair_disp.size(1), pair_disp.size(2), -1)\n\n        return pair_disp\n\n    def to_displacement_loss(self, pair_disp):\n        return torch.abs(pair_disp - self.disp_target)\n\n    def forward(self, *inputs):\n        x, return_loss = inputs\n        edge_out, dp_out = super().forward(x)\n\n        if return_loss is False:\n            return edge_out, dp_out\n\n        aff = self.to_affinity(torch.sigmoid(edge_out))\n        pos_aff_loss = (-1) * torch.log(aff + 1e-5)\n        neg_aff_loss = (-1) * torch.log(1. + 1e-5 - aff)\n\n        pair_disp = self.to_pair_displacement(dp_out)\n        dp_fg_loss = self.to_displacement_loss(pair_disp)\n        dp_bg_loss = torch.abs(pair_disp)\n\n        return pos_aff_loss, neg_aff_loss, dp_fg_loss, dp_bg_loss\n\n\nclass EdgeDisplacement(Net):\n\n    def __init__(self, crop_size=512, stride=4):\n        super(EdgeDisplacement, self).__init__()\n        self.crop_size = crop_size\n        self.stride = stride\n\n    def forward(self, x):\n        feat_size = (x.size(2)-1)//self.stride+1, (x.size(3)-1)//self.stride+1\n\n        x = F.pad(x, [0, self.crop_size-x.size(3), 0, self.crop_size-x.size(2)])\n        edge_out, dp_out = super().forward(x)\n        edge_out = edge_out[..., :feat_size[0], :feat_size[1]]\n        dp_out = dp_out[..., :feat_size[0], :feat_size[1]]\n\n        edge_out = torch.sigmoid(edge_out[0]/2 + edge_out[1].flip(-1)/2)\n        dp_out = dp_out[0]\n\n        return edge_out, dp_out\n\n\n'"
step/cam_to_ir_label.py,1,"b'\nimport os\nimport numpy as np\nimport imageio\n\nfrom torch import multiprocessing\nfrom torch.utils.data import DataLoader\n\nimport voc12.dataloader\nfrom misc import torchutils, imutils\n\n\ndef _work(process_id, infer_dataset, args):\n\n    databin = infer_dataset[process_id]\n    infer_data_loader = DataLoader(databin, shuffle=False, num_workers=0, pin_memory=False)\n\n    for iter, pack in enumerate(infer_data_loader):\n        img_name = voc12.dataloader.decode_int_filename(pack[\'name\'][0])\n        img = pack[\'img\'][0].numpy()\n        cam_dict = np.load(os.path.join(args.cam_out_dir, img_name + \'.npy\'), allow_pickle=True).item()\n\n        cams = cam_dict[\'high_res\']\n        keys = np.pad(cam_dict[\'keys\'] + 1, (1, 0), mode=\'constant\')\n\n        # 1. find confident fg & bg\n        fg_conf_cam = np.pad(cams, ((1, 0), (0, 0), (0, 0)), mode=\'constant\', constant_values=args.conf_fg_thres)\n        fg_conf_cam = np.argmax(fg_conf_cam, axis=0)\n        pred = imutils.crf_inference_label(img, fg_conf_cam, n_labels=keys.shape[0])\n        fg_conf = keys[pred]\n\n        bg_conf_cam = np.pad(cams, ((1, 0), (0, 0), (0, 0)), mode=\'constant\', constant_values=args.conf_bg_thres)\n        bg_conf_cam = np.argmax(bg_conf_cam, axis=0)\n        pred = imutils.crf_inference_label(img, bg_conf_cam, n_labels=keys.shape[0])\n        bg_conf = keys[pred]\n\n        # 2. combine confident fg & bg\n        conf = fg_conf.copy()\n        conf[fg_conf == 0] = 255\n        conf[bg_conf + fg_conf == 0] = 0\n\n        imageio.imwrite(os.path.join(args.ir_label_out_dir, img_name + \'.png\'),\n                        conf.astype(np.uint8))\n\n\n        if process_id == args.num_workers - 1 and iter % (len(databin) // 20) == 0:\n            print(""%d "" % ((5 * iter + 1) // (len(databin) // 20)), end=\'\')\n\ndef run(args):\n    dataset = voc12.dataloader.VOC12ImageDataset(args.train_list, voc12_root=args.voc12_root, img_normal=None, to_torch=False)\n    dataset = torchutils.split_dataset(dataset, args.num_workers)\n\n    print(\'[ \', end=\'\')\n    multiprocessing.spawn(_work, nprocs=args.num_workers, args=(dataset, args), join=True)\n    print(\']\')\n'"
step/eval_cam.py,0,"b""\nimport numpy as np\nimport os\nfrom chainercv.datasets import VOCSemanticSegmentationDataset\nfrom chainercv.evaluations import calc_semantic_segmentation_confusion\n\ndef run(args):\n    dataset = VOCSemanticSegmentationDataset(split=args.chainer_eval_set, data_dir=args.voc12_root)\n    labels = [dataset.get_example_by_keys(i, (1,))[0] for i in range(len(dataset))]\n\n    preds = []\n    for id in dataset.ids:\n        cam_dict = np.load(os.path.join(args.cam_out_dir, id + '.npy'), allow_pickle=True).item()\n        cams = cam_dict['high_res']\n        cams = np.pad(cams, ((1, 0), (0, 0), (0, 0)), mode='constant', constant_values=args.cam_eval_thres)\n        keys = np.pad(cam_dict['keys'] + 1, (1, 0), mode='constant')\n        cls_labels = np.argmax(cams, axis=0)\n        cls_labels = keys[cls_labels]\n        preds.append(cls_labels.copy())\n\n    confusion = calc_semantic_segmentation_confusion(preds, labels)\n\n    gtj = confusion.sum(axis=1)\n    resj = confusion.sum(axis=0)\n    gtjresj = np.diag(confusion)\n    denominator = gtj + resj - gtjresj\n    iou = gtjresj / denominator\n\n    print({'iou': iou, 'miou': np.nanmean(iou)})\n"""
step/eval_ins_seg.py,0,"b""\nimport numpy as np\nimport os\n\nimport chainercv\nfrom chainercv.datasets import VOCInstanceSegmentationDataset\n\ndef run(args):\n    dataset = VOCInstanceSegmentationDataset(split=args.chainer_eval_set, data_dir=args.voc12_root)\n    gt_masks = [dataset.get_example_by_keys(i, (1,))[0] for i in range(len(dataset))]\n    gt_labels = [dataset.get_example_by_keys(i, (2,))[0] for i in range(len(dataset))]\n\n    pred_class = []\n    pred_mask = []\n    pred_score = []\n    for id in dataset.ids:\n        ins_out = np.load(os.path.join(args.ins_seg_out_dir, id + '.npy'), allow_pickle=True).item()\n        pred_class.append(ins_out['class'])\n        pred_mask.append(ins_out['mask'])\n        pred_score.append(ins_out['score'])\n\n    print('0.5iou:', chainercv.evaluations.eval_instance_segmentation_voc(pred_mask, pred_class, pred_score,\n                                                         gt_masks, gt_labels, iou_thresh=0.5))"""
step/eval_sem_seg.py,0,"b""\nimport numpy as np\nimport os\nfrom chainercv.datasets import VOCSemanticSegmentationDataset\nfrom chainercv.evaluations import calc_semantic_segmentation_confusion\nimport imageio\n\ndef run(args):\n    dataset = VOCSemanticSegmentationDataset(split=args.chainer_eval_set, data_dir=args.voc12_root)\n    labels = [dataset.get_example_by_keys(i, (1,))[0] for i in range(len(dataset))]\n\n    preds = []\n    for id in dataset.ids:\n        cls_labels = imageio.imread(os.path.join(args.sem_seg_out_dir, id + '.png')).astype(np.uint8)\n        cls_labels[cls_labels == 255] = 0\n        preds.append(cls_labels.copy())\n\n    confusion = calc_semantic_segmentation_confusion(preds, labels)[:21, :21]\n\n    gtj = confusion.sum(axis=1)\n    resj = confusion.sum(axis=0)\n    gtjresj = np.diag(confusion)\n    denominator = gtj + resj - gtjresj\n    fp = 1. - gtj / denominator\n    fn = 1. - resj / denominator\n    iou = gtjresj / denominator\n\n    print(fp[0], fn[0])\n    print(np.mean(fp[1:]), np.mean(fn[1:]))\n\n    print({'iou': iou, 'miou': np.nanmean(iou)})\n"""
step/make_cam.py,13,"b'import torch\nfrom torch import multiprocessing, cuda\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch.backends import cudnn\n\nimport numpy as np\nimport importlib\nimport os\n\nimport voc12.dataloader\nfrom misc import torchutils, imutils\n\ncudnn.enabled = True\n\ndef _work(process_id, model, dataset, args):\n\n    databin = dataset[process_id]\n    n_gpus = torch.cuda.device_count()\n    data_loader = DataLoader(databin, shuffle=False, num_workers=args.num_workers // n_gpus, pin_memory=False)\n\n    with torch.no_grad(), cuda.device(process_id):\n\n        model.cuda()\n\n        for iter, pack in enumerate(data_loader):\n\n            img_name = pack[\'name\'][0]\n            label = pack[\'label\'][0]\n            size = pack[\'size\']\n\n            strided_size = imutils.get_strided_size(size, 4)\n            strided_up_size = imutils.get_strided_up_size(size, 16)\n\n            outputs = [model(img[0].cuda(non_blocking=True))\n                       for img in pack[\'img\']]\n\n            strided_cam = torch.sum(torch.stack(\n                [F.interpolate(torch.unsqueeze(o, 0), strided_size, mode=\'bilinear\', align_corners=False)[0] for o\n                 in outputs]), 0)\n\n            highres_cam = [F.interpolate(torch.unsqueeze(o, 1), strided_up_size,\n                                         mode=\'bilinear\', align_corners=False) for o in outputs]\n            highres_cam = torch.sum(torch.stack(highres_cam, 0), 0)[:, 0, :size[0], :size[1]]\n\n            valid_cat = torch.nonzero(label)[:, 0]\n\n            strided_cam = strided_cam[valid_cat]\n            strided_cam /= F.adaptive_max_pool2d(strided_cam, (1, 1)) + 1e-5\n\n            highres_cam = highres_cam[valid_cat]\n            highres_cam /= F.adaptive_max_pool2d(highres_cam, (1, 1)) + 1e-5\n\n            # save cams\n            np.save(os.path.join(args.cam_out_dir, img_name + \'.npy\'),\n                    {""keys"": valid_cat, ""cam"": strided_cam.cpu(), ""high_res"": highres_cam.cpu().numpy()})\n\n            if process_id == n_gpus - 1 and iter % (len(databin) // 20) == 0:\n                print(""%d "" % ((5*iter+1)//(len(databin) // 20)), end=\'\')\n\n\ndef run(args):\n    model = getattr(importlib.import_module(args.cam_network), \'CAM\')()\n    model.load_state_dict(torch.load(args.cam_weights_name + \'.pth\'), strict=True)\n    model.eval()\n\n    n_gpus = torch.cuda.device_count()\n\n    dataset = voc12.dataloader.VOC12ClassificationDatasetMSF(args.train_list,\n                                                             voc12_root=args.voc12_root, scales=args.cam_scales)\n    dataset = torchutils.split_dataset(dataset, n_gpus)\n\n    print(\'[ \', end=\'\')\n    multiprocessing.spawn(_work, nprocs=n_gpus, args=(model, dataset, args), join=True)\n    print(\']\')\n\n    torch.cuda.empty_cache()'"
step/make_cocoann.py,1,"b'import numpy as np\nimport voc12.dataloader\nfrom torch.utils.data import DataLoader\nfrom pycococreatortools import pycococreatortools\nimport os\nimport json\n\nVOC2012_JSON_FOLDER = """"\n\ndef run(args):\n\n    infer_dataset = voc12.dataloader.VOC12ImageDataset(args.infer_list, voc12_root=args.voc12_root)\n\n    infer_data_loader = DataLoader(infer_dataset, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n\n    val_json = json.load(open(os.path.join(VOC2012_JSON_FOLDER, \'pascal_val2012.json\')))\n    # Do not use this file for evaluation!\n\n    coco_output = {}\n    coco_output[""images""] = []\n    coco_output[""annotations""] = []\n    coco_output[\'categories\'] = val_json[\'categories\']\n    coco_output[\'type\'] = val_json[\'type\']\n\n    for iter, pack in enumerate(infer_data_loader):\n\n        img_name = pack[\'name\'][0]\n        img_id = int(img_name[:4] + img_name[5:])\n        img_size = pack[\'img\'].shape[2:]\n\n        image_info = pycococreatortools.create_image_info(\n            img_id, img_name + "".jpg"", (img_size[1], img_size[0]))\n        coco_output[""images""].append(image_info)\n        ann = np.load(os.path.join(args.ins_seg_out_dir, img_name) + \'.npy\', allow_pickle=True).item()\n\n        instance_id = 1\n\n        for score, mask, class_id in zip( ann[\'score\'], ann[\'mask\'], ann[\'class\']):\n            if score < 1e-5:\n                continue\n            category_info = {\'id\': class_id, \'is_crowd\': False}\n\n            annotation_info = pycococreatortools.create_annotation_info(\n                instance_id, img_id, category_info, mask, img_size[::-1], tolerance=0)\n            instance_id += 1\n            coco_output[\'annotations\'].append(annotation_info)\n\n    with open(\'voc2012_train_custom.json\', \'w\') as outfile:\n        json.dump(coco_output, outfile)\n\n\n'"
step/make_ins_seg_labels.py,11,"b'import torch\nfrom torch import multiprocessing, cuda\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch.backends import cudnn\n\nimport numpy as np\nimport importlib\nimport os\n\nimport skimage\nimport voc12.dataloader\nfrom misc import torchutils, imutils, pyutils, indexing\n\ncudnn.enabled = True\n\n\ndef find_centroids_with_refinement(displacement, iterations=300):\n    # iteration: the number of refinement steps (u), set to any integer >= 100.\n\n    height, width = displacement.shape[1:3]\n\n    # 1. initialize centroids as their coordinates\n    centroid_y = np.repeat(np.expand_dims(np.arange(height), 1), width, axis=1).astype(np.float32)\n    centroid_x = np.repeat(np.expand_dims(np.arange(width), 0), height, axis=0).astype(np.float32)\n\n    for i in range(iterations):\n\n        # 2. find numbers after the decimals\n        uy = np.ceil(centroid_y).astype(np.int32)\n        dy = np.floor(centroid_y).astype(np.int32)\n        y_c = centroid_y - dy\n\n        ux = np.ceil(centroid_x).astype(np.int32)\n        dx = np.floor(centroid_x).astype(np.int32)\n        x_c = centroid_x - dx\n\n        # 3. move centroids\n        centroid_y += displacement[0][uy, ux] * y_c * x_c + \\\n                      displacement[0][dy, ux] *(1 - y_c) * x_c + \\\n                      displacement[0][uy, dx] * y_c * (1 - x_c) + \\\n                      displacement[0][dy, dx] * (1 - y_c) * (1 - x_c)\n\n        centroid_x += displacement[1][uy, ux] * y_c * x_c + \\\n                      displacement[1][dy, ux] *(1 - y_c) * x_c + \\\n                      displacement[1][uy, dx] * y_c * (1 - x_c) + \\\n                      displacement[1][dy, dx] * (1 - y_c) * (1 - x_c)\n\n        # 4. bound centroids\n        centroid_y = np.clip(centroid_y, 0, height-1)\n        centroid_x = np.clip(centroid_x, 0, width-1)\n\n    centroid_y = np.round(centroid_y).astype(np.int32)\n    centroid_x = np.round(centroid_x).astype(np.int32)\n\n    return np.stack([centroid_y, centroid_x], axis=0)\n\ndef cluster_centroids(centroids, displacement, thres=2.5):\n    # thres: threshold for grouping centroid (see supp)\n\n    dp_strength = np.sqrt(displacement[1] ** 2 + displacement[0] ** 2)\n    height, width = dp_strength.shape\n\n    weak_dp_region = dp_strength < thres\n\n    dp_label = skimage.measure.label(weak_dp_region, connectivity=1, background=0)\n    dp_label_1d = dp_label.reshape(-1)\n\n    centroids_1d = centroids[0]*width + centroids[1]\n\n    clusters_1d = dp_label_1d[centroids_1d]\n\n    cluster_map = imutils.compress_range(clusters_1d.reshape(height, width) + 1)\n\n    return pyutils.to_one_hot(cluster_map)\n\ndef separte_score_by_mask(scores, masks):\n    instacne_map_expanded = torch.from_numpy(np.expand_dims(masks, 0).astype(np.float32))\n    instance_score = torch.unsqueeze(scores, 1) * instacne_map_expanded.cuda()\n    return instance_score\n\ndef detect_instance(score_map, mask, class_id, max_fragment_size=0):\n    # converting pixel-wise instance ids into detection form\n\n    pred_score = []\n    pred_label = []\n    pred_mask = []\n\n    for ag_score, ag_mask, ag_class in zip(score_map, mask, class_id):\n        if np.sum(ag_mask) < 1:\n            continue\n        segments = pyutils.to_one_hot(skimage.measure.label(ag_mask, connectivity=1, background=0))[1:]\n        # connected components analysis\n\n        for seg_mask in segments:\n            if np.sum(seg_mask) < max_fragment_size:\n                pred_score.append(0)\n            else:\n                pred_score.append(np.max(ag_score * seg_mask))\n            pred_label.append(ag_class)\n            pred_mask.append(seg_mask)\n\n    return {\'score\': np.stack(pred_score, 0),\n           \'mask\': np.stack(pred_mask, 0),\n           \'class\': np.stack(pred_label, 0)}\n\n\ndef _work(process_id, model, dataset, args):\n\n    n_gpus = torch.cuda.device_count()\n    databin = dataset[process_id]\n    data_loader = DataLoader(databin, shuffle=False, num_workers=args.num_workers // n_gpus, pin_memory=False)\n\n    with torch.no_grad(), cuda.device(process_id):\n\n        model.cuda()\n\n        for iter, pack in enumerate(data_loader):\n            img_name = pack[\'name\'][0]\n            size = np.asarray(pack[\'size\'])\n\n            edge, dp = model(pack[\'img\'][0].cuda(non_blocking=True))\n\n            dp = dp.cpu().numpy()\n\n            cam_dict = np.load(args.cam_out_dir + \'/\' + img_name + \'.npy\', allow_pickle=True).item()\n\n            cams = cam_dict[\'cam\'].cuda()\n            keys = cam_dict[\'keys\']\n\n            centroids = find_centroids_with_refinement(dp)\n            instance_map = cluster_centroids(centroids, dp)\n            instance_cam = separte_score_by_mask(cams, instance_map)\n\n            rw = indexing.propagate_to_edge(instance_cam, edge, beta=args.beta, exp_times=args.exp_times, radius=5)\n\n            rw_up = F.interpolate(rw, scale_factor=4, mode=\'bilinear\', align_corners=False)[:, 0, :size[0], :size[1]]\n            rw_up = rw_up / torch.max(rw_up)\n\n            rw_up_bg = F.pad(rw_up, (0, 0, 0, 0, 1, 0), value=args.ins_seg_bg_thres)\n\n            num_classes = len(keys)\n            num_instances = instance_map.shape[0]\n\n            instance_shape = torch.argmax(rw_up_bg, 0).cpu().numpy()\n            instance_shape = pyutils.to_one_hot(instance_shape, maximum_val=num_instances*num_classes+1)[1:]\n            instance_class_id = np.repeat(keys, num_instances)\n\n            detected = detect_instance(rw_up.cpu().numpy(), instance_shape, instance_class_id,\n                                       max_fragment_size=size[0] * size[1] * 0.01)\n\n            np.save(os.path.join(args.ins_seg_out_dir, img_name + \'.npy\'), detected)\n\n            if process_id == n_gpus - 1 and iter % (len(databin) // 20) == 0:\n                print(""%d "" % ((5*iter+1)//(len(databin) // 20)), end=\'\')\n\n\ndef run(args):\n    model = getattr(importlib.import_module(args.irn_network), \'EdgeDisplacement\')()\n    model.load_state_dict(torch.load(args.irn_weights_name), strict=False)\n    model.eval()\n\n    n_gpus = torch.cuda.device_count()\n\n    dataset = voc12.dataloader.VOC12ClassificationDatasetMSF(args.infer_list,\n                                                             voc12_root=args.voc12_root,\n                                                             scales=(1.0,))\n    dataset = torchutils.split_dataset(dataset, n_gpus)\n\n    print(""[ "", end=\'\')\n    multiprocessing.spawn(_work, nprocs=n_gpus, args=(model, dataset, args), join=True)\n    print(""]"")\n'"
step/make_sem_seg_labels.py,10,"b'import torch\nfrom torch import multiprocessing, cuda\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch.backends import cudnn\n\nimport numpy as np\nimport importlib\nimport os\nimport imageio\n\nimport voc12.dataloader\nfrom misc import torchutils, indexing\n\ncudnn.enabled = True\n\ndef _work(process_id, model, dataset, args):\n\n    n_gpus = torch.cuda.device_count()\n    databin = dataset[process_id]\n    data_loader = DataLoader(databin,\n                             shuffle=False, num_workers=args.num_workers // n_gpus, pin_memory=False)\n\n    with torch.no_grad(), cuda.device(process_id):\n\n        model.cuda()\n\n        for iter, pack in enumerate(data_loader):\n            img_name = voc12.dataloader.decode_int_filename(pack[\'name\'][0])\n            orig_img_size = np.asarray(pack[\'size\'])\n\n            edge, dp = model(pack[\'img\'][0].cuda(non_blocking=True))\n\n            cam_dict = np.load(args.cam_out_dir + \'/\' + img_name + \'.npy\', allow_pickle=True).item()\n\n            cams = cam_dict[\'cam\']\n            keys = np.pad(cam_dict[\'keys\'] + 1, (1, 0), mode=\'constant\')\n\n            cam_downsized_values = cams.cuda()\n\n            rw = indexing.propagate_to_edge(cam_downsized_values, edge, beta=args.beta, exp_times=args.exp_times, radius=5)\n\n            rw_up = F.interpolate(rw, scale_factor=4, mode=\'bilinear\', align_corners=False)[..., 0, :orig_img_size[0], :orig_img_size[1]]\n            rw_up = rw_up / torch.max(rw_up)\n\n            rw_up_bg = F.pad(rw_up, (0, 0, 0, 0, 1, 0), value=args.sem_seg_bg_thres)\n            rw_pred = torch.argmax(rw_up_bg, dim=0).cpu().numpy()\n\n            rw_pred = keys[rw_pred]\n\n            imageio.imsave(os.path.join(args.sem_seg_out_dir, img_name + \'.png\'), rw_pred.astype(np.uint8))\n\n            if process_id == n_gpus - 1 and iter % (len(databin) // 20) == 0:\n                print(""%d "" % ((5*iter+1)//(len(databin) // 20)), end=\'\')\n\n\ndef run(args):\n    model = getattr(importlib.import_module(args.irn_network), \'EdgeDisplacement\')()\n    model.load_state_dict(torch.load(args.irn_weights_name), strict=False)\n    model.eval()\n\n    n_gpus = torch.cuda.device_count()\n\n    dataset = voc12.dataloader.VOC12ClassificationDatasetMSF(args.infer_list,\n                                                             voc12_root=args.voc12_root,\n                                                             scales=(1.0,))\n    dataset = torchutils.split_dataset(dataset, n_gpus)\n\n    print(""["", end=\'\')\n    multiprocessing.spawn(_work, nprocs=n_gpus, args=(model, dataset, args), join=True)\n    print(""]"")\n\n    torch.cuda.empty_cache()\n'"
step/train_cam.py,7,"b'\nimport torch\nfrom torch.backends import cudnn\ncudnn.enabled = True\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\nimport importlib\n\nimport voc12.dataloader\nfrom misc import pyutils, torchutils\n\n\ndef validate(model, data_loader):\n    print(\'validating ... \', flush=True, end=\'\')\n\n    val_loss_meter = pyutils.AverageMeter(\'loss1\', \'loss2\')\n\n    model.eval()\n\n    with torch.no_grad():\n        for pack in data_loader:\n            img = pack[\'img\']\n\n            label = pack[\'label\'].cuda(non_blocking=True)\n\n            x = model(img)\n            loss1 = F.multilabel_soft_margin_loss(x, label)\n\n            val_loss_meter.add({\'loss1\': loss1.item()})\n\n    model.train()\n\n    print(\'loss: %.4f\' % (val_loss_meter.pop(\'loss1\')))\n\n    return\n\n\ndef run(args):\n\n    model = getattr(importlib.import_module(args.cam_network), \'Net\')()\n\n\n    train_dataset = voc12.dataloader.VOC12ClassificationDataset(args.train_list, voc12_root=args.voc12_root,\n                                                                resize_long=(320, 640), hor_flip=True,\n                                                                crop_size=512, crop_method=""random"")\n    train_data_loader = DataLoader(train_dataset, batch_size=args.cam_batch_size,\n                                   shuffle=True, num_workers=args.num_workers, pin_memory=True, drop_last=True)\n    max_step = (len(train_dataset) // args.cam_batch_size) * args.cam_num_epoches\n\n    val_dataset = voc12.dataloader.VOC12ClassificationDataset(args.val_list, voc12_root=args.voc12_root,\n                                                              crop_size=512)\n    val_data_loader = DataLoader(val_dataset, batch_size=args.cam_batch_size,\n                                 shuffle=False, num_workers=args.num_workers, pin_memory=True, drop_last=True)\n\n    param_groups = model.trainable_parameters()\n    optimizer = torchutils.PolyOptimizer([\n        {\'params\': param_groups[0], \'lr\': args.cam_learning_rate, \'weight_decay\': args.cam_weight_decay},\n        {\'params\': param_groups[1], \'lr\': 10*args.cam_learning_rate, \'weight_decay\': args.cam_weight_decay},\n    ], lr=args.cam_learning_rate, weight_decay=args.cam_weight_decay, max_step=max_step)\n\n    model = torch.nn.DataParallel(model).cuda()\n    model.train()\n\n    avg_meter = pyutils.AverageMeter()\n\n    timer = pyutils.Timer()\n\n    for ep in range(args.cam_num_epoches):\n\n        print(\'Epoch %d/%d\' % (ep+1, args.cam_num_epoches))\n\n        for step, pack in enumerate(train_data_loader):\n\n            img = pack[\'img\']\n            label = pack[\'label\'].cuda(non_blocking=True)\n\n            x = model(img)\n            loss = F.multilabel_soft_margin_loss(x, label)\n\n            avg_meter.add({\'loss1\': loss.item()})\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            if (optimizer.global_step-1)%100 == 0:\n                timer.update_progress(optimizer.global_step / max_step)\n\n                print(\'step:%5d/%5d\' % (optimizer.global_step - 1, max_step),\n                      \'loss:%.4f\' % (avg_meter.pop(\'loss1\')),\n                      \'imps:%.1f\' % ((step + 1) * args.cam_batch_size / timer.get_stage_elapsed()),\n                      \'lr: %.4f\' % (optimizer.param_groups[0][\'lr\']),\n                      \'etc:%s\' % (timer.str_estimated_complete()), flush=True)\n\n        else:\n            validate(model, val_data_loader)\n            timer.reset_stage()\n\n    torch.save(model.module.state_dict(), args.cam_weights_name + \'.pth\')\n    torch.cuda.empty_cache()'"
step/train_irn.py,13,"b'\nimport torch\nfrom torch.backends import cudnn\ncudnn.enabled = True\nfrom torch.utils.data import DataLoader\nimport voc12.dataloader\nfrom misc import pyutils, torchutils, indexing\nimport importlib\n\ndef run(args):\n\n    path_index = indexing.PathIndex(radius=10, default_size=(args.irn_crop_size // 4, args.irn_crop_size // 4))\n\n    model = getattr(importlib.import_module(args.irn_network), \'AffinityDisplacementLoss\')(\n        path_index)\n\n    train_dataset = voc12.dataloader.VOC12AffinityDataset(args.train_list,\n                                                          label_dir=args.ir_label_out_dir,\n                                                          voc12_root=args.voc12_root,\n                                                          indices_from=path_index.src_indices,\n                                                          indices_to=path_index.dst_indices,\n                                                          hor_flip=True,\n                                                          crop_size=args.irn_crop_size,\n                                                          crop_method=""random"",\n                                                          rescale=(0.5, 1.5)\n                                                          )\n    train_data_loader = DataLoader(train_dataset, batch_size=args.irn_batch_size,\n                                   shuffle=True, num_workers=args.num_workers, pin_memory=True, drop_last=True)\n\n    max_step = (len(train_dataset) // args.irn_batch_size) * args.irn_num_epoches\n\n    param_groups = model.trainable_parameters()\n    optimizer = torchutils.PolyOptimizer([\n        {\'params\': param_groups[0], \'lr\': 1*args.irn_learning_rate, \'weight_decay\': args.irn_weight_decay},\n        {\'params\': param_groups[1], \'lr\': 10*args.irn_learning_rate, \'weight_decay\': args.irn_weight_decay}\n    ], lr=args.irn_learning_rate, weight_decay=args.irn_weight_decay, max_step=max_step)\n\n    model = torch.nn.DataParallel(model).cuda()\n    model.train()\n\n    avg_meter = pyutils.AverageMeter()\n\n    timer = pyutils.Timer()\n\n    for ep in range(args.irn_num_epoches):\n\n        print(\'Epoch %d/%d\' % (ep+1, args.irn_num_epoches))\n\n        for iter, pack in enumerate(train_data_loader):\n\n            img = pack[\'img\'].cuda(non_blocking=True)\n            bg_pos_label = pack[\'aff_bg_pos_label\'].cuda(non_blocking=True)\n            fg_pos_label = pack[\'aff_fg_pos_label\'].cuda(non_blocking=True)\n            neg_label = pack[\'aff_neg_label\'].cuda(non_blocking=True)\n\n            pos_aff_loss, neg_aff_loss, dp_fg_loss, dp_bg_loss = model(img, True)\n\n            bg_pos_aff_loss = torch.sum(bg_pos_label * pos_aff_loss) / (torch.sum(bg_pos_label) + 1e-5)\n            fg_pos_aff_loss = torch.sum(fg_pos_label * pos_aff_loss) / (torch.sum(fg_pos_label) + 1e-5)\n            pos_aff_loss = bg_pos_aff_loss / 2 + fg_pos_aff_loss / 2\n            neg_aff_loss = torch.sum(neg_label * neg_aff_loss) / (torch.sum(neg_label) + 1e-5)\n\n            dp_fg_loss = torch.sum(dp_fg_loss * torch.unsqueeze(fg_pos_label, 1)) / (2 * torch.sum(fg_pos_label) + 1e-5)\n            dp_bg_loss = torch.sum(dp_bg_loss * torch.unsqueeze(bg_pos_label, 1)) / (2 * torch.sum(bg_pos_label) + 1e-5)\n\n            avg_meter.add({\'loss1\': pos_aff_loss.item(), \'loss2\': neg_aff_loss.item(),\n                           \'loss3\': dp_fg_loss.item(), \'loss4\': dp_bg_loss.item()})\n\n            total_loss = (pos_aff_loss + neg_aff_loss) / 2 + (dp_fg_loss + dp_bg_loss) / 2\n\n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n\n            if (optimizer.global_step - 1) % 50 == 0:\n                timer.update_progress(optimizer.global_step / max_step)\n\n                print(\'step:%5d/%5d\' % (optimizer.global_step - 1, max_step),\n                      \'loss:%.4f %.4f %.4f %.4f\' % (\n                      avg_meter.pop(\'loss1\'), avg_meter.pop(\'loss2\'), avg_meter.pop(\'loss3\'), avg_meter.pop(\'loss4\')),\n                      \'imps:%.1f\' % ((iter + 1) * args.irn_batch_size / timer.get_stage_elapsed()),\n                      \'lr: %.4f\' % (optimizer.param_groups[0][\'lr\']),\n                      \'etc:%s\' % (timer.str_estimated_complete()), flush=True)\n        else:\n            timer.reset_stage()\n\n    infer_dataset = voc12.dataloader.VOC12ImageDataset(args.infer_list,\n                                                       voc12_root=args.voc12_root,\n                                                       crop_size=args.irn_crop_size,\n                                                       crop_method=""top_left"")\n    infer_data_loader = DataLoader(infer_dataset, batch_size=args.irn_batch_size,\n                                   shuffle=False, num_workers=args.num_workers, pin_memory=True, drop_last=True)\n\n    model.eval()\n    print(\'Analyzing displacements mean ... \', end=\'\')\n\n    dp_mean_list = []\n\n    with torch.no_grad():\n        for iter, pack in enumerate(infer_data_loader):\n            img = pack[\'img\'].cuda(non_blocking=True)\n\n            aff, dp = model(img, False)\n\n            dp_mean_list.append(torch.mean(dp, dim=(0, 2, 3)).cpu())\n\n        model.module.mean_shift.running_mean = torch.mean(torch.stack(dp_mean_list), dim=0)\n    print(\'done.\')\n\n    torch.save(model.module.state_dict(), args.irn_weights_name)\n    torch.cuda.empty_cache()\n'"
voc12/dataloader.py,5,"b'\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport os.path\nimport imageio\nfrom misc import imutils\n\nIMG_FOLDER_NAME = ""JPEGImages""\nANNOT_FOLDER_NAME = ""Annotations""\nIGNORE = 255\n\nCAT_LIST = [\'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n        \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n        \'cow\', \'diningtable\', \'dog\', \'horse\',\n        \'motorbike\', \'person\', \'pottedplant\',\n        \'sheep\', \'sofa\', \'train\',\n        \'tvmonitor\']\n\nN_CAT = len(CAT_LIST)\n\nCAT_NAME_TO_NUM = dict(zip(CAT_LIST,range(len(CAT_LIST))))\n\ncls_labels_dict = np.load(\'voc12/cls_labels.npy\', allow_pickle=True).item()\n\ndef decode_int_filename(int_filename):\n    s = str(int(int_filename))\n    return s[:4] + \'_\' + s[4:]\n\ndef load_image_label_from_xml(img_name, voc12_root):\n    from xml.dom import minidom\n\n    elem_list = minidom.parse(os.path.join(voc12_root, ANNOT_FOLDER_NAME, decode_int_filename(img_name) + \'.xml\')).getElementsByTagName(\'name\')\n\n    multi_cls_lab = np.zeros((N_CAT), np.float32)\n\n    for elem in elem_list:\n        cat_name = elem.firstChild.data\n        if cat_name in CAT_LIST:\n            cat_num = CAT_NAME_TO_NUM[cat_name]\n            multi_cls_lab[cat_num] = 1.0\n\n    return multi_cls_lab\n\ndef load_image_label_list_from_xml(img_name_list, voc12_root):\n\n    return [load_image_label_from_xml(img_name, voc12_root) for img_name in img_name_list]\n\ndef load_image_label_list_from_npy(img_name_list):\n\n    return np.array([cls_labels_dict[img_name] for img_name in img_name_list])\n\ndef get_img_path(img_name, voc12_root):\n    if not isinstance(img_name, str):\n        img_name = decode_int_filename(img_name)\n    return os.path.join(voc12_root, IMG_FOLDER_NAME, img_name + \'.jpg\')\n\ndef load_img_name_list(dataset_path):\n\n    img_name_list = np.loadtxt(dataset_path, dtype=np.int32)\n\n    return img_name_list\n\n\nclass TorchvisionNormalize():\n    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, img):\n        imgarr = np.asarray(img)\n        proc_img = np.empty_like(imgarr, np.float32)\n\n        proc_img[..., 0] = (imgarr[..., 0] / 255. - self.mean[0]) / self.std[0]\n        proc_img[..., 1] = (imgarr[..., 1] / 255. - self.mean[1]) / self.std[1]\n        proc_img[..., 2] = (imgarr[..., 2] / 255. - self.mean[2]) / self.std[2]\n\n        return proc_img\n\nclass GetAffinityLabelFromIndices():\n\n    def __init__(self, indices_from, indices_to):\n\n        self.indices_from = indices_from\n        self.indices_to = indices_to\n\n    def __call__(self, segm_map):\n\n        segm_map_flat = np.reshape(segm_map, -1)\n\n        segm_label_from = np.expand_dims(segm_map_flat[self.indices_from], axis=0)\n        segm_label_to = segm_map_flat[self.indices_to]\n\n        valid_label = np.logical_and(np.less(segm_label_from, 21), np.less(segm_label_to, 21))\n\n        equal_label = np.equal(segm_label_from, segm_label_to)\n\n        pos_affinity_label = np.logical_and(equal_label, valid_label)\n\n        bg_pos_affinity_label = np.logical_and(pos_affinity_label, np.equal(segm_label_from, 0)).astype(np.float32)\n        fg_pos_affinity_label = np.logical_and(pos_affinity_label, np.greater(segm_label_from, 0)).astype(np.float32)\n\n        neg_affinity_label = np.logical_and(np.logical_not(equal_label), valid_label).astype(np.float32)\n\n        return torch.from_numpy(bg_pos_affinity_label), torch.from_numpy(fg_pos_affinity_label), \\\n               torch.from_numpy(neg_affinity_label)\n\n\nclass VOC12ImageDataset(Dataset):\n\n    def __init__(self, img_name_list_path, voc12_root,\n                 resize_long=None, rescale=None, img_normal=TorchvisionNormalize(), hor_flip=False,\n                 crop_size=None, crop_method=None, to_torch=True):\n\n        self.img_name_list = load_img_name_list(img_name_list_path)\n        self.voc12_root = voc12_root\n\n        self.resize_long = resize_long\n        self.rescale = rescale\n        self.crop_size = crop_size\n        self.img_normal = img_normal\n        self.hor_flip = hor_flip\n        self.crop_method = crop_method\n        self.to_torch = to_torch\n\n    def __len__(self):\n        return len(self.img_name_list)\n\n    def __getitem__(self, idx):\n        name = self.img_name_list[idx]\n        name_str = decode_int_filename(name)\n\n        img = np.asarray(imageio.imread(get_img_path(name_str, self.voc12_root)))\n\n        if self.resize_long:\n            img = imutils.random_resize_long(img, self.resize_long[0], self.resize_long[1])\n\n        if self.rescale:\n            img = imutils.random_scale(img, scale_range=self.rescale, order=3)\n\n        if self.img_normal:\n            img = self.img_normal(img)\n\n        if self.hor_flip:\n            img = imutils.random_lr_flip(img)\n\n        if self.crop_size:\n            if self.crop_method == ""random"":\n                img = imutils.random_crop(img, self.crop_size, 0)\n            else:\n                img = imutils.top_left_crop(img, self.crop_size, 0)\n\n        if self.to_torch:\n            img = imutils.HWC_to_CHW(img)\n\n        return {\'name\': name_str, \'img\': img}\n\nclass VOC12ClassificationDataset(VOC12ImageDataset):\n\n    def __init__(self, img_name_list_path, voc12_root,\n                 resize_long=None, rescale=None, img_normal=TorchvisionNormalize(), hor_flip=False,\n                 crop_size=None, crop_method=None):\n        super().__init__(img_name_list_path, voc12_root,\n                 resize_long, rescale, img_normal, hor_flip,\n                 crop_size, crop_method)\n        self.label_list = load_image_label_list_from_npy(self.img_name_list)\n\n    def __getitem__(self, idx):\n        out = super().__getitem__(idx)\n\n        out[\'label\'] = torch.from_numpy(self.label_list[idx])\n\n        return out\n\nclass VOC12ClassificationDatasetMSF(VOC12ClassificationDataset):\n\n    def __init__(self, img_name_list_path, voc12_root,\n                 img_normal=TorchvisionNormalize(),\n                 scales=(1.0,)):\n        self.scales = scales\n\n        super().__init__(img_name_list_path, voc12_root, img_normal=img_normal)\n        self.scales = scales\n\n    def __getitem__(self, idx):\n        name = self.img_name_list[idx]\n        name_str = decode_int_filename(name)\n\n        img = imageio.imread(get_img_path(name_str, self.voc12_root))\n\n        ms_img_list = []\n        for s in self.scales:\n            if s == 1:\n                s_img = img\n            else:\n                s_img = imutils.pil_rescale(img, s, order=3)\n            s_img = self.img_normal(s_img)\n            s_img = imutils.HWC_to_CHW(s_img)\n            ms_img_list.append(np.stack([s_img, np.flip(s_img, -1)], axis=0))\n        if len(self.scales) == 1:\n            ms_img_list = ms_img_list[0]\n\n        out = {""name"": name_str, ""img"": ms_img_list, ""size"": (img.shape[0], img.shape[1]),\n               ""label"": torch.from_numpy(self.label_list[idx])}\n        return out\n\nclass VOC12SegmentationDataset(Dataset):\n\n    def __init__(self, img_name_list_path, label_dir, crop_size, voc12_root,\n                 rescale=None, img_normal=TorchvisionNormalize(), hor_flip=False,\n                 crop_method = \'random\'):\n\n        self.img_name_list = load_img_name_list(img_name_list_path)\n        self.voc12_root = voc12_root\n\n        self.label_dir = label_dir\n\n        self.rescale = rescale\n        self.crop_size = crop_size\n        self.img_normal = img_normal\n        self.hor_flip = hor_flip\n        self.crop_method = crop_method\n\n    def __len__(self):\n        return len(self.img_name_list)\n\n    def __getitem__(self, idx):\n        name = self.img_name_list[idx]\n        name_str = decode_int_filename(name)\n\n        img = imageio.imread(get_img_path(name_str, self.voc12_root))\n        label = imageio.imread(os.path.join(self.label_dir, name_str + \'.png\'))\n\n        img = np.asarray(img)\n\n        if self.rescale:\n            img, label = imutils.random_scale((img, label), scale_range=self.rescale, order=(3, 0))\n\n        if self.img_normal:\n            img = self.img_normal(img)\n\n        if self.hor_flip:\n            img, label = imutils.random_lr_flip((img, label))\n\n        if self.crop_method == ""random"":\n            img, label = imutils.random_crop((img, label), self.crop_size, (0, 255))\n        else:\n            img = imutils.top_left_crop(img, self.crop_size, 0)\n            label = imutils.top_left_crop(label, self.crop_size, 255)\n\n        img = imutils.HWC_to_CHW(img)\n\n        return {\'name\': name, \'img\': img, \'label\': label}\n\nclass VOC12AffinityDataset(VOC12SegmentationDataset):\n    def __init__(self, img_name_list_path, label_dir, crop_size, voc12_root,\n                 indices_from, indices_to,\n                 rescale=None, img_normal=TorchvisionNormalize(), hor_flip=False, crop_method=None):\n        super().__init__(img_name_list_path, label_dir, crop_size, voc12_root, rescale, img_normal, hor_flip, crop_method=crop_method)\n\n        self.extract_aff_lab_func = GetAffinityLabelFromIndices(indices_from, indices_to)\n\n    def __len__(self):\n        return len(self.img_name_list)\n\n    def __getitem__(self, idx):\n        out = super().__getitem__(idx)\n\n        reduced_label = imutils.pil_rescale(out[\'label\'], 0.25, 0)\n\n        out[\'aff_bg_pos_label\'], out[\'aff_fg_pos_label\'], out[\'aff_neg_label\'] = self.extract_aff_lab_func(reduced_label)\n\n        return out\n\n'"
voc12/make_cls_labels.py,0,"b'import argparse\nimport voc12.dataloader\nimport numpy as np\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--train_list"", default=\'train_aug.txt\', type=str)\n    parser.add_argument(""--val_list"", default=\'val.txt\', type=str)\n    parser.add_argument(""--out"", default=""cls_labels.npy"", type=str)\n    parser.add_argument(""--voc12_root"", default=""../../../Dataset/VOC2012"", type=str)\n    args = parser.parse_args()\n\n    train_name_list = voc12.dataloader.load_img_name_list(args.train_list)\n    val_name_list = voc12.dataloader.load_img_name_list(args.val_list)\n\n    train_val_name_list = np.concatenate([train_name_list, val_name_list], axis=0)\n    label_list = voc12.dataloader.load_image_label_list_from_xml(train_val_name_list, args.voc12_root)\n\n    total_label = np.zeros(20)\n\n    d = dict()\n    for img_name, label in zip(train_val_name_list, label_list):\n        d[img_name] = label\n        total_label += label\n\n    print(total_label)\n    np.save(args.out, d)'"
