file_path,api_count,code
infer_aff.py,11,"b'import torch\nimport torchvision\nfrom tool import imutils\n\nimport argparse\nimport importlib\nimport numpy as np\n\nimport voc12.data\nfrom torch.utils.data import DataLoader\nimport scipy.misc\nimport torch.nn.functional as F\nimport os.path\n\ndef get_indices_in_radius(height, width, radius):\n\n    search_dist = []\n    for x in range(1, radius):\n        search_dist.append((0, x))\n\n    for y in range(1, radius):\n        for x in range(-radius+1, radius):\n            if x*x + y*y < radius*radius:\n                search_dist.append((y, x))\n\n    full_indices = np.reshape(np.arange(0, height * width, dtype=np.int64),\n                              (height, width))\n    radius_floor = radius-1\n    cropped_height = height - radius_floor\n    cropped_width = width - 2 * radius_floor\n\n    indices_from = np.reshape(full_indices[:-radius_floor, radius_floor:-radius_floor], [-1])\n\n    indices_from_to_list = []\n\n    for dy, dx in search_dist:\n\n        indices_to = full_indices[dy:dy + cropped_height, radius_floor + dx:radius_floor + dx + cropped_width]\n        indices_to = np.reshape(indices_to, [-1])\n\n        indices_from_to = np.stack((indices_from, indices_to), axis=1)\n\n        indices_from_to_list.append(indices_from_to)\n\n    concat_indices_from_to = np.concatenate(indices_from_to_list, axis=0)\n\n    return concat_indices_from_to\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--weights"", required=True, type=str)\n    parser.add_argument(""--network"", default=""network.vgg16_aff"", type=str)\n    parser.add_argument(""--infer_list"", default=""voc12/train_aug.txt"", type=str)\n    parser.add_argument(""--num_workers"", default=8, type=int)\n    parser.add_argument(""--cam_dir"", required=True, type=str)\n    parser.add_argument(""--voc12_root"", required=True, type=str)\n    parser.add_argument(""--alpha"", default=16, type=int)\n    parser.add_argument(""--out_rw"", required=True, type=str)\n    parser.add_argument(""--beta"", default=8, type=int)\n    parser.add_argument(""--logt"", default=8, type=int)\n\n    args = parser.parse_args()\n\n    model = getattr(importlib.import_module(args.network), \'Net\')()\n\n    model.load_state_dict(torch.load(args.weights))\n\n    model.eval()\n    model.cuda()\n\n    infer_dataset = voc12.data.VOC12ImageDataset(args.infer_list, voc12_root=args.voc12_root,\n                                               transform=torchvision.transforms.Compose(\n        [np.asarray,\n         model.normalize,\n         imutils.HWC_to_CHW]))\n    infer_data_loader = DataLoader(infer_dataset, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n\n    for iter, (name, img) in enumerate(infer_data_loader):\n\n        name = name[0]\n        print(iter)\n\n        orig_shape = img.shape\n        padded_size = (int(np.ceil(img.shape[2]/8)*8), int(np.ceil(img.shape[3]/8)*8))\n\n        p2d = (0, padded_size[1] - img.shape[3], 0, padded_size[0] - img.shape[2])\n        img = F.pad(img, p2d)\n\n        dheight = int(np.ceil(img.shape[2]/8))\n        dwidth = int(np.ceil(img.shape[3]/8))\n\n        cam = np.load(os.path.join(args.cam_dir, name + \'.npy\')).item()\n\n        cam_full_arr = np.zeros((21, orig_shape[2], orig_shape[3]), np.float32)\n        for k, v in cam.items():\n            cam_full_arr[k+1] = v\n        cam_full_arr[0] = (1 - np.max(cam_full_arr[1:], (0), keepdims=False))**args.alpha\n        cam_full_arr = np.pad(cam_full_arr, ((0, 0), (0, p2d[3]), (0, p2d[1])), mode=\'constant\')\n\n        with torch.no_grad():\n            aff_mat = torch.pow(model.forward(img.cuda(), True), args.beta)\n\n            trans_mat = aff_mat / torch.sum(aff_mat, dim=0, keepdim=True)\n            for _ in range(args.logt):\n                trans_mat = torch.matmul(trans_mat, trans_mat)\n\n            cam_full_arr = torch.from_numpy(cam_full_arr)\n            cam_full_arr = F.avg_pool2d(cam_full_arr, 8, 8)\n\n            cam_vec = cam_full_arr.view(21, -1)\n\n            cam_rw = torch.matmul(cam_vec.cuda(), trans_mat)\n            cam_rw = cam_rw.view(1, 21, dheight, dwidth)\n\n            cam_rw = torch.nn.Upsample((img.shape[2], img.shape[3]), mode=\'bilinear\')(cam_rw)\n            _, cam_rw_pred = torch.max(cam_rw, 1)\n\n            res = np.uint8(cam_rw_pred.cpu().data[0])[:orig_shape[2], :orig_shape[3]]\n\n            scipy.misc.imsave(os.path.join(args.out_rw, name + \'.png\'), res)\n'"
infer_cls.py,8,"b'\nimport numpy as np\nimport torch\nfrom torch.backends import cudnn\ncudnn.enabled = True\nimport voc12.data\nimport scipy.misc\nimport importlib\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom tool import imutils, pyutils\nimport argparse\nfrom PIL import Image\nimport torch.nn.functional as F\nimport os.path\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--weights"", required=True, type=str)\n    parser.add_argument(""--network"", default=""network.vgg16_cls"", type=str)\n    parser.add_argument(""--infer_list"", default=""voc12/val.txt"", type=str)\n    parser.add_argument(""--num_workers"", default=8, type=int)\n    parser.add_argument(""--voc12_root"", required=True, type=str)\n    parser.add_argument(""--low_alpha"", default=4, type=int)\n    parser.add_argument(""--high_alpha"", default=32, type=int)\n    parser.add_argument(""--out_cam"", default=None, type=str)\n    parser.add_argument(""--out_la_crf"", default=None, type=str)\n    parser.add_argument(""--out_ha_crf"", default=None, type=str)\n    parser.add_argument(""--out_cam_pred"", default=None, type=str)\n\n    args = parser.parse_args()\n\n    model = getattr(importlib.import_module(args.network), \'Net\')()\n    model.load_state_dict(torch.load(args.weights))\n\n    model.eval()\n    model.cuda()\n\n    infer_dataset = voc12.data.VOC12ClsDatasetMSF(args.infer_list, voc12_root=args.voc12_root,\n                                                   scales=(1, 0.5, 1.5, 2.0),\n                                                   inter_transform=torchvision.transforms.Compose(\n                                                       [np.asarray,\n                                                        model.normalize,\n                                                        imutils.HWC_to_CHW]))\n\n    infer_data_loader = DataLoader(infer_dataset, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n\n    n_gpus = torch.cuda.device_count()\n    model_replicas = torch.nn.parallel.replicate(model, list(range(n_gpus)))\n\n    for iter, (img_name, img_list, label) in enumerate(infer_data_loader):\n        img_name = img_name[0]; label = label[0]\n\n        img_path = voc12.data.get_img_path(img_name, args.voc12_root)\n        orig_img = np.asarray(Image.open(img_path))\n        orig_img_size = orig_img.shape[:2]\n\n        def _work(i, img):\n            with torch.no_grad():\n                with torch.cuda.device(i%n_gpus):\n                    cam = model_replicas[i%n_gpus].forward_cam(img.cuda())\n                    cam = F.upsample(cam, orig_img_size, mode=\'bilinear\', align_corners=False)[0]\n                    cam = cam.cpu().numpy() * label.clone().view(20, 1, 1).numpy()\n                    if i % 2 == 1:\n                        cam = np.flip(cam, axis=-1)\n                    return cam\n\n        thread_pool = pyutils.BatchThreader(_work, list(enumerate(img_list)),\n                                            batch_size=12, prefetch_size=0, processes=args.num_workers)\n\n        cam_list = thread_pool.pop_results()\n\n        sum_cam = np.sum(cam_list, axis=0)\n        norm_cam = sum_cam / (np.max(sum_cam, (1, 2), keepdims=True) + 1e-5)\n\n        cam_dict = {}\n        for i in range(20):\n            if label[i] > 1e-5:\n                cam_dict[i] = norm_cam[i]\n\n        if args.out_cam is not None:\n            np.save(os.path.join(args.out_cam, img_name + \'.npy\'), cam_dict)\n\n        if args.out_cam_pred is not None:\n            bg_score = [np.ones_like(norm_cam[0])*0.2]\n            pred = np.argmax(np.concatenate((bg_score, norm_cam)), 0)\n            scipy.misc.imsave(os.path.join(args.out_cam_pred, img_name + \'.png\'), pred.astype(np.uint8))\n\n        def _crf_with_alpha(cam_dict, alpha):\n            v = np.array(list(cam_dict.values()))\n            bg_score = np.power(1 - np.max(v, axis=0, keepdims=True), alpha)\n            bgcam_score = np.concatenate((bg_score, v), axis=0)\n            crf_score = imutils.crf_inference(orig_img, bgcam_score, labels=bgcam_score.shape[0])\n\n            n_crf_al = dict()\n\n            n_crf_al[0] = crf_score[0]\n            for i, key in enumerate(cam_dict.keys()):\n                n_crf_al[key+1] = crf_score[i+1]\n\n            return n_crf_al\n\n        if args.out_la_crf is not None:\n            crf_la = _crf_with_alpha(cam_dict, args.low_alpha)\n            np.save(os.path.join(args.out_la_crf, img_name + \'.npy\'), crf_la)\n\n        if args.out_ha_crf is not None:\n            crf_ha = _crf_with_alpha(cam_dict, args.high_alpha)\n            np.save(os.path.join(args.out_ha_crf, img_name + \'.npy\'), crf_ha)\n\n        print(iter)\n\n'"
train_aff.py,11,"b'\nimport numpy as np\nimport torch\nfrom torch.backends import cudnn\nfrom torch.utils.data import DataLoader\ncudnn.enabled = True\nfrom torchvision import transforms\nimport voc12.data\nfrom tool import pyutils, imutils, torchutils\nimport argparse\nimport importlib\n\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--batch_size"", default=8, type=int)\n    parser.add_argument(""--max_epoches"", default=8, type=int)\n    parser.add_argument(""--network"", default=""network.vgg16_aff"", type=str)\n    parser.add_argument(""--lr"", default=0.1, type=float)\n    parser.add_argument(""--num_workers"", default=8, type=int)\n    parser.add_argument(""--wt_dec"", default=5e-4, type=float)\n    parser.add_argument(""--train_list"", default=""voc12/train_aug.txt"", type=str)\n    parser.add_argument(""--val_list"", default=""voc12/val.txt"", type=str)\n    parser.add_argument(""--session_name"", default=""vgg_aff"", type=str)\n    parser.add_argument(""--crop_size"", default=448, type=int)\n    parser.add_argument(""--weights"", required=True, type=str)\n    parser.add_argument(""--voc12_root"", required=True, type=str)\n    parser.add_argument(""--la_crf_dir"", required=True, type=str)\n    parser.add_argument(""--ha_crf_dir"", required=True, type=str)\n    args = parser.parse_args()\n\n    pyutils.Logger(args.session_name + \'.log\')\n\n    print(vars(args))\n\n    model = getattr(importlib.import_module(args.network), \'Net\')()\n\n    print(model)\n\n\n    train_dataset = voc12.data.VOC12AffDataset(args.train_list, label_la_dir=args.la_crf_dir, label_ha_dir=args.ha_crf_dir,\n                                               voc12_root=args.voc12_root, cropsize=args.crop_size, radius=5,\n                    joint_transform_list=[\n                        None,\n                        None,\n                        imutils.RandomCrop(args.crop_size),\n                        imutils.RandomHorizontalFlip()\n                    ],\n                    img_transform_list=[\n                        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n                        np.asarray,\n                        model.normalize,\n                        imutils.HWC_to_CHW\n                    ],\n                    label_transform_list=[\n                        None,\n                        None,\n                        None,\n                        imutils.AvgPool2d(8)\n                    ])\n\n    train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers,\n                                   pin_memory=True, drop_last=True)\n    max_step = len(train_dataset) // args.batch_size * args.max_epoches\n\n    param_groups = model.get_parameter_groups()\n    optimizer = torchutils.PolyOptimizer([\n        {\'params\': param_groups[0], \'lr\': args.lr, \'weight_decay\': args.wt_dec},\n        {\'params\': param_groups[1], \'lr\': 2*args.lr, \'weight_decay\': 0},\n        {\'params\': param_groups[2], \'lr\': 10*args.lr, \'weight_decay\': args.wt_dec},\n        {\'params\': param_groups[3], \'lr\': 20*args.lr, \'weight_decay\': 0}\n    ], lr=args.lr, weight_decay=args.wt_dec, max_step=max_step)\n\n    if args.weights[-7:] == \'.params\':\n        import network.resnet38d\n        assert args.network == ""network.resnet38_aff""\n        weights_dict = network.resnet38d.convert_mxnet_to_torch(args.weights)\n    elif args.weights[-11:] == \'.caffemodel\':\n        import network.vgg16d\n        assert args.network == ""network.vgg16_aff""\n        weights_dict = network.vgg16d.convert_caffe_to_torch(args.weights)\n    else:\n        weights_dict = torch.load(args.weights)\n\n    model.load_state_dict(weights_dict, strict=False)\n    model = torch.nn.DataParallel(model).cuda()\n    model.train()\n\n    avg_meter = pyutils.AverageMeter(\'loss\', \'bg_loss\', \'fg_loss\', \'neg_loss\', \'bg_cnt\', \'fg_cnt\', \'neg_cnt\')\n\n    timer = pyutils.Timer(""Session started: "")\n\n    for ep in range(args.max_epoches):\n\n        for iter, pack in enumerate(train_data_loader):\n\n            aff = model.forward(pack[0])\n\n            bg_label = pack[1][0].cuda(non_blocking=True)\n            fg_label = pack[1][1].cuda(non_blocking=True)\n            neg_label = pack[1][2].cuda(non_blocking=True)\n\n            bg_count = torch.sum(bg_label) + 1e-5\n            fg_count = torch.sum(fg_label) + 1e-5\n            neg_count = torch.sum(neg_label) + 1e-5\n\n            bg_loss = torch.sum(- bg_label * torch.log(aff + 1e-5)) / bg_count\n            fg_loss = torch.sum(- fg_label * torch.log(aff + 1e-5)) / fg_count\n            neg_loss = torch.sum(- neg_label * torch.log(1. + 1e-5 - aff)) / neg_count\n\n            loss = bg_loss/4 + fg_loss/4 + neg_loss/2\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            avg_meter.add({\n                \'loss\': loss.item(),\n                \'bg_loss\': bg_loss.item(), \'fg_loss\': fg_loss.item(), \'neg_loss\': neg_loss.item(),\n                \'bg_cnt\': bg_count.item(), \'fg_cnt\': fg_count.item(), \'neg_cnt\': neg_count.item()\n            })\n\n            if (optimizer.global_step - 1) % 50 == 0:\n\n                timer.update_progress(optimizer.global_step / max_step)\n\n                print(\'Iter:%5d/%5d\' % (optimizer.global_step-1, max_step),\n                      \'loss:%.4f %.4f %.4f %.4f\' % avg_meter.get(\'loss\', \'bg_loss\', \'fg_loss\', \'neg_loss\'),\n                      \'cnt:%.0f %.0f %.0f\' % avg_meter.get(\'bg_cnt\', \'fg_cnt\', \'neg_cnt\'),\n                      \'imps:%.1f\' % ((iter+1) * args.batch_size / timer.get_stage_elapsed()),\n                      \'Fin:%s\' % (timer.str_est_finish()),\n                      \'lr: %.4f\' % (optimizer.param_groups[0][\'lr\']), flush=True)\n\n                avg_meter.pop()\n\n\n        else:\n            print(\'\')\n            timer.reset_stage()\n\n    torch.save(model.module.state_dict(), args.session_name + \'.pth\')\n'"
train_cls.py,9,"b'\nimport numpy as np\nimport torch\nfrom torch.backends import cudnn\ncudnn.enabled = True\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport voc12.data\nfrom tool import pyutils, imutils, torchutils\nimport argparse\nimport importlib\nimport torch.nn.functional as F\n\n\ndef validate(model, data_loader):\n    print(\'\\nvalidating ... \', flush=True, end=\'\')\n\n    val_loss_meter = pyutils.AverageMeter(\'loss\')\n\n    model.eval()\n\n    with torch.no_grad():\n        for pack in data_loader:\n            img = pack[1]\n            label = pack[2].cuda(non_blocking=True)\n\n            x = model(img)\n            loss = F.multilabel_soft_margin_loss(x, label)\n\n            val_loss_meter.add({\'loss\': loss.item()})\n\n    model.train()\n\n    print(\'loss:\', val_loss_meter.pop(\'loss\'))\n\n    return\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--batch_size"", default=16, type=int)\n    parser.add_argument(""--max_epoches"", default=15, type=int)\n    parser.add_argument(""--network"", default=""network.vgg16_cls"", type=str)\n    parser.add_argument(""--lr"", default=0.1, type=float)\n    parser.add_argument(""--num_workers"", default=8, type=int)\n    parser.add_argument(""--wt_dec"", default=5e-4, type=float)\n    parser.add_argument(""--weights"", required=True, type=str)\n    parser.add_argument(""--train_list"", default=""voc12/train_aug.txt"", type=str)\n    parser.add_argument(""--val_list"", default=""voc12/val.txt"", type=str)\n    parser.add_argument(""--session_name"", default=""vgg_cls"", type=str)\n    parser.add_argument(""--crop_size"", default=448, type=int)\n    parser.add_argument(""--voc12_root"", required=True, type=str)\n    args = parser.parse_args()\n\n    model = getattr(importlib.import_module(args.network), \'Net\')()\n\n    pyutils.Logger(args.session_name + \'.log\')\n\n    print(vars(args))\n\n    train_dataset = voc12.data.VOC12ClsDataset(args.train_list, voc12_root=args.voc12_root,\n                                               transform=transforms.Compose([\n                        imutils.RandomResizeLong(256, 512),\n                        transforms.RandomHorizontalFlip(),\n                        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n                        np.asarray,\n                        model.normalize,\n                        imutils.RandomCrop(args.crop_size),\n                        imutils.HWC_to_CHW,\n                        torch.from_numpy\n                    ]))\n\n    train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size,\n                                   shuffle=True, num_workers=args.num_workers, pin_memory=True, drop_last=True)\n\n    max_step = (len(train_dataset) // args.batch_size) * args.max_epoches\n\n    val_dataset = voc12.data.VOC12ClsDataset(args.val_list, voc12_root=args.voc12_root,\n                                             transform=transforms.Compose([\n                        np.asarray,\n                        model.normalize,\n                        imutils.CenterCrop(500),\n                        imutils.HWC_to_CHW,\n                        torch.from_numpy\n                    ]))\n    val_data_loader = DataLoader(val_dataset, batch_size=args.batch_size,\n                                 shuffle=False, num_workers=args.num_workers, pin_memory=True, drop_last=True)\n\n    param_groups = model.get_parameter_groups()\n    optimizer = torchutils.PolyOptimizer([\n        {\'params\': param_groups[0], \'lr\': args.lr, \'weight_decay\': args.wt_dec},\n        {\'params\': param_groups[1], \'lr\': 2*args.lr, \'weight_decay\': 0},\n        {\'params\': param_groups[2], \'lr\': 10*args.lr, \'weight_decay\': args.wt_dec},\n        {\'params\': param_groups[3], \'lr\': 20*args.lr, \'weight_decay\': 0}\n    ], lr=args.lr, weight_decay=args.wt_dec, max_step=max_step)\n\n    if args.weights[-7:] == \'.params\':\n        assert args.network == ""network.resnet38_cls""\n        import network.resnet38d\n        weights_dict = network.resnet38d.convert_mxnet_to_torch(args.weights)\n    elif args.weights[-11:] == \'.caffemodel\':\n        assert args.network == ""network.vgg16_cls""\n        import network.vgg16d\n        weights_dict = network.vgg16d.convert_caffe_to_torch(args.weights)\n    else:\n        weights_dict = torch.load(args.weights)\n\n    model.load_state_dict(weights_dict, strict=False)\n    model = torch.nn.DataParallel(model).cuda()\n    model.train()\n\n    avg_meter = pyutils.AverageMeter(\'loss\')\n\n    timer = pyutils.Timer(""Session started: "")\n\n    for ep in range(args.max_epoches):\n\n        for iter, pack in enumerate(train_data_loader):\n\n            img = pack[1]\n            label = pack[2].cuda(non_blocking=True)\n\n            x = model(img)\n            loss = F.multilabel_soft_margin_loss(x, label)\n\n            avg_meter.add({\'loss\': loss.item()})\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            if (optimizer.global_step-1)%50 == 0:\n                timer.update_progress(optimizer.global_step / max_step)\n\n                print(\'Iter:%5d/%5d\' % (optimizer.global_step - 1, max_step),\n                      \'Loss:%.4f\' % (avg_meter.pop(\'loss\')),\n                      \'imps:%.1f\' % ((iter+1) * args.batch_size / timer.get_stage_elapsed()),\n                      \'Fin:%s\' % (timer.str_est_finish()),\n                      \'lr: %.4f\' % (optimizer.param_groups[0][\'lr\']), flush=True)\n\n        else:\n            validate(model, val_data_loader)\n            timer.reset_stage()\n\n    torch.save(model.module.state_dict(), args.session_name + \'.pth\')\n'"
network/resnet38_aff.py,24,"b""import torch\nimport torch.nn as nn\nimport torch.sparse as sparse\nimport torch.nn.functional as F\n\nimport network.resnet38d\nfrom tool import pyutils\n\nclass Net(network.resnet38d.Net):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.f8_3 = torch.nn.Conv2d(512, 64, 1, bias=False)\n        self.f8_4 = torch.nn.Conv2d(1024, 128, 1, bias=False)\n        self.f8_5 = torch.nn.Conv2d(4096, 256, 1, bias=False)\n\n        self.f9 = torch.nn.Conv2d(448, 448, 1, bias=False)\n        \n        torch.nn.init.kaiming_normal_(self.f8_3.weight)\n        torch.nn.init.kaiming_normal_(self.f8_4.weight)\n        torch.nn.init.kaiming_normal_(self.f8_5.weight)\n        torch.nn.init.xavier_uniform_(self.f9.weight, gain=4)\n\n        self.not_training = [self.conv1a, self.b2, self.b2_1, self.b2_2]\n\n        self.from_scratch_layers = [self.f8_3, self.f8_4, self.f8_5, self.f9]\n\n        self.predefined_featuresize = int(448//8)\n        self.ind_from, self.ind_to = pyutils.get_indices_of_pairs(radius=5, size=(self.predefined_featuresize, self.predefined_featuresize))\n        self.ind_from = torch.from_numpy(self.ind_from); self.ind_to = torch.from_numpy(self.ind_to)\n\n        return\n\n    def forward(self, x, to_dense=False):\n\n        d = super().forward_as_dict(x)\n\n        f8_3 = F.elu(self.f8_3(d['conv4']))\n        f8_4 = F.elu(self.f8_4(d['conv5']))\n        f8_5 = F.elu(self.f8_5(d['conv6']))\n        x = F.elu(self.f9(torch.cat([f8_3, f8_4, f8_5], dim=1)))\n\n        if x.size(2) == self.predefined_featuresize and x.size(3) == self.predefined_featuresize:\n            ind_from = self.ind_from\n            ind_to = self.ind_to\n        else:\n            ind_from, ind_to = pyutils.get_indices_of_pairs(5, (x.size(2), x.size(3)))\n            ind_from = torch.from_numpy(ind_from); ind_to = torch.from_numpy(ind_to)\n\n        x = x.view(x.size(0), x.size(1), -1)\n\n        ff = torch.index_select(x, dim=2, index=ind_from.cuda(non_blocking=True))\n        ft = torch.index_select(x, dim=2, index=ind_to.cuda(non_blocking=True))\n\n        ff = torch.unsqueeze(ff, dim=2)\n        ft = ft.view(ft.size(0), ft.size(1), -1, ff.size(3))\n\n        aff = torch.exp(-torch.mean(torch.abs(ft-ff), dim=1))\n\n        if to_dense:\n            aff = aff.view(-1).cpu()\n\n            ind_from_exp = torch.unsqueeze(ind_from, dim=0).expand(ft.size(2), -1).contiguous().view(-1)\n            indices = torch.stack([ind_from_exp, ind_to])\n            indices_tp = torch.stack([ind_to, ind_from_exp])\n\n            area = x.size(2)\n            indices_id = torch.stack([torch.arange(0, area).long(), torch.arange(0, area).long()])\n\n            aff_mat = sparse.FloatTensor(torch.cat([indices, indices_id, indices_tp], dim=1),\n                                      torch.cat([aff, torch.ones([area]), aff])).to_dense().cuda()\n\n            return aff_mat\n\n        else:\n            return aff\n\n\n    def get_parameter_groups(self):\n        groups = ([], [], [], [])\n\n        for m in self.modules():\n\n            if (isinstance(m, nn.Conv2d) or isinstance(m, nn.modules.normalization.GroupNorm)):\n\n                if m.weight.requires_grad:\n                    if m in self.from_scratch_layers:\n                        groups[2].append(m.weight)\n                    else:\n                        groups[0].append(m.weight)\n\n                if m.bias is not None and m.bias.requires_grad:\n\n                    if m in self.from_scratch_layers:\n                        groups[3].append(m.bias)\n                    else:\n                        groups[1].append(m.bias)\n\n        return groups\n\n\n\n"""
network/resnet38_cls.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport network.resnet38d\n\n\nclass Net(network.resnet38d.Net):\n    def __init__(self):\n        super().__init__()\n\n        self.dropout7 = torch.nn.Dropout2d(0.5)\n\n        self.fc8 = nn.Conv2d(4096, 20, 1, bias=False)\n        torch.nn.init.xavier_uniform_(self.fc8.weight)\n\n        self.not_training = [self.conv1a, self.b2, self.b2_1, self.b2_2]\n        self.from_scratch_layers = [self.fc8]\n\n\n    def forward(self, x):\n        x = super().forward(x)\n        x = self.dropout7(x)\n\n        x = F.avg_pool2d(\n            x, kernel_size=(x.size(2), x.size(3)), padding=0)\n\n        x = self.fc8(x)\n        x = x.view(x.size(0), -1)\n\n        return x\n\n    def forward_cam(self, x):\n        x = super().forward(x)\n\n        x = F.conv2d(x, self.fc8.weight)\n        x = F.relu(x)\n\n        return x\n\n    def get_parameter_groups(self):\n        groups = ([], [], [], [])\n\n        for m in self.modules():\n\n            if isinstance(m, nn.Conv2d):\n\n                if m.weight.requires_grad:\n                    if m in self.from_scratch_layers:\n                        groups[2].append(m.weight)\n                    else:\n                        groups[0].append(m.weight)\n\n                if m.bias is not None and m.bias.requires_grad:\n\n                    if m in self.from_scratch_layers:\n                        groups[3].append(m.bias)\n                    else:\n                        groups[1].append(m.bias)\n\n        return groups'"
network/resnet38d.py,7,"b""import torch\nfrom torch import nn\nimport numpy as np\n\nimport torch.nn.functional as F\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, mid_channels, out_channels, stride=1, first_dilation=None, dilation=1):\n        super(ResBlock, self).__init__()\n\n        self.same_shape = (in_channels == out_channels and stride == 1)\n\n        if first_dilation == None: first_dilation = dilation\n\n        self.bn_branch2a = nn.BatchNorm2d(in_channels)\n\n        self.conv_branch2a = nn.Conv2d(in_channels, mid_channels, 3, stride,\n                                       padding=first_dilation, dilation=first_dilation, bias=False)\n\n        self.bn_branch2b1 = nn.BatchNorm2d(mid_channels)\n\n        self.conv_branch2b1 = nn.Conv2d(mid_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False)\n\n        if not self.same_shape:\n            self.conv_branch1 = nn.Conv2d(in_channels, out_channels, 1, stride, bias=False)\n\n    def forward(self, x, get_x_bn_relu=False):\n\n        branch2 = self.bn_branch2a(x)\n        branch2 = F.relu(branch2)\n\n        x_bn_relu = branch2\n\n        if not self.same_shape:\n            branch1 = self.conv_branch1(branch2)\n        else:\n            branch1 = x\n\n        branch2 = self.conv_branch2a(branch2)\n        branch2 = self.bn_branch2b1(branch2)\n        branch2 = F.relu(branch2)\n        branch2 = self.conv_branch2b1(branch2)\n\n        x = branch1 + branch2\n\n        if get_x_bn_relu:\n            return x, x_bn_relu\n\n        return x\n\n    def __call__(self, x, get_x_bn_relu=False):\n        return self.forward(x, get_x_bn_relu=get_x_bn_relu)\n\nclass ResBlock_bot(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, dilation=1, dropout=0.):\n        super(ResBlock_bot, self).__init__()\n\n        self.same_shape = (in_channels == out_channels and stride == 1)\n\n        self.bn_branch2a = nn.BatchNorm2d(in_channels)\n        self.conv_branch2a = nn.Conv2d(in_channels, out_channels//4, 1, stride, bias=False)\n\n        self.bn_branch2b1 = nn.BatchNorm2d(out_channels//4)\n        self.dropout_2b1 = torch.nn.Dropout2d(dropout)\n        self.conv_branch2b1 = nn.Conv2d(out_channels//4, out_channels//2, 3, padding=dilation, dilation=dilation, bias=False)\n\n        self.bn_branch2b2 = nn.BatchNorm2d(out_channels//2)\n        self.dropout_2b2 = torch.nn.Dropout2d(dropout)\n        self.conv_branch2b2 = nn.Conv2d(out_channels//2, out_channels, 1, bias=False)\n\n        if not self.same_shape:\n            self.conv_branch1 = nn.Conv2d(in_channels, out_channels, 1, stride, bias=False)\n\n    def forward(self, x, get_x_bn_relu=False):\n\n        branch2 = self.bn_branch2a(x)\n        branch2 = F.relu(branch2)\n        x_bn_relu = branch2\n\n        branch1 = self.conv_branch1(branch2)\n\n        branch2 = self.conv_branch2a(branch2)\n\n        branch2 = self.bn_branch2b1(branch2)\n        branch2 = F.relu(branch2)\n        branch2 = self.dropout_2b1(branch2)\n        branch2 = self.conv_branch2b1(branch2)\n\n        branch2 = self.bn_branch2b2(branch2)\n        branch2 = F.relu(branch2)\n        branch2 = self.dropout_2b2(branch2)\n        branch2 = self.conv_branch2b2(branch2)\n\n        x = branch1 + branch2\n\n        if get_x_bn_relu:\n            return x, x_bn_relu\n\n        return x\n\n    def __call__(self, x, get_x_bn_relu=False):\n        return self.forward(x, get_x_bn_relu=get_x_bn_relu)\n\nclass Normalize():\n    def __init__(self, mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)):\n\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, img):\n        imgarr = np.asarray(img)\n        proc_img = np.empty_like(imgarr, np.float32)\n\n        proc_img[..., 0] = (imgarr[..., 0] / 255. - self.mean[0]) / self.std[0]\n        proc_img[..., 1] = (imgarr[..., 1] / 255. - self.mean[1]) / self.std[1]\n        proc_img[..., 2] = (imgarr[..., 2] / 255. - self.mean[2]) / self.std[2]\n\n        return proc_img\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.conv1a = nn.Conv2d(3, 64, 3, padding=1, bias=False)\n\n        self.b2 = ResBlock(64, 128, 128, stride=2)\n        self.b2_1 = ResBlock(128, 128, 128)\n        self.b2_2 = ResBlock(128, 128, 128)\n\n        self.b3 = ResBlock(128, 256, 256, stride=2)\n        self.b3_1 = ResBlock(256, 256, 256)\n        self.b3_2 = ResBlock(256, 256, 256)\n\n        self.b4 = ResBlock(256, 512, 512, stride=2)\n        self.b4_1 = ResBlock(512, 512, 512)\n        self.b4_2 = ResBlock(512, 512, 512)\n        self.b4_3 = ResBlock(512, 512, 512)\n        self.b4_4 = ResBlock(512, 512, 512)\n        self.b4_5 = ResBlock(512, 512, 512)\n\n        self.b5 = ResBlock(512, 512, 1024, stride=1, first_dilation=1, dilation=2)\n        self.b5_1 = ResBlock(1024, 512, 1024, dilation=2)\n        self.b5_2 = ResBlock(1024, 512, 1024, dilation=2)\n\n        self.b6 = ResBlock_bot(1024, 2048, stride=1, dilation=4, dropout=0.3)\n\n        self.b7 = ResBlock_bot(2048, 4096, dilation=4, dropout=0.5)\n\n        self.bn7 = nn.BatchNorm2d(4096)\n\n        self.not_training = [self.conv1a]\n\n        self.normalize = Normalize()\n\n        return\n\n    def forward(self, x):\n        return self.forward_as_dict(x)['conv6']\n\n    def forward_as_dict(self, x):\n\n        x = self.conv1a(x)\n\n        x = self.b2(x)\n        x = self.b2_1(x)\n        x = self.b2_2(x)\n\n        x = self.b3(x)\n        x = self.b3_1(x)\n        x = self.b3_2(x)\n\n        x = self.b4(x)\n        x = self.b4_1(x)\n        x = self.b4_2(x)\n        x = self.b4_3(x)\n        x = self.b4_4(x)\n        x = self.b4_5(x)\n\n        x, conv4 = self.b5(x, get_x_bn_relu=True)\n        x = self.b5_1(x)\n        x = self.b5_2(x)\n\n        x, conv5 = self.b6(x, get_x_bn_relu=True)\n\n        x = self.b7(x)\n        conv6 = F.relu(self.bn7(x))\n\n        return dict({'conv4': conv4, 'conv5': conv5, 'conv6': conv6})\n\n\n    def train(self, mode=True):\n\n        super().train(mode)\n\n        for layer in self.not_training:\n\n            if isinstance(layer, torch.nn.Conv2d):\n                layer.weight.requires_grad = False\n\n            elif isinstance(layer, torch.nn.Module):\n                for c in layer.children():\n                    c.weight.requires_grad = False\n                    if c.bias is not None:\n                        c.bias.requires_grad = False\n\n        for layer in self.modules():\n\n            if isinstance(layer, torch.nn.BatchNorm2d):\n                layer.eval()\n                layer.bias.requires_grad = False\n                layer.weight.requires_grad = False\n\n        return\n\ndef convert_mxnet_to_torch(filename):\n    import mxnet\n\n    save_dict = mxnet.nd.load(filename)\n\n    renamed_dict = dict()\n\n    bn_param_mx_pt = {'beta': 'bias', 'gamma': 'weight', 'mean': 'running_mean', 'var': 'running_var'}\n\n    for k, v in save_dict.items():\n\n        v = torch.from_numpy(v.asnumpy())\n        toks = k.split('_')\n\n        if 'conv1a' in toks[0]:\n            renamed_dict['conv1a.weight'] = v\n\n        elif 'linear1000' in toks[0]:\n            pass\n\n        elif 'branch' in toks[1]:\n\n            pt_name = []\n\n            if toks[0][-1] != 'a':\n                pt_name.append('b' + toks[0][-3] + '_' + toks[0][-1])\n            else:\n                pt_name.append('b' + toks[0][-2])\n\n            if 'res' in toks[0]:\n                layer_type = 'conv'\n                last_name = 'weight'\n\n            else:  # 'bn' in toks[0]:\n                layer_type = 'bn'\n                last_name = bn_param_mx_pt[toks[-1]]\n\n            pt_name.append(layer_type + '_' + toks[1])\n\n            pt_name.append(last_name)\n\n            torch_name = '.'.join(pt_name)\n            renamed_dict[torch_name] = v\n\n        else:\n            last_name = bn_param_mx_pt[toks[-1]]\n            renamed_dict['bn7.' + last_name] = v\n\n    return renamed_dict\n\n"""
network/vgg16_aff.py,21,"b""import torch\nimport torch.nn as nn\nimport torch.sparse as sparse\nimport torch.nn.functional as F\nfrom tool import pyutils\n\nimport network.vgg16d\n\nclass Net(network.vgg16d.Net):\n    def __init__(self):\n        super(Net, self).__init__(fc6_dilation=4)\n\n        self.f8_3 = nn.Conv2d(512, 64, 1, bias=False)\n        self.f8_4 = nn.Conv2d(512, 128, 1, bias=False)\n        self.f8_5 = nn.Conv2d(1024, 256, 1, bias=False)\n        self.gn8_3 = nn.modules.normalization.GroupNorm(8, 64)\n        self.gn8_4 = nn.modules.normalization.GroupNorm(16, 128)\n        self.gn8_5 = nn.modules.normalization.GroupNorm(32, 256)\n\n        self.f9 = torch.nn.Conv2d(448, 448, 1, bias=False)\n\n        torch.nn.init.kaiming_normal_(self.f8_3.weight)\n        torch.nn.init.kaiming_normal_(self.f8_4.weight)\n        torch.nn.init.kaiming_normal_(self.f8_5.weight)\n        torch.nn.init.xavier_uniform_(self.f9.weight, gain=4)\n\n        self.not_training = [self.conv1_1, self.conv1_2, self.conv2_1, self.conv2_2]\n        self.from_scratch_layers = [self.f8_3, self.f8_4, self.f8_5, self.f9]\n\n        self.predefined_featuresize = int(448//8)\n        self.ind_from, self.ind_to = pyutils.get_indices_of_pairs(5, (self.predefined_featuresize, self.predefined_featuresize))\n        self.ind_from = torch.from_numpy(self.ind_from); self.ind_to = torch.from_numpy(self.ind_to)\n\n        return\n\n\n    def forward(self, x, to_dense=False):\n\n        d = super().forward_as_dict(x)\n\n        f8_3 = F.elu(self.gn8_3(self.f8_3(d['conv4'])))\n        f8_4 = F.elu(self.gn8_4(self.f8_4(d['conv5'])))\n        f8_5 = F.elu(self.gn8_5(self.f8_5(d['conv5fc'])))\n\n        x = torch.cat([f8_3, f8_4, f8_5], dim=1)\n        x = F.elu(self.f9(x))\n\n        if x.size(2) == self.predefined_featuresize and x.size(3) == self.predefined_featuresize:\n            ind_from = self.ind_from\n            ind_to = self.ind_to\n        else:\n            ind_from, ind_to = pyutils.get_indices_of_pairs(5, (x.size(2), x.size(3)))\n            ind_from = torch.from_numpy(ind_from); ind_to = torch.from_numpy(ind_to)\n\n        x = x.view(x.size(0), x.size(1), -1)\n\n        ff = torch.index_select(x, dim=2, index=ind_from.cuda(non_blocking=True))\n        ft = torch.index_select(x, dim=2, index=ind_to.cuda(non_blocking=True))\n\n        ff = torch.unsqueeze(ff, dim=2)\n        ft = ft.view(ft.size(0), ft.size(1), -1, ff.size(3))\n\n        aff = torch.exp(-torch.mean(torch.abs(ft-ff), dim=1))\n\n        if to_dense:\n            aff = aff.view(-1).cpu()\n\n            ind_from_exp = torch.unsqueeze(ind_from, dim=0).expand(ft.size(2), -1).contiguous().view(-1)\n            indices = torch.stack([ind_from_exp, ind_to])\n            indices_tp = torch.stack([ind_to, ind_from_exp])\n\n            area = x.size(2)\n            indices_id = torch.stack([torch.arange(0, area).long(), torch.arange(0, area).long()])\n\n            aff_mat = sparse.FloatTensor(torch.cat([indices, indices_id, indices_tp], dim=1),\n                                      torch.cat([aff, torch.ones([area]), aff])).to_dense().cuda()\n            return aff_mat\n\n        else:\n            return aff\n\n    def get_parameter_groups(self):\n        groups = ([], [], [], [])\n\n        for m in self.modules():\n\n            if (isinstance(m, nn.Conv2d) or isinstance(m, nn.modules.normalization.GroupNorm)):\n\n                if m.weight.requires_grad:\n                    if m in self.from_scratch_layers:\n                        groups[2].append(m.weight)\n                    else:\n                        groups[0].append(m.weight)\n\n                if m.bias is not None and m.bias.requires_grad:\n\n                    if m in self.from_scratch_layers:\n                        groups[3].append(m.bias)\n                    else:\n                        groups[1].append(m.bias)\n\n        return groups\n\n\n\n"""
network/vgg16_cls.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport network.vgg16d\n\nclass Net(network.vgg16d.Net):\n\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.drop7 = nn.Dropout2d(p=0.5)\n        self.fc8 = nn.Conv2d(1024, 20, 1, bias=False)\n        torch.nn.init.xavier_uniform_(self.fc8.weight)\n\n        self.not_training = [self.conv1_1, self.conv1_2,\n                             self.conv2_1, self.conv2_2]\n        self.from_scratch_layers = [self.fc8]\n\n    def forward(self, x):\n        x = super().forward(x)\n        x = self.drop7(x)\n\n        x = self.fc8(x)\n\n        x = F.avg_pool2d(x, kernel_size=(x.size(2), x.size(3)), padding=0)\n\n        x = x.view(-1, 20)\n\n        return x\n\n    def forward_cam(self, x):\n        x = super().forward(x)\n        x = self.fc8(x)\n        x = F.relu(x)\n        x = torch.sqrt(x)\n        return x\n\n    def fix_bn(self):\n        self.bn8.eval()\n        self.bn8.weight.requires_grad = False\n        self.bn8.bias.requires_grad = False\n\n    def get_parameter_groups(self):\n        groups = ([], [], [], [])\n\n        for m in self.modules():\n\n            if (isinstance(m, nn.Conv2d) or isinstance(m, nn.BatchNorm2d)):\n\n                if m.weight is not None and m.weight.requires_grad:\n                    if m in self.from_scratch_layers:\n                        groups[2].append(m.weight)\n                    else:\n                        groups[0].append(m.weight)\n\n                if m.bias is not None and m.bias.requires_grad:\n\n                    if m in self.from_scratch_layers:\n                        groups[3].append(m.bias)\n                    else:\n                        groups[1].append(m.bias)\n\n        return groups'"
network/vgg16d.py,4,"b""import torch\nfrom torch import nn\nimport numpy as np\nimport torch.nn.functional as F\n\nclass Normalize():\n    def __init__(self, mean = (122.675, 116.669, 104.008)):\n\n        self.mean = mean\n\n    def __call__(self, img):\n        imgarr = np.asarray(img)\n        proc_img = np.empty_like(imgarr, np.float32)\n\n        proc_img[..., 0] = (imgarr[..., 2] - self.mean[2])\n        proc_img[..., 1] = (imgarr[..., 1] - self.mean[1])\n        proc_img[..., 2] = (imgarr[..., 0] - self.mean[0])\n\n        return proc_img\n\nclass Net(nn.Module):\n    def __init__(self, fc6_dilation = 1):\n        super(Net, self).__init__()\n\n        self.conv1_1 = nn.Conv2d(3,64,3,padding = 1)\n        self.conv1_2 = nn.Conv2d(64,64,3,padding = 1)\n        self.pool1 = nn.MaxPool2d(kernel_size = 3, stride = 2, padding=1)\n        self.conv2_1 = nn.Conv2d(64,128,3,padding = 1)\n        self.conv2_2 = nn.Conv2d(128,128,3,padding = 1)\n        self.pool2 = nn.MaxPool2d(kernel_size = 3, stride = 2, padding=1)\n        self.conv3_1 = nn.Conv2d(128,256,3,padding = 1)\n        self.conv3_2 = nn.Conv2d(256,256,3,padding = 1)\n        self.conv3_3 = nn.Conv2d(256,256,3,padding = 1)\n        self.pool3 = nn.MaxPool2d(kernel_size = 3, stride = 2, padding=1)\n        self.conv4_1 = nn.Conv2d(256,512,3,padding = 1)\n        self.conv4_2 = nn.Conv2d(512,512,3,padding = 1)\n        self.conv4_3 = nn.Conv2d(512,512,3,padding = 1)\n        self.pool4 = nn.MaxPool2d(kernel_size = 3, stride = 1, padding=1)\n        self.conv5_1 = nn.Conv2d(512,512,3,padding = 2, dilation = 2)\n        self.conv5_2 = nn.Conv2d(512,512,3,padding = 2, dilation = 2)\n        self.conv5_3 = nn.Conv2d(512,512,3,padding = 2, dilation = 2)\n        self.pool5 = nn.MaxPool2d(kernel_size = 3, stride = 1, padding=1)\n        self.pool5a = nn.AvgPool2d(kernel_size = 3, stride = 1, padding=1)\n\n        self.fc6 = nn.Conv2d(512,1024, 3, padding = fc6_dilation, dilation = fc6_dilation)\n\n        self.drop6 = nn.Dropout2d(p=0.5)\n        self.fc7 = nn.Conv2d(1024,1024,1)\n\n        self.normalize = Normalize()\n\n        return\n\n    def forward(self, x):\n        return self.forward_as_dict(x)['conv5fc']\n\n    def forward_as_dict(self, x):\n\n        x = F.relu(self.conv1_1(x))\n        x = F.relu(self.conv1_2(x))\n        x = self.pool1(x)\n\n        x = F.relu(self.conv2_1(x))\n        x = F.relu(self.conv2_2(x))\n        x = self.pool2(x)\n\n        x = F.relu(self.conv3_1(x))\n        x = F.relu(self.conv3_2(x))\n        x = F.relu(self.conv3_3(x))\n        x = self.pool3(x)\n\n        x = F.relu(self.conv4_1(x))\n        x = F.relu(self.conv4_2(x))\n        x = F.relu(self.conv4_3(x))\n        conv4 = x\n\n        x = self.pool4(x)\n\n        x = F.relu(self.conv5_1(x))\n        x = F.relu(self.conv5_2(x))\n        x = F.relu(self.conv5_3(x))\n        conv5 = x\n\n        x = F.relu(self.fc6(x))\n        x = self.drop6(x)\n        x = F.relu(self.fc7(x))\n\n        conv5fc = x\n\n        return dict({'conv4': conv4, 'conv5': conv5, 'conv5fc': conv5fc})\n\n    def train(self, mode=True):\n\n        super().train(mode)\n\n        for layer in self.not_training:\n\n            if isinstance(layer, torch.nn.Conv2d):\n\n                layer.weight.requires_grad = False\n                layer.bias.requires_grad = False\n\ndef convert_caffe_to_torch(caffemodel_path, prototxt_path='network/vgg16_20M.prototxt'):\n    import caffe\n\n    caffe_model = caffe.Net(prototxt_path, caffemodel_path, caffe.TEST)\n\n    dict = {}\n    for caffe_name in list(caffe_model.params.keys()):\n        dict[caffe_name + '.weight'] = torch.from_numpy(caffe_model.params[caffe_name][0].data)\n        dict[caffe_name + '.bias'] = torch.from_numpy(caffe_model.params[caffe_name][1].data)\n\n    return dict\n\n\n\n\n"""
tool/imutils.py,0,"b'\nimport PIL.Image\nimport random\nimport numpy as np\n\nclass RandomResizeLong():\n\n    def __init__(self, min_long, max_long):\n        self.min_long = min_long\n        self.max_long = max_long\n\n    def __call__(self, img):\n\n        target_long = random.randint(self.min_long, self.max_long)\n        w, h = img.size\n\n        if w < h:\n            target_shape = (int(round(w * target_long / h)), target_long)\n        else:\n            target_shape = (target_long, int(round(h * target_long / w)))\n\n        img = img.resize(target_shape, resample=PIL.Image.CUBIC)\n\n        return img\n\n\nclass RandomCrop():\n\n    def __init__(self, cropsize):\n        self.cropsize = cropsize\n\n    def __call__(self, imgarr):\n\n        h, w, c = imgarr.shape\n\n        ch = min(self.cropsize, h)\n        cw = min(self.cropsize, w)\n\n        w_space = w - self.cropsize\n        h_space = h - self.cropsize\n\n        if w_space > 0:\n            cont_left = 0\n            img_left = random.randrange(w_space+1)\n        else:\n            cont_left = random.randrange(-w_space+1)\n            img_left = 0\n\n        if h_space > 0:\n            cont_top = 0\n            img_top = random.randrange(h_space+1)\n        else:\n            cont_top = random.randrange(-h_space+1)\n            img_top = 0\n\n        container = np.zeros((self.cropsize, self.cropsize, imgarr.shape[-1]), np.float32)\n        container[cont_top:cont_top+ch, cont_left:cont_left+cw] = \\\n            imgarr[img_top:img_top+ch, img_left:img_left+cw]\n\n        return container\n\ndef get_random_crop_box(imgsize, cropsize):\n    h, w = imgsize\n\n    ch = min(cropsize, h)\n    cw = min(cropsize, w)\n\n    w_space = w - cropsize\n    h_space = h - cropsize\n\n    if w_space > 0:\n        cont_left = 0\n        img_left = random.randrange(w_space + 1)\n    else:\n        cont_left = random.randrange(-w_space + 1)\n        img_left = 0\n\n    if h_space > 0:\n        cont_top = 0\n        img_top = random.randrange(h_space + 1)\n    else:\n        cont_top = random.randrange(-h_space + 1)\n        img_top = 0\n\n    return cont_top, cont_top+ch, cont_left, cont_left+cw, img_top, img_top+ch, img_left, img_left+cw\n\ndef crop_with_box(img, box):\n    if len(img.shape) == 3:\n        img_cont = np.zeros((max(box[1]-box[0], box[4]-box[5]), max(box[3]-box[2], box[7]-box[6]), img.shape[-1]), dtype=img.dtype)\n    else:\n        img_cont = np.zeros((max(box[1] - box[0], box[4] - box[5]), max(box[3] - box[2], box[7] - box[6])), dtype=img.dtype)\n    img_cont[box[0]:box[1], box[2]:box[3]] = img[box[4]:box[5], box[6]:box[7]]\n    return img_cont\n\n\ndef random_crop(images, cropsize, fills):\n    if isinstance(images[0], PIL.Image.Image):\n        imgsize = images[0].size[::-1]\n    else:\n        imgsize = images[0].shape[:2]\n    box = get_random_crop_box(imgsize, cropsize)\n\n    new_images = []\n    for img, f in zip(images, fills):\n\n        if isinstance(img, PIL.Image.Image):\n            img = img.crop((box[6], box[4], box[7], box[5]))\n            cont = PIL.Image.new(img.mode, (cropsize, cropsize))\n            cont.paste(img, (box[2], box[0]))\n            new_images.append(cont)\n\n        else:\n            if len(img.shape) == 3:\n                cont = np.ones((cropsize, cropsize, img.shape[2]), img.dtype)*f\n            else:\n                cont = np.ones((cropsize, cropsize), img.dtype)*f\n            cont[box[0]:box[1], box[2]:box[3]] = img[box[4]:box[5], box[6]:box[7]]\n            new_images.append(cont)\n\n    return new_images\n\n\nclass AvgPool2d():\n\n    def __init__(self, ksize):\n        self.ksize = ksize\n\n    def __call__(self, img):\n        import skimage.measure\n\n        return skimage.measure.block_reduce(img, (self.ksize, self.ksize, 1), np.mean)\n\n\nclass RandomHorizontalFlip():\n    def __init__(self):\n        return\n\n    def __call__(self, img):\n        if bool(random.getrandbits(1)):\n            img = np.fliplr(img).copy()\n        return img\n\n\nclass CenterCrop():\n\n    def __init__(self, cropsize, default_value=0):\n        self.cropsize = cropsize\n        self.default_value = default_value\n\n    def __call__(self, npimg):\n\n        h, w = npimg.shape[:2]\n\n        ch = min(self.cropsize, h)\n        cw = min(self.cropsize, w)\n\n        sh = h - self.cropsize\n        sw = w - self.cropsize\n\n        if sw > 0:\n            cont_left = 0\n            img_left = int(round(sw / 2))\n        else:\n            cont_left = int(round(-sw / 2))\n            img_left = 0\n\n        if sh > 0:\n            cont_top = 0\n            img_top = int(round(sh / 2))\n        else:\n            cont_top = int(round(-sh / 2))\n            img_top = 0\n\n        if len(npimg.shape) == 2:\n            container = np.ones((self.cropsize, self.cropsize), npimg.dtype)*self.default_value\n        else:\n            container = np.ones((self.cropsize, self.cropsize, npimg.shape[2]), npimg.dtype)*self.default_value\n\n        container[cont_top:cont_top+ch, cont_left:cont_left+cw] = \\\n            npimg[img_top:img_top+ch, img_left:img_left+cw]\n\n        return container\n\n\ndef HWC_to_CHW(img):\n    return np.transpose(img, (2, 0, 1))\n\n\nclass RescaleNearest():\n    def __init__(self, scale):\n        self.scale = scale\n\n    def __call__(self, npimg):\n        import cv2\n        return cv2.resize(npimg, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n\n\n\n\ndef crf_inference(img, probs, t=10, scale_factor=1, labels=21):\n    import pydensecrf.densecrf as dcrf\n    from pydensecrf.utils import unary_from_softmax\n\n    h, w = img.shape[:2]\n    n_labels = labels\n\n    d = dcrf.DenseCRF2D(w, h, n_labels)\n\n    unary = unary_from_softmax(probs)\n    unary = np.ascontiguousarray(unary)\n\n    d.setUnaryEnergy(unary)\n    d.addPairwiseGaussian(sxy=3/scale_factor, compat=3)\n    d.addPairwiseBilateral(sxy=80/scale_factor, srgb=13, rgbim=np.copy(img), compat=10)\n    Q = d.inference(t)\n\n    return np.array(Q).reshape((n_labels, h, w))'"
tool/pyutils.py,0,"b'\nimport numpy as np\nimport time\nimport sys\n\nclass Logger(object):\n    def __init__(self, outfile):\n        self.terminal = sys.stdout\n        self.log = open(outfile, ""w"")\n        sys.stdout = self\n\n    def write(self, message):\n        self.terminal.write(message)\n        self.log.write(message)\n\n    def flush(self):\n        self.terminal.flush()\n\n\nclass AverageMeter:\n    def __init__(self, *keys):\n        self.__data = dict()\n        for k in keys:\n            self.__data[k] = [0.0, 0]\n\n    def add(self, dict):\n        for k, v in dict.items():\n            self.__data[k][0] += v\n            self.__data[k][1] += 1\n\n    def get(self, *keys):\n        if len(keys) == 1:\n            return self.__data[keys[0]][0] / self.__data[keys[0]][1]\n        else:\n            v_list = [self.__data[k][0] / self.__data[k][1] for k in keys]\n            return tuple(v_list)\n\n    def pop(self, key=None):\n        if key is None:\n            for k in self.__data.keys():\n                self.__data[k] = [0.0, 0]\n        else:\n            v = self.get(key)\n            self.__data[key] = [0.0, 0]\n            return v\n\n\nclass Timer:\n    def __init__(self, starting_msg = None):\n        self.start = time.time()\n        self.stage_start = self.start\n\n        if starting_msg is not None:\n            print(starting_msg, time.ctime(time.time()))\n\n\n    def update_progress(self, progress):\n        self.elapsed = time.time() - self.start\n        self.est_total = self.elapsed / progress\n        self.est_remaining = self.est_total - self.elapsed\n        self.est_finish = int(self.start + self.est_total)\n\n\n    def str_est_finish(self):\n        return str(time.ctime(self.est_finish))\n\n    def get_stage_elapsed(self):\n        return time.time() - self.stage_start\n\n    def reset_stage(self):\n        self.stage_start = time.time()\n\n\nfrom multiprocessing.pool import ThreadPool\n\nclass BatchThreader:\n\n    def __init__(self, func, args_list, batch_size, prefetch_size=4, processes=12):\n        self.batch_size = batch_size\n        self.prefetch_size = prefetch_size\n\n        self.pool = ThreadPool(processes=processes)\n        self.async_result = []\n\n        self.func = func\n        self.left_args_list = args_list\n        self.n_tasks = len(args_list)\n\n        # initial work\n        self.__start_works(self.__get_n_pending_works())\n\n\n    def __start_works(self, times):\n        for _ in range(times):\n            args = self.left_args_list.pop(0)\n            self.async_result.append(\n                self.pool.apply_async(self.func, args))\n\n\n    def __get_n_pending_works(self):\n        return min((self.prefetch_size + 1) * self.batch_size - len(self.async_result)\n                   , len(self.left_args_list))\n\n\n\n    def pop_results(self):\n\n        n_inwork = len(self.async_result)\n\n        n_fetch = min(n_inwork, self.batch_size)\n        rtn = [self.async_result.pop(0).get()\n                for _ in range(n_fetch)]\n\n        to_fill = self.__get_n_pending_works()\n        if to_fill == 0:\n            self.pool.close()\n        else:\n            self.__start_works(to_fill)\n\n        return rtn\n\n\n\n\ndef get_indices_of_pairs(radius, size):\n\n    search_dist = []\n\n    for x in range(1, radius):\n        search_dist.append((0, x))\n\n    for y in range(1, radius):\n        for x in range(-radius + 1, radius):\n            if x * x + y * y < radius * radius:\n                search_dist.append((y, x))\n\n    radius_floor = radius - 1\n\n    full_indices = np.reshape(np.arange(0, size[0]*size[1], dtype=np.int64),\n                                   (size[0], size[1]))\n\n    cropped_height = size[0] - radius_floor\n    cropped_width = size[1] - 2 * radius_floor\n\n    indices_from = np.reshape(full_indices[:-radius_floor, radius_floor:-radius_floor],\n                              [-1])\n\n    indices_to_list = []\n\n    for dy, dx in search_dist:\n        indices_to = full_indices[dy:dy + cropped_height,\n                     radius_floor + dx:radius_floor + dx + cropped_width]\n        indices_to = np.reshape(indices_to, [-1])\n\n        indices_to_list.append(indices_to)\n\n    concat_indices_to = np.concatenate(indices_to_list, axis=0)\n\n    return indices_from, concat_indices_to\n\n'"
tool/torchutils.py,7,"b'\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport os.path\nimport random\nimport numpy as np\nfrom tool import imutils\n\nclass PolyOptimizer(torch.optim.SGD):\n\n    def __init__(self, params, lr, weight_decay, max_step, momentum=0.9):\n        super().__init__(params, lr, weight_decay)\n\n        self.global_step = 0\n        self.max_step = max_step\n        self.momentum = momentum\n\n        self.__initial_lr = [group[\'lr\'] for group in self.param_groups]\n\n\n    def step(self, closure=None):\n\n        if self.global_step < self.max_step:\n            lr_mult = (1 - self.global_step / self.max_step) ** self.momentum\n\n            for i in range(len(self.param_groups)):\n                self.param_groups[i][\'lr\'] = self.__initial_lr[i] * lr_mult\n\n        super().step(closure)\n\n        self.global_step += 1\n\n\nclass BatchNorm2dFixed(torch.nn.Module):\n\n    def __init__(self, num_features, eps=1e-5):\n        super(BatchNorm2dFixed, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.weight = torch.nn.Parameter(torch.Tensor(num_features))\n        self.bias = torch.nn.Parameter(torch.Tensor(num_features))\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n\n\n    def forward(self, input):\n\n        return F.batch_norm(\n            input, self.running_mean, self.running_var, self.weight, self.bias,\n            False, eps=self.eps)\n\n    def __call__(self, x):\n        return self.forward(x)\n\n\nclass SegmentationDataset(Dataset):\n    def __init__(self, img_name_list_path, img_dir, label_dir, rescale=None, flip=False, cropsize=None,\n                 img_transform=None, mask_transform=None):\n        self.img_name_list_path = img_name_list_path\n        self.img_dir = img_dir\n        self.label_dir = label_dir\n\n        self.img_transform = img_transform\n        self.mask_transform = mask_transform\n\n        self.img_name_list = open(self.img_name_list_path).read().splitlines()\n\n        self.rescale = rescale\n        self.flip = flip\n        self.cropsize = cropsize\n\n    def __len__(self):\n        return len(self.img_name_list)\n\n    def __getitem__(self, idx):\n\n        name = self.img_name_list[idx]\n\n        img = Image.open(os.path.join(self.img_dir, name + \'.jpg\')).convert(""RGB"")\n        mask = Image.open(os.path.join(self.label_dir, name + \'.png\'))\n\n        if self.rescale is not None:\n            s = self.rescale[0] + random.random() * (self.rescale[1] - self.rescale[0])\n            adj_size = (round(img.size[0]*s/8)*8, round(img.size[1]*s/8)*8)\n            img = img.resize(adj_size, resample=Image.CUBIC)\n            mask = img.resize(adj_size, resample=Image.NEAREST)\n\n        if self.img_transform is not None:\n            img = self.img_transform(img)\n        if self.mask_transform is not None:\n            mask = self.mask_transform(mask)\n\n        if self.cropsize is not None:\n            img, mask = imutils.random_crop([img, mask], self.cropsize, (0, 255))\n\n        mask = imutils.RescaleNearest(0.125)(mask)\n\n        if self.flip is True and bool(random.getrandbits(1)):\n            img = np.flip(img, 1).copy()\n            mask = np.flip(mask, 1).copy()\n\n        img = np.transpose(img, (2, 0, 1))\n\n        return name, img, mask\n\n\nclass ExtractAffinityLabelInRadius():\n\n    def __init__(self, cropsize, radius=5):\n        self.radius = radius\n\n        self.search_dist = []\n\n        for x in range(1, radius):\n            self.search_dist.append((0, x))\n\n        for y in range(1, radius):\n            for x in range(-radius+1, radius):\n                if x*x + y*y < radius*radius:\n                    self.search_dist.append((y, x))\n\n        self.radius_floor = radius-1\n\n        self.crop_height = cropsize - self.radius_floor\n        self.crop_width = cropsize - 2 * self.radius_floor\n        return\n\n    def __call__(self, label):\n\n        labels_from = label[:-self.radius_floor, self.radius_floor:-self.radius_floor]\n        labels_from = np.reshape(labels_from, [-1])\n\n        labels_to_list = []\n        valid_pair_list = []\n\n        for dy, dx in self.search_dist:\n            labels_to = label[dy:dy+self.crop_height, self.radius_floor+dx:self.radius_floor+dx+self.crop_width]\n            labels_to = np.reshape(labels_to, [-1])\n\n            valid_pair = np.logical_and(np.less(labels_to, 255), np.less(labels_from, 255))\n\n            labels_to_list.append(labels_to)\n            valid_pair_list.append(valid_pair)\n\n        bc_labels_from = np.expand_dims(labels_from, 0)\n        concat_labels_to = np.stack(labels_to_list)\n        concat_valid_pair = np.stack(valid_pair_list)\n\n        pos_affinity_label = np.equal(bc_labels_from, concat_labels_to)\n\n        bg_pos_affinity_label = np.logical_and(pos_affinity_label, np.equal(bc_labels_from, 0)).astype(np.float32)\n\n        fg_pos_affinity_label = np.logical_and(np.logical_and(pos_affinity_label, np.not_equal(bc_labels_from, 0)), concat_valid_pair).astype(np.float32)\n\n        neg_affinity_label = np.logical_and(np.logical_not(pos_affinity_label), concat_valid_pair).astype(np.float32)\n\n        return bg_pos_affinity_label, fg_pos_affinity_label, neg_affinity_label\n\nclass AffinityFromMaskDataset(SegmentationDataset):\n    def __init__(self, img_name_list_path, img_dir, label_dir, rescale=None, flip=False, cropsize=None,\n                 img_transform=None, mask_transform=None, radius=5):\n        super().__init__(img_name_list_path, img_dir, label_dir, rescale, flip, cropsize, img_transform, mask_transform)\n\n        self.radius = radius\n\n        self.extract_aff_lab_func = ExtractAffinityLabelInRadius(cropsize=cropsize//8, radius=radius)\n\n    def __getitem__(self, idx):\n        name, img, mask = super().__getitem__(idx)\n\n        aff_label = self.extract_aff_lab_func(mask)\n\n        return name, img, aff_label\n'"
voc12/data.py,3,"b'\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport PIL.Image\nimport os.path\nimport scipy.misc\n\nIMG_FOLDER_NAME = ""JPEGImages""\nANNOT_FOLDER_NAME = ""Annotations""\n\nCAT_LIST = [\'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n        \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n        \'cow\', \'diningtable\', \'dog\', \'horse\',\n        \'motorbike\', \'person\', \'pottedplant\',\n        \'sheep\', \'sofa\', \'train\',\n        \'tvmonitor\']\n\nCAT_NAME_TO_NUM = dict(zip(CAT_LIST,range(len(CAT_LIST))))\n\ndef load_image_label_from_xml(img_name, voc12_root):\n    from xml.dom import minidom\n\n    el_list = minidom.parse(os.path.join(voc12_root, ANNOT_FOLDER_NAME,img_name + \'.xml\')).getElementsByTagName(\'name\')\n\n    multi_cls_lab = np.zeros((20), np.float32)\n\n    for el in el_list:\n        cat_name = el.firstChild.data\n        if cat_name in CAT_LIST:\n            cat_num = CAT_NAME_TO_NUM[cat_name]\n            multi_cls_lab[cat_num] = 1.0\n\n    return multi_cls_lab\n\ndef load_image_label_list_from_xml(img_name_list, voc12_root):\n\n    return [load_image_label_from_xml(img_name, voc12_root) for img_name in img_name_list]\n\ndef load_image_label_list_from_npy(img_name_list):\n\n    cls_labels_dict = np.load(\'voc12/cls_labels.npy\').item()\n\n    return [cls_labels_dict[img_name] for img_name in img_name_list]\n\ndef get_img_path(img_name, voc12_root):\n    return os.path.join(voc12_root, IMG_FOLDER_NAME, img_name + \'.jpg\')\n\ndef load_img_name_list(dataset_path):\n\n    img_gt_name_list = open(dataset_path).read().splitlines()\n    img_name_list = [img_gt_name.split(\' \')[0][-15:-4] for img_gt_name in img_gt_name_list]\n\n    return img_name_list\n\nclass VOC12ImageDataset(Dataset):\n\n    def __init__(self, img_name_list_path, voc12_root, transform=None):\n        self.img_name_list = load_img_name_list(img_name_list_path)\n        self.voc12_root = voc12_root\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_name_list)\n\n    def __getitem__(self, idx):\n        name = self.img_name_list[idx]\n\n        img = PIL.Image.open(get_img_path(name, self.voc12_root)).convert(""RGB"")\n\n        if self.transform:\n            img = self.transform(img)\n\n        return name, img\n\n\nclass VOC12ClsDataset(VOC12ImageDataset):\n\n    def __init__(self, img_name_list_path, voc12_root, transform=None):\n        super().__init__(img_name_list_path, voc12_root, transform)\n        self.label_list = load_image_label_list_from_npy(self.img_name_list)\n\n    def __getitem__(self, idx):\n        name, img = super().__getitem__(idx)\n\n        label = torch.from_numpy(self.label_list[idx])\n\n        return name, img, label\n\n\nclass VOC12ClsDatasetMSF(VOC12ClsDataset):\n\n    def __init__(self, img_name_list_path, voc12_root, scales, inter_transform=None, unit=1):\n        super().__init__(img_name_list_path, voc12_root, transform=None)\n        self.scales = scales\n        self.unit = unit\n        self.inter_transform = inter_transform\n\n    def __getitem__(self, idx):\n        name, img, label = super().__getitem__(idx)\n\n        rounded_size = (int(round(img.size[0]/self.unit)*self.unit), int(round(img.size[1]/self.unit)*self.unit))\n\n        ms_img_list = []\n        for s in self.scales:\n            target_size = (round(rounded_size[0]*s),\n                           round(rounded_size[1]*s))\n            s_img = img.resize(target_size, resample=PIL.Image.CUBIC)\n            ms_img_list.append(s_img)\n\n        if self.inter_transform:\n            for i in range(len(ms_img_list)):\n                ms_img_list[i] = self.inter_transform(ms_img_list[i])\n\n        msf_img_list = []\n        for i in range(len(ms_img_list)):\n            msf_img_list.append(ms_img_list[i])\n            msf_img_list.append(np.flip(ms_img_list[i], -1).copy())\n\n        return name, msf_img_list, label\n\n\nclass ExtractAffinityLabelInRadius():\n\n    def __init__(self, cropsize, radius=5):\n        self.radius = radius\n\n        self.search_dist = []\n\n        for x in range(1, radius):\n            self.search_dist.append((0, x))\n\n        for y in range(1, radius):\n            for x in range(-radius+1, radius):\n                if x*x + y*y < radius*radius:\n                    self.search_dist.append((y, x))\n\n        self.radius_floor = radius-1\n\n        self.crop_height = cropsize - self.radius_floor\n        self.crop_width = cropsize - 2 * self.radius_floor\n        return\n\n    def __call__(self, label):\n\n        labels_from = label[:-self.radius_floor, self.radius_floor:-self.radius_floor]\n        labels_from = np.reshape(labels_from, [-1])\n\n        labels_to_list = []\n        valid_pair_list = []\n\n        for dy, dx in self.search_dist:\n            labels_to = label[dy:dy+self.crop_height, self.radius_floor+dx:self.radius_floor+dx+self.crop_width]\n            labels_to = np.reshape(labels_to, [-1])\n\n            valid_pair = np.logical_and(np.less(labels_to, 255), np.less(labels_from, 255))\n\n            labels_to_list.append(labels_to)\n            valid_pair_list.append(valid_pair)\n\n        bc_labels_from = np.expand_dims(labels_from, 0)\n        concat_labels_to = np.stack(labels_to_list)\n        concat_valid_pair = np.stack(valid_pair_list)\n\n        pos_affinity_label = np.equal(bc_labels_from, concat_labels_to)\n\n        bg_pos_affinity_label = np.logical_and(pos_affinity_label, np.equal(bc_labels_from, 0)).astype(np.float32)\n\n        fg_pos_affinity_label = np.logical_and(np.logical_and(pos_affinity_label, np.not_equal(bc_labels_from, 0)), concat_valid_pair).astype(np.float32)\n\n        neg_affinity_label = np.logical_and(np.logical_not(pos_affinity_label), concat_valid_pair).astype(np.float32)\n\n        return torch.from_numpy(bg_pos_affinity_label), torch.from_numpy(fg_pos_affinity_label), torch.from_numpy(neg_affinity_label)\n\n\nclass VOC12AffDataset(VOC12ImageDataset):\n\n    def __init__(self, img_name_list_path, label_la_dir, label_ha_dir, cropsize, voc12_root, radius=5,\n                 joint_transform_list=None, img_transform_list=None, label_transform_list=None):\n        super().__init__(img_name_list_path, voc12_root, transform=None)\n\n        self.label_la_dir = label_la_dir\n        self.label_ha_dir = label_ha_dir\n        self.voc12_root = voc12_root\n\n        self.joint_transform_list = joint_transform_list\n        self.img_transform_list = img_transform_list\n        self.label_transform_list = label_transform_list\n\n        self.extract_aff_lab_func = ExtractAffinityLabelInRadius(cropsize=cropsize//8, radius=radius)\n\n    def __len__(self):\n        return len(self.img_name_list)\n\n    def __getitem__(self, idx):\n        name, img = super().__getitem__(idx)\n\n        label_la_path = os.path.join(self.label_la_dir, name + \'.npy\')\n\n        label_ha_path = os.path.join(self.label_ha_dir, name + \'.npy\')\n\n        label_la = np.load(label_la_path).item()\n        label_ha = np.load(label_ha_path).item()\n\n        label = np.array(list(label_la.values()) + list(label_ha.values()))\n        label = np.transpose(label, (1, 2, 0))\n\n        for joint_transform, img_transform, label_transform \\\n                in zip(self.joint_transform_list, self.img_transform_list, self.label_transform_list):\n\n            if joint_transform:\n                img_label = np.concatenate((img, label), axis=-1)\n                img_label = joint_transform(img_label)\n                img = img_label[..., :3]\n                label = img_label[..., 3:]\n\n            if img_transform:\n                img = img_transform(img)\n            if label_transform:\n                label = label_transform(label)\n\n        no_score_region = np.max(label, -1) < 1e-5\n        label_la, label_ha = np.array_split(label, 2, axis=-1)\n        label_la = np.argmax(label_la, axis=-1).astype(np.uint8)\n        label_ha = np.argmax(label_ha, axis=-1).astype(np.uint8)\n        label = label_la.copy()\n        label[label_la == 0] = 255\n        label[label_ha == 0] = 0\n        label[no_score_region] = 255 # mostly outer of cropped region\n        label = self.extract_aff_lab_func(label)\n\n        return img, label\n\n\nclass VOC12AffGtDataset(VOC12ImageDataset):\n\n    def __init__(self, img_name_list_path, label_dir, cropsize, voc12_root, radius=5,\n                 joint_transform_list=None, img_transform_list=None, label_transform_list=None):\n        super().__init__(img_name_list_path, voc12_root, transform=None)\n\n        self.label_dir = label_dir\n        self.voc12_root = voc12_root\n\n        self.joint_transform_list = joint_transform_list\n        self.img_transform_list = img_transform_list\n        self.label_transform_list = label_transform_list\n\n        self.extract_aff_lab_func = ExtractAffinityLabelInRadius(cropsize=cropsize//8, radius=radius)\n\n    def __len__(self):\n        return len(self.img_name_list)\n\n    def __getitem__(self, idx):\n        name, img = super().__getitem__(idx)\n\n        label_path = os.path.join(self.label_dir, name + \'.png\')\n\n        label = scipy.misc.imread(label_path)\n\n        for joint_transform, img_transform, label_transform \\\n                in zip(self.joint_transform_list, self.img_transform_list, self.label_transform_list):\n\n            if joint_transform:\n                img_label = np.concatenate((img, label), axis=-1)\n                img_label = joint_transform(img_label)\n                img = img_label[..., :3]\n                label = img_label[..., 3:]\n\n            if img_transform:\n                img = img_transform(img)\n            if label_transform:\n                label = label_transform(label)\n\n        label = self.extract_aff_lab_func(label)\n\n        return img, label'"
voc12/make_cls_labels.py,0,"b'import argparse\nimport voc12.data\nimport numpy as np\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--train_list"", default=\'train_aug.txt\', type=str)\n    parser.add_argument(""--val_list"", default=\'val.txt\', type=str)\n    parser.add_argument(""--out"", default=""cls_labels.npy"", type=str)\n    parser.add_argument(""--voc12_root"", required=True, type=str)\n    args = parser.parse_args()\n\n    img_name_list = voc12.data.load_img_name_list(args.train_list)\n    img_name_list.extend(voc12.data.load_img_name_list(args.val_list))\n    label_list = voc12.data.load_image_label_list_from_xml(img_name_list, args.voc12_root)\n\n    d = dict()\n    for img_name, label in zip(img_name_list, label_list):\n        d[img_name] = label\n\n    np.save(args.out, d)'"
