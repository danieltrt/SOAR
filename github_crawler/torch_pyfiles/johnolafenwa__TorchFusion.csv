file_path,api_count,code
setup.py,0,"b'from setuptools import setup,find_packages\n\nsetup(name=""torchfusion"",\n      version=\'0.3.6\',\n      description=\'A modern deep learning framework built to accelerate research and development of AI systems\',\n      url=""https://github.com/johnolafenwa/TorchFusion"",\n      author=\'John Olafenwa and Moses Olafenwa\',\n      license=\'MIT\',\n      packages= find_packages(),\n      install_requires=[\'torchvision\',\'torchtext\',\'numpy\',\'matplotlib\',""tqdm"",""tensorboardX"",""visdom""],\n      zip_safe=False,\n      classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ]\n      )\n'"
torchfusion/__init__.py,0,"b'\nfrom .learners import *\nfrom .datasets import *\nfrom .metrics import *\nfrom .layers import *\nfrom .initializers import *\nfrom .utils import *\nfrom .transforms import *\nfrom .fp16_utils import *\n\n__version__ = ""0.3.4""\n\n__all__ = [""learners"",""datasets"",""metrics"",""layers"",""initializers"",""utils"",""transforms"",""fp16_utils""]\n\n\n'"
torchfusion/datasets/__init__.py,0,"b'from .datasets import mnist_loader,fashionmnist_loader,cifar10_loader,cifar100_loader,emnist_loader,IdenProf,idenprof_loader,svhn_loader,cmpfacades_loader,pathimages_loader,ImagesFromPaths,imagefolder_loader,DataFolder,ImagePool,download_file,extract_zip,extract_tar'"
torchfusion/datasets/datasets.py,5,"b'from zipfile import ZipFile\nimport requests\nimport shutil\nimport os\nimport json\nfrom io import open\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport tarfile\nimport torchvision.transforms.transforms as transformations\nimport numpy as np\nimport torch\nfrom torchvision.datasets import *\nfrom torchvision.datasets.folder import default_loader\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nimport random\n\n\ndef make_dataset(dir, class_to_idx, extensions):\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in sorted(class_to_idx.keys()):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if has_file_allowed_extension(fname, extensions):\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n\n    return images\n\ndef find_classes(self, dir):\n        """"""\n        Finds the class folders in a dataset.\n        Args:\n            dir (string): Root directory path.\n        Returns:\n            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n        Ensures:\n            No class is a subdirectory of another.\n        """"""\n        if sys.version_info >= (3, 5):\n            # Faster and available in Python 3.5 and above\n            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n        else:\n            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n        classes.sort()\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx\n\n\n\ndef download_file(url,path,extract_path=None):\n    """"""\n\n    :param url:\n    :param path:\n    :param extract_path:\n    :return:\n    """"""\n\n    data = requests.get(url, stream=True)\n    with open(path, ""wb"") as file:\n        shutil.copyfileobj(data.raw, file)\n\n    del data\n    if extract_path is not None:\n        if path.endswith("".gz"") or path.endswith("".tgz"") :\n            extract_tar(path,extract_path)\n        else:\n            extract_zip(path, extract_path)\n\ndef extract_zip(source_path,extract_path):\n    """"""\n\n    :param source_path:\n    :param extract_path:\n    :return:\n    """"""\n    extractor = ZipFile(source_path)\n    extractor.extractall(extract_path)\n    extractor.close()\n\ndef extract_tar(source_path,extract_path):\n    """"""\n\n    :param source_path:\n    :param extract_path:\n    :return:\n    """"""\n    with tarfile.open(source_path) as tar:\n        tar.extractall(extract_path)\n\n\n\nclass ImagePool():\n    def __init__(self,pool_size):\n        """"""\n\n        :param pool_size:\n        """"""\n\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.image_array = []\n            self.num_imgs = 0\n\n\n    def query(self,input_images):\n\n        if isinstance(input_images,Variable):\n            input_images = input_images.data\n\n        if self.pool_size == 0:\n            return input_images\n\n        ret_images = []\n\n        for image in input_images:\n            image = image.unsqueeze(0)\n\n            if self.num_imgs < self.pool_size:\n                self.image_array.append(image)\n                self.num_imgs += 1\n                ret_images.append(image)\n\n            else:\n                prob = random.uniform(0,1)\n\n                if prob > 0.5:\n                    random_image_index = random.randint(0,self.pool_size - 1)\n                    ret_images.append(self.image_array[random_image_index])\n                    self.image_array[random_image_index] = image\n                else:\n                    ret_images.append(image)\n\n            return torch.cat(ret_images,0)\n\n\n""""""Creates a dataset containing all images present in the paths specified in the image_paths array\n      Args:\n            image_paths: An array of paths, you can mix folders and files, relative and absolute paths\n            transformations: A set of transformations to be applied per image\n            recursive: causes the paths to be transvered recursively\n            allowed_exts: an array of allowed image extensions\n""""""\n\n\nclass ImagesFromPaths(Dataset):\n    def __init__(self,image_paths,transformations=None,recursive=True,allowed_exts=[\'jpg\', \'jpeg\', \'png\', \'ppm\', \'bmp\', \'pgm\', \'tif\']):\n\n        """"""\n\n        :param image_paths:\n        :param transformations:\n        :param recursive:\n        :param allowed_exts:\n        """"""\n\n        super(ImagesFromPaths,self).__init__()\n\n        assert (isinstance(image_paths,list) or isinstance(image_paths,tuple))\n\n        self.transformations = transformations\n\n        self.image_array = []\n\n        for path in image_paths:\n\n            if os.path.exists(path) == False:\n                path = os.path.join(os.getcwd(),path)\n\n            if os.path.isdir(path):\n\n                if recursive:\n                    for root, dirs, files in os.walk(path):\n                        for fname in files:\n                            fpath = os.path.join(root,fname)\n\n                            if self.__get_extension(fpath) in allowed_exts:\n                                self.image_array.append(fpath)\n                else:\n                    for fpath in os.listdir(path):\n                        fpath = os.path.join(path,fpath)\n                        if self.__get_extension(fpath) in allowed_exts or ""."" + self.__get_extension(fpath) in allowed_exts:\n                            self.image_array.append(fpath)\n\n            elif os.path.isfile(path):\n                if self.__get_extension(path) in allowed_exts or ""."" + self.__get_extension(path) in allowed_exts:\n                    self.image_array.append(path)\n\n    def __get_extension(self,fpath):\n        split = fpath.split(""."")\n        return split[len(split) - 1]\n\n\n    def random_sample(self,batch_size):\n        indexes = np.random.randint(0, self.__len__(), size=(batch_size))\n        images = []\n        for index in indexes:\n            img = Image.open(self.image_array[index]).convert(""RGB"")\n\n            if self.transformations is not None:\n                img = self.transformations(img)\n            images.append(img)\n\n        return torch.stack(images)\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.image_array[index]).convert(""RGB"")\n\n        if self.transformations is not None:\n            img = self.transformations(img)\n\n        return img\n\n    def __len__(self):\n        return len(self.image_array)\n\nclass CMPFacades(Dataset):\n    def __init__(self,root,source_transforms=None,target_transforms=None,set=""train"",download=False,reverse_mode=False):\n\n        """"""\n\n        :param root:\n        :param source_transforms:\n        :param target_transforms:\n        :param set:\n        :param download:\n        :param reverse_mode:\n        """"""\n\n        super(CMPFacades,self).__init__()\n\n        if set not in [""train"",""test"",""val""]:\n            raise ValueError(""Invalid set {}, must be train,test or val"".format(set))\n\n        self.images_ = []\n        self.reverse_mode = reverse_mode\n\n        self.source_transforms = source_transforms\n        self.target_transforms = target_transforms\n\n        path = os.path.join(root,""{}"".format(""facades"",set))\n\n        if os.path.exists(path) == False:\n            if download:\n                download_path = os.path.join(root,""facades.tar.gz"")\n                download_file(""https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz"",download_path,extract_path=root)\n            else:\n                raise ValueError(""Facades dataset not found, set download=True to download it"")\n        path = os.path.join(path,set)\n        for img_path in os.listdir(path):\n            file_ext = self.__get_extension(img_path)\n            if file_ext == ""jpg"":\n                self.images_.append(os.path.join(path,img_path))\n\n\n    def __get_extension(self, fpath):\n        split = fpath.split(""."")\n        return split[len(split) - 1]\n\n    def __len__(self):\n        return len(self.images_)\n\n\n    def __getitem__(self, index):\n        img = Image.open(self.images_[index]).convert(""RGB"")\n\n        if self.reverse_mode:\n            img_x = img.crop((0, 0, 256, 256))\n            img_y = img.crop((256, 0, 512, 256))\n        else:\n            img_y = img.crop((0, 0, 256, 256))\n            img_x = img.crop((256, 0, 512, 256))\n\n\n        if self.source_transforms is not None:\n            img_x = self.source_transforms(img_x)\n        if self.target_transforms is not None:\n            img_y = self.target_transforms(img_y)\n\n        return img_x,img_y\n\nclass IdenProf(ImageFolder):\n    def __init__(self,root, train=True, transform=None, target_transform=None, loader=default_loader):\n        """"""\n\n        :param root:\n        :param train:\n        :param transform:\n        :param target_transform:\n        :param loader:\n        """"""\n        self.transform = transform\n        self.target_transform = transform\n\n        if os.path.exists(os.path.join(root,""idenprof"",""train"",""chef"")) == False:\n                print(""Downloading {}"".format(""https://github.com/OlafenwaMoses/IdenProf/releases/download/v1.0/idenprof-jpg.zip""))\n                download_file(""https://github.com/OlafenwaMoses/IdenProf/releases/download/v1.0/idenprof-jpg.zip"", ""idenprof.zip"", extract_path=root)\n\n        super(IdenProf,self).__init__(root=os.path.join(root,""idenprof"",""train"" if train else ""test""),transform=transform,target_transform=target_transform,loader=loader)\n\ndef idenprof_loader(size=None,root=""./idenprof"",train=True,batch_size=32,mean=0.5,std=0.5,transform=""default"",target_transform=None,**loader_args):\n\n    """"""\n\n    :param size:\n    :param root:\n    :param train:\n    :param batch_size:\n    :param mean:\n    :param std:\n    :param transform:\n    :param target_transform:\n    :param loader_args:\n    :return:\n    """"""\n\n    if size is not None:\n        if not isinstance(size,tuple):\n            size = (size,size)\n\n    if transform == ""default"":\n        t = []\n        if size is not None:\n            t.append(transformations.Resize(size))\n\n        t.append(transformations.ToTensor())\n\n        if mean is not None and std is not None:\n            if not isinstance(mean, tuple):\n                mean = (mean,)\n            if not isinstance(std, tuple):\n                std = (std,)\n            t.append(transformations.Normalize(mean=mean, std=std))\n\n        trans = transformations.Compose(t)\n\n\n    else:\n        trans = transform\n\n    data = IdenProf(root,train=train,transform=trans,target_transform=target_transform)\n\n    return DataLoader(data,batch_size=batch_size,shuffle=train,**loader_args)\n\ndef mnist_loader(size=None,root=""./mnist"",train=True,batch_size=32,mean=0.5,std=0.5,transform=""default"",download=True,target_transform=None,**loader_args):\n\n    """"""\n\n    :param size:\n    :param root:\n    :param train:\n    :param batch_size:\n    :param mean:\n    :param std:\n    :param transform:\n    :param download:\n    :param target_transform:\n    :param loader_args:\n    :return:\n    """"""\n\n    if size is not None:\n        if not isinstance(size,tuple):\n            size = (size,size)\n\n    if transform == ""default"":\n        t = []\n        if size is not None:\n            t.append(transformations.Resize(size))\n\n        t.append(transformations.ToTensor())\n\n        if mean is not None and std is not None:\n            if not isinstance(mean, tuple):\n                mean = (mean,)\n            if not isinstance(std, tuple):\n                std = (std,)\n            t.append(transformations.Normalize(mean=mean, std=std))\n\n        trans = transformations.Compose(t)\n\n\n    else:\n        trans = transform\n\n    data = MNIST(root,train=train,transform=trans,download=download,target_transform=target_transform)\n\n    return DataLoader(data,batch_size=batch_size,shuffle=train,**loader_args)\n\n\ndef cifar10_loader(size=None,root=""./cifar10"",train=True,batch_size=32,mean=0.5,std=0.5,transform=""default"",download=True,target_transform=None,**loader_args):\n\n    """"""\n\n    :param size:\n    :param root:\n    :param train:\n    :param batch_size:\n    :param mean:\n    :param std:\n    :param transform:\n    :param download:\n    :param target_transform:\n    :param loader_args:\n    :return:\n    """"""\n\n    if size is not None:\n        if not isinstance(size,tuple):\n            size = (size,size)\n\n    if transform == ""default"":\n        t = []\n        if size is not None:\n            t.append(transformations.Resize(size))\n\n        t.append(transformations.ToTensor())\n\n        if mean is not None and std is not None:\n            if not isinstance(mean, tuple):\n                mean = (mean,)\n            if not isinstance(std, tuple):\n                std = (std,)\n            t.append(transformations.Normalize(mean=mean, std=std))\n\n        trans = transformations.Compose(t)\n    else:\n        trans = transform\n\n    data = CIFAR10(root,train=train,transform=trans,download=download,target_transform=target_transform)\n\n    return DataLoader(data,batch_size=batch_size,shuffle=train,**loader_args)\n\ndef cifar100_loader(size=None,root=""./cifar100"",train=True,batch_size=32,mean=0.5,std=0.5,transform=""default"",download=True,target_transform=None,**loader_args):\n    """"""\n\n    :param size:\n    :param root:\n    :param train:\n    :param batch_size:\n    :param mean:\n    :param std:\n    :param transform:\n    :param download:\n    :param target_transform:\n    :param loader_args:\n    :return:\n    """"""\n    if size is not None:\n        if not isinstance(size,tuple):\n            size = (size,size)\n\n    if transform == ""default"":\n        t = []\n        if size is not None:\n            t.append(transformations.Resize(size))\n\n        t.append(transformations.ToTensor())\n        if mean is not None and std is not None:\n            if not isinstance(mean, tuple):\n                mean = (mean,)\n            if not isinstance(std, tuple):\n                std = (std,)\n            t.append(transformations.Normalize(mean=mean, std=std))\n\n        trans = transformations.Compose(t)\n    else:\n        trans = transform\n\n    data = MNIST(root,train=train,transform=trans,download=download,target_transform=target_transform)\n\n    return DataLoader(data,batch_size=batch_size,shuffle=train,**loader_args)\n\ndef fashionmnist_loader(size=None,root=""./fashionmnist"",train=True,batch_size=32,mean=0.5,std=0.5,transform=""default"",download=True,target_transform=None,**loader_args):\n    """"""\n\n    :param size:\n    :param root:\n    :param train:\n    :param batch_size:\n    :param mean:\n    :param std:\n    :param transform:\n    :param download:\n    :param target_transform:\n    :param loader_args:\n    :return:\n    """"""\n\n    if size is not None:\n        if not isinstance(size,tuple):\n            size = (size,size)\n\n    if transform == ""default"":\n        t = []\n        if size is not None:\n            t.append(transformations.Resize(size))\n\n        t.append(transformations.ToTensor())\n        if mean is not None and std is not None:\n            if not isinstance(mean, tuple):\n                mean = (mean,)\n            if not isinstance(std, tuple):\n                std = (std,)\n            t.append(transformations.Normalize(mean=mean, std=std))\n\n        trans = transformations.Compose(t)\n    else:\n        trans = transform\n\n    data = FashionMNIST(root,train=train,transform=trans,download=download,target_transform=target_transform)\n\n    return DataLoader(data,batch_size=batch_size,shuffle=train,**loader_args)\n\ndef emnist_loader(size=None,root=""./emnist"",train=True,batch_size=32,mean=0.5,std=0.5,transform=""default"",download=True,set=""letters"",target_transform=None,**loader_args):\n    """"""\n\n    :param size:\n    :param root:\n    :param train:\n    :param batch_size:\n    :param mean:\n    :param std:\n    :param transform:\n    :param download:\n    :param set:\n    :param target_transform:\n    :param loader_args:\n    :return:\n    """"""\n    valid_sets = (\'byclass\', \'bymerge\', \'balanced\', \'letters\', \'digits\', \'mnist\')\n\n    if set not in valid_sets: raise ValueError(""set {}  is invalid, valid sets include {}"".format(set,valid_sets))\n\n    if size is not None:\n        if not isinstance(size,tuple):\n            size = (size,size)\n\n    if transform == ""default"":\n        t = []\n        if size is not None:\n            t.append(transformations.Resize(size))\n\n        t.append(transformations.ToTensor())\n\n        if mean is not None and std is not None:\n            if not isinstance(mean, tuple):\n                mean = (mean,)\n            if not isinstance(std, tuple):\n                std = (std,)\n            t.append(transformations.Normalize(mean=mean, std=std))\n\n        trans = transformations.Compose(t)\n    else:\n        trans = transform\n\n    data = EMNIST(root,train=train,transform=trans,download=download,split=set,target_transform=target_transform)\n\n    return DataLoader(data,batch_size=batch_size,shuffle=train,**loader_args)\n\n\ndef svhn_loader(size=None,root=""./shvn"",set=""train"",batch_size=32,mean=0.5,std=0.5,transform=""default"",download=True,target_transform=None,**loader_args):\n    """"""\n\n    :param size:\n    :param root:\n    :param set:\n    :param batch_size:\n    :param mean:\n    :param std:\n    :param transform:\n    :param download:\n    :param target_transform:\n    :param loader_args:\n    :return:\n    """"""\n    valid_sets = (\'train\', \'test\', \'extra\')\n\n    if set not in valid_sets: raise ValueError(""set {}  is invalid, valid sets include {}"".format(set,valid_sets))\n\n    if size is not None:\n        if not isinstance(size,tuple):\n            size = (size,size)\n\n    if transform == ""default"":\n        t = []\n        if size is not None:\n            t.append(transformations.Resize(size))\n\n        t.append(transformations.ToTensor())\n\n        if mean is not None and std is not None:\n            if not isinstance(mean, tuple):\n                mean = (mean,)\n            if not isinstance(std, tuple):\n                std = (std,)\n            t.append(transformations.Normalize(mean=mean, std=std))\n\n        trans = transformations.Compose(t)\n    else:\n        trans = transform\n    data = SVHN(root,split=set,transform=trans,download=download,target_transform=target_transform)\n    shuffle_mode = True if set == ""train"" else False\n    return DataLoader(data,batch_size=batch_size,shuffle=shuffle_mode,**loader_args)\n\ndef cmpfacades_loader(size=None,root=""./cmpfacades"",set=""train"",batch_size=32,mean=0.5,std=0.5,transform=""default"",download=True,reverse_mode=False,**loader_args):\n    """"""\n\n    :param size:\n    :param root:\n    :param set:\n    :param batch_size:\n    :param mean:\n    :param std:\n    :param transform:\n    :param download:\n    :param reverse_mode:\n    :param loader_args:\n    :return:\n    """"""\n    valid_sets = (\'train\', \'test\', \'val\')\n\n    if set not in valid_sets: raise ValueError(""set {}  is invalid, valid sets include {}"".format(set,valid_sets))\n\n    if size is not None:\n        if not isinstance(size,tuple):\n            size = (size,size)\n\n    if transform == ""default"":\n        t = []\n        if size is not None:\n            t.append(transformations.Resize(size))\n\n        t.append(transformations.ToTensor())\n\n        if mean is not None and std is not None:\n            if not isinstance(mean, tuple):\n                mean = (mean,)\n            if not isinstance(std, tuple):\n                std = (std,)\n            t.append(transformations.Normalize(mean=mean, std=std))\n\n        trans = transformations.Compose(t)\n    else:\n        trans = transform\n    data = CMPFacades(root,source_transforms=trans,target_transforms=trans,set=set,download=download,reverse_mode=reverse_mode)\n    shuffle_mode = True if set == ""train"" else False\n    return DataLoader(data,batch_size=batch_size,shuffle=shuffle_mode,**loader_args)\n\n\ndef pathimages_loader(image_paths,size=None,recursive=True,allowed_exts=[\'jpg\', \'jpeg\', \'png\', \'ppm\', \'bmp\', \'pgm\', \'tif\'],shuffle=False,batch_size=32,mean=0.5,std=0.5,transform=""default"",**loader_args):\n    """"""\n\n    :param image_paths:\n    :param size:\n    :param recursive:\n    :param allowed_exts:\n    :param shuffle:\n    :param batch_size:\n    :param mean:\n    :param std:\n    :param transform:\n    :param loader_args:\n    :return:\n    """"""\n    if size is not None:\n        if not isinstance(size,tuple):\n            size = (size,size)\n\n    if transform == ""default"":\n        t = []\n        if size is not None:\n            t.append(transformations.Resize(size))\n\n        t.append(transformations.ToTensor())\n\n        if mean is not None and std is not None:\n            if not isinstance(mean, tuple):\n                mean = (mean,)\n            if not isinstance(std, tuple):\n                std = (std,)\n            t.append(transformations.Normalize(mean=mean, std=std))\n\n        trans = transformations.Compose(t)\n    else:\n        trans = transform\n\n    data = ImagesFromPaths(image_paths,trans,recursive=recursive,allowed_exts=allowed_exts)\n\n    return DataLoader(data,batch_size=batch_size,shuffle=shuffle,**loader_args)\n\nclass DataFolder(DatasetFolder):\n    """"""A generic data loader where the samples are arranged in this way: ::\n\n        root/class_x/xxx.ext\n        root/class_x/xxy.ext\n        root/class_x/xxz.ext\n\n        root/class_y/123.ext\n        root/class_y/nsdf3.ext\n        root/class_y/asd932_.ext\n\n    Args:\n        root (string): Root directory path.\n        loader (callable): A function to load a sample given its path.\n        extensions (list[string]): A list of allowed extensions.\n        transform (callable, optional): A function/transform that takes in\n            a sample and returns a transformed version.\n            E.g, ``transforms.RandomCrop`` for images.\n        target_transform (callable, optional): A function/transform that takes\n            in the target and transforms it.\n        class_map (str): a path to json mapping from class names to class index\n\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        samples (list): List of (sample path, class_index) tuples\n    """"""\n\n    def __init__(self, root, loader, extensions, transform=None, target_transform=None,class_map=None):\n\n        if class_map is None:\n            classes, class_to_idx = find_classes(root)\n        else:\n            if os.path.exists(class_map):\n                with open(class_map) as f:\n                    c_map = json.load(f)\n                classes = [c for c in c_map]\n                class_to_idx = c_map\n            else:\n                classes, class_to_idx = find_classes(root)\n                with open(class_map,""w"") as f:\n                    json.dump(class_to_idx,f)\n\n\n        samples = make_dataset(root, class_to_idx, extensions)\n        if len(samples) == 0:\n            raise(RuntimeError(""Found 0 files in subfolders of: "" + root + ""\\n""\n                               ""Supported extensions are: "" + "","".join(extensions)))\n\n        self.root = root\n        self.loader = loader\n        self.extensions = extensions\n\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.samples = samples\n\n        self.transform = transform\n        self.target_transform = target_transform\n\n\n\ndef imagefolder_loader(size=None,root=""./data"",shuffle=False,class_map=None,batch_size=32,mean=0.5,std=0.5,transform=""default"",allowed_exts=[\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\', \'.tif\'],source=None,target_transform=None,**loader_args):\n\n    """"""\n\n    :param size:\n    :param root:\n    :param shuffle:\n    :param class_map:\n    :param batch_size:\n    :param mean:\n    :param std:\n    :param transform:\n    :param allowed_exts:\n    :param source:\n    :param target_transform:\n    :param loader_args:\n    :return:\n    """"""\n\n    if source is not None:\n        if os.path.exists(root) == False:\n            print(""Downloading {}"".format(source[0]))\n            download_file(source[0],source[1],extract_path=root)\n    elif len(os.listdir(root)) == 0:\n        print(""Downloading {}"".format(source[0]))\n        download_file(source[0], source[1], extract_path=root)\n\n    if size is not None:\n        if not isinstance(size,tuple):\n            size = (size,size)\n\n    if transform == ""default"":\n        t = []\n        if size is not None:\n            t.append(transformations.Resize(size))\n\n        t.append(transformations.ToTensor())\n\n        if mean is not None and std is not None:\n            if not isinstance(mean, tuple):\n                mean = (mean,)\n            if not isinstance(std, tuple):\n                std = (std,)\n            t.append(transformations.Normalize(mean=mean, std=std))\n\n        trans = transformations.Compose(t)\n    else:\n        trans = transform\n\n    data = DataFolder(root=root,loader=default_loader,extensions=allowed_exts,transform=trans,target_transform=target_transform,class_map=class_map)\n\n    return DataLoader(data,batch_size=batch_size,shuffle=shuffle,**loader_args)\n\n\n\n\n'"
torchfusion/fp16_utils/__init__.py,0,"b'from .fp16util import BN_convert_float, half_model, prep_param_lists, master_params_to_model_params, master_params_to_model_params, tofp16 , to_python_float, clip_grad_norm\nfrom .fp16util import (\n    BN_convert_float,\n    half_model,\n    prep_param_lists,\n    model_grads_to_master_grads,\n    master_params_to_model_params, \n    tofp16,\n    to_python_float,\n    clip_grad_norm,\n)\n\n\nfrom .fp16_optimizer import FP16_Optimizer\n\n\nfrom .loss_scaler import LossScaler, DynamicLossScaler\n'"
torchfusion/fp16_utils/fp16_optimizer.py,19,"b'import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\n\nfrom .loss_scaler import DynamicLossScaler, LossScaler\nfrom .fp16util import model_grads_to_master_grads, master_params_to_model_params, clip_grad_norm\n\n# TODO:  Update overflow check + downscale to use Carl\'s fused kernel.\nclass FP16_Optimizer(object):\n    """"""\n    :class:`FP16_Optimizer` is designed to wrap an existing PyTorch optimizer, \n    and manage static or dynamic loss scaling and master weights in a manner transparent to the user.\n    For standard use, only two lines must be changed:  creating the :class:`FP16_Optimizer` instance,\n    and changing the call to ``backward``.\n\n    Example::\n\n        model = torch.nn.Linear(D_in, D_out).cuda().half()\n        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n        # Name the FP16_Optimizer instance to replace the existing optimizer\n        # (recommended but not required):\n        optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n        ...\n        # loss.backward() becomes:\n        optimizer.backward(loss)\n        ...\n\n    Example with dynamic loss scaling::\n\n        ...\n        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n                                   # optional arg to control dynamic loss scaling behavior\n                                   # dynamic_loss_args={\'scale_window\' : 500})\n                                   # Usually, dynamic_loss_args is not necessary. \n\n    Args:\n        init_optimizer (torch.optim.optimizer):  Existing optimizer created with the parameters to optimize.  Internally, :class:`FP16_Optimizer` replaces the passed optimizer\'s fp16 parameters, if any, with fp32 master parameters copied from the original ones.  :class:`FP16_Optimizer` also stores references to the original fp16 parameters, and updates these fp16 parameters from the master fp32 copy at the end of each :attr:`step`.  \n        static_loss_scale (float, optional, default=1.0):  Loss scale used internally to scale gradients computed by the model.  Any fp16 gradients will be copied to fp32, then downscaled before being applied to the fp32 master params, so ``static_loss_scale`` should not affect learning rate.\n        dynamic_loss_scale (bool, optional, default=False):  Use dynamic loss scaling.  If True, this will override any ``static_loss_scale`` option.\n        dynamic_loss_args (dict, optional, default=None):  Dict of kwargs that will be forwarded to the internal :class:`DynamicLossScaler` instance\'s constructor.  Keys of this dict must match kwargs accepted by :class:`DynamicLossScaler`\'s constructor.  If ``dynamic_loss_args`` is unspecified, :class:`DynamicLossScaler`\'s defaults will be used.\n        verbose (bool, optional, default=True):  By default, FP16_Optimizer\'s constructor prints out the parameters and parameter groups it is ingesting, as a sanity check.  If this becomes annoying (e.g. for large models), it can be disabled by passing ``verbose=False``.  ``verbose=False`` will not disable printing when the loss scale is readjusted during dynamic loss scaling.\n\n    ``init_optimizer`` is expected to have been constructed in the ordinary way.  \n    It is recommended (although not required) that the newly constructed :class:`FP16_Optimizer` instance be \n    named to replace ``init_optimizer``, for two reasons:  \n    First, it means that references to the same name\n    later in the file will not have to change.  \n    Second, :class:`FP16_Optimizer` reserves the right (as an implementation detail) to \n    modify ``init_optimizer``.  If you do choose a unique name for the new\n    :class:`FP16_Optimizer` instance, you should only work with this new instance,\n    because the preexisting optimizer might no longer behave as expected.\n\n    ``init_optimizer`` may be any Pytorch optimizer. \n    It may contain a mixture of fp16 and fp32 parameters organized into any number of \n    ``param_groups`` with different hyperparameters.  The :class:`FP16_Optimizer` constructor will \n    ingest these ``param_groups`` and remember them. \n\n    Calls to ::\n\n        loss.backward() \n\n    must be replaced with ::\n\n        optimizer.backward(loss)  \n\n    because :class:`FP16_Optimizer` requires ownership of the backward pass to implement \n    loss scaling and copies to master gradients.\n\n    .. note::\n        Loss scaling, either static or dynamic, is orthogonal to learning rate, because gradients\n        are downscaled before being applied.  This means that adjusting the loss scale, or using\n        dynamic loss scaling, should not require retuning the learning rate or any other \n        hyperparameters.\n\n\n    **Advanced options**\n\n    **Closures**:  :class:`FP16_Optimizer` can wrap a Pytorch optimizer that receives a closure.\n    See docstring for :attr:`step`.\n\n    **Gradient clipping**:  Use :attr:`clip_master_grads`.\n    \n    **Multiple losses**:  If your model accumulates gradients from multiple losses,\n    this can be made more efficient by supplying ``update_master_grads=False``\n    to :attr:`backward`.  See docstring for :attr:`backward`.\n\n    **Manually adjusting loss scale**:  The current loss scale can be retrieved or set via ::\n\n        print(optimizer.loss_scale)\n        optimizer.loss_scale = new_loss_scale\n\n    For static loss scaling, manually adjusting the loss scale over time is a reasonable\n    thing to do.  During later epochs, gradients may become smaller, and a \n    higher loss scale may be required, analogous to scheduling the learning rate.  Dynamic loss\n    scaling is more subtle (see :class:`DynamicLossScaler`) and in this case, manually adjusting \n    the loss scale is not recommended.\n\n    **Multi_GPU training**:  If the wrapped ``init_optimizer`` was created from a model wrapped in\n    Pytorch DistributedDataParallel or Apex DistributedDataParallel, :class:`FP16_Optimizer` \n    should still work as intended.\n    """"""\n\n    def __init__(self, \n                 init_optimizer, \n                 static_loss_scale=1.0, \n                 dynamic_loss_scale=True,\n                 dynamic_loss_args=None,\n                 verbose=True):\n        if not torch.cuda.is_available:\n            raise SystemError(""Cannot use fp16 without CUDA."")\n\n        self.verbose = verbose\n\n        self.optimizer = init_optimizer\n        # init_state_dict sets up an alternative way to cast per-param state tensors.\n        # Stashing here in case https://github.com/pytorch/pytorch/issues/7733 makes it necessary.\n        # init_state_dict = init_optimizer.state_dict()\n\n        self.fp16_groups = []\n        self.fp32_from_fp16_groups = []\n        self.fp32_from_fp32_groups = []\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            self.maybe_print(""FP16_Optimizer processing param group {}:"".format(i))\n            fp16_params_this_group = []\n            fp32_params_this_group = []\n            fp32_from_fp16_params_this_group = []\n            for i, param in enumerate(param_group[\'params\']):\n                if param.requires_grad:\n                    if param.type() == \'torch.cuda.HalfTensor\':\n                        self.maybe_print(""FP16_Optimizer received torch.cuda.HalfTensor with {}""\n                                         .format(param.size()))\n                        fp16_params_this_group.append(param)\n                        master_param = param.detach().clone().float()\n                        master_param.requires_grad = True\n                        param_group[\'params\'][i] = master_param\n                        fp32_from_fp16_params_this_group.append(master_param)\n                        # Reset existing state dict key to the new master param.\n                        # We still need to recast per-param state tensors, if any, to FP32.\n                        if param in self.optimizer.state:\n                           self.optimizer.state[master_param] = self.optimizer.state.pop(param) \n                    elif param.type() == \'torch.cuda.FloatTensor\':\n                        self.maybe_print(""FP16_Optimizer received torch.cuda.FloatTensor with {}""\n                                         .format(param.size()))\n                        fp32_params_this_group.append(param)\n                        param_group[\'params\'][i] = param\n                    else:\n                        raise TypeError(""Wrapped parameters must be either ""\n                                        ""torch.cuda.FloatTensor or torch.cuda.HalfTensor. ""  \n                                        ""Received {}"".format(param.type()))\n            \n            self.fp16_groups.append(fp16_params_this_group)\n            self.fp32_from_fp16_groups.append(fp32_from_fp16_params_this_group)\n            self.fp32_from_fp32_groups.append(fp32_params_this_group)\n\n        # Leverage state_dict() and load_state_dict() to recast preexisting per-param state tensors\n        self.optimizer.load_state_dict(self.optimizer.state_dict())\n        # alternative way to cast per-param state tensors:\n        # self.optimizer.load_state_dict(init_state_dict)\n\n        if dynamic_loss_scale:\n            self.dynamic_loss_scale = True\n            if dynamic_loss_args is not None:\n                self.loss_scaler = DynamicLossScaler(**dynamic_loss_args)\n            else:\n                self.loss_scaler = DynamicLossScaler()\n        else:\n            self.dynamic_loss_scale = False\n            self.loss_scaler = LossScaler(static_loss_scale)\n\n        self.overflow = False\n        self.first_closure_call_this_step = True\n\n        self.clip_grad_norm = clip_grad_norm\n\n    def maybe_print(self, msg):\n        if self.verbose:\n            print(msg)\n            \n    def __getstate__(self):\n        raise RuntimeError(""FP16_Optimizer should be serialized using state_dict()."")\n\n    def __setstate__(self, state):\n        raise RuntimeError(""FP16_Optimizer should be deserialized using load_state_dict()."")\n\n    def zero_grad(self, set_grads_to_None=False):\n        """"""\n        Zero fp32 and fp16 parameter grads.\n        """"""\n        # In principle, only the .grad attributes of the model params need to be zeroed,\n        # because gradients are copied into the FP32 master params.  However, we zero\n        # all gradients owned by the optimizer, just to be safe:\n        for group in self.optimizer.param_groups:\n             for p in group[\'params\']:\n                 if set_grads_to_None:\n                     p.grad = None\n                 else:\n                     if p.grad is not None:\n                         p.grad.detach_()\n                         p.grad.zero_()\n\n        # Zero fp16 gradients owned by the model:\n        for fp16_group in self.fp16_groups:\n            for param in fp16_group:\n                if set_grads_to_None:\n                    param.grad = None\n                else:\n                    if param.grad is not None:\n                        param.grad.detach_() # as in torch.optim.optimizer.zero_grad()\n                        param.grad.zero_()\n\n    def _check_overflow(self):\n        params = [] \n        for group in self.fp16_groups:\n            for param in group:\n                params.append(param)\n        for group in self.fp32_from_fp32_groups:\n            for param in group:\n                params.append(param)\n        self.overflow = self.loss_scaler.has_overflow(params)\n\n    def _update_scale(self, has_overflow=False):\n        self.loss_scaler.update_scale(has_overflow)\n\n    def _master_params_to_model_params(self):\n        for fp16_group, fp32_from_fp16_group in zip(self.fp16_groups, self.fp32_from_fp16_groups):\n            master_params_to_model_params(fp16_group, fp32_from_fp16_group)\n\n    # To consider:  Integrate distributed with this wrapper by registering a hook on each variable \n    # that does the overflow check, gradient copy + downscale, and fp32 allreduce in a different stream.\n    def _model_grads_to_master_grads(self):\n        for fp16_group, fp32_from_fp16_group in zip(self.fp16_groups, self.fp32_from_fp16_groups):\n            model_grads_to_master_grads(fp16_group, fp32_from_fp16_group)\n\n    def _downscale_master(self):\n        if self.loss_scale != 1.0: \n            for group in self.optimizer.param_groups:\n                for param in group[\'params\']:\n                    if param.grad is not None:\n                        param.grad.data.mul_(1./self.loss_scale)\n\n    def clip_master_grads(self, max_norm, norm_type=2):\n        """"""\n        Clips fp32 master gradients via ``torch.nn.utils.clip_grad_norm``.\n\n        Args:\n            max_norm (float or int): max norm of the gradients\n            norm_type (float or int): type of the used p-norm. Can be ``\'inf\'`` for\n                infinity norm.\n\n        Returns:\n            Total norm of the current fp32 gradients (viewed as a single vector).\n\n        .. warning::\n            Returns -1 if the most recently computed fp16 gradients overflowed (that is, if ``self.overflow`` is ``True``).\n        """"""\n        if not self.overflow:\n            fp32_params = []\n            for param_group in self.optimizer.param_groups:\n                for param in param_group[\'params\']:\n                    fp32_params.append(param)\n            return self.clip_grad_norm(fp32_params, max_norm, norm_type)\n        else:\n            return -1\n\n    def state_dict(self):\n        """"""\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n\n            checkpoint = {}\n            checkpoint[\'model\'] = model.state_dict()\n            checkpoint[\'optimizer\'] = optimizer.state_dict()\n            torch.save(checkpoint, ""saved.pth"")\n        """"""\n        state_dict = {}\n        state_dict[\'loss_scaler\'] = self.loss_scaler\n        state_dict[\'dynamic_loss_scale\'] = self.dynamic_loss_scale\n        state_dict[\'overflow\'] = self.overflow\n        state_dict[\'first_closure_call_this_step\'] = self.first_closure_call_this_step\n        state_dict[\'optimizer_state_dict\'] = self.optimizer.state_dict()\n        state_dict[\'fp32_from_fp16\'] = self.fp32_from_fp16_groups\n        return state_dict\n\n    def load_state_dict(self, state_dict):\n        """"""\n        Loads a state_dict created by an earlier call to state_dict(). \n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``, \n        whose parameters in turn came from ``model``, it is expected that the user \n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n\n        Example::\n\n            model = torch.nn.Linear(D_in, D_out).cuda().half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(""saved.pth"")\n            model.load_state_dict(checkpoint[\'model\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        """"""\n        # I think it should actually be ok to reload the optimizer before the model.\n        self.loss_scaler = state_dict[\'loss_scaler\']\n        self.dynamic_loss_scale = state_dict[\'dynamic_loss_scale\']\n        self.overflow = state_dict[\'overflow\']\n        self.first_closure_call_this_step = state_dict[\'first_closure_call_this_step\']\n        self.optimizer.load_state_dict(state_dict[\'optimizer_state_dict\'])\n        # At this point, the optimizer\'s references to the model\'s fp32 parameters are up to date.\n        # The optimizer\'s hyperparameters and internal buffers are also up to date.  \n        # However, the fp32 master copies of the model\'s fp16 params stored by the optimizer are still\n        # out of date.  There are two options.  \n        # 1:  Refresh the master params from the model\'s fp16 params.  \n        # This requires less storage but incurs precision loss.\n        # 2:  Save and restore the fp32 master copies separately.\n        # We choose option 2.\n        # \n        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device \n        # of their associated parameters, because it\'s possible those buffers might not exist yet in \n        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been \n        # constructed in the same way as the one whose state_dict we are loading, the same master params\n        # are guaranteed to exist, so we can just copy_() from the saved master params.\n        for current_group, saved_group in zip(self.fp32_from_fp16_groups, state_dict[\'fp32_from_fp16\']):\n            for current, saved in zip(current_group, saved_group):\n                current.data.copy_(saved.data)\n\n    def step(self, closure=None): # could add clip option.\n        """"""\n        If no closure is supplied, :attr:`step` should be called after \n        ``fp16_optimizer_obj.backward(loss)``.\n        :attr:`step` updates the fp32 master copy of parameters using the optimizer supplied to\n        :class:`FP16_Optimizer`\'s constructor, then copies the updated fp32 params into the fp16 params\n        originally referenced by :class:`FP16_Optimizer`\'s constructor, so the user may immediately run\n        another forward pass using their model.\n\n        If a closure is supplied, :attr:`step` may be called without a prior call to \n        :attr:`backward(loss)`.\n        This control flow is identical to `ordinary Pytorch optimizer use`_ with closures.\n        However, the user should take care that any ``loss.backward()`` call within the closure\n        has been replaced by ``fp16_optimizer_obj.backward(loss)``.\n\n        Args:\n           closure (optional):  Closure that will be supplied to the underlying optimizer originally passed to :class:`FP16_Optimizer`\'s constructor.  closure should call :attr:`zero_grad()` on the :class:`FP16_Optimizer` object, compute the loss, call :attr:`backward(loss)`, and return the loss.\n\n        Example with closure::\n\n            # optimizer is assumed to be an FP16_Optimizer object, previously constructed from an \n            # existing pytorch optimizer.\n            for input, target in dataset:\n                def closure():\n                    optimizer.zero_grad()\n                    output = model(input)\n                    loss = loss_fn(output, target)\n                    # loss.backward() becomes:\n                    optimizer.backward(loss)\n                    return loss\n                optimizer.step(closure)\n\n        .. warning::\n            Currently, calling :attr:`step` with a closure is not compatible with dynamic loss scaling.\n\n        .. _`ordinary Pytorch optimizer use`:\n            http://pytorch.org/docs/master/optim.html#optimizer-step-closure\n        """"""\n\n        scale = self.loss_scaler.loss_scale\n        self._update_scale(self.overflow)\n\n        if self.overflow:\n            print(""OVERFLOW! Skipping step. Attempted loss scale: {}, reducing to {}""\n                .format(scale, self.loss_scale))\n            return\n        \n        if closure is not None:\n            retval = self._step_with_closure(closure)\n        else:\n            retval = self.optimizer.step()\n\n        self._master_params_to_model_params()\n\n        return retval\n\n    def _step_with_closure(self, closure):\n        def wrapped_closure():\n            # helpful for debugging\n            # print(""Calling wrapped_closure, first_closure_call_this_step = {}""\n            #       .format(self.first_closure_call_this_step))\n            if self.first_closure_call_this_step:\n                # We expect that the fp16 params are initially fresh on entering self.step(),\n                # so _master_params_to_model_params() is unnecessary the first time wrapped_closure()\n                # is called within self.optimizer.step().\n                self.first_closure_call_this_step = False\n            else:\n                # If self.optimizer.step() internally calls wrapped_closure more than once,\n                # it may update the fp32 params after each call.  However, self.optimizer \n                # doesn\'t know about the fp16 params at all.  If the fp32 params get updated,\n                # we can\'t rely on self.optimizer to refresh the fp16 params.  We need\n                # to handle that manually:\n                self._master_params_to_model_params()\n            # Our API expects the user to give us ownership of the backward() call by\n            # replacing all calls to loss.backward() with optimizer.backward(loss).\n            # This requirement holds whether or not the call to backward() is made within a closure.\n            # If the user is properly calling optimizer.backward(loss) within ""closure,"" \n            # calling closure() here will give the fp32 master params fresh gradients\n            # for the optimizer to play with, so all wrapped_closure needs to do is call \n            # closure() and return the loss.\n            temp_loss = closure() \n            while(self.overflow):\n                scale = self.loss_scaler.loss_scale\n                self._update_scale(self.overflow)\n                print(""OVERFLOW within closure! Skipping step. Attempted loss scale: {}, ""\n                      ""reducing to {}"".format(scale, self.loss_scale))\n                temp_loss = closure()\n            return temp_loss\n\n        retval = self.optimizer.step(wrapped_closure)\n\n        self.first_closure_call_this_step = True\n\n        return retval\n\n    def backward(self, loss, update_master_grads=True):\n        """""" \n        :attr:`backward` performs the following conceptual steps:\n\n        1. fp32_loss = loss.float() (see first Note below)\n        2. scaled_loss = fp32_loss*loss_scale\n        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model\'s leaves (which may be fp16, fp32, or a mixture, depending how your model was defined).\n        4. fp16 grads are then copied to the master params\' ``.grad`` attributes (see second Note), which are guaranteed to be fp32.\n        5. Finally, master grads are divided by loss_scale.\n\n        In this way, after :attr:`backward`, the master params have fresh gradients,\n        and :attr:`step` may be called.\n\n        .. note::\n            :attr:`backward` internally converts the loss to fp32 before applying the loss scale.\n            This provides some additional safety against overflow if the user has supplied an \n            fp16 loss value.  \n            However, for maximum overflow safety, the user should\n            compute the loss criterion (MSE, cross entropy, etc) in fp32 before supplying it to \n            :attr:`backward`.\n\n        .. warning::\n            The gradients found in a model\'s leaves after the call to \n            :attr:`backward` should not be regarded as valid in general, \n            because it\'s possible \n            they have been scaled (and in the case of dynamic loss scaling, \n            the scale factor may change over time).  \n            If the user wants to inspect gradients after a call to :attr:`backward`,  \n            only the master gradients should be regarded as valid.  These can be retrieved via\n            :attr:`inspect_master_grad_data()`.\n\n        Args:\n            loss:  The loss output by the user\'s model.  loss may be either float or half (but see first Note above).\n            update_master_grads (bool, optional, default=True):  Option to copy fp16 grads to fp32 grads on this call.  By setting this to False, the user can delay the copy, which is useful to eliminate redundant fp16->fp32 grad copies if :attr:`backward` is being called on multiple losses in one iteration.  If set to False, the user becomes responsible for calling :attr:`update_master_grads` before calling :attr:`step`.\n\n        Example::\n\n            # Ordinary operation:\n            optimizer.backward(loss)\n\n            # Naive operation with multiple losses (technically valid, but less efficient):\n            # fp32 grads will be correct after the second call,  but \n            # the first call incurs an unnecessary fp16->fp32 grad copy.\n            optimizer.backward(loss1)\n            optimizer.backward(loss2)\n\n            # More efficient way to handle multiple losses:\n            # The fp16->fp32 grad copy is delayed until fp16 grads from all \n            # losses have been accumulated.\n            optimizer.backward(loss1, update_master_grads=False)\n            optimizer.backward(loss2, update_master_grads=False)\n            optimizer.update_master_grads()\n        """""" \n        # To consider:  try multiple backward passes using retain_grad=True to find \n        # a loss scale that works.  After you find a loss scale that works, do a final dummy\n        # backward pass with retain_graph=False to tear down the graph.  Doing this would avoid \n        # discarding the iteration,  but probably wouldn\'t improve overall efficiency.  \n        self.loss_scaler.backward(loss.float())\n        if update_master_grads:\n            self.update_master_grads()\n\n    def update_master_grads(self):\n        """"""\n        Copy the ``.grad`` attribute from stored references to fp16 parameters to \n        the ``.grad`` attribute of the fp32 master parameters that are directly \n        updated by the optimizer.  :attr:`update_master_grads` only needs to be called if\n        ``fp16_optimizer_obj.backward`` was called with ``update_master_grads=False``.\n        """"""\n        if self.dynamic_loss_scale:\n            self._check_overflow()\n            if self.overflow: return\n        self._model_grads_to_master_grads()\n        self._downscale_master()\n\n    def inspect_master_grad_data(self):\n        """"""\n        When running with :class:`FP16_Optimizer`, \n        ``.grad`` attributes of a model\'s fp16 leaves should not be\n        regarded as truthful, because they might be scaled.  \n        After a call to :attr:`fp16_optimizer_obj.backward(loss)`, if no overflow was encountered,\n        the fp32 master params\' ``.grad``\n        attributes will contain valid gradients properly divided by the loss scale.  However, \n        because :class:`FP16_Optimizer` flattens some parameters, accessing them may be \n        nonintuitive.  :attr:`inspect_master_grad_data`\n        allows those gradients to be viewed with shapes corresponding to their associated model leaves.\n\n        Returns:\n            List of lists (one list for each parameter group).  The list for each parameter group\n            is a list of the ``.grad.data`` attributes of the fp32 master params belonging to that group.                 \n        """"""\n        raise NotImplementedError(""Currently not implemented, working on it..."")\n        fp32_grads_each_group = []\n        if self.overflow:\n            print(""Warning:  calling FP16_Optimizer.inspect_master_grad_data while in an overflow state.  ""\n                  ""Gradients are currently invalid (may be inf, nan, or stale).  Returning None."")\n            return None\n        else:\n            return None\n\n    # Promote loss scale so it can be retrieved or set via ""fp16_optimizer_instance.loss_scale""\n    def _get_loss_scale(self):\n        return self.loss_scaler.loss_scale\n\n    def _set_loss_scale(self, value):\n        self.loss_scaler.cur_scale = value\n\n    loss_scale = property(_get_loss_scale, _set_loss_scale)\n\n    # Promote state so it can be retrieved or set via ""fp16_optimizer_instance.state""\n    def _get_state(self):\n        return self.optimizer.state\n\n    def _set_state(self, value):\n        self.optimizer.state = value\n\n    state = property(_get_state, _set_state)\n\n    # Promote param_groups so it can be retrieved or set via ""fp16_optimizer_instance.param_groups""\n    # (for example, to adjust the learning rate)\n    def _get_param_groups(self):\n        return self.optimizer.param_groups\n\n    def _set_param_groups(self, value):\n        self.optimizer.param_groups = value\n\n    param_groups = property(_get_param_groups, _set_param_groups)\n\n'"
torchfusion/fp16_utils/fp16util.py,11,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\nfrom ..layers import MultiSequential\n\nclass tofp16(nn.Module):\n    """"""\n    Model wrapper that implements::\n\n        def forward(self, input):\n            return input.half()\n    """"""\n\n    def __init__(self):\n        super(tofp16, self).__init__()\n\n    def forward(self, *inputs):\n        \n        return (input.half() for input in inputs)\n\n\ndef BN_convert_float(module):\n    \'\'\'\n    Designed to work with network_to_half.\n    BatchNorm layers need parameters in single precision.\n    Find all layers and convert them back to float. This can\'t\n    be done with built in .apply as that function will apply\n    fn to all modules, parameters, and buffers. Thus we wouldn\'t\n    be able to guard the float conversion based on the module type.\n    \'\'\'\n    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n        module.float()\n    for child in module.children():\n        BN_convert_float(child)\n    return module\n\n\ndef half_model(model):\n    """"""\n    Convert model to half precision in a batchnorm-safe way.\n    """"""\n   \n    return MultiSequential(tofp16(), BN_convert_float(model.half()))\n\n\ndef backwards_debug_hook(grad):\n    raise RuntimeError(""master_params recieved a gradient in the backward pass!"")\n\ndef prep_param_lists(model, flat_master=False):\n    """"""\n    Creates a list of FP32 master parameters for a given model, as in \n    `Training Neural Networks with Mixed Precision:  Real Examples`_.\n\n    Args:\n        model (torch.nn.Module): Existing Pytorch model\n        flat_master (bool, optional, default=False):  Flatten the master parameters into a single tensor, as a performance optimization.\n    Returns:\n        A tuple (``model_params``, ``master_params``). ``model_params`` is a list of the model\'s parameters for later use with :func:`model_grads_to_master_grads` and :func:`master_params_to_model_params`.  ``master_params`` is a list of FP32 master gradients.  If ``flat_master=True``, ``master_params`` will be a list with one element.\n\n    Example::\n\n        model_params, master_params = prep_param_lists(model)\n\n    .. warning::\n        Currently, if ``flat_master=True``, all the model\'s parameters must be the same type.  If the model has parameters of different types, use ``flat_master=False``, or use :class:`FP16_Optimizer`.\n\n    .. _`Training Neural Networks with Mixed Precision:  Real Examples`:\n        http://on-demand.gputechconf.com/gtc/2018/video/S81012/\n    """"""\n    model_params = [param for param in model.parameters() if param.requires_grad]\n\n    if flat_master:\n        # Give the user some more useful error messages\n        try:\n            # flatten_dense_tensors returns a contiguous flat array.\n            # http://pytorch.org/docs/master/_modules/torch/_utils.html\n            master_params = _flatten_dense_tensors([param.data for param in model_params]).float()\n        except:\n            print(""Error in prep_param_lists:  model may contain a mixture of parameters ""\n                      ""of different types.  Use flat_master=False, or use F16_Optimizer."")\n            raise\n        master_params = torch.nn.Parameter(master_params)\n        master_params.requires_grad = True\n        # master_params.register_hook(backwards_debug_hook)\n        if master_params.grad is None:\n            master_params.grad = master_params.new(*master_params.size())\n        return model_params, [master_params]\n    else:\n        master_params = [param.clone().float().detach() for param in model_params]\n        for param in master_params:\n            param.requires_grad = True\n        return model_params, master_params\n\n\ndef model_grads_to_master_grads(model_params, master_params, flat_master=False):\n    """"""\n    Copy model gradients to master gradients.  \n\n    Args:\n        model_params:  List of model parameters created by :func:`prep_param_lists`.\n        master_params:  List of FP32 master parameters created by :func:`prep_param_lists`.  If ``master_params`` was created with ``flat_master=True``, ``flat_master=True`` should also be supplied to :func:`model_grads_to_master_grads`.\n    """"""\n    if flat_master:\n        # The flattening may incur one more deep copy than is necessary.\n        master_params[0].grad.data.copy_(\n            _flatten_dense_tensors([p.grad.data for p in model_params]))\n    else:\n        for model, master in zip(model_params, master_params):\n            if model.grad is not None:\n                if master.grad is None:\n                    master.grad = Variable(master.data.new(*master.data.size()))\n                master.grad.data.copy_(model.grad.data)\n            else:\n                master.grad = None\n\n\ndef master_params_to_model_params(model_params, master_params, flat_master=False):\n    """"""\n    Copy master parameters to model parameters.\n\n    Args:\n        model_params:  List of model parameters created by :func:`prep_param_lists`.\n        master_params:  List of FP32 master parameters created by :func:`prep_param_lists`.  If ``master_params`` was created with ``flat_master=True``, ``flat_master=True`` should also be supplied to :func:`master_params_to_model_params`.\n    """"""\n    if flat_master:\n        for model, master in zip(model_params, \n                                 _unflatten_dense_tensors(master_params[0].data, model_params)):\n            model.data.copy_(master)\n    else:\n        for model, master in zip(model_params, master_params):\n            model.data.copy_(master.data)\n\n# Backward compatibility fixes\n\ndef to_python_float(t):\n    if hasattr(t, \'item\'):\n        return t.item()\n    else:\n        return t[0]\n\nTORCH_MAJOR = int(torch.__version__.split(\'.\')[0])\nTORCH_MINOR = int(torch.__version__.split(\'.\')[1])\nif TORCH_MAJOR == 0 and TORCH_MINOR <= 4:\n    clip_grad_norm = torch.nn.utils.clip_grad_norm\nelse:\n    clip_grad_norm = torch.nn.utils.clip_grad_norm_\n'"
torchfusion/fp16_utils/loss_scaler.py,10,"b'import torch\n\n# item() is a recent addition, so this helps with backward compatibility.\ndef to_python_float(t):\n    if hasattr(t, \'item\'):\n        return t.item()\n    else:\n        return t[0]\n\nclass LossScaler:\n    """"""\n    Class that manages a static loss scale.  This class is intended to interact with\n    :class:`FP16_Optimizer`, and should not be directly manipulated by the user.\n\n    Use of :class:`LossScaler` is enabled via the ``static_loss_scale`` argument to \n    :class:`FP16_Optimizer`\'s constructor.\n\n    Args:\n        scale (float, optional, default=1.0):  The loss scale.\n    """"""\n\n    def __init__(self, scale=1):\n        self.cur_scale = scale\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow(self, params):\n        return False\n\n    # `x` is a torch.Tensor\n    def _has_inf_or_nan(x):\n        return False\n\n    def update_scale(self, overflow):\n        pass\n\n    @property\n    def loss_scale(self):\n        return self.cur_scale\n\n    def scale_gradient(self, module, grad_in, grad_out):\n        return tuple(self.loss_scale * g for g in grad_in)\n\n    def backward(self, loss):\n        scaled_loss = loss*self.loss_scale\n        scaled_loss.backward()\n\nclass DynamicLossScaler:\n    """"""\n    Class that manages dynamic loss scaling.  It is recommended to use :class:`DynamicLossScaler`\n    indirectly, by supplying ``dynamic_loss_scale=True`` to the constructor of \n    :class:`FP16_Optimizer`.  However, it\'s important to understand how :class:`DynamicLossScaler`\n    operates, because the default options can be changed using the\n    the ``dynamic_loss_args`` argument to :class:`FP16_Optimizer`\'s constructor.\n\n    Loss scaling is designed to combat the problem of underflowing gradients encountered at long\n    times when training fp16 networks.  Dynamic loss scaling begins by attempting a very high loss\n    scale.  Ironically, this may result in OVERflowing gradients.  If overflowing gradients are\n    encountered, :class:`DynamicLossScaler` informs :class:`FP16_Optimizer` that an overflow has \n    occurred.\n    :class:`FP16_Optimizer` then skips the update step for this particular iteration/minibatch,\n    and :class:`DynamicLossScaler` adjusts the loss scale to a lower value.  \n    If a certain number of iterations occur without overflowing gradients detected,\n    :class:`DynamicLossScaler` increases the loss scale once more.\n    In this way :class:`DynamicLossScaler` attempts to ""ride the edge"" of \n    always using the highest loss scale possible without incurring overflow.\n\n    Args:\n        init_scale (float, optional, default=2**32):  Initial loss scale attempted by :class:`DynamicLossScaler.`\n        scale_factor (float, optional, default=2.0):  Factor used when adjusting the loss scale. If an overflow is encountered, the loss scale is readjusted to loss scale/``scale_factor``.  If ``scale_window`` consecutive iterations take place without an overflow, the loss scale is readjusted to loss_scale*``scale_factor``. \n        scale_window (int, optional, default=1000):  Number of consecutive iterations without an overflow to wait before increasing the loss scale.\n    """"""\n\n    def __init__(self,\n                 init_scale=2**32,\n                 scale_factor=2.,\n                 scale_window=1000):\n        self.cur_scale = init_scale\n        self.cur_iter = 0\n        self.last_overflow_iter = -1\n        self.scale_factor = scale_factor\n        self.scale_window = scale_window\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow(self, params):\n        for p in params:\n            if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n                return True\n\n        return False\n\n    # `x` is a torch.Tensor\n    def _has_inf_or_nan(x):\n        try:\n            # if x is half, the .float() incurs an additional deep copy, but it\'s necessary if \n            # Pytorch\'s .sum() creates a one-element tensor of the same type as x \n            # (which is true for some recent version of pytorch).\n            cpu_sum = float(x.float().sum())\n            # More efficient version that can be used if .sum() returns a Python scalar\n            # cpu_sum = float(x.sum())\n        except RuntimeError as instance:\n            # We want to check if inst is actually an overflow exception.\n            # RuntimeError could come from a different error.\n            # If so, we still want the exception to propagate.\n            if ""value cannot be converted"" not in instance.args[0]:\n                raise\n            return True\n        else:\n            if cpu_sum == float(\'inf\') or cpu_sum == -float(\'inf\') or cpu_sum != cpu_sum:\n                return True\n            return False\n\n    # `overflow` is boolean indicating whether the gradient overflowed\n    def update_scale(self, overflow):\n        if overflow:\n            # self.cur_scale /= self.scale_factor\n            self.cur_scale = max(self.cur_scale/self.scale_factor, 1)\n            self.last_overflow_iter = self.cur_iter\n        else:\n            if (self.cur_iter - self.last_overflow_iter) % self.scale_window == 0:\n                self.cur_scale *= self.scale_factor\n        self.cur_iter += 1\n\n    @property\n    def loss_scale(self):\n        return self.cur_scale\n\n    def scale_gradient(self, module, grad_in, grad_out):\n        return tuple(self.loss_scale * g for g in grad_in)\n\n    def backward(self, loss):\n        scaled_loss = loss*self.loss_scale\n        scaled_loss.backward()\n        \n##############################################################        \n# Example usage below here -- assuming it\'s in a separate file\n##############################################################\n""""""\nTO-DO separate out into an example.\nif __name__ == ""__main__"":\n    import torch\n    from torch.autograd import Variable\n    from dynamic_loss_scaler import DynamicLossScaler\n\n    # N is batch size; D_in is input dimension;\n    # H is hidden dimension; D_out is output dimension.\n    N, D_in, H, D_out = 64, 1000, 100, 10\n\n    # Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n    x = Variable(torch.randn(N, D_in), requires_grad=False)\n    y = Variable(torch.randn(N, D_out), requires_grad=False)\n\n    w1 = Variable(torch.randn(D_in, H), requires_grad=True)\n    w2 = Variable(torch.randn(H, D_out), requires_grad=True)\n    parameters = [w1, w2]\n\n    learning_rate = 1e-6\n    optimizer = torch.optim.SGD(parameters, lr=learning_rate)\n    loss_scaler = DynamicLossScaler()\n\n    for t in range(500):\n        y_pred = x.mm(w1).clamp(min=0).mm(w2)\n        loss = (y_pred - y).pow(2).sum() * loss_scaler.loss_scale\n        print(\'Iter {} loss scale: {}\'.format(t, loss_scaler.loss_scale))\n        print(\'Iter {} scaled loss: {}\'.format(t, loss.data[0]))\n        print(\'Iter {} unscaled loss: {}\'.format(t, loss.data[0] / loss_scaler.loss_scale))\n\n        # Run backprop\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Check for overflow\n        has_overflow = DynamicLossScaler.has_overflow(parameters)\n        \n        # If no overflow, unscale grad and update as usual\n        if not has_overflow:\n            for param in parameters:\n                param.grad.data.mul_(1. / loss_scaler.loss_scale)\n            optimizer.step()\n        # Otherwise, don\'t do anything -- ie, skip iteration\n        else:\n            print(\'OVERFLOW!\')\n\n        # Update loss scale for next iteration\n        loss_scaler.update_scale(has_overflow)\n\n""""""\n'"
torchfusion/gan/__init__.py,0,"b'from .applications import MLPGenerator,MLPDiscriminator,DCGANDiscriminator,WGANDiscriminator,DCGANGenerator,WMLPDiscriminator\nfrom .layers import StandardDiscriminatorBlock,DiscriminatorResBlock,StandardGeneratorBlock,GeneratorResBlock,ConditionalBatchNorm2d,SelfAttention\nfrom .distributions import NormalDistribution\nfrom .learners import *\n\n__all__ = [""applications"",""layers"",""distributions"",""learners""]'"
torchfusion/gan/distributions.py,3,"b'from torch.utils.data import Dataset\nimport torch\nimport torch.distributions as distibutions\n\n"""""" A Dataset containing Normal/Gaussian vectors of the specified dimension\n    length: The total number of vectors\n    size: the size of each vector\n    mean: mean of the normal distribution\n    std: standard deviation of the normal distribution\n""""""\nclass NormalDistribution(Dataset):\n    def __init__(self,length,size,mean=0,std=1):\n        super(NormalDistribution,self)\n\n        self.size = size\n        self.length = length\n        self.mean = mean\n        self.std = std\n\n    def __getitem__(self, index):\n        return torch.randn(self.size).normal_(self.mean,self.std)\n\n    def __len__(self):\n        return self.length\n\n\n\n\n'"
torchfusion/initializers/__init__.py,0,"b'from .initializers import Kaiming_Normal,Kaiming_Uniform,Xavier_Normal,Xavier_Uniform, Normal,Uniform,Ones,Zeros,Constant,Eye,Dirac,Sparse,Orthogonal'"
torchfusion/initializers/initializers.py,1,"b'from torch.nn.init import *\n\nclass Normal(object):\n    def __init__(self,mean=0,std=1):\n        """"""\n\n        :param mean:\n        :param std:\n        """"""\n        self.mean = mean\n        self.std = std\n\n    def __call__(self,tensor):\n\n        return normal_(tensor,self.mean,self.std)\n\n\nclass Uniform(object):\n    def __init__(self, lower=0, upper=1):\n        """"""\n\n        :param lower:\n        :param upper:\n        """"""\n        self.lower = lower\n        self.upper = upper\n\n    def __call__(self, tensor):\n\n        return uniform_(tensor, self.lower, self.upper)\n\n\nclass Constant(object):\n    def __init__(self, value):\n        """"""\n\n        :param value:\n        """"""\n        self.value = value\n\n    def __call__(self, tensor):\n\n        return constant_(tensor,self.value)\n\nclass Eye(object):\n    def __call__(self, tensor):\n\n        return eye_(tensor)\n\nclass Dirac(object):\n    def __call__(self, tensor):\n\n        return dirac_(tensor)\n\nclass Ones(Constant):\n    def __init__(self):\n        super(Ones,self).__init__(1)\n\n\nclass Zeros(Constant):\n    def __init__(self):\n        super(Zeros,self).__init__(0)\n\nclass Sparse(object):\n    def __init__(self, sparsity_ratio,std=0.01):\n        """"""\n\n        :param sparsity_ratio:\n        :param std:\n        """"""\n        self.sparsity_ratio = sparsity_ratio\n        self.std = std\n\n    def __call__(self, tensor):\n\n        return sparse_(tensor,self.sparsity_ratio,self.std)\n\nclass Kaiming_Normal(object):\n    def __init__(self,neg_slope=0,mode=""fan_in"",non_linearity=""leaky_relu""):\n        """"""\n\n        :param neg_slope:\n        :param mode:\n        :param non_linearity:\n        """"""\n        self.neg_slope = neg_slope\n        self.mode = mode\n        self.non_linearity = non_linearity\n\n    def __call__(self, tensor):\n\n        return kaiming_normal_(tensor,self.neg_slope,self.mode,self.non_linearity)\n\n\nclass Kaiming_Uniform(object):\n    def __init__(self,neg_slope=0,mode=""fan_in"",non_linearity=""leaky_relu""):\n        """"""\n\n        :param neg_slope:\n        :param mode:\n        :param non_linearity:\n        """"""\n        self.neg_slope = neg_slope\n        self.mode = mode\n        self.non_linearity = non_linearity\n\n    def __call__(self, tensor):\n\n        return kaiming_uniform_(tensor,self.neg_slope,self.mode,self.non_linearity)\n\n\nclass Xavier_Normal(object):\n    def __init__(self, gain=1):\n        """"""\n\n        :param gain:\n        """"""\n        self.gain = gain\n    def __call__(self, tensor):\n        return xavier_normal_(tensor,self.gain)\n\nclass Xavier_Uniform(object):\n    def __init__(self, gain=1):\n        """"""\n\n        :param gain:\n        """"""\n        self.gain = gain\n    def __call__(self, tensor):\n        return xavier_uniform_(tensor,self.gain)\n\nclass Orthogonal(object):\n    def __init__(self, gain=1):\n        """"""\n\n        :param gain:\n        """"""\n        self.gain = gain\n    def __call__(self, tensor):\n        return orthogonal_(tensor,self.gain)'"
torchfusion/lang/__init__.py,0,b'from .datasets import *'
torchfusion/layers/__init__.py,0,"b'from .layers import MultiSequential, GRU,LSTM,RNNBase,RNN,Reshape,Flatten,Swish,Linear,Conv1d,Conv2d,Conv3d,DepthwiseConv1d,DepthwiseConv2d,DepthwiseConv3d,ConvTranspose1d,ConvTranspose2d,ConvTranspose3d,DepthwiseConvTranspose1d,DepthwiseConvTranspose2d,DepthwiseConvTranspose3d,GlobalAvgPool1d,GlobalAvgPool2d,GlobalAvgPool3d,GlobalMaxPool1d,GlobalMaxPool2d,GlobalMaxPool3d,LayerNorm,GroupNorm,Embedding,BatchNorm3d,BatchNorm2d,BatchNorm1d,BatchNorm'"
torchfusion/layers/layers.py,10,"b'import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom torchfusion.initializers import *\nfrom torch.nn.modules.conv import _ConvNd,_ConvTransposeMixin,_single,_pair,_triple\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\n\nclass MultiSequential(nn.Sequential):\n    def __init__(self, *args):\n        super(MultiSequential, self).__init__(*args)\n\n    def forward(self, *input):\n        for module in self._modules.values():\n            input = module(*input)\n        return input\n\n\nclass Conv1d(nn.Conv1d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True,weight_init=Kaiming_Normal(),bias_init=Zeros()):\n\n        super(Conv1d,self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, groups, bias)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n        \nclass Conv2d(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True,weight_init=Kaiming_Normal(),bias_init=Zeros()):\n\n        super(Conv2d,self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, groups, bias)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\nclass Conv3d(nn.Conv3d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True,weight_init=Kaiming_Normal(),bias_init=Zeros()):\n\n        super(Conv3d,self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, groups, bias)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\n\nclass DepthwiseConv1d(nn.Conv1d):\n    def __init__(self, in_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True,multiplier=1,weight_init=Kaiming_Normal(),bias_init=Zeros()):\n\n        super(DepthwiseConv1d,self).__init__(in_channels, in_channels*multiplier, kernel_size, stride,\n                 padding, dilation, in_channels, bias)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\nclass DepthwiseConv2d(nn.Conv2d):\n    def __init__(self, in_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True,multiplier=1,weight_init=Kaiming_Normal(),bias_init=Zeros()):\n\n        super(DepthwiseConv2d,self).__init__(in_channels, in_channels*multiplier, kernel_size, stride,\n                 pa2ding, dilation, in_channels, bias)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\nclass DepthwiseConv3d(nn.Conv3d):\n    def __init__(self, in_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True,multiplier=1,weight_init=Kaiming_Normal(),bias_init=Zeros()):\n\n        super(DepthwiseConv3d,self).__init__(in_channels, in_channels*multiplier, kernel_size, stride,\n                 pa2ding, dilation, in_channels, bias)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\n\nclass ConvTranspose1d(nn.ConvTranspose1d):\n    def __init__(self,in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1,weight_init=Kaiming_Normal(), bias_init=Zeros()):\n\n        super(ConvTranspose1d,self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, output_padding, groups, bias, dilation)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\nclass ConvTranspose2d(nn.ConvTranspose2d):\n    def __init__(self,in_channels, out_channels, kernel_size, stride=1,padding=0, output_padding=0, groups=1, bias=True, dilation=1,weight_init=Kaiming_Normal(), bias_init=Zeros()):\n\n        super(ConvTranspose2d,self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, output_padding, groups, bias, dilation)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\nclass ConvTranspose3d(nn.ConvTranspose3d):\n    def __init__(self,in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1,weight_init=Kaiming_Normal(), bias_init=Zeros()):\n\n        super(ConvTranspose3d,self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, output_padding, groups, bias, dilation)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\nclass DepthwiseConvTranspose1d(nn.ConvTranspose1d):\n    def __init__(self,in_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1,multiplier=1,weight_init=Kaiming_Normal(), bias_init=Zeros()):\n\n        super(DepthwiseConvTranspose1d,self).__init__(in_channels, in_channels*multiplier, kernel_size, stride,\n                 padding, output_padding, in_channels, bias, dilation)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\nclass DepthwiseConvTranspose2d(nn.ConvTranspose2d):\n    def __init__(self,in_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1,multiplier=1,weight_init=Kaiming_Normal(), bias_init=Zeros()):\n\n        super(DepthwiseConvTranspose2d,self).__init__(in_channels, in_channels*multiplier, kernel_size, stride,\n                 padding, output_padding, in_channels, bias, dilation)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\nclass DepthwiseConvTranspose3d(nn.ConvTranspose3d):\n    def __init__(self,in_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1,multiplier=1,weight_init=Kaiming_Normal(), bias_init=Zeros()):\n\n        super(DepthwiseConvTranspose3d,self).__init__(in_channels, in_channels*multiplier, kernel_size, stride,\n                 padding, output_padding, in_channels, bias, dilation)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\nclass Linear(nn.Linear):\n    def __init__(self,in_features,out_features,bias=True,weight_init=Xavier_Normal(),bias_init=Zeros()):\n        """"""\n\n        :param in_features:\n        :param out_features:\n        :param bias:\n        :param weight_init:\n        :param bias_init:\n        """"""\n        super(Linear,self).__init__(in_features,out_features,bias)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias and bias_init is not None:\n            bias_init(self.bias.data)\n\nclass Flatten(nn.Module):\n    def __init__(self,batch_first=True):\n        """"""\n\n        :param batch_first:\n        """"""\n        super(Flatten,self).__init__()\n        self.batch_first = batch_first\n\n    def forward(self,inputs):\n\n        if self.batch_first:\n            size = torch.prod(torch.LongTensor(list(inputs.size())[1:])).item()\n            return inputs.view(-1,size)\n        else:\n            size = torch.prod(torch.LongTensor(list(inputs.size())[:len(inputs.size())-1])).item()\n            return inputs.view(size,-1)\n\nclass Reshape(nn.Module):\n    def __init__(self,output_shape,batch_first=True):\n        """"""\n\n        :param output_shape:\n        :param batch_first:\n        """"""\n        super(Reshape,self).__init__()\n\n        self.output_shape = output_shape\n        self.batch_first = batch_first\n\n    def forward(self,inputs):\n        if isinstance(self.output_shape,int):\n            size = [self.output_shape]\n        else:\n            size = list(self.output_shape)\n\n        if self.batch_first:\n            input_total_size = torch.prod(torch.LongTensor(list(inputs.size())[1:])).item()\n        else:\n            input_total_size = torch.prod(torch.LongTensor(list(inputs.size())[:len(inputs.size())-1])).item()\n\n\n\n        target_total_size = torch.prod(torch.LongTensor(size)).item()\n\n        if input_total_size != target_total_size:\n            raise ValueError("" Reshape must preserve total dimension, input size: {} and output size: {}"".format(input.size()[1:],self.output_shape))\n\n        size = list(size)\n        if self.batch_first:\n            size = tuple([-1] + size)\n        else:\n            size = tuple(size + [-1])\n        outputs = inputs.view(size)\n\n        return outputs\n\n\nclass _GlobalPoolNd(nn.Module):\n    def __init__(self,flatten=True):\n        """"""\n\n        :param flatten:\n        """"""\n        super(_GlobalPoolNd,self).__init__()\n        self.flatten = flatten\n\n    def pool(self,input):\n        """"""\n\n        :param input:\n        :return:\n        """"""\n        raise NotImplementedError()\n\n    def forward(self,input):\n        """"""\n\n        :param input:\n        :return:\n        """"""\n        input = self.pool(input)\n        size_0 = input.size(1)\n        return input.view(-1,size_0) if self.flatten else input\n\nclass GlobalAvgPool1d(_GlobalPoolNd):\n    def __init__(self,flatten=True):\n        """"""\n\n        :param flatten:\n        """"""\n        super(GlobalAvgPool1d,self).__init__(flatten)\n\n    def pool(self, input):\n\n        return F.adaptive_avg_pool1d(input,1)\n\nclass GlobalAvgPool2d(_GlobalPoolNd):\n    def __init__(self, flatten=True):\n        """"""\n\n        :param flatten:\n        """"""\n        super(GlobalAvgPool2d,self).__init__(flatten)\n\n    def pool(self, input):\n        return F.adaptive_avg_pool2d(input,1)\n\nclass GlobalAvgPool3d(_GlobalPoolNd):\n    def __init__(self, flatten=True):\n        """"""\n\n        :param flatten:\n        """"""\n        super(GlobalAvgPool3d,self).__init__(flatten)\n\n    def pool(self, input):\n        return F.adaptive_avg_pool3d(input,1)\n\n\nclass GlobalMaxPool1d(_GlobalPoolNd):\n    def __init__(self, flatten=True):\n        """"""\n\n        :param flatten:\n        """"""\n        super(GlobalMaxPool1d,self).__init__(flatten)\n\n    def pool(self, input):\n        return F.adaptive_max_pool1d(input, 1)\n\n\nclass GlobalMaxPool2d(_GlobalPoolNd):\n    def __init__(self, flatten=True):\n        """"""\n\n        :param flatten:\n        """"""\n        super(GlobalMaxPool2d,self).__init__(flatten)\n\n    def pool(self, input):\n        return F.adaptive_max_pool2d(input, 1)\n\n\nclass GlobalMaxPool3d(_GlobalPoolNd):\n    def __init__(self, flatten=True):\n        """"""\n\n        :param flatten:\n        """"""\n        super(GlobalMaxPool3d,self).__init__(flatten)\n\n    def pool(self, input):\n        return F.adaptive_max_pool3d(input, 1)\n\n\nclass RNNBase(nn.RNNBase):\n    def __init__(self,mode, input_size, hidden_size,\n                 num_layers=1, bias=True, batch_first=False,\n                 dropout=0, bidirectional=False,weight_init=None):\n        """"""\n\n        :param mode:\n        :param input_size:\n        :param hidden_size:\n        :param num_layers:\n        :param bias:\n        :param batch_first:\n        :param dropout:\n        :param bidirectional:\n        :param weight_init:\n        """"""\n        super(RNNBase,self).__init__(mode, input_size, hidden_size,\n                 num_layers, bias, batch_first, dropout,bidirectional)\n\n        if weight_init is not None:\n            for weight in super(RNNBase, self).parameters():\n                weight_init(weight)\n\nclass RNN(RNNBase):\n    def __init__(self, *args, **kwargs):\n        """"""\n\n        :param args:\n        :param kwargs:\n        """"""\n        if \'nonlinearity\' in kwargs:\n            if kwargs[\'nonlinearity\'] == \'tanh\':\n                mode = \'RNN_TANH\'\n            elif kwargs[\'nonlinearity\'] == \'relu\':\n                mode = \'RNN_RELU\'\n            else:\n                raise ValueError(""Unknown nonlinearity \'{}\'"".format(\n                    kwargs[\'nonlinearity\']))\n            del kwargs[\'nonlinearity\']\n        else:\n            mode = \'RNN_TANH\'\n\n        super(RNN, self).__init__(mode, *args, **kwargs)\n\nclass GRU(RNNBase):\n    def __init__(self, *args, **kwargs):\n        """"""\n\n        :param args:\n        :param kwargs:\n        """"""\n        super(GRU, self).__init__(\'GRU\', *args, **kwargs)\n\nclass LSTM(RNNBase):\n    def __init__(self, *args, **kwargs):\n        """"""\n\n        :param args:\n        :param kwargs:\n        """"""\n        super(LSTM, self).__init__(\'LSTM\', *args, **kwargs)\n\nclass Swish(nn.Module):\n    def __init__(self):\n        super(Swish, self).__init__()\n\n    def forward(self, inputs):\n\n        return inputs * torch.sigmoid(inputs)\n\nclass GroupNorm(nn.GroupNorm):\n    def __init__(self, *args,weight_init=None,bias_init=None):\n        """"""\n\n        :param args:\n        :param weight_init:\n        :param bias_init:\n        """"""\n        super(GroupNorm,self).__init__(*args)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias_init is not None:\n            bias_init(self.bias.data)\n\nclass LayerNorm(nn.LayerNorm):\n    def __init__(self, *args,weight_init=None,bias_init=None):\n        """"""\n\n        :param args:\n        :param weight_init:\n        :param bias_init:\n        """"""\n        super(LayerNorm,self).__init__(*args)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n        if bias_init is not None:\n            bias_init(self.bias.data)\n\n\nclass Embedding(nn.Embedding):\n    def __init__(self,num_embeddings, embedding_dim, padding_idx=None,\n                 max_norm=None, norm_type=2, scale_grad_by_freq=False,\n                 sparse=False, _weight=None,weight_init=None):\n        """"""\n\n        :param num_embeddings:\n        :param embedding_dim:\n        :param padding_idx:\n        :param max_norm:\n        :param norm_type:\n        :param scale_grad_by_freq:\n        :param sparse:\n        :param _weight:\n        :param weight_init:\n        """"""\n        super(Embedding,self).__init__(num_embeddings, embedding_dim, padding_idx,\n                 max_norm, norm_type, scale_grad_by_freq,\n                 sparse, _weight)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n\n\nclass BatchNorm(_BatchNorm):\n    def __init__(self,num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True,weight_init=None,bias_init=None):\n        """"""\n\n        :param num_features:\n        :param eps:\n        :param momentum:\n        :param affine:\n        :param track_running_stats:\n        :param weight_init:\n        :param bias_init:\n        """"""\n        super(BatchNorm,self).__init__(num_features, eps, momentum,affine,\n                 track_running_stats)\n\n        if weight_init is not None:\n            weight_init(self.weight.data)\n\n        if bias_init is not None:\n            bias_init(self.bias.data)\n\n\n\nclass BatchNorm1d(BatchNorm):\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(\'expected 2D or 3D input (got {}D input)\'\n                             .format(input.dim()))\n\n\nclass BatchNorm2d(BatchNorm):\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\'expected 4D input (got {}D input)\'\n                             .format(input.dim()))\n\nclass BatchNorm3d(BatchNorm):\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError(\'expected 5D input (got {}D input)\')\n\n\n\n\n\n\n'"
torchfusion/learners/__init__.py,0,"b'from .learners import StandardLearner,TextClassifier,BaseLearner, BaseTextLearner,AbstractBaseLearner'"
torchfusion/learners/learners.py,34,"b'import torch\nfrom torch.autograd import Variable\nimport torch.cuda as cuda\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR\nimport os\nfrom time import time\nfrom math import ceil\nfrom io import open\nfrom ..utils import PlotInput, visualize, get_model_summary,get_batch_size,clip_grads,save_model,load_model\nfrom tensorboardX import SummaryWriter\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.onnx as onnx\nimport torch.backends.cudnn as cudnn\n\nr""""""Abstract base Model for training, evaluating and performing inference\nAll custom models should subclass this and implement train, evaluate and predict functions\n\n    Args:\n        use_cuda_if_available (boolean): If set to true, training would be done on a gpu if any is available\n    \n    """"""\n\n\nclass AbstractBaseLearner():\n    def __init__(self, use_cuda_if_available=True):\n\n        self.cuda = False\n        self.fp16_mode = False\n        if use_cuda_if_available and cuda.is_available():\n            self.cuda = True\n            cudnn.benchmark = True\n\n        self.epoch_start_funcs = []\n        self.batch_start_funcs = []\n        self.epoch_end_funcs = []\n        self.batch_end_funcs = []\n        self.train_completed_funcs = []\n\n    r""""""Defines the training loop\n    subclasses must override this\n    """"""\n    def train(self, *args):\n        raise NotImplementedError()\n\n    r""""""Defines the evaluation loop\n    subclasses must override this\n    """"""\n    def evaluate(self, *args):\n        raise NotImplementedError()\n\n    r""""""Defines the validation loop\n        subclasses must override this\n        """"""\n    def validate(self, *args):\n        raise NotImplementedError()\n\n    r""""""Defines the prediction logic\n    subclasses must override this\n    """"""\n    def predict(self, *args):\n        raise NotImplementedError()\n\n    r""""""Adds a function to be called at the start of each epoch\n        It should have the following signature::\n\n            func(epoch) -> None\n    """"""\n    def half(self):\n        self.fp16_mode = True\n\n    def add_on_epoch_start(self,func):\n        self.epoch_start_funcs.append(func)\n\n\n    r""""""Adds a function to be called at the end of each epoch\n        It should have the following signature::\n\n            func(epoch,data) -> None\n        data is a dictionary containining metric values, losses and vital details at the end of the epcoh\n    """"""\n    def add_on_epoch_end(self, func):\n        self.epoch_end_funcs.append(func)\n\n    r""""""Adds a function to be called at the start of each batch\n        It should have the following signature::\n\n            func(epoch,batch) -> None\n    """"""\n    def add_on_batch_start(self, func):\n        self.batch_start_funcs.append(func)\n\n    r""""""Adds a function to be called at the end of each batch\n        It should have the following signature::\n\n            func(epoch,batch,data) -> None\n        data is a dictionary containining metric values, losses and vital details at the end of the batch\n    """"""\n    def add_on_batch_end(self, func):\n        self.batch_end_funcs.append(func)\n\n    r""""""Adds a function to be called at the end of training\n        It should have the following signature::\n\n            func(data) -> None\n        data is a dictionary containining metric values, duration and vital details at the end of training\n    """"""\n    def add_on_training_completed(self, func):\n        self.train_completed_funcs.append(func)\n\n    r"""""" This function should return a dictionary containing information about the training including metric values.\n    Child classes must override this.\n        """"""\n    def get_train_history(self):\n        raise NotImplementedError()\n\n\nr""""""This is the base learner for training, evaluating and performing inference with a single model\nAll custom learners should subclass this and implement __train__, __evaluate__,__validate__ and __predict__ functions\nThis class already takes care of data loading, iterations and metrics, subclasses only need to define custom logic for training,\nevaluation and prediction\n\n    Args:\n        model (nn.Module):  the module to be used for training, evaluation and inference.\n        use_cuda_if_available (boolean): If set to true, training would be done on a gpu if any is available\n""""""\n\nclass BaseLearner(AbstractBaseLearner):\n    def __init__(self, model, use_cuda_if_available=True):\n        self.model = model\n        super(BaseLearner, self).__init__(use_cuda_if_available)\n        self.__train_history__ = {}\n        self.train_running_loss = None\n        self.train_metrics = None\n        self.test_metrics = None\n        self.val_metrics = None\n\n        self.iterations = 0\n        self.model_dir = os.getcwd()\n\n    r""""""Initialize model weights using pre-trained weights from the filepath\n\n        Args:\n            path (str): path to a compatible pre-defined model\n\n        """"""\n    def load_model(self, path):\n        load_model(self.model,path)\n\n\n    r""""""Saves the model to the path specified\n            Args:\n                path (str): path to save model\n                save_architecture (boolean): if True, both weights and architecture will be saved, default is False\n\n            """"""\n    def save_model(self, path,save_architecture=False):\n        save_model(self.model,path,save_architecture)\n\n    def train(self,*args):\n\n        self.__train_loop__(*args)\n\n    def __train_loop__(self, train_loader, train_metrics, test_loader=None, test_metrics=None, val_loader=None,val_metrics=None, num_epochs=10,lr_scheduler=None,\n              save_models=""all"", model_dir=os.getcwd(),save_model_interval=1,display_metrics=True, save_metrics=True, notebook_mode=False, batch_log=True, save_logs=None,\n              visdom_log=None,tensorboard_log=None, save_architecture=False):\n        """"""\n\n        :param train_loader:\n        :param train_metrics:\n        :param test_loader:\n        :param test_metrics:\n        :param val_loader:\n        :param val_metrics:\n        :param num_epochs:\n        :param lr_scheduler:\n        :param save_models:\n        :param model_dir:\n        :param save_model_interval:\n        :param display_metrics:\n        :param save_metrics:\n        :param notebook_mode:\n        :param batch_log:\n        :param save_logs:\n        :param visdom_log:\n        :param tensorboard_log:\n        :param save_architecture:\n        :return:\n        """"""\n\n        if save_models not in [""all"", ""best""]:\n            raise ValueError(""save models must be \'all\' or \'best\' , {} is invalid"".format(save_models))\n        if save_models == ""best"" and test_loader is None and val_loader is None:\n            raise ValueError(""save models can only be best when test_loader or val_loader is provided "")\n\n        if test_loader is not None:\n            if test_metrics is None:\n                raise ValueError(""You must provide a metric for your test data"")\n            elif len(test_metrics) == 0:\n                raise ValueError(""test metrics cannot be an empty list"")\n\n        if val_loader is not None:\n            if val_metrics is None:\n                raise ValueError(""You must provide a metric for your val data"")\n            elif len(val_metrics) == 0:\n                raise ValueError(""val metrics cannot be an empty list"")\n\n        self.train_metrics = train_metrics\n        self.test_metrics = test_metrics\n        self.val_metrics = val_metrics\n\n        self.model_dir = model_dir\n\n        if not os.path.exists(model_dir):\n            os.mkdir(model_dir)\n\n        models_all = os.path.join(model_dir, ""all_models"")\n        models_best = os.path.join(model_dir, ""best_models"")\n\n        if not os.path.exists(models_all):\n            os.mkdir(models_all)\n\n        if not os.path.exists(models_best) and (test_loader is not None or val_loader is not None):\n            os.mkdir(models_best)\n\n        from tqdm import tqdm_notebook\n        from tqdm import tqdm\n\n        best_test_metric = 0.0\n        best_val_metric = 0.0\n        train_start_time = time()\n        for e in range(num_epochs):\n\n            print(""Epoch {} of {}"".format(e + 1, num_epochs))\n\n            for metric in self.train_metrics:\n                metric.reset()\n\n            self.model.train()\n\n            for func in self.epoch_start_funcs:\n                func(e + 1)\n\n            self.train_running_loss = torch.Tensor([0.0])\n            train_loss = 0.0\n            data_len = 0\n\n            if notebook_mode and batch_log:\n                progress_ = tqdm_notebook(enumerate(train_loader))\n            elif batch_log:\n                progress_ = tqdm(enumerate(train_loader))\n            else:\n                progress_ = enumerate(train_loader)\n\n            max_batch_size = 0\n\n            init_time = time()\n\n            for i, data in progress_:\n                for func in self.batch_start_funcs:\n                    func(e + 1,i + 1)\n\n                batch_size = get_batch_size(data)\n\n                if max_batch_size < batch_size:\n                    max_batch_size = batch_size\n\n                self.__train_func__(data)\n                self.iterations += 1\n                data_len += batch_size\n                train_loss = self.train_running_loss.item() / data_len\n\n                if batch_log:\n                    progress_message = """"\n                    for metric in self.train_metrics:\n                        progress_message += ""Train {} : {}"".format(metric.name, metric.getValue())\n                    progress_.set_description(""{}/{} batches "".format(int(ceil(data_len / max_batch_size)),\n                                                                      int(ceil(len(\n                                                                          train_loader.dataset) / max_batch_size))))\n                    progress_dict = {""Train Loss"": train_loss}\n\n                    for metric in self.train_metrics:\n                        progress_dict[""Train "" + metric.name] = metric.getValue()\n\n                    progress_.set_postfix(progress_dict)\n                batch_info = {""train_loss"":train_loss}\n                for metric in self.train_metrics:\n\n                    metric_name = ""train_{}"".format(metric.name)\n                    batch_info[metric_name] = metric.getValue()\n\n                for func in self.batch_end_funcs:\n                    func(e + 1,i + 1,batch_info)\n            if self.cuda:\n                cuda.synchronize()\n            duration = time() - init_time\n\n            if ""duration"" in self.__train_history__:\n                self.__train_history__[""duration""].append(duration)\n            else:\n                self.__train_history__[""duration""] = [duration]\n\n\n            if ""train_loss"" in self.__train_history__:\n                self.__train_history__[""train_loss""].append(train_loss)\n            else:\n                self.__train_history__[""train_loss""] = [train_loss]\n\n            model_file = os.path.join(models_all, ""model_{}.pth"".format(e + 1))\n\n            if save_models == ""all"" and (e+1) % save_model_interval == 0:\n                self.save_model(model_file,save_architecture)\n\n            logfile = None\n            if save_logs is not None:\n                logfile = open(save_logs, ""a"")\n\n\n            print(os.linesep + ""Epoch: {}, Duration: {} , Train Loss: {}"".format(e + 1, duration, train_loss))\n            if logfile is not None:\n                logfile.write(\n                    os.linesep + ""Epoch: {}, Duration: {} , Train Loss: {}"".format(e + 1, duration, train_loss))\n\n            if val_loader is None and lr_scheduler is not None:\n                if isinstance(lr_scheduler,ReduceLROnPlateau):\n                    lr_scheduler.step(train_metrics[0].getValue())\n                else:\n                    lr_scheduler.step()\n\n            if test_loader is not None:\n                message = ""Test Accuracy did not improve, current best is {}"".format(best_test_metric)\n                current_best = best_test_metric\n                self.evaluate(test_loader, test_metrics)\n                result = self.test_metrics[0].getValue()\n\n                if result > current_best:\n                    best_test_metric = result\n                    message = ""Test {} improved from {} to {}"".format(test_metrics[0].name, current_best, result)\n                    model_file = os.path.join(models_best, ""model_{}.pth"".format(e + 1))\n                    self.save_model(model_file,save_architecture)\n\n                    print(os.linesep + ""{} New Best Model saved in {}"".format(message, model_file))\n                    if logfile is not None:\n                        logfile.write(os.linesep + ""{} New Best Model saved in {}"".format(message, model_file))\n\n                else:\n                    print(os.linesep + message)\n                    if logfile is not None:\n                        logfile.write(os.linesep + message)\n\n                for metric in self.test_metrics:\n                    metric_name = ""test_{}"".format(metric.name)\n                    if metric_name in self.__train_history__:\n                        self.__train_history__[metric_name].append(metric.getValue())\n                    else:\n                        self.__train_history__[metric_name] = [metric.getValue()]\n\n\n                    print(""Test {} : {}"".format(metric.name, metric.getValue()))\n                    if logfile is not None:\n                        logfile.write(os.linesep + ""Test {} : {}"".format(metric.name, metric.getValue()))\n\n            if val_loader is not None:\n                message = ""Val Accuracy did not improve, current best is {}"".format(best_val_metric)\n                current_best = best_val_metric\n                self.validate(val_loader, val_metrics)\n                result = self.val_metrics[0].getValue()\n\n                if lr_scheduler is not None:\n                    if isinstance(lr_scheduler, ReduceLROnPlateau):\n                        lr_scheduler.step(result)\n                    else:\n                        lr_scheduler.step()\n\n                if result > current_best:\n                    best_val_metric = result\n                    message = ""Val {} improved from {} to {}"".format(val_metrics[0].name, current_best, result)\n\n                    if test_loader is None:\n                        model_file = os.path.join(models_best, ""model_{}.pth"".format(e + 1))\n                        self.save_model(model_file,save_architecture)\n\n                        print(os.linesep + ""{} New Best Model saved in {}"".format(message, model_file))\n                        if logfile is not None:\n                            logfile.write(os.linesep + ""{} New Best Model saved in {}"".format(message, model_file))\n                    else:\n                        print(os.linesep + ""{}"".format(message))\n                        if logfile is not None:\n                            logfile.write(os.linesep + ""{}"".format(message))\n\n                else:\n                    print(os.linesep + message)\n                    if logfile is not None:\n                        logfile.write(os.linesep + message)\n\n                for metric in self.val_metrics:\n\n                    metric_name = ""val_{}"".format(metric.name)\n                    if metric_name in self.__train_history__:\n                        self.__train_history__[metric_name].append(metric.getValue())\n                    else:\n                        self.__train_history__[metric_name] = [metric.getValue()]\n\n                    print(""Val {} : {}"".format(metric.name, metric.getValue()))\n                    if logfile is not None:\n                        logfile.write(os.linesep + ""Val {} : {}"".format(metric.name, metric.getValue()))\n\n\n            for metric in self.train_metrics:\n\n                metric_name = ""train_{}"".format(metric.name)\n                if metric_name in self.__train_history__:\n                    self.__train_history__[metric_name].append(metric.getValue())\n                else:\n                    self.__train_history__[metric_name] = [metric.getValue()]\n\n                print(""Train {} : {}"".format(metric.name, metric.getValue()))\n                if logfile is not None:\n                    logfile.write(os.linesep + ""Train {} : {}"".format(metric.name, metric.getValue()))\n\n            if logfile is not None:\n                logfile.close()\n\n\n            if ""epoch"" in self.__train_history__:\n                self.__train_history__[""epoch""].append(e+1)\n            else:\n                self.__train_history__[""epoch""] = [e+1]\n            epoch_arr = self.__train_history__[""epoch""]\n            epoch_arr_tensor = torch.LongTensor(epoch_arr)\n\n            if visdom_log is not None:\n                visdom_log.plot_line(torch.FloatTensor(self.__train_history__[""train_loss""]),epoch_arr_tensor,win=""train_loss"",title=""Train Loss"")\n\n                if test_metrics is not None:\n                     for metric in test_metrics:\n                         metric_name = ""test_{}"".format(metric.name)\n                         visdom_log.plot_line(torch.FloatTensor(self.__train_history__[metric_name]),epoch_arr_tensor,win=""test_{}"".format(metric.name),title=""Test {}"".format(metric.name))\n                if val_metrics is not None:\n                     for metric in val_metrics:\n                         metric_name = ""val_{}"".format(metric.name)\n                         visdom_log.plot_line(torch.FloatTensor(self.__train_history__[metric_name]),epoch_arr_tensor,win=""val_{}"".format(metric.name),title=""Val {}"".format(metric.name))\n\n                for metric in train_metrics:\n                    metric_name = ""train_{}"".format(metric.name)\n                    visdom_log.plot_line(torch.FloatTensor(self.__train_history__[metric_name]), epoch_arr_tensor,\n                                         win=""train_{}"".format(metric.name), title=""Train {}"".format(metric.name))\n\n\n            if tensorboard_log is not None:\n                writer = SummaryWriter(os.path.join(model_dir,tensorboard_log))\n                writer.add_scalar(""logs/train_loss"", train_loss, global_step=e+1)\n\n                if test_metrics is not None:\n                     for metric in test_metrics:\n                         writer.add_scalar(""logs/test_metrics/{}"".format(metric.name), metric.getValue(),\n                                           global_step=e+1)\n                if val_metrics is not None:\n                     for metric in val_metrics:\n                         writer.add_scalar(""logs/val_metrics/{}"".format(metric.name), metric.getValue(),\n                                           global_step=e+1)\n                for metric in train_metrics:\n                    writer.add_scalar(""logs/train_metrics/{}"".format(metric.name), metric.getValue(),\n                                      global_step=e + 1)\n\n                writer.close()\n\n            if display_metrics or save_metrics:\n\n                save_path = None\n\n                if save_metrics:\n                    save_path = os.path.join(model_dir, ""epoch_{}_loss.png"".format(e + 1))\n                visualize(epoch_arr, [PlotInput(value=self.__train_history__[""train_loss""], name=""Train Loss"", color=""red"")],\n                          display=display_metrics,\n                          save_path=save_path)\n\n            if test_loader is not None and (display_metrics or save_metrics):\n                for metric in self.test_metrics:\n                    metric_name = ""test_{}"".format(metric.name)\n\n                    save_path = None\n\n                    if save_metrics:\n                        save_path = os.path.join(model_dir, ""test_{}_epoch_{}.png"".format(metric.name, e + 1))\n                    visualize(epoch_arr, [PlotInput(value=self.__train_history__[metric_name], name=""Test "" + metric.name, color=""blue"")],\n                              display=display_metrics,\n                              save_path=save_path)\n\n            if val_loader is not None and (display_metrics or save_metrics):\n                for metric in self.val_metrics:\n                    metric_name = ""val_{}"".format(metric.name)\n\n                    save_path = None\n\n                    if save_metrics:\n                        save_path = os.path.join(model_dir, ""val_{}_epoch_{}.png"".format(metric.name, e + 1))\n                    visualize(epoch_arr, [PlotInput(value=self.__train_history__[metric_name], name=""Val "" + metric.name, color=""blue"")],\n                              display=display_metrics,\n                              save_path=save_path)\n\n            for metric in self.train_metrics:\n                metric_name = ""train_{}"".format(metric.name)\n                save_path = None\n                if save_metrics:\n                    save_path = os.path.join(model_dir, ""train_{}_epoch_{}.png"".format(metric.name, e + 1))\n                visualize(epoch_arr, [PlotInput(value=self.__train_history__[metric_name], name=""Train "" + metric.name, color=""blue"")],\n                              display=display_metrics,\n                              save_path=save_path)\n            epoch_info = {""train_loss"": train_loss,""duration"":duration}\n            for metric in self.train_metrics:\n                metric_name = ""train_{}"".format(metric.name)\n                epoch_info[metric_name] = metric.getValue()\n            if self.test_metrics != None and test_loader != None:\n                for metric in self.test_metrics:\n\n                    metric_name = ""test_{}"".format(metric.name)\n                    epoch_info[metric_name] = metric.getValue()\n\n            if self.val_metrics != None and val_loader != None:\n                for metric in self.val_metrics:\n\n                    metric_name = ""val_{}"".format(metric.name)\n                    epoch_info[metric_name] = metric.getValue()\n\n            for func in self.epoch_end_funcs:\n                func(e + 1,epoch_info)\n\n        train_end_time = time() - train_start_time\n        train_info = {""train_duration"":train_end_time}\n        for metric in self.train_metrics:\n            metric_name = ""train_{}"".format(metric.name)\n            train_info[metric_name] = metric.getValue()\n\n        if self.test_metrics != None and test_loader != None:\n            for metric in self.test_metrics:\n                metric_name = ""test_{}"".format(metric.name)\n                train_info[metric_name] = metric.getValue()\n\n\n        if val_loader != None:\n            for metric in self.val_metrics:\n                metric_name = ""train_{}"".format(metric.name)\n                train_info[metric_name] = metric.getValue()\n\n        for func in self.train_completed_funcs:\n            func(train_info)\n\n\n\n    r""""""Training logic, all models must override this\n            Args:\n                data: a single batch of data from the train_loader\n    """"""\n\n    def __train_func__(self, data):\n        raise NotImplementedError()\n\n    r""""""Evaluates the dataset on the set of provided metrics\n            Args:\n                test_loader (DataLoader): an instance of DataLoader containing the test set\n                test_metrics ([]): an array of metrics for evaluating the test set\n        """"""\n    def evaluate(self, test_loader, metrics):\n\n        if self.test_metrics is None:\n            self.test_metrics = metrics\n\n        for metric in self.test_metrics:\n            metric.reset()\n\n        self.model.eval()\n\n        for i, data in enumerate(test_loader):\n            self.__eval_function__(data)\n\n\n    r""""""Evaluation logic, all models must override this\n            Args:\n                data: a single batch of data from the test_loader\n        """"""\n    def __eval_function__(self, data):\n        raise NotImplementedError()\n\n    r""""""Validates the dataset on the set of provided metrics\n                Args:\n                    val_loader (DataLoader): an instance of DataLoader containing the test set\n                    metrics ([]): an array of metrics for evaluating the test set\n            """"""\n\n    def validate(self, val_loader, metrics):\n\n        if self.val_metrics is None:\n            self.val_metrics = metrics\n\n        for metric in self.val_metrics:\n            metric.reset()\n\n        self.model.eval()\n\n        for i, data in enumerate(val_loader):\n            self.__val_function__(data)\n\n\n    r""""""Validation logic, all models must override this\n            Args:\n                data: a single batch of data from the test_loader\n        """"""\n\n    def __val_function__(self, data):\n        raise NotImplementedError()\n\n    r""""""Runs inference on the given input\n            Args:\n                inputs: a DataLoader or a tensor of input values.\n    """"""\n    def predict(self, inputs):\n        self.model.eval()\n\n        if isinstance(inputs, DataLoader):\n            predictions = []\n            for i, data in enumerate(inputs):\n                batch_pred = self.__predict_func__(data)\n\n                for pred in batch_pred:\n                    predictions.append(pred.unsqueeze(0))\n\n            return torch.cat(predictions)\n        else:\n            pred = self.__predict_func__(inputs)\n            return pred.squeeze(0)\n\n    r""""""Inference logic, all models must override this\n            Args:\n                input: a batch of data \n    """"""\n\n    def __predict_func__(self, input):\n        raise NotImplementedError()\n\n    r"""""" Returns a dictionary containing the values of metrics, epochs and loss during training.\n    """"""\n    def get_train_history(self):\n        return self.__train_history__\n\n\nclass BaseTextLearner(BaseLearner):\n    def __init__(self, model, source_field, target_field, batch_first=False,use_cuda_if_available=True):\n\n        super(BaseTextLearner, self).__init__(model,use_cuda_if_available)\n\n        self.batch_first = batch_first\n        self.source_field = source_field\n        self.target_field = target_field\n\n    def __train_loop__(self, train_loader, train_metrics, test_loader=None, test_metrics=None, val_loader=None,val_metrics=None, num_epochs=10,lr_scheduler=None,\n              save_models=""all"", model_dir=os.getcwd(),save_model_interval=1,display_metrics=True, save_metrics=True, notebook_mode=False, batch_log=True, save_logs=None,\n              visdom_log=None,tensorboard_log=None, save_architecture=False):\n        """"""\n\n        :param train_loader:\n        :param train_metrics:\n        :param test_loader:\n        :param test_metrics:\n        :param val_loader:\n        :param val_metrics:\n        :param num_epochs:\n        :param lr_scheduler:\n        :param save_models:\n        :param model_dir:\n        :param save_model_interval:\n        :param display_metrics:\n        :param save_metrics:\n        :param notebook_mode:\n        :param batch_log:\n        :param save_logs:\n        :param visdom_log:\n        :param tensorboard_log:\n        :param save_architecture:\n        :return:\n        """"""\n\n        if save_models not in [""all"", ""best""]:\n            raise ValueError(""save models must be \'all\' or \'best\' , {} is invalid"".format(save_models))\n        if save_models == ""best"" and test_loader is None and val_loader is None:\n            raise ValueError(""save models can only be best when test_loader or val_loader is provided "")\n\n        if test_loader is not None:\n            if test_metrics is None:\n                raise ValueError(""You must provide a metric for your test data"")\n            elif len(test_metrics) == 0:\n                raise ValueError(""test metrics cannot be an empty list"")\n\n        if val_loader is not None:\n            if val_metrics is None:\n                raise ValueError(""You must provide a metric for your val data"")\n            elif len(val_metrics) == 0:\n                raise ValueError(""val metrics cannot be an empty list"")\n\n        self.train_metrics = train_metrics\n        self.test_metrics = test_metrics\n        self.val_metrics = val_metrics\n\n        self.model_dir = model_dir\n\n        if not os.path.exists(model_dir):\n            os.mkdir(model_dir)\n\n        models_all = os.path.join(model_dir, ""all_models"")\n        models_best = os.path.join(model_dir, ""best_models"")\n\n        if not os.path.exists(models_all):\n            os.mkdir(models_all)\n\n        if not os.path.exists(models_best) and (test_loader is not None or val_loader is not None):\n            os.mkdir(models_best)\n\n        from tqdm import tqdm_notebook\n        from tqdm import tqdm\n\n        best_test_metric = 0.0\n        best_val_metric = 0.0\n        train_start_time = time()\n        for e in range(num_epochs):\n\n            print(""Epoch {} of {}"".format(e + 1, num_epochs))\n\n            for metric in self.train_metrics:\n                metric.reset()\n\n            self.model.train()\n\n            for func in self.epoch_start_funcs:\n                func(e + 1)\n\n            self.train_running_loss = torch.Tensor([0.0])\n            train_loss = 0.0\n            data_len = 0\n\n            if notebook_mode and batch_log:\n                progress_ = tqdm_notebook(enumerate(train_loader))\n            elif batch_log:\n                progress_ = tqdm(enumerate(train_loader))\n            else:\n                progress_ = enumerate(train_loader)\n\n            max_batch_size = 0\n\n            init_time = time()\n\n            for i, data in progress_:\n                for func in self.batch_start_funcs:\n                    func(e + 1,i + 1)\n\n                source = getattr(data, self.source_field)\n\n                batch_size = get_batch_size(source, self.batch_first)\n\n                if max_batch_size < batch_size:\n                    max_batch_size = batch_size\n\n                self.__train_func__(data)\n                self.iterations += 1\n                data_len += batch_size\n                train_loss = self.train_running_loss.item() / data_len\n\n                if batch_log:\n                    progress_message = """"\n                    for metric in self.train_metrics:\n                        progress_message += ""Train {} : {}"".format(metric.name, metric.getValue())\n                    progress_.set_description(""{}/{} batches "".format(int(ceil(data_len / max_batch_size)),\n                                                                      int(ceil(len(\n                                                                          train_loader.dataset) / max_batch_size))))\n                    progress_dict = {""Train Loss"": train_loss}\n\n                    for metric in self.train_metrics:\n                        progress_dict[""Train "" + metric.name] = metric.getValue()\n\n                    progress_.set_postfix(progress_dict)\n                batch_info = {""train_loss"":train_loss}\n                for metric in self.train_metrics:\n\n                    metric_name = ""train_{}"".format(metric.name)\n                    batch_info[metric_name] = metric.getValue()\n\n                for func in self.batch_end_funcs:\n                    func(e + 1,i + 1,batch_info)\n            if self.cuda:\n                cuda.synchronize()\n            duration = time() - init_time\n\n            if ""duration"" in self.__train_history__:\n                self.__train_history__[""duration""].append(duration)\n            else:\n                self.__train_history__[""duration""] = [duration]\n\n\n            if ""train_loss"" in self.__train_history__:\n                self.__train_history__[""train_loss""].append(train_loss)\n            else:\n                self.__train_history__[""train_loss""] = [train_loss]\n\n            model_file = os.path.join(models_all, ""model_{}.pth"".format(e + 1))\n\n            if save_models == ""all"" and (e+1) % save_model_interval == 0:\n                self.save_model(model_file,save_architecture)\n\n            logfile = None\n            if save_logs is not None:\n                logfile = open(save_logs, ""a"")\n\n\n            print(os.linesep + ""Epoch: {}, Duration: {} , Train Loss: {}"".format(e + 1, duration, train_loss))\n            if logfile is not None:\n                logfile.write(\n                    os.linesep + ""Epoch: {}, Duration: {} , Train Loss: {}"".format(e + 1, duration, train_loss))\n\n            if val_loader is None and lr_scheduler is not None:\n                if isinstance(lr_scheduler,ReduceLROnPlateau):\n                    lr_scheduler.step(train_metrics[0].getValue())\n                else:\n                    lr_scheduler.step()\n\n            if test_loader is not None:\n                message = ""Test Accuracy did not improve, current best is {}"".format(best_test_metric)\n                current_best = best_test_metric\n                self.evaluate(test_loader, test_metrics)\n                result = self.test_metrics[0].getValue()\n\n                if result > current_best:\n                    best_test_metric = result\n                    message = ""Test {} improved from {} to {}"".format(test_metrics[0].name, current_best, result)\n                    model_file = os.path.join(models_best, ""model_{}.pth"".format(e + 1))\n                    self.save_model(model_file,save_architecture)\n\n                    print(os.linesep + ""{} New Best Model saved in {}"".format(message, model_file))\n                    if logfile is not None:\n                        logfile.write(os.linesep + ""{} New Best Model saved in {}"".format(message, model_file))\n\n                else:\n                    print(os.linesep + message)\n                    if logfile is not None:\n                        logfile.write(os.linesep + message)\n\n                for metric in self.test_metrics:\n                    metric_name = ""test_{}"".format(metric.name)\n                    if metric_name in self.__train_history__:\n                        self.__train_history__[metric_name].append(metric.getValue())\n                    else:\n                        self.__train_history__[metric_name] = [metric.getValue()]\n\n\n                    print(""Test {} : {}"".format(metric.name, metric.getValue()))\n                    if logfile is not None:\n                        logfile.write(os.linesep + ""Test {} : {}"".format(metric.name, metric.getValue()))\n\n            if val_loader is not None:\n                message = ""Val Accuracy did not improve, current best is {}"".format(best_val_metric)\n                current_best = best_val_metric\n                self.validate(val_loader, val_metrics)\n                result = self.val_metrics[0].getValue()\n\n                if lr_scheduler is not None:\n                    if isinstance(lr_scheduler, ReduceLROnPlateau):\n                        lr_scheduler.step(result)\n                    else:\n                        lr_scheduler.step()\n\n                if result > current_best:\n                    best_val_metric = result\n                    message = ""Val {} improved from {} to {}"".format(val_metrics[0].name, current_best, result)\n\n                    if test_loader is None:\n                        model_file = os.path.join(models_best, ""model_{}.pth"".format(e + 1))\n                        self.save_model(model_file,save_architecture)\n\n                        print(os.linesep + ""{} New Best Model saved in {}"".format(message, model_file))\n                        if logfile is not None:\n                            logfile.write(os.linesep + ""{} New Best Model saved in {}"".format(message, model_file))\n                    else:\n                        print(os.linesep + ""{}"".format(message))\n                        if logfile is not None:\n                            logfile.write(os.linesep + ""{}"".format(message))\n\n                else:\n                    print(os.linesep + message)\n                    if logfile is not None:\n                        logfile.write(os.linesep + message)\n\n                for metric in self.val_metrics:\n\n                    metric_name = ""val_{}"".format(metric.name)\n                    if metric_name in self.__train_history__:\n                        self.__train_history__[metric_name].append(metric.getValue())\n                    else:\n                        self.__train_history__[metric_name] = [metric.getValue()]\n\n                    print(""Val {} : {}"".format(metric.name, metric.getValue()))\n                    if logfile is not None:\n                        logfile.write(os.linesep + ""Val {} : {}"".format(metric.name, metric.getValue()))\n\n\n            for metric in self.train_metrics:\n\n                metric_name = ""train_{}"".format(metric.name)\n                if metric_name in self.__train_history__:\n                    self.__train_history__[metric_name].append(metric.getValue())\n                else:\n                    self.__train_history__[metric_name] = [metric.getValue()]\n\n                print(""Train {} : {}"".format(metric.name, metric.getValue()))\n                if logfile is not None:\n                    logfile.write(os.linesep + ""Train {} : {}"".format(metric.name, metric.getValue()))\n\n            if logfile is not None:\n                logfile.close()\n\n\n            if ""epoch"" in self.__train_history__:\n                self.__train_history__[""epoch""].append(e+1)\n            else:\n                self.__train_history__[""epoch""] = [e+1]\n            epoch_arr = self.__train_history__[""epoch""]\n            epoch_arr_tensor = torch.LongTensor(epoch_arr)\n\n            if visdom_log is not None:\n                visdom_log.plot_line(torch.FloatTensor(self.__train_history__[""train_loss""]),epoch_arr_tensor,win=""train_loss"",title=""Train Loss"")\n\n                if test_metrics is not None:\n                     for metric in test_metrics:\n                         metric_name = ""test_{}"".format(metric.name)\n                         visdom_log.plot_line(torch.FloatTensor(self.__train_history__[metric_name]),epoch_arr_tensor,win=""test_{}"".format(metric.name),title=""Test {}"".format(metric.name))\n                if val_metrics is not None:\n                     for metric in val_metrics:\n                         metric_name = ""val_{}"".format(metric.name)\n                         visdom_log.plot_line(torch.FloatTensor(self.__train_history__[metric_name]),epoch_arr_tensor,win=""val_{}"".format(metric.name),title=""Val {}"".format(metric.name))\n\n                for metric in train_metrics:\n                    metric_name = ""train_{}"".format(metric.name)\n                    visdom_log.plot_line(torch.FloatTensor(self.__train_history__[metric_name]), epoch_arr_tensor,\n                                         win=""train_{}"".format(metric.name), title=""Train {}"".format(metric.name))\n\n\n            if tensorboard_log is not None:\n                writer = SummaryWriter(os.path.join(model_dir,tensorboard_log))\n                writer.add_scalar(""logs/train_loss"",train_loss,global_step=e+1)\n\n                if test_metrics is not None:\n                     for metric in test_metrics:\n                         writer.add_scalar(""logs/test_metrics/{}"".format(metric.name), metric.getValue(),\n                                           global_step=e+1)\n                if val_metrics is not None:\n                     for metric in val_metrics:\n                         writer.add_scalar(""logs/val_metrics/{}"".format(metric.name), metric.getValue(),\n                                           global_step=e+1)\n                for metric in train_metrics:\n                    writer.add_scalar(""logs/train_metrics/{}"".format(metric.name), metric.getValue(),\n                                      global_step=e + 1)\n\n                writer.close()\n\n            if display_metrics or save_metrics:\n\n                save_path = None\n\n                if save_metrics:\n                    save_path = os.path.join(model_dir, ""epoch_{}_loss.png"".format(e + 1))\n                visualize(epoch_arr, [PlotInput(value=self.__train_history__[""train_loss""], name=""Train Loss"", color=""red"")],\n                          display=display_metrics,\n                          save_path=save_path)\n\n            if test_loader is not None and (display_metrics or save_metrics):\n                for metric in self.test_metrics:\n                    metric_name = ""test_{}"".format(metric.name)\n\n                    save_path = None\n\n                    if save_metrics:\n                        save_path = os.path.join(model_dir, ""test_{}_epoch_{}.png"".format(metric.name, e + 1))\n                    visualize(epoch_arr, [PlotInput(value=self.__train_history__[metric_name], name=""Test "" + metric.name, color=""blue"")],\n                              display=display_metrics,\n                              save_path=save_path)\n\n            if val_loader is not None and (display_metrics or save_metrics):\n                for metric in self.val_metrics:\n                    metric_name = ""val_{}"".format(metric.name)\n\n                    save_path = None\n\n                    if save_metrics:\n                        save_path = os.path.join(model_dir, ""val_{}_epoch_{}.png"".format(metric.name, e + 1))\n                    visualize(epoch_arr, [PlotInput(value=self.__train_history__[metric_name], name=""Val "" + metric.name, color=""blue"")],\n                              display=display_metrics,\n                              save_path=save_path)\n\n            for metric in self.train_metrics:\n                metric_name = ""train_{}"".format(metric.name)\n                save_path = None\n                if save_metrics:\n                    save_path = os.path.join(model_dir, ""train_{}_epoch_{}.png"".format(metric.name, e + 1))\n                visualize(epoch_arr, [PlotInput(value=self.__train_history__[metric_name], name=""Train "" + metric.name, color=""blue"")],\n                              display=display_metrics,\n                              save_path=save_path)\n            epoch_info = {""train_loss"": train_loss,""duration"":duration}\n            for metric in self.train_metrics:\n                metric_name = ""train_{}"".format(metric.name)\n                epoch_info[metric_name] = metric.getValue()\n            if self.test_metrics != None and test_loader != None:\n                for metric in self.test_metrics:\n\n                    metric_name = ""test_{}"".format(metric.name)\n                    epoch_info[metric_name] = metric.getValue()\n\n            if self.val_metrics != None and val_loader != None:\n                for metric in self.val_metrics:\n\n                    metric_name = ""val_{}"".format(metric.name)\n                    epoch_info[metric_name] = metric.getValue()\n\n            for func in self.epoch_end_funcs:\n                func(e + 1,epoch_info)\n\n        train_end_time = time() - train_start_time\n        train_info = {""train_duration"":train_end_time}\n        for metric in self.train_metrics:\n            metric_name = ""train_{}"".format(metric.name)\n            train_info[metric_name] = metric.getValue()\n\n        if self.test_metrics != None and test_loader != None:\n            for metric in self.test_metrics:\n                metric_name = ""test_{}"".format(metric.name)\n                train_info[metric_name] = metric.getValue()\n\n\n        if val_loader != None:\n            for metric in self.val_metrics:\n                metric_name = ""train_{}"".format(metric.name)\n                train_info[metric_name] = metric.getValue()\n\n        for func in self.train_completed_funcs:\n            func(train_info)\n\n\n\n\nclass StandardLearner(BaseLearner):\n    def __init__(self, model, use_cuda_if_available=True):\n        super(StandardLearner,self).__init__(model, use_cuda_if_available)\n\n    """"""Train function\n\n               Args:\n                   train_loader (DataLoader): an instance of DataLoader containing the training set\n                   loss_fn (Loss): the loss function \n                   optimizer (Optimizer): an optimizer for updating parameters \n                   train_metrics ([]): an array of metrics for evaluating the training set\n                   test_loader (DataLoader): an instance of DataLoader containing the test set\n                   test_metrics ([]): an array of metrics for evaluating the test set\n                   num_epochs (int): The maximum number of training epochs\n                   lr_scheduler (_LRSchedular): Learning rate scheduler updated at every epoch\n                   save_models (str): If all, the model is saved at the end of each epoch while the best models based \n                       on the test set are also saved in best_models folder\n                       If \'best\', only the best models are saved,  test_loader and test_metrics must be provided\n                   model_dir (str) : a path in which to save the models\n                   save_model_interval (int): saves the models after every n epoch\n                   notebook_mode (boolean): Optimizes the progress bar for either jupyter notebooks or consoles\n                   display_metrics (boolean): Enables display of metrics and loss visualizations at the end of each epoch.\n                   save_metrics (boolean): Enables saving of metrics and loss visualizations at the end of each epoch.\n                   batch_log (boolean): Enables printing of logs at every batch iteration\n                   save_logs (str): Specifies a filepath in which to permanently save logs at every epoch\n                   visdom_log (VisdomLogger): Logs outputs and metrics to the visdom server\n                   tensorboard_log (str): Logs outputs and metrics to the filepath for visualization in tensorboard\n                   save_architecture (boolean): Saves the architecture as well as weights during model saving\n                   clip_grads: a tuple specifying the minimum and maximum gradient values\n\n                   """"""\n    def train(self, train_loader, loss_fn, optimizer, train_metrics, test_loader=None, test_metrics=None, val_loader=None,val_metrics=None, num_epochs=10,lr_scheduler=None,\n              save_models=""all"", model_dir=os.getcwd(),save_model_interval=1,display_metrics=False, save_metrics=False, notebook_mode=False, batch_log=True, save_logs=None,\n              visdom_log=None,tensorboard_log=None, save_architecture=False,clip_grads=None):\n\n        self.optimizer = optimizer\n        self.loss_fn = loss_fn\n        self.clip_grads = clip_grads\n        \n        super().__train_loop__(train_loader, train_metrics, test_loader, test_metrics, val_loader,val_metrics, num_epochs,lr_scheduler,\n              save_models, model_dir,save_model_interval,display_metrics, save_metrics, notebook_mode, batch_log, save_logs,\n              visdom_log,tensorboard_log, save_architecture)\n\n    def __train_func__(self, data):\n\n        self.optimizer.zero_grad()\n\n        if self.clip_grads is not None:\n\n            clip_grads(self.model,self.clip_grads[0],self.clip_grads[1])\n\n        train_x, train_y = data\n\n        batch_size = get_batch_size(train_x)\n\n        if isinstance(train_x,list) or isinstance(train_x,tuple):\n            train_x = (Variable(x.cuda() if self.cuda else x) for x in train_x)\n        else:\n            train_x = Variable(train_x.cuda() if self.cuda else train_x)\n\n        if isinstance(train_y,list) or isinstance(train_y,tuple):\n            train_y = (Variable(y.cuda() if self.cuda else y) for y in train_y)\n        else:\n            train_y = Variable(train_y.cuda() if self.cuda else train_y)\n\n        outputs = self.model(train_x)\n        loss = self.loss_fn(outputs, train_y)\n        if self.fp16_mode:\n            self.optimizer.backward(loss)\n        else:\n            loss.backward()\n\n        self.optimizer.step()\n        self.train_running_loss = self.train_running_loss + (loss.cpu().item() * batch_size)\n\n        for metric in self.train_metrics:\n            metric.update(outputs, train_y)\n\n    def __eval_function__(self, data):\n\n        test_x, test_y = data\n\n        if isinstance(test_x, list) or isinstance(test_x, tuple):\n            test_x = (x.cuda() if self.cuda else x for x in test_x)\n        else:\n            test_x = test_x.cuda() if self.cuda else test_x\n\n        if isinstance(test_y, list) or isinstance(test_y, tuple):\n            test_y = (y.cuda() if self.cuda else y for y in test_y)\n        else:\n            test_y = test_y.cuda() if self.cuda else test_y\n\n        outputs = self.model(test_x)\n\n        for metric in self.test_metrics:\n            metric.update(outputs, test_y)\n\n    def __val_function__(self, data):\n\n        val_x, val_y = data\n        if isinstance(val_x, list) or isinstance(val_x, tuple):\n            val_x = (x.cuda() if self.cuda else x for x in val_x)\n        else:\n            val_x = val_x.cuda() if self.cuda else val_x\n\n        if isinstance(val_y, list) or isinstance(val_y, tuple):\n            val_y = (y.cuda() if self.cuda else y for y in val_y)\n        else:\n            val_y = val_y.cuda() if self.cuda else val_y\n\n        outputs = self.model(val_x)\n\n        for metric in self.val_metrics:\n            metric.update(outputs, val_y)\n\n    def __predict_func__(self, inputs):\n        if isinstance(inputs, list) or isinstance(inputs, tuple):\n            inputs = (x.cuda() if self.cuda else x for x in inputs)\n        else:\n            inputs = inputs.cuda() if self.cuda else inputs\n\n        return self.model(inputs)\n\n    """"""returns a complete summary of the model\n\n                   Args:\n                       input_sizes: a single tuple or a list of tuples in the case of multiple inputs, specifying the\n                       size of the inputs to the model\n                       input_types: a single tensor type or a list of tensors in the case of multiple inputs, specifying the \n                       type of the inputs to the model\n                       item_length(int): the length of each item in the summary\n                       tensorboard_log(str): if enabled, the model will be serialized into a format readable by tensorboard,\n                       useful for visualizing the model in tensorboard.\n                       """"""\n\n    def summary(self,input_sizes,input_types=torch.FloatTensor,item_length=26,tensorboard_log=None):\n\n        if isinstance(input_sizes,list):\n            inputs = (torch.randn(input_size).type(input_type).unsqueeze(0) for input_size, input_type in zip(input_sizes,input_types))\n\n            inputs = (input.cuda() if self.cuda else input for input in inputs)\n        else:\n            inputs = torch.randn(input_sizes).type(input_types).unsqueeze(0)\n\n            inputs = inputs.cuda() if self.cuda else inputs\n\n\n        return get_model_summary(self.model,inputs,item_length=item_length,tensorboard_log=tensorboard_log)\n\n    """"""saves the model in onnx format\n\n                       Args:\n                           input_sizes: a single tuple or a list of tuples in the case of multiple inputs, specifying the\n                           size of the inputs to the model\n                           input_types: a single tensor type or a list of tensors in the case of multiple inputs, specifying the \n                           type of the inputs to the model\n                         """"""\n    def to_onnx(self,input_sizes,path,input_types=torch.FloatTensor,**kwargs):\n        if isinstance(input_sizes,list):\n            inputs = (torch.randn(input_size).type(input_type).unsqueeze(0) for input_size, input_type in zip(input_sizes,input_types))\n\n            inputs = (input.cuda() if self.cuda else input for input in inputs)\n        else:\n            inputs = torch.randn(input_sizes).type(input_types).unsqueeze(0)\n\n            inputs = inputs.cuda() if self.cuda else inputs\n\n        return onnx._export(self.model, inputs, f=path, **kwargs)\n\n\nclass TextClassifier(BaseTextLearner):\n    def __init__(self, model, source_field, target_field, batch_first=False, use_cuda_if_available=True):\n        super(TextClassifier, self).__init__(model, source_field, target_field, batch_first, use_cuda_if_available)\n\n    """"""Train function\n\n                   Args:\n                       train_loader (DataLoader): an instance of DataLoader containing the training set\n                       loss_fn (Loss): the loss function \n                       optimizer (Optimizer): an optimizer for updating parameters \n                       train_metrics ([]): an array of metrics for evaluating the training set\n                       test_loader (DataLoader): an instance of DataLoader containing the test set\n                       test_metrics ([]): an array of metrics for evaluating the test set\n                       num_epochs (int): The maximum number of training epochs\n                       lr_scheduler (_LRSchedular): Learning rate scheduler updated at every epoch\n                       save_models (str): If all, the model is saved at the end of each epoch while the best models based \n                           on the test set are also saved in best_models folder\n                           If \'best\', only the best models are saved,  test_loader and test_metrics must be provided\n                       model_dir (str) : a path in which to save the models\n                       save_model_interval (int): saves the models after every n epoch\n                       notebook_mode (boolean): Optimizes the progress bar for either jupyter notebooks or consoles\n                       display_metrics (boolean): Enables display of metrics and loss visualizations at the end of each epoch.\n                       save_metrics (boolean): Enables saving of metrics and loss visualizations at the end of each epoch.\n                       batch_log (boolean): Enables printing of logs at every batch iteration\n                       save_logs (str): Specifies a filepath in which to permanently save logs at every epoch\n                       visdom_log (VisdomLogger): Logs outputs and metrics to the visdom server\n                       tensorboard_log (str): Logs outputs and metrics to the filepath for visualization in tensorboard\n                       save_architecture (boolean): Saves the architecture as well as weights during model saving\n                       clip_grads: a tuple specifying the minimum and maximum gradient values\n\n                       """"""\n    def train(self, train_loader, loss_fn, optimizer, train_metrics, test_loader=None, test_metrics=None, val_loader=None,val_metrics=None, num_epochs=10,lr_scheduler=None,\n              save_models=""all"", model_dir=os.getcwd(),save_model_interval=1,display_metrics=False, save_metrics=False, notebook_mode=False, batch_log=True, save_logs=None,\n              visdom_log=None,tensorboard_log=None, save_architecture=False,clip_grads=None):\n\n        self.optimizer = optimizer\n        self.loss_fn = loss_fn\n        self.clip_grads = clip_grads\n        \n        super().__train_loop__(train_loader, train_metrics, test_loader, test_metrics, val_loader,val_metrics, num_epochs,lr_scheduler,\n              save_models, model_dir,save_model_interval,display_metrics, save_metrics, notebook_mode, batch_log, save_logs,\n              visdom_log,tensorboard_log, save_architecture)\n\n\n    def __train_func__(self, data):\n\n        self.optimizer.zero_grad()\n        if self.clip_grads is not None:\n\n            clip_grads(self.model,self.clip_grads[0],self.clip_grads[1])\n\n        train_x = getattr(data, self.source_field)\n        train_y = getattr(data, self.target_field)\n\n        batch_size = get_batch_size(train_x,self.batch_first)\n\n        if isinstance(train_x, list) or isinstance(train_x, tuple):\n            train_x = (x.cuda() if self.cuda else x for x in train_x)\n        else:\n            train_x = train_x.cuda() if self.cuda else train_x\n\n        if isinstance(train_y, list) or isinstance(train_y, tuple):\n            train_y = (y.cuda() if self.cuda else y for y in train_y)\n        else:\n            train_y = train_y.cuda() if self.cuda else train_y\n\n        outputs = self.model(train_x)\n        loss = self.loss_fn(outputs, train_y)\n        if self.fp16_mode:\n            self.optimizer.backward(loss)\n        else:\n            loss.backward()\n\n        self.optimizer.step()\n        self.train_running_loss = self.train_running_loss + (loss.cpu().item() * batch_size)\n        \n        for metric in self.train_metrics:\n            metric.update(outputs, train_y,self.batch_first)\n\n    def __eval_function__(self, data):\n\n        test_x = getattr(data, self.source_field)\n        test_y = getattr(data, self.target_field)\n        if isinstance(test_x, list) or isinstance(test_x, tuple):\n            test_x = (x.cuda() if self.cuda else x for x in test_x)\n        else:\n            test_x = test_x.cuda() if self.cuda else test_x\n\n        if isinstance(test_y, list) or isinstance(test_y, tuple):\n            test_y = (y.cuda() if self.cuda else y for y in test_y)\n        else:\n            test_y = test_y.cuda() if self.cuda else test_y\n\n        outputs = self.model(test_x)\n\n        for metric in self.test_metrics:\n            metric.update(outputs, test_y,self.batch_first)\n\n    def __val_function__(self, data):\n\n        val_x = getattr(data, self.source_field)\n        val_y = getattr(data, self.target_field)\n        if isinstance(val_x, list) or isinstance(val_x, tuple):\n            val_x = (x.cuda() if self.cuda else x for x in val_x)\n        else:\n            val_x = val_x.cuda() if self.cuda else val_x\n\n        if isinstance(val_y, list) or isinstance(val_y, tuple):\n            val_y = (y.cuda() if self.cuda else y for y in val_y)\n        else:\n            val_y = val_y.cuda() if self.cuda else val_y\n\n\n        outputs = self.model(val_x)\n\n        for metric in self.val_metrics:\n            metric.update(outputs.cpu().data , val_y.cpu().data,self.batch_first)\n\n    def __predict_func__(self, inputs):\n        if isinstance(inputs, list) or isinstance(inputs, tuple):\n            inputs = (x.cuda() if self.cuda else x for x in inputs)\n        else:\n            inputs = inputs.cuda() if self.cuda else inputs\n\n        return self.model(inputs)\n\n""""""returns a complete summary of the model\n\n                   Args:\n                       input_sizes: a single tuple or a list of tuples in the case of multiple inputs, specifying the\n                       size of the inputs to the model\n                       input_types: a single tensor type or a list of tensors in the case of multiple inputs, specifying the \n                       type of the inputs to the model\n                       item_length(int): the length of each item in the summary\n                       tensorboard_log(str): if enabled, the model will be serialized into a format readable by tensorboard,\n                       useful for visualizing the model in tensorboard.\n                       """"""\n\ndef summary(self, input_sizes, input_types=torch.FloatTensor, item_length=26, tensorboard_log=None):\n    if isinstance(input_sizes, list):\n        inputs = (torch.randn(input_size).type(input_type).unsqueeze(0) for input_size, input_type in zip(input_sizes, input_types))\n\n        inputs = (input.cuda() if self.cuda else input for input in inputs)\n    else:\n        inputs = torch.randn(input_sizes).type(input_types).unsqueeze(0)\n\n        inputs = inputs.cuda() if self.cuda else inputs\n\n    return get_model_summary(self.model, inputs, item_length=item_length, tensorboard_log=tensorboard_log)\n\n""""""saves the model in onnx format\n\n                       Args:\n                           input_sizes: a single tuple or a list of tuples in the case of multiple inputs, specifying the\n                           size of the inputs to the model\n                           input_types: a single tensor type or a list of tensors in the case of multiple inputs, specifying the \n                           type of the inputs to the model\n                         """"""\ndef to_onnx(self, input_sizes, path, input_types=torch.FloatTensor, **kwargs):\n    if isinstance(input_sizes, list):\n        inputs = (torch.randn(input_size).type(input_type).unsqueeze(0) for input_size, input_type in\n                  zip(input_sizes, input_types))\n\n        inputs = (input.cuda() if self.cuda else input for input in inputs)\n    else:\n        inputs = torch.randn(input_sizes).type(input_types).unsqueeze(0)\n\n        inputs = inputs.cuda() if self.cuda else inputs\n\n    return onnx._export(self.model, inputs, f=path, **kwargs)\n'"
torchfusion/metrics/__init__.py,0,"b'from .metrics import Accuracy,MSE,Metric,MeanConfidenceScore,MAE'"
torchfusion/metrics/metrics.py,9,"b'import torch\nimport torch.nn as nn\nfrom ..utils import get_batch_size\n\n"""""" Base class for all metrics, subclasses should implement the __compute__ function\n    Arguments:\n        name:  name of the metric\n""""""\n\nclass Metric():\n    def __init__(self,name):\n        self.name = name\n        self.__count = 0\n        self.__sum = 0.0\n\n    def reset(self):\n        """"""\n\n        :return:\n        """"""\n        self.__count = 0\n        self.__sum = 0.0\n\n    def update(self,predictions,targets,batch_first=True):\n        """"""\n\n        :param predictions:\n        :param targets:\n        :param batch_first:\n        :return:\n        """"""\n        val = self.__compute__(predictions,targets)\n\n        multiplier = 1\n        if isinstance(predictions,list) or isinstance(predictions,tuple):\n            multiplier *= len(predictions)\n\n        batch_size = get_batch_size(predictions,batch_first)\n\n        self.__sum = self.__sum + val\n        self.__count = self.__count + batch_size * multiplier\n    def getValue(self):\n        """"""\n\n        :return:\n        """"""\n        return (self.__sum.type(torch.FloatTensor)/self.__count).item()\n\n    def __compute__(self,prediction,label):\n        """"""\n\n        :param prediction:\n        :param label:\n        :return:\n        """"""\n        raise NotImplementedError()\n\n"""""" Acccuracy metric to compute topK accuracy\n    Arguments:\n        name:  name of the metric\n        topK: the topK values to consider\n""""""\nclass Accuracy(Metric):\n    def __init__(self,name=""Accuracy"",topK=1):\n        """"""\n\n        :param name:\n        :param topK:\n        """"""\n        super(Accuracy,self).__init__(name)\n        self.topK = topK\n\n    def __compute__(self,prediction,label):\n        """"""\n\n        :param prediction:\n        :param label:\n        :return:\n        """"""\n\n        if isinstance(prediction,list) or isinstance(prediction,tuple):\n            correct = None\n            for preds,labels in zip(prediction,label):\n                preds = preds.cpu().data\n                labels = labels.cpu().data.long()\n                _, pred = preds.topk(self.topK,1,True,True)\n                pred = pred.t()\n                if correct is None:\n                    correct = pred.eq(labels.view(1,-1).expand_as(pred))[:self.topK].view(-1).float().sum(0,True)\n                else:\n                    correct += pred.eq(labels.view(1, -1).expand_as(pred))[:self.topK].view(-1).float().sum(0, True)\n\n        else:\n\n            predictions = prediction.type(torch.float32).cpu().data\n            labels = label.cpu().data.long()\n            _, pred = predictions.topk(self.topK, 1, True, True)\n            pred = pred.t()\n            correct = pred.eq(labels.view(1, -1).expand_as(pred))[:self.topK].view(-1).float().sum(0, True)\n\n        return correct\n\n\nclass MeanConfidenceScore(Metric):\n    def __init__(self,name=""MeanConfidenceScore"",topK=1,apply_softmax=True):\n        """"""\n\n        :param name:\n        :param topK:\n        :param apply_softmax:\n        """"""\n        super(MeanConfidenceScore,self).__init__(name)\n        self.topK = topK\n        self.apply_softmax = apply_softmax\n\n    def __compute__(self,prediction,label):\n        """"""\n\n        :param prediction:\n        :param label:\n        :return:\n        """"""\n\n        if isinstance(prediction, list) or isinstance(prediction, tuple):\n            sum = None\n            for preds,labels in zip(prediction,label):\n                labels = labels.long()\n                if self.apply_softmax:\n                    preds = nn.Softmax(dim=1)(preds)\n\n                for i, pred in enumerate(preds):\n                    y_score = pred[labels[i]]\n                    val = y_score if y_score in pred.topk(self.topK)[0] else 0\n                    if sum is None:\n                        sum = val\n                    else:\n                        sum.add_(val)\n        else:\n            labels = label.long()\n            if self.apply_softmax:\n                prediction = nn.Softmax(dim=1)(prediction)\n\n            sum = None\n\n            for i, pred in enumerate(prediction):\n                y_score = pred[labels[i]]\n                val = y_score if y_score in pred.topk(self.topK)[0] else 0\n\n                if sum is None:\n                    sum = val\n                else:\n                    sum.add_(val)\n\n        return sum\n\n"""""" Mean Squared Error\n    Arguments:\n        name:  name of the metric\n    \n""""""\nclass MSE(Metric):\n    def __init__(self,name=""MSE""):\n        """"""\n\n        :param name:\n        """"""\n        super(MSE,self).__init__(name)\n\n    def __compute__(self,prediction,label):\n\n        \n        """"""\n\n        :param prediction:\n        :param label:\n        :return:\n        """"""\n        if isinstance(prediction, list) or isinstance(prediction, tuple):\n            sum = None\n\n            for preds,labels in zip(prediction,label):\n                if sum is None:\n                    sum = torch.sum((preds - labels) ** 2)\n                else:\n                    sum += torch.sum((preds - labels) ** 2)\n\n        else:\n            sum = torch.sum((prediction - label) ** 2)\n\n        return sum\n\nclass MAE(Metric):\n    def __init__(self,name=""MAE""):\n        """"""\n\n        :param name:\n        """"""\n        super(MAE,self).__init__(name)\n\n    def __compute__(self,prediction,label):\n        """"""\n\n        :param prediction:\n        :param label:\n        :return:\n        """"""\n        if isinstance(prediction, list) or isinstance(prediction, tuple):\n            sum = None\n\n            for preds,labels in zip(prediction,label):\n                if sum is None:\n                    sum = torch.sum(torch.abs(preds - labels))\n                else:\n                    sum += torch.sum(torch.abs(preds - labels))\n\n\n        else:\n            sum = torch.sum(torch.abs(prediction - label))\n\n        return sum\n'"
torchfusion/transforms/__init__.py,0,"b'from .transforms import ToTensor,Normalize,Compose'"
torchfusion/transforms/transforms.py,1,"b'import torch\n\n\nclass ToTensor(object):\n\n    def __call__(self, input):\n        """"""\n\n        :param input:\n        :return:\n        """"""\n        return torch.tensor(input)\n\nclass Normalize(object):\n    def __init__(self,mean,std):\n        """"""\n\n        :param mean:\n        :param std:\n        """"""\n        self.mean = mean\n        self.std = std\n    def __call__(self, input):\n        """"""\n\n        :param input:\n        :return:\n        """"""\n        return input.sub_(self.mean).div_(self.std)\n\nclass Compose(object):\n    def __init__(self,transforms):\n        """"""\n\n        :param transforms:\n        """"""\n\n        self.transforms = transforms\n\n    def __call__(self, input):\n        """"""\n\n        :param input:\n        :return:\n        """"""\n\n        for trans in self.transforms:\n            input = trans(input)\n        return input\n\n'"
torchfusion/utils/__init__.py,0,"b'from .logger import VisdomLogger\nfrom .utils import get_model_summary,clip_grads,get_batch_size,decode_imagenet,one_hot,FieldInput,adjust_learning_rate,visualize,PlotInput,load_image,download_file,extract_tar,extract_zip,save_model,load_model'"
torchfusion/utils/logger.py,0,"b'import visdom\n\nclass VisdomLogger(object):\n    def __init__(self,\n        server=\'http://localhost\',\n        endpoint=\'events\',\n        port=8097,\n        ipv6=True,\n        http_proxy_host=None,\n        http_proxy_port=None,\n        env=\'main\',\n        send=True,\n        raise_exceptions=None,\n        use_incoming_socket=True):\n\n        """"""\n\n        :param server:\n        :param endpoint:\n        :param port:\n        :param ipv6:\n        :param http_proxy_host:\n        :param http_proxy_port:\n        :param env:\n        :param send:\n        :param raise_exceptions:\n        :param use_incoming_socket:\n        """"""\n\n        self.viz = visdom.Visdom(server=server,endpoint=endpoint,port=port,ipv6=ipv6,http_proxy_host=http_proxy_host,http_proxy_port=http_proxy_port,\n                                 env=env,send=send,raise_exceptions=raise_exceptions,use_incoming_socket=use_incoming_socket)\n        self.env = env\n    def log_text(self,text,win,title=None, env=None, opts=None):\n        """"""\n\n        :param text:\n        :param win:\n        :param title:\n        :param env:\n        :param opts:\n        :return:\n        """"""\n        append = self.viz.win_exists(win,env)\n\n        opts = {""title"":title} if opts == None else opts\n\n        self.viz.text(text,win,env=env,opts=opts,append=append)\n    def log_image(self,image,win,title=None,env=None,opts=None):\n\n        """"""\n\n        :param image:\n        :param win:\n        :param title:\n        :param env:\n        :param opts:\n        :return:\n        """"""\n\n        opts = {""title"": title} if opts == None else opts\n\n        self.viz.image(image, win, env=env, opts=opts)\n\n    def log_images(self,image,win,title=None,env=None,opts=None,ncol=8,padding=2):\n\n        """"""\n\n        :param image:\n        :param win:\n        :param title:\n        :param env:\n        :param opts:\n        :param ncol:\n        :param padding:\n        :return:\n        """"""\n\n        opts = {""title"": title} if opts == None else opts\n\n        self.viz.images(image, win=win, env=env, opts=opts,nrow=ncol,padding=padding)\n\n    def plot_line(self,X,Y,win,title=None, env=None, opts=None,update_mode=None,name=None):\n\n        """"""\n\n        :param X:\n        :param Y:\n        :param win:\n        :param title:\n        :param env:\n        :param opts:\n        :param update_mode:\n        :param name:\n        :return:\n        """"""\n\n        opts = {""title"":title} if opts == None else opts\n\n        self.viz.line(X=X,Y=Y,win=win,env=env,opts=opts,update=update_mode,name=name)\n\n    def plot_bar(self,X,Y,win,title=None, env=None, opts=None):\n\n        """"""\n\n        :param X:\n        :param Y:\n        :param win:\n        :param title:\n        :param env:\n        :param opts:\n        :return:\n        """"""\n\n        opts = {""title"":title} if opts == None else opts\n\n        self.viz.bar(X=X,Y=Y,win=win,env=env,opts=opts)\n\n    def plot_quiver(self,X,Y,win,title=None,gridX=None,gridY=None, env=None, opts=None):\n\n        """"""\n\n        :param X:\n        :param Y:\n        :param win:\n        :param title:\n        :param gridX:\n        :param gridY:\n        :param env:\n        :param opts:\n        :return:\n        """"""\n\n        opts = {""title"":title} if opts == None else opts\n\n        self.viz.quiver(X=X,Y=Y,gridX=gridX,gridY=gridY,win=win,env=env,opts=opts)\n\n    def plot_pie(self, X,win, title=None, env=None, opts=None):\n        """"""\n\n        :param X:\n        :param win:\n        :param title:\n        :param env:\n        :param opts:\n        :return:\n        """"""\n        opts = {""title"": title} if opts == None else opts\n\n        self.viz.pie(X=X, win=win, env=env, opts=opts)\n\n    def plot_histogram(self, X,win, title=None, env=None, opts=None):\n        """"""\n\n        :param X:\n        :param win:\n        :param title:\n        :param env:\n        :param opts:\n        :return:\n        """"""\n        opts = {""title"": title} if opts == None else opts\n\n        self.viz.histogram(X=X, win=win, env=env, opts=opts)\n\n    def plot_boxplot(self, X,win, title=None, env=None, opts=None):\n        """"""\n\n        :param X:\n        :param win:\n        :param title:\n        :param env:\n        :param opts:\n        :return:\n        """"""\n        opts = {""title"": title} if opts == None else opts\n\n        self.viz.boxplot(X=X, win=win, env=env, opts=opts)\n\n    def save(self,envs=[""main""]):\n        """"""\n\n        :param envs:\n        :return:\n        """"""\n        self.viz.save(envs)\n\n\n\n'"
torchfusion/utils/utils.py,10,"b'import matplotlib.pyplot as plt\nfrom collections import namedtuple\nimport torch\nimport os\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport random\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport tarfile\nfrom zipfile import ZipFile\nimport requests\nimport shutil\nfrom ..layers import MultiSequential\nfrom ..fp16_utils import tofp16\n\n\ndef download_file(url,path,extract_path=None):\n\n    """"""\n\n    :param url:\n    :param path:\n    :param extract_path:\n    :return:\n    """"""\n\n    data = requests.get(url, stream=True)\n    with open(path, ""wb"") as file:\n        shutil.copyfileobj(data.raw, file)\n\n    del data\n    if extract_path is not None:\n        if path.endswith("".gz"") or path.endswith("".tgz"") :\n            extract_tar(path,extract_path)\n        else:\n            extract_zip(path, extract_path)\n\ndef extract_zip(source_path,extract_path):\n    """"""\n\n    :param source_path:\n    :param extract_path:\n    :return:\n    """"""\n    extractor = ZipFile(source_path)\n    extractor.extractall(extract_path)\n    extractor.close()\n\ndef extract_tar(source_path,extract_path):\n    """"""\n\n    :param source_path:\n    :param extract_path:\n    :return:\n    """"""\n    with tarfile.open(source_path) as tar:\n        tar.extractall(extract_path)\n\n\n"""""" Modifies the learning rate of the optimizer\n    Arguments:\n       lr: the new learning rate\n       optimizer: the optimizer whose learning rate will be adjusted\n""""""\ndef adjust_learning_rate(lr,optimizer):\n    """"""\n\n    :param lr:\n    :param optimizer:\n    :return:\n    """"""\n    for param_group in optimizer.param_groups:\n        param_group[""lr""] = lr\n\n\nPlotInput = namedtuple(""PlotInput"",[""value"",""name"",""color""])\n\n\nclass FieldInput(object):\n    def __init__(self,name,field,vocab=None,max_size=None, min_freq=1, specials=[\'<pad>\'],\n                 vectors=None, unk_init=None, vectors_cache=None):\n        """"""\n\n        :param name:\n        :param field:\n        :param vocab:\n        :param max_size:\n        :param min_freq:\n        :param specials:\n        :param vectors:\n        :param unk_init:\n        :param vectors_cache:\n\n        """"""\n\n        self.name = name\n        self.field = field\n        self.vocab = vocab\n        self.max_size = max_size\n        self.min_freq = min_freq\n        self.specials = specials\n        self.vectors = vectors\n        self.unk_init = unk_init\n        self.vectors_cache = vectors_cache\n\n\n"""""" Plots all arrays in x against y\n   Arguments:\n        y: the target array against which all other arrays will be plotted\n        x: an array of PlotInput \n        display: Shows the plot\n        save_path: saves the plot to the path specified\n        axis: specifies the plot axis\n        title: the title of the plot\n        \n    Example:\n        a = [20,30,40,50,60,70,80,90,110,120]\n        b = [2,3,4,5,6,7,8,9,11,12]\n        c = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.1,1.2]\n        \n        b = PlotInput(value=a,name=""B"",color=""r"")\n        c = PlotInput(value=c,name=""C"", color=""b"")\n        \n        visualize(a,[b,c],save_path=""graph.png"")\n\n""""""\ndef visualize(y, x,display=True,save_path=None,axis=None,title=None):\n\n    num_metrics = len(x)\n    plt.figure(1)\n    if axis is not None:\n        plt.axis(axis)\n    if title is not None:\n        plt.title(title)\n    for i,x_ in enumerate(x):\n        plt.subplot(num_metrics,1,i+1)\n        m = plt.plot(y,x_.value,x_.color)\n        plt.legend(m,[x_.name],loc=1)\n    if save_path is not None:\n        plt.savefig(save_path)\n    if display:\n        plt.show()\n    plt.close()\n\ndef get_model_summary(model,*input_tensors,item_length=26,tensorboard_log=None):\n    """"""\n\n    :param model:\n    :param input_tensors:\n    :param item_length:\n    :param tensorboard_log:\n    :return:\n    """"""\n\n    summary = []\n\n    ModuleDetails = namedtuple(""Layer"", [""name"", ""input_size"", ""output_size"",""num_parameters"",""multiply_adds""])\n    hooks = []\n    layer_instances = {}\n\n    def add_hooks(module):\n\n        def hook(module, input, output):\n\n            class_name = str(module.__class__.__name__)\n\n            instance_index = 1\n            if class_name not in layer_instances:\n                layer_instances[class_name] = instance_index\n            else:\n                instance_index = layer_instances[class_name] + 1\n                layer_instances[class_name] = instance_index\n\n            layer_name = class_name + ""_"" + str(instance_index)\n\n            params = 0\n            if hasattr(module,""weight""):\n                for param_ in module.parameters():\n                    params += param_.view(-1).size(0)\n\n            flops = ""Not Available""\n            if class_name.find(""Conv"") != -1 and hasattr(module, ""weight""):\n                flops = (\n                    torch.prod(torch.LongTensor(list(module.weight.data.size()))) * torch.prod(\n                        torch.LongTensor(list(output.size())[2:]))).item()\n\n            elif isinstance(module, nn.Linear):\n\n                flops = (torch.prod(torch.LongTensor(list(output.size()))) * input[0].size(1)).item()\n\n            summary.append(\n                ModuleDetails(name=layer_name, input_size=list(input[0].size()), output_size=list(          output.size()), num_parameters=params, multiply_adds=flops))\n\n        if not isinstance(module, nn.ModuleList) and not isinstance(module, nn.Sequential) and not isinstance(module,tofp16)  and module != model:\n            hooks.append(module.register_forward_hook(hook))\n\n    model.apply(add_hooks)\n\n    space_len = item_length\n\n    model(*input_tensors)\n    for hook in hooks:\n        hook.remove()\n\n    details = ""Model Summary"" + os.linesep + ""Name{}Input Size{}Output Size{}Parameters{}Multiply Adds (Flops){}"".format(\n        \' \' * (space_len - len(""Name"")), \' \' * (space_len - len(""Input Size"")),\n        \' \' * (space_len - len(""Output Size"")), \' \' * (space_len - len(""Parameters"")),\n        \' \' * (space_len - len(""Multiply Adds (Flops)""))) + os.linesep + \'-\' * space_len * 5 + os.linesep\n    params_sum = 0\n    flops_sum = 0\n    for layer in summary:\n        params_sum += layer.num_parameters\n        if layer.multiply_adds != ""Not Available"":\n            flops_sum += layer.multiply_adds\n        details += ""{}{}{}{}{}{}{}{}{}{}"".format(layer.name, \' \' * (space_len - len(layer.name)), layer.input_size,\n                                                 \' \' * (space_len - len(str(layer.input_size))), layer.output_size,\n                                                 \' \' * (space_len - len(str(layer.output_size))),\n                                                 layer.num_parameters,\n                                                 \' \' * (space_len - len(str(layer.num_parameters))),\n                                                 layer.multiply_adds,\n                                                 \' \' * (space_len - len(str(layer.multiply_adds)))) + os.linesep + \'-\' * space_len * 5 + os.linesep\n\n\n    details += os.linesep + ""Total Parameters: {}"".format(params_sum) + os.linesep + \'-\' * space_len * 5 + os.linesep\n    details += ""Total Multiply Adds (For Convolution and Linear Layers only): {}"".format(flops_sum) + os.linesep + \'-\' * space_len * 5 + os.linesep\n    details += ""Number of Layers"" + os.linesep\n    for layer in layer_instances:\n        details += ""{} : {} layers   "".format(layer, layer_instances[layer])\n\n    if tensorboard_log:\n        from tensorboardX import SummaryWriter\n\n        writer = SummaryWriter(tensorboard_log)\n        writer.add_graph(model,*input_tensors)\n\n    return details\n\n\nCLASS_INDEX = {""0"": [""n01440764"", ""tench""], ""1"": [""n01443537"", ""goldfish""], ""2"": [""n01484850"", ""great_white_shark""], ""3"": [""n01491361"", ""tiger_shark""], ""4"": [""n01494475"", ""hammerhead""], ""5"": [""n01496331"", ""electric_ray""], ""6"": [""n01498041"", ""stingray""], ""7"": [""n01514668"", ""cock""], ""8"": [""n01514859"", ""hen""], ""9"": [""n01518878"", ""ostrich""], ""10"": [""n01530575"", ""brambling""], ""11"": [""n01531178"", ""goldfinch""], ""12"": [""n01532829"", ""house_finch""], ""13"": [""n01534433"", ""junco""], ""14"": [""n01537544"", ""indigo_bunting""], ""15"": [""n01558993"", ""robin""], ""16"": [""n01560419"", ""bulbul""], ""17"": [""n01580077"", ""jay""], ""18"": [""n01582220"", ""magpie""], ""19"": [""n01592084"", ""chickadee""], ""20"": [""n01601694"", ""water_ouzel""], ""21"": [""n01608432"", ""kite""], ""22"": [""n01614925"", ""bald_eagle""], ""23"": [""n01616318"", ""vulture""], ""24"": [""n01622779"", ""great_grey_owl""], ""25"": [""n01629819"", ""European_fire_salamander""], ""26"": [""n01630670"", ""common_newt""], ""27"": [""n01631663"", ""eft""], ""28"": [""n01632458"", ""spotted_salamander""], ""29"": [""n01632777"", ""axolotl""], ""30"": [""n01641577"", ""bullfrog""], ""31"": [""n01644373"", ""tree_frog""], ""32"": [""n01644900"", ""tailed_frog""], ""33"": [""n01664065"", ""loggerhead""], ""34"": [""n01665541"", ""leatherback_turtle""], ""35"": [""n01667114"", ""mud_turtle""], ""36"": [""n01667778"", ""terrapin""], ""37"": [""n01669191"", ""box_turtle""], ""38"": [""n01675722"", ""banded_gecko""], ""39"": [""n01677366"", ""common_iguana""], ""40"": [""n01682714"", ""American_chameleon""], ""41"": [""n01685808"", ""whiptail""], ""42"": [""n01687978"", ""agama""], ""43"": [""n01688243"", ""frilled_lizard""], ""44"": [""n01689811"", ""alligator_lizard""], ""45"": [""n01692333"", ""Gila_monster""], ""46"": [""n01693334"", ""green_lizard""], ""47"": [""n01694178"", ""African_chameleon""], ""48"": [""n01695060"", ""Komodo_dragon""], ""49"": [""n01697457"", ""African_crocodile""], ""50"": [""n01698640"", ""American_alligator""], ""51"": [""n01704323"", ""triceratops""], ""52"": [""n01728572"", ""thunder_snake""], ""53"": [""n01728920"", ""ringneck_snake""], ""54"": [""n01729322"", ""hognose_snake""], ""55"": [""n01729977"", ""green_snake""], ""56"": [""n01734418"", ""king_snake""], ""57"": [""n01735189"", ""garter_snake""], ""58"": [""n01737021"", ""water_snake""], ""59"": [""n01739381"", ""vine_snake""], ""60"": [""n01740131"", ""night_snake""], ""61"": [""n01742172"", ""boa_constrictor""], ""62"": [""n01744401"", ""rock_python""], ""63"": [""n01748264"", ""Indian_cobra""], ""64"": [""n01749939"", ""green_mamba""], ""65"": [""n01751748"", ""sea_snake""], ""66"": [""n01753488"", ""horned_viper""], ""67"": [""n01755581"", ""diamondback""], ""68"": [""n01756291"", ""sidewinder""], ""69"": [""n01768244"", ""trilobite""], ""70"": [""n01770081"", ""harvestman""], ""71"": [""n01770393"", ""scorpion""], ""72"": [""n01773157"", ""black_and_gold_garden_spider""], ""73"": [""n01773549"", ""barn_spider""], ""74"": [""n01773797"", ""garden_spider""], ""75"": [""n01774384"", ""black_widow""], ""76"": [""n01774750"", ""tarantula""], ""77"": [""n01775062"", ""wolf_spider""], ""78"": [""n01776313"", ""tick""], ""79"": [""n01784675"", ""centipede""], ""80"": [""n01795545"", ""black_grouse""], ""81"": [""n01796340"", ""ptarmigan""], ""82"": [""n01797886"", ""ruffed_grouse""], ""83"": [""n01798484"", ""prairie_chicken""], ""84"": [""n01806143"", ""peacock""], ""85"": [""n01806567"", ""quail""], ""86"": [""n01807496"", ""partridge""], ""87"": [""n01817953"", ""African_grey""], ""88"": [""n01818515"", ""macaw""], ""89"": [""n01819313"", ""sulphur-crested_cockatoo""], ""90"": [""n01820546"", ""lorikeet""], ""91"": [""n01824575"", ""coucal""], ""92"": [""n01828970"", ""bee_eater""], ""93"": [""n01829413"", ""hornbill""], ""94"": [""n01833805"", ""hummingbird""], ""95"": [""n01843065"", ""jacamar""], ""96"": [""n01843383"", ""toucan""], ""97"": [""n01847000"", ""drake""], ""98"": [""n01855032"", ""red-breasted_merganser""], ""99"": [""n01855672"", ""goose""], ""100"": [""n01860187"", ""black_swan""], ""101"": [""n01871265"", ""tusker""], ""102"": [""n01872401"", ""echidna""], ""103"": [""n01873310"", ""platypus""], ""104"": [""n01877812"", ""wallaby""], ""105"": [""n01882714"", ""koala""], ""106"": [""n01883070"", ""wombat""], ""107"": [""n01910747"", ""jellyfish""], ""108"": [""n01914609"", ""sea_anemone""], ""109"": [""n01917289"", ""brain_coral""], ""110"": [""n01924916"", ""flatworm""], ""111"": [""n01930112"", ""nematode""], ""112"": [""n01943899"", ""conch""], ""113"": [""n01944390"", ""snail""], ""114"": [""n01945685"", ""slug""], ""115"": [""n01950731"", ""sea_slug""], ""116"": [""n01955084"", ""chiton""], ""117"": [""n01968897"", ""chambered_nautilus""], ""118"": [""n01978287"", ""Dungeness_crab""], ""119"": [""n01978455"", ""rock_crab""], ""120"": [""n01980166"", ""fiddler_crab""], ""121"": [""n01981276"", ""king_crab""], ""122"": [""n01983481"", ""American_lobster""], ""123"": [""n01984695"", ""spiny_lobster""], ""124"": [""n01985128"", ""crayfish""], ""125"": [""n01986214"", ""hermit_crab""], ""126"": [""n01990800"", ""isopod""], ""127"": [""n02002556"", ""white_stork""], ""128"": [""n02002724"", ""black_stork""], ""129"": [""n02006656"", ""spoonbill""], ""130"": [""n02007558"", ""flamingo""], ""131"": [""n02009229"", ""little_blue_heron""], ""132"": [""n02009912"", ""American_egret""], ""133"": [""n02011460"", ""bittern""], ""134"": [""n02012849"", ""crane""], ""135"": [""n02013706"", ""limpkin""], ""136"": [""n02017213"", ""European_gallinule""], ""137"": [""n02018207"", ""American_coot""], ""138"": [""n02018795"", ""bustard""], ""139"": [""n02025239"", ""ruddy_turnstone""], ""140"": [""n02027492"", ""red-backed_sandpiper""], ""141"": [""n02028035"", ""redshank""], ""142"": [""n02033041"", ""dowitcher""], ""143"": [""n02037110"", ""oystercatcher""], ""144"": [""n02051845"", ""pelican""], ""145"": [""n02056570"", ""king_penguin""], ""146"": [""n02058221"", ""albatross""], ""147"": [""n02066245"", ""grey_whale""], ""148"": [""n02071294"", ""killer_whale""], ""149"": [""n02074367"", ""dugong""], ""150"": [""n02077923"", ""sea_lion""], ""151"": [""n02085620"", ""Chihuahua""], ""152"": [""n02085782"", ""Japanese_spaniel""], ""153"": [""n02085936"", ""Maltese_dog""], ""154"": [""n02086079"", ""Pekinese""], ""155"": [""n02086240"", ""Shih-Tzu""], ""156"": [""n02086646"", ""Blenheim_spaniel""], ""157"": [""n02086910"", ""papillon""], ""158"": [""n02087046"", ""toy_terrier""], ""159"": [""n02087394"", ""Rhodesian_ridgeback""], ""160"": [""n02088094"", ""Afghan_hound""], ""161"": [""n02088238"", ""basset""], ""162"": [""n02088364"", ""beagle""], ""163"": [""n02088466"", ""bloodhound""], ""164"": [""n02088632"", ""bluetick""], ""165"": [""n02089078"", ""black-and-tan_coonhound""], ""166"": [""n02089867"", ""Walker_hound""], ""167"": [""n02089973"", ""English_foxhound""], ""168"": [""n02090379"", ""redbone""], ""169"": [""n02090622"", ""borzoi""], ""170"": [""n02090721"", ""Irish_wolfhound""], ""171"": [""n02091032"", ""Italian_greyhound""], ""172"": [""n02091134"", ""whippet""], ""173"": [""n02091244"", ""Ibizan_hound""], ""174"": [""n02091467"", ""Norwegian_elkhound""], ""175"": [""n02091635"", ""otterhound""], ""176"": [""n02091831"", ""Saluki""], ""177"": [""n02092002"", ""Scottish_deerhound""], ""178"": [""n02092339"", ""Weimaraner""], ""179"": [""n02093256"", ""Staffordshire_bullterrier""], ""180"": [""n02093428"", ""American_Staffordshire_terrier""], ""181"": [""n02093647"", ""Bedlington_terrier""], ""182"": [""n02093754"", ""Border_terrier""], ""183"": [""n02093859"", ""Kerry_blue_terrier""], ""184"": [""n02093991"", ""Irish_terrier""], ""185"": [""n02094114"", ""Norfolk_terrier""], ""186"": [""n02094258"", ""Norwich_terrier""], ""187"": [""n02094433"", ""Yorkshire_terrier""], ""188"": [""n02095314"", ""wire-haired_fox_terrier""], ""189"": [""n02095570"", ""Lakeland_terrier""], ""190"": [""n02095889"", ""Sealyham_terrier""], ""191"": [""n02096051"", ""Airedale""], ""192"": [""n02096177"", ""cairn""], ""193"": [""n02096294"", ""Australian_terrier""], ""194"": [""n02096437"", ""Dandie_Dinmont""], ""195"": [""n02096585"", ""Boston_bull""], ""196"": [""n02097047"", ""miniature_schnauzer""], ""197"": [""n02097130"", ""giant_schnauzer""], ""198"": [""n02097209"", ""standard_schnauzer""], ""199"": [""n02097298"", ""Scotch_terrier""], ""200"": [""n02097474"", ""Tibetan_terrier""], ""201"": [""n02097658"", ""silky_terrier""], ""202"": [""n02098105"", ""soft-coated_wheaten_terrier""], ""203"": [""n02098286"", ""West_Highland_white_terrier""], ""204"": [""n02098413"", ""Lhasa""], ""205"": [""n02099267"", ""flat-coated_retriever""], ""206"": [""n02099429"", ""curly-coated_retriever""], ""207"": [""n02099601"", ""golden_retriever""], ""208"": [""n02099712"", ""Labrador_retriever""], ""209"": [""n02099849"", ""Chesapeake_Bay_retriever""], ""210"": [""n02100236"", ""German_short-haired_pointer""], ""211"": [""n02100583"", ""vizsla""], ""212"": [""n02100735"", ""English_setter""], ""213"": [""n02100877"", ""Irish_setter""], ""214"": [""n02101006"", ""Gordon_setter""], ""215"": [""n02101388"", ""Brittany_spaniel""], ""216"": [""n02101556"", ""clumber""], ""217"": [""n02102040"", ""English_springer""], ""218"": [""n02102177"", ""Welsh_springer_spaniel""], ""219"": [""n02102318"", ""cocker_spaniel""], ""220"": [""n02102480"", ""Sussex_spaniel""], ""221"": [""n02102973"", ""Irish_water_spaniel""], ""222"": [""n02104029"", ""kuvasz""], ""223"": [""n02104365"", ""schipperke""], ""224"": [""n02105056"", ""groenendael""], ""225"": [""n02105162"", ""malinois""], ""226"": [""n02105251"", ""briard""], ""227"": [""n02105412"", ""kelpie""], ""228"": [""n02105505"", ""komondor""], ""229"": [""n02105641"", ""Old_English_sheepdog""], ""230"": [""n02105855"", ""Shetland_sheepdog""], ""231"": [""n02106030"", ""collie""], ""232"": [""n02106166"", ""Border_collie""], ""233"": [""n02106382"", ""Bouvier_des_Flandres""], ""234"": [""n02106550"", ""Rottweiler""], ""235"": [""n02106662"", ""German_shepherd""], ""236"": [""n02107142"", ""Doberman""], ""237"": [""n02107312"", ""miniature_pinscher""], ""238"": [""n02107574"", ""Greater_Swiss_Mountain_dog""], ""239"": [""n02107683"", ""Bernese_mountain_dog""], ""240"": [""n02107908"", ""Appenzeller""], ""241"": [""n02108000"", ""EntleBucher""], ""242"": [""n02108089"", ""boxer""], ""243"": [""n02108422"", ""bull_mastiff""], ""244"": [""n02108551"", ""Tibetan_mastiff""], ""245"": [""n02108915"", ""French_bulldog""], ""246"": [""n02109047"", ""Great_Dane""], ""247"": [""n02109525"", ""Saint_Bernard""], ""248"": [""n02109961"", ""Eskimo_dog""], ""249"": [""n02110063"", ""malamute""], ""250"": [""n02110185"", ""Siberian_husky""], ""251"": [""n02110341"", ""dalmatian""], ""252"": [""n02110627"", ""affenpinscher""], ""253"": [""n02110806"", ""basenji""], ""254"": [""n02110958"", ""pug""], ""255"": [""n02111129"", ""Leonberg""], ""256"": [""n02111277"", ""Newfoundland""], ""257"": [""n02111500"", ""Great_Pyrenees""], ""258"": [""n02111889"", ""Samoyed""], ""259"": [""n02112018"", ""Pomeranian""], ""260"": [""n02112137"", ""chow""], ""261"": [""n02112350"", ""keeshond""], ""262"": [""n02112706"", ""Brabancon_griffon""], ""263"": [""n02113023"", ""Pembroke""], ""264"": [""n02113186"", ""Cardigan""], ""265"": [""n02113624"", ""toy_poodle""], ""266"": [""n02113712"", ""miniature_poodle""], ""267"": [""n02113799"", ""standard_poodle""], ""268"": [""n02113978"", ""Mexican_hairless""], ""269"": [""n02114367"", ""timber_wolf""], ""270"": [""n02114548"", ""white_wolf""], ""271"": [""n02114712"", ""red_wolf""], ""272"": [""n02114855"", ""coyote""], ""273"": [""n02115641"", ""dingo""], ""274"": [""n02115913"", ""dhole""], ""275"": [""n02116738"", ""African_hunting_dog""], ""276"": [""n02117135"", ""hyena""], ""277"": [""n02119022"", ""red_fox""], ""278"": [""n02119789"", ""kit_fox""], ""279"": [""n02120079"", ""Arctic_fox""], ""280"": [""n02120505"", ""grey_fox""], ""281"": [""n02123045"", ""tabby""], ""282"": [""n02123159"", ""tiger_cat""], ""283"": [""n02123394"", ""Persian_cat""], ""284"": [""n02123597"", ""Siamese_cat""], ""285"": [""n02124075"", ""Egyptian_cat""], ""286"": [""n02125311"", ""cougar""], ""287"": [""n02127052"", ""lynx""], ""288"": [""n02128385"", ""leopard""], ""289"": [""n02128757"", ""snow_leopard""], ""290"": [""n02128925"", ""jaguar""], ""291"": [""n02129165"", ""lion""], ""292"": [""n02129604"", ""tiger""], ""293"": [""n02130308"", ""cheetah""], ""294"": [""n02132136"", ""brown_bear""], ""295"": [""n02133161"", ""American_black_bear""], ""296"": [""n02134084"", ""ice_bear""], ""297"": [""n02134418"", ""sloth_bear""], ""298"": [""n02137549"", ""mongoose""], ""299"": [""n02138441"", ""meerkat""], ""300"": [""n02165105"", ""tiger_beetle""], ""301"": [""n02165456"", ""ladybug""], ""302"": [""n02167151"", ""ground_beetle""], ""303"": [""n02168699"", ""long-horned_beetle""], ""304"": [""n02169497"", ""leaf_beetle""], ""305"": [""n02172182"", ""dung_beetle""], ""306"": [""n02174001"", ""rhinoceros_beetle""], ""307"": [""n02177972"", ""weevil""], ""308"": [""n02190166"", ""fly""], ""309"": [""n02206856"", ""bee""], ""310"": [""n02219486"", ""ant""], ""311"": [""n02226429"", ""grasshopper""], ""312"": [""n02229544"", ""cricket""], ""313"": [""n02231487"", ""walking_stick""], ""314"": [""n02233338"", ""cockroach""], ""315"": [""n02236044"", ""mantis""], ""316"": [""n02256656"", ""cicada""], ""317"": [""n02259212"", ""leafhopper""], ""318"": [""n02264363"", ""lacewing""], ""319"": [""n02268443"", ""dragonfly""], ""320"": [""n02268853"", ""damselfly""], ""321"": [""n02276258"", ""admiral""], ""322"": [""n02277742"", ""ringlet""], ""323"": [""n02279972"", ""monarch""], ""324"": [""n02280649"", ""cabbage_butterfly""], ""325"": [""n02281406"", ""sulphur_butterfly""], ""326"": [""n02281787"", ""lycaenid""], ""327"": [""n02317335"", ""starfish""], ""328"": [""n02319095"", ""sea_urchin""], ""329"": [""n02321529"", ""sea_cucumber""], ""330"": [""n02325366"", ""wood_rabbit""], ""331"": [""n02326432"", ""hare""], ""332"": [""n02328150"", ""Angora""], ""333"": [""n02342885"", ""hamster""], ""334"": [""n02346627"", ""porcupine""], ""335"": [""n02356798"", ""fox_squirrel""], ""336"": [""n02361337"", ""marmot""], ""337"": [""n02363005"", ""beaver""], ""338"": [""n02364673"", ""guinea_pig""], ""339"": [""n02389026"", ""sorrel""], ""340"": [""n02391049"", ""zebra""], ""341"": [""n02395406"", ""hog""], ""342"": [""n02396427"", ""wild_boar""], ""343"": [""n02397096"", ""warthog""], ""344"": [""n02398521"", ""hippopotamus""], ""345"": [""n02403003"", ""ox""], ""346"": [""n02408429"", ""water_buffalo""], ""347"": [""n02410509"", ""bison""], ""348"": [""n02412080"", ""ram""], ""349"": [""n02415577"", ""bighorn""], ""350"": [""n02417914"", ""ibex""], ""351"": [""n02422106"", ""hartebeest""], ""352"": [""n02422699"", ""impala""], ""353"": [""n02423022"", ""gazelle""], ""354"": [""n02437312"", ""Arabian_camel""], ""355"": [""n02437616"", ""llama""], ""356"": [""n02441942"", ""weasel""], ""357"": [""n02442845"", ""mink""], ""358"": [""n02443114"", ""polecat""], ""359"": [""n02443484"", ""black-footed_ferret""], ""360"": [""n02444819"", ""otter""], ""361"": [""n02445715"", ""skunk""], ""362"": [""n02447366"", ""badger""], ""363"": [""n02454379"", ""armadillo""], ""364"": [""n02457408"", ""three-toed_sloth""], ""365"": [""n02480495"", ""orangutan""], ""366"": [""n02480855"", ""gorilla""], ""367"": [""n02481823"", ""chimpanzee""], ""368"": [""n02483362"", ""gibbon""], ""369"": [""n02483708"", ""siamang""], ""370"": [""n02484975"", ""guenon""], ""371"": [""n02486261"", ""patas""], ""372"": [""n02486410"", ""baboon""], ""373"": [""n02487347"", ""macaque""], ""374"": [""n02488291"", ""langur""], ""375"": [""n02488702"", ""colobus""], ""376"": [""n02489166"", ""proboscis_monkey""], ""377"": [""n02490219"", ""marmoset""], ""378"": [""n02492035"", ""capuchin""], ""379"": [""n02492660"", ""howler_monkey""], ""380"": [""n02493509"", ""titi""], ""381"": [""n02493793"", ""spider_monkey""], ""382"": [""n02494079"", ""squirrel_monkey""], ""383"": [""n02497673"", ""Madagascar_cat""], ""384"": [""n02500267"", ""indri""], ""385"": [""n02504013"", ""Indian_elephant""], ""386"": [""n02504458"", ""African_elephant""], ""387"": [""n02509815"", ""lesser_panda""], ""388"": [""n02510455"", ""giant_panda""], ""389"": [""n02514041"", ""barracouta""], ""390"": [""n02526121"", ""eel""], ""391"": [""n02536864"", ""coho""], ""392"": [""n02606052"", ""rock_beauty""], ""393"": [""n02607072"", ""anemone_fish""], ""394"": [""n02640242"", ""sturgeon""], ""395"": [""n02641379"", ""gar""], ""396"": [""n02643566"", ""lionfish""], ""397"": [""n02655020"", ""puffer""], ""398"": [""n02666196"", ""abacus""], ""399"": [""n02667093"", ""abaya""], ""400"": [""n02669723"", ""academic_gown""], ""401"": [""n02672831"", ""accordion""], ""402"": [""n02676566"", ""acoustic_guitar""], ""403"": [""n02687172"", ""aircraft_carrier""], ""404"": [""n02690373"", ""airliner""], ""405"": [""n02692877"", ""airship""], ""406"": [""n02699494"", ""altar""], ""407"": [""n02701002"", ""ambulance""], ""408"": [""n02704792"", ""amphibian""], ""409"": [""n02708093"", ""analog_clock""], ""410"": [""n02727426"", ""apiary""], ""411"": [""n02730930"", ""apron""], ""412"": [""n02747177"", ""ashcan""], ""413"": [""n02749479"", ""assault_rifle""], ""414"": [""n02769748"", ""backpack""], ""415"": [""n02776631"", ""bakery""], ""416"": [""n02777292"", ""balance_beam""], ""417"": [""n02782093"", ""balloon""], ""418"": [""n02783161"", ""ballpoint""], ""419"": [""n02786058"", ""Band_Aid""], ""420"": [""n02787622"", ""banjo""], ""421"": [""n02788148"", ""bannister""], ""422"": [""n02790996"", ""barbell""], ""423"": [""n02791124"", ""barber_chair""], ""424"": [""n02791270"", ""barbershop""], ""425"": [""n02793495"", ""barn""], ""426"": [""n02794156"", ""barometer""], ""427"": [""n02795169"", ""barrel""], ""428"": [""n02797295"", ""barrow""], ""429"": [""n02799071"", ""baseball""], ""430"": [""n02802426"", ""basketball""], ""431"": [""n02804414"", ""bassinet""], ""432"": [""n02804610"", ""bassoon""], ""433"": [""n02807133"", ""bathing_cap""], ""434"": [""n02808304"", ""bath_towel""], ""435"": [""n02808440"", ""bathtub""], ""436"": [""n02814533"", ""beach_wagon""], ""437"": [""n02814860"", ""beacon""], ""438"": [""n02815834"", ""beaker""], ""439"": [""n02817516"", ""bearskin""], ""440"": [""n02823428"", ""beer_bottle""], ""441"": [""n02823750"", ""beer_glass""], ""442"": [""n02825657"", ""bell_cote""], ""443"": [""n02834397"", ""bib""], ""444"": [""n02835271"", ""bicycle-built-for-two""], ""445"": [""n02837789"", ""bikini""], ""446"": [""n02840245"", ""binder""], ""447"": [""n02841315"", ""binoculars""], ""448"": [""n02843684"", ""birdhouse""], ""449"": [""n02859443"", ""boathouse""], ""450"": [""n02860847"", ""bobsled""], ""451"": [""n02865351"", ""bolo_tie""], ""452"": [""n02869837"", ""bonnet""], ""453"": [""n02870880"", ""bookcase""], ""454"": [""n02871525"", ""bookshop""], ""455"": [""n02877765"", ""bottlecap""], ""456"": [""n02879718"", ""bow""], ""457"": [""n02883205"", ""bow_tie""], ""458"": [""n02892201"", ""brass""], ""459"": [""n02892767"", ""brassiere""], ""460"": [""n02894605"", ""breakwater""], ""461"": [""n02895154"", ""breastplate""], ""462"": [""n02906734"", ""broom""], ""463"": [""n02909870"", ""bucket""], ""464"": [""n02910353"", ""buckle""], ""465"": [""n02916936"", ""bulletproof_vest""], ""466"": [""n02917067"", ""bullet_train""], ""467"": [""n02927161"", ""butcher_shop""], ""468"": [""n02930766"", ""cab""], ""469"": [""n02939185"", ""caldron""], ""470"": [""n02948072"", ""candle""], ""471"": [""n02950826"", ""cannon""], ""472"": [""n02951358"", ""canoe""], ""473"": [""n02951585"", ""can_opener""], ""474"": [""n02963159"", ""cardigan""], ""475"": [""n02965783"", ""car_mirror""], ""476"": [""n02966193"", ""carousel""], ""477"": [""n02966687"", ""carpenter\'s_kit""], ""478"": [""n02971356"", ""carton""], ""479"": [""n02974003"", ""car_wheel""], ""480"": [""n02977058"", ""cash_machine""], ""481"": [""n02978881"", ""cassette""], ""482"": [""n02979186"", ""cassette_player""], ""483"": [""n02980441"", ""castle""], ""484"": [""n02981792"", ""catamaran""], ""485"": [""n02988304"", ""CD_player""], ""486"": [""n02992211"", ""cello""], ""487"": [""n02992529"", ""cellular_telephone""], ""488"": [""n02999410"", ""chain""], ""489"": [""n03000134"", ""chainlink_fence""], ""490"": [""n03000247"", ""chain_mail""], ""491"": [""n03000684"", ""chain_saw""], ""492"": [""n03014705"", ""chest""], ""493"": [""n03016953"", ""chiffonier""], ""494"": [""n03017168"", ""chime""], ""495"": [""n03018349"", ""china_cabinet""], ""496"": [""n03026506"", ""Christmas_stocking""], ""497"": [""n03028079"", ""church""], ""498"": [""n03032252"", ""cinema""], ""499"": [""n03041632"", ""cleaver""], ""500"": [""n03042490"", ""cliff_dwelling""], ""501"": [""n03045698"", ""cloak""], ""502"": [""n03047690"", ""clog""], ""503"": [""n03062245"", ""cocktail_shaker""], ""504"": [""n03063599"", ""coffee_mug""], ""505"": [""n03063689"", ""coffeepot""], ""506"": [""n03065424"", ""coil""], ""507"": [""n03075370"", ""combination_lock""], ""508"": [""n03085013"", ""computer_keyboard""], ""509"": [""n03089624"", ""confectionery""], ""510"": [""n03095699"", ""container_ship""], ""511"": [""n03100240"", ""convertible""], ""512"": [""n03109150"", ""corkscrew""], ""513"": [""n03110669"", ""cornet""], ""514"": [""n03124043"", ""cowboy_boot""], ""515"": [""n03124170"", ""cowboy_hat""], ""516"": [""n03125729"", ""cradle""], ""517"": [""n03126707"", ""crane""], ""518"": [""n03127747"", ""crash_helmet""], ""519"": [""n03127925"", ""crate""], ""520"": [""n03131574"", ""crib""], ""521"": [""n03133878"", ""Crock_Pot""], ""522"": [""n03134739"", ""croquet_ball""], ""523"": [""n03141823"", ""crutch""], ""524"": [""n03146219"", ""cuirass""], ""525"": [""n03160309"", ""dam""], ""526"": [""n03179701"", ""desk""], ""527"": [""n03180011"", ""desktop_computer""], ""528"": [""n03187595"", ""dial_telephone""], ""529"": [""n03188531"", ""diaper""], ""530"": [""n03196217"", ""digital_clock""], ""531"": [""n03197337"", ""digital_watch""], ""532"": [""n03201208"", ""dining_table""], ""533"": [""n03207743"", ""dishrag""], ""534"": [""n03207941"", ""dishwasher""], ""535"": [""n03208938"", ""disk_brake""], ""536"": [""n03216828"", ""dock""], ""537"": [""n03218198"", ""dogsled""], ""538"": [""n03220513"", ""dome""], ""539"": [""n03223299"", ""doormat""], ""540"": [""n03240683"", ""drilling_platform""], ""541"": [""n03249569"", ""drum""], ""542"": [""n03250847"", ""drumstick""], ""543"": [""n03255030"", ""dumbbell""], ""544"": [""n03259280"", ""Dutch_oven""], ""545"": [""n03271574"", ""electric_fan""], ""546"": [""n03272010"", ""electric_guitar""], ""547"": [""n03272562"", ""electric_locomotive""], ""548"": [""n03290653"", ""entertainment_center""], ""549"": [""n03291819"", ""envelope""], ""550"": [""n03297495"", ""espresso_maker""], ""551"": [""n03314780"", ""face_powder""], ""552"": [""n03325584"", ""feather_boa""], ""553"": [""n03337140"", ""file""], ""554"": [""n03344393"", ""fireboat""], ""555"": [""n03345487"", ""fire_engine""], ""556"": [""n03347037"", ""fire_screen""], ""557"": [""n03355925"", ""flagpole""], ""558"": [""n03372029"", ""flute""], ""559"": [""n03376595"", ""folding_chair""], ""560"": [""n03379051"", ""football_helmet""], ""561"": [""n03384352"", ""forklift""], ""562"": [""n03388043"", ""fountain""], ""563"": [""n03388183"", ""fountain_pen""], ""564"": [""n03388549"", ""four-poster""], ""565"": [""n03393912"", ""freight_car""], ""566"": [""n03394916"", ""French_horn""], ""567"": [""n03400231"", ""frying_pan""], ""568"": [""n03404251"", ""fur_coat""], ""569"": [""n03417042"", ""garbage_truck""], ""570"": [""n03424325"", ""gasmask""], ""571"": [""n03425413"", ""gas_pump""], ""572"": [""n03443371"", ""goblet""], ""573"": [""n03444034"", ""go-kart""], ""574"": [""n03445777"", ""golf_ball""], ""575"": [""n03445924"", ""golfcart""], ""576"": [""n03447447"", ""gondola""], ""577"": [""n03447721"", ""gong""], ""578"": [""n03450230"", ""gown""], ""579"": [""n03452741"", ""grand_piano""], ""580"": [""n03457902"", ""greenhouse""], ""581"": [""n03459775"", ""grille""], ""582"": [""n03461385"", ""grocery_store""], ""583"": [""n03467068"", ""guillotine""], ""584"": [""n03476684"", ""hair_slide""], ""585"": [""n03476991"", ""hair_spray""], ""586"": [""n03478589"", ""half_track""], ""587"": [""n03481172"", ""hammer""], ""588"": [""n03482405"", ""hamper""], ""589"": [""n03483316"", ""hand_blower""], ""590"": [""n03485407"", ""hand-held_computer""], ""591"": [""n03485794"", ""handkerchief""], ""592"": [""n03492542"", ""hard_disc""], ""593"": [""n03494278"", ""harmonica""], ""594"": [""n03495258"", ""harp""], ""595"": [""n03496892"", ""harvester""], ""596"": [""n03498962"", ""hatchet""], ""597"": [""n03527444"", ""holster""], ""598"": [""n03529860"", ""home_theater""], ""599"": [""n03530642"", ""honeycomb""], ""600"": [""n03532672"", ""hook""], ""601"": [""n03534580"", ""hoopskirt""], ""602"": [""n03535780"", ""horizontal_bar""], ""603"": [""n03538406"", ""horse_cart""], ""604"": [""n03544143"", ""hourglass""], ""605"": [""n03584254"", ""iPod""], ""606"": [""n03584829"", ""iron""], ""607"": [""n03590841"", ""jack-o\'-lantern""], ""608"": [""n03594734"", ""jean""], ""609"": [""n03594945"", ""jeep""], ""610"": [""n03595614"", ""jersey""], ""611"": [""n03598930"", ""jigsaw_puzzle""], ""612"": [""n03599486"", ""jinrikisha""], ""613"": [""n03602883"", ""joystick""], ""614"": [""n03617480"", ""kimono""], ""615"": [""n03623198"", ""knee_pad""], ""616"": [""n03627232"", ""knot""], ""617"": [""n03630383"", ""lab_coat""], ""618"": [""n03633091"", ""ladle""], ""619"": [""n03637318"", ""lampshade""], ""620"": [""n03642806"", ""laptop""], ""621"": [""n03649909"", ""lawn_mower""], ""622"": [""n03657121"", ""lens_cap""], ""623"": [""n03658185"", ""letter_opener""], ""624"": [""n03661043"", ""library""], ""625"": [""n03662601"", ""lifeboat""], ""626"": [""n03666591"", ""lighter""], ""627"": [""n03670208"", ""limousine""], ""628"": [""n03673027"", ""liner""], ""629"": [""n03676483"", ""lipstick""], ""630"": [""n03680355"", ""Loafer""], ""631"": [""n03690938"", ""lotion""], ""632"": [""n03691459"", ""loudspeaker""], ""633"": [""n03692522"", ""loupe""], ""634"": [""n03697007"", ""lumbermill""], ""635"": [""n03706229"", ""magnetic_compass""], ""636"": [""n03709823"", ""mailbag""], ""637"": [""n03710193"", ""mailbox""], ""638"": [""n03710637"", ""maillot""], ""639"": [""n03710721"", ""maillot""], ""640"": [""n03717622"", ""manhole_cover""], ""641"": [""n03720891"", ""maraca""], ""642"": [""n03721384"", ""marimba""], ""643"": [""n03724870"", ""mask""], ""644"": [""n03729826"", ""matchstick""], ""645"": [""n03733131"", ""maypole""], ""646"": [""n03733281"", ""maze""], ""647"": [""n03733805"", ""measuring_cup""], ""648"": [""n03742115"", ""medicine_chest""], ""649"": [""n03743016"", ""megalith""], ""650"": [""n03759954"", ""microphone""], ""651"": [""n03761084"", ""microwave""], ""652"": [""n03763968"", ""military_uniform""], ""653"": [""n03764736"", ""milk_can""], ""654"": [""n03769881"", ""minibus""], ""655"": [""n03770439"", ""miniskirt""], ""656"": [""n03770679"", ""minivan""], ""657"": [""n03773504"", ""missile""], ""658"": [""n03775071"", ""mitten""], ""659"": [""n03775546"", ""mixing_bowl""], ""660"": [""n03776460"", ""mobile_home""], ""661"": [""n03777568"", ""Model_T""], ""662"": [""n03777754"", ""modem""], ""663"": [""n03781244"", ""monastery""], ""664"": [""n03782006"", ""monitor""], ""665"": [""n03785016"", ""moped""], ""666"": [""n03786901"", ""mortar""], ""667"": [""n03787032"", ""mortarboard""], ""668"": [""n03788195"", ""mosque""], ""669"": [""n03788365"", ""mosquito_net""], ""670"": [""n03791053"", ""motor_scooter""], ""671"": [""n03792782"", ""mountain_bike""], ""672"": [""n03792972"", ""mountain_tent""], ""673"": [""n03793489"", ""mouse""], ""674"": [""n03794056"", ""mousetrap""], ""675"": [""n03796401"", ""moving_van""], ""676"": [""n03803284"", ""muzzle""], ""677"": [""n03804744"", ""nail""], ""678"": [""n03814639"", ""neck_brace""], ""679"": [""n03814906"", ""necklace""], ""680"": [""n03825788"", ""nipple""], ""681"": [""n03832673"", ""notebook""], ""682"": [""n03837869"", ""obelisk""], ""683"": [""n03838899"", ""oboe""], ""684"": [""n03840681"", ""ocarina""], ""685"": [""n03841143"", ""odometer""], ""686"": [""n03843555"", ""oil_filter""], ""687"": [""n03854065"", ""organ""], ""688"": [""n03857828"", ""oscilloscope""], ""689"": [""n03866082"", ""overskirt""], ""690"": [""n03868242"", ""oxcart""], ""691"": [""n03868863"", ""oxygen_mask""], ""692"": [""n03871628"", ""packet""], ""693"": [""n03873416"", ""paddle""], ""694"": [""n03874293"", ""paddlewheel""], ""695"": [""n03874599"", ""padlock""], ""696"": [""n03876231"", ""paintbrush""], ""697"": [""n03877472"", ""pajama""], ""698"": [""n03877845"", ""palace""], ""699"": [""n03884397"", ""panpipe""], ""700"": [""n03887697"", ""paper_towel""], ""701"": [""n03888257"", ""parachute""], ""702"": [""n03888605"", ""parallel_bars""], ""703"": [""n03891251"", ""park_bench""], ""704"": [""n03891332"", ""parking_meter""], ""705"": [""n03895866"", ""passenger_car""], ""706"": [""n03899768"", ""patio""], ""707"": [""n03902125"", ""pay-phone""], ""708"": [""n03903868"", ""pedestal""], ""709"": [""n03908618"", ""pencil_box""], ""710"": [""n03908714"", ""pencil_sharpener""], ""711"": [""n03916031"", ""perfume""], ""712"": [""n03920288"", ""Petri_dish""], ""713"": [""n03924679"", ""photocopier""], ""714"": [""n03929660"", ""pick""], ""715"": [""n03929855"", ""pickelhaube""], ""716"": [""n03930313"", ""picket_fence""], ""717"": [""n03930630"", ""pickup""], ""718"": [""n03933933"", ""pier""], ""719"": [""n03935335"", ""piggy_bank""], ""720"": [""n03937543"", ""pill_bottle""], ""721"": [""n03938244"", ""pillow""], ""722"": [""n03942813"", ""ping-pong_ball""], ""723"": [""n03944341"", ""pinwheel""], ""724"": [""n03947888"", ""pirate""], ""725"": [""n03950228"", ""pitcher""], ""726"": [""n03954731"", ""plane""], ""727"": [""n03956157"", ""planetarium""], ""728"": [""n03958227"", ""plastic_bag""], ""729"": [""n03961711"", ""plate_rack""], ""730"": [""n03967562"", ""plow""], ""731"": [""n03970156"", ""plunger""], ""732"": [""n03976467"", ""Polaroid_camera""], ""733"": [""n03976657"", ""pole""], ""734"": [""n03977966"", ""police_van""], ""735"": [""n03980874"", ""poncho""], ""736"": [""n03982430"", ""pool_table""], ""737"": [""n03983396"", ""pop_bottle""], ""738"": [""n03991062"", ""pot""], ""739"": [""n03992509"", ""potter\'s_wheel""], ""740"": [""n03995372"", ""power_drill""], ""741"": [""n03998194"", ""prayer_rug""], ""742"": [""n04004767"", ""printer""], ""743"": [""n04005630"", ""prison""], ""744"": [""n04008634"", ""projectile""], ""745"": [""n04009552"", ""projector""], ""746"": [""n04019541"", ""puck""], ""747"": [""n04023962"", ""punching_bag""], ""748"": [""n04026417"", ""purse""], ""749"": [""n04033901"", ""quill""], ""750"": [""n04033995"", ""quilt""], ""751"": [""n04037443"", ""racer""], ""752"": [""n04039381"", ""racket""], ""753"": [""n04040759"", ""radiator""], ""754"": [""n04041544"", ""radio""], ""755"": [""n04044716"", ""radio_telescope""], ""756"": [""n04049303"", ""rain_barrel""], ""757"": [""n04065272"", ""recreational_vehicle""], ""758"": [""n04067472"", ""reel""], ""759"": [""n04069434"", ""reflex_camera""], ""760"": [""n04070727"", ""refrigerator""], ""761"": [""n04074963"", ""remote_control""], ""762"": [""n04081281"", ""restaurant""], ""763"": [""n04086273"", ""revolver""], ""764"": [""n04090263"", ""rifle""], ""765"": [""n04099969"", ""rocking_chair""], ""766"": [""n04111531"", ""rotisserie""], ""767"": [""n04116512"", ""rubber_eraser""], ""768"": [""n04118538"", ""rugby_ball""], ""769"": [""n04118776"", ""rule""], ""770"": [""n04120489"", ""running_shoe""], ""771"": [""n04125021"", ""safe""], ""772"": [""n04127249"", ""safety_pin""], ""773"": [""n04131690"", ""saltshaker""], ""774"": [""n04133789"", ""sandal""], ""775"": [""n04136333"", ""sarong""], ""776"": [""n04141076"", ""sax""], ""777"": [""n04141327"", ""scabbard""], ""778"": [""n04141975"", ""scale""], ""779"": [""n04146614"", ""school_bus""], ""780"": [""n04147183"", ""schooner""], ""781"": [""n04149813"", ""scoreboard""], ""782"": [""n04152593"", ""screen""], ""783"": [""n04153751"", ""screw""], ""784"": [""n04154565"", ""screwdriver""], ""785"": [""n04162706"", ""seat_belt""], ""786"": [""n04179913"", ""sewing_machine""], ""787"": [""n04192698"", ""shield""], ""788"": [""n04200800"", ""shoe_shop""], ""789"": [""n04201297"", ""shoji""], ""790"": [""n04204238"", ""shopping_basket""], ""791"": [""n04204347"", ""shopping_cart""], ""792"": [""n04208210"", ""shovel""], ""793"": [""n04209133"", ""shower_cap""], ""794"": [""n04209239"", ""shower_curtain""], ""795"": [""n04228054"", ""ski""], ""796"": [""n04229816"", ""ski_mask""], ""797"": [""n04235860"", ""sleeping_bag""], ""798"": [""n04238763"", ""slide_rule""], ""799"": [""n04239074"", ""sliding_door""], ""800"": [""n04243546"", ""slot""], ""801"": [""n04251144"", ""snorkel""], ""802"": [""n04252077"", ""snowmobile""], ""803"": [""n04252225"", ""snowplow""], ""804"": [""n04254120"", ""soap_dispenser""], ""805"": [""n04254680"", ""soccer_ball""], ""806"": [""n04254777"", ""sock""], ""807"": [""n04258138"", ""solar_dish""], ""808"": [""n04259630"", ""sombrero""], ""809"": [""n04263257"", ""soup_bowl""], ""810"": [""n04264628"", ""space_bar""], ""811"": [""n04265275"", ""space_heater""], ""812"": [""n04266014"", ""space_shuttle""], ""813"": [""n04270147"", ""spatula""], ""814"": [""n04273569"", ""speedboat""], ""815"": [""n04275548"", ""spider_web""], ""816"": [""n04277352"", ""spindle""], ""817"": [""n04285008"", ""sports_car""], ""818"": [""n04286575"", ""spotlight""], ""819"": [""n04296562"", ""stage""], ""820"": [""n04310018"", ""steam_locomotive""], ""821"": [""n04311004"", ""steel_arch_bridge""], ""822"": [""n04311174"", ""steel_drum""], ""823"": [""n04317175"", ""stethoscope""], ""824"": [""n04325704"", ""stole""], ""825"": [""n04326547"", ""stone_wall""], ""826"": [""n04328186"", ""stopwatch""], ""827"": [""n04330267"", ""stove""], ""828"": [""n04332243"", ""strainer""], ""829"": [""n04335435"", ""streetcar""], ""830"": [""n04336792"", ""stretcher""], ""831"": [""n04344873"", ""studio_couch""], ""832"": [""n04346328"", ""stupa""], ""833"": [""n04347754"", ""submarine""], ""834"": [""n04350905"", ""suit""], ""835"": [""n04355338"", ""sundial""], ""836"": [""n04355933"", ""sunglass""], ""837"": [""n04356056"", ""sunglasses""], ""838"": [""n04357314"", ""sunscreen""], ""839"": [""n04366367"", ""suspension_bridge""], ""840"": [""n04367480"", ""swab""], ""841"": [""n04370456"", ""sweatshirt""], ""842"": [""n04371430"", ""swimming_trunks""], ""843"": [""n04371774"", ""swing""], ""844"": [""n04372370"", ""switch""], ""845"": [""n04376876"", ""syringe""], ""846"": [""n04380533"", ""table_lamp""], ""847"": [""n04389033"", ""tank""], ""848"": [""n04392985"", ""tape_player""], ""849"": [""n04398044"", ""teapot""], ""850"": [""n04399382"", ""teddy""], ""851"": [""n04404412"", ""television""], ""852"": [""n04409515"", ""tennis_ball""], ""853"": [""n04417672"", ""thatch""], ""854"": [""n04418357"", ""theater_curtain""], ""855"": [""n04423845"", ""thimble""], ""856"": [""n04428191"", ""thresher""], ""857"": [""n04429376"", ""throne""], ""858"": [""n04435653"", ""tile_roof""], ""859"": [""n04442312"", ""toaster""], ""860"": [""n04443257"", ""tobacco_shop""], ""861"": [""n04447861"", ""toilet_seat""], ""862"": [""n04456115"", ""torch""], ""863"": [""n04458633"", ""totem_pole""], ""864"": [""n04461696"", ""tow_truck""], ""865"": [""n04462240"", ""toyshop""], ""866"": [""n04465501"", ""tractor""], ""867"": [""n04467665"", ""trailer_truck""], ""868"": [""n04476259"", ""tray""], ""869"": [""n04479046"", ""trench_coat""], ""870"": [""n04482393"", ""tricycle""], ""871"": [""n04483307"", ""trimaran""], ""872"": [""n04485082"", ""tripod""], ""873"": [""n04486054"", ""triumphal_arch""], ""874"": [""n04487081"", ""trolleybus""], ""875"": [""n04487394"", ""trombone""], ""876"": [""n04493381"", ""tub""], ""877"": [""n04501370"", ""turnstile""], ""878"": [""n04505470"", ""typewriter_keyboard""], ""879"": [""n04507155"", ""umbrella""], ""880"": [""n04509417"", ""unicycle""], ""881"": [""n04515003"", ""upright""], ""882"": [""n04517823"", ""vacuum""], ""883"": [""n04522168"", ""vase""], ""884"": [""n04523525"", ""vault""], ""885"": [""n04525038"", ""velvet""], ""886"": [""n04525305"", ""vending_machine""], ""887"": [""n04532106"", ""vestment""], ""888"": [""n04532670"", ""viaduct""], ""889"": [""n04536866"", ""violin""], ""890"": [""n04540053"", ""volleyball""], ""891"": [""n04542943"", ""waffle_iron""], ""892"": [""n04548280"", ""wall_clock""], ""893"": [""n04548362"", ""wallet""], ""894"": [""n04550184"", ""wardrobe""], ""895"": [""n04552348"", ""warplane""], ""896"": [""n04553703"", ""washbasin""], ""897"": [""n04554684"", ""washer""], ""898"": [""n04557648"", ""water_bottle""], ""899"": [""n04560804"", ""water_jug""], ""900"": [""n04562935"", ""water_tower""], ""901"": [""n04579145"", ""whiskey_jug""], ""902"": [""n04579432"", ""whistle""], ""903"": [""n04584207"", ""wig""], ""904"": [""n04589890"", ""window_screen""], ""905"": [""n04590129"", ""window_shade""], ""906"": [""n04591157"", ""Windsor_tie""], ""907"": [""n04591713"", ""wine_bottle""], ""908"": [""n04592741"", ""wing""], ""909"": [""n04596742"", ""wok""], ""910"": [""n04597913"", ""wooden_spoon""], ""911"": [""n04599235"", ""wool""], ""912"": [""n04604644"", ""worm_fence""], ""913"": [""n04606251"", ""wreck""], ""914"": [""n04612504"", ""yawl""], ""915"": [""n04613696"", ""yurt""], ""916"": [""n06359193"", ""web_site""], ""917"": [""n06596364"", ""comic_book""], ""918"": [""n06785654"", ""crossword_puzzle""], ""919"": [""n06794110"", ""street_sign""], ""920"": [""n06874185"", ""traffic_light""], ""921"": [""n07248320"", ""book_jacket""], ""922"": [""n07565083"", ""menu""], ""923"": [""n07579787"", ""plate""], ""924"": [""n07583066"", ""guacamole""], ""925"": [""n07584110"", ""consomme""], ""926"": [""n07590611"", ""hot_pot""], ""927"": [""n07613480"", ""trifle""], ""928"": [""n07614500"", ""ice_cream""], ""929"": [""n07615774"", ""ice_lolly""], ""930"": [""n07684084"", ""French_loaf""], ""931"": [""n07693725"", ""bagel""], ""932"": [""n07695742"", ""pretzel""], ""933"": [""n07697313"", ""cheeseburger""], ""934"": [""n07697537"", ""hotdog""], ""935"": [""n07711569"", ""mashed_potato""], ""936"": [""n07714571"", ""head_cabbage""], ""937"": [""n07714990"", ""broccoli""], ""938"": [""n07715103"", ""cauliflower""], ""939"": [""n07716358"", ""zucchini""], ""940"": [""n07716906"", ""spaghetti_squash""], ""941"": [""n07717410"", ""acorn_squash""], ""942"": [""n07717556"", ""butternut_squash""], ""943"": [""n07718472"", ""cucumber""], ""944"": [""n07718747"", ""artichoke""], ""945"": [""n07720875"", ""bell_pepper""], ""946"": [""n07730033"", ""cardoon""], ""947"": [""n07734744"", ""mushroom""], ""948"": [""n07742313"", ""Granny_Smith""], ""949"": [""n07745940"", ""strawberry""], ""950"": [""n07747607"", ""orange""], ""951"": [""n07749582"", ""lemon""], ""952"": [""n07753113"", ""fig""], ""953"": [""n07753275"", ""pineapple""], ""954"": [""n07753592"", ""banana""], ""955"": [""n07754684"", ""jackfruit""], ""956"": [""n07760859"", ""custard_apple""], ""957"": [""n07768694"", ""pomegranate""], ""958"": [""n07802026"", ""hay""], ""959"": [""n07831146"", ""carbonara""], ""960"": [""n07836838"", ""chocolate_sauce""], ""961"": [""n07860988"", ""dough""], ""962"": [""n07871810"", ""meat_loaf""], ""963"": [""n07873807"", ""pizza""], ""964"": [""n07875152"", ""potpie""], ""965"": [""n07880968"", ""burrito""], ""966"": [""n07892512"", ""red_wine""], ""967"": [""n07920052"", ""espresso""], ""968"": [""n07930864"", ""cup""], ""969"": [""n07932039"", ""eggnog""], ""970"": [""n09193705"", ""alp""], ""971"": [""n09229709"", ""bubble""], ""972"": [""n09246464"", ""cliff""], ""973"": [""n09256479"", ""coral_reef""], ""974"": [""n09288635"", ""geyser""], ""975"": [""n09332890"", ""lakeside""], ""976"": [""n09399592"", ""promontory""], ""977"": [""n09421951"", ""sandbar""], ""978"": [""n09428293"", ""seashore""], ""979"": [""n09468604"", ""valley""], ""980"": [""n09472597"", ""volcano""], ""981"": [""n09835506"", ""ballplayer""], ""982"": [""n10148035"", ""groom""], ""983"": [""n10565667"", ""scuba_diver""], ""984"": [""n11879895"", ""rapeseed""], ""985"": [""n11939491"", ""daisy""], ""986"": [""n12057211"", ""yellow_lady\'s_slipper""], ""987"": [""n12144580"", ""corn""], ""988"": [""n12267677"", ""acorn""], ""989"": [""n12620546"", ""hip""], ""990"": [""n12768682"", ""buckeye""], ""991"": [""n12985857"", ""coral_fungus""], ""992"": [""n12998815"", ""agaric""], ""993"": [""n13037406"", ""gyromitra""], ""994"": [""n13040303"", ""stinkhorn""], ""995"": [""n13044778"", ""earthstar""], ""996"": [""n13052670"", ""hen-of-the-woods""], ""997"": [""n13054560"", ""bolete""], ""998"": [""n13133613"", ""ear""], ""999"": [""n15075141"", ""toilet_tissue""]}\n\ndef decode_imagenet(class_index):\n    """"""\n\n    :param class_index:\n    :return:\n    """"""\n\n    if isinstance(class_index,torch.Tensor):\n        class_index = class_index.item()\n\n    return CLASS_INDEX[str(class_index)][1]\n\n\ndef one_hot(input,num_classes):\n    """"""\n\n    :param input:\n    :param num_classes:\n    :return:\n    """"""\n    input_size = input.size()\n    input_type = input.type()\n    out_size = input_size[:len(input_size) - 1]\n    out_size = tuple(list(out_size) + [num_classes])\n\n    output = torch.zeros(out_size).type(input_type)\n    return output.scatter(len(input_size) - 1,input,1)\n\ndef get_batch_size(inputs,batch_first=True):\n    """"""\n\n    :param inputs:\n    :param batch_first:\n    :return:\n    """"""\n    if isinstance(inputs,list) or isinstance(inputs,tuple):\n        return get_batch_size(inputs[0],batch_first)\n    else:\n        return inputs.size(0 if batch_first else -1)\n\ndef clip_grads(model,lower,upper):\n    """"""\n\n    :param model:\n    :param lower:\n    :param upper:\n    :return:\n    """"""\n    for params in model.parameters():\n        params.data.clamp_(lower,upper)\n\ndef save_model(model,path,save_architecture):\n    if save_architecture:\n        torch.save(model, path)\n    else:\n        state = model.state_dict()\n        for key in state: state[key] = state[key].clone().cpu()\n        torch.save(state, path)\n\ndef load_model(model,path):\n    checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n    try:\n        model.load_state_dict(checkpoint)\n    except:\n        copy = dict()\n        for x, y in zip(model.state_dict(), checkpoint):\n            new_name = y[y.index(x):]\n            copy[new_name] = checkpoint[y]\n        model.load_state_dict(copy)\n\n\ndef load_image(file,grayscale=False,target_size=None,to_tensor=True,mean=0.5,std=0.5,interpolation = Image.BILINEAR):\n\n    """"""\n\n    :param file:\n    :param grayscale:\n    :param target_size:\n    :param to_tensor:\n    :param mean:\n    :param std:\n    :param interpolation:\n    :return:\n    """"""\n    img = Image.open(file).convert(""RGB"")\n\n    transformations = []\n\n    if grayscale:\n        transformations.append(transforms.Grayscale())\n\n    if target_size is not None:\n        target_ = target_size\n        if isinstance(target_size,int):\n            target_ = (target_size,target_size)\n        transformations.append(transforms.CenterCrop(target_))\n\n    if to_tensor:\n        transformations.append(transforms.ToTensor())\n\n    if mean is not None and std is not None:\n        if not isinstance(mean,tuple):\n            mean = (mean,)\n        if not isinstance(std,tuple):\n            std = (std,)\n        transformations.append(transforms.Normalize(mean=mean,std=std))\n\n    trans_ = transforms.Compose(transformations)\n\n    return trans_(img)\n'"
torchfusion/gan/applications/__init__.py,0,"b'from .applications import StandardGenerator,ResGenerator,DCGANDiscriminator,DCGANGenerator,WGANDiscriminator,MLPDiscriminator,MLPGenerator,WMLPDiscriminator,StandardProjectionDiscriminator,ResProjectionDiscriminator\n'"
torchfusion/gan/applications/applications.py,9,"b'from math import floor\n\nfrom torchfusion.gan.layers.layers import *\nfrom torchfusion.initializers import *\n\n"""""" The Resnet Generator with Spectral Normalization as proposed by Miyato et al. 2018 (https://arxiv.org/abs/1802.05957)\n    with optional self-attention proposed by Zhang et al. 2018 (https://arxiv.org/abs/1805.08318)\n    output_size: the size of the image to be generated\n    num_classes: the number of classes for conditional GANs\n    latent_size: the size of the noise vector\n    kernel_size: the size of the convolution kernel\n    activation: the activation function to use\n    conv_groups: number of convolution groups\n    attention: if true, attention is applied in the mid layer\n    dropout_ratio: Dropout rate for applying dropout in the residual blocks\n""""""\n\nclass ResGenerator(nn.Module):\n    def __init__(self,output_size,num_classes=0,latent_size=100,kernel_size=3,activation=nn.ReLU(),conv_groups=1,attention=False,dropout_ratio=0):\n\n\n        super(ResGenerator,self).__init__()\n\n        padding = floor(kernel_size/2)\n\n        self.num_classes = num_classes\n\n        output_channels = output_size[0]\n        self.size = output_size[1]\n\n        self.fc = Linear(latent_size,4 * 4 * self.size * 8)\n\n        current_size = 4\n\n        self.layers = nn.ModuleList()\n\n        in_channels = self.size * 8\n\n\n\n        while current_size < self.size:\n\n            self.layers.append(GeneratorResBlock(in_channels,in_channels // 2,num_classes,upsample_size=2,kernel_size=kernel_size,activation=activation,conv_groups=conv_groups if current_size == 4 else 1,dropout_ratio=dropout_ratio))\n            current_size *= 2\n\n            in_channels = in_channels//2\n\n            if current_size == self.size // 2 and attention:\n                self.layers.append(SelfAttention(in_channels))\n\n\n        self.net = nn.Sequential(\n            BatchNorm2d(in_channels, weight_init=Normal(1.0, 0.02)),\n            Conv2d(in_channels, output_channels, kernel_size=kernel_size, padding=padding, weight_init=Xavier_Uniform()),\n            nn.Tanh()\n        )\n\n    def forward(self,inputs,labels=None):\n        outputs = self.fc(inputs).view(-1,self.size * 8,4,4)\n\n        for layer in self.layers:\n            if self.num_classes > 1 and not isinstance(layer,SelfAttention):\n                outputs = layer(outputs,labels)\n            else:\n                outputs = layer(outputs)\n\n\n        return self.net(outputs)\n\n"""""" The Standard Generator with Spectral Normalization as proposed by Miyato et al. 2018 (https://arxiv.org/abs/1802.05957)\n    with optional self-attention proposed by Zhang et al. 2018 (https://arxiv.org/abs/1805.08318)\n    output_size: the size of the image to be generated\n    num_classes: the number of classes for conditional GANs\n    latent_size: the size of the noise vector\n    kernel_size: the size of the convolution kernel\n    activation: the activation function to use\n    conv_groups: number of convolution groups\n    attention: if true, attention is applied in the mid layer\n    dropout_ratio: Dropout rate for applying dropout after every Relu layer\n""""""\nclass StandardGenerator(nn.Module):\n    def __init__(self,output_size,num_classes=0,latent_size=100,activation=nn.LeakyReLU(0.2),conv_groups=1,attention=False,dropout_ratio=0):\n\n        super(StandardGenerator,self).__init__()\n\n        output_channels = output_size[0]\n        self.size = output_size[1]\n        self.latent_size = latent_size\n        self.num_classes = num_classes\n\n        current_size = 4\n\n        self.layers = nn.ModuleList()\n\n        in_channels = self.size * 8\n\n        self.layers.append(StandardGeneratorBlock(latent_size,in_channels,num_classes=num_classes,kernel_size=4,padding=0,stride=1,activation=activation))\n\n        while current_size < self.size:\n            current_size *= 2\n\n            if current_size < self.size:\n                self.layers.append(StandardGeneratorBlock(in_channels,in_channels // 2,num_classes=num_classes,kernel_size=4,stride=2,padding=1,activation=activation,conv_groups=conv_groups))\n                self.layers.append(nn.Dropout(dropout_ratio))\n                in_channels = in_channels // 2\n\n                if current_size == self.size // 2 and attention:\n                    self.layers.append(SelfAttention(in_channels))\n        self.final_conv = spectral_norm(ConvTranspose2d(in_channels,output_channels,kernel_size=4,stride=2,padding=1,weight_init=Xavier_Uniform()))\n\n    def forward(self,inputs,labels=None):\n        outputs = inputs.view(-1,self.latent_size ,1,1)\n\n        for layer in self.layers:\n            if self.num_classes > 1 and not isinstance(layer,nn.Dropout) and not isinstance(layer,SelfAttention):\n                outputs = layer(outputs,labels)\n            else:\n                outputs = layer(outputs)\n\n\n        return torch.tanh(self.final_conv(outputs))\n\n\n"""""" The Resnet Projection Discriminator with Spectral Normalization as proposed by Miyato et al. 2018 (https://arxiv.org/abs/1802.05957)\n    with optional self-attention proposed by Zhang et al. 2018 (https://arxiv.org/abs/1805.08318)\n    inpput_size: the size of the input image\n    num_classes: the number of classes for conditional GANs\n    kernel_size: the size of the convolution kernel\n    activation: the activation function to use\n    conv_groups: number of convolution groups\n    attention: if true, attention is applied in the mid layer\n    dropout_ratio: Dropout rate for applying dropout in the residual blocks\n""""""\n\nclass ResProjectionDiscriminator(nn.Module):\n    def __init__(self,input_size,num_classes=0,kernel_size=3,activation=nn.ReLU(),attention=True,apply_sigmoid=False,conv_groups=1,dropout_ratio=0):\n\n        super(ResProjectionDiscriminator,self).__init__()\n        self.num_classes = num_classes\n        in_channels = input_size[0]\n        out_channels = in_channels\n        size = input_size[1]\n        self.apply_sigmoid = apply_sigmoid\n\n        layers = [DiscriminatorResBlock(in_channels,size,kernel_size=kernel_size,activation=activation,initial_activation=False,dropout_ratio=dropout_ratio)]\n\n        current_size = size\n        in_channels = size\n\n        while current_size > 4:\n            layers.append(DiscriminatorResBlock(in_channels,in_channels * 2,kernel_size=kernel_size,downsample_size=2,activation=activation,conv_groups=conv_groups,dropout_ratio=dropout_ratio))\n            current_size /= 2\n            in_channels *= 2\n            if current_size == size//2 and attention:\n                layers.append(SelfAttention(in_channels))\n\n        layers.append(GlobalAvgPool2d())\n\n        self.fc = spectral_norm(Linear(in_channels,out_channels,weight_init=Xavier_Uniform()))\n\n        self.net = nn.Sequential(*layers)\n\n        if self.num_classes > 1:\n\n            self.embed = spectral_norm(Embedding(num_classes,in_channels,weight_init=Xavier_Uniform()))\n\n    def forward(self,inputs,labels=None):\n        outputs = self.net(inputs)\n\n        linear_out = self.fc(outputs)\n\n        if self.num_classes > 1:\n            embed = self.embed(labels.long()).squeeze(1)\n\n            size = outputs.size(1)\n\n            dot = torch.bmm(outputs.view(-1,1,size),embed.view(-1,size,1)).squeeze(2)\n\n            return torch.sigmoid(linear_out + dot) if self.apply_sigmoid else linear_out + dot\n        else:\n            return torch.sigmoid(linear_out) if self.apply_sigmoid else linear_out\n\n\n"""""" The Standard Projection Discriminator with Spectral Normalization as proposed by Miyato et al. 2018 (https://arxiv.org/abs/1802.05957)\n    with optional self-attention proposed by Zhang et al. 2018 (https://arxiv.org/abs/1805.08318)\n    inpput_size: the size of the input image\n    num_classes: the number of classes for conditional GANs\n    kernel_size: the size of the convolution kernel\n    activation: the activation function to use\n    conv_groups: number of convolution groups\n    attention: if true, attention is applied in the mid layer\n    dropout_ratio: Dropout rate for applying dropout in the residual blocks\n""""""\nclass StandardProjectionDiscriminator(nn.Module):\n    def __init__(self,input_size,num_classes=0,activation=nn.LeakyReLU(0.2),attention=True,apply_sigmoid=True,use_bn=False,conv_groups=1,dropout_ratio=0):\n\n        super(StandardProjectionDiscriminator,self).__init__()\n        self.num_classes = num_classes\n        in_channels = input_size[0]\n        out_channels = in_channels\n        size = input_size[1]\n        self.apply_sigmoid = apply_sigmoid\n\n        layers = [StandardDiscriminatorBlock(in_channels,size,kernel_size=3,stride=1,padding=1,use_bn=use_bn,activation=activation)]\n\n        current_size = size\n        in_channels = size\n\n        while current_size > 4:\n            layers.append(StandardDiscriminatorBlock(in_channels,in_channels * 2,kernel_size=4,stride=2,padding=1,use_bn=use_bn,conv_groups=conv_groups))\n            layers.append(nn.Dropout(dropout_ratio))\n            current_size /= 2\n            in_channels *= 2\n            if current_size == size//2 and attention:\n                layers.append(SelfAttention(in_channels))\n\n        layers.append(Flatten())\n        self.fc = spectral_norm(Linear(in_channels * 16,1,weight_init=Xavier_Uniform()))\n\n        self.net = nn.Sequential(*layers)\n\n        if self.num_classes > 1:\n\n            self.embed = spectral_norm(Embedding(num_classes,in_channels * 16,weight_init=Xavier_Uniform()))\n\n    def forward(self,inputs,labels=None):\n        outputs = self.net(inputs)\n\n        linear_out = self.fc(outputs)\n\n        if self.num_classes > 1:\n            embed = self.embed(labels.long()).squeeze(1)\n\n            size = outputs.size(1)\n\n            dot = torch.bmm(outputs.view(-1,1,size),embed.view(-1,size,1)).squeeze(2)\n\n            return torch.sigmoid(linear_out + dot) if self.apply_sigmoid else linear_out + dot\n        else:\n            return torch.sigmoid(linear_out) if self.apply_sigmoid else linear_out\n\n"""""" The standard DCGAN Generator as proposed by Radford et al. 2015 (https://arxiv.org/1511.06434)\n    latent_size: the size of the noise vector\n    output_size: the size of the image to be generated\n    dropout_ratio: Dropout rate for applying dropout after every Relu layer\n    use_bias: Enables or disables bias in the convolution layers\n    num_gpus: Parallelizes computation over the number of GPUs specified.\n""""""\n\nclass DCGANGenerator(nn.Module):\n    def __init__(self,latent_size,output_size,dropout_ratio=0.0,use_bias=False,num_gpus=1):\n\n        super(DCGANGenerator,self).__init__()\n\n        assert output_size[1] >= 32\n\n        self.num_gpus = num_gpus\n\n        in_channels = latent_size[0]\n\n        multiplier = 8\n        out_size = output_size[1]\n        layers = [ConvTranspose2d(in_channels=in_channels,out_channels=int(out_size * multiplier),kernel_size=4,stride=1,padding=0,bias=use_bias,weight_init=Normal(0.0,0.02)),\n                  BatchNorm2d(int(out_size * multiplier),weight_init=Normal(1.0,0.02)),\n                  nn.ReLU(inplace=True),\n                  nn.Dropout(dropout_ratio)\n                  ]\n\n        in_channels = int(out_size * multiplier)\n\n        size = 4 * latent_size[1]\n        while size < output_size[1]:\n            multiplier /= 2\n            size *= 2\n\n            if size < int(out_size * multiplier):\n                out_channels = int(out_size * multiplier)\n            else:\n                out_channels = out_size\n            if size == output_size[1]:\n                layers.append(ConvTranspose2d(in_channels=in_channels, out_channels=output_size[0], kernel_size=4, stride=2, padding=1, bias=use_bias,weight_init=Normal(0.0,0.02)))\n                layers.append(nn.Tanh())\n            else:\n                layers.append(ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1, bias=use_bias,weight_init=Normal(0.0,0.02)))\n                layers.append(BatchNorm2d(out_channels,weight_init=Normal(1.0,0.02)))\n                layers.append(nn.ReLU(inplace=True))\n                layers.append(nn.Dropout(dropout_ratio))\n                in_channels = out_channels\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self,inputs):\n\n        if inputs.is_cuda and self.num_gpus > 1:\n            out = nn.parallel.data_parallel(self.net,inputs,range(self.num_gpus))\n        else:\n            out = self.net(inputs)\n\n        return out\n\n"""""" The standard DCGAN Discriminator as proposed by Radford et al. 2015 (https://arxiv.org/1511.06434)\n    latent_size: the size of the noise vector\n    output_size: the size of the image to be generated\n    dropout_ratio: Dropout rate for applying dropout after every Relu layer\n    use_bias: Enables or disables bias in the convolution layers\n    num_gpus: Parallelizes computation over the number of GPUs specified.\n""""""\n\nclass DCGANDiscriminator(nn.Module):\n    def __init__(self,input_size,dropout_ratio=0.0,use_bias=False,num_gpus=1,apply_sigmoid=True):\n\n\n        super(DCGANDiscriminator,self).__init__()\n\n        assert input_size[1] >= 32\n\n        self.num_gpus = num_gpus\n\n        input_channels = input_size[0]\n        in_channels = input_channels\n        size = input_size[1]\n        self.apply_sigmoid = apply_sigmoid\n\n        channel_multiplier = 1\n\n        out_channels = size\n\n        layers = []\n\n        while size > 4:\n            layers.append(Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=4,stride=2,padding=1,bias=use_bias,weight_init=Normal(0.0,0.02)))\n            if size != input_size[1]:\n                layers.append(BatchNorm2d(out_channels,weight_init=Normal(1.0,0.02)))\n            layers.append(nn.LeakyReLU(0.2,inplace=True))\n            layers.append(nn.Dropout(dropout_ratio))\n            if channel_multiplier < 8:\n                channel_multiplier *= 2\n            size /= 2\n\n\n            in_channels = out_channels\n            out_channels = input_size[1] * channel_multiplier\n\n        layers.append(Conv2d(in_channels=in_channels,out_channels=1,kernel_size=4,padding=0,bias=use_bias,weight_init=Normal(0.0,0.02)))\n\n        self.net = nn.Sequential(*layers)\n\n\n    def forward(self,input):\n\n        if input.is_cuda and self.num_gpus > 1:\n            output = nn.parallel.data_parallel(self.net,input,range(self.num_gpus))\n        else:\n            output = self.net(input)\n\n        return torch.sigmoid(output.view(-1,1)) if self.apply_sigmoid else output.view(-1,1)\n\n"""""" The Wasserstein Discriminator as proposed by Arjovsky et al. 2017 (https://arxiv.org/1701.07875)\n    latent_size: the size of the noise vector\n    output_size: the size of the image to be generated\n    dropout_ratio: Dropout rate for applying dropout after every Relu layer\n    use_bias: Enables or disables bias in the convolution layers\n    num_gpus: Parallelizes computation over the number of GPUs specified.\n""""""\n\nclass WGANDiscriminator(nn.Module):\n    def __init__(self,input_size,dropout_ratio=0.0,use_bias=False,num_gpus=1):\n\n\n        super(WGANDiscriminator,self).__init__()\n\n        assert input_size[1] >= 32\n\n        self.num_gpus = num_gpus\n\n        input_channels = input_size[0]\n        in_channels = input_channels\n        size = input_size[1]\n\n        channel_multiplier = 1\n\n        out_channels = size\n\n        layers = []\n\n        while size > 4:\n            layers.append(Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=4,stride=2,padding=1,bias=use_bias,weight_init=Normal(0.0,0.02)))\n            layers.append(nn.LeakyReLU(0.2,inplace=True))\n            layers.append(nn.Dropout(dropout_ratio))\n            if channel_multiplier < 8:\n                channel_multiplier *= 2\n            size /= 2\n\n\n            in_channels = out_channels\n            out_channels = input_size[1] * channel_multiplier\n\n        layers.append(Conv2d(in_channels=in_channels,out_channels=1,kernel_size=4,padding=0,bias=use_bias,weight_init=Normal(0.0,0.02)))\n\n        self.net = nn.Sequential(*layers)\n\n\n    def forward(self,input):\n\n        if input.is_cuda and self.num_gpus > 1:\n            output = nn.parallel.data_parallel(self.net,input,range(self.num_gpus))\n        else:\n            output = self.net(input)\n\n        return output.view(-1,1).squeeze(1)\n\nclass MLPGenerator(nn.Module):\n    def __init__(self,latent_size,output_size,hidden_dims=512,depth=4,dropout_ratio=0.0,num_gpus=1):\n\n        """"""\n\n        :param latent_size:\n        :param output_size:\n        :param hidden_dims:\n        :param depth:\n        :param dropout_ratio:\n        :param num_gpus:\n        """"""\n\n        super(MLPGenerator,self).__init__()\n        self.num_gpus = num_gpus\n        self.output_size = output_size\n\n        layers = []\n        layers.append(Linear(latent_size, hidden_dims))\n        layers.append(nn.LeakyReLU(0.2))\n        layers.append(nn.Dropout(dropout_ratio))\n\n        for i in range(depth - 2):\n            layers.append(Linear(hidden_dims, hidden_dims))\n            layers.append(nn.LeakyReLU(0.2))\n            layers.append(nn.Dropout(dropout_ratio))\n\n        layers.append(Linear(hidden_dims,output_size[0]*output_size[1] * output_size[2]))\n        layers.append(nn.Tanh())\n        self.net = nn.Sequential(*layers)\n\n    def forward(self,input):\n        if input.is_cuda and self.num_gpus > 1:\n            output = nn.parallel.data_parallel(self.net,input,range(self.num_gpus))\n        else:\n            output = self.net(input)\n\n        return output.view(-1,self.output_size[0],self.output_size[1] ,self.output_size[2])\n\nclass MLPDiscriminator(nn.Module):\n    def __init__(self,input_size,hidden_dims=512,depth=4,dropout_ratio=0.0,num_gpus=1,apply_sigmoid=True):\n        """"""\n\n        :param input_size:\n        :param hidden_dims:\n        :param depth:\n        :param dropout_ratio:\n        :param num_gpus:\n        :param apply_sigmoid:\n        """"""\n\n        super(MLPDiscriminator,self).__init__()\n        self.num_gpus = num_gpus\n        self.input_size = input_size\n        self.apply_sigmoid = apply_sigmoid\n\n        layers = []\n        layers.append(Linear(input_size[0] * input_size[1] * input_size[2], hidden_dims))\n        layers.append(nn.LeakyReLU(0.2))\n        layers.append(nn.Dropout(dropout_ratio))\n\n        for i in range(depth - 2):\n            layers.append(Linear(hidden_dims, hidden_dims))\n            layers.append(nn.LeakyReLU(0.2))\n            layers.append(nn.Dropout(dropout_ratio))\n\n        layers.append(Linear(hidden_dims, 1))\n\n        self.net = nn.Sequential(*layers)\n    def forward(self,input):\n        input = input.view(-1,input.size(1) * input.size(2) * input.size(3))\n        if input.is_cuda and self.num_gpus > 1:\n            output = nn.parallel.data_parallel(self.net,input,range(self.num_gpus))\n        else:\n            output = self.net(input)\n\n        return torch.sigmoid(output.view(-1, 1)) if self.apply_sigmoid else output.view(-1,1)\n\nclass WMLPDiscriminator(nn.Module):\n    def __init__(self,input_size,hidden_dims=512,depth=4,dropout_ratio=0.0,num_gpus=1):\n\n        """"""\n\n        :param input_size:\n        :param hidden_dims:\n        :param depth:\n        :param dropout_ratio:\n        :param num_gpus:\n        """"""\n\n        super(WMLPDiscriminator,self).__init__()\n        self.num_gpus = num_gpus\n        self.input_size = input_size\n\n        layers = []\n        layers.append(Linear(input_size[0] * input_size[1] * input_size[2],hidden_dims))\n        layers.append(nn.LeakyReLU(0.2))\n        layers.append(nn.Dropout(dropout_ratio))\n\n        for i in range(depth - 2):\n            layers.append(Linear(hidden_dims,hidden_dims))\n            layers.append(nn.LeakyReLU(0.2))\n            layers.append(nn.Dropout(dropout_ratio))\n\n        layers.append(Linear(hidden_dims,1))\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self,input):\n        input = input.view(-1,input.size(1) * input.size(2) * input.size(3))\n        if input.is_cuda and self.num_gpus > 1:\n            output = nn.parallel.data_parallel(self.net,input,range(self.num_gpus))\n        else:\n            output = self.net(input)\n\n        return output.view(-1, 1)'"
torchfusion/gan/layers/__init__.py,0,"b'from .layers import SelfAttention,ConditionalBatchNorm2d,GeneratorResBlock,StandardGeneratorBlock,DiscriminatorResBlock,StandardDiscriminatorBlock'"
torchfusion/gan/layers/layers.py,8,"b'from torchfusion.layers import *\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import spectral_norm\nfrom math import floor,sqrt\nfrom torchfusion.initializers import *\nimport torch\n\nclass ConditionalBatchNorm2d(nn.Module):\n    def __init__(self, num_features, num_class, eps=1e-5, momentum=0.1,\n                 track_running_stats=True):\n        """"""\n\n        :param num_features:\n        :param num_class:\n        :param eps:\n        :param momentum:\n        :param track_running_stats:\n        """"""\n        super(ConditionalBatchNorm2d,self).__init__()\n\n        self.bn = BatchNorm2d(num_features=num_features,eps=eps,momentum=momentum,track_running_stats=track_running_stats, affine=False)\n        self.gamma_embed = Embedding(num_class, num_features)\n        self.beta_embed = Embedding(num_class, num_features)\n        self.gamma_embed.weight.data = torch.ones(self.gamma_embed.weight.size())\n        self.beta_embed.weight.data = torch.zeros(self.gamma_embed.weight.size())\n\n    def forward(self, input, class_id):\n        input = input.float()\n        class_id = class_id.long()\n        out = self.bn(input)\n        gamma = self.gamma_embed(class_id).squeeze(1).unsqueeze(2).unsqueeze(3)\n        beta = self.beta_embed(class_id).squeeze(1).unsqueeze(2).unsqueeze(3)\n\n        out = gamma * out.type(gamma.dtype) + beta\n\n\n        return out\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self,in_channels,weight_init=Kaiming_Normal(),bias_init=Zeros(),use_bias=False):\n        """"""\n\n        :param in_channels:\n        :param weight_init:\n        :param bias_init:\n        :param use_bias:\n        """"""\n        super(SelfAttention,self).__init__()\n\n        self.q = Conv2d(in_channels,in_channels//8,kernel_size=1,weight_init=weight_init,bias_init=bias_init,bias=use_bias)\n        self.k = Conv2d(in_channels,in_channels//8,kernel_size=1,weight_init=weight_init,bias_init=bias_init,bias=use_bias)\n\n        self.v = Conv2d(in_channels,in_channels,kernel_size=1,weight_init=weight_init,bias_init=bias_init,bias=use_bias)\n\n        self.softmax = nn.Softmax(dim=-1)\n\n        self.atten_weight = nn.Parameter(torch.tensor([0.0]))\n\n    def forward(self,input):\n        batch_size, channels, width, height = input.size()\n        res = input\n\n        queries = self.q(input).view(batch_size,-1,width*height).permute(0,2,1)\n        keys = self.k(input).view(batch_size,-1,width*height)\n        values = self.v(input).view(batch_size, -1, width * height)\n\n        atten_ = self.softmax(torch.bmm(queries, keys)).permute(0,2,1)\n\n        atten_values = torch.bmm(values,atten_).view(batch_size,channels,width,height)\n\n\n        return (self.atten_weight * atten_values) + res\n\n\n\nclass GeneratorResBlock(nn.Module):\n    def __init__(self,in_channels,out_channels,num_classes=0,upsample_size=1,kernel_size=3,activation=nn.ReLU(),conv_groups=1,dropout_ratio=0):\n        """"""\n\n        :param in_channels:\n        :param out_channels:\n        :param num_classes:\n        :param upsample_size:\n        :param kernel_size:\n        :param activation:\n        :param conv_groups:\n        :param dropout_ratio:\n        """"""\n        super(GeneratorResBlock,self).__init__()\n        padding = floor(kernel_size/2)\n        self.activation = activation\n        self.num_classes = num_classes\n        self.upsample_size = upsample_size\n\n        self.dropout = nn.Dropout(dropout_ratio)\n\n        self.conv1 = spectral_norm(Conv2d(in_channels,out_channels,kernel_size=kernel_size,padding=padding,weight_init=Xavier_Uniform(sqrt(2)),groups=conv_groups))\n        self.conv2 = spectral_norm(Conv2d(out_channels,out_channels,kernel_size=kernel_size,padding=padding,weight_init=Xavier_Uniform(sqrt(2)),groups=conv_groups))\n        if num_classes > 0:\n            self.bn1 = ConditionalBatchNorm2d(in_channels,num_classes)\n            self.bn2 = ConditionalBatchNorm2d(out_channels,num_classes)\n        else:\n            self.bn1 = BatchNorm2d(in_channels)\n            self.bn2 = BatchNorm2d(out_channels)\n\n        self.res_upsample = nn.Sequential()\n\n        if in_channels != out_channels or upsample_size > 1:\n            self.res_upsample = Conv2d(in_channels, out_channels, kernel_size=1,weight_init=Xavier_Uniform())\n\n    def forward(self,inputs,labels=None):\n        res = inputs\n\n        if labels is not None:\n            inputs = self.bn1(inputs,labels)\n        else:\n            inputs = self.bn1(inputs)\n\n        inputs = self.dropout(self.conv1(self.activation(inputs)))\n        if self.upsample_size > 1:\n            inputs = F.interpolate(inputs,scale_factor=self.upsample_size)\n\n        if labels is not None:\n            inputs = self.bn2(inputs,labels)\n        else:\n            inputs = self.bn2(inputs)\n\n        inputs = self.conv2(self.activation(inputs))\n\n        if self.upsample_size > 1:\n            return inputs + F.interpolate(self.res_upsample(res),scale_factor=self.upsample_size)\n        else:\n            return inputs + self.res_upsample(res)\n\nclass StandardGeneratorBlock(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size,padding,stride,num_classes=0,activation=nn.LeakyReLU(0.2),conv_groups=1):\n        """"""\n\n        :param in_channels:\n        :param out_channels:\n        :param kernel_size:\n        :param padding:\n        :param stride:\n        :param num_classes:\n        :param activation:\n        :param conv_groups:\n        """"""\n\n        super(StandardGeneratorBlock,self).__init__()\n\n        self.activation = activation\n        self.num_classes = num_classes\n\n\n        self.conv = spectral_norm(ConvTranspose2d(in_channels,out_channels,kernel_size=kernel_size,padding=padding,stride=stride,weight_init=Xavier_Uniform(),groups=conv_groups))\n        if num_classes > 0:\n            self.bn = ConditionalBatchNorm2d(out_channels,num_classes)\n        else:\n            self.bn = BatchNorm2d(out_channels)\n\n    def forward(self,inputs,labels=None):\n\n        inputs = self.conv(inputs)\n\n        if labels is not None:\n            inputs = self.bn(inputs,labels)\n        else:\n            inputs = self.bn(inputs)\n\n        inputs = self.activation(inputs)\n\n        return inputs\n\n\n\nclass DiscriminatorResBlock(nn.Module):\n    def __init__(self,in_channels,out_channels,downsample_size=1,kernel_size=3,activation=nn.ReLU(),initial_activation=True,conv_groups=1,dropout_ratio=0):\n        """"""\n\n        :param in_channels:\n        :param out_channels:\n        :param downsample_size:\n        :param kernel_size:\n        :param activation:\n        :param initial_activation:\n        :param conv_groups:\n        """"""\n\n        super(DiscriminatorResBlock,self).__init__()\n\n        padding = floor(kernel_size / 2)\n        self.activation = activation\n        self.initial_activation = initial_activation\n        self.dropout = nn.Dropout(dropout_ratio)\n\n        self.conv1 = spectral_norm(Conv2d(in_channels,out_channels,kernel_size=kernel_size,padding=padding,weight_init=Xavier_Uniform(),groups=conv_groups))\n        self.conv2 = spectral_norm(Conv2d(out_channels,out_channels,kernel_size=kernel_size,padding=padding,weight_init=Xavier_Uniform(),groups=conv_groups))\n        self.downsample = nn.Sequential()\n        if downsample_size > 1:\n            self.downsample = nn.AvgPool2d(kernel_size=downsample_size)\n\n        self.res_downsample = nn.Sequential()\n        if in_channels != out_channels or downsample_size  > 1:\n            self.res_downsample = nn.Sequential(\n                Conv2d(in_channels,out_channels,kernel_size=1,weight_init=Xavier_Uniform(sqrt(2))),\n                nn.AvgPool2d(kernel_size=downsample_size)\n            )\n\n    def forward(self,inputs):\n\n        res = inputs\n\n        if self.initial_activation:\n            inputs = self.activation(inputs)\n\n        inputs = self.conv1(inputs)\n        inputs = self.dropout(self.activation(inputs))\n        inputs = self.conv2(inputs)\n        inputs = self.downsample(inputs)\n\n        return inputs + self.res_downsample(res)\n\n\n\n\nclass StandardDiscriminatorBlock(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size,padding,stride,activation=nn.LeakyReLU(0.2),use_bn=False,conv_groups=1):\n        """"""\n\n        :param in_channels:\n        :param out_channels:\n        :param kernel_size:\n        :param padding:\n        :param stride:\n        :param activation:\n        :param use_bn:\n        :param conv_groups:\n        """"""\n\n        super(StandardDiscriminatorBlock,self).__init__()\n\n        self.activation = activation\n\n        self.conv = spectral_norm(Conv2d(in_channels,out_channels,kernel_size=kernel_size,padding=padding,stride=stride,weight_init=Xavier_Uniform(),groups=conv_groups))\n\n        self.bn = nn.Sequential()\n\n        if use_bn:\n            self.bn = BatchNorm2d(out_channels,weight_init=Normal(1.0,0.02))\n\n    def forward(self,inputs):\n\n        return self.activation(self.bn(self.conv(inputs)))\n\n'"
torchfusion/gan/learners/__init__.py,0,"b'from .learners import StandardGanLearner,RStandardGanLearner,RAvgStandardGanLearner,WGanLearner,HingeGanLearner,RHingeGanLearner,BaseGanLearner,StandardBaseGanLearner,RAvgHingeGanLearner,BaseGanCore'"
torchfusion/gan/learners/learners.py,62,"b'from collections import namedtuple\nimport torch.nn as nn\nfrom ...utils import *\nfrom ...learners import AbstractBaseLearner\n\nimport torch\nfrom torch.autograd import Variable, grad\nimport torch.cuda as cuda\nfrom torchvision import utils as vutils\nimport os\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom tensorboardX import SummaryWriter\nimport torch.distributions as distribution\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.onnx as onnx\nfrom math import ceil\n\n\n\nr""""""This is the base Learner for training, evaluating and performing inference with Generative Adversarial Networks\nGoodfellow et al. 2014 (https://arxiv.org/1406.2661)\nAll custom GAN learners that use a single Generator and a single Discriminator should subclass this \n\n    Args:\n        gen_model (Module):  the generator module.\n        disc_model (Module):  the discriminator module.\n        use_cuda_if_available (boolean): If set to true, training would be done on a gpu if any is available""""""\n\nclass BaseGanLearner(AbstractBaseLearner):\n    def __init__(self, gen_model,disc_model,use_cuda_if_available=True):\n        super(BaseGanLearner,self).__init__()\n        self.model_dir = os.getcwd()\n        self.gen_model = gen_model\n        self.disc_model = disc_model\n        self.cuda = False\n        if use_cuda_if_available and cuda.is_available():\n            self.cuda = True\n\n        self.__train_history__ = {}\n\n        self.gen_optimizer = None\n        self.disc_optimizer = None\n        self.gen_running_loss = None\n        self.disc_running_loss = None\n\n        self.visdom_log = None\n        self.tensorboard_log = None\n\n    r""""""Initialize generator model weights using pre-trained weights from the filepath\n\n        Args:\n            path (str): path to a compatible pre-defined model\n\n        """"""\n    def load_generator(self, path):\n        load_model(self.gen_model,path)\n\n\n    r""""""Initialize discriminator model weights using pre-trained weights from the filepath\n\n        Args:\n            path (str): path to a compatible pre-defined model\n\n        """"""\n\n    def load_discriminator(self, path):\n        load_model(self.disc_model, path)\n\n    r""""""Saves the generator model to the path specified\n            Args:\n                path (str): path to save model\n                save_architecture (boolean): if True, both weights and architecture will be saved, default is False\n\n            """"""\n    def save_generator(self, path,save_architecture=False):\n        save_model(self.gen_model,path,save_architecture)\n\n    r""""""Saves the discriminator model to the path specified\n            Args:\n                path (str): path to save model\n                save_architecture (boolean): if True, both weights and architecture will be saved, default is False\n\n            """"""\n    def save_discriminator(self, path,save_architecture=False):\n        save_model(self.disc_model, path, save_architecture)\n\n\n    def train(self,*args):\n        self.__train_loop__(*args)\n\n    def __train_loop__(self, train_loader,gen_optimizer,disc_optimizer,num_epochs=10, disc_steps=1,gen_lr_scheduler=None,disc_lr_scheduler=None, model_dir=os.getcwd(),save_model_interval=1, save_outputs_interval=100,display_outputs=True,notebook_mode=False,batch_log=True,save_logs=None,display_metrics=False,save_metrics=False,visdom_log=None,tensorboard_log=None,save_architecture=False):\n\n        """"""\n\n        :param train_loader:\n        :param gen_optimizer:\n        :param disc_optimizer:\n        :param num_epochs:\n        :param disc_steps:\n        :param gen_lr_scheduler:\n        :param disc_lr_scheduler:\n        :param model_dir:\n        :param save_model_interval:\n        :param save_outputs_interval:\n        :param display_outputs:\n        :param notebook_mode:\n        :param batch_log:\n        :param save_logs:\n        :param display_metrics:\n        :param save_metrics:\n        :param visdom_log:\n        :param tensorboard_log:\n        :param save_architecture:\n        :return:\n        """"""\n\n        assert(disc_steps < len(train_loader.dataset))\n\n        self.gen_optimizer = gen_optimizer\n        self.disc_optimizer = disc_optimizer\n        self.tensorboard_log = tensorboard_log\n        self.visdom_log = visdom_log\n\n        if not os.path.exists(model_dir):\n            os.mkdir(model_dir)\n\n        self.model_dir = model_dir\n        models_gen = os.path.join(model_dir, ""gen_models"")\n        models_disc = os.path.join(model_dir, ""disc_models"")\n\n        if not os.path.exists(models_gen):\n            os.mkdir(models_gen)\n\n        if not os.path.exists(models_disc):\n            os.mkdir(models_disc)\n\n        iterations = 0\n\n        from tqdm import tqdm_notebook\n        from tqdm import tqdm\n\n        train_start_time = time()\n\n        for e in range(num_epochs):\n            print(""Epoch {} of {}"".format(e + 1, num_epochs))\n\n            self.gen_model.train()\n            self.disc_model.train()\n            for func in self.epoch_start_funcs:\n                func(e + 1)\n\n            self.gen_running_loss = torch.Tensor([0.0])\n            self.disc_running_loss = torch.Tensor([0.0])\n            gen_loss = 0\n            disc_loss = 0\n\n            gen_data_len = 0\n            disc_data_len = 0\n\n            if notebook_mode and batch_log:\n                progress_ = tqdm_notebook(enumerate(train_loader))\n            elif batch_log:\n                progress_ = tqdm(enumerate(train_loader))\n            else:\n                progress_ = enumerate(train_loader)\n\n            max_batch_size = 0\n\n            init_time = time()\n\n            for i,t in progress_:\n\n                for func in self.batch_start_funcs:\n                    func(e + 1,i + 1)\n\n                batch_size = get_batch_size(t)\n                disc_data_len += batch_size\n\n                if max_batch_size < batch_size:\n                    max_batch_size = batch_size\n\n                self.__disc_train_func__(t)\n\n                disc_loss = self.disc_running_loss.data.item() / disc_data_len\n\n                if (i+1) % disc_steps == 0:\n                    self.__gen_train_func__(t)\n                    gen_data_len += batch_size\n\n                    gen_loss = self.gen_running_loss.data.item() / gen_data_len\n\n                if batch_log:\n                     progress_dict = {""Gen Loss"": gen_loss,""Disc Loss"":disc_loss}\n                     progress_.set_postfix(progress_dict)\n\n                iterations += 1\n\n                if iterations % save_outputs_interval == 0:\n                    self.__save__(iterations)\n                    if display_outputs:\n                        self.__show__(iterations)\n\n                if batch_log:\n\n                    progress_.set_description(""{}/{} batches "".format(int(ceil(disc_data_len / max_batch_size)),\n                                                                      int(ceil(len(\n                                                                          train_loader.dataset) / max_batch_size))))\n                    progress_dict = {""Disc Loss"": disc_loss,""Gen Loss"":gen_loss}\n\n                    progress_.set_postfix(progress_dict)\n\n                batch_info = {""gen_loss"":gen_loss , ""disc_loss"": disc_loss}\n\n                for func in self.batch_end_funcs:\n                    func(e + 1,i + 1,batch_info)\n\n\n            if self.cuda:\n                cuda.synchronize()\n            duration = time() - init_time\n\n            if ""duration"" in self.__train_history__:\n                self.__train_history__[""duration""].append(duration)\n            else:\n                self.__train_history__[""duration""] = [duration]\n\n            if gen_lr_scheduler is not None:\n                if isinstance(gen_lr_scheduler, ReduceLROnPlateau):\n                    gen_lr_scheduler.step(gen_loss)\n                else:\n                    gen_lr_scheduler.step()\n\n            if disc_lr_scheduler is not None:\n                if isinstance(disc_lr_scheduler, ReduceLROnPlateau):\n                    disc_lr_scheduler.step(gen_loss)\n                else:\n                    disc_lr_scheduler.step()\n\n            if ""disc_loss"" in self.__train_history__:\n                self.__train_history__[""disc_loss""].append(disc_loss)\n            else:\n                self.__train_history__[""disc_loss""] = [disc_loss]\n\n            if ""gen_loss"" in self.__train_history__:\n                self.__train_history__[""gen_loss""].append(gen_loss)\n            else:\n                self.__train_history__[""gen_loss""] = [gen_loss]\n\n            if ""epoch"" in self.__train_history__:\n                self.__train_history__[""epoch""].append(e + 1)\n            else:\n                self.__train_history__[""epoch""] = [e + 1]\n\n\n   \n            if (e+1) % save_model_interval == 0:\n\n                model_file = os.path.join(models_gen, ""gen_model_{}.pth"".format(e + 1))\n                self.save_generator(model_file, save_architecture)\n\n                print(""New Generator model saved at {}"".format(model_file))\n\n                model_file = os.path.join(models_disc, ""disc_model_{}.pth"".format(e + 1))\n                self.save_discriminator(model_file, save_architecture)\n\n                print(""New Discriminator model saved at {}"".format(model_file))\n\n\n            print(""Epoch: {}, Duration: {} , Gen Loss: {} Disc Loss: {}"".format(e+1, duration, gen_loss,disc_loss))\n\n            if save_logs is not None:\n                logfile = open(save_logs, ""a"")\n                logfile.write(""Epoch: {}, Duration: {} , Gen Loss: {} Disc Loss: {}"".format(e+1, duration, gen_loss,disc_loss))\n                logfile.close()\n\n            epoch_arr = self.__train_history__[""epoch""]\n            epoch_arr_tensor = torch.LongTensor(epoch_arr)\n\n            if display_metrics or save_metrics:\n\n                save_path = None\n\n                if save_metrics:\n                    save_path = os.path.join(model_dir, ""epoch_{}_loss.png"".format(e+1))\n\n                visualize(epoch_arr, [PlotInput(value=self.__train_history__[""gen_loss""], name=""Generator Loss"", color=""red""),\n                                      PlotInput(value=self.__train_history__[""disc_loss""], name=""Discriminator Loss"", color=""red"")],display=display_metrics,\n                          save_path=save_path,axis=""off"")\n\n\n            if visdom_log is not None:\n                visdom_log.plot_line(torch.FloatTensor(self.__train_history__[""gen_loss""]), epoch_arr_tensor, win=""gen_loss"",\n                                     title=""Generator Loss"")\n\n                visdom_log.plot_line(torch.FloatTensor(self.__train_history__[""disc_loss""]), epoch_arr_tensor, win=""disc_loss"",\n                                     title=""Discriminator Loss"")\n\n\n            if tensorboard_log is not None:\n                writer = SummaryWriter(os.path.join(model_dir, tensorboard_log))\n                writer.add_scalar(""logs/gen_loss"", gen_loss, global_step=e+1)\n                writer.add_scalar(""logs/disc_loss"", disc_loss, global_step=e+1)\n\n                writer.close()\n\n            epoch_info = {""gen_loss"": gen_loss, ""disc_loss"": disc_loss, ""duration"": duration}\n            for func in self.epoch_end_funcs:\n                func(e + 1, epoch_info)\n\n        train_end_time = time() - train_start_time\n        train_info = {""train_duration"": train_end_time}\n        for func in self.train_completed_funcs:\n            func(train_info)\n\n    """""" Abstract function containing the training logic for the generator, \n     all custom trainers must override this.\n\n        Args:\n            data: a single batch of data from the train set\n            """"""\n    def __gen_train_func__(self,data):\n        raise NotImplementedError()\n\n    """""" Abstract function containing the training logic for the discriminator, \n         all custom trainers must override this.\n\n            Args:\n                data: a single batch of data from the train set\n                """"""\n    def __disc_train_func__(self,data):\n        raise NotImplementedError()\n\n    """""" Abstract function containing logic to save the outputs of the generator, \n         all custom trainers must override this.\n\n            Args:\n                iterations: total number of batch iterations since the start of training\n                """"""\n    def __save__(self,iterations):\n        raise NotImplementedError()\n\n    """""" Abstract function containing logic to display the outputs of the generator, \n             all custom trainers must override this.\n\n                Args:\n                    iterations: total number of batch iterations since the start of training\n                    """"""\n\n    def  evaluate(self, *args):\n        pass\n    def validate(self, *args):\n        pass\n\n    def __show__(self,iterations):\n        raise NotImplementedError()\n\n    """""" Returns predictions for a given input tensor or a an instance of a DataLoader\n            Args:\n                inputs: input Tensor or DataLoader\n    """"""\n    def predict(self, inputs):\n        self.gen_model.eval()\n\n        if isinstance(inputs, DataLoader):\n            predictions = []\n            for i, data in enumerate(inputs):\n                batch_pred = self.__predict_func__(data)\n                for pred in batch_pred:\n                    predictions.append(pred.unsqueeze(0))\n            return torch.cat(predictions)\n\n        else:\n            pred = self.__predict_func__(inputs)\n\n            return pred.squeeze(0)\n\n    """""" Abstract function containing custom logic for performing inference, must return the output,\n            all custom trainers should implement this.\n            input: An input tensor or a batch of input tensors\n            """"""\n    def __predict_func__(self, input):\n        raise NotImplementedError()\n\n    r"""""" Returns a dictionary containing the values of epochs and loss during training.\n        """"""\n    def get_train_history(self):\n        return self.__train_history__\n\n\n\nclass BaseGanCore(BaseGanLearner):\n    def __init__(self, gen_model, disc_model, use_cuda_if_available=True):\n        super(BaseGanCore,self).__init__(gen_model, disc_model, use_cuda_if_available)\n\n        self.latent_size = None\n        self.dist = distribution.Normal(0,1)\n        self.fixed_source = None\n\n        self.conditional = False\n        self.classes = 0\n        self.num_samples = 5\n\n    def __disc_train_func__(self, data):\n\n        for params in self.disc_model.parameters():\n            params.requires_grad = True\n\n        for params in self.gen_model.parameters():\n            params.requires_grad = False\n\n        if isinstance(data, list) or isinstance(data, tuple):\n            x = data[0]\n\n        else:\n            if self.conditional:\n                raise ValueError(""Conditional mode is invalid for inputs with no labels, set num_classes to None or provide labels"")\n            x = data\n\n        batch_size = x.size(0)\n        if isinstance(self.latent_size, int):\n            source_size = list([self.latent_size])\n        else:\n            source_size = list(self.latent_size)\n        cond_samples_size = [self.num_samples] + source_size\n        source_size = [batch_size] + source_size\n\n        if self.fixed_source is None:\n\n            if self.conditional:\n                self.fixed_source = self.dist.sample(tuple(cond_samples_size))\n\n            else:\n                self.fixed_source = self.dist.sample(tuple(source_size))\n\n            if self.cuda:\n                self.fixed_source = self.fixed_source.cuda()\n\n            self.fixed_source = Variable(self.fixed_source)\n\n\n    def __gen_train_func__(self, data):\n\n        for params in self.gen_model.parameters():\n            params.requires_grad = True\n\n        for params in self.disc_model.parameters():\n            params.requires_grad = False\n\n\n    def __save__(self, iteration):\n\n        save_dir = os.path.join(self.model_dir, ""gen_images"")\n\n        if os.path.exists(save_dir) == False:\n            os.mkdir(save_dir)\n        if self.tensorboard_log is not None:\n            writer = SummaryWriter(self.tensorboard_log)\n\n        if self.conditional:\n\n            for i in range(self.classes):\n                class_path = os.path.join(save_dir,""class_{}"".format(i))\n                if not os.path.exists(class_path):\n                    os.mkdir(class_path)\n                class_labels = torch.randn((self.num_samples, 1)).type(torch.LongTensor).fill_(i)\n\n                if self.cuda:\n                    class_labels = class_labels.cuda()\n\n                outputs = self.gen_model(self.fixed_source, class_labels)\n                if self.fp16_mode:\n                    outputs = outputs.float()\n\n                images_file = os.path.join(class_path, ""iteration{}.png"".format(iteration))\n\n                images = vutils.make_grid(outputs.cpu().data, normalize=True)\n                vutils.save_image(outputs.cpu().data, images_file, normalize=True)\n\n                if self.tensorboard_log is not None:\n                    writer.add_image(""logs/gen_images/class_{}"".format(i), images, global_step=iteration)\n\n                if self.visdom_log is not None:\n                    self.visdom_log.log_images(images, win=""class_{}"".format(i), title=""Class {}"".format(i))\n\n\n        else:\n            outputs = self.gen_model(self.fixed_source)\n            if self.fp16_mode:\n                outputs = outputs.float()\n            images_file = os.path.join(save_dir, ""image_{}.png"".format(iteration))\n\n            images = vutils.make_grid(outputs.cpu().data, normalize=True)\n            vutils.save_image(outputs.cpu().data, images_file, normalize=True)\n\n            if self.tensorboard_log is not None:\n                writer.add_image(""logs/gen_images"", images, global_step=iteration)\n\n            if self.visdom_log is not None:\n                self.visdom_log.log_images(images, win=""gen_images"", title=""Generated Images"")\n        if self.tensorboard_log:\n            writer.close()\n\n    def __show__(self, iteration):\n\n        if self.conditional:\n            for i in range(self.classes):\n                class_labels = torch.randn((self.num_samples, 1)).type(torch.LongTensor).fill_(i)\n\n                if self.cuda:\n                    class_labels = class_labels.cuda()\n                \n                outputs = self.gen_model(self.fixed_source, class_labels)\n\n                if self.fp16_mode:\n                    outputs = outputs.float()\n\n                images = vutils.make_grid(outputs.cpu().data, normalize=True)\n                images = np.transpose(images.numpy(), (1, 2, 0))\n                plt.subplot(self.classes, 1, i + 1)\n                plt.axis(""off"")\n                # plt.title(""class {}"".format(i))\n                plt.imshow(images)\n\n            plt.show()\n\n        else:\n            outputs = self.gen_model(self.fixed_source)\n            if self.fp16_mode:\n                outputs = outputs.float()\n\n            images = vutils.make_grid(outputs.cpu().data, normalize=True)\n\n            images = np.transpose(images.numpy(), (1, 2, 0))\n            plt.imshow(images)\n            plt.axis(""off"")\n            plt.grid(False)\n            plt.show()\n\n    def __predict_func__(self, data):\n        labels = None\n        if isinstance(data, list) or isinstance(data, tuple):\n            source = data[0]\n            labels = data[1]\n        else:\n            source = data\n\n        if self.cuda:\n            source = source.cuda()\n            if labels is not None:\n                labels = labels.cuda()\n\n        if labels is not None:\n            labels = labels.unsqueeze(1) if len(labels.size()) == 1 else labels\n            outputs = self.gen_model(source, labels)\n        else:\n            outputs = self.gen_model(source)\n\n        return outputs\n\n    def gen_summary(self, input_size, label=None, input_type=torch.FloatTensor, item_length=26, tensorboard_log=None):\n        input = torch.randn(input_size).type(input_type).unsqueeze(0)\n        inputs = input.cuda() if self.cuda else input\n        if label is not None:\n            label = torch.randn(1, 1).fill_(label).long()\n            label = label.cuda() if self.cuda else label\n            return get_model_summary(self.gen_model, inputs,label, item_length=item_length, tensorboard_log=tensorboard_log)\n        else:\n            return get_model_summary(self.gen_model, inputs, item_length=item_length,tensorboard_log=tensorboard_log)\n\n    def disc_summary(self, input_size, label=None, input_type=torch.FloatTensor, item_length=26, tensorboard_log=None):\n        input = torch.randn(input_size).type(input_type).unsqueeze(0)\n        inputs = input.cuda() if self.cuda else input\n        if label is not None:\n            label = torch.randn(1, 1).fill_(label).long()\n            label = label.cuda() if self.cuda else label\n            return get_model_summary(self.gen_model, inputs, label, item_length=item_length,\n                                     tensorboard_log=tensorboard_log)\n        else:\n            return get_model_summary(self.gen_model, inputs, item_length=item_length, tensorboard_log=tensorboard_log)\n\n    def gen_to_onnx(self, path, input_size, label=None, input_type=torch.FloatTensor, **kwargs):\n        input = torch.randn(input_size).type(input_type).unsqueeze(0)\n        if label is None:\n            inputs = Variable(input.cuda() if self.cuda else input)\n        else:\n            label = torch.randn(1, 1).fill_(label)\n            inputs = [Variable(input.cuda() if self.cuda else input), label.cuda() if self.cuda else label]\n\n        return onnx._export(self.gen_model, inputs, f=path, **kwargs)\n\n    def disc_to_onnx(self, path, input_size, label=None, input_type=torch.FloatTensor, **kwargs):\n        input = torch.randn(input_size).type(input_type).unsqueeze(0)\n        if label is None:\n            inputs = Variable(input.cuda() if self.cuda else input)\n        else:\n            label = torch.randn(1, 1).fill_(label)\n            inputs = [Variable(input.cuda() if self.cuda else input), label.cuda() if self.cuda else label]\n\n        return onnx._export(self.disc_model, inputs, f=path, **kwargs)\n\n\nclass StandardBaseGanLearner(BaseGanCore):\n\n    def train(self, train_loader, gen_optimizer, disc_optimizer, latent_size,relative_mode=True, dist=distribution.Normal(0, 1),\n                  num_classes=0, num_samples=5,**kwargs):\n\n        self.latent_size = latent_size\n        self.dist = dist\n        self.classes = num_classes\n        self.num_samples = num_samples\n        self.conditional = (num_classes > 0)\n        self.relative_mode = relative_mode\n\n        super().__train_loop__(train_loader, gen_optimizer, disc_optimizer, **kwargs)\n\n    def __disc_train_func__(self, data):\n\n        super().__disc_train_func__(data)\n\n        self.disc_optimizer.zero_grad()\n\n        if isinstance(data, list) or isinstance(data, tuple):\n            x = data[0]\n            if self.conditional:\n                class_labels = data[1]\n        else:\n            x = data\n\n        batch_size = x.size(0)\n        if isinstance(self.latent_size, int):\n            source_size = list([self.latent_size])\n        else:\n            source_size = list(self.latent_size)\n        source_size = [batch_size] + source_size\n\n        source = self.dist.sample(tuple(source_size))\n\n        if self.cuda:\n            x = x.cuda()\n            source = source.cuda()\n\n            if self.conditional:\n                class_labels = class_labels.cuda()\n\n        x = Variable(x)\n        source = Variable(source)\n        if self.conditional:\n            class_labels = class_labels.unsqueeze(1) if len(class_labels.size())==1 else class_labels\n\n        if self.conditional:\n            outputs = self.disc_model(x, class_labels)\n        else:\n            outputs = self.disc_model(x)\n\n        if self.conditional:\n            random_labels = torch.from_numpy(np.random.randint(0, self.classes, size=(batch_size, 1))).long()\n            if self.cuda:\n                random_labels = random_labels.cuda()\n\n            \n\n            generated = self.gen_model(source, random_labels)\n            gen_outputs = self.disc_model(generated.detach(), random_labels)\n\n        else:\n            generated = self.gen_model(source)\n            gen_outputs = self.disc_model(generated.detach())\n\n        loss = self.__update_discriminator_loss__(x,generated,outputs,gen_outputs)\n        if self.fp16_mode:\n            self.disc_optimizer.backward(loss)\n        else:\n            loss.backward()\n        self.disc_optimizer.step()\n        self.disc_running_loss = self.disc_running_loss + (loss.cpu().item() * batch_size)\n\n    def __gen_train_func__(self, data):\n\n        super().__gen_train_func__(data)\n\n        self.gen_optimizer.zero_grad()\n\n        if isinstance(data, list) or isinstance(data, tuple):\n            x = data[0]\n            if self.conditional and self.relative_mode:\n                class_labels = data[1].unsqueeze(1) if len(data[1]) == 1 else data[1]\n        else:\n            x = data\n        batch_size = x.size(0)\n\n        if isinstance(self.latent_size, int):\n            source_size = list([self.latent_size])\n        else:\n            source_size = list(self.latent_size)\n\n        source_size = [batch_size] + source_size\n\n        source = self.dist.sample(tuple(source_size))\n        if self.conditional:\n            random_labels = torch.from_numpy(np.random.randint(0, self.classes, size=(batch_size, 1))).long()\n            if self.cuda:\n                random_labels = random_labels.cuda()\n\n           \n\n        if self.cuda:\n            source = source.cuda()\n\n            x = x.cuda()\n            if self.conditional and self.relative_mode:\n                class_labels = class_labels.cuda()\n\n        source = Variable(source)\n        x = Variable(x)\n\n        if self.conditional:\n            fake_images = self.gen_model(source, random_labels)\n            outputs = self.disc_model(fake_images, random_labels)\n            if self.relative_mode:\n                real_outputs = self.disc_model(x, class_labels)\n        else:\n            fake_images = self.gen_model(source)\n            outputs = self.disc_model(fake_images)\n            if self.relative_mode:\n                real_outputs = self.disc_model(x)\n\n        if not self.relative_mode:\n            real_outputs = None\n\n\n        loss = self.__update_generator_loss__(x,fake_images,real_outputs,outputs)\n\n        if self.fp16_mode:\n            self.gen_optimizer.backward(loss)\n        else:\n            loss.backward()\n\n        self.gen_optimizer.step()\n        self.gen_running_loss = self.gen_running_loss + (loss.cpu().item() * batch_size)\n\n    def __update_generator_loss__(self,real_images,gen_images,real_preds,gen_preds):\n        raise NotImplementedError()\n\n    def __update_discriminator_loss__(self,real_images,gen_images,real_preds,gen_preds):\n        raise NotImplementedError()\n\nclass StandardGanLearner(StandardBaseGanLearner):\n    def __init__(self, gen_model, disc_model, use_cuda_if_available=True):\n        super(StandardGanLearner, self).__init__(gen_model, disc_model, use_cuda_if_available)\n\n    r""""""Training function\n\n                Args:\n                    train_loader (DataLoader): an instance of DataLoader containing the training set\n                    gen_optimizer (Optimizer): an optimizer for updating parameters of the generator\n                    disc_optimizer (Optimizer): an optimizer for updating parameters of the discriminator\n                    latent_size (int): the size of the latent variable to be fed into the generator\n                    gen_loss_fn: the generator loss function\n                    disc_loss_fn: the generator loss function\n                    num_epochs (int): The maximum number of training epochs\n                    disc_steps (int): The number of times to train the discriminator before training generator\n                    save_models (str): If all, the model is saved at the end of each epoch while the best models based \n                    gen_lr_scheduler (_LRScheduler): Learning rate scheduler for the generator\n                    disc_lr_scheduler (_LRScheduler): Learning rate sheduler for the discriminator\n                        on the test set are also saved in best_models folder\n                        If \'best\', only the best models are saved,  test_loader and test_metrics must be provided\n                    model_dir (str) = a path in which to save the models\n                    save_model_interval (int): saves the models after every n epoch\n                    save_outputs_interval (int): saves sample outputs from the generator after every n iterations\n                    notebook_mode (boolean): Optimizes the progress bar for either jupyter notebooks or consoles\n                    display_metrics (boolean): Enables display of metrics and loss visualizations at the end of each epoch.\n                    save_metrics (boolean): Enables saving of metrics and loss visualizations at the end of each epoch.\n                    batch_log (boolean): Enables printing of logs at every batch iteration\n                    save_logs (str): Specifies a filepath in which to permanently save logs at every epoch\n                    visdom_log (VisdomLogger): Logs outputs and metrics to the visdom server\n                    tensorboard_log (str): Logs outputs and metrics to the filepath for visualization in tensorboard\n                    save_architecture (boolean): Saves the architecture as well as weights during model saving\n                    dist: A distribution from torch.distribution, used as the source of the latent vector fed to the generator\n                    real_labels (int): The value to be used for the real images\n                    fake_labels (int): The value to be used for the generated images\n                    num_classes (int): The number of classes for conditional generation\n                    num_samples (int): The number of samples to be generated per class in the conditional setting\n                    """"""\n\n    def train(self,train_loader, gen_optimizer,disc_optimizer,latent_size=100,gen_loss_fn=nn.BCELoss(), disc_loss_fn=nn.BCELoss(), real_labels=1,fake_labels=0,**kwargs):\n\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.real_labels = real_labels\n        self.fake_labels = fake_labels\n\n        super(StandardGanLearner,self).train(train_loader=train_loader,gen_optimizer=gen_optimizer,disc_optimizer=disc_optimizer,\n                                             latent_size=latent_size,relative_mode=False,**kwargs)\n\n\n    def __update_discriminator_loss__(self,x,gen_images,real_preds,gen_preds):\n\n        batch_size = x.size(0)\n        real_labels = torch.randn(batch_size, 1).fill_(self.real_labels)\n        fake_labels = torch.randn(batch_size, 1).fill_(self.fake_labels)\n\n        real_labels = real_labels.cuda() if self.cuda else real_labels\n        fake_labels = fake_labels.cuda() if self.cuda else fake_labels\n\n        real_loss = self.disc_loss_fn(real_preds,real_labels)\n\n        gen_loss = self.gen_loss_fn(gen_preds,fake_labels)\n\n\n        return real_loss + gen_loss\n\n    def __update_generator_loss__(self,x,gen_images,real_preds,gen_preds):\n        batch_size = x.size(0)\n        real_labels = torch.ones(batch_size, 1)\n\n        real_labels = real_labels.cuda() if self.cuda else real_labels\n\n        loss = self.gen_loss_fn(gen_preds,real_labels)\n\n        return loss\n\n\n"""""" Standard GANS that use the relativistic discriminator\n    See Alexia Jolicoeur-Martineau. 2018 (https://arxiv.org/abs/1807.00734)\n    Arguments:\n        gen_model:  the generator module.\n        disc_model:  the discriminator module.\n        use_cuda_if_available: If set to true, training would be done on a gpu if any is available\n""""""\n\nclass RStandardGanLearner(StandardBaseGanLearner):\n    def __init__(self, gen_model, disc_model, use_cuda_if_available=True):\n        super(RStandardGanLearner, self).__init__(gen_model, disc_model, use_cuda_if_available)\n\n    r""""""Training function\n\n                   Args:\n                       train_loader (DataLoader): an instance of DataLoader containing the training set\n                       gen_optimizer (Optimizer): an optimizer for updating parameters of the generator\n                       disc_optimizer (Optimizer): an optimizer for updating parameters of the discriminator\n                       latent_size (int): the size of the latent variable to be fed into the generator\n                       gen_loss_fn: the generator loss function\n                       disc_loss_fn: the generator loss function\n                       num_epochs (int): The maximum number of training epochs\n                       disc_steps (int): The number of times to train the discriminator before training generator\n                       save_models (str): If all, the model is saved at the end of each epoch while the best models based \n                       gen_lr_scheduler (_LRScheduler): Learning rate scheduler for the generator\n                       disc_lr_scheduler (_LRScheduler): Learning rate sheduler for the discriminator\n                           on the test set are also saved in best_models folder\n                           If \'best\', only the best models are saved,  test_loader and test_metrics must be provided\n                       model_dir (str) = a path in which to save the models\n                       save_model_interval (int): saves the models after every n epoch\n                       save_outputs_interval (int): saves sample outputs from the generator after every n iterations\n                       notebook_mode (boolean): Optimizes the progress bar for either jupyter notebooks or consoles\n                       display_metrics (boolean): Enables display of metrics and loss visualizations at the end of each epoch.\n                       save_metrics (boolean): Enables saving of metrics and loss visualizations at the end of each epoch.\n                       batch_log (boolean): Enables printing of logs at every batch iteration\n                       save_logs (str): Specifies a filepath in which to permanently save logs at every epoch\n                       visdom_log (VisdomLogger): Logs outputs and metrics to the visdom server\n                       tensorboard_log (str): Logs outputs and metrics to the filepath for visualization in tensorboard\n                       save_architecture (boolean): Saves the architecture as well as weights during model saving\n                       dist: A distribution from torch.distribution, used as the source of the latent vector fed to the generator\n                       real_labels (int): The value to be used for the real images\n                       fake_labels (int): The value to be used for the generated images\n                       num_classes (int): The number of classes for conditional generation\n                       num_samples (int): The number of samples to be generated per class in the conditional setting\n                       """"""\n\n    def train(self,train_loader, gen_optimizer,disc_optimizer,latent_size=100,gen_loss_fn=nn.BCEWithLogitsLoss(), disc_loss_fn=nn.BCEWithLogitsLoss(),**kwargs):\n\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n\n        super(RStandardGanLearner,self).train(train_loader=train_loader,gen_optimizer=gen_optimizer,disc_optimizer=disc_optimizer,\n                                             latent_size=latent_size,**kwargs)\n\n    def __update_discriminator_loss__(self,x,gen_images,real_preds,gen_preds):\n\n        batch_size = x.size(0)\n        real_labels = torch.ones(batch_size, 1)\n\n        real_labels = real_labels.cuda() if self.cuda else real_labels\n\n        loss = self.disc_loss_fn(real_preds - gen_preds,real_labels)\n\n        return loss\n\n    def __update_generator_loss__(self,x,gen_images,real_preds,gen_preds):\n        batch_size = x.size(0)\n        real_labels = torch.ones(batch_size, 1)\n\n        real_labels = real_labels.cuda() if self.cuda else real_labels\n\n        loss = self.gen_loss_fn(gen_preds - real_preds,real_labels)\n\n        return loss\n\n"""""" Standard GANS that use the average relativistic discriminator\n    See Alexia Jolicoeur-Martineau. 2018 (https://arxiv.org/abs/1807.00734)\n    Arguments:\n        gen_model:  the generator module.\n        disc_model:  the discriminator module.\n        use_cuda_if_available: If set to true, training would be done on a gpu if any is available\n""""""\nclass RAvgStandardGanLearner(StandardBaseGanLearner):\n    def __init__(self, gen_model, disc_model, use_cuda_if_available=True):\n        super(RAvgStandardGanLearner, self).__init__(gen_model, disc_model, use_cuda_if_available)\n\n    r""""""Training function\n\n                   Args:\n                       train_loader (DataLoader): an instance of DataLoader containing the training set\n                       gen_optimizer (Optimizer): an optimizer for updating parameters of the generator\n                       disc_optimizer (Optimizer): an optimizer for updating parameters of the discriminator\n                       latent_size (int): the size of the latent variable to be fed into the generator\n                       gen_loss_fn: the generator loss function\n                       disc_loss_fn: the generator loss function\n                       num_epochs (int): The maximum number of training epochs\n                       disc_steps (int): The number of times to train the discriminator before training generator\n                       save_models (str): If all, the model is saved at the end of each epoch while the best models based \n                       gen_lr_scheduler (_LRScheduler): Learning rate scheduler for the generator\n                       disc_lr_scheduler (_LRScheduler): Learning rate sheduler for the discriminator\n                           on the test set are also saved in best_models folder\n                           If \'best\', only the best models are saved,  test_loader and test_metrics must be provided\n                       model_dir (str) = a path in which to save the models\n                       save_model_interval (int): saves the models after every n epoch\n                       save_outputs_interval (int): saves sample outputs from the generator after every n iterations\n                       notebook_mode (boolean): Optimizes the progress bar for either jupyter notebooks or consoles\n                       display_metrics (boolean): Enables display of metrics and loss visualizations at the end of each epoch.\n                       save_metrics (boolean): Enables saving of metrics and loss visualizations at the end of each epoch.\n                       batch_log (boolean): Enables printing of logs at every batch iteration\n                       save_logs (str): Specifies a filepath in which to permanently save logs at every epoch\n                       visdom_log (VisdomLogger): Logs outputs and metrics to the visdom server\n                       tensorboard_log (str): Logs outputs and metrics to the filepath for visualization in tensorboard\n                       save_architecture (boolean): Saves the architecture as well as weights during model saving\n                       dist: A distribution from torch.distribution, used as the source of the latent vector fed to the generator\n                       real_labels (int): The value to be used for the real images\n                       fake_labels (int): The value to be used for the generated images\n                       num_classes (int): The number of classes for conditional generation\n                       num_samples (int): The number of samples to be generated per class in the conditional setting\n                       """"""\n\n    def train(self,train_loader, gen_optimizer,disc_optimizer,latent_size=100,gen_loss_fn=nn.BCEWithLogitsLoss(), disc_loss_fn=nn.BCEWithLogitsLoss(),**kwargs):\n\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n\n        super(RAvgStandardGanLearner,self).train(train_loader=train_loader,gen_optimizer=gen_optimizer,disc_optimizer=disc_optimizer,\n                                             latent_size=latent_size,**kwargs)\n\n    def __update_discriminator_loss__(self,x,gen_images,real_preds,gen_preds):\n\n        batch_size = x.size(0)\n        real_labels = torch.ones(batch_size, 1)\n        fake_labels = torch.zeros(batch_size, 1)\n\n        real_labels = real_labels.cuda() if self.cuda else real_labels\n        fake_labels = fake_labels.cuda() if self.cuda else fake_labels\n\n        loss = (self.disc_loss_fn(real_preds - torch.mean(gen_preds),real_labels) + self.gen_loss_fn(gen_preds - torch.mean(real_preds),fake_labels))/2\n\n        return loss\n\n    def __update_generator_loss__(self,x,gen_images,real_preds,gen_preds):\n        batch_size = x.size(0)\n        real_labels = torch.ones(batch_size, 1)\n        fake_labels = torch.zeros(batch_size, 1)\n\n        real_labels = real_labels.cuda() if self.cuda else real_labels\n        fake_labels = fake_labels.cuda() if self.cuda else fake_labels\n\n        loss = (self.gen_loss_fn(real_preds - torch.mean(gen_preds), fake_labels) + self.disc_loss_fn(gen_preds - torch.mean(real_preds),real_labels))/2\n\n        return loss\n\n"""""" GANS that use the hinge loss\n    See Ruohan Wang. 2018 (https://arxiv.org/abs/1704.03817)\n    Arguments:\n        gen_model:  the generator module.\n        disc_model:  the discriminator module.\n        use_cuda_if_available: If set to true, training would be done on a gpu if any is available\n""""""\n\nclass HingeGanLearner(StandardBaseGanLearner):\n    def __init__(self, gen_model, disc_model, use_cuda_if_available=True):\n        super(HingeGanLearner, self).__init__(gen_model, disc_model, use_cuda_if_available)\n\n    r""""""Training function\n\n                   Args:\n                       train_loader (DataLoader): an instance of DataLoader containing the training set\n                       gen_optimizer (Optimizer): an optimizer for updating parameters of the generator\n                       disc_optimizer (Optimizer): an optimizer for updating parameters of the discriminator\n                       latent_size (int): the size of the latent variable to be fed into the generator\n                       num_epochs (int): The maximum number of training epochs\n                       disc_steps (int): The number of times to train the discriminator before training generator\n                       save_models (str): If all, the model is saved at the end of each epoch while the best models based \n                       gen_lr_scheduler (_LRScheduler): Learning rate scheduler for the generator\n                       disc_lr_scheduler (_LRScheduler): Learning rate sheduler for the discriminator\n                           on the test set are also saved in best_models folder\n                           If \'best\', only the best models are saved,  test_loader and test_metrics must be provided\n                       model_dir (str) = a path in which to save the models\n                       save_model_interval (int): saves the models after every n epoch\n                       save_outputs_interval (int): saves sample outputs from the generator after every n iterations\n                       notebook_mode (boolean): Optimizes the progress bar for either jupyter notebooks or consoles\n                       display_metrics (boolean): Enables display of metrics and loss visualizations at the end of each epoch.\n                       save_metrics (boolean): Enables saving of metrics and loss visualizations at the end of each epoch.\n                       batch_log (boolean): Enables printing of logs at every batch iteration\n                       save_logs (str): Specifies a filepath in which to permanently save logs at every epoch\n                       visdom_log (VisdomLogger): Logs outputs and metrics to the visdom server\n                       tensorboard_log (str): Logs outputs and metrics to the filepath for visualization in tensorboard\n                       save_architecture (boolean): Saves the architecture as well as weights during model saving\n                       dist: A distribution from torch.distribution, used as the source of the latent vector fed to the generator\n                       num_classes (int): The number of classes for conditional generation\n                       num_samples (int): The number of samples to be generated per class in the conditional setting\n                       """"""\n    def train(self,train_loader, gen_optimizer, disc_optimizer, latent_size, dist=distribution.Normal(0, 1),\n                  num_classes=0, num_samples=5,**kwargs):\n\n        super(HingeGanLearner,self).train(train_loader, gen_optimizer, disc_optimizer, latent_size,False, dist,\n                  num_classes, num_samples,**kwargs)\n\n\n    def __update_discriminator_loss__(self,x,gen_images,real_preds,gen_preds):\n\n        return torch.mean(F.relu(1-real_preds)) + torch.mean(F.relu(1+gen_preds))\n\n    def __update_generator_loss__(self,x,gen_images,real_preds,gen_preds):\n\n        return -torch.mean(gen_preds)\n\n"""""" Hinge GANS that use the relativistic discriminator\n    See Alexia Jolicoeur-Martineau. 2018 (https://arxiv.org/abs/1807.00734)\n    Arguments:\n        gen_model:  the generator module.\n        disc_model:  the discriminator module.\n        use_cuda_if_available: If set to true, training would be done on a gpu if any is available\n""""""\nclass RHingeGanLearner(StandardBaseGanLearner):\n    def __init__(self, gen_model, disc_model, use_cuda_if_available=True):\n        super(RHingeGanLearner, self).__init__(gen_model, disc_model, use_cuda_if_available)\n\n    r""""""Training function\n\n                       Args:\n                           train_loader (DataLoader): an instance of DataLoader containing the training set\n                           gen_optimizer (Optimizer): an optimizer for updating parameters of the generator\n                           disc_optimizer (Optimizer): an optimizer for updating parameters of the discriminator\n                           latent_size (int): the size of the latent variable to be fed into the generator\n                           num_epochs (int): The maximum number of training epochs\n                           disc_steps (int): The number of times to train the discriminator before training generator\n                           save_models (str): If all, the model is saved at the end of each epoch while the best models based \n                           gen_lr_scheduler (_LRScheduler): Learning rate scheduler for the generator\n                           disc_lr_scheduler (_LRScheduler): Learning rate sheduler for the discriminator\n                               on the test set are also saved in best_models folder\n                               If \'best\', only the best models are saved,  test_loader and test_metrics must be provided\n                           model_dir (str) = a path in which to save the models\n                           save_model_interval (int): saves the models after every n epoch\n                           save_outputs_interval (int): saves sample outputs from the generator after every n iterations\n                           notebook_mode (boolean): Optimizes the progress bar for either jupyter notebooks or consoles\n                           display_metrics (boolean): Enables display of metrics and loss visualizations at the end of each epoch.\n                           save_metrics (boolean): Enables saving of metrics and loss visualizations at the end of each epoch.\n                           batch_log (boolean): Enables printing of logs at every batch iteration\n                           save_logs (str): Specifies a filepath in which to permanently save logs at every epoch\n                           visdom_log (VisdomLogger): Logs outputs and metrics to the visdom server\n                           tensorboard_log (str): Logs outputs and metrics to the filepath for visualization in tensorboard\n                           save_architecture (boolean): Saves the architecture as well as weights during model saving\n                           dist: A distribution from torch.distribution, used as the source of the latent vector fed to the generator\n                           num_classes (int): The number of classes for conditional generation\n                           num_samples (int): The number of samples to be generated per class in the conditional setting\n                           """"""\n\n    def train(self,train_loader, gen_optimizer, disc_optimizer, latent_size,dist=distribution.Normal(0, 1),\n                  num_classes=0, num_samples=5,**kwargs):\n\n        super(RHingeGanLearner,self).train(train_loader, gen_optimizer, disc_optimizer, latent_size,True, dist,\n                  num_classes, num_samples,**kwargs)\n\n    def __update_discriminator_loss__(self,x,gen_images,real_preds,gen_preds):\n\n        return torch.mean(F.relu(1-real_preds)) + torch.mean(F.relu(1+gen_preds))\n\n    def __update_generator_loss__(self,x,gen_images,real_preds,gen_preds):\n\n        return torch.mean(F.relu(1+real_preds)) + torch.mean(F.relu(1-gen_preds))\n\n"""""" Hinge GANS that use the average relativistic discriminator\n    See Alexia Jolicoeur-Martineau. 2018 (https://arxiv.org/abs/1807.00734)\n    Arguments:\n        gen_model:  the generator module.\n        disc_model:  the discriminator module.\n        use_cuda_if_available: If set to true, training would be done on a gpu if any is available\n""""""\n\nclass RAvgHingeGanLearner(StandardBaseGanLearner):\n    def __init__(self, gen_model, disc_model, use_cuda_if_available=True):\n        super(RAvgHingeGanLearner, self).__init__(gen_model, disc_model, use_cuda_if_available)\n\n    r""""""Training function\n\n                       Args:\n                           train_loader (DataLoader): an instance of DataLoader containing the training set\n                           gen_optimizer (Optimizer): an optimizer for updating parameters of the generator\n                           disc_optimizer (Optimizer): an optimizer for updating parameters of the discriminator\n                           latent_size (int): the size of the latent variable to be fed into the generator\n                           num_epochs (int): The maximum number of training epochs\n                           disc_steps (int): The number of times to train the discriminator before training generator\n                           save_models (str): If all, the model is saved at the end of each epoch while the best models based \n                           gen_lr_scheduler (_LRScheduler): Learning rate scheduler for the generator\n                           disc_lr_scheduler (_LRScheduler): Learning rate sheduler for the discriminator\n                               on the test set are also saved in best_models folder\n                               If \'best\', only the best models are saved,  test_loader and test_metrics must be provided\n                           model_dir (str) = a path in which to save the models\n                           save_model_interval (int): saves the models after every n epoch\n                           save_outputs_interval (int): saves sample outputs from the generator after every n iterations\n                           notebook_mode (boolean): Optimizes the progress bar for either jupyter notebooks or consoles\n                           display_metrics (boolean): Enables display of metrics and loss visualizations at the end of each epoch.\n                           save_metrics (boolean): Enables saving of metrics and loss visualizations at the end of each epoch.\n                           batch_log (boolean): Enables printing of logs at every batch iteration\n                           save_logs (str): Specifies a filepath in which to permanently save logs at every epoch\n                           visdom_log (VisdomLogger): Logs outputs and metrics to the visdom server\n                           tensorboard_log (str): Logs outputs and metrics to the filepath for visualization in tensorboard\n                           save_architecture (boolean): Saves the architecture as well as weights during model saving\n                           dist: A distribution from torch.distribution, used as the source of the latent vector fed to the generator\n                           num_classes (int): The number of classes for conditional generation\n                           num_samples (int): The number of samples to be generated per class in the conditional setting\n                           """"""\n\n    def train(self,train_loader, gen_optimizer, disc_optimizer, latent_size, dist=distribution.Normal(0, 1),\n                  num_classes=0, num_samples=5,**kwargs):\n\n        super(RAvgHingeGanLearner,self).train(train_loader, gen_optimizer, disc_optimizer, latent_size,True, dist,\n                  num_classes, num_samples,**kwargs)\n\n    def __update_discriminator_loss__(self,x,gen_images,real_preds,gen_preds):\n\n        return (torch.mean(F.relu(1-(real_preds - torch.mean(gen_preds)))) + torch.mean(F.relu(1+(gen_preds - torch.mean(real_preds)))))/2\n\n    def __update_generator_loss__(self,x,gen_images,real_preds,gen_preds):\n        return (torch.mean(F.relu(1 + (real_preds - torch.mean(gen_preds)))) + torch.mean(\n            F.relu(1 - (gen_preds - torch.mean(real_preds))))) / 2\n\n"""""" GANS that use the Improved Wasserstein Distance\n    See Gulrajani et al. 2017 (https://arxiv.org/1704.00028)\n    based on earlier work by Arjovsky et al. 2017 (https://arxiv.org/1701.07875)\n    Arguments:\n        gen_model:  the generator module.\n        disc_model:  the discriminator module.\n        use_cuda_if_available: If set to true, training would be done on a gpu if any is available\n""""""\n\nclass WGanLearner(BaseGanCore):\n\n    r""""""Training function\n\n                       Args:\n                           train_loader (DataLoader): an instance of DataLoader containing the training set\n                           gen_optimizer (Optimizer): an optimizer for updating parameters of the generator\n                           disc_optimizer (Optimizer): an optimizer for updating parameters of the discriminator\n                           latent_size (int): the size of the latent variable to be fed into the generator\n                           num_epochs (int): The maximum number of training epochs\n                           disc_steps (int): The number of times to train the discriminator before training generator\n                           save_models (str): If all, the model is saved at the end of each epoch while the best models based \n                           gen_lr_scheduler (_LRScheduler): Learning rate scheduler for the generator\n                           disc_lr_scheduler (_LRScheduler): Learning rate sheduler for the discriminator\n                               on the test set are also saved in best_models folder\n                               If \'best\', only the best models are saved,  test_loader and test_metrics must be provided\n                           model_dir (str) = a path in which to save the models\n                           save_model_interval (int): saves the models after every n epoch\n                           save_outputs_interval (int): saves sample outputs from the generator after every n iterations\n                           notebook_mode (boolean): Optimizes the progress bar for either jupyter notebooks or consoles\n                           display_metrics (boolean): Enables display of metrics and loss visualizations at the end of each epoch.\n                           save_metrics (boolean): Enables saving of metrics and loss visualizations at the end of each epoch.\n                           batch_log (boolean): Enables printing of logs at every batch iteration\n                           save_logs (str): Specifies a filepath in which to permanently save logs at every epoch\n                           visdom_log (VisdomLogger): Logs outputs and metrics to the visdom server\n                           tensorboard_log (str): Logs outputs and metrics to the filepath for visualization in tensorboard\n                           save_architecture (boolean): Saves the architecture as well as weights during model saving\n                           dist: A distribution from torch.distribution, used as the source of the latent vector fed to the generator\n                           num_classes (int): The number of classes for conditional generation\n                           num_samples (int): The number of samples to be generated per class in the conditional setting\n                           """"""\n\n    def train(self,train_loader, gen_optimizer,disc_optimizer,latent_size, dist=distribution.Normal(0,1),\n                 num_classes=0, num_samples=5,lambda_ = 0.25, **kwargs):\n\n        self.lambda_ = lambda_\n        self.latent_size = latent_size\n        self.dist = dist\n\n        self.conditional = (num_classes is not None)\n        self.classes = num_classes\n        self.num_samples = num_samples\n\n        super().__train_loop__(train_loader,gen_optimizer,disc_optimizer,**kwargs)\n\n    def __disc_train_func__(self, data):\n\n        super().__disc_train_func__(data)\n\n        self.disc_optimizer.zero_grad()\n\n        if isinstance(data, list) or isinstance(data, tuple):\n            x = data[0]\n            if self.conditional:\n                class_labels = data[1]\n        else:\n            x = data\n\n        batch_size = x.size(0)\n        if isinstance(self.latent_size, int):\n            source_size = list([self.latent_size])\n        else:\n            source_size = list(self.latent_size)\n        source_size = [batch_size] + source_size\n\n        source = self.dist.sample(tuple(source_size))\n\n        if self.cuda:\n            x = x.cuda()\n            source = source.cuda()\n\n            if self.conditional:\n                class_labels = class_labels.cuda()\n\n       \n        if self.conditional:\n            class_labels = class_labels.unsqueeze(1) if len(class_labels.size()) == 1 else class_labels\n\n        if self.conditional:\n            outputs = self.disc_model(x, class_labels)\n        else:\n            outputs = self.disc_model(x)\n\n        if self.conditional:\n            random_labels = torch.from_numpy(np.random.randint(0, self.classes, size=(batch_size, 1))).long()\n            if self.cuda:\n                random_labels = random_labels.cuda()\n\n            generated = self.gen_model(source, random_labels)\n            gen_outputs = self.disc_model(generated.detach(), random_labels)\n\n        else:\n            generated = self.gen_model(source)\n            gen_outputs = self.disc_model(generated.detach())\n\n        gen_loss = torch.mean(gen_outputs)\n\n        real_loss = -torch.mean(outputs)\n\n        eps = torch.randn(x.size()).uniform_(0, 1)\n\n        if self.cuda:\n            eps = eps.cuda()\n\n        x__ = Variable(eps * x.data + (1.0 - eps) * generated.detach().data, requires_grad=True)\n\n        if self.conditional:\n            pred__ = self.disc_model(x__,class_labels)\n        else:\n            pred__ = self.disc_model(x__)\n\n        grad_outputs = torch.ones(pred__.size())\n\n        if self.cuda:\n            grad_outputs = grad_outputs.cuda()\n\n        gradients = grad(outputs=pred__, inputs=x__, grad_outputs=grad_outputs, create_graph=True, retain_graph=True,\n                         only_inputs=True)[0]\n\n        gradient_penalty = self.lambda_ * ((gradients.view(gradients.size(0), -1).norm(2, 1) - 1) ** 2).mean()\n\n        loss = gen_loss + real_loss + gradient_penalty\n        if self.fp16_mode:\n            self.disc_optimizer.backward(loss)\n        else:\n            loss.backward()\n        self.disc_optimizer.step()\n        self.disc_running_loss = self.disc_running_loss + (loss.cpu().item() * batch_size)\n\n    def __gen_train_func__(self, data):\n\n        super().__gen_train_func__(data)\n\n        self.gen_optimizer.zero_grad()\n\n        if isinstance(data, list) or isinstance(data, tuple):\n            x = data[0]\n            if self.conditional:\n                class_labels = data[1].unsqueeze(1) if len(data[1].size()) == 1 else data[1]\n        else:\n            x = data\n        batch_size = x.size(0)\n\n        if isinstance(self.latent_size, int):\n            source_size = list([self.latent_size])\n        else:\n            source_size = list(self.latent_size)\n\n        source_size = [batch_size] + source_size\n\n        source = self.dist.sample(tuple(source_size))\n        if self.conditional:\n            random_labels = torch.from_numpy(np.random.randint(0, self.classes, size=(batch_size, 1))).long()\n            if self.cuda:\n                random_labels = random_labels.cuda()\n\n            random_labels = random_labels\n\n        if self.cuda:\n            source = source.cuda()\n\n            x = x.cuda()\n            if self.conditional:\n                class_labels = class_labels.cuda()\n\n        if self.conditional:\n            fake_images = self.gen_model(source, random_labels)\n            outputs = self.disc_model(fake_images, random_labels)\n\n        else:\n            fake_images = self.gen_model(source)\n            outputs = self.disc_model(fake_images)\n\n\n        loss = -torch.mean(outputs)\n\n        if self.fp16_mode:\n            self.gen_optimizer.backward(loss)\n        else:\n            loss.backward()\n\n        self.gen_optimizer.step()\n\n        self.gen_running_Loss = self.gen_running_loss + (loss.cpu().item() * batch_size)\n\n\n\n'"
torchfusion/lang/datasets/__init__.py,0,"b'from .datasets import csv_data_loader,load_tabular_set_split,load_tabular_set,csv_data,csv_data_split,csv_data_split_loader,json_data,json_data_loader,json_data_split,json_data_split_loader,tsv_data,tsv_data_loader,tsv_data_split,tsv_data_split_loader'"
torchfusion/lang/datasets/datasets.py,0,"b'from torchtext.data import TabularDataset,BucketIterator\nimport json\nimport os\n\n\n\ndef load_tabular_set(file_path,format,fields,split_ratio=None,split_seed=None,skip_header=False,save_vocab_path=os.getcwd(),**args):\n\n    """"""\n\n    :param file_path:\n    :param format:\n    :param fields:\n    :param split_ratio:\n    :param split_seed:\n    :param skip_header:\n    :param save_vocab_path:\n    :param args:\n    :return:\n    """"""\n    if os.path.exists(save_vocab_path) == False:\n        os.mkdir(save_vocab_path)\n\n    dataset_fields = []\n\n    for field in fields:\n        dataset_fields.append((field.name,field.field))\n\n    dataset = TabularDataset(file_path,format,dataset_fields,skip_header=skip_header,**args)\n\n    for f_input in fields:\n        name = f_input.name\n        field = f_input.field\n        vocab = f_input.vocab\n\n        if vocab is None:\n\n            field.build_vocab(dataset,max_size=f_input.max_size, min_freq=f_input.min_freq,\n                 vectors=f_input.vectors, unk_init=f_input.unk_init, vectors_cache=f_input.vectors_cache)\n\n            with open(os.path.join(save_vocab_path,""{}.json"".format(name)), ""w"") as jfile:\n                json.dump(field.vocab.stoi,jfile,sort_keys=True)\n\n        else:\n            with open(vocab, ""r"") as jfile:\n                dict_ = json.load(jfile)\n\n                field.build_vocab()\n                field.vocab.stoi = dict_\n\n\n\n    if split_ratio is not None:\n\n        dataset = dataset.split(split_ratio,random_state=split_seed)\n\n    return dataset\n\n\n\n\ndef load_tabular_set_split(root_path,format,fields,train=None,val=None,test=None,skip_header=False,save_vocab_path=os.getcwd(),**args):\n    """"""\n\n    :param root_path:\n    :param format:\n    :param fields:\n    :param train:\n    :param val:\n    :param test:\n    :param skip_header:\n    :param save_vocab_path:\n    :param args:\n    :return:\n    """"""\n    if os.path.exists(save_vocab_path) == False:\n        os.mkdir(save_vocab_path)\n\n    dataset_fields = []\n\n    for field in fields:\n        dataset_fields.append((field.name,field.field))\n    print(dataset_fields)\n    dataset = TabularDataset.splits(root_path,"".data"",train,val,test,fields=dataset_fields,skip_header=skip_header,format=format,**args)\n\n    for f_input in fields:\n        name = f_input.name\n        field = f_input.field\n        vocab = f_input.vocab\n\n        if vocab is None:\n            #verify if working properly\n            field.build_vocab(*dataset,max_size=f_input.max_size, min_freq=f_input.min_freq,\n                 vectors=f_input.vectors, unk_init=f_input.unk_init, vectors_cache=f_input.vectors_cache)\n\n            with open(os.path.join(save_vocab_path,""{}.json"".format(name)), ""w"") as jfile:\n                json.dump(field.vocab.stoi,jfile,sort_keys=True)\n\n        else:\n            with open(vocab, ""r"") as jfile:\n                dict_ = json.load(jfile)\n                field.build_vocab()\n                field.vocab.stoi = dict_\n\n\n    return dataset\n\n\ndef csv_data(file_path,fields,split_ratio=None,split_seed=None,skip_header=False,save_vocab_path=os.getcwd(),**args):\n    """"""\n\n    :param file_path:\n    :param fields:\n    :param split_ratio:\n    :param split_seed:\n    :param skip_header:\n    :param save_vocab_path:\n    :param args:\n    :return:\n    """"""\n\n    return load_tabular_set(file_path,""csv"",fields=fields,split_ratio=split_ratio,split_seed=split_seed,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n\ndef csv_data_split(root_path,fields,train,val=None,test=None,skip_header=False,save_vocab_path=os.getcwd(),**args):\n    """"""\n\n    :param root_path:\n    :param fields:\n    :param train:\n    :param val:\n    :param test:\n    :param skip_header:\n    :param save_vocab_path:\n    :param args:\n    :return:\n    """"""\n    return load_tabular_set_split(root_path,""csv"",fields=fields,train=train,val=val,test=test,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n\ndef tsv_data(file_path,fields,split_ratio=None,split_seed=None,skip_header=False,save_vocab_path=os.getcwd(),**args):\n    """"""\n\n    :param file_path:\n    :param fields:\n    :param split_ratio:\n    :param split_seed:\n    :param skip_header:\n    :param save_vocab_path:\n    :param args:\n    :return:\n    """"""\n\n    return load_tabular_set(file_path,""tsv"",fields=fields,split_ratio=split_ratio,split_seed=split_seed,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n\ndef tsv_data_split(root_path,fields,train,val=None,test=None,skip_header=False,save_vocab_path=os.getcwd(),**args):\n    """"""\n\n    :param root_path:\n    :param fields:\n    :param train:\n    :param val:\n    :param test:\n    :param skip_header:\n    :param save_vocab_path:\n    :param args:\n    :return:\n    """"""\n    return load_tabular_set_split(root_path,""tsv"",fields=fields,train=train,val=val,test=test,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n\n\ndef json_data(file_path,fields,split_ratio=None,split_seed=None,skip_header=False,save_vocab_path=os.getcwd(),**args):\n    """"""\n\n    :param file_path:\n    :param fields:\n    :param split_ratio:\n    :param split_seed:\n    :param skip_header:\n    :param save_vocab_path:\n    :param args:\n    :return:\n    """"""\n\n    return load_tabular_set(file_path,""json"",fields=fields,split_ratio=split_ratio,split_seed=split_seed,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n\ndef json_data_split(root_path,fields,train,val=None,test=None,skip_header=False,save_vocab_path=os.getcwd(),**args):\n    """"""\n\n    :param root_path:\n    :param fields:\n    :param train:\n    :param val:\n    :param test:\n    :param skip_header:\n    :param save_vocab_path:\n    :param args:\n    :return:\n    """"""\n    return load_tabular_set_split(root_path,""json"",fields=fields,train=train,val=val,test=test,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n\n\n\ndef csv_data_loader(file_path,fields,split_ratio=None,split_seed=None,skip_header=False,save_vocab_path=os.getcwd(),batch_size=32,device=None,train=True,**args):\n    """"""\n\n    :param file_path:\n    :param fields:\n    :param split_ratio:\n    :param split_seed:\n    :param skip_header:\n    :param save_vocab_path:\n    :param batch_size:\n    :param device:\n    :param train:\n    :param args:\n    :return:\n    """"""\n    dataset = load_tabular_set(file_path,""csv"",fields=fields,split_ratio=split_ratio,split_seed=split_seed,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n    return BucketIterator(dataset,batch_size=batch_size,device=device,train=True,shuffle=train,repeat=False)\n\ndef csv_data_split_loader(root_path,fields,train=None,val=None,test=None,skip_header=False,save_vocab_path=os.getcwd(),batch_size=32,device=None,**args):\n    """"""\n\n    :param root_path:\n    :param fields:\n    :param train:\n    :param val:\n    :param test:\n    :param skip_header:\n    :param save_vocab_path:\n    :param batch_size:\n    :param device:\n    :param args:\n    :return:\n    """"""\n    dataset = load_tabular_set_split(root_path,""csv"",fields=fields,train=train,val=val, test=test,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n    return BucketIterator(dataset, batch_size=batch_size, device=device, train=True, shuffle=train,repeat=False)\n\ndef tsv_data_loader(file_path,fields,split_ratio=None,split_seed=None,skip_header=False,save_vocab_path=os.getcwd(),batch_size=32,device=None,train=True,**args):\n    """"""\n\n    :param file_path:\n    :param fields:\n    :param split_ratio:\n    :param split_seed:\n    :param skip_header:\n    :param save_vocab_path:\n    :param batch_size:\n    :param device:\n    :param train:\n    :param args:\n    :return:\n    """"""\n    dataset = load_tabular_set(file_path,""tsv"",fields=fields,split_ratio=split_ratio,split_seed=split_seed,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n    return BucketIterator(dataset, batch_size=batch_size, device=device, train=True, shuffle=train,repeat=False)\n\ndef tsv_data_split_loader(root_path,fields,train=None,val=None,test=None,skip_header=False,save_vocab_path=os.getcwd(),batch_size=32,device=None,**args):\n    """"""\n\n    :param root_path:\n    :param fields:\n    :param train:\n    :param val:\n    :param test:\n    :param skip_header:\n    :param save_vocab_path:\n    :param batch_size:\n    :param device:\n    :param args:\n    :return:\n    """"""\n    dataset = load_tabular_set_split(root_path,""tsv"",fields=fields,train=train,val=val,test=test,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n    return BucketIterator(dataset, batch_size=batch_size, device=device, train=True, shuffle=train,repeat=False)\n\ndef json_data_loader(file_path,fields,split_ratio=None,split_seed=None,skip_header=False,save_vocab_path=os.getcwd(),batch_size=32,device=None,train=True,**args):\n    """"""\n\n    :param file_path:\n    :param fields:\n    :param split_ratio:\n    :param split_seed:\n    :param skip_header:\n    :param save_vocab_path:\n    :param batch_size:\n    :param device:\n    :param train:\n    :param args:\n    :return:\n    """"""\n    dataset = load_tabular_set(file_path,""json"",fields=fields,split_ratio=split_ratio,split_seed=split_seed,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n    return BucketIterator(dataset, batch_size=batch_size, device=device, train=True, shuffle=train,repeat=False)\n\ndef json_data_split_loader(root_path,fields,train=None,val=None,test=None,skip_header=False,save_vocab_path=os.getcwd(),batch_size=32,device=None,**args):\n    """"""\n\n    :param root_path:\n    :param fields:\n    :param train:\n    :param val:\n    :param test:\n    :param skip_header:\n    :param save_vocab_path:\n    :param batch_size:\n    :param device:\n    :param args:\n    :return:\n    """"""\n    dataset = load_tabular_set_split(root_path,""json"",fields=fields,train=train,val=val,test=test,skip_header=skip_header,save_vocab_path=save_vocab_path,**args)\n    return BucketIterator(dataset, batch_size=batch_size, device=device, train=True, shuffle=train,repeat=False)\n'"
