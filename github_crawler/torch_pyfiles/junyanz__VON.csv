file_path,api_count,code
test.py,0,"b""import os\nfrom os.path import join\nfrom options.test_options import TestOptions\nfrom data import create_dataset\nfrom models import create_model\nfrom util.visualizer import save_images, convert_image\nfrom util.util_voxel import save_vox_to_obj, render\nfrom util import html\nfrom tqdm import tqdm\n\n# options\nopt = TestOptions().parse()\nopt.num_threads = 0\nopt.serial_batches = True  # no shuffle\nopt.batch_size = 1  # force to be 1\nuse_df = opt.use_df or opt.dataset_mode.find('df') >= 0\n# create dataset\ndataset = create_dataset(opt)\nmodel = create_model(opt)\nmodel.setup(opt)\nmodel.netG_3D.eval()\nmodel.set_posepool(dataset.dataset.datasets[0].get_posepool())\nprint('Loading model %s' % opt.model)\n\n# create website\nweb_dir = os.path.join(opt.results_dir, '{:s}_views{}_shape{}_r{}'.format(opt.name, opt.n_views, opt.n_shapes, opt.random_view))\nwebpage = html.HTML(web_dir, 'Training = %s, %s, 2D = %s, 3D = %s' % (opt.name, opt.phase, opt.model2D_dir, opt.model3D_dir))\nmodel_path = os.path.join(web_dir, 'images')\ncount = 0\n\nprog_bar = tqdm(total=opt.n_shapes)\n\nwhile (True):\n    if count == opt.n_shapes:\n        break\n    for n, data in enumerate(dataset):\n        if count == opt.n_shapes:\n            break\n        count += 1\n        model.reset_shape(opt.reset_shape and not opt.real_shape)\n        model.reset_texture(opt.reset_texture and not opt.real_texture)\n        model.set_input(data, opt.reset_shape and opt.real_shape, opt.reset_texture and opt.real_texture)\n        # model.eval_rec()\n        if not opt.real_shape:\n            model.sample_3d()\n        all_images, all_names = [], []\n        if opt.render_25d:\n            all_depths, all_depth_names = [], []\n            all_masks, all_mask_names = [], []\n\n        if opt.show_input:\n            input_real = convert_image(model.input_B)\n            all_images.append(input_real)\n            all_names.append('real')\n\n        for k in range(opt.n_views):\n            model.reset_view(reset=True)\n            image, depth, mask = model.sample_2d(view_id=k, extra=True)\n            image_np = convert_image(image)\n            all_images.append(image_np)\n            all_names.append('view_{:03d}'.format(k))\n            if opt.render_25d:\n                depth_np = convert_image(depth)\n                mask_np = convert_image(mask)\n                all_depths.append(depth_np)\n                all_masks.append(mask_np)\n                all_depth_names.append('depth_{:03d}'.format(k))\n                all_mask_names.append('mask_{:03d}'.format(k))\n\n        if opt.render_3d:\n            obj_name = join(model_path, 'shape%03d.obj' % (count))\n            save_vox_to_obj(model.voxel.data.cpu().numpy(), 0.5 if not use_df else 0.85, obj_name)\n            render_prefix = join(model_path, 'shape{:03d}'.format(count))\n            render(obj_name, model.views, render_prefix, 512)\n\n        img_path = 'shape{:03d}'.format(count)\n        model.count += 1\n        save_images(webpage, all_images, all_names, img_path, None,\n                    width=opt.crop_size, aspect_ratio=opt.aspect_ratio)\n        if opt.render_25d:\n            save_images(webpage, all_depths, all_depth_names, img_path, None,\n                        width=opt.crop_size, aspect_ratio=opt.aspect_ratio)\n            save_images(webpage, all_masks, all_mask_names, img_path, None,\n                        width=opt.crop_size, aspect_ratio=opt.aspect_ratio)\n        webpage.save()\n        prog_bar.update(1)\n\n    webpage.save()\n"""
test_shape.py,1,"b""from options.test_options import TestOptions\nfrom models import create_model\nfrom os.path import join\nfrom skimage import measure\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nimport torch\nfrom util.util_voxel import render\n\n\ndef save_obj(vertices, faces, name='model'):\n    with open(name + '.obj', 'w') as f:\n        for v in vertices:\n            f.write('v {} {} {}\\n'.format(v[0] - 0.5, v[1] - 0.5, v[2] - 0.5))\n        for face in faces:\n            f.write('f {} {} {}\\n'.format(\n                face[0] + 1, face[1] + 1, face[2] + 1))\n\n\nopt = TestOptions().parse()\nopt.df_sigma = 8.0\nopt.use_df = opt.use_df or 'df' in opt.checkpoints_dir\nmodel = create_model(opt)\n# model.setup(opt)\nmodel.eval()\nnet_path = join(opt.checkpoints_dir, '%s_net_G_3D.pth' % opt.epoch)\nnet_dict = torch.load(net_path)\nmodel.netG_3D.module.load_state_dict(net_dict)\n\nsampled_shapes, interp_traj = model.sample(k=opt.n_shapes, interp_traj=2 if opt.interp_shape else 0, step=10)\nresult_root = join(opt.checkpoints_dir, 'test_epoch_%s' % opt.epoch)\nos.makedirs(result_root, exist_ok=True)\nshape_dir = join(result_root, 'sampled_shapes')\nos.makedirs(shape_dir, exist_ok=True)\nnp.savez(join(shape_dir, 'sample_shapes'), df=sampled_shapes)\nspace = 1.0 / float(opt.voxel_res)\nif not opt.use_df:\n    opt.ios_th = 0.5\nprint('thresholding = %f' % opt.ios_th)\nviews = np.zeros([6, 2])\nviews[:, 0] = 30\nviews[:, 1] = np.linspace(0, 360, 6, endpoint=False)\nfor idx, s in enumerate(tqdm(sampled_shapes)):\n    output = -np.log(s) / opt.df_sigma if opt.use_df else s\n    v, f, n, _ = measure.marching_cubes_lewiner(output, opt.ios_th, spacing=(space, space, space))\n    save_obj(v, f, join(shape_dir, '%04d' % idx))\n    if opt.render_3d:\n        render(join(shape_dir, '%04d.obj' % idx), views, '%04d' % idx, 512)\n\nif opt.interp_shape:\n    traj_dir = join(result_root, 'sampled_interpolation')\n    os.makedirs(traj_dir, exist_ok=True)\n    for idx, traj in enumerate(tqdm(interp_traj)):\n        save_dir = join(traj_dir, 'traj_%04d' % idx)\n        os.makedirs(save_dir, exist_ok=True)\n        for step, s in enumerate(traj):\n            output = -np.log(s) / opt.df_sigma if opt.use_df else s\n            v, f, n, _ = measure.marching_cubes_lewiner(output, opt.ios_th, spacing=(space, space, space))\n            save_obj(v, f, join(save_dir, 'step_%04d' % step))\n            if opt.render_3d:\n                render(join(shape_dir, 'step_%04d.obj' % idx), views, 'step_%04d' % idx, 512)\n"""
train.py,1,"b""# import torch.backends.cudnn as cudnn\nimport time\nfrom options.train_options import TrainOptions\nfrom data import create_dataset\nfrom models import create_model\nfrom util.visualizer import Visualizer\n\nopt = TrainOptions().parse()  # set CUDA_VISIBLE_DEVICES before import torch\ndataset = create_dataset(opt)\ndataset_size = len(dataset)\nprint('#training data = %d' % dataset_size)\nmodel = create_model(opt)\nmodel.setup(opt)\nvisualizer = Visualizer(opt)\ntotal_steps = 0\n\nfor epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n    epoch_start_time = time.time()\n    save_result = True\n    iter_data_time = time.time()\n\n    for i, data in enumerate(dataset):\n        iter_start_time = time.time()\n        total_steps += opt.batch_size\n        epoch_iter = total_steps - dataset_size * (epoch - opt.epoch_count)\n        model.set_input(data)\n        if model.skip():\n            continue\n        model.update_G()\n        model.update_D()\n        model.check_nan_inf()\n\n        if save_result or total_steps % opt.display_freq == 0:\n            save_result = save_result or total_steps % opt.update_html_freq == 0\n            if model.visual_names:\n                visualizer.display_current_results(model.get_current_visuals(), epoch, ncols=2, save_result=save_result)\n            save_result = False\n\n        if total_steps % opt.print_freq == 0:\n            losses = model.get_current_losses()\n            t_model = time.time() - iter_start_time\n            t_data = iter_start_time - iter_data_time\n            visualizer.print_current_losses(epoch, epoch_iter, losses, t_model, t_data)\n            visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, opt, losses)\n            model.clear_running_mean()\n\n        if total_steps % opt.save_latest_freq == 0:\n            print('saving the latest model (epoch %d, total_steps %d)' %\n                  (epoch, total_steps))\n            model.save_networks('latest')\n\n        iter_data_time = time.time()\n    if epoch % opt.save_epoch_freq == 0:\n        print('saving the model at the end of epoch %d, iters %d' %\n              (epoch, total_steps))\n        model.save_networks('latest')\n        model.save_networks(epoch)\n\n    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n          (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n\n    model.update_learning_rate()\n"""
data/__init__.py,2,"b'""""""This package includes all the modules related to data loading and preprocessing\n To add a custom dataset class called \'dummy\', you need to add a file called \'dummy_dataset.py\' and define a subclass \'DummyDataset\' inherited from BaseDataset.\n You need to implement four functions:\n    -- <__init__>:                      initialize the class, first call BaseDataset.__init__(self, opt).\n    -- <__len__>:                       return the size of dataset.\n    -- <__getitem__>:                   get a data point from data loader.\n    -- <modify_commandline_options>:    (optionally) add dataset-specific options and set default options.\nNow you can use the dataset class by specifying flag \'--dataset_mode dummy\'.\nSee our template dataset class \'template_dataset.py\' for more details.\n""""""\nimport importlib\nimport torch.utils.data\nfrom data.base_dataset import BaseDataset\n\n\ndef find_dataset_using_name(dataset_name):\n    """"""Import the module ""data/[dataset_name]_dataset.py"".\n    In the file, the class called DatasetNameDataset() will\n    be instantiated. It has to be a subclass of BaseDataset,\n    and it is case-insensitive.\n    """"""\n    dataset_filename = ""data."" + dataset_name + ""_dataset""\n    datasetlib = importlib.import_module(dataset_filename)\n\n    dataset = None\n    target_dataset_name = dataset_name.replace(\'_\', \'\') + \'dataset\'\n    for name, cls in datasetlib.__dict__.items():\n        if name.lower() == target_dataset_name.lower() \\\n           and issubclass(cls, BaseDataset):\n            dataset = cls\n\n    if dataset is None:\n        raise NotImplementedError(""In %s.py, there should be a subclass of BaseDataset with class name that matches %s in lowercase."" % (dataset_filename, target_dataset_name))\n\n    return dataset\n\n\ndef get_option_setter(dataset_name):\n    """"""Return the static method <modify_commandline_options> of the dataset class.""""""\n    dataset_class = find_dataset_using_name(dataset_name)\n    return dataset_class.modify_commandline_options\n\n\ndef create_dataset(opt):\n    """"""Create a dataset given the option.\n    This function wraps the class CustomDatasetDataLoader.\n        This is the main interface between this package and \'train.py\'/\'test.py\'\n    Example:\n        >>> from data import create_dataset\n        >>> dataset = create_dataset(opt)\n    """"""\n    data_loader = CustomDatasetDataLoader(opt)\n    dataset = data_loader.load_data()\n    return dataset\n\n\nclass CustomDatasetDataLoader():\n    """"""Wrapper class of Dataset class that performs multi-threaded data loading""""""\n\n    def __init__(self, opt):\n        """"""Initialize this class\n        Step 1: create a dataset instance given the name [dataset_mode]\n        Step 2: create a multi-threaded data loader.\n        """"""\n        self.opt = opt\n        dataset_class = find_dataset_using_name(opt.dataset_mode)\n        self.dataset = dataset_class(opt)\n        print(""dataset [%s] was created"" % type(self.dataset).__name__)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batch_size,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.num_threads))\n\n    def load_data(self):\n        return self\n\n    def __len__(self):\n        """"""Return the number of data in the dataset""""""\n        return min(len(self.dataset), self.opt.max_dataset_size)\n\n    def __iter__(self):\n        """"""Return a batch of data""""""\n        for i, data in enumerate(self.dataloader):\n            if i * self.opt.batch_size >= self.opt.max_dataset_size:\n                break\n            yield data\n'"
data/base_dataset.py,1,"b'""""""This module implements an abstract base class (ABC) \'BaseDataset\' for datasets.\nIt also includes common transformation functions (e.g., get_transform, __scale_width), which can be later used in subclasses.\n""""""\nimport torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom abc import ABC, abstractmethod\nimport numpy as np\n\n\nclass BaseDataset(data.Dataset, ABC):\n    """"""This class is an abstract base class (ABC) for datasets.\n    To create a subclass, you need to implement the following four functions:\n    -- <__init__>:                      initialize the class, first call BaseDataset.__init__(self, opt).\n    -- <__len__>:                       return the size of dataset.\n    -- <__getitem__>:                   get a data point.\n    -- <modify_commandline_options>:    (optionally) add dataset-specific options and set default options.\n    """"""\n\n    def __init__(self, opt):\n        """"""Initialize the class; save the options in the class\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        """"""\n        self.opt = opt\n        self.root = opt.dataroot\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        """"""Add new dataset-specific options, and rewrite default values for existing options.\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n        Returns:\n            the modified parser.\n        """"""\n        return parser\n\n    @abstractmethod\n    def __len__(self):\n        """"""Return the total number of images in the dataset.""""""\n        return 0\n\n    @abstractmethod\n    def __getitem__(self, index):\n        """"""Return a data point and its metadata information.\n        Parameters:\n            index - - a random integer for data indexing\n        Returns:\n            a dictionary of data with their names. It ususally contains the data itself and its metadata information.\n        """"""\n        pass\n\n\ndef get_transform(opt, has_mask=False, no_flip=None, no_normalize=False):\n    transform_list = []\n    if hasattr(opt, \'color_jitter\') and opt.color_jitter and not has_mask:\n        transform_list.append(transforms.ColorJitter(hue=0.1))\n    if opt.resize_or_crop == \'resize_and_crop\':\n        osize = [opt.load_size, opt.load_size]\n        transform_list.append(transforms.Resize(osize, Image.BICUBIC))\n        transform_list.append(transforms.RandomCrop(opt.crop_size))\n    elif opt.resize_or_crop == \'crop\':\n        transform_list.append(transforms.RandomCrop(opt.crop_size))\n    elif opt.resize_or_crop == \'scale_width\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.crop_size)))\n    elif opt.resize_or_crop == \'scale_width_and_crop\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.load_size)))\n        transform_list.append(transforms.RandomCrop(opt.crop_size))\n    elif opt.resize_or_crop == \'crop_real_im\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __pad_real_im(img, img_size=opt.load_size, padding_value=0 if has_mask else 255)))\n        transform_list.append(transforms.Resize((opt.load_size, opt.load_size), Image.BICUBIC))\n\n    if opt.isTrain and not no_flip:\n        transform_list.append(transforms.RandomHorizontalFlip())\n    if has_mask or no_normalize:\n        transform_list += [transforms.ToTensor()]\n    else:\n        transform_list += [transforms.ToTensor(), get_normaliztion()]\n    return transforms.Compose(transform_list)\n\n\ndef get_normaliztion():\n    return transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n\n\ndef __pad_real_im(img_pil, padding_pix_pct=0.03, img_size=256, padding_value=255):\n    img = np.asarray(img_pil)\n    padding_pix = int(img_size * padding_pix_pct)\n    """"""\n    make the bounding box larger by a few pixels (equiv. to\n    padding_pix pixels after resize), then add edge padding\n    to make it a square, then resize to desired resolution\n    """"""\n    y1, x1, y2, x2 = [0, 0, img.shape[1], img.shape[0]]\n    # print(img.shape)\n    # print(y1, x1, y2, x2)\n    w, h = img.shape[1], img.shape[0]\n    x_mid = (x1 + x2) / 2.\n    y_mid = (y1 + y2) / 2.\n    ll = max(x2 - x1, y2 - y1) * img_size / (img_size - 2. * padding_pix)\n    x1 = int(np.round(x_mid - ll / 2.))\n    x2 = int(np.round(x_mid + ll / 2.))\n    y1 = int(np.round(y_mid - ll / 2.))\n    y2 = int(np.round(y_mid + ll / 2.))\n    # print(y1, x1, y2, x2)\n    b_x = 0\n    if x1 < 0:\n        b_x = -x1\n        x1 = 0\n    b_y = 0\n    if y1 < 0:\n        b_y = -y1\n        y1 = 0\n    a_x = 0\n    if x2 >= h:\n        a_x = x2 - (h - 1)\n        x2 = h - 1\n    a_y = 0\n    if y2 >= w:\n        a_y = y2 - (w - 1)\n        y2 = w - 1\n    # print(y1, x1, y2, x2)\n    if len(img.shape) == 2:\n        crop = np.pad(img[x1: x2 + 1, y1: y2 + 1],\n                      ((b_x, a_x), (b_y, a_y)), mode=\'constant\', constant_values=padding_value)\n    else:\n        crop = np.pad(img[x1: x2 + 1, y1: y2 + 1],\n                      ((b_x, a_x), (b_y, a_y), (0, 0)), mode=\'constant\', constant_values=padding_value)\n    return Image.fromarray(np.uint8(crop))\n\n\ndef __scale_width(img, target_width):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img\n    w = target_width\n    h = int(target_width * oh / ow)\n    return img.resize((w, h), Image.BICUBIC)\n'"
data/concat_dataset.py,0,"b'from .base_dataset import BaseDataset\n\n\nclass ConcatDataset(BaseDataset):\n    def __init__(self, opt):\n        BaseDataset.__init__(self, opt)\n\n    def __getitem__(self, i):\n        return tuple(d[i] for d in self.datasets)\n\n    def __len__(self):\n        return max(len(d) for d in self.datasets)\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n'"
data/df_dataset.py,3,"b""from data.base_dataset import BaseDataset\nimport numpy as np\nimport torch\nfrom os.path import join, dirname\n\n\nclass DFDataset(BaseDataset):\n\n    def __init__(self, opt):\n        BaseDataset.__init__(self, opt)\n        dataroot = join(dirname(__file__), 'objects')\n        if opt.class_3d == 'chair':\n            filelist = join(dataroot, 'df_chair.txt')\n        elif opt.class_3d == 'car':\n            filelist = join(dataroot, 'df_car.txt')\n        else:\n            raise NotImplementedError('%s not supported' % opt.class_3d)\n        items = open(filelist).readlines()\n        self.items = [join(dirname(__file__), x.strip('\\n')) for x in items]\n        self.sigma = opt.df_sigma\n        self.size = len(self.items)\n        self.df_flipped = opt.df_flipped\n        self.return_raw = opt.dataset_mode == 'concat_real_df'\n        if hasattr(opt, 'real_shape'):\n            self.is_test_dummy = not opt.real_shape\n        else:\n            self.is_test_dummy = False\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.add_argument('--df_sigma', type=float, default=8.0, help='data = exp(-df_sigma * df)')\n        parser.add_argument('--df_flipped', action='store_true', help='if specified, flip the generated voxel to match rotation definition. Only used for early 3D GAN version')\n        return parser\n\n    def __getitem__(self, index):\n        if not self.is_test_dummy:\n            return self.get_item(index)\n        else:\n            return {'voxel': 0, 'path': 'dummy'}\n\n    def get_item(self, index):\n        index = index % len(self)\n        data = np.load(self.items[index])['df']\n        # 1 near surface instead of 0\n        data = np.exp(-self.sigma * data)\n        data = torch.from_numpy(data).float().unsqueeze(0)\n        if not self.df_flipped:\n            data = data.transpose(1, 2)\n            data = torch.flip(data, [1])\n            data = data.transpose(2, 3)\n            data = torch.flip(data, [2])\n            data = data.contiguous()\n        if self.return_raw:\n            return data\n        else:\n            return {'voxel': data, 'path': self.items[index]}\n\n    def __len__(self):\n        return self.size\n"""
data/image_and_df_dataset.py,0,"b'from .images_dataset import ImagesDataset\nfrom .df_dataset import DFDataset\nfrom .concat_dataset import ConcatDataset\n\n\nclass ImageAndDFDataset(ConcatDataset):\n    def __init__(self, opt):\n        ConcatDataset.__init__(self, opt)\n        self.datasets = [ImagesDataset(opt), DFDataset(opt)]\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        ImagesDataset.modify_commandline_options(parser, is_train)\n        DFDataset.modify_commandline_options(parser, is_train)\n        return parser\n'"
data/image_and_voxel_dataset.py,0,"b'from .images_dataset import ImagesDataset\nfrom .voxel_dataset import VoxelDataset\nfrom .concat_dataset import ConcatDataset\n\n\nclass ImageAndVoxelDataset(ConcatDataset):\n    def __init__(self, opt):\n        ConcatDataset.__init__(self, opt)\n        self.datasets = [ImagesDataset(opt), VoxelDataset(opt)]\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        ImagesDataset.modify_commandline_options(parser, is_train)\n        VoxelDataset.modify_commandline_options(parser, is_train)\n        return parser\n'"
data/images_dataset.py,8,"b""import os.path\nfrom data.base_dataset import BaseDataset, get_transform, get_normaliztion\nimport numpy as np\nimport random\nfrom PIL import Image\nimport torch\nfrom torch.nn.functional import pad as pad_tensor\nfrom os.path import join, dirname\n\n\nclass ImagesDataset(BaseDataset):\n    def __init__(self, opt):\n        BaseDataset.__init__(self, opt)\n        self.root = join(dirname(__file__), 'images')\n        if opt.class_3d == 'car':\n            pose_pool = np.load(join(self.root, 'pose_car.npz'))\n            azs = pose_pool['azs']\n            eles = pose_pool['eles']\n            self.pose_pool = np.zeros([len(azs), 2])\n            self.pose_pool[:, 0] = np.array(eles)\n            self.pose_pool[:, 1] = np.array(azs)\n            np.random.shuffle(self.pose_pool)\n            crawl_list = os.path.join(self.root, 'imgs_car.txt')\n        elif opt.class_3d == 'chair':\n            pose_pool = np.load(join(self.root, 'pose_chair.npz'))\n            azs = pose_pool['azs']\n            eles = pose_pool['eles']\n            self.pose_pool = np.zeros([len(azs), 2])\n            self.pose_pool[:, 0] = np.array(eles)\n            self.pose_pool[:, 1] = np.array(azs)\n            np.random.shuffle(self.pose_pool)\n            crawl_list = os.path.join(self.root, 'imgs_chair.txt')\n        else:\n            raise NotImplementedError\n\n        with open(crawl_list) as f:\n            imgs_paths = f.read().splitlines()\n        self.paths = [join(dirname(__file__), x) for x in imgs_paths]\n        self.size = len(self.paths)\n        self.transform_mask = get_transform(opt, has_mask=True, no_flip=True, no_normalize=True)\n        self.transform_rgb = get_transform(opt, has_mask=False, no_flip=True, no_normalize=True)\n        self.no_flip = opt.no_flip\n        self.random_shift = opt.random_shift\n        if hasattr(opt, 'real_texture'):\n            self.is_test_dummy = not opt.real_texture\n        else:\n            self.is_test_dummy = False\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--random_shift', action='store_true', help='add random shift to real images and rendered ones')\n        parser.add_argument('--color_jitter', action='store_true', help='jitter the hue of loaded images')\n        # type of  pose pool to sample from:\n        parser.add_argument('--pose_type', type=str, default='hack', choices=['hack'], help='select which pool of poses to sample from')\n        parser.add_argument('--pose_align', action='store_true', help='choose to shuffle pose or not. not shuffling == paired pose')\n        parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data argumentation')\n\n        return parser\n\n    def shift(self, rgb, mask):\n        shift_h = random.randint(-2, 2)\n        shift_v = random.randint(-2, 2)\n        rgb_shift = pad_tensor(rgb, (shift_v, -shift_v, shift_h, -shift_h), mode='constant', value=1)\n        mask_shift = pad_tensor(mask, (shift_v, -shift_v, shift_h, -shift_h), mode='constant', value=0)\n        return rgb_shift.data, mask_shift.data\n\n    def set_aligned(self, aligned):\n        self.aligned = aligned\n\n    @staticmethod\n    def azele2matrix(az=0, ele=0):\n        R0 = torch.zeros([3, 3])\n        R = torch.zeros([3, 4])\n        R0[0, 1] = 1\n        R0[1, 0] = -1\n        R0[2, 2] = 1\n        az = az * np.pi / 180\n        ele = ele * np.pi / 180\n        cos = np.cos\n        sin = np.sin\n        R_ele = torch.FloatTensor(\n            [[1, 0, 0], [0, cos(ele), -sin(ele)], [0, sin(ele), cos(ele)]])\n        R_az = torch.FloatTensor(\n            [[cos(az), -sin(az), 0], [sin(az), cos(az), 0], [0, 0, 1]])\n        R_rot = torch.mm(R_az, R_ele)\n        R_all = torch.mm(R_rot, R0)\n        R[:3, :3] = R_all\n        return R\n\n    def get_posepool(self):\n        return self.pose_pool\n\n    def __getitem__(self, index):\n        if not self.is_test_dummy:\n            return self.get_item(index)\n        else:\n            return {'image_paths': 'dummy', 'image': 0, 'rotation_matrix': 0, 'real_im_mask': 0, 'viewpoint': 0}\n\n    def __len__(self):\n        return len(self.paths)\n\n    def get_item(self, index):\n        index = index % len(self)  # .__len__()\n        pose_id = index\n        flip = not self.no_flip and random.random() < 0.5\n        azs = self.pose_pool[pose_id, 1]\n        eles = self.pose_pool[pose_id, 0]\n        if flip:\n            azs = -azs\n        R = self.azele2matrix(azs, self.pose_pool[pose_id, 0])\n        viewpoint = np.array([azs, eles]) * np.pi / 180\n        pathA = self.paths[index % self.size]\n        im = Image.open(pathA)\n        im_out = self.transform_mask(im)\n        mask = im_out[3, :, :]\n        mask = mask.unsqueeze(0)\n        rgb = im.convert('RGB')\n        rgb = self.transform_rgb(rgb)\n        if self.random_shift:\n            rgb, mask = self.shift(rgb, mask)\n        rgb = get_normaliztion()(rgb)\n        if flip:\n            idx = [i for i in range(rgb.size(2) - 1, -1, -1)]\n            idx = torch.LongTensor(idx)\n            rgb = rgb.index_select(2, idx)\n            mask = mask.index_select(2, idx)\n\n        return {'image_paths': pathA, 'image': rgb, 'rotation_matrix': R, 'real_im_mask': mask, 'viewpoint': viewpoint}\n"""
data/sampler.py,1,"b'\nfrom torch.utils.data.sampler import Sampler\nimport numpy as np\nimport math\n\n\nclass SubsetRandomSampler(Sampler):\n    """"""Samples elements randomly from a given list of indices, without replacement.\n    Arguments:\n        indices (list): a list of indices\n    """"""\n\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return (self.indices[i] for i in np.random.permutation(np.arange(len(self.indices))))\n\n    def __len__(self):\n        return len(self.indices)\n\n\nclass SubsetSequentialSampler(Sampler):\n    """"""Samples elements randomly from a given list of indices, without replacement.\n    Arguments:\n        indices (list): a list of indices\n    """"""\n\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return (self.indices[i] for i in range(len(self.indices)))\n\n    def __len__(self):\n        return len(self.indices)\n\n\nclass AdaptiveLengthSampler(Sampler):\n    """""" Samples elements from part of a predefined index list\n    Dataset size can change at each epoch, but not during epoch\n    Dataset\'s max size should be smaller than the size of the predefined index list.\n    """"""\n\n    def __init__(self, data_source, index_list, train_ratio, mode, shuffle=True):\n        self.dataset = data_source\n        self.index_list = np.array(index_list)\n        self.train_ratio = train_ratio\n        self.mode = mode\n        self.shuffle = shuffle\n        assert mode in (\'train\', \'eval\')\n\n    def __iter__(self):\n        assert len(self.dataset) <= len(self.index_list)\n        train_num = int(math.floor(self.train_ratio * len(self.dataset)))\n        all_index = self.index_list\n        if len(all_index) > len(self.dataset):\n            all_index = list(filter(lambda x: x < len(self.dataset), all_index))\n        index_list = all_index[:train_num] if self.mode == \'train\' else all_index[train_num:]\n        if self.shuffle:\n            return (index_list[i] for i in np.random.permutation(np.arange(len(index_list))))\n        else:\n            return (index_list[i] for i in range(len(index_list)))\n\n    def __len__(self):\n        train_num = int(math.floor(self.train_ratio * len(self.dataset)))\n        return train_num if self.mode == \'train\' else len(self.dataset) - train_num\n'"
data/voxel_dataset.py,1,"b'import torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset\nimport torch\nfrom util.util_voxel import downsample\nfrom util.util_print import str_verbose, str_error\nimport os\nfrom scipy.io import loadmat\nimport numpy as np\nimport glob\nfrom os.path import dirname, basename\n\npaths = {\n    \'canon_scaled\': {\n        \'merged\': None,\n        \'filelist\': \'/path/to/shapenet/ShapeNetCore.v2.scaled/{classid}_{res:d}.csv\',\n        \'filepath\': \'/path/to/shapenet/ShapeNetCore.v2/{classid}/*/models/model_normalized_{res:d}.mat\',\n        \'matname\': \'voxel\'\n    },\n    \'canon\': {\n        \'merged\': None,\n        \'filelist\': \'/path/to/shapenet/ShapeNetCore.v2/{classid}_{res:d}.csv\',\n        \'filepath\': \'/path/to/shapenet/ShapeNetCore.v2/{classid}/*/models/model_normalized_samescale_zup_{res:d}.mat\',\n        \'matname\': \'voxel\'\n    },\n    \'rotated_scaled\': {\n        \'merged\': None,\n        \'filelist\': \'/path/to/shapenet/ShapeNetCore.v2.scaled.rotate/{classid}_{res:d}.csv\',\n        \'filepath\': \'/path/to/shapenet/shapenet-core-v2_single-pass/{classid}/*/{classid}_*_view???_voxel_{res:d}.npz\',\n        \'matname\': \'voxel\',\n    },\n    \'rotated\': {\n        \'merged\': None,\n        \'filelist\': \'/path/to/shapenet/ShapeNetCore.v2.rotate/{classid}_{res:d}.csv\',\n        \'filepath\': \'/path/to/shapenet/shapenet-core-v2_single-pass/{classid}/*/{classid}_*_view???_gt_rotvox_samescale_{res:d}.npz\',\n        \'matname\': \'voxel\',\n    }\n}\n\n\ncommon_classes = {\n    \'all\': \'02691156+02747177+02773838+02801938+02808440+02818832+02828884+02843684+02871439+02876657+02880940+02924116+02933112+02942699+02946921+02954340+02958343+02992529+03001627+03046257+03085013+03207941+03211117+03261776+03325088+03337140+03467517+03513137+03593526+03624134+03636649+03642806+03691459+03710193+03759954+03761084+03790512+03797390+03928116+03938244+03948459+03991062+04004475+04074963+04090263+04099429+04225987+04256520+04330267+04379243+04401088+04460130+04468005+04530566+04554684\',\n    \'ikea\': \'02818832+02871439+03001627+04256520+04379243\',\n    \'7class\': \'02691156+02958343+03001627+04090263+04256520+04379243+04530566\',\n    \'test\': \'04074963\',\n    \'chair\': \'03001627\',\n    \'car\': \'02958343\',\n    \'plane\': \'02691156\',\n    \'mug\': \'03797390\',\n    \'lamp\': \'03636649\',\n    \'tl\': \'02818832+03001627+03337140+04256520+04379243\',\n    \'r2n2\': \'02691156+02828884+03337140+02958343+03001627+03211117+03636649+03691459+03948459+04090263+04256520+04379243+02992529+04401088+04530566\',\n    \'drc\': \'02691156+02958343+03001627\',\n}\ncommon_classes[\'ptset\'] = common_classes[\'r2n2\']\nlabel_to_class = common_classes[\'all\'].split(\'+\')\nclass_to_label = {label_to_class[idx]: idx for idx in range(len(label_to_class))}\n\n\nchair_subclass_file = \'/path/to/shapenet/shapenet-core-v2_subclass/filelist_03001627.txt\'\nchair_subclass_aliases = {\n    \'lt30\': \'+\'.join([str(x) for x in range(20, 46)]),\n    \'ge30\': \'+\'.join([str(x) for x in range(1, 20)])}\n\n\ndef _parse_class(class_str):\n    class_remove = None\n    if class_str in common_classes:\n        class_str = common_classes[class_str]\n    elif class_str[:8] == \'all_but_\':\n        class_remove = class_str[8:]\n        class_str = common_classes[\'all\']\n\n    classlist = class_str.split(\'+\')\n    if class_remove is not None:\n        for c in class_remove.split(\'+\'):\n            if c not in classlist:\n                print(str_error, \'removing a class not used: \' + c)\n            classlist.remove(c)\n\n    return sorted(classlist)\n\n\nclass VoxelDataset(BaseDataset):\n    def __init__(self, opt):\n        BaseDataset.__init(self, opt)\n        opt.downsample = 1\n        opt.pack_n = 10000\n        opt.excl_subclass = None\n\n        classes = opt.class_3d\n        assert classes is not None, \'dataset argument [classes] has to be set\'\n\n        self.res = opt.voxel_res\n\n        dataset_paths = paths[\'canon\']\n\n        # parse classes\n        classlist = _parse_class(classes)\n        self._class_str = \'+\'.join(classlist)\n\n        assert (opt.excl_subclass is None) or (classlist == [\'03001627\']), \'subclass exclusion only supported for chair\'\n\n        class_size = dict()\n        class_to_ind = dict()\n        filelist = list()\n        labellist = list()\n\n        self._matname = dataset_paths[\'matname\']\n        dataset_merged_csv = dataset_paths[\'filelist\']\n        dataset_path_format = dataset_paths[\'filepath\']\n        for i, c in enumerate(classlist):\n            merged_csvfile = dataset_merged_csv.format(classid=c, res=self.res)\n            if os.path.isfile(merged_csvfile):\n                with open(merged_csvfile, \'r\') as fin:\n                    class_filelist = list(map(str.strip, fin.readlines()))\n            else:\n                class_filelist = sorted(glob.glob(dataset_path_format.format(classid=c, res=self.res), recursive=False))\n            if len(class_filelist) == 0:\n                raise ValueError(\'No .mat files found for class: \' + c)\n            class_size[c] = len(class_filelist)\n            class_to_ind[c] = i\n            filelist += sorted(class_filelist)\n            labellist += [class_to_label[c]] * len(class_filelist)\n\n            # filter out specific subclasses\n            subclasses_excl = opt.excl_subclass\n            if subclasses_excl is not None:\n                assert (classlist == [\'03001627\']), \\\n                    ""Excluding subclasses only supported for chairs""\n                if subclasses_excl in chair_subclass_aliases:\n                    subclasses_excl = chair_subclass_aliases[subclasses_excl]\n                subclasses_excl = (set(subclasses_excl.split(\'+\')))\n                with open(chair_subclass_file, \'r\') as f:\n                    lines = f.readlines()\n                list_obj_ids = [l.split(\' \')[0] for l in lines]\n                list_subclasses = [set(l.split(\' \')[1].replace(\'\\n\', \'\').split(\',\')) for l in lines]\n                subclass_dict = dict(zip(list_obj_ids, list_subclasses))\n                idlist = [basename(dirname(dirname(filename))) for filename in filelist]\n                intersection_list = [(len(subclasses_excl.intersection(subclass_dict[id_])) if id_ in subclass_dict else -1) for id_ in idlist]\n                print(str_verbose, \'subclass exclusion: \')\n                print(str_verbose, \'\\tvoxels kept: %d\' % (np.array(intersection_list) == 0).sum())\n                print(str_verbose, \'\\tvoxels excluded: %d\' % (np.array(intersection_list) > 0).sum())\n                print(str_verbose, \'\\tvoxels missing subclass: %d\' % (np.array(intersection_list) < 0).sum())\n\n                # update filelist and labellist\n                filelist = [filelist[i] for i in range(len(filelist)) if intersection_list[i] == 0]\n                labellist = [labellist[i] for i in range(len(labellist)) if intersection_list[i] == 0]\n\n        self._classes = classlist\n        self._class_to_ind = class_to_ind\n        self._filelist = filelist\n        self._labellist = labellist\n\n        self._transform = self.get_transform(opt, opt.downsample)\n        self.size = len(self._filelist)\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        return parser\n\n    def _set_use_dict(self):\n        self.is_dict = True\n\n    def get_transform(self, opt, downsample_ratio):\n        transform_list = list()\n        if opt.downsample > 1:\n            transform_list.append(lambda v: downsample(v, times=downsample_ratio, use_max=True))\n        transform_list.append(lambda v: v[np.newaxis, :, :, :])\n        transform_list.append(lambda v: torch.from_numpy(v).float())\n        transform = transforms.Compose(transform_list)\n        return transform\n\n    def __getitem__(self, index):\n        index = index % len(self)\n        filename = self._filelist[index]\n        if filename.endswith(\'.mat\'):\n            voxel = loadmat(filename)[self._matname]\n        elif filename.endswith(\'.npz\'):\n            voxel = np.load(filename)[self._matname]\n        voxel[voxel > 1] = 1     # fix a numerial bug when generating rotated voxels\n        if self._transform:\n            voxel = self._transform(voxel)\n        return {\'voxel\': voxel}\n\n    def get_classes(self):\n        return self._class_str\n\n    def __len__(self):\n        return self.size\n'"
models/__init__.py,0,"b'""""""This package contains modules related to objective functions, optimizations, and network architectures.\nTo add a custom model class called \'dummy\', you need to add a file called \'dummy_model.py\' and define a subclass DummyModel inherited from BaseModel.\nYou need to implement the following five functions:\n    -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n    -- <set_input>:                     unpack data from dataset and apply preprocessing.\n    -- <forward>:                       produce intermediate results.\n    -- <optimize_parameters>:           calculate loss, gradients, and update network weights.\n    -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\nIn the function <__init__>, you need to define four lists:\n    -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n    -- self.model_names (str list):         specify the images that you want to display and save.\n    -- self.visual_names (str list):        define networks used in our training.\n    -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an usage.\nNow you can use the model class by specifying flag \'--model dummy\'.\nSee our template model class \'template_model.py\' for more details.\n""""""\n\nimport importlib\nfrom models.base_model import BaseModel\n\n\ndef find_model_using_name(model_name):\n    """"""Import the module ""models/[model_name]_model.py"".\n    In the file, the class called DatasetNameModel() will\n    be instantiated. It has to be a subclass of BaseModel,\n    and it is case-insensitive.\n    """"""\n    model_filename = ""models."" + model_name + ""_model""\n    modellib = importlib.import_module(model_filename)\n    model = None\n    target_model_name = model_name.replace(\'_\', \'\') + \'model\'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_model_name.lower() \\\n           and issubclass(cls, BaseModel):\n            model = cls\n\n    if model is None:\n        print(""In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase."" % (model_filename, target_model_name))\n        exit(0)\n\n    return model\n\n\ndef get_option_setter(model_name):\n    """"""Return the static method <modify_commandline_options> of the model class.""""""\n    model_class = find_model_using_name(model_name)\n    return model_class.modify_commandline_options\n\n\ndef create_model(opt):\n    """"""Create a model given the option.\n    This function warps the class CustomDatasetDataLoader.\n    This is the main interface between this package and \'train.py\'/\'test.py\'\n    Example:\n        >>> from models import create_model\n        >>> model = create_model(opt)\n    """"""\n    model = find_model_using_name(opt.model)\n    instance = model(opt)\n    print(""model [%s] was created"" % type(instance).__name__)\n    return instance\n'"
models/base_model.py,15,"b""import os\nimport torch\nfrom . import networks, networks_3d\nfrom collections import OrderedDict\nfrom abc import ABC, abstractmethod\nfrom .basics import get_scheduler\nimport numpy as np\n\n\nclass BaseModel(ABC):\n    @staticmethod\n    def dict_grad_hook_factory(add_func=lambda x: x):\n        saved_dict = dict()\n\n        def hook_gen(name):\n            def grad_hook(grad):\n                saved_vals = add_func(grad)\n                saved_dict[name] = saved_vals\n            return grad_hook\n        return hook_gen, saved_dict\n\n    # modify parser to add command line options,\n    # and also change the default values if needed\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def __init__(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.use_cuda = len(self.gpu_ids) > 0\n        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n        self.isTrain = opt.isTrain\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        torch.manual_seed(opt.seed)\n        self.is_loaded = False\n        self.model_names = []\n        self.loss_names = []\n        self.visual_names = []\n        self.cuda_names = []\n        self.optimizers = []\n        self.is_skip = False\n        if opt.resize_or_crop != 'scale_width':\n            print('enable cudnn benchmark')\n            torch.backends.cudnn.benchmark = True\n\n    def deduplicate_names(self):\n        self.model_names = list(set(self.model_names))\n        self.loss_names = list(set(self.loss_names))\n        # self.visual_names = list(set(self.visual_names))\n        self.cuda_names = list(set(self.cuda_names))\n\n    def setup(self, opt, parser=None):\n        if self.isTrain:\n            self.schedulers = [get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n        if (not self.isTrain or opt.continue_train) and not self.is_loaded:\n            self.load_networks(opt.epoch)\n        self.print_networks(opt.verbose)\n\n    def define_G_3D(self):\n        opt = self.opt\n        netG = networks_3d.define_G_3D(nz=opt.nz_shape, res=opt.voxel_res, model=opt.netG_3D,\n                                       ngf=opt.ngf_3d, norm=opt.G_norm_3D,\n                                       init_type=opt.init_type, init_param=opt.init_param, gpu_ids=opt.gpu_ids)\n        if opt.model3D_dir:\n            self.load_network(netG, opt.model3D_dir + '_G_3D.pth')\n        return netG\n\n    def define_D_3D(self):\n        opt = self.opt\n        netD = networks_3d.define_D_3D(res=opt.voxel_res, model=opt.netD_3D,\n                                       ndf=opt.ndf_3d, norm=opt.D_norm_3D,\n                                       init_type=opt.init_type, init_param=opt.init_param, gpu_ids=opt.gpu_ids)\n        if opt.model3D_dir:\n            self.load_network(netD, opt.model3D_dir + '_D_3D.pth')\n        return netD\n\n    def define_G(self, input_nc, output_nc, nz, ext=''):\n        opt = self.opt\n        netG = networks.define_G(input_nc, output_nc, nz, opt.ngf,\n                                 model=opt.netG, crop_size=opt.crop_size,\n                                 norm=opt.norm, nl=opt.nl, use_dropout=opt.use_dropout, init_type=opt.init_type, init_param=opt.init_param,\n                                 gpu_ids=self.gpu_ids, where_add=self.opt.where_add)\n\n        if opt.model2D_dir:\n            self.load_network(netG, opt.model2D_dir + '_net_G_%s.pth' % ext)\n        return netG\n\n    def define_D(self, input_nc, ext=''):\n        opt = self.opt\n        netD = networks.define_D(input_nc, opt.ndf,\n                                 model=opt.netD, crop_size=opt.crop_size,\n                                 norm=opt.norm, nl=opt.nl, init_type=opt.init_type,\n                                 init_param=opt.init_param, num_Ds=opt.num_Ds, gpu_ids=self.gpu_ids)\n        # if opt.model2D_dir: # skip loading Ds\n        # self.load_network(netD, opt.model2D_dir + '_net_D_%s.pth' % ext)\n        return netD\n\n    def define_E(self, input_nc, vae):\n        opt = self.opt\n        netE = networks.define_E(input_nc, opt.nz_texture, opt.nef,\n                                 model=opt.netE, crop_size=opt.crop_size,\n                                 norm=opt.norm, nl=opt.nl,\n                                 init_type=opt.init_type, gpu_ids=self.gpu_ids,\n                                 vae=vae)\n        if opt.model2D_dir:\n            self.load_network(netE, opt.model2D_dir + '_net_E.pth')\n        return netE\n\n    def define_E_all_z(self, input_nc, output_list, vae_list):\n        opt = self.opt\n        netE = networks.define_E_all_z(input_nc, output_list, opt.nef,\n                                       model=opt.netE_all_z, crop_size=opt.crop_size,\n                                       norm=opt.norm, nl=opt.nl,\n                                       init_type=opt.init_type, gpu_ids=self.gpu_ids, vae_list=vae_list)\n        if opt.model2D_dir:\n            self.load_network(netE, opt.model2D_dir + '_net_E_all_z.pth', notfound_ok=True)\n        return netE\n\n    @abstractmethod\n    def update_D(self):\n        pass\n\n    @abstractmethod\n    def update_G(self):\n        pass\n\n    def encode(self, input_image, vae=False):\n        if vae:\n            mu, logvar = self.netE(input_image)\n            std = logvar.mul(0.5).exp_()\n            eps = self.get_z_random(std.size(0), std.size(1), 'gauss')\n            return eps.mul(std).add_(mu), mu, logvar\n        else:\n            z = self.netE(input_image)\n            return z, None, None\n\n    def load_network(self, net, path, notfound_ok=False):\n        if os.path.exists(path):\n            print('loading model from %s' % path)\n            net.module.load_state_dict(torch.load(path))\n        else:\n            if notfound_ok:\n                print('Warning: network file %s not found. starting from scratch' % path)\n            else:\n                raise ValueError('Network file %s not found.' % path)\n\n    def update_learning_rate(self):\n        for scheduler in self.schedulers:\n            scheduler.step(None)\n        lr = self.optimizers[0].param_groups[0]['lr']\n        print('learning rate = %.7f' % lr)\n\n    def apply_mask(self, input_image, mask, bg_color):\n        output = mask * input_image + (1 - mask) * bg_color\n        return output\n\n    def setup_DR(self, opt):\n        from render_module.render_sketch import VoxelRenderLayer, CroppingLayer, GetRotationMatrix, FineSizeCroppingLayer\n        self.angles_2_rotmat = GetRotationMatrix()\n        vsize = opt.load_size\n        voxel_shape = torch.Size([opt.batch_size, 1, vsize, vsize, vsize])\n        self.renderLayer = VoxelRenderLayer(voxel_shape, res=vsize, nsamples_factor=2.5, camera_distance=2)\n        self.croppinglayer = CroppingLayer(output_size=vsize, no_largest=opt.no_largest)\n        self.fineCropingLayer = FineSizeCroppingLayer(opt.crop_size)\n        self.angles_2_rotmat.to(self.device)\n        self.renderLayer.to(self.device)\n        self.croppinglayer.to(self.device)\n        self.fineCropingLayer.to(self.device)\n\n    def get_z_random(self, batch_size, nz, random_type='gauss'):\n        if random_type == 'uni':\n            z = torch.rand(batch_size, nz) * 2.0 - 1.0\n        elif random_type == 'gauss':\n            z = torch.randn(batch_size, nz)\n        return z.to(self.device)\n\n    def crop_image(self, mask_A_full, real_A_full, mask_B_full, real_B_full):\n        if not self.isTrain:\n            return mask_A_full, real_A_full, mask_B_full, real_B_full\n        random_A = np.random.random()\n        random_B = random_A if self.opt.crop_align else np.random.random()\n        mask_A = self.fineCropingLayer(mask_A_full, random_A)\n        real_A = self.fineCropingLayer(real_A_full, random_A)\n        mask_B = self.fineCropingLayer(mask_B_full, random_B)\n        real_B = self.fineCropingLayer(real_B_full, random_B)\n        return mask_A, real_A, mask_B, real_B\n\n    # testing models\n    @abstractmethod\n    def set_input(self, input):\n        pass\n\n    def get_image_paths(self):\n        return self.image_paths\n\n    # return visualization images. train.py will display these images, and save the images to a html\n    def get_current_visuals(self):\n        visual_ret = OrderedDict()\n        for name in self.visual_names:\n            if isinstance(name, str):\n                visual_ret[name] = getattr(self, name)\n        return visual_ret\n\n    # return traning losses. train.py will print out these losses as debugging information\n    def get_current_losses(self):\n        losses_ret = OrderedDict()\n        for name in self.loss_names:\n            if isinstance(name, str):\n                var = getattr(self, 'loss_' + name)\n                if hasattr(var, 'requires_grad'):\n                    if var.requires_grad:\n                        var = var.item()\n                losses_ret[name] = var\n        return losses_ret\n\n    def check_nan_inf(self):\n        losses = self.get_current_losses()\n        for k, v in losses.items():\n            if np.isnan(v):\n                print('%s is nan!' % k)\n            elif np.isinf(v):\n                print('%s is inf!' % k)\n            else:\n                continue\n\n    def clear_running_mean(self):\n        for name in self.loss_names:\n            self._safe_set('loss_' + name, 0.0)\n            self._safe_set('count_' + name, 0)\n\n    def _safe_set(self, name, value):\n        if hasattr(self, name):\n            setattr(self, name, value)\n\n    def eval(self):\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                net.eval()\n\n    # save models nto the disk\n    def save_networks(self, epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                save_filename = '%s_net_%s.pth' % (epoch, name)\n                save_path = os.path.join(self.save_dir, save_filename)\n                net = getattr(self, 'net' + name)\n\n                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n                    torch.save(net.module.cpu().state_dict(), save_path)\n                    net.cuda(self.gpu_ids[0])\n                else:\n                    torch.save(net.cpu().state_dict(), save_path)\n\n    def get_depth(self, voxel, rot_mat, use_df=False, flipped=False):\n        if use_df:\n            voxel = torch.where(voxel < self.opt.df_th, torch.tensor(0.0).to(self.device), voxel)\n        if flipped:\n            voxel = voxel.transpose(3, 4)\n            voxel = torch.flip(voxel, [3])\n        silhouette_orig, depth_orig = self.renderLayer(voxel, rot_mat)\n        self.sil_orig = silhouette_orig\n        self.depth_orig = depth_orig\n        silhouette, depth = self.croppinglayer(silhouette_orig, depth_orig)\n        min_depth = depth.data.min()\n        max_depth = depth.data.max()\n        # normalize the depth to [-1, 1]\n        depth2 = 1 - (depth - min_depth) / (max_depth - min_depth) * 2\n        return silhouette, depth2\n\n    def move_to_cuda(self, gpu_idx=0):\n        for name in self.cuda_names:\n            if isinstance(name, str):\n                var = getattr(self, name)\n                setattr(self, name, var.to(self.device))\n\n    # load models from the disk\n    def load_networks(self, epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                load_filename = '%s_net_%s.pth' % (epoch, name)\n                load_path = os.path.join(self.save_dir, load_filename)\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                print('loading the model from %s' % load_path)\n                state_dict = torch.load(load_path, map_location=str(self.device))\n                if hasattr(state_dict, '_metadata'):\n                    del state_dict._metadata\n\n                net.load_state_dict(state_dict)\n\n    # print network information\n    def print_networks(self, verbose):\n        print('---------- Networks -------------')\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                if verbose:\n                    print(net)\n                print('[%s] Total #parameters : %.3f M' % (name, num_params / 1e6))\n        print('-----------------------------------------------')\n\n    # set requies_grad=Fasle to avoid computation\n    def set_requires_grad(self, nets, requires_grad=False):\n        if not isinstance(nets, list):\n            nets = [nets]\n        for net in nets:\n            if net is not None:\n                for param in net.parameters():\n                    param.requires_grad = requires_grad\n\n    def skip(self):\n        return self.is_skip\n\n    @staticmethod\n    def accumulate_loss(*args):\n        'safely ignore None and non-tensor values'\n        v = 0\n        for arg in args:\n            if arg is not None and hasattr(arg, 'requires_grad'):\n                v += arg\n        return v\n\n    def safe_render(self, netG, batch_size, nz, flipped=None):\n        success = False\n        MAX_RETRY = 10\n        cnt = 0\n        while not success and cnt < MAX_RETRY:\n            try:\n                z_shape = self.get_z_random(batch_size, nz).view(batch_size, nz, 1, 1, 1).to(self.device)\n                voxel = netG(z_shape)\n                if self.opt.print_grad:\n                    voxel.register_hook(self.grad_hook_gen('2d_grad'))\n                mask_A_full, real_A_full = self.get_depth(voxel, self.rot_mat, use_df=self.use_df, flipped=self.use_df if flipped is None else flipped)\n                if torch.isnan(mask_A_full).any() or torch.isnan(real_A_full).any():\n                    raise ValueError('nan found in rendered mask and depth')\n                    success = False\n                else:\n                    success = True\n            except Exception as e:\n                if cnt >= MAX_RETRY:\n                    print(e)\n                    raise RuntimeError('Maximum number of retries reached.')\n                cnt += 1\n                print(e)\n                print('Retry sampling %02d/%02d' % (cnt, MAX_RETRY))\n                del voxel\n        if success:\n            return mask_A_full, real_A_full, z_shape\n        else:\n            return None, None, None\n"""
models/basics.py,5,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.optim import lr_scheduler\n\n\n###############################################################################\n# Helper functions\n###############################################################################\ndef init_weights(net, init_type='normal', init_param=0.02):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            if init_type == 'normal':\n                init.normal_(m.weight.data, 0.0, init_param)\n            elif init_type == 'xavier':\n                init.xavier_normal_(m.weight.data, gain=init_param)\n            elif init_type == 'kaiming':\n                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n            elif init_type == 'orth':\n                init.orthogonal_(m.weight.data, gain=init_param)\n            else:\n                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n            if hasattr(m, 'bias') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find('BatchNorm2d') != -1:\n            init.normal_(m.weight.data, 1.0, init_param)\n            init.constant_(m.bias.data, 0.0)\n    net.apply(init_func)\n\n\ndef init_net(net, init_type='normal', init_param=0.02, gpu_ids=[]):\n    print('initialization method [%s]' % init_type)\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        net.to(gpu_ids[0])\n        net = torch.nn.DataParallel(net, gpu_ids)\n    init_weights(net, init_type, init_param)\n    return net\n\n\ndef get_scheduler(optimizer, opt):\n    if opt.lr_policy == 'linear':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n            return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == 'step':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.niter_decay, gamma=0.1)\n    elif opt.lr_policy == 'plateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n    else:\n        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n    return scheduler\n\n\ndef get_norm_layer(layer_type='inst'):\n    if layer_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif layer_type == 'batch3d':\n        norm_layer = functools.partial(nn.BatchNorm3d, affine=True)\n    elif layer_type == 'inst':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n    elif layer_type == 'inst3d':\n        norm_layer = functools.partial(nn.InstanceNorm3d, affine=False, track_running_stats=False)\n    elif layer_type == 'none':\n        norm_layer = None\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % layer_type)\n    return norm_layer\n\n\ndef get_non_linearity(layer_type='relu'):\n    if layer_type == 'relu':\n        nl_layer = functools.partial(nn.ReLU, inplace=True)\n    elif layer_type == 'lrelu':\n        nl_layer = functools.partial(nn.LeakyReLU, negative_slope=0.2, inplace=True)\n    elif layer_type == 'elu':\n        nl_layer = functools.partial(nn.ELU, inplace=True)\n    else:\n        raise NotImplementedError('nonlinearity activitation [%s] is not found' % layer_type)\n    return nl_layer\n\n\ndef print_network(net, name):\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(net)\n    print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n"""
models/full_model.py,0,"b""from .texture_real_model import TextureRealModel\nfrom .shape_gan_model import ShapeGANModel\n\n\nclass FullModel(TextureRealModel, ShapeGANModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        TextureRealModel.modify_commandline_options(parser, is_train)\n        ShapeGANModel.modify_commandline_options(parser, is_train)\n        return parser\n\n    def __init__(self, opt):\n        TextureRealModel.__init__(self, opt, base_init=True)\n        ShapeGANModel.__init__(self, opt, base_init=False)\n        if opt.lambda_GAN_3D == 0.0:\n            self.loss_names = [x for x in self.loss_names if '3D' not in x]\n        if opt.print_grad:\n            self.loss_names += ['2d_grad', '3d_grad']\n            self.loss_2d_grad = 0.0\n            self.loss_3d_grad = 0.0\n            self.grad_hook_gen, self.grad_stats = self.dict_grad_hook_factory(add_func=lambda x: {'mean': x.data.mean(), 'std': x.data.std()})\n\n    def set_input(self, input):\n        self.input_B = input[0]['image']\n        self.mask_B = input[0]['real_im_mask']\n        self.rot_mat = input[0]['rotation_matrix']\n        self.bs = self.input_B.size(0)\n        self.z_texture = self.get_z_random(self.bs, self.nz_texture)\n        self.z_shape = self.get_z_random(self.bs, self.nz_shape).view(self.bs, self.nz_shape, 1, 1, 1)\n        self.voxel_real = input[1]['voxel']\n        self.move_to_cuda()\n\n        mask_A_full, real_A_full, _ = self.safe_render(self.netG_3D, self.bs, self.nz_shape, flipped=False)\n\n        self.mask_A, self.real_A, self.mask_B, self.real_B = self.crop_image(mask_A_full, real_A_full, self.mask_B, self.input_B)\n\n    def update_D(self):\n        TextureRealModel.update_D(self)\n        if self.opt.lambda_GAN_3D > 0.0:\n            ShapeGANModel.update_D(self)\n\n    def update_G(self):\n        self.optimizer_G_3D.zero_grad()\n        TextureRealModel.update_G(self)\n        self.optimizer_G_3D.step()\n        if self.opt.lambda_GAN_3D > 0.0:\n            ShapeGANModel.update_G(self)\n        if self.opt.print_grad:\n            self.loss_2d_grad = self.grad_stats['2d_grad']['std'] * 1e3\n            if self.opt.lambda_GAN_3D > 0.0:\n                self.loss_3d_grad = self.grad_stats['3d_grad']['std'] * 1e3\n\n    def save_networks(self, epoch):\n        ShapeGANModel.save_networks(self, epoch)\n\n    def sample(self, k=10, interp_traj=2, step=10):\n        ShapeGANModel(self, k, interp_traj, step)\n"""
models/networks.py,14,"b'import torch\nimport torch.nn as nn\nimport math\nfrom .basics import get_norm_layer, get_non_linearity, init_net\n\n\n###############################################################################\n# Functions\n###############################################################################\ndef _cal_kl(mu, logvar, lambda_kl):\n    if lambda_kl > 0.0:\n        logvar = torch.clamp(logvar, max=10.0)  # to prevent nan\n        loss_kl = torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * (-0.5 * lambda_kl)\n    else:\n        loss_kl = 0.0\n    return loss_kl\n\n\ndef cat_feature(x, y):\n    y_expand = y.view(y.size(0), y.size(1), 1, 1).expand(\n        y.size(0), y.size(1), x.size(2), x.size(3))\n    x_cat = torch.cat([x, y_expand], 1)\n    return x_cat\n\n\ndef define_G(input_nc, output_nc, nz, ngf,\n             model, crop_size=128, norm=\'batch\', nl=\'relu\',\n             use_dropout=False, init_type=\'xavier\', init_param=0.02, gpu_ids=[], where_add=\'input\'):\n    netG = None\n    norm_layer = get_norm_layer(layer_type=norm)\n    nl_layer = get_non_linearity(layer_type=nl)\n\n    if nz == 0:\n        where_add = \'input\'\n\n    n_blocks = int(math.log(crop_size, 2))\n\n    if model == \'unet\' and where_add == \'input\':\n        netG = G_Unet_add_input(input_nc, output_nc, nz, n_blocks, ngf, norm_layer=norm_layer, nl_layer=nl_layer, use_dropout=use_dropout)\n    elif model == \'unet\' and where_add == \'all\':\n        netG = G_Unet_add_all(input_nc, output_nc, nz, n_blocks, ngf, norm_layer=norm_layer, nl_layer=nl_layer, use_dropout=use_dropout)\n    elif model == \'resnet_cat\':\n        netG = G_Resnet(input_nc, output_nc, nz, num_downs=2, n_res=n_blocks - 4, ngf=ngf, norm=norm, nl_layer=nl)\n    else:\n        raise NotImplementedError(\'Generator model name [%s] is not recognized\' % model)\n\n    return init_net(netG, init_type, init_param, gpu_ids)\n\n\ndef define_D(input_nc, ndf, model, crop_size=128,\n             norm=\'batch\', nl=\'lrelu\', init_type=\'xavier\', init_param=0.02, num_Ds=1, gpu_ids=[]):\n    netD = None\n    norm_layer = get_norm_layer(layer_type=norm)\n    nl = \'lrelu\'  # use leaky relu for D\n    nl_layer = get_non_linearity(layer_type=nl)\n\n    n_layers = int(math.log(crop_size, 2)) - 5\n    if model == \'single\':\n        netD = D_NLayers(input_nc, ndf, n_layers=n_layers, norm_layer=norm_layer, nl_layer=nl_layer)\n    elif model == \'multi\':\n        netD = D_NLayersMulti(input_nc=input_nc, ndf=ndf, n_layers=n_layers, norm_layer=norm_layer, num_D=num_Ds)\n    else:\n        raise NotImplementedError(\'Discriminator model name [%s] is not recognized\' % model)\n\n    return init_net(netD, init_type, init_param, gpu_ids)\n\n\ndef define_E(input_nc, output_nc, nef, model, crop_size=128,\n             norm=\'batch\', nl=\'lrelu\',\n             init_type=\'xavier\', init_param=0.02, gpu_ids=[], vae=False):\n    netE = None\n    norm_layer = get_norm_layer(layer_type=norm)\n    nl = \'lrelu\'  # use leaky relu for E\n    nl_layer = get_non_linearity(layer_type=nl)\n    n_blocks = int(math.log(crop_size, 2)) - 3\n    if model == \'resnet\':\n        netE = E_ResNet(input_nc, output_nc, nef, n_blocks=n_blocks, norm_layer=norm_layer, nl_layer=nl_layer, vae=vae)\n    elif model == \'conv\':\n        netE = E_NLayers(input_nc, output_nc, nef, n_layers=n_blocks, norm_layer=norm_layer, nl_layer=nl_layer, vae=vae)\n    elif model == \'adaIN\':\n        netE = E_adaIN(input_nc, output_nc, nef, n_layers=n_blocks - 1, nl_layer=nl, vae=vae)\n    else:\n        raise NotImplementedError(\'Encoder model name [%s] is not recognized\' % model)\n\n    return init_net(netE, init_type, init_param, gpu_ids)\n\n\nclass D_NLayersMulti(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, num_D=1):\n        super(D_NLayersMulti, self).__init__()\n        self.num_D = num_D\n        if num_D == 1:\n            layers = self.get_layers(input_nc, ndf, n_layers, norm_layer)\n            self.model = nn.Sequential(*layers)\n        else:\n            layers = self.get_layers(input_nc, ndf, n_layers, norm_layer)\n            self.add_module(""model_0"", nn.Sequential(*layers))\n            self.down = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n            for i in range(1, num_D):\n                ndf_i = int(round(ndf / (2**i)))  # Just using ndf also works\n                layers = self.get_layers(input_nc, ndf_i, n_layers, norm_layer)\n                self.add_module(""model_%d"" % i, nn.Sequential(*layers))\n\n    def get_layers(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n        kw = 4\n        padw = 1\n        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        return sequence\n\n    def forward(self, input):\n        if self.num_D == 1:\n            return self.model(input)\n        result = []\n        down = input\n        for i in range(self.num_D):\n            model = getattr(self, ""model_%d"" % i)\n            result.append(model(down))\n            if i != self.num_D - 1:\n                down = self.down(down)\n        return result\n\n\n# Defines the conv discriminator with the specified arguments.\nclass G_NLayers(nn.Module):\n    def __init__(self, output_nc=3, nz=100, ngf=64, n_layers=3, norm_layer=None, nl_layer=None):\n        super(G_NLayers, self).__init__()\n\n        kw, s, padw = 4, 2, 1\n        sequence = [nn.ConvTranspose2d(nz, ngf * 4, kernel_size=kw, stride=1, padding=0, bias=True)]\n        if norm_layer is not None:\n            sequence += [norm_layer(ngf * 4)]\n\n        sequence += [nl_layer()]\n\n        nf_mult = 4\n        nf_mult_prev = 4\n        for n in range(n_layers, 0, -1):\n            nf_mult_prev = nf_mult\n            nf_mult = min(n, 4)\n            sequence += [nn.ConvTranspose2d(ngf * nf_mult_prev, ngf * nf_mult,\n                                            kernel_size=kw, stride=s, padding=padw, bias=True)]\n            if norm_layer is not None:\n                sequence += [norm_layer(ngf * nf_mult)]\n            sequence += [nl_layer()]\n\n        sequence += [nn.ConvTranspose2d(ngf, output_nc, kernel_size=4, stride=s, padding=padw, bias=True)]\n        sequence += [nn.Tanh()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\nclass D_NLayers(nn.Module):\n    def __init__(self, input_nc=3, ndf=64, n_layers=3, norm_layer=None, nl_layer=None):\n        super(D_NLayers, self).__init__()\n\n        kw, padw, use_bias = 4, 1, True\n        # st()\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n            nl_layer()\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                                   kernel_size=kw, stride=2, padding=padw, bias=use_bias)]\n            if norm_layer is not None:\n                sequence += [norm_layer(ndf * nf_mult)]\n            sequence += [nl_layer()]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias)]\n        if norm_layer is not None:\n            sequence += [norm_layer(ndf * nf_mult)]\n        sequence += [nl_layer()]\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=4, stride=1, padding=0, bias=use_bias)]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        output = self.model(input)\n        return output\n\n\ndef print_network(net, name=\'\', verbose=False):\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    if verbose:\n        print(net)\n    print(\'network {:s}, #parameters: {:f}M\'.format(name, num_params / 1e6))\n\n\n##############################################################################\n# Classes\n##############################################################################\n\n# Defines the GAN loss based on the argument gan_mode (lsgan | dcgan | wgangp | hinge)\n# This class abstracts away the need to create the target label tensor\n# that has the same size as the input\nclass GANLoss(nn.Module):\n    """"""Define different GAN objectives.\n    The GANLoss class abstracts away the need to create the target label tensor\n    that has the same size as the input.\n    """"""\n\n    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n        """""" Initialize the GANLoss class.\n        Parameters:\n            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n            target_real_label (bool) - - label for a real image\n            target_fake_label (bool) - - label of a fake image\n        Note: Do not use sigmoid as the last layer of Discriminator.\n        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n        """"""\n        super(GANLoss, self).__init__()\n        self.register_buffer(\'real_label\', torch.tensor(target_real_label))\n        self.register_buffer(\'fake_label\', torch.tensor(target_fake_label))\n        self.gan_mode = gan_mode\n        if gan_mode == \'lsgan\':\n            self.loss = nn.MSELoss()\n        elif gan_mode == \'vanilla\':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif gan_mode in [\'wgangp\']:\n            self.loss = None\n        else:\n            raise NotImplementedError(\'gan mode %s not implemented\' % gan_mode)\n\n    def get_target_tensor(self, prediction, target_is_real):\n        """"""Create label tensors with the same size as the input.\n        Parameters:\n            prediction (tensor) - - tpyically the prediction from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n        Returns:\n            A label tensor filled with ground truth label, and with the size of the input\n        """"""\n\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(prediction)\n\n    def __call__(self, predictions, target_is_real):\n        """"""Calculate loss given Discriminator\'s output and grount truth labels.\n        Parameters:\n            predictions (tensor list) - - tpyically the prediction output from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n        Returns:\n            the calculated loss.\n        """"""\n        all_losses = []\n        for prediction in predictions:\n            if self.gan_mode in [\'lsgan\', \'vanilla\']:\n                target_tensor = self.get_target_tensor(prediction, target_is_real)\n                loss = self.loss(prediction, target_tensor)\n            elif self.gan_mode == \'wgangp\':\n                if target_is_real:\n                    loss = -prediction.mean()\n                else:\n                    loss = prediction.mean()\n            all_losses.append(loss)\n\n        return sum(all_losses)\n\n\n# Defines the Unet generator.\n# |num_downs|: number of downsamplings in UNet. For example,\n# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n# at the bottleneck\nclass G_Unet_add_input(nn.Module):\n    def __init__(self, input_nc, output_nc, nz, num_downs, ngf=64,\n                 norm_layer=None, nl_layer=None, use_dropout=False, upsample=\'basic\'):\n        super(G_Unet_add_input, self).__init__()\n        self.nz = nz\n        # currently support only input_nc == output_nc\n        # assert(input_nc == output_nc)\n        max_nchn = 8\n        # construct unet structure\n        unet_block = UnetBlock(ngf * max_nchn, ngf * max_nchn, ngf * max_nchn,\n                               innermost=True, norm_layer=norm_layer, nl_layer=nl_layer, upsample=upsample)\n        for i in range(num_downs - 5):\n            unet_block = UnetBlock(ngf * max_nchn, ngf * max_nchn, ngf * max_nchn, unet_block, norm_layer=norm_layer, nl_layer=nl_layer, use_dropout=use_dropout, upsample=upsample)\n        unet_block = UnetBlock(ngf * 4, ngf * 4, ngf * max_nchn, unet_block, norm_layer=norm_layer, nl_layer=nl_layer, upsample=upsample)\n        unet_block = UnetBlock(ngf * 2, ngf * 2, ngf * 4, unet_block, norm_layer=norm_layer, nl_layer=nl_layer, upsample=upsample)\n        unet_block = UnetBlock(ngf, ngf, ngf * 2, unet_block, norm_layer=norm_layer, nl_layer=nl_layer, upsample=upsample)\n        unet_block = UnetBlock(input_nc + nz, output_nc, ngf, unet_block,\n                               outermost=True, norm_layer=norm_layer, nl_layer=nl_layer, upsample=upsample)\n\n        self.model = unet_block\n\n    def forward(self, x, z=None):\n        if self.nz > 0:\n            z_img = z.view(z.size(0), z.size(1), 1, 1).expand(z.size(0), z.size(1), x.size(2), x.size(3))\n            x_with_z = torch.cat([x, z_img], 1)\n        else:\n            x_with_z = x  # no z\n\n        return self.model(x_with_z)\n\n\nclass Upsample(nn.Module):\n    def __init__(self, scale_factor, mode=\'nearest\'):\n        super().__init__()\n        self.factor = scale_factor\n        self.mode = mode\n\n    def forward(self, x):\n        return torch.nn.functional.interpolate(x, scale_factor=self.factor, mode=self.mode)\n\n\ndef upsampleLayer(inplanes, outplanes, upsample=\'basic\', padding_type=\'zero\'):\n    if upsample == \'basic\':\n        upconv = [nn.ConvTranspose2d(inplanes, outplanes, kernel_size=4, stride=2, padding=1)]\n    elif upsample == \'bilinear\':\n        upconv = [Upsample(scale_factor=2, mode=\'bilinear\'),\n                  nn.ReflectionPad2d(1),\n                  nn.Conv2d(inplanes, outplanes, kernel_size=3, stride=1, padding=0)]\n    else:\n        raise NotImplementedError(\'upsample layer [%s] not implemented\' % upsample)\n    return upconv\n\n\n# Defines the submodule with skip connection.\n# X -------------------identity---------------------- X\n#   |-- downsampling -- |submodule| -- upsampling --|\nclass UnetBlock(nn.Module):\n    def __init__(self, input_nc, outer_nc, inner_nc,\n                 submodule=None, outermost=False, innermost=False,\n                 norm_layer=None, nl_layer=None, use_dropout=False, upsample=\'basic\', padding_type=\'zero\'):\n        super(UnetBlock, self).__init__()\n        self.outermost = outermost\n        p = 0\n        downconv = []\n        if padding_type == \'reflect\':\n            downconv += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            downconv += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n        downconv += [nn.Conv2d(input_nc, inner_nc, kernel_size=4, stride=2, padding=p)]\n        downrelu = nn.LeakyReLU(0.2, True)  # downsample is different from upsample\n        downnorm = norm_layer(inner_nc) if norm_layer is not None else None\n        uprelu = nl_layer()\n        upnorm = norm_layer(outer_nc) if norm_layer is not None else None\n\n        if outermost:\n            upconv = upsampleLayer(inner_nc * 2, outer_nc, upsample=upsample, padding_type=padding_type)\n            down = downconv\n            up = [uprelu] + upconv + [nn.Tanh()]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = upsampleLayer(inner_nc, outer_nc, upsample=upsample, padding_type=padding_type)\n            down = [downrelu] + downconv\n            up = [uprelu] + upconv\n            if upnorm is not None:\n                up += [upnorm]\n            model = down + up\n        else:\n            upconv = upsampleLayer(inner_nc * 2, outer_nc, upsample=upsample, padding_type=padding_type)\n            down = [downrelu] + downconv\n            if downnorm is not None:\n                down += [downnorm]\n            up = [uprelu] + upconv\n            if upnorm is not None:\n                up += [upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([self.model(x), x], 1)\n\n\ndef conv3x3(in_planes, out_planes):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n                     padding=1, bias=True)\n\n\n# two usage cases, depend on kw and padw\ndef upsampleConv(inplanes, outplanes, kw, padw):\n    sequence = []\n    sequence += [Upsample(scale_factor=2, mode=\'nearest\')]\n    sequence += [nn.Conv2d(inplanes, outplanes, kernel_size=kw, stride=1, padding=padw, bias=True)]\n    return nn.Sequential(*sequence)\n\n\ndef meanpoolConv(inplanes, outplanes):\n    sequence = []\n    sequence += [nn.AvgPool2d(kernel_size=2, stride=2)]\n    sequence += [nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0, bias=True)]\n    return nn.Sequential(*sequence)\n\n\ndef convMeanpool(inplanes, outplanes):\n    sequence = []\n    sequence += [conv3x3(inplanes, outplanes)]\n    sequence += [nn.AvgPool2d(kernel_size=2, stride=2)]\n    return nn.Sequential(*sequence)\n\n\nclass BasicBlockUp(nn.Module):\n    def __init__(self, inplanes, outplanes, norm_layer=None, nl_layer=None):\n        super(BasicBlockUp, self).__init__()\n        layers = []\n        if norm_layer is not None:\n            layers += [norm_layer(inplanes)]\n        layers += [nl_layer()]\n        layers += [upsampleConv(inplanes, outplanes, kw=3, padw=1)]\n        if norm_layer is not None:\n            layers += [norm_layer(outplanes)]\n        layers += [conv3x3(outplanes, outplanes)]\n        self.conv = nn.Sequential(*layers)\n        self.shortcut = upsampleConv(inplanes, outplanes, kw=1, padw=0)\n\n    def forward(self, x):\n        out = self.conv(x) + self.shortcut(x)\n        return out\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, inplanes, outplanes, norm_layer=None, nl_layer=None):\n        super(BasicBlock, self).__init__()\n        layers = []\n        if norm_layer is not None:\n            layers += [norm_layer(inplanes)]\n        layers += [nl_layer()]\n        layers += [conv3x3(inplanes, inplanes)]\n        if norm_layer is not None:\n            layers += [norm_layer(inplanes)]\n        layers += [nl_layer()]\n        layers += [convMeanpool(inplanes, outplanes)]\n        self.conv = nn.Sequential(*layers)\n        self.shortcut = meanpoolConv(inplanes, outplanes)\n\n    def forward(self, x):\n        out = self.conv(x) + self.shortcut(x)\n        return out\n\n\nclass E_ResNet(nn.Module):\n    def __init__(self, input_nc=3, output_nc=1, nef=64, n_blocks=4,\n                 norm_layer=None, nl_layer=None, vae=False, all_z=False):\n        super(E_ResNet, self).__init__()\n        self.vae = vae\n        max_ndf = 4\n        conv_layers = [nn.Conv2d(input_nc, nef, kernel_size=4, stride=2, padding=1, bias=True)]\n        for n in range(1, n_blocks):\n            input_ndf = nef * min(max_ndf, n)  # 2**(n-1)\n            output_ndf = nef * min(max_ndf, n + 1)  # 2**n\n            conv_layers += [BasicBlock(input_ndf, output_ndf, norm_layer, nl_layer)]\n        conv_layers += [nl_layer(), nn.AvgPool2d(8)]\n        if vae:\n            self.fc = nn.Sequential(*[nn.Linear(output_ndf, output_nc)])\n            self.fcVar = nn.Sequential(*[nn.Linear(output_ndf, output_nc)])\n        else:\n            self.fc = nn.Sequential(*[nn.Linear(output_ndf, output_nc)])\n        self.conv = nn.Sequential(*conv_layers)\n\n    def forward(self, x):\n        x_conv = self.conv(x)\n        conv_flat = x_conv.view(x.size(0), -1)\n        output = self.fc(conv_flat)\n        if self.vae:\n            outputVar = self.fcVar(conv_flat)\n            return output, outputVar\n        else:\n            return output\n        return output\n\n\n# Defines the Unet generator.\n# |num_downs|: number of downsamplings in UNet. For example,\n# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n# at the bottleneck\nclass G_Unet_add_all(nn.Module):\n    def __init__(self, input_nc, output_nc, nz, num_downs, ngf=64,\n                 norm_layer=None, nl_layer=None, use_dropout=False, upsample=\'basic\'):\n        super(G_Unet_add_all, self).__init__()\n        self.nz = nz\n        # construct unet structure\n        unet_block = UnetBlock_with_z(ngf * 8, ngf * 8, ngf * 8, nz, None, innermost=True,\n                                      norm_layer=norm_layer, nl_layer=nl_layer, upsample=upsample)\n        unet_block = UnetBlock_with_z(ngf * 8, ngf * 8, ngf * 8, nz, unet_block,\n                                      norm_layer=norm_layer, nl_layer=nl_layer, use_dropout=use_dropout, upsample=upsample)\n        for i in range(num_downs - 6):\n            unet_block = UnetBlock_with_z(ngf * 8, ngf * 8, ngf * 8, nz, unet_block,\n                                          norm_layer=norm_layer, nl_layer=nl_layer, use_dropout=use_dropout, upsample=upsample)\n        unet_block = UnetBlock_with_z(ngf * 4, ngf * 4, ngf * 8, nz, unet_block, norm_layer=norm_layer, nl_layer=nl_layer, upsample=upsample)\n        unet_block = UnetBlock_with_z(ngf * 2, ngf * 2, ngf * 4, nz, unet_block, norm_layer=norm_layer, nl_layer=nl_layer, upsample=upsample)\n        unet_block = UnetBlock_with_z(ngf, ngf, ngf * 2, nz, unet_block, norm_layer=norm_layer, nl_layer=nl_layer, upsample=upsample)\n        unet_block = UnetBlock_with_z(input_nc, output_nc, ngf, nz, unet_block, outermost=True, norm_layer=norm_layer, nl_layer=nl_layer, upsample=upsample)\n        self.model = unet_block\n\n    def forward(self, x, z):\n        return self.model(x, z)\n\n\nclass UnetBlock_with_z(nn.Module):\n    def __init__(self, input_nc, outer_nc, inner_nc, nz=0,\n                 submodule=None, outermost=False, innermost=False,\n                 norm_layer=None, nl_layer=None, use_dropout=False, upsample=\'basic\', padding_type=\'zero\'):\n        super(UnetBlock_with_z, self).__init__()\n        p = 0\n        downconv = []\n        if padding_type == \'reflect\':\n            downconv += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            downconv += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        self.outermost = outermost\n        self.innermost = innermost\n        self.nz = nz\n        input_nc = input_nc + nz\n        downconv += [nn.Conv2d(input_nc, inner_nc, kernel_size=4, stride=2, padding=p)]\n        downrelu = nn.LeakyReLU(0.2, True)  # downsample is different from upsample\n        uprelu = nl_layer()\n\n        if outermost:\n            upconv = upsampleLayer(inner_nc * 2, outer_nc, upsample=upsample, padding_type=padding_type)\n            down = downconv\n            up = [uprelu] + upconv + [nn.Tanh()]\n        elif innermost:\n            upconv = upsampleLayer(inner_nc, outer_nc, upsample=upsample, padding_type=padding_type)\n            down = [downrelu] + downconv\n            up = [uprelu] + upconv\n            if norm_layer is not None:\n                up += [norm_layer(outer_nc)]\n        else:\n            upconv = upsampleLayer(inner_nc * 2, outer_nc, upsample=upsample, padding_type=padding_type)\n            down = [downrelu] + downconv\n            if norm_layer is not None:\n                down += [norm_layer(inner_nc)]\n            up = [uprelu] + upconv\n\n            if norm_layer is not None:\n                up += [norm_layer(outer_nc)]\n\n            if use_dropout:\n                up += [nn.Dropout(0.5)]\n        self.down = nn.Sequential(*down)\n        self.submodule = submodule\n        self.up = nn.Sequential(*up)\n\n    def forward(self, x, z):\n        # print(x.size())\n        if self.nz > 0:\n            z_img = z.view(z.size(0), z.size(1), 1, 1).expand(z.size(0), z.size(1), x.size(2), x.size(3))\n            x_and_z = torch.cat([x, z_img], 1)\n        else:\n            x_and_z = x\n\n        if self.outermost:\n            x1 = self.down(x_and_z)\n            x2 = self.submodule(x1, z)\n            return self.up(x2)\n        elif self.innermost:\n            x1 = self.up(self.down(x_and_z))\n            return torch.cat([x1, x], 1)\n        else:\n            x1 = self.down(x_and_z)\n            x2 = self.submodule(x1, z)\n            return torch.cat([self.up(x2), x], 1)\n\n\nclass E_NLayers(nn.Module):\n    def __init__(self, input_nc, output_nc=1, ndf=64, n_layers=3,\n                 norm_layer=None, nl_layer=None, vae=False):\n        super(E_NLayers, self).__init__()\n        self.vae = vae\n\n        kw, padw = 4, 1\n        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nl_layer()]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 4)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw)]\n            if norm_layer is not None:\n                sequence += [norm_layer(ndf * nf_mult)]\n            sequence += [nl_layer()]\n        sequence += [nn.AvgPool2d(8)]\n        self.conv = nn.Sequential(*sequence)\n        self.fc = nn.Sequential(*[nn.Linear(ndf * nf_mult, output_nc)])\n        if vae:\n            self.fcVar = nn.Sequential(*[nn.Linear(ndf * nf_mult, output_nc)])\n\n    def forward(self, x):\n        x_conv = self.conv(x)\n        conv_flat = x_conv.view(x.size(0), -1)\n        output = self.fc(conv_flat)\n        if self.vae:\n            outputVar = self.fcVar(conv_flat)\n            return output, outputVar\n        return output\n\n# The following network achitecture code is modified from MUNIT (https://github.com/NVlabs/MUNIT)\n# Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz, ""Multimodal Unsupervised Image-to-Image Translation"", ECCV 2018\n# Licensed under the CC BY-NC-SA 4.0 license\n\n\nclass G_Resnet(nn.Module):\n    def __init__(self, input_nc, output_nc, nz, num_downs, n_res, ngf=64,\n                 norm=None, nl_layer=None):\n        super(G_Resnet, self).__init__()\n        n_downsample = num_downs\n        pad_type = \'reflect\'\n        self.enc_content = ContentEncoder(n_downsample, n_res, input_nc, ngf, norm, nl_layer, pad_type=pad_type)\n        if nz == 0:\n            self.dec = Decoder(n_downsample, n_res, self.enc_content.output_dim, output_nc, norm=norm, activ=nl_layer, pad_type=pad_type, nz=nz)\n        else:\n            self.dec = Decoder_all(n_downsample, n_res, self.enc_content.output_dim, output_nc, norm=norm, activ=nl_layer, pad_type=pad_type, nz=nz)\n\n    def decode(self, content, style=None):\n        return self.dec(content, style)\n\n    def forward(self, image, style=None):\n        content = self.enc_content(image)\n        images_recon = self.decode(content, style)\n        return images_recon\n\n##################################################################################\n# Encoder and Decoders\n##################################################################################\n\n\nclass E_adaIN(nn.Module):\n    def __init__(self, input_nc, output_nc=1, nef=64, n_layers=4,\n                 norm=None, nl_layer=None, vae=False):\n        # style encoder\n        super(E_adaIN, self).__init__()\n        self.enc_style = StyleEncoder(n_layers, input_nc, nef, output_nc, norm=\'none\', activ=\'relu\', vae=vae)\n\n    def forward(self, image):\n        style = self.enc_style(image)\n        return style\n\n\nclass StyleEncoder(nn.Module):\n    def __init__(self, n_downsample, input_dim, dim, style_dim, norm, activ, vae=False):\n        super(StyleEncoder, self).__init__()\n        self.vae = vae\n        self.model = []\n        self.model += [Conv2dBlock(input_dim, dim, 7, 1, 3, norm=norm, activation=activ, pad_type=\'reflect\')]\n        for i in range(2):\n            self.model += [Conv2dBlock(dim, 2 * dim, 4, 2, 1, norm=norm, activation=activ, pad_type=\'reflect\')]\n            dim *= 2\n        for i in range(n_downsample - 2):\n            self.model += [Conv2dBlock(dim, dim, 4, 2, 1, norm=norm, activation=activ, pad_type=\'reflect\')]\n        self.model += [nn.AdaptiveAvgPool2d(1)]  # global average pooling\n        if self.vae:\n            self.fc_mean = nn.Linear(dim, style_dim)  # , 1, 1, 0)\n            self.fc_var = nn.Linear(dim, style_dim)  # , 1, 1, 0)\n        else:\n            self.model += [nn.Conv2d(dim, style_dim, 1, 1, 0)]\n\n        self.model = nn.Sequential(*self.model)\n        self.output_dim = dim\n\n    def forward(self, x):\n        if self.vae:\n            output = self.model(x)\n            output = output.view(x.size(0), -1)\n            output_mean = self.fc_mean(output)\n            output_var = self.fc_var(output)\n            return output_mean, output_var\n        else:\n            return self.model(x).view(x.size(0), -1)\n\n\nclass ContentEncoder(nn.Module):\n    def __init__(self, n_downsample, n_res, input_dim, dim, norm, activ, pad_type=\'zero\'):\n        super(ContentEncoder, self).__init__()\n        self.model = []\n        self.model += [Conv2dBlock(input_dim, dim, 7, 1, 3, norm=norm, activation=activ, pad_type=\'reflect\')]\n        # downsampling blocks\n        for i in range(n_downsample):\n            self.model += [Conv2dBlock(dim, 2 * dim, 4, 2, 1, norm=norm, activation=activ, pad_type=\'reflect\')]\n            dim *= 2\n        # residual blocks\n        self.model += [ResBlocks(n_res, dim, norm=norm, activation=activ, pad_type=pad_type)]\n        self.model = nn.Sequential(*self.model)\n        self.output_dim = dim\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass Decoder_all(nn.Module):\n    def __init__(self, n_upsample, n_res, dim, output_dim, norm=\'batch\', activ=\'relu\', pad_type=\'zero\', nz=0):\n        super(Decoder_all, self).__init__()\n        # AdaIN residual blocks\n        self.resnet_block = ResBlocks(n_res, dim, norm, activ, pad_type=pad_type, nz=nz)\n        self.n_blocks = 0\n        # upsampling blocks\n        for i in range(n_upsample):\n            block = [Upsample(scale_factor=2), Conv2dBlock(dim + nz, dim // 2, 5, 1, 2, norm=\'ln\', activation=activ, pad_type=\'reflect\')]\n            setattr(self, \'block_{:d}\'.format(self.n_blocks), nn.Sequential(*block))\n            self.n_blocks += 1\n            dim //= 2\n        # use reflection padding in the last conv layer\n        setattr(self, \'block_{:d}\'.format(self.n_blocks), Conv2dBlock(dim + nz, output_dim, 7, 1, 3, norm=\'none\', activation=\'tanh\', pad_type=\'reflect\'))\n        self.n_blocks += 1\n\n    def forward(self, x, y=None):\n        if y is not None:\n            output = self.resnet_block(cat_feature(x, y))\n            for n in range(self.n_blocks):\n                block = getattr(self, \'block_{:d}\'.format(n))\n                if n > 0:\n                    output = block(cat_feature(output, y))\n                else:\n                    output = block(output)\n            return output\n\n\nclass Decoder(nn.Module):\n    def __init__(self, n_upsample, n_res, dim, output_dim, norm=\'batch\', activ=\'relu\', pad_type=\'zero\', nz=0):\n        super(Decoder, self).__init__()\n\n        self.model = []\n        # AdaIN residual blocks\n        self.model += [ResBlocks(n_res, dim, norm, activ, pad_type=pad_type, nz=nz)]\n        # upsampling blocks\n        for i in range(n_upsample):\n            if i == 0:\n                input_dim = dim + nz\n            else:\n                input_dim = dim\n            self.model += [Upsample(scale_factor=2), Conv2dBlock(input_dim, dim // 2, 5, 1, 2, norm=\'ln\', activation=activ, pad_type=\'reflect\')]\n            dim //= 2\n        # use reflection padding in the last conv layer\n        self.model += [Conv2dBlock(dim, output_dim, 7, 1, 3, norm=\'none\', activation=\'tanh\', pad_type=\'reflect\')]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, x, y=None):\n        if y is not None:\n            return self.model(cat_feature(x, y))\n        else:\n            return self.model(x)\n\n##################################################################################\n# Sequential Models\n##################################################################################\n\n\nclass ResBlocks(nn.Module):\n    def __init__(self, num_blocks, dim, norm=\'inst\', activation=\'relu\', pad_type=\'zero\', nz=0):\n        super(ResBlocks, self).__init__()\n        self.model = []\n        for i in range(num_blocks):\n            self.model += [ResBlock(dim, norm=norm, activation=activation, pad_type=pad_type, nz=nz)]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, x):\n        return self.model(x)\n\n\n##################################################################################\n# Basic Blocks\n##################################################################################\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, dim, norm=\'inst\', activation=\'relu\', pad_type=\'zero\', nz=0):\n        super(ResBlock, self).__init__()\n\n        model = []\n        model += [Conv2dBlock(dim + nz, dim, 3, 1, 1, norm=norm, activation=activation, pad_type=pad_type)]\n        model += [Conv2dBlock(dim, dim + nz, 3, 1, 1, norm=norm, activation=\'none\', pad_type=pad_type)]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        residual = x\n        out = self.model(x)\n        out += residual\n        return out\n\n\nclass Conv2dBlock(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, stride,\n                 padding=0, norm=\'none\', activation=\'relu\', pad_type=\'zero\'):\n        super(Conv2dBlock, self).__init__()\n        self.use_bias = True\n        # initialize padding\n        if pad_type == \'reflect\':\n            self.pad = nn.ReflectionPad2d(padding)\n        elif pad_type == \'zero\':\n            self.pad = nn.ZeroPad2d(padding)\n        else:\n            assert 0, ""Unsupported padding type: {}"".format(pad_type)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == \'batch\':\n            self.norm = nn.BatchNorm2d(norm_dim)\n        elif norm == \'inst\':\n            self.norm = nn.InstanceNorm2d(norm_dim, track_running_stats=False)\n        elif norm == \'ln\':\n            self.norm = LayerNorm(norm_dim)\n        elif norm == \'none\':\n            self.norm = None\n        else:\n            assert 0, ""Unsupported normalization: {}"".format(norm)\n\n        # initialize activation\n        if activation == \'relu\':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == \'lrelu\':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == \'prelu\':\n            self.activation = nn.PReLU()\n        elif activation == \'selu\':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == \'tanh\':\n            self.activation = nn.Tanh()\n        elif activation == \'none\':\n            self.activation = None\n        else:\n            assert 0, ""Unsupported activation: {}"".format(activation)\n\n        # initialize convolution\n        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, bias=self.use_bias)\n\n    def forward(self, x):\n        x = self.conv(self.pad(x))\n        if self.norm:\n            x = self.norm(x)\n        if self.activation:\n            x = self.activation(x)\n        return x\n\n\nclass LinearBlock(nn.Module):\n    def __init__(self, input_dim, output_dim, norm=\'none\', activation=\'relu\'):\n        super(LinearBlock, self).__init__()\n        use_bias = True\n        # initialize fully connected layer\n        self.fc = nn.Linear(input_dim, output_dim, bias=use_bias)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == \'batch\':\n            self.norm = nn.BatchNorm1d(norm_dim)\n        elif norm == \'inst\':\n            self.norm = nn.InstanceNorm1d(norm_dim)\n        elif norm == \'ln\':\n            self.norm = LayerNorm(norm_dim)\n        elif norm == \'none\':\n            self.norm = None\n        else:\n            assert 0, ""Unsupported normalization: {}"".format(norm)\n\n        # initialize activation\n        if activation == \'relu\':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == \'lrelu\':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == \'prelu\':\n            self.activation = nn.PReLU()\n        elif activation == \'selu\':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == \'tanh\':\n            self.activation = nn.Tanh()\n        elif activation == \'none\':\n            self.activation = None\n        else:\n            assert 0, ""Unsupported activation: {}"".format(activation)\n\n    def forward(self, x):\n        out = self.fc(x)\n        if self.norm:\n            out = self.norm(out)\n        if self.activation:\n            out = self.activation(out)\n        return out\n\n##################################################################################\n# Normalization layers\n##################################################################################\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, affine=True):\n        super(LayerNorm, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n\n        if self.affine:\n            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n            self.beta = nn.Parameter(torch.zeros(num_features))\n\n    def forward(self, x):\n        shape = [-1] + [1] * (x.dim() - 1)\n        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n        std = x.view(x.size(0), -1).std(1).view(*shape)\n        x = (x - mean) / (std + self.eps)\n\n        if self.affine:\n            shape = [1, -1] + [1] * (x.dim() - 2)\n            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n        return x\n'"
models/networks_3d.py,4,"b'import math\nfrom torch import nn\nimport torch\nfrom .basics import get_norm_layer, init_net\n\n\ndef define_G_3D(nz=200, res=128, model=\'G0\', ngf=64, norm=\'batch3d\',\n                init_type=\'xavier\', init_param=0.02, gpu_ids=[]):\n    if model == \'G0\':\n        netG = _netG0(bias=False, res=res, nz=nz, ngf=ngf, norm=norm)\n    else:\n        raise NotImplementedError(\'3D G [%s] is not implemented\' % model)\n    return init_net(netG, init_type, init_param, gpu_ids)\n\n\ndef define_D_3D(res=128, model=\'D0\', ndf=64, norm=\'none\',\n                init_type=\'xavier\', init_param=0.02, gpu_ids=[]):\n    if model == \'D0\':\n        netD = _netD0(bias=False, res=res, nc=1, ndf=ndf, norm=norm)\n    else:\n        raise NotImplementedError(\'3D D [%s] is not implemented\' % model)\n    return init_net(netD, init_type, init_param, gpu_ids)\n\n\ndef deconvBlock(input_nc, output_nc, bias, norm_layer=None, nl=\'relu\'):\n    layers = [nn.ConvTranspose3d(input_nc, output_nc, 4, 2, 1, bias=bias)]\n\n    if norm_layer is not None:\n        layers += [norm_layer(output_nc)]\n    if nl == \'relu\':\n        layers += [nn.ReLU(True)]\n    elif nl == \'lrelu\':\n        layers += [nn.LeakyReLU(0.2, inplace=True)]\n    else:\n        raise NotImplementedError(\'NL layer {} is not implemented\' % nl)\n    return nn.Sequential(*layers)\n\n\n# the last layer of a generator\ndef toRGB(input_nc, output_nc, bias, zero_mean=False, sig=True):\n    layers = [nn.ConvTranspose3d(input_nc, output_nc, 4, 2, 1, bias=bias)]\n    if sig:\n        layers += [nn.Sigmoid()]\n    return nn.Sequential(*layers)\n\n\nclass _netG0(nn.Module):\n    def __init__(self, bias, res, nz=200, ngf=64, max_nf=8, nc=1, norm=\'batch\'):\n        super(_netG0, self).__init__()\n        norm_layer = get_norm_layer(layer_type=norm)\n        self.res = res\n        self.block_0 = nn.Sequential(*[nn.ConvTranspose3d(nz, ngf * max_nf, 4, 1, 0, bias=bias), norm_layer(ngf * 8), nn.ReLU(True)])\n        self.n_blocks = 1\n        input_dim = ngf * max_nf\n        n_layers = int(math.log(res, 2)) - 3\n        for n in range(n_layers):\n            input_nc = int(max(ngf, input_dim))\n            output_nc = int(max(ngf, input_dim // 2))\n            setattr(self, \'block_{:d}\'.format(self.n_blocks), deconvBlock(input_nc, output_nc, bias, norm_layer=norm_layer, nl=\'relu\'))\n            input_dim /= 2\n            self.n_blocks += 1\n\n        setattr(self, \'toRGB_{:d}\'.format(res), toRGB(output_nc, nc, bias, sig=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input, return_stat=False):\n        output = input\n        for n in range(self.n_blocks):\n            block = getattr(self, \'block_{:d}\'.format(n))\n            output = block(output)\n        toRGB = getattr(self, \'toRGB_{:d}\'.format(self.res))\n        output = toRGB(output)\n        output = output / 2  # HACK\n        if return_stat:\n            stat = [output.max().item(), output.min().item(), output.std().item(), output.mean().item()]\n            return self.sigmoid(output), stat\n        else:\n            return self.sigmoid(output)\n\n\ndef convBlock(input_nc, output_nc, bias, norm_layer=None):\n    layers = [nn.Conv3d(input_nc, output_nc, 4, 2, 1, bias=bias)]\n    if norm_layer is not None:\n        layers += [norm_layer(output_nc)]\n    layers += [nn.LeakyReLU(0.2, inplace=True)]\n    return nn.Sequential(*layers)\n\n\nclass _netD0(nn.Module):\n    def __init__(self, bias=False, res=128, final_res=128, nc=1, ndf=64, max_nf=8, norm=\'none\'):\n        super(_netD0, self).__init__()\n        self.res = res\n        self.n_blocks = 0\n        norm_layer = get_norm_layer(layer_type=norm)\n        n_layers = int(math.log(res, 2)) - 3\n        n_final_layers = int(math.log(final_res, 2)) - 3\n        self.offset = n_final_layers - n_layers\n        setattr(self, \'fromRGB_{:d}\'.format(res), fromRGB(1, ndf * min(2 ** max(0, self.offset - 1), max_nf), bias))\n        for n in range(n_final_layers - n_layers, n_final_layers):\n            input_nc = ndf * min(2 ** max(0, n - 1), max_nf)\n            output_nc = ndf * min(2 ** n, max_nf)\n            block_name = \'block_{}\'.format(n)\n            setattr(self, block_name, convBlock(input_nc, output_nc, bias, norm_layer))\n            self.n_blocks += 1\n        block_name = \'block_{:d}\'.format(n_final_layers)\n        setattr(self, block_name, nn.Conv3d(ndf * max_nf, 1, 4, 1, 0, bias=bias))\n        self.n_blocks += 1\n\n    def forward(self, input):\n        fromRGB = getattr(self, \'fromRGB_{:d}\'.format(self.res))\n        output = fromRGB(input)\n        for n in range(self.n_blocks):\n            block = getattr(self, \'block_{:d}\'.format(n + self.offset))\n            output = block(output)\n        return output.view(-1, 1).squeeze(1)\n\n\n# the first layer of a discriminator\ndef fromRGB(input_nc, output_nc, bias):\n    layers = []\n    layers += [nn.Conv3d(input_nc, output_nc, 4, 2, 1, bias=bias)]\n    layers += [nn.LeakyReLU(0.2, inplace=True)]\n    return nn.Sequential(*layers)\n\n\ndef _calc_grad_penalty(netD, real_data, fake_data, device, type=\'mixed\', constant=1.0, lambda_gp=10.0):\n    """"""Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n    Arguments:\n        netD (network)              -- discriminator network\n        real_data (tensor array)    -- real images\n        fake_data (tensor array)    -- generated images from the generator\n        device (str)                -- GPU / CPU: from torch.device(\'cuda:{}\'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device(\'cpu\')\n        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2\n        lambda_gp (float)           -- weight for this loss\n    Returns the gradient penalty loss\n    """"""\n    if lambda_gp > 0.0:\n        if type == \'real\':   # either use real images, fake images, or a linear interpolation of two.\n            interpolatesv = real_data\n        elif type == \'fake\':\n            interpolatesv = fake_data\n        elif type == \'mixed\':\n            alpha = torch.rand(real_data.shape[0], 1)\n            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(*real_data.shape)\n            alpha = alpha.to(device)\n            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n        else:\n            raise NotImplementedError(\'{} not implemented\'.format(type))\n        interpolatesv.requires_grad_(True)\n        disc_interpolates = netD(interpolatesv)\n        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n                                        grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n                                        create_graph=True, retain_graph=True, only_inputs=True)\n        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp        # added eps\n        return gradient_penalty, gradients\n    else:\n        return 0.0, None\n'"
models/shape_gan_model.py,13,"b""import torch\nfrom .base_model import BaseModel\nfrom .networks_3d import _calc_grad_penalty\nfrom .networks import GANLoss\nimport numpy as np\nimport os\n\n\nclass ShapeGANModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--lambda_GAN_3D', type=float, default=1.0, help='GANLoss weight for end to end finetuning; set as 1.0 for shap generation; set as 0.05 for full model')\n        parser.add_argument('--lambda_gp_3D', type=float, default=10, help='WGANGP gradient penality coefficient')\n        parser.add_argument('--gan_mode_3D', type=str, default='wgangp', help='dcgan | lsgan | wgangp | hinge')\n        parser.add_argument('--gp_norm_3D', type=float, default=1.0, help='WGANGP gradient penality norm')\n        parser.add_argument('--gp_type_3D', type=str, default='mixed', help='WGANGP graident penalty type')\n        parser.add_argument('--vis_batch_num', type=int, default=2, help='number of batch to visulize on epoch end')\n        parser.add_argument('--lr_3d', type=float, default=0.0001, help='initial learning rate for adam')\n        return parser\n\n    def __init__(self, opt, base_init=True):\n        if base_init:\n            BaseModel.__init__(self, opt)\n        self.loss_names += ['G_3D', 'D_real_3D', 'D_fake_3D', 'D_3D']\n        self.nz_shape = opt.nz_shape\n        if self.isTrain:\n            self.model_names += ['D_3D']\n            if opt.lambda_gp_3D > 0.0:\n                self.loss_names += ['GP_3D']\n\n        if 'G_3D' not in self.model_names:\n            self.model_names += ['G_3D']\n            self.netG_3D = self.define_G_3D()\n\n        if self.isTrain:\n            self.netD_3D = self.define_D_3D()\n\n        if self.isTrain:\n            self.critGAN_3D = GANLoss(gan_mode=opt.gan_mode_3D).to(self.device)\n            self.optimizer_G_3D = torch.optim.Adam(self.netG_3D.parameters(), lr=opt.lr_3d, betas=(opt.beta1, 0.9))\n            self.optimizer_D_3D = torch.optim.Adam(self.netD_3D.parameters(), lr=opt.lr_3d, betas=(opt.beta1, 0.9))\n            self.optimizers += [self.optimizer_G_3D, self.optimizer_G_3D]\n            for name in self.loss_names:\n                setattr(self, 'loss_' + name, 0.0)\n                setattr(self, 'count_' + name, 0)\n        self.cuda_names += ['z_shape', 'voxel_real']\n        self.deduplicate_names()\n\n    def set_input(self, input):\n        self.voxel_real = input['voxel'].to(self.device)\n        if self.voxel_real.dim() == 4:\n            self.voxel_real = self.voxel_real.unsqueeze(1)\n        self.bs = self.voxel_real.shape[0]\n        self.z_shape = self.get_z_random(self.bs, self.nz_shape).view(self.bs, self.nz_shape, 1, 1, 1)\n        self.move_to_cuda()\n\n    def _record_loss(self, name, value):\n        loss = getattr(self, 'loss_' + name)\n        count = getattr(self, 'count_' + name)\n        v = loss * count / (count + 1)\n\n        if type(value) != float:\n            value = value.item()\n        v += value / (count + 1)\n        setattr(self, 'count_' + name, count + 1)\n        setattr(self, 'loss_' + name, v)\n\n    def backward_D_3D(self):\n        self.z_shape.normal_(0, 1)\n        with torch.no_grad():\n            fake = self.netG_3D(self.z_shape)\n        pred_fake = self.netD_3D(fake)\n        pred_real = self.netD_3D(self.voxel_real)\n        errD_fake = self.critGAN_3D(pred_fake, False)\n        errD_real = self.critGAN_3D(pred_real, True)\n        loss_GP, gradients = _calc_grad_penalty(self.netD_3D, self.voxel_real, fake.detach(), self.device, type=self.opt.gp_type_3D, constant=self.opt.gp_norm_3D,\n                                                lambda_gp=self.opt.lambda_gp_3D * self.opt.lambda_GAN_3D)\n\n        if type(loss_GP) != float:\n            loss_GP.backward(retain_graph=True)\n            self._record_loss('GP_3D', loss_GP)\n        errD = (errD_fake + errD_real) * self.opt.lambda_GAN_3D\n        errD.backward()\n        self._record_loss('D_real_3D', errD_real)\n        self._record_loss('D_fake_3D', errD_fake)\n        self._record_loss('D_3D', errD_real + errD_fake)\n\n    def backward_G_3D(self):\n        self.z_shape.normal_(0, 1)\n        fake = self.netG_3D(self.z_shape, return_stat=False)\n        if self.opt.print_grad:  # HACK\n            fake.register_hook(self.grad_hook_gen('3d_grad'))\n        pred_fake = self.netD_3D(fake)\n        errG = self.critGAN_3D(pred_fake, True) * self.opt.lambda_GAN_3D\n        self._record_loss('G_3D', errG)\n        errG.backward()\n\n    def update_D(self):\n        self.optimizer_D_3D.zero_grad()\n        self.backward_D_3D()\n        self.optimizer_D_3D.step()\n\n    def update_G(self):\n        self.set_requires_grad([self.netD_3D], False)\n        self.optimizer_G_3D.zero_grad()\n        self.backward_G_3D()\n        self.optimizer_G_3D.step()\n        self.set_requires_grad([self.netD_3D], True)\n\n    def save_networks(self, epoch):\n        BaseModel.save_networks(self, epoch)\n        epoch_path = os.path.join(self.save_dir, 'epoch_%s' % epoch)\n        random_vec_path = os.path.join(self.save_dir, 'random_vec.pt')\n        os.makedirs(epoch_path, exist_ok=True)\n        if not os.path.exists(random_vec_path):  # save random z vector\n            noise = torch.zeros(self.opt.vis_batch_num, self.opt.batch_size, self.opt.nz_shape, 1, 1, 1).float().normal_()\n            torch.save(noise, random_vec_path)\n        else:\n            noise = torch.load(random_vec_path)\n\n        for batch_id in range(noise.shape[0]):   # save cached results\n            batchz_shape = noise[batch_id, :, :, :, :, :].to(self.device)\n            with torch.no_grad():\n                pred = self.netG_3D(batchz_shape)\n            batch_name = os.path.join(epoch_path, 'vali_%04d' % batch_id)\n            np.savez(batch_name, pred=pred.cpu().numpy())\n\n    def sample(self, k=10, interp_traj=2, step=10):\n        if k % 2 != 0:\n            k += 1\n        noise = torch.zeros(2, self.opt.nz_shape, 1, 1, 1).float().normal_().to(self.device)\n        self.netG_3D.eval()\n        sample_shapes = []\n        for idx in range(k // 2):\n            with torch.no_grad():\n                noise.normal_(0, 1)\n                pred = self.netG_3D(noise)\n                pred = pred.transpose(2, 3)\n                pred = torch.flip(pred, [2])\n                pred_numpy = pred.data.cpu().numpy()\n                sample_shapes.append(pred_numpy[0, 0, :, :, :])\n                sample_shapes.append(pred_numpy[1, 0, :, :, :])\n        interp_traj_list = []\n        noise_end = noise.clone()\n        with torch.no_grad():\n            for k in range(interp_traj // 2):\n                noise_end.normal_(0, 1)\n                noise.normal_(0, 1)\n                traj_1 = []\n                traj_2 = []\n                for alpha in torch.linspace(0, 1, step, device=noise.device):\n                    interpz_shape = alpha * noise + (1 - alpha * noise_end)\n                    pred = self.netG_3D(interpz_shape)\n                    pred = pred.transpose(2, 3)\n                    pred = torch.flip(pred, [2])\n                    pred_numpy = pred.data.cpu().numpy()\n                    traj_1.append(pred_numpy[0, 0, :, :, :])\n                    traj_2.append(pred_numpy[1, 0, :, :, :])\n                interp_traj_list.append(traj_1[:])\n                interp_traj_list.append(traj_2[:])\n        return sample_shapes, interp_traj_list\n"""
models/test_model.py,11,"b""from .base_model import BaseModel\nimport numpy as np\nimport torch\n\n\nclass TestModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        return parser\n\n    def __init__(self, opt):\n        assert(not opt.isTrain)\n        BaseModel.__init__(self, opt)\n        self.vae = True\n        # specify the training losses you want to print out. The program will call base_model.get_current_losses\n        self.loss_names = []\n        # specify the images you want to save/display. The program will call base_model.get_current_visuals\n        self.visual_names = ['mask', 'depth', 'image']\n        # specify the models you want to save to the disk. The program will call base_model.save_networks and base_model.load_networks\n        self.model_names = ['G_AB', 'G_3D']\n        self.model_names += 'E'\n        self.cuda_names = ['z_shape', 'z_texture', 'rot_mat']\n        self.use_df = opt.use_df or opt.dataset_mode.find('df') >= 0\n\n        self.netG_3D = self.define_G_3D()\n        self.netG_AB = self.define_G(opt.input_nc, opt.output_nc, opt.nz_texture, ext='AB')\n        self.netE = self.define_E(opt.output_nc, self.vae)\n        self.is_loaded = True\n        self.n_views = opt.n_views\n        self.bs = opt.batch_size\n        self.nz_shape = opt.nz_shape\n        self.nz_texture = opt.nz_texture\n        self.n_shapes = opt.n_shapes\n        self.setup_DR(opt)\n        self.bg_B = 1\n        self.bg_A = -1\n        self.random_view = opt.random_view\n        self.interp_shape = opt.interp_shape\n        self.interp_texture = opt.interp_texture\n        self.critCycle = torch.nn.L1Loss().to(self.device)\n        with torch.no_grad():\n            self.z0_s = self.get_z_random(self.bs, self.nz_shape).view(self.bs, self.nz_shape, 1, 1, 1)\n            self.z1_s = self.get_z_random(self.bs, self.nz_shape).view(self.bs, self.nz_shape, 1, 1, 1)\n            self.z0_t = self.get_z_random(self.bs, self.nz_texture).view(self.bs, self.nz_texture, 1, 1, 1)\n            self.z1_t = self.get_z_random(self.bs, self.nz_texture).view(self.bs, self.nz_texture, 1, 1, 1)\n\n        self.count = 0\n\n    def set_input(self, input, reset_shape=False, reset_texture=False):\n        self.input_B = input[0]['image'].to(self.device)\n        self.mask_B = input[0]['real_im_mask'].to(self.device)\n        if reset_shape or not hasattr(self, 'voxel'):\n            self.voxel = input[1]['voxel'].to(self.device)\n        if reset_texture or not hasattr(self, 'z_texture'):\n            with torch.no_grad():\n                self.z_texture, mu, var = self.encode(self.input_B, vae=self.vae)\n                self.z_texture = mu\n\n    def set_posepool(self, posepool):\n        self.posepool = posepool\n        self.total_views = posepool.shape[0]\n        if not self.random_view:\n            self.elevation = (posepool[:, 0] // 3) * 3\n            self.azimuth = (posepool[:, 1] // 5) * 5\n            views = np.zeros((self.n_views, 2))\n            hist, bins = np.histogram(self.azimuth, bins=range(-90, 91, 5))\n            sort_ids = hist.argsort()[::-1][: self.n_views]\n            top_a = bins[sort_ids]\n            top_a.sort()\n            for n, az in enumerate(top_a):\n                ids = np.where(self.azimuth == az)\n                ele = self.elevation[ids]\n                values, counts = np.unique(ele, return_counts=True)\n                counts[0] = 0\n                id2 = np.argmax(counts)\n                views[n, 0] = values[id2]\n                views[n, 1] = az\n            self.views = views\n\n    def reset_shape(self, reset=True):\n        if reset or not hasattr(self, 'z_shape'):\n            if self.interp_shape:\n                alpha = self.count / float(self.n_shapes)\n                self.z_shape = (1 - alpha) * self.z0_s + alpha * self.z1_s\n            else:\n                self.z_shape = self.get_z_random(self.bs, self.nz_shape).view(self.bs, self.nz_shape, 1, 1, 1)\n            self.z_shape = self.z_shape.to(self.device)\n\n    def reset_texture(self, reset=True):\n        if reset or not hasattr(self, 'z_texture'):\n            if self.interp_texture:\n                alpha = self.count / float(self.n_shapes)\n                self.z_texture = (1 - alpha) * self.z0_t + alpha * self.z1_t\n            else:\n                self.z_texture = self.get_z_random(self.bs, self.nz_texture)\n\n            self.z_texture = self.z_texture.to(self.device)\n\n    def reset_view(self, reset=False):\n        if self.random_view:\n            if reset or not hasattr(self, 'views'):\n                rand_ids = np.random.randint(self.total_views, size=self.n_views)\n                self.views = self.posepool[rand_ids, :]\n\n    def sample_3d(self):\n        with torch.no_grad():\n            self.voxel = self.netG_3D(self.z_shape)\n\n    def sample_2d(self, view_id, extra=False):\n        assert view_id >= 0 and view_id <= self.n_views\n        view = self.views[view_id, :]\n        with torch.no_grad():\n            self.rot_mat = self.azele2matrix(az=view[1], ele=view[0]).unsqueeze(0).repeat(self.bs, 1, 1)\n            self.rot_mat = self.rot_mat.to(self.device)\n            self.mask, self.depth = self.get_depth(self.voxel, self.rot_mat, use_df=self.use_df)\n            self.image = self.apply_mask(self.netG_AB(self.depth, self.z_texture), self.mask, self.bg_B)\n        if extra:\n            return self.image, self.mask, self.depth\n        else:\n            return self.image\n\n    @staticmethod\n    def azele2matrix(az=0, ele=0):\n        R0 = torch.zeros([3, 3])\n        R = torch.zeros([3, 4])\n        R0[0, 1] = 1\n        R0[1, 0] = -1\n        R0[2, 2] = 1\n        az = az * np.pi / 180\n        ele = ele * np.pi / 180\n        cos = np.cos\n        sin = np.sin\n        R_ele = torch.FloatTensor(\n            [[1, 0, 0], [0, cos(ele), -sin(ele)], [0, sin(ele), cos(ele)]])\n        R_az = torch.FloatTensor(\n            [[cos(az), -sin(az), 0], [sin(az), cos(az), 0], [0, 0, 1]])\n        R_rot = torch.mm(R_az, R_ele)\n        R_all = torch.mm(R_rot, R0)\n        R[:3, :3] = R_all\n        return R\n\n    def update_D(self):\n        pass\n\n    def update_G(self):\n        pass\n"""
models/texture_model.py,1,"b""import torch\nfrom .texture_real_model import TextureRealModel\n\n\nclass TextureModel(TextureRealModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        TextureRealModel.modify_commandline_options(parser, is_train)\n        return parser\n\n    def __init__(self, opt, base_init=True):\n        assert opt.input_nc == 1 and opt.output_nc == 3\n        TextureRealModel.__init__(self, opt, base_init)\n        self.nz_shape = opt.nz_shape\n        self.netG_3D = self.define_G_3D()\n        self.netG_3D.eval()\n        self.model_names.append('G_3D')\n        self.cuda_names.append('z_shape')\n        self.cuda_names.remove('voxel_real')\n        self.deduplicate_names()\n\n    def set_input(self, input):\n        self.input_B = input[0]['image']\n        self.mask_B = input[0]['real_im_mask']\n        self.rot_mat = input[0]['rotation_matrix']\n        self.bs = self.input_B.size(0)\n        self.is_skip = self.bs < self.opt.batch_size\n        if self.is_skip:\n            return\n        self.z_texture = self.get_z_random(self.bs, self.nz_texture)\n        self.z_shape = self.get_z_random(self.bs, self.nz_shape).view(self.bs, self.nz_shape, 1, 1, 1)\n        self.move_to_cuda()\n        with torch.no_grad():\n            mask_A_full, real_A_full, _ = self.safe_render(self.netG_3D, self.bs, self.nz_shape, flipped=False)\n            self.mask_A, self.real_A, self.mask_B, self.real_B = self.crop_image(mask_A_full, real_A_full, self.mask_B, self.input_B)\n"""
models/texture_real_model.py,5,"b""from .base_model import BaseModel\nfrom .networks import _cal_kl, GANLoss\nfrom util.image_pool import ImagePool\nimport itertools\nimport torch\nfrom .networks_3d import _calc_grad_penalty\n\n\nclass TextureRealModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        parser.add_argument('--lambda_cycle_A', type=float, default=10.0, help='weight for forward cycle')\n        parser.add_argument('--lambda_cycle_B', type=float, default=25.0, help='weight for backward cycle')\n        parser.add_argument('--lambda_z', type=float, default=1.0, help='weight for ||E(G(random_z)) - random_z||')\n        parser.add_argument('--lambda_kl_real', type=float, default=0.001, help='weight for KL loss, real')\n        parser.add_argument('--lambda_mask', type=float, default=2.5, help='mask loss')\n        parser.add_argument('--gp_norm', type=float, default=1.0, help='WGANGP gradient penality norm')\n        parser.add_argument('--gp_type', type=str, default='mixed', help='WGANGP graident penalty type')\n        parser.add_argument('--lambda_gp', type=float, default=10, help='WGANGP gradient penality coefficient')\n\n        return parser\n\n    def __init__(self, opt, base_init=True):\n        assert opt.input_nc == 1 and opt.output_nc == 3\n        if base_init:\n            BaseModel.__init__(self, opt)\n        self.nz_texture = opt.nz_texture\n        self.use_df = opt.use_df or opt.dataset_mode.find('df') >= 0\n        self.vae = opt.lambda_kl_real > 0.0\n        self.bg_B = 1\n        self.bg_A = -1\n        if self.isTrain:\n            self.model_names += ['G_AB', 'G_BA', 'D_A', 'D_B', 'E']\n        else:\n            self.model_names += ['G_AB', 'G_BA', 'E']\n\n        # load/define networks: define G\n        self.netG_AB = self.define_G(opt.input_nc, opt.output_nc, opt.nz_texture, ext='AB')\n        self.netG_BA = self.define_G(opt.output_nc, opt.input_nc, 0, ext='BA')\n        self.netE = self.define_E(opt.output_nc, self.vae)\n        # define D\n        if opt.isTrain:\n            self.netD_A = self.define_D(opt.output_nc, ext='A')\n            self.netD_B = self.define_D(opt.input_nc, ext='B')\n        self.setup_DR(opt)\n        self.visual_names += ['real_A', 'mask_A', 'fake_B', 'rec_A', 'real_B', 'mask_B', 'fake_A', 'rec_B']\n        self.loss_names += ['G', 'G_AB', 'G_BA', 'cycle_A', 'cycle_B', 'cycle_z', 'D_A', 'D_B']\n        self.cuda_names += ['input_B', 'rot_mat', 'mask_B', 'voxel_real', 'z_texture']\n        if opt.gan_mode == 'wgangp':\n            self.loss_names += ['gp_A', 'gp_B']\n\n        if opt.lambda_kl_real > 0.0:\n            self.loss_names += ['kl_real', 'mu_enc', 'var_enc']\n        if opt.lambda_mask > 0.0:\n            self.loss_names += ['mask_B']\n\n        if opt.isTrain:\n            self.fake_A_pool = ImagePool(opt.pool_size)\n            self.fake_B_pool = ImagePool(opt.pool_size)\n            self.critGAN = GANLoss(gan_mode=opt.gan_mode).to(self.device)\n            self.critCycle = torch.nn.L1Loss().to(self.device)\n            # initialize optimizers\n            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_AB.parameters(), self.netG_BA.parameters(), self.netE.parameters()),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers += [self.optimizer_G, self.optimizer_D]\n\n    def set_input(self, input):\n        self.input_B = input[0]['image']\n        self.mask_B = input[0]['real_im_mask']\n        self.rot_mat = input[0]['rotation_matrix']\n        self.bs = self.input_B.size(0)\n        self.is_skip = self.bs < self.opt.batch_size\n        if self.is_skip:\n            return\n        self.z_texture = self.get_z_random(self.bs, self.nz_texture)\n        self.voxel_real = input[1]['voxel']\n        self.move_to_cuda()\n        mask_A_full, real_A_full = self.get_depth(self.voxel_real, self.rot_mat, use_df=self.use_df)\n        self.mask_A, self.real_A, self.mask_B, self.real_B = self.crop_image(mask_A_full, real_A_full, self.mask_B, self.input_B)\n\n    def backward_D_B(self):\n        fake_A = self.fake_A_pool.query(self.fake_A)\n        self.loss_D_B, self.loss_gp_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n\n    def backward_D_A(self):\n        fake_B = self.fake_B_pool.query(self.fake_B)\n        self.loss_D_A, self.loss_gp_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n\n    def backward_D_basic(self, netD, real, fake):\n        loss_D_real = self.critGAN(netD(real.detach()), True)  # real\n        loss_D_fake = self.critGAN(netD(fake.detach()), False)  # fake\n        loss_D = loss_D_real + loss_D_fake  # combined loss\n        loss_D.backward()  # backward\n\n        if self.opt.gan_mode == 'wgangp':\n            loss_gp, _ = _calc_grad_penalty(netD, real, fake.detach(), self.device, type='mixed', constant=self.opt.gp_norm, ll=self.opt.lambda_gp)\n            loss_gp.backward(retain_graph=True)\n        else:\n            loss_gp = 0.0\n\n        return loss_D, loss_gp\n\n    def backward_GE(self):\n        # GAN loss D_A(G_A(A))\n        self.fake_B = self.apply_mask(self.netG_AB(self.real_A, self.z_texture), self.mask_A, self.bg_B)\n        self.loss_G_AB = self.critGAN(self.netD_A(self.fake_B), True)\n        # GAN loss D_B(G_B(B))\n        self.fake_A = self.apply_mask(self.netG_BA(self.real_B), self.mask_B, self.bg_A)\n        self.loss_G_BA = self.critGAN(self.netD_B(self.fake_A), True)\n        # Forward image cycle loss\n        self.rec_A = self.apply_mask(self.netG_BA(self.fake_B), self.mask_A, self.bg_A)\n        self.loss_cycle_A = self.critCycle(self.rec_A, self.real_A.detach()) * self.opt.lambda_cycle_A\n        # Backward latent cycle loss\n        self.z_encoded, mu1, logvar1 = self.encode(self.real_B, self.vae)\n        if self.opt.lambda_kl_real > 0.0:\n            self.loss_mu_enc = torch.mean(torch.abs(mu1))\n            self.loss_var_enc = torch.mean(logvar1.exp())\n        self.rec_B = self.apply_mask(self.netG_AB(self.fake_A, self.z_encoded), self.mask_B, self.bg_B)\n        self.loss_cycle_B = self.critCycle(self.rec_B, self.real_B) * self.opt.lambda_cycle_B\n        # latent cycle loss\n        z_predict, mu2, logvar2 = self.encode(self.fake_B, self.vae)\n        self.loss_cycle_z = self.critCycle(z_predict, self.z_texture) * self.opt.lambda_z\n        self.loss_kl_real = _cal_kl(mu1, logvar1, self.opt.lambda_kl_real)\n        # mask B consistency loss\n        self.loss_mask_B = self.critCycle(self.fake_A, self.mask_B * 2 - 1) * self.opt.lambda_mask\n        # combined loss\n        self.loss_G = self.loss_G_AB + self.loss_G_BA + self.loss_cycle_A + self.loss_cycle_B \\\n            + self.loss_cycle_z + self.loss_kl_real + self.loss_mask_B\n        self.loss_G.backward()\n\n    def update_D(self):\n        self.set_requires_grad([self.netD_A, self.netD_B], True)\n        self.optimizer_D.zero_grad()\n        self.backward_D_A()\n        self.backward_D_B()\n        self.optimizer_D.step()\n\n    def update_G(self):\n        self.set_requires_grad([self.netD_A, self.netD_B], False)\n        self.optimizer_G.zero_grad()\n        self.backward_GE()\n        self.optimizer_G.step()\n"""
options/__init__.py,0,b''
options/base_options.py,1,"b'import argparse\nimport os\nfrom util import util\nimport pickle\nimport models\nimport data\nimport torch\n\n\nclass BaseOptions():\n    """"""This class defines options used during both training and test time.\n\n    It also implements several helper functions such as parsing, printing, and saving the options.\n    It also gathers additional options defined in <modify_commandline_options> functions in both dataset class and model class.\n    """"""\n\n    def __init__(self):\n        self.initialized = False\n\n    def initialize(self, parser):\n        parser.add_argument(\'--dataroot\', type=str, default=None, help=\'path to images (should have subfolders trainA, trainB, valA, valB, etc)\')\n        parser.add_argument(\'--batch_size\', type=int, default=12, help=\'batch size\')\n        parser.add_argument(\'--load_size\', type=int, default=128, help=\'scale images to this size\')\n        parser.add_argument(\'--crop_size\', type=int, default=128, help=\'then crop to this size\')\n        parser.add_argument(\'--input_nc\', type=int, default=1, help=\'# of input image channels\')\n        parser.add_argument(\'--output_nc\', type=int, default=3, help=\'# of output image channels\')\n        parser.add_argument(\'--nz_texture\', type=int, default=8, help=\'the dimension of texture code\')\n        parser.add_argument(\'--nz_shape\', type=int, default=200, help=\'the dimension of shape code\')\n        parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2, -1 for CPU mode\')\n        parser.add_argument(\'--name\', type=str, default=\'experiment_name\', help=\'name of the experiment. It decides where to store samples and models\')\n        parser.add_argument(\'--model\', type=str, default=\'base\', help=\'choose which model to use: base | shape_gan | texture_real | texture | full | test\')\n        parser.add_argument(\'--epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        parser.add_argument(\'--phase\', type=str, default=\'val\', help=\'train | val | test, etc\')\n        parser.add_argument(\'--num_threads\', default=6, type=int, help=\'# sthreads for loading data\')\n        parser.add_argument(\'--checkpoints_dir\', type=str, default=\'../../results_texture/\', help=\'models are saved here\')\n        parser.add_argument(\'--display_winsize\', type=int, default=128, help=\'display window size\')\n        # dataset\n        parser.add_argument(\'--dataset_mode\', type=str, default=\'base\', help=\'chooses how datasets are loaded: base | image_and_df | image_and_voxel | df | voxel\')\n        parser.add_argument(\'--resize_or_crop\', type=str, default=\'crop_real_im\', help=\'crop_real_im | resize_and_crop | crop | scale_width | scale_width_and_crop\')\n        parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n        # models\n        parser.add_argument(\'--num_Ds\', type=int, default=2, help=\'the number of discriminators\')\n        parser.add_argument(\'--netD\', type=str, default=\'multi\', help=\'selects model to use for netD: single | multi\')\n        parser.add_argument(\'--netG\', type=str, default=\'resnet_cat\', help=\'selects model to use for netG: unet | resnet_cat\')\n        parser.add_argument(\'--use_dropout\', action=\'store_true\', help=\'use dropout for the generator\')\n        parser.add_argument(\'--netE\', type=str, default=\'adaIN\', help=\'selects model to use for netE: resnet | conv | adaIN\')\n        parser.add_argument(\'--where_add\', type=str, default=\'all\', help=\'where to add z in the network G: input | all\')\n        parser.add_argument(\'--netG_3D\', type=str, default=\'G0\', help=\'selects model to use for netG_3D: G0\')\n        parser.add_argument(\'--netD_3D\', type=str, default=\'D0\', help=\'selects model to use for netD_3D: D0\')\n        parser.add_argument(\'--norm\', type=str, default=\'inst\', help=\'instance normalization or batch normalization: batch | inst | none\')\n        parser.add_argument(\'--nl\', type=str, default=\'relu\', help=\'non-linearity activation: relu | lrelu | elu (we hard-coded lrelu for the discriminator)\')\n        parser.add_argument(\'--G_norm_3D\', type=str, default=\'batch3d\', help=\'normalization layer for G: inst3d | batch3d | none\')\n        parser.add_argument(\'--D_norm_3D\', type=str, default=\'none\', help=\'normalization layer for D: inst3d | batch3d | none\')\n        # number of channels in our networks\n        parser.add_argument(\'--nef\', type=int, default=64, help=\'# of encoder filters in the first conv layer\')\n        parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in the last conv layer\')\n        parser.add_argument(\'--ndf\', type=int, default=64, help=\'# of discrim filters in the first conv layer\')\n        parser.add_argument(\'--ngf_3d\', type=int, default=64, help=\'# of 3D gen filters in the last conv layer\')\n        parser.add_argument(\'--ndf_3d\', type=int, default=64, help=\'# of 3D discrim filters in the first conv layer\')\n        # extra parameters\n        parser.add_argument(\'--gan_mode\', type=str, default=\'lsgan\', help=\'dcgan | lsgan | wgangp | hinge\')  # for 2D texture network; not for 3D; use gan_mode_3D for 3D shape\n        parser.add_argument(\'--init_type\', type=str, default=\'kaiming\', help=\'network initialization: normal | xavier | kaiming | orth\')\n        parser.add_argument(\'--init_param\', type=float, default=0.02, help=\'scaling factor for normal, xavier and orthogonal.\')\n        # 3D paramters:\n        parser.add_argument(\'--voxel_res\', type=int, default=128, help=\'the resolution of voxelized data\')\n        parser.add_argument(\'--class_3d\', type=str, default=\'car\', choices=[\'car\', \'chair\'], help=\'3d model class\')\n        parser.add_argument(\'--model3D_dir\', type=str, default=None, help=\'directory to store pretrained 3D model\')\n        parser.add_argument(\'--model2D_dir\', type=str, default=None, help=\'directory to store pretrained 2D model\')\n        parser.add_argument(\'--use_df\', action=\'store_true\', help=\'use distance function (DF) representation\')\n        parser.add_argument(\'--df_th\', type=float, default=0.90, help=\'threshold for rendering distance function (DF)\')\n        # misc\n        parser.add_argument(\'--no_largest\', action=\'store_true\', help=\'disable using the largest connected component during rendering\')\n        parser.add_argument(\'--crop_align\', action=\'store_true\', help=\'if the croping is aligned between real and fake\')\n        parser.add_argument(\'--suffix\', default=\'\', type=str, help=\'customized suffix: e.g., {netD}_{netG}_voxel{voxel_res}\')\n        parser.add_argument(\'--verbose\', action=\'store_true\', help=\'if specified, print more debugging information\')\n        parser.add_argument(\'--print_grad\', action=\'store_true\', help=\'if print grad for 2D and 3D gan loss\')\n        parser.add_argument(\'--seed\', type=int, default=0, help=\'seed\')\n        self.initialized = True\n        return parser\n\n    def gather_options(self):\n        """"""Initialize our parser with basic options(only once).\n        Add additional model-specific and dataset-specific options.\n        These options are difined in the <modify_commandline_options> function\n        in model and dataset classes.\n        """"""\n        if not self.initialized:  # check if it has been initialized\n            parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n            parser = self.initialize(parser)\n\n        # get the basic options\n        opt, _ = parser.parse_known_args()\n\n        # modify model-related parser options\n        model_name = opt.model\n        model_option_setter = models.get_option_setter(model_name)\n        parser = model_option_setter(parser, self.isTrain)\n        opt, _ = parser.parse_known_args()  # parse again with new defaults\n\n        # modify dataset-related parser options\n        dataset_name = opt.dataset_mode\n        dataset_option_setter = data.get_option_setter(dataset_name)\n        parser = dataset_option_setter(parser, self.isTrain)\n\n        # save and return the parser\n        self.parser = parser\n        return parser.parse_args()\n\n    def print_options(self, opt):\n        """"""Print and save options\n\n        It will print both current options and default values(if different).\n        It will save options into a text file / [checkpoints_dir] / opt.txt\n        """"""\n        message = \'\'\n        message += \'----------------- Options ---------------\\n\'\n        for k, v in sorted(vars(opt).items()):\n            comment = \'\'\n            default = self.parser.get_default(k)\n            if v != default:\n                comment = \'\\t[default: %s]\' % str(default)\n            message += \'{:>25}: {:<30}{}\\n\'.format(str(k), str(v), comment)\n        message += \'----------------- End -------------------\'\n        print(message)\n\n        # save to the disk\n        if self.isTrain:\n            expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n            util.mkdirs(expr_dir)\n            file_name = os.path.join(expr_dir, \'train_opt.txt\')\n            with open(file_name, \'wt\') as opt_file:\n                opt_file.write(message)\n                opt_file.write(\'\\n\')\n            pkl_file = os.path.join(expr_dir, \'train_opt.pkl\')\n            pickle.dump(opt, open(pkl_file, \'wb\'))\n        else:\n            util.mkdirs(opt.results_dir)\n\n    def parse(self):\n        """"""Parse our options, create checkpoints directory suffix, and set up gpu device.""""""\n        opt = self.gather_options()\n        opt.isTrain = self.isTrain   # train or test\n\n        # process opt.suffix\n        if opt.suffix:\n            opt.name = (opt.suffix.format(**vars(opt))) if opt.suffix != \'\' else \'\'\n\n        self.print_options(opt)\n\n        # set gpu ids\n        str_ids = opt.gpu_ids.split(\',\')\n        opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                opt.gpu_ids.append(id)\n        if len(opt.gpu_ids) > 0:\n            torch.cuda.set_device(opt.gpu_ids[0])\n\n        self.opt = opt\n        return self.opt\n'"
options/test_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self, parser):\n        BaseOptions.initialize(self, parser)\n        parser.add_argument('--results_dir', type=str, default='../results/', help='saves results here.')\n        parser.add_argument('--n_shapes', type=int, default=10, help='the number of sampled shapes')\n        parser.add_argument('--n_views', type=int, default=10, help='the number of sampled views')\n        parser.add_argument('--reset_shape', action='store_true', help='sample a different shape')\n        parser.add_argument('--reset_texture', action='store_true', help='sample a different texture')\n        parser.add_argument('--real_shape', action='store_true', help='use real voxels')\n        parser.add_argument('--real_texture', action='store_true', help='use real textures')\n        parser.add_argument('--render_3d', action='store_true', help='use blender to render 3d')\n        parser.add_argument('--render_25d', action='store_true', help='use blender to render 2.5d')\n        parser.add_argument('--random_view', action='store_true', help='show random views')\n        parser.add_argument('--show_input', action='store_true', help='show input image')\n        parser.add_argument('--interp_shape', action='store_true', help='interpolate in shape space')\n        parser.add_argument('--interp_texture', action='store_true', help='interpolate in texture space')\n        parser.add_argument('--aspect_ratio', type=float, default=1.0, help='aspect ratio for the results')\n        parser.add_argument('--ios_th', default=0.01, type=float, help='thresholding for isosurface')\n\n        self.isTrain = False\n        return parser\n"""
options/train_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self, parser):\n        BaseOptions.initialize(self, parser)\n        # display and print\n        parser.add_argument('--display_freq', type=int, default=400, help='frequency of showing training results on screen')\n        parser.add_argument('--display_id', type=int, default=1, help='window id of the web display')\n        parser.add_argument('--display_port', type=int, default=8097, help='visdom display port')\n        parser.add_argument('--update_html_freq', type=int, default=4000, help='frequency of saving training results to html')\n        parser.add_argument('--print_freq', type=int, default=400, help='frequency of showing training results on console')\n        parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        # saving and training models\n        parser.add_argument('--save_latest_freq', type=int, default=10000, help='frequency of saving the latest results')\n        parser.add_argument('--save_epoch_freq', type=int, default=10, help='frequency of saving checkpoints at the end of epochs')\n        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n        parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n        # learning rate\n        parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n        parser.add_argument('--lr_policy', type=str, default='linear', help='learning rate policy: linear | step')\n        parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n        self.isTrain = True\n        return parser\n"""
render_module/__init__.py,0,b''
render_module/render_sketch.py,40,"b""try:\n    from .vtn.vtn.functions import grid_sample3d, affine_grid3d\nexcept ImportError:\n    from vtn.vtn.functions import grid_sample3d, affine_grid3d\ntry:\n    from .calc_prob.calc_prob.functions.calc_prob import CalcStopProb\nexcept ImportError:\n    from calc_prob.calc_prob.functions.calc_prob import CalcStopProb\nfrom torch import nn\nimport torch\nimport numpy as np\nfrom torch.autograd import Variable\nfrom torch.nn.functional import grid_sample\nfrom scipy import ndimage\n\n\ndef azele2matrix(az=0, ele=0):\n    R0 = torch.zeros([3, 3])\n    R0[0, 1] = 1\n    R0[1, 0] = -1\n    R0[2, 2] = 1\n    az = az * np.pi / 180\n    ele = ele * np.pi / 180\n    cos = np.cos\n    sin = np.sin\n    R_ele = torch.from_numpy(np.array([[1, 0, 0], [0, cos(ele), -sin(ele)], [0, sin(ele), cos(ele)]])).float()\n    R_az = torch.from_numpy(np.array([[cos(az), -sin(az), 0], [sin(az), cos(az), 0], [0, 0, 1]])).float()\n    R_rot = torch.mm(R_az, R_ele)\n    R_all = torch.mm(R_rot, R0)\n    return R_all\n\n\nclass GetRotationMatrix(nn.Module):\n    def __init__(self, az_min=-np.pi / 2, az_max=np.pi / 2, ele_min=0, ele_max=2 * np.pi / 9):\n        super().__init__()\n        self.az_max = az_max\n        self.az_min = az_min\n        self.ele_max = ele_max\n        self.ele_min = ele_min\n\n    def forward(self, angles_in):\n        is_cuda = angles_in.is_cuda\n        assert(angles_in.shape[1] == 2)\n        bn = angles_in.shape[0]\n        az_in = angles_in[:, 0]\n        ele_in = angles_in[:, 1]\n        az_in = torch.clamp(az_in, self.az_min, self.az_max)\n        ele_in = torch.clamp(ele_in, self.ele_min, self.ele_max)\n        az_sin = torch.sin(az_in)\n        az_cos = torch.cos(az_in)\n        ele_sin = torch.sin(ele_in)\n        ele_cos = torch.cos(ele_in)\n        R_az = self.create_Raz(az_cos, az_sin)\n        R_ele = self.create_Rele(ele_cos, ele_sin)\n        # print(R_ele)\n        R_rot = torch.bmm(R_az, R_ele)\n        R_0 = angles_in.data.new(\n            bn, 3, 3).zero_()\n        R_0[:, 0, 1] = 1\n        R_0[:, 1, 0] = -1\n        R_0[:, 2, 2] = 1\n        R_0 = R_0.requires_grad_(True)\n        if is_cuda:\n            R_0 = R_0.cuda()\n        R = torch.bmm(R_rot, R_0)\n        zeros = angles_in.data.new_zeros([bn, 3, 1]).zero_().requires_grad_(True)\n        return torch.cat((R, zeros), dim=2)\n\n    def create_Rele(self, ele_cos, ele_sin):\n        bn = ele_cos.shape[0]\n        one = Variable(ele_cos.data.new(bn, 1, 1).fill_(1))\n        zero = Variable(ele_cos.data.new(bn, 1, 1).zero_())\n        ele_cos = ele_cos.view(bn, 1, 1)\n        ele_sin = ele_sin.view(bn, 1, 1)\n        c1 = torch.cat((one, zero, zero), dim=1)\n        c2 = torch.cat((zero, ele_cos, ele_sin), dim=1)\n        c3 = torch.cat((zero, -ele_sin, ele_cos), dim=1)\n        return torch.cat((c1, c2, c3), dim=2)\n\n    def create_Raz(self, cos, sin):\n        bn = cos.shape[0]\n        one = Variable(cos.data.new(bn, 1, 1).fill_(1))\n        zero = Variable(cos.data.new(bn, 1, 1).zero_())\n        cos = cos.view(bn, 1, 1)\n        sin = sin.view(bn, 1, 1)\n        c1 = torch.cat((cos, sin, zero), dim=1)\n        c2 = torch.cat((-sin, cos, zero), dim=1)\n        c3 = torch.cat((zero, zero, one), dim=1)\n        return torch.cat((c1, c2, c3), dim=2)\n\n\nclass FineSizeCroppingLayer(nn.Module):\n    '''\n    crop the input list of images to specified size.\n    '''\n\n    def __init__(self, output_size):\n        super().__init__()\n        self.output_size = output_size\n\n    def forward(self, x, random_number):\n        '''\n        random number is from 0 to 1\n        '''\n        N, C, H, W = x.shape\n        output_size = self.output_size\n        assert(H >= output_size and W >= output_size)\n        h_beg = round((H - output_size) * random_number)\n        w_beg = round((W - output_size) * random_number)\n        return x[:, :, h_beg:h_beg + output_size, w_beg:w_beg + output_size]\n\n\nclass CroppingLayer(nn.Module):\n    '''\n    crop and pad output to have consistant size\n    '''\n\n    def __init__(self, output_size, sil_th=0.8, padding_pct=0.03, no_largest=False):\n        super().__init__()\n        self.output_size = output_size\n        self.threshold = nn.Threshold(sil_th, 0)\n        size = [output_size, output_size]\n        h = torch.arange(0, size[0]).float() / (size[0] - 1.0) * 2.0 - 1.0\n        w = torch.arange(0, size[1]).float() / (size[1] - 1.0) * 2.0 - 1.0\n        # create grid\n        grid = torch.zeros(size[0], size[1], 2)\n        grid[:, :, 0] = w.unsqueeze(0).repeat(size[0], 1)\n        grid[:, :, 1] = h.unsqueeze(0).repeat(size[1], 1).transpose(0, 1)\n        # expand to match batch size\n        grid = grid.unsqueeze(0)\n        self.register_buffer('grid', grid)\n        self.kernel = np.ones((5, 5), np.uint8)\n        self.padding_pct = padding_pct\n        self.th = sil_th\n        self.no_largest = no_largest\n\n    def forward(self, exp_sil, exp_depth):\n\n        sil, depth, _, _ = self.crop_depth_sil(exp_sil, exp_depth)\n        return sil, depth\n\n    def bbox_from_sil(self, exp_sil, padding_pct=0.03):\n        n, c, h, w = exp_sil.shape\n        assert c == 1  # sil only\n        mask_th = exp_sil.data.cpu().numpy()\n        # find largest connected component:\n        mask_th_binary = np.where(mask_th < self.th, 0.0, 1.0)\n        # lefttop_h, lefttop_w, rightbottom_h, rightbottom_w\n        bbox = np.zeros([n, 4]).astype(int)\n        mask_largest_batch = torch.FloatTensor(n, c, h, w).zero_()\n        for x in range(n):\n            if self.no_largest:\n                nz = np.nonzero(mask_th_binary[x, 0, :, :])\n                bbox[x, 0] = np.min(nz[0])\n                bbox[x, 1] = np.min(nz[1])\n                bbox[x, 2] = np.max(nz[0])\n                bbox[x, 3] = np.max(nz[1])\n                mask_largest_batch[x, 0, :, :] = torch.from_numpy(mask_th_binary[x, 0, :, :].astype(np.float32)).float()\n            else:\n                mask_th_binary_pad = np.pad(mask_th_binary[x, 0, :, :], ((1, 1),), 'constant', constant_values=0).astype(np.uint8)\n                labeled, nr_objects = ndimage.measurements.label(mask_th_binary_pad)\n                counts = np.bincount(labeled.flatten())\n                largest = np.argmax(counts[1:]) + 1\n                mask_largest = np.where(labeled == largest, 1, 0)[1:-1, 1:-1]\n                mask_largest_batch[x, 0, :, :] = torch.from_numpy(mask_largest.astype(np.float32)).float()\n                # mask_th2 = cv2.morphologyEx(mask_th[x], cv2.MORPH_OPEN, self.kernel)\n                # nz = np.nonzero(mask_th2[0, :, :])\n                nz = np.nonzero(mask_largest)\n                bbox[x, 0] = np.min(nz[0])\n                bbox[x, 1] = np.min(nz[1])\n                bbox[x, 2] = np.max(nz[0])\n                bbox[x, 3] = np.max(nz[1])\n\n        return bbox, mask_largest_batch\n\n    def crop_depth_sil(self, exp_sil_full, exp_depth_full, is_debug=False):\n        # also keeps track of coordinate change\n        # output a\n\n        bbox, mask_largest_batch = self.bbox_from_sil(exp_sil_full)\n        mask_largest_batch = exp_sil_full.new_tensor(mask_largest_batch)\n\n        bbox = bbox.astype(np.int32)\n        exp_sil = exp_sil_full * mask_largest_batch\n        exp_depth = exp_depth_full * mask_largest_batch\n        exp_depth = exp_depth + 3 * (1 - mask_largest_batch)\n        n, c, h, w = exp_sil.shape\n        new_sil = []\n        new_depth = []\n\n        shape_stat = []\n        for x in range(n):\n            h = bbox[x, 2] + 1 - bbox[x, 0]\n            w = bbox[x, 3] + 1 - bbox[x, 1]\n            h = int(h)\n            w = int(w)\n            cropped_sil = exp_sil[x, 0, bbox[x, 0]:bbox[x, 0] + h,\n                                  bbox[x, 1]:bbox[x, 1] + w]\n            cropped_sil = cropped_sil.contiguous().view(1, 1, h, w)\n            cropped_depth = exp_depth[x, 0, bbox[x, 0]:bbox[x, 0] +\n                                      h, bbox[x, 1]:bbox[x, 1] + w]\n            cropped_depth = cropped_depth.contiguous().view(1, 1, h, w)\n            if h > w:\n                dim = h\n                m_sil = nn.ConstantPad2d(((h - w) // 2, (h - w) // 2, 0, 0), 0)\n                m_depth = nn.ConstantPad2d(\n                    ((h - w) // 2, (h - w) // 2, 0, 0), 3)\n            else:\n                dim = w\n                m_sil = nn.ConstantPad2d((0, 0, (w - h) // 2, (w - h) // 2), 0)\n                m_depth = nn.ConstantPad2d(\n                    (0, 0, (w - h) // 2, (w - h) // 2), 3)\n            pad = int(np.floor(dim * self.padding_pct))\n            space_pad_depth = nn.ConstantPad2d((pad, pad, pad, pad), 3)\n            space_pad_sil = nn.ConstantPad2d((pad, pad, pad, pad), 0)\n            sq_depth = space_pad_depth(m_depth(cropped_depth))\n            sq_sil = space_pad_sil(m_sil(cropped_sil))\n            shape_stat.append(sq_depth.shape)\n            new_sil.append(grid_sample(sq_sil, self.grid))\n            new_depth.append(grid_sample(sq_depth, self.grid))\n        new_sil = torch.cat(new_sil, dim=0)\n        new_depth = torch.cat(new_depth, dim=0)\n        if is_debug:\n            return sq_sil, cropped_depth, bbox, sq_depth.shape\n        else:\n            return new_sil, new_depth, bbox, shape_stat\n\n\nclass VoxelRenderLayer(nn.Module):\n    def __init__(self, voxel_shape, camera_distance=2.0, fl=0.050, w=0.0612, res=128, nsamples_factor=1.5):\n        super().__init__()\n        self.camera_distance = camera_distance\n        self.fl = fl\n        self.w = w\n        self.voxel_shape = voxel_shape\n        self.nsamples_factor = nsamples_factor\n        self.res = res\n        self.register_buffer('grid', self.grid_gen())\n        self.grid_sampler3d = grid_sample3d\n        self.calc_stop_prob = CalcStopProb().apply\n        self.affine_grid3d = affine_grid3d\n\n    def forward(self, voxel_in, rotation_matrix=None):\n        if rotation_matrix is None:\n            voxel_rot = voxel_in\n        else:\n            voxel_rot_grid = self.affine_grid3d(\n                rotation_matrix, voxel_in.shape)\n            voxel_rot = self.grid_sampler3d(voxel_in, voxel_rot_grid)\n        voxel_align = self.grid_sampler3d(voxel_rot, self.grid)\n        voxel_align = voxel_align.permute(0, 1, 3, 4, 2)\n        voxel_align = torch.clamp(voxel_align, 1e-4, 1 - (1e-4))\n        voxel_align = voxel_align.contiguous()\n        stop_prob = self.calc_stop_prob(voxel_align)\n        exp_depth = torch.matmul(\n            stop_prob, self.depth_weight)\n        back_groud_prob = torch.prod((1.0) - voxel_align, dim=4)\n        back_groud_prob = torch.clamp(back_groud_prob, 1e-4, 1 - (1e-4))\n        back_groud_prob = back_groud_prob * (self.camera_distance + 1.0)\n        exp_depth = exp_depth + back_groud_prob\n        exp_sil = torch.sum(stop_prob, dim=4)\n        return torch.transpose(exp_sil, 2, 3), torch.transpose(exp_depth, 2, 3)\n\n    def grid_gen(self, numtype=np.float32):\n        n, c, sx, sy, sz = self.voxel_shape\n        nsamples = int(sz * self.nsamples_factor)\n        res = self.res\n        w = self.w\n        dist = self.camera_distance\n        self.register_buffer('depth_weight', torch.linspace(\n            dist - 1, dist + 1, nsamples))\n        fl = self.fl\n        grid = np.zeros([n, nsamples, res, res, 3], dtype=numtype)\n        h_linspace = np.linspace(w / 2, -w / 2, res)\n        w_linspace = np.linspace(w / 2, -w / 2, res)\n        H, W = np.meshgrid(h_linspace, w_linspace)\n        cam = np.array([[[-dist, 0, 0]]])\n        grid_vec = np.zeros([res, res, 3], dtype=numtype)\n        grid_vec[:, :, 1] = W\n        grid_vec[:, :, 2] = H\n        grid_vec[:, :, 0] = -(dist - fl)\n        grid_vec = grid_vec - cam\n        self.grid_vec = grid_vec\n        grid_vec_a = grid_vec * ((dist - 1) / fl)\n        grid_vec_b = grid_vec * ((dist + 1) / fl)\n        for idn in range(n):\n            for ids in range(nsamples):\n                grid[idn, ids, :, :, :] = grid_vec_b - \\\n                    (1 - (ids / nsamples)) * (grid_vec_b - grid_vec_a)\n        grid = grid + cam\n        return torch.from_numpy(grid.astype(numtype))\n"""
util/__init__.py,0,b''
util/html.py,0,"b'import dominate\nfrom dominate.tags import meta, h3, table, tr, td, p, a, img, br\nimport os\n\n\nclass HTML:\n    """"""This HTML class allows us to save images and write texts into a single HTML file.\n     It consists of functions such as <add_header> (add a text header to the HTML file),\n     <add_images> (add a row of images to the HTML file), and <save> (save the HTML to the disk).\n     It is based on Python library \'dominate\', a Python library for creating and manipulating HTML documents using a DOM API.\n    """"""\n\n    def __init__(self, web_dir, title, refresh=0):\n        """"""Initialize the HTML classes\n        Parameters:\n            web_dir (str) -- a directory that stores the webpage. HTML file will be created at <web_dir>/index.html; images will be saved at <web_dir/images/\n            title (str)   -- the webpage name\n            reflect (int) -- how often the website refresh itself; if 0; no refreshing\n        """"""\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if refresh > 0:\n            with self.doc.head:\n                meta(http_equiv=""refresh"", content=str(refresh))\n\n    def get_image_dir(self):\n        """"""Return the directory that stores images""""""\n        return self.img_dir\n\n    def add_header(self, text):\n        """"""Insert a header to the HTML file\n        Parameters:\n            text (str) -- the header text\n        """"""\n        with self.doc:\n            h3(text)\n\n    def add_images(self, ims, txts, links, width=400):\n        """"""add images to the HTML file\n        Parameters:\n            ims (str list)   -- a list of image paths\n            txts (str list)  -- a list of image names shown on the website\n            links (str list) --  a list of hyperref links; when you click an image, it will redirect you to a new page\n        """"""\n        self.t = table(border=1, style=""table-layout: fixed;"")  # Insert a table\n        self.doc.add(self.t)\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        """"""save the current content to the HMTL file""""""\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':  # we show an example usage here.\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims, txts, links = [], [], []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
util/image_pool.py,2,"b'import random\nimport torch\n\n\nclass ImagePool():\n    """"""This class implements an image buffer that stores previously generated images.\n    This buffer enables us to update discriminators using a history of generated images\n    rather than the ones produced by the latest generators.\n    """"""\n\n    def __init__(self, pool_size):\n        """"""Initialize the ImagePool class\n        Parameters:\n            pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be created\n        """"""\n        self.pool_size = pool_size\n        if self.pool_size > 0:  # create an empty pool\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        """"""Return an image from the pool.\n        Parameters:\n            images: the latest generated images from the generator\n        Returns images from the buffer.\n        By 50/100, the buffer will return input images.\n        By 50/100, the buffer will return images previously stored in the buffer,\n        and insert the current images to the buffer.\n        """"""\n        if self.pool_size == 0:  # if the buffer size is 0, do nothing\n            return images\n        return_images = []\n        for image in images:\n            image = torch.unsqueeze(image.data, 0)\n            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer\n                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:       # by another 50% chance, the buffer will return the current image\n                    return_images.append(image)\n        return_images = torch.cat(return_images, 0)   # collect all the images and return\n        return return_images\n'"
util/util.py,3,"b""from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport os\nimport pickle\nimport glob\nimport functools\n\n\ndef partialclass(cls, *args, **kwds):\n    class NewCls(cls):\n        __init__ = functools.partialmethod(cls.__init__, *args, **kwds)\n    return NewCls\n\n\ndef tensor2im(image_tensor, imtype=np.uint8, cvt_rgb=True):\n    if image_tensor.requires_grad:\n        image_numpy = image_tensor[0].detach().cpu().float().numpy()\n    else:\n        image_numpy = image_tensor[0].cpu().float().numpy()\n    if image_numpy.shape[0] == 1 and cvt_rgb:\n        image_numpy = np.tile(image_numpy, (3, 1, 1))\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    return image_numpy.astype(imtype)\n\n\ndef get_subdir_list(path):\n    return [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n\n\ndef get_image_list(path):\n    exts = ['*.jpg', '*.gif', '*.png', '*.tga', '*.jpeg']\n    file_list = []\n    for ext in exts:\n        file_list += glob.glob(os.path.join(path, ext))\n    return file_list\n\n\ndef tensor2vec(vector_tensor):\n    numpy_vec = vector_tensor.data.cpu().numpy()\n    if numpy_vec.ndim == 4:\n        return numpy_vec[:, :, 0, 0]\n    else:\n        return numpy_vec\n\n\ndef pickle_load(file_name):\n    data = None\n    with open(file_name, 'rb') as f:\n        data = pickle.load(f)\n    return data\n\n\ndef pickle_save(file_name, data):\n    with open(file_name, 'wb') as f:\n        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n\ndef diagnose_network(net, name='network'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print('name', name)\n    print('mean', mean)\n\n\ndef interp_z(z0, z1, num_frames, interp_mode='linear'):\n    zs = []\n    if interp_mode == 'linear':\n        for n in range(num_frames):\n            ratio = n / float(num_frames - 1)\n            z_t = (1 - ratio) * z0 + ratio * z1\n            zs.append(z_t[np.newaxis, :])\n        zs = np.concatenate(zs, axis=0).astype(np.float32)\n\n    if interp_mode == 'slerp':\n        # st()\n        z0_n = z0 / (np.linalg.norm(z0) + 1e-10)\n        z1_n = z1 / (np.linalg.norm(z1) + 1e-10)\n        omega = np.arccos(np.dot(z0_n, z1_n))\n        sin_omega = np.sin(omega)\n        if sin_omega < 1e-10 and sin_omega > -1e-10:\n            zs = interp_z(z0, z1, num_frames, interp_mode='linear')\n        else:\n            for n in range(num_frames):\n                ratio = n / float(num_frames - 1)\n                z_t = np.sin((1 - ratio) * omega) / sin_omega * z0 + np.sin(ratio * omega) / sin_omega * z1\n                zs.append(z_t[np.newaxis, :])\n        zs = np.concatenate(zs, axis=0).astype(np.float32)\n    return zs\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path, 'JPEG', quality=100)\n\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print('shape,', x.shape)\n    if val:\n        x = x.flatten()\n        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef normalize_tensor(in_feat, eps=1e-10):\n    norm_factor = torch.sqrt(torch.sum(in_feat**2, dim=1)).repeat(1, in_feat.size()[1], 1, 1)\n    return in_feat / (norm_factor + eps)\n\n\ndef cos_sim(in0, in1):\n    in0_norm = normalize_tensor(in0)\n    in1_norm = normalize_tensor(in1)\n    return torch.mean(torch.sum(in0_norm * in1_norm, dim=1))\n"""
util/util_print.py,0,"b""class bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n\nstr_stage = bcolors.OKBLUE + '==>' + bcolors.ENDC\nstr_verbose = bcolors.OKGREEN + '[Verbose]' + bcolors.ENDC\nstr_warning = bcolors.WARNING + '[Warning]' + bcolors.ENDC\nstr_error = bcolors.FAIL + '[Error]' + bcolors.ENDC\n"""
util/util_render.py,0,"b'import os\nimport numpy as np\nimport math\nimport bpy\nfrom mathutils import Vector\n\n\ndef setup_cam_and_lights_old(p0, p1):\n    scn = bpy.context.scene\n\n    # set camera view and lightling\n    bpy.ops.object.camera_add()\n    scn.camera = bpy.data.objects[\'Camera\']\n\n    # Constrain camera to object\n    bpy.ops.object.constraint_add(type=""TRACK_TO"")\n    bpy.ops.object.empty_add(type=\'CUBE\', radius=0.1, location=(0, 0, 0))\n    scn.camera.constraints[""Track To""].target = bpy.data.objects[\'Empty\']\n    scn.camera.constraints[""Track To""].track_axis = \'TRACK_NEGATIVE_Z\'\n    scn.camera.constraints[""Track To""].up_axis = \'UP_Y\'\n\n    scn.camera.location = Vector(p1) * 1.2\n    scn.camera.data.clip_end = max(p1) * 10   # camera clipping range\n\n    # Set camera light\n    bpy.ops.object.lamp_add(type=\'POINT\')\n    camlight = bpy.context.active_object\n    camlight.name = \'camlight\'\n    camlight.location = scn.camera.location\n    camlight.data.use_specular = False\n    camlight.data.energy = 1\n    camlight.data.distance = 100\n    bpy.ops.object.constraint_add(type=""CHILD_OF"")\n    camlight.constraints[\'Child Of\'].target = scn.camera\n\n    # Set 6 background light, one for each face\n    sun_dis = (p1 - p0).max() * 1.25\n    center = (p0 + p1) / 2\n    sun_locs = np.dot(np.ones((6, 1)), center.reshape(1, 3))\n    sun_locs[:3, :] += np.eye(3) * sun_dis\n    sun_locs[3:, :] -= np.eye(3) * sun_dis\n    print(p0, p1)\n    print(sun_locs)\n    for i in range(6):\n        bpy.ops.object.lamp_add(type=\'POINT\')\n        bpy.context.active_object.name = \'facelight_\' + str(i)\n    sun_counter = 0\n    for light in [obj for obj in bpy.data.objects if obj.name[:10] == \'facelight_\']:\n        light.location = Vector(sun_locs[sun_counter])\n        light.data.use_specular = False\n        light.data.energy = 0.00\n        sun_counter += 1\n\n\ndef render(model_path, render_prefix, az, ele, view_id, res=512):\n    bpy.ops.object.select_all(action=\'SELECT\')\n    bpy.ops.object.delete()\n\n    # load obj\n    bpy.ops.import_scene.obj(filepath=model_path, axis_up=\'Z\', axis_forward=\'-X\')\n    obj = bpy.context.selected_objects[0]\n\n    # find bounding box\n    coords = [(obj.matrix_world * v.co) for v in obj.data.vertices]\n    p0 = np.array([min([c.x for c in coords]), min(\n        [c.y for c in coords]), min([c.z for c in coords])])\n    p1 = np.array([max([c.x for c in coords]), max(\n        [c.y for c in coords]), max([c.z for c in coords])])\n\n    setup_cam_and_lights_old(p0, p1)\n\n    scn = bpy.context.scene\n    scn.render.resolution_x = 512\n    scn.render.resolution_y = 512\n    scn.render.image_settings.file_format = \'PNG\'\n    scn.render.image_settings.color_mode = \'RGBA\'\n    scn.render.alpha_mode = \'TRANSPARENT\'\n    scn.render.resolution_percentage = 100\n    camera = scn.camera\n    # Render\n\n    angle = az * np.pi / 180\n    up_angle = ele * np.pi / 180\n    x = 2.0 * math.cos(up_angle) * math.cos(angle)\n    y = 2.0 * math.cos(up_angle) * math.sin(angle)\n    z = 2.0 * math.sin(up_angle)\n    camera.location = (x, y, z)\n\n    scn.render.filepath = os.path.join(render_prefix + \'view%03d.jpg\' % view_id)\n    bpy.ops.render.render(write_still=True)\n    print(\'Image saved: \' + scn.render.filepath)\n\n\nif __name__ == \'__main__\':\n    import sys\n    obj_name = sys.argv[4]\n    prefix = (sys.argv[5])\n    ele = float(sys.argv[6])\n    az = float(sys.argv[7])\n    view_id = int(sys.argv[8])\n    render(obj_name, prefix, az, ele, view_id)\n'"
util/util_voxel.py,0,"b'import numpy as np\nimport numba\nfrom scipy.interpolate import RegularGridInterpolator as rgi\nfrom skimage import measure\nfrom subprocess import call\nfrom os.path import dirname, join\nimport cv2\n\ntry:\n    from .util_print import str_warning\nexcept ImportError:\n    str_warning = \'[Warning]\'\nfrom glob import glob\n\n\ndef render(obj_name, views, render_prefix, res):\n    render_script = join(dirname(__file__), \'util_render.py\')\n    cmd = \'blender --background --python %s %s %s\' % (render_script, obj_name, render_prefix)\n    for view_id in range(views.shape[0]):\n        cmd_view = cmd + \' %f %f %d >/dev/null\' % (views[view_id, 0], views[view_id, 1], view_id)\n        call(cmd_view, shell=True)\n    imgs = glob(render_prefix + \'*.png\')\n    crop_and_pad(imgs)\n\n\ndef crop_and_pad(img_paths):\n    for img_path in img_paths:\n        try:\n            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n            mask = img[:, :, 3] / 255\n            mask = mask[:, :, np.newaxis]\n            bg = np.ones([img.shape[0], img.shape[1], 3]) * 255\n            img = img[:, :, :-1] * mask + bg * (1 - mask)\n            idh, idw, _ = np.where(mask > 0.8)\n            idh_min, idh_max, idw_min, idw_max = [np.min(idh), np.max(idh), np.min(idw), np.max(idw)]\n            img = img[idh_min:idh_max, idw_min:idw_max]\n            cropped = __pad_real_im(img).astype(np.uint8)\n            cv2.imwrite(img_path, cropped)\n        except IOError:\n            continue\n\n\ndef __pad_real_im(img, padding_pix_pct=0.03, img_size=256, padding_value=255):\n    padding_pix = int(img_size * padding_pix_pct)\n    """"""\n    make the bounding box larger by a few pixels (equiv. to\n    padding_pix pixels after resize), then add edge padding\n    to make it a square, then resize to desired resolution\n    """"""\n    y1, x1, y2, x2 = [0, 0, img.shape[1], img.shape[0]]\n    w, h = img.shape[1], img.shape[0]\n    x_mid = (x1 + x2) / 2.\n    y_mid = (y1 + y2) / 2.\n    ll = max(x2 - x1, y2 - y1) * img_size / (img_size - 2. * padding_pix)\n    x1 = int(np.round(x_mid - ll / 2.))\n    x2 = int(np.round(x_mid + ll / 2.))\n    y1 = int(np.round(y_mid - ll / 2.))\n    y2 = int(np.round(y_mid + ll / 2.))\n    b_x = 0\n    if x1 < 0:\n        b_x = -x1\n        x1 = 0\n    b_y = 0\n    if y1 < 0:\n        b_y = -y1\n        y1 = 0\n    a_x = 0\n    if x2 >= h:\n        a_x = x2 - (h - 1)\n        x2 = h - 1\n    a_y = 0\n    if y2 >= w:\n        a_y = y2 - (w - 1)\n        y2 = w - 1\n    if len(img.shape) == 2:\n        crop = np.pad(img[x1: x2 + 1, y1: y2 + 1],\n                      ((b_x, a_x), (b_y, a_y)), mode=\'constant\', constant_values=padding_value)\n    else:\n        crop = np.pad(img[x1: x2 + 1, y1: y2 + 1],\n                      ((b_x, a_x), (b_y, a_y), (0, 0)), mode=\'constant\', constant_values=padding_value)\n    return crop\n\n\n@numba.jit(nopython=True, cache=True)\ndef downsample(vox_in, times, use_max=True):\n    if vox_in.shape[0] % times != 0:\n        print(\'WARNING: not dividing the space evenly.\')\n    dim = vox_in.shape[0] // times\n    vox_out = np.zeros((dim, dim, dim))\n    for x in range(dim):\n        for y in range(dim):\n            for z in range(dim):\n                subx = x * times\n                suby = y * times\n                subz = z * times\n                subvox = vox_in[subx:subx + times - 1,\n                                suby:suby + times - 1, subz:subz + times - 1]\n                if use_max:\n                    vox_out[x, y, z] = np.max(subvox)\n                else:\n                    vox_out[x, y, z] = np.mean(subvox)\n    return vox_out\n\n\ndef find_bound(voxel, *, threshold=0.5):\n    """""" find the boundary of a 3D voxel matrix. return boundaries in two matrices.\n    Note that lower bound is inclusive while the higher bound is not""""""\n    assert voxel.ndim == 3\n    bmin = np.zeros(voxel.ndim, dtype=int)\n    bmax = np.zeros(voxel.ndim, dtype=int)\n\n    voxel_binary = (voxel > threshold)\n    if not voxel_binary.any():\n        print(str_warning, \'Empty voxel found\')\n        return bmin, bmax\n\n    for dim in range(voxel.ndim):\n        voxel_dim = voxel_binary\n        for i in range(dim):\n            voxel_dim = voxel_dim.any(0)\n        for i in range(voxel.ndim - dim - 1):\n            voxel_dim = voxel_dim.any(1)\n        inds = voxel_dim.nonzero()[0]\n        bmin[dim] = inds.min()\n        bmax[dim] = inds.max() + 1\n    return bmin, bmax\n\n\ndef save_obj(vertices, faces, path):\n    with open(path, \'w\') as f:\n        for v in vertices:\n            f.write(\'v {} {} {}\\n\'.format(v[0], v[1], v[2]))\n        for face in faces:\n            f.write(\'f {} {} {}\\n\'.format(\n                face[0] + 1, face[1] + 1, face[2] + 1))\n\n\ndef save_vox_to_obj(voxel, th, save_path):\n    if len(voxel.shape) == 5:\n        voxel_iso = voxel[0, 0, :, :, :]\n    verts, faces, normals, values = measure.marching_cubes_lewiner(voxel_iso, th, spacing=(1 / 128, 1 / 128, 1 / 128),)\n    save_obj(verts - 0.5, faces, save_path)\n\n\ndef bounding_box_align(voxel, gt, *, threshold=0.5):\n    bminv, bmaxv = find_bound(voxel, threshold=threshold)\n    bming, bmaxg = find_bound(voxel, threshold=threshold)\n    scale = (bmaxg - bming).max() / (bmaxv - bminv).max()\n    scales = np.array((scale, scale, scale))\n    offset = (bmaxg + bming) / 2 - (bmaxv + bminv) / 2\n    return transform(voxel, scales=scales, offset=offset)\n\n\ndef translate(voxel, *, offset=None, translate_type=None):\n    """""" Translate a voxel by a specific offset """"""\n    assert voxel.ndim == 3\n    bmin, bmax = find_bound(voxel, threshold=0.5)\n    if offset is None:\n        assert translate_type is not None\n        min_offset = -bmin\n        max_offset = np.array(voxel.shape) - bmax\n        if translate_type == \'random\':\n            offset = np.random.rand(\n                voxel.ndim) * (max_offset - min_offset + 1) + min_offset - 1\n            offset = np.ceil(offset).astype(int)\n        elif translate_type == \'origin\':\n            offset = min_offset\n        elif translate_type == \'middle\':\n            offset = np.ceil((min_offset + max_offset) / 2).astype(int)\n        else:\n            raise ValueError(\'unknown translate_type: \' + str(translate_type))\n    else:\n        assert translate_type is None\n        offset = np.array(offset).astype(int)\n    assert (bmin + offset >= 0).all() and (bmax +\n                                           offset <= np.array(voxel.shape)).all()\n    res = np.zeros(voxel.shape)\n    res[bmin[0] + offset[0]:bmax[0] + offset[0], bmin[1] + offset[1]:bmax[1] + offset[1], bmin[2] +\n        offset[2]:bmax[2] + offset[2]] = voxel[bmin[0]:bmax[0], bmin[1]:bmax[1], bmin[2]:bmax[2]]\n    return res\n\n\ndef dim_unify(voxel):\n    if voxel.ndim == 5:\n        assert voxel.shape[1] == 1\n        voxel = voxel[:, 0, :, :, :]\n    elif voxel.ndim == 3:\n        voxel = np.expand_dims(voxel, 0)\n    else:\n        assert voxel.ndim == 4, \'voxel matrix must have dimensions of 3, 4, 5\'\n    return voxel\n\n###########################################################\n# For non-discretized transformation\n\n\ndef _get_centeralized_mesh_grid(sx, sy, sz):\n    x = np.arange(sx) - sx / 2.\n    y = np.arange(sy) - sy / 2.\n    z = np.arange(sz) - sz / 2.\n    return np.meshgrid(x, y, z, indexing=\'ij\')\n\n\ndef get_rotation_matrix(angles):\n    # # legacy code\n    # alpha, beta, gamma = angles\n    # R_alpha = np.array([[np.cos(alpha), -np.sin(alpha), 0], [np.sin(alpha), np.cos(alpha), 0], [0, 0, 1]])\n    # R_beta = np.array([[1, 0, 0], [0, np.cos(beta), -np.sin(beta)], [0, np.sin(beta), np.cos(beta)]])\n    # R_gamma = np.array([[np.cos(gamma), 0, -np.sin(gamma)], [0, 1, 0], [np.sin(gamma), 0, np.cos(gamma)]])\n    # R = np.dot(np.dot(R_alpha, R_beta), R_gamma)\n    alpha, beta, gamma = angles\n    R_alpha = np.array([[1, 0, 0], [0, np.cos(alpha), -np.sin(alpha)], [0, np.sin(alpha), np.cos(alpha)]])\n    R_beta = np.array([[np.cos(beta), 0, -np.sin(beta)], [0, 1, 0], [np.sin(beta), 0, np.cos(beta)]])\n    R_gamma = np.array([[np.cos(gamma), -np.sin(gamma), 0], [np.sin(gamma), np.cos(gamma), 0], [0, 0, 1]])\n    R = np.dot(np.dot(R_alpha, R_beta), R_gamma)\n    return R\n\n\ndef get_scale_matrix(scales):\n    return np.diag(scales)\n\n\ndef transform_by_matrix(voxel, matrix, offset):\n    """"""\n    transform a voxel by matrix, then apply an offset\n    Note that the offset is applied after the transformation\n    """"""\n    sx, sy, sz = voxel.shape\n    gridx, gridy, gridz = _get_centeralized_mesh_grid(sx, sy, sz)  # the coordinate grid of the new voxel\n    mesh = np.array([gridx.reshape(-1), gridy.reshape(-1), gridz.reshape(-1)])\n    mesh_rot = np.dot(np.linalg.inv(matrix), mesh) + np.array([sx / 2, sy / 2, sz / 2]).reshape(3, 1)\n    mesh_rot = mesh_rot - np.array(offset).reshape(3, 1)    # grid for new_voxel should get a negative offset\n\n    interp = rgi((np.arange(sx), np.arange(sy), np.arange(sz)), voxel,\n                 method=\'linear\', bounds_error=False, fill_value=0)\n    new_voxel = interp(mesh_rot.T).reshape(sx, sy, sz)  # todo: move mesh to center\n    return new_voxel\n\n\ndef transform(voxel, angles=(0, 0, 0), scales=(1, 1, 1), offset=(0, 0, 0), threshold=None, clamp=False):\n    """"""\n    transform a voxel by first rotate, then scale, then add offset.\n    shortcut for transform_by_matrix\n    """"""\n    matrix = np.dot(get_rotation_matrix(angles), get_scale_matrix(scales))\n    new_voxel = transform_by_matrix(voxel, matrix, offset)\n    if clamp:\n        new_voxel = np.clip(new_voxel, 0, 1)\n    if threshold is not None:\n        new_voxel = (new_voxel > threshold).astype(np.uint8)\n    return new_voxel\n\n############################################################\n# test\n\n\ndef _fill(*, input_array, six_way=True):\n    """"""\n    fill an voxel array with dfs\n    The algorithm pad the input_array with 2 voxels in each direction\n        (1 to avoid complex border cases when checking neighbors,\n         1 for making outer voxels connected so that we only search for one connected component)\n    Note that this script considers ALL non-zero values as surface voxels\n    """"""\n    UNKNOWN = 200   # must be in [0, 255] for uint8\n\n    sz0, sz1, sz2 = input_array.shape\n    output_array = np.zeros((sz0 + 4, sz1 + 4, sz2 + 4), dtype=np.uint8)\n    output_array[1:-1, 1:-1, 1:-1] = UNKNOWN\n    input_padded = np.zeros((sz0 + 4, sz1 + 4, sz2 + 4), dtype=np.uint8)\n    input_padded[2:-2, 2:-2, 2:-2] = input_array\n\n    stack = [(1, 1, 1)]\n    output_array[1, 1, 1] = 0\n    while len(stack) > 0:\n        i, j, k = stack.pop()\n        output_array[i, j, k] = 0\n        if six_way:\n            neighbors = [(i - 1, j, k),\n                         (i, j - 1, k),\n                         (i, j, k - 1),\n                         (i, j, k + 1),\n                         (i, j + 1, k),\n                         (i + 1, j, k), ]\n        else:\n            neighbors = [(i - 1, j - 1, k - 1),\n                         (i - 1, j - 1, k),\n                         (i - 1, j - 1, k + 1),\n                         (i - 1, j, k - 1),\n                         (i - 1, j, k),\n                         (i - 1, j, k + 1),\n                         (i - 1, j + 1, k - 1),\n                         (i - 1, j + 1, k),\n                         (i - 1, j + 1, k + 1),\n                         (i, j - 1, k - 1),\n                         (i, j - 1, k),\n                         (i, j - 1, k + 1),\n                         (i, j, k - 1),\n                         (i, j, k + 1),\n                         (i, j + 1, k - 1),\n                         (i, j + 1, k),\n                         (i, j + 1, k + 1),\n                         (i + 1, j - 1, k - 1),\n                         (i + 1, j - 1, k),\n                         (i + 1, j - 1, k + 1),\n                         (i + 1, j, k - 1),\n                         (i + 1, j, k),\n                         (i + 1, j, k + 1),\n                         (i + 1, j + 1, k - 1),\n                         (i + 1, j + 1, k),\n                         (i + 1, j + 1, k + 1), ]\n        for i_, j_, k_ in neighbors:\n            if output_array[i_, j_, k_] == UNKNOWN and input_padded[i_, j_, k_] == 0:\n                stack.append((i_, j_, k_))\n                output_array[i_, j_, k_] = 0\n    output_array = (output_array != 0).astype(np.uint8)\n    return output_array[2:-2, 2:-2, 2:-2]\n\n\ndef fill(use_compile=False, compile_flag={\'cache\': True, \'nopython\': True}, **kwargs):\n    """"""\n    common compile flags: {\'cache\': True, \'nopython\': True}\n    """"""\n    if use_compile:\n        from numba import jit\n        return jit(**compile_flag)(_fill)(**kwargs)\n    else:\n        return _fill(**kwargs)\n\n\nif __name__ == \'__main__\':\n    import os\n    from scipy.io import loadmat\n    test_filename = \'/path/to/model_normalized_128.mat\'\n    gt = loadmat(test_filename)[\'voxel\']\n    gt = downsample(gt, 4)\n    print(gt.shape)\n    results = [np.copy(gt)]     # 0\n\n    # translation\n    results.append(translate(gt, offset=(0, 0, 4)))   # 1\n    results.append(translate(gt, offset=(0, 0, -4)))  # 2\n\n    # transform_offset\n    results.append(transform(gt, threshold=None))    # 3\n    results.append(transform(gt, offset=(0, 0, 4), threshold=None))    # 4\n    results.append(transform(gt, offset=(0, 0, -4), threshold=None))   # 5\n\n    # transform_rotate\n    results.append(transform(gt, angles=(1., 0, 0), threshold=None))  # 6\n    results.append(transform(gt, angles=(0, 1., 0), threshold=None))  # 7\n    results.append(transform(gt, angles=(0, 0, 1.), threshold=None))  # 8\n\n    # transform_scale\n    results.append(transform(gt, scales=(0.8, 0.8, 0.8), threshold=None))      # 9\n    results.append(transform(gt, scales=(1.2, 1.2, 1.2), threshold=None, clamp=True))    # 10\n\n    # check that gt is not changed\n    assert (gt == results[0]).all()\n\n    save_test_filename = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'util_voxel_test.npz\')\n    np.savez_compressed(save_test_filename, voxels=np.stack(results))\n\n    s = \'blender --background --python /path/to/voxel_render.py -- \'\n    s += \' --input %s \' % save_test_filename\n    s += "" --saveobj 1 --saveimg 0 --skimage 1 --threshold 0.5""  # note that this is visualization threshold, since all output are converted to 0/1\n    print(s)\n'"
util/visualizer.py,2,"b'import numpy as np\nimport os\nimport ntpath\nfrom . import util\nfrom . import html\nfrom scipy.misc import imresize\nimport math\nimport torch\nfrom subprocess import Popen, PIPE\nimport sys\nimport socket\n\n\ndef convert_image(input_image):\n    if torch.is_tensor(input_image):\n        if input_image.requires_grad:\n            image = util.tensor2im(input_image.data)\n        else:\n            image = util.tensor2im(input_image)\n    else:\n        image = input_image\n    return image\n\n\ndef convert_error(input_error):\n    error = input_error\n    if torch.is_tensor(input_error):\n        error = input_error.mean()\n        if getattr(error, \'is_cuda\'):\n            error = error.cpu()\n        if error.requires_grad:\n            error = error.item()\n    else:\n        error = input_error\n    return error\n\n\ndef save_images(webpage, images, names, image_path, title=None, width=256, aspect_ratio=1.0):\n    image_dir = webpage.get_image_dir()\n    short_path = ntpath.basename(image_path)\n    # name = os.path.splitext(short_path)[0]\n    name = short_path\n    if not title:\n        title = name\n    webpage.add_header(title)\n    ims = []\n    txts = []\n    links = []\n\n    for label, im in zip(names, images):\n        image_name = \'%s_%s.jpg\' % (name, label)\n        save_path = os.path.join(image_dir, image_name)\n        h, w, _ = im.shape\n        if aspect_ratio > 1.0:\n            im = imresize(im, (h, int(w * aspect_ratio)), interp=\'bicubic\')\n        if aspect_ratio < 1.0:\n            im = imresize(im, (int(h / aspect_ratio), w), interp=\'bicubic\')\n        util.save_image(im, save_path)\n\n        ims.append(image_name)\n        txts.append(label)\n        links.append(image_name)\n    webpage.add_images(ims, txts, links, width=width)\n\n\nclass Visualizer():\n    def __init__(self, opt):\n        # self.opt = opt\n        self.display_id = opt.display_id\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        self.port = opt.display_port\n        self.log_path = os.path.join(\n            opt.checkpoints_dir, opt.name, \'train_log.txt\')\n        self.prefix = socket.gethostname() + \' gpu %s\' % (str(opt.gpu_ids))\n        if self.display_id > 0:\n            import visdom\n            self.vis = visdom.Visdom(port=opt.display_port)\n            if not self.vis.check_connection():\n                self.throw_visdom_connection_error()\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            util.mkdirs([self.web_dir, self.img_dir])\n\n    # |visuals|: dictionary of images to display or save\n\n    def throw_visdom_connection_error(self):\n        cmd = sys.executable + \' -m visdom.server -p %d &>/dev/null &\' % self.port\n        print(\'\\n\\nCould not connect to Visdom server. \\n Trying to start a server....\')\n        Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)\n\n    def display_current_results(self, visuals, epoch, ncols=2, save_result=False, image_format=\'jpg\'):\n        if self.display_id > 0:  # show images in the browser\n            title = self.name\n            nrows = int(math.ceil(len(visuals.items()) / float(ncols)))\n            images = []\n            idx = 0\n            for label, image in visuals.items():\n                image_numpy = convert_image(image)\n                title += "" | "" if idx % nrows == 0 else "", ""\n                title += label\n                images.append(image_numpy.transpose([2, 0, 1]))\n                idx += 1\n            if len(visuals.items()) % ncols != 0:\n                white_image = np.ones_like(\n                    image_numpy.transpose([2, 0, 1])) * 255\n                images.append(white_image)\n            try:\n                self.vis.images(images, nrow=nrows, win=self.display_id + 1,\n                                opts=dict(title=title))\n            except IOError:\n                self.throw_visdom_connection_error()\n                self.vis.images(images, nrow=nrows, win=self.display_id + 1,\n                                opts=dict(title=title))\n\n        if self.use_html and save_result:  # save images to a html file\n            for label, image in visuals.items():\n                image_numpy = convert_image(image)\n                img_path = os.path.join(\n                    self.img_dir, \'epoch%.3d_%s.%s\' % (epoch, label, image_format))\n                util.save_image(image_numpy, img_path)\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, refresh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims = []\n                txts = []\n                links = []\n                for label, image_numpy in visuals.items():\n                    img_path = \'epoch%.3d_%s.%s\' % (n, label, image_format)\n                    ims.append(img_path)\n                    txts.append(label)\n                    links.append(img_path)\n                webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n\n    # losses: dictionary of error labels and values\n    def plot_current_losses(self, epoch, counter_ratio, opt, losses):\n        if self.display_id <= 0:\n            return\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\': [], \'Y\': [], \'legend\': list(losses.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append(\n            [convert_error(losses[k]) for k in self.plot_data[\'legend\']])\n        try:\n            self.vis.line(\n                X=np.stack([np.array(self.plot_data[\'X\'])] *\n                           len(self.plot_data[\'legend\']), 1),\n                Y=np.array(self.plot_data[\'Y\']),\n                opts={\n                    \'title\': self.name + \' loss over time\',\n                    \'legend\': self.plot_data[\'legend\'],\n                    \'xlabel\': \'epoch\',\n                    \'ylabel\': \'loss\'},\n                win=self.display_id)\n        except IOError:\n            self.throw_visdom_connection_error()\n            self.vis.line(\n                X=np.stack([np.array(self.plot_data[\'X\'])] *\n                           len(self.plot_data[\'legend\']), 1),\n                Y=np.array(self.plot_data[\'Y\']),\n                opts={\n                    \'title\': self.name + \' loss over time\',\n                    \'legend\': self.plot_data[\'legend\'],\n                    \'xlabel\': \'epoch\',\n                    \'ylabel\': \'loss\'},\n                win=self.display_id)\n\n    # losses: same format as |losses| of plotCurrentlosses\n    def print_current_losses(self, epoch, i, losses, t_model, t_data):\n        message = \'%s:  (epoch: %d, iters: %d, model: %.3f, data: %.3f) \' % (self.prefix, epoch, i, t_model, t_data)\n        for k, v in losses.items():\n            message += \', %s: %.3f\' % (k, v)\n\n        print(message)\n        # write losses to text file as well\n        with open(self.log_path, ""a"") as log_file:\n            log_file.write(message + \'\\n\')\n'"
render_module/calc_prob/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\n\nextra_compile_args = list()\nextra_compile_args.append('-std=c++11')\n\nextra_objects = list()\nassert(torch.cuda.is_available())\nsources = ['calc_prob/src/calc_prob.c']\nheaders = ['calc_prob/src/calc_prob.h']\ndefines = [('WITH_CUDA', True)]\nwith_cuda = True\n\nextra_objects = ['calc_prob/src/calc_prob_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi_params = {\n    'headers': headers,\n    'sources': sources,\n    'define_macros': defines,\n    'relative_to': __file__,\n    'with_cuda': with_cuda,\n    'extra_objects': extra_objects,\n    'include_dirs': [os.path.join(this_file, 'calc_prob/src')],\n    'extra_compile_args': extra_compile_args,\n}\n\nffi = create_extension(\n    'calc_prob._ext.calc_prob_lib',\n    package=True,\n    **ffi_params\n)\n\nif __name__ == '__main__':\n    ffi = create_extension(\n        'calc_prob._ext.calc_prob_lib',\n        package=False,\n        **ffi_params)\n    ffi.build()\n"""
render_module/calc_prob/setup.py,0,"b'import os\nfrom setuptools import setup, find_packages\n\n\nthis_file = os.path.dirname(__file__)\n\nsetup(\n    name=""pytorch_calc_stop_problility"",\n    version=""0.1.0"",\n    description=""Pytorch extension of calcualting ray stop probability"",\n    url=""https://bluhbluhbluh"",\n    author=""Zhoutong Zhang"",\n    author_email=""ztzhang@mit.edu"",\n    # Require cffi.\n    install_requires=[""cffi>=1.0.0""],\n    setup_requires=[""cffi>=1.0.0""],\n    # Exclude the build files.\n    packages=find_packages(exclude=[""build"", ""test""]),\n    # Package where to put the extensions. Has to be a prefix of build.py.\n    ext_package="""",\n    # Extensions to compile.\n    cffi_modules=[\n        os.path.join(this_file, ""build.py:ffi"")\n    ],\n)\n'"
render_module/vtn/build.py,2,"b""import os\nimport sys\nimport torch\nfrom torch.utils.ffi import create_extension\n\nsources = ['vtn/src/vtn.c']\nheaders = ['vtn/src/vtn.h']\ndefines = []\nwith_cuda = False\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\n\nextra_compile_args = list()\nextra_compile_args.append('-std=c++11')\nif sys.platform == 'linux':\n    extra_compile_args.append('-fopenmp')   # -fopenmp not supported on MacOS\nelse:\n    assert sys.platform == 'darwin'\n\nextra_objects = list()\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['vtn/src/vtn_cuda_generic.c']\n    headers += ['vtn/src/vtn_cuda_generic.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\n    extra_objects = ['vtn/src/vtn_cuda_kernel_generic.cu.o']\n    extra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi_params = {\n    'headers': headers,\n    'sources': sources,\n    'define_macros': defines,\n    'relative_to': __file__,\n    'with_cuda': with_cuda,\n    'extra_objects': extra_objects,\n    'include_dirs': [os.path.join(this_file, 'vtn/src')],\n    'extra_compile_args': extra_compile_args,\n}\n\nffi = create_extension(\n    'vtn._ext.vtn_lib',\n    package=True,\n    **ffi_params\n)\n\nif __name__ == '__main__':\n    ffi = create_extension(\n        'vtn._ext.vtn_lib',\n        package=False,\n        **ffi_params)\n    ffi.build()\n"""
render_module/vtn/setup.py,0,"b'import os\nfrom setuptools import setup, find_packages\n\n\nthis_file = os.path.dirname(__file__)\n\nsetup(\n    name=""pytorch_vtn"",\n    version=""0.1.0"",\n    description=""Pytorch extension of 3D transformer network"",\n    url=""https://github.mit.edu/ckzhang/pytorch-vtn.git"",\n    author=""Chengkai Zhang"",\n    author_email=""ckzhang@mit.edu"",\n    # Require cffi.\n    install_requires=[""cffi>=1.0.0""],\n    setup_requires=[""cffi>=1.0.0""],\n    # Exclude the build files.\n    packages=find_packages(exclude=[""build"", ""test""]),\n    # Package where to put the extensions. Has to be a prefix of build.py.\n    ext_package="""",\n    # Extensions to compile.\n    cffi_modules=[\n        os.path.join(this_file, ""build.py:ffi"")\n    ],\n)\n'"
render_module/calc_prob/calc_prob/__init__.py,0,b''
render_module/vtn/vtn/__init__.py,0,b''
render_module/calc_prob/calc_prob/functions/__init__.py,0,b'from .calc_prob import CalcStopProb\n'
render_module/calc_prob/calc_prob/functions/calc_prob.py,5,"b""import torch\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom .._ext import calc_prob_lib\nfrom cffi import FFI\nffi = FFI()\n\n\nclass CalcStopProb(Function):\n    @staticmethod\n    def forward(ctx, prob_in):\n        assert prob_in.dim() == 5\n        assert prob_in.type() == 'torch.cuda.FloatTensor'\n\n        stop_prob = prob_in.new_zeros(prob_in.shape)\n        # stop_prob.zero_()\n        calc_prob_lib.calc_prob_forward(prob_in, stop_prob)\n        ctx.save_for_backward(prob_in, stop_prob)\n        return stop_prob\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_in):\n        prob_in, stop_prob = ctx.saved_tensors\n        grad_out = grad_in.new_zeros(grad_in.shape)\n        # grad_out.zero_()\n        stop_prob_weighted = stop_prob * grad_in\n        calc_prob_lib.calc_prob_backward(prob_in, stop_prob_weighted, grad_out)\n        if torch.isnan(grad_out).any():\n            print('nan gradient found')\n        elif torch.isinf(grad_out).any():\n            print('inf gradient found')\n        return grad_out\n"""
render_module/vtn/vtn/functions/__init__.py,0,b'from .affine_grid3d import affine_grid3d\nfrom .grid_sample3d import grid_sample3d\n'
render_module/vtn/vtn/functions/affine_grid3d.py,10,"b'# functions/add.py\nimport torch\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\n\n\nclass AffineGridGen3DFunction(Function):\n    """"""\n    Generate a 3D affine grid of size (batch*sz1*sz2*sz3*3)\n    The affine grid is defined by a 3x4 matrix theta.\n    The grid is initialized as a grid in [-1,1] in all dimensions,\n    then transformed by matrix multiplication by theta.\n\n    When theta is set to eye(3,4), the grid should match the original grid in a box.\n    """"""\n    @staticmethod\n    def forward(ctx, theta, size):          # note that ctx is pytorch context\n        assert type(size) == torch.Size\n        assert len(size) == 5, \'Grid size should be specified by size of tensor to interpolate (5D)\'\n        assert theta.dim() == 3 and theta.size()[1:] == torch.Size([3, 4]), \'3D affine transformation defined by a 3D matrix of batch*3*4\'\n        assert theta.size(0) == size[0], \'batch size mismatch\'\n        N, C, sz1, sz2, sz3 = size\n        ctx.size = size\n        ctx.is_cuda = theta.is_cuda\n        theta = theta.contiguous()\n        base_grid = theta.new(N, sz1, sz2, sz3, 4)\n        linear_points = torch.linspace(-1, 1, sz1) if sz1 > 1 else torch.Tensor([-1])\n        base_grid[:, :, :, :, 0] = linear_points.view(1, -1, 1, 1).expand_as(base_grid[:, :, :, :, 0])\n        linear_points = torch.linspace(-1, 1, sz2) if sz2 > 1 else torch.Tensor([-1])\n        base_grid[:, :, :, :, 1] = linear_points.view(1, 1, -1, 1).expand_as(base_grid[:, :, :, :, 1])\n        linear_points = torch.linspace(-1, 1, sz3) if sz3 > 1 else torch.Tensor([-1])\n        base_grid[:, :, :, :, 2] = linear_points.view(1, 1, 1, -1).expand_as(base_grid[:, :, :, :, 2])\n        base_grid[:, :, :, :, 3] = 1\n        ctx.base_grid = base_grid\n\n        grid = torch.bmm(base_grid.view(N, sz1 * sz2 * sz3, 4), theta.transpose(1, 2))\n        grid = grid.view(N, sz1, sz2, sz3, 3)\n        return grid\n\n    @staticmethod\n    @once_differentiable    # Used so that backward runs on tensors instead of variables. Note that this disables use of gradient of gradients.\n    def backward(ctx, grad_output):\n        N, C, sz1, sz2, sz3 = ctx.size\n        assert grad_output.size() == torch.Size([N, sz1, sz2, sz3, 3])\n        assert ctx.is_cuda == grad_output.is_cuda\n        grad_output = grad_output.contiguous()\n        base_grid = ctx.base_grid\n        grad_theta = torch.bmm(\n            base_grid.view(N, sz1 * sz2 * sz3, 4).transpose(1, 2),\n            grad_output.view(N, sz1 * sz2 * sz3, 3))\n        grad_theta = grad_theta.transpose(1, 2)         # actually one transpose would do, but we choose to follow pytorch imp here.\n        return grad_theta, None\n\n\ndef affine_grid3d(theta, size):\n    return AffineGridGen3DFunction.apply(theta, size)\n'"
render_module/vtn/vtn/functions/grid_sample3d.py,6,"b'from torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom .._ext import vtn_lib\nfrom cffi import FFI\nffi = FFI()\n\ntypename_to_func_infix = {\n    \'torch.FloatTensor\': \'VTN_Float_\',\n    \'torch.DoubleTensor\': \'VTN_Double_\',\n    \'torch.cuda.FloatTensor\': \'VTN_Cuda_\',\n    \'torch.cuda.DoubleTensor\': \'VTN_CudaDouble_\',\n}\n\n\ndef function_by_type(name_, typename):\n    assert typename in typename_to_func_infix, \'GridSampler3D only support data type: %s, got: %s\' % (str(list(typename_to_func_infix.keys())), typename)\n    return typename_to_func_infix[typename] + name_\n\n\nclass GridSampler3D(Function):\n\n    @staticmethod\n    def forward(ctx, input, grid):\n        assert input.dim() == 5\n        assert grid.dim() == 5 and grid.size(4) == 3\n        assert grid.size(0) == input.size(0)\n        assert input.is_cuda == grid.is_cuda\n        assert input.type() == grid.type(), \'sampler input and grid must have same DataType. Types got: %s, %s\' % (input.type(), grid.type())\n        ctx.save_for_backward(input, grid)\n        ctx.is_cuda = input.is_cuda\n        grid_sz = grid.size()\n        output = input.new_zeros([grid_sz[0], input.size(1), grid_sz[1], grid_sz[2], grid_sz[3]])\n\n        func_name = function_by_type(\'BilinearSampler3DChannelFirst_updateOutput\', input.type())\n        getattr(vtn_lib, func_name)(input, grid, output)\n\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        assert ctx.is_cuda == grad_output.is_cuda\n        input, grid = ctx.saved_tensors\n        assert input.type() == grad_output.type(), \'sampler input and grad_output must have same DataType. Types got: %s, %s\' % (input.type(), grad_output.type())\n        grad_input = input.new(input.size())\n        grad_grid = grid.new(grid.size())\n\n        func_name = function_by_type(\'BilinearSampler3DChannelFirst_updateGradInput\', input.type())\n        getattr(vtn_lib, func_name)(input, grid, grad_input, grad_grid, grad_output)\n\n        return grad_input, grad_grid\n\n\ndef grid_sample3d(input, grid):\n    """"""\n    Perform trilinear interpolation on 3D matrices\n    input: batch * channel * x * y * z\n    grid: batch * gridx * gridy * gridz * 3\n    output: batch * channel * gridx * gridy * gridz\n    The interpolation is performed on each channel independently\n    """"""\n    return GridSampler3D.apply(input, grid)\n'"
render_module/vtn/vtn/modules/AffineGridGen3D.py,0,"b'from torch import nn\nfrom ..functions.affine_grid3d import affine_grid3d\n\n\nclass AffineGridGen3D(nn.Module):\n    def forward(self, theta, size):\n        return affine_grid3d(theta, size)\n'"
render_module/vtn/vtn/modules/GridSampler3D.py,0,"b'from torch import nn\nfrom ..functions.grid_sample3d import grid_sample3d\n\n\nclass GridSampler3D(nn.Module):\n    def forward(self, theta, size):\n        return grid_sample3d(theta, size)\n'"
render_module/vtn/vtn/modules/__init__.py,0,b'from .AffineGridGen3D import AffineGridGen3D\nfrom .GridSampler3D import GridSampler3D\n'
