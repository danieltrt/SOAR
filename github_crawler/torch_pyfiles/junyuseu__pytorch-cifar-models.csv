file_path,api_count,code
gen_mean_std.py,1,"b""import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\n\ndef gen_mean_std(dataset):\n\tdataloader = torch.utils.data.DataLoader(dataset, batch_size=50000, shuffle=False, num_workers=2)\n        train = iter(dataloader).next()[0]\n        mean = np.mean(train.numpy(), axis=(0, 2, 3))\n        std = np.std(train.numpy(), axis=(0, 2, 3))\n        return mean, std\n\nif __name__=='__main__':\n    # cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n    cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n    mean,std = gen_mean_std(cifar100)\n    print(mean, std)\n\n"""
main.py,12,"b'import argparse\nimport os\nimport time\nimport shutil\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom models import *\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch Cifar10 Training\')\nparser.add_argument(\'--epochs\', default=200, type=int, metavar=\'N\', help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\', help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=128, type=int, metavar=\'N\', help=\'mini-batch size (default: 128),only used for train\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float, metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\', help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float, metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int, metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\', help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\', help=\'evaluate model on validation set\')\nparser.add_argument(\'-ct\', \'--cifar-type\', default=\'10\', type=int, metavar=\'CT\', help=\'10 for cifar10,100 for cifar100 (default: 10)\')\n\nbest_prec = 0\n\ndef main():\n    global args, best_prec\n    args = parser.parse_args()\n    use_gpu = torch.cuda.is_available()\n\n    # Model building\n    print(\'=> Building model...\')\n    if use_gpu:\n        # model can be set to anyone that I have defined in models folder\n        # note the model should match to the cifar type !\n\n        model = resnet20_cifar()\n        # model = resnet32_cifar()\n        # model = resnet44_cifar()\n        # model = resnet110_cifar()\n        # model = preact_resnet110_cifar()\n        # model = resnet164_cifar(num_classes=100)\n        # model = resnet1001_cifar(num_classes=100)\n        # model = preact_resnet164_cifar(num_classes=100)\n        # model = preact_resnet1001_cifar(num_classes=100)\n\n        # model = wide_resnet_cifar(depth=26, width=10, num_classes=100)\n\n        # model = resneXt_cifar(depth=29, cardinality=16, baseWidth=64, num_classes=100)\n        \n        #model = densenet_BC_cifar(depth=190, k=40, num_classes=100)\n\n        # mkdir a new folder to store the checkpoint and best model\n        if not os.path.exists(\'result\'):\n            os.makedirs(\'result\')\n        fdir = \'result/resnet20_cifar10\'\n        if not os.path.exists(fdir):\n            os.makedirs(fdir)\n\n        # adjust the lr according to the model type\n        if isinstance(model, (ResNet_Cifar, PreAct_ResNet_Cifar)):\n            model_type = 1\n        elif isinstance(model, Wide_ResNet_Cifar):\n            model_type = 2\n        elif isinstance(model, (ResNeXt_Cifar, DenseNet_Cifar)):\n            model_type = 3\n        else:\n            print(\'model type unrecognized...\')\n            return\n\n        model = nn.DataParallel(model).cuda()\n        criterion = nn.CrossEntropyLoss().cuda()\n        optimizer = optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n        cudnn.benchmark = True\n    else:\n        print(\'Cuda is not available!\')\n        return\n\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(\'=> loading checkpoint ""{}""\'.format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_prec = checkpoint[\'best_prec\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})"".format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n    # Data loading and preprocessing\n    # CIFAR10\n    if args.cifar_type == 10:\n        print(\'=> loading cifar10 data...\')\n        normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n\n        train_dataset = torchvision.datasets.CIFAR10(\n            root=\'./data\', \n            train=True, \n            download=True,\n            transform=transforms.Compose([\n                transforms.RandomCrop(32, padding=4),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                normalize,\n            ]))\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n\n        test_dataset = torchvision.datasets.CIFAR10(\n            root=\'./data\',\n            train=False,\n            download=True,\n            transform=transforms.Compose([\n                transforms.ToTensor(),\n                normalize,\n            ]))\n        testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n    # CIFAR100\n    else:\n        print(\'=> loading cifar100 data...\')\n        normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n\n        train_dataset = torchvision.datasets.CIFAR100(\n            root=\'./data\',\n            train=True,\n            download=True,\n            transform=transforms.Compose([\n                transforms.RandomCrop(32, padding=4),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                normalize,\n            ]))\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n\n        test_dataset = torchvision.datasets.CIFAR100(\n            root=\'./data\',\n            train=False,\n            download=True,\n            transform=transforms.Compose([\n                transforms.ToTensor(),\n                normalize,\n            ]))\n        testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n\n    if args.evaluate:\n        validate(testloader, model, criterion)\n        return\n\n    for epoch in range(args.start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch, model_type)\n\n        # train for one epoch\n        train(trainloader, model, criterion, optimizer, epoch)\n\n        # evaluate on test set\n        prec = validate(testloader, model, criterion)\n\n        # remember best precision and save checkpoint\n        is_best = prec > best_prec\n        best_prec = max(prec,best_prec)\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'state_dict\': model.state_dict(),\n            \'best_prec\': best_prec,\n            \'optimizer\': optimizer.state_dict(),\n        }, is_best, fdir)\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef train(trainloader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(trainloader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input, target = input.cuda(), target.cuda()\n\n        # compute output\n        output = model(input)\n        loss = criterion(output, target)\n\n        # measure accuracy and record loss\n        prec = accuracy(output, target)[0]\n        losses.update(loss.item(), input.size(0))\n        top1.update(prec.item(), input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec {top1.val:.3f}% ({top1.avg:.3f}%)\'.format(\n                   epoch, i, len(trainloader), batch_time=batch_time,\n                   data_time=data_time, loss=losses, top1=top1))\n\n\ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    with torch.no_grad():\n        for i, (input, target) in enumerate(val_loader):\n            input, target = input.cuda(), target.cuda()\n\n            # compute output\n            output = model(input)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            prec = accuracy(output, target)[0]\n            losses.update(loss.item(), input.size(0))\n            top1.update(prec.item(), input.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.print_freq == 0:\n                print(\'Test: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec {top1.val:.3f}% ({top1.avg:.3f}%)\'.format(\n                   i, len(val_loader), batch_time=batch_time, loss=losses,\n                   top1=top1))\n\n    print(\' * Prec {top1.avg:.3f}% \'.format(top1=top1))\n\n    return top1.avg\n\n\ndef save_checkpoint(state, is_best, fdir):\n    filepath = os.path.join(fdir, \'checkpoint.pth\')\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(fdir, \'model_best.pth.tar\'))\n\n\ndef adjust_learning_rate(optimizer, epoch, model_type):\n    """"""For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs""""""\n    if model_type == 1:\n        if epoch < 80:\n            lr = args.lr\n        elif epoch < 120:\n            lr = args.lr * 0.1\n        else:\n            lr = args.lr * 0.01\n    elif model_type == 2:\n        if epoch < 60:\n            lr = args.lr\n        elif epoch < 120:\n            lr = args.lr * 0.2\n        elif epoch < 160:\n            lr = args.lr * 0.04\n        else:\n            lr = args.lr * 0.008\n    elif model_type == 3:\n        if epoch < 150:\n            lr = args.lr\n        elif epoch < 225:\n            lr = args.lr * 0.1\n        else:\n            lr = args.lr * 0.01\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\nif __name__==\'__main__\':\n    main()\n\n'"
models/__init__.py,0,b'from models.resnet_cifar import *\nfrom models.wide_resnet_cifar import *\nfrom models.resnext_cifar import *\nfrom models.densenet_cifar import *\n'
models/densenet_cifar.py,4,"b'""""""\nDenseNet for cifar with pytorch\n\nReference:\n[1] H. Gao, Z. Liu, L. Maaten and K. Weinberger. Densely connected convolutional networks. In CVPR, 2017\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nimport math\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass DenseNet_Cifar(nn.Module):\n    r""""""Densenet-BC model class, based on\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    """"""\n    def __init__(self, growth_rate=12, block_config=(16, 16, 16),\n                 num_init_features=24, bn_size=4, drop_rate=0, num_classes=10):\n\n        super(DenseNet_Cifar, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n        \n        # initialize conv and bn parameters\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.avg_pool2d(out, kernel_size=8, stride=1).view(features.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n\ndef densenet_BC_cifar(depth, k, **kwargs):\n    N = (depth - 4) // 6\n    model = DenseNet_Cifar(growth_rate=k, block_config=[N, N, N], num_init_features=2*k, **kwargs)\n    return model\n\n\nif __name__ == \'__main__\':\n    net = densenet_BC_cifar(190, 40, num_classes=100)\n    input = torch.randn(1, 3, 32, 32)\n    y = net(input)\n    print(net)\n    print(y.size())\n'"
models/resnet_cifar.py,2,"b'\'\'\'\nresnet for cifar in pytorch\n\nReference:\n[1] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\n[2] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016.\n\'\'\'\n\nimport torch\nimport torch.nn as nn\nimport math\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    "" 3x3 convolution with padding ""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion=1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion=4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes*4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes*4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass PreActBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(PreActBasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(out)\n\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out += residual\n\n        return out\n\n\nclass PreActBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes*4, kernel_size=1, bias=False)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(out)\n\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        out += residual\n\n        return out\n\n\nclass ResNet_Cifar(nn.Module):\n\n    def __init__(self, block, layers, num_classes=10):\n        super(ResNet_Cifar, self).__init__()\n        self.inplanes = 16\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 16, layers[0])\n        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n        self.avgpool = nn.AvgPool2d(8, stride=1)\n        self.fc = nn.Linear(64 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion)\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\nclass PreAct_ResNet_Cifar(nn.Module):\n\n    def __init__(self, block, layers, num_classes=10):\n        super(PreAct_ResNet_Cifar, self).__init__()\n        self.inplanes = 16\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.layer1 = self._make_layer(block, 16, layers[0])\n        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n        self.bn = nn.BatchNorm2d(64*block.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(8, stride=1)\n        self.fc = nn.Linear(64*block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes*block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes*block.expansion, kernel_size=1, stride=stride, bias=False)\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes*block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n\ndef resnet20_cifar(**kwargs):\n    model = ResNet_Cifar(BasicBlock, [3, 3, 3], **kwargs)\n    return model\n\n\ndef resnet32_cifar(**kwargs):\n    model = ResNet_Cifar(BasicBlock, [5, 5, 5], **kwargs)\n    return model\n\n\ndef resnet44_cifar(**kwargs):\n    model = ResNet_Cifar(BasicBlock, [7, 7, 7], **kwargs)\n    return model\n\n\ndef resnet56_cifar(**kwargs):\n    model = ResNet_Cifar(BasicBlock, [9, 9, 9], **kwargs)\n    return model\n\n\ndef resnet110_cifar(**kwargs):\n    model = ResNet_Cifar(BasicBlock, [18, 18, 18], **kwargs)\n    return model\n\n\ndef resnet1202_cifar(**kwargs):\n    model = ResNet_Cifar(BasicBlock, [200, 200, 200], **kwargs)\n    return model\n\n\ndef resnet164_cifar(**kwargs):\n    model = ResNet_Cifar(Bottleneck, [18, 18, 18], **kwargs)\n    return model\n\n\ndef resnet1001_cifar(**kwargs):\n    model = ResNet_Cifar(Bottleneck, [111, 111, 111], **kwargs)\n    return model\n\n\ndef preact_resnet110_cifar(**kwargs):\n    model = PreAct_ResNet_Cifar(PreActBasicBlock, [18, 18, 18], **kwargs)\n    return model\n\n\ndef preact_resnet164_cifar(**kwargs):\n    model = PreAct_ResNet_Cifar(PreActBottleneck, [18, 18, 18], **kwargs)\n    return model\n\n\ndef preact_resnet1001_cifar(**kwargs):\n    model = PreAct_ResNet_Cifar(PreActBottleneck, [111, 111, 111], **kwargs)\n    return model\n\n\nif __name__ == \'__main__\':\n    net = resnet20_cifar()\n    y = net(torch.randn(1, 3, 64, 64))\n    print(net)\n    print(y.size())\n\n'"
models/resnext_cifar.py,2,"b'""""""\nresneXt for cifar with pytorch\n\nReference:\n[1] S. Xie, G. Ross, P. Dollar, Z. Tu and K. He Aggregated residual transformations for deep neural networks. In CVPR, 2017\n""""""\n\nimport torch\nimport torch.nn as nn\nimport math\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, cardinality, baseWidth, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        D = int(planes * (baseWidth / 64.))\n        C = cardinality\n        self.conv1 = nn.Conv2d(inplanes, D*C, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(D*C)\n        self.conv2 = nn.Conv2d(D*C, D*C, kernel_size=3, stride=stride, padding=1, groups=C, bias=False)\n        self.bn2 = nn.BatchNorm2d(D*C)\n        self.conv3 = nn.Conv2d(D*C, planes*4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes*4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        \n        if residual.size() != out.size():\n            print(out.size(), residual.size())\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNeXt_Cifar(nn.Module):\n\n    def __init__(self, block, layers, cardinality, baseWidth, num_classes=10):\n        super(ResNeXt_Cifar, self).__init__()\n        self.inplanes = 64\n        self.cardinality = cardinality\n        self.baseWidth = baseWidth\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.avgpool = nn.AvgPool2d(8, stride=1)\n        self.fc = nn.Linear(256 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion)\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, self.cardinality, self.baseWidth, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, self.cardinality, self.baseWidth))\n        \n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resneXt_cifar(depth, cardinality, baseWidth, **kwargs):\n    assert (depth - 2) % 9 == 0\n    n = (depth - 2) / 9\n    model = ResNeXt_Cifar(Bottleneck, [n, n, n], cardinality, baseWidth, **kwargs)\n    return model\n\n\nif __name__ == \'__main__\':\n    net = resneXt_cifar(29, 16, 64)\n    y = net(torch.randn(1, 3, 32, 32))\n    print(net)\n    print(y.size())\n'"
models/wide_resnet_cifar.py,2,"b'""""""\nwide resnet for cifar in pytorch\n\nReference:\n[1] S. Zagoruyko and N. Komodakis. Wide residual networks. In BMVC, 2016.\n""""""\nimport torch\nimport torch.nn as nn\nimport math\nfrom models.resnet_cifar import BasicBlock\n\n\nclass Wide_ResNet_Cifar(nn.Module):\n\n    def __init__(self, block, layers, wfactor, num_classes=10):\n        super(Wide_ResNet_Cifar, self).__init__()\n        self.inplanes = 16\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 16*wfactor, layers[0])\n        self.layer2 = self._make_layer(block, 32*wfactor, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 64*wfactor, layers[2], stride=2)\n        self.avgpool = nn.AvgPool2d(8, stride=1)\n        self.fc = nn.Linear(64*block.expansion*wfactor, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion)\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef wide_resnet_cifar(depth, width, **kwargs):\n    assert (depth - 2) % 6 == 0\n    n = (depth - 2) / 6\n    return Wide_ResNet_Cifar(BasicBlock, [n, n, n], width, **kwargs)\n\n\nif __name__==\'__main__\':\n    net = wide_resnet_cifar(20, 10)\n    y = net(torch.randn(1, 3, 32, 32))\n    print(isinstance(net, Wide_ResNet_Cifar))\n    print(y.size())\n\n'"
