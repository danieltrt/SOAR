file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nimport os\nimport re\n\ndef resolve_requirements(file):\n    requirements = []\n    with open(file) as f:\n        req = f.read().splitlines()\n        for r in req:\n            if r.startswith(""-r""):\n                requirements += resolve_requirements(\n                    os.path.join(os.path.dirname(file), r.split("" "")[1]))\n            else:\n                requirements.append(r)\n    return requirements\n\n\ndef read_file(file):\n    with open(file) as f:\n        content = f.read()\n    return content\n\n\ndef find_version(file):\n    content = read_file(file)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"", content,\n                              re.M)\n    if version_match:\n        return version_match.group(1)\n\nrequirements = resolve_requirements(os.path.join(os.path.dirname(__file__),\n                                                 \'requirements.txt\'))\n\nreadme = read_file(os.path.join(os.path.dirname(__file__), ""README.md""))\nlicense = read_file(os.path.join(os.path.dirname(__file__), ""LICENSE""))\nshapenet_version = find_version(os.path.join(os.path.dirname(__file__), ""shapenet"",\n                                           ""__init__.py""))\n\nsetup(\n    name=\'shapenet\',\n    version=shapenet_version,\n    packages=find_packages(),\n    url=\'https://github.com/justussschock/shapenet\',\n    author=\'Justus Schock\',\n    author_email=\'justus.schock@rwth-aachen.de\',\n    description=\'\',\n    test_suite=""pytest"",\n    long_description=readme,\n    license=license,\n    install_requires=requirements,\n    tests_require=[""pytest-cov""],\n    classifiers=[\n        ""Development Status :: 4 - Beta"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Education"",\n        ""Intended Audience :: Science/Research"",\n        ""Natural Language :: English"",\n        ""Programming Language :: Python :: 3"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""Topic :: Scientific/Engineering :: Medical Science Apps.""\n    ]\n)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nimport re\nsys.path.insert(0, os.path.abspath(\'../.\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'shapenet\'\ncopyright = \'2019, Justus Schock\'\nauthor = \'Justus Schock\'\n\n\ndef read_file(file):\n    with open(file) as f:\n        content = f.read()\n    return content\n\n\ndef find_version(file):\n    content = read_file(file)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"", content,\n                              re.M)\n    if version_match:\n        return version_match.group(1)\n\n\n# The short X.Y version\nversion = find_version(""../shapenet/__init__.py"")\n# The full version, including alpha/beta/rc tags\nrelease = version\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.inheritance_diagram\',\n    \'sphinx.ext.autosectionlabel\',\n\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\nhtml_sidebars = {\n    \'**\': [\n        \'relations.html\',  # needs \'show_related\': True theme option to display\n        \'searchbox.html\',\n        \'localtoc.html\',\n        \'sourcelink.html\',\n    ]\n}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'shapedatadoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'shapenet.tex\', \'shapenet Documentation\',\n     \'Justus Schock\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'shapenet\', \'shapenet Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'shapenet\', \'shapedata Documentation\',\n     author, \'shapedata\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\'https://docs.python.org/\': None,\n                       \'delira\': (\'https://delira.readthedocs.io/en/master/\', None),\n                       \'scikit-image\': (\'http://scikit-image.org/docs/stable/\', None),\n                       \'matplotlib\': (\'https://matplotlib.org/\', None)\n                       }\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\nadd_module_names = False\n\nautodoc_default_flags = [\'members\',\n                         \'undoc-members\',\n                         \'private-members\',\n                         \'inherited-members\',\n                         \'show-inheritance\']\n\nautodoc_inherit_docstrings = True\n\nautodoc_mock_imports = [\n    ""numpy"",\n    ""pandas"",\n    ""torch"",\n    ""skimage"",\n    ""sklearn"",\n    ""jupyter"",\n    ""flake8""\n    ""pytest-cov"",\n    ""autopep8"",\n    ""ipython"",\n    ""joblib"",\n    ""pillow"",\n    ""SimpleITK"",\n    ""pylint"",\n    ""tqdm"",\n    ""visdom"",\n    ""pyyaml"",\n    ""psutil"",\n    ""nested_lookup"",\n    ""colorlover"",\n    ""flask"",\n    ""graphviz"",\n    ""matplotlib"",\n    ""seaborn"",\n    ""scipy"",\n    ""scipy.ndimage"",\n    ""PIL"",\n    ""yaml"",\n    ""delira"",\n    ""shapedata"",\n    ""kaggle""\n]\n'"
shapenet/__init__.py,0,"b""__version__ = '0.2.0'\n\nfrom .networks import SingleShapeNetwork\nfrom .layer import HomogeneousTransformationLayer\n"""
tests/__init__.py,0,b'\n'
shapenet/jit/__init__.py,0,b'from .shape_network import ShapeNetwork as JitShapeNetwork\nfrom .shape_layer import ShapeLayer as JitShapeLayer\nfrom .homogeneous_shape_layer import HomogeneousShapeLayer as \\\n                                        JitHomogeneousShapeLayer\nfrom .homogeneous_transform_layer import HomogeneousTransformationLayer as \\\n                                        JitHomogeneousTransformationLayer'
shapenet/jit/abstract_network.py,8,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport torch\nfrom abc import abstractmethod\n\n\nclass AbstractShapeNetwork(torch.jit.ScriptModule):\n    """"""\n    Abstract JIT Network\n\n    """"""\n\n    def __init__(self, **kwargs):\n\n        super().__init__(optimize=True)\n\n    @staticmethod\n    def norm_type_to_class(norm_type):\n        norm_dict = {\'instance\': torch.nn.InstanceNorm2d,\n                     \'batch\': torch.nn.BatchNorm2d}\n\n        norm_class = norm_dict.get(norm_type, None)\n\n        return norm_class\n\n\nclass AbstractFeatureExtractor(torch.jit.ScriptModule):\n    """"""\n    Abstract Feature Extractor Class all further feature extractors\n    should be derived from\n\n    """"""\n\n    def __init__(self, in_channels, out_params, norm_class, p_dropout=0):\n        """"""\n\n        Parameters\n        ----------\n        in_channels : int\n            number of input channels\n        out_params : int\n            number of outputs\n        norm_class : Any\n            Class implementing a normalization\n        p_dropout : float\n            dropout probability\n\n        """"""\n        super().__init__()\n        self.model = self._build_model(in_channels, out_params, norm_class,\n                                       p_dropout)\n\n    @torch.jit.script_method\n    def forward(self, input_batch):\n        """"""\n        Feed batch through network\n\n        Parameters\n        ----------\n        input_batch : :class:`torch.Tensor`\n            batch to feed through network\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            extracted features\n\n        """"""\n        return self.model(input_batch)\n\n    @staticmethod\n    @abstractmethod\n    def _build_model(in_channels, out_features, norm_class, p_dropout):\n        """"""\n        Build the actual model structure\n\n        Parameters\n        ----------\n        in_channels : int\n            number of input channels\n        out_features : int\n            number of outputs\n        norm_class : Any\n            class implementing a normalization\n        p_dropout : float\n            dropout probability\n\n        Returns\n        -------\n        :class:`torch.jit.ScriptModule`\n            ensembled model\n\n        """"""\n        raise NotImplementedError\n'"
shapenet/jit/feature_extractors.py,14,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport torch\nfrom .abstract_network import AbstractFeatureExtractor\n\n\nclass Conv2dRelu(torch.jit.ScriptModule):\n    """"""\n    Block holding one Conv2d and one ReLU layer\n    \n    """"""\n\n    def __init__(self, *args, **kwargs):\n        """"""\n\n        Parameters\n        ----------\n        **args :\n            positional arguments (passed to Conv2d)\n        **kwargs : dict\n            keyword arguments (passed to Conv2d)\n\n        """"""\n        super().__init__()\n        self._conv = torch.nn.Conv2d(*args, **kwargs)\n        self._relu = torch.nn.ReLU()\n\n    @torch.jit.script_method\n    def forward(self, input_batch):\n        """"""\n        Forward batch though layers\n\n        Parameters\n        ----------\n        input_batch : class:`torch.Tensor`\n            input batch\n\n        Returns\n        -------\n        class:`torch.Tensor`\n            result\n\n        """"""\n        return self._relu(self._conv(input_batch))\n\n\nclass Img224x224Kernel7x7SeparatedDims(AbstractFeatureExtractor):\n    @staticmethod\n    def _build_model(in_channels, out_params, norm_class, p_dropout):\n        """"""\n        Build the actual model structure\n\n        Parameters\n        ----------\n        in_channels : int\n            number of input channels\n        out_params : int\n            number of outputs\n        norm_class : Any\n            class implementing a normalization\n        p_dropout : float\n            dropout probability\n\n        Returns\n        -------\n        :class:`torch.jit.ScriptModule`\n            ensembled model\n\n        """"""\n        model = torch.nn.Sequential()\n\n        model.add_module(""conv_1"", Conv2dRelu(in_channels, 64, (7, 1)))\n        model.add_module(""conv_2"", Conv2dRelu(64, 64, (1, 7)))\n\n        model.add_module(""down_conv_1"", Conv2dRelu(64, 128, (7, 7), stride=2))\n        if norm_class is not None:\n            model.add_module(""norm_1"", norm_class(128))\n        if p_dropout:\n            model.add_module(""dropout_1"", torch.nn.Dropout2d(p_dropout))\n\n        model.add_module(""conv_3"", Conv2dRelu(128, 128, (7, 1)))\n        model.add_module(""conv_4"", Conv2dRelu(128, 128, (1, 7)))\n\n        model.add_module(""down_conv_2"", Conv2dRelu(128, 256, (7, 7), stride=2))\n        if norm_class is not None:\n            model.add_module(""norm_2"", norm_class(256))\n        if p_dropout:\n            model.add_module(""dropout_2"", torch.nn.Dropout2d(p_dropout))\n\n        model.add_module(""conv_5"", Conv2dRelu(256, 256, (5, 1)))\n        model.add_module(""conv_6"", Conv2dRelu(256, 256, (1, 5)))\n\n        model.add_module(""down_conv_3"", Conv2dRelu(256, 256, (5, 5), stride=2))\n        if norm_class is not None:\n            model.add_module(""norm_3"", norm_class(256))\n        if p_dropout:\n            model.add_module(""dropout_3"", torch.nn.Dropout2d(p_dropout))\n\n        model.add_module(""conv_7"", Conv2dRelu(256, 256, (5, 1)))\n        model.add_module(""conv_8"", Conv2dRelu(256, 256, (1, 5)))\n\n        model.add_module(""down_conv_4"", Conv2dRelu(256, 128, (5, 5), stride=2))\n        if norm_class is not None:\n            model.add_module(""norm_4"", norm_class(128))\n        if p_dropout:\n            model.add_module(""dropout_4"", torch.nn.Dropout2d(p_dropout))\n\n        model.add_module(""conv_9"", Conv2dRelu(128, 128, (3, 1)))\n        model.add_module(""conv_10"", Conv2dRelu(128, 128, (1, 3)))\n        model.add_module(""conv_11"", Conv2dRelu(128, 128, (3, 1)))\n        model.add_module(""conv_12"", Conv2dRelu(128, 128, (1, 3)))\n\n        model.add_module(""final_conv"", torch.nn.Conv2d(128, out_params,\n                                                       (2, 2)))\n\n        return torch.jit.trace(model, torch.rand(5, in_channels, 224, 224))\n'"
shapenet/jit/homogeneous_shape_layer.py,7,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport torch\nfrom .shape_layer import ShapeLayer\nfrom .homogeneous_transform_layer import HomogeneousTransformationLayer\n\n\nclass HomogeneousShapeLayer(torch.jit.ScriptModule):\n    """"""\n    Module to Perform a Shape Prediction\n    (including a global homogeneous transformation)\n\n    """"""\n\n    def __init__(self, shapes, n_dims, use_cpp=False):\n        """"""\n\n        Parameters\n        ----------\n        shapes : np.ndarray\n            shapes to construct a :class:`ShapeLayer`\n        n_dims : int\n            number of shape dimensions\n        use_cpp : bool\n            whether or not to use (experimental) C++ Implementation\n\n        See Also\n        --------\n        :class:`ShapeLayer`\n        :class:`HomogeneousTransformationLayer`\n\n        """"""\n        super().__init__()\n\n        self._shape_layer = ShapeLayer(shapes, use_cpp)\n        self._homogen_trafo = HomogeneousTransformationLayer(n_dims, use_cpp)\n\n        self.register_buffer(""_indices_shape_params"",\n                             torch.arange(self._shape_layer.num_params))\n        self.register_buffer(""_indices_homogen_params"",\n                             torch.arange(self._shape_layer.num_params,\n                                          self.num_params))\n\n    @torch.jit.script_method\n    def forward(self, params: torch.Tensor):\n        """"""\n        Performs the actual prediction\n\n        Parameters\n        ----------\n        params : :class:`torch.Tensor`\n            input parameters\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            predicted shape\n\n        """"""\n\n        shape_params = params.index_select(\n            dim=1, index=getattr(self, ""_indices_shape_params"")\n        )\n\n        transformation_params = params.index_select(\n            dim=1, index=getattr(self, ""_indices_homogen_params"")\n        )\n        shapes = self._shape_layer(shape_params)\n        transformed_shapes = self._homogen_trafo(shapes, transformation_params)\n\n        return transformed_shapes\n\n    @property\n    def num_params(self):\n        """"""\n        Property to access these layer\'s number of parameters\n\n        Returns\n        -------\n        int\n            number of parameters\n\n        """"""\n        return self._shape_layer.num_params + self._homogen_trafo.num_params\n'"
shapenet/jit/homogeneous_transform_layer.py,56,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport torch\nimport os\nfrom torch.utils.cpp_extension import load as load_cpp\n\n\nclass HomogeneousTransformationLayer(torch.jit.ScriptModule):\n    """"""\n    Wrapper Class to Wrap the Python and C++ API into a combined python API\n\n    """"""\n    def __init__(self, n_dims: int, use_cpp=False):\n        """"""\n\n        Parameters\n        ----------\n        n_dims : int\n            number of dimensions\n        use_cpp : bool\n            whether or not to use C++ implementation\n\n        Raises\n        ------\n        AssertionError\n            if ``use_cpp`` is True, currently only the python version is\n            supported\n\n        """"""\n\n        assert use_cpp==False, ""Currently only the python version is supported""\n        super().__init__()\n\n        self._n_params = {}\n\n        if n_dims == 2:\n            self._n_params[""scale""] = 1\n            self._n_params[""rotation""] = 1\n            self._n_params[""translation""] = 2\n        elif n_dims == 3:\n            self._n_params[""scale""] = 3\n            self._n_params[""rotation""] = 3\n            self._n_params[""translation""] = 3\n\n        self._layer = _HomogeneousTransformationLayerPy(n_dims)\n\n        total_params = 0\n        for key, val in self._n_params.items():\n            self.register_buffer(""_indices_%s_params"" % key,\n                                 torch.arange(total_params, total_params + val)\n                                 )\n            total_params += val\n\n    @torch.jit.script_method\n    def forward(self, shapes: torch.Tensor, params: torch.Tensor):\n        """"""\n        Selects individual parameters from ``params`` and forwards them through \n        the actual layer implementation\n        \n        Parameters\n        ----------\n        shapes : :class:`torch.Tensor`\n            shapes to transform\n        params : :class:`torch.Tensor`\n            parameters specifying the affine transformation\n        \n        Returns\n        -------\n        Returns\n        -------\n        :class:`torch.Tensor`\n            the transformed shapes in cartesian coordinates\n\n        """"""\n\n        rotation_params = params.index_select(\n            dim=1, index=getattr(self, ""_indices_rotation_params"")\n        )\n        scale_params = params.index_select(\n            dim=1, index=getattr(self, ""_indices_scale_params"")\n        )\n        translation_params = params.index_select(\n            dim=1, index=getattr(self, ""_indices_translation_params"")\n        )\n\n        return self._layer(shapes, rotation_params, translation_params,\n                           scale_params)\n\n    @property\n    def num_params(self):\n        num_params = 0\n        for key, val in self._n_params.items():\n            num_params += val\n\n        return num_params\n\n\nclass _HomogeneousTransformationLayerPy(torch.jit.ScriptModule):\n    """"""\n    Module to perform homogeneous transformations in 2D and 3D\n    (Implemented in Python)\n\n    """"""\n\n    __constants__ = [""_n_dims""]\n\n    def __init__(self, n_dims):\n        """"""\n\n        Parameters\n        ----------\n        n_dims: int\n            number of dimensions\n        """"""\n        super().__init__()\n\n        homogen_trafo = torch.zeros(1, n_dims+1, n_dims+1)\n        homogen_trafo[:, -1, :-1] = 0.\n        homogen_trafo[:, -1, -1] = 1.\n\n        self.register_buffer(""_trafo_matrix"", homogen_trafo)\n        self._n_dims = n_dims\n\n    @torch.jit.script_method\n    def forward(self, shapes: torch.Tensor, rotation_params: torch.Tensor,\n                translation_params: torch.Tensor, scale_params: torch.Tensor):\n        """"""\n        ensembles the homogeneous transformation matrix and applies it to the\n        shape tensor\n\n        Parameters\n        ----------\n        shapes : :class:`torch.Tensor`\n            shapes to transform\n        rotation_params : :class:`torch.Tensor`\n            parameters specifying the rotation (one per DoF)\n        translation_params : :class:`torch.Tensor`\n            parameters specifying the translation (one per dimension)\n        scale_params : :class:`torch.Tensor`\n            parameter specifying the global scaling factor\n            (currently only isotropic scaling supported)\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            the transformed shapes in cartesian coordinates\n\n        """"""\n\n        assert shapes.size(-1) == self._n_dims, ""Layer for other "" \\\n                                                ""dimensionality specified""\n\n        trafo_matrix = self._ensemble_trafo(rotation_params,\n                                            translation_params, scale_params)\n\n        homogen_shapes = torch.cat([shapes,\n                                    torch.ones([shapes.size(0),\n                                               shapes.size(1), 1],\n                                               dtype=shapes.dtype,\n                                               device=shapes.device)],\n                                   dim=-1)\n\n        transformed_shapes = torch.bmm(homogen_shapes,\n                                       trafo_matrix.permute(0, 2, 1))\n\n        return transformed_shapes[:, :, :-1]\n\n    @torch.jit.script_method\n    def _ensemble_trafo(self, rotation_params: torch.Tensor,\n                        translation_params: torch.Tensor,\n                        scale_params: torch.Tensor):\n        """"""\n        ensembles the transformation matrix in 2D and 3D\n\n        Parameters\n        ----------\n        rotation_params : :class:`torch.Tensor`\n            parameters specifying the rotation (one per DoF)\n        translation_params : :class:`torch.Tensor`\n            parameters specifying the translation (one per dimension)\n        scale_params : :class:`torch.Tensor`\n            parameter specifying the global scaling factor\n            (currently only isotropic scaling supported)\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            transformation matrix\n\n        """"""\n\n        rotation_params = rotation_params.view(rotation_params.size()[:2])\n        translation_params = translation_params.view(\n            translation_params.size()[:2])\n        scale_params = scale_params.view(scale_params.size()[:2])\n\n        if self._n_dims == 2:\n            trafo = self._ensemble_2d_matrix(rotation_params,\n                                            translation_params, scale_params)\n        else:\n            trafo = self._ensemble_3d_matrix(rotation_params,\n                                            translation_params, scale_params)\n\n        return trafo\n\n    @torch.jit.script_method\n    def _ensemble_2d_matrix(self, rotation_params: torch.Tensor,\n                            translation_params: torch.Tensor,\n                            scale_params: torch.Tensor):\n        """"""\n        ensembles the homogeneous transformation matrix for 2D\n\n        Parameters\n        ----------\n        rotation_params : :class:`torch.Tensor`\n            parameters specifying the rotation (one parameter)\n        translation_params : :class:`torch.Tensor`\n            parameters specifying the translation (two parameters)\n        scale_params : :class:`torch.Tensor`\n            parameter specifying the global scaling factor (one parameter)\n            (currently only isotropic scaling supported)\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            2D transformation matrix\n\n        """"""\n\n        homogen_trafo = getattr(self, ""_trafo_matrix"").repeat(\n            scale_params.size(0), 1, 1).clone()\n\n        homogen_trafo[:, 0, 0] = (scale_params *\n                                  rotation_params.cos())[:, 0].clone()\n        # s*sin\\theta\n        homogen_trafo[:, 0, 1] = (scale_params *\n                                  rotation_params.sin())[:, 0].clone()\n        # -s*sin\\theta\n        homogen_trafo[:, 1, 0] = (-scale_params *\n                                  rotation_params.sin())[:, 0].clone()\n        # s*cos\\theta\n        homogen_trafo[:, 1, 1] = (scale_params *\n                                  rotation_params.cos())[:, 0].clone()\n\n        # translation params\n        homogen_trafo[:, :-1, -1] = translation_params.clone()\n\n        return homogen_trafo\n\n    @torch.jit.script_method\n    def _ensemble_3d_matrix(self, rotation_params: torch.Tensor,\n                            translation_params: torch.Tensor,\n                            scale_params: torch.Tensor):\n        """"""\n        ensembles the homogeneous transformation matrix for 3D\n\n        Parameters\n        ----------\n        rotation_params : :class:`torch.Tensor`\n            parameters specifying the rotation (three parameters)\n        translation_params : :class:`torch.Tensor`\n            parameters specifying the translation (three parameters)\n        scale_params : :class:`torch.Tensor`\n            parameter specifying the global scaling factor (one parameter)\n            (currently only isotropic scaling supported)\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            3D transformation matrix\n\n        """"""\n\n        homogen_trafo = getattr(self, ""_trafo_matrix"").repeat(\n            scale_params.size(0), 1, 1).clone()\n\n        roll = rotation_params[:, 2].unsqueeze(-1)\n        pitch = rotation_params[:, 1].unsqueeze(-1)\n        yaw = rotation_params[:, 0].unsqueeze(-1)\n\n        # Note that the elements inside the transformation matrix are swapped\n        # due to the zyx convention\n\n        # s*(cos(pitch)*cos(roll))\n        homogen_trafo[:, 0, 0] = (scale_params *\n                                  (pitch.cos() * roll.cos()))[:, 0].clone()\n\n        # s*(cos(pitch)*sin(roll))\n        homogen_trafo[:, 0, 1] = (scale_params *\n                                  (pitch.cos() * roll.sin()))[:, 0].clone()\n\n        # s*(-sin(pitch))\n        homogen_trafo[:, 0, 2] = (scale_params *\n                                  (-pitch.sin()))[:, 0].clone()\n\n        # s*(sin(yaw)*sin(pitch)*cos(roll) - cos(yaw)*sin(roll))\n        homogen_trafo[:, 1, 0] = (scale_params *\n                                  (yaw.sin() * pitch.sin() * roll.cos() -\n                                   yaw.cos() * roll.sin()))[:, 0].clone()\n\n        # s*(sin(yaw)*sin(pitch)*sin(roll) + cos(yaw)*cos(roll))\n        homogen_trafo[:, 1, 1] = (scale_params *\n                                  (yaw.sin() * pitch.sin() * roll.sin() +\n                                   yaw.cos() * roll.cos()))[:, 0].clone()\n\n        # s*(sin(yaw)*cos(pitch))\n        homogen_trafo[:, 1, 2] = (scale_params *\n                                  (yaw.sin() * pitch.cos()))[:, 0].clone()\n\n        # s*(cos(yaw)*sin(pitch)*cos(roll) + sin(yaw)*sin(roll))\n        homogen_trafo[:, 2, 0] = (scale_params *\n                                  (yaw.cos() * pitch.sin() * roll.cos() +\n                                   yaw.sin() * roll.sin()))[:, 0].clone()\n\n        # s*(cos(yaw)*sin(pitch)*sin(roll)-sin(yaw)*cos(roll))\n        homogen_trafo[:, 2, 1] = (scale_params *\n                                  (yaw.cos() * pitch.sin() * roll.sin() -\n                                   yaw.sin() * roll.cos()))[:, 0].clone()\n\n        # s*(cos(yaw)*cos(pitch))\n        homogen_trafo[:, 2, 2] = (scale_params *\n                                  (yaw.cos() * pitch.cos()))[:, 0].clone()\n\n        # translation params\n        homogen_trafo[:, :-1, -1] = translation_params.clone()\n\n        return homogen_trafo\n\n\nif __name__ == \'__main__\':\n    shapes_2d = torch.rand(10, 68, 2)\n    rotation_params_2d = torch.rand(10, 1, 1, 1)\n    # translation_params_2d = torch.rand(10, 2, 1, 1)\n    translation_params_2d = torch.rand(10, 2, 1, 1)\n    scale_params_2d = torch.rand(10, 1, 1, 1)\n\n    print(""Creating Python Layer"")\n    layer_2d_py = _HomogeneousTransformationLayerPy(n_dims=2)\n\n\n    result_2d_py = layer_2d_py(shapes_2d, rotation_params_2d, translation_params_2d,\n                               scale_params_2d)\n    shapes_3d = torch.rand(10, 68, 3)\n    rotation_params_3d = torch.rand(10, 3, 1, 1)\n    # rotation_params_3d = torch.zeros(10, 3, 1, 1)\n    translation_params_3d = torch.rand(10, 3, 1, 1)\n    # translation_params_3d = torch.zeros(10, 3, 1, 1)\n    scale_params_3d = torch.rand(10, 3, 1, 1)\n\n    layer_3d_py = _HomogeneousTransformationLayerPy(n_dims=3)\n    result_3d_py = layer_3d_py(shapes_3d, rotation_params_3d, translation_params_3d,\n                               scale_params_3d)\n\n'"
shapenet/jit/shape_layer.py,11,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport numpy as np\nimport torch\n\n\nclass ShapeLayer(torch.jit.ScriptModule):\n    def __init__(self, shapes, use_cpp=False):\n        """"""\n\n        Parameters\n        ----------\n        shapes : np.ndarray\n            the shape components needed by the actual shape layer implementation\n        use_cpp : bool\n            whether to use cpp implementation or not\n            (Currently only the python version is supported)\n\n        """"""\n        super().__init__()\n\n        self._layer = _ShapeLayerPy(shapes)\n        assert not use_cpp, ""Currently only the Python Version is supported""\n\n    @torch.jit.script_method\n    def forward(self, shape_params: torch.Tensor):\n        return self._layer(shape_params)\n\n    @property\n    def num_params(self):\n        return self._layer.num_params\n\n\nclass _ShapeLayerPy(torch.jit.ScriptModule):\n    """"""\n    Python Implementation of Shape Layer\n\n    """"""\n\n    def __init__(self, shapes):\n        """"""\n\n        Parameters\n        ----------\n        shapes : np.ndarray\n            eigen shapes (obtained by PCA)\n\n        """"""\n        super().__init__()\n\n        self.register_buffer(""_shape_mean"", torch.from_numpy(\n            shapes[0]).float().unsqueeze(0))\n        components = []\n        for i, _shape in enumerate(shapes[1:]):\n            components.append(torch.from_numpy(\n                _shape).float().unsqueeze(0))\n\n        component_tensor = torch.cat(components).unsqueeze(0)\n        self.register_buffer(""_shape_components"", component_tensor)\n\n    @torch.jit.script_method\n    def forward(self, shape_params: torch.Tensor):\n        """"""\n        Ensemble shape from parameters\n\n        Parameters\n        ----------\n        shape_params : :class:`torch.Tensor`\n            shape parameters\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            ensembled shape\n\n        """"""\n\n        shapes = getattr(self, ""_shape_mean"").clone()\n        shapes = shapes.expand(shape_params.size(0), shapes.size(1),\n                               shapes.size(2))\n\n        components = getattr(self, ""_shape_components"")\n        components = components.expand(shape_params.size(0),\n                                       components.size(1), components.size(2),\n                                       components.size(3))\n\n        weighted_components = components.mul(\n            shape_params.expand_as(components))\n\n        shapes = shapes.add(weighted_components.sum(dim=1))\n\n        return shapes\n\n    @property\n    def num_params(self):\n        return getattr(self, ""_shape_components"").size(1)\n'"
shapenet/jit/shape_network.py,17,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport torch\nimport torchvision.models\nimport logging\n\nfrom .feature_extractors import Img224x224Kernel7x7SeparatedDims\nfrom .abstract_network import AbstractShapeNetwork\n\nlogger = logging.getLogger(__file__)\n\n\nclass ShapeNetwork(AbstractShapeNetwork):\n    """"""\n    Network to Predict a single shape\n\n    """"""\n\n    __constants__ = [\'num_out_params\']\n\n    def __init__(self, layer_cls,\n                 layer_kwargs,\n                 in_channels=1,\n                 norm_type=\'instance\',\n                 img_size=224,\n                 tiny=False,\n                 feature_extractor=None,\n                 **kwargs\n                 ):\n        """"""\n\n        Parameters\n        ----------\n        layer_cls : type, subclass of :class:`torch.nn.Module`\n            Class to instantiate the last layer (usually a shape-constrained\n            or transformation layer)\n        layer_kwargs : dict\n            keyword arguments to create an instance of ``layer_cls``\n        in_channels : int\n            number of input channels\n        norm_type : str or None\n            Indicates the type of normalization used in this network;\n            Must be one of [None, \'instance\', \'batch\', \'group\']\n        **kwargs :\n            additional keyword arguments\n\n        """"""\n\n        super().__init__()\n\n        self._kwargs = kwargs\n\n        self._out_layer = layer_cls(**layer_kwargs)\n        self.num_out_params = self._out_layer.num_params\n        self.img_size = img_size\n        norm_class = self.norm_type_to_class(norm_type)\n\n        args = [in_channels, self.num_out_params, norm_class]\n        feature_kwargs = {}\n\n        if img_size == 224:\n            if feature_extractor and hasattr(torchvision.models,\n                                             feature_extractor):\n                feature_extractor_cls = getattr(torchvision.models,\n                                                feature_extractor)\n                args = [False]\n                feature_kwargs = {""num_classes"": self.num_out_params}\n\n            else:\n                feature_extractor_cls = Img224x224Kernel7x7SeparatedDims\n\n        else:\n            raise ValueError(""No known dimension for image size found"")\n        # self._model = Img224x224Kernel7x7SeparatedDims(\n        #     in_channels, self._out_layer.num_params, norm_class\n        # )\n\n        model = feature_extractor_cls(*args, **feature_kwargs)\n\n        if isinstance(model, torchvision.models.VGG):\n            model.features = torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n                *list(model.features.children())[1:]\n            )\n\n        elif isinstance(model, torchvision.models.ResNet):\n            model.conv1 = torch.nn.Conv2d(in_channels, 64, kernel_size=7,\n                                          stride=2, padding=3,\n                                          bias=False)\n\n        elif isinstance(model, torchvision.models.Inception3):\n            model.Conv2d_1a_3x3 = \\\n                torchvision.models.inception.BasicConv2d(in_channels, 32,\n                                                         kernel_size=3,\n                                                         stride=2)\n\n        elif isinstance(model, torchvision.models.DenseNet):\n            out_channels = list(model.features.children()\n                                )[0].out_channels\n            model.features = torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, out_channels, kernel_size=7,\n                                stride=2, padding=3, bias=False),\n                *list(model.features.children())[1:]\n            )\n\n        elif isinstance(model, torchvision.models.SqueezeNet):\n            out_channels = list(model.features.children()\n                                )[0].out_channels\n            model.features = torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, out_channels, kernel_size=7,\n                                stride=2),\n                *list(model.features.children())[1:]\n            )\n\n        elif isinstance(model, torchvision.models.AlexNet):\n            out_channels = list(model.features.children()\n                                )[0].out_channels\n            model.features = torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, out_channels, kernel_size=11,\n                                stride=4, padding=2),\n                *list(model.features.children())[1:]\n            )\n\n        self._model = torch.jit.trace(model,\n                                      torch.rand(10, in_channels,\n                                                 img_size, img_size))\n\n    @torch.jit.script_method\n    def forward(self, input_images):\n        """"""\n        Forward input batch through network and shape layer\n\n        Parameters\n        ----------\n        input_images : :class:`torch.Tensor`\n            input batch\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            predicted shapes\n\n        """"""\n\n        features = self._model(input_images)\n\n        return self._out_layer(features.view(input_images.size(0),\n                                             self.num_out_params, 1, 1))\n\n    @property\n    def model(self):\n        return self._model\n\n    @model.setter\n    def model(self, model: torch.nn.Module):\n        if isinstance(model, torch.nn.Module):\n            self._model = model\n        else:\n            raise AttributeError(""Invalid Model"")\n'"
shapenet/layer/__init__.py,0,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\n""""""\nModule to Implement Custom Layers for Shape Prediction and Homogeneous\nTransformation\n""""""\n\n__version__ = \'0.1.0\'\n\nfrom .homogeneous_shape_layer import HomogeneousShapeLayer\nfrom .shape_layer import ShapeLayer\nfrom .homogeneous_transform_layer import HomogeneousTransformationLayer\n'"
shapenet/layer/homogeneous_shape_layer.py,6,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport torch\nfrom .shape_layer import ShapeLayer\nfrom .homogeneous_transform_layer import HomogeneousTransformationLayer\n\n\nclass HomogeneousShapeLayer(torch.nn.Module):\n    """"""\n    Module to Perform a Shape Prediction\n    (including a global homogeneous transformation)\n\n    """"""\n    def __init__(self, shapes, n_dims, use_cpp=False):\n        """"""\n\n        Parameters\n        ----------\n        shapes : np.ndarray\n            shapes to construct a :class:`ShapeLayer`\n        n_dims : int\n            number of shape dimensions\n        use_cpp : bool\n            whether or not to use (experimental) C++ Implementation\n\n        See Also\n        --------\n        :class:`ShapeLayer`\n        :class:`HomogeneousTransformationLayer`\n\n        """"""\n        super().__init__()\n\n        self._shape_layer = ShapeLayer(shapes, use_cpp)\n        self._homogen_trafo = HomogeneousTransformationLayer(n_dims, use_cpp)\n\n        self.register_buffer(""_indices_shape_params"",\n                             torch.arange(self._shape_layer.num_params))\n        self.register_buffer(""_indices_homogen_params"",\n                             torch.arange(self._shape_layer.num_params,\n                                          self.num_params))\n\n    def forward(self, params: torch.Tensor):\n        """"""\n        Performs the actual prediction\n\n        Parameters\n        ----------\n        params : :class:`torch.Tensor`\n            input parameters\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            predicted shape\n\n        """"""\n\n        shape_params = params.index_select(\n            dim=1, index=getattr(self, ""_indices_shape_params"")\n        )\n\n        transformation_params = params.index_select(\n            dim=1, index=getattr(self, ""_indices_homogen_params"")\n        )\n        shapes = self._shape_layer(shape_params)\n        transformed_shapes = self._homogen_trafo(shapes, transformation_params)\n\n        return transformed_shapes\n\n    @property\n    def num_params(self):\n        """"""\n        Property to access these layer\'s number of parameters\n\n        Returns\n        -------\n        int\n            number of parameters\n\n        """"""\n        return self._shape_layer.num_params + self._homogen_trafo.num_params\n'"
shapenet/layer/homogeneous_transform_layer.py,59,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport torch\nimport os\nfrom torch.utils.cpp_extension import load as load_cpp\n\n\nclass HomogeneousTransformationLayer(torch.nn.Module):\n    """"""\n    Wrapper Class to Wrap the Python and C++ API into a combined python API\n\n    """"""\n    def __init__(self, n_dims: int, use_cpp=False):\n        """"""\n\n        Parameters\n        ----------\n        n_dims : int\n            number of dimensions\n        use_cpp : bool\n            whether or not to use C++ implementation\n\n        """"""\n        super().__init__()\n\n        self._n_params = {}\n\n        if n_dims == 2:\n            self._n_params[""scale""] = 1\n            self._n_params[""rotation""] = 1\n            self._n_params[""translation""] = 2\n        elif n_dims == 3:\n            self._n_params[""scale""] = 3\n            self._n_params[""rotation""] = 3\n            self._n_params[""translation""] = 3\n\n        if use_cpp:\n            self._layer = _HomogeneousTransformationLayerCpp(n_dims)\n        else:\n            self._layer = _HomogeneousTransformationLayerPy(n_dims)\n\n        total_params = 0\n        for key, val in self._n_params.items():\n            self.register_buffer(""_indices_%s_params"" % key,\n                                 torch.arange(total_params, total_params + val)\n                                 )\n            total_params += val\n\n    def forward(self, shapes: torch.Tensor, params: torch.Tensor):\n        """"""\n        Actual prediction\n\n        Parameters\n        ----------\n        shapes : :class:`torch.Tensor`\n            shapes before applied global transformation\n        params : :class:`torch.Tensor`\n            parameters specifying the global transformation\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            Transformed shapes\n\n        """"""\n        rotation_params = params.index_select(\n            dim=1, index=getattr(self, ""_indices_rotation_params"")\n        )\n        scale_params = params.index_select(\n            dim=1, index=getattr(self, ""_indices_scale_params"")\n        )\n        translation_params = params.index_select(\n            dim=1, index=getattr(self, ""_indices_translation_params"")\n        )\n\n        return self._layer(shapes, rotation_params, translation_params,\n                           scale_params)\n\n    @property\n    def num_params(self):\n        num_params = 0\n        for key, val in self._n_params.items():\n            num_params += val\n\n        return num_params\n\n\nclass _HomogeneousTransformationLayerCpp(torch.nn.Module):\n    """"""\n    Module to perform homogeneous transformations in 2D and 3D\n    (Implemented in C++)\n\n    """"""\n    def __init__(self, n_dims, verbose=True):\n        """"""\n\n        Parameters\n        ----------\n        n_dims : int\n            number of dimensions\n        verbose : float\n            if True: verbosity during C++ loading\n\n        """"""\n        super().__init__()\n\n        homogen_trafo = torch.zeros(1, n_dims + 1, n_dims + 1)\n        homogen_trafo[:, -1, :-1] = 0.\n        homogen_trafo[:, -1, -1] = 1.\n\n        self.register_buffer(""_trafo_matrix"", homogen_trafo)\n        self._n_dims = n_dims\n\n        self._func = load_cpp(""homogeneous_transform_function"",\n                              sources=[\n                                  os.path.join(\n                                      os.path.split(__file__)[0],\n                                      ""homogeneous_transform_layer.cpp"")],\n                              verbose=verbose)\n\n    def forward(self, shapes: torch.Tensor, rotation_params: torch.Tensor,\n                translation_params: torch.Tensor, scale_params: torch.Tensor):\n        """"""\n        ensembles the homogeneous transformation matrix and applies it to the\n        shape tensor\n\n        Parameters\n        ----------\n        shapes : :class:`torch.Tensor`\n            shapes to transform\n        rotation_params : :class:`torch.Tensor`\n            parameters specifying the rotation (one per DoF)\n        translation_params : :class:`torch.Tensor`\n            parameters specifying the translation (one per dimension)\n        scale_params : :class:`torch.Tensor`\n            parameter specifying the global scaling factor\n            (currently only isotropic scaling supported)\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            the transformed shapes in cartesian coordinates\n\n        """"""\n        transformed_shapes = self._func.forward(shapes,\n                                                getattr(self,\n                                                        ""_trafo_matrix""),\n                                                rotation_params,\n                                                translation_params,\n                                                scale_params\n                                                )\n\n        return transformed_shapes\n\n\nclass _HomogeneousTransformationLayerPy(torch.nn.Module):\n    """"""\n    Module to perform homogeneous transformations in 2D and 3D\n    (Implemented in Python)\n\n    """"""\n\n    def __init__(self, n_dims):\n        """"""\n\n        Parameters\n        ----------\n        n_dims : int\n            number of dimensions\n\n        """"""\n        super().__init__()\n\n        homogen_trafo = torch.zeros(1, n_dims+1, n_dims+1)\n        homogen_trafo[:, -1, :-1] = 0.\n        homogen_trafo[:, -1, -1] = 1.\n\n        self.register_buffer(""_trafo_matrix"", homogen_trafo)\n        self._n_dims = n_dims\n\n    def forward(self, shapes: torch.Tensor, rotation_params: torch.Tensor,\n                translation_params: torch.Tensor, scale_params: torch.Tensor):\n        """"""\n        ensembles the homogeneous transformation matrix and applies it to the\n        shape tensor\n\n        Parameters\n        ----------\n        shapes : :class:`torch.Tensor`\n            shapes to transform\n        rotation_params : :class:`torch.Tensor`\n            parameters specifying the rotation (one per DoF)\n        translation_params : :class:`torch.Tensor`\n            parameters specifying the translation (one per dimension)\n        scale_params : :class:`torch.Tensor`\n            parameter specifying the global scaling factor\n            (currently only isotropic scaling supported)\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            the transformed shapes in cartesian coordinates\n\n        """"""\n\n        assert shapes.size(-1) == self._n_dims, ""Layer for other "" \\\n                                                ""dimensionality specified""\n\n        trafo_matrix = self._ensemble_trafo(rotation_params,\n                                            translation_params, scale_params)\n\n        homogen_shapes = torch.cat([shapes,\n                                    shapes.new_ones(*shapes.size()[:-1], 1)],\n                                   dim=-1)\n\n        transformed_shapes = torch.bmm(homogen_shapes,\n                                       trafo_matrix.permute(0, 2, 1))\n\n        transformed_shapes = transformed_shapes[..., :-1]\n        # transformed_shapes = transformed_shapes[..., :-1] / transformed_shapes[..., -1].unsqueeze(-1)\n        \n        return transformed_shapes\n\n    def _ensemble_trafo(self, rotation_params: torch.Tensor,\n                        translation_params: torch.Tensor,\n                        scale_params: torch.Tensor):\n        """"""\n        ensembles the transformation matrix in 2D and 3D\n\n        Parameters\n        ----------\n        rotation_params : :class:`torch.Tensor`\n            parameters specifying the rotation (one per DoF)\n        translation_params : :class:`torch.Tensor`\n            parameters specifying the translation (one per dimension)\n        scale_params : :class:`torch.Tensor`\n            parameter specifying the global scaling factor\n            (currently only isotropic scaling supported)\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            transformation matrix\n\n        """"""\n\n        rotation_params = rotation_params.view(rotation_params.size()[:2])\n        translation_params = translation_params.view(\n            translation_params.size()[:2])\n        scale_params = scale_params.view(scale_params.size()[:2])\n\n        if self._n_dims == 2:\n            return self._ensemble_2d_matrix(rotation_params,\n                                            translation_params, scale_params)\n        elif self._n_dims == 3:\n            return self._ensemble_3d_matrix(rotation_params,\n                                            translation_params, scale_params)\n        else:\n            raise NotImplementedError(""Implementation for n_dims = %d ""\n                                      ""not available"" % self._n_dims)\n\n    def _ensemble_2d_matrix(self, rotation_params: torch.Tensor,\n                            translation_params: torch.Tensor,\n                            scale_params: torch.Tensor):\n        """"""\n        ensembles the homogeneous transformation matrix for 2D\n\n        Parameters\n        ----------\n        rotation_params : :class:`torch.Tensor`\n            parameters specifying the rotation (one parameter)\n        translation_params : :class:`torch.Tensor`\n            parameters specifying the translation (two parameters)\n        scale_params : :class:`torch.Tensor`\n            parameter specifying the global scaling factor (one parameter)\n            (currently only isotropic scaling supported)\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            2D transformation matrix\n\n        """"""\n\n        homogen_trafo = getattr(self, ""_trafo_matrix"").repeat(\n            scale_params.size(0), 1, 1).clone()\n\n        homogen_trafo[:, 0, 0] = (scale_params *\n                                  rotation_params.cos())[:, 0].clone()\n        # s*sin\\theta\n        homogen_trafo[:, 0, 1] = (scale_params *\n                                  rotation_params.sin())[:, 0].clone()\n        # -s*sin\\theta\n        homogen_trafo[:, 1, 0] = (-scale_params *\n                                  rotation_params.sin())[:, 0].clone()\n        # s*cos\\theta\n        homogen_trafo[:, 1, 1] = (scale_params *\n                                  rotation_params.cos())[:, 0].clone()\n\n        # translation params\n        homogen_trafo[:, :-1, -1] = translation_params.clone()\n\n        return homogen_trafo\n\n    def _ensemble_3d_matrix(self, rotation_params: torch.Tensor,\n                            translation_params: torch.Tensor,\n                            scale_params: torch.Tensor):\n        """"""\n        ensembles the homogeneous transformation matrix for 3D\n\n        Parameters\n        ----------\n        rotation_params : :class:`torch.Tensor`\n            parameters specifying the rotation (three parameters)\n        translation_params : :class:`torch.Tensor`\n            parameters specifying the translation (three parameters)\n        scale_params : :class:`torch.Tensor`\n            parameter specifying the global scaling factor (one parameter)\n            (currently only isotropic scaling supported)\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            3D transformation matrix\n\n        """"""\n\n        homogen_trafo = getattr(self, ""_trafo_matrix"").repeat(\n            scale_params.size(0), 1, 1).clone()\n\n        roll = rotation_params[:, 2].unsqueeze(-1)\n        pitch = rotation_params[:, 1].unsqueeze(-1)\n        yaw = rotation_params[:, 0].unsqueeze(-1)\n\n        # Note that the elements inside the transformation matrix are swapped\n        # due to the zyx convention\n\n        # s*(cos(pitch)*cos(roll))\n        homogen_trafo[:, 0, 0] = (scale_params *\n                                  (pitch.cos() * roll.cos()))[:, 0].clone()\n\n        # s*(cos(pitch)*sin(roll))\n        homogen_trafo[:, 0, 1] = (scale_params *\n                                  (pitch.cos() * roll.sin()))[:, 0].clone()\n\n        # s*(-sin(pitch))\n        homogen_trafo[:, 0, 2] = (scale_params *\n                                  (-pitch.sin()))[:, 0].clone()\n\n        # s*(sin(yaw)*sin(pitch)*cos(roll) - cos(yaw)*sin(roll))\n        homogen_trafo[:, 1, 0] = (scale_params *\n                                  (yaw.sin() * pitch.sin() * roll.cos() -\n                                   yaw.cos() * roll.sin()))[:, 0].clone()\n\n        # s*(sin(yaw)*sin(pitch)*sin(roll) + cos(yaw)*cos(roll))\n        homogen_trafo[:, 1, 1] = (scale_params *\n                                  (yaw.sin() * pitch.sin() * roll.sin() +\n                                   yaw.cos() * roll.cos()))[:, 0].clone()\n\n        # s*(sin(yaw)*cos(pitch))\n        homogen_trafo[:, 1, 2] = (scale_params *\n                                  (yaw.sin() * pitch.cos()))[:, 0].clone()\n\n        # s*(cos(yaw)*sin(pitch)*cos(roll) + sin(yaw)*sin(roll))\n        homogen_trafo[:, 2, 0] = (scale_params *\n                                  (yaw.cos() * pitch.sin() * roll.cos() +\n                                   yaw.sin() * roll.sin()))[:, 0].clone()\n\n        # s*(cos(yaw)*sin(pitch)*sin(roll)-sin(yaw)*cos(roll))\n        homogen_trafo[:, 2, 1] = (scale_params *\n                                  (yaw.cos() * pitch.sin() * roll.sin() -\n                                   yaw.sin() * roll.cos()))[:, 0].clone()\n\n        # s*(cos(yaw)*cos(pitch))\n        homogen_trafo[:, 2, 2] = (scale_params *\n                                  (yaw.cos() * pitch.cos()))[:, 0].clone()\n\n        # translation params\n        homogen_trafo[:, :-1, -1] = translation_params.clone()\n\n        return homogen_trafo\n\n\nif __name__ == \'__main__\':\n    shapes_2d = torch.rand(10, 68, 2)\n    rotation_params_2d = torch.rand(10, 1, 1, 1)\n    # translation_params_2d = torch.rand(10, 2, 1, 1)\n    translation_params_2d = torch.rand(10, 2, 1, 1)\n    scale_params_2d = torch.rand(10, 1, 1, 1)\n\n    print(""Creating Python Layer"")\n    layer_2d_py = _HomogeneousTransformationLayerPy(n_dims=2)\n    print(""Creating Cpp shapelayer"")\n    layer_2d_cpp = _HomogeneousTransformationLayerCpp(n_dims=2)\n\n    result_2d_py = layer_2d_py(shapes_2d, rotation_params_2d, translation_params_2d,\n                               scale_params_2d)\n    result_2d_cpp = layer_2d_cpp(shapes_2d, rotation_params_2d, translation_params_2d,\n                                 scale_params_2d)\n\n    shapes_3d = torch.rand(10, 68, 3)\n    rotation_params_3d = torch.rand(10, 3, 1, 1)\n    # rotation_params_3d = torch.zeros(10, 3, 1, 1)\n    translation_params_3d = torch.rand(10, 3, 1, 1)\n    # translation_params_3d = torch.zeros(10, 3, 1, 1)\n    scale_params_3d = torch.rand(10, 3, 1, 1)\n\n    layer_3d_py = _HomogeneousTransformationLayerPy(n_dims=3)\n    layer_3d_cpp = _HomogeneousTransformationLayerCpp(n_dims=3)\n\n    result_3d_py = layer_3d_py(shapes_3d, rotation_params_3d, translation_params_3d,\n                               scale_params_3d)\n    result_3d_cpp = layer_3d_cpp(shapes_3d, rotation_params_3d, translation_params_3d,\n                                 scale_params_3d)\n\n    print(""Diff 2d: %f"" % (result_2d_py-result_2d_cpp).abs().sum())\n    print(""Diff 3d: %f"" % (result_3d_py-result_3d_cpp).abs().sum())\n\n'"
shapenet/layer/shape_layer.py,19,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport numpy as np\nimport torch\nimport os\nfrom torch.utils.cpp_extension import load as load_cpp\n\n\nclass ShapeLayer(torch.nn.Module):\n    """"""\n    Wrapper to compine Python and C++ Implementation under Single API\n\n    """"""\n    def __init__(self, shapes, use_cpp=False):\n        """"""\n\n        Parameters\n        ----------\n        shapes : np.ndarray\n            the actual shape components\n        use_cpp : bool\n            whether or not to use the (experimental) C++ Implementation\n        """"""\n        super().__init__()\n\n        if use_cpp:\n            self._layer = _ShapeLayerCpp(shapes)\n        else:\n            self._layer = _ShapeLayerPy(shapes)\n\n    def forward(self, shape_params: torch.Tensor):\n        """"""\n        Forwards parameters to Python or C++ Implementation\n\n        Parameters\n        ----------\n        shape_params : :class:`torch.Tensor`\n            parameters for shape ensembling\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            Ensempled Shape\n\n        """"""\n        return self._layer(shape_params)\n\n    @property\n    def num_params(self):\n        """"""\n        Property to access these layer\'s parameters\n\n        Returns\n        -------\n        int\n            number of parameters\n\n        """"""\n        return self._layer.num_params\n\n\nclass _ShapeLayerPy(torch.nn.Module):\n    """"""\n    Python Implementation of Shape Layer\n\n    """"""\n    def __init__(self, shapes):\n        """"""\n\n        Parameters\n        ----------\n        shapes : np.ndarray\n            eigen shapes (obtained by PCA)\n\n        """"""\n        super().__init__()\n\n        self.register_buffer(""_shape_mean"", torch.from_numpy(\n            shapes[0]).float().unsqueeze(0))\n        components = []\n        for i, _shape in enumerate(shapes[1:]):\n            components.append(torch.from_numpy(\n                _shape).float().unsqueeze(0))\n\n        component_tensor = torch.cat(components).unsqueeze(0)\n        self.register_buffer(""_shape_components"", component_tensor)\n\n    def forward(self, shape_params: torch.Tensor):\n        """"""\n        Ensemble shape from parameters\n\n        Parameters\n        ----------\n        shape_params : :class:`torch.Tensor`\n            shape parameters\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            ensembled shape\n\n        """"""\n\n        shapes = getattr(self, ""_shape_mean"").clone()\n        shapes = shapes.expand(shape_params.size(0), *shapes.size()[1:])\n\n        components = getattr(self, ""_shape_components"")\n        components = components.expand(shape_params.size(0),\n                                       *components.size()[1:])\n\n        weighted_components = components.mul(\n            shape_params.expand_as(components))\n\n        shapes = shapes.add(weighted_components.sum(dim=1))\n\n        return shapes\n\n    @property\n    def num_params(self):\n        """"""\n        Property to access these layer\'s parameters\n\n        Returns\n        -------\n        int\n            number of parameters\n\n        """"""\n        return getattr(self, ""_shape_components"").size(1)\n\n\nclass _ShapeLayerCpp(torch.nn.Module):\n    """"""\n    C++ Implementation of Shape Layer\n\n    """"""\n    def __init__(self, shapes, verbose=True):\n        """"""\n\n        Parameters\n        ----------\n        shapes : np.ndarray\n            eigen shapes (obtained by PCA)\n\n        """"""\n        super().__init__()\n\n        self.register_buffer(""_shape_mean"",\n                             torch.from_numpy(shapes[0]).float().unsqueeze(0))\n        components = []\n        for i, _shape in enumerate(shapes[1:]):\n            components.append(torch.from_numpy(_shape).float().unsqueeze(0))\n\n        component_tensor = torch.cat(components).unsqueeze(0)\n        self.register_buffer(""_shape_components"", component_tensor)\n        self._func = load_cpp(""shape_function"",\n                              sources=[os.path.join(os.path.split(__file__)[0],\n                                                    ""shape_layer.cpp"")],\n                              verbose=verbose)\n\n    def forward(self, shape_params: torch.Tensor):\n        """"""\n        Ensemble shape from parameters\n\n        Parameters\n        ----------\n        shape_params : :class:`torch.Tensor`\n            shape parameters\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            ensembled shape\n        """"""\n\n        shapes = self._func.forward(shape_params,\n                                    getattr(self, ""_shape_mean""),\n                                    getattr(self, ""_shape_components""))\n\n        return shapes\n\n    @property\n    def num_params(self):\n        """"""\n        Property to access these layer\'s parameters\n\n        Returns\n        -------\n        int\n            number of parameters\n\n        """"""\n        return getattr(self, ""_shape_components"").size(1)\n\n'"
shapenet/networks/__init__.py,0,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n""""""\nModule to implement Shape Predictions and Prediction-Refinements\n""""""\n__version__ = \'0.1.0\'\n\nfrom .single_shape import SingleShapeNetwork\nfrom .abstract_network import AbstractFeatureExtractor\n'"
shapenet/networks/abstract_network.py,7,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport torch\nfrom abc import abstractmethod\nfrom delira.models import AbstractPyTorchNetwork\nfrom .utils import CustomGroupNorm\n\n\nclass AbstractShapeNetwork(AbstractPyTorchNetwork):\n    """"""\n    Abstract base Class to provide a convenient norm_class_mapping\n\n    """"""\n    def __init__(self, **kwargs):\n        """"""\n\n        Parameters\n        ----------\n        **kwargs :\n            keyword arguments (forwarded to parent class)\n\n        """"""\n        super().__init__(**kwargs)\n\n    @staticmethod\n    def norm_type_to_class(norm_type):\n        """"""\n        helper function to map a string to an actual normalization class\n\n        Parameters\n        ----------\n        norm_type : str\n            string specifying the normalization class\n\n        Returns\n        -------\n        type\n            Normalization Class (subclass of :class:`torch.nn.Module`)\n\n        """"""\n        norm_dict = {\'instance\': torch.nn.InstanceNorm2d,\n                     \'batch\': torch.nn.BatchNorm2d,\n                     \'group\': CustomGroupNorm}\n\n        norm_class = norm_dict.get(norm_type, None)\n\n        return norm_class\n\n\nclass AbstractFeatureExtractor(torch.nn.Module):\n    """"""\n    Abstract Feature Extractor Class all further feature extracotrs\n    should be derived from\n\n    """"""\n    def __init__(self, in_channels, out_params, norm_class, p_dropout=0):\n        """"""\n\n        Parameters\n        ----------\n        in_channels : int\n            number of input channels\n        out_params : int\n            number of outputs\n        norm_class : Any\n            Class implementing a normalization\n        p_dropout : float\n            dropout probability\n\n        """"""\n        super().__init__()\n        self.model = self._build_model(in_channels, out_params, norm_class,\n                                       p_dropout)\n\n    def forward(self, input_batch):\n        """"""\n        Feed batch through network\n\n        Parameters\n        ----------\n        input_batch : :class:`torch.Tensor`\n            batch to feed through network\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            exracted features\n\n        """"""\n        return self.model(input_batch)\n\n    @staticmethod\n    @abstractmethod\n    def _build_model(in_channels, out_features, norm_class, p_dropout):\n        """"""\n        Build the actual model structure\n\n        Parameters\n        ----------\n        in_channels : int\n            number of input channels\n        out_features : int\n            number of outputs\n        norm_class : Any\n            class implementing a normalization\n        p_dropout : float\n            dropout probability\n\n        Returns\n        -------\n        :class:`torch.nn.Module`\n            ensembled model\n        """"""\n        raise NotImplementedError\n'"
shapenet/networks/feature_extractors.py,12,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport torch\nfrom .abstract_network import AbstractFeatureExtractor\n\n\nclass Conv2dRelu(torch.nn.Module):\n    """"""\n    Block holding one Conv2d and one ReLU layer\n    """"""\n    def __init__(self, *args, **kwargs):\n        """"""\n\n        Parameters\n        ----------\n        *args :\n            positional arguments (passed to Conv2d)\n        **kwargs :\n            keyword arguments (passed to Conv2d)\n\n        """"""\n        super().__init__()\n        self._conv = torch.nn.Conv2d(*args, **kwargs)\n        self._relu = torch.nn.ReLU()\n\n    def forward(self, input_batch):\n        """"""\n        Forward batch though layers\n\n        Parameters\n        ----------\n        input_batch : :class:`torch.Tensor`\n            input batch\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            result\n        """"""\n        return self._relu(self._conv(input_batch))\n\n\nclass Img224x224Kernel7x7SeparatedDims(AbstractFeatureExtractor):\n    @staticmethod\n    def _build_model(in_channels, out_params, norm_class, p_dropout):\n        """"""\n        Build the actual model structure\n\n        Parameters\n        ----------\n        in_channels : int\n            number of input channels\n        out_params : int\n            number of outputs\n        norm_class : Any\n            class implementing a normalization\n        p_dropout : float\n            dropout probability\n\n        Returns\n        -------\n        :class:`torch.nn.Module`\n            ensembled model\n\n        """"""\n        model = torch.nn.Sequential()\n\n        model.add_module(""conv_1"", Conv2dRelu(in_channels, 64, (7, 1)))\n        model.add_module(""conv_2"", Conv2dRelu(64, 64, (1, 7)))\n\n        model.add_module(""down_conv_1"", Conv2dRelu(64, 128, (7, 7), stride=2))\n        if norm_class is not None:\n            model.add_module(""norm_1"", norm_class(128))\n        if p_dropout:\n            model.add_module(""dropout_1"", torch.nn.Dropout2d(p_dropout))\n\n        model.add_module(""conv_3"", Conv2dRelu(128, 128, (7, 1)))\n        model.add_module(""conv_4"", Conv2dRelu(128, 128, (1, 7)))\n\n        model.add_module(""down_conv_2"", Conv2dRelu(128, 256, (7, 7), stride=2))\n        if norm_class is not None:\n            model.add_module(""norm_2"", norm_class(256))\n        if p_dropout:\n            model.add_module(""dropout_2"", torch.nn.Dropout2d(p_dropout))\n\n        model.add_module(""conv_5"", Conv2dRelu(256, 256, (5, 1)))\n        model.add_module(""conv_6"", Conv2dRelu(256, 256, (1, 5)))\n\n        model.add_module(""down_conv_3"", Conv2dRelu(256, 256, (5, 5), stride=2))\n        if norm_class is not None:\n            model.add_module(""norm_3"", norm_class(256))\n        if p_dropout:\n            model.add_module(""dropout_3"", torch.nn.Dropout2d(p_dropout))\n\n        model.add_module(""conv_7"", Conv2dRelu(256, 256, (5, 1)))\n        model.add_module(""conv_8"", Conv2dRelu(256, 256, (1, 5)))\n\n        model.add_module(""down_conv_4"", Conv2dRelu(256, 128, (5, 5), stride=2))\n        if norm_class is not None:\n            model.add_module(""norm_4"", norm_class(128))\n        if p_dropout:\n            model.add_module(""dropout_4"", torch.nn.Dropout2d(p_dropout))\n\n        model.add_module(""conv_9"", Conv2dRelu(128, 128, (3, 1)))\n        model.add_module(""conv_10"", Conv2dRelu(128, 128, (1, 3)))\n        model.add_module(""conv_11"", Conv2dRelu(128, 128, (3, 1)))\n        model.add_module(""conv_12"", Conv2dRelu(128, 128, (1, 3)))\n\n        model.add_module(""final_conv"", torch.nn.Conv2d(128, out_params,\n                                                       (2, 2)))\n\n        return model\n'"
shapenet/networks/utils.py,4,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport torch\n\n\nclass CustomGroupNorm(torch.nn.Module):\n    """"""\n    Custom Group Norm which adds n_groups=2 as default parameter\n    """"""\n\n    def __init__(self, n_features, n_groups=2):\n        """"""\n\n        Parameters\n        ----------\n        n_features : int\n            number of input features\n        n_groups : int\n            number of normalization groups\n        """"""\n        super().__init__()\n        self.norm = torch.nn.GroupNorm(n_groups, n_features)\n\n    def forward(self, x):\n        """"""\n        Forward batch through network\n\n        Parameters\n        ----------\n        x : :class:`torch.Tensor`\n            batch to forward\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            normalized results\n\n        """"""\n        return self.norm(x)\n'"
shapenet/scripts/__init__.py,0,b'\n'
shapenet/scripts/export_to_jit.py,4,"b'from ..jit import JitHomogeneousShapeLayer\nfrom ..jit import JitShapeNetwork\nimport torch\nimport argparse\nimport numpy as np\nimport os\n\n\ndef create_jit_net_from_config_and_weight(config_dict, weight_file):\n    """"""\n    Creates a JIT Network from config dict and weight file\n    \n    Parameters\n    ----------\n    config_dict : dict\n        dict containing network configuration\n    weight_file : str\n        path to file containing weights\n    \n    Returns\n    -------\n    :class:`torch.jit.ScriptModule`\n        jitted network\n        \n    """"""\n\n    shapes = np.load(os.path.abspath(\n        config_dict[""layer""].pop(""pca_path""))\n    )[""shapes""][:config_dict[""layer""].pop(""num_shape_params"") + 1]\n\n    net = JitShapeNetwork(JitHomogeneousShapeLayer, {\n        ""shapes"": shapes,\n        ""n_dims"": config_dict[""layer""][""n_dims""],\n        ""use_cpp"": False})\n\n    input_tensor = torch.rand(1, config_dict[""network""][""in_channels""],\n                              config_dict[""data""][""img_size""],\n                              config_dict[""data""][""img_size""]\n                              )\n\n    state = torch.load(weight_file, map_location=""cpu"")\n    try:\n        state = state[""state_dict""][""model""]\n    except KeyError:\n        try:\n            state = state[""model""]\n        except KeyError:\n            pass\n    net.load_state_dict(state)\n    traced = torch.jit.trace(net, (input_tensor,))\n\n    return traced\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-c"", ""--config_file"", type=str, help=""Configuration"")\n    parser.add_argument(""-w"", ""--weight_file"", type=str, help=""Weights"")\n    parser.add_argument(""-o"", ""--out_file"", type=str, help=""Outputfile"")\n\n    args = parser.parse_args()\n\n    import os\n    os.makedirs(os.path.split(args.out_file)[0], exist_ok=True)\n\n    from shapenet.utils import Config\n    traced = create_jit_net_from_config_and_weight(Config()(\n        os.path.abspath(args.config_file)),\n        os.path.abspath(args.weight_file))\n\n    traced.save(os.path.abspath(args.out_file))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
shapenet/scripts/predict_from_net.py,7,"b'\ndef predict():\n    """"""\n    Predicts file directory with network specified by files to output path\n    \n    """"""\n\n    import numpy as np\n    import torch\n    from tqdm import tqdm\n    import os\n    from matplotlib import pyplot as plt\n    from ..utils import Config\n    from ..layer import HomogeneousShapeLayer\n    from ..networks import SingleShapeNetwork\n    from shapedata.single_shape import SingleShapeDataProcessing, \\\n                                        SingleShapeSingleImage2D\n    from shapedata.io import pts_exporter\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-v"", ""--visualize"", action=""store_true"",\n                        help=""If Flag is specified, results will be plotted"")\n    parser.add_argument(""-d"", ""--in_path"", type=str, help=""Input Data Dir"")\n    parser.add_argument(""-s"", ""--out_path"", default=""./outputs"", type=str,\n                        help=""Output Data Dir"")\n    parser.add_argument(""-w"", ""--weight_file"", type=str, help=""Model Weights"")\n    parser.add_argument(""-c"", ""--config_file"", type=str, help=""Configuration"")\n\n    args = parser.parse_args()\n    config = Config()\n    config_dict = config(os.path.abspath(args.config_file))\n\n    try:\n        net = torch.jit.load(os.path.abspath(args.weight_file))\n        net.eval()\n        net.cpu()\n\n    except RuntimeError:\n        net_layer = HomogeneousShapeLayer\n\n        if config_dict[""training""].pop(""mixed_prec"", False):\n            try:\n                from apex import amp\n                amp.init()\n            except:\n                pass\n\n        shapes = np.load(os.path.abspath(config_dict[""layer""].pop(""pca_path""))\n                        )[""shapes""][:config_dict[""layer""].pop(""num_shape_params"") + 1]\n\n        net = SingleShapeNetwork(\n            net_layer, {""shapes"": shapes,\n                        **config_dict[""layer""]},\n            img_size=config_dict[""data""][""img_size""],\n            **config_dict[""network""])\n\n        state = torch.load(os.path.abspath(args.weight_file))\n        try:\n            net.load_state_dict(state[""state_dict""][""model""])\n        except KeyError:\n            try:\n                net.load_state_dict(state[""model""])\n            except KeyError:\n                net.load_state_dict(state)\n\n        net = net.to(""cpu"")\n        net = net.eval()\n\n    data = SingleShapeDataProcessing._get_files(\n        os.path.abspath(args.in_path), extensions=["".png"", "".jpg""])\n\n    def process_sample(sample, img_size, net, device, crop=0.1):\n        lmk_bounds = sample.get_landmark_bounds(sample.lmk)\n        min_y, min_x, max_y, max_x = lmk_bounds\n        range_x = max_x - min_x\n        range_y = max_y - min_y\n\n        center_x = min_x + range_x / 2\n        center_y = min_y + range_y / 2\n\n        max_range = np.floor(max(range_x, range_y) * (1 + crop))\n\n        tmp = sample.crop(center_y - max_range / 2,\n                          center_x - max_range / 2,\n                          center_y + max_range / 2,\n                          center_x + max_range / 2)\n\n        max_range += 1\n\n        crop_range_x = tmp.img.shape[1]\n        crop_range_y = tmp.img.shape[0]\n        colour_channels = tmp.img.shape[2]\n\n        # zero padding\n        if max_range - crop_range_x != 0:\n            img_temp = np.zeros((int(round(crop_range_y)),\n                                 int(round(max_range - crop_range_x)),\n                                 colour_channels))\n\n            tmp.img = np.concatenate((tmp.img, img_temp), axis=1)\n\n        # zero padding\n        if max_range - crop_range_y != 0:\n            img_temp = np.zeros((int(round(max_range - crop_range_y)),\n                                 int(round(max_range)),\n                                 colour_channels))\n\n            tmp.img = np.concatenate((tmp.img, img_temp), axis=0)\n\n        # convert to torch tensor\n        img_tensor = torch.from_numpy(\n            tmp.to_grayscale().resize((img_size, img_size)).img.transpose(2, 0,\n                                                                          1)\n        ).to(torch.float).unsqueeze(0).to(device)\n\n        # obtain prediction\n        pred = net(img_tensor)[\'pred\'][0].cpu().numpy() \n\n        # remap to original image\n        pred = pred * np.array([max_range / img_size, max_range / img_size])\n        img_add_bound_x = max(center_x - max_range / 2, 0)\n        img_add_bound_y = max(center_y - max_range / 2, 0)\n        pred = pred + np.asarray([img_add_bound_y,\n                                  img_add_bound_x])\n        return pred\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    with torch.no_grad():\n\n        if torch.cuda.is_available():\n            net = net.cuda()\n\n        if args.visualize:\n            pred_path = os.path.join(os.path.abspath(args.out_path), ""pred"")\n            vis_path = os.path.join(os.path.abspath(args.out_path),\n                                    ""visualization"")\n            os.makedirs(vis_path, exist_ok=True)\n        else:\n            pred_path = os.path.abspath(args.out_path)\n\n        os.makedirs(pred_path, exist_ok=True)\n\n        for idx, file in enumerate(tqdm(data)):\n\n            _data = SingleShapeSingleImage2D.from_files(file)\n\n            pred = process_sample(_data, img_size=config_dict[""data""][""img_size""], net=net,\n                                  device=device)\n\n            fname = os.path.split(_data.img_file)[-1].rsplit(""."", 1)[0]\n\n            if args.visualize:\n                view_kwargs = {}\n                if _data.is_gray:\n                    view_kwargs[""cmap""] = ""gray""\n                fig = _data.view(True, **view_kwargs)\n                plt.gca().scatter(pred[:, 1], pred[:, 0], s=5, c=""C1"")\n                plt.gca().legend([""GT"", ""Pred""])\n                plt.gcf().savefig(os.path.join(vis_path, fname + "".png""))\n                plt.close()\n\n            _data.save(pred_path, fname, ""PTS"")\n            pts_exporter(pred, os.path.join(pred_path, fname + ""_pred.pts""))\n\nif __name__ == \'__main__\':\n    predict()\n'"
shapenet/scripts/prepare_datasets.py,0,"b'import kaggle\nimport os\nimport zipfile\nfrom shapedata.io import pts_exporter\nimport shutil\nimport pandas as pd\nfrom multiprocessing import Pool\nfrom functools import partial\nfrom sklearn.model_selection import train_test_split\nfrom shapedata import SingleShapeDataProcessing\nimport numpy as np\n\n\ndef _make_pca(data_dir, out_file, normalize_rot=False, rotation_idxs=()):\n    """"""\n    Creates a PCA from data in a given directory\n    \n    Parameters\n    ----------\n    data_dir : str\n        directory containing the image and landmark files\n    out_file : str\n        file the pca will be saved to\n    normalize_rot : bool, optional\n        whether or not to normalize the data\'s rotation\n    rotation_idxs : tuple, optional\n        indices for rotation normalization, msut be specified if \n        ``normalize_rot=True``\n    \n    """"""\n\n    data_dir = os.path.abspath(data_dir)\n    out_file = os.path.abspath(out_file)\n\n    data = SingleShapeDataProcessing.from_dir(data_dir)\n    if normalize_rot:\n        for idx in range(len(data)):\n            data[idx] = data[idx].normalize_rotation(rotation_idxs[0],\n                                                     rotation_idxs[1])\n\n    pca = data.lmk_pca(True, True)\n\n    if out_file.endswith("".npz""):\n        np.savez(out_file, shapes=pca)\n    elif out_file.endswith("".npy""):\n        np.save(out_file, pca)\n    elif out_file.endswith("".txt""):\n        np.savetxt(out_file, pca)\n    else:\n        np.savez(out_file + "".npz"", shapes=pca)\n\n\ndef _process_single_cat_file(file, target_dir):\n    """"""\n    Processes a single file of the cat dataset\n    \n    Parameters\n    ----------\n    file : str\n        the file to process\n    target_dir : str\n        the target directory\n    \n    """"""\n\n    file = os.path.abspath(file)\n    target_dir = os.path.abspath(target_dir)\n\n    pd_frame = pd.read_csv(str(file) + "".cat"", sep=\' \', header=None)\n    landmarks = (pd_frame.as_matrix()[0][1:-1]).reshape((-1, 2))\n    # switch xy\n    landmarks[:, [0, 1]] = landmarks[:, [1, 0]]\n\n    target_file = os.path.join(target_dir, os.path.split(\n        os.path.split(file)[0])[-1] + ""_"" + os.path.split(file)[-1])\n\n    # export landmarks\n    pts_exporter(landmarks, str(target_file.rsplit(""."", 1)[0]) + "".pts"")\n\n    # move image file\n    shutil.move(file, target_file)\n    os.remove(file + "".cat"")\n\n\ndef _prepare_cats(out_dir, remove_zip=False, normalize_pca_rot=False,\n                  **split_options):\n    """"""\n    Prepares the cat dataset (with multiprocessing)\n    \n    Parameters\n    ----------\n    out_dir : str\n        the output directory\n    remove_zip : bool, optional\n        whether or not to remove the ZIP file after finishing the preparation\n    normalize_pca_rot : bool, optional\n        whether or not to normalize the data\'s rotation during PCA\n\n    See Also\n    --------\n    `Cat Dataset <https://www.kaggle.com/crawford/cat-dataset>`_\n    \n    """"""\n\n\n    out_dir = os.path.abspath(out_dir)\n\n    data_path = os.path.join(out_dir, ""Cats"")\n    os.makedirs(data_path, exist_ok=True)\n\n    if not os.path.isfile(os.path.join(data_path, ""cats.zip"")):\n        print(""\\tDownloading Data"")\n        kaggle.api.dataset_download_cli(""crawford/cat-dataset"",\n                                        path=data_path, unzip=True)\n\n    if not (os.path.isdir(os.path.join(data_path, ""train"")) and\n            os.path.isdir(os.path.join(data_path, ""test""))):\n\n        if not os.path.isdir(os.path.join(data_path, ""tmp_data"")):\n            print(""\\tExtracting Data"")\n            with zipfile.ZipFile(os.path.join(data_path, ""cats.zip"")) as zip_ref:\n                zip_ref.extractall(os.path.join(data_path, ""tmp_data""))\n\n        # get all jpeg files\n        sub_dirs = [os.path.join(data_path, ""tmp_data"", x)\n                    for x in os.listdir(os.path.join(data_path, ""tmp_data""))\n                    if os.path.isdir(os.path.join(data_path, ""tmp_data"", x))]\n\n        img_files = []\n        for _dir in sub_dirs:\n            img_files += [os.path.join(_dir, x) for x in os.listdir(_dir)\n                          if x.endswith("".jpg"")]\n\n        train_files, test_files = train_test_split(img_files, **split_options)\n\n        if not (os.path.isdir(os.path.join(data_path, ""train"")) and\n                os.path.isdir(os.path.join(data_path, ""test""))):\n\n            print(""Preprocessing Data"")\n\n            os.makedirs(os.path.join(data_path, ""train""), exist_ok=True)\n            with Pool() as p:\n                p.map(partial(_process_single_cat_file,\n                              target_dir=os.path.join(data_path, ""train"")),\n                      train_files)\n\n            os.makedirs(os.path.join(data_path, ""test""), exist_ok=True)\n            with Pool() as p:\n                p.map(partial(_process_single_cat_file,\n                              target_dir=os.path.join(data_path, ""test"")),\n                      test_files)\n\n        shutil.rmtree(os.path.join(data_path, ""tmp_data""))\n\n    print(""Make PCA"")\n    _make_pca(os.path.join(data_path, ""train""),\n              os.path.join(data_path, ""train_pca.npz""),\n              normalize_rot=normalize_pca_rot, rotation_idxs=(0, 1))\n\n    if remove_zip:\n        os.remove(os.path.join(data_path, ""cats.zip""))\n\n\ndef _prepare_ibug_dset(zip_file, dset_name, out_dir, remove_zip=False,\n                       normalize_pca_rot=True):\n    """"""\n    Prepares an ibug dataset (from a given zipfile)\n    \n    Parameters\n    ----------\n    zip_file : str\n        the zip archive containing the data\n    dset_name : str\n        the dataset\'s name\n    out_dir : str\n        the output directory\n    remove_zip : bool, optional\n        whether or not to remove the ZIP file after finishing the preparation\n    normalize_pca_rot : bool, optional\n        whether or not to normalize the data\'s rotation during PCA\n\n    See Also\n    --------\n    `iBug Datasets <https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/>`_\n    \n    """"""\n\n    zip_file = os.path.abspath(zip_file)\n    out_dir = os.path.abspath(out_dir)\n\n    data_path = os.path.join(out_dir, dset_name)\n    os.makedirs(data_path, exist_ok=True)\n\n    print(""\\tExtracting Data"")\n    with zipfile.ZipFile(zip_file) as zip_ref:\n        zip_ref.extractall(data_path)\n\n    print(""\\tPreprocessing Data"")\n    _make_pca(os.path.join(data_path, ""trainset""),\n              os.path.join(data_path, ""train_pca.npz""),\n              normalize_rot=normalize_pca_rot, rotation_idxs=(37, 46))\n\n    if remove_zip:\n        os.remove(zip_file)\n\n\ndef prepare_lfpw_dset():\n    """"""\n    Prepares the LFPW Dataset from commandline arguments\n\n    See Also\n    --------\n    :meth:`_prepare_ibug_dset`\n    `iBug Datasets <https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/>`_\n    `LFPW Dataset <https://neerajkumar.org/databases/lfpw/>`_\n    \n    """"""\n\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--zip_file"", type=str,\n                        help=""Zipfile containing the lfpw database"")\n\n    parser.add_argument(""-d"", ""--ddir"", type=str,\n                        help=""Target data directory"")\n\n    parser.add_argument(""--normalize_pca_rot"", action=""store_true"",\n                        help=""Whether or not to normalize the pca\'s rotation"")\n\n    parser.add_argument(""--remove_zip"", action=""store_true"",\n                        help=""Zipfiles will be removed after processing data"",\n                        default=False)\n\n    args = parser.parse_args()\n\n    _prepare_ibug_dset(args.zip_file, ""lfpw"", args.ddir, args.remove_zip,\n                       args.normalize_pca_rot)\n\n\ndef prepare_helen_dset():\n    """"""\n    Prepares the HELEN Dataset from commandline arguments\n\n    See Also\n    --------\n    :meth:`_prepare_ibug_dset`\n    `iBug Datasets <https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/>`_\n    `HELEN Dataset <http://www.ifp.illinois.edu/~vuongle2/helen/>`_\n\n    """"""\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--zip_file"", type=str,\n                        help=""Zipfile containing the helen database"")\n\n    parser.add_argument(""-d"", ""--ddir"", type=str,\n                        help=""Target data directory"")\n\n    parser.add_argument(""--normalize_pca_rot"", action=""store_true"",\n                        help=""Whether or not to normalize the pca\'s rotation"")\n\n    parser.add_argument(""--remove_zip"", action=""store_true"",\n                        help=""Zipfiles will be removed after processing data"",\n                        default=False)\n\n    args = parser.parse_args()\n\n    _prepare_ibug_dset(args.zip_file, ""helen"", args.ddir, args.remove_zip,\n                       args.normalize_pca_rot)\n\n\ndef prepare_cat_dset():\n    """"""\n    Prepares the Cat Dataset from commandline arguments\n    \n    See Also\n    --------\n    :meth:`_prepare_cats`\n    `Cat Dataset <https://www.kaggle.com/crawford/cat-dataset>`_\n\n    """"""\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-d"", ""--ddir"", type=str,\n                        help=""Target data directory"")\n    parser.add_argument(""--normalize_pca_rot"", action=""store_true"",\n                        help=""Whether or not to normalize the pca\'s rotation"")\n    parser.add_argument(""--test_size"", type=float, default=0.25,\n                        help=""Testsize for \\\n                            sklearn.model_selection.train_test_split"")\n    parser.add_argument(""--train_size"", type=float, default=None,\n                        help=""Testsize for \\\n                            sklearn.model_selection.train_test_split"")\n    parser.add_argument(""--no_shuffle"", action=""store_true"",\n                        help=""If specified, data will not be shuffled during \\\n                            train_test_split "")\n    parser.add_argument(""--random_state"", type=int, default=None,\n                        help=""random state for \\\n                            sklearn.model_selection.train_test_split "")\n    parser.add_argument(""--remove_zip"", action=""store_true"",\n                        help=""Zipfiles will be removed after processing data"",\n                        default=False)\n    args = parser.parse_args()\n\n    split_options = {\n        ""test_size"": args.test_size,\n        ""train_size"": args.train_size,\n        ""shuffle"": False if args.no_shuffle else True,\n        ""random_state"": args.random_state\n    }\n    _prepare_cats(args.ddir, args.remove_zip, args.normalize_pca_rot,\n                  **split_options)\n\n\ndef prepare_all_data():\n    """"""\n    Prepares all Datasets from commandline arguments\n\n    See Also\n    --------\n    :meth:`_prepare_ibug_dset`\n    :meth:`_prepare_cats`\n    \n    """"""\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--lfpw"", action=""store_true"",\n                        help=""If Flag is set, the lfpw database will be \\\n                            preprocessed; Must specify \'--lzip\' argument "",\n                        default=False)\n    parser.add_argument(""--helen"", action=""store_true"",\n                        help=""If Flag is set, the helen database will be \\\n                            preprocessed; Must specify \'--hzip\' argument "",\n                        default=False)\n    parser.add_argument(""--cats"", action=""store_true"",\n                        help=""If Flag is set, the cat database will be \\\n                        downloaded and preprocessed "",\n                        default=False)\n    parser.add_argument(""--lzip"", type=str, default=None,\n                        help=""Zipfile containing the lfpw database"")\n    parser.add_argument(""--hzip"", type=str, default=None,\n                        help=""Zipfile containing the helen database"")\n\n    parser.add_argument(""-d"", ""--ddir"", type=str,\n                        help=""Target data directory"")\n    parser.add_argument(""--test_size"", type=float, default=0.25,\n                        help=""Testsize for \\\n                        sklearn.model_selection.train_test_split"")\n    parser.add_argument(""--train_size"", type=float, default=None,\n                        help=""Testsize for \\\n                        sklearn.model_selection.train_test_split"")\n    parser.add_argument(""--no_shuffle"", action=""store_true"",\n                        help=""If specified, data will not be shuffled during \\\n                        train_test_split "")\n    parser.add_argument(""--random_state"", type=int, default=None,\n                        help=""random state for \\\n                        sklearn.model_selection.train_test_split "")\n    parser.add_argument(""--remove_zip"", action=""store_true"",\n                        help=""Zipfiles will be removed after processing data"",\n                        default=False)\n\n    parser.add_argument(""--normalize_pca_rot"", action=""store_true"",\n                        help=""Whether or not to normalize the pca\'s rotation"")\n\n    args = parser.parse_args()\n\n    data_dir = args.ddir\n\n    split_options = {\n        ""test_size"": args.test_size,\n        ""train_size"": args.train_size,\n        ""shuffle"": False if args.no_shuffle else True,\n        ""random_state"": args.random_state\n    }\n\n    if args.remove_zip:\n        remove_zip = True\n    else:\n        args.remove_zip = False\n\n    if args.cats:\n        print(""Prepare Cats Dataset"")\n        _prepare_cats(data_dir, remove_zip=remove_zip,\n                      normalize_pca_rot=args.normalize_pca_rot,\n                      **split_options)\n\n    if args.lfpw and args.lzip is not None:\n        print(""Prepare LFPW Dataset"")\n        _prepare_ibug_dset(args.lzip, ""lfpw"", data_dir, remove_zip=remove_zip,\n                           normalize_pca_rot=args.normalize_pca_rot,\n                           )\n\n    if args.helen and args.hzip is not None:\n        print(""Prepare HELEN Dataset"")\n        _prepare_ibug_dset(args.hzip, ""helen"", data_dir, remove_zip=remove_zip,\n                           normalize_pca_rot=args.normalize_pca_rot,\n                           )\n\n    print(""Preprocessed all dataset!"")\n\n\nif __name__ == \'__main__\':\n    prepare_all_data()\n'"
shapenet/scripts/train_single_shapenet.py,3,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\n\ndef train_shapenet():\n    """"""\n    Trains a single shapenet with config file from comandline arguments\n\n    See Also\n    --------\n    :class:`delira.training.PyTorchNetworkTrainer`\n    \n    """"""\n\n    import logging\n    import numpy as np\n    import torch\n    from shapedata.single_shape import SingleShapeDataset\n    from delira.training import PyTorchNetworkTrainer\n    from ..utils import Config\n    from ..layer import HomogeneousShapeLayer\n    from ..networks import SingleShapeNetwork\n    from delira.logging import TrixiHandler\n    from trixi.logger import PytorchVisdomLogger\n    from delira.training.callbacks import ReduceLROnPlateauCallbackPyTorch\n    from delira.data_loading import BaseDataManager, RandomSampler, \\\n        SequentialSampler\n    import os\n    import argparse\n    from sklearn.metrics import mean_squared_error\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-c"", ""--config"", type=str,\n                        help=""Path to configuration file"")\n    parser.add_argument(""-v"", ""--verbose"", action=""store_true"")\n    args = parser.parse_args()\n    config = Config()\n\n    config_dict = config(os.path.abspath(args.config))\n\n    shapes = np.load(os.path.abspath(config_dict[""layer""].pop(""pca_path""))\n                     )[""shapes""][:config_dict[""layer""].pop(""num_shape_params"") + 1]\n\n# layer_cls = HomogeneousShapeLayer\n\n    net = SingleShapeNetwork(\n        HomogeneousShapeLayer, {""shapes"": shapes,\n                                **config_dict[""layer""]},\n        img_size=config_dict[""data""][""img_size""],\n        **config_dict[""network""])\n\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n\n    if args.verbose:\n        print(""Number of Parameters: %d"" % num_params)\n\n    criterions = {""L1"": torch.nn.L1Loss()}\n    metrics = {""MSE"": torch.nn.MSELoss()}\n    \n    def numpy_mse(pred, target):\n        pred = pred.reshape(pred.shape[0], -1)\n        target = target.reshape(target.shape[0], -1)\n        \n        return mean_squared_error(target, pred)\n\n    mixed_prec = config_dict[""training""].pop(""mixed_prec"", False)\n\n    config_dict[""training""][""save_path""] = os.path.abspath(\n        config_dict[""training""][""save_path""])\n\n    trainer = PyTorchNetworkTrainer(\n        net, losses=criterions, train_metrics=metrics,\n        val_metrics={""MSE"": numpy_mse},\n        lr_scheduler_cls=ReduceLROnPlateauCallbackPyTorch,\n        lr_scheduler_params=config_dict[""scheduler""],\n        optimizer_cls=torch.optim.Adam,\n        optimizer_params=config_dict[""optimizer""],\n        mixed_precision=mixed_prec,\n        key_mapping={""input_images"": ""data""},\n        **config_dict[""training""])\n\n    if args.verbose:\n        print(trainer.input_device)\n\n        print(""Load Data"")\n    dset_train = SingleShapeDataset(\n        os.path.abspath(config_dict[""data""][""train_path""]),\n        config_dict[""data""][""img_size""], config_dict[""data""][""crop""],\n        config_dict[""data""][""landmark_extension_train""],\n        cached=config_dict[""data""][""cached""],\n        rotate=config_dict[""data""][""rotate_train""],\n        random_offset=config_dict[""data""][""offset_train""]\n    )\n\n    if config_dict[""data""][""test_path""]:\n        dset_val = SingleShapeDataset(\n            os.path.abspath(config_dict[""data""][""test_path""]),\n            config_dict[""data""][""img_size""], config_dict[""data""][""crop""],\n            config_dict[""data""][""landmark_extension_test""],\n            cached=config_dict[""data""][""cached""],\n            rotate=config_dict[""data""][""rotate_test""],\n            random_offset=config_dict[""data""][""offset_test""]\n        )\n\n    else:\n        dset_val = None\n\n    mgr_train = BaseDataManager(\n        dset_train,\n        batch_size=config_dict[""data""][""batch_size""],\n        n_process_augmentation=config_dict[""data""][""num_workers""],\n        transforms=None,\n        sampler_cls=RandomSampler\n    )\n    mgr_val = BaseDataManager(\n        dset_val,\n        batch_size=config_dict[""data""][""batch_size""],\n        n_process_augmentation=config_dict[""data""][""num_workers""],\n        transforms=None,\n        sampler_cls=SequentialSampler\n    )\n\n    if args.verbose:\n        print(""Data loaded"")\n    if config_dict[""logging""].pop(""enable"", False):\n        logger_cls = PytorchVisdomLogger\n\n        logging.basicConfig(level=logging.INFO,\n                            handlers=[\n                                TrixiHandler(\n                                    logger_cls, **config_dict[""logging""])\n                            ])\n\n    else:\n        logging.basicConfig(level=logging.INFO,\n                            handlers=[logging.NullHandler()])\n\n    logger = logging.getLogger(""Test Logger"")\n    logger.info(""Start Training"")\n\n    if args.verbose:\n        print(""Start Training"")\n\n    trainer.train(config_dict[""training""][""num_epochs""], mgr_train, mgr_val,\n                  config_dict[""training""][""val_score_key""],\n                  val_score_mode=\'lowest\')\n\n\nif __name__ == \'__main__\':\n    from multiprocessing import freeze_support\n    freeze_support()\n    train_shapenet()\n'"
shapenet/utils/__init__.py,0,b'from .load_config_file import Config\nfrom .misc import now'
shapenet/utils/load_config_file.py,0,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport yaml\n\n\nclass Config(object):\n    """"""\n    Implements parser for configuration files\n\n    """"""\n\n    def __init__(self, verbose=False):\n        """"""\n\n        Parameters\n        ----------\n        verbose : bool\n            verbosity\n\n        """"""\n        self.verbose = verbose\n\n    def __call__(self, config_file, config_group=None):\n        """"""\n        Actual parsing\n\n        Parameters\n        ----------\n        config_file : string\n            path to YAML file with configuration\n\n        config_group : string or None\n            group key to return\n            if None: return dict of all keys\n            if string: return only values of specified group\n\n        Returns\n        -------\n        dict\n            configuration dict\n            \n        """"""\n        state_dict = {}\n\n        # open config file\n        with open(config_file, \'r\') as file:\n            docs = yaml.load_all(file)\n\n            # iterate over document\n            for doc in docs:\n\n                # iterate over groups\n                for group, group_dict in doc.items():\n                    for key, vals in group_dict.items():\n\n                        # set attributes with value \'None\' to None\n                        if vals == \'None\':\n                            group_dict[key] = None\n\n                    state_dict[group] = group_dict\n\n                    if self.verbose:\n                        print(""LOADED_CONFIG: \\n%s\\n%s:\\n%s\\n%s"" % (\n                            ""="" * 20, str(group), ""-"" * 20, str(group_dict)))\n\n        if config_group is not None:\n            return state_dict[config_group]\n        else:\n            return state_dict\n'"
shapenet/utils/misc.py,0,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport datetime\n\n\ndef now():\n    """"""Return current time as YYYY-MM-DD_HH-MM-SS\n\n    Returns\n    -------\n    string\n        current time\n    """"""\n\n    return datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')\n'"
tests/jit/__init__.py,0,b''
tests/jit/test_jit_equality.py,2,"b'from shapenet.jit import JitHomogeneousShapeLayer\nfrom shapenet.jit import JitShapeNetwork\n\nimport torch\nimport numpy as np\n\n\ndef test_jit_equality():\n    layer_kwargs = {""shapes"": np.random.rand(26, 68, 2),\n                    ""n_dims"": 2,\n                    ""use_cpp"": False}\n\n    input_tensor = torch.rand(10, 1, 224, 224)\n\n    jit_net = JitShapeNetwork(JitHomogeneousShapeLayer, layer_kwargs)\n\n    assert torch.jit.trace(jit_net, (torch.rand(1, 1, 224, 224)))\n'"
tests/layer/__init__.py,0,b''
tests/layer/test_homogeneous_shape_layer.py,9,"b'from shapenet.layer import HomogeneousShapeLayer\nimport torch\nimport numpy as np\nimport pytest\nimport warnings\n\n\n@pytest.mark.parametrize(""shapes,n_dims,use_cpp,params,target"",\n                         [\n                             (np.zeros((45, 128, 2)), 2, False,\n                              torch.ones(10, 44 + 1 + 1 + 2, 1, 1),\n                              torch.ones(10, 128, 2)),\n                             (np.zeros((45, 128, 2)), 2, True,\n                              torch.ones(10, 44 + 1 + 1 + 2, 1, 1),\n                              torch.ones(10, 128, 2)),\n                             (np.zeros((45, 128, 3)), 3, False,\n                              torch.ones(10, 44 + 3 + 3 + 3, 1, 1),\n                              torch.ones(10, 128, 3)),\n                             (np.zeros((45, 128, 3)), 3, True,\n                              torch.ones(10, 44 + 3 + 3 + 3, 1, 1),\n                              torch.ones(10, 128, 3))\n                         ]\n                         )\ndef test_homogeneous_shape_layer(shapes, n_dims, use_cpp, params, target):\n    layer = HomogeneousShapeLayer(shapes, n_dims, use_cpp)\n    params.requires_grad_(True)\n\n    assert (layer(params.float()) == target.float()).all()\n    try:\n        result = layer(params.float())\n        result.backward(torch.ones_like(result))\n    except:\n        assert False, ""Backward not successful""\n'"
tests/layer/test_homogeneous_transform_layer.py,20,"b'from shapenet.layer.homogeneous_transform_layer import \\\n    HomogeneousTransformationLayer, \\\n    _HomogeneousTransformationLayerCpp, \\\n    _HomogeneousTransformationLayerPy\nimport torch\n\n\ndef test_homogeneous_transform_layer():\n    shapes_2d = torch.rand(10, 68, 2, requires_grad=True)\n    rotation_params_2d = torch.rand(10, 1, 1, 1, requires_grad=True)\n    translation_params_2d = torch.rand(10, 2, 1, 1, requires_grad=True)\n    scale_params_2d = torch.rand(10, 1, 1, 1, requires_grad=True)\n\n    layer_2d_py = _HomogeneousTransformationLayerPy(n_dims=2)\n    layer_2d_cpp = _HomogeneousTransformationLayerCpp(n_dims=2)\n\n    result_2d_py = layer_2d_py(shapes_2d, rotation_params_2d,\n                               translation_params_2d, scale_params_2d)\n    result_2d_cpp = layer_2d_cpp(shapes_2d, rotation_params_2d,\n                                 translation_params_2d, scale_params_2d)\n\n    assert (result_2d_py - result_2d_cpp).abs().sum() < 1e-6\n\n    try:\n        result_2d_py.backward(torch.ones_like(result_2d_py))\n    except:\n        assert False, ""Backward not successful""\n\n    try:\n        result_2d_cpp.backward(torch.ones_like(result_2d_cpp))\n    except:\n        assert False, ""Backward not successful""\n\n    layer_2d_py = HomogeneousTransformationLayer(2, False)\n    layer_2d_cpp = HomogeneousTransformationLayer(2, True)\n\n    result_2d_py = layer_2d_py(shapes_2d,\n                               torch.cat([rotation_params_2d,\n                                          translation_params_2d,\n                                          scale_params_2d], dim=1))\n    result_2d_cpp = layer_2d_cpp(shapes_2d,\n                                 torch.cat([rotation_params_2d,\n                                            translation_params_2d,\n                                            scale_params_2d], dim=1))\n\n    assert (result_2d_py - result_2d_cpp).abs().sum() < 1e-6\n\n    try:\n        result_2d_py.backward(torch.ones_like(result_2d_py))\n    except:\n        assert False, ""Backward not successful""\n\n    try:\n        result_2d_cpp.backward(torch.ones_like(result_2d_cpp))\n    except:\n        assert False, ""Backward not successful""\n\n    shapes_3d = torch.rand(10, 68, 3, requires_grad=True)\n    rotation_params_3d = torch.rand(10, 3, 1, 1, requires_grad=True)\n    translation_params_3d = torch.rand(10, 3, 1, 1, requires_grad=True)\n    scale_params_3d = torch.rand(10, 3, 1, 1, requires_grad=True)\n\n    layer_3d_py = _HomogeneousTransformationLayerPy(n_dims=3)\n    layer_3d_cpp = _HomogeneousTransformationLayerCpp(n_dims=3)\n\n    result_3d_py = layer_3d_py(shapes_3d, rotation_params_3d,\n                               translation_params_3d, scale_params_3d)\n    result_3d_cpp = layer_3d_cpp(shapes_3d, rotation_params_3d,\n                                 translation_params_3d, scale_params_3d)\n\n    assert (result_3d_py - result_3d_cpp).abs().sum() < 1e-6\n\n    try:\n        result_3d_py.backward(torch.ones_like(result_3d_py))\n    except:\n        assert False, ""Backward not successful""\n\n    try:\n        result_3d_cpp.backward(torch.ones_like(result_3d_cpp))\n    except:\n        assert False, ""Backward not successful""\n\n    layer_3d_py = HomogeneousTransformationLayer(3, False)\n    layer_3d_cpp = HomogeneousTransformationLayer(3, True)\n\n    result_3d_py = layer_3d_py(shapes_3d,\n                               torch.cat([rotation_params_3d,\n                                          translation_params_3d,\n                                          scale_params_3d], dim=1))\n    result_3d_cpp = layer_3d_cpp(shapes_3d,\n                                 torch.cat([rotation_params_3d,\n                                            translation_params_3d,\n                                            scale_params_3d], dim=1))\n\n    assert (result_3d_py - result_3d_cpp).abs().sum() < 1e-6\n\n    try:\n        result_3d_py.backward(torch.ones_like(result_3d_py))\n    except:\n        assert False, ""Backward not successful""\n\n    try:\n        result_3d_cpp.backward(torch.ones_like(result_3d_cpp))\n    except:\n        assert False, ""Backward not successful""\n'"
tests/networks/__init__.py,0,b''
tests/networks/test_feature_extractor.py,5,"b'from shapenet.networks.feature_extractors import \\\n    Img224x224Kernel7x7SeparatedDims\nfrom shapenet.networks.utils import CustomGroupNorm\nimport torch\nimport pytest\n\n\n@pytest.mark.parametrize(""num_outputs,num_in_channels,norm_class,p_dropout"",\n                         [\n                             (16, 1, torch.nn.InstanceNorm2d, 0.1),\n                             (16, 1, torch.nn.BatchNorm2d, 0.5),\n                             (16, 1, CustomGroupNorm, 0.5),\n                             (75, 125, torch.nn.InstanceNorm2d, 0.),\n                             (75, 125, torch.nn.BatchNorm2d, 0.),\n                             (75, 125, CustomGroupNorm, 0.)\n                         ])\ndef test_224_img_size_7_kernel_size_separated_dims(num_outputs, num_in_channels,\n                                                   norm_class, p_dropout):\n    net = Img224x224Kernel7x7SeparatedDims(num_in_channels, num_outputs,\n                                           norm_class, p_dropout)\n\n    input_tensor = torch.rand(16, num_in_channels, 224, 224)\n\n    assert net(input_tensor).shape == (16, num_outputs, 1, 1)\n'"
tests/networks/test_single_shape_network.py,5,"b'from shapenet.networks import SingleShapeNetwork\nfrom shapenet.layer import HomogeneousShapeLayer\nimport torch\nimport numpy as np\nimport pytest\nfrom delira.utils.context_managers import DefaultOptimWrapperTorch\n\n\n@pytest.mark.parametrize(""feature_extractor,num_params,in_channels,norm_type,\\\n                        img_size "",\n                         [\n                             (""custom"", 20, 1, ""instance"", 224),\n                             (""custom"", 20, 1, ""batch"", 224),\n                             (""custom"", 20, 1, ""group"", 224),\n                             (""resnet18"", 20, 1, ""instance"", 224),\n                             (""resnet18"", 20, 1, ""batch"", 224),\n                             (""resnet18"", 20, 1, ""group"", 224),\n                             (""vgg11"", 20, 1, ""instance"", 224),\n                             (""vgg11"", 20, 1, ""batch"", 224),\n                             (""vgg11"", 20, 1, ""group"", 224),\n                             (""inception_v3"", 20, 1, ""instance"", 299),\n                             (""inception_v3"", 20, 1, ""batch"", 299),\n                             (""inception_v3"", 20, 1, ""group"", 299),\n                         ])\ndef test_single_shapenet(feature_extractor, num_params, in_channels, norm_type,\n                         img_size):\n\n    layer_cls = HomogeneousShapeLayer\n    layer_kwargs = {\n        ""shapes"": np.random.rand(num_params + 1, 16, 2),\n        ""n_dims"": 2,\n        ""use_cpp"": False\n    }\n    net = SingleShapeNetwork(layer_cls, layer_kwargs, in_channels, norm_type,\n                             img_size, feature_extractor)\n\n    input_tensor = torch.rand(10, in_channels, img_size, img_size)\n\n    result = net(input_tensor)[""pred""]\n\n    assert result.shape == (10, 16, 2)\n\n    net.closure(\n        model=net,\n        data_dict={""data"": input_tensor, ""label"": torch.rand(10, 16, 2)},\n        optimizers={""default"": DefaultOptimWrapperTorch(torch.optim.Adam(\n                    net.parameters()))},\n        criterions={""l1"": torch.nn.L1Loss()},\n        metrics={""mse"": torch.nn.MSELoss()})\n'"
tests/utils/__init__.py,0,b''
tests/utils/test_config.py,0,"b'from shapenet.utils import Config\nimport os\n\ndef test_config():\n    config = Config()(os.path.join(os.path.split(os.path.abspath(__file__))[0],\n                        ""dummy.config""))\n    \n    target_dict = {\n        ""network"":\n        {\n            ""in_channels"": 1,\n            ""norm_type"": \'instance\',\n            ""feature_extractor"": False\n        },\n        ""layer"":\n        {\n            ""pca_path"": ""test123"",\n            ""num_shape_params"": 25,\n            ""n_dims"": 2,\n            ""use_cpp"": False\n        },\n        ""optimizer"":\n        {\n            ""lr"": 0.0001\n        },\n        ""scheduler"":\n        {\n            ""factor"": 0.1,\n            ""patience"": 5,\n            ""cooldown"": 0\n        },\n        ""training"":\n        {\n            ""save_path"": ""test234"",\n            ""gpu_ids"": [0],\n            ""save_freq"": 1,\n            ""num_epochs"": 200,\n            ""val_score_key"": ""val_MSE""\n        },\n        ""data"":\n        {\n            ""train_path"": ""test345"",\n            ""test_path"": ""test456"",\n            ""crop"": 0.1,\n            ""landmark_extension_train"": "".pts"",\n            ""landmark_extension_test"": "".pts"",\n            ""batch_size"": 1,\n            ""cached"": False,\n            ""num_workers"": 1,\n            ""img_size"": 224,\n            ""rotate_train"": 90,\n            ""rotate_test"": 45,\n            ""offset_train"": 30,\n            ""offset_test"": 20\n        },\n        ""logging"":\n        {\n            ""enable"": False,\n            ""port"": 9999,\n            ""name"": ""TEST"",\n            ""server"": ""http://localhost""\n        }\n    }\n\n    assert config == target_dict\n'"
shapenet/networks/single_shape/__init__.py,0,b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\n\nfrom .shape_network import ShapeNetwork as SingleShapeNetwork'
shapenet/networks/single_shape/shape_network.py,19,"b'# author: Justus Schock (justus.schock@rwth-aachen.de)\n\nimport torch\nimport torchvision.models\nimport logging\n\nfrom ..feature_extractors import Img224x224Kernel7x7SeparatedDims\nfrom ..abstract_network import AbstractShapeNetwork\n\nlogger = logging.getLogger(__file__)\n\n\nclass ShapeNetwork(AbstractShapeNetwork):\n    """"""\n    Network to Predict a single shape\n    """"""\n\n    def __init__(self, layer_cls,\n                 layer_kwargs,\n                 in_channels=1,\n                 norm_type=\'instance\',\n                 img_size=224,\n                 feature_extractor=None,\n                 **kwargs\n                 ):\n        """"""\n\n        Parameters\n        ----------\n        layer_cls :\n            Class to instantiate the last layer (usually a shape-constrained\n            or transformation layer)\n        layer_kwargs : dict\n            keyword arguments to create an instance of `layer_cls`\n        in_channels : int\n            number of input channels\n        norm_type : string or None\n            Indicates the type of normalization used in this network;\n            Must be one of [None, \'instance\', \'batch\', \'group\']\n        kwargs :\n            additional keyword arguments\n\n        """"""\n\n        super().__init__(layer_cls=layer_cls,\n                         layer_kwargs=layer_kwargs,\n                         in_channels=in_channels,\n                         norm_type=norm_type,\n                         img_size=img_size,\n                         feature_extractor=feature_extractor,\n                         **kwargs)\n        self._kwargs = kwargs\n\n        self._model = None\n        self._out_layer = layer_cls(**layer_kwargs)\n        self.num_out_params = self._out_layer.num_params\n        self.img_size = img_size\n        norm_class = self.norm_type_to_class(norm_type)\n\n        args = [in_channels, self.num_out_params, norm_class]\n        feature_kwargs = {}\n\n        if img_size == 224:\n            if feature_extractor and hasattr(torchvision.models,\n                                             feature_extractor):\n                feature_extractor_cls = getattr(torchvision.models,\n                                                feature_extractor)\n                args = [False]\n                feature_kwargs = {""num_classes"": self.num_out_params}\n\n            else:\n                feature_extractor_cls = Img224x224Kernel7x7SeparatedDims\n\n        elif img_size == 299 and feature_extractor == ""inception_v3"":\n            feature_extractor_cls = torchvision.models.inception_v3\n            args = [False]\n            feature_kwargs = {""num_classes"": self.num_out_params,\n                              ""aux_logits"": False}\n\n        else:\n            raise ValueError(""No known dimension for image size found"")\n        # self._model = Img224x224Kernel7x7SeparatedDims(\n        #     in_channels, self._out_layer.num_params, norm_class\n        # )\n\n        self._model = feature_extractor_cls(*args, **feature_kwargs)\n\n        if isinstance(self._model, torchvision.models.VGG):\n            self._model.features = torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n                *list(self._model.features.children())[1:]\n            )\n\n        elif isinstance(self._model, torchvision.models.ResNet):\n            self._model.conv1 = torch.nn.Conv2d(in_channels, 64, kernel_size=7,\n                                                stride=2, padding=3,\n                                                bias=False)\n\n        elif isinstance(self._model, torchvision.models.Inception3):\n            self._model.Conv2d_1a_3x3 = \\\n                torchvision.models.inception.BasicConv2d(in_channels, 32,\n                                                         kernel_size=3,\n                                                         stride=2)\n\n        elif isinstance(self._model, torchvision.models.DenseNet):\n            out_channels = list(self._model.features.children()\n                                )[0].out_channels\n            self._model.features = torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, out_channels, kernel_size=7,\n                                stride=2, padding=3, bias=False),\n                *list(self._model.features.children())[1:]\n            )\n\n        elif isinstance(self._model, torchvision.models.SqueezeNet):\n            out_channels = list(self._model.features.children()\n                                )[0].out_channels\n            self._model.features = torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, out_channels, kernel_size=7,\n                                stride=2),\n                *list(self._model.features.children())[1:]\n            )\n\n        elif isinstance(self._model, torchvision.models.AlexNet):\n            out_channels = list(self._model.features.children()\n                                )[0].out_channels\n            self._model.features = torch.nn.Sequential(\n                torch.nn.Conv2d(in_channels, out_channels, kernel_size=11,\n                                stride=4, padding=2),\n                *list(self._model.features.children())[1:]\n            )\n\n    def forward(self, input_images):\n        """"""\n        Forward input batch through network and shape layer\n\n        Parameters\n        ----------\n        input_images : :class:`torch.Tensor`\n            input batch\n\n        Returns\n        -------\n        :class:`torch.Tensor`\n            predicted shapes\n\n        """"""\n\n        features = self._model(input_images)\n\n        return {""pred"": self._out_layer(features.view(input_images.size(0),\n                                             self.num_out_params, 1, 1))}\n\n    @property\n    def model(self):\n        return self._model\n\n    @model.setter\n    def model(self, model: torch.nn.Module):\n        if isinstance(model, torch.nn.Module):\n            self._model = model\n        else:\n            raise AttributeError(""Invalid Model"")\n\n    @staticmethod\n    def closure(model, data_dict: dict,\n                optimizers: dict, criterions={}, metrics={},\n                fold=0, **kwargs):\n        """"""\n        closure method to do a single backpropagation step\n\n        Parameters\n        ----------\n        model : :class:`ShapeNetwork`\n            trainable model\n        data_dict : dict\n            dictionary containing the data\n        optimizers : dict\n            dictionary of optimizers to optimize model\'s parameters\n        criterions : dict\n            dict holding the criterions to calculate errors\n            (gradients from different criterions will be accumulated)\n        metrics : dict\n            dict holding the metrics to calculate\n        fold : int\n            Current Fold in Crossvalidation (default: 0)\n        **kwargs :\n            additional keyword arguments\n\n        Returns\n        -------\n        dict\n            Metric values (with same keys as input dict metrics)\n        dict\n            Loss values (with same keys as input dict criterions)\n        list\n            Arbitrary number of predictions as :class:`torch.Tensor`\n\n        Raises\n        ------\n        AssertionError\n            if optimizers or criterions are empty or the optimizers are not\n            specified\n        """"""\n        \n        if not criterions:\n            criterions = kwargs.pop(\'losses\', {})\n\n        assert (optimizers and criterions) or not optimizers, \\\n            ""Criterion dict cannot be emtpy, if optimizers are passed""\n\n        loss_vals = {}\n        metric_vals = {}\n        total_loss = 0\n\n        # choose suitable context manager:\n        if optimizers:\n            context_man = torch.enable_grad\n\n        else:\n            context_man = torch.no_grad\n\n        with context_man():\n\n            inputs = data_dict.pop(""data"")\n            preds = model(inputs)\n\n            if data_dict:\n\n                for key, crit_fn in criterions.items():\n                    _loss_val = crit_fn(preds[""pred""], *data_dict.values())\n                    loss_vals[key] = _loss_val.detach()\n                    total_loss += _loss_val\n\n                with torch.no_grad():\n                    for key, metric_fn in metrics.items():\n                        metric_vals[key] = metric_fn(\n                            preds[""pred""], *data_dict.values())\n\n        if optimizers:\n            optimizers[\'default\'].zero_grad()\n            total_loss.backward()\n            optimizers[\'default\'].step()\n\n        else:\n\n            # add prefix ""val"" in validation mode\n            eval_loss_vals, eval_metrics_vals = {}, {}\n            for key in loss_vals.keys():\n                eval_loss_vals[""val_"" + str(key)] = loss_vals[key]\n\n            for key in metric_vals:\n                eval_metrics_vals[""val_"" + str(key)] = metric_vals[key]\n\n            loss_vals = eval_loss_vals\n            metric_vals = eval_metrics_vals\n\n        for key, val in {**metric_vals, **loss_vals}.items():\n            logging.info({""value"": {""value"": val.item(), ""name"": key,\n                                    ""env_appendix"": ""_%02d"" % fold\n                                    }})\n            \n        for key, val in metric_vals.items():\n            if isinstance(val, torch.Tensor):\n                metric_vals[key] = val.detach().cpu().numpy()\n                \n        for key, val in loss_vals.items():\n            if isinstance(val, torch.Tensor):\n                loss_vals[key] = val.detach().cpu().numpy()\n\n        return metric_vals, loss_vals, {k: v.detach() \n                                        for k, v in preds.items()}\n'"
