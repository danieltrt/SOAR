file_path,api_count,code
CenterLoss.py,8,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd.function import Function\n\nclass CenterLoss(nn.Module):\n    def __init__(self, num_classes, feat_dim, size_average=True):\n        super(CenterLoss, self).__init__()\n        self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))\n        self.centerlossfunc = CenterlossFunc.apply\n        self.feat_dim = feat_dim\n        self.size_average = size_average\n\n    def forward(self, label, feat):\n        batch_size = feat.size(0)\n        feat = feat.view(batch_size, -1)\n        # To check the dim of centers and features\n        if feat.size(1) != self.feat_dim:\n            raise ValueError(""Center\'s dim: {0} should be equal to input feature\'s \\\n                            dim: {1}"".format(self.feat_dim,feat.size(1)))\n        batch_size_tensor = feat.new_empty(1).fill_(batch_size if self.size_average else 1)\n        loss = self.centerlossfunc(feat, label, self.centers, batch_size_tensor)\n        return loss\n\n\nclass CenterlossFunc(Function):\n    @staticmethod\n    def forward(ctx, feature, label, centers, batch_size):\n        ctx.save_for_backward(feature, label, centers, batch_size)\n        centers_batch = centers.index_select(0, label.long())\n        return (feature - centers_batch).pow(2).sum() / 2.0 / batch_size\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        feature, label, centers, batch_size = ctx.saved_tensors\n        centers_batch = centers.index_select(0, label.long())\n        diff = centers_batch - feature\n        # init every iteration\n        counts = centers.new_ones(centers.size(0))\n        ones = centers.new_ones(label.size(0))\n        grad_centers = centers.new_zeros(centers.size())\n\n        counts = counts.scatter_add_(0, label.long(), ones)\n        grad_centers.scatter_add_(0, label.unsqueeze(1).expand(feature.size()).long(), diff)\n        grad_centers = grad_centers/counts.view(-1, 1)\n        return - grad_output * diff / batch_size, None, grad_centers / batch_size, None\n\n\ndef main(test_cuda=False):\n    print(\'-\'*80)\n    device = torch.device(""cuda"" if test_cuda else ""cpu"")\n    ct = CenterLoss(10,2,size_average=True).to(device)\n    y = torch.Tensor([0,0,2,1]).to(device)\n    feat = torch.zeros(4,2).to(device).requires_grad_()\n    print (list(ct.parameters()))\n    print (ct.centers.grad)\n    out = ct(y,feat)\n    print(out.item())\n    out.backward()\n    print(ct.centers.grad)\n    print(feat.grad)\n\nif __name__ == \'__main__\':\n    torch.manual_seed(999)\n    main(test_cuda=False)\n    if torch.cuda.is_available():\n        main(test_cuda=True)\n'"
MNIST_with_centerloss.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom  torch.utils.data import DataLoader\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom CenterLoss import CenterLoss\nimport matplotlib.pyplot as plt\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n        self.prelu1_1 = nn.PReLU()\n        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n        self.prelu1_2 = nn.PReLU()\n        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n        self.prelu2_1 = nn.PReLU()\n        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n        self.prelu2_2 = nn.PReLU()\n        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n        self.prelu3_1 = nn.PReLU()\n        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=5, padding=2)\n        self.prelu3_2 = nn.PReLU()\n        self.preluip1 = nn.PReLU()\n        self.ip1 = nn.Linear(128*3*3, 2)\n        self.ip2 = nn.Linear(2, 10, bias=False)\n\n    def forward(self, x):\n        x = self.prelu1_1(self.conv1_1(x))\n        x = self.prelu1_2(self.conv1_2(x))\n        x = F.max_pool2d(x,2)\n        x = self.prelu2_1(self.conv2_1(x))\n        x = self.prelu2_2(self.conv2_2(x))\n        x = F.max_pool2d(x,2)\n        x = self.prelu3_1(self.conv3_1(x))\n        x = self.prelu3_2(self.conv3_2(x))\n        x = F.max_pool2d(x,2)\n        x = x.view(-1, 128*3*3)\n        ip1 = self.preluip1(self.ip1(x))\n        ip2 = self.ip2(ip1)\n        return ip1, F.log_softmax(ip2, dim=1)\n\ndef visualize(feat, labels, epoch):\n    plt.ion()\n    c = [\'#ff0000\', \'#ffff00\', \'#00ff00\', \'#00ffff\', \'#0000ff\',\n         \'#ff00ff\', \'#990000\', \'#999900\', \'#009900\', \'#009999\']\n    plt.clf()\n    for i in range(10):\n        plt.plot(feat[labels == i, 0], feat[labels == i, 1], \'.\', c=c[i])\n    plt.legend([\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\'], loc = \'upper right\')\n    plt.xlim(xmin=-8,xmax=8)\n    plt.ylim(ymin=-8,ymax=8)\n    plt.text(-7.8,7.3,""epoch=%d"" % epoch)\n    plt.savefig(\'./images/epoch=%d.jpg\' % epoch)\n    plt.draw()\n    plt.pause(0.001)\n\n\ndef train(epoch):\n    print ""Training... Epoch = %d"" % epoch\n    ip1_loader = []\n    idx_loader = []\n    for i,(data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        ip1, pred = model(data)\n        loss = nllloss(pred, target) + loss_weight * centerloss(target, ip1)\n\n        optimizer4nn.zero_grad()\n        optimzer4center.zero_grad()\n\n        loss.backward()\n\n        optimizer4nn.step()\n        optimzer4center.step()\n\n        ip1_loader.append(ip1)\n        idx_loader.append((target))\n\n    feat = torch.cat(ip1_loader, 0)\n    labels = torch.cat(idx_loader, 0)\n    visualize(feat.data.cpu().numpy(),labels.data.cpu().numpy(),epoch)\n\nuse_cuda = torch.cuda.is_available() and True\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n# Dataset\ntrainset = datasets.MNIST(\'../MNIST\', download=True,train=True, transform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))]))\ntrain_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n\n# Model\nmodel = Net().to(device)\n\n# NLLLoss\nnllloss = nn.NLLLoss().to(device) #CrossEntropyLoss = log_softmax + NLLLoss\n# CenterLoss\nloss_weight = 1\ncenterloss = CenterLoss(10, 2).to(device)\n\n# optimzer4nn\noptimizer4nn = optim.SGD(model.parameters(),lr=0.001,momentum=0.9, weight_decay=0.0005)\nsheduler = lr_scheduler.StepLR(optimizer4nn,20,gamma=0.8)\n\n# optimzer4center\noptimzer4center = optim.SGD(centerloss.parameters(), lr =0.5)\n\nfor epoch in range(100):\n    sheduler.step()\n    # print optimizer4nn.param_groups[0][\'lr\']\n    train(epoch+1)\n\n\n'"
