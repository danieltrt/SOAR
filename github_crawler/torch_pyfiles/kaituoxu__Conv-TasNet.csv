file_path,api_count,code
src/conv_tasnet.py,20,"b'# Created on 2018/12\n# Author: Kaituo XU\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom utils import overlap_and_add\n\nEPS = 1e-8\n\n\nclass ConvTasNet(nn.Module):\n    def __init__(self, N, L, B, H, P, X, R, C, norm_type=""gLN"", causal=False,\n                 mask_nonlinear=\'relu\'):\n        """"""\n        Args:\n            N: Number of filters in autoencoder\n            L: Length of the filters (in samples)\n            B: Number of channels in bottleneck 1 \xc3\x97 1-conv block\n            H: Number of channels in convolutional blocks\n            P: Kernel size in convolutional blocks\n            X: Number of convolutional blocks in each repeat\n            R: Number of repeats\n            C: Number of speakers\n            norm_type: BN, gLN, cLN\n            causal: causal or non-causal\n            mask_nonlinear: use which non-linear function to generate mask\n        """"""\n        super(ConvTasNet, self).__init__()\n        # Hyper-parameter\n        self.N, self.L, self.B, self.H, self.P, self.X, self.R, self.C = N, L, B, H, P, X, R, C\n        self.norm_type = norm_type\n        self.causal = causal\n        self.mask_nonlinear = mask_nonlinear\n        # Components\n        self.encoder = Encoder(L, N)\n        self.separator = TemporalConvNet(N, B, H, P, X, R, C, norm_type, causal, mask_nonlinear)\n        self.decoder = Decoder(N, L)\n        # init\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_normal_(p)\n\n    def forward(self, mixture):\n        """"""\n        Args:\n            mixture: [M, T], M is batch size, T is #samples\n        Returns:\n            est_source: [M, C, T]\n        """"""\n        mixture_w = self.encoder(mixture)\n        est_mask = self.separator(mixture_w)\n        est_source = self.decoder(mixture_w, est_mask)\n\n        # T changed after conv1d in encoder, fix it here\n        T_origin = mixture.size(-1)\n        T_conv = est_source.size(-1)\n        est_source = F.pad(est_source, (0, T_origin - T_conv))\n        return est_source\n\n    @classmethod\n    def load_model(cls, path):\n        # Load to CPU\n        package = torch.load(path, map_location=lambda storage, loc: storage)\n        model = cls.load_model_from_package(package)\n        return model\n\n    @classmethod\n    def load_model_from_package(cls, package):\n        model = cls(package[\'N\'], package[\'L\'], package[\'B\'], package[\'H\'],\n                    package[\'P\'], package[\'X\'], package[\'R\'], package[\'C\'],\n                    norm_type=package[\'norm_type\'], causal=package[\'causal\'],\n                    mask_nonlinear=package[\'mask_nonlinear\'])\n        model.load_state_dict(package[\'state_dict\'])\n        return model\n\n    @staticmethod\n    def serialize(model, optimizer, epoch, tr_loss=None, cv_loss=None):\n        package = {\n            # hyper-parameter\n            \'N\': model.N, \'L\': model.L, \'B\': model.B, \'H\': model.H,\n            \'P\': model.P, \'X\': model.X, \'R\': model.R, \'C\': model.C,\n            \'norm_type\': model.norm_type, \'causal\': model.causal,\n            \'mask_nonlinear\': model.mask_nonlinear,\n            # state\n            \'state_dict\': model.state_dict(),\n            \'optim_dict\': optimizer.state_dict(),\n            \'epoch\': epoch\n        }\n        if tr_loss is not None:\n            package[\'tr_loss\'] = tr_loss\n            package[\'cv_loss\'] = cv_loss\n        return package\n\n\nclass Encoder(nn.Module):\n    """"""Estimation of the nonnegative mixture weight by a 1-D conv layer.\n    """"""\n    def __init__(self, L, N):\n        super(Encoder, self).__init__()\n        # Hyper-parameter\n        self.L, self.N = L, N\n        # Components\n        # 50% overlap\n        self.conv1d_U = nn.Conv1d(1, N, kernel_size=L, stride=L // 2, bias=False)\n\n    def forward(self, mixture):\n        """"""\n        Args:\n            mixture: [M, T], M is batch size, T is #samples\n        Returns:\n            mixture_w: [M, N, K], where K = (T-L)/(L/2)+1 = 2T/L-1\n        """"""\n        mixture = torch.unsqueeze(mixture, 1)  # [M, 1, T]\n        mixture_w = F.relu(self.conv1d_U(mixture))  # [M, N, K]\n        return mixture_w\n\n\nclass Decoder(nn.Module):\n    def __init__(self, N, L):\n        super(Decoder, self).__init__()\n        # Hyper-parameter\n        self.N, self.L = N, L\n        # Components\n        self.basis_signals = nn.Linear(N, L, bias=False)\n\n    def forward(self, mixture_w, est_mask):\n        """"""\n        Args:\n            mixture_w: [M, N, K]\n            est_mask: [M, C, N, K]\n        Returns:\n            est_source: [M, C, T]\n        """"""\n        # D = W * M\n        source_w = torch.unsqueeze(mixture_w, 1) * est_mask  # [M, C, N, K]\n        source_w = torch.transpose(source_w, 2, 3) # [M, C, K, N]\n        # S = DV\n        est_source = self.basis_signals(source_w)  # [M, C, K, L]\n        est_source = overlap_and_add(est_source, self.L//2) # M x C x T\n        return est_source\n\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, N, B, H, P, X, R, C, norm_type=""gLN"", causal=False,\n                 mask_nonlinear=\'relu\'):\n        """"""\n        Args:\n            N: Number of filters in autoencoder\n            B: Number of channels in bottleneck 1 \xc3\x97 1-conv block\n            H: Number of channels in convolutional blocks\n            P: Kernel size in convolutional blocks\n            X: Number of convolutional blocks in each repeat\n            R: Number of repeats\n            C: Number of speakers\n            norm_type: BN, gLN, cLN\n            causal: causal or non-causal\n            mask_nonlinear: use which non-linear function to generate mask\n        """"""\n        super(TemporalConvNet, self).__init__()\n        # Hyper-parameter\n        self.C = C\n        self.mask_nonlinear = mask_nonlinear\n        # Components\n        # [M, N, K] -> [M, N, K]\n        layer_norm = ChannelwiseLayerNorm(N)\n        # [M, N, K] -> [M, B, K]\n        bottleneck_conv1x1 = nn.Conv1d(N, B, 1, bias=False)\n        # [M, B, K] -> [M, B, K]\n        repeats = []\n        for r in range(R):\n            blocks = []\n            for x in range(X):\n                dilation = 2**x\n                padding = (P - 1) * dilation if causal else (P - 1) * dilation // 2\n                blocks += [TemporalBlock(B, H, P, stride=1,\n                                         padding=padding,\n                                         dilation=dilation,\n                                         norm_type=norm_type,\n                                         causal=causal)]\n            repeats += [nn.Sequential(*blocks)]\n        temporal_conv_net = nn.Sequential(*repeats)\n        # [M, B, K] -> [M, C*N, K]\n        mask_conv1x1 = nn.Conv1d(B, C*N, 1, bias=False)\n        # Put together\n        self.network = nn.Sequential(layer_norm,\n                                     bottleneck_conv1x1,\n                                     temporal_conv_net,\n                                     mask_conv1x1)\n\n    def forward(self, mixture_w):\n        """"""\n        Keep this API same with TasNet\n        Args:\n            mixture_w: [M, N, K], M is batch size\n        returns:\n            est_mask: [M, C, N, K]\n        """"""\n        M, N, K = mixture_w.size()\n        score = self.network(mixture_w)  # [M, N, K] -> [M, C*N, K]\n        score = score.view(M, self.C, N, K) # [M, C*N, K] -> [M, C, N, K]\n        if self.mask_nonlinear == \'softmax\':\n            est_mask = F.softmax(score, dim=1)\n        elif self.mask_nonlinear == \'relu\':\n            est_mask = F.relu(score)\n        else:\n            raise ValueError(""Unsupported mask non-linear function"")\n        return est_mask\n\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride, padding, dilation, norm_type=""gLN"", causal=False):\n        super(TemporalBlock, self).__init__()\n        # [M, B, K] -> [M, H, K]\n        conv1x1 = nn.Conv1d(in_channels, out_channels, 1, bias=False)\n        prelu = nn.PReLU()\n        norm = chose_norm(norm_type, out_channels)\n        # [M, H, K] -> [M, B, K]\n        dsconv = DepthwiseSeparableConv(out_channels, in_channels, kernel_size,\n                                        stride, padding, dilation, norm_type,\n                                        causal)\n        # Put together\n        self.net = nn.Sequential(conv1x1, prelu, norm, dsconv)\n\n    def forward(self, x):\n        """"""\n        Args:\n            x: [M, B, K]\n        Returns:\n            [M, B, K]\n        """"""\n        residual = x\n        out = self.net(x)\n        # TODO: when P = 3 here works fine, but when P = 2 maybe need to pad?\n        return out + residual  # look like w/o F.relu is better than w/ F.relu\n        # return F.relu(out + residual)\n\n\nclass DepthwiseSeparableConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride, padding, dilation, norm_type=""gLN"", causal=False):\n        super(DepthwiseSeparableConv, self).__init__()\n        # Use `groups` option to implement depthwise convolution\n        # [M, H, K] -> [M, H, K]\n        depthwise_conv = nn.Conv1d(in_channels, in_channels, kernel_size,\n                                   stride=stride, padding=padding,\n                                   dilation=dilation, groups=in_channels,\n                                   bias=False)\n        if causal:\n            chomp = Chomp1d(padding)\n        prelu = nn.PReLU()\n        norm = chose_norm(norm_type, in_channels)\n        # [M, H, K] -> [M, B, K]\n        pointwise_conv = nn.Conv1d(in_channels, out_channels, 1, bias=False)\n        # Put together\n        if causal:\n            self.net = nn.Sequential(depthwise_conv, chomp, prelu, norm,\n                                     pointwise_conv)\n        else:\n            self.net = nn.Sequential(depthwise_conv, prelu, norm,\n                                     pointwise_conv)\n\n    def forward(self, x):\n        """"""\n        Args:\n            x: [M, H, K]\n        Returns:\n            result: [M, B, K]\n        """"""\n        return self.net(x)\n\n\nclass Chomp1d(nn.Module):\n    """"""To ensure the output length is the same as the input.\n    """"""\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        """"""\n        Args:\n            x: [M, H, Kpad]\n        Returns:\n            [M, H, K]\n        """"""\n        return x[:, :, :-self.chomp_size].contiguous()\n\n\ndef chose_norm(norm_type, channel_size):\n    """"""The input of normlization will be (M, C, K), where M is batch size,\n       C is channel size and K is sequence length.\n    """"""\n    if norm_type == ""gLN"":\n        return GlobalLayerNorm(channel_size)\n    elif norm_type == ""cLN"":\n        return ChannelwiseLayerNorm(channel_size)\n    else: # norm_type == ""BN"":\n        # Given input (M, C, K), nn.BatchNorm1d(C) will accumulate statics\n        # along M and K, so this BN usage is right.\n        return nn.BatchNorm1d(channel_size)\n\n\n# TODO: Use nn.LayerNorm to impl cLN to speed up\nclass ChannelwiseLayerNorm(nn.Module):\n    """"""Channel-wise Layer Normalization (cLN)""""""\n    def __init__(self, channel_size):\n        super(ChannelwiseLayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.Tensor(1, channel_size, 1))  # [1, N, 1]\n        self.beta = nn.Parameter(torch.Tensor(1, channel_size,1 ))  # [1, N, 1]\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.gamma.data.fill_(1)\n        self.beta.data.zero_()\n\n    def forward(self, y):\n        """"""\n        Args:\n            y: [M, N, K], M is batch size, N is channel size, K is length\n        Returns:\n            cLN_y: [M, N, K]\n        """"""\n        mean = torch.mean(y, dim=1, keepdim=True)  # [M, 1, K]\n        var = torch.var(y, dim=1, keepdim=True, unbiased=False)  # [M, 1, K]\n        cLN_y = self.gamma * (y - mean) / torch.pow(var + EPS, 0.5) + self.beta\n        return cLN_y\n\n\nclass GlobalLayerNorm(nn.Module):\n    """"""Global Layer Normalization (gLN)""""""\n    def __init__(self, channel_size):\n        super(GlobalLayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.Tensor(1, channel_size, 1))  # [1, N, 1]\n        self.beta = nn.Parameter(torch.Tensor(1, channel_size,1 ))  # [1, N, 1]\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.gamma.data.fill_(1)\n        self.beta.data.zero_()\n\n    def forward(self, y):\n        """"""\n        Args:\n            y: [M, N, K], M is batch size, N is channel size, K is length\n        Returns:\n            gLN_y: [M, N, K]\n        """"""\n        # TODO: in torch 1.0, torch.mean() support dim list\n        mean = y.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True) #[M, 1, 1]\n        var = (torch.pow(y-mean, 2)).mean(dim=1, keepdim=True).mean(dim=2, keepdim=True)\n        gLN_y = self.gamma * (y - mean) / torch.pow(var + EPS, 0.5) + self.beta\n        return gLN_y\n\n\nif __name__ == ""__main__"":\n    torch.manual_seed(123)\n    M, N, L, T = 2, 3, 4, 12\n    K = 2*T//L-1\n    B, H, P, X, R, C, norm_type, causal = 2, 3, 3, 3, 2, 2, ""gLN"", False\n    mixture = torch.randint(3, (M, T))\n    # test Encoder\n    encoder = Encoder(L, N)\n    encoder.conv1d_U.weight.data = torch.randint(2, encoder.conv1d_U.weight.size())\n    mixture_w = encoder(mixture)\n    print(\'mixture\', mixture)\n    print(\'U\', encoder.conv1d_U.weight)\n    print(\'mixture_w\', mixture_w)\n    print(\'mixture_w size\', mixture_w.size())\n\n    # test TemporalConvNet\n    separator = TemporalConvNet(N, B, H, P, X, R, C, norm_type=norm_type, causal=causal)\n    est_mask = separator(mixture_w)\n    print(\'est_mask\', est_mask)\n\n    # test Decoder\n    decoder = Decoder(N, L)\n    est_mask = torch.randint(2, (B, K, C, N))\n    est_source = decoder(mixture_w, est_mask)\n    print(\'est_source\', est_source)\n\n    # test Conv-TasNet\n    conv_tasnet = ConvTasNet(N, L, B, H, P, X, R, C, norm_type=norm_type)\n    est_source = conv_tasnet(mixture)\n    print(\'est_source\', est_source)\n    print(\'est_source size\', est_source.size())\n\n'"
src/data.py,11,"b'# Created on 2018/12\n# Author: Kaituo XU\n""""""\nLogic:\n1. AudioDataLoader generate a minibatch from AudioDataset, the size of this\n   minibatch is AudioDataLoader\'s batchsize. For now, we always set\n   AudioDataLoader\'s batchsize as 1. The real minibatch size we care about is\n   set in AudioDataset\'s __init__(...). So actually, we generate the\n   information of one minibatch in AudioDataset.\n2. After AudioDataLoader getting one minibatch from AudioDataset,\n   AudioDataLoader calls its collate_fn(batch) to process this minibatch.\n\nInput:\n    Mixtured WJS0 tr, cv and tt path\nOutput:\n    One batch at a time.\n    Each inputs\'s shape is B x T\n    Each targets\'s shape is B x C x T\n""""""\n\nimport json\nimport math\nimport os\n\nimport numpy as np\nimport torch\nimport torch.utils.data as data\n\nimport librosa\n\n\nclass AudioDataset(data.Dataset):\n\n    def __init__(self, json_dir, batch_size, sample_rate=8000, segment=4.0, cv_maxlen=8.0):\n        """"""\n        Args:\n            json_dir: directory including mix.json, s1.json and s2.json\n            segment: duration of audio segment, when set to -1, use full audio\n\n        xxx_infos is a list and each item is a tuple (wav_file, #samples)\n        """"""\n        super(AudioDataset, self).__init__()\n        mix_json = os.path.join(json_dir, \'mix.json\')\n        s1_json = os.path.join(json_dir, \'s1.json\')\n        s2_json = os.path.join(json_dir, \'s2.json\')\n        with open(mix_json, \'r\') as f:\n            mix_infos = json.load(f)\n        with open(s1_json, \'r\') as f:\n            s1_infos = json.load(f)\n        with open(s2_json, \'r\') as f:\n            s2_infos = json.load(f)\n        # sort it by #samples (impl bucket)\n        def sort(infos): return sorted(\n            infos, key=lambda info: int(info[1]), reverse=True)\n        sorted_mix_infos = sort(mix_infos)\n        sorted_s1_infos = sort(s1_infos)\n        sorted_s2_infos = sort(s2_infos)\n        if segment >= 0.0:\n            # segment length and count dropped utts\n            segment_len = int(segment * sample_rate)  # 4s * 8000/s = 32000 samples\n            drop_utt, drop_len = 0, 0\n            for _, sample in sorted_mix_infos:\n                if sample < segment_len:\n                    drop_utt += 1\n                    drop_len += sample\n            print(""Drop {} utts({:.2f} h) which is short than {} samples"".format(\n                drop_utt, drop_len/sample_rate/36000, segment_len))\n            # generate minibach infomations\n            minibatch = []\n            start = 0\n            while True:\n                num_segments = 0\n                end = start\n                part_mix, part_s1, part_s2 = [], [], []\n                while num_segments < batch_size and end < len(sorted_mix_infos):\n                    utt_len = int(sorted_mix_infos[end][1])\n                    if utt_len >= segment_len:  # skip too short utt\n                        num_segments += math.ceil(utt_len / segment_len)\n                        # Ensure num_segments is less than batch_size\n                        if num_segments > batch_size:\n                            # if num_segments of 1st audio > batch_size, skip it\n                            if start == end: end += 1\n                            break\n                        part_mix.append(sorted_mix_infos[end])\n                        part_s1.append(sorted_s1_infos[end])\n                        part_s2.append(sorted_s2_infos[end])\n                    end += 1\n                if len(part_mix) > 0:\n                    minibatch.append([part_mix, part_s1, part_s2,\n                                      sample_rate, segment_len])\n                if end == len(sorted_mix_infos):\n                    break\n                start = end\n            self.minibatch = minibatch\n        else:  # Load full utterance but not segment\n            # generate minibach infomations\n            minibatch = []\n            start = 0\n            while True:\n                end = min(len(sorted_mix_infos), start + batch_size)\n                # Skip long audio to avoid out-of-memory issue\n                if int(sorted_mix_infos[start][1]) > cv_maxlen * sample_rate:\n                    start = end\n                    continue\n                minibatch.append([sorted_mix_infos[start:end],\n                                  sorted_s1_infos[start:end],\n                                  sorted_s2_infos[start:end],\n                                  sample_rate, segment])\n                if end == len(sorted_mix_infos):\n                    break\n                start = end\n            self.minibatch = minibatch\n\n    def __getitem__(self, index):\n        return self.minibatch[index]\n\n    def __len__(self):\n        return len(self.minibatch)\n\n\nclass AudioDataLoader(data.DataLoader):\n    """"""\n    NOTE: just use batchsize=1 here, so drop_last=True makes no sense here.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(AudioDataLoader, self).__init__(*args, **kwargs)\n        self.collate_fn = _collate_fn\n\n\ndef _collate_fn(batch):\n    """"""\n    Args:\n        batch: list, len(batch) = 1. See AudioDataset.__getitem__()\n    Returns:\n        mixtures_pad: B x T, torch.Tensor\n        ilens : B, torch.Tentor\n        sources_pad: B x C x T, torch.Tensor\n    """"""\n    # batch should be located in list\n    assert len(batch) == 1\n    mixtures, sources = load_mixtures_and_sources(batch[0])\n\n    # get batch of lengths of input sequences\n    ilens = np.array([mix.shape[0] for mix in mixtures])\n\n    # perform padding and convert to tensor\n    pad_value = 0\n    mixtures_pad = pad_list([torch.from_numpy(mix).float()\n                             for mix in mixtures], pad_value)\n    ilens = torch.from_numpy(ilens)\n    sources_pad = pad_list([torch.from_numpy(s).float()\n                            for s in sources], pad_value)\n    # N x T x C -> N x C x T\n    sources_pad = sources_pad.permute((0, 2, 1)).contiguous()\n    return mixtures_pad, ilens, sources_pad\n\n\n# Eval data part\nfrom preprocess import preprocess_one_dir\n\nclass EvalDataset(data.Dataset):\n\n    def __init__(self, mix_dir, mix_json, batch_size, sample_rate=8000):\n        """"""\n        Args:\n            mix_dir: directory including mixture wav files\n            mix_json: json file including mixture wav files\n        """"""\n        super(EvalDataset, self).__init__()\n        assert mix_dir != None or mix_json != None\n        if mix_dir is not None:\n            # Generate mix.json given mix_dir\n            preprocess_one_dir(mix_dir, mix_dir, \'mix\',\n                               sample_rate=sample_rate)\n            mix_json = os.path.join(mix_dir, \'mix.json\')\n        with open(mix_json, \'r\') as f:\n            mix_infos = json.load(f)\n        # sort it by #samples (impl bucket)\n        def sort(infos): return sorted(\n            infos, key=lambda info: int(info[1]), reverse=True)\n        sorted_mix_infos = sort(mix_infos)\n        # generate minibach infomations\n        minibatch = []\n        start = 0\n        while True:\n            end = min(len(sorted_mix_infos), start + batch_size)\n            minibatch.append([sorted_mix_infos[start:end],\n                              sample_rate])\n            if end == len(sorted_mix_infos):\n                break\n            start = end\n        self.minibatch = minibatch\n\n    def __getitem__(self, index):\n        return self.minibatch[index]\n\n    def __len__(self):\n        return len(self.minibatch)\n\n\nclass EvalDataLoader(data.DataLoader):\n    """"""\n    NOTE: just use batchsize=1 here, so drop_last=True makes no sense here.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(EvalDataLoader, self).__init__(*args, **kwargs)\n        self.collate_fn = _collate_fn_eval\n\n\ndef _collate_fn_eval(batch):\n    """"""\n    Args:\n        batch: list, len(batch) = 1. See AudioDataset.__getitem__()\n    Returns:\n        mixtures_pad: B x T, torch.Tensor\n        ilens : B, torch.Tentor\n        filenames: a list contain B strings\n    """"""\n    # batch should be located in list\n    assert len(batch) == 1\n    mixtures, filenames = load_mixtures(batch[0])\n\n    # get batch of lengths of input sequences\n    ilens = np.array([mix.shape[0] for mix in mixtures])\n\n    # perform padding and convert to tensor\n    pad_value = 0\n    mixtures_pad = pad_list([torch.from_numpy(mix).float()\n                             for mix in mixtures], pad_value)\n    ilens = torch.from_numpy(ilens)\n    return mixtures_pad, ilens, filenames\n\n\n# ------------------------------ utils ------------------------------------\ndef load_mixtures_and_sources(batch):\n    """"""\n    Each info include wav path and wav duration.\n    Returns:\n        mixtures: a list containing B items, each item is T np.ndarray\n        sources: a list containing B items, each item is T x C np.ndarray\n        T varies from item to item.\n    """"""\n    mixtures, sources = [], []\n    mix_infos, s1_infos, s2_infos, sample_rate, segment_len = batch\n    # for each utterance\n    for mix_info, s1_info, s2_info in zip(mix_infos, s1_infos, s2_infos):\n        mix_path = mix_info[0]\n        s1_path = s1_info[0]\n        s2_path = s2_info[0]\n        assert mix_info[1] == s1_info[1] and s1_info[1] == s2_info[1]\n        # read wav file\n        mix, _ = librosa.load(mix_path, sr=sample_rate)\n        s1, _ = librosa.load(s1_path, sr=sample_rate)\n        s2, _ = librosa.load(s2_path, sr=sample_rate)\n        # merge s1 and s2\n        s = np.dstack((s1, s2))[0]  # T x C, C = 2\n        utt_len = mix.shape[-1]\n        if segment_len >= 0:\n            # segment\n            for i in range(0, utt_len - segment_len + 1, segment_len):\n                mixtures.append(mix[i:i+segment_len])\n                sources.append(s[i:i+segment_len])\n            if utt_len % segment_len != 0:\n                mixtures.append(mix[-segment_len:])\n                sources.append(s[-segment_len:])\n        else:  # full utterance\n            mixtures.append(mix)\n            sources.append(s)\n    return mixtures, sources\n\n\ndef load_mixtures(batch):\n    """"""\n    Returns:\n        mixtures: a list containing B items, each item is T np.ndarray\n        filenames: a list containing B strings\n        T varies from item to item.\n    """"""\n    mixtures, filenames = [], []\n    mix_infos, sample_rate = batch\n    # for each utterance\n    for mix_info in mix_infos:\n        mix_path = mix_info[0]\n        # read wav file\n        mix, _ = librosa.load(mix_path, sr=sample_rate)\n        mixtures.append(mix)\n        filenames.append(mix_path)\n    return mixtures, filenames\n\n\ndef pad_list(xs, pad_value):\n    n_batch = len(xs)\n    max_len = max(x.size(0) for x in xs)\n    pad = xs[0].new(n_batch, max_len, * xs[0].size()[1:]).fill_(pad_value)\n    for i in range(n_batch):\n        pad[i, :xs[i].size(0)] = xs[i]\n    return pad\n\n\nif __name__ == ""__main__"":\n    import sys\n    json_dir, batch_size = sys.argv[1:3]\n    dataset = AudioDataset(json_dir, int(batch_size))\n    data_loader = AudioDataLoader(dataset, batch_size=1,\n                                  num_workers=4)\n    for i, batch in enumerate(data_loader):\n        mixtures, lens, sources = batch\n        print(i)\n        print(mixtures.size())\n        print(sources.size())\n        print(lens)\n        if i < 10:\n            print(mixtures)\n            print(sources)\n'"
src/evaluate.py,1,"b'#!/usr/bin/env python\n\n# Created on 2018/12\n# Author: Kaituo XU\n\nimport argparse\nimport os\n\nimport librosa\nfrom mir_eval.separation import bss_eval_sources\nimport numpy as np\nimport torch\n\nfrom data import AudioDataLoader, AudioDataset\nfrom pit_criterion import cal_loss\nfrom conv_tasnet import ConvTasNet\nfrom utils import remove_pad\n\n\nparser = argparse.ArgumentParser(\'Evaluate separation performance using Conv-TasNet\')\nparser.add_argument(\'--model_path\', type=str, required=True,\n                    help=\'Path to model file created by training\')\nparser.add_argument(\'--data_dir\', type=str, required=True,\n                    help=\'directory including mix.json, s1.json and s2.json\')\nparser.add_argument(\'--cal_sdr\', type=int, default=0,\n                    help=\'Whether calculate SDR, add this option because calculation of SDR is very slow\')\nparser.add_argument(\'--use_cuda\', type=int, default=0,\n                    help=\'Whether use GPU\')\nparser.add_argument(\'--sample_rate\', default=8000, type=int,\n                    help=\'Sample rate\')\nparser.add_argument(\'--batch_size\', default=1, type=int,\n                    help=\'Batch size\')\n\n\ndef evaluate(args):\n    total_SISNRi = 0\n    total_SDRi = 0\n    total_cnt = 0\n\n    # Load model\n    model = ConvTasNet.load_model(args.model_path)\n    print(model)\n    model.eval()\n    if args.use_cuda:\n        model.cuda()\n\n    # Load data\n    dataset = AudioDataset(args.data_dir, args.batch_size,\n                           sample_rate=args.sample_rate, segment=-1)\n    data_loader = AudioDataLoader(dataset, batch_size=1, num_workers=2)\n\n    with torch.no_grad():\n        for i, (data) in enumerate(data_loader):\n            # Get batch data\n            padded_mixture, mixture_lengths, padded_source = data\n            if args.use_cuda:\n                padded_mixture = padded_mixture.cuda()\n                mixture_lengths = mixture_lengths.cuda()\n                padded_source = padded_source.cuda()\n            # Forward\n            estimate_source = model(padded_mixture)  # [B, C, T]\n            loss, max_snr, estimate_source, reorder_estimate_source = \\\n                cal_loss(padded_source, estimate_source, mixture_lengths)\n            # Remove padding and flat\n            mixture = remove_pad(padded_mixture, mixture_lengths)\n            source = remove_pad(padded_source, mixture_lengths)\n            # NOTE: use reorder estimate source\n            estimate_source = remove_pad(reorder_estimate_source,\n                                         mixture_lengths)\n            # for each utterance\n            for mix, src_ref, src_est in zip(mixture, source, estimate_source):\n                print(""Utt"", total_cnt + 1)\n                # Compute SDRi\n                if args.cal_sdr:\n                    avg_SDRi = cal_SDRi(src_ref, src_est, mix)\n                    total_SDRi += avg_SDRi\n                    print(""\\tSDRi={0:.2f}"".format(avg_SDRi))\n                # Compute SI-SNRi\n                avg_SISNRi = cal_SISNRi(src_ref, src_est, mix)\n                print(""\\tSI-SNRi={0:.2f}"".format(avg_SISNRi))\n                total_SISNRi += avg_SISNRi\n                total_cnt += 1\n    if args.cal_sdr:\n        print(""Average SDR improvement: {0:.2f}"".format(total_SDRi / total_cnt))\n    print(""Average SISNR improvement: {0:.2f}"".format(total_SISNRi / total_cnt))\n\n\ndef cal_SDRi(src_ref, src_est, mix):\n    """"""Calculate Source-to-Distortion Ratio improvement (SDRi).\n    NOTE: bss_eval_sources is very very slow.\n    Args:\n        src_ref: numpy.ndarray, [C, T]\n        src_est: numpy.ndarray, [C, T], reordered by best PIT permutation\n        mix: numpy.ndarray, [T]\n    Returns:\n        average_SDRi\n    """"""\n    src_anchor = np.stack([mix, mix], axis=0)\n    sdr, sir, sar, popt = bss_eval_sources(src_ref, src_est)\n    sdr0, sir0, sar0, popt0 = bss_eval_sources(src_ref, src_anchor)\n    avg_SDRi = ((sdr[0]-sdr0[0]) + (sdr[1]-sdr0[1])) / 2\n    # print(""SDRi1: {0:.2f}, SDRi2: {1:.2f}"".format(sdr[0]-sdr0[0], sdr[1]-sdr0[1]))\n    return avg_SDRi\n\n\ndef cal_SISNRi(src_ref, src_est, mix):\n    """"""Calculate Scale-Invariant Source-to-Noise Ratio improvement (SI-SNRi)\n    Args:\n        src_ref: numpy.ndarray, [C, T]\n        src_est: numpy.ndarray, [C, T], reordered by best PIT permutation\n        mix: numpy.ndarray, [T]\n    Returns:\n        average_SISNRi\n    """"""\n    sisnr1 = cal_SISNR(src_ref[0], src_est[0])\n    sisnr2 = cal_SISNR(src_ref[1], src_est[1])\n    sisnr1b = cal_SISNR(src_ref[0], mix)\n    sisnr2b = cal_SISNR(src_ref[1], mix)\n    # print(""SISNR base1 {0:.2f} SISNR base2 {1:.2f}, avg {2:.2f}"".format(\n    #     sisnr1b, sisnr2b, (sisnr1b+sisnr2b)/2))\n    # print(""SISNRi1: {0:.2f}, SISNRi2: {1:.2f}"".format(sisnr1, sisnr2))\n    avg_SISNRi = ((sisnr1 - sisnr1b) + (sisnr2 - sisnr2b)) / 2\n    return avg_SISNRi\n\n\ndef cal_SISNR(ref_sig, out_sig, eps=1e-8):\n    """"""Calcuate Scale-Invariant Source-to-Noise Ratio (SI-SNR)\n    Args:\n        ref_sig: numpy.ndarray, [T]\n        out_sig: numpy.ndarray, [T]\n    Returns:\n        SISNR\n    """"""\n    assert len(ref_sig) == len(out_sig)\n    ref_sig = ref_sig - np.mean(ref_sig)\n    out_sig = out_sig - np.mean(out_sig)\n    ref_energy = np.sum(ref_sig ** 2) + eps\n    proj = np.sum(ref_sig * out_sig) * ref_sig / ref_energy\n    noise = out_sig - proj\n    ratio = np.sum(proj ** 2) / (np.sum(noise ** 2) + eps)\n    sisnr = 10 * np.log(ratio + eps) / np.log(10.0)\n    return sisnr\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    print(args)\n    evaluate(args)\n'"
src/pit_criterion.py,23,"b'# Created on 2018/12\n# Author: Kaituo XU\n\nfrom itertools import permutations\n\nimport torch\nimport torch.nn.functional as F\n\nEPS = 1e-8\n\n\ndef cal_loss(source, estimate_source, source_lengths):\n    """"""\n    Args:\n        source: [B, C, T], B is batch size\n        estimate_source: [B, C, T]\n        source_lengths: [B]\n    """"""\n    max_snr, perms, max_snr_idx = cal_si_snr_with_pit(source,\n                                                      estimate_source,\n                                                      source_lengths)\n    loss = 0 - torch.mean(max_snr)\n    reorder_estimate_source = reorder_source(estimate_source, perms, max_snr_idx)\n    return loss, max_snr, estimate_source, reorder_estimate_source\n\n\ndef cal_si_snr_with_pit(source, estimate_source, source_lengths):\n    """"""Calculate SI-SNR with PIT training.\n    Args:\n        source: [B, C, T], B is batch size\n        estimate_source: [B, C, T]\n        source_lengths: [B], each item is between [0, T]\n    """"""\n    assert source.size() == estimate_source.size()\n    B, C, T = source.size()\n    # mask padding position along T\n    mask = get_mask(source, source_lengths)\n    estimate_source *= mask\n\n    # Step 1. Zero-mean norm\n    num_samples = source_lengths.view(-1, 1, 1).float()  # [B, 1, 1]\n    mean_target = torch.sum(source, dim=2, keepdim=True) / num_samples\n    mean_estimate = torch.sum(estimate_source, dim=2, keepdim=True) / num_samples\n    zero_mean_target = source - mean_target\n    zero_mean_estimate = estimate_source - mean_estimate\n    # mask padding position along T\n    zero_mean_target *= mask\n    zero_mean_estimate *= mask\n\n    # Step 2. SI-SNR with PIT\n    # reshape to use broadcast\n    s_target = torch.unsqueeze(zero_mean_target, dim=1)  # [B, 1, C, T]\n    s_estimate = torch.unsqueeze(zero_mean_estimate, dim=2)  # [B, C, 1, T]\n    # s_target = <s\', s>s / ||s||^2\n    pair_wise_dot = torch.sum(s_estimate * s_target, dim=3, keepdim=True)  # [B, C, C, 1]\n    s_target_energy = torch.sum(s_target ** 2, dim=3, keepdim=True) + EPS  # [B, 1, C, 1]\n    pair_wise_proj = pair_wise_dot * s_target / s_target_energy  # [B, C, C, T]\n    # e_noise = s\' - s_target\n    e_noise = s_estimate - pair_wise_proj  # [B, C, C, T]\n    # SI-SNR = 10 * log_10(||s_target||^2 / ||e_noise||^2)\n    pair_wise_si_snr = torch.sum(pair_wise_proj ** 2, dim=3) / (torch.sum(e_noise ** 2, dim=3) + EPS)\n    pair_wise_si_snr = 10 * torch.log10(pair_wise_si_snr + EPS)  # [B, C, C]\n\n    # Get max_snr of each utterance\n    # permutations, [C!, C]\n    perms = source.new_tensor(list(permutations(range(C))), dtype=torch.long)\n    # one-hot, [C!, C, C]\n    index = torch.unsqueeze(perms, 2)\n    perms_one_hot = source.new_zeros((*perms.size(), C)).scatter_(2, index, 1)\n    # [B, C!] <- [B, C, C] einsum [C!, C, C], SI-SNR sum of each permutation\n    snr_set = torch.einsum(\'bij,pij->bp\', [pair_wise_si_snr, perms_one_hot])\n    max_snr_idx = torch.argmax(snr_set, dim=1)  # [B]\n    # max_snr = torch.gather(snr_set, 1, max_snr_idx.view(-1, 1))  # [B, 1]\n    max_snr, _ = torch.max(snr_set, dim=1, keepdim=True)\n    max_snr /= C\n    return max_snr, perms, max_snr_idx\n\n\ndef reorder_source(source, perms, max_snr_idx):\n    """"""\n    Args:\n        source: [B, C, T]\n        perms: [C!, C], permutations\n        max_snr_idx: [B], each item is between [0, C!)\n    Returns:\n        reorder_source: [B, C, T]\n    """"""\n    B, C, *_ = source.size()\n    # [B, C], permutation whose SI-SNR is max of each utterance\n    # for each utterance, reorder estimate source according this permutation\n    max_snr_perm = torch.index_select(perms, dim=0, index=max_snr_idx)\n    # print(\'max_snr_perm\', max_snr_perm)\n    # maybe use torch.gather()/index_select()/scatter() to impl this?\n    reorder_source = torch.zeros_like(source)\n    for b in range(B):\n        for c in range(C):\n            reorder_source[b, c] = source[b, max_snr_perm[b][c]]\n    return reorder_source\n\n\ndef get_mask(source, source_lengths):\n    """"""\n    Args:\n        source: [B, C, T]\n        source_lengths: [B]\n    Returns:\n        mask: [B, 1, T]\n    """"""\n    B, _, T = source.size()\n    mask = source.new_ones((B, 1, T))\n    for i in range(B):\n        mask[i, :, source_lengths[i]:] = 0\n    return mask\n\n\nif __name__ == ""__main__"":\n    torch.manual_seed(123)\n    B, C, T = 2, 3, 12\n    # fake data\n    source = torch.randint(4, (B, C, T))\n    estimate_source = torch.randint(4, (B, C, T))\n    source[1, :, -3:] = 0\n    estimate_source[1, :, -3:] = 0\n    source_lengths = torch.LongTensor([T, T-3])\n    print(\'source\', source)\n    print(\'estimate_source\', estimate_source)\n    print(\'source_lengths\', source_lengths)\n    \n    loss, max_snr, estimate_source, reorder_estimate_source = cal_loss(source, estimate_source, source_lengths)\n    print(\'loss\', loss)\n    print(\'max_snr\', max_snr)\n    print(\'reorder_estimate_source\', reorder_estimate_source)\n'"
src/preprocess.py,0,"b'#!/usr/bin/env python\n# Created on 2018/12\n# Author: Kaituo XU\n\nimport argparse\nimport json\nimport os\n\nimport librosa\n\n\ndef preprocess_one_dir(in_dir, out_dir, out_filename, sample_rate=8000):\n    file_infos = []\n    in_dir = os.path.abspath(in_dir)\n    wav_list = os.listdir(in_dir)\n    for wav_file in wav_list:\n        if not wav_file.endswith(\'.wav\'):\n            continue\n        wav_path = os.path.join(in_dir, wav_file)\n        samples, _ = librosa.load(wav_path, sr=sample_rate)\n        file_infos.append((wav_path, len(samples)))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    with open(os.path.join(out_dir, out_filename + \'.json\'), \'w\') as f:\n        json.dump(file_infos, f, indent=4)\n\n\ndef preprocess(args):\n    for data_type in [\'tr\', \'cv\', \'tt\']:\n        for speaker in [\'mix\', \'s1\', \'s2\']:\n            preprocess_one_dir(os.path.join(args.in_dir, data_type, speaker),\n                               os.path.join(args.out_dir, data_type),\n                               speaker,\n                               sample_rate=args.sample_rate)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(""WSJ0 data preprocessing"")\n    parser.add_argument(\'--in-dir\', type=str, default=None,\n                        help=\'Directory path of wsj0 including tr, cv and tt\')\n    parser.add_argument(\'--out-dir\', type=str, default=None,\n                        help=\'Directory path to put output files\')\n    parser.add_argument(\'--sample-rate\', type=int, default=8000,\n                        help=\'Sample rate of audio file\')\n    args = parser.parse_args()\n    print(args)\n    preprocess(args)\n'"
src/separate.py,1,"b'#!/usr/bin/env python\n\n# Created on 2018/12\n# Author: Kaituo XU\n\nimport argparse\nimport os\n\nimport librosa\nimport torch\n\nfrom data import EvalDataLoader, EvalDataset\nfrom conv_tasnet import ConvTasNet\nfrom utils import remove_pad\n\n\nparser = argparse.ArgumentParser(\'Separate speech using Conv-TasNet\')\nparser.add_argument(\'--model_path\', type=str, required=True,\n                    help=\'Path to model file created by training\')\nparser.add_argument(\'--mix_dir\', type=str, default=None,\n                    help=\'Directory including mixture wav files\')\nparser.add_argument(\'--mix_json\', type=str, default=None,\n                    help=\'Json file including mixture wav files\')\nparser.add_argument(\'--out_dir\', type=str, default=\'exp/result\',\n                    help=\'Directory putting separated wav files\')\nparser.add_argument(\'--use_cuda\', type=int, default=0,\n                    help=\'Whether use GPU to separate speech\')\nparser.add_argument(\'--sample_rate\', default=8000, type=int,\n                    help=\'Sample rate\')\nparser.add_argument(\'--batch_size\', default=1, type=int,\n                    help=\'Batch size\')\n\n\ndef separate(args):\n    if args.mix_dir is None and args.mix_json is None:\n        print(""Must provide mix_dir or mix_json! When providing mix_dir, ""\n              ""mix_json is ignored."")\n\n    # Load model\n    model = ConvTasNet.load_model(args.model_path)\n    print(model)\n    model.eval()\n    if args.use_cuda:\n        model.cuda()\n\n    # Load data\n    eval_dataset = EvalDataset(args.mix_dir, args.mix_json,\n                               batch_size=args.batch_size,\n                               sample_rate=args.sample_rate)\n    eval_loader =  EvalDataLoader(eval_dataset, batch_size=1)\n    os.makedirs(args.out_dir, exist_ok=True)\n\n    def write(inputs, filename, sr=args.sample_rate):\n        librosa.output.write_wav(filename, inputs, sr)# norm=True)\n\n    with torch.no_grad():\n        for (i, data) in enumerate(eval_loader):\n            # Get batch data\n            mixture, mix_lengths, filenames = data\n            if args.use_cuda:\n                mixture, mix_lengths = mixture.cuda(), mix_lengths.cuda()\n            # Forward\n            estimate_source = model(mixture)  # [B, C, T]\n            # Remove padding and flat\n            flat_estimate = remove_pad(estimate_source, mix_lengths)\n            mixture = remove_pad(mixture, mix_lengths)\n            # Write result\n            for i, filename in enumerate(filenames):\n                filename = os.path.join(args.out_dir,\n                                        os.path.basename(filename).strip(\'.wav\'))\n                write(mixture[i], filename + \'.wav\')\n                C = flat_estimate[i].shape[0]\n                for c in range(C):\n                    write(flat_estimate[i][c], filename + \'_s{}.wav\'.format(c+1))\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    print(args)\n    separate(args)\n\n'"
src/solver.py,10,"b'# Created on 2018/12\n# Author: Kaituo XU\n\nimport os\nimport time\n\nimport torch\n\nfrom pit_criterion import cal_loss\n\n\nclass Solver(object):\n    \n    def __init__(self, data, model, optimizer, args):\n        self.tr_loader = data[\'tr_loader\']\n        self.cv_loader = data[\'cv_loader\']\n        self.model = model\n        self.optimizer = optimizer\n\n        # Training config\n        self.use_cuda = args.use_cuda\n        self.epochs = args.epochs\n        self.half_lr = args.half_lr\n        self.early_stop = args.early_stop\n        self.max_norm = args.max_norm\n        # save and load model\n        self.save_folder = args.save_folder\n        self.checkpoint = args.checkpoint\n        self.continue_from = args.continue_from\n        self.model_path = args.model_path\n        # logging\n        self.print_freq = args.print_freq\n        # visualizing loss using visdom\n        self.tr_loss = torch.Tensor(self.epochs)\n        self.cv_loss = torch.Tensor(self.epochs)\n        self.visdom = args.visdom\n        self.visdom_epoch = args.visdom_epoch\n        self.visdom_id = args.visdom_id\n        if self.visdom:\n            from visdom import Visdom\n            self.vis = Visdom(env=self.visdom_id)\n            self.vis_opts = dict(title=self.visdom_id,\n                                 ylabel=\'Loss\', xlabel=\'Epoch\',\n                                 legend=[\'train loss\', \'cv loss\'])\n            self.vis_window = None\n            self.vis_epochs = torch.arange(1, self.epochs + 1)\n\n        self._reset()\n\n    def _reset(self):\n        # Reset\n        if self.continue_from:\n            print(\'Loading checkpoint model %s\' % self.continue_from)\n            package = torch.load(self.continue_from)\n            self.model.module.load_state_dict(package[\'state_dict\'])\n            self.optimizer.load_state_dict(package[\'optim_dict\'])\n            self.start_epoch = int(package.get(\'epoch\', 1))\n            self.tr_loss[:self.start_epoch] = package[\'tr_loss\'][:self.start_epoch]\n            self.cv_loss[:self.start_epoch] = package[\'cv_loss\'][:self.start_epoch]\n        else:\n            self.start_epoch = 0\n        # Create save folder\n        os.makedirs(self.save_folder, exist_ok=True)\n        self.prev_val_loss = float(""inf"")\n        self.best_val_loss = float(""inf"")\n        self.halving = False\n        self.val_no_impv = 0\n\n    def train(self):\n        # Train model multi-epoches\n        for epoch in range(self.start_epoch, self.epochs):\n            # Train one epoch\n            print(""Training..."")\n            self.model.train()  # Turn on BatchNorm & Dropout\n            start = time.time()\n            tr_avg_loss = self._run_one_epoch(epoch)\n            print(\'-\' * 85)\n            print(\'Train Summary | End of Epoch {0} | Time {1:.2f}s | \'\n                  \'Train Loss {2:.3f}\'.format(\n                      epoch + 1, time.time() - start, tr_avg_loss))\n            print(\'-\' * 85)\n\n            # Save model each epoch\n            if self.checkpoint:\n                file_path = os.path.join(\n                    self.save_folder, \'epoch%d.pth.tar\' % (epoch + 1))\n                torch.save(self.model.module.serialize(self.model.module,\n                                                       self.optimizer, epoch + 1,\n                                                       tr_loss=self.tr_loss,\n                                                       cv_loss=self.cv_loss),\n                           file_path)\n                print(\'Saving checkpoint model to %s\' % file_path)\n\n            # Cross validation\n            print(\'Cross validation...\')\n            self.model.eval()  # Turn off Batchnorm & Dropout\n            val_loss = self._run_one_epoch(epoch, cross_valid=True)\n            print(\'-\' * 85)\n            print(\'Valid Summary | End of Epoch {0} | Time {1:.2f}s | \'\n                  \'Valid Loss {2:.3f}\'.format(\n                      epoch + 1, time.time() - start, val_loss))\n            print(\'-\' * 85)\n\n            # Adjust learning rate (halving)\n            if self.half_lr:\n                if val_loss >= self.prev_val_loss:\n                    self.val_no_impv += 1\n                    if self.val_no_impv >= 3:\n                        self.halving = True\n                    if self.val_no_impv >= 10 and self.early_stop:\n                        print(""No imporvement for 10 epochs, early stopping."")\n                        break\n                else:\n                    self.val_no_impv = 0\n            if self.halving:\n                optim_state = self.optimizer.state_dict()\n                optim_state[\'param_groups\'][0][\'lr\'] = \\\n                    optim_state[\'param_groups\'][0][\'lr\'] / 2.0\n                self.optimizer.load_state_dict(optim_state)\n                print(\'Learning rate adjusted to: {lr:.6f}\'.format(\n                    lr=optim_state[\'param_groups\'][0][\'lr\']))\n                self.halving = False\n            self.prev_val_loss = val_loss\n\n            # Save the best model\n            self.tr_loss[epoch] = tr_avg_loss\n            self.cv_loss[epoch] = val_loss\n            if val_loss < self.best_val_loss:\n                self.best_val_loss = val_loss\n                file_path = os.path.join(self.save_folder, self.model_path)\n                torch.save(self.model.module.serialize(self.model.module,\n                                                       self.optimizer, epoch + 1,\n                                                       tr_loss=self.tr_loss,\n                                                       cv_loss=self.cv_loss),\n                           file_path)\n                print(""Find better validated model, saving to %s"" % file_path)\n\n            # visualizing loss using visdom\n            if self.visdom:\n                x_axis = self.vis_epochs[0:epoch + 1]\n                y_axis = torch.stack(\n                    (self.tr_loss[0:epoch + 1], self.cv_loss[0:epoch + 1]), dim=1)\n                if self.vis_window is None:\n                    self.vis_window = self.vis.line(\n                        X=x_axis,\n                        Y=y_axis,\n                        opts=self.vis_opts,\n                    )\n                else:\n                    self.vis.line(\n                        X=x_axis.unsqueeze(0).expand(y_axis.size(\n                            1), x_axis.size(0)).transpose(0, 1),  # Visdom fix\n                        Y=y_axis,\n                        win=self.vis_window,\n                        update=\'replace\',\n                    )\n\n    def _run_one_epoch(self, epoch, cross_valid=False):\n        start = time.time()\n        total_loss = 0\n\n        data_loader = self.tr_loader if not cross_valid else self.cv_loader\n\n        # visualizing loss using visdom\n        if self.visdom_epoch and not cross_valid:\n            vis_opts_epoch = dict(title=self.visdom_id + "" epoch "" + str(epoch),\n                                  ylabel=\'Loss\', xlabel=\'Epoch\')\n            vis_window_epoch = None\n            vis_iters = torch.arange(1, len(data_loader) + 1)\n            vis_iters_loss = torch.Tensor(len(data_loader))\n\n        for i, (data) in enumerate(data_loader):\n            padded_mixture, mixture_lengths, padded_source = data\n            if self.use_cuda:\n                padded_mixture = padded_mixture.cuda()\n                mixture_lengths = mixture_lengths.cuda()\n                padded_source = padded_source.cuda()\n            estimate_source = self.model(padded_mixture)\n            loss, max_snr, estimate_source, reorder_estimate_source = \\\n                cal_loss(padded_source, estimate_source, mixture_lengths)\n            if not cross_valid:\n                self.optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n                                               self.max_norm)\n                self.optimizer.step()\n\n            total_loss += loss.item()\n\n            if i % self.print_freq == 0:\n                print(\'Epoch {0} | Iter {1} | Average Loss {2:.3f} | \'\n                      \'Current Loss {3:.6f} | {4:.1f} ms/batch\'.format(\n                          epoch + 1, i + 1, total_loss / (i + 1),\n                          loss.item(), 1000 * (time.time() - start) / (i + 1)),\n                      flush=True)\n\n            # visualizing loss using visdom\n            if self.visdom_epoch and not cross_valid:\n                vis_iters_loss[i] = loss.item()\n                if i % self.print_freq == 0:\n                    x_axis = vis_iters[:i+1]\n                    y_axis = vis_iters_loss[:i+1]\n                    if vis_window_epoch is None:\n                        vis_window_epoch = self.vis.line(X=x_axis, Y=y_axis,\n                                                         opts=vis_opts_epoch)\n                    else:\n                        self.vis.line(X=x_axis, Y=y_axis, win=vis_window_epoch,\n                                      update=\'replace\')\n\n        return total_loss / (i + 1)\n'"
src/train.py,3,"b'#!/usr/bin/env python\n\n# Created on 2018/12\n# Author: Kaituo XU\n\nimport argparse\n\nimport torch\n\nfrom data import AudioDataLoader, AudioDataset\nfrom solver import Solver\nfrom conv_tasnet import ConvTasNet\n\n\nparser = argparse.ArgumentParser(\n    ""Fully-Convolutional Time-domain Audio Separation Network (Conv-TasNet) ""\n    ""with Permutation Invariant Training"")\n# General config\n# Task related\nparser.add_argument(\'--train_dir\', type=str, default=None,\n                    help=\'directory including mix.json, s1.json and s2.json\')\nparser.add_argument(\'--valid_dir\', type=str, default=None,\n                    help=\'directory including mix.json, s1.json and s2.json\')\nparser.add_argument(\'--sample_rate\', default=8000, type=int,\n                    help=\'Sample rate\')\nparser.add_argument(\'--segment\', default=4, type=float,\n                    help=\'Segment length (seconds)\')\nparser.add_argument(\'--cv_maxlen\', default=8, type=float,\n                    help=\'max audio length (seconds) in cv, to avoid OOM issue.\')\n# Network architecture\nparser.add_argument(\'--N\', default=256, type=int,\n                    help=\'Number of filters in autoencoder\')\nparser.add_argument(\'--L\', default=20, type=int,\n                    help=\'Length of the filters in samples (40=5ms at 8kHZ)\')\nparser.add_argument(\'--B\', default=256, type=int,\n                    help=\'Number of channels in bottleneck 1 \xc3\x97 1-conv block\')\nparser.add_argument(\'--H\', default=512, type=int,\n                    help=\'Number of channels in convolutional blocks\')\nparser.add_argument(\'--P\', default=3, type=int,\n                    help=\'Kernel size in convolutional blocks\')\nparser.add_argument(\'--X\', default=8, type=int,\n                    help=\'Number of convolutional blocks in each repeat\')\nparser.add_argument(\'--R\', default=4, type=int,\n                    help=\'Number of repeats\')\nparser.add_argument(\'--C\', default=2, type=int,\n                    help=\'Number of speakers\')\nparser.add_argument(\'--norm_type\', default=\'gLN\', type=str,\n                    choices=[\'gLN\', \'cLN\', \'BN\'], help=\'Layer norm type\')\nparser.add_argument(\'--causal\', type=int, default=0,\n                    help=\'Causal (1) or noncausal(0) training\')\nparser.add_argument(\'--mask_nonlinear\', default=\'relu\', type=str,\n                    choices=[\'relu\', \'softmax\'], help=\'non-linear to generate mask\')\n# Training config\nparser.add_argument(\'--use_cuda\', type=int, default=1,\n                    help=\'Whether use GPU\')\nparser.add_argument(\'--epochs\', default=30, type=int,\n                    help=\'Number of maximum epochs\')\nparser.add_argument(\'--half_lr\', dest=\'half_lr\', default=0, type=int,\n                    help=\'Halving learning rate when get small improvement\')\nparser.add_argument(\'--early_stop\', dest=\'early_stop\', default=0, type=int,\n                    help=\'Early stop training when no improvement for 10 epochs\')\nparser.add_argument(\'--max_norm\', default=5, type=float,\n                    help=\'Gradient norm threshold to clip\')\n# minibatch\nparser.add_argument(\'--shuffle\', default=0, type=int,\n                    help=\'reshuffle the data at every epoch\')\nparser.add_argument(\'--batch_size\', default=128, type=int,\n                    help=\'Batch size\')\nparser.add_argument(\'--num_workers\', default=4, type=int,\n                    help=\'Number of workers to generate minibatch\')\n# optimizer\nparser.add_argument(\'--optimizer\', default=\'adam\', type=str,\n                    choices=[\'sgd\', \'adam\'],\n                    help=\'Optimizer (support sgd and adam now)\')\nparser.add_argument(\'--lr\', default=1e-3, type=float,\n                    help=\'Init learning rate\')\nparser.add_argument(\'--momentum\', default=0.0, type=float,\n                    help=\'Momentum for optimizer\')\nparser.add_argument(\'--l2\', default=0.0, type=float,\n                    help=\'weight decay (L2 penalty)\')\n# save and load model\nparser.add_argument(\'--save_folder\', default=\'exp/temp\',\n                    help=\'Location to save epoch models\')\nparser.add_argument(\'--checkpoint\', dest=\'checkpoint\', default=0, type=int,\n                    help=\'Enables checkpoint saving of model\')\nparser.add_argument(\'--continue_from\', default=\'\',\n                    help=\'Continue from checkpoint model\')\nparser.add_argument(\'--model_path\', default=\'final.pth.tar\',\n                    help=\'Location to save best validation model\')\n# logging\nparser.add_argument(\'--print_freq\', default=10, type=int,\n                    help=\'Frequency of printing training infomation\')\nparser.add_argument(\'--visdom\', dest=\'visdom\', type=int, default=0,\n                    help=\'Turn on visdom graphing\')\nparser.add_argument(\'--visdom_epoch\', dest=\'visdom_epoch\', type=int, default=0,\n                    help=\'Turn on visdom graphing each epoch\')\nparser.add_argument(\'--visdom_id\', default=\'TasNet training\',\n                    help=\'Identifier for visdom run\')\n\n\ndef main(args):\n    # Construct Solver\n    # data\n    tr_dataset = AudioDataset(args.train_dir, args.batch_size,\n                              sample_rate=args.sample_rate, segment=args.segment)\n    cv_dataset = AudioDataset(args.valid_dir, batch_size=1,  # 1 -> use less GPU memory to do cv\n                              sample_rate=args.sample_rate,\n                              segment=-1, cv_maxlen=args.cv_maxlen)  # -1 -> use full audio\n    tr_loader = AudioDataLoader(tr_dataset, batch_size=1,\n                                shuffle=args.shuffle,\n                                num_workers=args.num_workers)\n    cv_loader = AudioDataLoader(cv_dataset, batch_size=1,\n                                num_workers=0)\n    data = {\'tr_loader\': tr_loader, \'cv_loader\': cv_loader}\n    # model\n    model = ConvTasNet(args.N, args.L, args.B, args.H, args.P, args.X, args.R,\n                       args.C, norm_type=args.norm_type, causal=args.causal,\n                       mask_nonlinear=args.mask_nonlinear)\n    print(model)\n    if args.use_cuda:\n        model = torch.nn.DataParallel(model)\n        model.cuda()\n    # optimizer\n    if args.optimizer == \'sgd\':\n        optimizier = torch.optim.SGD(model.parameters(),\n                                     lr=args.lr,\n                                     momentum=args.momentum,\n                                     weight_decay=args.l2)\n    elif args.optimizer == \'adam\':\n        optimizier = torch.optim.Adam(model.parameters(),\n                                      lr=args.lr,\n                                      weight_decay=args.l2)\n    else:\n        print(""Not support optimizer"")\n        return\n\n    # solver\n    solver = Solver(data, model, optimizier, args)\n    solver.train()\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    print(args)\n    main(args)\n\n'"
src/utils.py,5,"b'# Created on 2018/12\n# Author: Kaituo XU\n\nimport math\n\nimport torch\n\n\ndef overlap_and_add(signal, frame_step):\n    """"""Reconstructs a signal from a framed representation.\n\n    Adds potentially overlapping frames of a signal with shape\n    `[..., frames, frame_length]`, offsetting subsequent frames by `frame_step`.\n    The resulting tensor has shape `[..., output_size]` where\n\n        output_size = (frames - 1) * frame_step + frame_length\n\n    Args:\n        signal: A [..., frames, frame_length] Tensor. All dimensions may be unknown, and rank must be at least 2.\n        frame_step: An integer denoting overlap offsets. Must be less than or equal to frame_length.\n\n    Returns:\n        A Tensor with shape [..., output_size] containing the overlap-added frames of signal\'s inner-most two dimensions.\n        output_size = (frames - 1) * frame_step + frame_length\n\n    Based on https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/signal/python/ops/reconstruction_ops.py\n    """"""\n    outer_dimensions = signal.size()[:-2]\n    frames, frame_length = signal.size()[-2:]\n\n    subframe_length = math.gcd(frame_length, frame_step)  # gcd=Greatest Common Divisor\n    subframe_step = frame_step // subframe_length\n    subframes_per_frame = frame_length // subframe_length\n    output_size = frame_step * (frames - 1) + frame_length\n    output_subframes = output_size // subframe_length\n\n    subframe_signal = signal.view(*outer_dimensions, -1, subframe_length)\n\n    frame = torch.arange(0, output_subframes).unfold(0, subframes_per_frame, subframe_step)\n    frame = signal.new_tensor(frame).long()  # signal may in GPU or CPU\n    frame = frame.contiguous().view(-1)\n\n    result = signal.new_zeros(*outer_dimensions, output_subframes, subframe_length)\n    result.index_add_(-2, frame, subframe_signal)\n    result = result.view(*outer_dimensions, -1)\n    return result\n\n\ndef remove_pad(inputs, inputs_lengths):\n    """"""\n    Args:\n        inputs: torch.Tensor, [B, C, T] or [B, T], B is batch size\n        inputs_lengths: torch.Tensor, [B]\n    Returns:\n        results: a list containing B items, each item is [C, T], T varies\n    """"""\n    results = []\n    dim = inputs.dim()\n    if dim == 3:\n        C = inputs.size(1)\n    for input, length in zip(inputs, inputs_lengths):\n        if dim == 3: # [B, C, T]\n            results.append(input[:,:length].view(C, -1).cpu().numpy())\n        elif dim == 2:  # [B, T]\n            results.append(input[:length].view(-1).cpu().numpy())\n    return results\n\n\nif __name__ == \'__main__\':\n    torch.manual_seed(123)\n    M, C, K, N = 2, 2, 3, 4\n    frame_step = 2\n    signal = torch.randint(5, (M, C, K, N))\n    result = overlap_and_add(signal, frame_step)\n    print(signal)\n    print(result)\n'"
test/learn_conv1d.py,7,"b""# Created on 2018/12/10\n# Author: Kaituo XU\n\nimport torch\ntorch.manual_seed(123)\n\nCin, Cout, F, S = 4, 3, 2, 1\nB, Lin = 2, 5\nD = 2\nprint('Cin, Cout, F, S, B, Lin, D=', Cin, Cout, F, S, B, Lin, D)\nconv1d = torch.nn.Conv1d(Cin, Cin, F, S, bias=False, padding=(F-1)*D, dilation=D, groups=Cin)\n#conv1d = torch.nn.Conv1d(Cin, Cin, F, S, bias=False, padding=1, dilation=1, groups=Cin)\ninputs = torch.randint(3, (B, Cin, Lin))\nconv1d.weight.data = torch.randint(5, conv1d.weight.size())\noutputs = conv1d(inputs)\n# Lout = (Lin - F) / S + 1\n\nprint('weight', conv1d.weight.size())\nprint('inputs', inputs.size())\nprint('outputs', outputs.size())\n\nprint('inputs\\n', inputs)\nprint('weight\\n', conv1d.weight)\nprint('outputs\\n', outputs)\nprint('chomp outputs\\n', outputs[:,:,:-(F-1)*D])\n\n# m = torch.nn.Conv1d(16, 33, 3, stride=2)\n# print(m.weight.size())\n# input = torch.randn(20, 16, 50)\n# output = m(input)\n# print(output.size())"""
