file_path,api_count,code
src/__init__.py,0,b''
test/learn_pytorch.py,7,"b'# If I\'m not sure waht some function or class actually doing, I will write\n# snippet codes to confirm my unstanding.\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef learn_cross_entropy():\n    IGNORE_ID = -1\n    torch.manual_seed(123)\n\n    input = torch.randn(4, 5, requires_grad=True)  # N x C\n    target = torch.randint(5, (4,), dtype=torch.int64)  # N\n    target[-1] = IGNORE_ID\n    print(""input:\\n"", input)\n    print(""target:\\n"", target)\n\n    # PART 1: confirm F.cross_entropy() == F.log_softmax() + F.nll_loss()\n    ce = F.cross_entropy(\n        input, target, ignore_index=IGNORE_ID, reduction=\'elementwise_mean\')\n    print(""### Using F.cross_entropy()"")\n    print(""ce ="", ce)\n    ls = F.log_softmax(input, dim=1)\n    nll = F.nll_loss(ls, target, ignore_index=IGNORE_ID,\n                     reduction=\'elementwise_mean\')\n    print(""### Using F.log_softmax() + F.nll_loss()"")\n    print(""nll ="", nll)\n    print(""### [CONFIRM] F.cross_entropy() == F.log_softmax() + F.nll_loss()\\n"")\n\n    # PART 2: confirm log_softmax = log + softmax\n    print(""log_softmax():\\n"", ls)\n    softmax = F.softmax(input, dim=1)\n    log_softmax = torch.log(softmax)\n    print(""softmax():\\n"", softmax)\n    print(""log() + softmax():\\n"", log_softmax)\n    print(""### [CONFIRM] log_softmax() == log() + softmax()\\n"")\n\n    # PART 3: confirm ignore_index works\n    non_ignore_index = target[target != IGNORE_ID]\n    print(non_ignore_index)\n    print(log_softmax[target != IGNORE_ID])\n    loss_each_sample = torch.stack([log_softmax[i][idx]\n                                    for i, idx in enumerate(non_ignore_index)], dim=0)\n    print(loss_each_sample)\n    print(-1 * torch.mean(loss_each_sample))\n    print(""### [CONFIRM] ignore_index in F.cross_entropy() works\\n"")\n\n    # PART 4: confirm cross_entropy()\'s backward() works correctly when set ignore_index\n    # nll = 1/N * -1 * sum(log(softmax(input, dim=1))[target])\n    # d_nll / d_input = 1/N * (softmax(input, dim=1) - target)\n    print(""softmax:\\n"", softmax)\n    print(""non ignore softmax:"")\n    print(softmax[:len(non_ignore_index)])\n    print(softmax[range(len(non_ignore_index)), non_ignore_index])\n    print(""target\\n"", target)\n    grad = softmax\n    grad[range(len(non_ignore_index)), non_ignore_index] -= 1\n    grad /= len(non_ignore_index)\n    grad[-1] = 0.0  # IGNORE_ID postition\n    print(""my gradient:\\n"", grad)\n    ce.backward()\n    print(""pytorch gradient:\\n"", input.grad)\n    print(""### [CONFIRM] F.cross_entropy()\'s backward() works correctly when ""\n          ""set ignore_index"")\n\n\nif __name__ == ""__main__"":\n    learn_cross_entropy()\n'"
test/learn_visdom.py,3,"b'# INSTALL\n# $ pip install visdom\n# START\n# $ visdom\n# or \n# $ python -m visdom.server\n# open browser and visit http://localhost:8097\n\nimport torch\nimport visdom\n\nvis = visdom.Visdom(env=""model_1"")\nvis.text(\'Hello, world\', win=\'text1\')\nvis.text(\'Hi, Kaituo\', win=\'text1\', append=True)\nfor i in range(10):\n    vis.line(X=torch.FloatTensor([i]), Y=torch.FloatTensor([i**2]), win=\'loss\', update=\'append\' if i> 0 else None)\n\n\nepochs = 20\nloss_result = torch.Tensor(epochs)\nfor i in range(epochs):\n\tloss_result[i] = i ** 2\nopts = dict(title=\'LAS\', ylabel=\'loss\', xlabel=\'epoch\')\nx_axis = torch.arange(1, epochs+1)\ny_axis = loss_result[:epochs]\nvis2 = visdom.Visdom(env=""view_loss"")\nvis2.line(X=x_axis, Y=y_axis, opts=opts)\n\n\nwhile True:\n\tcontinue\n'"
test/test_data.py,1,"b'import json\n\nfrom data import AudioDataset\nfrom data import AudioDataLoader\n\n\nif __name__ == ""__main__"":\n    train_json = ""data/data.json""\n    batch_size = 2\n    max_length_in = 1000\n    max_length_out = 1000\n    num_batches = 10\n    num_workers = 2\n    batch_frames = 2000\n\n    # test batch_frames\n    train_dataset = AudioDataset(\n        train_json, batch_size, max_length_in, max_length_out, num_batches,\n        batch_frames=batch_frames)\n    for i, minibatch in enumerate(train_dataset):\n        print(i)\n        print(minibatch)\n    exit(0)\n\n    # test\n    train_dataset = AudioDataset(\n        train_json, batch_size, max_length_in, max_length_out, num_batches)\n    # NOTE: must set batch_size=1 here.\n    train_loader = AudioDataLoader(\n        train_dataset, batch_size=1, num_workers=num_workers, LFR_m=4, LFR_n=3)\n\n    import torch\n    #torch.set_printoptions(threshold=10000000)\n    for i, (data) in enumerate(train_loader):\n        inputs, inputs_lens, targets = data\n        print(i)\n        # print(inputs)\n        print(inputs.size())\n        print(inputs_lens)\n        # print(targets)\n        print(""*""*20)\n'"
test/test_decode.py,4,"b'import argparse\n\nimport torch\n\nfrom encoder import Encoder\nfrom decoder import Decoder\nfrom transformer import Transformer\n\n\nif __name__ == ""__main__"":\n    D = 3\n    beam_size = 5\n    nbest = 5\n    defaults = dict(beam_size=beam_size,\n                    nbest=nbest,\n                    decode_max_len=0,\n                    d_input = D,\n                    LFR_m = 1,\n                    n_layers_enc = 2,\n                    n_head = 2,\n                    d_k = 6,\n                    d_v = 6,\n                    d_model = 12,\n                    d_inner = 8,\n                    dropout=0.1,\n                    pe_maxlen=100,\n                    d_word_vec=12,\n                    n_layers_dec = 2,\n                    tgt_emb_prj_weight_sharing=1)\n    args = argparse.Namespace(**defaults)\n    char_list = [""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", ""h"", ""<sos>"", ""<eos>""]\n    sos_id, eos_id = 8, 9\n    vocab_size = len(char_list)\n    # model\n    encoder = Encoder(args.d_input * args.LFR_m, args.n_layers_enc, args.n_head,\n                      args.d_k, args.d_v, args.d_model, args.d_inner,\n                      dropout=args.dropout, pe_maxlen=args.pe_maxlen)\n    decoder = Decoder(sos_id, eos_id, vocab_size,\n                      args.d_word_vec, args.n_layers_dec, args.n_head,\n                      args.d_k, args.d_v, args.d_model, args.d_inner,\n                      dropout=args.dropout,\n                      tgt_emb_prj_weight_sharing=args.tgt_emb_prj_weight_sharing,\n                      pe_maxlen=args.pe_maxlen)\n    model = Transformer(encoder, decoder)\n\n    for i in range(3):\n        print(""\\n***** Utt"", i+1)\n        Ti = i + 20\n        input = torch.randn(Ti, D)\n        length = torch.tensor([Ti], dtype=torch.int)\n        nbest_hyps = model.recognize(input, length, char_list, args)\n\n    file_path = ""./temp.pth""\n    optimizer = torch.optim.Adam(model.parameters())\n    torch.save(model.serialize(model, optimizer, 1, LFR_m=1, LFR_n=1), file_path)\n    model, LFR_m, LFR_n = Transformer.load_model(file_path)\n    print(model)\n\n    import os\n    os.remove(file_path)\n'"
src/bin/recognize.py,3,"b'#!/usr/bin/env python\nimport argparse\nimport json\n\nimport torch\n\nimport kaldi_io\nfrom transformer import Transformer\nfrom utils import add_results_to_json, process_dict\nfrom data import build_LFR_features\n\nparser = argparse.ArgumentParser(\n    ""End-to-End Automatic Speech Recognition Decoding."")\n# data\nparser.add_argument(\'--recog-json\', type=str, required=True,\n                    help=\'Filename of recognition data (json)\')\nparser.add_argument(\'--dict\', type=str, required=True,\n                    help=\'Dictionary which should include <unk> <sos> <eos>\')\nparser.add_argument(\'--result-label\', type=str, required=True,\n                    help=\'Filename of result label data (json)\')\n# model\nparser.add_argument(\'--model-path\', type=str, required=True,\n                    help=\'Path to model file created by training\')\n# decode\nparser.add_argument(\'--beam-size\', default=1, type=int,\n                    help=\'Beam size\')\nparser.add_argument(\'--nbest\', default=1, type=int,\n                    help=\'Nbest size\')\nparser.add_argument(\'--decode-max-len\', default=0, type=int,\n                    help=\'Max output length. If ==0 (default), it uses a \'\n                    \'end-detect function to automatically find maximum \'\n                    \'hypothesis lengths\')\n\n\ndef recognize(args):\n    model, LFR_m, LFR_n = Transformer.load_model(args.model_path)\n    print(model)\n    model.eval()\n    model.cuda()\n    char_list, sos_id, eos_id = process_dict(args.dict)\n    assert model.decoder.sos_id == sos_id and model.decoder.eos_id == eos_id\n\n    # read json data\n    with open(args.recog_json, \'rb\') as f:\n        js = json.load(f)[\'utts\']\n\n    # decode each utterance\n    new_js = {}\n    with torch.no_grad():\n        for idx, name in enumerate(js.keys(), 1):\n            print(\'(%d/%d) decoding %s\' %\n                  (idx, len(js.keys()), name), flush=True)\n            input = kaldi_io.read_mat(js[name][\'input\'][0][\'feat\'])  # TxD\n            input = build_LFR_features(input, LFR_m, LFR_n)\n            input = torch.from_numpy(input).float()\n            input_length = torch.tensor([input.size(0)], dtype=torch.int)\n            input = input.cuda()\n            input_length = input_length.cuda()\n            nbest_hyps = model.recognize(input, input_length, char_list, args)\n            new_js[name] = add_results_to_json(js[name], nbest_hyps, char_list)\n\n    with open(args.result_label, \'wb\') as f:\n        f.write(json.dumps({\'utts\': new_js}, indent=4,\n                           sort_keys=True).encode(\'utf_8\'))\n\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n    print(args, flush=True)\n    recognize(args)\n'"
src/bin/train.py,1,"b'#!/usr/bin/env python\nimport argparse\n\nimport torch\n\nfrom data import AudioDataLoader, AudioDataset\nfrom decoder import Decoder\nfrom encoder import Encoder\nfrom transformer import Transformer\nfrom solver import Solver\nfrom utils import process_dict\nfrom optimizer import TransformerOptimizer\n\nparser = argparse.ArgumentParser(\n    ""End-to-End Automatic Speech Recognition Training ""\n    ""(Transformer framework)."")\n# General config\n# Task related\nparser.add_argument(\'--train-json\', type=str, default=None,\n                    help=\'Filename of train label data (json)\')\nparser.add_argument(\'--valid-json\', type=str, default=None,\n                    help=\'Filename of validation label data (json)\')\nparser.add_argument(\'--dict\', type=str, required=True,\n                    help=\'Dictionary which should include <unk> <sos> <eos>\')\n# Low Frame Rate (stacking and skipping frames)\nparser.add_argument(\'--LFR_m\', default=4, type=int,\n                    help=\'Low Frame Rate: number of frames to stack\')\nparser.add_argument(\'--LFR_n\', default=3, type=int,\n                    help=\'Low Frame Rate: number of frames to skip\')\n# Network architecture\n# encoder\n# TODO: automatically infer input dim\nparser.add_argument(\'--d_input\', default=80, type=int,\n                    help=\'Dim of encoder input (before LFR)\')\nparser.add_argument(\'--n_layers_enc\', default=6, type=int,\n                    help=\'Number of encoder stacks\')\nparser.add_argument(\'--n_head\', default=8, type=int,\n                    help=\'Number of Multi Head Attention (MHA)\')\nparser.add_argument(\'--d_k\', default=64, type=int,\n                    help=\'Dimension of key\')\nparser.add_argument(\'--d_v\', default=64, type=int,\n                    help=\'Dimension of value\')\nparser.add_argument(\'--d_model\', default=512, type=int,\n                    help=\'Dimension of model\')\nparser.add_argument(\'--d_inner\', default=2048, type=int,\n                    help=\'Dimension of inner\')\nparser.add_argument(\'--dropout\', default=0.1, type=float,\n                    help=\'Dropout rate\')\nparser.add_argument(\'--pe_maxlen\', default=5000, type=int,\n                    help=\'Positional Encoding max len\')\n# decoder\nparser.add_argument(\'--d_word_vec\', default=512, type=int,\n                    help=\'Dim of decoder embedding\')\nparser.add_argument(\'--n_layers_dec\', default=6, type=int,\n                    help=\'Number of decoder stacks\')\nparser.add_argument(\'--tgt_emb_prj_weight_sharing\', default=1, type=int,\n                    help=\'share decoder embedding with decoder projection\')\n# Loss\nparser.add_argument(\'--label_smoothing\', default=0.1, type=float,\n                    help=\'label smoothing\')\n\n# Training config\nparser.add_argument(\'--epochs\', default=30, type=int,\n                    help=\'Number of maximum epochs\')\n# minibatch\nparser.add_argument(\'--shuffle\', default=0, type=int,\n                    help=\'reshuffle the data at every epoch\')\nparser.add_argument(\'--batch-size\', default=32, type=int,\n                    help=\'Batch size\')\nparser.add_argument(\'--batch_frames\', default=0, type=int,\n                    help=\'Batch frames. If this is not 0, batch size will make no sense\')\nparser.add_argument(\'--maxlen-in\', default=800, type=int, metavar=\'ML\',\n                    help=\'Batch size is reduced if the input sequence length > ML\')\nparser.add_argument(\'--maxlen-out\', default=150, type=int, metavar=\'ML\',\n                    help=\'Batch size is reduced if the output sequence length > ML\')\nparser.add_argument(\'--num-workers\', default=4, type=int,\n                    help=\'Number of workers to generate minibatch\')\n# optimizer\nparser.add_argument(\'--k\', default=1.0, type=float,\n                    help=\'tunable scalar multiply to learning rate\')\nparser.add_argument(\'--warmup_steps\', default=4000, type=int,\n                    help=\'warmup steps\')\n# save and load model\nparser.add_argument(\'--save-folder\', default=\'exp/temp\',\n                    help=\'Location to save epoch models\')\nparser.add_argument(\'--checkpoint\', dest=\'checkpoint\', default=0, type=int,\n                    help=\'Enables checkpoint saving of model\')\nparser.add_argument(\'--continue-from\', default=\'\',\n                    help=\'Continue from checkpoint model\')\nparser.add_argument(\'--model-path\', default=\'final.pth.tar\',\n                    help=\'Location to save best validation model\')\n# logging\nparser.add_argument(\'--print-freq\', default=10, type=int,\n                    help=\'Frequency of printing training infomation\')\nparser.add_argument(\'--visdom\', dest=\'visdom\', type=int, default=0,\n                    help=\'Turn on visdom graphing\')\nparser.add_argument(\'--visdom_lr\', dest=\'visdom_lr\', type=int, default=0,\n                    help=\'Turn on visdom graphing learning rate\')\nparser.add_argument(\'--visdom_epoch\', dest=\'visdom_epoch\', type=int, default=0,\n                    help=\'Turn on visdom graphing each epoch\')\nparser.add_argument(\'--visdom-id\', default=\'Transformer training\',\n                    help=\'Identifier for visdom run\')\n\n\ndef main(args):\n    # Construct Solver\n    # data\n    tr_dataset = AudioDataset(args.train_json, args.batch_size,\n                              args.maxlen_in, args.maxlen_out,\n                              batch_frames=args.batch_frames)\n    cv_dataset = AudioDataset(args.valid_json, args.batch_size,\n                              args.maxlen_in, args.maxlen_out,\n                              batch_frames=args.batch_frames)\n    tr_loader = AudioDataLoader(tr_dataset, batch_size=1,\n                                num_workers=args.num_workers,\n                                shuffle=args.shuffle,\n                                LFR_m=args.LFR_m, LFR_n=args.LFR_n)\n    cv_loader = AudioDataLoader(cv_dataset, batch_size=1,\n                                num_workers=args.num_workers,\n                                LFR_m=args.LFR_m, LFR_n=args.LFR_n)\n    # load dictionary and generate char_list, sos_id, eos_id\n    char_list, sos_id, eos_id = process_dict(args.dict)\n    vocab_size = len(char_list)\n    data = {\'tr_loader\': tr_loader, \'cv_loader\': cv_loader}\n    # model\n    encoder = Encoder(args.d_input * args.LFR_m, args.n_layers_enc, args.n_head,\n                      args.d_k, args.d_v, args.d_model, args.d_inner,\n                      dropout=args.dropout, pe_maxlen=args.pe_maxlen)\n    decoder = Decoder(sos_id, eos_id, vocab_size,\n                      args.d_word_vec, args.n_layers_dec, args.n_head,\n                      args.d_k, args.d_v, args.d_model, args.d_inner,\n                      dropout=args.dropout,\n                      tgt_emb_prj_weight_sharing=args.tgt_emb_prj_weight_sharing,\n                      pe_maxlen=args.pe_maxlen)\n    model = Transformer(encoder, decoder)\n    print(model)\n    model.cuda()\n    # optimizer\n    optimizier = TransformerOptimizer(\n        torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n        args.k,\n        args.d_model,\n        args.warmup_steps)\n\n    # solver\n    solver = Solver(data, model, optimizier, args)\n    solver.train()\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    print(args)\n    main(args)\n'"
src/data/__init__.py,0,b''
src/data/data.py,8,"b'""""""\nLogic:\n1. AudioDataLoader generate a minibatch from AudioDataset, the size of this\n   minibatch is AudioDataLoader\'s batchsize. For now, we always set\n   AudioDataLoader\'s batchsize as 1. The real minibatch size we care about is\n   set in AudioDataset\'s __init__(...). So actually, we generate the \n   information of one minibatch in AudioDataset.\n2. After AudioDataLoader getting one minibatch from AudioDataset,\n   AudioDataLoader calls its collate_fn(batch) to process this minibatch.\n""""""\nimport json\n\nimport numpy as np\nimport torch\nimport torch.utils.data as data\n\nimport kaldi_io\nfrom utils import IGNORE_ID, pad_list\n\n\nclass AudioDataset(data.Dataset):\n    """"""\n    TODO: this is a little HACK now, put batch_size here now.\n          remove batch_size to dataloader later.\n    """"""\n\n    def __init__(self, data_json_path, batch_size, max_length_in, max_length_out,\n                 num_batches=0, batch_frames=0):\n        # From: espnet/src/asr/asr_utils.py: make_batchset()\n        """"""\n        Args:\n            data: espnet/espnet json format file.\n            num_batches: for debug. only use num_batches minibatch but not all.\n        """"""\n        super(AudioDataset, self).__init__()\n        with open(data_json_path, \'rb\') as f:\n            data = json.load(f)[\'utts\']\n        # sort it by input lengths (long to short)\n        sorted_data = sorted(data.items(), key=lambda data: int(\n            data[1][\'input\'][0][\'shape\'][0]), reverse=True)\n        # change batchsize depending on the input and output length\n        minibatch = []\n        # Method 1: Generate minibatch based on batch_size\n        # i.e. each batch contains #batch_size utterances\n        if batch_frames == 0:\n            start = 0\n            while True:\n                ilen = int(sorted_data[start][1][\'input\'][0][\'shape\'][0])\n                olen = int(sorted_data[start][1][\'output\'][0][\'shape\'][0])\n                factor = max(int(ilen / max_length_in), int(olen / max_length_out))\n                # if ilen = 1000 and max_length_in = 800\n                # then b = batchsize / 2\n                # and max(1, .) avoids batchsize = 0\n                b = max(1, int(batch_size / (1 + factor)))\n                end = min(len(sorted_data), start + b)\n                minibatch.append(sorted_data[start:end])\n                # DEBUG\n                # total= 0\n                # for i in range(start, end):\n                #     total += int(sorted_data[i][1][\'input\'][0][\'shape\'][0])\n                # print(total, end-start)\n                if end == len(sorted_data):\n                    break\n                start = end\n        # Method 2: Generate minibatch based on batch_frames\n        # i.e. each batch contains approximately #batch_frames frames\n        else:  # batch_frames > 0\n            print(""NOTE: Generate minibatch based on batch_frames."")\n            print(""i.e. each batch contains approximately #batch_frames frames"")\n            start = 0\n            while True:\n                total_frames = 0\n                end = start\n                while total_frames < batch_frames and end < len(sorted_data):\n                    ilen = int(sorted_data[end][1][\'input\'][0][\'shape\'][0])\n                    total_frames += ilen\n                    end += 1\n                # print(total_frames, end-start)\n                minibatch.append(sorted_data[start:end])\n                if end == len(sorted_data):\n                    break\n                start = end\n        if num_batches > 0:\n            minibatch = minibatch[:num_batches]\n        self.minibatch = minibatch\n\n    def __getitem__(self, index):\n        return self.minibatch[index]\n\n    def __len__(self):\n        return len(self.minibatch)\n\n\nclass AudioDataLoader(data.DataLoader):\n    """"""\n    NOTE: just use batchsize=1 here, so drop_last=True makes no sense here.\n    """"""\n\n    def __init__(self, *args, LFR_m=1, LFR_n=1, **kwargs):\n        super(AudioDataLoader, self).__init__(*args, **kwargs)\n        self.collate_fn = LFRCollate(LFR_m=LFR_m, LFR_n=LFR_n)\n\n\nclass LFRCollate(object):\n    """"""Build this wrapper to pass arguments(LFR_m, LFR_n) to _collate_fn""""""\n    def __init__(self, LFR_m=1, LFR_n=1):\n        self.LFR_m = LFR_m\n        self.LFR_n = LFR_n\n\n    def __call__(self, batch):\n        return _collate_fn(batch, LFR_m=self.LFR_m, LFR_n=self.LFR_n)\n\n\n# From: espnet/src/asr/asr_pytorch.py: CustomConverter:__call__\ndef _collate_fn(batch, LFR_m=1, LFR_n=1):\n    """"""\n    Args:\n        batch: list, len(batch) = 1. See AudioDataset.__getitem__()\n    Returns:\n        xs_pad: N x Ti x D, torch.Tensor\n        ilens : N, torch.Tentor\n        ys_pad: N x To, torch.Tensor\n    """"""\n    # batch should be located in list\n    assert len(batch) == 1\n    batch = load_inputs_and_targets(batch[0], LFR_m=LFR_m, LFR_n=LFR_n)\n    xs, ys = batch\n\n    # TODO: perform subsamping\n\n    # get batch of lengths of input sequences\n    ilens = np.array([x.shape[0] for x in xs])\n\n    # perform padding and convert to tensor\n    xs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0)\n    ilens = torch.from_numpy(ilens)\n    ys_pad = pad_list([torch.from_numpy(y).long() for y in ys], IGNORE_ID)\n    return xs_pad, ilens, ys_pad\n\n\n# ------------------------------ utils ------------------------------------\ndef load_inputs_and_targets(batch, LFR_m=1, LFR_n=1):\n    # From: espnet/src/asr/asr_utils.py: load_inputs_and_targets\n    # load acoustic features and target sequence of token ids\n    # for b in batch:\n    #     print(b[1][\'input\'][0][\'feat\'])\n    xs = [kaldi_io.read_mat(b[1][\'input\'][0][\'feat\']) for b in batch]\n    ys = [b[1][\'output\'][0][\'tokenid\'].split() for b in batch]\n\n    if LFR_m != 1 or LFR_n != 1:\n        # xs = build_LFR_features(xs, LFR_m, LFR_n)\n        xs = [build_LFR_features(x, LFR_m, LFR_n) for x in xs]\n\n    # get index of non-zero length samples\n    nonzero_idx = filter(lambda i: len(ys[i]) > 0, range(len(xs)))\n    # sort in input lengths\n    nonzero_sorted_idx = sorted(nonzero_idx, key=lambda i: -len(xs[i]))\n    if len(nonzero_sorted_idx) != len(xs):\n        print(""warning: Target sequences include empty tokenid"")\n\n    # remove zero-lenght samples\n    xs = [xs[i] for i in nonzero_sorted_idx]\n    ys = [np.fromiter(map(int, ys[i]), dtype=np.int64)\n          for i in nonzero_sorted_idx]\n\n    return xs, ys\n\n\ndef build_LFR_features(inputs, m, n):\n    """"""\n    Actually, this implements stacking frames and skipping frames.\n    if m = 1 and n = 1, just return the origin features.\n    if m = 1 and n > 1, it works like skipping.\n    if m > 1 and n = 1, it works like stacking but only support right frames.\n    if m > 1 and n > 1, it works like LFR.\n\n    Args:\n        inputs_batch: inputs is T x D np.ndarray\n        m: number of frames to stack\n        n: number of frames to skip\n    """"""\n    # LFR_inputs_batch = []\n    # for inputs in inputs_batch:\n    LFR_inputs = []\n    T = inputs.shape[0]\n    T_lfr = int(np.ceil(T / n))\n    for i in range(T_lfr):\n        if m <= T - i * n:\n            LFR_inputs.append(np.hstack(inputs[i*n:i*n+m]))\n        else: # process last LFR frame\n            num_padding = m - (T - i * n)\n            frame = np.hstack(inputs[i*n:])\n            for _ in range(num_padding):\n                frame = np.hstack((frame, inputs[-1]))\n            LFR_inputs.append(frame)\n    return np.vstack(LFR_inputs)\n    #     LFR_inputs_batch.append(np.vstack(LFR_inputs))\n    # return LFR_inputs_batch\n'"
src/solver/__init__.py,0,b''
src/solver/solver.py,9,"b'import os\nimport time\n\nimport torch\n\nfrom loss import cal_performance\nfrom utils import IGNORE_ID\n\n\nclass Solver(object):\n    """"""\n    """"""\n\n    def __init__(self, data, model, optimizer, args):\n        self.tr_loader = data[\'tr_loader\']\n        self.cv_loader = data[\'cv_loader\']\n        self.model = model\n        self.optimizer = optimizer\n\n        # Low frame rate feature\n        self.LFR_m = args.LFR_m\n        self.LFR_n = args.LFR_n\n\n        # Training config\n        self.epochs = args.epochs\n        self.label_smoothing = args.label_smoothing\n        # save and load model\n        self.save_folder = args.save_folder\n        self.checkpoint = args.checkpoint\n        self.continue_from = args.continue_from\n        self.model_path = args.model_path\n        # logging\n        self.print_freq = args.print_freq\n        # visualizing loss using visdom\n        self.tr_loss = torch.Tensor(self.epochs)\n        self.cv_loss = torch.Tensor(self.epochs)\n        self.visdom = args.visdom\n        self.visdom_lr = args.visdom_lr\n        self.visdom_epoch = args.visdom_epoch\n        self.visdom_id = args.visdom_id\n        if self.visdom:\n            from visdom import Visdom\n            self.vis = Visdom(env=self.visdom_id)\n            self.vis_opts = dict(title=self.visdom_id,\n                                 ylabel=\'Loss\', xlabel=\'Epoch\',\n                                 legend=[\'train loss\', \'cv loss\'])\n            self.vis_window = None\n            self.vis_epochs = torch.arange(1, self.epochs + 1)\n            self.optimizer.set_visdom(self.visdom_lr, self.vis)\n\n        self._reset()\n\n    def _reset(self):\n        # Reset\n        if self.continue_from:\n            print(\'Loading checkpoint model %s\' % self.continue_from)\n            package = torch.load(self.continue_from)\n            self.model.load_state_dict(package[\'state_dict\'])\n            self.optimizer.load_state_dict(package[\'optim_dict\'])\n            self.start_epoch = int(package.get(\'epoch\', 1))\n            self.tr_loss[:self.start_epoch] = package[\'tr_loss\'][:self.start_epoch]\n            self.cv_loss[:self.start_epoch] = package[\'cv_loss\'][:self.start_epoch]\n        else:\n            self.start_epoch = 0\n        # Create save folder\n        os.makedirs(self.save_folder, exist_ok=True)\n        self.prev_val_loss = float(""inf"")\n        self.best_val_loss = float(""inf"")\n        self.halving = False\n\n    def train(self):\n        # Train model multi-epoches\n        for epoch in range(self.start_epoch, self.epochs):\n            # Train one epoch\n            print(""Training..."")\n            self.model.train()  # Turn on BatchNorm & Dropout\n            start = time.time()\n            tr_avg_loss = self._run_one_epoch(epoch)\n            print(\'-\' * 85)\n            print(\'Train Summary | End of Epoch {0} | Time {1:.2f}s | \'\n                  \'Train Loss {2:.3f}\'.format(\n                      epoch + 1, time.time() - start, tr_avg_loss))\n            print(\'-\' * 85)\n\n            # Save model each epoch\n            if self.checkpoint:\n                file_path = os.path.join(\n                    self.save_folder, \'epoch%d.pth.tar\' % (epoch + 1))\n                torch.save(self.model.serialize(self.model, self.optimizer, epoch + 1,\n                                                self.LFR_m, self.LFR_n,\n                                                tr_loss=self.tr_loss,\n                                                cv_loss=self.cv_loss),\n                           file_path)\n                print(\'Saving checkpoint model to %s\' % file_path)\n\n            # Cross validation\n            print(\'Cross validation...\')\n            self.model.eval()  # Turn off Batchnorm & Dropout\n            val_loss = self._run_one_epoch(epoch, cross_valid=True)\n            print(\'-\' * 85)\n            print(\'Valid Summary | End of Epoch {0} | Time {1:.2f}s | \'\n                  \'Valid Loss {2:.3f}\'.format(\n                      epoch + 1, time.time() - start, val_loss))\n            print(\'-\' * 85)\n\n            # Save the best model\n            self.tr_loss[epoch] = tr_avg_loss\n            self.cv_loss[epoch] = val_loss\n            if val_loss < self.best_val_loss:\n                self.best_val_loss = val_loss\n                file_path = os.path.join(self.save_folder, self.model_path)\n                torch.save(self.model.serialize(self.model, self.optimizer, epoch + 1,\n                                                self.LFR_m, self.LFR_n,\n                                                tr_loss=self.tr_loss,\n                                                cv_loss=self.cv_loss),\n                           file_path)\n                print(""Find better validated model, saving to %s"" % file_path)\n\n            # visualizing loss using visdom\n            if self.visdom:\n                x_axis = self.vis_epochs[0:epoch + 1]\n                y_axis = torch.stack(\n                    (self.tr_loss[0:epoch + 1], self.cv_loss[0:epoch + 1]), dim=1)\n                if self.vis_window is None:\n                    self.vis_window = self.vis.line(\n                        X=x_axis,\n                        Y=y_axis,\n                        opts=self.vis_opts,\n                    )\n                else:\n                    self.vis.line(\n                        X=x_axis.unsqueeze(0).expand(y_axis.size(\n                            1), x_axis.size(0)).transpose(0, 1),  # Visdom fix\n                        Y=y_axis,\n                        win=self.vis_window,\n                        update=\'replace\',\n                    )\n\n    def _run_one_epoch(self, epoch, cross_valid=False):\n        start = time.time()\n        total_loss = 0\n\n        data_loader = self.tr_loader if not cross_valid else self.cv_loader\n\n        # visualizing loss using visdom\n        if self.visdom_epoch and not cross_valid:\n            vis_opts_epoch = dict(title=self.visdom_id + "" epoch "" + str(epoch),\n                                  ylabel=\'Loss\', xlabel=\'Epoch\')\n            vis_window_epoch = None\n            vis_iters = torch.arange(1, len(data_loader) + 1)\n            vis_iters_loss = torch.Tensor(len(data_loader))\n\n        for i, (data) in enumerate(data_loader):\n            padded_input, input_lengths, padded_target = data\n            padded_input = padded_input.cuda()\n            input_lengths = input_lengths.cuda()\n            padded_target = padded_target.cuda()\n            pred, gold = self.model(padded_input, input_lengths, padded_target)\n            loss, n_correct = cal_performance(pred, gold,\n                                              smoothing=self.label_smoothing)\n            if not cross_valid:\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n            total_loss += loss.item()\n            non_pad_mask = gold.ne(IGNORE_ID)\n            n_word = non_pad_mask.sum().item()\n\n            if i % self.print_freq == 0:\n                print(\'Epoch {0} | Iter {1} | Average Loss {2:.3f} | \'\n                      \'Current Loss {3:.6f} | {4:.1f} ms/batch\'.format(\n                          epoch + 1, i + 1, total_loss / (i + 1),\n                          loss.item(), 1000 * (time.time() - start) / (i + 1)),\n                      flush=True)\n\n            # visualizing loss using visdom\n            if self.visdom_epoch and not cross_valid:\n                vis_iters_loss[i] = loss.item()\n                if i % self.print_freq == 0:\n                    x_axis = vis_iters[:i+1]\n                    y_axis = vis_iters_loss[:i+1]\n                    if vis_window_epoch is None:\n                        vis_window_epoch = self.vis.line(X=x_axis, Y=y_axis,\n                                                         opts=vis_opts_epoch)\n                    else:\n                        self.vis.line(X=x_axis, Y=y_axis, win=vis_window_epoch,\n                                      update=\'replace\')\n\n        return total_loss / (i + 1)\n'"
src/transformer/__init__.py,0,b''
src/transformer/attention.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\n\n\nclass MultiHeadAttention(nn.Module):\n    ''' Multi-Head Attention module '''\n\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k)\n        self.w_ks = nn.Linear(d_model, n_head * d_k)\n        self.w_vs = nn.Linear(d_model, n_head * d_v)\n        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n\n        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5),\n                                                   attn_dropout=dropout)\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.fc = nn.Linear(n_head * d_v, d_model)\n        nn.init.xavier_normal_(self.fc.weight)\n\n        self.dropout = nn.Dropout(dropout)\n\n\n    def forward(self, q, k, v, mask=None):\n\n        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n\n        sz_b, len_q, _ = q.size()\n        sz_b, len_k, _ = k.size()\n        sz_b, len_v, _ = v.size()\n\n        residual = q\n\n        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n\n        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k) # (n*b) x lq x dk\n        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k) # (n*b) x lk x dk\n        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v) # (n*b) x lv x dv\n\n        if mask is not None:\n            mask = mask.repeat(n_head, 1, 1) # (n*b) x .. x ..\n\n        output, attn = self.attention(q, k, v, mask=mask)\n\n        output = output.view(n_head, sz_b, len_q, d_v)\n        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1) # b x lq x (n*dv)\n\n        output = self.dropout(self.fc(output))\n        output = self.layer_norm(output + residual)\n\n        return output, attn\n\n\nclass ScaledDotProductAttention(nn.Module):\n    ''' Scaled Dot-Product Attention '''\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self, q, k, v, mask=None):\n\n        attn = torch.bmm(q, k.transpose(1, 2))\n        attn = attn / self.temperature\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -np.inf)\n\n        attn = self.softmax(attn)\n        attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n\n        return output, attn\n"""
src/transformer/decoder.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom attention import MultiHeadAttention\nfrom module import PositionalEncoding, PositionwiseFeedForward\nfrom utils import (IGNORE_ID, get_attn_key_pad_mask, get_attn_pad_mask,\n                   get_non_pad_mask, get_subsequent_mask, pad_list)\n\n\nclass Decoder(nn.Module):\n    \'\'\' A decoder model with self attention mechanism. \'\'\'\n\n    def __init__(\n            self, sos_id, eos_id,\n            n_tgt_vocab, d_word_vec,\n            n_layers, n_head, d_k, d_v,\n            d_model, d_inner, dropout=0.1,\n            tgt_emb_prj_weight_sharing=True,\n            pe_maxlen=5000):\n        super(Decoder, self).__init__()\n        # parameters\n        self.sos_id = sos_id  # Start of Sentence\n        self.eos_id = eos_id  # End of Sentence\n        self.n_tgt_vocab = n_tgt_vocab\n        self.d_word_vec = d_word_vec\n        self.n_layers = n_layers\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n        self.tgt_emb_prj_weight_sharing = tgt_emb_prj_weight_sharing\n        self.pe_maxlen = pe_maxlen\n\n        self.tgt_word_emb = nn.Embedding(n_tgt_vocab, d_word_vec)\n        self.positional_encoding = PositionalEncoding(d_model, max_len=pe_maxlen)\n        self.dropout = nn.Dropout(dropout)\n\n        self.layer_stack = nn.ModuleList([\n            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n            for _ in range(n_layers)])\n\n        self.tgt_word_prj = nn.Linear(d_model, n_tgt_vocab, bias=False)\n        nn.init.xavier_normal_(self.tgt_word_prj.weight)\n\n        if tgt_emb_prj_weight_sharing:\n            # Share the weight matrix between target word embedding & the final logit dense layer\n            self.tgt_word_prj.weight = self.tgt_word_emb.weight\n            self.x_logit_scale = (d_model ** 0.5)\n        else:\n            self.x_logit_scale = 1.\n\n    def preprocess(self, padded_input):\n        """"""Generate decoder input and output label from padded_input\n        Add <sos> to decoder input, and add <eos> to decoder output label\n        """"""\n        ys = [y[y != IGNORE_ID] for y in padded_input]  # parse padded ys\n        # prepare input and output word sequences with sos/eos IDs\n        eos = ys[0].new([self.eos_id])\n        sos = ys[0].new([self.sos_id])\n        ys_in = [torch.cat([sos, y], dim=0) for y in ys]\n        ys_out = [torch.cat([y, eos], dim=0) for y in ys]\n        # padding for ys with -1\n        # pys: utt x olen\n        ys_in_pad = pad_list(ys_in, self.eos_id)\n        ys_out_pad = pad_list(ys_out, IGNORE_ID)\n        assert ys_in_pad.size() == ys_out_pad.size()\n        return ys_in_pad, ys_out_pad\n\n    def forward(self, padded_input, encoder_padded_outputs,\n                encoder_input_lengths, return_attns=False):\n        """"""\n        Args:\n            padded_input: N x To\n            encoder_padded_outputs: N x Ti x H\n\n        Returns:\n        """"""\n        dec_slf_attn_list, dec_enc_attn_list = [], []\n\n        # Get Deocder Input and Output\n        ys_in_pad, ys_out_pad = self.preprocess(padded_input)\n\n        # Prepare masks\n        non_pad_mask = get_non_pad_mask(ys_in_pad, pad_idx=self.eos_id)\n\n        slf_attn_mask_subseq = get_subsequent_mask(ys_in_pad)\n        slf_attn_mask_keypad = get_attn_key_pad_mask(seq_k=ys_in_pad,\n                                                     seq_q=ys_in_pad,\n                                                     pad_idx=self.eos_id)\n        slf_attn_mask = (slf_attn_mask_keypad + slf_attn_mask_subseq).gt(0)\n\n        output_length = ys_in_pad.size(1)\n        dec_enc_attn_mask = get_attn_pad_mask(encoder_padded_outputs,\n                                              encoder_input_lengths,\n                                              output_length)\n\n        # Forward\n        dec_output = self.dropout(self.tgt_word_emb(ys_in_pad) * self.x_logit_scale +\n                                  self.positional_encoding(ys_in_pad))\n\n        for dec_layer in self.layer_stack:\n            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n                dec_output, encoder_padded_outputs,\n                non_pad_mask=non_pad_mask,\n                slf_attn_mask=slf_attn_mask,\n                dec_enc_attn_mask=dec_enc_attn_mask)\n\n            if return_attns:\n                dec_slf_attn_list += [dec_slf_attn]\n                dec_enc_attn_list += [dec_enc_attn]\n\n        # before softmax\n        seq_logit = self.tgt_word_prj(dec_output)\n\n        # Return\n        pred, gold = seq_logit, ys_out_pad\n\n        if return_attns:\n            return pred, gold, dec_slf_attn_list, dec_enc_attn_list\n        return pred, gold\n\n\n    def recognize_beam(self, encoder_outputs, char_list, args):\n        """"""Beam search, decode one utterence now.\n        Args:\n            encoder_outputs: T x H\n            char_list: list of character\n            args: args.beam\n\n        Returns:\n            nbest_hyps:\n        """"""\n        # search params\n        beam = args.beam_size\n        nbest = args.nbest\n        if args.decode_max_len == 0:\n            maxlen = encoder_outputs.size(0)\n        else:\n            maxlen = args.decode_max_len\n\n        encoder_outputs = encoder_outputs.unsqueeze(0)\n\n        # prepare sos\n        ys = torch.ones(1, 1).fill_(self.sos_id).type_as(encoder_outputs).long()\n\n        # yseq: 1xT\n        hyp = {\'score\': 0.0, \'yseq\': ys}\n        hyps = [hyp]\n        ended_hyps = []\n\n        for i in range(maxlen):\n            hyps_best_kept = []\n            for hyp in hyps:\n                ys = hyp[\'yseq\']  # 1 x i\n\n                # -- Prepare masks\n                non_pad_mask = torch.ones_like(ys).float().unsqueeze(-1) # 1xix1\n                slf_attn_mask = get_subsequent_mask(ys)\n\n                # -- Forward\n                dec_output = self.dropout(\n                    self.tgt_word_emb(ys) * self.x_logit_scale +\n                    self.positional_encoding(ys))\n\n                for dec_layer in self.layer_stack:\n                    dec_output, _, _ = dec_layer(\n                        dec_output, encoder_outputs,\n                        non_pad_mask=non_pad_mask,\n                        slf_attn_mask=slf_attn_mask,\n                        dec_enc_attn_mask=None)\n\n                seq_logit = self.tgt_word_prj(dec_output[:, -1])\n\n                local_scores = F.log_softmax(seq_logit, dim=1)\n                # topk scores\n                local_best_scores, local_best_ids = torch.topk(\n                    local_scores, beam, dim=1)\n\n                for j in range(beam):\n                    new_hyp = {}\n                    new_hyp[\'score\'] = hyp[\'score\'] + local_best_scores[0, j]\n                    new_hyp[\'yseq\'] = torch.ones(1, (1+ys.size(1))).type_as(encoder_outputs).long()\n                    new_hyp[\'yseq\'][:, :ys.size(1)] = hyp[\'yseq\']\n                    new_hyp[\'yseq\'][:, ys.size(1)] = int(local_best_ids[0, j])\n                    # will be (2 x beam) hyps at most\n                    hyps_best_kept.append(new_hyp)\n\n\n                hyps_best_kept = sorted(hyps_best_kept,\n                                        key=lambda x: x[\'score\'],\n                                        reverse=True)[:beam]\n            # end for hyp in hyps\n            hyps = hyps_best_kept\n\n            # add eos in the final loop to avoid that there are no ended hyps\n            if i == maxlen - 1:\n                for hyp in hyps:\n                    hyp[\'yseq\'] = torch.cat([hyp[\'yseq\'],\n                                             torch.ones(1, 1).fill_(self.eos_id).type_as(encoder_outputs).long()], dim=1)\n\n            # add ended hypothes to a final list, and removed them from current hypothes\n            # (this will be a probmlem, number of hyps < beam)\n            remained_hyps = []\n            for hyp in hyps:\n                if hyp[\'yseq\'][0, -1] == self.eos_id:\n                    ended_hyps.append(hyp)\n                else:\n                    remained_hyps.append(hyp)\n\n            hyps = remained_hyps\n            if len(hyps) > 0:\n                print(\'remeined hypothes: \' + str(len(hyps)))\n            else:\n                print(\'no hypothesis. Finish decoding.\')\n                break\n\n            for hyp in hyps:\n                print(\'hypo: \' + \'\'.join([char_list[int(x)]\n                                          for x in hyp[\'yseq\'][0, 1:]]))\n        # end for i in range(maxlen)\n        nbest_hyps = sorted(ended_hyps, key=lambda x: x[\'score\'], reverse=True)[\n            :min(len(ended_hyps), nbest)]\n        # compitable with LAS implementation\n        for hyp in nbest_hyps:\n            hyp[\'yseq\'] = hyp[\'yseq\'][0].cpu().numpy().tolist()\n        return nbest_hyps\n\n\nclass DecoderLayer(nn.Module):\n    \'\'\' Compose with three layers \'\'\'\n\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(self, dec_input, enc_output, non_pad_mask=None, slf_attn_mask=None, dec_enc_attn_mask=None):\n        dec_output, dec_slf_attn = self.slf_attn(\n            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n        dec_output *= non_pad_mask\n\n        dec_output, dec_enc_attn = self.enc_attn(\n            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n        dec_output *= non_pad_mask\n\n        dec_output = self.pos_ffn(dec_output)\n        dec_output *= non_pad_mask\n\n        return dec_output, dec_slf_attn, dec_enc_attn\n'"
src/transformer/encoder.py,1,"b'import torch.nn as nn\n\nfrom attention import MultiHeadAttention\nfrom module import PositionalEncoding, PositionwiseFeedForward\nfrom utils import get_non_pad_mask, get_attn_pad_mask\n\n\nclass Encoder(nn.Module):\n    """"""Encoder of Transformer including self-attention and feed forward.\n    """"""\n\n    def __init__(self, d_input, n_layers, n_head, d_k, d_v,\n                 d_model, d_inner, dropout=0.1, pe_maxlen=5000):\n        super(Encoder, self).__init__()\n        # parameters\n        self.d_input = d_input\n        self.n_layers = n_layers\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout_rate = dropout\n        self.pe_maxlen = pe_maxlen\n\n        # use linear transformation with layer norm to replace input embedding\n        self.linear_in = nn.Linear(d_input, d_model)\n        self.layer_norm_in = nn.LayerNorm(d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_len=pe_maxlen)\n        self.dropout = nn.Dropout(dropout)\n\n        self.layer_stack = nn.ModuleList([\n            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n            for _ in range(n_layers)])\n\n    def forward(self, padded_input, input_lengths, return_attns=False):\n        """"""\n        Args:\n            padded_input: N x T x D\n            input_lengths: N\n\n        Returns:\n            enc_output: N x T x H\n        """"""\n        enc_slf_attn_list = []\n\n        # Prepare masks\n        non_pad_mask = get_non_pad_mask(padded_input, input_lengths=input_lengths)\n        length = padded_input.size(1)\n        slf_attn_mask = get_attn_pad_mask(padded_input, input_lengths, length)\n\n        # Forward\n        enc_output = self.dropout(\n            self.layer_norm_in(self.linear_in(padded_input)) +\n            self.positional_encoding(padded_input))\n\n        for enc_layer in self.layer_stack:\n            enc_output, enc_slf_attn = enc_layer(\n                enc_output,\n                non_pad_mask=non_pad_mask,\n                slf_attn_mask=slf_attn_mask)\n            if return_attns:\n                enc_slf_attn_list += [enc_slf_attn]\n\n        if return_attns:\n            return enc_output, enc_slf_attn_list\n        return enc_output,\n\n\nclass EncoderLayer(nn.Module):\n    """"""Compose with two sub-layers.\n        1. A multi-head self-attention mechanism\n        2. A simple, position-wise fully connected feed-forward network.\n    """"""\n\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(\n            n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(\n            d_model, d_inner, dropout=dropout)\n\n    def forward(self, enc_input, non_pad_mask=None, slf_attn_mask=None):\n        enc_output, enc_slf_attn = self.slf_attn(\n            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n        enc_output *= non_pad_mask\n\n        enc_output = self.pos_ffn(enc_output)\n        enc_output *= non_pad_mask\n\n        return enc_output, enc_slf_attn\n'"
src/transformer/loss.py,2,"b'import torch\nimport torch.nn.functional as F\n\nfrom utils import IGNORE_ID\n\n\ndef cal_performance(pred, gold, smoothing=0.0):\n    """"""Calculate cross entropy loss, apply label smoothing if needed.\n    Args:\n        pred: N x T x C, score before softmax\n        gold: N x T\n    """"""\n\n    pred = pred.view(-1, pred.size(2))\n    gold = gold.contiguous().view(-1)\n\n    loss = cal_loss(pred, gold, smoothing)\n\n    pred = pred.max(1)[1]\n    non_pad_mask = gold.ne(IGNORE_ID)\n    n_correct = pred.eq(gold)\n    n_correct = n_correct.masked_select(non_pad_mask).sum().item()\n\n    return loss, n_correct\n\ndef cal_loss(pred, gold, smoothing=0.0):\n    """"""Calculate cross entropy loss, apply label smoothing if needed.\n    """"""\n\n    if smoothing > 0.0:\n        eps = smoothing\n        n_class = pred.size(1)\n\n        # Generate one-hot matrix: N x C.\n        # Only label position is 1 and all other positions are 0\n        # gold include -1 value (IGNORE_ID) and this will lead to assert error\n        gold_for_scatter = gold.ne(IGNORE_ID).long() * gold\n        one_hot = torch.zeros_like(pred).scatter(1, gold_for_scatter.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / n_class\n        log_prb = F.log_softmax(pred, dim=1)\n\n        non_pad_mask = gold.ne(IGNORE_ID)\n        n_word = non_pad_mask.sum().item()\n        loss = -(one_hot * log_prb).sum(dim=1)\n        loss = loss.masked_select(non_pad_mask).sum() / n_word\n    else:\n        loss = F.cross_entropy(pred, gold,\n                               ignore_index=IGNORE_ID,\n                               reduction=\'elementwise_mean\')\n\n    return loss\n'"
src/transformer/module.py,7,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PositionalEncoding(nn.Module):\n    """"""Implement the positional encoding (PE) function.\n\n    PE(pos, 2i)   = sin(pos/(10000^(2i/dmodel)))\n    PE(pos, 2i+1) = cos(pos/(10000^(2i/dmodel)))\n    """"""\n\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model, requires_grad=False)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                             -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\'pe\', pe)\n\n    def forward(self, input):\n        """"""\n        Args:\n            input: N x T x D\n        """"""\n        length = input.size(1)\n        return self.pe[:, :length]\n\n\nclass PositionwiseFeedForward(nn.Module):\n    """"""Implements position-wise feedforward sublayer.\n\n    FFN(x) = max(0, xW1 + b1)W2 + b2\n    """"""\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        residual = x\n        output = self.w_2(F.relu(self.w_1(x)))\n        output = self.dropout(output)\n        output = self.layer_norm(output + residual)\n        return output\n\n\n# Another implementation\nclass PositionwiseFeedForwardUseConv(nn.Module):\n    """"""A two-feed-forward-layer module""""""\n\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super(PositionwiseFeedForwardUseConv, self).__init__()\n        self.w_1 = nn.Conv1d(d_in, d_hid, 1)  # position-wise\n        self.w_2 = nn.Conv1d(d_hid, d_in, 1)  # position-wise\n        self.layer_norm = nn.LayerNorm(d_in)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        residual = x\n        output = x.transpose(1, 2)\n        output = self.w_2(F.relu(self.w_1(output)))\n        output = output.transpose(1, 2)\n        output = self.dropout(output)\n        output = self.layer_norm(output + residual)\n        return output\n'"
src/transformer/optimizer.py,6,"b'""""""A wrapper class for optimizer""""""\nimport torch\n\n\nclass TransformerOptimizer(object):\n    """"""A simple wrapper class for learning rate scheduling""""""\n\n    def __init__(self, optimizer, k, d_model, warmup_steps=4000):\n        self.optimizer = optimizer\n        self.k = k\n        self.init_lr = d_model ** (-0.5)\n        self.warmup_steps = warmup_steps\n        self.step_num = 0\n        self.visdom_lr = None\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def step(self):\n        self._update_lr()\n        self._visdom()\n        self.optimizer.step()\n\n    def _update_lr(self):\n        self.step_num += 1\n        lr = self.k * self.init_lr * min(self.step_num ** (-0.5),\n                                         self.step_num * (self.warmup_steps ** (-1.5)))\n        for param_group in self.optimizer.param_groups:\n            param_group[\'lr\'] = lr\n\n    def load_state_dict(self, state_dict):\n        self.optimizer.load_state_dict(state_dict)\n\n    def state_dict(self):\n        return self.optimizer.state_dict()\n\n    def set_k(self, k):\n        self.k = k\n\n    def set_visdom(self, visdom_lr, vis):\n        self.visdom_lr = visdom_lr  # Turn on/off visdom of learning rate\n        self.vis = vis  # visdom enviroment\n        self.vis_opts = dict(title=\'Learning Rate\',\n                             ylabel=\'Leanring Rate\', xlabel=\'step\')\n        self.vis_window = None\n        self.x_axis = torch.LongTensor()\n        self.y_axis = torch.FloatTensor()\n\n    def _visdom(self):\n        if self.visdom_lr is not None:\n            self.x_axis = torch.cat(\n                [self.x_axis, torch.LongTensor([self.step_num])])\n            self.y_axis = torch.cat(\n                [self.y_axis, torch.FloatTensor([self.optimizer.param_groups[0][\'lr\']])])\n            if self.vis_window is None:\n                self.vis_window = self.vis.line(X=self.x_axis, Y=self.y_axis,\n                                                opts=self.vis_opts)\n            else:\n                self.vis.line(X=self.x_axis, Y=self.y_axis, win=self.vis_window,\n                              update=\'replace\')\n'"
src/transformer/transformer.py,2,"b'import torch\nimport torch.nn as nn\n\nfrom decoder import Decoder\nfrom encoder import Encoder\n\n\nclass Transformer(nn.Module):\n    """"""An encoder-decoder framework only includes attention.\n    """"""\n\n    def __init__(self, encoder, decoder):\n        super(Transformer, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, padded_input, input_lengths, padded_target):\n        """"""\n        Args:\n            padded_input: N x Ti x D\n            input_lengths: N\n            padded_targets: N x To\n        """"""\n        encoder_padded_outputs, *_ = self.encoder(padded_input, input_lengths)\n        # pred is score before softmax\n        pred, gold, *_ = self.decoder(padded_target, encoder_padded_outputs,\n                                      input_lengths)\n        return pred, gold\n\n    def recognize(self, input, input_length, char_list, args):\n        """"""Sequence-to-Sequence beam search, decode one utterence now.\n        Args:\n            input: T x D\n            char_list: list of characters\n            args: args.beam\n        Returns:\n            nbest_hyps:\n        """"""\n        encoder_outputs, *_ = self.encoder(input.unsqueeze(0), input_length)\n        nbest_hyps = self.decoder.recognize_beam(encoder_outputs[0],\n                                                 char_list,\n                                                 args)\n        return nbest_hyps\n\n    @classmethod\n    def load_model(cls, path):\n        # Load to CPU\n        package = torch.load(path, map_location=lambda storage, loc: storage)\n        model, LFR_m, LFR_n = cls.load_model_from_package(package)\n        return model, LFR_m, LFR_n\n\n    @classmethod\n    def load_model_from_package(cls, package):\n        encoder = Encoder(package[\'d_input\'],\n                          package[\'n_layers_enc\'],\n                          package[\'n_head\'],\n                          package[\'d_k\'],\n                          package[\'d_v\'],\n                          package[\'d_model\'],\n                          package[\'d_inner\'],\n                          dropout=package[\'dropout\'],\n                          pe_maxlen=package[\'pe_maxlen\'])\n        decoder = Decoder(package[\'sos_id\'],\n                          package[\'eos_id\'],\n                          package[\'vocab_size\'],\n                          package[\'d_word_vec\'],\n                          package[\'n_layers_dec\'],\n                          package[\'n_head\'],\n                          package[\'d_k\'],\n                          package[\'d_v\'],\n                          package[\'d_model\'],\n                          package[\'d_inner\'],\n                          dropout=package[\'dropout\'],\n                          tgt_emb_prj_weight_sharing=package[\'tgt_emb_prj_weight_sharing\'],\n                          pe_maxlen=package[\'pe_maxlen\'],\n                          )\n        model = cls(encoder, decoder)\n        model.load_state_dict(package[\'state_dict\'])\n        LFR_m, LFR_n = package[\'LFR_m\'], package[\'LFR_n\']\n        return model, LFR_m, LFR_n\n\n    @staticmethod\n    def serialize(model, optimizer, epoch, LFR_m, LFR_n, tr_loss=None, cv_loss=None):\n        package = {\n            # Low Frame Rate Feature\n            \'LFR_m\': LFR_m,\n            \'LFR_n\': LFR_n,\n            # encoder\n            \'d_input\': model.encoder.d_input,\n            \'n_layers_enc\': model.encoder.n_layers,\n            \'n_head\': model.encoder.n_head,\n            \'d_k\': model.encoder.d_k,\n            \'d_v\': model.encoder.d_v,\n            \'d_model\': model.encoder.d_model,\n            \'d_inner\': model.encoder.d_inner,\n            \'dropout\': model.encoder.dropout_rate,\n            \'pe_maxlen\': model.encoder.pe_maxlen,\n            # decoder\n            \'sos_id\': model.decoder.sos_id,\n            \'eos_id\': model.decoder.eos_id,\n            \'vocab_size\': model.decoder.n_tgt_vocab,\n            \'d_word_vec\': model.decoder.d_word_vec,\n            \'n_layers_dec\': model.decoder.n_layers,\n            \'tgt_emb_prj_weight_sharing\': model.decoder.tgt_emb_prj_weight_sharing,\n            # state\n            \'state_dict\': model.state_dict(),\n            \'optim_dict\': optimizer.state_dict(),\n            \'epoch\': epoch\n        }\n        if tr_loss is not None:\n            package[\'tr_loss\'] = tr_loss\n            package[\'cv_loss\'] = cv_loss\n        return package\n'"
src/utils/__init__.py,0,b''
src/utils/filt.py,0,"b'#!/usr/bin/env python\n\n# Apache 2.0\n\nimport sys\nimport argparse\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--exclude\', \'-v\', dest=\'exclude\',\n                        action=\'store_true\', help=\'exclude filter words\')\n    parser.add_argument(\'filt\', type=str, help=\'filter list\')\n    parser.add_argument(\'infile\', type=str, help=\'input file\')\n    args = parser.parse_args()\n\n    vocab = set()\n    with open(args.filt) as vocabfile:\n        for line in vocabfile:\n            vocab.add(line.strip())\n\n    with open(args.infile) as textfile:\n        for line in textfile:\n            if args.exclude:\n                print("" "".join(\n                    map(lambda word: word if not word in vocab else \'\', line.strip().split())))\n            # else:\n            #     print("" "".join(map(lambda word: word if word in vocab else \'<UNK>\', unicode(line, \'utf_8\').strip().split())).encode(\'utf_8\'))\n'"
src/utils/json2trn.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport json\nimport argparse\nimport logging\nfrom utils import process_dict\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'json\', type=str, help=\'json files\')\n    parser.add_argument(\'dict\', type=str, help=\'dict\')\n    parser.add_argument(\'ref\', type=str, help=\'ref\')\n    parser.add_argument(\'hyp\', type=str, help=\'hyp\')\n    args = parser.parse_args()\n\n    # logging info\n    logging.basicConfig(\n        level=logging.INFO, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n\n    logging.info(""reading %s"", args.json)\n    with open(args.json, \'r\') as f:\n        j = json.load(f)\n\n    logging.info(""reading %s"", args.dict)\n    char_list, sos_id, eos_id = process_dict(args.dict)\n    # with open(args.dict, \'r\') as f:\n    #     dictionary = f.readlines()\n    # char_list = [unicode(entry.split(\' \')[0], \'utf_8\') for entry in dictionary]\n    # char_list.insert(0, \'<blank>\')\n    # char_list.append(\'<eos>\')\n    # print([x.encode(\'utf-8\') for x in char_list])\n\n    logging.info(""writing hyp trn to %s"", args.hyp)\n    logging.info(""writing ref trn to %s"", args.ref)\n    h = open(args.hyp, \'w\')\n    r = open(args.ref, \'w\')\n\n    for x in j[\'utts\']:\n        seq = [char_list[int(i)] for i in j[\'utts\'][x]\n               [\'output\'][0][\'rec_tokenid\'].split()]\n        h.write("" "".join(seq).replace(\'<eos>\', \'\')),\n        h.write(\n            "" ("" + j[\'utts\'][x][\'utt2spk\'].replace(\'-\', \'_\') + ""-"" + x + "")\\n"")\n\n        seq = [char_list[int(i)] for i in j[\'utts\'][x]\n               [\'output\'][0][\'tokenid\'].split()]\n        r.write("" "".join(seq).replace(\'<eos>\', \'\')),\n        r.write(\n            "" ("" + j[\'utts\'][x][\'utt2spk\'].replace(\'-\', \'_\') + ""-"" + x + "")\\n"")\n'"
src/utils/mergejson.py,0,"b'#!/usr/bin/env python2\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport json\nimport logging\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'jsons\', type=str, nargs=\'+\',\n                        help=\'json files\')\n    parser.add_argument(\'--multi\', \'-m\', type=int,\n                        help=\'Test the json file for multiple input/output\', default=0)\n    parser.add_argument(\'--verbose\', \'-V\', default=0, type=int,\n                        help=\'Verbose option\')\n    args = parser.parse_args()\n    \n    # logging info\n    if args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n    else:\n        logging.basicConfig(\n            level=logging.WARN, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n\n    # make intersection set for utterance keys\n    js = []\n    intersec_ks = []\n    for x in args.jsons:\n        with open(x, \'r\') as f:\n            j = json.load(f)\n        ks = j[\'utts\'].keys()\n        logging.info(x + \': has \' + str(len(ks)) + \' utterances\')\n        if len(intersec_ks) > 0:\n            intersec_ks = intersec_ks.intersection(set(ks))\n        else:\n            intersec_ks = set(ks)\n        js.append(j)\n    logging.info(\'new json has \' + str(len(intersec_ks)) + \' utterances\')\n        \n    old_dic = dict()\n    for k in intersec_ks:\n        v = js[0][\'utts\'][k]\n        for j in js[1:]:\n            v.update(j[\'utts\'][k])\n        old_dic[k] = v\n\n    new_dic = dict()\n    for id in old_dic:\n        dic = old_dic[id]\n\n        in_dic = {}\n        if dic.has_key(unicode(\'idim\', \'utf-8\')):\n            in_dic[unicode(\'shape\', \'utf-8\')] = (int(dic[unicode(\'ilen\', \'utf-8\')]), int(dic[unicode(\'idim\', \'utf-8\')]))\n        in_dic[unicode(\'name\', \'utf-8\')] = unicode(\'input1\', \'utf-8\')\n        in_dic[unicode(\'feat\', \'utf-8\')] = dic[unicode(\'feat\', \'utf-8\')]\n\n        out_dic = {}\n        out_dic[unicode(\'name\', \'utf-8\')] = unicode(\'target1\', \'utf-8\')\n        out_dic[unicode(\'shape\', \'utf-8\')] = (int(dic[unicode(\'olen\', \'utf-8\')]), int(dic[unicode(\'odim\', \'utf-8\')]))\n        out_dic[unicode(\'text\', \'utf-8\')] = dic[unicode(\'text\', \'utf-8\')]\n        out_dic[unicode(\'token\', \'utf-8\')] = dic[unicode(\'token\', \'utf-8\')]\n        out_dic[unicode(\'tokenid\', \'utf-8\')] = dic[unicode(\'tokenid\', \'utf-8\')]\n\n\n        new_dic[id] = {unicode(\'input\', \'utf-8\'):[in_dic], unicode(\'output\', \'utf-8\'):[out_dic],\n            unicode(\'utt2spk\', \'utf-8\'):dic[unicode(\'utt2spk\', \'utf-8\')]}\n    \n    # ensure ""ensure_ascii=False"", which is a bug\n    jsonstring = json.dumps({\'utts\': new_dic}, indent=4, ensure_ascii=False, sort_keys=True).encode(\'utf_8\')\n    print(jsonstring)\n'"
src/utils/scp2json.py,0,"b'#!/usr/bin/env python2\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport sys\nimport json\nimport argparse\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--key\', \'-k\', type=str,\n                        help=\'key\')\n    args = parser.parse_args()\n\n    l = {}\n    line = sys.stdin.readline()\n    while line:\n        x = unicode(line, \'utf_8\').rstrip().split()\n        v = {args.key: \' \'.join(x[1:]).encode(\'utf_8\')}\n        l[x[0].encode(\'utf_8\')] = v\n        line = sys.stdin.readline()\n\n    all_l = {\'utts\': l}\n\n    # ensure ""ensure_ascii=False"", which is a bug\n    jsonstring = json.dumps(all_l, indent=4, ensure_ascii=False)\n    print(jsonstring)\n'"
src/utils/text2token.py,0,"b'#!/usr/bin/env python2\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport sys\nimport argparse\nimport re\n\n\ndef exist_or_not(i, match_pos):\n    start_pos = None\n    end_pos = None\n    for pos in match_pos:\n        if pos[0] <= i < pos[1]:\n            start_pos = pos[0]\n            end_pos = pos[1]\n            break\n\n    return start_pos, end_pos\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--nchar\', \'-n\', default=1, type=int,\n                        help=\'number of characters to split, i.e., \\\n                        aabb -> a a b b with -n 1 and aa bb with -n 2\')\n    parser.add_argument(\'--skip-ncols\', \'-s\', default=0, type=int,\n                        help=\'skip first n columns\')\n    parser.add_argument(\'--space\', default=\'<space>\', type=str,\n                        help=\'space symbol\')\n    parser.add_argument(\'--non-lang-syms\', \'-l\', default=None, type=str,\n                        help=\'list of non-linguistic symobles, e.g., <NOISE> etc.\')\n    parser.add_argument(\'text\', type=str, default=False, nargs=\'?\',\n                        help=\'input text\')\n    args = parser.parse_args()\n\n    rs = []\n    if args.non_lang_syms is not None:\n        with open(args.non_lang_syms, \'r\') as f:\n            nls = [unicode(x.rstrip(), \'utf_8\') for x in f.readlines()]\n            rs = [re.compile(re.escape(x)) for x in nls]\n\n    if args.text:\n        f = open(args.text)\n    else:\n        f = sys.stdin\n    line = f.readline()\n    n = args.nchar\n    while line:\n        x = unicode(line, \'utf_8\').split()\n        print \' \'.join(x[:args.skip_ncols]).encode(\'utf_8\'),\n        a = \' \'.join(x[args.skip_ncols:])\n\n        # get all matched positions\n        match_pos = []\n        for r in rs:\n            i = 0\n            while i >= 0:\n                m = r.search(a, i)\n                if m:\n                    match_pos.append([m.start(), m.end()])\n                    i = m.end()\n                else:\n                    break\n\n        if len(match_pos) > 0:\n            chars = []\n            i = 0\n            while i < len(a):\n                start_pos, end_pos = exist_or_not(i, match_pos)\n                if start_pos is not None:\n                    chars.append(a[start_pos:end_pos])\n                    i = end_pos\n                else:\n                    chars.append(a[i])\n                    i += 1\n            a = chars\n\n        a = [a[i:i + n] for i in range(0, len(a), n)]\n\n        a_flat = []\n        for z in a:\n            a_flat.append("""".join(z))\n\n        a_chars = [z.replace(\' \', args.space) for z in a_flat]\n        print \' \'.join(a_chars).encode(\'utf_8\')\n        line = f.readline()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
src/utils/utils.py,2,"b'#!/usr/bin/env python3\nIGNORE_ID = -1\n\n\ndef pad_list(xs, pad_value):\n    # From: espnet/src/nets/e2e_asr_th.py: pad_list()\n    n_batch = len(xs)\n    max_len = max(x.size(0) for x in xs)\n    pad = xs[0].new(n_batch, max_len, * xs[0].size()[1:]).fill_(pad_value)\n    for i in range(n_batch):\n        pad[i, :xs[i].size(0)] = xs[i]\n    return pad\n\n\ndef process_dict(dict_path):\n    with open(dict_path, \'rb\') as f:\n        dictionary = f.readlines()\n    char_list = [entry.decode(\'utf-8\').split(\' \')[0]\n                 for entry in dictionary]\n    sos_id = char_list.index(\'<sos>\')\n    eos_id = char_list.index(\'<eos>\')\n    return char_list, sos_id, eos_id\n\n\nif __name__ == ""__main__"":\n    import sys\n    path = sys.argv[1]\n    char_list, sos_id, eos_id = process_dict(path)\n    print(char_list, sos_id, eos_id)\n\n# * ------------------ recognition related ------------------ *\n\n\ndef parse_hypothesis(hyp, char_list):\n    """"""Function to parse hypothesis\n\n    :param list hyp: recognition hypothesis\n    :param list char_list: list of characters\n    :return: recognition text strinig\n    :return: recognition token strinig\n    :return: recognition tokenid string\n    """"""\n    # remove sos and get results\n    tokenid_as_list = list(map(int, hyp[\'yseq\'][1:]))\n    token_as_list = [char_list[idx] for idx in tokenid_as_list]\n    score = float(hyp[\'score\'])\n\n    # convert to string\n    tokenid = "" "".join([str(idx) for idx in tokenid_as_list])\n    token = "" "".join(token_as_list)\n    text = """".join(token_as_list).replace(\'<space>\', \' \')\n\n    return text, token, tokenid, score\n\n\ndef add_results_to_json(js, nbest_hyps, char_list):\n    """"""Function to add N-best results to json\n\n    :param dict js: groundtruth utterance dict\n    :param list nbest_hyps: list of hypothesis\n    :param list char_list: list of characters\n    :return: N-best results added utterance dict\n    """"""\n    # copy old json info\n    new_js = dict()\n    new_js[\'utt2spk\'] = js[\'utt2spk\']\n    new_js[\'output\'] = []\n\n    for n, hyp in enumerate(nbest_hyps, 1):\n        # parse hypothesis\n        rec_text, rec_token, rec_tokenid, score = parse_hypothesis(\n            hyp, char_list)\n\n        # copy ground-truth\n        out_dic = dict(js[\'output\'][0].items())\n\n        # update name\n        out_dic[\'name\'] += \'[%d]\' % n\n\n        # add recognition results\n        out_dic[\'rec_text\'] = rec_text\n        out_dic[\'rec_token\'] = rec_token\n        out_dic[\'rec_tokenid\'] = rec_tokenid\n        out_dic[\'score\'] = score\n\n        # add to list of N-best result dicts\n        new_js[\'output\'].append(out_dic)\n\n        # show 1-best result\n        if n == 1:\n            print(\'groundtruth: %s\' % out_dic[\'text\'])\n            print(\'prediction : %s\' % out_dic[\'rec_text\'])\n\n    return new_js\n\n\n# -- Transformer Related --\nimport torch\n\ndef get_non_pad_mask(padded_input, input_lengths=None, pad_idx=None):\n    """"""padding position is set to 0, either use input_lengths or pad_idx\n    """"""\n    assert input_lengths is not None or pad_idx is not None\n    if input_lengths is not None:\n        # padded_input: N x T x ..\n        N = padded_input.size(0)\n        non_pad_mask = padded_input.new_ones(padded_input.size()[:-1])  # N x T\n        for i in range(N):\n            non_pad_mask[i, input_lengths[i]:] = 0\n    if pad_idx is not None:\n        # padded_input: N x T\n        assert padded_input.dim() == 2\n        non_pad_mask = padded_input.ne(pad_idx).float()\n    # unsqueeze(-1) for broadcast\n    return non_pad_mask.unsqueeze(-1)\n\ndef get_subsequent_mask(seq):\n    \'\'\' For masking out the subsequent info. \'\'\'\n\n    sz_b, len_s = seq.size()\n    subsequent_mask = torch.triu(\n        torch.ones((len_s, len_s), device=seq.device, dtype=torch.uint8), diagonal=1)\n    subsequent_mask = subsequent_mask.unsqueeze(0).expand(sz_b, -1, -1)  # b x ls x ls\n\n    return subsequent_mask\n\ndef get_attn_key_pad_mask(seq_k, seq_q, pad_idx):\n    \'\'\' For masking out the padding part of key sequence. \'\'\'\n\n    # Expand to fit the shape of key query attention matrix.\n    len_q = seq_q.size(1)\n    padding_mask = seq_k.eq(pad_idx)\n    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n\n    return padding_mask\n\ndef get_attn_pad_mask(padded_input, input_lengths, expand_length):\n    """"""mask position is set to 1""""""\n    # N x Ti x 1\n    non_pad_mask = get_non_pad_mask(padded_input, input_lengths=input_lengths)\n    # N x Ti, lt(1) like not operation\n    pad_mask = non_pad_mask.squeeze(-1).lt(1)\n    attn_mask = pad_mask.unsqueeze(1).expand(-1, expand_length, -1)\n    return attn_mask\n'"
