file_path,api_count,code
setup.py,1,"b'""""""\ntorchgpipe\n==========\n\nA GPipe_ implementation in PyTorch_.\n\n.. _GPipe: https://arxiv.org/abs/1811.06965\n.. _PyTorch: https://pytorch.org/\n\n.. sourcecode:: python\n\n   from torchgpipe import GPipe\n\n   model = nn.Sequential(a, b, c, d)\n   model = GPipe(model, balance=[1, 1, 1, 1], chunks=8)\n\n   for input in data_loader:\n       output = model(input)\n\nWhat is GPipe?\n~~~~~~~~~~~~~~\n\nGPipe is a scalable pipeline parallelism library published by Google Brain,\nwhich allows efficient training of large, memory-consuming models. According to\nthe paper, GPipe can train a 25x larger model by using 8x devices (TPU), and\ntrain a model 3.5x faster by using 4x devices.\n\n`GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism\n<https://arxiv.org/abs/1811.06965>`_\n\nGoogle trained AmoebaNet-B with 557M parameters over GPipe. This model has\nachieved 84.3% top-1 and 97.0% top-5 accuracy on ImageNet classification\nbenchmark (the state-of-the-art performance as of May 2019).\n\nLinks\n~~~~~\n\n- Source Code: https://github.com/kakaobrain/torchgpipe\n- Documentation: https://torchgpipe.readthedocs.io/\n- Original Paper: https://arxiv.org/abs/1811.06965\n\n""""""\nfrom setuptools import setup\n\n\nabout = {}  # type: ignore\nwith open(\'torchgpipe/__version__.py\') as f:\n    exec(f.read(), about)  # pylint: disable=W0122\nversion = about[\'__version__\']\ndel about\n\n\nsetup(\n    name=\'torchgpipe\',\n\n    version=version,\n\n    license=\'Apache License 2.0\',\n    url=\'https://github.com/kakaobrain/torchgpipe\',\n    author=\'Kakao Brain\',\n    maintainer=\'Heungsub Lee, Myungryong Jeong, Chiheon Kim\',\n\n    description=\'GPipe for PyTorch\',\n    long_description=__doc__,\n    keywords=\'pytorch gpipe\',\n\n    zip_safe=False,\n\n    packages=[\'torchgpipe\', \'torchgpipe.balance\', \'torchgpipe.skip\'],\n    package_data={\'torchgpipe\': [\'py.typed\']},\n    py_modules=[\'torchgpipe_balancing\'],\n\n    install_requires=[\'torch>=1.1\'],\n    setup_requires=[\'pytest-runner\'],\n    tests_require=[\'pytest>=4\'],\n\n    classifiers=[\n        \'Development Status :: 3 - Alpha\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3 :: Only\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Typing :: Typed\',\n    ],\n)\n'"
torchgpipe_balancing.py,0,"b'# \'torchgpipe_balancing\' has moved to \'torchgpipe.balance\' in v0.0.5.\nraise ImportError(""import \'torchgpipe.balance\' instead"")\n'"
docs/conf.py,1,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'torchgpipe\'\ncopyright = \'2019, Kakao Brain\'\nauthor = \'Kakao Brain\'\n\n# The full version, including alpha/beta/rc tags\nabout = {}\nwith open(\'../torchgpipe/__version__.py\') as f:\n    exec(f.read(), about)\nrelease = about[\'__version__\']\ndel about\n\nmaster_doc = \'index\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    # We follow Google style docstrings just like PyTorch.\n    \'sphinx.ext.napoleon\',\n\n    # Allow reference sections using its title.\n    \'sphinx.ext.autosectionlabel\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# Link to PyTorch\'s documentation.\nextensions.append(\'sphinx.ext.intersphinx\')\nintersphinx_mapping = {\n    \'torch\': (\'https://pytorch.org/docs/stable/\', None),\n    \'numpy\': (\'https://numpy.org/devdocs/\', None),\n    \'python\': (\'https://docs.python.org/3\', None),\n}\n\n# Mock up \'torch\' to make sure build on Read the Docs.\nautodoc_mock_imports = [\'torch\']\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\nhtml_theme_options = {\n    \'logo\': \'not-pipe.svg\',\n    \'logo_name\': True,\n    \'description\': \'GPipe for PyTorch\',\n\n    \'github_user\': \'kakaobrain\',\n    \'github_repo\': \'torchgpipe\',\n    \'github_type\': \'star\',\n\n    \'extra_nav_links\': {\n        \'Source Code\': \'https://github.com/kakaobrain/torchgpipe\',\n        \'Technical Report\': \'https://arxiv.org/abs/2004.09910\',\n        \'Original Paper\': \'https://arxiv.org/abs/1811.06965\',\n    },\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n'"
tests/__init__.py,0,b'# tests/__init__.py makes pytest can import the application without custom sys.path or PYTHONPATH.\n# See also: https://docs.pytest.org/en/latest/goodpractices.html\n'
tests/conftest.py,7,"b""import pytest\nimport torch\n\n\n@pytest.fixture(autouse=True)\ndef manual_seed_zero():\n    torch.manual_seed(0)\n\n\n@pytest.fixture(scope='session')\ndef cuda_sleep():\n    # Warm-up CUDA.\n    torch.empty(1, device='cuda')\n\n    # From test/test_cuda.py in PyTorch.\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    torch.cuda._sleep(1000000)\n    end.record()\n    end.synchronize()\n    cycles_per_ms = 1000000 / start.elapsed_time(end)\n\n    def cuda_sleep(seconds):\n        torch.cuda._sleep(int(seconds * cycles_per_ms * 1000))\n    return cuda_sleep\n\n\ndef pytest_report_header():\n    return f'torch: {torch.__version__}'\n"""
tests/test_balance.py,17,"b""import time\n\nimport pytest\nimport torch\nfrom torch import nn\n\nfrom torchgpipe.balance import balance_by_size, balance_by_time, blockpartition\nfrom torchgpipe.balance.profile import layerwise_sandbox\n\nskip_if_no_cuda = pytest.mark.skipif(not torch.cuda.is_available(), reason='cuda required')\n\ndevices = ['cpu']\nif torch.cuda.is_available():\n    devices.append('cuda')\n\n\ndef test_blockpartition():\n    assert blockpartition.solve([1, 2, 3, 4, 5, 6], partitions=2) == [[1, 2, 3, 4], [5, 6]]\n\n\ndef test_blockpartition_zeros():\n    assert blockpartition.solve([0, 0], partitions=2) == [[0], [0]]\n\n\ndef test_blockpartition_non_positive_partitions():\n    with pytest.raises(ValueError):\n        blockpartition.solve([42], partitions=0)\n    with pytest.raises(ValueError):\n        blockpartition.solve([42], partitions=-1)\n\n\ndef test_blockpartition_short_sequence():\n    with pytest.raises(ValueError):\n        blockpartition.solve([], partitions=1)\n    with pytest.raises(ValueError):\n        blockpartition.solve([42], partitions=2)\n\n\n@pytest.mark.parametrize('device', devices)\ndef test_balance_by_time(device):\n    class Delay(nn.Module):\n        def __init__(self, seconds):\n            super().__init__()\n            self.seconds = seconds\n\n        def forward(self, x):\n            time.sleep(self.seconds)\n            return x\n\n    model = nn.Sequential(*[Delay(i/100) for i in [1, 2, 3, 4, 5, 6]])\n    sample = torch.rand(1)\n    balance = balance_by_time(2, model, sample, device=device)\n    assert balance == [4, 2]\n\n\ndef test_balance_by_time_loop_resets_input():\n    # nn.Flatten was introduced at PyTorch 1.2.0.\n    class Flatten(nn.Module):\n        def forward(self, x):\n            return x.flatten(1)\n\n    model = nn.Sequential(nn.Conv2d(3, 2, 1), Flatten(), nn.Linear(128, 10))\n    sample = torch.rand(10, 3, 8, 8)\n    balance = balance_by_time(2, model, sample, device='cpu')\n    assert balance == [1, 2]\n\n\n@skip_if_no_cuda\ndef test_balance_by_size_latent():\n    class Expand(nn.Module):\n        def __init__(self, times):\n            super().__init__()\n            self.times = times\n\n        def forward(self, x):\n            for i in range(self.times):\n                x = x + torch.rand_like(x, requires_grad=True)\n            return x\n\n    sample = torch.rand(10, 100, 100)\n\n    model = nn.Sequential(*[Expand(i) for i in [1, 2, 3, 4, 5, 6]])\n    balance = balance_by_size(2, model, sample)\n    assert balance == [4, 2]\n\n    model = nn.Sequential(*[Expand(i) for i in [6, 5, 4, 3, 2, 1]])\n    balance = balance_by_size(2, model, sample)\n    assert balance == [2, 4]\n\n\n@skip_if_no_cuda\ndef test_balance_by_size_param():\n    model = nn.Sequential(*[nn.Linear(i+1, i+2) for i in range(6)])\n    sample = torch.rand(7, 1)\n    balance = balance_by_size(2, model, sample, param_scale=100)\n    assert balance == [4, 2]\n\n    model = nn.Sequential(*[nn.Linear(i+2, i+1) for i in reversed(range(6))])\n    sample = torch.rand(1, 7)\n    balance = balance_by_size(2, model, sample, param_scale=100)\n    assert balance == [2, 4]\n\n\n@skip_if_no_cuda\ndef test_balance_by_size_param_scale():\n    class Tradeoff(nn.Module):\n        def __init__(self, param_size, latent_size):\n            super().__init__()\n            self.fc = nn.Linear(param_size, param_size)\n            self.latent_size = latent_size\n\n        def forward(self, x):\n            for i in range(self.latent_size):\n                x = x + torch.rand_like(x, requires_grad=True)\n            return x\n\n    model = nn.Sequential(\n        Tradeoff(param_size=1, latent_size=6),\n        Tradeoff(param_size=2, latent_size=5),\n        Tradeoff(param_size=3, latent_size=4),\n        Tradeoff(param_size=4, latent_size=3),\n        Tradeoff(param_size=5, latent_size=2),\n        Tradeoff(param_size=6, latent_size=1),\n    )\n\n    sample = torch.rand(1, requires_grad=True)\n\n    balance = balance_by_size(2, model, sample, param_scale=0)\n    assert balance == [2, 4]\n\n    balance = balance_by_size(2, model, sample, param_scale=100)\n    assert balance == [4, 2]\n\n\n@pytest.mark.parametrize('device', devices)\ndef test_layerwise_sandbox(device):\n    model = nn.Sequential(nn.Conv2d(3, 3, 1), nn.BatchNorm2d(3))\n    model.eval()\n\n    for layer in layerwise_sandbox(model, torch.device(device)):\n        assert layer.training\n        assert all(p.device.type == device for p in layer.parameters())\n\n    assert all(not l.training for l in model)\n    assert all(p.device.type == 'cpu' for p in model.parameters())\n\n\n@pytest.mark.parametrize('device', devices)\ndef test_sandbox_during_profiling(device):\n    model = nn.Sequential(nn.BatchNorm2d(3))\n\n    before = {k: v.clone() for k, v in model.state_dict().items()}\n\n    sample = torch.rand(1, 3, 10, 10)\n    balance_by_time(1, model, sample, device=device)\n\n    after = model.state_dict()\n\n    assert before.keys() == after.keys()\n    for key, value in before.items():\n        assert torch.allclose(after[key], value), key\n\n\ndef test_not_training():\n    class AssertTraining(nn.Module):\n        def forward(self, x):\n            assert self.training\n            return x\n    model = nn.Sequential(AssertTraining())\n\n    model.eval()\n    assert not model.training\n\n    sample = torch.rand(1)\n    balance_by_time(1, model, sample, device='cpu')\n\n    assert not model.training\n\n\ndef test_deprecated_torchgpipe_balancing():\n    with pytest.raises(ImportError, match='torchgpipe.balance'):\n        __import__('torchgpipe_balancing')\n\n\ndef test_balance_by_time_tuple():\n    class Twin(nn.Module):\n        def forward(self, x):\n            return x, x.detach()\n\n    class Add(nn.Module):\n        def forward(self, a_b):\n            a, b = a_b\n            return a + b\n\n    model = nn.Sequential(Twin(), Add())\n    sample = torch.rand(1, requires_grad=True)\n    balance_by_time(1, model, sample, device='cpu')\n\n\n@skip_if_no_cuda\ndef test_balance_by_size_tuple():\n    class Twin(nn.Module):\n        def forward(self, x):\n            return x, x.detach()\n\n    class Add(nn.Module):\n        def forward(self, a_b):\n            a, b = a_b\n            return a + b\n\n    model = nn.Sequential(Twin(), Add())\n    sample = torch.rand(1, requires_grad=True)\n    balance_by_size(1, model, sample)\n\n\ndef test_already_has_grad():\n    model = nn.Sequential(nn.Conv2d(3, 3, 1))\n    sample = torch.rand(1, 3, 32, 32)\n    model(sample).norm().backward()\n\n    with pytest.raises(ValueError, match='some parameter already has gradient'):\n        balance_by_time(1, model, sample, device='cpu')\n"""
tests/test_bugs.py,15,"b""import pytest\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom torchgpipe import GPipe\n\n\ndef test_python_autograd_function():\n    # A Python autograd function might fail with this error:\n    #\n    #   RuntimeError: Returning Variables sharing storage with other Variables\n    #   that require grad is not supported in Python functions. Please submit a\n    #   feature request if you hit this error.\n    #\n    # It doesn't look like an essential restriction. But it happens on the\n    # current PyTorch version. To avoid it, we should detach the tensor before\n    # returning by identity autograd functions, such as Wait, Fork, and Join.\n    #\n    class Identity(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, input):\n            return input\n\n        @staticmethod\n        def backward(ctx, grad):\n            return grad\n\n    class M(nn.Module):\n        def forward(self, input):\n            return Identity.apply(input)\n\n    model = nn.Sequential(M(), M())\n    model = GPipe(model, [1, 1], devices=['cpu', 'cpu'], checkpoint='always')\n\n    x = torch.rand(42)\n    y = model(x)\n    assert torch.allclose(x, y)\n\n\ndef test_exception_no_hang():\n    # In v0.0.2, once a failed partition receives a normal message\n    # (non-closing) for the next micro-batch, a hang occured. The reason was\n    # that a failed partition didn't call in_queue.task_done() on a normal\n    # message. So the former partition was blocked at out_queue.join() for the\n    # next of next micro-batch.\n    class ExpectedException(Exception):\n        pass\n\n    class Pass(nn.Module):\n        def forward(self, x):\n            return x\n\n    class Raise(nn.Module):\n        def forward(self, x):\n            raise ExpectedException()\n\n    model = nn.Sequential(Pass(), Pass(), Raise())\n    model = GPipe(model, [1, 1, 1], devices=['cpu', 'cpu', 'cpu'], chunks=3)\n\n    with pytest.raises(ExpectedException):\n        model(torch.rand(3))\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason='2 cuda devices required')\ndef test_tuple_wait(cuda_sleep):\n    # In v0.0.3, Wait is applied to only the first tensor on a micro-batch.\n    # Under this behavior, if checkpointing was disabled, there's a possibility\n    # that gradient accumulations on other tensors are not synchronized\n    # properly to the copy stream.\n    class Sleep(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, x):\n            return x.detach()\n\n        @staticmethod\n        def backward(ctx, grad):\n            with torch.cuda.device(grad.device):\n                cuda_sleep(0.05)\n            return grad\n\n    class Layer1(nn.Module):\n        def forward(self, pair):\n            a, b = pair\n            return a*1, b*2, b*3\n\n    class Layer2(nn.Module):\n        def forward(self, triple):\n            a, b, c = triple\n            b = Sleep.apply(b)\n            return a+b+c\n\n    model = nn.Sequential(Layer1(), Layer2())\n    model = GPipe(model, [1, 1], devices=[0, 1], chunks=32, checkpoint='never')\n\n    a = torch.rand(1024, 3, 32, 32, device=0, requires_grad=True)\n    b = torch.rand(1024, 3, 32, 32, device=0, requires_grad=True)\n\n    y = model((a, b))\n    y.norm().backward()\n\n    torch.cuda.synchronize(0)\n    torch.cuda.synchronize(1)\n\n    assert torch.isclose(b.grad.norm().cpu(), torch.tensor(5.000))\n\n\ndef test_parallel_randoms():\n    class Dropouts(nn.Module):\n        def forward(self, x):\n            for _ in range(100):\n                x = F.dropout(x, p=0.001)\n            return x\n\n    model = nn.Sequential(Dropouts(), Dropouts())\n\n    x = torch.rand(10, 10, requires_grad=True)\n    model = GPipe(model, [1, 1], devices=['cpu', 'cpu'], chunks=10, checkpoint='always')\n    y = model(x)\n    y.norm().backward()\n\n    assert y.to(torch.bool).tolist() == x.grad.to(torch.bool).tolist()\n"""
tests/test_checkpoint.py,16,"b""from functools import partial\n\nimport pytest\nimport torch\nfrom torch import nn\nimport torch.cuda\n\nfrom torchgpipe.checkpoint import Checkpointing, checkpoint, is_checkpointing, is_recomputing\nfrom torchgpipe.dependency import fork, join\nfrom torchgpipe.microbatch import Batch\n\ndevices = ['cpu']\nif torch.cuda.is_available():\n    devices.append('cuda')\n\n\n@pytest.mark.parametrize('device', devices)\ndef test_serial_checkpoints(device):\n    # Copied from https://github.com/pytorch/pytorch/pull/18568.\n    timeline = []\n\n    class Log(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, name, x):\n            ctx.name = name\n            timeline.append(f'{name}:forward')\n            return x.detach()\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            name = ctx.name\n            timeline.append(f'{name}:backward')\n            return None, grad_output\n\n    a = torch.rand(1, device=device, requires_grad=True)\n    b = torch.rand(1, device=device, requires_grad=True)\n\n    # Increase the next function sequence number.\n    _ = a + 1 + 2 + 3 + 4 + 5\n\n    a = checkpoint(partial(Log.apply, 'a'), a)\n\n    a, phony = fork(a)\n    b = join(b, phony)\n\n    b = checkpoint(partial(Log.apply, 'b'), b)\n\n    c = torch.cat((a, b))\n\n    out = c.sum()\n\n    #                        +--> {a} --Checkpoint(Log)--> {a}\n    # {out} --Sum--> {c} --Cat     ^-----------------------------+\n    #                        +--> {b} --Checkpoint(Log)--> {b} --First--> {b}\n    out.backward()\n\n    assert timeline == \\\n        ['a:forward', 'b:forward', 'b:forward', 'b:backward', 'a:forward', 'a:backward']\n    #    |----------------------|  |-----------------------|  |-----------------------|\n    #          forward pass            Checkpoint(Log[b])         Checkpoint(Log[a])\n\n\ndef test_not_requires_grad():\n    x = Batch(torch.rand(1, requires_grad=False))\n    assert not x[0].requires_grad\n\n    def f(x):\n        return x * 2\n\n    chk = Checkpointing(f, x)\n    x = chk.checkpoint()\n    assert x[0].requires_grad\n\n    chk.recompute(x)\n    assert x[0].requires_grad\n\n    x.tensor.backward()\n\n\ndef test_not_requires_grad_with_parameter():\n    x = torch.rand(1, requires_grad=False)\n    a = torch.rand(1, requires_grad=True)\n\n    def f(x):\n        return x * a\n\n    y = checkpoint(f, x)\n    y.backward()\n\n    assert a.grad is not None\n\n\n@pytest.mark.parametrize('device', devices)\ndef test_random_in_checkpoint(device):\n    dropout = nn.Dropout(p=0.5)\n\n    torch.manual_seed(0)\n    x = torch.randn(3, 3, device=device, requires_grad=True)\n    y = dropout(x)\n    y.norm().backward()\n\n    torch.manual_seed(0)\n    chk_x = torch.randn(3, 3, device=device, requires_grad=True)\n    chk_y = checkpoint(dropout, chk_x)\n    chk_y.norm().backward()\n\n    assert torch.allclose(x.grad, chk_x.grad)\n\n\ndef test_detect_checkpointing_recomputing():\n    logs = []\n\n    class Detect(nn.Module):\n        def forward(self, input):\n            logs.append((is_checkpointing(), is_recomputing()))\n            return input\n\n    model = Detect()\n    input = torch.rand(1, requires_grad=True)\n\n    output = checkpoint(model, input)\n    output.backward()\n\n    assert logs == [(True, False), (False, True)]\n\n\ndef test_detect_checkpointing_recomputing_without_checkpoint():\n    logs = []\n\n    class Detect(nn.Module):\n        def forward(self, input):\n            logs.append((is_checkpointing(), is_recomputing()))\n            return input\n\n    model = Detect()\n    input = torch.rand(1, requires_grad=True)\n\n    output = model(input)\n    output.backward()\n\n    assert logs == [(False, False)]\n"""
tests/test_copy.py,10,"b""import pytest\nimport torch\n\nfrom torchgpipe.copy import Copy, Wait\nfrom torchgpipe.stream import (CPUStream, current_stream, get_device, is_cuda, new_stream,\n                               use_stream)\n\nskip_if_no_cuda = pytest.mark.skipif(not torch.cuda.is_available(), reason='cuda required')\n\n\ndef _test_copy_wait(prev_stream, next_stream, cuda_sleep=None):\n    device = get_device(prev_stream)\n\n    with use_stream(prev_stream):\n        if is_cuda(prev_stream):\n            cuda_sleep(0.5)\n        x = torch.ones(100, device=device, requires_grad=True)\n\n    y, = Copy.apply(prev_stream, next_stream, x)\n    y, = Wait.apply(prev_stream, next_stream, x)\n\n    with use_stream(next_stream):\n        assert torch.allclose(y.sum(), torch.tensor(100.0, device=device))\n        y.norm().backward()\n    with use_stream(prev_stream):\n        assert torch.allclose(x.grad.sum(), torch.tensor(10.0, device=device))\n\n\ndef test_copy_wait_cpu_cpu():\n    prev_stream = CPUStream\n    next_stream = CPUStream\n    _test_copy_wait(prev_stream, next_stream)\n\n\n@skip_if_no_cuda\ndef test_copy_wait_cpu_cuda(cuda_sleep):\n    prev_stream = CPUStream\n    next_stream = current_stream(torch.device('cuda'))\n    _test_copy_wait(prev_stream, next_stream, cuda_sleep)\n\n\n@skip_if_no_cuda\ndef test_copy_wait_cuda_cpu(cuda_sleep):\n    prev_stream = current_stream(torch.device('cuda'))\n    next_stream = CPUStream\n    _test_copy_wait(prev_stream, next_stream, cuda_sleep)\n\n\n@skip_if_no_cuda\ndef test_copy_wait_cuda_cuda(cuda_sleep):\n    prev_stream = current_stream(torch.device('cuda'))\n    next_stream = new_stream(torch.device('cuda'))\n    _test_copy_wait(prev_stream, next_stream, cuda_sleep)\n\n\ndef test_wait_multiple_tensors():\n    a = torch.rand(1, requires_grad=True)\n    b = torch.rand(1, requires_grad=True)\n\n    a, b = Wait.apply(CPUStream, CPUStream, a, b)\n\n    assert a.grad_fn is b.grad_fn\n    assert a.grad_fn.__class__ is Wait._backward_cls\n"""
tests/test_deferred_batch_norm.py,21,"b""from copy import deepcopy\nfrom itertools import chain\n\nimport pytest\nimport torch\nfrom torch import nn, optim\n\nfrom torchgpipe.batchnorm import DeferredBatchNorm\n\nCHUNKS = 4\n\n\ndef tilt_dist(input):\n    # Tilt variance by channel.\n    rgb = input.transpose(0, 1)\n    rgb[0] *= 1\n    rgb[1] *= 10\n    rgb[2] *= 100\n\n    # Tilt mean by single batch.\n    for i, single in enumerate(input):\n        single += 2**i\n\n    return input\n\n\ndef chunked_forward(model, input, chunks=CHUNKS):\n    output_chunks = []\n\n    for chunk in input.chunk(chunks):\n        output_chunks.append(model(chunk))\n\n    return torch.cat(output_chunks)\n\n\n@pytest.mark.parametrize('chunks', [1, 4])\n@pytest.mark.parametrize('input_requires_grad', [True, False])\ndef test_transparency(chunks, input_requires_grad):\n    bn = nn.BatchNorm2d(3)\n    dbn = DeferredBatchNorm.convert_deferred_batch_norm(deepcopy(bn), chunks=chunks)\n\n    input1 = torch.rand(16, 3, 224, 224)\n    input1 = tilt_dist(input1)\n    input2 = input1.clone()\n    input1.requires_grad = input_requires_grad\n    input2.requires_grad = input_requires_grad\n\n    output1 = chunked_forward(bn, input1, chunks=chunks)\n    output2 = chunked_forward(dbn, input2, chunks=chunks)\n\n    assert torch.allclose(output1, output2, atol=1e-4)\n\n    output1.mean().backward()\n    output2.mean().backward()\n\n    assert torch.allclose(bn.weight.grad, dbn.weight.grad, atol=1e-4)\n\n    if input_requires_grad:\n        assert input1.grad is not None\n        assert input2.grad is not None\n        assert torch.allclose(input1.grad, input2.grad, atol=1e-4)\n\n\n@pytest.mark.parametrize('momentum', [0.1, None])\ndef test_running_stats(momentum):\n    bn = nn.BatchNorm2d(3, momentum=momentum)\n    dbn = DeferredBatchNorm.convert_deferred_batch_norm(deepcopy(bn), chunks=CHUNKS)\n\n    input = torch.rand(16, 3, 224, 224)\n    input = tilt_dist(input)\n\n    bn(input)\n    chunked_forward(dbn, input)\n\n    assert torch.allclose(bn.running_mean, dbn.running_mean, atol=1e-4)\n    assert torch.allclose(bn.running_var, dbn.running_var, atol=1e-4)\n\n\ndef test_convert_deferred_batch_norm():\n    bn = nn.BatchNorm2d(3, track_running_stats=False)\n    bn = DeferredBatchNorm.convert_deferred_batch_norm(bn, chunks=CHUNKS)\n    assert type(bn) is nn.BatchNorm2d  # because of track_running_stats=False\n\n    dbn = DeferredBatchNorm(3, chunks=CHUNKS)\n    dbn_again = DeferredBatchNorm.convert_deferred_batch_norm(dbn, chunks=CHUNKS)\n    assert dbn is dbn_again\n\n    dbn_again = DeferredBatchNorm.convert_deferred_batch_norm(dbn, chunks=CHUNKS + 1)\n    assert dbn is not dbn_again  # because of different chunks\n\n\ndef test_eval():\n    bn = nn.BatchNorm2d(3)\n    dbn = DeferredBatchNorm.convert_deferred_batch_norm(deepcopy(bn), chunks=CHUNKS)\n\n    input = torch.rand(16, 3, 224, 224)\n    input = tilt_dist(input)\n\n    bn(input)\n    chunked_forward(dbn, input)\n\n    bn.eval()\n    dbn.eval()\n\n    assert torch.allclose(bn(input), dbn(input), atol=1e-4)\n\n\ndef test_optimize():\n    bn = nn.BatchNorm2d(3)\n    dbn = DeferredBatchNorm.convert_deferred_batch_norm(deepcopy(bn), chunks=CHUNKS)\n\n    opt = optim.SGD(chain(bn.parameters(), dbn.parameters()), lr=1.0)\n\n    for i in range(5):\n        input = torch.rand(16, 3, 224, 224)\n        input = tilt_dist(input)\n\n        # train\n        y = bn(input)\n        a = y.sum()\n        a.backward()\n\n        y = chunked_forward(dbn, input)\n        b = y.sum()\n        b.backward()\n\n        opt.step()\n\n        # eval\n        bn.eval()\n        dbn.eval()\n\n        with torch.no_grad():\n            assert torch.allclose(bn(input), dbn(input), atol=1e-1 * (10**i))\n\n\ndef test_conv_bn():\n    bn = nn.Sequential(nn.Conv2d(3, 3, 1), nn.BatchNorm2d(3))\n    dbn = DeferredBatchNorm.convert_deferred_batch_norm(deepcopy(bn), chunks=CHUNKS)\n\n    input = torch.rand(16, 3, 224, 224)\n    input = tilt_dist(input)\n\n    opt = optim.SGD(chain(bn.parameters(), dbn.parameters()), lr=0.1)\n\n    # 1st step\n    a = bn(input)\n    b = chunked_forward(dbn, input)\n\n    # Outputs are different. (per-mini-batch vs. per-micro-batch)\n    assert not torch.allclose(a, b)\n\n    a.sum().backward()\n    b.sum().backward()\n    opt.step()\n    opt.zero_grad()\n\n    # Conv layers are also trained differently because of their different outputs.\n    assert not torch.allclose(bn[0].weight, dbn[0].weight)\n\n    # But BNs track identical running stats.\n    assert torch.allclose(bn[1].running_mean, dbn[1].running_mean, atol=1e-4)\n    assert torch.allclose(bn[1].running_var, dbn[1].running_var, atol=1e+3)\n\n    # 2nd step\n    a = bn(input)\n    b = chunked_forward(dbn, input)\n    a.sum().backward()\n    b.sum().backward()\n\n    # BNs can't track identical running stats due to the different conv layers.\n    assert not torch.allclose(bn[1].running_mean, dbn[1].running_mean, atol=1e-4)\n    assert not torch.allclose(bn[1].running_var, dbn[1].running_var, atol=1e+3)\n\n\ndef test_input_requiring_grad():\n    dbn = DeferredBatchNorm(3, chunks=CHUNKS)\n\n    input = torch.rand(16, 3, 224, 224, requires_grad=True)\n    input = tilt_dist(input)\n\n    chunked_forward(dbn, input)\n\n    assert not dbn.sum.requires_grad\n    assert dbn.sum.grad_fn is None\n"""
tests/test_dependency.py,15,"b""import weakref\n\nimport pytest\nimport torch\n\nfrom torchgpipe.dependency import Fork, Join, fork, join\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason='cuda required')\ndef test_fork_join():\n    logs = []\n\n    class Log(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, number, tensor):\n            ctx.number = number\n            return tensor.detach()\n\n        @staticmethod\n        def backward(ctx, grad):\n            logs.append(ctx.number)\n            return None, grad\n\n    a = torch.rand(1, device='cpu', requires_grad=True)\n    b = torch.rand(1, device='cuda', requires_grad=True)\n\n    a = Log.apply(1, a)\n\n    a, phony = fork(a)\n    b = join(a, phony)\n\n    b = Log.apply(2, b)\n    b = b.to('cpu')\n\n    (a+b).backward()\n\n    assert logs == [2, 1]\n\n\ndef test_fork_join_enable_grad():\n    x = torch.rand(1, requires_grad=True)\n\n    with torch.enable_grad():\n        x2, p = fork(x)\n\n    assert p.requires_grad\n    assert x2 is not x\n    x = x2\n\n    assert x.requires_grad\n    assert p.requires_grad\n    assert x.grad_fn.__class__ is Fork._backward_cls\n    assert p.grad_fn.__class__ is Fork._backward_cls\n\n    with torch.enable_grad():\n        x2 = join(x, p)\n\n    assert x2 is not x\n    x = x2\n\n    assert x.requires_grad\n    assert x.grad_fn.__class__ is Join._backward_cls\n\n\ndef test_fork_join_no_grad(monkeypatch):\n    def do_not_apply(*args):\n        raise AssertionError('Function.apply called')\n    monkeypatch.setattr('torch.autograd.Function.apply', do_not_apply)\n\n    x = torch.rand(1, requires_grad=True)\n\n    with torch.no_grad():\n        x2, p = fork(x)\n\n    assert not p.requires_grad\n    assert x2 is x\n    x = x2\n\n    with torch.no_grad():\n        x2 = join(x, p)\n\n    assert x2 is x\n    x = x2\n\n\ndef test_fork_leak():\n    leak = None\n\n    class F(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, input):\n            return input\n\n        @staticmethod\n        def backward(ctx, grad):\n            nonlocal leak\n            leak = weakref.ref(ctx)\n            return grad\n\n    x = torch.rand(1, requires_grad=True)\n    x = F.apply(x)\n    x, phony = fork(x)\n    x = join(x, phony)\n\n    x.backward()\n    del x, phony\n\n    assert leak() is None\n\n\ndef test_join_when_fork_not_requires_grad():\n    x = torch.rand(2, 1)\n    a, b = x.chunk(2)\n\n    assert not a.requires_grad\n    a, p = fork(a)\n    assert not a.requires_grad\n    assert not p.requires_grad\n\n    assert not b.requires_grad\n    b = join(b, p)\n    assert not b.requires_grad\n\n\ndef test_join_when_fork_requires_grad():\n    x = torch.rand(2, 1)\n    a, b = x.chunk(2)\n\n    a.requires_grad_()\n    assert a.requires_grad\n    a, p = fork(a)\n    assert a.requires_grad\n    assert p.requires_grad\n\n    assert not b.requires_grad\n    b = join(b, p)\n    assert b.requires_grad\n"""
tests/test_gpipe.py,30,"b'from collections import OrderedDict\nfrom copy import deepcopy\nimport time\n\nimport pytest\nimport torch\nfrom torch import nn\n\nfrom torchgpipe import GPipe\nfrom torchgpipe.gpipe import verify_module\n\n\ndef test_parameters():\n    model = nn.Sequential(nn.Linear(1, 1))\n    gpipe = GPipe(model, balance=[1], devices=[\'cpu\'], chunks=1)\n    assert list(gpipe.parameters()) != []\n\n\ndef test_public_attrs():\n    class MyString:\n        def __init__(self, value):\n            self.value = value\n\n        def __str__(self):\n            return self.value\n\n    model = nn.Sequential(nn.Linear(1, 1))\n    gpipe = GPipe(model,\n                  balance=(1,),\n                  devices=(\'cpu\',),\n                  chunks=42.000,\n                  checkpoint=MyString(\'always\'))\n\n    assert gpipe.balance == [1]\n    assert gpipe.devices == [torch.device(\'cpu\')]\n    assert gpipe.chunks == 42\n    assert isinstance(gpipe.chunks, int)\n    assert gpipe.checkpoint == \'always\'\n    assert isinstance(gpipe.checkpoint, str)\n\n\n@pytest.mark.parametrize(\'balance\', [[2], [1, 1]])\ndef test_sequential_like(balance):\n    a = nn.Linear(1, 1)\n    b = nn.Linear(1, 1)\n\n    model = nn.Sequential(a, b)\n    model = GPipe(model, balance, devices=[\'cpu\', \'cpu\'])\n\n    assert len(model) == 2\n    assert list(model) == [a, b]\n\n    assert model[0] is a\n    assert model[1] is b\n    with pytest.raises(IndexError):\n        _ = model[2]\n\n    assert model[-1] is b\n    assert model[-2] is a\n\n\ndef test_balance_wrong_length():\n    a = nn.Linear(1, 1)\n    b = nn.Linear(1, 1)\n\n    model = nn.Sequential(a, b)\n\n    with pytest.raises(ValueError):\n        GPipe(model, balance=[1])\n\n    with pytest.raises(ValueError):\n        GPipe(model, balance=[3])\n\n\ndef test_balance_less_than_1():\n    a = nn.Linear(1, 1)\n    b = nn.Linear(1, 1)\n\n    model = nn.Sequential(a, b)\n\n    with pytest.raises(ValueError):\n        GPipe(model, balance=[0, 2])\n\n    with pytest.raises(ValueError):\n        GPipe(model, balance=[-1, 3])\n\n\ndef test_chunks_less_than_1():\n    model = nn.Sequential(nn.Linear(1, 1))\n\n    with pytest.raises(ValueError):\n        GPipe(model, balance=[1], devices=[\'cpu\'], chunks=0)\n\n    with pytest.raises(ValueError):\n        GPipe(model, balance=[1], devices=[\'cpu\'], chunks=-1)\n\n\ndef test_too_few_devices():\n    model = nn.Sequential(nn.Linear(1, 1), nn.Linear(1, 1), nn.Linear(1, 1), nn.Linear(1, 1))\n\n    with pytest.raises(IndexError):\n        # len(balance) > len(devices)\n        model = GPipe(model, balance=[1, 1, 1, 1], devices=[\'cpu\'])\n\n\ndef test_batch_size_indivisible():\n    model = nn.Sequential(nn.Linear(1, 1))\n    model = GPipe(model, balance=[1], devices=[\'cpu\'], chunks=4)\n\n    with pytest.warns(None) as record:\n        model(torch.rand(7, 1))\n\n    # Indivisible batch size is legal.\n    assert not record\n\n\ndef test_batch_size_small():\n    model = nn.Sequential(nn.Linear(1, 1))\n    model = GPipe(model, balance=[1], devices=[\'cpu\'], chunks=4)\n\n    with pytest.warns(None) as record:\n        model(torch.rand(2, 1))\n\n    # Batch size smaller than chunks is legal.\n    assert not record\n\n\ndef test_checkpoint_mode():\n    def count_grad_fn(grad_fn, name, visited=set()):\n        if grad_fn in visited:\n            return 0\n        visited.add(grad_fn)\n\n        if grad_fn is None:\n            return 0\n        if grad_fn.__class__.__name__ == name:\n            return 1\n\n        counter = 0\n        for next_grad_fn, _ in grad_fn.next_functions:\n            counter += count_grad_fn(next_grad_fn, name, visited=visited)\n        return counter\n\n    model = nn.Sequential(nn.Linear(1, 1))\n    input = torch.rand(2, 1)\n\n    always = GPipe(model, balance=[1], devices=[\'cpu\'], chunks=2, checkpoint=\'always\')\n    except_last = GPipe(model, balance=[1], devices=[\'cpu\'], chunks=2, checkpoint=\'except_last\')\n    never = GPipe(model, balance=[1], devices=[\'cpu\'], chunks=2, checkpoint=\'never\')\n\n    always_output = always(input)\n    except_last_output = except_last(input)\n    never_output = never(input)\n\n    assert count_grad_fn(always_output.grad_fn, \'CheckpointBackward\') == 2\n    assert count_grad_fn(except_last_output.grad_fn, \'CheckpointBackward\') == 1\n    assert count_grad_fn(never_output.grad_fn, \'CheckpointBackward\') == 0\n\n\ndef test_checkpoint_mode_invalid():\n    model = nn.Sequential(nn.Linear(1, 1))\n\n    with pytest.raises(ValueError,\n                       match=""checkpoint is not one of \'always\', \'except_last\', or \'never\'""):\n        GPipe(model, balance=[1], devices=[\'cpu\'], chunks=2, checkpoint=\'INVALID_CHECKPOINT\')\n\n\ndef test_checkpoint_mode_when_chunks_1():\n    model = nn.Sequential(nn.Linear(1, 1))\n\n    # All checkpoint modes are fine.\n    GPipe(model, balance=[1], devices=[\'cpu\'], chunks=1, checkpoint=\'except_last\')\n    GPipe(model, balance=[1], devices=[\'cpu\'], chunks=1, checkpoint=\'always\')\n    GPipe(model, balance=[1], devices=[\'cpu\'], chunks=1, checkpoint=\'never\')\n\n\ndef test_checkpoint_eval():\n    model = nn.Sequential(nn.Linear(1, 1))\n    model = GPipe(model, balance=[1], devices=[\'cpu\'], chunks=2)\n    input = torch.rand(2, 1)\n\n    def find_grad_fn(grad_fn, name):\n        if grad_fn is None:\n            return False\n        if grad_fn.__class__.__name__ == name:\n            return True\n        for next_grad_fn, _ in grad_fn.next_functions:\n            if find_grad_fn(next_grad_fn, name):\n                return True\n        return False\n\n    model.train()\n    train_output = model(input)\n    assert find_grad_fn(train_output.grad_fn, \'CheckpointBackward\')\n    assert find_grad_fn(train_output.grad_fn, \'RecomputeBackward\')\n\n    model.eval()\n    eval_output = model(input)\n    assert not find_grad_fn(eval_output.grad_fn, \'CheckpointBackward\')\n    assert not find_grad_fn(eval_output.grad_fn, \'RecomputeBackward\')\n\n\ndef test_no_grad():\n    model = nn.Sequential(nn.Linear(1, 1))\n    model = GPipe(model, balance=[1], devices=[\'cpu\'], chunks=2)\n    input = torch.rand(2, 1)\n\n    latent = None\n\n    def hook(module, input, output):\n        _ = module\n        _ = input\n\n        nonlocal latent\n        latent = output\n\n    partition = model.partitions[0]\n    partition.register_forward_hook(hook)\n\n    with torch.no_grad():\n        model(input)\n\n    assert latent.grad_fn is None\n\n\ndef test_exception():\n    class ExpectedException(Exception):\n        pass\n\n    class Raise(nn.Module):\n        def forward(self, *_):\n            raise ExpectedException()\n\n    model = nn.Sequential(Raise())\n    model = GPipe(model, balance=[1], devices=[\'cpu\'], chunks=1)\n\n    with pytest.raises(ExpectedException):\n        model(torch.rand(1))\n\n\ndef test_exception_early_stop_asap():\n    """"""Even the first partitions have finished to process, the partition before\n    the failed partition should be killed as soon as possible.\n    """"""\n    class ExpectedException(Exception):\n        pass\n\n    class Pass(nn.Module):\n        def forward(self, x):\n            return x\n\n    counter = 0\n\n    class Counter(nn.Module):\n        def forward(self, x):\n            time.sleep(0.1)\n\n            nonlocal counter\n            counter += 1\n\n            return x\n\n    class Raise(nn.Module):\n        def forward(self, x):\n            raise ExpectedException()\n\n    model = nn.Sequential(Pass(), Pass(), Counter(), Raise())\n    model = GPipe(model, [1, 1, 1, 1], devices=[\'cpu\', \'cpu\', \'cpu\', \'cpu\'], chunks=3)\n\n    with pytest.raises(ExpectedException):\n        model(torch.rand(3))\n\n    # If the early stop doesn\'t work, it would be 3 instead.\n    assert counter == 2\n\n\ndef test_input_pair():\n    class Two(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.fc_a = nn.Linear(1, 1)\n            self.fc_b = nn.Linear(1, 1)\n\n        def forward(self, a_and_b):\n            a, b = a_and_b\n            return (self.fc_a(a), self.fc_b(b))\n\n    model = nn.Sequential(Two())\n    model = GPipe(model, balance=[1], devices=[\'cpu\'], chunks=2)\n\n    a = torch.rand(10, 1, requires_grad=True)\n    b = torch.rand(10, 1, requires_grad=True)\n\n    a_out, b_out = model((a, b))\n    loss = (a_out + b_out).mean()\n    loss.backward()\n\n    assert a.grad is not None\n    assert b.grad is not None\n\n\ndef test_input_singleton():\n    class One(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.fc = nn.Linear(1, 1)\n\n        def forward(self, only_a):\n            a, = only_a\n            return (self.fc(a),)\n\n    model = nn.Sequential(One())\n    model = GPipe(model, balance=[1], devices=[\'cpu\'], chunks=2)\n\n    a = torch.rand(10, 1, requires_grad=True)\n\n    a_out, = model((a,))\n    loss = a_out.mean()\n    loss.backward()\n\n    assert all(p.grad is not None for p in model.parameters())\n    assert a.grad is not None\n\n\ndef test_input_varargs():\n    model = nn.Sequential(nn.Linear(1, 1))\n    model = GPipe(model, balance=[1], devices=[\'cpu\'])\n\n    a = torch.rand(1)\n    b = torch.rand(1)\n\n    # TypeError: forward() takes 2 positional arguments but 3 were given\n    with pytest.raises(TypeError):\n        model(a, b)\n\n\ndef test_non_tensor():\n    class NonTensor(nn.Module):\n        def forward(self, _):\n            return \'hello\'\n\n    model = nn.Sequential(NonTensor())\n    model = GPipe(model, balance=[1], devices=[\'cpu\'])\n    x = torch.rand(1)\n\n    # TypeError: expected Tensor as element 0 in argument 0, but got str\n    with pytest.raises(TypeError):\n        model(x)\n\n    # TypeError: expected Tensor to scatter, but got str\n    with pytest.raises(TypeError):\n        model(\'hello\')\n\n\ndef test_non_tensor_tuple():\n    class NonTensorTuple(nn.Module):\n        def forward(self, x):\n            return (x, \'hello\')\n\n    model = nn.Sequential(NonTensorTuple())\n    model = GPipe(model, balance=[1], devices=[\'cpu\'])\n    x = torch.rand(1)\n\n    # TypeError: CheckpointBackward.forward: expected Variable (got str) for return value 1\n    with pytest.raises(TypeError):\n        model(x)\n\n    # TypeError: expected Tensor to scatter, but got str\n    with pytest.raises(TypeError):\n        model((x, \'hello\'))\n\n\n@pytest.mark.parametrize(\'checkpoint\', [\'never\', \'always\', \'except_last\'])\ndef test_deferred_batch_norm(checkpoint):\n    bn = nn.BatchNorm2d(3)\n    gpipe_bn = deepcopy(bn)\n    gpipe = GPipe(nn.Sequential(gpipe_bn), balance=[1], devices=[\'cpu\'], chunks=2,\n                  checkpoint=checkpoint, deferred_batch_norm=True)\n\n    x = torch.rand(4, 3, 10, 10)\n    gpipe(x).mean().backward()\n    bn(x).mean().backward()\n\n    assert torch.allclose(gpipe[0].running_mean, bn.running_mean, atol=1e-4)\n    assert torch.allclose(gpipe[0].running_var, bn.running_var, atol=1e-4)\n\n\n@pytest.mark.parametrize(\'checkpoint\', [\'never\', \'always\'])\ndef test_deferred_batch_norm_params(checkpoint):\n    bn = nn.BatchNorm2d(3)\n    gpipe_bn = deepcopy(bn)\n    gpipe = GPipe(nn.Sequential(gpipe_bn), balance=[1], devices=[\'cpu\'], chunks=1,\n                  checkpoint=checkpoint, deferred_batch_norm=True)\n\n    x = torch.rand(4, 3, 10, 10)\n    gpipe(x).mean().backward()\n    bn(x).mean().backward()\n\n    assert gpipe[0].weight.grad is not None\n    assert gpipe[0].bias.grad is not None\n\n    assert torch.allclose(gpipe[0].weight.grad, bn.weight.grad, atol=1e-4)\n    assert torch.allclose(gpipe[0].bias.grad, bn.bias.grad, atol=1e-4)\n\n\ndef test_devices():\n    a = nn.Linear(1, 1)\n    b = nn.Linear(1, 1)\n    c = nn.Linear(1, 1)\n\n    # There are extra two devices.\n    devices = [\'cpu\', \'cpu\', \'cpu\', \'cpu\', \'cpu\']\n\n    model = nn.Sequential(a, b, c)\n    model = GPipe(model, [1, 1, 1], devices=devices)\n\n    cpu = torch.device(\'cpu\')\n    # Extra devices must be discarded.\n    assert model.devices == [cpu, cpu, cpu]\n\n\ndef test_partitions():\n    a = nn.Linear(1, 1)\n    b = nn.Linear(1, 1)\n\n    model = nn.Sequential(a, b)\n    model = GPipe(model, [1, 1], devices=[\'cpu\', \'cpu\'])\n\n    assert isinstance(model.partitions, nn.ModuleList)\n    assert isinstance(model.partitions[0], nn.Sequential)\n    assert isinstance(model.partitions[1], nn.Sequential)\n\n    assert \'partitions.0.0.weight\' in model.state_dict()\n\n\ndef test_deny_moving():\n    a = nn.Linear(1, 1)\n    b = nn.Linear(1, 1)\n\n    model = nn.Sequential(a, b)\n    model = GPipe(model, [1, 1], devices=[\'cpu\', \'cpu\'])\n\n    # Moving is denied.\n    with pytest.raises(TypeError):\n        model.cuda()\n\n    with pytest.raises(TypeError):\n        model.cpu()\n\n    with pytest.raises(TypeError):\n        model.to(torch.device(\'cuda\'))\n\n    with pytest.raises(TypeError):\n        model.to(0)\n\n    with pytest.raises(TypeError):\n        model.to(\'cuda\')\n\n    with pytest.raises(TypeError):\n        model.to(device=0)\n\n    with pytest.raises(TypeError):\n        model.to(torch.rand(1))\n\n    with pytest.raises(TypeError):\n        model.to(tensor=torch.rand(1))\n\n    # Casting is allowed.\n    model.half()\n    model.to(torch.double)\n    model.to(dtype=torch.float)\n\n\ndef test_empty_module():\n    # Empty sequential module is not illegal.\n    model = nn.Sequential()\n    model = GPipe(model, [])\n\n    assert model(torch.tensor(42)) == torch.tensor(42)\n    assert model((torch.tensor(42),)) == (torch.tensor(42),)\n\n    # But only tensor or tensors is legal in GPipe.\n    with pytest.raises(TypeError):\n        model(42)\n\n\ndef test_named_children():\n    a = nn.Linear(1, 1)\n    b = nn.Linear(1, 1)\n\n    model = nn.Sequential(OrderedDict([(\'a\', a), (\'b\', b)]))\n    model = GPipe(model, [1, 1], devices=[\'cpu\', \'cpu\'])\n\n    names = set(n for n, _ in model.named_modules())\n    assert \'partitions.0.a\' in names\n    assert \'partitions.1.b\' in names\n\n    # GPipe doesn\'t support __getattr__. Unlike nn.Sequential, GPipe requires\n    # several methods in its namespace.\n    with pytest.raises(AttributeError):\n        model.a\n\n\ndef test_recommend_auto_balance():\n    with pytest.raises(ValueError, match=\'torchgpipe.balance\'):\n        # balance is required\n        GPipe(nn.Sequential())\n\n    with pytest.raises(ValueError, match=\'torchgpipe.balance\'):\n        # module and sum of balance have differen length (module: 0, sum of balance: 1)\n        GPipe(nn.Sequential(), [1])\n\n    with pytest.raises(ValueError, match=\'torchgpipe.balance\'):\n        # module and sum of balance have different length (module: 2, sum of balance: 1)\n        GPipe(nn.Sequential(nn.Linear(1, 1), nn.Linear(1, 1)), [1])\n\n\ndef test_verify_module_non_sequential():\n    with pytest.raises(TypeError, match=\'module must be nn.Sequential to be partitioned\'):\n        verify_module(nn.Module())\n\n\ndef test_verify_module_duplicate_children():\n    conv = nn.Conv2d(3, 3, 1)\n    model = nn.Sequential(conv, conv)\n\n    with pytest.raises(ValueError, match=\'module with duplicate children is not supported\'):\n        verify_module(model)\n\n\ndef test_verify_module_duplicate_parameters_in_distinct_children():\n    class Surrogate(nn.Module):\n        def __init__(self, module):\n            super().__init__()\n            self.module = module\n\n    conv = nn.Conv2d(3, 3, 1)\n    model = nn.Sequential(Surrogate(conv), Surrogate(conv))\n\n    with pytest.raises(ValueError, match=\'module with duplicate parameters in \'\n                                         \'distinct children is not supported\'):\n        verify_module(model)\n'"
tests/test_inplace.py,4,"b""import pytest\nimport torch\nfrom torch import nn\n\nfrom torchgpipe import GPipe\n\n\ndef test_inplace_on_requires_grad():\n    model = nn.Sequential(nn.Linear(1, 1), nn.ReLU(inplace=True))\n    model = GPipe(model, [1, 1], devices=['cpu', 'cpu'], checkpoint='always')\n\n    x = torch.rand(1)\n    y = model(x)\n\n    match = 'a leaf Variable that requires grad (is being|has been) used in an in-place operation.'\n    with pytest.raises(RuntimeError, match=match):\n        y.backward()\n\n\n@pytest.mark.xfail(strict=True)\ndef test_inplace_on_not_requires_grad():\n    # In-place operation on a tensor not requiring grad doesn't cause a\n    # RuntimeError. Currently, we cannot detect this case.\n    model = nn.Sequential(nn.ReLU(inplace=True))\n    model = GPipe(model, [1], devices=['cpu'], checkpoint='always')\n\n    x = torch.rand(1)\n    y = model(x)\n\n    match = 'a leaf Variable that requires grad (is being|has been) used in an in-place operation.'\n    with pytest.raises(RuntimeError, match=match):\n        y.backward()\n\n\n@pytest.mark.xfail(strict=True)\ndef test_inplace_incorrect_grad():\n    class M(nn.Module):\n        def forward(self, foo_bar):\n            # 'foo' requires grad but 'bar' does not. In-place operation on\n            # 'bar' won't cause a RuntimeError.\n            foo, bar = foo_bar\n\n            # add_(1) is not idempotent, in contrast to relu_(). If it is\n            # executed multiple times, it will accumulates each difference onto\n            # 'bar'.\n            bar.add_(1)\n\n            # 'bar' is still captured by checkpointing. 'foo' will get\n            # incorrect grad.\n            return foo * bar\n\n    model = nn.Sequential(M())\n    model = GPipe(model, [1], devices=['cpu'], checkpoint='always')\n\n    foo = torch.tensor([1.], requires_grad=True)\n    bar = torch.tensor([1.])\n\n    output = model((foo, bar))\n    output.backward()\n\n    # The gradient of 'foo' should be 2, but it is 3 actually because\n    # bar.add_(1) was executed twice due to checkpointing.\n    assert foo.grad.item() == 2.\n"""
tests/test_microbatch.py,22,"b""import pytest\nimport torch\nimport torch.cuda\n\nfrom torchgpipe.microbatch import Batch, check, gather, scatter\n\n\ndef test_batch_atomic():\n    x = torch.tensor(42)\n    b = Batch(x)\n\n    assert b.atomic\n\n    assert b.tensor is x\n    with pytest.raises(AttributeError):\n        b.tensors\n\n    assert list(b) == [x]\n    assert len(b) == 1\n    assert b[0] is x\n\n\ndef test_batch_non_atomic():\n    x, y = torch.tensor(42), torch.tensor(21)\n    b = Batch((x, y))\n\n    assert not b.atomic\n\n    with pytest.raises(AttributeError):\n        b.tensor\n    assert b.tensors == (x, y)\n\n    assert list(b) == [x, y]\n    assert len(b) == 2\n    assert b[0] is x\n    assert b[1] is y\n\n\ndef test_batch_call():\n    a = Batch(torch.tensor(42))\n    b = Batch((torch.tensor(42), torch.tensor(21)))\n\n    def f(x):\n        return x\n\n    assert a.call(f).atomic\n    assert not b.call(f).atomic\n\n\ndef test_batch_setitem_by_index():\n    a = Batch(torch.tensor(42))\n    b = Batch((torch.tensor(42), torch.tensor(21)))\n\n    a[0] = torch.tensor(0)\n    b[0] = torch.tensor(0)\n\n    assert a.atomic\n    assert a[0].item() == 0\n\n    assert not b.atomic\n    assert len(b) == 2\n    assert b[0].item() == 0\n    assert b[1].item() == 21\n\n\ndef test_batch_setitem_by_slice():\n    a = Batch(torch.tensor(42))\n    b = Batch((torch.tensor(42), torch.tensor(21)))\n\n    a[:] = (torch.tensor(0),)\n    b[:] = (torch.tensor(0),)\n\n    assert a.atomic\n    assert a[0].item() == 0\n\n    assert not b.atomic\n    assert len(b) == 1\n    assert b[0].item() == 0\n\n\ndef test_check():\n    check(torch.tensor(42))\n    check((torch.tensor(4), torch.tensor(2)))\n\n    with pytest.raises(TypeError):\n        check(42)\n\n    with pytest.raises(TypeError):\n        check('str')\n\n    with pytest.raises(TypeError):\n        check((torch.tensor(4), 2))\n\n\ndef test_gather_tensors():\n    a = torch.zeros(1, 1)\n    b = torch.zeros(1, 1)\n\n    ab = gather([Batch(a), Batch(b)])\n\n    assert ab.size() == (2, 1)\n\n\ndef test_gather_tuples():\n    a = (torch.zeros(1, 1), torch.zeros(2, 2))\n    b = (torch.zeros(1, 1), torch.zeros(2, 2))\n\n    ab = gather([Batch(a), Batch(b)])\n\n    assert isinstance(ab, tuple)\n    assert ab[0].size() == (2, 1)\n    assert ab[1].size() == (4, 2)\n\n\ndef test_scatter_tensor():\n    ab = torch.zeros(2, 1)\n\n    a, b = scatter(ab, chunks=2)\n\n    assert a.tensor.size() == (1, 1)\n    assert b.tensor.size() == (1, 1)\n\n\ndef test_scatter_tuple():\n    ab = (torch.zeros(2, 1), torch.zeros(4, 2))\n\n    a, b = scatter(ab, chunks=2)\n\n    assert a.tensors[0].size() == (1, 1)\n    assert b.tensors[0].size() == (1, 1)\n    assert a.tensors[1].size() == (2, 2)\n    assert b.tensors[1].size() == (2, 2)\n"""
tests/test_phony.py,10,"b""import torch\n\nfrom torchgpipe.phony import get_phony\n\n\ndef test_phony_size():\n    p = get_phony(torch.device('cpu'), requires_grad=False)\n    assert p.size() == (0,)\n\n\ndef test_phony_requires_grad():\n    p1 = get_phony(torch.device('cpu'), requires_grad=True)\n    p2 = get_phony(torch.device('cpu'), requires_grad=False)\n    assert p1.requires_grad\n    assert not p2.requires_grad\n\n\ndef test_cached_phony():\n    p1 = get_phony(torch.device('cpu'), requires_grad=True)\n    p2 = get_phony(torch.device('cpu'), requires_grad=True)\n    assert p1 is p2\n\n    p3 = get_phony(torch.device('cpu'), requires_grad=False)\n    p4 = get_phony(torch.device('cpu'), requires_grad=False)\n    assert p3 is p4\n\n    assert p1 is not p3\n\n\ndef test_phony_in_autograd_function():\n    class Phonify(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, input):\n            phony = get_phony(input.device, requires_grad=False)\n            return phony.detach()\n\n    x = torch.rand(1, requires_grad=True)\n\n    p1 = Phonify.apply(x)\n    p2 = get_phony(torch.device('cpu'), requires_grad=True)\n\n    assert p1 is not p2\n    assert p1.grad_fn is not None\n    assert p2.grad_fn is None\n"""
tests/test_pipeline.py,1,"b'import time\n\nimport torch\nfrom torch import nn\n\nfrom torchgpipe.microbatch import Batch\nfrom torchgpipe.pipeline import Pipeline, clock_cycles\n\n\ndef test_clock_cycles():\n    assert list(clock_cycles(1, 1)) == [[(0, 0)]]\n    assert list(clock_cycles(1, 3)) == [[(0, 0)], [(0, 1)], [(0, 2)]]\n    assert list(clock_cycles(3, 1)) == [[(0, 0)], [(1, 0)], [(2, 0)]]\n\n    assert list(clock_cycles(3, 3)) == [  # noqa\n        [(0, 0)],\n        [(1, 0), (0, 1)],\n        [(2, 0), (1, 1), (0, 2)],\n                [(2, 1), (1, 2)],\n                        [(2, 2)],\n    ]\n\n    assert list(clock_cycles(4, 2)) == [  # noqa\n        [(0, 0)],\n        [(1, 0), (0, 1)],\n        [(2, 0), (1, 1)],\n        [(3, 0), (2, 1)],\n                [(3, 1)],\n    ]\n\n\ndef test_forward_lockstep():\n    timeline = []\n\n    class DelayedLog(nn.Module):\n        def __init__(self, j, seconds):\n            super().__init__()\n            self.i = 0\n            self.j = j\n            self.seconds = seconds\n\n        def forward(self, x):\n            time.sleep(self.seconds)\n\n            timeline.append((self.i, self.j))\n            self.i += 1\n\n            return x\n\n    batches = [Batch(torch.rand(1, 1)) for _ in range(3)]\n    partitions = [nn.Sequential(DelayedLog(0, seconds=0)),\n                  nn.Sequential(DelayedLog(1, seconds=0.1))]\n\n    pipeline = Pipeline(batches, partitions)\n    pipeline.run()\n\n    # Expected timeline: (Logs are recorded at !)\n    #\n    # Partition #0: 0! 1!   2!\n    # Partition #1:    000! 111! 222!\n    #\n    assert timeline == [(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (2, 1)]\n'"
tests/test_stream.py,38,"b""import pytest\nimport torch\n\nfrom torchgpipe.stream import (CPUStream, current_stream, default_stream, get_device, is_cuda,\n                               new_stream, record_stream, use_device, use_stream, wait_stream)\n\nskip_if_no_cuda = pytest.mark.skipif(not torch.cuda.is_available(), reason='cuda required')\n\n\nclass TestNewStream:\n    def test_new_stream_cpu(self):\n        stream = new_stream(torch.device('cpu'))\n        assert stream is CPUStream\n\n    @skip_if_no_cuda\n    def test_new_stream_cuda(self):\n        stream = new_stream(torch.device('cuda'))\n        assert isinstance(stream, torch.cuda.Stream)\n        assert stream != torch.cuda.default_stream()\n\n\nclass TestCurrentStream:\n    def test_current_stream_cpu(self):\n        stream = current_stream(torch.device('cpu'))\n        assert stream is CPUStream\n\n    @skip_if_no_cuda\n    def test_current_stream_cuda(self):\n        stream = current_stream(torch.device('cuda'))\n        assert isinstance(stream, torch.cuda.Stream)\n        assert stream == torch.cuda.current_stream()\n\n\nclass TestDefaultStream:\n    def test_default_stream_cpu(self):\n        stream = default_stream(torch.device('cpu'))\n        assert stream is CPUStream\n\n    @skip_if_no_cuda\n    def test_default_stream_cuda(self):\n        stream = default_stream(torch.device('cuda'))\n        assert isinstance(stream, torch.cuda.Stream)\n        assert stream == torch.cuda.default_stream()\n\n\nclass TestUseDevice:\n    def test_use_device_cpu(self):\n        with use_device(torch.device('cpu')):\n            pass\n\n    @skip_if_no_cuda\n    def test_use_device_cuda(self):\n        with use_device(torch.device('cuda')):\n            pass\n\n\nclass TestUseStream:\n    def test_use_stream_cpu(self):\n        with use_stream(CPUStream):\n            pass\n\n    @skip_if_no_cuda\n    def test_use_stream_cuda(self):\n        stream = new_stream(torch.device('cuda'))\n        with use_stream(stream):\n            assert current_stream(torch.device('cuda')) == stream\n\n\nclass TestGetDevice:\n    def test_get_device_cpu(self):\n        assert get_device(CPUStream).type == 'cpu'\n\n    @skip_if_no_cuda\n    def test_get_device_cuda(self):\n        stream = current_stream(torch.device('cuda'))\n        assert get_device(stream).type == 'cuda'\n\n\nclass TestWaitStream:\n    def _test_wait_stream(self, source, target, cuda_sleep=None):\n        with use_stream(target):\n            if is_cuda(target):\n                cuda_sleep(0.5)\n            x = torch.ones(100, 100, device=get_device(target))\n\n        wait_stream(source, target)\n\n        with use_stream(source):\n            assert x.sum().item() == 10000\n\n    def test_wait_stream_cpu_cpu(self):\n        source = CPUStream\n        target = CPUStream\n        self._test_wait_stream(source, target)\n\n    @skip_if_no_cuda\n    def test_wait_stream_cpu_cuda(self, cuda_sleep):\n        source = CPUStream\n        target = new_stream(torch.device('cuda'))\n        self._test_wait_stream(source, target, cuda_sleep)\n\n    @skip_if_no_cuda\n    def test_wait_stream_cuda_cpu(self, cuda_sleep):\n        source = new_stream(torch.device('cuda'))\n        target = CPUStream\n        self._test_wait_stream(source, target, cuda_sleep)\n\n    @skip_if_no_cuda\n    def test_wait_stream_cuda_cuda(self, cuda_sleep):\n        source = current_stream(torch.device('cuda'))\n        target = new_stream(torch.device('cuda'))\n        self._test_wait_stream(source, target, cuda_sleep)\n\n\nclass TestRecordStream:\n    def test_record_stream_cpu(self):\n        # It should silently ignore CPU tensors.\n        x = torch.rand(1, device=torch.device('cpu'))\n        record_stream(x, CPUStream)\n\n    @skip_if_no_cuda\n    def test_record_stream_cuda(self, cuda_sleep):\n        # This test detects unexpected block reallocation. For reliable test,\n        # the stream to allocate tensors is isolated. The allocator will not\n        # reuse free blocks which were allocated from another stream.\n        stream_alloc = new_stream(torch.device('cuda'))\n        with torch.cuda.stream(stream_alloc):\n            x = torch.rand(1, device=torch.device('cuda'))\n\n        stream = new_stream(torch.device('cuda'))\n        record_stream(x, stream)\n        with use_stream(stream):\n            cuda_sleep(0.5)\n\n        # 'x' is deleted at Python's perspective. But the block of 'x' is still\n        # required for 'stream'. 'y' shouldn't be allocated to the block.\n        data_ptr = x.data_ptr()\n        del x\n        stream_alloc.synchronize()\n        with torch.cuda.stream(stream_alloc):\n            y = torch.rand(1, device=torch.device('cuda'))\n        assert y.data_ptr() != data_ptr\n\n        # Pause Python until 'stream' finishes tasks queued. Now the block of\n        # 'x' is free to be reallocated.\n        wait_stream(CPUStream, stream)\n        with torch.cuda.stream(stream_alloc):\n            z = torch.rand(1, device=torch.device('cuda'))\n        assert z.data_ptr() == data_ptr\n\n    @skip_if_no_cuda\n    def test_record_stream_shifted_view(self, cuda_sleep):\n        # Issue: https://github.com/pytorch/pytorch/issues/27366\n        stream_alloc = new_stream(torch.device('cuda'))\n        with torch.cuda.stream(stream_alloc):\n            x = torch.rand(2, device=torch.device('cuda'))\n\n        y = x[1:]\n        assert y.data_ptr() > x.data_ptr()\n\n        stream = new_stream(torch.device('cuda'))\n        with use_stream(stream):\n            cuda_sleep(0.5)\n        record_stream(y, stream)\n\n        data_ptr = x.data_ptr()\n        del x, y\n\n        stream_alloc.synchronize()\n        with torch.cuda.stream(stream_alloc):\n            z = torch.rand(2, device=torch.device('cuda'))\n        assert z.data_ptr() != data_ptr\n"""
tests/test_transparency.py,2,"b""import torch\nfrom torch import nn\n\nfrom torchgpipe import GPipe\n\n\ndef test_simple_linears():\n    def sum_grad(parameters):\n        return sum([p.grad.sum() for p in parameters if p.grad is not None])\n\n    def zero_grad(parameters):\n        for p in parameters:\n            p.grad = None\n\n    inputs = torch.rand(8, 1)\n    model = nn.Sequential(\n        nn.Linear(1, 2),\n        nn.Linear(2, 4),\n        nn.Linear(4, 2),\n        nn.Linear(2, 1),\n    )\n\n    # Without GPipe\n    outputs = model(inputs)\n    loss = outputs.mean()\n    loss.backward()\n\n    grad_without_gpipe = sum_grad(model.parameters())\n\n    zero_grad(model.parameters())\n\n    # With GPipe\n    model = GPipe(model, [2, 2], devices=['cpu', 'cpu'], chunks=4)\n\n    outputs = model(inputs)\n    loss = outputs.mean()\n    loss.backward()\n\n    grad_with_gpipe = sum_grad(model.parameters())\n\n    # Both grads should be identical.\n    assert torch.allclose(grad_with_gpipe, grad_without_gpipe)\n"""
tests/test_worker.py,9,"b'import threading\nimport time\n\nimport pytest\nimport torch\n\nfrom torchgpipe.microbatch import Batch\nfrom torchgpipe.stream import CPUStream\nfrom torchgpipe.worker import Task, spawn_workers\n\n\nclass fake_device:\n    """"""A test double for :class:`torch.device`. Every fake device is different\n    with each other.\n    """"""\n    type = \'fake\'\n    index = None\n\n\ndef test_join_running_workers():\n    count = 0\n\n    def counter():\n        nonlocal count\n        time.sleep(0.1)\n        count += 1\n        return Batch(())\n\n    with spawn_workers([fake_device() for _ in range(10)]) as (in_queues, out_queues):\n        def call_in_worker(i, f):\n            task = Task(CPUStream, compute=f, finalize=None)\n            in_queues[i].put(task)\n\n        for i in range(10):\n            call_in_worker(i, counter)\n\n    # There\'s no nondeterminism because \'spawn_workers\' joins all running\n    # workers.\n    assert count == 10\n\n\ndef test_join_running_workers_with_exception():\n    class ExpectedException(Exception):\n        pass\n\n    count = 0\n\n    def counter():\n        nonlocal count\n        time.sleep(0.1)\n        count += 1\n        return Batch(())\n\n    with pytest.raises(ExpectedException):\n        with spawn_workers([fake_device() for _ in range(10)]) as (in_queues, out_queues):\n            def call_in_worker(i, f):\n                task = Task(CPUStream, compute=f, finalize=None)\n                in_queues[i].put(task)\n\n            for i in range(10):\n                call_in_worker(i, counter)\n\n            raise ExpectedException\n\n    # There\'s no nondeterminism because only 1 task can be placed in input\n    # queues.\n    assert count == 10\n\n\ndef test_compute_multithreading():\n    """"""Task.compute should be executed on multiple threads.""""""\n    thread_ids = set()\n\n    def log_thread_id():\n        thread_id = threading.current_thread().ident\n        thread_ids.add(thread_id)\n        return Batch(())\n\n    with spawn_workers([fake_device() for _ in range(2)]) as (in_queues, out_queues):\n        for i in range(2):\n            t = Task(CPUStream, compute=log_thread_id, finalize=None)\n            in_queues[i].put(t)\n        for i in range(2):\n            out_queues[i].get()\n\n    assert len(thread_ids) == 2\n\n\ndef test_compute_success():\n    """"""Task.compute returns (True, (task, batch)) on success.""""""\n    def _42():\n        return Batch(torch.tensor(42))\n\n    with spawn_workers([torch.device(\'cpu\')]) as (in_queues, out_queues):\n        t = Task(CPUStream, compute=_42, finalize=None)\n        in_queues[0].put(t)\n        ok, (task, batch) = out_queues[0].get()\n\n        assert ok\n        assert task is t\n        assert isinstance(batch, Batch)\n        assert batch[0].item() == 42\n\n\ndef test_compute_exception():\n    """"""Task.compute returns (False, exc_info) on failure.""""""\n    def zero_div():\n        0/0\n\n    with spawn_workers([torch.device(\'cpu\')]) as (in_queues, out_queues):\n        t = Task(CPUStream, compute=zero_div, finalize=None)\n        in_queues[0].put(t)\n        ok, exc_info = out_queues[0].get()\n\n        assert not ok\n        assert isinstance(exc_info, tuple)\n        assert issubclass(exc_info[0], ZeroDivisionError)\n\n\n@pytest.mark.parametrize(\'grad_mode\', [True, False])\ndef test_grad_mode(grad_mode):\n    def detect_grad_enabled():\n        x = torch.rand(1, requires_grad=torch.is_grad_enabled())\n        return Batch(x)\n\n    with torch.set_grad_enabled(grad_mode):\n        with spawn_workers([torch.device(\'cpu\')]) as (in_queues, out_queues):\n            task = Task(CPUStream, compute=detect_grad_enabled, finalize=None)\n            in_queues[0].put(task)\n\n            ok, (_, batch) = out_queues[0].get()\n\n            assert ok\n            assert batch[0].requires_grad == grad_mode\n\n\ndef test_worker_per_device():\n    cpu = torch.device(\'cpu\')\n    cpu0 = torch.device(\'cpu\', index=0)\n    fake1 = fake_device()\n    fake2 = fake_device()\n\n    with spawn_workers([cpu, cpu, cpu0, fake1, fake2]) as (in_queues, out_queues):\n        assert len(in_queues) == len(out_queues) == 5\n\n        # 0: cpu, 1: cpu, 2: cpu0\n        assert in_queues[0] is in_queues[1] is in_queues[2]\n        assert out_queues[0] is out_queues[1] is out_queues[2]\n\n        # 3: fake1, 4: fake2\n        assert in_queues[3] is not in_queues[4]\n        assert out_queues[3] is not out_queues[4]\n'"
torchgpipe/__init__.py,0,"b'""""""A GPipe implementation in PyTorch.""""""\nfrom torchgpipe.__version__ import __version__  # noqa\nfrom torchgpipe.checkpoint import is_checkpointing, is_recomputing\nfrom torchgpipe.gpipe import GPipe\n\n__all__ = [\'GPipe\', \'is_checkpointing\', \'is_recomputing\']\n'"
torchgpipe/__version__.py,0,"b""__version__ = '0.0.6'\n"""
torchgpipe/batchnorm.py,5,"b'""""""Tracks the running statistics per mini-batch instead of micro-batch.""""""\nfrom typing import Optional, TypeVar, cast\n\nimport torch\nfrom torch import Tensor, nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nfrom torchgpipe.checkpoint import is_recomputing\n\n__all__ = [\'DeferredBatchNorm\']\n\n\nTModule = TypeVar(\'TModule\', bound=nn.Module)\n\n\nclass DeferredBatchNorm(_BatchNorm):\n    """"""A BatchNorm layer tracks multiple micro-batches to update running\n    statistics per mini-batch.\n    """"""\n    sum: Tensor\n    sum_squares: Tensor\n\n    def __init__(self,\n                 num_features: int,\n                 eps: float = 1e-5,\n                 momentum: Optional[float] = 0.1,\n                 affine: bool = True,\n                 chunks: int = 1,\n                 ) -> None:\n        super().__init__(num_features, eps, momentum, affine, track_running_stats=True)\n\n        self.register_buffer(\'sum\', torch.zeros_like(self.running_mean))\n        self.register_buffer(\'sum_squares\', torch.zeros_like(self.running_var))\n\n        self.counter = 0\n        self.tracked = 0\n        self.chunks = chunks\n\n    def _check_input_dim(self, input: Tensor) -> None:\n        # It\'s the typical _check_input_dim() implementation in PyTorch.\n        if input.dim() <= 2:\n            raise ValueError(\'expected at least 3D input (got %dD input)\' % input.dim())\n\n    def _track(self, input: Tensor) -> bool:\n        """"""Tracks statistics of a micro-batch.""""""\n        # Dimensions except channel. For example, (0, 2, 3) is for BatchNorm2d.\n        dim = [0]\n        dim.extend(range(2, input.dim()))\n\n        with torch.no_grad():\n            self.sum += input.sum(dim)\n            self.sum_squares += (input**2).sum(dim)\n\n        size = input.size().numel() // input.size(1)\n        self.counter += size\n        self.tracked += 1\n\n        return (self.tracked == self.chunks)\n\n    def _commit(self) -> None:\n        """"""Updates the running statistics of a mini-batch.""""""\n        exponential_average_factor = 0.0\n        self.num_batches_tracked += 1\n        if self.momentum is None:  # use cumulative moving average\n            exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n        else:  # use exponential moving average\n            exponential_average_factor = self.momentum\n\n        mean = self.sum/self.counter\n        var = self.sum_squares/self.counter - mean**2\n\n        # Calculate the exponential moving average here.\n        m = exponential_average_factor\n\n        self.running_mean *= 1 - m\n        self.running_mean += mean * m\n\n        self.running_var *= 1 - m\n        self.running_var += var * m\n\n        self.sum.zero_()\n        self.sum_squares.zero_()\n        self.counter = 0\n        self.tracked = 0\n\n    def forward(self, input: Tensor) -> Tensor:  # type: ignore\n        if not self.training:\n            # Don\'t train parameters on the evaluation mode.\n            return F.batch_norm(\n                input,\n                running_mean=self.running_mean,\n                running_var=self.running_var,\n                weight=self.weight,\n                bias=self.bias,\n                training=False,\n                momentum=0.0,\n                eps=self.eps,\n            )\n\n        if not is_recomputing():\n            # Track a micro-batch on the training mode\n            # but not under a recomputation.\n            tracked_enough = self._track(input)\n\n            # Update the running statistics for a mini-batch\n            # if it has tracked enough micro-batches.\n            if tracked_enough:\n                self._commit()\n\n        # Normalize a micro-batch and train the parameters.\n        return F.batch_norm(\n            input,\n            running_mean=None,\n            running_var=None,\n            weight=self.weight,\n            bias=self.bias,\n            training=True,\n            momentum=0.0,\n            eps=self.eps,\n        )\n\n    @classmethod\n    def convert_deferred_batch_norm(cls, module: TModule, chunks: int = 1) -> TModule:\n        """"""Converts a :class:`nn.BatchNorm` or underlying\n        :class:`nn.BatchNorm`s into :class:`DeferredBatchNorm`::\n\n            from torchvision.models.resnet import resnet101\n            from torchgpipe.batchnorm import DeferredBatchNorm\n            model = resnet101()\n            model = DeferredBatchNorm.convert_deferred_batch_norm(model)\n\n        """"""\n        if isinstance(module, DeferredBatchNorm) and module.chunks is chunks:\n            return cast(TModule, module)\n\n        module_output: nn.Module = module\n\n        if isinstance(module, _BatchNorm) and module.track_running_stats:\n            module_output = DeferredBatchNorm(module.num_features,\n                                              module.eps,\n                                              module.momentum,\n                                              module.affine,\n                                              chunks)\n            if module.affine:\n                module_output.register_parameter(\'weight\', module.weight)\n                module_output.register_parameter(\'bias\', module.bias)\n            module_output.register_buffer(\'running_mean\', module.running_mean)\n            module_output.register_buffer(\'running_var\', module.running_var)\n            module_output.register_buffer(\'num_batches_tracked\', module.num_batches_tracked)\n\n        for name, child in module.named_children():\n            module_output.add_module(name, cls.convert_deferred_batch_norm(child, chunks))\n\n        return cast(TModule, module_output)\n'"
torchgpipe/checkpoint.py,16,"b'""""""Checkpointing with preceding recomputation.\n\nPyTorch already provides the official checkpointing utilities in\n:mod:`torch.utils.checkpoint`. The official checkpointing combines\nrecomputation and recursive backpropagation into one autograd function named\n``CheckpointFunction``. Hence, the recomputation can be started only when the\ngradients arrive to the function. In GPipe, the recomputation needs to precede\nthe gradient arrival to minimize the GPU idle time.\n\nWe solve this problem by introducing separate autograd functions named\n:class:`Recompute` and :class:`Checkpoint`. Each function represents\nrecomputation and recursive backpropagation, respectively. We can manipulate\nthe control flow in aspect of both the autograd engine and CUDA with a pair of\nthe functions.\n\nSpecifically, we place CUDA stream synchronization between :class:`Recompute`\nand :class:`Checkpoint` to delay only :class:`Checkpoint` until the gradient is\ncopied entirely.\n\n""""""\nfrom collections import deque\nfrom contextlib import contextmanager\nimport threading\nfrom typing import TYPE_CHECKING, Deque, Generator, List, Optional, Tuple, Union\n\nimport torch\nfrom torch import ByteTensor, Tensor\nimport torch.autograd\n\nfrom torchgpipe.dependency import fork, join\nfrom torchgpipe.microbatch import Batch\nfrom torchgpipe.phony import get_phony\n\n__all__ = [\'is_checkpointing\', \'is_recomputing\']\n\n\nTensors = Tuple[Tensor, ...]\nTensorOrTensors = Union[Tensor, Tensors]\n\n# Types for shared memory between Checkpoint and Recompute.\nRecomputed = Tuple[TensorOrTensors, Tensors]         # (output, input_leaf)\nRNGStates = Tuple[ByteTensor, Optional[ByteTensor]]  # (cpu_rng_state, gpu_rng_state)\n\n\nif TYPE_CHECKING:\n    from typing_extensions import Protocol\nelse:\n    Protocol = object\n\n\n# Protocol with __call__ instead of Callable can be used as an attribute type.\n# See: https://github.com/python/mypy/issues/708#issuecomment-561735949\nclass Function(Protocol):\n    def __call__(self, input: TensorOrTensors) -> TensorOrTensors:\n        ...\n\n\ndef checkpoint(function: Function, input: TensorOrTensors) -> TensorOrTensors:\n    """"""Makes a checkpoint with a simple interface like\n    :func:`torch.utils.checkpoint.checkpoint`. It\'s only used to test or debug\n    :class:`Checkpoint` and :class:`Recompute` without boilerplate.\n    """"""\n    batch = Batch(input)\n\n    chk = Checkpointing(function, batch)\n    batch = chk.checkpoint()\n    chk.recompute(batch)\n\n    return batch.tensor_or_tensors\n\n\nclass Checkpointing:\n    """"""Generates a pair of :class:`Checkpoint` and :class:`Recompute`.""""""\n\n    def __init__(self, function: Function, batch: Batch) -> None:\n        self.function = function\n        self.batch = batch\n\n        # Shared memory between Checkpoint and Recompute. 1-length deque is\n        # used for mutability and length limitation.\n        self.recomputed: Deque[Recomputed] = deque(maxlen=1)\n        self.rng_states: Deque[RNGStates] = deque(maxlen=1)\n\n    def checkpoint(self) -> Batch:\n        """"""Returns a batch applied by :class:`Checkpoint`.""""""\n        input_atomic = self.batch.atomic\n        input = tuple(self.batch)\n\n        # Use a phony which requires grad to ensure that Checkpoint can be\n        # tracked by the autograd engine even when none of the input tensors\n        # require grad.\n        phony = get_phony(self.batch[0].device, requires_grad=True)\n\n        output = Checkpoint.apply(phony, self.recomputed, self.rng_states,\n                                  self.function, input_atomic, *input)\n        return Batch(output)\n\n    def recompute(self, batch: Batch) -> None:\n        """"""Applies :class:`Recompute` to the batch in place.""""""\n        input_atomic = self.batch.atomic\n        input = tuple(self.batch)\n\n        # batch[0] is always requiring grad, because it has been passed\n        # checkpoint with a phony requiring grad.\n        batch[0], phony = fork(batch[0])\n        phony = Recompute.apply(phony, self.recomputed, self.rng_states,\n                                self.function, input_atomic, *input)\n        batch[0] = join(batch[0], phony)\n\n\nclass ThreadLocal(threading.local):\n    def __init__(self) -> None:\n        self.is_checkpointing = False\n        self.is_recomputing = False\n\n\nthread_local = ThreadLocal()\n\n\n@contextmanager\ndef enable_checkpointing() -> Generator[None, None, None]:\n    """"""Makes :func:`is_checkpointing` return :data:`True` within a context.""""""\n    orig = thread_local.is_checkpointing\n    thread_local.is_checkpointing = True\n    try:\n        yield\n    finally:\n        thread_local.is_checkpointing = orig\n\n\n@contextmanager\ndef enable_recomputing() -> Generator[None, None, None]:\n    """"""Makes :func:`is_recomputing` return :data:`True` within a context.""""""\n    orig = thread_local.is_recomputing\n    thread_local.is_recomputing = True\n    try:\n        yield\n    finally:\n        thread_local.is_recomputing = orig\n\n\ndef is_checkpointing() -> bool:\n    """"""Whether the current forward propagation is under checkpointing.\n\n    Returns:\n        bool: :data:`True` if it\'s under checkpointing.\n\n    """"""\n    return thread_local.is_checkpointing\n\n\ndef is_recomputing() -> bool:\n    """"""Whether the current forward propagation is under checkpoint\n    recomputation. Use this to prevent duplicated side-effects at forward\n    propagation::\n\n        class Counter(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.counter = 0\n\n            def forward(self, input):\n                if not is_recomputing():\n                    self.counter += 1\n                return input\n\n    Returns:\n        bool: :data:`True` if it\'s under checkpoint recomputation.\n\n    .. seealso:: :ref:`Detecting Recomputation`\n\n    """"""\n    return thread_local.is_recomputing\n\n\nclass Context:\n    """"""The common interface between the :class:`Checkpoint` and\n    :class:`Recompute` context.\n    """"""\n    recomputed: Deque[Recomputed]\n    rng_states: Deque[RNGStates]\n    function: Function\n    input_atomic: bool\n\n    saved_tensors: Tuple[Tensor, ...]\n\n    def save_for_backward(self, *tensors: Tensor) -> None:  # pragma: no cover\n        pass\n\n\ndef save_rng_states(device: torch.device,\n                    rng_states: Deque[RNGStates],\n                    ) -> None:\n    """""":meth:`Checkpoint.forward` captures the current PyTorch\'s random number\n    generator states at CPU and GPU to reuse in :meth:`Recompute.backward`.\n\n    .. seealso:: :ref:`Referential Transparency`\n\n    """"""\n    cpu_rng_state = torch.get_rng_state()\n\n    gpu_rng_state: Optional[ByteTensor]\n    if device.type == \'cuda\':\n        gpu_rng_state = torch.cuda.get_rng_state(device)\n    else:\n        gpu_rng_state = None\n\n    rng_states.append((cpu_rng_state, gpu_rng_state))\n\n\n@contextmanager\ndef restore_rng_states(device: torch.device,\n                       rng_states: Deque[RNGStates],\n                       ) -> Generator[None, None, None]:\n    """""":meth:`Recompute.backward` restores the random number generator states\n    captured by :func:`save_rng_states` within its context.\n\n    .. seealso:: :ref:`Referential Transparency`\n\n    """"""\n    cpu_rng_state, gpu_rng_state = rng_states.pop()\n\n    gpu_devices: List[torch.device] = []\n    if device.type == \'cuda\':\n        gpu_devices.append(device)\n\n    with torch.random.fork_rng(gpu_devices):\n        torch.set_rng_state(cpu_rng_state)\n        if gpu_rng_state is not None:\n            torch.cuda.set_rng_state(gpu_rng_state, device)\n        yield\n\n\nclass Checkpoint(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx: Context,  # type: ignore\n                phony: Tensor,\n                recomputed: Deque[Recomputed],\n                rng_states: Deque[RNGStates],\n                function: Function,\n                input_atomic: bool,\n                *input: Tensor,\n                ) -> TensorOrTensors:\n        ctx.recomputed = recomputed\n        ctx.rng_states = rng_states\n\n        save_rng_states(input[0].device, ctx.rng_states)\n\n        ctx.function = function\n        ctx.input_atomic = input_atomic\n        ctx.save_for_backward(*input)\n\n        with torch.no_grad(), enable_checkpointing():\n            output = function(input[0] if input_atomic else input)\n\n        return output\n\n    @staticmethod\n    def backward(ctx: Context,\n                 *grad_output: Tensor,\n                 ) -> Tuple[Optional[Tensor], ...]:  # pragma: no cover\n        output, input_leaf = ctx.recomputed.pop()\n\n        if isinstance(output, tuple):\n            tensors = output\n        else:\n            tensors = (output,)\n        if any(y.requires_grad for y in tensors):\n            torch.autograd.backward(tensors, grad_output)\n\n        grad_input: List[Optional[Tensor]] = [None, None, None, None, None]\n        grad_input.extend(x.grad for x in input_leaf)\n        return tuple(grad_input)\n\n\nclass Recompute(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx: Context,  # type: ignore\n                phony: Tensor,\n                recomputed: Deque[Recomputed],\n                rng_states: Deque[RNGStates],\n                function: Function,\n                input_atomic: bool,\n                *input: Tensor,\n                ) -> Tensor:\n        ctx.recomputed = recomputed\n        ctx.rng_states = rng_states\n\n        ctx.function = function\n        ctx.input_atomic = input_atomic\n        ctx.save_for_backward(*input)\n\n        return phony\n\n    @staticmethod\n    def backward(ctx: Context, *grad_output: Tensor) -> Tuple[None, ...]:  # pragma: no cover\n        input = ctx.saved_tensors\n        input_leaf = tuple(x.detach().requires_grad_(x.requires_grad) for x in input)\n\n        with restore_rng_states(input[0].device, ctx.rng_states):\n            with torch.enable_grad(), enable_recomputing():\n                output = ctx.function(input_leaf[0] if ctx.input_atomic else input_leaf)\n\n        ctx.recomputed.append((output, input_leaf))\n\n        grad_input: List[None] = [None, None, None, None, None]\n        grad_input.extend(None for _ in ctx.saved_tensors)\n        return tuple(grad_input)\n'"
torchgpipe/copy.py,2,"b'""""""Autograd functions for stream-aware CUDA copy. It is used to overlap copy\nand computation on the same GPU.\n""""""\nfrom collections import deque\nfrom typing import Deque, List, Optional, Tuple\n\nimport torch\nfrom torch import Tensor\n\nfrom torchgpipe.stream import (AbstractStream, current_stream, get_device, record_stream,\n                               use_stream, wait_stream)\n\n__all__: List[str] = []\n\n\nTensors = Tuple[Tensor, ...]\n\n\n# Common interface between :class:`Copy` and :class:`Wait`.\nclass Context:\n    prev_stream: AbstractStream\n    next_stream: AbstractStream\n\n\nclass Copy(torch.autograd.Function):\n    """"""Copies tensors on specific streams.""""""\n    @staticmethod\n    def forward(ctx: Context,  # type: ignore\n                prev_stream: AbstractStream,\n                next_stream: AbstractStream,\n                *input: Tensor,\n                ) -> Tensors:\n        ctx.prev_stream = prev_stream\n        ctx.next_stream = next_stream\n\n        output = []\n        output_stream = current_stream(get_device(next_stream))\n\n        with use_stream(prev_stream), use_stream(next_stream):\n            for x in input:\n                y = x.to(get_device(next_stream))\n                output.append(y)\n\n                # \'prev_stream\' is not where \'x\' has been allocated.\n                record_stream(x, prev_stream)\n                # \'y\' has been allocated on \'next_stream\'.\n                # It might be used on the current stream captured as \'output_stream\'.\n                record_stream(y, output_stream)\n\n        return tuple(output)\n\n    @staticmethod\n    def backward(ctx: Context,\n                 *grad_output: Tensor,\n                 ) -> Tuple[Optional[Tensor], ...]:\n        prev_stream = ctx.prev_stream\n        next_stream = ctx.next_stream\n\n        grad_input: Deque[Tensor] = deque(maxlen=len(grad_output))\n        input_stream = current_stream(get_device(prev_stream))\n\n        with use_stream(prev_stream), use_stream(next_stream):\n            for x in reversed(grad_output):\n                y = x.to(get_device(prev_stream))\n                grad_input.appendleft(y)\n\n                # \'next_stream\' is not where \'x\' has been allocated.\n                record_stream(x, next_stream)\n                # \'y\' has been allocated on \'prev_stream\'.\n                # It might be used on the current stream captured as \'input_stream\'.\n                record_stream(y, input_stream)\n\n        grad_streams: Tuple[Optional[Tensor], ...] = (None, None)\n        return grad_streams + tuple(grad_input)\n\n\nclass Wait(torch.autograd.Function):\n    """"""Synchronizes a stream to another stream.\n\n    Place it just before you want to start an operation on the next stream,\n    provided that all operations on the previous stream are done.\n\n    """"""\n    @staticmethod\n    def forward(ctx: Context,  # type: ignore\n                prev_stream: AbstractStream,\n                next_stream: AbstractStream,\n                *input: Tensor,\n                ) -> Tensors:\n        ctx.prev_stream = prev_stream\n        ctx.next_stream = next_stream\n\n        wait_stream(next_stream, prev_stream)\n\n        return tuple(x.detach() for x in input)\n\n    @staticmethod\n    def backward(ctx: Context,\n                 *grad_input: Tensor,\n                 ) -> Tuple[Optional[Tensor], ...]:\n        prev_stream = ctx.prev_stream\n        next_stream = ctx.next_stream\n\n        wait_stream(prev_stream, next_stream)\n\n        grad_streams: Tuple[Optional[Tensor], ...] = (None, None)\n        return grad_streams + grad_input\n'"
torchgpipe/dependency.py,4,"b'""""""Arbitrary dependency between two autograd lanes.""""""\nfrom typing import List, Tuple\n\nimport torch\nfrom torch import Tensor\n\nfrom torchgpipe.phony import get_phony\n\n__all__: List[str] = []\n\n\ndef fork(input: Tensor) -> Tuple[Tensor, Tensor]:\n    """"""Branches out from an autograd lane of the given tensor.""""""\n    if torch.is_grad_enabled() and input.requires_grad:\n        input, phony = Fork.apply(input)\n    else:\n        phony = get_phony(input.device, requires_grad=False)\n\n    return input, phony\n\n\nclass Fork(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx: \'Fork\', input: Tensor) -> Tuple[Tensor, Tensor]:  # type: ignore\n        phony = get_phony(input.device, requires_grad=False)\n        return input.detach(), phony.detach()\n\n    @staticmethod\n    def backward(ctx: \'Fork\', grad_input: Tensor, grad_grad: Tensor) -> Tensor:  # type: ignore\n        return grad_input\n\n\ndef join(input: Tensor, phony: Tensor) -> Tensor:\n    """"""Merges two autograd lanes.""""""\n    if torch.is_grad_enabled() and (input.requires_grad or phony.requires_grad):\n        input = Join.apply(input, phony)\n\n    return input\n\n\nclass Join(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx: \'Join\', input: Tensor, phony: Tensor) -> Tensor:  # type: ignore\n        return input.detach()\n\n    @staticmethod\n    def backward(ctx: \'Join\', grad_input: Tensor) -> Tuple[Tensor, None]:  # type: ignore\n        return grad_input, None\n'"
torchgpipe/gpipe.py,19,"b'""""""The GPipe interface.""""""\nfrom collections import OrderedDict\nfrom typing import TYPE_CHECKING, Any, Iterable, List, Optional, Tuple, Union, cast\n\nimport torch\nfrom torch import Tensor, nn\nimport torch.autograd\nimport torch.cuda\n\nfrom torchgpipe import microbatch\nfrom torchgpipe.batchnorm import DeferredBatchNorm\nfrom torchgpipe.pipeline import Pipeline\nfrom torchgpipe.skip.layout import inspect_skip_layout\nfrom torchgpipe.skip.skippable import verify_skippables\nfrom torchgpipe.stream import AbstractStream, new_stream\n\n__all__ = [\'GPipe\']\n\n\nDevice = Union[torch.device, int, str]\nDevices = Union[Iterable[Device], List[Device]]\n\nTensors = Tuple[Tensor, ...]\nTensorOrTensors = Union[Tensor, Tensors]\n\nif TYPE_CHECKING:\n    Module = nn.Module[TensorOrTensors]\n    NamedModules = OrderedDict[str, Module]\nelse:\n    Module = nn.Module\n    NamedModules = OrderedDict\n\n\ndef recommend_auto_balance(message: str) -> str:\n    """"""Expands a message with recommendation to :mod:`torchgpipe.balance`.""""""\n    return f\'\'\'{message}\n\nIf your model is still under development, its optimal balance would change\nfrequently. In this case, we highly recommend \'torchgpipe.balance\' for naive\nautomatic balancing:\n\n  from torchgpipe import GPipe\n  from torchgpipe.balance import balance_by_time\n\n  partitions = torch.cuda.device_count()\n  sample = torch.empty(...)\n  balance = balance_by_time(partitions, model, sample)\n\n  model = GPipe(model, balance, ...)\n\'\'\'\n\n\ndef verify_module(module: nn.Sequential) -> None:\n    if not isinstance(module, nn.Sequential):\n        raise TypeError(\'module must be nn.Sequential to be partitioned\')\n\n    named_children = list(module.named_children())\n    if len(named_children) != len(module):\n        raise ValueError(\'module with duplicate children is not supported\')\n\n    num_parameters = len(list(module.parameters()))\n    num_child_parameters = sum(len(list(child.parameters())) for child in module.children())\n    if num_parameters != num_child_parameters:\n        raise ValueError(\'module with duplicate parameters in distinct children is not supported\')\n\n\nclass BalanceError(ValueError):\n    pass\n\n\ndef split_module(module: nn.Sequential,\n                 balance: Iterable[int],\n                 devices: List[torch.device],\n                 ) -> Tuple[List[nn.Sequential], List[int], List[torch.device]]:\n    """"""Splits a module into multiple partitions.\n\n    Returns:\n        A tuple of (partitions, balance, devices).\n\n        Partitions are represented as a :class:`~torch.nn.ModuleList` whose\n        item is a partition. All layers in a partition are placed in the\n        same device.\n\n    Raises:\n        BalanceError:\n            wrong balance\n        IndexError:\n            the number of devices is fewer than the number of partitions.\n\n    """"""\n    balance = list(balance)\n\n    if len(module) != sum(balance):\n        raise BalanceError(\'module and sum of balance have different length \'\n                           f\'(module: {len(module)}, sum of balance: {sum(balance)})\')\n\n    if any(x <= 0 for x in balance):\n        raise BalanceError(f\'all balance numbers must be positive integer (balance: {balance})\')\n\n    if len(balance) > len(devices):\n        raise IndexError(\'too few devices to hold given partitions \'\n                         f\'(devices: {len(devices)}, partitions: {len(balance)})\')\n\n    j = 0\n    partitions = []\n    layers: NamedModules = OrderedDict()\n\n    for name, layer in module.named_children():\n        layers[name] = layer\n\n        if len(layers) == balance[j]:\n            # Group buffered layers as a partition.\n            partition = nn.Sequential(layers)\n\n            device = devices[j]\n            partition.to(device)\n\n            partitions.append(partition)\n\n            # Prepare for the next partition.\n            layers.clear()\n            j += 1\n\n    partitions = cast(List[nn.Sequential], nn.ModuleList(partitions))\n    del devices[j:]\n\n    return partitions, balance, devices\n\n\nMOVING_DENIED = TypeError(\'denied to move parameters and buffers, \'\n                          \'because GPipe should manage device placement\')\n\n\nclass GPipe(Module):\n    """"""Wraps an arbitrary :class:`nn.Sequential <torch.nn.Sequential>` module\n    to train on GPipe_. If the module requires lots of memory, GPipe will be\n    very efficient.\n    ::\n\n        model = nn.Sequential(a, b, c, d)\n        model = GPipe(model, balance=[1, 1, 1, 1], chunks=8)\n        output = model(input)\n\n    .. _GPipe: https://arxiv.org/abs/1811.06965\n\n    GPipe combines pipeline parallelism with checkpointing to reduce peak\n    memory required to train while minimizing device under-utilization.\n\n    You should determine the balance when defining a :class:`GPipe` module, as\n    balancing will not be done automatically. The module will be partitioned\n    into multiple devices according to the given balance. You may rely on\n    heuristics to find your own optimal configuration.\n\n    Args:\n        module (torch.nn.Sequential):\n            sequential module to be parallelized\n        balance (ints):\n            list of number of layers in each partition\n\n    Keyword Args:\n        devices (iterable of devices):\n            devices to use (default: all CUDA devices)\n        chunks (int):\n            number of micro-batches (default: ``1``)\n        checkpoint (str):\n            when to enable checkpointing, one of ``\'always\'``,\n            ``\'except_last\'``, or ``\'never\'`` (default: ``\'except_last\'``)\n        deferred_batch_norm (bool):\n            whether to use deferred BatchNorm moving statistics (default:\n            :data:`False`, see :ref:`Deferred Batch Normalization` for more\n            details)\n\n    Raises:\n        TypeError:\n            the module is not a :class:`nn.Sequential <torch.nn.Sequential>`.\n        ValueError:\n            invalid arguments, or wrong balance\n        IndexError:\n            the number of devices is fewer than the number of partitions.\n\n    """"""\n\n    #: The number of layers in each partition.\n    balance: List[int] = []\n    #                    ^^\n    # The default value [] required for Sphinx\'s autoattribute.\n\n    #: The devices mapped to each partition.\n    #:\n    #: ``devices[-1]`` refers to the device of the last partition, which means\n    #: it is the output device. Probably, you need to use it to transfer the\n    #: target to calculate the loss without a device mismatch\n    #: :exc:`RuntimeError`. For example::\n    #:\n    #:     out_device = gpipe.devices[-1]\n    #:\n    #:     for input, target in loader:\n    #:         target = target.to(out_device, non_blocking=True)\n    #:         output = gpipe(input)\n    #:         loss = F.cross_entropy(output, target)\n    #:\n    devices: List[torch.device] = []\n\n    #: The number of micro-batches.\n    chunks: int = 1\n\n    #: The checkpoint mode to determine when to enable checkpointing. It is one\n    #: of ``\'always\'``, ``\'except_last\'``, or ``\'never\'``.\n    checkpoint: str = \'except_last\'\n\n    def __init__(self,\n                 module: nn.Sequential,\n                 balance: Optional[Iterable[int]] = None,\n                 *,\n                 devices: Optional[Devices] = None,\n                 chunks: int = chunks,\n                 checkpoint: str = checkpoint,\n                 deferred_batch_norm: bool = False,\n                 ) -> None:\n        super().__init__()\n\n        chunks = int(chunks)\n        checkpoint = str(checkpoint)\n\n        if balance is None:\n            raise ValueError(recommend_auto_balance(\'balance is required\'))\n        if chunks <= 0:\n            raise ValueError(\'number of chunks must be positive integer\')\n        if checkpoint not in [\'always\', \'except_last\', \'never\']:\n            raise ValueError(""checkpoint is not one of \'always\', \'except_last\', or \'never\'"")\n\n        verify_module(module)\n\n        # Verify if the underlying skippable modules satisfy integrity. The\n        # integrity can be verified before forward() because it is static.\n        verify_skippables(module)\n\n        self.chunks = chunks\n        self.checkpoint = checkpoint\n\n        if deferred_batch_norm:\n            module = DeferredBatchNorm.convert_deferred_batch_norm(module, chunks)\n\n        if devices is None:\n            devices = range(torch.cuda.device_count())\n        devices = [torch.device(d) for d in devices]\n        devices = cast(List[torch.device], devices)\n\n        try:\n            self.partitions, self.balance, self.devices = split_module(module, balance, devices)\n        except BalanceError as exc:\n            raise ValueError(recommend_auto_balance(str(exc)))\n\n        self._copy_streams: List[List[AbstractStream]] = []\n        self._skip_layout = inspect_skip_layout(self.partitions)\n\n    def __len__(self) -> int:\n        """"""Counts the length of the underlying sequential module.""""""\n        return sum(len(p) for p in self.partitions)\n\n    def __getitem__(self, index: int) -> nn.Module:\n        """"""Gets a layer in the underlying sequential module.""""""\n        partitions = self.partitions\n        if index < 0:\n            partitions = partitions[::-1]\n\n        for partition in partitions:\n            try:\n                return partition[index]\n            except IndexError:\n                pass\n\n            shift = len(partition)\n\n            if index < 0:\n                index += shift\n            else:\n                index -= shift\n\n        raise IndexError\n\n    def __iter__(self) -> Iterable[nn.Module]:\n        """"""Iterates over children of the underlying sequential module.""""""\n        for partition in self.partitions:\n            yield from partition\n\n    # GPipe should manage the device of each partition.\n    # Deny cuda(), cpu(), and to() with device, by TypeError.\n    def cuda(self, device: Optional[Device] = None) -> \'GPipe\':\n        raise MOVING_DENIED\n\n    def cpu(self) -> \'GPipe\':\n        raise MOVING_DENIED\n\n    def to(self, *args: Any, **kwargs: Any) -> \'GPipe\':\n        # Deny these usages:\n        #\n        # - to(device[, dtype, non_blocking])\n        # - to(tensor[, non_blocking])\n        #\n        # But allow this:\n        #\n        # - to(dtype[, non_blocking])\n        #\n        if \'device\' in kwargs or \'tensor\' in kwargs:\n            raise MOVING_DENIED\n\n        if args:\n            if isinstance(args[0], (torch.device, int, str)):\n                raise MOVING_DENIED\n            if torch.is_tensor(args[0]):\n                raise MOVING_DENIED\n\n        return super().to(*args, **kwargs)\n\n    def _ensure_copy_streams(self) -> List[List[AbstractStream]]:\n        """"""Ensures that :class:`GPipe` caches CUDA streams for copy.\n\n        It\'s worth to cache CUDA streams although PyTorch already manages a\n        pool of pre-allocated CUDA streams, because it may reduce GPU memory\n        fragementation when the number of micro-batches is small.\n\n        """"""\n        if not self._copy_streams:\n            for device in self.devices:\n                self._copy_streams.append([new_stream(device) for _ in range(self.chunks)])\n\n        return self._copy_streams\n\n    def forward(self, input: TensorOrTensors) -> TensorOrTensors:  # type: ignore\n        """""":class:`GPipe` is a fairly transparent module wrapper. It doesn\'t\n        modify the input and output signature of the underlying module. But\n        there\'s type restriction. Input and output have to be a\n        :class:`~torch.Tensor` or a tuple of tensors. This restriction is\n        applied at partition boundaries too.\n\n        Args:\n            input (torch.Tensor or tensors): input mini-batch\n\n        Returns:\n            tensor or tensors: output mini-batch\n\n        Raises:\n            TypeError: input is not a tensor or tensors.\n\n        """"""\n        microbatch.check(input)\n\n        if not self.devices:\n            # Empty sequential module is not illegal.\n            return input\n\n        # Divide a mini-batch into micro-batches.\n        batches = microbatch.scatter(input, self.chunks)\n\n        # Separate CUDA streams for copy.\n        copy_streams = self._ensure_copy_streams()\n\n        # The micro-batch index where the checkpointing stops.\n        if self.training:\n            checkpoint_stop = {\n                \'always\': self.chunks,\n                \'except_last\': self.chunks-1,\n                \'never\': 0,\n            }[self.checkpoint]\n        else:\n            checkpoint_stop = 0\n\n        # Run pipeline parallelism.\n        pipeline = Pipeline(batches,\n                            self.partitions,\n                            self.devices,\n                            copy_streams,\n                            self._skip_layout,\n                            checkpoint_stop)\n        pipeline.run()\n\n        # Merge the micro-batches into one mini-batch.\n        output = microbatch.gather(batches)\n        return output\n'"
torchgpipe/microbatch.py,4,"b'""""""Manipulation of micro-batches.""""""\nimport typing\nfrom typing import Callable, Iterable, Iterator, List, Tuple, Union, cast\n\nimport torch\nfrom torch import Tensor\nimport torch.cuda.comm\n\n__all__: List[str] = []\n\n\nTensors = Tuple[Tensor, ...]\nTensorOrTensors = Union[Tensor, Tensors]\nFunction = Callable[[TensorOrTensors], TensorOrTensors]\n\n\nclass Batch:\n    """"""An abstraction of an atomic tensor or a tuple of tensors. This\n    eliminates every boilerplate code to classify an atomic tensor or a tuple\n    of tensors.\n    ::\n\n        x = generate_tensor_or_tensors()\n        x = Batch(x)\n\n        # in-place update\n        x[0] = F.apply(x[0])\n        x[:] = F.apply(*x)\n\n        # f(x) if x is a tensor.\n        # f(*x) if x is a tuple of tensors.\n        # y is also a batch.\n        y = x.call(f)\n\n    """"""\n\n    def __init__(self, value: TensorOrTensors) -> None:\n        self.value = value\n        self.atomic = torch.is_tensor(value)\n\n    @property\n    def tensor(self) -> Tensor:\n        """"""Retrieves the underlying tensor.""""""\n        if not self.atomic:\n            raise AttributeError(\'not atomic batch\')\n        return cast(Tensor, self.value)\n\n    @property\n    def tensors(self) -> Tensors:\n        """"""Retrieves the underlying tensors.""""""\n        if self.atomic:\n            raise AttributeError(\'batch is atomic\')\n        return cast(Tensors, self.value)\n\n    @property\n    def tensor_or_tensors(self) -> TensorOrTensors:\n        """"""Retrieves the underlying tensor or tensors regardless of type.""""""\n        return self.value\n\n    def call(self, function: Function) -> \'Batch\':\n        """"""Calls a function by the underlying tensor or tensors. It also wraps\n        the output with :class:`Batch`.\n        """"""\n        return Batch(function(self.value))\n\n    def __repr__(self) -> str:\n        return f\'Batch[atomic={self.atomic!r}]({self.value!r})\'\n\n    def __iter__(self) -> Iterator[Tensor]:\n        if self.atomic:\n            yield self.tensor\n        else:\n            yield from self.tensors\n\n    def __len__(self) -> int:\n        return 1 if self.atomic else len(self.tensors)\n\n    def __getitem__(self, index: int) -> Tensor:\n        if not self.atomic:\n            return self.tensors[index]\n\n        if index != 0:\n            raise IndexError(\'atomic batch allows index 0 only\')\n\n        return self.tensor\n\n    # NOTE(sublee): pyflakes can\'t detect ""overload"" instead of ""typing.overload"".\n    @typing.overload\n    def __setitem__(self, index: int, value: Tensor) -> None: ...\n\n    @typing.overload\n    def __setitem__(self, index: slice, value: Tensors) -> None: ...\n\n    def __setitem__(self, index: Union[int, slice], value: TensorOrTensors) -> None:\n        if isinstance(index, int):\n            value = cast(Tensor, value)\n            self._setitem_by_index(index, value)\n        else:\n            value = cast(Tensors, value)\n            self._setitem_by_slice(index, value)\n\n    def _setitem_by_index(self, index: int, value: Tensor) -> None:\n        if not self.atomic:\n            i = index\n            self.value = self.value[:i] + (value,) + self.value[i+1:]\n            return\n\n        if index != 0:\n            raise IndexError(\'atomic batch allows index 0 only\')\n\n        self.value = value\n\n    def _setitem_by_slice(self, index: slice, value: Tensors) -> None:\n        if not (index.start is index.stop is index.step is None):\n            raise NotImplementedError(\'only slice [:] supported\')\n\n        if not self.atomic:\n            self.value = value\n            return\n\n        if len(value) != 1:\n            raise IndexError(\'atomic batch cannot be replaced with multiple tensors\')\n\n        self.value = value[0]\n\n\ndef check(input: TensorOrTensors) -> None:\n    """"""Checks whether the input is a tensor or tensors.\n\n    Raises:\n        TypeError: input is not a tensor or tensors.\n\n    """"""\n    if isinstance(input, tuple):\n        for x in input:\n            check(x)\n        return\n\n    if not isinstance(input, Tensor):\n        raise TypeError(f\'expected Tensor, but got {input.__class__.__name__}\')\n\n\ndef scatter(input: TensorOrTensors, chunks: int) -> List[Batch]:\n    """"""Splits an input mini-batch into multiple micro-batches.""""""\n    inputs: Iterable[TensorOrTensors]\n\n    if isinstance(input, Tensor):\n        inputs = input.chunk(chunks)\n    else:\n        rotated: List[Tensors] = []\n\n        for tensor in input:\n            tensors = tensor.chunk(chunks)\n            rotated.append(cast(Tensors, tensors))\n\n        inputs = zip(*rotated)\n\n    return [Batch(x) for x in inputs]\n\n\ndef gather(outputs: List[Batch]) -> TensorOrTensors:\n    """"""Concatenates output micro-batches into a mini-batch.""""""\n    output: TensorOrTensors\n\n    if outputs[0].atomic:\n        tensors = tuple(b.tensor for b in outputs)\n        output = torch.cat(tensors)\n    else:\n        rotated = [b.tensors for b in outputs]\n        output_buf = []\n\n        for tensors in zip(*rotated):\n            output_buf.append(torch.cat(tensors))\n\n        output = tuple(output_buf)\n\n    return output\n'"
torchgpipe/phony.py,4,"b'""""""Provides phony for arbitrary dependency in a autograd graph.""""""\nfrom typing import Dict, List, Tuple\n\nimport torch\nfrom torch import Tensor\n\nfrom torchgpipe.stream import default_stream, use_stream\n\n__all__: List[str] = []\n\n\n_phonies: Dict[Tuple[torch.device, bool], Tensor] = {}\n\n\ndef get_phony(device: torch.device, *, requires_grad: bool) -> Tensor:\n    """"""Gets a phony. Phony is tensor without space. It is useful to make\n    arbitrary dependency in a autograd graph because it doesn\'t require any\n    gradient accumulation.\n\n    .. note::\n\n        Phonies for each device are cached. If an autograd function gets a phony\n        internally, the phony must be detached to be returned. Otherwise, the\n        autograd engine will mutate the cached phony in-place::\n\n            class Phonify(torch.autograd.Function):\n                @staticmethod\n                def forward(ctx, input):\n                    phony = get_phony(input.device, requires_grad=False)\n                    return phony.detach()  # detach() is necessary.\n\n    """"""\n    key = (device, requires_grad)\n\n    try:\n        phony = _phonies[key]\n    except KeyError:\n        with use_stream(default_stream(device)):\n            phony = torch.empty(0, device=device, requires_grad=requires_grad)\n\n        _phonies[key] = phony\n\n    return phony\n'"
torchgpipe/pipeline.py,2,"b'""""""The pipeline parallelism of GPipe.""""""\nfrom queue import Queue\nfrom types import TracebackType\nfrom typing import TYPE_CHECKING, Iterable, List, Optional, Tuple, Type, Union, cast\n\nimport torch\nfrom torch import Tensor, nn\n\nfrom torchgpipe.checkpoint import Checkpointing\nfrom torchgpipe.copy import Copy, Wait\nfrom torchgpipe.dependency import fork, join\nfrom torchgpipe.microbatch import Batch\nfrom torchgpipe.skip.layout import SkipLayout, inspect_skip_layout\nfrom torchgpipe.skip.tracker import SkipTrackerThroughPotals, use_skip_tracker\nfrom torchgpipe.stream import AbstractStream, current_stream, use_device\nfrom torchgpipe.worker import Task, spawn_workers\n\n__all__: List[str] = []\n\n\nTensors = Tuple[Tensor, ...]\nTensorOrTensors = Union[Tensor, Tensors]\n\nExcInfo = Tuple[Type[BaseException], BaseException, TracebackType]\n\n# Queue is generic only in stubs.\n# https://mypy.readthedocs.io/en/latest/common_issues.html#using-classes-that-are-generic-in-stubs-but-not-at-runtime\nif TYPE_CHECKING:\n    InQueue = Queue[Optional[\'Task\']]\n    OutQueue = Queue[Tuple[bool, Union[Tuple[\'Task\', Batch], ExcInfo, None]]]\nelse:\n    InQueue = Queue\n    OutQueue = Queue\n\n\ndef depend(fork_from: Batch, join_to: Batch) -> None:\n    fork_from[0], phony = fork(fork_from[0])\n    join_to[0] = join(join_to[0], phony)\n\n\ndef copy(batch: Batch, prev_stream: AbstractStream, next_stream: AbstractStream) -> None:\n    batch[:] = Copy.apply(prev_stream, next_stream, *batch)\n\n\ndef wait(batch: Batch, prev_stream: AbstractStream, next_stream: AbstractStream) -> None:\n    batch[:] = Wait.apply(prev_stream, next_stream, *batch)\n\n\ndef clock_cycles(m: int, n: int) -> Iterable[List[Tuple[int, int]]]:\n    """"""Generates schedules for each clock cycle.""""""\n    # m: number of micro-batches\n    # n: number of partitions\n    # i: index of micro-batch\n    # j: index of partition\n    # k: clock number\n    #\n    # k (i,j) (i,j) (i,j)\n    # - ----- ----- -----\n    # 0 (0,0)\n    # 1 (1,0) (0,1)\n    # 2 (2,0) (1,1) (0,2)\n    # 3       (2,1) (1,2)\n    # 4             (2,2)\n    for k in range(m+n-1):\n        yield [(k-j, j) for j in range(max(1+k-m, 0), min(1+k, n))]\n\n\nclass Pipeline:\n    """"""The pipeline parallelism for GPipe.""""""\n\n    def __init__(self,\n                 batches: List[Batch],\n                 partitions: List[nn.Sequential],\n                 devices: Optional[List[torch.device]] = None,\n                 copy_streams: Optional[List[List[AbstractStream]]] = None,\n                 skip_layout: Optional[SkipLayout] = None,\n                 checkpoint_stop: int = 0,\n                 ) -> None:\n        self.batches = batches\n        self.partitions = partitions\n\n        if devices is None:\n            devices = [torch.device(\'cpu\') for _ in partitions]\n        self.devices = devices\n\n        if copy_streams is None:\n            copy_streams = [[current_stream(d)] * len(batches) for d in devices]\n        self.copy_streams = copy_streams\n\n        if skip_layout is None:\n            skip_layout = inspect_skip_layout(partitions)\n\n        self.skip_layout = skip_layout\n        self.checkpoint_stop = checkpoint_stop\n\n    def run(self) -> None:\n        """"""Runs pipeline parallelism.\n\n        It modifies the given batches in place.\n\n        """"""\n        batches = self.batches\n        partitions = self.partitions\n        devices = self.devices\n        skip_layout = self.skip_layout\n\n        m = len(batches)\n        n = len(partitions)\n\n        skip_trackers = [SkipTrackerThroughPotals(skip_layout) for _ in batches]\n\n        with spawn_workers(devices) as (in_queues, out_queues):\n            for schedule in clock_cycles(m, n):\n                self.fence(schedule, skip_trackers)\n                self.compute(schedule, skip_trackers, in_queues, out_queues)\n\n    def fence(self,\n              schedule: List[Tuple[int, int]],\n              skip_trackers: List[SkipTrackerThroughPotals],\n              ) -> None:\n        """"""Copies micro-batches after computation for the previous\n        micro-batches.\n        """"""\n        batches = self.batches\n        copy_streams = self.copy_streams\n        skip_layout = self.skip_layout\n\n        for i, j in schedule:\n            # Ensure that batches[i-1] is executed after batches[i] in\n            # backpropagation by an explicit dependency.\n            if i != 0:\n                depend(batches[i-1], batches[i])\n\n            next_stream = copy_streams[j][i]\n\n            for prev_j, ns, name in skip_layout.copy_policy(j):\n                prev_stream = copy_streams[prev_j][i]\n                skip_trackers[i].copy(batches[i], prev_stream, next_stream, ns, name)\n\n            if j != 0:\n                prev_stream = copy_streams[j-1][i]\n                copy(batches[i], prev_stream, next_stream)\n\n    def compute(self,\n                schedule: List[Tuple[int, int]],\n                skip_trackers: List[SkipTrackerThroughPotals],\n                in_queues: List[InQueue],\n                out_queues: List[OutQueue],\n                ) -> None:\n        """"""Runs tasks with synchronization to copy streams.""""""\n        batches = self.batches\n        partitions = self.partitions\n        devices = self.devices\n        copy_streams = self.copy_streams\n        checkpoint_stop = self.checkpoint_stop\n\n        n = len(partitions)\n        streams = [current_stream(d) for d in devices]\n        exc_info: Optional[ExcInfo] = None\n\n        # With checkpointing, the autograd graph looks like this diagram:\n        # \xe2\x94\x8c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb8\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x90\n        # \xe2\x94\x82    Copy    \xe2\x94\x82\n        # \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb0\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x98   (fence)\n        # \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x95\x82 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80\n        #       \xe2\x94\x83          (compute)\n        # \xe2\x94\x8c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb8\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x90\n        # \xe2\x94\x82    Wait    \xe2\x94\x82 [1] Synchronize the current stream with the copy stream.\n        # \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb0\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x98\n        # \xe2\x94\x8c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb8\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x90\n        # \xe2\x94\x82 Checkpoint \xe2\x94\x82 [2] Compute a partition within checkpointing.\n        # \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb0\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x98\n        # \xe2\x94\x8c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb8\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x90\n        # \xe2\x94\x82    Wait    \xe2\x94\x82 [3] Synchronize the copy stream with the current stream.\n        # \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb0\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x98\n        #       \xe2\x94\xa0 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x90\n        #       \xe2\x94\x83 \xe2\x94\x8c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb4\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x90\n        #       \xe2\x94\x83 \xe2\x94\x82 Recompute \xe2\x94\x82 [4] Schedule the recomputation at backpropagation.\n        #       \xe2\x94\x83 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xac\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x98\n        #       \xe2\x94\xa0 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x98\n        #       \xe2\x94\x83\n        # \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x95\x82 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80 \xe2\x94\x80\n        # \xe2\x94\x8c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb8\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x90   (fence)\n        # \xe2\x94\x82    Copy    \xe2\x94\x82\n        # \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xb0\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x98\n        for i, j in schedule:\n            batch = batches[i]\n            partition = partitions[j]\n\n            # Synchronize with the copied input. ([1] in the diagram)\n            if j != 0:\n                wait(batch, copy_streams[j][i], streams[j])\n\n            # Determine whether checkpointing or not.\n            checkpoint = (i < checkpoint_stop)\n            if checkpoint:\n                def function(input: TensorOrTensors,\n                             partition: nn.Sequential = partition,\n                             skip_tracker: SkipTrackerThroughPotals = skip_trackers[i],\n                             ) -> TensorOrTensors:\n                    with use_skip_tracker(skip_tracker):\n                        return partition(input)\n\n                chk = Checkpointing(function, batch)\n                task = Task(streams[j], compute=chk.checkpoint, finalize=chk.recompute)\n                del function, chk\n\n            else:\n                def compute(batch: Batch = batch,\n                            partition: nn.Sequential = partition,\n                            skip_tracker: SkipTrackerThroughPotals = skip_trackers[i],\n                            ) -> Batch:\n                    with use_skip_tracker(skip_tracker):\n                        return batch.call(partition)\n\n                task = Task(streams[j], compute=compute, finalize=None)\n                del compute\n\n            # Compute tasks in parallel. ([2] in the diagram)\n            in_queues[j].put(task)\n\n        for i, j in schedule:\n            ok, payload = out_queues[j].get()\n\n            # Hold the first exception.\n            if exc_info is not None:\n                continue\n            elif not ok:\n                exc_info = cast(ExcInfo, payload)\n                continue\n\n            task, batch = cast(Tuple[Task, Batch], payload)\n\n            # The copy stream synchronizes to copy the output. ([3] in the\n            # diagram)\n            if j != n-1:\n                wait(batch, streams[j], copy_streams[j][i])\n\n            # Finalize tasks. If checkpointing is enabled, here the\n            # recomputation is scheduled at backpropagation. ([4] in the\n            # diagram)\n            with use_device(devices[j]):\n                task.finalize(batch)\n\n            batches[i] = batch\n\n        # Fail at the first exception.\n        if exc_info is not None:\n            raise exc_info[0].with_traceback(exc_info[1], exc_info[2])\n'"
torchgpipe/stream.py,22,"b'""""""Utilities for eliminating boilerplate code to handle abstract streams with\nCPU device.\n""""""\nfrom contextlib import contextmanager\nfrom typing import Generator, List, Union, cast\n\nimport torch\n\n__all__: List[str] = []\n\n\nclass CPUStreamType:\n    pass\n\n\n# The placeholder on place of streams for the CPU device instead of CUDA.\nCPUStream = CPUStreamType()\n\n# It represents both CUDA streams and the CPU stream.\nAbstractStream = Union[torch.cuda.Stream, CPUStreamType]\n\n\ndef new_stream(device: torch.device) -> AbstractStream:\n    """"""Creates a new stream for either CPU or CUDA device.""""""\n    if device.type != \'cuda\':\n        return CPUStream\n    return torch.cuda.Stream(device)\n\n\ndef current_stream(device: torch.device) -> AbstractStream:\n    """""":func:`torch.cuda.current_stream` for either CPU or CUDA device.""""""\n    if device.type != \'cuda\':\n        return CPUStream\n    return torch.cuda.current_stream(device)\n\n\ndef default_stream(device: torch.device) -> AbstractStream:\n    """""":func:`torch.cuda.default_stream` for either CPU or CUDA device.""""""\n    if device.type != \'cuda\':\n        return CPUStream\n    return torch.cuda.default_stream(device)\n\n\n@contextmanager\ndef use_device(device: torch.device) -> Generator[None, None, None]:\n    """""":func:`torch.cuda.device` for either CPU or CUDA device.""""""\n    if device.type != \'cuda\':\n        yield\n        return\n\n    with torch.cuda.device(device):\n        yield\n\n\n@contextmanager\ndef use_stream(stream: AbstractStream) -> Generator[None, None, None]:\n    """""":func:`torch.cuda.stream` for either CPU or CUDA stream.""""""\n    if not is_cuda(stream):\n        yield\n        return\n\n    with torch.cuda.stream(as_cuda(stream)):\n        yield\n\n\ndef get_device(stream: AbstractStream) -> torch.device:\n    """"""Gets the device from CPU or CUDA stream.""""""\n    if is_cuda(stream):\n        return as_cuda(stream).device\n    return torch.device(\'cpu\')\n\n\ndef wait_stream(source: AbstractStream, target: AbstractStream) -> None:\n    """""":meth:`torch.cuda.Stream.wait_stream` for either CPU or CUDA stream. It\n    makes the source stream wait until the target stream completes work queued.\n    """"""\n    if is_cuda(target):\n        if is_cuda(source):\n            # A CUDA stream waits another CUDA stream.\n            as_cuda(source).wait_stream(as_cuda(target))\n        else:\n            # CPU waits a CUDA stream.\n            as_cuda(target).synchronize()\n\n    # If the target is CPU, synchronization is not required.\n\n\ndef record_stream(tensor: torch.Tensor, stream: AbstractStream) -> None:\n    """""":meth:`torch.Tensor.record_stream` for either CPU or CUDA stream.""""""\n    if is_cuda(stream):\n        # NOTE(sublee): record_stream() on a shifted view tensor throws\n        # RuntimeError in PyTorch 1.1.0, and does nothing in 1.2.0. To safely\n        # protect the tensor against unexpected reallocation, here we use a\n        # temporal tensor associated with the same storage without shifting as\n        # a workaround.\n        #\n        # Issue: https://github.com/pytorch/pytorch/issues/27366\n        #\n        tensor = tensor.new_empty([0]).set_(tensor.storage())\n\n        tensor.record_stream(as_cuda(stream))\n\n\ndef is_cuda(stream: AbstractStream) -> bool:\n    """"""Returns ``True`` if the given stream is a valid CUDA stream.""""""\n    return stream is not CPUStream\n\n\ndef as_cuda(stream: AbstractStream) -> torch.cuda.Stream:\n    """"""Casts the given stream as :class:`torch.cuda.Stream`.""""""\n    return cast(torch.cuda.Stream, stream)\n'"
torchgpipe/worker.py,8,"b'""""""Multithreading in pipeline parallelism.""""""\nfrom contextlib import contextmanager\nfrom queue import Queue\nimport sys\nfrom threading import Thread\nfrom types import TracebackType\nfrom typing import (TYPE_CHECKING, Callable, Dict, Generator, List, Optional, Tuple, Type, Union,\n                    cast)\n\nimport torch\n\nfrom torchgpipe.microbatch import Batch\nfrom torchgpipe.stream import AbstractStream, use_device, use_stream\n\n__all__: List[str] = []\n\n\nExcInfo = Tuple[Type[BaseException], BaseException, TracebackType]\n\n# Queue is generic only in stubs.\n# https://mypy.readthedocs.io/en/latest/common_issues.html#using-classes-that-are-generic-in-stubs-but-not-at-runtime\nif TYPE_CHECKING:\n    InQueue = Queue[Optional[\'Task\']]\n    OutQueue = Queue[Tuple[bool, Union[Tuple[\'Task\', Batch], ExcInfo, None]]]\nelse:\n    InQueue = Queue\n    OutQueue = Queue\n\n\nclass Task:\n    """"""A task represents how to compute a micro-batch on a partition.\n\n    It consists of two parts: :meth:`compute` and :meth:`finalize`.\n    :meth:`compute` should be executed in worker threads concurrently.\n    :meth:`finalize` should be executed after when worker threads complete to\n    execute :meth:`compute`.\n\n    :meth:`compute` might be boosted by worker threads. Because it produces\n    several CUDA API calls by user code. In PyTorch, parallel CUDA API calls\n    are not serialized through GIL. So more than one CUDA API call can be\n    produced at the same time.\n\n    """"""\n\n    def __init__(self,\n                 stream: AbstractStream,\n                 *,\n                 compute: Callable[[], Batch],\n                 finalize: Optional[Callable[[Batch], None]],\n                 ) -> None:\n        self.stream = stream\n        self._compute = compute\n        self._finalize = finalize\n\n    def compute(self) -> Batch:\n        with use_stream(self.stream):\n            return self._compute()\n\n    def finalize(self, batch: Batch) -> None:\n        if self._finalize is None:\n            return\n        with use_stream(self.stream):\n            self._finalize(batch)\n\n\ndef worker(in_queue: InQueue,\n           out_queue: OutQueue,\n           device: torch.device,\n           grad_mode: bool,\n           ) -> None:\n    """"""The main loop of a worker thread.""""""\n    torch.set_grad_enabled(grad_mode)\n\n    with use_device(device):\n        while True:\n            task = in_queue.get()\n\n            if task is None:\n                break\n\n            try:\n                batch = task.compute()\n            except Exception:\n                exc_info = cast(ExcInfo, sys.exc_info())\n                out_queue.put((False, exc_info))\n                continue\n\n            out_queue.put((True, (task, batch)))\n\n    done = (False, None)\n    out_queue.put(done)\n\n\n@contextmanager\ndef spawn_workers(devices: List[torch.device],\n                  ) -> Generator[Tuple[List[InQueue], List[OutQueue]], None, None]:\n    """"""Spawns worker threads. A worker thread is bound to a device.""""""\n    in_queues: List[InQueue] = []\n    out_queues: List[OutQueue] = []\n\n    # Spawn workers.\n    workers: Dict[torch.device, Tuple[InQueue, OutQueue]] = {}\n\n    def normalize_device(device: torch.device) -> torch.device:\n        if device.type == \'cuda\' and device.index is None:\n            return torch.device(\'cuda\', index=torch.cuda.current_device())\n\n        if device.type == \'cpu\' and device.index is not None:\n            return torch.device(\'cpu\')\n\n        return device\n\n    for device in devices:\n        device = normalize_device(device)\n\n        try:\n            in_queue, out_queue = workers[device]\n        except KeyError:\n            in_queue = Queue()\n            out_queue = Queue()\n            workers[device] = (in_queue, out_queue)\n\n            t = Thread(\n                target=worker,\n                args=(in_queue, out_queue, device, torch.is_grad_enabled()),\n                daemon=True,\n            )\n            t.start()\n\n        in_queues.append(in_queue)\n        out_queues.append(out_queue)\n\n    try:\n        yield (in_queues, out_queues)\n    finally:\n        # Close workers.\n        for in_queue in set(in_queues):\n            in_queue.put(None)\n\n        # Join running workers.\n        running = set(out_queues)\n        while running:\n            out_queue = running.pop()\n            ok, payload = out_queue.get()\n\n            done = (False, None)\n            if (ok, payload) == done:\n                continue\n\n            running.add(out_queue)\n'"
benchmarks/amoebanetd-memory/main.py,17,"b'""""""AmoebaNet-D (L, D) Memory Benchmark""""""\nimport platform\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, cast\n\nimport click\nimport torch\nfrom torch import Tensor, nn\nimport torch.nn.functional as F\nfrom torch.optim import RMSprop\n\nfrom amoebanet import amoebanetd\nimport torchgpipe\nfrom torchgpipe import GPipe\n\nStuffs = Tuple[nn.Module, int, int, List[torch.device]]  # (model, L, D, devices)\nExperiment = Callable[[List[int]], Stuffs]\n\n\nclass Experiments:\n\n    @staticmethod\n    def baseline(devices: List[int]) -> Stuffs:\n        L, D = 18, 208\n\n        model = amoebanetd(num_classes=1000, num_layers=L, num_filters=D)\n        device = devices[0]\n        model.to(device)\n\n        return model, L, D, [torch.device(device)]\n\n    @staticmethod\n    def pipeline1(devices: List[int]) -> Stuffs:\n        L, D = 18, 416\n        balance = [24]\n\n        model: nn.Module = amoebanetd(num_classes=1000, num_layers=L, num_filters=D)\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=128)\n\n        return model, L, D, list(model.devices)\n\n    @staticmethod\n    def pipeline2(devices: List[int]) -> Stuffs:\n        L, D = 18, 544\n        balance = [16, 8]\n\n        model: nn.Module = amoebanetd(num_classes=1000, num_layers=L, num_filters=D)\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=128)\n\n        return model, L, D, list(model.devices)\n\n    @staticmethod\n    def pipeline4(devices: List[int]) -> Stuffs:\n        L, D = 36, 544\n        balance = [16, 12, 7, 7]\n\n        model: nn.Module = amoebanetd(num_classes=1000, num_layers=L, num_filters=D)\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=128)\n\n        return model, L, D, list(model.devices)\n\n    @staticmethod\n    def pipeline8(devices: List[int]) -> Stuffs:\n        L, D = 72, 512\n        balance = [23, 16, 11, 6, 6, 5, 5, 6]\n\n        model: nn.Module = amoebanetd(num_classes=1000, num_layers=L, num_filters=D)\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=128)\n\n        return model, L, D, list(model.devices)\n\n\nEXPERIMENTS: Dict[str, Experiment] = {\n    \'baseline\': Experiments.baseline,\n    \'pipeline-1\': Experiments.pipeline1,\n    \'pipeline-2\': Experiments.pipeline2,\n    \'pipeline-4\': Experiments.pipeline4,\n    \'pipeline-8\': Experiments.pipeline8,\n}\n\n\ndef hr() -> None:\n    """"""Prints a horizontal line.""""""\n    width, _ = click.get_terminal_size()\n    click.echo(\'-\' * width)\n\n\ndef parse_devices(ctx: Any, param: Any, value: Optional[str]) -> List[int]:\n    if value is None:\n        return list(range(torch.cuda.device_count()))\n    return [int(x) for x in value.split(\',\')]\n\n\n@click.command()\n@click.pass_context\n@click.argument(\n    \'experiment\',\n    type=click.Choice(sorted(EXPERIMENTS.keys())),\n)\n@click.option(\n    \'--devices\', \'-d\',\n    metavar=\'0,1,2,3\',\n    callback=parse_devices,\n    help=\'Device IDs to use (default: all CUDA devices)\',\n)\ndef cli(ctx: click.Context,\n        experiment: str,\n        devices: List[int],\n        ) -> None:\n    """"""AmoebaNet-D (L, D) Memory Benchmark""""""\n    f = EXPERIMENTS[experiment]\n    try:\n        model, L, D, _devices = f(devices)\n    except ValueError as exc:\n        # Examples:\n        #   ValueError: too few devices to hold given partitions (devices: 1, paritions: 2)\n        ctx.fail(str(exc))\n\n    optimizer = RMSprop(model.parameters())\n\n    in_device = _devices[0]\n    out_device = _devices[-1]\n    torch.cuda.set_device(in_device)\n\n    input = torch.rand(128, 3, 224, 224, device=in_device)\n    target = torch.randint(1000, (128,), device=out_device)\n\n    # HEADER ======================================================================================\n\n    title = f\'{experiment}, AmoebaNet-D ({L}, {D})\'\n    click.echo(title)\n\n    if isinstance(model, GPipe):\n        click.echo(f\'balance: {model.balance}\')\n\n    click.echo(\'torchgpipe: %s, python: %s, torch: %s, cudnn: %s, cuda: %s, gpu: %s\' % (\n        torchgpipe.__version__,\n        platform.python_version(),\n        torch.__version__,\n        torch.backends.cudnn.version(),\n        torch.version.cuda,\n        torch.cuda.get_device_name(in_device)))\n\n    hr()\n\n    # PARAMETERS ==================================================================================\n\n    param_count = sum(p.storage().size() for p in model.parameters())\n    param_size = sum(p.storage().size() * p.storage().element_size() for p in model.parameters())\n    param_scale = 3  # param + grad + RMSprop.quare_avg\n\n    click.echo(f\'# of Model Parameters: {param_count:,}\')\n    click.echo(f\'Total Model Parameter Memory: {param_size*param_scale:,} Bytes\')\n\n    # ACTIVATIONS =================================================================================\n\n    try:\n        torch.cuda.empty_cache()\n        for d in _devices:\n            torch.cuda.reset_max_memory_cached(d)\n\n        for _ in range(2):\n            output = model(input)\n            loss = F.cross_entropy(cast(Tensor, output), target)\n            loss.backward()\n            optimizer.step()\n\n        max_memory = 0\n        for d in _devices:\n            torch.cuda.synchronize(d)\n            max_memory += torch.cuda.max_memory_cached(d)\n\n        latent_size = max_memory - param_size*param_scale\n        click.echo(f\'Peak Activation Memory: {latent_size:,} Bytes\')\n        click.echo(f\'Total Memory: {max_memory:,} Bytes\')\n\n    # MAX MEMORY PER DEVICE =======================================================================\n\n    finally:\n        hr()\n\n        for d in _devices:\n            memory_usage = torch.cuda.memory_cached(d)\n            click.echo(f\'{d!s}: {memory_usage:,} Bytes\')\n\n\nif __name__ == \'__main__\':\n    cli()\n'"
benchmarks/amoebanetd-speed/main.py,14,"b'""""""AmoebaNet-D (18, 256) Speed Benchmark""""""\nimport platform\nimport time\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, cast\n\nimport click\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.optim import SGD\nimport torch.utils.data\n\nfrom amoebanet import amoebanetd\nimport torchgpipe\nfrom torchgpipe import GPipe\n\nStuffs = Tuple[nn.Module, int, List[torch.device]]  # (model, batch_size, devices)\nExperiment = Callable[[nn.Module, List[int]], Stuffs]\n\n\ndef _gpipe(model: nn.Module,\n           devices: List[int],\n           batch_size: int,\n           chunks: int,\n           balance: List[int],\n           checkpoint: str,\n           ) -> Stuffs:\n    model = cast(nn.Sequential, model)\n    model = GPipe(model, balance, devices=devices, chunks=chunks, checkpoint=checkpoint)\n    return model, batch_size, list(model.devices)\n\n\nclass Experiments:\n\n    @staticmethod\n    def n2m1(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 96\n        chunks = 1\n        balance = [7, 17]\n        return _gpipe(model, devices, batch_size, chunks, balance, checkpoint=\'always\')\n\n    @staticmethod\n    def n2m4(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 256\n        chunks = 4\n        balance = [9, 15]\n        return _gpipe(model, devices, batch_size, chunks, balance, checkpoint=\'except_last\')\n\n    @staticmethod\n    def n2m32(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 1280\n        chunks = 32\n        balance = [9, 15]\n        return _gpipe(model, devices, batch_size, chunks, balance, checkpoint=\'except_last\')\n\n    @staticmethod\n    def n4m1(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 160\n        chunks = 1\n        balance = [3, 4, 5, 12]\n        return _gpipe(model, devices, batch_size, chunks, balance, checkpoint=\'always\')\n\n    @staticmethod\n    def n4m4(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 360\n        chunks = 4\n        balance = [3, 6, 7, 8]\n        return _gpipe(model, devices, batch_size, chunks, balance, checkpoint=\'except_last\')\n\n    @staticmethod\n    def n4m32(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 1152\n        chunks = 32\n        balance = [3, 6, 7, 8]\n        return _gpipe(model, devices, batch_size, chunks, balance, checkpoint=\'except_last\')\n\n    @staticmethod\n    def n8m1(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 196\n        chunks = 1\n        balance = [2, 2, 2, 2, 2, 3, 4, 7]\n        return _gpipe(model, devices, batch_size, chunks, balance, checkpoint=\'always\')\n\n    @staticmethod\n    def n8m4(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 480\n        chunks = 4\n        balance = [2, 2, 2, 3, 3, 4, 4, 4]\n        return _gpipe(model, devices, batch_size, chunks, balance, checkpoint=\'except_last\')\n\n    @staticmethod\n    def n8m32(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 1280\n        chunks = 32\n        balance = [2, 2, 2, 3, 3, 4, 4, 4]\n        return _gpipe(model, devices, batch_size, chunks, balance, checkpoint=\'except_last\')\n\n\nEXPERIMENTS: Dict[str, Experiment] = {\n    \'n2m1\': Experiments.n2m1,\n    \'n2m4\': Experiments.n2m4,\n    \'n2m32\': Experiments.n2m32,\n    \'n4m1\': Experiments.n4m1,\n    \'n4m4\': Experiments.n4m4,\n    \'n4m32\': Experiments.n4m32,\n    \'n8m1\': Experiments.n8m1,\n    \'n8m4\': Experiments.n8m4,\n    \'n8m32\': Experiments.n8m32,\n}\n\n\nBASE_TIME: float = 0\n\n\ndef hr() -> None:\n    """"""Prints a horizontal line.""""""\n    width, _ = click.get_terminal_size()\n    click.echo(\'-\' * width)\n\n\ndef log(msg: str, clear: bool = False, nl: bool = True) -> None:\n    """"""Prints a message with elapsed time.""""""\n    if clear:\n        # Clear the output line to overwrite.\n        width, _ = click.get_terminal_size()\n        click.echo(\'\\b\\r\', nl=False)\n        click.echo(\' \' * width, nl=False)\n        click.echo(\'\\b\\r\', nl=False)\n\n    t = time.time() - BASE_TIME\n    h = t // 3600\n    t %= 3600\n    m = t // 60\n    t %= 60\n    s = t\n\n    click.echo(\'%02d:%02d:%02d | \' % (h, m, s), nl=False)\n    click.echo(msg, nl=nl)\n\n\ndef parse_devices(ctx: Any, param: Any, value: Optional[str]) -> List[int]:\n    if value is None:\n        return list(range(torch.cuda.device_count()))\n    return [int(x) for x in value.split(\',\')]\n\n\n@click.command()\n@click.pass_context\n@click.argument(\n    \'experiment\',\n    type=click.Choice(sorted(EXPERIMENTS.keys())),\n)\n@click.option(\n    \'--epochs\', \'-e\',\n    type=int,\n    default=10,\n    help=\'Number of epochs (default: 10)\',\n)\n@click.option(\n    \'--skip-epochs\', \'-k\',\n    type=int,\n    default=1,\n    help=\'Number of epochs to skip in result (default: 1)\',\n)\n@click.option(\n    \'--devices\', \'-d\',\n    metavar=\'0,1,2,3\',\n    callback=parse_devices,\n    help=\'Device IDs to use (default: all CUDA devices)\',\n)\ndef cli(ctx: click.Context,\n        experiment: str,\n        epochs: int,\n        skip_epochs: int,\n        devices: List[int],\n        ) -> None:\n    """"""AmoebaNet-D (18, 256) Speed Benchmark""""""\n    if skip_epochs >= epochs:\n        ctx.fail(\'--skip-epochs=%d must be less than --epochs=%d\' % (skip_epochs, epochs))\n\n    model: nn.Module = amoebanetd(num_classes=1000, num_layers=18, num_filters=256)\n\n    f: Experiment = EXPERIMENTS[experiment]\n    try:\n        model, batch_size, _devices = f(model, devices)\n    except ValueError as exc:\n        # Examples:\n        #   ValueError: too few devices to hold given partitions (devices: 1, paritions: 2)\n        ctx.fail(str(exc))\n\n    optimizer = SGD(model.parameters(), lr=0.1)\n\n    in_device = _devices[0]\n    out_device = _devices[-1]\n    torch.cuda.set_device(in_device)\n\n    # This experiment cares about only training speed, rather than accuracy.\n    # To eliminate any overhead due to data loading, we use fake random 224x224\n    # images over 1000 labels.\n    dataset_size = 10000\n\n    input = torch.rand(batch_size, 3, 224, 224, device=in_device)\n    target = torch.randint(1000, (batch_size,), device=out_device)\n    data = [(input, target)] * (dataset_size//batch_size)\n\n    if dataset_size % batch_size != 0:\n        last_input = input[:dataset_size % batch_size]\n        last_target = target[:dataset_size % batch_size]\n        data.append((last_input, last_target))\n\n    # HEADER ======================================================================================\n\n    title = f\'{experiment}, {skip_epochs+1}-{epochs} epochs\'\n    click.echo(title)\n\n    if isinstance(model, GPipe):\n        click.echo(f\'batch size: {batch_size}, chunks: {model.chunks}, \'\n                   f\'balance: {model.balance}, checkpoint: {model.checkpoint}\')\n    else:\n        click.echo(f\'batch size: {batch_size}\')\n\n    click.echo(\'torchgpipe: %s, python: %s, torch: %s, cudnn: %s, cuda: %s, gpu: %s\' % (\n        torchgpipe.__version__,\n        platform.python_version(),\n        torch.__version__,\n        torch.backends.cudnn.version(),\n        torch.version.cuda,\n        torch.cuda.get_device_name(in_device)))\n\n    # TRAIN =======================================================================================\n\n    global BASE_TIME\n    BASE_TIME = time.time()\n\n    def run_epoch(epoch: int) -> Tuple[float, float]:\n        torch.cuda.synchronize(in_device)\n        tick = time.time()\n\n        data_trained = 0\n        for i, (input, target) in enumerate(data):\n            data_trained += input.size(0)\n\n            output = model(input)\n            loss = F.cross_entropy(output, target)\n            loss.backward()\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # 00:01:02 | 1/20 epoch (42%) | 200.000 samples/sec (estimated)\n            percent = (i+1) / len(data) * 100\n            throughput = data_trained / (time.time()-tick)\n            log(\'%d/%d epoch (%d%%) | %.3f samples/sec (estimated)\'\n                \'\' % (epoch+1, epochs, percent, throughput), clear=True, nl=False)\n\n        torch.cuda.synchronize(in_device)\n        tock = time.time()\n\n        # 00:02:03 | 1/20 epoch | 200.000 samples/sec, 123.456 sec/epoch\n        elapsed_time = tock - tick\n        throughput = dataset_size / elapsed_time\n        log(\'%d/%d epoch | %.3f samples/sec, %.3f sec/epoch\'\n            \'\' % (epoch+1, epochs, throughput, elapsed_time), clear=True)\n\n        return throughput, elapsed_time\n\n    throughputs = []\n    elapsed_times = []\n\n    hr()\n    for epoch in range(epochs):\n        throughput, elapsed_time = run_epoch(epoch)\n\n        if epoch < skip_epochs:\n            continue\n\n        throughputs.append(throughput)\n        elapsed_times.append(elapsed_time)\n    hr()\n\n    # RESULT ======================================================================================\n\n    # pipeline-4, 2-10 epochs | 200.000 samples/sec, 123.456 sec/epoch (average)\n    n = len(throughputs)\n    throughput = sum(throughputs) / n\n    elapsed_time = sum(elapsed_times) / n\n    click.echo(\'%s | %.3f samples/sec, %.3f sec/epoch (average)\'\n               \'\' % (title, throughput, elapsed_time))\n\n\nif __name__ == \'__main__\':\n    cli()\n'"
benchmarks/resnet101-accuracy/main.py,22,"b'""""""ResNet-101 Accuracy Benchmark""""""\nimport platform\nimport time\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, cast\n\nimport click\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.optim import SGD\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.utils.data import DataLoader\nimport torchvision\n\nfrom resnet import resnet101\nfrom torchgpipe import GPipe\n\nStuffs = Tuple[nn.Module, int, List[torch.device]]  # (model, batch_size, devices)\nExperiment = Callable[[nn.Module, List[int]], Stuffs]\n\n\nclass Experiments:\n\n    @staticmethod\n    def naive128(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 128\n        device = devices[0]\n        model.to(device)\n        return model, batch_size, [torch.device(device)]\n\n    @staticmethod\n    def dataparallel256(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 256\n\n        devices = [devices[0], devices[1]]\n        model.to(devices[0])\n        model = nn.DataParallel(model, device_ids=devices, output_device=devices[-1])\n\n        return model, batch_size, [torch.device(device) for device in devices]\n\n    @staticmethod\n    def dataparallel1k(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 1024\n\n        devices = [devices[0], devices[1], devices[2], devices[3]]\n        model.to(devices[0])\n        model = nn.DataParallel(model, device_ids=devices, output_device=devices[-1])\n\n        return model, batch_size, [torch.device(device) for device in devices]\n\n    @staticmethod\n    def gpipe256(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 256\n        chunks = 4\n        balance = [140, 230]\n\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=chunks)\n\n        return model, batch_size, list(model.devices)\n\n    @staticmethod\n    def gpipe1k(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 1024\n        chunks = 64\n        balance = [26, 22, 33, 44, 44, 66, 66, 69]\n\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=chunks)\n\n        return model, batch_size, list(model.devices)\n\n    @staticmethod\n    def gpipe4k(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 4096\n        chunks = 64\n        balance = [26, 22, 33, 44, 44, 66, 66, 69]\n\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=chunks)\n\n        return model, batch_size, list(model.devices)\n\n\nEXPERIMENTS: Dict[str, Experiment] = {\n    \'naive-128\': Experiments.naive128,\n    \'dataparallel-256\': Experiments.dataparallel256,\n    \'dataparallel-1k\': Experiments.dataparallel1k,\n    \'gpipe-256\': Experiments.gpipe256,\n    \'gpipe-1k\': Experiments.gpipe1k,\n    \'gpipe-4k\': Experiments.gpipe4k,\n}\n\n\ndef dataloaders(batch_size: int, num_workers: int = 32) -> Tuple[DataLoader, DataLoader]:\n    num_workers = num_workers if batch_size <= 4096 else num_workers // 2\n\n    post_transforms = torchvision.transforms.Compose([\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ])\n\n    train_dataset = torchvision.datasets.ImageNet(\n        root=\'imagenet\',\n        split=\'train\',\n        transform=torchvision.transforms.Compose([\n            torchvision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0)),\n            torchvision.transforms.RandomHorizontalFlip(),\n            post_transforms,\n        ])\n    )\n    test_dataset = torchvision.datasets.ImageNet(\n        root=\'imagenet\',\n        split=\'val\',\n        transform=torchvision.transforms.Compose([\n            torchvision.transforms.Resize(256),\n            torchvision.transforms.CenterCrop(224),\n            post_transforms,\n        ])\n    )\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=True,\n        shuffle=True,\n    )\n    test_dataloader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=True,\n        drop_last=False,\n        shuffle=False,\n    )\n\n    return train_dataloader, test_dataloader\n\n\nBASE_TIME: float = 0\n\n\ndef hr() -> None:\n    """"""Prints a horizontal line.""""""\n    width, _ = click.get_terminal_size()\n    click.echo(\'-\' * width)\n\n\ndef log(msg: str, clear: bool = False, nl: bool = True) -> None:\n    """"""Prints a message with elapsed time.""""""\n    if clear:\n        # Clear the output line to overwrite.\n        width, _ = click.get_terminal_size()\n        click.echo(\'\\b\\r\', nl=False)\n        click.echo(\' \' * width, nl=False)\n        click.echo(\'\\b\\r\', nl=False)\n\n    t = time.time() - BASE_TIME\n    h = t // 3600\n    t %= 3600\n    m = t // 60\n    t %= 60\n    s = t\n\n    click.echo(\'%02d:%02d:%02d | \' % (h, m, s), nl=False)\n    click.echo(msg, nl=nl)\n\n\ndef parse_devices(ctx: Any, param: Any, value: Optional[str]) -> List[int]:\n    if value is None:\n        return list(range(torch.cuda.device_count()))\n    return [int(x) for x in value.split(\',\')]\n\n\n@click.command()\n@click.pass_context\n@click.argument(\n    \'experiment\',\n    type=click.Choice(sorted(EXPERIMENTS.keys())),\n)\n@click.option(\n    \'--epochs\', \'-e\',\n    type=int,\n    default=90,\n    help=\'Number of epochs (default: 10)\',\n)\n@click.option(\n    \'--skip-epochs\', \'-k\',\n    type=int,\n    default=1,\n    help=\'Number of epochs to skip in result (default: 1)\',\n)\n@click.option(\n    \'--devices\', \'-d\',\n    metavar=\'0,1,2,3\',\n    callback=parse_devices,\n    help=\'Device IDs to use (default: all CUDA devices)\',\n)\ndef cli(ctx: click.Context,\n        experiment: str,\n        epochs: int,\n        skip_epochs: int,\n        devices: List[int],\n        ) -> None:\n    """"""ResNet-101 Accuracy Benchmark""""""\n    if skip_epochs > epochs:\n        ctx.fail(\'--skip-epochs=%d must be less than --epochs=%d\' % (skip_epochs, epochs))\n\n    relu_inplace = \'gpipe\' not in experiment\n    model: nn.Module = resnet101(num_classes=1000, inplace=relu_inplace)\n\n    f = EXPERIMENTS[experiment]\n    try:\n        model, batch_size, _devices = f(model, devices)\n    except ValueError as exc:\n        # Examples:\n        #   ValueError: too few devices to hold given partitions (devices: 1, paritions: 2)\n        ctx.fail(str(exc))\n\n    # Prepare dataloaders.\n    train_dataloader, valid_dataloader = dataloaders(batch_size)\n\n    # Optimizer with LR scheduler\n    steps = len(train_dataloader)\n    lr_multiplier = max(1.0, batch_size / 256)\n    optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4, nesterov=True)\n\n    def gradual_warmup_linear_scaling(step: int) -> float:\n        epoch = step / steps\n\n        # Gradual warmup\n        warmup_ratio = min(4.0, epoch) / 4.0\n        multiplier = warmup_ratio * (lr_multiplier - 1.0) + 1.0\n\n        if epoch < 30:\n            return 1.0 * multiplier\n        elif epoch < 60:\n            return 0.1 * multiplier\n        elif epoch < 80:\n            return 0.01 * multiplier\n        return 0.001 * multiplier\n\n    scheduler = LambdaLR(optimizer, lr_lambda=gradual_warmup_linear_scaling)\n\n    in_device = _devices[0]\n    out_device = _devices[-1]\n    torch.cuda.set_device(in_device)\n\n    # HEADER ======================================================================================\n\n    title = \'%s, %d devices, %d batch, %d-%d epochs\'\\\n            \'\' % (experiment, len(_devices), batch_size, skip_epochs+1, epochs)\n    click.echo(title)\n    click.echo(\'python: %s, torch: %s, cudnn: %s, cuda: %s, gpu: %s\' % (\n        platform.python_version(),\n        torch.__version__,\n        torch.backends.cudnn.version(),\n        torch.version.cuda,\n        torch.cuda.get_device_name(in_device)))\n\n    # TRAIN =======================================================================================\n\n    global BASE_TIME\n    BASE_TIME = time.time()\n\n    def evaluate(dataloader: DataLoader) -> Tuple[float, float]:\n        tick = time.time()\n        steps = len(dataloader)\n        data_tested = 0\n        loss_sum = torch.zeros(1, device=out_device)\n        accuracy_sum = torch.zeros(1, device=out_device)\n        model.eval()\n        with torch.no_grad():\n            for i, (input, target) in enumerate(dataloader):\n                current_batch = input.size(0)\n                data_tested += current_batch\n                input = input.to(device=in_device)\n                target = target.to(device=out_device)\n\n                output = model(input)\n\n                loss = F.cross_entropy(output, target)\n                loss_sum += loss * current_batch\n\n                _, predicted = torch.max(output, 1)\n                correct = (predicted == target).sum()\n                accuracy_sum += correct\n\n                percent = i / steps * 100\n                throughput = data_tested / (time.time() - tick)\n                log(\'valid | %d%% | %.3f samples/sec (estimated)\'\n                    \'\' % (percent, throughput), clear=True, nl=False)\n\n        loss = loss_sum / data_tested\n        accuracy = accuracy_sum / data_tested\n\n        return loss.item(), accuracy.item()\n\n    def run_epoch(epoch: int) -> Tuple[float, float]:\n        torch.cuda.synchronize(in_device)\n        tick = time.time()\n\n        steps = len(train_dataloader)\n        data_trained = 0\n        loss_sum = torch.zeros(1, device=out_device)\n        model.train()\n        for i, (input, target) in enumerate(train_dataloader):\n            data_trained += batch_size\n            input = input.to(device=in_device, non_blocking=True)\n            target = target.to(device=out_device, non_blocking=True)\n\n            output = model(input)\n            loss = F.cross_entropy(output, target)\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            scheduler.step()\n\n            loss_sum += loss.detach() * batch_size\n\n            percent = i / steps * 100\n            throughput = data_trained / (time.time()-tick)\n            log(\'train | %d/%d epoch (%d%%) | lr:%.5f | %.3f samples/sec (estimated)\'\n                \'\' % (epoch+1, epochs, percent, scheduler.get_lr()[0], throughput),\n                clear=True, nl=False)\n\n        torch.cuda.synchronize(in_device)\n        tock = time.time()\n\n        train_loss = loss_sum.item() / data_trained\n        valid_loss, valid_accuracy = evaluate(valid_dataloader)\n        torch.cuda.synchronize(in_device)\n\n        elapsed_time = tock - tick\n        throughput = data_trained / elapsed_time\n        log(\'%d/%d epoch | lr:%.5f | train loss:%.3f %.3f samples/sec | \'\n            \'valid loss:%.3f accuracy:%.3f\'\n            \'\' % (epoch+1, epochs, scheduler.get_lr()[0], train_loss, throughput,\n                  valid_loss, valid_accuracy),\n            clear=True)\n\n        return throughput, elapsed_time\n\n    throughputs = []\n    elapsed_times = []\n\n    hr()\n    for epoch in range(epochs):\n        throughput, elapsed_time = run_epoch(epoch)\n\n        if epoch < skip_epochs:\n            continue\n\n        throughputs.append(throughput)\n        elapsed_times.append(elapsed_time)\n\n    _, valid_accuracy = evaluate(valid_dataloader)\n    hr()\n\n    # RESULT ======================================================================================\n\n    # pipeline-4, 2-10 epochs | 200.000 samples/sec, 123.456 sec/epoch (average)\n    n = len(throughputs)\n    throughput = sum(throughputs) / n if n > 0 else 0.0\n    elapsed_time = sum(elapsed_times) / n if n > 0 else 0.0\n    click.echo(\'%s | valid accuracy: %.4f | %.3f samples/sec, %.3f sec/epoch (average)\'\n               \'\' % (title, valid_accuracy, throughput, elapsed_time))\n\n\nif __name__ == \'__main__\':\n    cli()\n'"
benchmarks/resnet101-speed/main.py,14,"b'""""""ResNet-101 Speed Benchmark""""""\nimport platform\nimport time\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, cast\n\nimport click\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.optim import SGD\n\nfrom resnet import resnet101\nimport torchgpipe\nfrom torchgpipe import GPipe\n\nStuffs = Tuple[nn.Module, int, List[torch.device]]  # (model, batch_size, devices)\nExperiment = Callable[[nn.Module, List[int]], Stuffs]\n\n\nclass Experiments:\n\n    @staticmethod\n    def baseline(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 118\n        device = devices[0]\n        model.to(device)\n        return model, batch_size, [torch.device(device)]\n\n    @staticmethod\n    def pipeline1(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 220\n        chunks = 2\n        balance = [370]\n\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=chunks)\n        return model, batch_size, list(model.devices)\n\n    @staticmethod\n    def pipeline2(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 25000\n        chunks = 1667\n        balance = [135, 235]\n\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=chunks)\n        return model, batch_size, list(model.devices)\n\n    @staticmethod\n    def pipeline4(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 5632\n        chunks = 256\n        balance = [44, 92, 124, 110]\n\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=chunks)\n        return model, batch_size, list(model.devices)\n\n    @staticmethod\n    def pipeline8(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 5400\n        chunks = 150\n        balance = [26, 22, 33, 44, 44, 66, 66, 69]\n\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=chunks)\n        return model, batch_size, list(model.devices)\n\n\nEXPERIMENTS: Dict[str, Experiment] = {\n    \'baseline\': Experiments.baseline,\n    \'pipeline-1\': Experiments.pipeline1,\n    \'pipeline-2\': Experiments.pipeline2,\n    \'pipeline-4\': Experiments.pipeline4,\n    \'pipeline-8\': Experiments.pipeline8,\n}\n\n\nBASE_TIME: float = 0\n\n\ndef hr() -> None:\n    """"""Prints a horizontal line.""""""\n    width, _ = click.get_terminal_size()\n    click.echo(\'-\' * width)\n\n\ndef log(msg: str, clear: bool = False, nl: bool = True) -> None:\n    """"""Prints a message with elapsed time.""""""\n    if clear:\n        # Clear the output line to overwrite.\n        width, _ = click.get_terminal_size()\n        click.echo(\'\\b\\r\', nl=False)\n        click.echo(\' \' * width, nl=False)\n        click.echo(\'\\b\\r\', nl=False)\n\n    t = time.time() - BASE_TIME\n    h = t // 3600\n    t %= 3600\n    m = t // 60\n    t %= 60\n    s = t\n\n    click.echo(\'%02d:%02d:%02d | \' % (h, m, s), nl=False)\n    click.echo(msg, nl=nl)\n\n\ndef parse_devices(ctx: Any, param: Any, value: Optional[str]) -> List[int]:\n    if value is None:\n        return list(range(torch.cuda.device_count()))\n    return [int(x) for x in value.split(\',\')]\n\n\n@click.command()\n@click.pass_context\n@click.argument(\n    \'experiment\',\n    type=click.Choice(sorted(EXPERIMENTS.keys())),\n)\n@click.option(\n    \'--epochs\', \'-e\',\n    type=int,\n    default=10,\n    help=\'Number of epochs (default: 10)\',\n)\n@click.option(\n    \'--skip-epochs\', \'-k\',\n    type=int,\n    default=1,\n    help=\'Number of epochs to skip in result (default: 1)\',\n)\n@click.option(\n    \'--devices\', \'-d\',\n    metavar=\'0,1,2,3\',\n    callback=parse_devices,\n    help=\'Device IDs to use (default: all CUDA devices)\',\n)\ndef cli(ctx: click.Context,\n        experiment: str,\n        epochs: int,\n        skip_epochs: int,\n        devices: List[int],\n        ) -> None:\n    """"""ResNet-101 Speed Benchmark""""""\n    if skip_epochs >= epochs:\n        ctx.fail(\'--skip-epochs=%d must be less than --epochs=%d\' % (skip_epochs, epochs))\n\n    model: nn.Module = resnet101(num_classes=1000)\n\n    f = EXPERIMENTS[experiment]\n    try:\n        model, batch_size, _devices = f(model, devices)\n    except ValueError as exc:\n        # Examples:\n        #   ValueError: too few devices to hold given partitions (devices: 1, paritions: 2)\n        ctx.fail(str(exc))\n\n    optimizer = SGD(model.parameters(), lr=0.1)\n\n    in_device = _devices[0]\n    out_device = _devices[-1]\n    torch.cuda.set_device(in_device)\n\n    # This experiment cares about only training speed, rather than accuracy.\n    # To eliminate any overhead due to data loading, we use fake random 224x224\n    # images over 1000 labels.\n    dataset_size = 50000\n\n    input = torch.rand(batch_size, 3, 224, 224, device=in_device)\n    target = torch.randint(1000, (batch_size,), device=out_device)\n    data = [(input, target)] * (dataset_size//batch_size)\n\n    if dataset_size % batch_size != 0:\n        last_input = input[:dataset_size % batch_size]\n        last_target = target[:dataset_size % batch_size]\n        data.append((last_input, last_target))\n\n    # HEADER ======================================================================================\n\n    title = f\'{experiment}, {skip_epochs+1}-{epochs} epochs\'\n    click.echo(title)\n\n    if isinstance(model, GPipe):\n        click.echo(f\'batch size: {batch_size}, chunks: {model.chunks}, balance: {model.balance}\')\n    else:\n        click.echo(f\'batch size: {batch_size}\')\n\n    click.echo(\'torchgpipe: %s, python: %s, torch: %s, cudnn: %s, cuda: %s, gpu: %s\' % (\n        torchgpipe.__version__,\n        platform.python_version(),\n        torch.__version__,\n        torch.backends.cudnn.version(),\n        torch.version.cuda,\n        torch.cuda.get_device_name(in_device)))\n\n    # TRAIN =======================================================================================\n\n    global BASE_TIME\n    BASE_TIME = time.time()\n\n    def run_epoch(epoch: int) -> Tuple[float, float]:\n        torch.cuda.synchronize(in_device)\n        tick = time.time()\n\n        data_trained = 0\n        for i, (input, target) in enumerate(data):\n            data_trained += input.size(0)\n\n            output = model(input)\n            loss = F.cross_entropy(output, target)\n            loss.backward()\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # 00:01:02 | 1/20 epoch (42%) | 200.000 samples/sec (estimated)\n            percent = (i+1) / len(data) * 100\n            throughput = data_trained / (time.time()-tick)\n            log(\'%d/%d epoch (%d%%) | %.3f samples/sec (estimated)\'\n                \'\' % (epoch+1, epochs, percent, throughput), clear=True, nl=False)\n\n        torch.cuda.synchronize(in_device)\n        tock = time.time()\n\n        # 00:02:03 | 1/20 epoch | 200.000 samples/sec, 123.456 sec/epoch\n        elapsed_time = tock - tick\n        throughput = dataset_size / elapsed_time\n        log(\'%d/%d epoch | %.3f samples/sec, %.3f sec/epoch\'\n            \'\' % (epoch+1, epochs, throughput, elapsed_time), clear=True)\n\n        return throughput, elapsed_time\n\n    throughputs = []\n    elapsed_times = []\n\n    hr()\n    for epoch in range(epochs):\n        throughput, elapsed_time = run_epoch(epoch)\n\n        if epoch < skip_epochs:\n            continue\n\n        throughputs.append(throughput)\n        elapsed_times.append(elapsed_time)\n    hr()\n\n    # RESULT ======================================================================================\n\n    # pipeline-4, 2-10 epochs | 200.000 samples/sec, 123.456 sec/epoch (average)\n    n = len(throughputs)\n    throughput = sum(throughputs) / n\n    elapsed_time = sum(elapsed_times) / n\n    click.echo(\'%s | %.3f samples/sec, %.3f sec/epoch (average)\'\n               \'\' % (title, throughput, elapsed_time))\n\n\nif __name__ == \'__main__\':\n    cli()\n'"
benchmarks/unet-memory/main.py,17,"b'""""""U-Net Memory Benchmark""""""\nimport platform\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, cast\n\nimport click\nimport torch\nfrom torch import Tensor, nn\nimport torch.nn.functional as F\nfrom torch.optim import SGD\n\nimport torchgpipe\nfrom torchgpipe import GPipe\nfrom unet import unet\n\nStuffs = Tuple[nn.Module, int, int, List[torch.device]]  # (model, B, C, devices)\nExperiment = Callable[[List[int]], Stuffs]\n\n\nclass Experiments:\n\n    @staticmethod\n    def baseline(devices: List[int]) -> Stuffs:\n        B, C = 6, 72\n\n        model = unet(depth=5, num_convs=B, base_channels=C,\n                     input_channels=3, output_channels=1)\n        device = devices[0]\n        model.to(device)\n\n        return model, B, C, [torch.device(device)]\n\n    @staticmethod\n    def pipeline1(devices: List[int]) -> Stuffs:\n        B, C = 11, 128\n        balance = [505]\n\n        model: nn.Module = unet(depth=5, num_convs=B, base_channels=C,\n                                input_channels=3, output_channels=1)\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=32)\n\n        return model, B, C, list(model.devices)\n\n    @staticmethod\n    def pipeline2(devices: List[int]) -> Stuffs:\n        B, C = 24, 128\n        balance = [526, 551]\n\n        model: nn.Module = unet(depth=5, num_convs=B, base_channels=C,\n                                input_channels=3, output_channels=1)\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=32)\n\n        return model, B, C, list(model.devices)\n\n    @staticmethod\n    def pipeline4(devices: List[int]) -> Stuffs:\n        B, C = 24, 160\n        balance = [472, 54, 36, 515]\n\n        model: nn.Module = unet(depth=5, num_convs=B, base_channels=C,\n                                input_channels=3, output_channels=1)\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=32)\n\n        return model, B, C, list(model.devices)\n\n    @staticmethod\n    def pipeline8(devices: List[int]) -> Stuffs:\n        B, C = 48, 160\n        balance = [800, 140, 62, 36, 36, 36, 36, 987]\n\n        model: nn.Module = unet(depth=5, num_convs=B, base_channels=C,\n                                input_channels=3, output_channels=1)\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=128)\n\n        return model, B, C, list(model.devices)\n\n\nEXPERIMENTS: Dict[str, Experiment] = {\n    \'baseline\': Experiments.baseline,\n    \'pipeline-1\': Experiments.pipeline1,\n    \'pipeline-2\': Experiments.pipeline2,\n    \'pipeline-4\': Experiments.pipeline4,\n    \'pipeline-8\': Experiments.pipeline8,\n}\n\n\ndef hr() -> None:\n    """"""Prints a horizontal line.""""""\n    width, _ = click.get_terminal_size()\n    click.echo(\'-\' * width)\n\n\ndef parse_devices(ctx: Any, param: Any, value: Optional[str]) -> List[int]:\n    if value is None:\n        return list(range(torch.cuda.device_count()))\n    return [int(x) for x in value.split(\',\')]\n\n\n@click.command()\n@click.pass_context\n@click.argument(\n    \'experiment\',\n    type=click.Choice(sorted(EXPERIMENTS.keys())),\n)\n@click.option(\n    \'--devices\', \'-d\',\n    metavar=\'0,1,2,3\',\n    callback=parse_devices,\n    help=\'Device IDs to use (default: all CUDA devices)\',\n)\ndef cli(ctx: click.Context,\n        experiment: str,\n        devices: List[int],\n        ) -> None:\n    """"""U-Net (B, C) Memory Benchmark""""""\n    f = EXPERIMENTS[experiment]\n    try:\n        model, B, C, _devices = f(devices)\n    except ValueError as exc:\n        # Examples:\n        #   ValueError: too few devices to hold given partitions (devices: 1, paritions: 2)\n        ctx.fail(str(exc))\n\n    optimizer = SGD(model.parameters(), lr=0.1)\n\n    in_device = _devices[0]\n    out_device = _devices[-1]\n    torch.cuda.set_device(in_device)\n\n    input = torch.rand(32, 3, 192, 192, device=in_device)\n    target = torch.rand(32, 1, 192, 192, device=out_device)\n\n    # HEADER ======================================================================================\n\n    title = f\'{experiment}, U-Net ({B}, {C})\'\n    click.echo(title)\n\n    if isinstance(model, GPipe):\n        click.echo(f\'balance: {model.balance}\')\n\n    click.echo(\'torchgpipe: %s, python: %s, torch: %s, cudnn: %s, cuda: %s, gpu: %s\' % (\n        torchgpipe.__version__,\n        platform.python_version(),\n        torch.__version__,\n        torch.backends.cudnn.version(),\n        torch.version.cuda,\n        torch.cuda.get_device_name(in_device)))\n\n    hr()\n\n    # PARAMETERS ==================================================================================\n\n    param_count = sum(p.storage().size() for p in model.parameters())\n    param_size = sum(p.storage().size() * p.storage().element_size() for p in model.parameters())\n    param_scale = 2  # param + grad\n\n    click.echo(f\'# of Model Parameters: {param_count:,}\')\n    click.echo(f\'Total Model Parameter Memory: {param_size*param_scale:,} Bytes\')\n\n    # ACTIVATIONS =================================================================================\n\n    try:\n        torch.cuda.empty_cache()\n        for d in _devices:\n            torch.cuda.reset_max_memory_cached(d)\n\n        for _ in range(2):\n            output = model(input)\n            output = cast(Tensor, output)\n            loss = F.binary_cross_entropy_with_logits(output, target)\n            loss.backward()\n            optimizer.step()\n\n        max_memory = 0\n        for d in _devices:\n            torch.cuda.synchronize(d)\n            max_memory += torch.cuda.max_memory_cached(d)\n\n        latent_size = max_memory - param_size*param_scale\n        click.echo(f\'Peak Activation Memory: {latent_size:,} Bytes\')\n        click.echo(f\'Total Memory: {max_memory:,} Bytes\')\n\n    # MAX MEMORY PER DEVICE =======================================================================\n\n    finally:\n        hr()\n\n        for d in _devices:\n            memory_usage = torch.cuda.memory_cached(d)\n            click.echo(f\'{d!s}: {memory_usage:,} Bytes\')\n\n\nif __name__ == \'__main__\':\n    cli()\n'"
benchmarks/unet-speed/main.py,15,"b'""""""U-Net Speed Benchmark""""""\nimport platform\nimport time\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, cast\n\nimport click\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.optim import SGD\nimport torch.utils.data\n\nimport torchgpipe\nfrom torchgpipe import GPipe\nfrom unet import unet\n\nStuffs = Tuple[nn.Module, int, List[torch.device]]  # (model, batch_size, devices)\nExperiment = Callable[[nn.Module, List[int]], Stuffs]\n\n\nclass Experiments:\n\n    @staticmethod\n    def baseline(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 40\n        device = devices[0]\n        model.to(device)\n        return model, batch_size, [torch.device(device)]\n\n    @staticmethod\n    def pipeline1(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 80\n        chunks = 2\n        balance = [241]\n\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=chunks)\n        return model, batch_size, list(model.devices)\n\n    @staticmethod\n    def pipeline2(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 512\n        chunks = 32\n        balance = [104, 137]\n\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=chunks)\n        return model, batch_size, list(model.devices)\n\n    @staticmethod\n    def pipeline4(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 512\n        chunks = 16\n        balance = [30, 66, 84, 61]\n\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=chunks)\n        return model, batch_size, list(model.devices)\n\n    @staticmethod\n    def pipeline8(model: nn.Module, devices: List[int]) -> Stuffs:\n        batch_size = 640\n        chunks = 40\n        balance = [16, 27, 31, 44, 22, 57, 27, 17]\n\n        model = cast(nn.Sequential, model)\n        model = GPipe(model, balance, devices=devices, chunks=chunks)\n        return model, batch_size, list(model.devices)\n\n\nEXPERIMENTS: Dict[str, Experiment] = {\n    \'baseline\': Experiments.baseline,\n    \'pipeline-1\': Experiments.pipeline1,\n    \'pipeline-2\': Experiments.pipeline2,\n    \'pipeline-4\': Experiments.pipeline4,\n    \'pipeline-8\': Experiments.pipeline8,\n}\n\n\nBASE_TIME: float = 0\n\n\ndef hr() -> None:\n    """"""Prints a horizontal line.""""""\n    width, _ = click.get_terminal_size()\n    click.echo(\'-\' * width)\n\n\ndef log(msg: str, clear: bool = False, nl: bool = True) -> None:\n    """"""Prints a message with elapsed time.""""""\n    if clear:\n        # Clear the output line to overwrite.\n        width, _ = click.get_terminal_size()\n        click.echo(\'\\b\\r\', nl=False)\n        click.echo(\' \' * width, nl=False)\n        click.echo(\'\\b\\r\', nl=False)\n\n    t = time.time() - BASE_TIME\n    h = t // 3600\n    t %= 3600\n    m = t // 60\n    t %= 60\n    s = t\n\n    click.echo(\'%02d:%02d:%02d | \' % (h, m, s), nl=False)\n    click.echo(msg, nl=nl)\n\n\ndef parse_devices(ctx: Any, param: Any, value: Optional[str]) -> List[int]:\n    if value is None:\n        return list(range(torch.cuda.device_count()))\n    return [int(x) for x in value.split(\',\')]\n\n\n@click.command()\n@click.pass_context\n@click.argument(\n    \'experiment\',\n    type=click.Choice(sorted(EXPERIMENTS.keys())),\n)\n@click.option(\n    \'--epochs\', \'-e\',\n    type=int,\n    default=10,\n    help=\'Number of epochs (default: 10)\',\n)\n@click.option(\n    \'--skip-epochs\', \'-k\',\n    type=int,\n    default=1,\n    help=\'Number of epochs to skip in result (default: 1)\',\n)\n@click.option(\n    \'--devices\', \'-d\',\n    metavar=\'0,1,2,3\',\n    callback=parse_devices,\n    help=\'Device IDs to use (default: all CUDA devices)\',\n)\ndef cli(ctx: click.Context,\n        experiment: str,\n        epochs: int,\n        skip_epochs: int,\n        devices: List[int],\n        ) -> None:\n    """"""U-Net Speed Benchmark""""""\n    if skip_epochs >= epochs:\n        ctx.fail(\'--skip-epochs=%d must be less than --epochs=%d\' % (skip_epochs, epochs))\n\n    model: nn.Module = unet(depth=5, num_convs=5, base_channels=64,\n                            input_channels=3, output_channels=1)\n\n    f: Experiment = EXPERIMENTS[experiment]\n    try:\n        model, batch_size, _devices = f(model, devices)\n    except ValueError as exc:\n        # Examples:\n        #   ValueError: too few devices to hold given partitions (devices: 1, paritions: 2)\n        ctx.fail(str(exc))\n\n    optimizer = SGD(model.parameters(), lr=0.1)\n\n    in_device = _devices[0]\n    out_device = _devices[-1]\n    torch.cuda.set_device(in_device)\n\n    dataset_size = 10000\n\n    input = torch.rand(batch_size, 3, 192, 192, device=in_device)\n    target = torch.ones(batch_size, 1, 192, 192, device=out_device)\n    data = [(input, target)] * (dataset_size//batch_size)\n\n    if dataset_size % batch_size != 0:\n        last_input = input[:dataset_size % batch_size]\n        last_target = target[:dataset_size % batch_size]\n        data.append((last_input, last_target))\n\n    # HEADER ======================================================================================\n\n    title = f\'{experiment}, {skip_epochs+1}-{epochs} epochs\'\n    click.echo(title)\n\n    if isinstance(model, GPipe):\n        click.echo(f\'batch size: {batch_size}, chunks: {model.chunks}, \'\n                   f\'balance: {model.balance}, checkpoint: {model.checkpoint}\')\n    else:\n        click.echo(f\'batch size: {batch_size}\')\n\n    click.echo(\'torchgpipe: %s, python: %s, torch: %s, cudnn: %s, cuda: %s, gpu: %s\' % (\n        torchgpipe.__version__,\n        platform.python_version(),\n        torch.__version__,\n        torch.backends.cudnn.version(),\n        torch.version.cuda,\n        torch.cuda.get_device_name(in_device)))\n\n    # TRAIN =======================================================================================\n\n    global BASE_TIME\n    BASE_TIME = time.time()\n\n    def run_epoch(epoch: int) -> Tuple[float, float]:\n        torch.cuda.synchronize(in_device)\n        tick = time.time()\n\n        data_trained = 0\n        for i, (input, target) in enumerate(data):\n            data_trained += input.size(0)\n\n            output = model(input)\n            loss = F.binary_cross_entropy_with_logits(output, target)\n            loss.backward()\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # 00:01:02 | 1/20 epoch (42%) | 200.000 samples/sec (estimated)\n            percent = (i+1) / len(data) * 100\n            throughput = data_trained / (time.time()-tick)\n            log(\'%d/%d epoch (%d%%) | %.3f samples/sec (estimated)\'\n                \'\' % (epoch+1, epochs, percent, throughput), clear=True, nl=False)\n\n        torch.cuda.synchronize(in_device)\n        tock = time.time()\n\n        # 00:02:03 | 1/20 epoch | 200.000 samples/sec, 123.456 sec/epoch\n        elapsed_time = tock - tick\n        throughput = data_trained / elapsed_time\n        log(\'%d/%d epoch | %.3f samples/sec, %.3f sec/epoch\'\n            \'\' % (epoch+1, epochs, throughput, elapsed_time), clear=True)\n\n        return throughput, elapsed_time\n\n    throughputs = []\n    elapsed_times = []\n\n    hr()\n    for epoch in range(epochs):\n        throughput, elapsed_time = run_epoch(epoch)\n\n        if epoch < skip_epochs:\n            continue\n\n        throughputs.append(throughput)\n        elapsed_times.append(elapsed_time)\n    hr()\n\n    # RESULT ======================================================================================\n\n    # pipeline-4, 2-10 epochs | 200.000 samples/sec, 123.456 sec/epoch (average)\n    n = len(throughputs)\n    throughput = sum(throughputs) / n\n    elapsed_time = sum(elapsed_times) / n\n    click.echo(\'%s | %.3f samples/sec, %.3f sec/epoch (average)\'\n               \'\' % (title, throughput, elapsed_time))\n\n\nif __name__ == \'__main__\':\n    cli()\n'"
benchmarks/unet-timeline/gpu_utils.py,0,"b'from collections import deque\nfrom contextlib import contextmanager\nimport multiprocessing as mp\nimport subprocess\nfrom typing import Dict, Generator, List\n\n\ndef collect_gpu_utils(device_ids: List[int]) -> List[int]:\n    """"""Collects GPU% via nvidia-smi. It expects that nvidia-smi v418.43 is\n    executable in the running system.\n    """"""\n    unique_device_ids = deque(sorted(set(device_ids)))\n\n    # Execute nvidia-smi.\n    i_arg = \',\'.join(str(i) for i in unique_device_ids)\n    cmd = [\'nvidia-smi\', \'-q\', \'-i\', i_arg, \'-d\', \'utilization\']\n    p = subprocess.run(cmd, stdout=subprocess.PIPE)\n\n    # Parse GPU utilizations.\n    gpu_utils: Dict[int, int] = {}\n    for line in p.stdout.decode().split(\'\\n\'):\n        if \'Gpu\' in line:\n            for word in line.split():\n                if word.isdigit():\n                    i = unique_device_ids.popleft()\n                    gpu_utils[i] = int(word)\n\n    return [gpu_utils[i] for i in device_ids]\n\n\ndef _worker(device_ids: List[int],\n            interval: float,\n            conn: mp.connection.Connection,\n            ) -> None:\n    gpu_timeline: List[List[int]] = []\n    conn.send(None)\n\n    while not conn.poll(timeout=interval):\n        gpu_utils = collect_gpu_utils(device_ids)\n        gpu_timeline.append(gpu_utils)\n\n    conn.send(gpu_timeline)\n\n\n@contextmanager\ndef track_gpu_utils(device_ids: List[int],\n                    interval: float = 0.05,\n                    ) -> Generator[List[float], None, None]:\n    # Spawn a worker.\n    ctx = mp.get_context(\'spawn\')\n    conn, conn_worker = ctx.Pipe(duplex=True)\n    p = ctx.Process(target=_worker, args=(device_ids, interval, conn_worker))\n    p.start()\n    conn.recv()\n\n    # GPU% will be filled to this.\n    gpu_utils: List[float] = []\n    yield gpu_utils\n\n    # Stop the worker and receive the timeline.\n    conn.send(None)\n    gpu_timeline = conn.recv()\n    p.join()\n\n    # Fill the GPU%.\n    if gpu_timeline:\n        gpu_utils.extend(sum(t)/len(t)/100 for t in zip(*gpu_timeline))\n    else:\n        gpu_utils.extend(0.0 for _ in device_ids)\n'"
benchmarks/unet-timeline/main.py,15,"b'""""""U-Net Timeline Benchmark""""""\nimport platform\nimport time\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, cast\n\nimport click\nimport torch\nfrom torch import Tensor, nn\nimport torch.nn.functional as F\nfrom torch.optim import SGD\n\nfrom gpu_utils import track_gpu_utils\nimport torchgpipe\nfrom torchgpipe import GPipe\nfrom tuplify_skips import tuplify_skips\nfrom unet import unet\n\nExperiment = Callable[[nn.Sequential, List[int], List[int], int], GPipe]\n\n\nclass Experiments:\n\n    @staticmethod\n    def baseline(model: nn.Sequential,\n                 balance: List[int],\n                 devices: List[int],\n                 chunks: int,\n                 ) -> GPipe:\n        import torchgpipe.pipeline\n        torchgpipe.pipeline.depend = lambda *args: None\n        model = tuplify_skips(model)\n        gpipe = GPipe(model, balance, devices=devices, chunks=chunks)\n        gpipe._copy_streams = [[torch.cuda.default_stream(d)] * chunks for d in gpipe.devices]\n        return gpipe\n\n    @staticmethod\n    def dep_x_x(model: nn.Sequential,\n                balance: List[int],\n                devices: List[int],\n                chunks: int,\n                ) -> GPipe:\n        # Disable portals.\n        model = tuplify_skips(model)\n        gpipe = GPipe(model, balance, devices=devices, chunks=chunks)\n\n        # Disable streams.\n        gpipe._copy_streams = [[torch.cuda.default_stream(d)] * chunks for d in gpipe.devices]\n        return gpipe\n\n    @staticmethod\n    def dep_str_x(model: nn.Sequential,\n                  balance: List[int],\n                  devices: List[int],\n                  chunks: int,\n                  ) -> GPipe:\n        # Disable portals.\n        model = tuplify_skips(model)\n        return GPipe(model, balance, devices=devices, chunks=chunks)\n\n    @staticmethod\n    def dep_str_ptl(model: nn.Sequential,\n                    balance: List[int],\n                    devices: List[int],\n                    chunks: int,\n                    ) -> GPipe:\n        return GPipe(model, balance, devices=devices, chunks=chunks)\n\n\nEXPERIMENTS: Dict[str, Experiment] = {\n    \'baseline\': Experiments.baseline,\n    \'dep-x-x\': Experiments.dep_x_x,\n    \'dep-str-x\': Experiments.dep_str_x,\n    \'dep-str-ptl\': Experiments.dep_str_ptl,\n}\n\n\nBASE_TIME: float = 0\n\n\ndef hr() -> None:\n    """"""Prints a horizontal line.""""""\n    width, _ = click.get_terminal_size()\n    click.echo(\'-\' * width)\n\n\ndef log(msg: str, clear: bool = False, nl: bool = True) -> None:\n    """"""Prints a message with elapsed time.""""""\n    if clear:\n        # Clear the output line to overwrite.\n        width, _ = click.get_terminal_size()\n        click.echo(\'\\b\\r\', nl=False)\n        click.echo(\' \' * width, nl=False)\n        click.echo(\'\\b\\r\', nl=False)\n\n    t = time.time() - BASE_TIME\n    h = t // 3600\n    t %= 3600\n    m = t // 60\n    t %= 60\n    s = t\n\n    click.echo(\'%02d:%02d:%02d | \' % (h, m, s), nl=False)\n    click.echo(msg, nl=nl)\n\n\ndef parse_devices(ctx: Any, param: Any, value: Optional[str]) -> List[int]:\n    if value is None:\n        return list(range(torch.cuda.device_count()))\n    return [int(x) for x in value.split(\',\')]\n\n\n@click.command()\n@click.pass_context\n@click.argument(\n    \'experiment\',\n    type=click.Choice(sorted(EXPERIMENTS.keys())),\n)\n@click.option(\n    \'--epochs\', \'-e\',\n    type=int,\n    default=10,\n    help=\'Number of epochs (default: 10)\',\n)\n@click.option(\n    \'--skip-epochs\', \'-k\',\n    type=int,\n    default=1,\n    help=\'Number of epochs to skip in result (default: 1)\',\n)\n@click.option(\n    \'--devices\', \'-d\',\n    metavar=\'0,1,2,3\',\n    callback=parse_devices,\n    help=\'Device IDs to use (default: all CUDA devices)\',\n)\ndef cli(ctx: click.Context,\n        experiment: str,\n        epochs: int,\n        skip_epochs: int,\n        devices: List[int],\n        ) -> None:\n    """"""U-Net Timeline Benchmark""""""\n    if skip_epochs >= epochs:\n        ctx.fail(\'--skip-epochs=%d must be less than --epochs=%d\' % (skip_epochs, epochs))\n\n    model = unet(depth=5, num_convs=5, base_channels=64, input_channels=3, output_channels=1)\n    balance = [34, 76, 70, 61]\n    chunks = 8\n\n    f = EXPERIMENTS[experiment]\n    try:\n        gpipe = f(model, balance, devices, chunks)\n    except ValueError as exc:\n        # Examples:\n        #   ValueError: too few devices to hold given partitions (devices: 1, paritions: 2)\n        ctx.fail(str(exc))\n\n    optimizer = SGD(model.parameters(), lr=0.1)\n\n    in_device = gpipe.devices[0]\n    out_device = gpipe.devices[-1]\n    torch.cuda.set_device(in_device)\n\n    side = 192\n    steps = 16\n    batch_size = 128\n\n    input = torch.rand(batch_size, 3, side, side, device=in_device)\n    target = torch.ones(batch_size, 1, side, side, device=out_device)\n    data = [(input, target)] * steps\n\n    # HEADER ======================================================================================\n\n    title = f\'{experiment}, {skip_epochs+1}-{epochs} epochs\'\n    click.echo(title)\n    click.echo(\'torchgpipe: %s, python: %s, torch: %s, cudnn: %s, cuda: %s, gpu: %s\' % (\n        torchgpipe.__version__,\n        platform.python_version(),\n        torch.__version__,\n        torch.backends.cudnn.version(),\n        torch.version.cuda,\n        torch.cuda.get_device_name(in_device)))\n\n    # TRAIN =======================================================================================\n\n    global BASE_TIME\n    BASE_TIME = time.time()\n\n    def run_epoch(epoch: int) -> Tuple[float, float, List[float]]:\n        with track_gpu_utils([d.index for d in gpipe.devices]) as gpu_utils:\n            torch.cuda.synchronize(in_device)\n            tick = time.time()\n\n            data_trained = 0\n            for i, (input, target) in enumerate(data):\n                data_trained += input.size(0)\n\n                output = gpipe(input)\n                output = cast(Tensor, output)\n                loss = F.binary_cross_entropy_with_logits(output, target)\n                loss.backward()\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n                # 00:01:02 | 1/20 epoch (42%) | 200.000 samples/sec (estimated)\n                percent = (i+1) / len(data) * 100\n                throughput = data_trained / (time.time()-tick)\n                log(\'%d/%d epoch (%d%%) | %.3f samples/sec (estimated)\'\n                    \'\' % (epoch+1, epochs, percent, throughput), clear=True, nl=False)\n\n            torch.cuda.synchronize(in_device)\n            tock = time.time()\n\n        # 00:02:03 | 1/20 epoch | 200.000 samples/sec, 123.456 sec/epoch | 100.0%, 96%, 89%, 100%\n        elapsed_time = tock - tick\n        throughput = data_trained / elapsed_time\n        _gpu_utils = \', \'.join([f\'{u:.0%}\' for u in gpu_utils])\n        _gpu_utils += f\' (total {sum(gpu_utils)/len(gpu_utils):.0%})\'\n\n        log(\'%d/%d epoch | %.3f samples/sec, %.3f sec/epoch | %s\'\n            \'\' % (epoch+1, epochs, throughput, elapsed_time, _gpu_utils), clear=True)\n\n        return throughput, elapsed_time, gpu_utils\n\n    throughputs = []\n    elapsed_times = []\n    gpu_timeline = []\n\n    hr()\n    for epoch in range(epochs):\n        throughput, elapsed_time, gpu_utils = run_epoch(epoch)\n\n        if epoch < skip_epochs:\n            continue\n\n        throughputs.append(throughput)\n        elapsed_times.append(elapsed_time)\n        gpu_timeline.append(gpu_utils)\n    hr()\n\n    # RESULT ======================================================================================\n\n    # optimal, 2-10 epochs\n    click.echo(title)\n\n    # speed  | 53.006 samples/sec, 38.637 sec/epoch (average)\n    n = len(throughputs)\n    throughput = sum(throughputs) / n\n    elapsed_time = sum(elapsed_times) / n\n    click.echo(\'speed  | \', nl=False)\n    click.echo(f\'{throughput:.3f} samples/sec, {elapsed_time:.3f} sec/epoch (average)\')\n\n    # gpu%   | 59%, 71%, 74%, 64% (total 67%)\n    gpu_utils = [sum(t)/len(t) for t in zip(*gpu_timeline)]\n    click.echo(\'gpu%   | \', nl=False)\n    click.echo(\', \'.join([f\'{u:.0%}\' for u in gpu_utils]), nl=False)\n    click.echo(f\' (total {sum(gpu_utils)/len(gpu_utils):.0%})\')\n\n    # memory | 17301 MB, 13650 MB, 7262 MB, 18759 MB (total 56972 MB)\n    memory_usages: List[int] = []\n    for d in gpipe.devices:\n        memory_usages.append(torch.cuda.max_memory_cached(d))\n    click.echo(\'memory | \', nl=False)\n    click.echo(\', \'.join(f\'{m/1024/1024/1024:.1f} GiB\' for m in memory_usages), nl=False)\n    click.echo(f\' (total {sum(memory_usages)/1024/1024/1024:.1f} GiB)\')\n\n\nif __name__ == \'__main__\':\n    cli()\n'"
benchmarks/unet-timeline/tuplify_skips.py,1,"b'""""""Converts skip connections with ``@skippable`` to ``tuple``.""""""\nfrom collections import OrderedDict, deque\nfrom typing import TYPE_CHECKING, Deque, List, Optional, Tuple, Union, cast\n\nimport torch\nfrom torch import Tensor, nn\nfrom torchgpipe import is_checkpointing, is_recomputing\nfrom torchgpipe.skip import Namespace\nfrom torchgpipe.skip.skippable import Skippable\n\nTensors = Tuple[Tensor, ...]\nTensorOrTensors = Union[Tensor, Tensors]\n\nif TYPE_CHECKING:\n    Seats = OrderedDict[Tuple[Namespace, str], int]\nelse:\n    Seats = OrderedDict\n\n\nclass SkipsAsTuple(nn.Module):\n    """"""The base module for old-fashioned skip connections. It handles arguments\n    including the input and the skips.\n    """"""\n\n    def __init__(self,\n                 num_skips: int,\n                 unpack_input: Deque[bool],\n                 unpack_output: Deque[bool],\n                 ) -> None:\n        super().__init__()\n        self.num_skips = num_skips\n        self.unpack_input = unpack_input\n        self.unpack_input_for_recomputing: List[bool] = []\n        self.unpack_output = unpack_output\n\n    def forward(self, input_skips: TensorOrTensors) -> TensorOrTensors:  # type: ignore\n        input: TensorOrTensors = input_skips\n        skips: Tensors = ()\n\n        # num_skips=0 means that this module is the first module of the skip\n        # connections.\n        if self.num_skips:\n            assert isinstance(input_skips, tuple)\n\n            input = input_skips[:-self.num_skips]\n            skips = input_skips[-self.num_skips:]\n\n            if is_recomputing():\n                unpack_input = self.unpack_input_for_recomputing.pop()\n            else:\n                unpack_input = self.unpack_input.popleft()\n                if is_checkpointing():\n                    self.unpack_input_for_recomputing.append(unpack_input)\n\n            if unpack_input:\n                input = input[0]\n\n        output, skips = self._forward(input, skips)\n\n        unpack_output = torch.is_tensor(output)\n        self.unpack_output.append(unpack_output)\n\n        if not skips:\n            return output\n\n        if unpack_output:\n            return (cast(Tensor, output), *skips)\n        else:\n            return output + skips\n\n    def _forward(self, input: TensorOrTensors, skips: Tensors) -> Tuple[TensorOrTensors, Tensors]:\n        # Interface: (input, skips) -> (output, skips)\n        raise NotImplementedError\n\n\nclass Gutter(SkipsAsTuple):\n    """"""Just passes the incoming skips.""""""\n\n    def __init__(self,\n                 module: nn.Module,\n                 num_skips: int,\n                 unpack_input: Deque[bool],\n                 unpack_output: Deque[bool],\n                 ) -> None:\n        super().__init__(num_skips, unpack_input, unpack_output)\n        self.module = module\n\n    def _forward(self, input: TensorOrTensors, skips: Tensors) -> Tuple[TensorOrTensors, Tensors]:\n        output = self.module(input)\n        return output, skips\n\n\nclass Branch(SkipsAsTuple):\n    """"""Stashes or pops skips.""""""\n\n    def __init__(self,\n                 skippable: Skippable,\n                 prev_seats: Seats,\n                 next_seats: Seats,\n                 num_skips: int,\n                 unpack_input: Deque[bool],\n                 unpack_output: Deque[bool],\n                 ) -> None:\n        super().__init__(num_skips, unpack_input, unpack_output)\n        self.skippable = skippable\n        self.prev_seats = prev_seats\n        self.next_seats = next_seats\n\n    def _forward(self, input: TensorOrTensors, skips: Tensors) -> Tuple[TensorOrTensors, Tensors]:\n        stashed = {}\n\n        def handle_stash(name: str, tensor: Optional[Tensor]) -> None:\n            ns, name = self.skippable.namespaced(name)\n            stashed[(ns, name)] = tensor\n\n        def handle_pop(name: str) -> Optional[Tensor]:\n            ns, name = self.skippable.namespaced(name)\n            i = self.prev_seats[(ns, name)]\n            return skips[i]\n\n        output = self.skippable.dispatch(input, handle_stash, handle_pop)\n\n        next_skips = []\n        for ns, name in self.next_seats:\n            if (ns, name) in stashed:\n                skip = stashed[(ns, name)]\n            else:\n                i = self.prev_seats[(ns, name)]\n                skip = skips[i]\n            assert skip is not None\n            next_skips.append(skip)\n\n        return output, tuple(next_skips)\n\n\ndef tuplify_skips(module: nn.Sequential) -> nn.Sequential:\n    """"""Converts ``@skippable`` modules and intermediate modules between\n    ``@skippable``s to represent the skips as ``tuple``.\n    """"""\n    seats: Seats = OrderedDict()\n    next_seats = seats\n    unpack_output: Deque[bool] = deque()\n    layers: List[nn.Module] = []\n\n    for layer in module.children():\n        num_skips = len(seats)\n        unpack_input = unpack_output\n        unpack_output = deque()\n\n        if isinstance(layer, Skippable):\n            for ns, name in layer.stashable():\n                seats[(ns, name)] = -1\n            for ns, name in layer.poppable():\n                del seats[(ns, name)]\n            for i, (ns, name) in enumerate(seats):\n                seats[(ns, name)] = i\n\n            prev_seats, next_seats = next_seats, seats.copy()\n\n            layer = Branch(layer, prev_seats, next_seats, num_skips, unpack_input, unpack_output)\n\n        elif seats:\n            layer = Gutter(layer, num_skips, unpack_input, unpack_output)\n\n        layers.append(layer)\n    return nn.Sequential(*layers)\n'"
tests/skip/test_api.py,0,"b""import copy\n\nfrom torch import nn\n\nfrom torchgpipe.skip import Namespace, skippable, stash\n\n\ndef test_namespace_difference():\n    ns1 = Namespace()\n    ns2 = Namespace()\n    assert ns1 != ns2\n\n\ndef test_namespace_copy():\n    ns = Namespace()\n    assert copy.copy(ns) == ns\n    assert copy.copy(ns) is not ns\n\n\ndef test_skippable_repr():\n    @skippable(stash=['hello'])\n    class Hello(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = nn.Conv2d(1, 1, 1)\n\n        def forward(self, x):\n            yield stash('hello', x)\n            return self.conv(x)\n\n    m = Hello()\n    assert repr(m) == '''\n@skippable(Hello(\n  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n))\n'''.strip()\n"""
tests/skip/test_gpipe.py,6,"b""import pytest\nimport torch\nfrom torch import nn\n\nfrom torchgpipe import GPipe\nfrom torchgpipe.skip import pop, skippable, stash\nfrom torchgpipe.skip.portal import PortalBlue, PortalCopy, PortalOrange\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason='cuda required')\n@pytest.mark.parametrize('balance', [[3], [1, 2], [2, 1], [1, 1, 1]],\n                         ids=['3', '1:2', '2:1', '1:1:1'])\n@pytest.mark.parametrize('checkpoint', ['never', 'always', 'except_last'])\ndef test_1to3(balance, checkpoint):\n    if torch.cuda.device_count() < len(balance):\n        pytest.skip('at least %d cuda devices required' % len(balance))\n\n    @skippable(stash=['1to3'])\n    class Layer1(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = nn.Conv2d(3, 3, 1)\n\n        def forward(self, input):\n            yield stash('1to3', input)\n            output = self.conv(input)\n            return output\n\n    class Layer2(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = nn.Conv2d(3, 3, 1)\n\n        def forward(self, input):\n            output = self.conv(input)\n            return output\n\n    @skippable(pop=['1to3'])\n    class Layer3(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = nn.Conv2d(3, 3, 1)\n\n        def forward(self, input):\n            skip_1to3 = yield pop('1to3')\n            output = self.conv(input) + skip_1to3\n            return output\n\n    model = nn.Sequential(Layer1(), Layer2(), Layer3())\n    model = GPipe(model, balance, chunks=3, checkpoint=checkpoint)\n\n    in_device = model.devices[0]\n    out_device = model.devices[-1]\n\n    input = torch.rand(30, 3, 224, 224, device=in_device, requires_grad=True)\n    output = model(input)\n    loss = output.mean()\n    loss.backward()\n\n    assert torch.allclose(output.norm(), torch.tensor(1039.159, device=out_device))\n    assert torch.allclose(input.grad.norm(), torch.tensor(0.0004533053, device=in_device))\n\n\ndef test_none_skip():\n    @skippable(stash=['none'])\n    class Stash(nn.Module):\n        def forward(self, input):\n            yield stash('none', None)\n            return input\n\n    @skippable(pop=['none'])\n    class Pop(nn.Module):\n        def forward(self, input):\n            none = yield pop('none')\n            assert none is None\n            return input\n\n    model = nn.Sequential(Stash(), Pop())\n    model = GPipe(model, [1, 1], devices=['cpu', 'cpu'], chunks=5)\n\n    input = torch.rand(10, requires_grad=True)\n    output = model(input)\n\n    def assert_grad_fn_is_not_portal(grad_fn, visited=set()):\n        if grad_fn in visited or grad_fn is None:\n            return\n\n        assert not isinstance(grad_fn, PortalBlue._backward_cls)\n        assert not isinstance(grad_fn, PortalCopy._backward_cls)\n        assert not isinstance(grad_fn, PortalOrange._backward_cls)\n\n        visited.add(grad_fn)\n        for next_grad_fn, _ in grad_fn.next_functions:\n            assert_grad_fn_is_not_portal(next_grad_fn, visited)\n\n    assert_grad_fn_is_not_portal(output.grad_fn)\n\n    output.sum().backward()\n    assert input.grad.mean().item() == 1\n"""
tests/skip/test_inspect_skip_layout.py,0,"b""from torch import nn\n\nfrom torchgpipe.skip import Namespace, pop, skippable, stash\nfrom torchgpipe.skip.layout import inspect_skip_layout\n\n\nclass Pass(nn.Module):\n    def forward(self, input):\n        return input\n\n\n@skippable(stash=['foo'])\nclass StashFoo(nn.Module):\n    def forward(self, input):\n        yield stash('foo', input)\n        return input\n\n\n@skippable(pop=['foo'])\nclass PopFoo(nn.Module):\n    def forward(self, input):\n        foo = yield stash('foo')\n        return input + foo\n\n\n@skippable(stash=['bar'])\nclass StashBar(nn.Module):\n    def forward(self, input):\n        yield stash('bar', input)\n        return input\n\n\n@skippable(pop=['bar'])\nclass PopBar(nn.Module):\n    def forward(self, input):\n        bar = yield pop('bar')\n        return input + bar\n\n\ndef test_no_skippables():\n    p1 = nn.Sequential(Pass())\n    p2 = nn.Sequential(Pass())\n\n    layout = inspect_skip_layout([p1, p2])\n    policy = [list(layout.copy_policy(i)) for i in range(2)]\n\n    assert policy == [[], []]\n\n\ndef test_inner_partition():\n    p1 = nn.Sequential(StashFoo(), PopFoo())\n    p2 = nn.Sequential(Pass())\n\n    layout = inspect_skip_layout([p1, p2])\n    policy = [list(layout.copy_policy(i)) for i in range(2)]\n\n    assert policy == [[], []]\n\n\ndef test_adjoining_partitions():\n    p1 = nn.Sequential(StashFoo())\n    p2 = nn.Sequential(PopFoo())\n\n    layout = inspect_skip_layout([p1, p2])\n    policy = [list(layout.copy_policy(i)) for i in range(2)]\n\n    assert policy == [[], [(0, None, 'foo')]]\n\n\ndef test_far_partitions():\n    p1 = nn.Sequential(StashFoo())\n    p2 = nn.Sequential(Pass())\n    p3 = nn.Sequential(PopFoo())\n\n    layout = inspect_skip_layout([p1, p2, p3])\n    policy = [list(layout.copy_policy(i)) for i in range(3)]\n\n    assert policy == [[], [], [(0, None, 'foo')]]\n\n\ndef test_pop_2_from_different_partitions():\n    p1 = nn.Sequential(StashFoo())\n    p2 = nn.Sequential(StashBar())\n    p3 = nn.Sequential(PopBar(), PopFoo())\n\n    layout = inspect_skip_layout([p1, p2, p3])\n    policy = [list(layout.copy_policy(i)) for i in range(3)]\n\n    # p3 pops 'bar' before 'foo', but the plan is sorted by source partition index.\n    assert policy == [[], [], [(0, None, 'foo'), (1, None, 'bar')]]\n\n\ndef test_namespace():\n    ns1 = Namespace()\n    ns2 = Namespace()\n\n    p1 = nn.Sequential(StashFoo().isolate(ns1))\n    p2 = nn.Sequential(StashFoo().isolate(ns2))\n    p3 = nn.Sequential(PopFoo().isolate(ns2), PopFoo().isolate(ns1))\n\n    layout = inspect_skip_layout([p1, p2, p3])\n    policy = [list(layout.copy_policy(i)) for i in range(3)]\n\n    # p3 pops 'bar' before 'foo', but the plan is sorted by source partition index.\n    assert policy == [[], [], [(0, ns1, 'foo'), (1, ns2, 'foo')]]\n"""
tests/skip/test_leak.py,5,"b""import pytest\nimport torch\nfrom torch import nn\n\nfrom torchgpipe import GPipe, is_checkpointing, is_recomputing\nfrom torchgpipe.skip import pop, skippable, stash\nfrom torchgpipe.skip.tracker import current_skip_tracker\n\n\n@skippable(stash=['skip'])\nclass Stash(nn.Module):\n    def forward(self, input):\n        yield stash('skip', input)\n        return input\n\n\n@skippable(pop=['skip'])\nclass Pop(nn.Module):\n    def forward(self, input):\n        skip = yield pop('skip')\n        return input + skip\n\n\n@pytest.mark.parametrize('train', [True, False], ids=['train', 'eval'])\n@pytest.mark.parametrize('checkpoint', ['always', 'except_last', 'never'])\ndef test_delete_portal_tensor(train, checkpoint):\n    # Without checkpointing:\n    # +- Stash --+  +--- Pop ----+ - - - layers\n    # | 2,blue,1 |--| 1,orange,0 | - - - tensor_life and portal function\n    # +----------+  +------------+\n    #\n    # With checkpointing:\n    # +- Stash --+  +--- Pop ----+  +--- Pop'----+  +- Stash'--+\n    # | 3,blue,2 |--| 2,orange,1 |--| 1,orange,0 |--| 1,blue,0 |\n    # +----------+  +------------+  +------------+  +----------+\n\n    def portal_tensor_life_is(tensor_life, skip_tracker=None):\n        if skip_tracker is None:\n            skip_tracker = current_skip_tracker()\n\n        # Get the current portal.\n        portal = list(skip_tracker.portals.values())[0]\n\n        if tensor_life == 0:\n            return portal.tensor_life == 0 and portal.tensor is None\n        else:\n            return portal.tensor_life == tensor_life and portal.tensor is not None\n\n    # Check the portal tensor after 'Stash'.\n    stash_ = Stash()\n\n    @stash_.register_forward_hook\n    def check_portal_tensor_after_stash(*_):\n        if is_checkpointing():\n            assert portal_tensor_life_is(2)\n        elif is_recomputing():\n            assert portal_tensor_life_is(0)\n        else:\n            assert portal_tensor_life_is(1)\n\n    pop_ = Pop()\n\n    @pop_.register_forward_hook\n    def check_portal_tensor_after_pop(*_):\n        if is_checkpointing():\n            assert portal_tensor_life_is(1)\n        elif is_recomputing():\n            assert portal_tensor_life_is(0)\n        else:\n            assert portal_tensor_life_is(0)\n\n    class NoPortalTensorAtBackward(nn.Module):\n        class F(torch.autograd.Function):\n            @staticmethod\n            def forward(ctx, input):\n                ctx.skip_tracker = current_skip_tracker()\n                return input.detach()\n\n            @staticmethod\n            def backward(ctx, grad):\n                assert portal_tensor_life_is(0, skip_tracker=ctx.skip_tracker)\n                return grad\n\n        def forward(self, input):\n            return self.F.apply(input)\n\n    model = nn.Sequential(NoPortalTensorAtBackward(), stash_, pop_)\n    model = GPipe(model,\n                  balance=[2, 1],\n                  devices=['cpu', 'cpu'],\n                  chunks=2,\n                  checkpoint=checkpoint)\n\n    input = torch.rand(10, requires_grad=True)\n\n    if train:\n        model.train()\n        output = model(input)\n        output.norm().backward()\n    else:\n        model.eval()\n        with torch.no_grad():\n            model(input)\n\n\n@pytest.mark.parametrize('train', [True, False], ids=['train', 'eval'])\ndef test_no_portal_without_gpipe(train, monkeypatch):\n    def deny(*args, **kwargs):\n        raise AssertionError('tried to create Portal without GPipe')\n\n    monkeypatch.setattr('torchgpipe.skip.portal.Portal.__init__', deny)\n\n    model = nn.Sequential(Stash(), Pop())\n\n    input = torch.rand(10, requires_grad=True)\n\n    if train:\n        model.train()\n        output = model(input)\n        output.norm().backward()\n    else:\n        model.eval()\n        with torch.no_grad():\n            model(input)\n"""
tests/skip/test_portal.py,15,"b""import pytest\nimport torch\n\nfrom torchgpipe.dependency import fork, join\nfrom torchgpipe.skip.portal import Portal\nfrom torchgpipe.stream import default_stream\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason='cuda required')\ndef test_copy_returns_on_next_device():\n    portal = Portal(torch.rand(1), tensor_life=1)\n\n    prev_stream = default_stream(torch.device('cpu'))\n    next_stream = default_stream(torch.device('cuda'))\n\n    phony = torch.zeros(0, requires_grad=True)\n    assert phony.device.type == 'cpu'\n\n    phony = portal.copy(prev_stream, next_stream, phony)\n    assert phony.device.type == 'cuda'\n\n\ndef test_blue_orange():\n    tensor1 = torch.rand(1, requires_grad=True)\n    tensor2 = torch.rand(1, requires_grad=True)\n\n    # Same with: output = tensor1*2 + tensor2\n    #\n    #                +----------------------+\n    #                |                      |\n    # tensor2 -- PortalBlue -+      +- PortalOrange -+\n    #                        |      |                |\n    # tensor1 ------------ Join -- Fork --- Mul --- Add -- output\n    #\n    main = tensor1\n    portal = Portal(tensor2, tensor_life=2)\n    phony = portal.blue()\n    main = join(main, phony)\n    main, phony = fork(main)\n    sub = portal.orange(phony)\n    output = main*2 + sub\n\n    output.backward()\n\n    assert torch.allclose(tensor1.grad, torch.tensor([2.]))\n    assert torch.allclose(tensor2.grad, torch.tensor([1.]))\n\n\ndef test_blue_orange_not_requires_grad():\n    tensor1 = torch.rand(1, requires_grad=True)\n    tensor2 = torch.rand(1)\n\n    # Same with: output = tensor1*2 + tensor2\n    #\n    #                +----------------------+\n    #                |                      |\n    # tensor2 -- PortalBlue -+      +- PortalOrange -+\n    #                        |      |                |\n    # tensor1 ------------ Join -- Fork --- Mul --- Add -- output\n    #\n    main = tensor1\n    portal = Portal(tensor2, tensor_life=2)\n    phony = portal.blue()\n    main = join(main, phony)\n    main, phony = fork(main)\n    sub = portal.orange(phony)\n    output = main*2 + sub\n\n    output.backward()\n\n    assert torch.allclose(tensor1.grad, torch.tensor([2.]))\n    assert tensor2.grad is None\n\n\ndef test_use_grad():\n    tensor = torch.rand(1, requires_grad=True)\n    portal = Portal(tensor, tensor_life=1)\n\n    portal.put_grad(tensor)\n    assert portal.use_grad() is tensor\n\n    # Gradient in a portal is ephemeral.\n    with pytest.raises(RuntimeError):\n        portal.use_grad()\n\n\nclass TestTensorLife:\n    @pytest.fixture\n    def new_portal(self):\n        portal = None\n\n        def new_portal(tensor_life):\n            nonlocal portal\n            tensor = torch.rand(1, requires_grad=True)\n            portal = Portal(tensor, tensor_life)\n            return portal, tensor\n\n        yield new_portal\n\n        # A test using this fixture must exhaust the tensor in the portal.\n        with pytest.raises(RuntimeError):\n            portal.check_tensor_life()\n        assert portal.tensor is None\n\n    def test_tensor_life_0(self, new_portal):\n        portal, tensor = new_portal(0)\n        assert portal.tensor is None\n\n    def test_tensor_life_1(self, new_portal):\n        portal, tensor = new_portal(1)\n        assert portal.tensor is tensor\n\n        portal.blue()\n\n    def test_tensor_life_2(self, new_portal):\n        portal, tensor = new_portal(2)\n        assert portal.tensor is tensor\n\n        phony = portal.blue()\n        assert portal.orange(phony).data_ptr() == tensor.data_ptr()\n\n    def test_tensor_life_3(self, new_portal):\n        portal, tensor = new_portal(3)\n        assert portal.tensor is tensor\n\n        phony = portal.blue()\n        assert portal.orange(phony).data_ptr() == tensor.data_ptr()\n        assert portal.orange(phony).data_ptr() == tensor.data_ptr()\n\n    def test_tensor_life_4(self, new_portal):\n        portal, tensor = new_portal(4)\n        assert portal.tensor is tensor\n\n        phony = portal.blue()\n        assert portal.orange(phony).data_ptr() == tensor.data_ptr()\n        assert portal.orange(phony).data_ptr() == tensor.data_ptr()\n        portal.blue()\n\n    def test_tensor_life_3_plus_1(self, new_portal):\n        portal, tensor = new_portal(3)\n        assert portal.tensor is tensor\n\n        phony = portal.blue()\n        assert portal.orange(phony).data_ptr() == tensor.data_ptr()\n        assert portal.orange(phony).data_ptr() == tensor.data_ptr()\n\n        another_tensor = torch.rand(1, requires_grad=True)\n        portal.put_tensor(another_tensor, tensor_life=1)\n        portal.blue()\n"""
tests/skip/test_stash_pop.py,8,"b""import pytest\nimport torch\nfrom torch import nn\n\nfrom torchgpipe.skip import pop, skippable, stash\nfrom torchgpipe.skip.tracker import SkipTracker, use_skip_tracker\n\n\n@pytest.fixture(autouse=True)\ndef skip_tracker():\n    skip_tracker = SkipTracker()\n    with use_skip_tracker(skip_tracker):\n        yield skip_tracker\n\n\ndef test_stash(skip_tracker):\n    @skippable(stash=['foo'])\n    class Stash(nn.Module):\n        def forward(self, input):\n            yield stash('foo', input)\n            return input * 2\n\n    l1 = Stash()\n\n    assert len(skip_tracker.tensors) == 0\n\n    with use_skip_tracker(skip_tracker):\n        l1(torch.tensor(42))\n\n    assert len(skip_tracker.tensors) == 1\n\n\ndef test_pop():\n    @skippable(stash=['foo'])\n    class Stash(nn.Module):\n        def forward(self, input):\n            yield stash('foo', input)\n            return input * 2\n\n    @skippable(pop=['foo'])\n    class Pop(nn.Module):\n        def forward(self, input):\n            foo = yield pop('foo')\n            return foo\n\n    l1 = Stash()\n    l2 = Pop()\n\n    output = l2(l1(torch.tensor(42)))\n\n    assert output.item() == 42\n\n\ndef test_declare_but_not_use():\n    @skippable(stash=['foo'])\n    class Stash(nn.Module):\n        def forward(self, input):\n            return input * 2\n\n    @skippable(pop=['foo'])\n    class Pop(nn.Module):\n        def forward(self, input):\n            return input * 3\n\n    l1 = Stash()\n    l2 = Pop()\n\n    with pytest.raises(RuntimeError):\n        l1(torch.tensor(42))\n\n    with pytest.raises(RuntimeError):\n        l2(torch.tensor(42))\n\n\ndef test_stash_not_declared():\n    @skippable()\n    class Stash(nn.Module):\n        def forward(self, input):\n            yield stash('foo', input)\n            return input * 2\n\n    l1 = Stash()\n\n    with pytest.raises(RuntimeError):\n        l1(torch.tensor(42))\n\n\ndef test_pop_not_declared():\n    @skippable(stash=['foo'])\n    class Stash(nn.Module):\n        def forward(self, input):\n            yield stash('foo', input)\n            return input * 2\n\n    @skippable()\n    class Pop(nn.Module):\n        def forward(self, input):\n            foo = yield pop('foo')\n            return foo\n\n    l1 = Stash()\n    l2 = Pop()\n\n    latent = l1(torch.tensor(42))\n\n    with pytest.raises(RuntimeError):\n        l2(latent)\n\n\ndef test_pop_not_stashed():\n    @skippable(pop=['foo'])\n    class Pop(nn.Module):\n        def forward(self, input):\n            yield pop('foo')\n\n    l1 = Pop()\n\n    with pytest.raises(RuntimeError):\n        l1(torch.tensor(42))\n\n\ndef test_stash_none():\n    @skippable(stash=['foo'])\n    class Stash(nn.Module):\n        def forward(self, input):\n            yield stash('foo', None)\n            return input * 2\n\n    l1 = Stash()\n    l1(torch.tensor(42))\n"""
tests/skip/test_tracker.py,13,"b""from queue import Queue\nimport threading\n\nimport pytest\nimport torch\nfrom torch import nn\n\nfrom torchgpipe.checkpoint import enable_checkpointing, enable_recomputing\nfrom torchgpipe.microbatch import Batch\nfrom torchgpipe.skip import pop, skippable, stash\nfrom torchgpipe.skip.layout import SkipLayout\nfrom torchgpipe.skip.tracker import SkipTracker, SkipTrackerThroughPotals, current_skip_tracker\n\n\ndef test_default_skip_tracker():\n    q = Queue()\n\n    def f():\n        q.put(current_skip_tracker())\n\n    t = threading.Thread(target=f)\n    t.start()\n    t.join()\n\n    skip_tracker = q.get()\n\n    assert type(skip_tracker) is SkipTracker\n    assert type(skip_tracker) is not SkipTrackerThroughPotals\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason='cuda required')\ndef test_default_skip_tracker_by_data_parallel():\n    @skippable(stash=['foo'])\n    class Stash(nn.Module):\n        def forward(self, input):\n            yield stash('foo', input)\n            return input * 2\n\n    @skippable(pop=['foo'])\n    class Pop(nn.Module):\n        def forward(self, input):\n            foo = yield pop('foo')\n            return foo\n\n    model = nn.Sequential(Stash(), Pop())\n    model = nn.DataParallel(model, device_ids=[0, 0], output_device=0)\n\n    input = torch.rand(10, device=0)\n    output = model(input)\n\n    assert torch.allclose(output, input)\n\n\ndef test_reuse_portal():\n    skip_layout = SkipLayout(num_partitions=2, skip_routes={(None, 'test'): (0, 1)})\n    skip_tracker = SkipTrackerThroughPotals(skip_layout)\n\n    batch = Batch(torch.tensor([1.0]))\n    a = torch.tensor([2.0])\n    b = torch.tensor([2.0])\n\n    skip_tracker.save(batch, None, 'test', a)\n    portal = skip_tracker.portals[(None, 'test')]\n\n    skip_tracker.save(batch, None, 'test', b)\n    assert portal is skip_tracker.portals[(None, 'test')]\n\n\ndef test_no_copy_no_portal():\n    skip_layout = SkipLayout(num_partitions=2, skip_routes={\n        (None, 'copy'): (0, 1),\n        (None, 'not_copy'): (0, 0),\n    })\n    skip_tracker = SkipTrackerThroughPotals(skip_layout)\n\n    batch = Batch(torch.tensor([1.0]))\n    a = torch.tensor([2.0])\n    b = torch.tensor([2.0])\n\n    skip_tracker.save(batch, None, 'copy', a)\n    skip_tracker.save(batch, None, 'not_copy', b)\n\n    assert (None, 'copy') in skip_tracker.portals\n    assert (None, 'copy') not in skip_tracker.tensors\n    assert (None, 'not_copy') in skip_tracker.tensors\n    assert (None, 'not_copy') not in skip_tracker.portals\n\n\ndef test_tensor_life_without_checkpointing():\n    skip_layout = SkipLayout(num_partitions=2, skip_routes={(None, 'test'): (0, 1)})\n    skip_tracker = SkipTrackerThroughPotals(skip_layout)\n\n    batch = Batch(torch.tensor([1.0]))\n    tensor = torch.tensor([2.0])\n\n    skip_tracker.save(batch, None, 'test', tensor)\n    assert skip_tracker.portals[(None, 'test')].tensor_life == 1\n\n    skip_tracker.load(batch, None, 'test')\n    assert skip_tracker.portals[(None, 'test')].tensor_life == 0\n\n\ndef test_tensor_life_with_checkpointing():\n    skip_layout = SkipLayout(num_partitions=2, skip_routes={(None, 'test'): (0, 1)})\n    skip_tracker = SkipTrackerThroughPotals(skip_layout)\n\n    batch = Batch(torch.tensor([1.0]))\n    tensor = torch.tensor([2.0])\n\n    with enable_checkpointing():\n        skip_tracker.save(batch, None, 'test', tensor)\n    assert skip_tracker.portals[(None, 'test')].tensor_life == 2\n\n    with enable_checkpointing():\n        skip_tracker.load(batch, None, 'test')\n    assert skip_tracker.portals[(None, 'test')].tensor_life == 1\n\n    with enable_recomputing():\n        skip_tracker.load(batch, None, 'test')\n    assert skip_tracker.portals[(None, 'test')].tensor_life == 0\n\n    with enable_recomputing():\n        skip_tracker.save(batch, None, 'test', tensor)\n    assert skip_tracker.portals[(None, 'test')].tensor_life == 0\n"""
tests/skip/test_verify_skippables.py,0,"b'import pytest\nfrom torch import nn\n\nfrom torchgpipe.skip import Namespace, skippable, verify_skippables\n\n\ndef test_matching():\n    @skippable(stash=[\'foo\'])\n    class Layer1(nn.Module):\n        pass\n\n    @skippable(pop=[\'foo\'])\n    class Layer2(nn.Module):\n        pass\n\n    verify_skippables(nn.Sequential(Layer1(), Layer2()))\n\n\ndef test_stash_not_pop():\n    @skippable(stash=[\'foo\'])\n    class Layer1(nn.Module):\n        pass\n\n    with pytest.raises(TypeError) as e:\n        verify_skippables(nn.Sequential(Layer1()))\n    assert ""no module declared \'foo\' as poppable but stashed"" in str(e.value)\n\n\ndef test_pop_unknown():\n    @skippable(pop=[\'foo\'])\n    class Layer1(nn.Module):\n        pass\n\n    with pytest.raises(TypeError) as e:\n        verify_skippables(nn.Sequential(Layer1()))\n    assert ""\'0\' declared \'foo\' as poppable but it was not stashed"" in str(e.value)\n\n\ndef test_stash_again():\n    @skippable(stash=[\'foo\'])\n    class Layer1(nn.Module):\n        pass\n\n    @skippable(stash=[\'foo\'])\n    class Layer2(nn.Module):\n        pass\n\n    @skippable(pop=[\'foo\'])\n    class Layer3(nn.Module):\n        pass\n\n    with pytest.raises(TypeError) as e:\n        verify_skippables(nn.Sequential(Layer1(), Layer2(), Layer3()))\n    assert ""\'1\' redeclared \'foo\' as stashable"" in str(e.value)\n\n\ndef test_pop_again():\n    @skippable(stash=[\'foo\'])\n    class Layer1(nn.Module):\n        pass\n\n    @skippable(pop=[\'foo\'])\n    class Layer2(nn.Module):\n        pass\n\n    @skippable(pop=[\'foo\'])\n    class Layer3(nn.Module):\n        pass\n\n    with pytest.raises(TypeError) as e:\n        verify_skippables(nn.Sequential(Layer1(), Layer2(), Layer3()))\n    assert ""\'2\' redeclared \'foo\' as poppable"" in str(e.value)\n\n\ndef test_stash_pop_together_different_names():\n    @skippable(stash=[\'foo\'])\n    class Layer1(nn.Module):\n        pass\n\n    @skippable(pop=[\'foo\'], stash=[\'bar\'])\n    class Layer2(nn.Module):\n        pass\n\n    @skippable(pop=[\'bar\'])\n    class Layer3(nn.Module):\n        pass\n\n    verify_skippables(nn.Sequential(Layer1(), Layer2(), Layer3()))\n\n\ndef test_stash_pop_together_same_name():\n    @skippable(stash=[\'foo\'], pop=[\'foo\'])\n    class Layer1(nn.Module):\n        pass\n\n    with pytest.raises(TypeError) as e:\n        verify_skippables(nn.Sequential(Layer1()))\n    assert ""\'0\' declared \'foo\' both as stashable and as poppable"" in str(e.value)\n\n\ndef test_double_stash_pop():\n    @skippable(stash=[\'foo\'])\n    class Layer1(nn.Module):\n        pass\n\n    @skippable(pop=[\'foo\'])\n    class Layer2(nn.Module):\n        pass\n\n    @skippable(stash=[\'foo\'])\n    class Layer3(nn.Module):\n        pass\n\n    @skippable(pop=[\'foo\'])\n    class Layer4(nn.Module):\n        pass\n\n    with pytest.raises(TypeError) as e:\n        verify_skippables(nn.Sequential(Layer1(), Layer2(), Layer3(), Layer4()))\n    assert ""\'2\' redeclared \'foo\' as stashable"" in str(e.value)\n    assert ""\'3\' redeclared \'foo\' as poppable"" in str(e.value)\n\n\ndef test_double_stash_pop_but_isolated():\n    @skippable(stash=[\'foo\'])\n    class Layer1(nn.Module):\n        pass\n\n    @skippable(pop=[\'foo\'])\n    class Layer2(nn.Module):\n        pass\n\n    @skippable(stash=[\'foo\'])\n    class Layer3(nn.Module):\n        pass\n\n    @skippable(pop=[\'foo\'])\n    class Layer4(nn.Module):\n        pass\n\n    ns1 = Namespace()\n    ns2 = Namespace()\n\n    verify_skippables(nn.Sequential(\n        Layer1().isolate(ns1),\n        Layer2().isolate(ns1),\n        Layer3().isolate(ns2),\n        Layer4().isolate(ns2),\n    ))\n'"
torchgpipe/balance/__init__.py,16,"b'""""""A helper to roughly balance a sequential module.\n\nUsage::\n\n    import torch\n    from torchgpipe import GPipe\n    from torchgpipe.balance import balance_by_time\n\n    sample = torch.empty(128, 3, 224, 224)\n    balance = balance_by_time(torch.cuda.device_count(), model, sample)\n\n    gpipe = GPipe(model, balance, chunks=8)\n\n""""""\nfrom typing import List, Tuple, Union\n\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\n\nfrom torchgpipe.balance import blockpartition\nfrom torchgpipe.balance.profile import profile_sizes, profile_times\n\n__all__ = [\'balance_by_time\', \'balance_by_size\']\n\n\nDevice = Union[torch.device, int, str]\n\nTensors = Tuple[Tensor, ...]\nTensorOrTensors = Union[Tensor, Tensors]\n\n\ndef balance_cost(cost: List[int], partitions: int) -> List[int]:\n    partitioned = blockpartition.solve(cost, partitions)\n    return [len(p) for p in partitioned]\n\n\ndef balance_by_time(partitions: int,\n                    module: nn.Sequential,\n                    sample: TensorOrTensors,\n                    *,\n                    timeout: float = 1.0,\n                    device: Device = torch.device(\'cuda\'),\n                    ) -> List[int]:\n    """"""Naive automatic balancing by elapsed time per layer.\n    ::\n\n        sample = torch.empty(128, 3, 224, 224)\n        balance = balance_by_time(torch.cuda.device_count(), model, sample)\n        gpipe = GPipe(model, balance, chunks=8)\n\n    Args:\n        partitions (int):\n            intended number of partitions\n        module (torch.nn.Sequential):\n            sequential module to be partitioned\n        sample (torch.Tensor):\n            example input with arbitrary batch size\n\n    Keyword Args:\n        timeout (float):\n            profiling iterates again if the timeout (in second) is not exceeded\n            (default: ``1.0``)\n        device (\'cpu\' or \'cuda\' device):\n            CPU or CUDA device where each layer is profiled (default: the\n            current CUDA device)\n\n    Returns:\n        A list of number of layers in each partition. Use it for the `balance`\n        parameter of :class:`~torchgpipe.GPipe`.\n\n    .. note::\n        `module` and `sample` must be placed on the same device.\n\n    """"""\n    times = profile_times(module, sample, timeout, torch.device(device))\n    return balance_cost(times, partitions)\n\n\ndef balance_by_size(partitions: int,\n                    module: nn.Sequential,\n                    input: TensorOrTensors,\n                    *,\n                    chunks: int = 1,\n                    param_scale: float = 2.0,\n                    device: Device = torch.device(\'cuda\'),\n                    ) -> List[int]:\n    """"""Naive automatic balancing by CUDA memory usage per layer.\n\n    During training, required memory for parameters depends on which optimizer\n    is used. Optimizers may use buffers for each parameter to track\n    optimization statistics internally, such as momentum buffer in SGD.\n\n    To get more reliable size based balance, you should specify `param_scale`\n    with regard to your optimizer. The default `param_scale` is 2 instead of 1\n    due to gradient accumulation which is necessary for every optimizer.\n\n    Follow this guide to choose correct `param_scale` for typical optimizers:\n\n    =========  =============  =========================================\n    Optimizer  `param_scale`  Internal State\n    =========  =============  =========================================\n    SGD        2--3           (momentum_buffer)\n    Adam       4--5           exp_avg, exp_avg_sq, (max_exp_avg_sq)\n    Adadelta   4              square_avg, acc_delta\n    Adagrad    3              sum\n    RMSprop    3--5           square_avg, (momentum_buffer), (grad_avg)\n    =========  =============  =========================================\n\n    Here\'s a simple example with the Adam optimizer::\n\n        balance = balance_by_size(\n            torch.cuda.device_count(),\n            model,\n\n            # Same size with mini-batch to train\n            torch.empty(1024, 3, 224, 224),\n\n            # Number of micro-batches to train with GPipe\n            chunks=8,\n\n            # 4 for Adam\n            param_scale=4.0,\n        )\n\n        gpipe = GPipe(model, balance, chunks=8)\n        adam = Adam(gpipe.parameters())\n\n    Args:\n        partitions (int):\n            intended number of partitions\n        module (torch.nn.Sequential):\n            sequential module to be partitioned\n        input (torch.Tensor):\n            example mini-batch with the same size to train\n\n    Keyword Args:\n        chunks (int):\n            number of micro-batches will be used to train (default: ``1``)\n        param_scale (float):\n            how many copies of parameters would be allocated for training. It\n            depends on optimizer. See the above guide. (default: ``2.0``)\n        device (\'cuda\' device):\n            CUDA device where each layer is profiled (default: the current CUDA\n            device)\n\n    Returns:\n        A list of number of layers in each partition. Use it for the `balance`\n        parameter of :class:`~torchgpipe.GPipe`.\n\n    .. note::\n        `module` and `input` must be placed on the same CUDA device.\n\n    """"""\n    sizes = profile_sizes(module, input, chunks, param_scale, torch.device(device))\n    return balance_cost(sizes, partitions)\n'"
torchgpipe/balance/blockpartition.py,0,"b'""""""Implements ""Block Partitions of Sequences"" by Imre B\xc3\xa1r\xc3\xa1ny et al.\n\nPaper: https://arxiv.org/pdf/1308.2452.pdf\n\n""""""\nfrom typing import Iterator, List, Tuple\n\n__all__ = [\'solve\']\n\n\ndef solve(sequence: List[int], partitions: int = 1) -> List[List[int]]:\n    """"""Splits a sequence into several partitions to minimize variance for each\n    partition.\n\n    The result might not be optimal. However, it can be done only in O(kn\xc2\xb3),\n    where k is the number of partitions and n is the length of the sequence.\n\n    """"""\n    if partitions < 1:\n        raise ValueError(f\'partitions must be a positive integer ({partitions} < 1)\')\n\n    n = len(sequence)\n    if n < partitions:\n        raise ValueError(f\'sequence is shorter than intended partitions ({n} < {partitions})\')\n\n    # Normalize the sequence in [0, 1].\n    minimum = min(sequence)\n    maximum = max(sequence) - minimum\n\n    normal_sequence: List[float]\n    if maximum == 0:\n        normal_sequence = [0 for _ in sequence]\n    else:\n        normal_sequence = [(x-minimum)/maximum for x in sequence]\n\n    splits = [n//partitions * (x+1) for x in range(partitions-1)] + [n]\n\n    def block_size(i: int) -> float:\n        start = splits[i-1] if i > 0 else 0\n        stop = splits[i]\n        return sum(normal_sequence[start:stop])\n\n    def leaderboard() -> Iterator[Tuple[float, int]]:\n        return ((block_size(i), i) for i in range(partitions))\n\n    while True:\n        """"""\n        (1) Fix p \xe2\x88\x88 [k] with M(P) = bp. So Bp is a maximal block of P.\n        """"""\n        # max_size: M(P)\n        max_size, p = max(leaderboard())\n\n        while True:\n            """"""\n            (2) If M(P) \xe2\x89\xa4 m(P) + 1, then stop.\n            """"""\n            # min_size: m(P)\n            min_size, q = min(leaderboard())\n\n            if max_size <= min_size + 1:\n                return [sequence[i:j] for i, j in zip([0]+splits[:-1], splits)]\n\n            """"""\n            (3) If M(P) > m(P) + 1, then let m(P) = bq for the q \xe2\x88\x88 [k] which is\n            closest to p (ties broken arbitrarily). Thus Bq is a minimal block\n            of P. Let Bh be the block next to Bq between Bp and Bq. (Note that\n            Bh is a non-empty block: if it were, then m(P) = 0 and we should\n            have chosen Bh instead of Bq.)\n            """"""\n            if p < q:\n                """"""\n                So either p < q and then h = q\xe2\x88\x921 and we define P \xe2\x88\x97 by moving\n                the last element from Bh = Bq\xe2\x88\x921 to Bq,\n                """"""\n                h = q - 1\n                splits[h] -= 1\n            else:\n                """"""\n                or q < p, and then h = q + 1 and P \xe2\x88\x97 is obtained by moving the\n                first element of Bh = Bq+1 to Bq.\n                """"""\n                h = q + 1\n                splits[q] += 1\n\n            """"""\n            Set P = P \xe2\x88\x97 . If p = h, then go to (1), else go to (2).\n            """"""\n            if p == h:\n                break\n'"
torchgpipe/balance/profile.py,10,"b'""""""Per-layer profilers.""""""\nimport copy\nimport time\nfrom typing import Generator, List, Tuple, Union\n\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\n\nfrom torchgpipe.microbatch import Batch\n\n__all__: List[str] = []\n\n\nDevice = Union[torch.device, int, str]\n\nTensors = Tuple[Tensor, ...]\nTensorOrTensors = Union[Tensor, Tensors]\n\n\ndef layerwise_sandbox(module: nn.Sequential,\n                      device: torch.device,\n                      ) -> Generator[nn.Module, None, None]:\n    """"""Copies layers for ease to profile. It doesn\'t modify the given\n    module.\n    """"""\n    for layer in module:\n        layer_copy = copy.deepcopy(layer)\n        layer_copy.to(device)\n        layer_copy.train()\n        yield layer_copy\n\n\ndef detach(batch: Batch) -> None:\n    """"""Detaches from autograd graph.""""""\n    for i, x in enumerate(batch):\n        batch[i] = x.detach().requires_grad_(x.requires_grad)\n\n\ndef profile_times(module: nn.Sequential,\n                  sample: TensorOrTensors,\n                  timeout: float,\n                  device: torch.device,\n                  ) -> List[int]:\n    """"""Profiles elapsed times per layer.""""""\n    if any(p.grad is not None for p in module.parameters()):\n        raise ValueError(\'some parameter already has gradient\')\n\n    _batch = Batch(sample)\n    for i, x in enumerate(_batch):\n        _batch[i] = x.detach().to(device).requires_grad_(x.requires_grad)\n\n    time_bufs: List[List[float]] = [[] for _ in module]\n    begun_at = time.time()\n\n    while time.time() - begun_at < timeout:\n        batch = _batch\n\n        for i, layer in enumerate(layerwise_sandbox(module, device)):\n            detach(batch)\n\n            if device.type == \'cuda\':\n                torch.cuda.synchronize(device)\n            tick = time.time()\n\n            # Forward\n            batch = batch.call(layer)\n\n            # Backward\n            backward_tensors = tuple(y for y in batch if y.requires_grad)\n            if backward_tensors:\n                torch.autograd.backward(backward_tensors, backward_tensors)\n\n            if device.type == \'cuda\':\n                torch.cuda.synchronize(device)\n            tock = time.time()\n\n            time_bufs[i].append(tock - tick)\n\n    us = 1_000_000\n    return [sum(int(t*us) for t in buf) for buf in time_bufs]\n\n\ndef profile_sizes(module: nn.Sequential,\n                  input: TensorOrTensors,\n                  chunks: int,\n                  param_scale: float,\n                  device: torch.device,\n                  ) -> List[int]:\n    """"""Profiles CUDA memory usage per layer.""""""\n    if device.type != \'cuda\':\n        raise ValueError(\'size profiler supports only CUDA device\')\n\n    batch = Batch(input)\n    sizes: List[int] = []\n\n    latent_scale = batch[0].size(0) / chunks\n    for i, x in enumerate(batch):\n        batch[i] = x[:1].detach().to(device).requires_grad_(x.requires_grad)\n\n    for layer in layerwise_sandbox(module, device):\n        detach(batch)\n\n        # Detect memory usage at forward.\n        memory_before = torch.cuda.memory_allocated(device)\n        batch = batch.call(layer)\n        memory_after = torch.cuda.memory_allocated(device)\n        latent_size = memory_after - memory_before\n\n        # Analyze size of parameters.\n        param_size = sum(p.storage().size() * p.storage().element_size()\n                         for p in layer.parameters())\n\n        # Combine size of parameters and activations with normalize scales.\n        size = latent_size*latent_scale + param_size*param_scale\n        sizes.append(int(size))\n\n    return sizes\n'"
torchgpipe/skip/__init__.py,0,"b'""""""Supports efficiency with skip connections.""""""\nfrom torchgpipe.skip.namespace import Namespace\nfrom torchgpipe.skip.skippable import pop, skippable, stash, verify_skippables\n\n__all__ = [\'skippable\', \'stash\', \'pop\', \'verify_skippables\', \'Namespace\']\n'"
torchgpipe/skip/layout.py,0,"b'""""""Static skip connection layout of ``@skippable`` modules.""""""\nfrom typing import Dict, Iterable, List, Tuple\n\nfrom torch import nn\n\nfrom torchgpipe.skip.namespace import Namespace\n\n__all__: List[str] = []\n\n\nclass SkipLayout:\n    """"""Represents a skip connection layout across partitions.""""""\n\n    # Skip routes indexed by \'ns, name\': {(ns, name): (prev_j, next_j), ...}\n    by_ns_name: Dict[Tuple[Namespace, str], Tuple[int, int]]\n\n    # Skip routes indexed by partition number \'j\': [[next_j]: [(prev_j, ns, name), ...], ...]\n    by_partition: List[List[Tuple[int, Namespace, str]]]\n\n    def __init__(self,\n                 num_partitions: int,\n                 skip_routes: Dict[Tuple[Namespace, str], Tuple[int, int]],\n                 ) -> None:\n        # The skip routes are already indexed by \'ns, name\'.\n        self.by_ns_name = skip_routes\n\n        # Index skip routes by partition number \'j\'.\n        self.by_partition = [[] for _ in range(num_partitions)]\n\n        for (ns, name), (prev_j, next_j) in skip_routes.items():\n            self.by_partition[next_j].append((prev_j, ns, name))\n\n        for p in self.by_partition:\n            p.sort()\n\n    def copy_policy(self, next_j: int) -> Iterable[Tuple[int, Namespace, str]]:\n        """"""Generates skip routes for the given destination partition number.\n        The skip routes are sorted by source partition number in ascending\n        order.\n\n        Yields:\n            Each tuple of (source partition number, namespace, name).\n\n        """"""\n        for prev_j, ns, name in self.by_partition[next_j]:\n            if prev_j == next_j:\n                # This skip tensor will be popped at the same partition where\n                # it is stashed. In this case, copy is not required.\n                continue\n\n            yield (prev_j, ns, name)\n\n    def requires_copy(self, ns: Namespace, name: str) -> bool:\n        """"""Whether the given namespace and name requires partition-to-partition\n        copy or not.\n        """"""\n        prev_j, next_j = self.by_ns_name.get((ns, name), (-1, -1))\n        return prev_j != next_j\n\n\ndef inspect_skip_layout(partitions: List[nn.Sequential]) -> SkipLayout:\n    """"""Inspects the skip connection layout in the given partitions.""""""\n    # NOTE(sublee): Hide circular import inside this subroutine. Circular\n    # import is not ideal but placing this logic near to SkipLayout may\n    # increase cohesion of code.\n    from torchgpipe.skip.skippable import Skippable\n\n    skip_routes: Dict[Tuple[Namespace, str], Tuple[int, int]] = {}\n    stashed_at: Dict[Tuple[Namespace, str], int] = {}\n\n    for j, partition in enumerate(partitions):\n        for layer in partition:\n            if not isinstance(layer, Skippable):\n                continue\n\n            for ns, name in layer.stashable():\n                stashed_at[(ns, name)] = j\n\n            for ns, name in layer.poppable():\n                prev_j = stashed_at.pop((ns, name))\n                skip_routes[(ns, name)] = (prev_j, j)\n\n    return SkipLayout(len(partitions), skip_routes)\n'"
torchgpipe/skip/namespace.py,0,"b'""""""Provides isolated namespace of skip tensors.""""""\nimport abc\nfrom functools import total_ordering\nfrom typing import Any\nimport uuid\n\n__all__ = [\'Namespace\']\n\n\n@total_ordering\nclass Namespace(metaclass=abc.ABCMeta):\n    """"""Namespace for isolating skip tensors used by :meth:`isolate()\n    <torchgpipe.skip.skippable.Skippable.isolate>`.\n    """"""\n    __slots__ = (\'id\',)\n\n    def __init__(self) -> None:\n        self.id = uuid.uuid4()\n\n    def __repr__(self) -> str:\n        return f""<Namespace \'{self.id}\'>""\n\n    def __hash__(self) -> int:\n        return hash(self.id)\n\n    # Namespaces should support ordering, since SkipLayout will sort tuples\n    # including a namespace. But actual order between namespaces is not\n    # important. That\'s why they are ordered by version 4 UUID which generates\n    # random numbers.\n    def __lt__(self, other: Any) -> bool:\n        if isinstance(other, Namespace):\n            return self.id < other.id\n        return False\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, Namespace):\n            return self.id == other.id\n        return False\n\n\n# \'None\' is the default namespace,\n# which means that \'isinstance(None, Namespace)\' is \'True\'.\nNamespace.register(type(None))\n'"
torchgpipe/skip/portal.py,5,"b'""""""Portal keeps a tensor in the pocket plane. The tensor becomes hidden to the\nautograd engine. The shared context of three functions (:class:`PortalBlue`,\n:class:`PortalOrange`, and :class:`PortalCopy`) out of the computation graph is\none of the most important feature of :mod:`torchgpipe.skip`.\n\nThe metaphor is inspired by Portal\xe2\x84\xa2 from Valve.\n\n""""""\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import Tensor\n\nfrom torchgpipe.copy import Context as CopyContext\nfrom torchgpipe.copy import Copy\nfrom torchgpipe.phony import get_phony\nfrom torchgpipe.stream import AbstractStream, get_device\n\n__all__: List[str] = []\n\n\nclass Portal:\n    """"""A portal for a tensor.""""""\n\n    def __init__(self, tensor: Optional[Tensor], tensor_life: int) -> None:\n        self.put_tensor(tensor, tensor_life)\n        self.grad: Optional[Tensor] = None\n\n    def blue(self) -> Tensor:\n        """"""Creates a :class:`PortalBlue` which hides the underlying tensor from\n        the autograd engine.\n\n        Join the returning phony to the main lane of the autograd graph to\n        assure the correct backpropagation::\n\n            PortalBlue --+\n                         |\n            ---------- Join --\n\n        """"""\n        tensor = self.use_tensor()\n\n        if tensor is None:\n            return get_phony(torch.device(\'cpu\'), requires_grad=False)\n\n        return PortalBlue.apply(self, tensor)\n\n    def orange(self, phony: Tensor) -> Optional[Tensor]:\n        """"""Creates a :class:`PortalOrange` which retrieves the hidden tensor\n        without losing ability of backpropagation.\n\n        Give a phony forked from the main lane of an autograd graph::\n\n                +-- PortalOrange --+\n                |                  |\n            -- Fork --------- f(a, b) --\n\n        """"""\n        self.check_tensor_life()\n\n        if self.tensor is None:\n            return self.use_tensor()\n\n        return PortalOrange.apply(self, phony)\n\n    def copy(self,\n             prev_stream: AbstractStream,\n             next_stream: AbstractStream,\n             phony: Tensor,\n             ) -> Tensor:\n        """"""Copies the hidden tensor by a :class:`PortalCopy`.\n\n        Give a phony and use the returning phony to keep backpropagation::\n\n                +-- PortalCopy --+\n                |                |\n            -- Fork ---------- Join --\n\n        """"""\n        if self.tensor is None:\n            return get_phony(torch.device(\'cpu\'), requires_grad=False)\n\n        return PortalCopy.apply(self, prev_stream, next_stream, phony)\n\n    def check_tensor_life(self) -> None:\n        if self.tensor_life <= 0:\n            raise RuntimeError(\'tensor in portal has been removed\')\n\n    def put_tensor(self, tensor: Optional[Tensor], tensor_life: int) -> None:\n        """"""Stores a tensor into this portal.""""""\n        # [Life of Tensor through Portal]\n        #\n        # The tensor can be retrieved by use_tensor() up to \'tensor_life\'\n        # times. When the life becomes 0, the tensor will be deleted for\n        # deallocation in CUDA memory.\n        #\n        # The below events participate in a tensor through a portal.\n        # Note that [x] denotes the events which call use_tensor():\n        #\n        #  1. [x] blue()\n        #  2. [ ]   PortalBlue.forward\n        #  3. [ ] copy()\n        #  4. [ ]   PortalCopy.forward\n        #  5. [ ] orange()\n        #  6. [x]   PortalOrange.forward\n        # - - - - - - - - - - - - - - - - - - - - - - - - - - -\n        #  7. [ ] orange() (recomputed)\n        #  8. [x]   PortalOrange.forward (recomputed)\n        #  9. [ ]   PortalOrange.backward\n        # 10. [ ] PortalCopy.backward\n        # 11. [x] blue() (recomputed)\n        # 12. [ ]   PortalBlue.forward (recomputed)\n        # 13. [ ]   PortalBlue.backward\n        #\n        self.tensor_life = tensor_life\n\n        if tensor_life > 0:\n            self.tensor = tensor\n        else:\n            self.tensor = None\n\n    def use_tensor(self) -> Optional[Tensor]:\n        """"""Retrieves the underlying tensor and decreases the tensor  life. When\n        the life becomes 0, it the tensor will be removed.\n        """"""\n        self.check_tensor_life()\n\n        tensor = self.tensor\n\n        self.tensor_life -= 1\n\n        if self.tensor_life <= 0:\n            self.tensor = None\n\n        return tensor\n\n    def put_grad(self, grad: Tensor) -> None:\n        """"""Stores a gradient into this portal.""""""\n        self.grad = grad\n\n    def use_grad(self) -> Tensor:\n        """"""Retrieves and removes the underlying gradient. The gradient is\n        always ephemeral.\n        """"""\n        if self.grad is None:\n            raise RuntimeError(\'grad in portal has been removed or never set\')\n\n        grad = self.grad\n        self.grad = None\n        return grad\n\n\n# Common interface between :class:`PortalBlue`, :class:`PortalOrange`, and\n# :class:`PortalCopy`.\nclass Context(CopyContext):\n    portal: Portal\n\n\nclass PortalBlue(torch.autograd.Function):\n    """"""Hides a tensor from the autograd engine by a :class:`Portal`.""""""\n    @staticmethod\n    def forward(ctx: Context,  # type: ignore\n                portal: Portal,\n                # This tensor must be retrieved by portal.use_tensor().\n                tensor: Tensor,\n                ) -> Tensor:\n        ctx.portal = portal\n\n        phony = get_phony(tensor.device, requires_grad=False)\n        return phony.detach()\n\n    @staticmethod\n    def backward(ctx: Context,  # type: ignore\n                 grad_phony: Tensor,\n                 ) -> Tuple[None, Tensor]:\n        # The paired PortalOrange should keep the gradient.\n        grad = ctx.portal.use_grad()\n        return None, grad\n\n\nclass PortalOrange(torch.autograd.Function):\n    """"""Retrieves the hidden tensor from a :class:`Portal`.""""""\n    @staticmethod\n    def forward(ctx: Context, portal: Portal, phony: Tensor) -> Tensor:  # type: ignore\n        ctx.portal = portal\n\n        tensor = portal.use_tensor()\n        assert tensor is not None\n\n        return tensor.detach()\n\n    @staticmethod\n    def backward(ctx: Context, grad: Tensor) -> Tuple[None, None]:  # type: ignore\n        # The paired PortalBlue will use the gradient.\n        ctx.portal.put_grad(grad)\n        return None, None\n\n\nclass PortalCopy(torch.autograd.Function):\n    """"""Copies the hidden tensor in a :class:`Portal`. It replaces the hidden\n    tensor with copied one.\n    """"""\n    @staticmethod\n    def forward(ctx: Context,  # type: ignore\n                portal: Portal,\n                prev_stream: AbstractStream,\n                next_stream: AbstractStream,\n                phony: Tensor,\n                ) -> Tensor:\n        ctx.portal = portal\n\n        assert portal.tensor is not None\n        portal.tensor, = Copy.forward(ctx, prev_stream, next_stream, portal.tensor)\n\n        phony = get_phony(get_device(next_stream), requires_grad=False)\n        return phony.detach()\n\n    @staticmethod\n    def backward(ctx: Context,  # type: ignore\n                 grad_phony: Tensor,\n                 ) -> Tuple[None, None, None, None]:\n        portal = ctx.portal\n\n        assert portal.grad is not None\n        _, _, portal.grad = Copy.backward(ctx, portal.grad)\n\n        return None, None, None, None\n'"
torchgpipe/skip/skippable.py,2,"b'""""""The user interface to define skip connections.""""""\nfrom typing import (TYPE_CHECKING, Any, Callable, ClassVar, Dict, FrozenSet, Generator, Iterable,\n                    List, Optional, Set, Tuple, Type, TypeVar, Union, cast)\n\nfrom torch import Tensor, nn\n\nfrom torchgpipe.microbatch import Batch\nfrom torchgpipe.skip.namespace import Namespace\nfrom torchgpipe.skip.tracker import current_skip_tracker\n\n__all__ = [\'skippable\', \'stash\', \'pop\', \'verify_skippables\']\n\n\nTensors = Tuple[Tensor, ...]\nTensorOrTensors = Union[Tensor, Tensors]\n\nStashPop = Union[\'stash\', \'pop\']\nStashPopGenerator = Generator[StashPop, Optional[Tensor], TensorOrTensors]\nif TYPE_CHECKING:\n    SkippableModule = nn.Module[Union[StashPopGenerator, TensorOrTensors]]\nelse:\n    SkippableModule = nn.Module\n\nT = TypeVar(\'T\', bound=\'Skippable\')\n\n\nclass Skippable(nn.Module):\n    """"""The base class for skippable modules.\n\n    Do not use this class directly. Define a subclass by :func:`skippable`\n    instead.\n\n    """"""\n    module_cls: ClassVar[Type[SkippableModule]]\n    stashable_names: ClassVar[FrozenSet[str]]\n    poppable_names: ClassVar[FrozenSet[str]]\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__()\n        self.module = self.module_cls(*args, **kwargs)  # type: ignore\n        self.namespaces: Dict[str, Namespace] = {}\n\n    def __repr__(self) -> str:\n        return f\'@skippable({self.module})\'\n\n    def namespaced(self, name: str) -> Tuple[Namespace, str]:\n        """"""Prepends namespace for the given skip name.""""""\n        ns = self.namespaces.get(name)\n        ns = cast(Namespace, ns)\n        return (ns, name)\n\n    def stashable(self) -> Iterable[Tuple[Namespace, str]]:\n        """"""Iterates over namespaced skip names to be stashed.""""""\n        for name in self.stashable_names:\n            yield self.namespaced(name)\n\n    def poppable(self) -> Iterable[Tuple[Namespace, str]]:\n        """"""Iterates over namespaced skip names to be popped.""""""\n        for name in self.poppable_names:\n            yield self.namespaced(name)\n\n    def isolate(self: T, ns: Namespace, *, only: Optional[Iterable[str]] = None) -> T:\n        r""""""Isolates a specified subset or the whole set of skip tensors into a\n        namespace. In a single sequential module, skip tensors with the same\n        name are not allowed unless they are isolated by different namespaces.\n\n        Here\'s an example using the same name for skip tensors twice. Each pair\n        of ``Layer1`` and ``Layer2`` is isolated with its own namespace ``ns1``\n        and ``ns2``. There is no conflict anymore::\n\n            ns1 = Namespace()\n            ns2 = Namespace()\n\n            model = nn.Sequential(\n                Layer1().isolate(ns1),\n                Layer1().isolate(ns2),\n                Layer2(),\n                Layer3().isolate(ns2),\n                Layer3().isolate(ns1),\n            )\n\n        When `only` parameter is omitted, all skip tensors are isolated. You\n        can isolate a subset of skip tensors by passing `only` parameter::\n\n            ns_alice = Namespace()\n            ns_bob = Namespace()\n\n            model = nn.Sequential(\n                ...\n                StashStashPop().isolate(ns_alice, only=[\'alice\']) \\\n                               .isolate(ns_bob, only=[\'bob\']),\n                ...\n            )\n\n        Args:\n            ns (Namespace):\n                namespace for isolation\n\n        Keyword Args:\n            only (iterable of strs):\n                names of specific skip tensors to be isolated (omit this option\n                to isolate all skip tensors declared in this module)\n\n        Returns:\n            this module itself\n\n        """"""\n        names: Iterable[str]\n\n        if only is None:\n            names = self.stashable_names | self.poppable_names\n        else:\n            names = set(only)\n\n        for name in names:\n            self.namespaces[name] = ns\n\n        return self\n\n    def dispatch(self,\n                 input: TensorOrTensors,\n                 handle_stash: Callable[[str, Optional[Tensor]], None],\n                 handle_pop: Callable[[str], Optional[Tensor]],\n                 ) -> TensorOrTensors:\n        """"""Dispatches :class:`stash` or :class:`pop` commands generated by the\n        module\'s ``forward()``.\n        """"""\n        generator = self.module(input)\n\n        if not isinstance(generator, Generator):\n            # The underlying module returned output without any yield.\n            output = generator\n            return output\n\n        try:\n            op = next(generator)\n\n            while True:\n                if isinstance(op, stash):\n                    handle_stash(op.name, op.tensor)\n                    op = next(generator)\n                    continue\n\n                if isinstance(op, pop):\n                    tensor = handle_pop(op.name)\n                    op = generator.send(tensor)\n                    continue\n\n                raise TypeError(\'%r is not a command from @skippable\' % op)\n\n        except StopIteration as stop:\n            output = stop.args[0]\n            return output\n\n    def forward(self, input: TensorOrTensors) -> TensorOrTensors:  # type: ignore\n        """"""Performs the forward propagation. :class:`stash` or :class:`pop`\n        commands will be handled by portals silently. The portals won\'t be\n        exposed to users.\n\n        Raises:\n            RuntimeError:\n                illegal \'stash\' or \'pop\' is found.\n\n        """"""\n        skip_tracker = current_skip_tracker()\n        stashed_tensors: Dict[str, Optional[Tensor]] = {}\n\n        # Load skip tensors that might be popped.\n        poppable_tensors = {}\n        batch = Batch(input)\n        for ns, name in self.poppable():\n            try:\n                poppable_tensors[name] = skip_tracker.load(batch, ns, name)\n            except KeyError:\n                raise RuntimeError(f""\'{name}\' has not been stashed"")\n        input = batch.tensor_or_tensors\n\n        # Handle skip commands.\n        def handle_stash(name: str, tensor: Optional[Tensor]) -> None:\n            if name not in self.stashable_names:\n                raise RuntimeError(f""\'{name}\' has not been declared as stashable"")\n            stashed_tensors[name] = tensor\n\n        def handle_pop(name: str) -> Optional[Tensor]:\n            if name not in self.poppable_names:\n                raise RuntimeError(f""\'{name}\' has not been declared as poppable"")\n            return poppable_tensors.pop(name)\n\n        output = self.dispatch(input, handle_stash, handle_pop)\n\n        # All declared skips must be stashed or popped.\n        not_stashed = self.stashable_names - stashed_tensors.keys()\n        if not_stashed:\n            comma_names = \', \'.join(""\'%s\'"" % n for n in not_stashed)\n            raise RuntimeError(f\'{comma_names} must be stashed but have not\')\n\n        not_popped = poppable_tensors.keys()\n        if not_popped:\n            comma_names = \', \'.join(""\'%s\'"" % n for n in not_popped)\n            raise RuntimeError(f\'{comma_names} must be popped but have not\')\n\n        # Save stashed skip tensors.\n        batch = Batch(output)\n        for ns, name in self.stashable():\n            tensor = stashed_tensors[name]\n            skip_tracker.save(batch, ns, name, tensor)\n        output = batch.tensor_or_tensors\n\n        return output\n\n\n# TODO(sublee): Move to above of Skippable class for better read flow.\ndef skippable(stash: Iterable[str] = (),\n              pop: Iterable[str] = (),\n              ) -> Callable[[Type[SkippableModule]], Type[Skippable]]:\n    """"""The decorator to define a :class:`nn.Module <torch.nn.Module>` with skip\n    connections. Decorated modules are called ""skippable"". This functionality\n    works perfectly fine even when the module is not wrapped by\n    :class:`~torchgpipe.GPipe`.\n\n    Each skip tensor is managed by its name. Before manipulating skip tensors,\n    a skippable module must statically declare the names for skip tensors by\n    `stash` and/or `pop` parameters. Skip tensors with pre-declared name can be\n    stashed by ``yield stash(name, tensor)`` or popped by ``tensor = yield\n    pop(name)``.\n\n    Here is an example with three layers. A skip tensor named ""1to3"" is stashed\n    and popped at the first and last layer, respectively::\n\n        @skippable(stash=[\'1to3\'])\n        class Layer1(nn.Module):\n            def forward(self, input):\n                yield stash(\'1to3\', input)\n                return f1(input)\n\n        class Layer2(nn.Module):\n            def forward(self, input):\n                return f2(input)\n\n        @skippable(pop=[\'1to3\'])\n        class Layer3(nn.Module):\n            def forward(self, input):\n                skip_1to3 = yield pop(\'1to3\')\n                return f3(input) + skip_1to3\n\n        model = nn.Sequential(Layer1(), Layer2(), Layer3())\n\n    One skippable module can stash or pop multiple skip tensors::\n\n        @skippable(stash=[\'alice\', \'bob\'], pop=[\'carol\'])\n        class StashStashPop(nn.Module):\n            def forward(self, input):\n                yield stash(\'alice\', f_alice(input))\n                yield stash(\'bob\', f_bob(input))\n                carol = yield pop(\'carol\')\n                return input + carol\n\n    Every skip tensor must be associated with exactly one pair of `stash` and\n    `pop`. :class:`~torchgpipe.GPipe` checks this restriction automatically\n    when wrapping a module. You can also check the restriction by\n    :func:`~torchgpipe.skip.verify_skippables` without\n    :class:`~torchgpipe.GPipe`.\n\n    .. note::\n\n        :func:`@skippable <skippable>` changes the type of the wrapped class.\n        But currently (mypy v0.740), mypy could not understand class decorators\n        yet (`#3135 <https://github.com/python/mypy/issues/3135>`_).\n\n        There are two workarounds:\n\n        1. Naively ignore type errors by ``# type: ignore``.\n        2. Use ``skippable()()`` as a function instead of a decorator.\n\n    .. seealso:: :ref:`Long Skip Connections`\n\n    """"""\n    stashable_names = frozenset(stash)\n    poppable_names = frozenset(pop)\n\n    def extend_skippable(module_cls: Type[SkippableModule]) -> Type[Skippable]:\n        name = module_cls.__name__\n        bases = (Skippable,)\n        attrs = {\'module_cls\': module_cls,\n                 \'stashable_names\': stashable_names,\n                 \'poppable_names\': poppable_names}\n        return type(name, bases, attrs)\n\n    return extend_skippable\n\n\nclass stash:\n    """"""The command to stash a skip tensor.\n\n    ::\n\n        def forward(self, input):\n            yield stash(\'name\', input)\n            return f(input)\n\n    Args:\n        name (str): name of skip tensor\n        input (torch.Tensor or None): tensor to pass to the skip connection\n\n    """"""\n    __slots__ = (\'name\', \'tensor\')\n\n    def __init__(self, name: str, tensor: Optional[Tensor]) -> None:\n        self.name = name\n        self.tensor = tensor\n\n\nclass pop:\n    """"""The command to pop a skip tensor.\n\n    ::\n\n        def forward(self, input):\n            skip = yield pop(\'name\')\n            return f(input) + skip\n\n    Args:\n        name (str): name of skip tensor\n\n    Returns:\n        the skip tensor previously stashed by another layer under the same name\n\n    """"""\n    __slots__ = (\'name\',)\n\n    def __init__(self, name: str) -> None:\n        self.name = name\n\n\ndef verify_skippables(module: nn.Sequential) -> None:\n    """"""Verifies if the underlying skippable modules satisfy integrity.\n\n    Every skip tensor must have only one pair of `stash` and `pop`. If there\n    are one or more unmatched pairs, it will raise :exc:`TypeError` with the\n    detailed messages.\n\n    Here are a few failure cases. :func:`verify_skippables` will report failure\n    for these cases::\n\n        # Layer1 stashes ""1to3"".\n        # Layer3 pops ""1to3"".\n\n        nn.Sequential(Layer1(), Layer2())\n        #               \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80 ?\n\n        nn.Sequential(Layer2(), Layer3())\n        #                   ? \xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x98\n\n        nn.Sequential(Layer1(), Layer2(), Layer3(), Layer3())\n        #               \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x98       ^^^^^^\n\n        nn.Sequential(Layer1(), Layer1(), Layer2(), Layer3())\n        #             ^^^^^^      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x98\n\n    To use the same name for multiple skip tensors, they must be isolated by\n    different namespaces. See :meth:`isolate()\n    <torchgpipe.skip.skippable.Skippable.isolate>`.\n\n    Raises:\n        TypeError:\n            one or more pairs of `stash` and `pop` are not matched.\n\n    """"""\n    stashed: Set[Tuple[Namespace, str]] = set()\n    popped: Set[Tuple[Namespace, str]] = set()\n    msgs: List[str] = []\n\n    for layer_name, layer in module.named_children():\n        if not isinstance(layer, Skippable):\n            continue\n\n        for name in layer.stashable_names & layer.poppable_names:\n            msg = f""\'{layer_name}\' declared \'{name}\' both as stashable and as poppable""\n            msgs.append(msg)\n\n        for ns, name in layer.stashable():\n            if name in layer.poppable_names:\n                continue\n\n            if (ns, name) in stashed:\n                msg = (f""\'{layer_name}\' redeclared \'{name}\' as stashable ""\n                       \'but not isolated by namespace\')\n                msgs.append(msg)\n                continue\n\n            stashed.add((ns, name))\n\n        for ns, name in layer.poppable():\n            if name in layer.stashable_names:\n                continue\n\n            if (ns, name) in popped:\n                msg = (f""\'{layer_name}\' redeclared \'{name}\' as poppable ""\n                       \'but not isolated by namespace\')\n                msgs.append(msg)\n                continue\n\n            if (ns, name) not in stashed:\n                msg = f""\'{layer_name}\' declared \'{name}\' as poppable but it was not stashed""\n                msgs.append(msg)\n                continue\n\n            popped.add((ns, name))\n\n    for (_, name) in stashed - popped:\n        msg = f""no module declared \'{name}\' as poppable but stashed""\n        msgs.append(msg)\n\n    if msgs:\n        raise TypeError(\'one or more pairs of stash and pop do not match:\\n\\n%s\'\n                        \'\' % \'\\n\'.join(\'* %s\' % x for x in msgs))\n'"
torchgpipe/skip/tracker.py,0,"b'""""""Tracks skip tensors on a thread.""""""\nfrom contextlib import contextmanager\nimport threading\nfrom typing import Dict, Generator, List, Optional, Tuple\n\nfrom torch import Tensor\n\nfrom torchgpipe.checkpoint import is_checkpointing\nfrom torchgpipe.dependency import fork, join\nfrom torchgpipe.microbatch import Batch\nfrom torchgpipe.skip.layout import SkipLayout\nfrom torchgpipe.skip.namespace import Namespace\nfrom torchgpipe.skip.portal import Portal\nfrom torchgpipe.stream import AbstractStream\n\n__all__: List[str] = []\n\n\nclass SkipTracker:\n    """"""Tracks saved skip tensors.\n\n    It will update the given micro-batch in place. This is because when it\n    manipulates the underlying skip tensors, the current micro-batch also has\n    to be connected with the skip tensors.\n\n    One thread has one skip tracker. Call :func:`current_skip_tracker` to get\n    the skip tracker on the current thread.\n\n    """"""\n\n    def __init__(self) -> None:\n        self.tensors: Dict[Tuple[Namespace, str], Optional[Tensor]] = {}\n\n    def save(self, batch: Batch, ns: Namespace, name: str, tensor: Optional[Tensor]) -> None:\n        self.tensors[(ns, name)] = tensor\n\n    def load(self, batch: Batch, ns: Namespace, name: str) -> Optional[Tensor]:\n        return self.tensors.pop((ns, name))\n\n    def copy(self,\n             batch: Batch,\n             prev_stream: AbstractStream,\n             next_stream: AbstractStream,\n             ns: Namespace,\n             name: str,\n             ) -> None:\n        raise TypeError(\'copy is not supported for non-portal skip tensors\')\n\n\nclass SkipTrackerThroughPotals(SkipTracker):\n    """"""Tracks saved skip tensors through portals. The skip tensors will be\n    hidden in portals so that the autograd engine does not need to track them.\n\n    This tracker is only used when the training or evaluating module is wrapped\n    with :class:`torchgpipe.GPipe`.\n\n    """"""\n\n    def __init__(self, skip_layout: SkipLayout) -> None:\n        super().__init__()\n        self.skip_layout = skip_layout\n        self.portals: Dict[Tuple[Namespace, str], Portal] = {}\n\n    def save(self, batch: Batch, ns: Namespace, name: str, tensor: Optional[Tensor]) -> None:\n        """"""Saves the stashed skip tensor in a portal. The portal is then\n        connected to the given micro-batch with :class:`Join`.\n        """"""\n        if not self.skip_layout.requires_copy(ns, name):\n            super().save(batch, ns, name, tensor)\n            return\n\n        # See [Tensor Life of Portal] at Portal.put_tensor() to understand the\n        # below tensor_life values. Here are the selected events which retrieve\n        # the tensor in portal:\n        #\n        #  1. [x] blue()\n        #     ...\n        #  6. [x]   PortalOrange.forward\n        #     ...\n        #  8. [x]   PortalOrange.forward (recomputed)\n        #     ...\n        # 11. [x] blue() (recomputed)\n        #\n        if (ns, name) not in self.portals:\n            if is_checkpointing():\n                # Under checkpointing, the tensor used by the first\n                # PortalOrange should be alive in the portal. This tensor will\n                # be used again by the second PortalOrange during the\n                # recomputation.\n                tensor_life = 3  # Delete at [8. PortalOrange.forward (recomputed)]\n            else:\n                tensor_life = 2  # Delete at [6. PortalOrange.forward]\n\n            portal = Portal(tensor, tensor_life)\n            self.portals[(ns, name)] = portal\n\n        else:\n            # Under recomputation, the portal already exists.\n            portal = self.portals[(ns, name)]\n\n            # The existing tensor life already became 0. It should be reset as\n            # 1 to delete the tensor after the second PortalBlue immediately.\n            tensor_life = 1  # Delete at [11. blue() (recomputed)]\n\n            portal.put_tensor(tensor, tensor_life)\n\n        phony = portal.blue()\n        batch[0] = join(batch[0], phony)\n\n    def load(self, batch: Batch, ns: Namespace, name: str) -> Optional[Tensor]:\n        """"""Loads a skip tensor from the corresponding portal to pop. The given\n        micro-batch is connected to the portal with :class:`Fork`.\n        """"""\n        if not self.skip_layout.requires_copy(ns, name):\n            tensor = super().load(batch, ns, name)\n            return tensor\n\n        portal = self.portals[(ns, name)]\n        batch[0], phony = fork(batch[0])\n        tensor = portal.orange(phony)\n        return tensor\n\n    def copy(self,\n             batch: Batch,\n             prev_stream: AbstractStream,\n             next_stream: AbstractStream,\n             ns: Namespace,\n             name: str,\n             ) -> None:\n        """"""Copies the skip tensor in the corresponding portal. The given\n        micro-batch and the portal will be tied with :class:`Fork` and\n        :class:`Join`.\n        """"""\n        assert self.skip_layout.requires_copy(ns, name)\n\n        batch[0], phony = fork(batch[0])\n\n        portal = self.portals[(ns, name)]\n        phony = portal.copy(prev_stream, next_stream, phony)\n\n        batch[0] = join(batch[0], phony)\n\n\nclass ThreadLocal(threading.local):\n    def __init__(self) -> None:\n        self.skip_tracker: Optional[SkipTracker] = None\n\n\nthread_local = ThreadLocal()\n\n\n@contextmanager\ndef use_skip_tracker(skip_tracker: SkipTracker) -> Generator[None, None, None]:\n    """"""Registers the given skip tracker on the current thread within a\n    context::\n\n        with use_skip_tracker(my_skip_tracker):\n            ...\n\n    """"""\n    orig = thread_local.skip_tracker\n\n    thread_local.skip_tracker = skip_tracker\n\n    try:\n        yield\n    finally:\n        thread_local.skip_tracker = orig\n\n\ndef current_skip_tracker() -> SkipTracker:\n    """"""Gets the skip tracker on the current thread.""""""\n    skip_tracker = thread_local.skip_tracker\n\n    if skip_tracker is None:\n        skip_tracker = SkipTracker()\n        thread_local.skip_tracker = skip_tracker\n\n    return skip_tracker\n'"
benchmarks/models/amoebanet/__init__.py,1,"b'""""""AmoebaNet-D for ImageNet""""""\nfrom collections import OrderedDict\nfrom typing import TYPE_CHECKING, Iterator, List, Tuple, Union, cast\n\nimport torch\nfrom torch import Tensor, nn\n\nfrom amoebanet.genotype import (NORMAL_CONCAT, NORMAL_OPERATIONS, REDUCTION_CONCAT,\n                                REDUCTION_OPERATIONS)\nfrom amoebanet.operations import FactorizedReduce\n\n__all__ = [\'amoebanetd\']\n\nif TYPE_CHECKING:\n    NamedModules = OrderedDict[str, nn.Module]\nelse:\n    NamedModules = OrderedDict\n\n\ndef relu_conv_bn(in_channels: int,\n                 out_channels: int,\n                 kernel_size: int = 1,\n                 stride: int = 1,\n                 padding: int = 0,\n                 ) -> nn.Module:\n    return nn.Sequential(\n        nn.ReLU(inplace=False),\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n        nn.BatchNorm2d(out_channels),\n    )\n\n\nclass Classify(nn.Module):\n\n    def __init__(self, channels_prev: int, num_classes: int):\n        super().__init__()\n        self.pool = nn.AvgPool2d(7)\n        self.flat = nn.Flatten()\n        self.fc = nn.Linear(channels_prev, num_classes)\n\n    def forward(self, states: Tuple[Tensor, Tensor]) -> Tensor:  # type: ignore\n        x, _ = states\n        x = self.pool(x)\n        x = self.flat(x)\n        x = self.fc(x)\n        return x\n\n\nclass Stem(nn.Module):\n    def __init__(self, channels: int) -> None:\n        super().__init__()\n\n        self.relu = nn.ReLU(inplace=False)\n        self.conv = nn.Conv2d(3, channels, 3, stride=2, padding=1, bias=False)\n        self.bn = nn.BatchNorm2d(channels)\n\n    def forward(self, input: Tensor) -> Tensor:  # type: ignore\n        x = input\n        x = self.relu(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass Cell(nn.Module):\n    def __init__(self,\n                 channels_prev_prev: int,\n                 channels_prev: int,\n                 channels: int,\n                 reduction: bool,\n                 reduction_prev: bool,\n                 ) -> None:\n        super().__init__()\n\n        self.reduce1 = relu_conv_bn(in_channels=channels_prev, out_channels=channels)\n\n        self.reduce2: nn.Module = nn.Identity()\n        if reduction_prev:\n            self.reduce2 = FactorizedReduce(channels_prev_prev, channels)\n        elif channels_prev_prev != channels:\n            self.reduce2 = relu_conv_bn(in_channels=channels_prev_prev, out_channels=channels)\n\n        if reduction:\n            self.indices, op_classes = zip(*REDUCTION_OPERATIONS)\n            self.concat = REDUCTION_CONCAT\n        else:\n            self.indices, op_classes = zip(*NORMAL_OPERATIONS)\n            self.concat = NORMAL_CONCAT\n\n        self.operations = nn.ModuleList()\n\n        for i, op_class in zip(self.indices, op_classes):\n            if reduction and i < 2:\n                stride = 2\n            else:\n                stride = 1\n\n            op = op_class(channels, stride)\n            self.operations.append(op)\n\n    def extra_repr(self) -> str:\n        return f\'indices: {self.indices}\'\n\n    def forward(self,  # type: ignore\n                input_or_states: Union[Tensor, Tuple[Tensor, Tensor]],\n                ) -> Tuple[Tensor, Tensor]:\n        if isinstance(input_or_states, tuple):\n            s1, s2 = input_or_states\n        else:\n            s1 = s2 = input_or_states\n\n        skip = s1\n\n        s1 = self.reduce1(s1)\n        s2 = self.reduce2(s2)\n\n        _states = [s1, s2]\n\n        operations = cast(nn.ModuleList, self.operations)\n        indices = cast(List[int], self.indices)\n\n        for i in range(0, len(operations), 2):\n            h1 = _states[indices[i]]\n            h2 = _states[indices[i+1]]\n\n            op1 = operations[i]\n            op2 = operations[i+1]\n\n            h1 = op1(h1)\n            h2 = op2(h2)\n\n            s = h1 + h2\n            _states.append(s)\n\n        return torch.cat([_states[i] for i in self.concat], dim=1), skip\n\n\ndef amoebanetd(num_classes: int = 10,\n               num_layers: int = 4,\n               num_filters: int = 512,\n               ) -> nn.Sequential:\n    """"""Builds an AmoebaNet-D model for ImageNet.""""""\n    layers: NamedModules = OrderedDict()\n\n    assert num_layers % 3 == 0\n    repeat_normal_cells = num_layers // 3\n\n    channels = num_filters // 4\n    channels_prev_prev = channels_prev = channels\n    reduction_prev = False\n\n    def make_cells(reduction: bool, channels_scale: int, repeat: int) -> Iterator[Cell]:\n        nonlocal channels_prev_prev\n        nonlocal channels_prev\n        nonlocal channels\n        nonlocal reduction_prev\n\n        channels *= channels_scale\n\n        for i in range(repeat):\n            cell = Cell(channels_prev_prev,\n                        channels_prev,\n                        channels,\n                        reduction,\n                        reduction_prev)\n\n            channels_prev_prev = channels_prev\n            channels_prev = channels * len(cell.concat)\n            reduction_prev = reduction\n\n            yield cell\n\n    def reduction_cell() -> Cell:\n        return next(make_cells(reduction=True, channels_scale=2, repeat=1))\n\n    def normal_cells() -> Iterator[Tuple[int, Cell]]:\n        return enumerate(make_cells(reduction=False, channels_scale=1, repeat=repeat_normal_cells))\n\n    # Stem for ImageNet\n    layers[\'stem1\'] = Stem(channels)\n    layers[\'stem2\'] = reduction_cell()\n    layers[\'stem3\'] = reduction_cell()\n\n    # AmoebaNet cells\n    layers.update((f\'cell1_normal{i+1}\', cell) for i, cell in normal_cells())\n    layers[\'cell2_reduction\'] = reduction_cell()\n    layers.update((f\'cell3_normal{i+1}\', cell) for i, cell in normal_cells())\n    layers[\'cell4_reduction\'] = reduction_cell()\n    layers.update((f\'cell5_normal{i+1}\', cell) for i, cell in normal_cells())\n\n    # Finally, classifier\n    layers[\'classify\'] = Classify(channels_prev, num_classes)\n\n    return nn.Sequential(layers)\n'"
benchmarks/models/amoebanet/genotype.py,0,"b""from typing import List\n\nfrom amoebanet.operations import (avg_pool_3x3, conv_1x1, conv_1x7_7x1, conv_3x3, max_pool_2x2,\n                                  max_pool_3x3, none)\n\n__all__: List[str] = []\n\n# The genotype for AmoebaNet-D\nNORMAL_OPERATIONS = [\n    # (0, max_pool_3x3),\n    # (0, conv_1x1),\n    # (2, none),\n    # (2, max_pool_3x3),\n    # (0, none),\n    # (1, conv_1x7_7x1),\n    # (1, conv_1x1),\n    # (1, conv_1x7_7x1),\n    # (0, avg_pool_3x3),\n    # (3, conv_1x1),\n\n    (1, conv_1x1),\n    (1, max_pool_3x3),\n    (1, none),\n    (0, conv_1x7_7x1),\n    (0, conv_1x1),\n    (0, conv_1x7_7x1),\n    (2, max_pool_3x3),\n    (2, none),\n    (1, avg_pool_3x3),\n    (5, conv_1x1),\n]\n\n# According to the paper for AmoebaNet-D, 'normal_concat' should be [4, 5, 6]\n# just like 'reduce_concat'. But 'normal_concat' in the reference AmoebaNet-D\n# implementation by TensorFlow is defined as [0, 3, 4, 6], which is different\n# with the paper.\n#\n# For now, we couldn't be sure which is correct. But the GPipe paper seems to\n# rely on the setting of TensorFlow's implementation. With this, we can\n# reproduce the size of model parameters reported at Table 1 in the paper,\n# exactly.\n#\n# Regularized Evolution for Image Classifier Architecture Search\n#   https://arxiv.org/pdf/1802.01548.pdf\n#\n# GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism\n#   https://arxiv.org/pdf/1811.06965.pdf\n#\n# The AmoebaNet-D implementation by TensorFlow\n#   https://github.com/tensorflow/tpu/blob/c753c0a/models/official/amoeba_net\n#\nNORMAL_CONCAT = [0, 3, 4, 6]\n\nREDUCTION_OPERATIONS = [\n    (0, max_pool_2x2),\n    (0, max_pool_3x3),\n    (2, none),\n    (1, conv_3x3),\n    (2, conv_1x7_7x1),\n    (2, max_pool_3x3),\n    (3, none),\n    (1, max_pool_2x2),\n    (2, avg_pool_3x3),\n    (3, conv_1x1),\n]\nREDUCTION_CONCAT = [4, 5, 6]\n"""
benchmarks/models/amoebanet/operations.py,1,"b'from typing import Any, List\n\nimport torch\nfrom torch import Tensor, nn\n\n__all__: List[str] = []\n\n\nclass Operation(nn.Module):\n    """"""Includes the operation name into the representation string for\n    debugging.\n    """"""\n\n    def __init__(self, name: str, module: nn.Module):\n        super().__init__()\n        self.name = name\n        self.module = module\n\n    def __repr__(self) -> str:\n        return f\'{self.__class__.__name__}[{self.name}]\'\n\n    def forward(self, *args: Any) -> Any:  # type: ignore\n        return self.module(*args)\n\n\nclass FactorizedReduce(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n        self.relu = nn.ReLU(inplace=False)\n        self.pad = nn.ZeroPad2d((0, 1, 0, 1))\n        self.conv1 = nn.Conv2d(in_channels, out_channels//2, kernel_size=1, stride=2, bias=False)\n        self.conv2 = nn.Conv2d(in_channels, out_channels//2, kernel_size=1, stride=2, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n\n    def forward(self, input: Tensor) -> Tensor:  # type: ignore\n        x = input\n        x = self.relu(x)\n        x = torch.cat([self.conv1(x), self.conv2(self.pad(x[:, :, 1:, 1:]))], dim=1)\n        x = self.bn(x)\n        return x\n\n\ndef none(channels: int, stride: int) -> Operation:\n    module: nn.Module\n    if stride == 1:\n        module = nn.Identity()\n    else:\n        module = FactorizedReduce(channels, channels)\n    return Operation(\'none\', module)\n\n\ndef avg_pool_3x3(channels: int, stride: int) -> Operation:\n    module = nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False)\n    return Operation(\'avg_pool_3x3\', module)\n\n\ndef max_pool_3x3(channels: int, stride: int) -> Operation:\n    module = nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False)\n    return Operation(\'max_pool_3x3\', module)\n\n\ndef max_pool_2x2(channels: int, stride: int) -> Operation:\n    module = nn.MaxPool2d(2, stride=stride, padding=0)\n    return Operation(\'max_pool_2x2\', module)\n\n\ndef conv_1x7_7x1(channels: int, stride: int) -> Operation:\n    c = channels\n    module = nn.Sequential(\n        nn.ReLU(inplace=False),\n        nn.Conv2d(c, c//4, kernel_size=1, stride=1, padding=0, bias=False),\n        nn.BatchNorm2d(c//4),\n\n        nn.ReLU(inplace=False),\n        nn.Conv2d(c//4, c//4, kernel_size=(1, 7), stride=(1, stride), padding=(0, 3), bias=False),\n        nn.BatchNorm2d(c//4),\n\n        nn.ReLU(inplace=False),\n        nn.Conv2d(c//4, c//4, kernel_size=(7, 1), stride=(stride, 1), padding=(3, 0), bias=False),\n        nn.BatchNorm2d(c//4),\n\n        nn.ReLU(inplace=False),\n        nn.Conv2d(c//4, c, kernel_size=1, stride=1, padding=0, bias=False),\n        nn.BatchNorm2d(c),\n    )\n    return Operation(\'conv_1x7_7x1\', module)\n\n\ndef conv_1x1(channels: int, stride: int) -> Operation:\n    c = channels\n    module = nn.Sequential(\n        nn.ReLU(inplace=False),\n        nn.Conv2d(c, c, kernel_size=1, stride=stride, bias=False),\n        nn.BatchNorm2d(c),\n    )\n    return Operation(\'conv_1x1\', module)\n\n\ndef conv_3x3(channels: int, stride: int) -> Operation:\n    c = channels\n    module = nn.Sequential(\n        nn.ReLU(inplace=False),\n        nn.Conv2d(c, c//4, kernel_size=1, bias=False),\n        nn.BatchNorm2d(c//4),\n\n        nn.ReLU(inplace=False),\n        nn.Conv2d(c//4, c//4, kernel_size=3, stride=stride, padding=1, bias=False),\n        nn.BatchNorm2d(c//4),\n\n        nn.ReLU(inplace=False),\n        nn.Conv2d(c//4, c, kernel_size=1, bias=False),\n        nn.BatchNorm2d(c),\n    )\n    return Operation(\'conv_3x3\', module)\n'"
benchmarks/models/resnet/__init__.py,0,"b'""""""A ResNet implementation but using :class:`nn.Sequential`. :func:`resnet101`\nreturns a :class:`nn.Sequential` instead of ``ResNet``.\n\nThis code is transformed :mod:`torchvision.models.resnet`.\n\n""""""\nfrom collections import OrderedDict\nfrom typing import Any, List\n\nfrom torch import nn\n\nfrom resnet.bottleneck import bottleneck\nfrom resnet.flatten_sequential import flatten_sequential\n\n__all__ = [\'resnet101\']\n\n\ndef build_resnet(layers: List[int],\n                 num_classes: int = 1000,\n                 inplace: bool = False\n                 ) -> nn.Sequential:\n    """"""Builds a ResNet as a simple sequential model.\n\n    Note:\n        The implementation is copied from :mod:`torchvision.models.resnet`.\n\n    """"""\n    inplanes = 64\n\n    def make_layer(planes: int,\n                   blocks: int,\n                   stride: int = 1,\n                   inplace: bool = False,\n                   ) -> nn.Sequential:\n        nonlocal inplanes\n\n        downsample = None\n        if stride != 1 or inplanes != planes * 4:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * 4,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * 4),\n            )\n\n        layers = []\n        layers.append(bottleneck(inplanes, planes, stride, downsample, inplace))\n        inplanes = planes * 4\n        for _ in range(1, blocks):\n            layers.append(bottleneck(inplanes, planes, inplace=inplace))\n\n        return nn.Sequential(*layers)\n\n    # Build ResNet as a sequential model.\n    model = nn.Sequential(OrderedDict([\n        (\'conv1\', nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)),\n        (\'bn1\', nn.BatchNorm2d(64)),\n        (\'relu\', nn.ReLU()),\n        (\'maxpool\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n\n        (\'layer1\', make_layer(64, layers[0], inplace=inplace)),\n        (\'layer2\', make_layer(128, layers[1], stride=2, inplace=inplace)),\n        (\'layer3\', make_layer(256, layers[2], stride=2, inplace=inplace)),\n        (\'layer4\', make_layer(512, layers[3], stride=2, inplace=inplace)),\n\n        (\'avgpool\', nn.AdaptiveAvgPool2d((1, 1))),\n        (\'flat\', nn.Flatten()),\n        (\'fc\', nn.Linear(512 * 4, num_classes)),\n    ]))\n\n    # Flatten nested sequentials.\n    model = flatten_sequential(model)\n\n    # Initialize weights for Conv2d and BatchNorm2d layers.\n    # Stolen from torchvision-0.4.0.\n    def init_weight(m: nn.Module) -> None:\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            return\n\n        if isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n            return\n\n    model.apply(init_weight)\n\n    return model\n\n\ndef resnet101(**kwargs: Any) -> nn.Sequential:\n    """"""Constructs a ResNet-101 model.""""""\n    return build_resnet([3, 4, 23, 3], **kwargs)\n'"
benchmarks/models/resnet/bottleneck.py,0,"b'""""""A ResNet bottleneck implementation but using :class:`nn.Sequential`.""""""\nfrom collections import OrderedDict\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\n\nfrom torch import Tensor, nn\n\nfrom torchgpipe.skip import Namespace, pop, skippable, stash\n\n__all__ = [\'bottleneck\']\n\nTensors = Tuple[Tensor, ...]\nTensorOrTensors = Union[Tensor, Tensors]\n\nif TYPE_CHECKING:\n    NamedModules = OrderedDict[str, nn.Module]\nelse:\n    NamedModules = OrderedDict\n\n\ndef conv3x3(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\n@skippable(stash=[\'identity\'])\nclass Identity(nn.Module):\n    def forward(self, tensor: Tensor) -> Tensor:  # type: ignore\n        yield stash(\'identity\', tensor)\n        return tensor\n\n\n@skippable(pop=[\'identity\'])\nclass Residual(nn.Module):\n    """"""A residual block for ResNet.""""""\n\n    def __init__(self, downsample: Optional[nn.Module] = None):\n        super().__init__()\n        self.downsample = downsample\n\n    def forward(self, input: Tensor) -> Tensor:  # type: ignore\n        identity = yield pop(\'identity\')\n        if self.downsample is not None:\n            identity = self.downsample(identity)\n        return input + identity\n\n\ndef bottleneck(inplanes: int,\n               planes: int,\n               stride: int = 1,\n               downsample: Optional[nn.Module] = None,\n               inplace: bool = False,\n               ) -> nn.Sequential:\n    """"""Creates a bottleneck block in ResNet as a :class:`nn.Sequential`.""""""\n\n    layers: NamedModules = OrderedDict()\n\n    ns = Namespace()\n    layers[\'identity\'] = Identity().isolate(ns)  # type: ignore\n\n    layers[\'conv1\'] = conv1x1(inplanes, planes)\n    layers[\'bn1\'] = nn.BatchNorm2d(planes)\n    layers[\'relu1\'] = nn.ReLU(inplace=inplace)\n\n    layers[\'conv2\'] = conv3x3(planes, planes, stride)\n    layers[\'bn2\'] = nn.BatchNorm2d(planes)\n    layers[\'relu2\'] = nn.ReLU(inplace=inplace)\n\n    layers[\'conv3\'] = conv1x1(planes, planes * 4)\n    layers[\'bn3\'] = nn.BatchNorm2d(planes * 4)\n    layers[\'residual\'] = Residual(downsample).isolate(ns)  # type: ignore\n    layers[\'relu3\'] = nn.ReLU(inplace=inplace)\n\n    return nn.Sequential(layers)\n'"
benchmarks/models/resnet/flatten_sequential.py,0,"b'from collections import OrderedDict\nfrom typing import Iterator, Tuple\n\nfrom torch import nn\n\n\ndef flatten_sequential(module: nn.Sequential) -> nn.Sequential:\n    """"""flatten_sequentials a nested sequential module.""""""\n    if not isinstance(module, nn.Sequential):\n        raise TypeError(\'not sequential\')\n\n    return nn.Sequential(OrderedDict(_flatten_sequential(module)))\n\n\ndef _flatten_sequential(module: nn.Sequential) -> Iterator[Tuple[str, nn.Module]]:\n    for name, child in module.named_children():\n        # flatten_sequential child sequential layers only.\n        if isinstance(child, nn.Sequential):\n            for sub_name, sub_child in _flatten_sequential(child):\n                yield (f\'{name}_{sub_name}\', sub_child)\n        else:\n            yield (name, child)\n'"
benchmarks/models/unet/__init__.py,2,"b'""""""Simplified U-Net""""""\nfrom collections import OrderedDict\nfrom typing import TYPE_CHECKING, Dict, Generator, List\n\nimport torch\nfrom torch import Tensor, nn\nimport torch.nn.functional as F\n\nfrom torchgpipe.skip import Namespace, pop, skippable, stash\nfrom unet.flatten_sequential import flatten_sequential\n\nif TYPE_CHECKING:\n    NamedModules = OrderedDict[str, nn.Module]\nelse:\n    NamedModules = OrderedDict\n\n\n@skippable(stash=[\'skip\'], pop=[])\nclass Stash(nn.Module):\n    def forward(self, input: Tensor) -> Generator[stash, None, Tensor]:  # type: ignore\n        yield stash(\'skip\', input)\n        return input\n\n\n@skippable(stash=[], pop=[\'skip\'])\nclass PopCat(nn.Module):\n    def forward(self, input: Tensor) -> Generator[pop, Tensor, Tensor]:  # type: ignore\n        skipped_input = yield pop(\'skip\')\n\n        skip_shape = skipped_input.shape[2:]\n        input_shape = input.shape[2:]\n\n        if input_shape != skip_shape:\n            pad = [d2 - d1 for d1, d2 in zip(input_shape, skip_shape)]\n            pad = sum([[0, p] for p in pad[::-1]], [])\n            input = F.pad(input, pad=pad)\n\n        output = torch.cat((input, skipped_input), dim=1)\n        return output\n\n\ndef conv_dropout_norm_relu(in_channels: int, out_channels: int) -> nn.Sequential:\n    layers: NamedModules = OrderedDict()\n    layers[\'conv\'] = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n    layers[\'dropout\'] = nn.Dropout2d(p=0.1)\n    layers[\'norm\'] = nn.InstanceNorm2d(out_channels)\n    layers[\'relu\'] = nn.LeakyReLU(negative_slope=1e-2)\n    return nn.Sequential(layers)\n\n\ndef stacked_convs(in_channels: int,\n                  hidden_channels: int,\n                  out_channels: int,\n                  num_convs: int,\n                  ) -> nn.Sequential:\n    layers: List[nn.Module] = []\n\n    if num_convs == 1:\n        layers.append(conv_dropout_norm_relu(in_channels, out_channels))\n\n    elif num_convs > 1:\n        layers.append(conv_dropout_norm_relu(in_channels, hidden_channels))\n\n        for _ in range(num_convs - 2):\n            layers.append(conv_dropout_norm_relu(hidden_channels, hidden_channels))\n\n        layers.append(conv_dropout_norm_relu(hidden_channels, out_channels))\n\n    return nn.Sequential(*layers)\n\n\ndef unet(depth: int = 5,\n         num_convs: int = 5,\n         base_channels: int = 64,\n         input_channels: int = 3,\n         output_channels: int = 1,\n         ) -> nn.Sequential:\n    """"""Builds a simplified U-Net model.""""""\n    # The U-Net structure\n    encoder_channels = [{\n        \'in\': input_channels if i == 0 else base_channels * (2 ** (i - 1)),\n        \'mid\': base_channels * (2 ** i),\n        \'out\': base_channels * (2 ** i),\n    } for i in range(depth)]\n\n    bottleneck_channels = [{\n        \'in\': base_channels * (2 ** (depth - 1)),\n        \'mid\': base_channels * (2 ** depth),\n        \'out\': base_channels * (2 ** (depth - 1)),\n    }]\n\n    inverted_decoder_channels = [{\n        \'in\': base_channels * (2 ** (i + 1)),\n        \'mid\': int(base_channels * (2 ** (i - 1))),\n        \'out\': int(base_channels * (2 ** (i - 1))),\n    } for i in range(depth)]\n\n    # Build cells.\n    def cell(ch: Dict[str, int]) -> nn.Sequential:\n        return stacked_convs(ch[\'in\'], ch[\'mid\'], ch[\'out\'], num_convs)\n\n    encoder_cells = [cell(c) for c in encoder_channels]\n    bottleneck_cells = [cell(c) for c in bottleneck_channels]\n    decoder_cells = [cell(c) for c in inverted_decoder_channels]\n\n    # Link long skip connections.\n    #\n    # [ encoder ]--------------[ decoder ]--[ segment ]\n    #    [ encoder ]--------[ decoder ]\n    #       [ encoder ]--[ decoder ]\n    #            [ bottleneck ]\n    #\n    namespaces = [Namespace() for _ in range(depth)]\n\n    encoder_layers: List[nn.Module] = []\n    for i in range(depth):\n        ns = namespaces[i]\n        encoder_layers.append(nn.Sequential(OrderedDict([\n            (\'encode\', encoder_cells[i]),\n            (\'skip\', Stash().isolate(ns)),  # type: ignore\n            (\'down\', nn.MaxPool2d(2, stride=2))\n        ])))\n    encoder = nn.Sequential(*encoder_layers)\n\n    bottleneck = nn.Sequential(*bottleneck_cells)\n\n    decoder_layers: List[nn.Module] = []\n    for i in reversed(range(depth)):\n        ns = namespaces[i]\n        decoder_layers.append(nn.Sequential(OrderedDict([\n            (\'up\', nn.Upsample(scale_factor=2)),\n            (\'skip\', PopCat().isolate(ns)),  # type: ignore\n            (\'decode\', decoder_cells[i])\n        ])))\n    decoder = nn.Sequential(*decoder_layers)\n\n    final_channels = inverted_decoder_channels[0][\'out\']\n    segment = nn.Conv2d(final_channels, output_channels, kernel_size=1, bias=False)\n\n    # Construct a U-Net model as nn.Sequential.\n    model = nn.Sequential(OrderedDict([\n        (\'encoder\', encoder),\n        (\'bottleneck\', bottleneck),\n        (\'decoder\', decoder),\n        (\'segment\', segment)\n    ]))\n    model = flatten_sequential(model)\n    return model\n'"
benchmarks/models/unet/flatten_sequential.py,0,"b'from collections import OrderedDict\nfrom typing import Iterator, Tuple\n\nfrom torch import nn\n\n\ndef flatten_sequential(module: nn.Sequential) -> nn.Sequential:\n    """"""flatten_sequentials a nested sequential module.""""""\n    if not isinstance(module, nn.Sequential):\n        raise TypeError(\'not sequential\')\n\n    return nn.Sequential(OrderedDict(_flatten_sequential(module)))\n\n\ndef _flatten_sequential(module: nn.Sequential) -> Iterator[Tuple[str, nn.Module]]:\n    for name, child in module.named_children():\n        # flatten_sequential child sequential layers only.\n        if isinstance(child, nn.Sequential):\n            for sub_name, sub_child in _flatten_sequential(child):\n                yield (f\'{name}_{sub_name}\', sub_child)\n        else:\n            yield (name, child)\n'"
