file_path,api_count,code
api.py,0,"b'from flask import Flask,request,jsonify\nfrom flask_cors import CORS\n\nfrom bert import Ner\n\napp = Flask(__name__)\nCORS(app)\n\nmodel = Ner(""out_!x"")\n\n@app.route(""/predict"",methods=[\'POST\'])\ndef predict():\n    text = request.json[""text""]\n    try:\n        out = model.predict(text)\n        return jsonify({""result"":out})\n    except Exception as e:\n        print(e)\n        return jsonify({""result"":""Model Failed""})\n\nif __name__ == ""__main__"":\n    app.run(\'0.0.0.0\',port=8000)'"
bert.py,9,"b'""""""BERT NER Inference.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport os\n\nimport torch\nimport torch.nn.functional as F\nfrom nltk import word_tokenize\nfrom pytorch_transformers import (BertConfig, BertForTokenClassification,\n                                  BertTokenizer)\n\n\nclass BertNer(BertForTokenClassification):\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, valid_ids=None):\n        sequence_output = self.bert(input_ids, token_type_ids, attention_mask, head_mask=None)[0]\n        batch_size,max_len,feat_dim = sequence_output.shape\n        valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device=\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        for i in range(batch_size):\n            jj = -1\n            for j in range(max_len):\n                    if valid_ids[i][j].item() == 1:\n                        jj += 1\n                        valid_output[i][jj] = sequence_output[i][j]\n        sequence_output = self.dropout(valid_output)\n        logits = self.classifier(sequence_output)\n        return logits\n\nclass Ner:\n\n    def __init__(self,model_dir: str):\n        self.model , self.tokenizer, self.model_config = self.load_model(model_dir)\n        self.label_map = self.model_config[""label_map""]\n        self.max_seq_length = self.model_config[""max_seq_length""]\n        self.label_map = {int(k):v for k,v in self.label_map.items()}\n        self.device = ""cuda"" if torch.cuda.is_available() else ""cpu""\n        self.model = self.model.to(self.device)\n        self.model.eval()\n\n    def load_model(self, model_dir: str, model_config: str = ""model_config.json""):\n        model_config = os.path.join(model_dir,model_config)\n        model_config = json.load(open(model_config))\n        model = BertNer.from_pretrained(model_dir)\n        tokenizer = BertTokenizer.from_pretrained(model_dir, do_lower_case=model_config[""do_lower""])\n        return model, tokenizer, model_config\n\n    def tokenize(self, text: str):\n        """""" tokenize input""""""\n        words = word_tokenize(text)\n        tokens = []\n        valid_positions = []\n        for i,word in enumerate(words):\n            token = self.tokenizer.tokenize(word)\n            tokens.extend(token)\n            for i in range(len(token)):\n                if i == 0:\n                    valid_positions.append(1)\n                else:\n                    valid_positions.append(0)\n        return tokens, valid_positions\n\n    def preprocess(self, text: str):\n        """""" preprocess """"""\n        tokens, valid_positions = self.tokenize(text)\n        ## insert ""[CLS]""\n        tokens.insert(0,""[CLS]"")\n        valid_positions.insert(0,1)\n        ## insert ""[SEP]""\n        tokens.append(""[SEP]"")\n        valid_positions.append(1)\n        segment_ids = []\n        for i in range(len(tokens)):\n            segment_ids.append(0)\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        input_mask = [1] * len(input_ids)\n        while len(input_ids) < self.max_seq_length:\n            input_ids.append(0)\n            input_mask.append(0)\n            segment_ids.append(0)\n            valid_positions.append(0)\n        return input_ids,input_mask,segment_ids,valid_positions\n\n    def predict(self, text: str):\n        input_ids,input_mask,segment_ids,valid_ids = self.preprocess(text)\n        input_ids = torch.tensor([input_ids],dtype=torch.long,device=self.device)\n        input_mask = torch.tensor([input_mask],dtype=torch.long,device=self.device)\n        segment_ids = torch.tensor([segment_ids],dtype=torch.long,device=self.device)\n        valid_ids = torch.tensor([valid_ids],dtype=torch.long,device=self.device)\n        with torch.no_grad():\n            logits = self.model(input_ids, segment_ids, input_mask,valid_ids)\n        logits = F.softmax(logits,dim=2)\n        logits_label = torch.argmax(logits,dim=2)\n        logits_label = logits_label.detach().cpu().numpy().tolist()[0]\n\n        logits_confidence = [values[label].item() for values,label in zip(logits[0],logits_label)]\n\n        logits = []\n        pos = 0\n        for index,mask in enumerate(valid_ids[0]):\n            if index == 0:\n                continue\n            if mask == 1:\n                logits.append((logits_label[index-pos],logits_confidence[index-pos]))\n            else:\n                pos += 1\n        logits.pop()\n\n        labels = [(self.label_map[label],confidence) for label,confidence in logits]\n        words = word_tokenize(text)\n        assert len(labels) == len(words)\n        output = [{""word"":word,""tag"":label,""confidence"":confidence} for word,(label,confidence) in zip(words,labels)]\n        return output\n\n'"
run_ner.py,32,"b'from __future__ import absolute_import, division, print_function\n\nimport argparse\nimport csv\nimport json\nimport logging\nimport os\nimport random\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom pytorch_transformers import (WEIGHTS_NAME, AdamW, BertConfig,\n                                  BertForTokenClassification, BertTokenizer,\n                                  WarmupLinearSchedule)\nfrom torch import nn\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm, trange\n\nfrom seqeval.metrics import classification_report\n\nlogging.basicConfig(format = \'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\',\n                    datefmt = \'%m/%d/%Y %H:%M:%S\',\n                    level = logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass Ner(BertForTokenClassification):\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,valid_ids=None,attention_mask_label=None):\n        sequence_output = self.bert(input_ids, token_type_ids, attention_mask,head_mask=None)[0]\n        batch_size,max_len,feat_dim = sequence_output.shape\n        valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device=\'cuda\')\n        for i in range(batch_size):\n            jj = -1\n            for j in range(max_len):\n                    if valid_ids[i][j].item() == 1:\n                        jj += 1\n                        valid_output[i][jj] = sequence_output[i][j]\n        sequence_output = self.dropout(valid_output)\n        logits = self.classifier(sequence_output)\n\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss(ignore_index=0)\n            # Only keep active parts of the loss\n            #attention_mask_label = None\n            if attention_mask_label is not None:\n                active_loss = attention_mask_label.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass InputExample(object):\n    """"""A single training/test example for simple sequence classification.""""""\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        """"""Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        """"""\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n        self.valid_ids = valid_ids\n        self.label_mask = label_mask\n\ndef readfile(filename):\n    \'\'\'\n    read file\n    \'\'\'\n    f = open(filename)\n    data = []\n    sentence = []\n    label= []\n    for line in f:\n        if len(line)==0 or line.startswith(\'-DOCSTART\') or line[0]==""\\n"":\n            if len(sentence) > 0:\n                data.append((sentence,label))\n                sentence = []\n                label = []\n            continue\n        splits = line.split(\' \')\n        sentence.append(splits[0])\n        label.append(splits[-1][:-1])\n\n    if len(sentence) >0:\n        data.append((sentence,label))\n        sentence = []\n        label = []\n    return data\n\nclass DataProcessor(object):\n    """"""Base class for data converters for sequence classification data sets.""""""\n\n    def get_train_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the train set.""""""\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\n        raise NotImplementedError()\n\n    def get_labels(self):\n        """"""Gets the list of labels for this data set.""""""\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        """"""Reads a tab separated value file.""""""\n        return readfile(input_file)\n\n\nclass NerProcessor(DataProcessor):\n    """"""Processor for the CoNLL-2003 data set.""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.txt"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""valid.txt"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.txt"")), ""test"")\n\n    def get_labels(self):\n        return [""O"", ""B-MISC"", ""I-MISC"",  ""B-PER"", ""I-PER"", ""B-ORG"", ""I-ORG"", ""B-LOC"", ""I-LOC"", ""[CLS]"", ""[SEP]""]\n\n    def _create_examples(self,lines,set_type):\n        examples = []\n        for i,(sentence,label) in enumerate(lines):\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = \' \'.join(sentence)\n            text_b = None\n            label = label\n            examples.append(InputExample(guid=guid,text_a=text_a,text_b=text_b,label=label))\n        return examples\n\ndef convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n    """"""Loads a data file into a list of `InputBatch`s.""""""\n\n    label_map = {label : i for i, label in enumerate(label_list,1)}\n\n    features = []\n    for (ex_index,example) in enumerate(examples):\n        textlist = example.text_a.split(\' \')\n        labellist = example.label\n        tokens = []\n        labels = []\n        valid = []\n        label_mask = []\n        for i, word in enumerate(textlist):\n            token = tokenizer.tokenize(word)\n            tokens.extend(token)\n            label_1 = labellist[i]\n            for m in range(len(token)):\n                if m == 0:\n                    labels.append(label_1)\n                    valid.append(1)\n                    label_mask.append(1)\n                else:\n                    valid.append(0)\n        if len(tokens) >= max_seq_length - 1:\n            tokens = tokens[0:(max_seq_length - 2)]\n            labels = labels[0:(max_seq_length - 2)]\n            valid = valid[0:(max_seq_length - 2)]\n            label_mask = label_mask[0:(max_seq_length - 2)]\n        ntokens = []\n        segment_ids = []\n        label_ids = []\n        ntokens.append(""[CLS]"")\n        segment_ids.append(0)\n        valid.insert(0,1)\n        label_mask.insert(0,1)\n        label_ids.append(label_map[""[CLS]""])\n        for i, token in enumerate(tokens):\n            ntokens.append(token)\n            segment_ids.append(0)\n            if len(labels) > i:\n                label_ids.append(label_map[labels[i]])\n        ntokens.append(""[SEP]"")\n        segment_ids.append(0)\n        valid.append(1)\n        label_mask.append(1)\n        label_ids.append(label_map[""[SEP]""])\n        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n        input_mask = [1] * len(input_ids)\n        label_mask = [1] * len(label_ids)\n        while len(input_ids) < max_seq_length:\n            input_ids.append(0)\n            input_mask.append(0)\n            segment_ids.append(0)\n            label_ids.append(0)\n            valid.append(1)\n            label_mask.append(0)\n        while len(label_ids) < max_seq_length:\n            label_ids.append(0)\n            label_mask.append(0)\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n        assert len(label_ids) == max_seq_length\n        assert len(valid) == max_seq_length\n        assert len(label_mask) == max_seq_length\n\n        if ex_index < 5:\n            logger.info(""*** Example ***"")\n            logger.info(""guid: %s"" % (example.guid))\n            logger.info(""tokens: %s"" % "" "".join(\n                    [str(x) for x in tokens]))\n            logger.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n            logger.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n            logger.info(\n                    ""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n            # logger.info(""label: %s (id = %d)"" % (example.label, label_ids))\n\n        features.append(\n                InputFeatures(input_ids=input_ids,\n                              input_mask=input_mask,\n                              segment_ids=segment_ids,\n                              label_id=label_ids,\n                              valid_ids=valid,\n                              label_mask=label_mask))\n    return features\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(""--data_dir"",\n                        default=None,\n                        type=str,\n                        required=True,\n                        help=""The input data dir. Should contain the .tsv files (or other data files) for the task."")\n    parser.add_argument(""--bert_model"", default=None, type=str, required=True,\n                        help=""Bert pre-trained model selected in the list: bert-base-uncased, ""\n                        ""bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, ""\n                        ""bert-base-multilingual-cased, bert-base-chinese."")\n    parser.add_argument(""--task_name"",\n                        default=None,\n                        type=str,\n                        required=True,\n                        help=""The name of the task to train."")\n    parser.add_argument(""--output_dir"",\n                        default=None,\n                        type=str,\n                        required=True,\n                        help=""The output directory where the model predictions and checkpoints will be written."")\n\n    ## Other parameters\n    parser.add_argument(""--cache_dir"",\n                        default="""",\n                        type=str,\n                        help=""Where do you want to store the pre-trained models downloaded from s3"")\n    parser.add_argument(""--max_seq_length"",\n                        default=128,\n                        type=int,\n                        help=""The maximum total input sequence length after WordPiece tokenization. \\n""\n                             ""Sequences longer than this will be truncated, and sequences shorter \\n""\n                             ""than this will be padded."")\n    parser.add_argument(""--do_train"",\n                        action=\'store_true\',\n                        help=""Whether to run training."")\n    parser.add_argument(""--do_eval"",\n                        action=\'store_true\',\n                        help=""Whether to run eval or not."")\n    parser.add_argument(""--eval_on"",\n                        default=""dev"",\n                        help=""Whether to run eval on the dev set or test set."")\n    parser.add_argument(""--do_lower_case"",\n                        action=\'store_true\',\n                        help=""Set this flag if you are using an uncased model."")\n    parser.add_argument(""--train_batch_size"",\n                        default=32,\n                        type=int,\n                        help=""Total batch size for training."")\n    parser.add_argument(""--eval_batch_size"",\n                        default=8,\n                        type=int,\n                        help=""Total batch size for eval."")\n    parser.add_argument(""--learning_rate"",\n                        default=5e-5,\n                        type=float,\n                        help=""The initial learning rate for Adam."")\n    parser.add_argument(""--num_train_epochs"",\n                        default=3.0,\n                        type=float,\n                        help=""Total number of training epochs to perform."")\n    parser.add_argument(""--warmup_proportion"",\n                        default=0.1,\n                        type=float,\n                        help=""Proportion of training to perform linear learning rate warmup for. ""\n                             ""E.g., 0.1 = 10%% of training."")\n    parser.add_argument(""--weight_decay"", default=0.01, type=float,\n                        help=""Weight deay if we apply some."")\n    parser.add_argument(""--adam_epsilon"", default=1e-8, type=float,\n                        help=""Epsilon for Adam optimizer."")\n    parser.add_argument(""--max_grad_norm"", default=1.0, type=float,\n                        help=""Max gradient norm."")\n    parser.add_argument(""--no_cuda"",\n                        action=\'store_true\',\n                        help=""Whether not to use CUDA when available"")\n    parser.add_argument(""--local_rank"",\n                        type=int,\n                        default=-1,\n                        help=""local_rank for distributed training on gpus"")\n    parser.add_argument(\'--seed\',\n                        type=int,\n                        default=42,\n                        help=""random seed for initialization"")\n    parser.add_argument(\'--gradient_accumulation_steps\',\n                        type=int,\n                        default=1,\n                        help=""Number of updates steps to accumulate before performing a backward/update pass."")\n    parser.add_argument(\'--fp16\',\n                        action=\'store_true\',\n                        help=""Whether to use 16-bit float precision instead of 32-bit"")\n    parser.add_argument(\'--fp16_opt_level\', type=str, default=\'O1\',\n                        help=""For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].""\n                             ""See details at https://nvidia.github.io/apex/amp.html"")\n    parser.add_argument(\'--loss_scale\',\n                        type=float, default=0,\n                        help=""Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n""\n                             ""0 (default value): dynamic loss scaling.\\n""\n                             ""Positive power of 2: static loss scaling value.\\n"")\n    parser.add_argument(\'--server_ip\', type=str, default=\'\', help=""Can be used for distant debugging."")\n    parser.add_argument(\'--server_port\', type=str, default=\'\', help=""Can be used for distant debugging."")\n    args = parser.parse_args()\n\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n        print(""Waiting for debugger attach"")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    processors = {""ner"":NerProcessor}\n\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        n_gpu = torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(""cuda"", args.local_rank)\n        n_gpu = 1\n        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.distributed.init_process_group(backend=\'nccl\')\n    logger.info(""device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}"".format(\n        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                            args.gradient_accumulation_steps))\n\n    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    if not args.do_train and not args.do_eval:\n        raise ValueError(""At least one of `do_train` or `do_eval` must be True."")\n\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n        raise ValueError(""Output directory ({}) already exists and is not empty."".format(args.output_dir))\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n\n    task_name = args.task_name.lower()\n\n    if task_name not in processors:\n        raise ValueError(""Task not found: %s"" % (task_name))\n\n    processor = processors[task_name]()\n    label_list = processor.get_labels()\n    num_labels = len(label_list) + 1\n\n    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n\n    train_examples = None\n    num_train_optimization_steps = 0\n    if args.do_train:\n        train_examples = processor.get_train_examples(args.data_dir)\n        num_train_optimization_steps = int(\n            len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n        if args.local_rank != -1:\n            num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    # Prepare model\n    config = BertConfig.from_pretrained(args.bert_model, num_labels=num_labels, finetuning_task=args.task_name)\n    model = Ner.from_pretrained(args.bert_model,\n              from_tf = False,\n              config = config)\n\n    if args.local_rank == 0:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    model.to(device)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\'bias\',\'LayerNorm.weight\']\n    optimizer_grouped_parameters = [\n        {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay\': args.weight_decay},\n        {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0}\n        ]\n    warmup_steps = int(args.warmup_proportion * num_train_optimization_steps)\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=num_train_optimization_steps)\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n                                                          output_device=args.local_rank,\n                                                          find_unused_parameters=True)\n\n    global_step = 0\n    nb_tr_steps = 0\n    tr_loss = 0\n    label_map = {i : label for i, label in enumerate(label_list,1)}\n    if args.do_train:\n        train_features = convert_examples_to_features(\n            train_examples, label_list, args.max_seq_length, tokenizer)\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num examples = %d"", len(train_examples))\n        logger.info(""  Batch size = %d"", args.train_batch_size)\n        logger.info(""  Num steps = %d"", num_train_optimization_steps)\n        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n        all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)\n        all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)\n        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,all_valid_ids,all_lmask_ids)\n        if args.local_rank == -1:\n            train_sampler = RandomSampler(train_data)\n        else:\n            train_sampler = DistributedSampler(train_data)\n        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n\n        model.train()\n        for _ in trange(int(args.num_train_epochs), desc=""Epoch""):\n            tr_loss = 0\n            nb_tr_examples, nb_tr_steps = 0, 0\n            for step, batch in enumerate(tqdm(train_dataloader, desc=""Iteration"")):\n                batch = tuple(t.to(device) for t in batch)\n                input_ids, input_mask, segment_ids, label_ids, valid_ids,l_mask = batch\n                loss = model(input_ids, segment_ids, input_mask, label_ids,valid_ids,l_mask)\n                if n_gpu > 1:\n                    loss = loss.mean() # mean() to average on multi-gpu.\n                if args.gradient_accumulation_steps > 1:\n                    loss = loss / args.gradient_accumulation_steps\n\n                if args.fp16:\n                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n                        scaled_loss.backward()\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n                tr_loss += loss.item()\n                nb_tr_examples += input_ids.size(0)\n                nb_tr_steps += 1\n                if (step + 1) % args.gradient_accumulation_steps == 0:\n                    optimizer.step()\n                    scheduler.step()  # Update learning rate schedule\n                    model.zero_grad()\n                    global_step += 1\n\n        # Save a trained model and the associated configuration\n        model_to_save = model.module if hasattr(model, \'module\') else model  # Only save the model it-self\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        label_map = {i : label for i, label in enumerate(label_list,1)}\n        model_config = {""bert_model"":args.bert_model,""do_lower"":args.do_lower_case,""max_seq_length"":args.max_seq_length,""num_labels"":len(label_list)+1,""label_map"":label_map}\n        json.dump(model_config,open(os.path.join(args.output_dir,""model_config.json""),""w""))\n        # Load a trained model and config that you have fine-tuned\n    else:\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = Ner.from_pretrained(args.output_dir)\n        tokenizer = BertTokenizer.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n\n    model.to(device)\n\n    if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        if args.eval_on == ""dev"":\n            eval_examples = processor.get_dev_examples(args.data_dir)\n        elif args.eval_on == ""test"":\n            eval_examples = processor.get_test_examples(args.data_dir)\n        else:\n            raise ValueError(""eval on dev or test set only"")\n        eval_features = convert_examples_to_features(eval_examples, label_list, args.max_seq_length, tokenizer)\n        logger.info(""***** Running evaluation *****"")\n        logger.info(""  Num examples = %d"", len(eval_examples))\n        logger.info(""  Batch size = %d"", args.eval_batch_size)\n        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n        all_valid_ids = torch.tensor([f.valid_ids for f in eval_features], dtype=torch.long)\n        all_lmask_ids = torch.tensor([f.label_mask for f in eval_features], dtype=torch.long)\n        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,all_valid_ids,all_lmask_ids)\n        # Run prediction for full data\n        eval_sampler = SequentialSampler(eval_data)\n        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n        model.eval()\n        eval_loss, eval_accuracy = 0, 0\n        nb_eval_steps, nb_eval_examples = 0, 0\n        y_true = []\n        y_pred = []\n        label_map = {i : label for i, label in enumerate(label_list,1)}\n        for input_ids, input_mask, segment_ids, label_ids,valid_ids,l_mask in tqdm(eval_dataloader, desc=""Evaluating""):\n            input_ids = input_ids.to(device)\n            input_mask = input_mask.to(device)\n            segment_ids = segment_ids.to(device)\n            valid_ids = valid_ids.to(device)\n            label_ids = label_ids.to(device)\n            l_mask = l_mask.to(device)\n\n            with torch.no_grad():\n                logits = model(input_ids, segment_ids, input_mask,valid_ids=valid_ids,attention_mask_label=l_mask)\n\n            logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n            logits = logits.detach().cpu().numpy()\n            label_ids = label_ids.to(\'cpu\').numpy()\n            input_mask = input_mask.to(\'cpu\').numpy()\n\n            for i, label in enumerate(label_ids):\n                temp_1 = []\n                temp_2 = []\n                for j,m in enumerate(label):\n                    if j == 0:\n                        continue\n                    elif label_ids[i][j] == len(label_map):\n                        y_true.append(temp_1)\n                        y_pred.append(temp_2)\n                        break\n                    else:\n                        temp_1.append(label_map[label_ids[i][j]])\n                        temp_2.append(label_map[logits[i][j]])\n\n        report = classification_report(y_true, y_pred,digits=4)\n        logger.info(""\\n%s"", report)\n        output_eval_file = os.path.join(args.output_dir, ""eval_results.txt"")\n        with open(output_eval_file, ""w"") as writer:\n            logger.info(""***** Eval results *****"")\n            logger.info(""\\n%s"", report)\n            writer.write(report)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
