file_path,api_count,code
api.py,0,"b'from flask import Flask,request,jsonify\nfrom flask_cors import CORS\n\nfrom bert import QA\n\napp = Flask(__name__)\nCORS(app)\n\nmodel = QA(""model"")\n\n@app.route(""/predict"",methods=[\'POST\'])\ndef predict():\n    doc = request.json[""document""]\n    q = request.json[""question""]\n    try:\n        out = model.predict(doc,q)\n        return jsonify({""result"":out})\n    except Exception as e:\n        print(e)\n        return jsonify({""result"":""Model Failed""})\n\nif __name__ == ""__main__"":\n    app.run(\'0.0.0.0\',port=8000)'"
bert.py,7,"b'from __future__ import absolute_import, division, print_function\n\nimport collections\nimport logging\nimport math\n\nimport numpy as np\nimport torch\nfrom pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n                                  BertForQuestionAnswering, BertTokenizer)\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n\nfrom utils import (get_answer, input_to_squad_example,\n                   squad_examples_to_features, to_list)\n\nRawResult = collections.namedtuple(""RawResult"",\n                                   [""unique_id"", ""start_logits"", ""end_logits""])\n\n\n\nclass QA:\n\n    def __init__(self,model_path: str):\n        self.max_seq_length = 384\n        self.doc_stride = 128\n        self.do_lower_case = True\n        self.max_query_length = 64\n        self.n_best_size = 20\n        self.max_answer_length = 30\n        self.model, self.tokenizer = self.load_model(model_path)\n        if torch.cuda.is_available():\n            self.device = \'cuda\'\n        else:\n            self.device = \'cpu\'\n        self.model.to(self.device)\n        self.model.eval()\n\n\n    def load_model(self,model_path: str,do_lower_case=False):\n        config = BertConfig.from_pretrained(model_path + ""/bert_config.json"")\n        tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=do_lower_case)\n        model = BertForQuestionAnswering.from_pretrained(model_path, from_tf=False, config=config)\n        return model, tokenizer\n    \n    def predict(self,passage :str,question :str):\n        example = input_to_squad_example(passage,question)\n        features = squad_examples_to_features(example,self.tokenizer,self.max_seq_length,self.doc_stride,self.max_query_length)\n        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n                                all_example_index)\n        eval_sampler = SequentialSampler(dataset)\n        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=1)\n        all_results = []\n        for batch in eval_dataloader:\n            batch = tuple(t.to(self.device) for t in batch)\n            with torch.no_grad():\n                inputs = {\'input_ids\':      batch[0],\n                        \'attention_mask\': batch[1],\n                        \'token_type_ids\': batch[2]  \n                        }\n                example_indices = batch[3]\n                outputs = self.model(**inputs)\n\n            for i, example_index in enumerate(example_indices):\n                eval_feature = features[example_index.item()]\n                unique_id = int(eval_feature.unique_id)\n                result = RawResult(unique_id    = unique_id,\n                                    start_logits = to_list(outputs[0][i]),\n                                    end_logits   = to_list(outputs[1][i]))\n                all_results.append(result)\n        answer = get_answer(example,features,all_results,self.n_best_size,self.max_answer_length,self.do_lower_case)\n        return answer\n'"
utils.py,1,"b'from __future__ import absolute_import, division, print_function\n\nimport collections\nimport math\n\nimport numpy as np\nimport torch\nfrom pytorch_transformers.tokenization_bert import (BasicTokenizer,\n                                                    whitespace_tokenize)\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n\n\nclass SquadExample(object):\n    """"""\n    A single training/test example for the Squad dataset.\n    For examples without an answer, the start and end position are -1.\n    """"""\n\n    def __init__(self,\n                 qas_id,\n                 question_text,\n                 doc_tokens,\n                 orig_answer_text=None,\n                 start_position=None,\n                 end_position=None):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.doc_tokens = doc_tokens\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = """"\n        s += ""qas_id: %s"" % (self.qas_id)\n        s += "", question_text: %s"" % (\n            self.question_text)\n        s += "", doc_tokens: [%s]"" % ("" "".join(self.doc_tokens))\n        if self.start_position:\n            s += "", start_position: %d"" % (self.start_position)\n        if self.end_position:\n            s += "", end_position: %d"" % (self.end_position)\n        return s\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self,\n                 unique_id,\n                 example_index,\n                 doc_span_index,\n                 tokens,\n                 token_to_orig_map,\n                 token_is_max_context,\n                 input_ids,\n                 input_mask,\n                 segment_ids,\n                 paragraph_len,\n                 start_position=None,\n                 end_position=None,):\n        self.unique_id = unique_id\n        self.example_index = example_index\n        self.doc_span_index = doc_span_index\n        self.tokens = tokens\n        self.token_to_orig_map = token_to_orig_map\n        self.token_is_max_context = token_is_max_context\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.paragraph_len = paragraph_len\n        self.start_position = start_position\n        self.end_position = end_position\n\ndef input_to_squad_example(passage, question):\n    """"""Convert input passage and question into a SquadExample.""""""\n\n    def is_whitespace(c):\n        if c == "" "" or c == ""\\t"" or c == ""\\r"" or c == ""\\n"" or ord(c) == 0x202F:\n            return True\n        return False\n\n    paragraph_text = passage\n    doc_tokens = []\n    char_to_word_offset = []\n    prev_is_whitespace = True\n    for c in paragraph_text:\n        if is_whitespace(c):\n            prev_is_whitespace = True\n        else:\n            if prev_is_whitespace:\n                doc_tokens.append(c)\n            else:\n                doc_tokens[-1] += c\n            prev_is_whitespace = False\n        char_to_word_offset.append(len(doc_tokens) - 1)\n\n    qas_id = 0\n    question_text = question\n    start_position = None\n    end_position = None\n    orig_answer_text = None\n\n    example = SquadExample(\n        qas_id=qas_id,\n        question_text=question_text,\n        doc_tokens=doc_tokens,\n        orig_answer_text=orig_answer_text,\n        start_position=start_position,\n        end_position=end_position)\n                \n    return example\n\ndef _check_is_max_context(doc_spans, cur_span_index, position):\n    """"""Check if this is the \'max context\' doc span for the token.""""""\n\n    # Because of the sliding window approach taken to scoring documents, a single\n    # token can appear in multiple documents. E.g.\n    #  Doc: the man went to the store and bought a gallon of milk\n    #  Span A: the man went to the\n    #  Span B: to the store and bought\n    #  Span C: and bought a gallon of\n    #  ...\n    #\n    # Now the word \'bought\' will have two scores from spans B and C. We only\n    # want to consider the score with ""maximum context"", which we define as\n    # the *minimum* of its left and right context (the *sum* of left and\n    # right context will always be the same, of course).\n    #\n    # In the example the maximum context for \'bought\' would be span C since\n    # it has 1 left context and 3 right context, while span B has 4 left context\n    # and 0 right context.\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\ndef squad_examples_to_features(example, tokenizer, max_seq_length,\n                                 doc_stride, max_query_length,cls_token_at_end=False,\n                                 cls_token=\'[CLS]\', sep_token=\'[SEP]\', pad_token=0,\n                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n                                 cls_token_segment_id=0, pad_token_segment_id=0,\n                                 mask_padding_with_zero=True):\n    """"""Loads a data file into a list of `InputBatch`s.""""""\n\n    unique_id = 1000000000\n    # cnt_pos, cnt_neg = 0, 0\n    # max_N, max_M = 1024, 1024\n    # f = np.zeros((max_N, max_M), dtype=np.float32)\n    example_index = 0\n    features = []\n    # if example_index % 100 == 0:\n    #     logger.info(\'Converting %s/%s pos %s neg %s\', example_index, len(examples), cnt_pos, cnt_neg)\n\n    query_tokens = tokenizer.tokenize(example.question_text)\n\n    if len(query_tokens) > max_query_length:\n        query_tokens = query_tokens[0:max_query_length]\n\n    tok_to_orig_index = []\n    orig_to_tok_index = []\n    all_doc_tokens = []\n    for (i, token) in enumerate(example.doc_tokens):\n        orig_to_tok_index.append(len(all_doc_tokens))\n        sub_tokens = tokenizer.tokenize(token)\n        for sub_token in sub_tokens:\n            tok_to_orig_index.append(i)\n            all_doc_tokens.append(sub_token)\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        ""DocSpan"", [""start"", ""length""])\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n        length = len(all_doc_tokens) - start_offset\n        if length > max_tokens_for_doc:\n            length = max_tokens_for_doc\n        doc_spans.append(_DocSpan(start=start_offset, length=length))\n        if start_offset + length == len(all_doc_tokens):\n            break\n        start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n        tokens = []\n        token_to_orig_map = {}\n        token_is_max_context = {}\n        segment_ids = []\n\n        # CLS token at the beginning\n        if not cls_token_at_end:\n            tokens.append(cls_token)\n            segment_ids.append(cls_token_segment_id)\n\n        # Query\n        for token in query_tokens:\n            tokens.append(token)\n            segment_ids.append(sequence_a_segment_id)\n\n        # SEP token\n        tokens.append(sep_token)\n        segment_ids.append(sequence_a_segment_id)\n\n        # Paragraph\n        for i in range(doc_span.length):\n            split_token_index = doc_span.start + i\n            token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n            is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                                    split_token_index)\n            token_is_max_context[len(tokens)] = is_max_context\n            tokens.append(all_doc_tokens[split_token_index])\n            segment_ids.append(sequence_b_segment_id)\n        paragraph_len = doc_span.length\n\n        # SEP token\n        tokens.append(sep_token)\n        segment_ids.append(sequence_b_segment_id)\n\n        # CLS token at the end\n        if cls_token_at_end:\n            tokens.append(cls_token)\n            segment_ids.append(cls_token_segment_id)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        while len(input_ids) < max_seq_length:\n            input_ids.append(pad_token)\n            input_mask.append(0 if mask_padding_with_zero else 1)\n            segment_ids.append(pad_token_segment_id)\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        start_position = None\n        end_position = None\n\n        features.append(\n            InputFeatures(\n                unique_id=unique_id,\n                example_index=example_index,\n                doc_span_index=doc_span_index,\n                tokens=tokens,\n                token_to_orig_map=token_to_orig_map,\n                token_is_max_context=token_is_max_context,\n                input_ids=input_ids,\n                input_mask=input_mask,\n                segment_ids=segment_ids,\n                paragraph_len=paragraph_len,\n                start_position=start_position,\n                end_position=end_position))\n        unique_id += 1\n\n    return features\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\ndef _get_best_indexes(logits, n_best_size):\n    """"""Get the n-best logits from a list.""""""\n    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n    best_indexes = []\n    for i in range(len(index_and_score)):\n        if i >= n_best_size:\n            break\n        best_indexes.append(index_and_score[i][0])\n    return best_indexes\n\nRawResult = collections.namedtuple(""RawResult"",[""unique_id"", ""start_logits"", ""end_logits""])\n\ndef get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n    """"""Project the tokenized prediction back to the original text.""""""\n\n    # When we created the data, we kept track of the alignment between original\n    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n    # now `orig_text` contains the span of our original text corresponding to the\n    # span that we predicted.\n    #\n    # However, `orig_text` may contain extra characters that we don\'t want in\n    # our prediction.\n    #\n    # For example, let\'s say:\n    #   pred_text = steve smith\n    #   orig_text = Steve Smith\'s\n    #\n    # We don\'t want to return `orig_text` because it contains the extra ""\'s"".\n    #\n    # We don\'t want to return `pred_text` because it\'s already been normalized\n    # (the SQuAD eval script also does punctuation stripping/lower casing but\n    # our tokenizer does additional normalization like stripping accent\n    # characters).\n    #\n    # What we really want to return is ""Steve Smith"".\n    #\n    # Therefore, we have to apply a semi-complicated alignment heuristic between\n    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n    # can fail in certain cases in which case we just return `orig_text`.\n\n    def _strip_spaces(text):\n        ns_chars = []\n        ns_to_s_map = collections.OrderedDict()\n        for (i, c) in enumerate(text):\n            if c == "" "":\n                continue\n            ns_to_s_map[len(ns_chars)] = i\n            ns_chars.append(c)\n        ns_text = """".join(ns_chars)\n        return (ns_text, ns_to_s_map)\n\n    # We first tokenize `orig_text`, strip whitespace from the result\n    # and `pred_text`, and check if they are the same length. If they are\n    # NOT the same length, the heuristic has failed. If they are the same\n    # length, we assume the characters are one-to-one aligned.\n    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n\n    tok_text = "" "".join(tokenizer.tokenize(orig_text))\n\n    start_position = tok_text.find(pred_text)\n    if start_position == -1:\n        return orig_text\n    end_position = start_position + len(pred_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n    if len(orig_ns_text) != len(tok_ns_text):\n        return orig_text\n\n    # We then project the characters in `pred_text` back to `orig_text` using\n    # the character-to-character alignment.\n    tok_s_to_ns_map = {}\n    for (i, tok_index) in tok_ns_to_s_map.items():\n        tok_s_to_ns_map[tok_index] = i\n\n    orig_start_position = None\n    if start_position in tok_s_to_ns_map:\n        ns_start_position = tok_s_to_ns_map[start_position]\n        if ns_start_position in orig_ns_to_s_map:\n            orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n    if orig_start_position is None:\n        return orig_text\n\n    orig_end_position = None\n    if end_position in tok_s_to_ns_map:\n        ns_end_position = tok_s_to_ns_map[end_position]\n        if ns_end_position in orig_ns_to_s_map:\n            orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n    if orig_end_position is None:\n        return orig_text\n\n    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n    return output_text\n\ndef _compute_softmax(scores):\n    """"""Compute softmax probability over raw logits.""""""\n    if not scores:\n        return []\n\n    max_score = None\n    for score in scores:\n        if max_score is None or score > max_score:\n            max_score = score\n\n    exp_scores = []\n    total_sum = 0.0\n    for score in scores:\n        x = math.exp(score - max_score)\n        exp_scores.append(x)\n        total_sum += x\n\n    probs = []\n    for score in exp_scores:\n        probs.append(score / total_sum)\n    return probs\n\ndef get_answer(example, features, all_results, n_best_size,\n                max_answer_length, do_lower_case):\n    example_index_to_features = collections.defaultdict(list)\n    for feature in features:\n        example_index_to_features[feature.example_index].append(feature)\n    \n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n    \n    _PrelimPrediction = collections.namedtuple( ""PrelimPrediction"",[""feature_index"", ""start_index"", ""end_index"", ""start_logit"", ""end_logit""])\n\n    example_index = 0\n    features = example_index_to_features[example_index]\n\n    prelim_predictions = []\n\n    for (feature_index, feature) in enumerate(features):\n        result = unique_id_to_result[feature.unique_id]\n        start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n        end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n        for start_index in start_indexes:\n            for end_index in end_indexes:\n                # We could hypothetically create invalid predictions, e.g., predict\n                # that the start of the span is in the question. We throw out all\n                # invalid predictions.\n                if start_index >= len(feature.tokens):\n                    continue\n                if end_index >= len(feature.tokens):\n                    continue\n                if start_index not in feature.token_to_orig_map:\n                    continue\n                if end_index not in feature.token_to_orig_map:\n                    continue\n                if not feature.token_is_max_context.get(start_index, False):\n                    continue\n                if end_index < start_index:\n                    continue\n                length = end_index - start_index + 1\n                if length > max_answer_length:\n                    continue\n                prelim_predictions.append(\n                    _PrelimPrediction(\n                        feature_index=feature_index,\n                        start_index=start_index,\n                        end_index=end_index,\n                        start_logit=result.start_logits[start_index],\n                        end_logit=result.end_logits[end_index]))\n    prelim_predictions = sorted(prelim_predictions,key=lambda x: (x.start_logit + x.end_logit),reverse=True)\n    _NbestPrediction = collections.namedtuple(""NbestPrediction"",\n                        [""text"", ""start_logit"", ""end_logit"",""start_index"",""end_index""])\n    seen_predictions = {}\n    nbest = []\n    for pred in prelim_predictions:\n        if len(nbest) >= n_best_size:\n            break\n        feature = features[pred.feature_index]\n        orig_doc_start = -1\n        orig_doc_end = -1\n        if pred.start_index > 0:  # this is a non-null prediction\n            tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n            orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n            tok_text = "" "".join(tok_tokens)\n\n            # De-tokenize WordPieces that have been split off.\n            tok_text = tok_text.replace("" ##"", """")\n            tok_text = tok_text.replace(""##"", """")\n\n            # Clean whitespace\n            tok_text = tok_text.strip()\n            tok_text = "" "".join(tok_text.split())\n            orig_text = "" "".join(orig_tokens)\n\n            final_text = get_final_text(tok_text, orig_text,do_lower_case)\n            if final_text in seen_predictions:\n                continue\n\n            seen_predictions[final_text] = True\n        else:\n            final_text = """"\n            seen_predictions[final_text] = True\n\n        nbest.append(\n            _NbestPrediction(\n                text=final_text,\n                start_logit=pred.start_logit,\n                end_logit=pred.end_logit,\n                start_index=orig_doc_start,\n                end_index=orig_doc_end))\n\n    if not nbest:\n        nbest.append(_NbestPrediction(text=""empty"", start_logit=0.0, end_logit=0.0,start_index=-1,\n                end_index=-1))\n\n    assert len(nbest) >= 1\n\n    total_scores = []\n    for entry in nbest:\n        total_scores.append(entry.start_logit + entry.end_logit)\n\n    probs = _compute_softmax(total_scores)\n    \n    answer = {""answer"" : nbest[0].text,\n               ""start"" : nbest[0].start_index,\n               ""end"" : nbest[0].end_index,\n               ""confidence"" : probs[0],\n               ""document"" : example.doc_tokens\n             }\n    return answer'"
training/run_squad.py,30,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Finetuning the library models for question-answering on SQuAD (Bert, XLM, XLNet).""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport logging\nimport os\nimport random\nimport glob\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm, trange\n\nfrom tensorboardX import SummaryWriter\n\nfrom pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n                                  BertForQuestionAnswering, BertTokenizer,\n                                  XLMConfig, XLMForQuestionAnswering,\n                                  XLMTokenizer, XLNetConfig,\n                                  XLNetForQuestionAnswering,\n                                  XLNetTokenizer)\n\nfrom pytorch_transformers import AdamW, WarmupLinearSchedule\n\nfrom utils_squad import (read_squad_examples, convert_examples_to_features,\n                         RawResult, write_predictions,\n                         RawResultExtended, write_predictions_extended)\n\n# The follwing import is the official SQuAD evaluation script (2.0).\n# You can remove it from the dependencies if you are using this script outside of the library\n# We\'ve added it here for automated tests (see examples/test_examples.py file)\nfrom utils_squad_evaluate import EVAL_OPTS, main as evaluate_on_squad\n\nlogger = logging.getLogger(__name__)\n\nALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) \\\n                  for conf in (BertConfig, XLNetConfig, XLMConfig)), ())\n\nMODEL_CLASSES = {\n    \'bert\': (BertConfig, BertForQuestionAnswering, BertTokenizer),\n    \'xlnet\': (XLNetConfig, XLNetForQuestionAnswering, XLNetTokenizer),\n    \'xlm\': (XLMConfig, XLMForQuestionAnswering, XLMTokenizer),\n}\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\ndef train(args, train_dataset, model, tokenizer):\n    """""" Train the model """"""\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\'bias\', \'LayerNorm.weight\']\n    optimizer_grouped_parameters = [\n        {\'params\': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \'weight_decay\': args.weight_decay},\n        {\'params\': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0}\n        ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n                                                          output_device=args.local_rank,\n                                                          find_unused_parameters=True)\n\n    # Train!\n    logger.info(""***** Running training *****"")\n    logger.info(""  Num examples = %d"", len(train_dataset))\n    logger.info(""  Num Epochs = %d"", args.num_train_epochs)\n    logger.info(""  Instantaneous batch size per GPU = %d"", args.per_gpu_train_batch_size)\n    logger.info(""  Total train batch size (w. parallel, distributed & accumulation) = %d"",\n                   args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info(""  Gradient Accumulation steps = %d"", args.gradient_accumulation_steps)\n    logger.info(""  Total optimization steps = %d"", t_total)\n\n    global_step = 0\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    train_iterator = trange(int(args.num_train_epochs), desc=""Epoch"", disable=args.local_rank not in [-1, 0])\n    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=""Iteration"", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            inputs = {\'input_ids\':       batch[0],\n                      \'attention_mask\':  batch[1], \n                      \'token_type_ids\':  None if args.model_type == \'xlm\' else batch[2],  \n                      \'start_positions\': batch[3], \n                      \'end_positions\':   batch[4]}\n            if args.model_type in [\'xlnet\', \'xlm\']:\n                inputs.update({\'cls_index\': batch[5],\n                               \'p_mask\':    batch[6]})\n            outputs = model(**inputs)\n            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean() # mean() to average on multi-gpu parallel (not distributed) training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                scheduler.step()  # Update learning rate schedule\n                optimizer.step()\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\'eval_{}\'.format(key), value, global_step)\n                    tb_writer.add_scalar(\'lr\', scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\'loss\', (tr_loss - logging_loss)/args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \'checkpoint-{}\'.format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = model.module if hasattr(model, \'module\') else model  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, \'training_args.bin\'))\n                    logger.info(""Saving model checkpoint to %s"", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step\n\n\ndef evaluate(args, model, tokenizer, prefix=""""):\n    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n\n    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(args.output_dir)\n\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Note that DistributedSampler samples randomly\n    eval_sampler = SequentialSampler(dataset) if args.local_rank == -1 else DistributedSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n    # Eval!\n    logger.info(""***** Running evaluation {} *****"".format(prefix))\n    logger.info(""  Num examples = %d"", len(dataset))\n    logger.info(""  Batch size = %d"", args.eval_batch_size)\n    all_results = []\n    for batch in tqdm(eval_dataloader, desc=""Evaluating""):\n        model.eval()\n        batch = tuple(t.to(args.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\'input_ids\':      batch[0],\n                      \'attention_mask\': batch[1],\n                      \'token_type_ids\': None if args.model_type == \'xlm\' else batch[2]  # XLM don\'t use segment_ids\n                      }\n            example_indices = batch[3]\n            if args.model_type in [\'xlnet\', \'xlm\']:\n                inputs.update({\'cls_index\': batch[4],\n                               \'p_mask\':    batch[5]})\n            outputs = model(**inputs)\n\n        for i, example_index in enumerate(example_indices):\n            eval_feature = features[example_index.item()]\n            unique_id = int(eval_feature.unique_id)\n            if args.model_type in [\'xlnet\', \'xlm\']:\n                # XLNet uses a more complex post-processing procedure\n                result = RawResultExtended(unique_id            = unique_id,\n                                           start_top_log_probs  = to_list(outputs[0][i]),\n                                           start_top_index      = to_list(outputs[1][i]),\n                                           end_top_log_probs    = to_list(outputs[2][i]),\n                                           end_top_index        = to_list(outputs[3][i]),\n                                           cls_logits           = to_list(outputs[4][i]))\n            else:\n                result = RawResult(unique_id    = unique_id,\n                                   start_logits = to_list(outputs[0][i]),\n                                   end_logits   = to_list(outputs[1][i]))\n            all_results.append(result)\n\n    # Compute predictions\n    output_prediction_file = os.path.join(args.output_dir, ""predictions_{}.json"".format(prefix))\n    output_nbest_file = os.path.join(args.output_dir, ""nbest_predictions_{}.json"".format(prefix))\n    if args.version_2_with_negative:\n        output_null_log_odds_file = os.path.join(args.output_dir, ""null_odds_{}.json"".format(prefix))\n    else:\n        output_null_log_odds_file = None\n\n    if args.model_type in [\'xlnet\', \'xlm\']:\n        # XLNet uses a more complex post-processing procedure\n        write_predictions_extended(examples, features, all_results, args.n_best_size,\n                        args.max_answer_length, output_prediction_file,\n                        output_nbest_file, output_null_log_odds_file, args.predict_file,\n                        model.config.start_n_top, model.config.end_n_top,\n                        args.version_2_with_negative, tokenizer, args.verbose_logging)\n    else:\n        write_predictions(examples, features, all_results, args.n_best_size,\n                        args.max_answer_length, args.do_lower_case, output_prediction_file,\n                        output_nbest_file, output_null_log_odds_file, args.verbose_logging,\n                        args.version_2_with_negative, args.null_score_diff_threshold)\n\n    # Evaluate with the official SQuAD script\n    evaluate_options = EVAL_OPTS(data_file=args.predict_file,\n                                 pred_file=output_prediction_file,\n                                 na_prob_file=output_null_log_odds_file)\n    results = evaluate_on_squad(evaluate_options)\n    return results\n\n\ndef load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n    # Load data features from cache or dataset file\n    input_file = args.predict_file if evaluate else args.train_file\n    cached_features_file = os.path.join(os.path.dirname(input_file), \'cached_{}_{}_{}\'.format(\n        \'dev\' if evaluate else \'train\',\n        list(filter(None, args.model_name_or_path.split(\'/\'))).pop(),\n        str(args.max_seq_length)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache and not output_examples:\n        logger.info(""Loading features from cached file %s"", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(""Creating features from dataset file at %s"", input_file)\n        examples = read_squad_examples(input_file=input_file,\n                                                is_training=not evaluate,\n                                                version_2_with_negative=args.version_2_with_negative)\n        features = convert_examples_to_features(examples=examples,\n                                                tokenizer=tokenizer,\n                                                max_seq_length=args.max_seq_length,\n                                                doc_stride=args.doc_stride,\n                                                max_query_length=args.max_query_length,\n                                                is_training=not evaluate)\n        if args.local_rank in [-1, 0]:\n            logger.info(""Saving features into cached file %s"", cached_features_file)\n            torch.save(features, cached_features_file)\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n    all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n    if evaluate:\n        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n                                all_example_index, all_cls_index, all_p_mask)\n    else:\n        all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n        all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n                                all_start_positions, all_end_positions,\n                                all_cls_index, all_p_mask)\n\n    if output_examples:\n        return dataset, examples, features\n    return dataset\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(""--train_file"", default=None, type=str, required=True,\n                        help=""SQuAD json for training. E.g., train-v1.1.json"")\n    parser.add_argument(""--predict_file"", default=None, type=str, required=True,\n                        help=""SQuAD json for predictions. E.g., dev-v1.1.json or test-v1.1.json"")\n    parser.add_argument(""--model_type"", default=None, type=str, required=True,\n                        help=""Model type selected in the list: "" + "", "".join(MODEL_CLASSES.keys()))\n    parser.add_argument(""--model_name_or_path"", default=None, type=str, required=True,\n                        help=""Path to pre-trained model or shortcut name selected in the list: "" + "", "".join(ALL_MODELS))\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True,\n                        help=""The output directory where the model checkpoints and predictions will be written."")\n\n    ## Other parameters\n    parser.add_argument(""--config_name"", default="""", type=str,\n                        help=""Pretrained config name or path if not the same as model_name"")\n    parser.add_argument(""--tokenizer_name"", default="""", type=str,\n                        help=""Pretrained tokenizer name or path if not the same as model_name"")\n    parser.add_argument(""--cache_dir"", default="""", type=str,\n                        help=""Where do you want to store the pre-trained models downloaded from s3"")\n\n    parser.add_argument(\'--version_2_with_negative\', action=\'store_true\',\n                        help=\'If true, the SQuAD examples contain some that do not have an answer.\')\n    parser.add_argument(\'--null_score_diff_threshold\', type=float, default=0.0,\n                        help=""If null_score - best_non_null is greater than the threshold predict null."")\n\n    parser.add_argument(""--max_seq_length"", default=384, type=int,\n                        help=""The maximum total input sequence length after WordPiece tokenization. Sequences ""\n                             ""longer than this will be truncated, and sequences shorter than this will be padded."")\n    parser.add_argument(""--doc_stride"", default=128, type=int,\n                        help=""When splitting up a long document into chunks, how much stride to take between chunks."")\n    parser.add_argument(""--max_query_length"", default=64, type=int,\n                        help=""The maximum number of tokens for the question. Questions longer than this will ""\n                             ""be truncated to this length."")\n    parser.add_argument(""--do_train"", action=\'store_true\',\n                        help=""Whether to run training."")\n    parser.add_argument(""--do_eval"", action=\'store_true\',\n                        help=""Whether to run eval on the dev set."")\n    parser.add_argument(""--evaluate_during_training"", action=\'store_true\',\n                        help=""Rul evaluation during training at each logging step."")\n    parser.add_argument(""--do_lower_case"", action=\'store_true\',\n                        help=""Set this flag if you are using an uncased model."")\n\n    parser.add_argument(""--per_gpu_train_batch_size"", default=8, type=int,\n                        help=""Batch size per GPU/CPU for training."")\n    parser.add_argument(""--per_gpu_eval_batch_size"", default=8, type=int,\n                        help=""Batch size per GPU/CPU for evaluation."")\n    parser.add_argument(""--learning_rate"", default=5e-5, type=float,\n                        help=""The initial learning rate for Adam."")\n    parser.add_argument(\'--gradient_accumulation_steps\', type=int, default=1,\n                        help=""Number of updates steps to accumulate before performing a backward/update pass."")\n    parser.add_argument(""--weight_decay"", default=0.0, type=float,\n                        help=""Weight deay if we apply some."")\n    parser.add_argument(""--adam_epsilon"", default=1e-8, type=float,\n                        help=""Epsilon for Adam optimizer."")\n    parser.add_argument(""--max_grad_norm"", default=1.0, type=float,\n                        help=""Max gradient norm."")\n    parser.add_argument(""--num_train_epochs"", default=3.0, type=float,\n                        help=""Total number of training epochs to perform."")\n    parser.add_argument(""--max_steps"", default=-1, type=int,\n                        help=""If > 0: set total number of training steps to perform. Override num_train_epochs."")\n    parser.add_argument(""--warmup_steps"", default=0, type=int,\n                        help=""Linear warmup over warmup_steps."")\n    parser.add_argument(""--n_best_size"", default=20, type=int,\n                        help=""The total number of n-best predictions to generate in the nbest_predictions.json output file."")\n    parser.add_argument(""--max_answer_length"", default=30, type=int,\n                        help=""The maximum length of an answer that can be generated. This is needed because the start ""\n                             ""and end predictions are not conditioned on one another."")\n    parser.add_argument(""--verbose_logging"", action=\'store_true\',\n                        help=""If true, all of the warnings related to data processing will be printed. ""\n                             ""A number of warnings are expected for a normal SQuAD evaluation."")\n\n    parser.add_argument(\'--logging_steps\', type=int, default=50,\n                        help=""Log every X updates steps."")\n    parser.add_argument(\'--save_steps\', type=int, default=50,\n                        help=""Save checkpoint every X updates steps."")\n    parser.add_argument(""--eval_all_checkpoints"", action=\'store_true\',\n                        help=""Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number"")\n    parser.add_argument(""--no_cuda"", action=\'store_true\',\n                        help=""Whether not to use CUDA when available"")\n    parser.add_argument(\'--overwrite_output_dir\', action=\'store_true\',\n                        help=""Overwrite the content of the output directory"")\n    parser.add_argument(\'--overwrite_cache\', action=\'store_true\',\n                        help=""Overwrite the cached training and evaluation sets"")\n    parser.add_argument(\'--seed\', type=int, default=42,\n                        help=""random seed for initialization"")\n\n    parser.add_argument(""--local_rank"", type=int, default=-1,\n                        help=""local_rank for distributed training on gpus"")\n    parser.add_argument(\'--fp16\', action=\'store_true\',\n                        help=""Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit"")\n    parser.add_argument(\'--fp16_opt_level\', type=str, default=\'O1\',\n                        help=""For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].""\n                             ""See details at https://nvidia.github.io/apex/amp.html"")\n    parser.add_argument(\'--server_ip\', type=str, default=\'\', help=""Can be used for distant debugging."")\n    parser.add_argument(\'--server_port\', type=str, default=\'\', help=""Can be used for distant debugging."")\n    args = parser.parse_args()\n\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n        raise ValueError(""Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome."".format(args.output_dir))\n\n    # Setup distant debugging if needed\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n        print(""Waiting for debugger attach"")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        args.n_gpu = torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(""cuda"", args.local_rank)\n        torch.distributed.init_process_group(backend=\'nccl\')\n        args.n_gpu = 1\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(format = \'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\',\n                        datefmt = \'%m/%d/%Y %H:%M:%S\',\n                        level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.warning(""Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s"",\n                    args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n\n    # Set seed\n    set_seed(args)\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    args.model_type = args.model_type.lower()\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case)\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool(\'.ckpt\' in args.model_name_or_path), config=config)\n\n    if args.local_rank == 0:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    model.to(args.device)\n\n    logger.info(""Training/evaluation parameters %s"", args)\n\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info("" global_step = %s, average loss = %s"", global_step, tr_loss)\n\n\n    # Save the trained model and the tokenizer\n    if args.local_rank == -1 or torch.distributed.get_rank() == 0:\n        # Create output directory if needed\n        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(args.output_dir)\n\n        logger.info(""Saving model checkpoint to %s"", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = model.module if hasattr(model, \'module\') else model  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, \'training_args.bin\'))\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = model_class.from_pretrained(args.output_dir)\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n        model.to(args.device)\n\n\n    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \'/**/\' + WEIGHTS_NAME, recursive=True)))\n            logging.getLogger(""pytorch_transformers.modeling_utils"").setLevel(logging.WARN)  # Reduce model loading logs\n\n        logger.info(""Evaluate the following checkpoints: %s"", checkpoints)\n\n        for checkpoint in checkpoints:\n            # Reload the model\n            global_step = checkpoint.split(\'-\')[-1] if len(checkpoints) > 1 else """"\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n\n            # Evaluate\n            result = evaluate(args, model, tokenizer, prefix=global_step)\n\n            result = dict((k + (\'_{}\'.format(global_step) if global_step else \'\'), v) for k, v in result.items())\n            results.update(result)\n\n    logger.info(""Results: {}"".format(results))\n\n    return results\n\n\nif __name__ == ""__main__"":\n    main()\n'"
training/utils_squad.py,0,"b'\n# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Load SQuAD dataset. """"""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport json\nimport logging\nimport math\nimport collections\nfrom io import open\n\nfrom pytorch_transformers.tokenization_bert import BasicTokenizer, whitespace_tokenize\n\n# Required by XLNet evaluation method to compute optimal threshold (see write_predictions_extended() method)\nfrom utils_squad_evaluate import find_all_best_thresh_v2, make_qid_to_has_ans, get_raw_scores\n\nlogger = logging.getLogger(__name__)\n\n\nclass SquadExample(object):\n    """"""\n    A single training/test example for the Squad dataset.\n    For examples without an answer, the start and end position are -1.\n    """"""\n\n    def __init__(self,\n                 qas_id,\n                 question_text,\n                 doc_tokens,\n                 orig_answer_text=None,\n                 start_position=None,\n                 end_position=None,\n                 is_impossible=None):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.doc_tokens = doc_tokens\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n        self.is_impossible = is_impossible\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = """"\n        s += ""qas_id: %s"" % (self.qas_id)\n        s += "", question_text: %s"" % (\n            self.question_text)\n        s += "", doc_tokens: [%s]"" % ("" "".join(self.doc_tokens))\n        if self.start_position:\n            s += "", start_position: %d"" % (self.start_position)\n        if self.end_position:\n            s += "", end_position: %d"" % (self.end_position)\n        if self.is_impossible:\n            s += "", is_impossible: %r"" % (self.is_impossible)\n        return s\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self,\n                 unique_id,\n                 example_index,\n                 doc_span_index,\n                 tokens,\n                 token_to_orig_map,\n                 token_is_max_context,\n                 input_ids,\n                 input_mask,\n                 segment_ids,\n                 cls_index,\n                 p_mask,\n                 paragraph_len,\n                 start_position=None,\n                 end_position=None,\n                 is_impossible=None):\n        self.unique_id = unique_id\n        self.example_index = example_index\n        self.doc_span_index = doc_span_index\n        self.tokens = tokens\n        self.token_to_orig_map = token_to_orig_map\n        self.token_is_max_context = token_is_max_context\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.cls_index = cls_index\n        self.p_mask = p_mask\n        self.paragraph_len = paragraph_len\n        self.start_position = start_position\n        self.end_position = end_position\n        self.is_impossible = is_impossible\n\n\ndef read_squad_examples(input_file, is_training, version_2_with_negative):\n    """"""Read a SQuAD json file into a list of SquadExample.""""""\n    with open(input_file, ""r"", encoding=\'utf-8\') as reader:\n        input_data = json.load(reader)[""data""]\n\n    def is_whitespace(c):\n        if c == "" "" or c == ""\\t"" or c == ""\\r"" or c == ""\\n"" or ord(c) == 0x202F:\n            return True\n        return False\n\n    examples = []\n    for entry in input_data:\n        for paragraph in entry[""paragraphs""]:\n            paragraph_text = paragraph[""context""]\n            doc_tokens = []\n            char_to_word_offset = []\n            prev_is_whitespace = True\n            for c in paragraph_text:\n                if is_whitespace(c):\n                    prev_is_whitespace = True\n                else:\n                    if prev_is_whitespace:\n                        doc_tokens.append(c)\n                    else:\n                        doc_tokens[-1] += c\n                    prev_is_whitespace = False\n                char_to_word_offset.append(len(doc_tokens) - 1)\n\n            for qa in paragraph[""qas""]:\n                qas_id = qa[""id""]\n                question_text = qa[""question""]\n                start_position = None\n                end_position = None\n                orig_answer_text = None\n                is_impossible = False\n                if is_training:\n                    if version_2_with_negative:\n                        is_impossible = qa[""is_impossible""]\n                    if (len(qa[""answers""]) != 1) and (not is_impossible):\n                        raise ValueError(\n                            ""For training, each question should have exactly 1 answer."")\n                    if not is_impossible:\n                        answer = qa[""answers""][0]\n                        orig_answer_text = answer[""text""]\n                        answer_offset = answer[""answer_start""]\n                        answer_length = len(orig_answer_text)\n                        start_position = char_to_word_offset[answer_offset]\n                        end_position = char_to_word_offset[answer_offset + answer_length - 1]\n                        # Only add answers where the text can be exactly recovered from the\n                        # document. If this CAN\'T happen it\'s likely due to weird Unicode\n                        # stuff so we will just skip the example.\n                        #\n                        # Note that this means for training mode, every example is NOT\n                        # guaranteed to be preserved.\n                        actual_text = "" "".join(doc_tokens[start_position:(end_position + 1)])\n                        cleaned_answer_text = "" "".join(\n                            whitespace_tokenize(orig_answer_text))\n                        if actual_text.find(cleaned_answer_text) == -1:\n                            logger.warning(""Could not find answer: \'%s\' vs. \'%s\'"",\n                                           actual_text, cleaned_answer_text)\n                            continue\n                    else:\n                        start_position = -1\n                        end_position = -1\n                        orig_answer_text = """"\n\n                example = SquadExample(\n                    qas_id=qas_id,\n                    question_text=question_text,\n                    doc_tokens=doc_tokens,\n                    orig_answer_text=orig_answer_text,\n                    start_position=start_position,\n                    end_position=end_position,\n                    is_impossible=is_impossible)\n                examples.append(example)\n    return examples\n\n\ndef convert_examples_to_features(examples, tokenizer, max_seq_length,\n                                 doc_stride, max_query_length, is_training,\n                                 cls_token_at_end=False,\n                                 cls_token=\'[CLS]\', sep_token=\'[SEP]\', pad_token=0,\n                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n                                 cls_token_segment_id=0, pad_token_segment_id=0,\n                                 mask_padding_with_zero=True):\n    """"""Loads a data file into a list of `InputBatch`s.""""""\n\n    unique_id = 1000000000\n    # cnt_pos, cnt_neg = 0, 0\n    # max_N, max_M = 1024, 1024\n    # f = np.zeros((max_N, max_M), dtype=np.float32)\n\n    features = []\n    for (example_index, example) in enumerate(examples):\n\n        # if example_index % 100 == 0:\n        #     logger.info(\'Converting %s/%s pos %s neg %s\', example_index, len(examples), cnt_pos, cnt_neg)\n\n        query_tokens = tokenizer.tokenize(example.question_text)\n\n        if len(query_tokens) > max_query_length:\n            query_tokens = query_tokens[0:max_query_length]\n\n        tok_to_orig_index = []\n        orig_to_tok_index = []\n        all_doc_tokens = []\n        for (i, token) in enumerate(example.doc_tokens):\n            orig_to_tok_index.append(len(all_doc_tokens))\n            sub_tokens = tokenizer.tokenize(token)\n            for sub_token in sub_tokens:\n                tok_to_orig_index.append(i)\n                all_doc_tokens.append(sub_token)\n\n        tok_start_position = None\n        tok_end_position = None\n        if is_training and example.is_impossible:\n            tok_start_position = -1\n            tok_end_position = -1\n        if is_training and not example.is_impossible:\n            tok_start_position = orig_to_tok_index[example.start_position]\n            if example.end_position < len(example.doc_tokens) - 1:\n                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n            else:\n                tok_end_position = len(all_doc_tokens) - 1\n            (tok_start_position, tok_end_position) = _improve_answer_span(\n                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n                example.orig_answer_text)\n\n        # The -3 accounts for [CLS], [SEP] and [SEP]\n        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n        # We can have documents that are longer than the maximum sequence length.\n        # To deal with this we do a sliding window approach, where we take chunks\n        # of the up to our max length with a stride of `doc_stride`.\n        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n            ""DocSpan"", [""start"", ""length""])\n        doc_spans = []\n        start_offset = 0\n        while start_offset < len(all_doc_tokens):\n            length = len(all_doc_tokens) - start_offset\n            if length > max_tokens_for_doc:\n                length = max_tokens_for_doc\n            doc_spans.append(_DocSpan(start=start_offset, length=length))\n            if start_offset + length == len(all_doc_tokens):\n                break\n            start_offset += min(length, doc_stride)\n\n        for (doc_span_index, doc_span) in enumerate(doc_spans):\n            tokens = []\n            token_to_orig_map = {}\n            token_is_max_context = {}\n            segment_ids = []\n\n            # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n            # Original TF implem also keep the classification token (set to 0) (not sure why...)\n            p_mask = []\n\n            # CLS token at the beginning\n            if not cls_token_at_end:\n                tokens.append(cls_token)\n                segment_ids.append(cls_token_segment_id)\n                p_mask.append(0)\n                cls_index = 0\n\n            # Query\n            for token in query_tokens:\n                tokens.append(token)\n                segment_ids.append(sequence_a_segment_id)\n                p_mask.append(1)\n\n            # SEP token\n            tokens.append(sep_token)\n            segment_ids.append(sequence_a_segment_id)\n            p_mask.append(1)\n\n            # Paragraph\n            for i in range(doc_span.length):\n                split_token_index = doc_span.start + i\n                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                                       split_token_index)\n                token_is_max_context[len(tokens)] = is_max_context\n                tokens.append(all_doc_tokens[split_token_index])\n                segment_ids.append(sequence_b_segment_id)\n                p_mask.append(0)\n            paragraph_len = doc_span.length\n\n            # SEP token\n            tokens.append(sep_token)\n            segment_ids.append(sequence_b_segment_id)\n            p_mask.append(1)\n\n            # CLS token at the end\n            if cls_token_at_end:\n                tokens.append(cls_token)\n                segment_ids.append(cls_token_segment_id)\n                p_mask.append(0)\n                cls_index = len(tokens) - 1  # Index of classification token\n\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n            # tokens are attended to.\n            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n            # Zero-pad up to the sequence length.\n            while len(input_ids) < max_seq_length:\n                input_ids.append(pad_token)\n                input_mask.append(0 if mask_padding_with_zero else 1)\n                segment_ids.append(pad_token_segment_id)\n                p_mask.append(1)\n\n            assert len(input_ids) == max_seq_length\n            assert len(input_mask) == max_seq_length\n            assert len(segment_ids) == max_seq_length\n\n            span_is_impossible = example.is_impossible\n            start_position = None\n            end_position = None\n            if is_training and not span_is_impossible:\n                # For training, if our document chunk does not contain an annotation\n                # we throw it out, since there is nothing to predict.\n                doc_start = doc_span.start\n                doc_end = doc_span.start + doc_span.length - 1\n                out_of_span = False\n                if not (tok_start_position >= doc_start and\n                        tok_end_position <= doc_end):\n                    out_of_span = True\n                if out_of_span:\n                    start_position = 0\n                    end_position = 0\n                    span_is_impossible = True\n                else:\n                    doc_offset = len(query_tokens) + 2\n                    start_position = tok_start_position - doc_start + doc_offset\n                    end_position = tok_end_position - doc_start + doc_offset\n\n            if is_training and span_is_impossible:\n                start_position = cls_index\n                end_position = cls_index\n\n            if example_index < 20:\n                logger.info(""*** Example ***"")\n                logger.info(""unique_id: %s"" % (unique_id))\n                logger.info(""example_index: %s"" % (example_index))\n                logger.info(""doc_span_index: %s"" % (doc_span_index))\n                logger.info(""tokens: %s"" % "" "".join(tokens))\n                logger.info(""token_to_orig_map: %s"" % "" "".join([\n                    ""%d:%d"" % (x, y) for (x, y) in token_to_orig_map.items()]))\n                logger.info(""token_is_max_context: %s"" % "" "".join([\n                    ""%d:%s"" % (x, y) for (x, y) in token_is_max_context.items()\n                ]))\n                logger.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n                logger.info(\n                    ""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n                logger.info(\n                    ""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n                if is_training and span_is_impossible:\n                    logger.info(""impossible example"")\n                if is_training and not span_is_impossible:\n                    answer_text = "" "".join(tokens[start_position:(end_position + 1)])\n                    logger.info(""start_position: %d"" % (start_position))\n                    logger.info(""end_position: %d"" % (end_position))\n                    logger.info(\n                        ""answer: %s"" % (answer_text))\n\n            features.append(\n                InputFeatures(\n                    unique_id=unique_id,\n                    example_index=example_index,\n                    doc_span_index=doc_span_index,\n                    tokens=tokens,\n                    token_to_orig_map=token_to_orig_map,\n                    token_is_max_context=token_is_max_context,\n                    input_ids=input_ids,\n                    input_mask=input_mask,\n                    segment_ids=segment_ids,\n                    cls_index=cls_index,\n                    p_mask=p_mask,\n                    paragraph_len=paragraph_len,\n                    start_position=start_position,\n                    end_position=end_position,\n                    is_impossible=span_is_impossible))\n            unique_id += 1\n\n    return features\n\n\ndef _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text):\n    """"""Returns tokenized answer spans that better match the annotated answer.""""""\n\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a ""better match"". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be ""(1895-1943)."". However\n    # after tokenization, our tokens will be ""( 1895 - 1943 ) ."". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose ""Japan"" as a character sub-span of\n    # the word ""Japanese"". Since our WordPiece tokenizer does not split\n    # ""Japanese"", we just use ""Japanese"" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = "" "".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = "" "".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n\ndef _check_is_max_context(doc_spans, cur_span_index, position):\n    """"""Check if this is the \'max context\' doc span for the token.""""""\n\n    # Because of the sliding window approach taken to scoring documents, a single\n    # token can appear in multiple documents. E.g.\n    #  Doc: the man went to the store and bought a gallon of milk\n    #  Span A: the man went to the\n    #  Span B: to the store and bought\n    #  Span C: and bought a gallon of\n    #  ...\n    #\n    # Now the word \'bought\' will have two scores from spans B and C. We only\n    # want to consider the score with ""maximum context"", which we define as\n    # the *minimum* of its left and right context (the *sum* of left and\n    # right context will always be the same, of course).\n    #\n    # In the example the maximum context for \'bought\' would be span C since\n    # it has 1 left context and 3 right context, while span B has 4 left context\n    # and 0 right context.\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n\nRawResult = collections.namedtuple(""RawResult"",\n                                   [""unique_id"", ""start_logits"", ""end_logits""])\n\ndef write_predictions(all_examples, all_features, all_results, n_best_size,\n                      max_answer_length, do_lower_case, output_prediction_file,\n                      output_nbest_file, output_null_log_odds_file, verbose_logging,\n                      version_2_with_negative, null_score_diff_threshold):\n    """"""Write final predictions to the json file and log-odds of null if needed.""""""\n    logger.info(""Writing predictions to: %s"" % (output_prediction_file))\n    logger.info(""Writing nbest to: %s"" % (output_nbest_file))\n\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""PrelimPrediction"",\n        [""feature_index"", ""start_index"", ""end_index"", ""start_logit"", ""end_logit""])\n\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict()\n\n    for (example_index, example) in enumerate(all_examples):\n        features = example_index_to_features[example_index]\n\n        prelim_predictions = []\n        # keep track of the minimum score of null start+end of position 0\n        score_null = 1000000  # large and positive\n        min_null_feature_index = 0  # the paragraph slice with min null score\n        null_start_logit = 0  # the start logit at the slice with min null score\n        null_end_logit = 0  # the end logit at the slice with min null score\n        for (feature_index, feature) in enumerate(features):\n            result = unique_id_to_result[feature.unique_id]\n            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n            # if we could have irrelevant answers, get the min score of irrelevant\n            if version_2_with_negative:\n                feature_null_score = result.start_logits[0] + result.end_logits[0]\n                if feature_null_score < score_null:\n                    score_null = feature_null_score\n                    min_null_feature_index = feature_index\n                    null_start_logit = result.start_logits[0]\n                    null_end_logit = result.end_logits[0]\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= len(feature.tokens):\n                        continue\n                    if end_index >= len(feature.tokens):\n                        continue\n                    if start_index not in feature.token_to_orig_map:\n                        continue\n                    if end_index not in feature.token_to_orig_map:\n                        continue\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n                    prelim_predictions.append(\n                        _PrelimPrediction(\n                            feature_index=feature_index,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_logit=result.start_logits[start_index],\n                            end_logit=result.end_logits[end_index]))\n        if version_2_with_negative:\n            prelim_predictions.append(\n                _PrelimPrediction(\n                    feature_index=min_null_feature_index,\n                    start_index=0,\n                    end_index=0,\n                    start_logit=null_start_logit,\n                    end_logit=null_end_logit))\n        prelim_predictions = sorted(\n            prelim_predictions,\n            key=lambda x: (x.start_logit + x.end_logit),\n            reverse=True)\n\n        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            ""NbestPrediction"", [""text"", ""start_logit"", ""end_logit""])\n\n        seen_predictions = {}\n        nbest = []\n        for pred in prelim_predictions:\n            if len(nbest) >= n_best_size:\n                break\n            feature = features[pred.feature_index]\n            if pred.start_index > 0:  # this is a non-null prediction\n                tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n                orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n                tok_text = "" "".join(tok_tokens)\n\n                # De-tokenize WordPieces that have been split off.\n                tok_text = tok_text.replace("" ##"", """")\n                tok_text = tok_text.replace(""##"", """")\n\n                # Clean whitespace\n                tok_text = tok_text.strip()\n                tok_text = "" "".join(tok_text.split())\n                orig_text = "" "".join(orig_tokens)\n\n                final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n                if final_text in seen_predictions:\n                    continue\n\n                seen_predictions[final_text] = True\n            else:\n                final_text = """"\n                seen_predictions[final_text] = True\n\n            nbest.append(\n                _NbestPrediction(\n                    text=final_text,\n                    start_logit=pred.start_logit,\n                    end_logit=pred.end_logit))\n        # if we didn\'t include the empty option in the n-best, include it\n        if version_2_with_negative:\n            if """" not in seen_predictions:\n                nbest.append(\n                    _NbestPrediction(\n                        text="""",\n                        start_logit=null_start_logit,\n                        end_logit=null_end_logit))\n                \n            # In very rare edge cases we could only have single null prediction.\n            # So we just create a nonce prediction in this case to avoid failure.\n            if len(nbest)==1:\n                nbest.insert(0,\n                    _NbestPrediction(text=""empty"", start_logit=0.0, end_logit=0.0))\n\n        # In very rare edge cases we could have no valid predictions. So we\n        # just create a nonce prediction in this case to avoid failure.\n        if not nbest:\n            nbest.append(\n                _NbestPrediction(text=""empty"", start_logit=0.0, end_logit=0.0))\n\n        assert len(nbest) >= 1\n\n        total_scores = []\n        best_non_null_entry = None\n        for entry in nbest:\n            total_scores.append(entry.start_logit + entry.end_logit)\n            if not best_non_null_entry:\n                if entry.text:\n                    best_non_null_entry = entry\n\n        probs = _compute_softmax(total_scores)\n\n        nbest_json = []\n        for (i, entry) in enumerate(nbest):\n            output = collections.OrderedDict()\n            output[""text""] = entry.text\n            output[""probability""] = probs[i]\n            output[""start_logit""] = entry.start_logit\n            output[""end_logit""] = entry.end_logit\n            nbest_json.append(output)\n\n        assert len(nbest_json) >= 1\n\n        if not version_2_with_negative:\n            all_predictions[example.qas_id] = nbest_json[0][""text""]\n        else:\n            # predict """" iff the null score - the score of best non-null > threshold\n            score_diff = score_null - best_non_null_entry.start_logit - (\n                best_non_null_entry.end_logit)\n            scores_diff_json[example.qas_id] = score_diff\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example.qas_id] = """"\n            else:\n                all_predictions[example.qas_id] = best_non_null_entry.text\n        all_nbest_json[example.qas_id] = nbest_json\n\n    with open(output_prediction_file, ""w"") as writer:\n        writer.write(json.dumps(all_predictions, indent=4) + ""\\n"")\n\n    with open(output_nbest_file, ""w"") as writer:\n        writer.write(json.dumps(all_nbest_json, indent=4) + ""\\n"")\n\n    if version_2_with_negative:\n        with open(output_null_log_odds_file, ""w"") as writer:\n            writer.write(json.dumps(scores_diff_json, indent=4) + ""\\n"")\n\n    return all_predictions\n\n\n# For XLNet (and XLM which uses the same head)\nRawResultExtended = collections.namedtuple(""RawResultExtended"",\n    [""unique_id"", ""start_top_log_probs"", ""start_top_index"",\n     ""end_top_log_probs"", ""end_top_index"", ""cls_logits""])\n\n\ndef write_predictions_extended(all_examples, all_features, all_results, n_best_size,\n                                max_answer_length, output_prediction_file,\n                                output_nbest_file,\n                                output_null_log_odds_file, orig_data_file,\n                                start_n_top, end_n_top, version_2_with_negative,\n                                tokenizer, verbose_logging):\n    """""" XLNet write prediction logic (more complex than Bert\'s).\n        Write final predictions to the json file and log-odds of null if needed.\n\n        Requires utils_squad_evaluate.py\n    """"""\n    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""PrelimPrediction"",\n        [""feature_index"", ""start_index"", ""end_index"",\n        ""start_log_prob"", ""end_log_prob""])\n\n    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""NbestPrediction"", [""text"", ""start_log_prob"", ""end_log_prob""])\n\n    logger.info(""Writing predictions to: %s"", output_prediction_file)\n    # logger.info(""Writing nbest to: %s"" % (output_nbest_file))\n\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict()\n\n    for (example_index, example) in enumerate(all_examples):\n        features = example_index_to_features[example_index]\n\n        prelim_predictions = []\n        # keep track of the minimum score of null start+end of position 0\n        score_null = 1000000  # large and positive\n\n        for (feature_index, feature) in enumerate(features):\n            result = unique_id_to_result[feature.unique_id]\n\n            cur_null_score = result.cls_logits\n\n            # if we could have irrelevant answers, get the min score of irrelevant\n            score_null = min(score_null, cur_null_score)\n\n            for i in range(start_n_top):\n                for j in range(end_n_top):\n                    start_log_prob = result.start_top_log_probs[i]\n                    start_index = result.start_top_index[i]\n\n                    j_index = i * end_n_top + j\n\n                    end_log_prob = result.end_top_log_probs[j_index]\n                    end_index = result.end_top_index[j_index]\n\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= feature.paragraph_len - 1:\n                        continue\n                    if end_index >= feature.paragraph_len - 1:\n                        continue\n\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n\n                    prelim_predictions.append(\n                        _PrelimPrediction(\n                            feature_index=feature_index,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_log_prob=start_log_prob,\n                            end_log_prob=end_log_prob))\n\n        prelim_predictions = sorted(\n            prelim_predictions,\n            key=lambda x: (x.start_log_prob + x.end_log_prob),\n            reverse=True)\n\n        seen_predictions = {}\n        nbest = []\n        for pred in prelim_predictions:\n            if len(nbest) >= n_best_size:\n                break\n            feature = features[pred.feature_index]\n\n            # XLNet un-tokenizer\n            # Let\'s keep it simple for now and see if we need all this later.\n            # \n            # tok_start_to_orig_index = feature.tok_start_to_orig_index\n            # tok_end_to_orig_index = feature.tok_end_to_orig_index\n            # start_orig_pos = tok_start_to_orig_index[pred.start_index]\n            # end_orig_pos = tok_end_to_orig_index[pred.end_index]\n            # paragraph_text = example.paragraph_text\n            # final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()\n\n            # Previously used Bert untokenizer\n            tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n            orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n            tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n\n            # Clean whitespace\n            tok_text = tok_text.strip()\n            tok_text = "" "".join(tok_text.split())\n            orig_text = "" "".join(orig_tokens)\n\n            final_text = get_final_text(tok_text, orig_text, tokenizer.do_lower_case,\n                                        verbose_logging)\n\n            if final_text in seen_predictions:\n                continue\n\n            seen_predictions[final_text] = True\n\n            nbest.append(\n                _NbestPrediction(\n                    text=final_text,\n                    start_log_prob=pred.start_log_prob,\n                    end_log_prob=pred.end_log_prob))\n\n        # In very rare edge cases we could have no valid predictions. So we\n        # just create a nonce prediction in this case to avoid failure.\n        if not nbest:\n            nbest.append(\n                _NbestPrediction(text="""", start_log_prob=-1e6,\n                end_log_prob=-1e6))\n\n        total_scores = []\n        best_non_null_entry = None\n        for entry in nbest:\n            total_scores.append(entry.start_log_prob + entry.end_log_prob)\n            if not best_non_null_entry:\n                best_non_null_entry = entry\n\n        probs = _compute_softmax(total_scores)\n\n        nbest_json = []\n        for (i, entry) in enumerate(nbest):\n            output = collections.OrderedDict()\n            output[""text""] = entry.text\n            output[""probability""] = probs[i]\n            output[""start_log_prob""] = entry.start_log_prob\n            output[""end_log_prob""] = entry.end_log_prob\n            nbest_json.append(output)\n\n        assert len(nbest_json) >= 1\n        assert best_non_null_entry is not None\n\n        score_diff = score_null\n        scores_diff_json[example.qas_id] = score_diff\n        # note(zhiliny): always predict best_non_null_entry\n        # and the evaluation script will search for the best threshold\n        all_predictions[example.qas_id] = best_non_null_entry.text\n\n        all_nbest_json[example.qas_id] = nbest_json\n\n    with open(output_prediction_file, ""w"") as writer:\n        writer.write(json.dumps(all_predictions, indent=4) + ""\\n"")\n\n    with open(output_nbest_file, ""w"") as writer:\n        writer.write(json.dumps(all_nbest_json, indent=4) + ""\\n"")\n\n    if version_2_with_negative:\n        with open(output_null_log_odds_file, ""w"") as writer:\n            writer.write(json.dumps(scores_diff_json, indent=4) + ""\\n"")\n\n    with open(orig_data_file, ""r"", encoding=\'utf-8\') as reader:\n        orig_data = json.load(reader)[""data""]\n\n    qid_to_has_ans = make_qid_to_has_ans(orig_data)\n    has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n    no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n    exact_raw, f1_raw = get_raw_scores(orig_data, all_predictions)\n    out_eval = {}\n\n    find_all_best_thresh_v2(out_eval, all_predictions, exact_raw, f1_raw, scores_diff_json, qid_to_has_ans)\n\n    return out_eval\n\n\ndef get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n    """"""Project the tokenized prediction back to the original text.""""""\n\n    # When we created the data, we kept track of the alignment between original\n    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n    # now `orig_text` contains the span of our original text corresponding to the\n    # span that we predicted.\n    #\n    # However, `orig_text` may contain extra characters that we don\'t want in\n    # our prediction.\n    #\n    # For example, let\'s say:\n    #   pred_text = steve smith\n    #   orig_text = Steve Smith\'s\n    #\n    # We don\'t want to return `orig_text` because it contains the extra ""\'s"".\n    #\n    # We don\'t want to return `pred_text` because it\'s already been normalized\n    # (the SQuAD eval script also does punctuation stripping/lower casing but\n    # our tokenizer does additional normalization like stripping accent\n    # characters).\n    #\n    # What we really want to return is ""Steve Smith"".\n    #\n    # Therefore, we have to apply a semi-complicated alignment heuristic between\n    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n    # can fail in certain cases in which case we just return `orig_text`.\n\n    def _strip_spaces(text):\n        ns_chars = []\n        ns_to_s_map = collections.OrderedDict()\n        for (i, c) in enumerate(text):\n            if c == "" "":\n                continue\n            ns_to_s_map[len(ns_chars)] = i\n            ns_chars.append(c)\n        ns_text = """".join(ns_chars)\n        return (ns_text, ns_to_s_map)\n\n    # We first tokenize `orig_text`, strip whitespace from the result\n    # and `pred_text`, and check if they are the same length. If they are\n    # NOT the same length, the heuristic has failed. If they are the same\n    # length, we assume the characters are one-to-one aligned.\n    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n\n    tok_text = "" "".join(tokenizer.tokenize(orig_text))\n\n    start_position = tok_text.find(pred_text)\n    if start_position == -1:\n        if verbose_logging:\n            logger.info(\n                ""Unable to find text: \'%s\' in \'%s\'"" % (pred_text, orig_text))\n        return orig_text\n    end_position = start_position + len(pred_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n    if len(orig_ns_text) != len(tok_ns_text):\n        if verbose_logging:\n            logger.info(""Length not equal after stripping spaces: \'%s\' vs \'%s\'"",\n                        orig_ns_text, tok_ns_text)\n        return orig_text\n\n    # We then project the characters in `pred_text` back to `orig_text` using\n    # the character-to-character alignment.\n    tok_s_to_ns_map = {}\n    for (i, tok_index) in tok_ns_to_s_map.items():\n        tok_s_to_ns_map[tok_index] = i\n\n    orig_start_position = None\n    if start_position in tok_s_to_ns_map:\n        ns_start_position = tok_s_to_ns_map[start_position]\n        if ns_start_position in orig_ns_to_s_map:\n            orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n    if orig_start_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map start position"")\n        return orig_text\n\n    orig_end_position = None\n    if end_position in tok_s_to_ns_map:\n        ns_end_position = tok_s_to_ns_map[end_position]\n        if ns_end_position in orig_ns_to_s_map:\n            orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n    if orig_end_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map end position"")\n        return orig_text\n\n    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n    return output_text\n\n\ndef _get_best_indexes(logits, n_best_size):\n    """"""Get the n-best logits from a list.""""""\n    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n    best_indexes = []\n    for i in range(len(index_and_score)):\n        if i >= n_best_size:\n            break\n        best_indexes.append(index_and_score[i][0])\n    return best_indexes\n\n\ndef _compute_softmax(scores):\n    """"""Compute softmax probability over raw logits.""""""\n    if not scores:\n        return []\n\n    max_score = None\n    for score in scores:\n        if max_score is None or score > max_score:\n            max_score = score\n\n    exp_scores = []\n    total_sum = 0.0\n    for score in scores:\n        x = math.exp(score - max_score)\n        exp_scores.append(x)\n        total_sum += x\n\n    probs = []\n    for score in exp_scores:\n        probs.append(score / total_sum)\n    return probs\n'"
