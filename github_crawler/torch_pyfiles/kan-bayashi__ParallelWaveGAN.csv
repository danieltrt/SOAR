file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Setup Parallel WaveGAN libarary.""""""\n\nimport os\nimport pip\nimport sys\n\nfrom distutils.version import LooseVersion\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nif LooseVersion(sys.version) < LooseVersion(""3.6""):\n    raise RuntimeError(\n        ""parallel-wavegan requires Python>=3.6, ""\n        ""but your Python is {}"".format(sys.version))\nif LooseVersion(pip.__version__) < LooseVersion(""19""):\n    raise RuntimeError(\n        ""pip>=19.0.0 is required, but your pip is {}. ""\n        ""Try again after \\""pip install -U pip\\"""".format(pip.__version__))\n\nrequirements = {\n    ""install"": [\n        ""torch>=1.0.1"",\n        ""setuptools>=38.5.1"",\n        ""librosa>=0.7.0"",\n        ""soundfile>=0.10.2"",\n        ""tensorboardX>=1.8"",\n        ""matplotlib>=3.1.0"",\n        ""PyYAML>=3.12"",\n        ""tqdm>=4.26.1"",\n        ""kaldiio>=2.14.1"",\n        ""h5py>=2.10.0"",\n        ""yq>=2.10.0"",\n    ],\n    ""setup"": [\n        ""numpy"",\n        ""pytest-runner"",\n    ],\n    ""test"": [\n        ""pytest>=3.3.0"",\n        ""hacking>=1.1.0"",\n        ""flake8>=3.7.8"",\n        ""flake8-docstrings>=1.3.1"",\n    ]\n}\nentry_points = {\n    ""console_scripts"": [\n        ""parallel-wavegan-preprocess=parallel_wavegan.bin.preprocess:main"",\n        ""parallel-wavegan-compute-statistics=parallel_wavegan.bin.compute_statistics:main"",\n        ""parallel-wavegan-normalize=parallel_wavegan.bin.normalize:main"",\n        ""parallel-wavegan-train=parallel_wavegan.bin.train:main"",\n        ""parallel-wavegan-decode=parallel_wavegan.bin.decode:main"",\n    ]\n}\n\ninstall_requires = requirements[""install""]\nsetup_requires = requirements[""setup""]\ntests_require = requirements[""test""]\nextras_require = {k: v for k, v in requirements.items()\n                  if k not in [""install"", ""setup""]}\n\ndirname = os.path.dirname(__file__)\nsetup(name=""parallel_wavegan"",\n      version=""0.4.0"",\n      url=""http://github.com/kan-bayashi/ParallelWaveGAN"",\n      author=""Tomoki Hayashi"",\n      author_email=""hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp"",\n      description=""Parallel WaveGAN implementation"",\n      long_description=open(os.path.join(dirname, ""README.md""),\n                            encoding=""utf-8"").read(),\n      long_description_content_type=""text/markdown"",\n      license=""MIT License"",\n      packages=find_packages(include=[""parallel_wavegan*""]),\n      install_requires=install_requires,\n      setup_requires=setup_requires,\n      tests_require=tests_require,\n      extras_require=extras_require,\n      entry_points=entry_points,\n      classifiers=[\n          ""Programming Language :: Python :: 3.6"",\n          ""Programming Language :: Python :: 3.7"",\n          ""Intended Audience :: Science/Research"",\n          ""Operating System :: POSIX :: Linux"",\n          ""License :: OSI Approved :: MIT License"",\n          ""Topic :: Software Development :: Libraries :: Python Modules""],\n      )\n'"
parallel_wavegan/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\n__version__ = ""0.4.0""\n'"
test/test_layers.py,8,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\nimport logging\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom parallel_wavegan.layers import CausalConv1d\nfrom parallel_wavegan.layers import CausalConvTranspose1d\nfrom parallel_wavegan.layers import Conv1d\nfrom parallel_wavegan.layers import Conv1d1x1\nfrom parallel_wavegan.layers import Conv2d\nfrom parallel_wavegan.layers import ConvInUpsampleNetwork\nfrom parallel_wavegan.layers import PQMF\nfrom parallel_wavegan.layers import UpsampleNetwork\n\nlogging.basicConfig(\n    level=logging.WARN, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n\n\ndef test_conv_initialization():\n    conv = Conv1d(10, 10, 3, bias=True)\n    np.testing.assert_array_equal(conv.bias.data.numpy(),\n                                  np.zeros_like(conv.bias.data.numpy()))\n    conv1x1 = Conv1d1x1(10, 10, bias=True)\n    np.testing.assert_array_equal(conv1x1.bias.data.numpy(),\n                                  np.zeros_like(conv1x1.bias.data.numpy()))\n    kernel_size = (10, 10)\n    conv2d = Conv2d(10, 10, kernel_size, bias=True)\n    np.testing.assert_array_equal(conv2d.weight.data.numpy(),\n                                  np.ones_like(conv2d.weight.data.numpy()) / np.prod(kernel_size))\n    np.testing.assert_array_equal(conv2d.bias.data.numpy(),\n                                  np.zeros_like(conv2d.bias.data.numpy()))\n    kernel_size = (1, 10)\n    conv2d = Conv2d(10, 10, kernel_size, bias=True)\n    np.testing.assert_array_equal(conv2d.weight.data.numpy(),\n                                  np.ones_like(conv2d.weight.data.numpy()) / np.prod(kernel_size))\n    np.testing.assert_array_equal(conv2d.bias.data.numpy(),\n                                  np.zeros_like(conv2d.bias.data.numpy()))\n\n\n@pytest.mark.parametrize(\n    ""use_causal_conv"", [\n        (False),\n        (True),\n    ])\ndef test_upsample(use_causal_conv):\n    length = 10\n    scales = [4, 4]\n    x = torch.randn(1, 10, length)\n    upsample = UpsampleNetwork(scales)\n    y = upsample(x)\n    assert x.size(-1) * np.prod(scales) == y.size(-1)\n\n    for aux_context_window in [0, 1, 2, 3]:\n        conv_upsample = ConvInUpsampleNetwork(scales,\n                                              aux_channels=x.size(1),\n                                              aux_context_window=aux_context_window,\n                                              use_causal_conv=use_causal_conv)\n        y = conv_upsample(x)\n        assert (x.size(-1) - 2 * aux_context_window) * np.prod(scales) == y.size(-1)\n\n\n@torch.no_grad()\n@pytest.mark.parametrize(\n    ""kernel_size, dilation, pad, pad_params"", [\n        (3, 1, ""ConstantPad1d"", {""value"": 0.0}),\n        (3, 3, ""ConstantPad1d"", {""value"": 0.0}),\n        (2, 1, ""ConstantPad1d"", {""value"": 0.0}),\n        (2, 3, ""ConstantPad1d"", {""value"": 0.0}),\n        (5, 1, ""ConstantPad1d"", {""value"": 0.0}),\n        (5, 3, ""ConstantPad1d"", {""value"": 0.0}),\n        (3, 3, ""ReflectionPad1d"", {}),\n        (2, 1, ""ReflectionPad1d"", {}),\n        (2, 3, ""ReflectionPad1d"", {}),\n        (5, 1, ""ReflectionPad1d"", {}),\n        (5, 3, ""ReflectionPad1d"", {}),\n    ])\ndef test_causal_conv(kernel_size, dilation, pad, pad_params):\n    x = torch.randn(1, 1, 32)\n    conv = CausalConv1d(1, 1, kernel_size, dilation,\n                        pad=pad, pad_params=pad_params)\n    y1 = conv(x)\n    x[:, :, 16:] += torch.randn(1, 1, 16)\n    y2 = conv(x)\n    assert x.size(2) == y1.size(2)\n    np.testing.assert_array_equal(\n        y1[:, :, :16].cpu().numpy(),\n        y2[:, :, :16].cpu().numpy(),\n    )\n\n\n@torch.no_grad()\n@pytest.mark.parametrize(\n    ""kernel_size, stride"", [\n        (4, 2),\n        (6, 3),\n        (10, 5),\n    ])\ndef test_causal_conv_transpose(kernel_size, stride):\n    deconv = CausalConvTranspose1d(1, 1, kernel_size, stride)\n    x = torch.randn(1, 1, 32)\n    y1 = deconv(x)\n    x[:, :, 19:] += torch.randn(1, 1, 32 - 19)\n    y2 = deconv(x)\n    assert x.size(2) * stride == y1.size(2)\n    np.testing.assert_array_equal(\n        y1[:, :, :19 * stride].cpu().numpy(),\n        y2[:, :, :19 * stride].cpu().numpy(),\n    )\n\n\n@pytest.mark.parametrize(\n    ""subbands"", [\n        (3),\n        (4),\n    ])\ndef test_pqmf(subbands):\n    pqmf = PQMF(subbands)\n    x = torch.randn(1, 1, subbands * 32)\n    y = pqmf.analysis(x)\n    assert y.shape[2] * subbands == x.shape[2]\n    x_hat = pqmf.synthesis(y)\n    assert x.shape[2] == x_hat.shape[2]\n'"
test/test_melgan.py,10,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\nimport logging\n\nimport numpy as np\nimport pytest\nimport torch\nimport torch.nn.functional as F\n\nfrom parallel_wavegan.losses import MultiResolutionSTFTLoss\nfrom parallel_wavegan.models import MelGANGenerator\nfrom parallel_wavegan.models import MelGANMultiScaleDiscriminator\nfrom parallel_wavegan.models import ParallelWaveGANDiscriminator\nfrom parallel_wavegan.models import ResidualParallelWaveGANDiscriminator\nfrom parallel_wavegan.optimizers import RAdam\n\nfrom test_parallel_wavegan import make_discriminator_args\nfrom test_parallel_wavegan import make_mutli_reso_stft_loss_args\nfrom test_parallel_wavegan import make_residual_discriminator_args\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n\n\ndef make_melgan_generator_args(**kwargs):\n    defaults = dict(\n        in_channels=80,\n        out_channels=1,\n        kernel_size=7,\n        channels=512,\n        bias=True,\n        upsample_scales=[8, 8, 2, 2],\n        stack_kernel_size=3,\n        stacks=3,\n        nonlinear_activation=""LeakyReLU"",\n        nonlinear_activation_params={""negative_slope"": 0.2},\n        pad=""ReflectionPad1d"",\n        pad_params={},\n        use_final_nonlinear_activation=True,\n        use_weight_norm=True,\n        use_causal_conv=False,\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\ndef make_melgan_discriminator_args(**kwargs):\n    defaults = dict(\n        in_channels=1,\n        out_channels=1,\n        scales=3,\n        downsample_pooling=""AvgPool1d"",\n        # follow the official implementation setting\n        downsample_pooling_params={\n            ""kernel_size"": 4,\n            ""stride"": 2,\n            ""padding"": 1,\n            ""count_include_pad"": False,\n        },\n        kernel_sizes=[5, 3],\n        channels=16,\n        max_downsample_channels=1024,\n        bias=True,\n        downsample_scales=[4, 4, 4, 4],\n        nonlinear_activation=""LeakyReLU"",\n        nonlinear_activation_params={""negative_slope"": 0.2},\n        pad=""ReflectionPad1d"",\n        pad_params={},\n        use_weight_norm=True,\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\n@pytest.mark.parametrize(\n    ""dict_g, dict_d, dict_loss"", [\n        ({}, {}, {}),\n        ({""kernel_size"": 3}, {}, {}),\n        ({""channels"": 1024}, {}, {}),\n        ({""stack_kernel_size"": 5}, {}, {}),\n        ({""stack_kernel_size"": 5, ""stacks"": 2}, {}, {}),\n        ({""upsample_scales"": [4, 4, 4, 4]}, {}, {}),\n        ({""upsample_scales"": [8, 8, 2, 2, 2]}, {}, {}),\n        ({""channels"": 1024, ""upsample_scales"": [8, 8, 2, 2, 2, 2]}, {}, {}),\n        ({""pad"": ""ConstantPad1d"", ""pad_params"": {""value"": 0.0}}, {}, {}),\n        ({""nonlinear_activation"": ""ReLU"", ""nonlinear_activation_params"": {}}, {}, {}),\n        ({""bias"": False}, {}, {}),\n        ({""use_final_nonlinear_activation"": False}, {}, {}),\n        ({""use_weight_norm"": False}, {}, {}),\n        ({""use_causal_conv"": True}, {}, {}),\n    ])\ndef test_melgan_trainable(dict_g, dict_d, dict_loss):\n    # setup\n    batch_size = 4\n    batch_length = 4096\n    args_g = make_melgan_generator_args(**dict_g)\n    args_d = make_discriminator_args(**dict_d)\n    args_loss = make_mutli_reso_stft_loss_args(**dict_loss)\n    y = torch.randn(batch_size, 1, batch_length)\n    c = torch.randn(batch_size, args_g[""in_channels""],\n                    batch_length // np.prod(\n                        args_g[""upsample_scales""]))\n    model_g = MelGANGenerator(**args_g)\n    model_d = ParallelWaveGANDiscriminator(**args_d)\n    aux_criterion = MultiResolutionSTFTLoss(**args_loss)\n    optimizer_g = RAdam(model_g.parameters())\n    optimizer_d = RAdam(model_d.parameters())\n\n    # check generator trainable\n    y_hat = model_g(c)\n    p_hat = model_d(y_hat)\n    y, y_hat, p_hat = y.squeeze(1), y_hat.squeeze(1), p_hat.squeeze(1)\n    adv_loss = F.mse_loss(p_hat, p_hat.new_ones(p_hat.size()))\n    sc_loss, mag_loss = aux_criterion(y_hat, y)\n    aux_loss = sc_loss + mag_loss\n    loss_g = adv_loss + aux_loss\n    optimizer_g.zero_grad()\n    loss_g.backward()\n    optimizer_g.step()\n\n    # check discriminator trainable\n    y, y_hat = y.unsqueeze(1), y_hat.unsqueeze(1).detach()\n    p = model_d(y)\n    p_hat = model_d(y_hat)\n    p, p_hat = p.squeeze(1), p_hat.squeeze(1)\n    loss_d = F.mse_loss(p, p.new_ones(p.size())) + F.mse_loss(p_hat, p_hat.new_zeros(p_hat.size()))\n    optimizer_d.zero_grad()\n    loss_d.backward()\n    optimizer_d.step()\n\n\n@pytest.mark.parametrize(\n    ""dict_g, dict_d, dict_loss"", [\n        ({}, {}, {}),\n        ({""kernel_size"": 3}, {}, {}),\n        ({""channels"": 1024}, {}, {}),\n        ({""stack_kernel_size"": 5}, {}, {}),\n        ({""stack_kernel_size"": 5, ""stacks"": 2}, {}, {}),\n        ({""upsample_scales"": [4, 4, 4, 4]}, {}, {}),\n        ({""upsample_scales"": [8, 8, 2, 2, 2]}, {}, {}),\n        ({""channels"": 1024, ""upsample_scales"": [8, 8, 2, 2, 2, 2]}, {}, {}),\n        ({""pad"": ""ConstantPad1d"", ""pad_params"": {""value"": 0.0}}, {}, {}),\n        ({""nonlinear_activation"": ""ReLU"", ""nonlinear_activation_params"": {}}, {}, {}),\n        ({""bias"": False}, {}, {}),\n        ({""use_final_nonlinear_activation"": False}, {}, {}),\n        ({""use_weight_norm"": False}, {}, {}),\n    ])\ndef test_melgan_trainable_with_residual_discriminator(dict_g, dict_d, dict_loss):\n    # setup\n    batch_size = 4\n    batch_length = 4096\n    args_g = make_melgan_generator_args(**dict_g)\n    args_d = make_residual_discriminator_args(**dict_d)\n    args_loss = make_mutli_reso_stft_loss_args(**dict_loss)\n    y = torch.randn(batch_size, 1, batch_length)\n    c = torch.randn(batch_size, args_g[""in_channels""],\n                    batch_length // np.prod(\n                        args_g[""upsample_scales""]))\n    model_g = MelGANGenerator(**args_g)\n    model_d = ResidualParallelWaveGANDiscriminator(**args_d)\n    aux_criterion = MultiResolutionSTFTLoss(**args_loss)\n    optimizer_g = RAdam(model_g.parameters())\n    optimizer_d = RAdam(model_d.parameters())\n\n    # check generator trainable\n    y_hat = model_g(c)\n    p_hat = model_d(y_hat)\n    y, y_hat, p_hat = y.squeeze(1), y_hat.squeeze(1), p_hat.squeeze(1)\n    adv_loss = F.mse_loss(p_hat, p_hat.new_ones(p_hat.size()))\n    sc_loss, mag_loss = aux_criterion(y_hat, y)\n    aux_loss = sc_loss + mag_loss\n    loss_g = adv_loss + aux_loss\n    optimizer_g.zero_grad()\n    loss_g.backward()\n    optimizer_g.step()\n\n    # check discriminator trainable\n    y, y_hat = y.unsqueeze(1), y_hat.unsqueeze(1).detach()\n    p = model_d(y)\n    p_hat = model_d(y_hat)\n    p, p_hat = p.squeeze(1), p_hat.squeeze(1)\n    loss_d = F.mse_loss(p, p.new_ones(p.size())) + F.mse_loss(p_hat, p_hat.new_zeros(p_hat.size()))\n    optimizer_d.zero_grad()\n    loss_d.backward()\n    optimizer_d.step()\n\n\n@pytest.mark.parametrize(\n    ""dict_g, dict_d, dict_loss"", [\n        ({}, {}, {}),\n        ({}, {""scales"": 4}, {}),\n        ({}, {""kernel_sizes"": [7, 5]}, {}),\n        ({}, {""max_downsample_channels"": 128}, {}),\n        ({}, {""downsample_scales"": [4, 4]}, {}),\n        ({}, {""pad"": ""ConstantPad1d"", ""pad_params"": {""value"": 0.0}}, {}),\n        ({}, {""nonlinear_activation"": ""ReLU"", ""nonlinear_activation_params"": {}}, {}),\n    ])\ndef test_melgan_trainable_with_melgan_discriminator(dict_g, dict_d, dict_loss):\n    # setup\n    batch_size = 4\n    batch_length = 4096\n    args_g = make_melgan_generator_args(**dict_g)\n    args_d = make_melgan_discriminator_args(**dict_d)\n    args_loss = make_mutli_reso_stft_loss_args(**dict_loss)\n    y = torch.randn(batch_size, 1, batch_length)\n    c = torch.randn(batch_size, args_g[""in_channels""],\n                    batch_length // np.prod(\n                        args_g[""upsample_scales""]))\n    model_g = MelGANGenerator(**args_g)\n    model_d = MelGANMultiScaleDiscriminator(**args_d)\n    aux_criterion = MultiResolutionSTFTLoss(**args_loss)\n    optimizer_g = RAdam(model_g.parameters())\n    optimizer_d = RAdam(model_d.parameters())\n\n    # check generator trainable\n    y_hat = model_g(c)\n    p_hat = model_d(y_hat)\n    y, y_hat = y.squeeze(1), y_hat.squeeze(1)\n    sc_loss, mag_loss = aux_criterion(y_hat, y)\n    aux_loss = sc_loss + mag_loss\n    adv_loss = 0.0\n    for i in range(len(p_hat)):\n        adv_loss += F.mse_loss(\n            p_hat[i][-1], p_hat[i][-1].new_ones(p_hat[i][-1].size()))\n    adv_loss /= (i + 1)\n    with torch.no_grad():\n        p = model_d(y.unsqueeze(1))\n    fm_loss = 0.0\n    for i in range(len(p_hat)):\n        for j in range(len(p_hat[i]) - 1):\n            fm_loss += F.l1_loss(p_hat[i][j], p[i][j].detach())\n    fm_loss /= (i + 1) * j\n    loss_g = adv_loss + aux_loss + fm_loss\n    optimizer_g.zero_grad()\n    loss_g.backward()\n    optimizer_g.step()\n\n    # check discriminator trainable\n    y, y_hat = y.unsqueeze(1), y_hat.unsqueeze(1).detach()\n    p = model_d(y)\n    p_hat = model_d(y_hat)\n    real_loss = 0.0\n    fake_loss = 0.0\n    for i in range(len(p)):\n        real_loss += F.mse_loss(\n            p[i][-1], p[i][-1].new_ones(p[i][-1].size()))\n        fake_loss += F.mse_loss(\n            p_hat[i][-1], p_hat[i][-1].new_zeros(p_hat[i][-1].size()))\n    real_loss /= (i + 1)\n    fake_loss /= (i + 1)\n    loss_d = real_loss + fake_loss\n    optimizer_d.zero_grad()\n    loss_d.backward()\n    optimizer_d.step()\n\n\n@pytest.mark.parametrize(\n    ""dict_g"", [\n        ({""use_causal_conv"": True}),\n        ({""use_causal_conv"": True, ""upsample_scales"": [4, 4, 2, 2]}),\n        ({""use_causal_conv"": True, ""upsample_scales"": [4, 5, 4, 3]}),\n    ])\ndef test_causal_melgan(dict_g):\n    batch_size = 4\n    batch_length = 4096\n    args_g = make_melgan_generator_args(**dict_g)\n    upsampling_factor = np.prod(args_g[""upsample_scales""])\n    c = torch.randn(batch_size, args_g[""in_channels""],\n                    batch_length // upsampling_factor)\n    model_g = MelGANGenerator(**args_g)\n    c_ = c.clone()\n    c_[..., c.size(-1) // 2:] = torch.randn(c[..., c.size(-1) // 2:].shape)\n    try:\n        # check not equal\n        np.testing.assert_array_equal(c.numpy(), c_.numpy())\n    except AssertionError:\n        pass\n    else:\n        raise AssertionError(""Must be different."")\n\n    # check causality\n    y = model_g(c)\n    y_ = model_g(c_)\n    assert y.size(2) == c.size(2) * upsampling_factor\n    np.testing.assert_array_equal(\n        y[..., :c.size(-1) // 2 * upsampling_factor].detach().cpu().numpy(),\n        y_[..., :c_.size(-1) // 2 * upsampling_factor].detach().cpu().numpy(),\n    )\n'"
test/test_parallel_wavegan.py,13,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\nimport logging\n\nimport numpy as np\nimport pytest\nimport torch\nimport torch.nn.functional as F\n\nfrom parallel_wavegan.losses import MultiResolutionSTFTLoss\nfrom parallel_wavegan.models import ParallelWaveGANDiscriminator\nfrom parallel_wavegan.models import ParallelWaveGANGenerator\nfrom parallel_wavegan.models import ResidualParallelWaveGANDiscriminator\nfrom parallel_wavegan.optimizers import RAdam\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n\n\ndef make_generator_args(**kwargs):\n    defaults = dict(\n        in_channels=1,\n        out_channels=1,\n        kernel_size=3,\n        layers=6,\n        stacks=3,\n        residual_channels=8,\n        gate_channels=16,\n        skip_channels=8,\n        aux_channels=10,\n        aux_context_window=0,\n        dropout=1 - 0.95,\n        use_weight_norm=True,\n        use_causal_conv=False,\n        upsample_conditional_features=True,\n        upsample_net=""ConvInUpsampleNetwork"",\n        upsample_params={""upsample_scales"": [4, 4]},\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\ndef make_discriminator_args(**kwargs):\n    defaults = dict(\n        in_channels=1,\n        out_channels=1,\n        kernel_size=3,\n        layers=5,\n        conv_channels=16,\n        nonlinear_activation=""LeakyReLU"",\n        nonlinear_activation_params={""negative_slope"": 0.2},\n        bias=True,\n        use_weight_norm=True,\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\ndef make_residual_discriminator_args(**kwargs):\n    defaults = dict(\n        in_channels=1,\n        out_channels=1,\n        kernel_size=3,\n        layers=10,\n        stacks=1,\n        residual_channels=8,\n        gate_channels=16,\n        skip_channels=8,\n        dropout=0.0,\n        use_weight_norm=True,\n        use_causal_conv=False,\n        nonlinear_activation_params={""negative_slope"": 0.2},\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\ndef make_mutli_reso_stft_loss_args(**kwargs):\n    defaults = dict(\n        fft_sizes=[64, 128, 256],\n        hop_sizes=[32, 64, 128],\n        win_lengths=[48, 96, 192],\n        window=\'hann_window\',\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\n@pytest.mark.parametrize(\n    ""dict_g, dict_d, dict_loss"", [\n        ({}, {}, {}),\n        ({""layers"": 1, ""stacks"": 1}, {}, {}),\n        ({}, {""layers"": 1}, {}),\n        ({""kernel_size"": 5}, {}, {}),\n        ({}, {""kernel_size"": 5}, {}),\n        ({""gate_channels"": 8}, {}, {}),\n        ({""stacks"": 1}, {}, {}),\n        ({""use_weight_norm"": False}, {""use_weight_norm"": False}, {}),\n        ({""aux_context_window"": 2}, {}, {}),\n        ({""upsample_net"": ""UpsampleNetwork""}, {}, {}),\n        ({""upsample_params"": {""upsample_scales"": [4], ""freq_axis_kernel_size"": 3}}, {}, {}),\n        ({""upsample_params"": {""upsample_scales"": [4], ""nonlinear_activation"": ""ReLU""}}, {}, {}),\n        ({""upsample_conditional_features"": False, ""upsample_params"": {""upsample_scales"": [1]}}, {}, {}),\n        ({}, {""nonlinear_activation"": ""ReLU"", ""nonlinear_activation_params"": {}}, {}),\n        ({""use_causal_conv"": True}, {}, {}),\n        ({""use_causal_conv"": True, ""upsample_net"": ""UpsampleNetwork""}, {}, {}),\n        ({""use_causal_conv"": True, ""aux_context_window"": 1}, {}, {}),\n        ({""use_causal_conv"": True, ""aux_context_window"": 2}, {}, {}),\n        ({""use_causal_conv"": True, ""aux_context_window"": 3}, {}, {}),\n        ({""aux_channels"": 16, ""upsample_net"": ""MelGANGenerator"", ""upsample_params"": {\n            ""upsample_scales"": [4, 4], ""in_channels"": 16, ""out_channels"": 16}}, {}, {}),\n    ])\ndef test_parallel_wavegan_trainable(dict_g, dict_d, dict_loss):\n    # setup\n    batch_size = 4\n    batch_length = 4096\n    args_g = make_generator_args(**dict_g)\n    args_d = make_discriminator_args(**dict_d)\n    args_loss = make_mutli_reso_stft_loss_args(**dict_loss)\n    z = torch.randn(batch_size, 1, batch_length)\n    y = torch.randn(batch_size, 1, batch_length)\n    c = torch.randn(batch_size, args_g[""aux_channels""],\n                    batch_length // np.prod(\n                        args_g[""upsample_params""][""upsample_scales""]) + 2 * args_g[""aux_context_window""])\n    model_g = ParallelWaveGANGenerator(**args_g)\n    model_d = ParallelWaveGANDiscriminator(**args_d)\n    aux_criterion = MultiResolutionSTFTLoss(**args_loss)\n    optimizer_g = RAdam(model_g.parameters())\n    optimizer_d = RAdam(model_d.parameters())\n\n    # check generator trainable\n    y_hat = model_g(z, c)\n    p_hat = model_d(y_hat)\n    y, y_hat, p_hat = y.squeeze(1), y_hat.squeeze(1), p_hat.squeeze(1)\n    adv_loss = F.mse_loss(p_hat, p_hat.new_ones(p_hat.size()))\n    sc_loss, mag_loss = aux_criterion(y_hat, y)\n    aux_loss = sc_loss + mag_loss\n    loss_g = adv_loss + aux_loss\n    optimizer_g.zero_grad()\n    loss_g.backward()\n    optimizer_g.step()\n\n    # check discriminator trainable\n    y, y_hat = y.unsqueeze(1), y_hat.unsqueeze(1).detach()\n    p = model_d(y)\n    p_hat = model_d(y_hat)\n    p, p_hat = p.squeeze(1), p_hat.squeeze(1)\n    loss_d = F.mse_loss(p, p.new_ones(p.size())) + F.mse_loss(p_hat, p_hat.new_zeros(p_hat.size()))\n    optimizer_d.zero_grad()\n    loss_d.backward()\n    optimizer_d.step()\n\n\n@pytest.mark.parametrize(\n    ""dict_g, dict_d, dict_loss"", [\n        ({}, {}, {}),\n        ({""layers"": 1, ""stacks"": 1}, {}, {}),\n        ({}, {""layers"": 1}, {}),\n        ({""kernel_size"": 5}, {}, {}),\n        ({}, {""kernel_size"": 5}, {}),\n        ({""gate_channels"": 8}, {}, {}),\n        ({""stacks"": 1}, {}, {}),\n        ({""use_weight_norm"": False}, {""use_weight_norm"": False}, {}),\n        ({""aux_context_window"": 2}, {}, {}),\n        ({""upsample_net"": ""UpsampleNetwork""}, {}, {}),\n        ({""upsample_params"": {""upsample_scales"": [4], ""freq_axis_kernel_size"": 3}}, {}, {}),\n        ({""upsample_params"": {""upsample_scales"": [4], ""nonlinear_activation"": ""ReLU""}}, {}, {}),\n        ({""upsample_conditional_features"": False, ""upsample_params"": {""upsample_scales"": [1]}}, {}, {}),\n        ({}, {""nonlinear_activation"": ""ReLU"", ""nonlinear_activation_params"": {}}, {}),\n        ({""use_causal_conv"": True}, {}, {}),\n        ({""use_causal_conv"": True, ""upsample_net"": ""UpsampleNetwork""}, {}, {}),\n        ({""use_causal_conv"": True, ""aux_context_window"": 1}, {}, {}),\n        ({""use_causal_conv"": True, ""aux_context_window"": 2}, {}, {}),\n        ({""use_causal_conv"": True, ""aux_context_window"": 3}, {}, {}),\n        ({""aux_channels"": 16, ""upsample_net"": ""MelGANGenerator"", ""upsample_params"": {\n            ""upsample_scales"": [4, 4], ""in_channels"": 16, ""out_channels"": 16}}, {}, {}),\n    ])\ndef test_parallel_wavegan_with_residual_discriminator_trainable(dict_g, dict_d, dict_loss):\n    # setup\n    batch_size = 4\n    batch_length = 4096\n    args_g = make_generator_args(**dict_g)\n    args_d = make_residual_discriminator_args(**dict_d)\n    args_loss = make_mutli_reso_stft_loss_args(**dict_loss)\n    z = torch.randn(batch_size, 1, batch_length)\n    y = torch.randn(batch_size, 1, batch_length)\n    c = torch.randn(batch_size, args_g[""aux_channels""],\n                    batch_length // np.prod(\n                        args_g[""upsample_params""][""upsample_scales""]) + 2 * args_g[""aux_context_window""])\n    model_g = ParallelWaveGANGenerator(**args_g)\n    model_d = ResidualParallelWaveGANDiscriminator(**args_d)\n    aux_criterion = MultiResolutionSTFTLoss(**args_loss)\n    optimizer_g = RAdam(model_g.parameters())\n    optimizer_d = RAdam(model_d.parameters())\n\n    # check generator trainable\n    y_hat = model_g(z, c)\n    p_hat = model_d(y_hat)\n    y, y_hat, p_hat = y.squeeze(1), y_hat.squeeze(1), p_hat.squeeze(1)\n    adv_loss = F.mse_loss(p_hat, p_hat.new_ones(p_hat.size()))\n    sc_loss, mag_loss = aux_criterion(y_hat, y)\n    aux_loss = sc_loss + mag_loss\n    loss_g = adv_loss + aux_loss\n    optimizer_g.zero_grad()\n    loss_g.backward()\n    optimizer_g.step()\n\n    # check discriminator trainable\n    y, y_hat = y.unsqueeze(1), y_hat.unsqueeze(1).detach()\n    p = model_d(y)\n    p_hat = model_d(y_hat)\n    p, p_hat = p.squeeze(1), p_hat.squeeze(1)\n    loss_d = F.mse_loss(p, p.new_ones(p.size())) + F.mse_loss(p_hat, p_hat.new_zeros(p_hat.size()))\n    optimizer_d.zero_grad()\n    loss_d.backward()\n    optimizer_d.step()\n\n\n@pytest.mark.parametrize(\n    ""upsample_net, aux_context_window"", [\n        (""ConvInUpsampleNetwork"", 0),\n        (""ConvInUpsampleNetwork"", 1),\n        (""ConvInUpsampleNetwork"", 2),\n        (""ConvInUpsampleNetwork"", 3),\n        (""UpsampleNetwork"", 0),\n    ])\ndef test_causal_parallel_wavegan(upsample_net, aux_context_window):\n    batch_size = 1\n    batch_length = 4096\n    args_g = make_generator_args(use_causal_conv=True,\n                                 upsample_net=upsample_net,\n                                 aux_context_window=aux_context_window,\n                                 dropout=0.0)\n    model_g = ParallelWaveGANGenerator(**args_g)\n    z = torch.randn(batch_size, 1, batch_length)\n    c = torch.randn(batch_size, args_g[""aux_channels""],\n                    batch_length // np.prod(args_g[""upsample_params""][""upsample_scales""]))\n\n    z_ = z.clone()\n    c_ = c.clone()\n    z_[..., z.size(-1) // 2:] = torch.randn(z[..., z.size(-1) // 2:].shape)\n    c_[..., c.size(-1) // 2:] = torch.randn(c[..., c.size(-1) // 2:].shape)\n    c = torch.nn.ConstantPad1d(args_g[""aux_context_window""], 0.0)(c)\n    c_ = torch.nn.ConstantPad1d(args_g[""aux_context_window""], 0.0)(c_)\n    try:\n        # check not equal\n        np.testing.assert_array_equal(c.numpy(), c_.numpy())\n    except AssertionError:\n        pass\n    else:\n        raise AssertionError(""Must be different."")\n    try:\n        # check not equal\n        np.testing.assert_array_equal(z.numpy(), z_.numpy())\n    except AssertionError:\n        pass\n    else:\n        raise AssertionError(""Must be different."")\n\n    # check causality\n    y = model_g(z, c)\n    y_ = model_g(z_, c_)\n    np.testing.assert_array_equal(\n        y[..., :y.size(-1) // 2].detach().cpu().numpy(),\n        y_[..., :y_.size(-1) // 2].detach().cpu().numpy(),\n    )\n'"
parallel_wavegan/bin/__init__.py,0,b''
parallel_wavegan/bin/compute_statistics.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Calculate statistics of feature files.""""""\n\nimport argparse\nimport logging\nimport os\n\nimport numpy as np\nimport yaml\n\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\n\nfrom parallel_wavegan.datasets import MelDataset\nfrom parallel_wavegan.datasets import MelSCPDataset\nfrom parallel_wavegan.utils import read_hdf5\nfrom parallel_wavegan.utils import write_hdf5\n\n\ndef main():\n    """"""Run preprocessing process.""""""\n    parser = argparse.ArgumentParser(\n        description=""Compute mean and variance of dumped raw features ""\n                    ""(See detail in parallel_wavegan/bin/compute_statistics.py)."")\n    parser.add_argument(""--feats-scp"", ""--scp"", default=None, type=str,\n                        help=""kaldi-style feats.scp file. ""\n                             ""you need to specify either feats-scp or rootdir."")\n    parser.add_argument(""--rootdir"", type=str, required=True,\n                        help=""directory including feature files. ""\n                             ""you need to specify either feats-scp or rootdir."")\n    parser.add_argument(""--config"", type=str, required=True,\n                        help=""yaml format configuration file."")\n    parser.add_argument(""--dumpdir"", default=None, type=str,\n                        help=""directory to save statistics. if not provided, ""\n                             ""stats will be saved in the above root directory. (default=None)"")\n    parser.add_argument(""--verbose"", type=int, default=1,\n                        help=""logging level. higher is more logging. (default=1)"")\n    args = parser.parse_args()\n\n    # set logger\n    if args.verbose > 1:\n        logging.basicConfig(\n            level=logging.DEBUG, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n    elif args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n    else:\n        logging.basicConfig(\n            level=logging.WARN, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n        logging.warning(\'Skip DEBUG/INFO messages\')\n\n    # load config\n    with open(args.config) as f:\n        config = yaml.load(f, Loader=yaml.Loader)\n    config.update(vars(args))\n\n    # check arguments\n    if (args.feats_scp is not None and args.rootdir is not None) or \\\n            (args.feats_scp is None and args.rootdir is None):\n        raise ValueError(""Please specify either --rootdir or --feats-scp."")\n\n    # check directory existence\n    if args.dumpdir is None:\n        args.dumpdir = os.path.dirname(args.rootdir)\n    if not os.path.exists(args.dumpdir):\n        os.makedirs(args.dumpdir)\n\n    # get dataset\n    if args.feats_scp is None:\n        if config[""format""] == ""hdf5"":\n            mel_query = ""*.h5""\n            mel_load_fn = lambda x: read_hdf5(x, ""feats"")  # NOQA\n        elif config[""format""] == ""npy"":\n            mel_query = ""*-feats.npy""\n            mel_load_fn = np.load\n        else:\n            raise ValueError(""support only hdf5 or npy format."")\n        dataset = MelDataset(\n            args.rootdir,\n            mel_query=mel_query,\n            mel_load_fn=mel_load_fn)\n    else:\n        dataset = MelSCPDataset(args.feats_scp)\n    logging.info(f""The number of files = {len(dataset)}."")\n\n    # calculate statistics\n    scaler = StandardScaler()\n    for mel in tqdm(dataset):\n        scaler.partial_fit(mel)\n\n    if config[""format""] == ""hdf5"":\n        write_hdf5(os.path.join(args.dumpdir, ""stats.h5""), ""mean"", scaler.mean_.astype(np.float32))\n        write_hdf5(os.path.join(args.dumpdir, ""stats.h5""), ""scale"", scaler.scale_.astype(np.float32))\n    else:\n        stats = np.stack([scaler.mean_, scaler.scale_], axis=0)\n        np.save(os.path.join(args.dumpdir, ""stats.npy""), stats.astype(np.float32), allow_pickle=False)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
parallel_wavegan/bin/decode.py,8,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Decode with trained Parallel WaveGAN Generator.""""""\n\nimport argparse\nimport logging\nimport os\nimport time\n\nimport numpy as np\nimport soundfile as sf\nimport torch\nimport yaml\n\nfrom tqdm import tqdm\n\nimport parallel_wavegan.models\n\nfrom parallel_wavegan.datasets import MelDataset\nfrom parallel_wavegan.datasets import MelSCPDataset\nfrom parallel_wavegan.layers import PQMF\nfrom parallel_wavegan.utils import read_hdf5\n\n\ndef main():\n    """"""Run decoding process.""""""\n    parser = argparse.ArgumentParser(\n        description=""Decode dumped features with trained Parallel WaveGAN Generator ""\n                    ""(See detail in parallel_wavegan/bin/decode.py)."")\n    parser.add_argument(""--feats-scp"", ""--scp"", default=None, type=str,\n                        help=""kaldi-style feats.scp file. ""\n                             ""you need to specify either feats-scp or dumpdir."")\n    parser.add_argument(""--dumpdir"", default=None, type=str,\n                        help=""directory including feature files. ""\n                             ""you need to specify either feats-scp or dumpdir."")\n    parser.add_argument(""--outdir"", type=str, required=True,\n                        help=""directory to save generated speech."")\n    parser.add_argument(""--checkpoint"", type=str, required=True,\n                        help=""checkpoint file to be loaded."")\n    parser.add_argument(""--config"", default=None, type=str,\n                        help=""yaml format configuration file. if not explicitly provided, ""\n                             ""it will be searched in the checkpoint directory. (default=None)"")\n    parser.add_argument(""--verbose"", type=int, default=1,\n                        help=""logging level. higher is more logging. (default=1)"")\n    args = parser.parse_args()\n\n    # set logger\n    if args.verbose > 1:\n        logging.basicConfig(\n            level=logging.DEBUG, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n    elif args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n    else:\n        logging.basicConfig(\n            level=logging.WARN, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # check directory existence\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n\n    # load config\n    if args.config is None:\n        dirname = os.path.dirname(args.checkpoint)\n        args.config = os.path.join(dirname, ""config.yml"")\n    with open(args.config) as f:\n        config = yaml.load(f, Loader=yaml.Loader)\n    config.update(vars(args))\n\n    # check arguments\n    if (args.feats_scp is not None and args.dumpdir is not None) or \\\n            (args.feats_scp is None and args.dumpdir is None):\n        raise ValueError(""Please specify either --dumpdir or --feats-scp."")\n\n    # get dataset\n    if args.dumpdir is not None:\n        if config[""format""] == ""hdf5"":\n            mel_query = ""*.h5""\n            mel_load_fn = lambda x: read_hdf5(x, ""feats"")  # NOQA\n        elif config[""format""] == ""npy"":\n            mel_query = ""*-feats.npy""\n            mel_load_fn = np.load\n        else:\n            raise ValueError(""support only hdf5 or npy format."")\n        dataset = MelDataset(\n            args.dumpdir,\n            mel_query=mel_query,\n            mel_load_fn=mel_load_fn,\n            return_utt_id=True,\n        )\n    else:\n        dataset = MelSCPDataset(\n            feats_scp=args.feats_scp,\n            return_utt_id=True,\n        )\n    logging.info(f""The number of features to be decoded = {len(dataset)}."")\n\n    # setup\n    if torch.cuda.is_available():\n        device = torch.device(""cuda"")\n    else:\n        device = torch.device(""cpu"")\n    model_class = getattr(\n        parallel_wavegan.models,\n        config.get(""generator_type"", ""ParallelWaveGANGenerator""))\n    model = model_class(**config[""generator_params""])\n    model.load_state_dict(\n        torch.load(args.checkpoint, map_location=""cpu"")[""model""][""generator""])\n    logging.info(f""Loaded model parameters from {args.checkpoint}."")\n    model.remove_weight_norm()\n    model = model.eval().to(device)\n    use_noise_input = not isinstance(\n        model, parallel_wavegan.models.MelGANGenerator)\n    pad_fn = torch.nn.ReplicationPad1d(\n        config[""generator_params""].get(""aux_context_window"", 0))\n    if config[""generator_params""][""out_channels""] > 1:\n        pqmf = PQMF(config[""generator_params""][""out_channels""]).to(device)\n\n    # start generation\n    total_rtf = 0.0\n    with torch.no_grad(), tqdm(dataset, desc=""[decode]"") as pbar:\n        for idx, (utt_id, c) in enumerate(pbar, 1):\n            # setup input\n            x = ()\n            if use_noise_input:\n                z = torch.randn(1, 1, len(c) * config[""hop_size""]).to(device)\n                x += (z,)\n            c = pad_fn(torch.tensor(c, dtype=torch.float).unsqueeze(0).transpose(2, 1)).to(device)\n            x += (c,)\n\n            # generate\n            start = time.time()\n            if config[""generator_params""][""out_channels""] == 1:\n                y = model(*x).view(-1).cpu().numpy()\n            else:\n                y = pqmf.synthesis(model(*x)).view(-1).cpu().numpy()\n            rtf = (time.time() - start) / (len(y) / config[""sampling_rate""])\n            pbar.set_postfix({""RTF"": rtf})\n            total_rtf += rtf\n\n            # save as PCM 16 bit wav file\n            sf.write(os.path.join(config[""outdir""], f""{utt_id}_gen.wav""),\n                     y, config[""sampling_rate""], ""PCM_16"")\n\n    # report average RTF\n    logging.info(f""Finished generation of {idx} utterances (RTF = {total_rtf / idx:.03f})."")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
parallel_wavegan/bin/normalize.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Normalize feature files and dump them.""""""\n\nimport argparse\nimport logging\nimport os\n\nimport numpy as np\nimport yaml\n\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\n\nfrom parallel_wavegan.datasets import AudioMelDataset\nfrom parallel_wavegan.datasets import AudioMelSCPDataset\nfrom parallel_wavegan.datasets import MelDataset\nfrom parallel_wavegan.datasets import MelSCPDataset\nfrom parallel_wavegan.utils import read_hdf5\nfrom parallel_wavegan.utils import write_hdf5\n\n\ndef main():\n    """"""Run preprocessing process.""""""\n    parser = argparse.ArgumentParser(\n        description=""Normalize dumped raw features (See detail in parallel_wavegan/bin/normalize.py)."")\n    parser.add_argument(""--rootdir"", default=None, type=str,\n                        help=""directory including feature files to be normalized. ""\n                             ""you need to specify either *-scp or rootdir."")\n    parser.add_argument(""--wav-scp"", default=None, type=str,\n                        help=""kaldi-style wav.scp file. ""\n                             ""you need to specify either *-scp or rootdir."")\n    parser.add_argument(""--feats-scp"", default=None, type=str,\n                        help=""kaldi-style feats.scp file. ""\n                             ""you need to specify either *-scp or rootdir."")\n    parser.add_argument(""--segments"", default=None, type=str,\n                        help=""kaldi-style segments file."")\n    parser.add_argument(""--dumpdir"", type=str, required=True,\n                        help=""directory to dump normalized feature files."")\n    parser.add_argument(""--stats"", type=str, required=True,\n                        help=""statistics file."")\n    parser.add_argument(""--skip-wav-copy"", default=False, action=""store_true"",\n                        help=""whether to skip the copy of wav files."")\n    parser.add_argument(""--config"", type=str, required=True,\n                        help=""yaml format configuration file."")\n    parser.add_argument(""--verbose"", type=int, default=1,\n                        help=""logging level. higher is more logging. (default=1)"")\n    args = parser.parse_args()\n\n    # set logger\n    if args.verbose > 1:\n        logging.basicConfig(\n            level=logging.DEBUG, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n    elif args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n    else:\n        logging.basicConfig(\n            level=logging.WARN, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n        logging.warning(\'Skip DEBUG/INFO messages\')\n\n    # load config\n    with open(args.config) as f:\n        config = yaml.load(f, Loader=yaml.Loader)\n    config.update(vars(args))\n\n    # check arguments\n    if (args.feats_scp is not None and args.rootdir is not None) or \\\n            (args.feats_scp is None and args.rootdir is None):\n        raise ValueError(""Please specify either --rootdir or --feats-scp."")\n\n    # check directory existence\n    if not os.path.exists(args.dumpdir):\n        os.makedirs(args.dumpdir)\n\n    # get dataset\n    if args.rootdir is not None:\n        if config[""format""] == ""hdf5"":\n            audio_query, mel_query = ""*.h5"", ""*.h5""\n            audio_load_fn = lambda x: read_hdf5(x, ""wave"")  # NOQA\n            mel_load_fn = lambda x: read_hdf5(x, ""feats"")  # NOQA\n        elif config[""format""] == ""npy"":\n            audio_query, mel_query = ""*-wave.npy"", ""*-feats.npy""\n            audio_load_fn = np.load\n            mel_load_fn = np.load\n        else:\n            raise ValueError(""support only hdf5 or npy format."")\n        if not args.skip_wav_copy:\n            dataset = AudioMelDataset(\n                root_dir=args.rootdir,\n                audio_query=audio_query,\n                mel_query=mel_query,\n                audio_load_fn=audio_load_fn,\n                mel_load_fn=mel_load_fn,\n                return_utt_id=True,\n            )\n        else:\n            dataset = MelDataset(\n                root_dir=args.rootdir,\n                mel_query=mel_query,\n                mel_load_fn=mel_load_fn,\n                return_utt_id=True,\n            )\n    else:\n        if not args.skip_wav_copy:\n            dataset = AudioMelSCPDataset(\n                wav_scp=args.wav_scp,\n                feats_scp=args.feats_scp,\n                segments=args.segments,\n                return_utt_id=True,\n            )\n        else:\n            dataset = MelSCPDataset(\n                feats_scp=args.feats_scp,\n                return_utt_id=True,\n            )\n    logging.info(f""The number of files = {len(dataset)}."")\n\n    # restore scaler\n    scaler = StandardScaler()\n    if config[""format""] == ""hdf5"":\n        scaler.mean_ = read_hdf5(args.stats, ""mean"")\n        scaler.scale_ = read_hdf5(args.stats, ""scale"")\n    elif config[""format""] == ""npy"":\n        scaler.mean_ = np.load(args.stats)[0]\n        scaler.scale_ = np.load(args.stats)[1]\n    else:\n        raise ValueError(""support only hdf5 or npy format."")\n    # from version 0.23.0, this information is needed\n    scaler.n_features_in_ = scaler.mean_.shape[0]\n\n    # process each file\n    for items in tqdm(dataset):\n        if not args.skip_wav_copy:\n            utt_id, audio, mel = items\n        else:\n            utt_id, mel = items\n\n        # normalize\n        mel = scaler.transform(mel)\n\n        # save\n        if config[""format""] == ""hdf5"":\n            write_hdf5(os.path.join(args.dumpdir, f""{utt_id}.h5""),\n                       ""feats"", mel.astype(np.float32))\n            if not args.skip_wav_copy:\n                write_hdf5(os.path.join(args.dumpdir, f""{utt_id}.h5""),\n                           ""wave"", audio.astype(np.float32))\n        elif config[""format""] == ""npy"":\n            np.save(os.path.join(args.dumpdir, f""{utt_id}-feats.npy""),\n                    mel.astype(np.float32), allow_pickle=False)\n            if not args.skip_wav_copy:\n                np.save(os.path.join(args.dumpdir, f""{utt_id}-wave.npy""),\n                        audio.astype(np.float32), allow_pickle=False)\n        else:\n            raise ValueError(""support only hdf5 or npy format."")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
parallel_wavegan/bin/preprocess.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Perform preprocessing and raw feature extraction.""""""\n\nimport argparse\nimport logging\nimport os\n\nimport librosa\nimport numpy as np\nimport soundfile as sf\nimport yaml\n\nfrom tqdm import tqdm\n\nfrom parallel_wavegan.datasets import AudioDataset\nfrom parallel_wavegan.datasets import AudioSCPDataset\nfrom parallel_wavegan.utils import write_hdf5\n\n\ndef logmelfilterbank(audio,\n                     sampling_rate,\n                     fft_size=1024,\n                     hop_size=256,\n                     win_length=None,\n                     window=""hann"",\n                     num_mels=80,\n                     fmin=None,\n                     fmax=None,\n                     eps=1e-10,\n                     ):\n    """"""Compute log-Mel filterbank feature.\n\n    Args:\n        audio (ndarray): Audio signal (T,).\n        sampling_rate (int): Sampling rate.\n        fft_size (int): FFT size.\n        hop_size (int): Hop size.\n        win_length (int): Window length. If set to None, it will be the same as fft_size.\n        window (str): Window function type.\n        num_mels (int): Number of mel basis.\n        fmin (int): Minimum frequency in mel basis calculation.\n        fmax (int): Maximum frequency in mel basis calculation.\n        eps (float): Epsilon value to avoid inf in log calculation.\n\n    Returns:\n        ndarray: Log Mel filterbank feature (#frames, num_mels).\n\n    """"""\n    # get amplitude spectrogram\n    x_stft = librosa.stft(audio, n_fft=fft_size, hop_length=hop_size,\n                          win_length=win_length, window=window, pad_mode=""reflect"")\n    spc = np.abs(x_stft).T  # (#frames, #bins)\n\n    # get mel basis\n    fmin = 0 if fmin is None else fmin\n    fmax = sampling_rate / 2 if fmax is None else fmax\n    mel_basis = librosa.filters.mel(sampling_rate, fft_size, num_mels, fmin, fmax)\n\n    return np.log10(np.maximum(eps, np.dot(spc, mel_basis.T)))\n\n\ndef main():\n    """"""Run preprocessing process.""""""\n    parser = argparse.ArgumentParser(\n        description=""Preprocess audio and then extract features (See detail in parallel_wavegan/bin/preprocess.py)."")\n    parser.add_argument(""--wav-scp"", ""--scp"", default=None, type=str,\n                        help=""kaldi-style wav.scp file. you need to specify either scp or rootdir."")\n    parser.add_argument(""--segments"", default=None, type=str,\n                        help=""kaldi-style segments file. if use, you must to specify both scp and segments."")\n    parser.add_argument(""--rootdir"", default=None, type=str,\n                        help=""directory including wav files. you need to specify either scp or rootdir."")\n    parser.add_argument(""--dumpdir"", type=str, required=True,\n                        help=""directory to dump feature files."")\n    parser.add_argument(""--config"", type=str, required=True,\n                        help=""yaml format configuration file."")\n    parser.add_argument(""--verbose"", type=int, default=1,\n                        help=""logging level. higher is more logging. (default=1)"")\n    args = parser.parse_args()\n\n    # set logger\n    if args.verbose > 1:\n        logging.basicConfig(\n            level=logging.DEBUG, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n    elif args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n    else:\n        logging.basicConfig(\n            level=logging.WARN, format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n        logging.warning(\'Skip DEBUG/INFO messages\')\n\n    # load config\n    with open(args.config) as f:\n        config = yaml.load(f, Loader=yaml.Loader)\n    config.update(vars(args))\n\n    # check arguments\n    if (args.wav_scp is not None and args.rootdir is not None) or \\\n            (args.wav_scp is None and args.rootdir is None):\n        raise ValueError(""Please specify either --rootdir or --wav-scp."")\n\n    # get dataset\n    if args.rootdir is not None:\n        dataset = AudioDataset(\n            args.rootdir, ""*.wav"",\n            audio_load_fn=sf.read,\n            return_utt_id=True,\n        )\n    else:\n        dataset = AudioSCPDataset(\n            args.wav_scp,\n            segments=args.segments,\n            return_utt_id=True,\n            return_sampling_rate=True,\n        )\n\n    # check directly existence\n    if not os.path.exists(args.dumpdir):\n        os.makedirs(args.dumpdir, exist_ok=True)\n\n    # process each data\n    for utt_id, (audio, fs) in tqdm(dataset):\n        # check\n        assert len(audio.shape) == 1, \\\n            f""{utt_id} seems to be multi-channel signal.""\n        assert np.abs(audio).max() <= 1.0, \\\n            f""{utt_id} seems to be different from 16 bit PCM.""\n        assert fs == config[""sampling_rate""], \\\n            f""{utt_id} seems to have a different sampling rate.""\n\n        # trim silence\n        if config[""trim_silence""]:\n            audio, _ = librosa.effects.trim(audio,\n                                            top_db=config[""trim_threshold_in_db""],\n                                            frame_length=config[""trim_frame_size""],\n                                            hop_length=config[""trim_hop_size""])\n\n        if ""sampling_rate_for_feats"" not in config:\n            x = audio\n            sampling_rate = config[""sampling_rate""]\n            hop_size = config[""hop_size""]\n        else:\n            # NOTE(kan-bayashi): this procedure enables to train the model with different\n            #   sampling rate for feature and audio, e.g., training with mel extracted\n            #   using 16 kHz audio and 24 kHz audio as a target waveform\n            x = librosa.resample(audio, fs, config[""sampling_rate_for_feats""])\n            sampling_rate = config[""sampling_rate_for_feats""]\n            assert config[""hop_size""] * config[""sampling_rate_for_feats""] % fs == 0, \\\n                ""hop_size must be int value. please check sampling_rate_for_feats is correct.""\n            hop_size = config[""hop_size""] * config[""sampling_rate_for_feats""] // fs\n\n        # extract feature\n        mel = logmelfilterbank(x,\n                               sampling_rate=sampling_rate,\n                               hop_size=hop_size,\n                               fft_size=config[""fft_size""],\n                               win_length=config[""win_length""],\n                               window=config[""window""],\n                               num_mels=config[""num_mels""],\n                               fmin=config[""fmin""],\n                               fmax=config[""fmax""])\n\n        # make sure the audio length and feature length are matched\n        audio = np.pad(audio, (0, config[""fft_size""]), mode=""edge"")\n        audio = audio[:len(mel) * config[""hop_size""]]\n        assert len(mel) * config[""hop_size""] == len(audio)\n\n        # apply global gain\n        if config[""global_gain_scale""] > 0.0:\n            audio *= config[""global_gain_scale""]\n        if np.abs(audio).max() >= 1.0:\n            logging.warn(f""{utt_id} causes clipping. ""\n                         f""it is better to re-consider global gain scale."")\n            continue\n\n        # save\n        if config[""format""] == ""hdf5"":\n            write_hdf5(os.path.join(args.dumpdir, f""{utt_id}.h5""), ""wave"", audio.astype(np.float32))\n            write_hdf5(os.path.join(args.dumpdir, f""{utt_id}.h5""), ""feats"", mel.astype(np.float32))\n        elif config[""format""] == ""npy"":\n            np.save(os.path.join(args.dumpdir, f""{utt_id}-wave.npy""),\n                    audio.astype(np.float32), allow_pickle=False)\n            np.save(os.path.join(args.dumpdir, f""{utt_id}-feats.npy""),\n                    mel.astype(np.float32), allow_pickle=False)\n        else:\n            raise ValueError(""support only hdf5 or npy format."")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
parallel_wavegan/bin/train.py,26,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Train Parallel WaveGAN.""""""\n\nimport argparse\nimport logging\nimport os\nimport sys\n\nfrom collections import defaultdict\n\nimport matplotlib\nimport numpy as np\nimport soundfile as sf\nimport torch\nimport yaml\n\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport parallel_wavegan\nimport parallel_wavegan.models\nimport parallel_wavegan.optimizers\n\nfrom parallel_wavegan.datasets import AudioMelDataset\nfrom parallel_wavegan.datasets import AudioMelSCPDataset\nfrom parallel_wavegan.layers import PQMF\nfrom parallel_wavegan.losses import MultiResolutionSTFTLoss\nfrom parallel_wavegan.utils import read_hdf5\n\n# set to avoid matplotlib error in CLI environment\nmatplotlib.use(""Agg"")\n\n\nclass Trainer(object):\n    """"""Customized trainer module for Parallel WaveGAN training.""""""\n\n    def __init__(self,\n                 steps,\n                 epochs,\n                 data_loader,\n                 model,\n                 criterion,\n                 optimizer,\n                 scheduler,\n                 config,\n                 device=torch.device(""cpu""),\n                 ):\n        """"""Initialize trainer.\n\n        Args:\n            steps (int): Initial global steps.\n            epochs (int): Initial global epochs.\n            data_loader (dict): Dict of data loaders. It must contrain ""train"" and ""dev"" loaders.\n            model (dict): Dict of models. It must contrain ""generator"" and ""discriminator"" models.\n            criterion (dict): Dict of criterions. It must contrain ""stft"" and ""mse"" criterions.\n            optimizer (dict): Dict of optimizers. It must contrain ""generator"" and ""discriminator"" optimizers.\n            scheduler (dict): Dict of schedulers. It must contrain ""generator"" and ""discriminator"" schedulers.\n            config (dict): Config dict loaded from yaml format configuration file.\n            device (torch.deive): Pytorch device instance.\n\n        """"""\n        self.steps = steps\n        self.epochs = epochs\n        self.data_loader = data_loader\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.config = config\n        self.device = device\n        self.writer = SummaryWriter(config[""outdir""])\n        self.finish_train = False\n        self.total_train_loss = defaultdict(float)\n        self.total_eval_loss = defaultdict(float)\n\n    def run(self):\n        """"""Run training.""""""\n        self.tqdm = tqdm(initial=self.steps,\n                         total=self.config[""train_max_steps""],\n                         desc=""[train]"")\n        while True:\n            # train one epoch\n            self._train_epoch()\n\n            # check whether training is finished\n            if self.finish_train:\n                break\n\n        self.tqdm.close()\n        logging.info(""Finished training."")\n\n    def save_checkpoint(self, checkpoint_path):\n        """"""Save checkpoint.\n\n        Args:\n            checkpoint_path (str): Checkpoint path to be saved.\n\n        """"""\n        state_dict = {\n            ""optimizer"": {\n                ""generator"": self.optimizer[""generator""].state_dict(),\n                ""discriminator"": self.optimizer[""discriminator""].state_dict(),\n            },\n            ""scheduler"": {\n                ""generator"": self.scheduler[""generator""].state_dict(),\n                ""discriminator"": self.scheduler[""discriminator""].state_dict(),\n            },\n            ""steps"": self.steps,\n            ""epochs"": self.epochs,\n        }\n        if self.config[""distributed""]:\n            state_dict[""model""] = {\n                ""generator"": self.model[""generator""].module.state_dict(),\n                ""discriminator"": self.model[""discriminator""].module.state_dict(),\n            }\n        else:\n            state_dict[""model""] = {\n                ""generator"": self.model[""generator""].state_dict(),\n                ""discriminator"": self.model[""discriminator""].state_dict(),\n            }\n\n        if not os.path.exists(os.path.dirname(checkpoint_path)):\n            os.makedirs(os.path.dirname(checkpoint_path))\n        torch.save(state_dict, checkpoint_path)\n\n    def load_checkpoint(self, checkpoint_path, load_only_params=False):\n        """"""Load checkpoint.\n\n        Args:\n            checkpoint_path (str): Checkpoint path to be loaded.\n            load_only_params (bool): Whether to load only model parameters.\n\n        """"""\n        state_dict = torch.load(checkpoint_path, map_location=""cpu"")\n        if self.config[""distributed""]:\n            self.model[""generator""].module.load_state_dict(state_dict[""model""][""generator""])\n            self.model[""discriminator""].module.load_state_dict(state_dict[""model""][""discriminator""])\n        else:\n            self.model[""generator""].load_state_dict(state_dict[""model""][""generator""])\n            self.model[""discriminator""].load_state_dict(state_dict[""model""][""discriminator""])\n        if not load_only_params:\n            self.steps = state_dict[""steps""]\n            self.epochs = state_dict[""epochs""]\n            self.optimizer[""generator""].load_state_dict(state_dict[""optimizer""][""generator""])\n            self.optimizer[""discriminator""].load_state_dict(state_dict[""optimizer""][""discriminator""])\n            self.scheduler[""generator""].load_state_dict(state_dict[""scheduler""][""generator""])\n            self.scheduler[""discriminator""].load_state_dict(state_dict[""scheduler""][""discriminator""])\n\n    def _train_step(self, batch):\n        """"""Train model one step.""""""\n        # parse batch\n        x, y = batch\n        x = tuple([x_.to(self.device) for x_ in x])\n        y = y.to(self.device)\n\n        #######################\n        #      Generator      #\n        #######################\n        y_ = self.model[""generator""](*x)\n\n        # reconstruct the signal from multi-band signal\n        if self.config[""generator_params""][""out_channels""] > 1:\n            y_mb_ = y_\n            y_ = self.criterion[""pqmf""].synthesis(y_mb_)\n\n        # multi-resolution sfft loss\n        sc_loss, mag_loss = self.criterion[""stft""](y_.squeeze(1), y.squeeze(1))\n        self.total_train_loss[""train/spectral_convergence_loss""] += sc_loss.item()\n        self.total_train_loss[""train/log_stft_magnitude_loss""] += mag_loss.item()\n        gen_loss = sc_loss + mag_loss\n\n        # subband multi-resolution stft loss\n        if self.config.get(""use_subband_stft_loss"", False):\n            gen_loss *= 0.5  # for balancing with subband stft loss\n            y_mb = self.criterion[""pqmf""].analysis(y)\n            y_mb = y_mb.view(-1, y_mb.size(2))  # (B, C, T) -> (B x C, T)\n            y_mb_ = y_mb_.view(-1, y_mb_.size(2))  # (B, C, T) -> (B x C, T)\n            sub_sc_loss, sub_mag_loss = self.criterion[""sub_stft""](y_mb_, y_mb)\n            self.total_train_loss[\n                ""train/sub_spectral_convergence_loss""] += sub_sc_loss.item()\n            self.total_train_loss[\n                ""train/sub_log_stft_magnitude_loss""] += sub_mag_loss.item()\n            gen_loss += 0.5 * (sub_sc_loss + sub_mag_loss)\n\n        # adversarial loss\n        if self.steps > self.config[""discriminator_train_start_steps""]:\n            p_ = self.model[""discriminator""](y_)\n            if not isinstance(p_, list):\n                # for standard discriminator\n                adv_loss = self.criterion[""mse""](p_, p_.new_ones(p_.size()))\n                self.total_train_loss[""train/adversarial_loss""] += adv_loss.item()\n            else:\n                # for multi-scale discriminator\n                adv_loss = 0.0\n                for i in range(len(p_)):\n                    adv_loss += self.criterion[""mse""](\n                        p_[i][-1], p_[i][-1].new_ones(p_[i][-1].size()))\n                adv_loss /= (i + 1)\n                self.total_train_loss[""train/adversarial_loss""] += adv_loss.item()\n\n                # feature matching loss\n                if self.config[""use_feat_match_loss""]:\n                    # no need to track gradients\n                    with torch.no_grad():\n                        p = self.model[""discriminator""](y)\n                    fm_loss = 0.0\n                    for i in range(len(p_)):\n                        for j in range(len(p_[i]) - 1):\n                            fm_loss += self.criterion[""l1""](p_[i][j], p[i][j].detach())\n                    fm_loss /= (i + 1) * (j + 1)\n                    self.total_train_loss[""train/feature_matching_loss""] += fm_loss.item()\n                    adv_loss += self.config[""lambda_feat_match""] * fm_loss\n\n            # add adversarial loss to generator loss\n            gen_loss += self.config[""lambda_adv""] * adv_loss\n\n        self.total_train_loss[""train/generator_loss""] += gen_loss.item()\n\n        # update generator\n        self.optimizer[""generator""].zero_grad()\n        gen_loss.backward()\n        if self.config[""generator_grad_norm""] > 0:\n            torch.nn.utils.clip_grad_norm_(\n                self.model[""generator""].parameters(),\n                self.config[""generator_grad_norm""])\n        self.optimizer[""generator""].step()\n        self.scheduler[""generator""].step()\n\n        #######################\n        #    Discriminator    #\n        #######################\n        if self.steps > self.config[""discriminator_train_start_steps""]:\n            # re-compute y_ which leads better quality\n            with torch.no_grad():\n                y_ = self.model[""generator""](*x)\n            if self.config[""generator_params""][""out_channels""] > 1:\n                y_ = self.criterion[""pqmf""].synthesis(y_)\n\n            # discriminator loss\n            p = self.model[""discriminator""](y)\n            p_ = self.model[""discriminator""](y_.detach())\n            if not isinstance(p, list):\n                # for standard discriminator\n                real_loss = self.criterion[""mse""](p, p.new_ones(p.size()))\n                fake_loss = self.criterion[""mse""](p_, p_.new_zeros(p_.size()))\n                dis_loss = real_loss + fake_loss\n            else:\n                # for multi-scale discriminator\n                real_loss = 0.0\n                fake_loss = 0.0\n                for i in range(len(p)):\n                    real_loss += self.criterion[""mse""](\n                        p[i][-1], p[i][-1].new_ones(p[i][-1].size()))\n                    fake_loss += self.criterion[""mse""](\n                        p_[i][-1], p_[i][-1].new_zeros(p_[i][-1].size()))\n                real_loss /= (i + 1)\n                fake_loss /= (i + 1)\n                dis_loss = real_loss + fake_loss\n\n            self.total_train_loss[""train/real_loss""] += real_loss.item()\n            self.total_train_loss[""train/fake_loss""] += fake_loss.item()\n            self.total_train_loss[""train/discriminator_loss""] += dis_loss.item()\n\n            # update discriminator\n            self.optimizer[""discriminator""].zero_grad()\n            dis_loss.backward()\n            if self.config[""discriminator_grad_norm""] > 0:\n                torch.nn.utils.clip_grad_norm_(\n                    self.model[""discriminator""].parameters(),\n                    self.config[""discriminator_grad_norm""])\n            self.optimizer[""discriminator""].step()\n            self.scheduler[""discriminator""].step()\n\n        # update counts\n        self.steps += 1\n        self.tqdm.update(1)\n        self._check_train_finish()\n\n    def _train_epoch(self):\n        """"""Train model one epoch.""""""\n        for train_steps_per_epoch, batch in enumerate(self.data_loader[""train""], 1):\n            # train one step\n            self._train_step(batch)\n\n            # check interval\n            if self.config[""rank""] == 0:\n                self._check_log_interval()\n                self._check_eval_interval()\n                self._check_save_interval()\n\n            # check whether training is finished\n            if self.finish_train:\n                return\n\n        # update\n        self.epochs += 1\n        self.train_steps_per_epoch = train_steps_per_epoch\n        logging.info(f""(Steps: {self.steps}) Finished {self.epochs} epoch training ""\n                     f""({self.train_steps_per_epoch} steps per epoch)."")\n\n    @torch.no_grad()\n    def _eval_step(self, batch):\n        """"""Evaluate model one step.""""""\n        # parse batch\n        x, y = batch\n        x = tuple([x_.to(self.device) for x_ in x])\n        y = y.to(self.device)\n\n        #######################\n        #      Generator      #\n        #######################\n        y_ = self.model[""generator""](*x)\n        if self.config[""generator_params""][""out_channels""] > 1:\n            y_mb_ = y_\n            y_ = self.criterion[""pqmf""].synthesis(y_mb_)\n\n        # multi-resolution stft loss\n        sc_loss, mag_loss = self.criterion[""stft""](y_.squeeze(1), y.squeeze(1))\n        aux_loss = sc_loss + mag_loss\n\n        # subband multi-resolution stft loss\n        if self.config.get(""use_subband_stft_loss"", False):\n            aux_loss *= 0.5  # for balancing with subband stft loss\n            y_mb = self.criterion[""pqmf""].analysis(y)\n            y_mb = y_mb.view(-1, y_mb.size(2))  # (B, C, T) -> (B x C, T)\n            y_mb_ = y_mb_.view(-1, y_mb_.size(2))  # (B, C, T) -> (B x C, T)\n            sub_sc_loss, sub_mag_loss = self.criterion[""sub_stft""](y_mb_, y_mb)\n            self.total_eval_loss[\n                ""eval/sub_spectral_convergence_loss""] += sub_sc_loss.item()\n            self.total_eval_loss[\n                ""eval/sub_log_stft_magnitude_loss""] += sub_mag_loss.item()\n            aux_loss += 0.5 * (sub_sc_loss + sub_mag_loss)\n\n        # adversarial loss\n        p_ = self.model[""discriminator""](y_)\n        if not isinstance(p_, list):\n            # for standard discriminator\n            adv_loss = self.criterion[""mse""](p_, p_.new_ones(p_.size()))\n            gen_loss = aux_loss + self.config[""lambda_adv""] * adv_loss\n        else:\n            # for multi-scale discriminator\n            adv_loss = 0.0\n            for i in range(len(p_)):\n                adv_loss += self.criterion[""mse""](\n                    p_[i][-1], p_[i][-1].new_ones(p_[i][-1].size()))\n            adv_loss /= (i + 1)\n            gen_loss = aux_loss + self.config[""lambda_adv""] * adv_loss\n\n            # feature matching loss\n            if self.config[""use_feat_match_loss""]:\n                p = self.model[""discriminator""](y)\n                fm_loss = 0.0\n                for i in range(len(p_)):\n                    for j in range(len(p_[i]) - 1):\n                        fm_loss += self.criterion[""l1""](p_[i][j], p[i][j])\n                fm_loss /= (i + 1) * (j + 1)\n                self.total_eval_loss[""eval/feature_matching_loss""] += fm_loss.item()\n                gen_loss += self.config[""lambda_adv""] * self.config[""lambda_feat_match""] * fm_loss\n\n        #######################\n        #    Discriminator    #\n        #######################\n        p = self.model[""discriminator""](y)\n        p_ = self.model[""discriminator""](y_)\n\n        # discriminator loss\n        if not isinstance(p_, list):\n            # for standard discriminator\n            real_loss = self.criterion[""mse""](p, p.new_ones(p.size()))\n            fake_loss = self.criterion[""mse""](p_, p_.new_zeros(p_.size()))\n            dis_loss = real_loss + fake_loss\n        else:\n            # for multi-scale discriminator\n            real_loss = 0.0\n            fake_loss = 0.0\n            for i in range(len(p)):\n                real_loss += self.criterion[""mse""](\n                    p[i][-1], p[i][-1].new_ones(p[i][-1].size()))\n                fake_loss += self.criterion[""mse""](\n                    p_[i][-1], p_[i][-1].new_zeros(p_[i][-1].size()))\n            real_loss /= (i + 1)\n            fake_loss /= (i + 1)\n            dis_loss = real_loss + fake_loss\n\n        # add to total eval loss\n        self.total_eval_loss[""eval/adversarial_loss""] += adv_loss.item()\n        self.total_eval_loss[""eval/spectral_convergence_loss""] += sc_loss.item()\n        self.total_eval_loss[""eval/log_stft_magnitude_loss""] += mag_loss.item()\n        self.total_eval_loss[""eval/generator_loss""] += gen_loss.item()\n        self.total_eval_loss[""eval/real_loss""] += real_loss.item()\n        self.total_eval_loss[""eval/fake_loss""] += fake_loss.item()\n        self.total_eval_loss[""eval/discriminator_loss""] += dis_loss.item()\n\n    def _eval_epoch(self):\n        """"""Evaluate model one epoch.""""""\n        logging.info(f""(Steps: {self.steps}) Start evaluation."")\n        # change mode\n        for key in self.model.keys():\n            self.model[key].eval()\n\n        # calculate loss for each batch\n        for eval_steps_per_epoch, batch in enumerate(tqdm(self.data_loader[""dev""], desc=""[eval]""), 1):\n            # eval one step\n            self._eval_step(batch)\n\n            # save intermediate result\n            if eval_steps_per_epoch == 1:\n                self._genearete_and_save_intermediate_result(batch)\n\n        logging.info(f""(Steps: {self.steps}) Finished evaluation ""\n                     f""({eval_steps_per_epoch} steps per epoch)."")\n\n        # average loss\n        for key in self.total_eval_loss.keys():\n            self.total_eval_loss[key] /= eval_steps_per_epoch\n            logging.info(f""(Steps: {self.steps}) {key} = {self.total_eval_loss[key]:.4f}."")\n\n        # record\n        self._write_to_tensorboard(self.total_eval_loss)\n\n        # reset\n        self.total_eval_loss = defaultdict(float)\n\n        # restore mode\n        for key in self.model.keys():\n            self.model[key].train()\n\n    @torch.no_grad()\n    def _genearete_and_save_intermediate_result(self, batch):\n        """"""Generate and save intermediate result.""""""\n        # delayed import to avoid error related backend error\n        import matplotlib.pyplot as plt\n\n        # generate\n        x_batch, y_batch = batch\n        x_batch = tuple([x.to(self.device) for x in x_batch])\n        y_batch = y_batch.to(self.device)\n        y_batch_ = self.model[""generator""](*x_batch)\n        if self.config[""generator_params""][""out_channels""] > 1:\n            y_batch_ = self.criterion[""pqmf""].synthesis(y_batch_)\n\n        # check directory\n        dirname = os.path.join(self.config[""outdir""], f""predictions/{self.steps}steps"")\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        for idx, (y, y_) in enumerate(zip(y_batch, y_batch_), 1):\n            # convert to ndarray\n            y, y_ = y.view(-1).cpu().numpy(), y_.view(-1).cpu().numpy()\n\n            # plot figure and save it\n            figname = os.path.join(dirname, f""{idx}.png"")\n            plt.subplot(2, 1, 1)\n            plt.plot(y)\n            plt.title(""groundtruth speech"")\n            plt.subplot(2, 1, 2)\n            plt.plot(y_)\n            plt.title(f""generated speech @ {self.steps} steps"")\n            plt.tight_layout()\n            plt.savefig(figname)\n            plt.close()\n\n            # save as wavfile\n            y = np.clip(y, -1, 1)\n            y_ = np.clip(y_, -1, 1)\n            sf.write(figname.replace("".png"", ""_ref.wav""), y,\n                     self.config[""sampling_rate""], ""PCM_16"")\n            sf.write(figname.replace("".png"", ""_gen.wav""), y_,\n                     self.config[""sampling_rate""], ""PCM_16"")\n\n            if idx >= self.config[""num_save_intermediate_results""]:\n                break\n\n    def _write_to_tensorboard(self, loss):\n        """"""Write to tensorboard.""""""\n        for key, value in loss.items():\n            self.writer.add_scalar(key, value, self.steps)\n\n    def _check_save_interval(self):\n        if self.steps % self.config[""save_interval_steps""] == 0:\n            self.save_checkpoint(\n                os.path.join(self.config[""outdir""], f""checkpoint-{self.steps}steps.pkl""))\n            logging.info(f""Successfully saved checkpoint @ {self.steps} steps."")\n\n    def _check_eval_interval(self):\n        if self.steps % self.config[""eval_interval_steps""] == 0:\n            self._eval_epoch()\n\n    def _check_log_interval(self):\n        if self.steps % self.config[""log_interval_steps""] == 0:\n            for key in self.total_train_loss.keys():\n                self.total_train_loss[key] /= self.config[""log_interval_steps""]\n                logging.info(f""(Steps: {self.steps}) {key} = {self.total_train_loss[key]:.4f}."")\n            self._write_to_tensorboard(self.total_train_loss)\n\n            # reset\n            self.total_train_loss = defaultdict(float)\n\n    def _check_train_finish(self):\n        if self.steps >= self.config[""train_max_steps""]:\n            self.finish_train = True\n\n\nclass Collater(object):\n    """"""Customized collater for Pytorch DataLoader in training.""""""\n\n    def __init__(self,\n                 batch_max_steps=20480,\n                 hop_size=256,\n                 aux_context_window=2,\n                 use_noise_input=False,\n                 ):\n        """"""Initialize customized collater for PyTorch DataLoader.\n\n        Args:\n            batch_max_steps (int): The maximum length of input signal in batch.\n            hop_size (int): Hop size of auxiliary features.\n            aux_context_window (int): Context window size for auxiliary feature conv.\n            use_noise_input (bool): Whether to use noise input.\n\n        """"""\n        if batch_max_steps % hop_size != 0:\n            batch_max_steps += -(batch_max_steps % hop_size)\n        assert batch_max_steps % hop_size == 0\n        self.batch_max_steps = batch_max_steps\n        self.batch_max_frames = batch_max_steps // hop_size\n        self.hop_size = hop_size\n        self.aux_context_window = aux_context_window\n        self.use_noise_input = use_noise_input\n\n        # set useful values in random cutting\n        self.start_offset = aux_context_window\n        self.end_offset = -(self.batch_max_frames + aux_context_window)\n        self.mel_threshold = self.batch_max_frames + 2 * aux_context_window\n\n    def __call__(self, batch):\n        """"""Convert into batch tensors.\n\n        Args:\n            batch (list): list of tuple of the pair of audio and features.\n\n        Returns:\n            Tensor: Gaussian noise batch (B, 1, T).\n            Tensor: Auxiliary feature batch (B, C, T\'), where\n                T = (T\' - 2 * aux_context_window) * hop_size.\n            Tensor: Target signal batch (B, 1, T).\n\n        """"""\n        # check length\n        batch = [self._adjust_length(*b) for b in batch if len(b[1]) > self.mel_threshold]\n        xs, cs = [b[0] for b in batch], [b[1] for b in batch]\n\n        # make batch with random cut\n        c_lengths = [len(c) for c in cs]\n        start_frames = np.array([np.random.randint(\n            self.start_offset, cl + self.end_offset) for cl in c_lengths])\n        x_starts = start_frames * self.hop_size\n        x_ends = x_starts + self.batch_max_steps\n        c_starts = start_frames - self.aux_context_window\n        c_ends = start_frames + self.batch_max_frames + self.aux_context_window\n        y_batch = [x[start: end] for x, start, end in zip(xs, x_starts, x_ends)]\n        c_batch = [c[start: end] for c, start, end in zip(cs, c_starts, c_ends)]\n\n        # convert each batch to tensor, asuume that each item in batch has the same length\n        y_batch = torch.tensor(y_batch, dtype=torch.float).unsqueeze(1)  # (B, 1, T)\n        c_batch = torch.tensor(c_batch, dtype=torch.float).transpose(2, 1)  # (B, C, T\')\n\n        # make input noise signal batch tensor\n        if self.use_noise_input:\n            z_batch = torch.randn(y_batch.size())  # (B, 1, T)\n            return (z_batch, c_batch), y_batch\n        else:\n            return (c_batch,), y_batch\n\n    def _adjust_length(self, x, c):\n        """"""Adjust the audio and feature lengths.\n\n        Note:\n            Basically we assume that the length of x and c are adjusted\n            through preprocessing stage, but if we use other library processed\n            features, this process will be needed.\n\n        """"""\n        if len(x) < len(c) * self.hop_size:\n            x = np.pad(x, (0, len(c) * self.hop_size - len(x)), mode=""edge"")\n\n        # check the legnth is valid\n        assert len(x) == len(c) * self.hop_size\n\n        return x, c\n\n\ndef main():\n    """"""Run training process.""""""\n    parser = argparse.ArgumentParser(\n        description=""Train Parallel WaveGAN (See detail in parallel_wavegan/bin/train.py)."")\n    parser.add_argument(""--train-wav-scp"", default=None, type=str,\n                        help=""kaldi-style wav.scp file for training. ""\n                             ""you need to specify either train-*-scp or train-dumpdir."")\n    parser.add_argument(""--train-feats-scp"", default=None, type=str,\n                        help=""kaldi-style feats.scp file for training. ""\n                             ""you need to specify either train-*-scp or train-dumpdir."")\n    parser.add_argument(""--train-segments"", default=None, type=str,\n                        help=""kaldi-style segments file for training."")\n    parser.add_argument(""--train-dumpdir"", default=None, type=str,\n                        help=""directory including training data. ""\n                             ""you need to specify either train-*-scp or train-dumpdir."")\n    parser.add_argument(""--dev-wav-scp"", default=None, type=str,\n                        help=""kaldi-style wav.scp file for validation. ""\n                             ""you need to specify either dev-*-scp or dev-dumpdir."")\n    parser.add_argument(""--dev-feats-scp"", default=None, type=str,\n                        help=""kaldi-style feats.scp file for vaidation. ""\n                             ""you need to specify either dev-*-scp or dev-dumpdir."")\n    parser.add_argument(""--dev-segments"", default=None, type=str,\n                        help=""kaldi-style segments file for validation."")\n    parser.add_argument(""--dev-dumpdir"", default=None, type=str,\n                        help=""directory including development data. ""\n                             ""you need to specify either dev-*-scp or dev-dumpdir."")\n    parser.add_argument(""--outdir"", type=str, required=True,\n                        help=""directory to save checkpoints."")\n    parser.add_argument(""--config"", type=str, required=True,\n                        help=""yaml format configuration file."")\n    parser.add_argument(""--pretrain"", default="""", type=str, nargs=""?"",\n                        help=""checkpoint file path to load pretrained params. (default=\\""\\"")"")\n    parser.add_argument(""--resume"", default="""", type=str, nargs=""?"",\n                        help=""checkpoint file path to resume training. (default=\\""\\"")"")\n    parser.add_argument(""--verbose"", type=int, default=1,\n                        help=""logging level. higher is more logging. (default=1)"")\n    parser.add_argument(""--rank"", ""--local_rank"", default=0, type=int,\n                        help=""rank for distributed training. no need to explictly specify."")\n    args = parser.parse_args()\n\n    args.distributed = False\n    if not torch.cuda.is_available():\n        device = torch.device(""cpu"")\n    else:\n        device = torch.device(""cuda"")\n        # effective when using fixed size inputs\n        # see https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n        torch.backends.cudnn.benchmark = True\n        torch.cuda.set_device(args.rank)\n        # setup for distributed training\n        # see example: https://github.com/NVIDIA/apex/tree/master/examples/simple/distributed\n        if ""WORLD_SIZE"" in os.environ:\n            args.world_size = int(os.environ[""WORLD_SIZE""])\n            args.distributed = args.world_size > 1\n        if args.distributed:\n            torch.distributed.init_process_group(backend=""nccl"", init_method=""env://"")\n\n    # suppress logging for distributed training\n    if args.rank != 0:\n        sys.stdout = open(os.devnull, ""w"")\n\n    # set logger\n    if args.verbose > 1:\n        logging.basicConfig(\n            level=logging.DEBUG, stream=sys.stdout,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n    elif args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO, stream=sys.stdout,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n    else:\n        logging.basicConfig(\n            level=logging.WARN, stream=sys.stdout,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"")\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # check directory existence\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n\n    # check arguments\n    if (args.train_feats_scp is not None and args.train_dumpdir is not None) or \\\n            (args.train_feats_scp is None and args.train_dumpdir is None):\n        raise ValueError(""Please specify either --train-dumpdir or --train-*-scp."")\n    if (args.dev_feats_scp is not None and args.dev_dumpdir is not None) or \\\n            (args.dev_feats_scp is None and args.dev_dumpdir is None):\n        raise ValueError(""Please specify either --dev-dumpdir or --dev-*-scp."")\n\n    # load and save config\n    with open(args.config) as f:\n        config = yaml.load(f, Loader=yaml.Loader)\n    config.update(vars(args))\n    config[""version""] = parallel_wavegan.__version__  # add version info\n    with open(os.path.join(args.outdir, ""config.yml""), ""w"") as f:\n        yaml.dump(config, f, Dumper=yaml.Dumper)\n    for key, value in config.items():\n        logging.info(f""{key} = {value}"")\n\n    # get dataset\n    if config[""remove_short_samples""]:\n        mel_length_threshold = config[""batch_max_steps""] // config[""hop_size""] + \\\n            2 * config[""generator_params""].get(""aux_context_window"", 0)\n    else:\n        mel_length_threshold = None\n    if args.train_wav_scp is None or args.dev_wav_scp is None:\n        if config[""format""] == ""hdf5"":\n            audio_query, mel_query = ""*.h5"", ""*.h5""\n            audio_load_fn = lambda x: read_hdf5(x, ""wave"")  # NOQA\n            mel_load_fn = lambda x: read_hdf5(x, ""feats"")  # NOQA\n        elif config[""format""] == ""npy"":\n            audio_query, mel_query = ""*-wave.npy"", ""*-feats.npy""\n            audio_load_fn = np.load\n            mel_load_fn = np.load\n        else:\n            raise ValueError(""support only hdf5 or npy format."")\n    if args.train_dumpdir is not None:\n        train_dataset = AudioMelDataset(\n            root_dir=args.train_dumpdir,\n            audio_query=audio_query,\n            mel_query=mel_query,\n            audio_load_fn=audio_load_fn,\n            mel_load_fn=mel_load_fn,\n            mel_length_threshold=mel_length_threshold,\n            allow_cache=config.get(""allow_cache"", False),  # keep compatibility\n        )\n    else:\n        train_dataset = AudioMelSCPDataset(\n            wav_scp=args.train_wav_scp,\n            feats_scp=args.train_feats_scp,\n            segments=args.train_segments,\n            mel_length_threshold=mel_length_threshold,\n            allow_cache=config.get(""allow_cache"", False),  # keep compatibility\n        )\n    logging.info(f""The number of training files = {len(train_dataset)}."")\n    if args.dev_dumpdir is not None:\n        dev_dataset = AudioMelDataset(\n            root_dir=args.dev_dumpdir,\n            audio_query=audio_query,\n            mel_query=mel_query,\n            audio_load_fn=audio_load_fn,\n            mel_load_fn=mel_load_fn,\n            mel_length_threshold=mel_length_threshold,\n            allow_cache=config.get(""allow_cache"", False),  # keep compatibility\n        )\n    else:\n        dev_dataset = AudioMelSCPDataset(\n            wav_scp=args.dev_wav_scp,\n            feats_scp=args.dev_feats_scp,\n            segments=args.dev_segments,\n            mel_length_threshold=mel_length_threshold,\n            allow_cache=config.get(""allow_cache"", False),  # keep compatibility\n        )\n    logging.info(f""The number of development files = {len(dev_dataset)}."")\n    dataset = {\n        ""train"": train_dataset,\n        ""dev"": dev_dataset,\n    }\n\n    # get data loader\n    collater = Collater(\n        batch_max_steps=config[""batch_max_steps""],\n        hop_size=config[""hop_size""],\n        # keep compatibility\n        aux_context_window=config[""generator_params""].get(""aux_context_window"", 0),\n        # keep compatibility\n        use_noise_input=config.get(\n            ""generator_type"", ""ParallelWaveGANGenerator"") != ""MelGANGenerator"",\n    )\n    train_sampler, dev_sampler = None, None\n    if args.distributed:\n        # setup sampler for distributed training\n        from torch.utils.data.distributed import DistributedSampler\n        train_sampler = DistributedSampler(\n            dataset=dataset[""train""],\n            num_replicas=args.world_size,\n            rank=args.rank,\n            shuffle=True,\n        )\n        dev_sampler = DistributedSampler(\n            dataset=dataset[""dev""],\n            num_replicas=args.world_size,\n            rank=args.rank,\n            shuffle=False,\n        )\n    data_loader = {\n        ""train"": DataLoader(\n            dataset=dataset[""train""],\n            shuffle=False if args.distributed else True,\n            collate_fn=collater,\n            batch_size=config[""batch_size""],\n            num_workers=config[""num_workers""],\n            sampler=train_sampler,\n            pin_memory=config[""pin_memory""],\n        ),\n        ""dev"": DataLoader(\n            dataset=dataset[""dev""],\n            shuffle=False if args.distributed else True,\n            collate_fn=collater,\n            batch_size=config[""batch_size""],\n            num_workers=config[""num_workers""],\n            sampler=dev_sampler,\n            pin_memory=config[""pin_memory""],\n        ),\n    }\n\n    # define models and optimizers\n    generator_class = getattr(\n        parallel_wavegan.models,\n        # keep compatibility\n        config.get(""generator_type"", ""ParallelWaveGANGenerator""),\n    )\n    discriminator_class = getattr(\n        parallel_wavegan.models,\n        # keep compatibility\n        config.get(""discriminator_type"", ""ParallelWaveGANDiscriminator""),\n    )\n    model = {\n        ""generator"": generator_class(\n            **config[""generator_params""]).to(device),\n        ""discriminator"": discriminator_class(\n            **config[""discriminator_params""]).to(device),\n    }\n    criterion = {\n        ""stft"": MultiResolutionSTFTLoss(\n            **config[""stft_loss_params""]).to(device),\n        ""mse"": torch.nn.MSELoss().to(device),\n    }\n    if config.get(""use_feat_match_loss"", False):  # keep compatibility\n        criterion[""l1""] = torch.nn.L1Loss().to(device)\n    if config[""generator_params""][""out_channels""] > 1:\n        criterion[""pqmf""] = PQMF(\n            config[""generator_params""][""out_channels""]).to(device)\n    if config.get(""use_subband_stft_loss"", False):  # keep compatibility\n        assert config[""generator_params""][""out_channels""] > 1\n        criterion[""sub_stft""] = MultiResolutionSTFTLoss(\n            **config[""subband_stft_loss_params""]).to(device)\n    generator_optimizer_class = getattr(\n        parallel_wavegan.optimizers,\n        # keep compatibility\n        config.get(""generator_optimizer_type"", ""RAdam""),\n    )\n    discriminator_optimizer_class = getattr(\n        parallel_wavegan.optimizers,\n        # keep compatibility\n        config.get(""discriminator_optimizer_type"", ""RAdam""),\n    )\n    optimizer = {\n        ""generator"": generator_optimizer_class(\n            model[""generator""].parameters(),\n            **config[""generator_optimizer_params""],\n        ),\n        ""discriminator"": discriminator_optimizer_class(\n            model[""discriminator""].parameters(),\n            **config[""discriminator_optimizer_params""],\n        ),\n    }\n    generator_scheduler_class = getattr(\n        torch.optim.lr_scheduler,\n        # keep compatibility\n        config.get(""generator_scheduler_type"", ""StepLR""),\n    )\n    discriminator_scheduler_class = getattr(\n        torch.optim.lr_scheduler,\n        # keep compatibility\n        config.get(""discriminator_scheduler_type"", ""StepLR""),\n    )\n    scheduler = {\n        ""generator"": generator_scheduler_class(\n            optimizer=optimizer[""generator""],\n            **config[""generator_scheduler_params""],\n        ),\n        ""discriminator"": discriminator_scheduler_class(\n            optimizer=optimizer[""discriminator""],\n            **config[""discriminator_scheduler_params""],\n        ),\n    }\n    if args.distributed:\n        # wrap model for distributed training\n        try:\n            from apex.parallel import DistributedDataParallel\n        except ImportError:\n            raise ImportError(""apex is not installed. please check https://github.com/NVIDIA/apex."")\n        model[""generator""] = DistributedDataParallel(model[""generator""])\n        model[""discriminator""] = DistributedDataParallel(model[""discriminator""])\n    logging.info(model[""generator""])\n    logging.info(model[""discriminator""])\n\n    # define trainer\n    trainer = Trainer(\n        steps=0,\n        epochs=0,\n        data_loader=data_loader,\n        model=model,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        config=config,\n        device=device,\n    )\n\n    # load pretrained parameters from checkpoint\n    if len(args.pretrain) != 0:\n        trainer.load_checkpoint(args.pretrain, load_only_params=True)\n        logging.info(f""Successfully load parameters from {args.pretrain}."")\n\n    # resume from checkpoint\n    if len(args.resume) != 0:\n        trainer.load_checkpoint(args.resume)\n        logging.info(f""Successfully resumed from {args.resume}."")\n\n    # run training loop\n    try:\n        trainer.run()\n    except KeyboardInterrupt:\n        trainer.save_checkpoint(\n            os.path.join(config[""outdir""], f""checkpoint-{trainer.steps}steps.pkl""))\n        logging.info(f""Successfully saved checkpoint @ {trainer.steps}steps."")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
parallel_wavegan/datasets/__init__.py,0,b'from parallel_wavegan.datasets.audio_mel_dataset import *  # NOQA\nfrom parallel_wavegan.datasets.scp_dataset import *  # NOQA\n'
parallel_wavegan/datasets/audio_mel_dataset.py,1,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Dataset modules.""""""\n\nimport logging\nimport os\n\nfrom multiprocessing import Manager\n\nimport numpy as np\n\nfrom torch.utils.data import Dataset\n\nfrom parallel_wavegan.utils import find_files\nfrom parallel_wavegan.utils import read_hdf5\n\n\nclass AudioMelDataset(Dataset):\n    """"""PyTorch compatible audio and mel dataset.""""""\n\n    def __init__(self,\n                 root_dir,\n                 audio_query=""*.h5"",\n                 mel_query=""*.h5"",\n                 audio_load_fn=lambda x: read_hdf5(x, ""wave""),\n                 mel_load_fn=lambda x: read_hdf5(x, ""feats""),\n                 audio_length_threshold=None,\n                 mel_length_threshold=None,\n                 return_utt_id=False,\n                 allow_cache=False,\n                 ):\n        """"""Initialize dataset.\n\n        Args:\n            root_dir (str): Root directory including dumped files.\n            audio_query (str): Query to find audio files in root_dir.\n            mel_query (str): Query to find feature files in root_dir.\n            audio_load_fn (func): Function to load audio file.\n            mel_load_fn (func): Function to load feature file.\n            audio_length_threshold (int): Threshold to remove short audio files.\n            mel_length_threshold (int): Threshold to remove short feature files.\n            return_utt_id (bool): Whether to return the utterance id with arrays.\n            allow_cache (bool): Whether to allow cache of the loaded files.\n\n        """"""\n        # find all of audio and mel files\n        audio_files = sorted(find_files(root_dir, audio_query))\n        mel_files = sorted(find_files(root_dir, mel_query))\n\n        # filter by threshold\n        if audio_length_threshold is not None:\n            audio_lengths = [audio_load_fn(f).shape[0] for f in audio_files]\n            idxs = [idx for idx in range(len(audio_files)) if audio_lengths[idx] > audio_length_threshold]\n            if len(audio_files) != len(idxs):\n                logging.warning(f""Some files are filtered by audio length threshold ""\n                                f""({len(audio_files)} -> {len(idxs)})."")\n            audio_files = [audio_files[idx] for idx in idxs]\n            mel_files = [mel_files[idx] for idx in idxs]\n        if mel_length_threshold is not None:\n            mel_lengths = [mel_load_fn(f).shape[0] for f in mel_files]\n            idxs = [idx for idx in range(len(mel_files)) if mel_lengths[idx] > mel_length_threshold]\n            if len(mel_files) != len(idxs):\n                logging.warning(f""Some files are filtered by mel length threshold ""\n                                f""({len(mel_files)} -> {len(idxs)})."")\n            audio_files = [audio_files[idx] for idx in idxs]\n            mel_files = [mel_files[idx] for idx in idxs]\n\n        # assert the number of files\n        assert len(audio_files) != 0, f""Not found any audio files in ${root_dir}.""\n        assert len(audio_files) == len(mel_files), \\\n            f""Number of audio and mel files are different ({len(audio_files)} vs {len(mel_files)}).""\n\n        self.audio_files = audio_files\n        self.audio_load_fn = audio_load_fn\n        self.mel_load_fn = mel_load_fn\n        self.mel_files = mel_files\n        if "".npy"" in audio_query:\n            self.utt_ids = [os.path.basename(f).replace(""-wave.npy"", """") for f in audio_files]\n        else:\n            self.utt_ids = [os.path.splitext(os.path.basename(f))[0] for f in audio_files]\n        self.return_utt_id = return_utt_id\n        self.allow_cache = allow_cache\n        if allow_cache:\n            # NOTE(kan-bayashi): Manager is need to share memory in dataloader with num_workers > 0\n            self.manager = Manager()\n            self.caches = self.manager.list()\n            self.caches += [() for _ in range(len(audio_files))]\n\n    def __getitem__(self, idx):\n        """"""Get specified idx items.\n\n        Args:\n            idx (int): Index of the item.\n\n        Returns:\n            str: Utterance id (only in return_utt_id = True).\n            ndarray: Audio signal (T,).\n            ndarray: Feature (T\', C).\n\n        """"""\n        if self.allow_cache and len(self.caches[idx]) != 0:\n            return self.caches[idx]\n\n        utt_id = self.utt_ids[idx]\n        audio = self.audio_load_fn(self.audio_files[idx])\n        mel = self.mel_load_fn(self.mel_files[idx])\n\n        if self.return_utt_id:\n            items = utt_id, audio, mel\n        else:\n            items = audio, mel\n\n        if self.allow_cache:\n            self.caches[idx] = items\n\n        return items\n\n    def __len__(self):\n        """"""Return dataset length.\n\n        Returns:\n            int: The length of dataset.\n\n        """"""\n        return len(self.audio_files)\n\n\nclass AudioDataset(Dataset):\n    """"""PyTorch compatible audio dataset.""""""\n\n    def __init__(self,\n                 root_dir,\n                 audio_query=""*-wave.npy"",\n                 audio_length_threshold=None,\n                 audio_load_fn=np.load,\n                 return_utt_id=False,\n                 allow_cache=False,\n                 ):\n        """"""Initialize dataset.\n\n        Args:\n            root_dir (str): Root directory including dumped files.\n            audio_query (str): Query to find audio files in root_dir.\n            audio_load_fn (func): Function to load audio file.\n            audio_length_threshold (int): Threshold to remove short audio files.\n            return_utt_id (bool): Whether to return the utterance id with arrays.\n            allow_cache (bool): Whether to allow cache of the loaded files.\n\n        """"""\n        # find all of audio and mel files\n        audio_files = sorted(find_files(root_dir, audio_query))\n\n        # filter by threshold\n        if audio_length_threshold is not None:\n            audio_lengths = [audio_load_fn(f).shape[0] for f in audio_files]\n            idxs = [idx for idx in range(len(audio_files)) if audio_lengths[idx] > audio_length_threshold]\n            if len(audio_files) != len(idxs):\n                logging.waning(f""some files are filtered by audio length threshold ""\n                               f""({len(audio_files)} -> {len(idxs)})."")\n            audio_files = [audio_files[idx] for idx in idxs]\n\n        # assert the number of files\n        assert len(audio_files) != 0, f""Not found any audio files in ${root_dir}.""\n\n        self.audio_files = audio_files\n        self.audio_load_fn = audio_load_fn\n        self.return_utt_id = return_utt_id\n        if "".npy"" in audio_query:\n            self.utt_ids = [os.path.basename(f).replace(""-wave.npy"", """") for f in audio_files]\n        else:\n            self.utt_ids = [os.path.splitext(os.path.basename(f))[0] for f in audio_files]\n        self.allow_cache = allow_cache\n        if allow_cache:\n            # NOTE(kan-bayashi): Manager is need to share memory in dataloader with num_workers > 0\n            self.manager = Manager()\n            self.caches = self.manager.list()\n            self.caches += [() for _ in range(len(audio_files))]\n\n    def __getitem__(self, idx):\n        """"""Get specified idx items.\n\n        Args:\n            idx (int): Index of the item.\n\n        Returns:\n            str: Utterance id (only in return_utt_id = True).\n            ndarray: Audio (T,).\n\n        """"""\n        if self.allow_cache and len(self.caches[idx]) != 0:\n            return self.caches[idx]\n\n        utt_id = self.utt_ids[idx]\n        audio = self.audio_load_fn(self.audio_files[idx])\n\n        if self.return_utt_id:\n            items = utt_id, audio\n        else:\n            items = audio\n\n        if self.allow_cache:\n            self.caches[idx] = items\n\n        return items\n\n    def __len__(self):\n        """"""Return dataset length.\n\n        Returns:\n            int: The length of dataset.\n\n        """"""\n        return len(self.audio_files)\n\n\nclass MelDataset(Dataset):\n    """"""PyTorch compatible mel dataset.""""""\n\n    def __init__(self,\n                 root_dir,\n                 mel_query=""*-feats.npy"",\n                 mel_length_threshold=None,\n                 mel_load_fn=np.load,\n                 return_utt_id=False,\n                 allow_cache=False,\n                 ):\n        """"""Initialize dataset.\n\n        Args:\n            root_dir (str): Root directory including dumped files.\n            mel_query (str): Query to find feature files in root_dir.\n            mel_load_fn (func): Function to load feature file.\n            mel_length_threshold (int): Threshold to remove short feature files.\n            return_utt_id (bool): Whether to return the utterance id with arrays.\n            allow_cache (bool): Whether to allow cache of the loaded files.\n\n        """"""\n        # find all of the mel files\n        mel_files = sorted(find_files(root_dir, mel_query))\n\n        # filter by threshold\n        if mel_length_threshold is not None:\n            mel_lengths = [mel_load_fn(f).shape[0] for f in mel_files]\n            idxs = [idx for idx in range(len(mel_files)) if mel_lengths[idx] > mel_length_threshold]\n            if len(mel_files) != len(idxs):\n                logging.warning(f""Some files are filtered by mel length threshold ""\n                                f""({len(mel_files)} -> {len(idxs)})."")\n            mel_files = [mel_files[idx] for idx in idxs]\n\n        # assert the number of files\n        assert len(mel_files) != 0, f""Not found any mel files in ${root_dir}.""\n\n        self.mel_files = mel_files\n        self.mel_load_fn = mel_load_fn\n        self.utt_ids = [os.path.splitext(os.path.basename(f))[0] for f in mel_files]\n        if "".npy"" in mel_query:\n            self.utt_ids = [os.path.basename(f).replace(""-feats.npy"", """") for f in mel_files]\n        else:\n            self.utt_ids = [os.path.splitext(os.path.basename(f))[0] for f in mel_files]\n        self.return_utt_id = return_utt_id\n        self.allow_cache = allow_cache\n        if allow_cache:\n            # NOTE(kan-bayashi): Manager is need to share memory in dataloader with num_workers > 0\n            self.manager = Manager()\n            self.caches = self.manager.list()\n            self.caches += [() for _ in range(len(mel_files))]\n\n    def __getitem__(self, idx):\n        """"""Get specified idx items.\n\n        Args:\n            idx (int): Index of the item.\n\n        Returns:\n            str: Utterance id (only in return_utt_id = True).\n            ndarray: Feature (T\', C).\n\n        """"""\n        if self.allow_cache and len(self.caches[idx]) != 0:\n            return self.caches[idx]\n\n        utt_id = self.utt_ids[idx]\n        mel = self.mel_load_fn(self.mel_files[idx])\n\n        if self.return_utt_id:\n            items = utt_id, mel\n        else:\n            items = mel\n\n        if self.allow_cache:\n            self.caches[idx] = items\n\n        return items\n\n    def __len__(self):\n        """"""Return dataset length.\n\n        Returns:\n            int: The length of dataset.\n\n        """"""\n        return len(self.mel_files)\n'"
parallel_wavegan/datasets/scp_dataset.py,1,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Dataset modules based on kaldi-style scp files.""""""\n\nimport logging\n\nfrom multiprocessing import Manager\n\nimport kaldiio\nimport numpy as np\n\nfrom torch.utils.data import Dataset\n\nfrom parallel_wavegan.utils import HDF5ScpLoader\n\n\ndef _check_feats_scp_type(feats_scp):\n    # read the first line of feats.scp file\n    with open(feats_scp) as f:\n        key, value = f.readlines()[0].replace(""\\n"", """").split()\n\n    # check scp type\n    if "":"" in value:\n        value_1, value_2 = value.split("":"")\n        if value_1.endswith("".ark""):\n            # kaldi-ark case: utt_id_1 /path/to/utt_id_1.ark:index\n            return ""mat""\n        elif value_1.endswith("".h5""):\n            # hdf5 case with path in hdf5: utt_id_1 /path/to/utt_id_1.h5:feats\n            return ""hdf5""\n        else:\n            raise ValueError(""Not supported feats.scp type."")\n    else:\n        if value.endswith("".h5""):\n            # hdf5 case without path in hdf5: utt_id_1 /path/to/utt_id_1.h5\n            return ""hdf5""\n        else:\n            raise ValueError(""Not supported feats.scp type."")\n\n\nclass AudioMelSCPDataset(Dataset):\n    """"""PyTorch compatible audio and mel dataset based on kaldi-stype scp files.""""""\n\n    def __init__(self,\n                 wav_scp,\n                 feats_scp,\n                 segments=None,\n                 audio_length_threshold=None,\n                 mel_length_threshold=None,\n                 return_utt_id=False,\n                 return_sampling_rate=False,\n                 allow_cache=False,\n                 ):\n        """"""Initialize dataset.\n\n        Args:\n            wav_scp (str): Kaldi-style wav.scp file.\n            feats_scp (str): Kaldi-style fests.scp file.\n            segments (str): Kaldi-style segments file.\n            audio_length_threshold (int): Threshold to remove short audio files.\n            mel_length_threshold (int): Threshold to remove short feature files.\n            return_utt_id (bool): Whether to return utterance id.\n            return_sampling_rate (bool): Wheter to return sampling rate.\n            allow_cache (bool): Whether to allow cache of the loaded files.\n\n        """"""\n        # load scp as lazy dict\n        audio_loader = kaldiio.load_scp(wav_scp, segments=segments)\n        if _check_feats_scp_type(feats_scp) == ""mat"":\n            mel_loader = kaldiio.load_scp(feats_scp)\n        else:\n            mel_loader = HDF5ScpLoader(feats_scp)\n        audio_keys = list(audio_loader.keys())\n        mel_keys = list(mel_loader.keys())\n\n        # filter by threshold\n        if audio_length_threshold is not None:\n            audio_lengths = [audio.shape[0] for _, audio in audio_loader.values()]\n            idxs = [idx for idx in range(len(audio_keys)) if audio_lengths[idx] > audio_length_threshold]\n            if len(audio_keys) != len(idxs):\n                logging.warning(f""Some files are filtered by audio length threshold ""\n                                f""({len(audio_keys)} -> {len(idxs)})."")\n            audio_keys = [audio_keys[idx] for idx in idxs]\n            mel_keys = [mel_keys[idx] for idx in idxs]\n        if mel_length_threshold is not None:\n            mel_lengths = [mel.shape[0] for mel in mel_loader.values()]\n            idxs = [idx for idx in range(len(mel_keys)) if mel_lengths[idx] > mel_length_threshold]\n            if len(mel_keys) != len(idxs):\n                logging.warning(f""Some files are filtered by mel length threshold ""\n                                f""({len(mel_keys)} -> {len(idxs)})."")\n            audio_keys = [audio_keys[idx] for idx in idxs]\n            mel_keys = [mel_keys[idx] for idx in idxs]\n\n        # assert the number of files\n        assert len(audio_keys) == len(mel_keys), \\\n            f""Number of audio and mel files are different ({len(audio_keys)} vs {len(mel_keys)}).""\n\n        self.audio_loader = audio_loader\n        self.mel_loader = mel_loader\n        self.utt_ids = audio_keys\n        self.return_utt_id = return_utt_id\n        self.return_sampling_rate = return_sampling_rate\n        self.allow_cache = allow_cache\n\n        if allow_cache:\n            # NOTE(kan-bayashi): Manager is need to share memory in dataloader with num_workers > 0\n            self.manager = Manager()\n            self.caches = self.manager.list()\n            self.caches += [() for _ in range(len(self.utt_ids))]\n\n    def __getitem__(self, idx):\n        """"""Get specified idx items.\n\n        Args:\n            idx (int): Index of the item.\n\n        Returns:\n            str: Utterance id (only in return_utt_id = True).\n            ndarray or tuple: Audio signal (T,) or (w/ sampling rate if return_sampling_rate = True).\n            ndarray: Feature (T\', C).\n\n        """"""\n        if self.allow_cache and len(self.caches[idx]) != 0:\n            return self.caches[idx]\n\n        utt_id = self.utt_ids[idx]\n        fs, audio = self.audio_loader[utt_id]\n        mel = self.mel_loader[utt_id]\n\n        # normalize audio signal to be [-1, 1]\n        audio = audio.astype(np.float32)\n        audio /= (1 << (16 - 1))  # assume that wav is PCM 16 bit\n\n        if self.return_sampling_rate:\n            audio = (audio, fs)\n\n        if self.return_utt_id:\n            items = utt_id, audio, mel\n        else:\n            items = audio, mel\n\n        if self.allow_cache:\n            self.caches[idx] = items\n\n        return items\n\n    def __len__(self):\n        """"""Return dataset length.\n\n        Returns:\n            int: The length of dataset.\n\n        """"""\n        return len(self.utt_ids)\n\n\nclass AudioSCPDataset(Dataset):\n    """"""PyTorch compatible audio dataset based on kaldi-stype scp files.""""""\n\n    def __init__(self,\n                 wav_scp,\n                 segments=None,\n                 audio_length_threshold=None,\n                 return_utt_id=False,\n                 return_sampling_rate=False,\n                 allow_cache=False,\n                 ):\n        """"""Initialize dataset.\n\n        Args:\n            wav_scp (str): Kaldi-style wav.scp file.\n            segments (str): Kaldi-style segments file.\n            audio_length_threshold (int): Threshold to remove short audio files.\n            return_utt_id (bool): Whether to return utterance id.\n            return_sampling_rate (bool): Wheter to return sampling rate.\n            allow_cache (bool): Whether to allow cache of the loaded files.\n\n        """"""\n        # load scp as lazy dict\n        audio_loader = kaldiio.load_scp(wav_scp, segments=segments)\n        audio_keys = list(audio_loader.keys())\n\n        # filter by threshold\n        if audio_length_threshold is not None:\n            audio_lengths = [audio.shape[0] for _, audio in audio_loader.values()]\n            idxs = [idx for idx in range(len(audio_keys)) if audio_lengths[idx] > audio_length_threshold]\n            if len(audio_keys) != len(idxs):\n                logging.warning(f""Some files are filtered by audio length threshold ""\n                                f""({len(audio_keys)} -> {len(idxs)})."")\n            audio_keys = [audio_keys[idx] for idx in idxs]\n\n        self.audio_loader = audio_loader\n        self.utt_ids = audio_keys\n        self.return_utt_id = return_utt_id\n        self.return_sampling_rate = return_sampling_rate\n        self.allow_cache = allow_cache\n\n        if allow_cache:\n            # NOTE(kan-bayashi): Manager is need to share memory in dataloader with num_workers > 0\n            self.manager = Manager()\n            self.caches = self.manager.list()\n            self.caches += [() for _ in range(len(self.utt_ids))]\n\n    def __getitem__(self, idx):\n        """"""Get specified idx items.\n\n        Args:\n            idx (int): Index of the item.\n\n        Returns:\n            str: Utterance id (only in return_utt_id = True).\n            ndarray or tuple: Audio signal (T,) or (w/ sampling rate if return_sampling_rate = True).\n\n        """"""\n        if self.allow_cache and len(self.caches[idx]) != 0:\n            return self.caches[idx]\n\n        utt_id = self.utt_ids[idx]\n        fs, audio = self.audio_loader[utt_id]\n\n        # normalize audio signal to be [-1, 1]\n        audio = audio.astype(np.float32)\n        audio /= (1 << (16 - 1))  # assume that wav is PCM 16 bit\n\n        if self.return_sampling_rate:\n            audio = (audio, fs)\n\n        if self.return_utt_id:\n            items = utt_id, audio\n        else:\n            items = audio\n\n        if self.allow_cache:\n            self.caches[idx] = items\n\n        return items\n\n    def __len__(self):\n        """"""Return dataset length.\n\n        Returns:\n            int: The length of dataset.\n\n        """"""\n        return len(self.utt_ids)\n\n\nclass MelSCPDataset(Dataset):\n    """"""PyTorch compatible mel dataset based on kaldi-stype scp files.""""""\n\n    def __init__(self,\n                 feats_scp,\n                 mel_length_threshold=None,\n                 return_utt_id=False,\n                 allow_cache=False,\n                 ):\n        """"""Initialize dataset.\n\n        Args:\n            feats_scp (str): Kaldi-style fests.scp file.\n            mel_length_threshold (int): Threshold to remove short feature files.\n            return_utt_id (bool): Whether to return utterance id.\n            allow_cache (bool): Whether to allow cache of the loaded files.\n\n        """"""\n        # load scp as lazy dict\n        if _check_feats_scp_type(feats_scp) == ""mat"":\n            mel_loader = kaldiio.load_scp(feats_scp)\n        else:\n            mel_loader = HDF5ScpLoader(feats_scp)\n        mel_keys = list(mel_loader.keys())\n\n        # filter by threshold\n        if mel_length_threshold is not None:\n            mel_lengths = [mel.shape[0] for mel in mel_loader.values()]\n            idxs = [idx for idx in range(len(mel_keys)) if mel_lengths[idx] > mel_length_threshold]\n            if len(mel_keys) != len(idxs):\n                logging.warning(f""Some files are filtered by mel length threshold ""\n                                f""({len(mel_keys)} -> {len(idxs)})."")\n            mel_keys = [mel_keys[idx] for idx in idxs]\n\n        self.mel_loader = mel_loader\n        self.utt_ids = mel_keys\n        self.return_utt_id = return_utt_id\n        self.allow_cache = allow_cache\n\n        if allow_cache:\n            # NOTE(kan-bayashi): Manager is need to share memory in dataloader with num_workers > 0\n            self.manager = Manager()\n            self.caches = self.manager.list()\n            self.caches += [() for _ in range(len(self.utt_ids))]\n\n    def __getitem__(self, idx):\n        """"""Get specified idx items.\n\n        Args:\n            idx (int): Index of the item.\n\n        Returns:\n            str: Utterance id (only in return_utt_id = True).\n            ndarray: Feature (T\', C).\n\n        """"""\n        if self.allow_cache and len(self.caches[idx]) != 0:\n            return self.caches[idx]\n\n        utt_id = self.utt_ids[idx]\n        mel = self.mel_loader[utt_id]\n\n        if self.return_utt_id:\n            items = utt_id, mel\n        else:\n            items = mel\n\n        if self.allow_cache:\n            self.caches[idx] = items\n\n        return items\n\n    def __len__(self):\n        """"""Return dataset length.\n\n        Returns:\n            int: The length of dataset.\n\n        """"""\n        return len(self.utt_ids)\n'"
parallel_wavegan/distributed/__init__.py,0,b''
parallel_wavegan/distributed/launch.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Distributed process launcher.\n\nThis code is modified from https://github.com/pytorch/pytorch/blob/v1.3.0/torch/distributed/launch.py.\n\n""""""\nimport os\nimport subprocess\nimport sys\n\nfrom argparse import ArgumentParser\nfrom argparse import REMAINDER\n\n\ndef parse_args():\n    """"""Parse arguments.""""""\n    parser = ArgumentParser(description=""PyTorch distributed training launch ""\n                                        ""helper utilty that will spawn up ""\n                                        ""multiple distributed processes"")\n\n    # Optional arguments for the launch helper\n    parser.add_argument(""--nnodes"", type=int, default=1,\n                        help=""The number of nodes to use for distributed ""\n                             ""training"")\n    parser.add_argument(""--node_rank"", type=int, default=0,\n                        help=""The rank of the node for multi-node distributed ""\n                             ""training"")\n    parser.add_argument(""--nproc_per_node"", type=int, default=1,\n                        help=""The number of processes to launch on each node, ""\n                             ""for GPU training, this is recommended to be set ""\n                             ""to the number of GPUs in your system so that ""\n                             ""each process can be bound to a single GPU."")\n    parser.add_argument(""--master_addr"", default=""127.0.0.1"", type=str,\n                        help=""Master node (rank 0)\'s address, should be either ""\n                             ""the IP address or the hostname of node 0, for ""\n                             ""single node multi-proc training, the ""\n                             ""--master_addr can simply be 127.0.0.1"")\n    parser.add_argument(""--master_port"", default=29500, type=int,\n                        help=""Master node (rank 0)\'s free port that needs to ""\n                             ""be used for communciation during distributed ""\n                             ""training"")\n    parser.add_argument(""--use_env"", default=False, action=""store_true"",\n                        help=""Use environment variable to pass ""\n                             ""\'local rank\'. For legacy reasons, the default value is False. ""\n                             ""If set to True, the script will not pass ""\n                             ""--local_rank as argument, and will instead set LOCAL_RANK."")\n    parser.add_argument(""-m"", ""--module"", default=False, action=""store_true"",\n                        help=""Changes each process to interpret the launch script ""\n                             ""as a python module, executing with the same behavior as""\n                             ""\'python -m\'."")\n    parser.add_argument(""-c"", ""--command"", default=False, action=""store_true"",\n                        help=""Changes each process to interpret the launch script ""\n                             ""as a command."")\n\n    # positional\n    parser.add_argument(""training_script"", type=str,\n                        help=""The full path to the single GPU training ""\n                             ""program/script/command to be launched in parallel, ""\n                             ""followed by all the arguments for the ""\n                             ""training script"")\n\n    # rest from the training program\n    parser.add_argument(\'training_script_args\', nargs=REMAINDER)\n    return parser.parse_args()\n\n\ndef main():\n    """"""Launch distributed processes.""""""\n    args = parse_args()\n\n    # world size in terms of number of processes\n    dist_world_size = args.nproc_per_node * args.nnodes\n\n    # set PyTorch distributed related environmental variables\n    current_env = os.environ.copy()\n    current_env[""MASTER_ADDR""] = args.master_addr\n    current_env[""MASTER_PORT""] = str(args.master_port)\n    current_env[""WORLD_SIZE""] = str(dist_world_size)\n\n    processes = []\n\n    if \'OMP_NUM_THREADS\' not in os.environ and args.nproc_per_node > 1:\n        current_env[""OMP_NUM_THREADS""] = str(1)\n        print(""*****************************************\\n""\n              ""Setting OMP_NUM_THREADS environment variable for each process ""\n              ""to be {} in default, to avoid your system being overloaded, ""\n              ""please further tune the variable for optimal performance in ""\n              ""your application as needed. \\n""\n              ""*****************************************"".format(current_env[""OMP_NUM_THREADS""]))\n\n    for local_rank in range(0, args.nproc_per_node):\n        # each process\'s rank\n        dist_rank = args.nproc_per_node * args.node_rank + local_rank\n        current_env[""RANK""] = str(dist_rank)\n        current_env[""LOCAL_RANK""] = str(local_rank)\n\n        # spawn the processes\n        if args.command:\n            cmd = [args.training_script]\n        else:\n            cmd = [sys.executable, ""-u""]\n            if args.module:\n                cmd.append(""-m"")\n            cmd.append(args.training_script)\n\n        if not args.use_env:\n            cmd.append(""--local_rank={}"".format(local_rank))\n\n        cmd.extend(args.training_script_args)\n\n        process = subprocess.Popen(cmd, env=current_env)\n        processes.append(process)\n\n    for process in processes:\n        process.wait()\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(\n                returncode=process.returncode, cmd=cmd)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
parallel_wavegan/layers/__init__.py,0,b'from parallel_wavegan.layers.causal_conv import *  # NOQA\nfrom parallel_wavegan.layers.pqmf import *  # NOQA\nfrom parallel_wavegan.layers.residual_block import *  # NOQA\nfrom parallel_wavegan.layers.residual_stack import *  # NOQA\nfrom parallel_wavegan.layers.upsample import *  # NOQA\n'
parallel_wavegan/layers/causal_conv.py,5,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2020 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Causal convolusion layer modules.""""""\n\n\nimport torch\n\n\nclass CausalConv1d(torch.nn.Module):\n    """"""CausalConv1d module with customized initialization.""""""\n\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 dilation=1, bias=True, pad=""ConstantPad1d"", pad_params={""value"": 0.0}):\n        """"""Initialize CausalConv1d module.""""""\n        super(CausalConv1d, self).__init__()\n        self.pad = getattr(torch.nn, pad)((kernel_size - 1) * dilation, **pad_params)\n        self.conv = torch.nn.Conv1d(in_channels, out_channels, kernel_size,\n                                    dilation=dilation, bias=bias)\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input tensor (B, in_channels, T).\n\n        Returns:\n            Tensor: Output tensor (B, out_channels, T).\n\n        """"""\n        return self.conv(self.pad(x))[:, :, :x.size(2)]\n\n\nclass CausalConvTranspose1d(torch.nn.Module):\n    """"""CausalConvTranspose1d module with customized initialization.""""""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, bias=True):\n        """"""Initialize CausalConvTranspose1d module.""""""\n        super(CausalConvTranspose1d, self).__init__()\n        self.deconv = torch.nn.ConvTranspose1d(\n            in_channels, out_channels, kernel_size, stride, bias=bias)\n        self.stride = stride\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input tensor (B, in_channels, T_in).\n\n        Returns:\n            Tensor: Output tensor (B, out_channels, T_out).\n\n        """"""\n        return self.deconv(x)[:, :, :-self.stride]\n'"
parallel_wavegan/layers/pqmf.py,6,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2020 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Pseudo QMF modules.""""""\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom scipy.signal import kaiser\n\n\ndef design_prototype_filter(taps=62, cutoff_ratio=0.15, beta=9.0):\n    """"""Design prototype filter for PQMF.\n\n    This method is based on `A Kaiser window approach for the design of prototype\n    filters of cosine modulated filterbanks`_.\n\n    Args:\n        taps (int): The number of filter taps.\n        cutoff_ratio (float): Cut-off frequency ratio.\n        beta (float): Beta coefficient for kaiser window.\n\n    Returns:\n        ndarray: Impluse response of prototype filter (taps + 1,).\n\n    .. _`A Kaiser window approach for the design of prototype filters of cosine modulated filterbanks`:\n        https://ieeexplore.ieee.org/abstract/document/681427\n\n    """"""\n    # check the arguments are valid\n    assert taps % 2 == 0, ""The number of taps mush be even number.""\n    assert 0.0 < cutoff_ratio < 1.0, ""Cutoff ratio must be > 0.0 and < 1.0.""\n\n    # make initial filter\n    omega_c = np.pi * cutoff_ratio\n    with np.errstate(invalid=\'ignore\'):\n        h_i = np.sin(omega_c * (np.arange(taps + 1) - 0.5 * taps)) \\\n            / (np.pi * (np.arange(taps + 1) - 0.5 * taps))\n    h_i[taps // 2] = np.cos(0) * cutoff_ratio  # fix nan due to indeterminate form\n\n    # apply kaiser window\n    w = kaiser(taps + 1, beta)\n    h = h_i * w\n\n    return h\n\n\nclass PQMF(torch.nn.Module):\n    """"""PQMF module.\n\n    This module is based on `Near-perfect-reconstruction pseudo-QMF banks`_.\n\n    .. _`Near-perfect-reconstruction pseudo-QMF banks`:\n        https://ieeexplore.ieee.org/document/258122\n\n    """"""\n\n    def __init__(self, subbands=4, taps=62, cutoff_ratio=0.15, beta=9.0):\n        """"""Initilize PQMF module.\n\n        Args:\n            subbands (int): The number of subbands.\n            taps (int): The number of filter taps.\n            cutoff_ratio (float): Cut-off frequency ratio.\n            beta (float): Beta coefficient for kaiser window.\n\n        """"""\n        super(PQMF, self).__init__()\n\n        # define filter coefficient\n        h_proto = design_prototype_filter(taps, cutoff_ratio, beta)\n        h_analysis = np.zeros((subbands, len(h_proto)))\n        h_synthesis = np.zeros((subbands, len(h_proto)))\n        for k in range(subbands):\n            h_analysis[k] = 2 * h_proto * np.cos(\n                (2 * k + 1) * (np.pi / (2 * subbands)) *\n                (np.arange(taps + 1) - ((taps - 1) / 2)) +\n                (-1) ** k * np.pi / 4)\n            h_synthesis[k] = 2 * h_proto * np.cos(\n                (2 * k + 1) * (np.pi / (2 * subbands)) *\n                (np.arange(taps + 1) - ((taps - 1) / 2)) -\n                (-1) ** k * np.pi / 4)\n\n        # convert to tensor\n        analysis_filter = torch.from_numpy(h_analysis).float().unsqueeze(1)\n        synthesis_filter = torch.from_numpy(h_synthesis).float().unsqueeze(0)\n\n        # register coefficients as beffer\n        self.register_buffer(""analysis_filter"", analysis_filter)\n        self.register_buffer(""synthesis_filter"", synthesis_filter)\n\n        # filter for downsampling & upsampling\n        updown_filter = torch.zeros((subbands, subbands, subbands)).float()\n        for k in range(subbands):\n            updown_filter[k, k, 0] = 1.0\n        self.register_buffer(""updown_filter"", updown_filter)\n        self.subbands = subbands\n\n        # keep padding info\n        self.pad_fn = torch.nn.ConstantPad1d(taps // 2, 0.0)\n\n    def analysis(self, x):\n        """"""Analysis with PQMF.\n\n        Args:\n            x (Tensor): Input tensor (B, 1, T).\n\n        Returns:\n            Tensor: Output tensor (B, subbands, T // subbands).\n\n        """"""\n        x = F.conv1d(self.pad_fn(x), self.analysis_filter)\n        return F.conv1d(x, self.updown_filter, stride=self.subbands)\n\n    def synthesis(self, x):\n        """"""Synthesis with PQMF.\n\n        Args:\n            x (Tensor): Input tensor (B, subbands, T // subbands).\n\n        Returns:\n            Tensor: Output tensor (B, 1, T).\n\n        """"""\n        # NOTE(kan-bayashi): Power will be dreased so here multipy by # subbands.\n        #   Not sure this is the correct way, it is better to check again.\n        # TODO(kan-bayashi): Understand the reconstruction procedure\n        x = F.conv_transpose1d(x, self.updown_filter * self.subbands, stride=self.subbands)\n        return F.conv1d(self.pad_fn(x), self.synthesis_filter)\n'"
parallel_wavegan/layers/residual_block.py,6,"b'# -*- coding: utf-8 -*-\n\n""""""Residual block module in WaveNet.\n\nThis code is modified from https://github.com/r9y9/wavenet_vocoder.\n\n""""""\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\n\nclass Conv1d(torch.nn.Conv1d):\n    """"""Conv1d module with customized initialization.""""""\n\n    def __init__(self, *args, **kwargs):\n        """"""Initialize Conv1d module.""""""\n        super(Conv1d, self).__init__(*args, **kwargs)\n\n    def reset_parameters(self):\n        """"""Reset parameters.""""""\n        torch.nn.init.kaiming_normal_(self.weight, nonlinearity=""relu"")\n        if self.bias is not None:\n            torch.nn.init.constant_(self.bias, 0.0)\n\n\nclass Conv1d1x1(Conv1d):\n    """"""1x1 Conv1d with customized initialization.""""""\n\n    def __init__(self, in_channels, out_channels, bias):\n        """"""Initialize 1x1 Conv1d module.""""""\n        super(Conv1d1x1, self).__init__(in_channels, out_channels,\n                                        kernel_size=1, padding=0,\n                                        dilation=1, bias=bias)\n\n\nclass ResidualBlock(torch.nn.Module):\n    """"""Residual block module in WaveNet.""""""\n\n    def __init__(self,\n                 kernel_size=3,\n                 residual_channels=64,\n                 gate_channels=128,\n                 skip_channels=64,\n                 aux_channels=80,\n                 dropout=0.0,\n                 dilation=1,\n                 bias=True,\n                 use_causal_conv=False\n                 ):\n        """"""Initialize ResidualBlock module.\n\n        Args:\n            kernel_size (int): Kernel size of dilation convolution layer.\n            residual_channels (int): Number of channels for residual connection.\n            skip_channels (int): Number of channels for skip connection.\n            aux_channels (int): Local conditioning channels i.e. auxiliary input dimension.\n            dropout (float): Dropout probability.\n            dilation (int): Dilation factor.\n            bias (bool): Whether to add bias parameter in convolution layers.\n            use_causal_conv (bool): Whether to use use_causal_conv or non-use_causal_conv convolution.\n\n        """"""\n        super(ResidualBlock, self).__init__()\n        self.dropout = dropout\n        # no future time stamps available\n        if use_causal_conv:\n            padding = (kernel_size - 1) * dilation\n        else:\n            assert (kernel_size - 1) % 2 == 0, ""Not support even number kernel size.""\n            padding = (kernel_size - 1) // 2 * dilation\n        self.use_causal_conv = use_causal_conv\n\n        # dilation conv\n        self.conv = Conv1d(residual_channels, gate_channels, kernel_size,\n                           padding=padding, dilation=dilation, bias=bias)\n\n        # local conditioning\n        if aux_channels > 0:\n            self.conv1x1_aux = Conv1d1x1(aux_channels, gate_channels, bias=False)\n        else:\n            self.conv1x1_aux = None\n\n        # conv output is split into two groups\n        gate_out_channels = gate_channels // 2\n        self.conv1x1_out = Conv1d1x1(gate_out_channels, residual_channels, bias=bias)\n        self.conv1x1_skip = Conv1d1x1(gate_out_channels, skip_channels, bias=bias)\n\n    def forward(self, x, c):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input tensor (B, residual_channels, T).\n            c (Tensor): Local conditioning auxiliary tensor (B, aux_channels, T).\n\n        Returns:\n            Tensor: Output tensor for residual connection (B, residual_channels, T).\n            Tensor: Output tensor for skip connection (B, skip_channels, T).\n\n        """"""\n        residual = x\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.conv(x)\n\n        # remove future time steps if use_causal_conv conv\n        x = x[:, :, :residual.size(-1)] if self.use_causal_conv else x\n\n        # split into two part for gated activation\n        splitdim = 1\n        xa, xb = x.split(x.size(splitdim) // 2, dim=splitdim)\n\n        # local conditioning\n        if c is not None:\n            assert self.conv1x1_aux is not None\n            c = self.conv1x1_aux(c)\n            ca, cb = c.split(c.size(splitdim) // 2, dim=splitdim)\n            xa, xb = xa + ca, xb + cb\n\n        x = torch.tanh(xa) * torch.sigmoid(xb)\n\n        # for skip connection\n        s = self.conv1x1_skip(x)\n\n        # for residual connection\n        x = (self.conv1x1_out(x) + residual) * math.sqrt(0.5)\n\n        return x, s\n'"
parallel_wavegan/layers/residual_stack.py,12,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2020 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Residual stack module in MelGAN.""""""\n\nimport torch\n\nfrom parallel_wavegan.layers import CausalConv1d\n\n\nclass ResidualStack(torch.nn.Module):\n    """"""Residual stack module introduced in MelGAN.""""""\n\n    def __init__(self,\n                 kernel_size=3,\n                 channels=32,\n                 dilation=1,\n                 bias=True,\n                 nonlinear_activation=""LeakyReLU"",\n                 nonlinear_activation_params={""negative_slope"": 0.2},\n                 pad=""ReflectionPad1d"",\n                 pad_params={},\n                 use_causal_conv=False,\n                 ):\n        """"""Initialize ResidualStack module.\n\n        Args:\n            kernel_size (int): Kernel size of dilation convolution layer.\n            channels (int): Number of channels of convolution layers.\n            dilation (int): Dilation factor.\n            bias (bool): Whether to add bias parameter in convolution layers.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n            pad (str): Padding function module name before dilated convolution layer.\n            pad_params (dict): Hyperparameters for padding function.\n            use_causal_conv (bool): Whether to use causal convolution.\n\n        """"""\n        super(ResidualStack, self).__init__()\n\n        # defile residual stack part\n        if not use_causal_conv:\n            assert (kernel_size - 1) % 2 == 0, ""Not support even number kernel size.""\n            self.stack = torch.nn.Sequential(\n                getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params),\n                getattr(torch.nn, pad)((kernel_size - 1) // 2 * dilation, **pad_params),\n                torch.nn.Conv1d(channels, channels, kernel_size, dilation=dilation, bias=bias),\n                getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params),\n                torch.nn.Conv1d(channels, channels, 1, bias=bias),\n            )\n        else:\n            self.stack = torch.nn.Sequential(\n                getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params),\n                CausalConv1d(channels, channels, kernel_size, dilation=dilation,\n                             bias=bias, pad=pad, pad_params=pad_params),\n                getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params),\n                torch.nn.Conv1d(channels, channels, 1, bias=bias),\n            )\n\n        # defile extra layer for skip connection\n        self.skip_layer = torch.nn.Conv1d(channels, channels, 1, bias=bias)\n\n    def forward(self, c):\n        """"""Calculate forward propagation.\n\n        Args:\n            c (Tensor): Input tensor (B, channels, T).\n\n        Returns:\n            Tensor: Output tensor (B, chennels, T).\n\n        """"""\n        return self.stack(c) + self.skip_layer(c)\n'"
parallel_wavegan/layers/tf_layers.py,1,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2020 MINH ANH (@dathudeptrai)\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Tensorflow Layer modules complatible with pytorch.""""""\n\nimport tensorflow as tf\n\n\nclass TFReflectionPad1d(tf.keras.layers.Layer):\n    """"""Tensorflow ReflectionPad1d module.""""""\n\n    def __init__(self, padding_size):\n        """"""Initialize TFReflectionPad1d module.\n\n        Args:\n            padding_size (int): Padding size.\n\n        """"""\n        super(TFReflectionPad1d, self).__init__()\n        self.padding_size = padding_size\n\n    @tf.function\n    def call(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input tensor (B, T, 1, C).\n\n        Returns:\n            Tensor: Padded tensor (B, T + 2 * padding_size, 1, C).\n\n        """"""\n        return tf.pad(x, [[0, 0], [self.padding_size, self.padding_size], [0, 0], [0, 0]], ""REFLECT"")\n\n\nclass TFConvTranspose1d(tf.keras.layers.Layer):\n    """"""Tensorflow ConvTranspose1d module.""""""\n\n    def __init__(self, channels, kernel_size, stride, padding):\n        """"""Initialize TFConvTranspose1d( module.\n\n        Args:\n            channels (int): Number of channels.\n            kernel_size (int): kernel size.\n            strides (int): Stride width.\n            padding (str): Padding type (""same"" or ""valid"").\n\n        """"""\n        super(TFConvTranspose1d, self).__init__()\n        self.conv1d_transpose = tf.keras.layers.Conv2DTranspose(\n            filters=channels,\n            kernel_size=(kernel_size, 1),\n            strides=(stride, 1),\n            padding=padding,\n        )\n\n    @tf.function\n    def call(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input tensor (B, T, 1, C).\n\n        Returns:\n            Tensors: Output tensor (B, T\', 1, C\').\n\n        """"""\n        x = self.conv1d_transpose(x)\n        return x\n\n\nclass TFResidualStack(tf.keras.layers.Layer):\n    """"""Tensorflow ResidualStack module.""""""\n\n    def __init__(self,\n                 kernel_size,\n                 channels,\n                 dilation,\n                 bias,\n                 nonlinear_activation,\n                 nonlinear_activation_params,\n                 padding,\n                 ):\n        """"""Initialize TFResidualStack module.\n\n        Args:\n            kernel_size (int): Kernel size.\n            channles (int): Number of channels.\n            dilation (int): Dilation ine.\n            bias (bool): Whether to add bias parameter in convolution layers.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n            padding (str): Padding type (""same"" or ""valid"").\n\n        """"""\n        super(TFResidualStack, self).__init__()\n        self.block = [\n            getattr(tf.keras.layers, nonlinear_activation)(**nonlinear_activation_params),\n            TFReflectionPad1d(dilation),\n            tf.keras.layers.Conv2D(\n                filters=channels,\n                kernel_size=(kernel_size, 1),\n                dilation_rate=(dilation, 1),\n                use_bias=bias,\n                padding=""valid"",\n            ),\n            getattr(tf.keras.layers, nonlinear_activation)(**nonlinear_activation_params),\n            tf.keras.layers.Conv2D(filters=channels, kernel_size=1, use_bias=bias)\n        ]\n        self.shortcut = tf.keras.layers.Conv2D(filters=channels, kernel_size=1, use_bias=bias)\n\n    @tf.function\n    def call(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input tensor (B, T, 1, C).\n\n        Returns:\n            Tensor: Output tensor (B, T, 1, C).\n\n        """"""\n        _x = tf.identity(x)\n        for i, layer in enumerate(self.block):\n            _x = layer(_x)\n        shortcut = self.shortcut(x)\n        return shortcut + _x\n'"
parallel_wavegan/layers/upsample.py,8,"b'# -*- coding: utf-8 -*-\n\n""""""Upsampling module.\n\nThis code is modified from https://github.com/r9y9/wavenet_vocoder.\n\n""""""\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom parallel_wavegan.layers import Conv1d\n\n\nclass Stretch2d(torch.nn.Module):\n    """"""Stretch2d module.""""""\n\n    def __init__(self, x_scale, y_scale, mode=""nearest""):\n        """"""Initialize Stretch2d module.\n\n        Args:\n            x_scale (int): X scaling factor (Time axis in spectrogram).\n            y_scale (int): Y scaling factor (Frequency axis in spectrogram).\n            mode (str): Interpolation mode.\n\n        """"""\n        super(Stretch2d, self).__init__()\n        self.x_scale = x_scale\n        self.y_scale = y_scale\n        self.mode = mode\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input tensor (B, C, F, T).\n\n        Returns:\n            Tensor: Interpolated tensor (B, C, F * y_scale, T * x_scale),\n\n        """"""\n        return F.interpolate(\n            x, scale_factor=(self.y_scale, self.x_scale), mode=self.mode)\n\n\nclass Conv2d(torch.nn.Conv2d):\n    """"""Conv2d module with customized initialization.""""""\n\n    def __init__(self, *args, **kwargs):\n        """"""Initialize Conv2d module.""""""\n        super(Conv2d, self).__init__(*args, **kwargs)\n\n    def reset_parameters(self):\n        """"""Reset parameters.""""""\n        self.weight.data.fill_(1. / np.prod(self.kernel_size))\n        if self.bias is not None:\n            torch.nn.init.constant_(self.bias, 0.0)\n\n\nclass UpsampleNetwork(torch.nn.Module):\n    """"""Upsampling network module.""""""\n\n    def __init__(self,\n                 upsample_scales,\n                 nonlinear_activation=None,\n                 nonlinear_activation_params={},\n                 interpolate_mode=""nearest"",\n                 freq_axis_kernel_size=1,\n                 use_causal_conv=False,\n                 ):\n        """"""Initialize upsampling network module.\n\n        Args:\n            upsample_scales (list): List of upsampling scales.\n            nonlinear_activation (str): Activation function name.\n            nonlinear_activation_params (dict): Arguments for specified activation function.\n            interpolate_mode (str): Interpolation mode.\n            freq_axis_kernel_size (int): Kernel size in the direction of frequency axis.\n\n        """"""\n        super(UpsampleNetwork, self).__init__()\n        self.use_causal_conv = use_causal_conv\n        self.up_layers = torch.nn.ModuleList()\n        for scale in upsample_scales:\n            # interpolation layer\n            stretch = Stretch2d(scale, 1, interpolate_mode)\n            self.up_layers += [stretch]\n\n            # conv layer\n            assert (freq_axis_kernel_size - 1) % 2 == 0, ""Not support even number freq axis kernel size.""\n            freq_axis_padding = (freq_axis_kernel_size - 1) // 2\n            kernel_size = (freq_axis_kernel_size, scale * 2 + 1)\n            if use_causal_conv:\n                padding = (freq_axis_padding, scale * 2)\n            else:\n                padding = (freq_axis_padding, scale)\n            conv = Conv2d(1, 1, kernel_size=kernel_size, padding=padding, bias=False)\n            self.up_layers += [conv]\n\n            # nonlinear\n            if nonlinear_activation is not None:\n                nonlinear = getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params)\n                self.up_layers += [nonlinear]\n\n    def forward(self, c):\n        """"""Calculate forward propagation.\n\n        Args:\n            c : Input tensor (B, C, T).\n\n        Returns:\n            Tensor: Upsampled tensor (B, C, T\'), where T\' = T * prod(upsample_scales).\n\n        """"""\n        c = c.unsqueeze(1)  # (B, 1, C, T)\n        for f in self.up_layers:\n            if self.use_causal_conv and isinstance(f, Conv2d):\n                c = f(c)[..., :c.size(-1)]\n            else:\n                c = f(c)\n        return c.squeeze(1)  # (B, C, T\')\n\n\nclass ConvInUpsampleNetwork(torch.nn.Module):\n    """"""Convolution + upsampling network module.""""""\n\n    def __init__(self,\n                 upsample_scales,\n                 nonlinear_activation=None,\n                 nonlinear_activation_params={},\n                 interpolate_mode=""nearest"",\n                 freq_axis_kernel_size=1,\n                 aux_channels=80,\n                 aux_context_window=0,\n                 use_causal_conv=False\n                 ):\n        """"""Initialize convolution + upsampling network module.\n\n        Args:\n            upsample_scales (list): List of upsampling scales.\n            nonlinear_activation (str): Activation function name.\n            nonlinear_activation_params (dict): Arguments for specified activation function.\n            mode (str): Interpolation mode.\n            freq_axis_kernel_size (int): Kernel size in the direction of frequency axis.\n            aux_channels (int): Number of channels of pre-convolutional layer.\n            aux_context_window (int): Context window size of the pre-convolutional layer.\n            use_causal_conv (bool): Whether to use causal structure.\n\n        """"""\n        super(ConvInUpsampleNetwork, self).__init__()\n        self.aux_context_window = aux_context_window\n        self.use_causal_conv = use_causal_conv and aux_context_window > 0\n        # To capture wide-context information in conditional features\n        kernel_size = aux_context_window + 1 if use_causal_conv else 2 * aux_context_window + 1\n        # NOTE(kan-bayashi): Here do not use padding because the input is already padded\n        self.conv_in = Conv1d(aux_channels, aux_channels, kernel_size=kernel_size, bias=False)\n        self.upsample = UpsampleNetwork(\n            upsample_scales=upsample_scales,\n            nonlinear_activation=nonlinear_activation,\n            nonlinear_activation_params=nonlinear_activation_params,\n            interpolate_mode=interpolate_mode,\n            freq_axis_kernel_size=freq_axis_kernel_size,\n            use_causal_conv=use_causal_conv,\n        )\n\n    def forward(self, c):\n        """"""Calculate forward propagation.\n\n        Args:\n            c : Input tensor (B, C, T\').\n\n        Returns:\n            Tensor: Upsampled tensor (B, C, T),\n                where T = (T\' - aux_context_window * 2) * prod(upsample_scales).\n\n        Note:\n            The length of inputs considers the context window size.\n\n        """"""\n        c_ = self.conv_in(c)\n        c = c_[:, :, :-self.aux_context_window] if self.use_causal_conv else c_\n        return self.upsample(c)\n'"
parallel_wavegan/losses/__init__.py,0,b'from parallel_wavegan.losses.stft_loss import *  # NOQA\n'
parallel_wavegan/losses/stft_loss.py,10,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""STFT-based Loss modules.""""""\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef stft(x, fft_size, hop_size, win_length, window):\n    """"""Perform STFT and convert to magnitude spectrogram.\n\n    Args:\n        x (Tensor): Input signal tensor (B, T).\n        fft_size (int): FFT size.\n        hop_size (int): Hop size.\n        win_length (int): Window length.\n        window (str): Window function type.\n\n    Returns:\n        Tensor: Magnitude spectrogram (B, #frames, fft_size // 2 + 1).\n\n    """"""\n    x_stft = torch.stft(x, fft_size, hop_size, win_length, window)\n    real = x_stft[..., 0]\n    imag = x_stft[..., 1]\n\n    # NOTE(kan-bayashi): clamp is needed to avoid nan or inf\n    return torch.sqrt(torch.clamp(real ** 2 + imag ** 2, min=1e-7)).transpose(2, 1)\n\n\nclass SpectralConvergengeLoss(torch.nn.Module):\n    """"""Spectral convergence loss module.""""""\n\n    def __init__(self):\n        """"""Initilize spectral convergence loss module.""""""\n        super(SpectralConvergengeLoss, self).__init__()\n\n    def forward(self, x_mag, y_mag):\n        """"""Calculate forward propagation.\n\n        Args:\n            x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n            y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n\n        Returns:\n            Tensor: Spectral convergence loss value.\n\n        """"""\n        return torch.norm(y_mag - x_mag, p=""fro"") / torch.norm(y_mag, p=""fro"")\n\n\nclass LogSTFTMagnitudeLoss(torch.nn.Module):\n    """"""Log STFT magnitude loss module.""""""\n\n    def __init__(self):\n        """"""Initilize los STFT magnitude loss module.""""""\n        super(LogSTFTMagnitudeLoss, self).__init__()\n\n    def forward(self, x_mag, y_mag):\n        """"""Calculate forward propagation.\n\n        Args:\n            x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n            y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n\n        Returns:\n            Tensor: Log STFT magnitude loss value.\n\n        """"""\n        return F.l1_loss(torch.log(y_mag), torch.log(x_mag))\n\n\nclass STFTLoss(torch.nn.Module):\n    """"""STFT loss module.""""""\n\n    def __init__(self, fft_size=1024, shift_size=120, win_length=600, window=""hann_window""):\n        """"""Initialize STFT loss module.""""""\n        super(STFTLoss, self).__init__()\n        self.fft_size = fft_size\n        self.shift_size = shift_size\n        self.win_length = win_length\n        self.window = getattr(torch, window)(win_length)\n        self.spectral_convergenge_loss = SpectralConvergengeLoss()\n        self.log_stft_magnitude_loss = LogSTFTMagnitudeLoss()\n\n    def forward(self, x, y):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Predicted signal (B, T).\n            y (Tensor): Groundtruth signal (B, T).\n\n        Returns:\n            Tensor: Spectral convergence loss value.\n            Tensor: Log STFT magnitude loss value.\n\n        """"""\n        x_mag = stft(x, self.fft_size, self.shift_size, self.win_length, self.window)\n        y_mag = stft(y, self.fft_size, self.shift_size, self.win_length, self.window)\n        sc_loss = self.spectral_convergenge_loss(x_mag, y_mag)\n        mag_loss = self.log_stft_magnitude_loss(x_mag, y_mag)\n\n        return sc_loss, mag_loss\n\n\nclass MultiResolutionSTFTLoss(torch.nn.Module):\n    """"""Multi resolution STFT loss module.""""""\n\n    def __init__(self,\n                 fft_sizes=[1024, 2048, 512],\n                 hop_sizes=[120, 240, 50],\n                 win_lengths=[600, 1200, 240],\n                 window=""hann_window""):\n        """"""Initialize Multi resolution STFT loss module.\n\n        Args:\n            fft_sizes (list): List of FFT sizes.\n            hop_sizes (list): List of hop sizes.\n            win_lengths (list): List of window lengths.\n            window (str): Window function type.\n\n        """"""\n        super(MultiResolutionSTFTLoss, self).__init__()\n        assert len(fft_sizes) == len(hop_sizes) == len(win_lengths)\n        self.stft_losses = torch.nn.ModuleList()\n        for fs, ss, wl in zip(fft_sizes, hop_sizes, win_lengths):\n            self.stft_losses += [STFTLoss(fs, ss, wl, window)]\n\n    def forward(self, x, y):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Predicted signal (B, T).\n            y (Tensor): Groundtruth signal (B, T).\n\n        Returns:\n            Tensor: Multi resolution spectral convergence loss value.\n            Tensor: Multi resolution log STFT magnitude loss value.\n\n        """"""\n        sc_loss = 0.0\n        mag_loss = 0.0\n        for f in self.stft_losses:\n            sc_l, mag_l = f(x, y)\n            sc_loss += sc_l\n            mag_loss += mag_l\n        sc_loss /= len(self.stft_losses)\n        mag_loss /= len(self.stft_losses)\n\n        return sc_loss, mag_loss\n'"
parallel_wavegan/models/__init__.py,0,b'from parallel_wavegan.models.melgan import *  # NOQA\nfrom parallel_wavegan.models.parallel_wavegan import *  # NOQA\n'
parallel_wavegan/models/melgan.py,35,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2020 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""MelGAN Modules.""""""\n\nimport logging\n\nimport numpy as np\nimport torch\n\nfrom parallel_wavegan.layers import CausalConv1d\nfrom parallel_wavegan.layers import CausalConvTranspose1d\nfrom parallel_wavegan.layers import ResidualStack\n\n\nclass MelGANGenerator(torch.nn.Module):\n    """"""MelGAN generator module.""""""\n\n    def __init__(self,\n                 in_channels=80,\n                 out_channels=1,\n                 kernel_size=7,\n                 channels=512,\n                 bias=True,\n                 upsample_scales=[8, 8, 2, 2],\n                 stack_kernel_size=3,\n                 stacks=3,\n                 nonlinear_activation=""LeakyReLU"",\n                 nonlinear_activation_params={""negative_slope"": 0.2},\n                 pad=""ReflectionPad1d"",\n                 pad_params={},\n                 use_final_nonlinear_activation=True,\n                 use_weight_norm=True,\n                 use_causal_conv=False,\n                 ):\n        """"""Initialize MelGANGenerator module.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            kernel_size (int): Kernel size of initial and final conv layer.\n            channels (int): Initial number of channels for conv layer.\n            bias (bool): Whether to add bias parameter in convolution layers.\n            upsample_scales (list): List of upsampling scales.\n            stack_kernel_size (int): Kernel size of dilated conv layers in residual stack.\n            stacks (int): Number of stacks in a single residual stack.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n            pad (str): Padding function module name before dilated convolution layer.\n            pad_params (dict): Hyperparameters for padding function.\n            use_final_nonlinear_activation (torch.nn.Module): Activation function for the final layer.\n            use_weight_norm (bool): Whether to use weight norm.\n                If set to true, it will be applied to all of the conv layers.\n            use_causal_conv (bool): Whether to use causal convolution.\n\n        """"""\n        super(MelGANGenerator, self).__init__()\n\n        # check hyper parameters is valid\n        assert channels >= np.prod(upsample_scales)\n        assert channels % (2 ** len(upsample_scales)) == 0\n        if not use_causal_conv:\n            assert (kernel_size - 1) % 2 == 0, ""Not support even number kernel size.""\n\n        # add initial layer\n        layers = []\n        if not use_causal_conv:\n            layers += [\n                getattr(torch.nn, pad)((kernel_size - 1) // 2, **pad_params),\n                torch.nn.Conv1d(in_channels, channels, kernel_size, bias=bias),\n            ]\n        else:\n            layers += [\n                CausalConv1d(in_channels, channels, kernel_size,\n                             bias=bias, pad=pad, pad_params=pad_params),\n            ]\n\n        for i, upsample_scale in enumerate(upsample_scales):\n            # add upsampling layer\n            layers += [getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params)]\n            if not use_causal_conv:\n                layers += [\n                    torch.nn.ConvTranspose1d(\n                        channels // (2 ** i),\n                        channels // (2 ** (i + 1)),\n                        upsample_scale * 2,\n                        stride=upsample_scale,\n                        padding=upsample_scale // 2 + upsample_scale % 2,\n                        output_padding=upsample_scale % 2,\n                        bias=bias,\n                    )\n                ]\n            else:\n                layers += [\n                    CausalConvTranspose1d(\n                        channels // (2 ** i),\n                        channels // (2 ** (i + 1)),\n                        upsample_scale * 2,\n                        stride=upsample_scale,\n                        bias=bias,\n                    )\n                ]\n\n            # add residual stack\n            for j in range(stacks):\n                layers += [\n                    ResidualStack(\n                        kernel_size=stack_kernel_size,\n                        channels=channels // (2 ** (i + 1)),\n                        dilation=stack_kernel_size ** j,\n                        bias=bias,\n                        nonlinear_activation=nonlinear_activation,\n                        nonlinear_activation_params=nonlinear_activation_params,\n                        pad=pad,\n                        pad_params=pad_params,\n                        use_causal_conv=use_causal_conv,\n                    )\n                ]\n\n        # add final layer\n        layers += [getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params)]\n        if not use_causal_conv:\n            layers += [\n                getattr(torch.nn, pad)((kernel_size - 1) // 2, **pad_params),\n                torch.nn.Conv1d(channels // (2 ** (i + 1)), out_channels, kernel_size, bias=bias),\n            ]\n        else:\n            layers += [\n                CausalConv1d(channels // (2 ** (i + 1)), out_channels, kernel_size,\n                             bias=bias, pad=pad, pad_params=pad_params),\n            ]\n        if use_final_nonlinear_activation:\n            layers += [torch.nn.Tanh()]\n\n        # define the model as a single function\n        self.melgan = torch.nn.Sequential(*layers)\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n        # reset parameters\n        self.reset_parameters()\n\n    def forward(self, c):\n        """"""Calculate forward propagation.\n\n        Args:\n            c (Tensor): Input tensor (B, channels, T).\n\n        Returns:\n            Tensor: Output tensor (B, 1, T ** prod(upsample_scales)).\n\n        """"""\n        return self.melgan(c)\n\n    def remove_weight_norm(self):\n        """"""Remove weight normalization module from all of the layers.""""""\n        def _remove_weight_norm(m):\n            try:\n                logging.debug(f""Weight norm is removed from {m}."")\n                torch.nn.utils.remove_weight_norm(m)\n            except ValueError:  # this module didn\'t have weight norm\n                return\n\n        self.apply(_remove_weight_norm)\n\n    def apply_weight_norm(self):\n        """"""Apply weight normalization module from all of the layers.""""""\n        def _apply_weight_norm(m):\n            if isinstance(m, torch.nn.Conv1d) or isinstance(m, torch.nn.ConvTranspose1d):\n                torch.nn.utils.weight_norm(m)\n                logging.debug(f""Weight norm is applied to {m}."")\n\n        self.apply(_apply_weight_norm)\n\n    def reset_parameters(self):\n        """"""Reset parameters.\n\n        This initialization follows official implementation manner.\n        https://github.com/descriptinc/melgan-neurips/blob/master/mel2wav/modules.py\n\n        """"""\n        def _reset_parameters(m):\n            if isinstance(m, torch.nn.Conv1d) or isinstance(m, torch.nn.ConvTranspose1d):\n                m.weight.data.normal_(0.0, 0.02)\n                logging.debug(f""Reset parameters in {m}."")\n\n        self.apply(_reset_parameters)\n\n\nclass MelGANDiscriminator(torch.nn.Module):\n    """"""MelGAN discriminator module.""""""\n\n    def __init__(self,\n                 in_channels=1,\n                 out_channels=1,\n                 kernel_sizes=[5, 3],\n                 channels=16,\n                 max_downsample_channels=1024,\n                 bias=True,\n                 downsample_scales=[4, 4, 4, 4],\n                 nonlinear_activation=""LeakyReLU"",\n                 nonlinear_activation_params={""negative_slope"": 0.2},\n                 pad=""ReflectionPad1d"",\n                 pad_params={},\n                 ):\n        """"""Initilize MelGAN discriminator module.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            kernel_sizes (list): List of two kernel sizes. The prod will be used for the first conv layer,\n                and the first and the second kernel sizes will be used for the last two layers.\n                For example if kernel_sizes = [5, 3], the first layer kernel size will be 5 * 3 = 15,\n                the last two layers\' kernel size will be 5 and 3, respectively.\n            channels (int): Initial number of channels for conv layer.\n            max_downsample_channels (int): Maximum number of channels for downsampling layers.\n            bias (bool): Whether to add bias parameter in convolution layers.\n            downsample_scales (list): List of downsampling scales.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n            pad (str): Padding function module name before dilated convolution layer.\n            pad_params (dict): Hyperparameters for padding function.\n\n        """"""\n        super(MelGANDiscriminator, self).__init__()\n        self.layers = torch.nn.ModuleList()\n\n        # check kernel size is valid\n        assert len(kernel_sizes) == 2\n        assert kernel_sizes[0] % 2 == 1\n        assert kernel_sizes[1] % 2 == 1\n\n        # add first layer\n        self.layers += [\n            torch.nn.Sequential(\n                getattr(torch.nn, pad)((np.prod(kernel_sizes) - 1) // 2, **pad_params),\n                torch.nn.Conv1d(in_channels, channels, np.prod(kernel_sizes), bias=bias),\n                getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params),\n            )\n        ]\n\n        # add downsample layers\n        in_chs = channels\n        for downsample_scale in downsample_scales:\n            out_chs = min(in_chs * downsample_scale, max_downsample_channels)\n            self.layers += [\n                torch.nn.Sequential(\n                    torch.nn.Conv1d(\n                        in_chs, out_chs,\n                        kernel_size=downsample_scale * 10 + 1,\n                        stride=downsample_scale,\n                        padding=downsample_scale * 5,\n                        groups=in_chs // 4,\n                        bias=bias,\n                    ),\n                    getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params),\n                )\n            ]\n            in_chs = out_chs\n\n        # add final layers\n        out_chs = min(in_chs * 2, max_downsample_channels)\n        self.layers += [\n            torch.nn.Sequential(\n                torch.nn.Conv1d(\n                    in_chs, out_chs, kernel_sizes[0],\n                    padding=(kernel_sizes[0] - 1) // 2,\n                    bias=bias,\n                ),\n                getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params),\n            )\n        ]\n        self.layers += [\n            torch.nn.Conv1d(\n                out_chs, out_channels, kernel_sizes[1],\n                padding=(kernel_sizes[1] - 1) // 2,\n                bias=bias,\n            ),\n        ]\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input noise signal (B, 1, T).\n\n        Returns:\n            List: List of output tensors of each layer.\n\n        """"""\n        outs = []\n        for f in self.layers:\n            x = f(x)\n            outs += [x]\n\n        return outs\n\n\nclass MelGANMultiScaleDiscriminator(torch.nn.Module):\n    """"""MelGAN multi-scale discriminator module.""""""\n\n    def __init__(self,\n                 in_channels=1,\n                 out_channels=1,\n                 scales=3,\n                 downsample_pooling=""AvgPool1d"",\n                 # follow the official implementation setting\n                 downsample_pooling_params={\n                     ""kernel_size"": 4,\n                     ""stride"": 2,\n                     ""padding"": 1,\n                     ""count_include_pad"": False,\n                 },\n                 kernel_sizes=[5, 3],\n                 channels=16,\n                 max_downsample_channels=1024,\n                 bias=True,\n                 downsample_scales=[4, 4, 4, 4],\n                 nonlinear_activation=""LeakyReLU"",\n                 nonlinear_activation_params={""negative_slope"": 0.2},\n                 pad=""ReflectionPad1d"",\n                 pad_params={},\n                 use_weight_norm=True,\n                 ):\n        """"""Initilize MelGAN multi-scale discriminator module.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            downsample_pooling (str): Pooling module name for downsampling of the inputs.\n            downsample_pooling_params (dict): Parameters for the above pooling module.\n            kernel_sizes (list): List of two kernel sizes. The sum will be used for the first conv layer,\n                and the first and the second kernel sizes will be used for the last two layers.\n            channels (int): Initial number of channels for conv layer.\n            max_downsample_channels (int): Maximum number of channels for downsampling layers.\n            bias (bool): Whether to add bias parameter in convolution layers.\n            downsample_scales (list): List of downsampling scales.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n            pad (str): Padding function module name before dilated convolution layer.\n            pad_params (dict): Hyperparameters for padding function.\n            use_causal_conv (bool): Whether to use causal convolution.\n\n        """"""\n        super(MelGANMultiScaleDiscriminator, self).__init__()\n        self.discriminators = torch.nn.ModuleList()\n\n        # add discriminators\n        for _ in range(scales):\n            self.discriminators += [\n                MelGANDiscriminator(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    kernel_sizes=kernel_sizes,\n                    channels=channels,\n                    max_downsample_channels=max_downsample_channels,\n                    bias=bias,\n                    downsample_scales=downsample_scales,\n                    nonlinear_activation=nonlinear_activation,\n                    nonlinear_activation_params=nonlinear_activation_params,\n                    pad=pad,\n                    pad_params=pad_params,\n                )\n            ]\n        self.pooling = getattr(torch.nn, downsample_pooling)(**downsample_pooling_params)\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n        # reset parameters\n        self.reset_parameters()\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input noise signal (B, 1, T).\n\n        Returns:\n            List: List of list of each discriminator outputs, which consists of each layer output tensors.\n\n        """"""\n        outs = []\n        for f in self.discriminators:\n            outs += [f(x)]\n            x = self.pooling(x)\n\n        return outs\n\n    def remove_weight_norm(self):\n        """"""Remove weight normalization module from all of the layers.""""""\n        def _remove_weight_norm(m):\n            try:\n                logging.debug(f""Weight norm is removed from {m}."")\n                torch.nn.utils.remove_weight_norm(m)\n            except ValueError:  # this module didn\'t have weight norm\n                return\n\n        self.apply(_remove_weight_norm)\n\n    def apply_weight_norm(self):\n        """"""Apply weight normalization module from all of the layers.""""""\n        def _apply_weight_norm(m):\n            if isinstance(m, torch.nn.Conv1d) or isinstance(m, torch.nn.ConvTranspose1d):\n                torch.nn.utils.weight_norm(m)\n                logging.debug(f""Weight norm is applied to {m}."")\n\n        self.apply(_apply_weight_norm)\n\n    def reset_parameters(self):\n        """"""Reset parameters.\n\n        This initialization follows official implementation manner.\n        https://github.com/descriptinc/melgan-neurips/blob/master/mel2wav/modules.py\n\n        """"""\n        def _reset_parameters(m):\n            if isinstance(m, torch.nn.Conv1d) or isinstance(m, torch.nn.ConvTranspose1d):\n                m.weight.data.normal_(0.0, 0.02)\n                logging.debug(f""Reset parameters in {m}."")\n\n        self.apply(_reset_parameters)\n'"
parallel_wavegan/models/parallel_wavegan.py,24,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Parallel WaveGAN Modules.""""""\n\nimport logging\nimport math\n\nimport torch\n\nfrom parallel_wavegan.layers import Conv1d\nfrom parallel_wavegan.layers import Conv1d1x1\nfrom parallel_wavegan.layers import ResidualBlock\nfrom parallel_wavegan.layers import upsample\nfrom parallel_wavegan import models\n\n\nclass ParallelWaveGANGenerator(torch.nn.Module):\n    """"""Parallel WaveGAN Generator module.""""""\n\n    def __init__(self,\n                 in_channels=1,\n                 out_channels=1,\n                 kernel_size=3,\n                 layers=30,\n                 stacks=3,\n                 residual_channels=64,\n                 gate_channels=128,\n                 skip_channels=64,\n                 aux_channels=80,\n                 aux_context_window=2,\n                 dropout=0.0,\n                 bias=True,\n                 use_weight_norm=True,\n                 use_causal_conv=False,\n                 upsample_conditional_features=True,\n                 upsample_net=""ConvInUpsampleNetwork"",\n                 upsample_params={""upsample_scales"": [4, 4, 4, 4]},\n                 ):\n        """"""Initialize Parallel WaveGAN Generator module.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            kernel_size (int): Kernel size of dilated convolution.\n            layers (int): Number of residual block layers.\n            stacks (int): Number of stacks i.e., dilation cycles.\n            residual_channels (int): Number of channels in residual conv.\n            gate_channels (int):  Number of channels in gated conv.\n            skip_channels (int): Number of channels in skip conv.\n            aux_channels (int): Number of channels for auxiliary feature conv.\n            aux_context_window (int): Context window size for auxiliary feature.\n            dropout (float): Dropout rate. 0.0 means no dropout applied.\n            bias (bool): Whether to use bias parameter in conv layer.\n            use_weight_norm (bool): Whether to use weight norm.\n                If set to true, it will be applied to all of the conv layers.\n            use_causal_conv (bool): Whether to use causal structure.\n            upsample_conditional_features (bool): Whether to use upsampling network.\n            upsample_net (str): Upsampling network architecture.\n            upsample_params (dict): Upsampling network parameters.\n\n        """"""\n        super(ParallelWaveGANGenerator, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.aux_channels = aux_channels\n        self.layers = layers\n        self.stacks = stacks\n        self.kernel_size = kernel_size\n\n        # check the number of layers and stacks\n        assert layers % stacks == 0\n        layers_per_stack = layers // stacks\n\n        # define first convolution\n        self.first_conv = Conv1d1x1(in_channels, residual_channels, bias=True)\n\n        # define conv + upsampling network\n        if upsample_conditional_features:\n            upsample_params.update({\n                ""use_causal_conv"": use_causal_conv,\n            })\n            if upsample_net == ""MelGANGenerator"":\n                assert aux_context_window == 0\n                upsample_params.update({\n                    ""use_weight_norm"": False,  # not to apply twice\n                    ""use_final_nonlinear_activation"": False,\n                })\n                self.upsample_net = getattr(models, upsample_net)(**upsample_params)\n            else:\n                if upsample_net == ""ConvInUpsampleNetwork"":\n                    upsample_params.update({\n                        ""aux_channels"": aux_channels,\n                        ""aux_context_window"": aux_context_window,\n                    })\n                self.upsample_net = getattr(upsample, upsample_net)(**upsample_params)\n        else:\n            self.upsample_net = None\n\n        # define residual blocks\n        self.conv_layers = torch.nn.ModuleList()\n        for layer in range(layers):\n            dilation = 2 ** (layer % layers_per_stack)\n            conv = ResidualBlock(\n                kernel_size=kernel_size,\n                residual_channels=residual_channels,\n                gate_channels=gate_channels,\n                skip_channels=skip_channels,\n                aux_channels=aux_channels,\n                dilation=dilation,\n                dropout=dropout,\n                bias=bias,\n                use_causal_conv=use_causal_conv,\n            )\n            self.conv_layers += [conv]\n\n        # define output layers\n        self.last_conv_layers = torch.nn.ModuleList([\n            torch.nn.ReLU(inplace=True),\n            Conv1d1x1(skip_channels, skip_channels, bias=True),\n            torch.nn.ReLU(inplace=True),\n            Conv1d1x1(skip_channels, out_channels, bias=True),\n        ])\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n    def forward(self, x, c):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input noise signal (B, 1, T).\n            c (Tensor): Local conditioning auxiliary features (B, C ,T\').\n\n        Returns:\n            Tensor: Output tensor (B, out_channels, T)\n\n        """"""\n        # perform upsampling\n        if c is not None and self.upsample_net is not None:\n            c = self.upsample_net(c)\n            assert c.size(-1) == x.size(-1)\n\n        # encode to hidden representation\n        x = self.first_conv(x)\n        skips = 0\n        for f in self.conv_layers:\n            x, h = f(x, c)\n            skips += h\n        skips *= math.sqrt(1.0 / len(self.conv_layers))\n\n        # apply final layers\n        x = skips\n        for f in self.last_conv_layers:\n            x = f(x)\n\n        return x\n\n    def remove_weight_norm(self):\n        """"""Remove weight normalization module from all of the layers.""""""\n        def _remove_weight_norm(m):\n            try:\n                logging.debug(f""Weight norm is removed from {m}."")\n                torch.nn.utils.remove_weight_norm(m)\n            except ValueError:  # this module didn\'t have weight norm\n                return\n\n        self.apply(_remove_weight_norm)\n\n    def apply_weight_norm(self):\n        """"""Apply weight normalization module from all of the layers.""""""\n        def _apply_weight_norm(m):\n            if isinstance(m, torch.nn.Conv1d) or isinstance(m, torch.nn.Conv2d):\n                torch.nn.utils.weight_norm(m)\n                logging.debug(f""Weight norm is applied to {m}."")\n\n        self.apply(_apply_weight_norm)\n\n    @staticmethod\n    def _get_receptive_field_size(layers, stacks, kernel_size,\n                                  dilation=lambda x: 2 ** x):\n        assert layers % stacks == 0\n        layers_per_cycle = layers // stacks\n        dilations = [dilation(i % layers_per_cycle) for i in range(layers)]\n        return (kernel_size - 1) * sum(dilations) + 1\n\n    @property\n    def receptive_field_size(self):\n        """"""Return receptive field size.""""""\n        return self._get_receptive_field_size(self.layers, self.stacks, self.kernel_size)\n\n\nclass ParallelWaveGANDiscriminator(torch.nn.Module):\n    """"""Parallel WaveGAN Discriminator module.""""""\n\n    def __init__(self,\n                 in_channels=1,\n                 out_channels=1,\n                 kernel_size=3,\n                 layers=10,\n                 conv_channels=64,\n                 dilation_factor=1,\n                 nonlinear_activation=""LeakyReLU"",\n                 nonlinear_activation_params={""negative_slope"": 0.2},\n                 bias=True,\n                 use_weight_norm=True,\n                 ):\n        """"""Initialize Parallel WaveGAN Discriminator module.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            kernel_size (int): Number of output channels.\n            layers (int): Number of conv layers.\n            conv_channels (int): Number of chnn layers.\n            dilation_factor (int): Dilation factor. For example, if dilation_factor = 2,\n                the dilation will be 2, 4, 8, ..., and so on.\n            nonlinear_activation (str): Nonlinear function after each conv.\n            nonlinear_activation_params (dict): Nonlinear function parameters\n            bias (bool): Whether to use bias parameter in conv.\n            use_weight_norm (bool) Whether to use weight norm.\n                If set to true, it will be applied to all of the conv layers.\n\n        """"""\n        super(ParallelWaveGANDiscriminator, self).__init__()\n        assert (kernel_size - 1) % 2 == 0, ""Not support even number kernel size.""\n        assert dilation_factor > 0, ""Dilation factor must be > 0.""\n        self.conv_layers = torch.nn.ModuleList()\n        conv_in_channels = in_channels\n        for i in range(layers - 1):\n            if i == 0:\n                dilation = 1\n            else:\n                dilation = i if dilation_factor == 1 else dilation_factor ** i\n                conv_in_channels = conv_channels\n            padding = (kernel_size - 1) // 2 * dilation\n            conv_layer = [\n                Conv1d(conv_in_channels, conv_channels,\n                       kernel_size=kernel_size, padding=padding,\n                       dilation=dilation, bias=bias),\n                getattr(torch.nn, nonlinear_activation)(inplace=True, **nonlinear_activation_params)\n            ]\n            self.conv_layers += conv_layer\n        padding = (kernel_size - 1) // 2\n        last_conv_layer = Conv1d(\n            conv_in_channels, out_channels,\n            kernel_size=kernel_size, padding=padding, bias=bias)\n        self.conv_layers += [last_conv_layer]\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input noise signal (B, 1, T).\n\n        Returns:\n            Tensor: Output tensor (B, 1, T)\n\n        """"""\n        for f in self.conv_layers:\n            x = f(x)\n        return x\n\n    def apply_weight_norm(self):\n        """"""Apply weight normalization module from all of the layers.""""""\n        def _apply_weight_norm(m):\n            if isinstance(m, torch.nn.Conv1d) or isinstance(m, torch.nn.Conv2d):\n                torch.nn.utils.weight_norm(m)\n                logging.debug(f""Weight norm is applied to {m}."")\n\n        self.apply(_apply_weight_norm)\n\n    def remove_weight_norm(self):\n        """"""Remove weight normalization module from all of the layers.""""""\n        def _remove_weight_norm(m):\n            try:\n                logging.debug(f""Weight norm is removed from {m}."")\n                torch.nn.utils.remove_weight_norm(m)\n            except ValueError:  # this module didn\'t have weight norm\n                return\n\n        self.apply(_remove_weight_norm)\n\n\nclass ResidualParallelWaveGANDiscriminator(torch.nn.Module):\n    """"""Parallel WaveGAN Discriminator module.""""""\n\n    def __init__(self,\n                 in_channels=1,\n                 out_channels=1,\n                 kernel_size=3,\n                 layers=30,\n                 stacks=3,\n                 residual_channels=64,\n                 gate_channels=128,\n                 skip_channels=64,\n                 dropout=0.0,\n                 bias=True,\n                 use_weight_norm=True,\n                 use_causal_conv=False,\n                 nonlinear_activation=""LeakyReLU"",\n                 nonlinear_activation_params={""negative_slope"": 0.2},\n                 ):\n        """"""Initialize Parallel WaveGAN Discriminator module.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            kernel_size (int): Kernel size of dilated convolution.\n            layers (int): Number of residual block layers.\n            stacks (int): Number of stacks i.e., dilation cycles.\n            residual_channels (int): Number of channels in residual conv.\n            gate_channels (int):  Number of channels in gated conv.\n            skip_channels (int): Number of channels in skip conv.\n            dropout (float): Dropout rate. 0.0 means no dropout applied.\n            bias (bool): Whether to use bias parameter in conv.\n            use_weight_norm (bool): Whether to use weight norm.\n                If set to true, it will be applied to all of the conv layers.\n            use_causal_conv (bool): Whether to use causal structure.\n            nonlinear_activation_params (dict): Nonlinear function parameters\n\n        """"""\n        super(ResidualParallelWaveGANDiscriminator, self).__init__()\n        assert (kernel_size - 1) % 2 == 0, ""Not support even number kernel size.""\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.layers = layers\n        self.stacks = stacks\n        self.kernel_size = kernel_size\n\n        # check the number of layers and stacks\n        assert layers % stacks == 0\n        layers_per_stack = layers // stacks\n\n        # define first convolution\n        self.first_conv = torch.nn.Sequential(\n            Conv1d1x1(in_channels, residual_channels, bias=True),\n            getattr(torch.nn, nonlinear_activation)(\n                inplace=True, **nonlinear_activation_params),\n        )\n\n        # define residual blocks\n        self.conv_layers = torch.nn.ModuleList()\n        for layer in range(layers):\n            dilation = 2 ** (layer % layers_per_stack)\n            conv = ResidualBlock(\n                kernel_size=kernel_size,\n                residual_channels=residual_channels,\n                gate_channels=gate_channels,\n                skip_channels=skip_channels,\n                aux_channels=-1,\n                dilation=dilation,\n                dropout=dropout,\n                bias=bias,\n                use_causal_conv=use_causal_conv,\n            )\n            self.conv_layers += [conv]\n\n        # define output layers\n        self.last_conv_layers = torch.nn.ModuleList([\n            getattr(torch.nn, nonlinear_activation)(\n                inplace=True, **nonlinear_activation_params),\n            Conv1d1x1(skip_channels, skip_channels, bias=True),\n            getattr(torch.nn, nonlinear_activation)(\n                inplace=True, **nonlinear_activation_params),\n            Conv1d1x1(skip_channels, out_channels, bias=True),\n        ])\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input noise signal (B, 1, T).\n\n        Returns:\n            Tensor: Output tensor (B, 1, T)\n\n        """"""\n        x = self.first_conv(x)\n\n        skips = 0\n        for f in self.conv_layers:\n            x, h = f(x, None)\n            skips += h\n        skips *= math.sqrt(1.0 / len(self.conv_layers))\n\n        # apply final layers\n        x = skips\n        for f in self.last_conv_layers:\n            x = f(x)\n        return x\n\n    def apply_weight_norm(self):\n        """"""Apply weight normalization module from all of the layers.""""""\n        def _apply_weight_norm(m):\n            if isinstance(m, torch.nn.Conv1d) or isinstance(m, torch.nn.Conv2d):\n                torch.nn.utils.weight_norm(m)\n                logging.debug(f""Weight norm is applied to {m}."")\n\n        self.apply(_apply_weight_norm)\n\n    def remove_weight_norm(self):\n        """"""Remove weight normalization module from all of the layers.""""""\n        def _remove_weight_norm(m):\n            try:\n                logging.debug(f""Weight norm is removed from {m}."")\n                torch.nn.utils.remove_weight_norm(m)\n            except ValueError:  # this module didn\'t have weight norm\n                return\n\n        self.apply(_remove_weight_norm)\n'"
parallel_wavegan/models/tf_models.py,2,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2020 MINH ANH (@dathudeptrai)\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Tensorflow MelGAN modules complatible with pytorch.""""""\n\nimport tensorflow as tf\n\nimport numpy as np\n\nfrom parallel_wavegan.layers.tf_layers import TFConvTranspose1d\nfrom parallel_wavegan.layers.tf_layers import TFReflectionPad1d\nfrom parallel_wavegan.layers.tf_layers import TFResidualStack\n\n\nclass TFMelGANGenerator(tf.keras.layers.Layer):\n    """"""Tensorflow MelGAN generator module.""""""\n\n    def __init__(self,\n                 in_channels=80,\n                 out_channels=1,\n                 kernel_size=7,\n                 channels=512,\n                 bias=True,\n                 upsample_scales=[8, 8, 2, 2],\n                 stack_kernel_size=3,\n                 stacks=3,\n                 nonlinear_activation=""LeakyReLU"",\n                 nonlinear_activation_params={""alpha"": 0.2},\n                 pad=""ReflectionPad1d"",\n                 pad_params={},\n                 use_final_nonlinear_activation=True,\n                 use_weight_norm=True,\n                 use_causal_conv=False,\n                 ):\n        """"""Initialize TFMelGANGenerator module.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            kernel_size (int): Kernel size of initial and final conv layer.\n            channels (int): Initial number of channels for conv layer.\n            bias (bool): Whether to add bias parameter in convolution layers.\n            upsample_scales (list): List of upsampling scales.\n            stack_kernel_size (int): Kernel size of dilated conv layers in residual stack.\n            stacks (int): Number of stacks in a single residual stack.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n            pad (str): Padding function module name before dilated convolution layer.\n            pad_params (dict): Hyperparameters for padding function.\n            use_final_nonlinear_activation (torch.nn.Module): Activation function for the final layer.\n            use_weight_norm (bool): No effect but keep it as is to be the same as pytorch version.\n            use_causal_conv (bool): Whether to use causal convolution.\n\n        """"""\n        super(TFMelGANGenerator, self).__init__()\n\n        # check hyper parameters is valid\n        assert not use_causal_conv, ""Not supported yet.""\n        assert channels >= np.prod(upsample_scales)\n        assert channels % (2 ** len(upsample_scales)) == 0\n        assert pad == ""ReflectionPad1d"", f""Not supported (pad={pad}).""\n\n        # add initial layer\n        layers = []\n        layers += [\n            TFReflectionPad1d((kernel_size - 1) // 2),\n            tf.keras.layers.Conv2D(filters=channels,\n                                   kernel_size=(kernel_size, 1),\n                                   padding=""valid"",\n                                   use_bias=bias)\n        ]\n\n        for i, upsample_scale in enumerate(upsample_scales):\n            # add upsampling layer\n            layers += [\n                getattr(tf.keras.layers, nonlinear_activation)(**nonlinear_activation_params),\n                TFConvTranspose1d(\n                    channels=channels // (2 ** (i + 1)),\n                    kernel_size=upsample_scale * 2,\n                    stride=upsample_scale,\n                    padding=""same"",\n                )\n            ]\n\n            # add residual stack\n            for j in range(stacks):\n                layers += [\n                    TFResidualStack(\n                        kernel_size=stack_kernel_size,\n                        channels=channels // (2 ** (i + 1)),\n                        dilation=stack_kernel_size ** j,\n                        bias=bias,\n                        nonlinear_activation=nonlinear_activation,\n                        nonlinear_activation_params=nonlinear_activation_params,\n                        padding=""same"",\n                    )\n                ]\n\n        # add final layer\n        layers += [\n            getattr(tf.keras.layers, nonlinear_activation)(**nonlinear_activation_params),\n            TFReflectionPad1d((kernel_size - 1) // 2),\n            tf.keras.layers.Conv2D(filters=out_channels,\n                                   kernel_size=(kernel_size, 1),\n                                   use_bias=bias),\n        ]\n        if use_final_nonlinear_activation:\n            layers += [tf.keras.layers.Activation(""tanh"")]\n\n        self.melgan = tf.keras.models.Sequential(layers)\n\n    # TODO(kan-bayashi): Fix hard coded dimension\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, None, 80], dtype=tf.float32)])\n    def call(self, c):\n        """"""Calculate forward propagation.\n\n        Args:\n            c (Tensor): Input tensor (B, T, in_channels).\n\n        Returns:\n            Tensor: Output tensor (B, T ** prod(upsample_scales), out_channels).\n\n        """"""\n        c = tf.expand_dims(c, 2)\n        c = self.melgan(c)\n        return c[:, :, 0, :]\n'"
parallel_wavegan/optimizers/__init__.py,1,b'from torch.optim import *  # NOQA\nfrom parallel_wavegan.optimizers.radam import *  # NOQA\n'
parallel_wavegan/optimizers/radam.py,3,"b'# -*- coding: utf-8 -*-\n\n""""""RAdam optimizer.\n\nThis code is drived from https://github.com/LiyuanLucasLiu/RAdam.\n""""""\n\nimport math\nimport torch\n\nfrom torch.optim.optimizer import Optimizer\n\n\nclass RAdam(Optimizer):\n    """"""Rectified Adam optimizer.""""""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        """"""Initilize RAdam optimizer.""""""\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        """"""Set state.""""""\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n        """"""Run one step.""""""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'RAdam does not support sparse gradients\')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[\'step\'] += 1\n                buffered = self.buffer[int(state[\'step\'] % 10)]\n                if state[\'step\'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\'step\']\n                    beta2_t = beta2 ** state[\'step\']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it\'s an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt(\n                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[\'step\'])  # NOQA\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state[\'step\'])\n                    buffered[2] = step_size\n\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n\n                # more conservative since it\'s an approximated value\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n                    p_data_fp32.addcdiv_(-step_size * group[\'lr\'], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group[\'lr\'], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n'"
parallel_wavegan/utils/__init__.py,0,b'from parallel_wavegan.utils.utils import *  # NOQA\n'
parallel_wavegan/utils/utils.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n""""""Utility functions.""""""\n\nimport fnmatch\nimport logging\nimport os\nimport sys\n\nimport h5py\nimport numpy as np\n\n\ndef find_files(root_dir, query=""*.wav"", include_root_dir=True):\n    """"""Find files recursively.\n\n    Args:\n        root_dir (str): Root root_dir to find.\n        query (str): Query to find.\n        include_root_dir (bool): If False, root_dir name is not included.\n\n    Returns:\n        list: List of found filenames.\n\n    """"""\n    files = []\n    for root, dirnames, filenames in os.walk(root_dir, followlinks=True):\n        for filename in fnmatch.filter(filenames, query):\n            files.append(os.path.join(root, filename))\n    if not include_root_dir:\n        files = [file_.replace(root_dir + ""/"", """") for file_ in files]\n\n    return files\n\n\ndef read_hdf5(hdf5_name, hdf5_path):\n    """"""Read hdf5 dataset.\n\n    Args:\n        hdf5_name (str): Filename of hdf5 file.\n        hdf5_path (str): Dataset name in hdf5 file.\n\n    Return:\n        any: Dataset values.\n\n    """"""\n    if not os.path.exists(hdf5_name):\n        logging.error(f""There is no such a hdf5 file ({hdf5_name})."")\n        sys.exit(1)\n\n    hdf5_file = h5py.File(hdf5_name, ""r"")\n\n    if hdf5_path not in hdf5_file:\n        logging.error(f""There is no such a data in hdf5 file. ({hdf5_path})"")\n        sys.exit(1)\n\n    hdf5_data = hdf5_file[hdf5_path][()]\n    hdf5_file.close()\n\n    return hdf5_data\n\n\ndef write_hdf5(hdf5_name, hdf5_path, write_data, is_overwrite=True):\n    """"""Write dataset to hdf5.\n\n    Args:\n        hdf5_name (str): Hdf5 dataset filename.\n        hdf5_path (str): Dataset path in hdf5.\n        write_data (ndarray): Data to write.\n        is_overwrite (bool): Whether to overwrite dataset.\n\n    """"""\n    # convert to numpy array\n    write_data = np.array(write_data)\n\n    # check folder existence\n    folder_name, _ = os.path.split(hdf5_name)\n    if not os.path.exists(folder_name) and len(folder_name) != 0:\n        os.makedirs(folder_name)\n\n    # check hdf5 existence\n    if os.path.exists(hdf5_name):\n        # if already exists, open with r+ mode\n        hdf5_file = h5py.File(hdf5_name, ""r+"")\n        # check dataset existence\n        if hdf5_path in hdf5_file:\n            if is_overwrite:\n                logging.warning(""Dataset in hdf5 file already exists. ""\n                                ""recreate dataset in hdf5."")\n                hdf5_file.__delitem__(hdf5_path)\n            else:\n                logging.error(""Dataset in hdf5 file already exists. ""\n                              ""if you want to overwrite, please set is_overwrite = True."")\n                hdf5_file.close()\n                sys.exit(1)\n    else:\n        # if not exists, open with w mode\n        hdf5_file = h5py.File(hdf5_name, ""w"")\n\n    # write data to hdf5\n    hdf5_file.create_dataset(hdf5_path, data=write_data)\n    hdf5_file.flush()\n    hdf5_file.close()\n\n\nclass HDF5ScpLoader(object):\n    """"""Loader class for a fests.scp file of hdf5 file.\n\n    Examples:\n        key1 /some/path/a.h5:feats\n        key2 /some/path/b.h5:feats\n        key3 /some/path/c.h5:feats\n        key4 /some/path/d.h5:feats\n        ...\n        >>> loader = HDF5ScpLoader(""hdf5.scp"")\n        >>> array = loader[""key1""]\n\n        key1 /some/path/a.h5\n        key2 /some/path/b.h5\n        key3 /some/path/c.h5\n        key4 /some/path/d.h5\n        ...\n        >>> loader = HDF5ScpLoader(""hdf5.scp"", ""feats"")\n        >>> array = loader[""key1""]\n\n        key1 /some/path/a.h5:feats_1,feats_2\n        key2 /some/path/b.h5:feats_1,feats_2\n        key3 /some/path/c.h5:feats_1,feats_2\n        key4 /some/path/d.h5:feats_1,feats_2\n        ...\n        >>> loader = HDF5ScpLoader(""hdf5.scp"")\n        # feats_1 and feats_2 will be concatenated\n        >>> array = loader[""key1""]\n\n    """"""\n\n    def __init__(self, feats_scp, default_hdf5_path=""feats""):\n        """"""Initialize HDF5 scp loader.\n\n        Args:\n            feats_scp (str): Kaldi-style feats.scp file with hdf5 format.\n            default_hdf5_path (str): Path in hdf5 file. If the scp contain the info, not used.\n\n        """"""\n        self.default_hdf5_path = default_hdf5_path\n        with open(feats_scp) as f:\n            lines = [line.replace(""\\n"", """") for line in f.readlines()]\n        self.data = {}\n        for line in lines:\n            key, value = line.split()\n            self.data[key] = value\n\n    def get_path(self, key):\n        """"""Get hdf5 file path for a given key.""""""\n        return self.data[key]\n\n    def __getitem__(self, key):\n        """"""Get ndarray for a given key.""""""\n        p = self.data[key]\n        if "":"" in p:\n            if len(p.split("","")) == 1:\n                return read_hdf5(*p.split("":""))\n            else:\n                p1, p2 = p.split("":"")\n                feats = [read_hdf5(p1, p) for p in p2.split("","")]\n                return np.concatenate([f if len(f.shape) != 1 else f.reshape(-1, 1) for f in feats], 1)\n        else:\n            return read_hdf5(p, self.default_hdf5_path)\n\n    def __len__(self):\n        """"""Return the length of the scp file.""""""\n        return len(self.data)\n\n    def __iter__(self):\n        """"""Return the iterator of the scp file.""""""\n        return iter(self.data)\n\n    def keys(self):\n        """"""Return the keys of the scp file.""""""\n        return self.data.keys()\n\n    def values(self):\n        """"""Return the values of the scp file.""""""\n        for key in self.keys():\n            yield self[key]\n'"
