file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport os\nimport pip\nimport sys\n\nfrom distutils.version import LooseVersion\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n\nif LooseVersion(sys.version) < LooseVersion(""3.6""):\n    raise RuntimeError(\n        ""Python>=3.6 is required, ""\n        ""but your Python is {}"".format(sys.version))\nif LooseVersion(pip.__version__) < LooseVersion(""19""):\n    raise RuntimeError(\n        ""pip>=19.0.0 is required, but your pip is {}. ""\n        ""Try again after \\""pip install -U pip\\"""".format(pip.__version__))\n\nrequirements = {\n    ""install"": [\n        ""h5py>=2.8.0"",\n        ""scikit-learn==0.22.2"",\n        ""librosa>=0.6.2"",\n        ""soundfile>=0.10.2"",\n        ""torch>=1.0.1"",\n        ""torchvision>=0.2.2"",\n        ""sprocket-vc>=0.18.2"",\n        ""matplotlib>=3.0.3"",\n    ],\n    ""setup"": [\n        ""numpy"",\n        ""pytest-runner""\n    ],\n    ""test"": [\n        ""pytest>=3.3.0"",\n        ""hacking==1.1.0"",\n        ""autopep8==1.2.4"",\n    ]}\ninstall_requires = requirements[""install""]\nsetup_requires = requirements[""setup""]\ntests_require = requirements[""test""]\nextras_require = {k: v for k, v in requirements.items()\n                  if k not in [""install"", ""setup""]}\n\ndirname = os.path.dirname(__file__)\nsetup(name=""wavenet_vocoder"",\n      version=""0.1.1"",\n      url=""http://github.com/kan-bayashi/PytorchWaveNetVocoder"",\n      author=""Tomoki Hayashi"",\n      author_email=""hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp"",\n      description=""Pytorch WaveNet Vocoder"",\n      long_description=open(os.path.join(dirname, ""README.md""),\n                            encoding=""utf-8"").read(),\n      license=""Apache Software License"",\n      packages=find_packages(include=""wavenet_vocoder*""),\n      install_requires=install_requires,\n      setup_requires=setup_requires,\n      tests_require=tests_require,\n      extras_require=extras_require,\n      classifiers=[\n          ""Programming Language :: Python"",\n          ""Programming Language :: Python :: 3.6"",\n          ""Intended Audience :: Science/Research"",\n          ""Operating System :: POSIX :: Linux"",\n          ""License :: OSI Approved :: Apache Software License"",\n          ""Topic :: Software Development :: Libraries :: Python Modules""],\n      )\n'"
test/test_generator.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport os\n\nfrom wavenet_vocoder.bin.decode import decode_generator\nfrom wavenet_vocoder.bin.feature_extract import melspectrogram_extract\nfrom wavenet_vocoder.bin.feature_extract import world_feature_extract\nfrom wavenet_vocoder.bin.train import train_generator\nfrom wavenet_vocoder.utils import find_files\n\nfrom test_preprocessing import make_args as make_feature_args\nfrom test_preprocessing import make_dummy_wav\n\n\ndef make_train_generator_args(**kwargs):\n    defaults = dict(\n        wav_list=None,\n        feat_list=None,\n        receptive_field=1000,\n        batch_length=3000,\n        batch_size=5,\n        feature_type=""world"",\n        wav_transform=None,\n        feat_transform=None,\n        shuffle=False,\n        upsampling_factor=80,\n        use_upsampling_layer=True,\n        use_speaker_code=False\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\ndef make_decode_generator_args(**kwargs):\n    defaults = dict(\n        feat_list=None,\n        batch_size=5,\n        feature_type=""world"",\n        wav_transform=None,\n        feat_transform=None,\n        upsampling_factor=80,\n        use_upsampling_layer=True,\n        use_speaker_code=False\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\ndef test_train_generator():\n    # make dummy wavfiles\n    wavdir = ""data/wav""\n    if not os.path.exists(wavdir):\n        os.makedirs(wavdir)\n    for i in range(5):\n        make_dummy_wav(wavdir + ""/%d.wav"" % i)\n\n    # make features\n    feat_args = make_feature_args()\n    wav_list = find_files(wavdir, ""*.wav"")\n    if not os.path.exists(feat_args.wavdir):\n        os.makedirs(feat_args.wavdir)\n    feat_args.feature_type = ""melspc""\n    melspectrogram_extract(wav_list, feat_args)\n    feat_args.feature_type = ""world""\n    world_feature_extract(wav_list, feat_args)\n    feat_list = find_files(feat_args.hdf5dir, ""*.h5"")\n\n    for ft in [""world"", ""melspc""]:\n        # ----------------------------------\n        # minibatch without upsampling layer\n        # ----------------------------------\n        generator_args = make_train_generator_args(\n            wav_list=wav_list,\n            feat_list=feat_list,\n            feature_type=ft,\n            use_upsampling_layer=False,\n            batch_length=10000,\n            batch_size=5\n        )\n        generator = train_generator(**vars(generator_args))\n        (x, h), t = next(generator)\n        assert x.size(0) == t.size(0) == h.size(0)\n        assert x.size(1) == t.size(1) == h.size(2)\n\n        # ----------------------------------------\n        # utterance batch without upsampling layer\n        # ----------------------------------------\n        generator_args = make_train_generator_args(\n            wav_list=wav_list,\n            feat_list=feat_list,\n            feature_type=ft,\n            use_upsampling_layer=False,\n            batch_length=None,\n            batch_size=5\n        )\n        generator = train_generator(**vars(generator_args))\n        (x, h), t = next(generator)\n        assert x.size(0) == t.size(0) == h.size(0) == 1\n        assert x.size(1) == t.size(1) == h.size(2)\n\n        # -------------------------------\n        # minibatch with upsampling layer\n        # -------------------------------\n        generator_args = make_train_generator_args(\n            wav_list=wav_list,\n            feat_list=feat_list,\n            feature_type=ft,\n            use_upsampling_layer=True,\n            batch_length=10000,\n            batch_size=5\n        )\n        generator = train_generator(**vars(generator_args))\n        (x, h), t = next(generator)\n        assert x.size(0) == t.size(0) == h.size(0)\n        assert x.size(1) == t.size(1) == h.size(2) * generator_args.upsampling_factor\n\n        # -------------------------------------\n        # utterance batch with upsampling layer\n        # -------------------------------------\n        generator_args = make_train_generator_args(\n            wav_list=wav_list,\n            feat_list=feat_list,\n            feature_type=ft,\n            use_upsampling_layer=True,\n            batch_length=None,\n            batch_size=5\n        )\n        generator = train_generator(**vars(generator_args))\n        (x, h), t = next(generator)\n        assert x.size(0) == t.size(0) == h.size(0) == 1\n        assert x.size(1) == t.size(1) == h.size(2) * generator_args.upsampling_factor\n\n\ndef test_decode_generator():\n    # make dummy wavfiles\n    wavdir = ""data/wav""\n    if not os.path.exists(wavdir):\n        os.makedirs(wavdir)\n    for i in range(5):\n        make_dummy_wav(wavdir + ""/%d.wav"" % i)\n\n    # make features\n    feat_args = make_feature_args()\n    wav_list = find_files(wavdir, ""*.wav"")\n    if not os.path.exists(feat_args.wavdir):\n        os.makedirs(feat_args.wavdir)\n    feat_args.feature_type = ""melspc""\n    melspectrogram_extract(wav_list, feat_args)\n    feat_args.feature_type = ""world""\n    world_feature_extract(wav_list, feat_args)\n    feat_list = find_files(feat_args.hdf5dir, ""*.h5"")\n\n    for ft in [""world"", ""melspc""]:\n        # ----------------------------------\n        # non-batch without upsampling layer\n        # ----------------------------------\n        generator_args = make_decode_generator_args(\n            feat_list=feat_list,\n            feature_type=ft,\n            use_upsampling_layer=False,\n            batch_size=1\n        )\n        generator = decode_generator(**vars(generator_args))\n        _, (x, h, n_samples) = next(generator)\n        assert x.size(0) == h.size(0) == 1\n        assert h.size(2) == n_samples + 1\n\n        # -------------------------------\n        # non-batch with upsampling layer\n        # -------------------------------\n        generator_args = make_decode_generator_args(\n            feat_list=feat_list,\n            feature_type=ft,\n            use_upsampling_layer=True,\n            batch_size=1\n        )\n        generator = decode_generator(**vars(generator_args))\n        _, (x, h, n_samples) = next(generator)\n        assert x.size(0) == h.size(0) == 1\n        assert h.size(2) * generator_args.upsampling_factor == n_samples + 1\n\n        # ----------------------------------\n        # minibatch without upsampling layer\n        # ----------------------------------\n        generator_args = make_decode_generator_args(\n            feat_list=feat_list,\n            feature_type=ft,\n            use_upsampling_layer=False,\n            batch_size=5\n        )\n        generator = decode_generator(**vars(generator_args))\n        _, (batch_x, batch_h, n_samples_list) = next(generator)\n        assert batch_x.size(0) == batch_h.size(0) == len(n_samples_list)\n        assert batch_h.size(2) == max(n_samples_list) + 1\n\n        # -------------------------------\n        # minibatch with upsampling layer\n        # -------------------------------\n        generator_args = make_decode_generator_args(\n            feat_list=feat_list,\n            feature_type=ft,\n            use_upsampling_layer=True,\n            batch_size=5\n        )\n        generator = decode_generator(**vars(generator_args))\n        _, (batch_x, batch_h, n_samples_list) = next(generator)\n        assert batch_x.size(0) == batch_h.size(0) == len(n_samples_list)\n        assert batch_h.size(2) * generator_args.upsampling_factor == max(n_samples_list) + 1\n'"
test/test_preprocessing.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport os\nimport shutil\n\nimport numpy as np\nimport pytest\n\nfrom scipy.io import wavfile\n\nfrom wavenet_vocoder.bin.calc_stats import calc_stats\nfrom wavenet_vocoder.bin.feature_extract import melcepstrum_extract\nfrom wavenet_vocoder.bin.feature_extract import melspectrogram_extract\nfrom wavenet_vocoder.bin.feature_extract import world_feature_extract\nfrom wavenet_vocoder.bin.noise_shaping import convert_mcep_to_mlsa_coef\nfrom wavenet_vocoder.bin.noise_shaping import noise_shaping\nfrom wavenet_vocoder.utils import check_hdf5\nfrom wavenet_vocoder.utils import find_files\nfrom wavenet_vocoder.utils import read_hdf5\nfrom wavenet_vocoder.utils import write_hdf5\n\n\ndef make_dummy_wav(name, maxlen=32000, fs=16000):\n    length = np.random.randint(maxlen // 2, maxlen)\n    x = np.random.randn(length)\n    x = x / np.abs(x).max()\n    x = np.int16(x * (np.iinfo(np.int16).max + 1))\n    wavfile.write(name, fs, x)\n\n\ndef make_args(**kwargs):\n    defaults = dict(\n        hdf5dir=""tmp/hdf5"",\n        wavdir=""tmp/wav_filtered"",\n        outdir=""tmp/wav_nwf"",\n        stats=""tmp/stats.h5"",\n        feature_type=""world"",\n        fs=16000,\n        shiftms=5,\n        minf0=40,\n        maxf0=400,\n        mspc_dim=80,\n        mcep_dim=24,\n        mcep_alpha=0.41,\n        fftl=1024,\n        highpass_cutoff=70,\n        mcep_dim_start=2,\n        mcep_dim_end=25,\n        fmin=None,\n        fmax=None,\n        mag=0.5,\n        save_wav=True,\n        inv=False,\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\n@pytest.mark.parametrize(""feature_type"", [\n    (""melspc""), (""world""), (""mcep""),\n])\ndef test_preprocessing(feature_type):\n    # make arguments\n    args = make_args(feature_type=feature_type)\n\n    # prepare dummy wav files\n    wavdir = ""tmp/wav""\n    if not os.path.exists(wavdir):\n        os.makedirs(wavdir)\n    for i in range(5):\n        make_dummy_wav(wavdir + ""/%d.wav"" % i, 8000, args.fs)\n\n    # feature extract\n    wav_list = find_files(wavdir, ""*.wav"")\n    if not os.path.exists(args.wavdir):\n        os.makedirs(args.wavdir)\n    if args.feature_type == ""world"":\n        world_feature_extract(wav_list, args)\n    elif args.feature_type == ""melspc"":\n        melspectrogram_extract(wav_list, args)\n    else:\n        melcepstrum_extract(wav_list, args)\n\n    # calc_stats\n    file_list = find_files(args.hdf5dir, ""*.h5"")\n    calc_stats(file_list, args)\n\n    # noise shaping\n    if feature_type != ""melspc"":\n        wav_list = find_files(args.wavdir, ""*.wav"")\n        if not os.path.exists(args.outdir):\n            os.makedirs(args.outdir)\n        if not check_hdf5(args.stats, ""/mlsa/coef""):\n            avg_mcep = read_hdf5(args.stats, args.feature_type + ""/mean"")\n            if args.feature_type == ""world"":\n                avg_mcep = avg_mcep[args.mcep_dim_start:args.mcep_dim_end]\n            mlsa_coef = convert_mcep_to_mlsa_coef(avg_mcep, args.mag, args.mcep_alpha)\n            write_hdf5(args.stats, ""/mlsa/coef"", mlsa_coef)\n            write_hdf5(args.stats, ""/mlsa/alpha"", args.mcep_alpha)\n        noise_shaping(wav_list, args)\n\n    # remove\n    shutil.rmtree(""tmp"")\n'"
test/test_upsampling.py,1,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport numpy as np\nimport torch\n\nfrom wavenet_vocoder.nets import initialize\nfrom wavenet_vocoder.nets import UpSampling\n\n\ndef test_upsampling():\n    aux = np.random.randn(1, 28, 1000)\n    conv = UpSampling(10)\n    conv.apply(initialize)\n    batch = torch.from_numpy(aux).float()\n    out = conv(batch)\n    out = out.detach().numpy()\n    assert out.shape[-1] == aux.shape[-1] * 10\n'"
test/test_wavenet.py,30,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport logging\n\nimport numpy as np\nimport torch\n\nfrom wavenet_vocoder.nets import encode_mu_law\nfrom wavenet_vocoder.nets import initialize\nfrom wavenet_vocoder.nets import WaveNet\n\n# set log level\nlogging.basicConfig(level=logging.DEBUG,\n                    format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                    datefmt=\'%m/%d/%Y %I:%M:%S\')\n\n\ndef sine_generator(seq_size=100, mu=256):\n    t = np.linspace(0, 1, 16000)\n    data = np.sin(2 * np.pi * 220 * t) + np.sin(2 * np.pi * 224 * t)\n    data = data / 2\n    while True:\n        ys = data[:seq_size]\n        ys = encode_mu_law(data, mu)\n        yield torch.from_numpy(ys[:seq_size])\n\n\ndef test_forward():\n    # get batch\n    generator = sine_generator(100)\n    batch = next(generator)\n    batch_input = batch.view(1, -1)\n    batch_aux = torch.rand(1, 28, batch_input.size(1)).float()\n\n    # define model without upsampling with kernel size = 2\n    net = WaveNet(256, 28, 32, 128, 10, 1, 2)\n    net.apply(initialize)\n    net.eval()\n    y = net(batch_input, batch_aux)[0]\n    assert y.size(0) == batch_input.size(1)\n    assert y.size(1) == 256\n\n    # define model without upsampling with kernel size = 3\n    net = WaveNet(256, 28, 32, 128, 10, 1, 2)\n    net.apply(initialize)\n    net.eval()\n    y = net(batch_input, batch_aux)[0]\n    assert y.size(0) == batch_input.size(1)\n    assert y.size(1) == 256\n\n    batch_input = batch.view(1, -1)\n    batch_aux = torch.rand(1, 28, batch_input.size(1) // 10).float()\n\n    # define model with upsampling and kernel size = 2\n    net = WaveNet(256, 28, 32, 128, 10, 1, 2, 10)\n    net.apply(initialize)\n    net.eval()\n    y = net(batch_input, batch_aux)[0]\n    assert y.size(0) == batch_input.size(1)\n    assert y.size(1) == 256\n\n    # define model with upsampling and kernel size = 3\n    net = WaveNet(256, 28, 32, 128, 10, 1, 3, 10)\n    net.apply(initialize)\n    net.eval()\n    y = net(batch_input, batch_aux)[0]\n    assert y.size(0) == batch_input.size(1)\n    assert y.size(1) == 256\n\n\ndef test_generate():\n    batch = 2\n    x = np.random.randint(0, 256, size=(batch, 1))\n    h = np.random.randn(batch, 28, 10)\n    length = h.shape[-1] - 1\n    with torch.no_grad():\n        net = WaveNet(256, 28, 4, 4, 10, 3, 2)\n        net.apply(initialize)\n        net.eval()\n        for x_, h_ in zip(x, h):\n            batch_x = torch.from_numpy(np.expand_dims(x_, 0)).long()\n            batch_h = torch.from_numpy(np.expand_dims(h_, 0)).float()\n            net.generate(batch_x, batch_h, length, 1, ""sampling"")\n            net.fast_generate(batch_x, batch_h, length, 1, ""sampling"")\n        batch_x = torch.from_numpy(x).long()\n        batch_h = torch.from_numpy(h).float()\n        net.batch_fast_generate(batch_x, batch_h, [length] * batch, 1, ""sampling"")\n\n\ndef test_assert_fast_generation():\n    # get batch\n    batch = 2\n    x = np.random.randint(0, 256, size=(batch, 1))\n    h = np.random.randn(batch, 28, 32)\n    length = h.shape[-1] - 1\n\n    with torch.no_grad():\n        # --------------------------------------------------------\n        # define model without upsampling and with kernel size = 2\n        # --------------------------------------------------------\n        net = WaveNet(256, 28, 4, 4, 10, 3, 2)\n        net.apply(initialize)\n        net.eval()\n\n        # sample-by-sample generation\n        gen1_list = []\n        gen2_list = []\n        for x_, h_ in zip(x, h):\n            batch_x = torch.from_numpy(np.expand_dims(x_, 0)).long()\n            batch_h = torch.from_numpy(np.expand_dims(h_, 0)).float()\n            gen1 = net.generate(batch_x, batch_h, length, 1, ""argmax"")\n            gen2 = net.fast_generate(batch_x, batch_h, length, 1, ""argmax"")\n            np.testing.assert_array_equal(gen1, gen2)\n            gen1_list += [gen1]\n            gen2_list += [gen2]\n        gen1 = np.stack(gen1_list)\n        gen2 = np.stack(gen2_list)\n        np.testing.assert_array_equal(gen1, gen2)\n\n        # batch generation\n        batch_x = torch.from_numpy(x).long()\n        batch_h = torch.from_numpy(h).float()\n        gen3_list = net.batch_fast_generate(batch_x, batch_h, [length] * batch, 1, ""argmax"")\n        gen3 = np.stack(gen3_list)\n        np.testing.assert_array_equal(gen3, gen2)\n\n        # --------------------------------------------------------\n        # define model without upsampling and with kernel size = 3\n        # --------------------------------------------------------\n        net = WaveNet(256, 28, 4, 4, 10, 3, 3)\n        net.apply(initialize)\n        net.eval()\n\n        # sample-by-sample generation\n        gen1_list = []\n        gen2_list = []\n        for x_, h_ in zip(x, h):\n            batch_x = torch.from_numpy(np.expand_dims(x_, 0)).long()\n            batch_h = torch.from_numpy(np.expand_dims(h_, 0)).float()\n            gen1 = net.generate(batch_x, batch_h, length, 1, ""argmax"")\n            gen2 = net.fast_generate(batch_x, batch_h, length, 1, ""argmax"")\n            np.testing.assert_array_equal(gen1, gen2)\n            gen1_list += [gen1]\n            gen2_list += [gen2]\n        gen1 = np.stack(gen1_list)\n        gen2 = np.stack(gen2_list)\n        np.testing.assert_array_equal(gen1, gen2)\n\n        # batch generation\n        batch_x = torch.from_numpy(x).long()\n        batch_h = torch.from_numpy(h).float()\n        gen3_list = net.batch_fast_generate(batch_x, batch_h, [length] * batch, 1, ""argmax"")\n        gen3 = np.stack(gen3_list)\n        np.testing.assert_array_equal(gen3, gen2)\n\n        # get batch\n        batch = 2\n        upsampling_factor = 10\n        x = np.random.randint(0, 256, size=(batch, 1))\n        h = np.random.randn(batch, 28, 3)\n        length = h.shape[-1] * upsampling_factor - 1\n\n        # -----------------------------------------------------\n        # define model with upsampling and with kernel size = 2\n        # -----------------------------------------------------\n        net = WaveNet(256, 28, 4, 4, 10, 3, 2, upsampling_factor)\n        net.apply(initialize)\n        net.eval()\n\n        # sample-by-sample generation\n        gen1_list = []\n        gen2_list = []\n        for x_, h_ in zip(x, h):\n            batch_x = torch.from_numpy(np.expand_dims(x_, 0)).long()\n            batch_h = torch.from_numpy(np.expand_dims(h_, 0)).float()\n            gen1 = net.generate(batch_x, batch_h, length, 1, ""argmax"")\n            gen2 = net.fast_generate(batch_x, batch_h, length, 1, ""argmax"")\n            np.testing.assert_array_equal(gen1, gen2)\n            gen1_list += [gen1]\n            gen2_list += [gen2]\n        gen1 = np.stack(gen1_list)\n        gen2 = np.stack(gen2_list)\n        np.testing.assert_array_equal(gen1, gen2)\n\n        # batch generation\n        batch_x = torch.from_numpy(x).long()\n        batch_h = torch.from_numpy(h).float()\n        gen3_list = net.batch_fast_generate(batch_x, batch_h, [length] * batch, 1, ""argmax"")\n        gen3 = np.stack(gen3_list)\n        np.testing.assert_array_equal(gen3, gen2)\n\n        # -----------------------------------------------------\n        # define model with upsampling and with kernel size = 3\n        # -----------------------------------------------------\n        net = WaveNet(256, 28, 4, 4, 10, 3, 2, upsampling_factor)\n        net.apply(initialize)\n        net.eval()\n\n        # sample-by-sample generation\n        gen1_list = []\n        gen2_list = []\n        for x_, h_ in zip(x, h):\n            batch_x = torch.from_numpy(np.expand_dims(x_, 0)).long()\n            batch_h = torch.from_numpy(np.expand_dims(h_, 0)).float()\n            gen1 = net.generate(batch_x, batch_h, length, 1, ""argmax"")\n            gen2 = net.fast_generate(batch_x, batch_h, length, 1, ""argmax"")\n            np.testing.assert_array_equal(gen1, gen2)\n            gen1_list += [gen1]\n            gen2_list += [gen2]\n        gen1 = np.stack(gen1_list)\n        gen2 = np.stack(gen2_list)\n        np.testing.assert_array_equal(gen1, gen2)\n\n        # batch generation\n        batch_x = torch.from_numpy(x).long()\n        batch_h = torch.from_numpy(h).float()\n        gen3_list = net.batch_fast_generate(batch_x, batch_h, [length] * batch, 1, ""argmax"")\n        gen3 = np.stack(gen3_list)\n        np.testing.assert_array_equal(gen3, gen2)\n\n\ndef test_assert_different_length_batch_generation():\n    # prepare batch\n    batch = 4\n    length = 32\n    x = np.random.randint(0, 256, size=(batch, 1))\n    h = np.random.randn(batch, 28, length)\n    length_list = sorted(list(np.random.randint(length // 2, length - 1, batch)))\n\n    with torch.no_grad():\n        net = WaveNet(256, 28, 4, 4, 10, 3, 2)\n        net.apply(initialize)\n        net.eval()\n\n        # sample-by-sample generation\n        gen1_list = []\n        for x_, h_, length in zip(x, h, length_list):\n            batch_x = torch.from_numpy(np.expand_dims(x_, 0)).long()\n            batch_h = torch.from_numpy(np.expand_dims(h_, 0)).float()\n            gen1 = net.fast_generate(batch_x, batch_h, length, 1, ""argmax"")\n            gen1_list += [gen1]\n\n        # batch generation\n        batch_x = torch.from_numpy(x).long()\n        batch_h = torch.from_numpy(h).float()\n        gen2_list = net.batch_fast_generate(batch_x, batch_h, length_list, 1, ""argmax"")\n\n        # assertion\n        for gen1, gen2 in zip(gen1_list, gen2_list):\n            np.testing.assert_array_equal(gen1, gen2)\n'"
wavenet_vocoder/__init__.py,0,b''
wavenet_vocoder/bin/__init__.py,0,b''
wavenet_vocoder/bin/calc_stats.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport logging\n\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom wavenet_vocoder.utils import read_hdf5\nfrom wavenet_vocoder.utils import read_txt\nfrom wavenet_vocoder.utils import write_hdf5\n\n\ndef calc_stats(file_list, args):\n    """"""CALCULATE STATISTICS.""""""\n    scaler = StandardScaler()\n\n    # process over all of data\n    for i, filename in enumerate(file_list):\n        logging.info(""now processing %s (%d/%d)"" % (filename, i + 1, len(file_list)))\n        feat = read_hdf5(filename, ""/"" + args.feature_type)\n        scaler.partial_fit(feat)\n\n    # add uv term\n    mean = scaler.mean_\n    scale = scaler.scale_\n    if args.feature_type == ""world"":\n        mean[0] = 0.0\n        scale[0] = 1.0\n\n    # write to hdf5\n    write_hdf5(args.stats, ""/"" + args.feature_type + ""/mean"", np.float32(mean))\n    write_hdf5(args.stats, ""/"" + args.feature_type + ""/scale"", np.float32(scale))\n\n\ndef main():\n    """"""RUN CALCULATION OF STATISTICS.""""""\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        ""--feats"", default=None, required=True,\n        type=str, help=""name of the list of hdf5 files"")\n    parser.add_argument(\n        ""--stats"", default=None, required=True,\n        type=str, help=""filename of hdf5 format"")\n    parser.add_argument(\n        ""--feature_type"", default=""world"", choices=[""world"", ""melspc"", ""mcep""],\n        type=str, help=""feature type"")\n    parser.add_argument(\n        ""--verbose"", default=1,\n        type=int, help=""log message level"")\n\n    args = parser.parse_args()\n\n    # set log level\n    if args.verbose == 1:\n        logging.basicConfig(level=logging.INFO,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n    elif args.verbose > 1:\n        logging.basicConfig(level=logging.DEBUG,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n    else:\n        logging.basicConfig(level=logging.WARNING,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n        logging.warning(""logging is disabled."")\n\n    # show arguments\n    for key, value in vars(args).items():\n        logging.info(""%s = %s"" % (key, str(value)))\n\n    # read file list\n    file_list = read_txt(args.feats)\n    logging.info(""number of utterances = %d"" % len(file_list))\n\n    # calculate statistics\n    calc_stats(file_list, args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
wavenet_vocoder/bin/decode.py,13,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport logging\nimport math\nimport os\nimport sys\n\nimport numpy as np\nimport soundfile as sf\nimport torch\nimport torch.multiprocessing as mp\n\nfrom sklearn.preprocessing import StandardScaler\nfrom torchvision import transforms\n\nfrom wavenet_vocoder.nets import decode_mu_law\nfrom wavenet_vocoder.nets import encode_mu_law\nfrom wavenet_vocoder.nets import WaveNet\nfrom wavenet_vocoder.utils import extend_time\nfrom wavenet_vocoder.utils import find_files\nfrom wavenet_vocoder.utils import read_hdf5\nfrom wavenet_vocoder.utils import read_txt\nfrom wavenet_vocoder.utils import shape_hdf5\n\n\ndef pad_list(batch_list, pad_value=0.0):\n    """"""PAD VALUE.\n\n    Args:\n        batch_list (list): List of batch, where the shape of i-th batch (T_i, C).\n        pad_value (float): Value to pad.\n\n    Returns:\n        ndarray: Padded batch with the shape (B, T_max, C).\n\n    """"""\n    batch_size = len(batch_list)\n    maxlen = max([batch.shape[0] for batch in batch_list])\n    n_feats = batch_list[0].shape[-1]\n    batch_pad = np.zeros((batch_size, maxlen, n_feats))\n    for idx, batch in enumerate(batch_list):\n        batch_pad[idx, :batch.shape[0]] = batch\n\n    return batch_pad\n\n\ndef decode_generator(feat_list,\n                     batch_size=32,\n                     feature_type=""world"",\n                     wav_transform=None,\n                     feat_transform=None,\n                     upsampling_factor=80,\n                     use_upsampling_layer=True,\n                     use_speaker_code=False):\n    """"""GENERATE DECODING BATCH.\n\n    Args:\n        feat_list (list): List of feature files.\n        batch_size (int): Batch size in decoding.\n        feature_type (str): Feature type.\n        wav_transform (func): Preprocessing function for waveform.\n        feat_transform (func): Preprocessing function for aux feats.\n        upsampling_factor (int): Upsampling factor.\n        use_upsampling_layer (bool): Whether to use upsampling layer.\n        use_speaker_code (bool): Whether to use speaker code>\n\n    Returns:\n        generator: Generator instance.\n\n    """"""\n    # ---------------------------\n    # sample-by-sample generation\n    # ---------------------------\n    if batch_size == 1:\n        for featfile in feat_list:\n            x = np.zeros((1))\n            h = read_hdf5(featfile, ""/"" + feature_type)\n            if not use_upsampling_layer:\n                h = extend_time(h, upsampling_factor)\n            if use_speaker_code:\n                sc = read_hdf5(featfile, ""/speaker_code"")\n                sc = np.tile(sc, [h.shape[0], 1])\n                h = np.concatenate([h, sc], axis=1)\n\n            # perform pre-processing\n            if wav_transform is not None:\n                x = wav_transform(x)\n            if feat_transform is not None:\n                h = feat_transform(h)\n\n            # convert to torch variable\n            x = torch.from_numpy(x).long()\n            h = torch.from_numpy(h).float()\n            x = x.unsqueeze(0)  # 1 => 1 x 1\n            h = h.transpose(0, 1).unsqueeze(0)  # T x C => 1 x C x T\n\n            # send to cuda\n            if torch.cuda.is_available():\n                x = x.cuda()\n                h = h.cuda()\n\n            # get target length and file id\n            if not use_upsampling_layer:\n                n_samples = h.size(2) - 1\n            else:\n                n_samples = h.size(2) * upsampling_factor - 1\n            feat_id = os.path.basename(featfile).replace("".h5"", """")\n\n            yield feat_id, (x, h, n_samples)\n\n    # ----------------\n    # batch generation\n    # ----------------\n    else:\n        # sort with the feature length\n        shape_list = [shape_hdf5(f, ""/"" + feature_type)[0] for f in feat_list]\n        idx = np.argsort(shape_list)\n        feat_list = [feat_list[i] for i in idx]\n\n        # divide into batch list\n        n_batch = math.ceil(len(feat_list) / batch_size)\n        batch_lists = np.array_split(feat_list, n_batch)\n        batch_lists = [f.tolist() for f in batch_lists]\n\n        for batch_list in batch_lists:\n            batch_x = []\n            batch_h = []\n            n_samples_list = []\n            feat_ids = []\n            for featfile in batch_list:\n                # make seed waveform and load aux feature\n                x = np.zeros((1))\n                h = read_hdf5(featfile, ""/"" + feature_type)\n                if not use_upsampling_layer:\n                    h = extend_time(h, upsampling_factor)\n                if use_speaker_code:\n                    sc = read_hdf5(featfile, ""/speaker_code"")\n                    sc = np.tile(sc, [h.shape[0], 1])\n                    h = np.concatenate([h, sc], axis=1)\n\n                # perform pre-processing\n                if wav_transform is not None:\n                    x = wav_transform(x)\n                if feat_transform is not None:\n                    h = feat_transform(h)\n\n                # append to list\n                batch_x += [x]\n                batch_h += [h]\n                if not use_upsampling_layer:\n                    n_samples_list += [h.shape[0] - 1]\n                else:\n                    n_samples_list += [h.shape[0] * upsampling_factor - 1]\n                feat_ids += [os.path.basename(featfile).replace("".h5"", """")]\n\n            # convert list to ndarray\n            batch_x = np.stack(batch_x, axis=0)\n            batch_h = pad_list(batch_h)\n\n            # convert to torch variable\n            batch_x = torch.from_numpy(batch_x).long()\n            batch_h = torch.from_numpy(batch_h).float().transpose(1, 2)\n\n            # send to cuda\n            if torch.cuda.is_available():\n                batch_x = batch_x.cuda()\n                batch_h = batch_h.cuda()\n\n            yield feat_ids, (batch_x, batch_h, n_samples_list)\n\n\ndef main():\n    """"""RUN DECODING.""""""\n    parser = argparse.ArgumentParser()\n    # decode setting\n    parser.add_argument(""--feats"", required=True,\n                        type=str, help=""list or directory of aux feat files"")\n    parser.add_argument(""--checkpoint"", required=True,\n                        type=str, help=""model file"")\n    parser.add_argument(""--outdir"", required=True,\n                        type=str, help=""directory to save generated samples"")\n    parser.add_argument(""--stats"", default=None,\n                        type=str, help=""hdf5 file including statistics"")\n    parser.add_argument(""--config"", default=None,\n                        type=str, help=""configure file"")\n    parser.add_argument(""--fs"", default=16000,\n                        type=int, help=""sampling rate"")\n    parser.add_argument(""--batch_size"", default=32,\n                        type=int, help=""number of batch size in decoding"")\n    parser.add_argument(""--n_gpus"", default=1,\n                        type=int, help=""number of gpus"")\n    # other setting\n    parser.add_argument(""--intervals"", default=1000,\n                        type=int, help=""log interval"")\n    parser.add_argument(""--seed"", default=1,\n                        type=int, help=""seed number"")\n    parser.add_argument(""--verbose"", default=1,\n                        type=int, help=""log level"")\n    args = parser.parse_args()\n\n    # set log level\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n    elif args.verbose > 1:\n        logging.basicConfig(level=logging.DEBUG,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n    else:\n        logging.basicConfig(level=logging.WARNING,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n        logging.warning(""logging is disabled."")\n\n    # show arguments\n    for key, value in vars(args).items():\n        logging.info(""%s = %s"" % (key, str(value)))\n\n    # check arguments\n    if args.stats is None:\n        args.stats = os.path.dirname(args.checkpoint) + ""/stats.h5""\n    if args.config is None:\n        args.config = os.path.dirname(args.checkpoint) + ""/model.conf""\n    if not os.path.exists(args.stats):\n        raise FileNotFoundError(""statistics file is missing (%s)."" % (args.stats))\n    if not os.path.exists(args.config):\n        raise FileNotFoundError(""config file is missing (%s)."" % (args.config))\n\n    # check directory existence\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n\n    # fix seed\n    os.environ[\'PYTHONHASHSEED\'] = str(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    # fix slow computation of dilated conv\n    # https://github.com/pytorch/pytorch/issues/15054#issuecomment-450191923\n    torch.backends.cudnn.benchmark = True\n\n    # load config\n    config = torch.load(args.config)\n\n    # get file list\n    if os.path.isdir(args.feats):\n        feat_list = sorted(find_files(args.feats, ""*.h5""))\n    elif os.path.isfile(args.feats):\n        feat_list = read_txt(args.feats)\n    else:\n        logging.error(""--feats should be directory or list."")\n        sys.exit(1)\n\n    # prepare the file list for parallel decoding\n    feat_lists = np.array_split(feat_list, args.n_gpus)\n    feat_lists = [f_list.tolist() for f_list in feat_lists]\n\n    # define transform\n    scaler = StandardScaler()\n    scaler.mean_ = read_hdf5(args.stats, ""/"" + config.feature_type + ""/mean"")\n    scaler.scale_ = read_hdf5(args.stats, ""/"" + config.feature_type + ""/scale"")\n    wav_transform = transforms.Compose([\n        lambda x: encode_mu_law(x, config.n_quantize)])\n    feat_transform = transforms.Compose([\n        lambda x: scaler.transform(x)])\n\n    # define gpu decode function\n    def gpu_decode(feat_list, gpu):\n        # set default gpu and do not track gradient\n        torch.cuda.set_device(gpu)\n        torch.set_grad_enabled(False)\n\n        # define model and load parameters\n        if config.use_upsampling_layer:\n            upsampling_factor = config.upsampling_factor\n        else:\n            upsampling_factor = 0\n        model = WaveNet(\n            n_quantize=config.n_quantize,\n            n_aux=config.n_aux,\n            n_resch=config.n_resch,\n            n_skipch=config.n_skipch,\n            dilation_depth=config.dilation_depth,\n            dilation_repeat=config.dilation_repeat,\n            kernel_size=config.kernel_size,\n            upsampling_factor=upsampling_factor)\n        model.load_state_dict(torch.load(\n            args.checkpoint,\n            map_location=lambda storage,\n            loc: storage)[""model""])\n        model.eval()\n        model.cuda()\n\n        # define generator\n        generator = decode_generator(\n            feat_list,\n            batch_size=args.batch_size,\n            feature_type=config.feature_type,\n            wav_transform=wav_transform,\n            feat_transform=feat_transform,\n            upsampling_factor=config.upsampling_factor,\n            use_upsampling_layer=config.use_upsampling_layer,\n            use_speaker_code=config.use_speaker_code)\n\n        # decode\n        if args.batch_size > 1:\n            for feat_ids, (batch_x, batch_h, n_samples_list) in generator:\n                logging.info(""decoding start"")\n                samples_list = model.batch_fast_generate(\n                    batch_x, batch_h, n_samples_list, args.intervals)\n                for feat_id, samples in zip(feat_ids, samples_list):\n                    wav = decode_mu_law(samples, config.n_quantize)\n                    sf.write(args.outdir + ""/"" + feat_id + "".wav"", wav, args.fs, ""PCM_16"")\n                    logging.info(""wrote %s.wav in %s."" % (feat_id, args.outdir))\n        else:\n            for feat_id, (x, h, n_samples) in generator:\n                logging.info(""decoding %s (length = %d)"" % (feat_id, n_samples))\n                samples = model.fast_generate(x, h, n_samples, args.intervals)\n                wav = decode_mu_law(samples, config.n_quantize)\n                sf.write(args.outdir + ""/"" + feat_id + "".wav"", wav, args.fs, ""PCM_16"")\n                logging.info(""wrote %s.wav in %s."" % (feat_id, args.outdir))\n\n    # parallel decode\n    processes = []\n    for gpu, feat_list in enumerate(feat_lists):\n        p = mp.Process(target=gpu_decode, args=(feat_list, gpu,))\n        p.start()\n        processes.append(p)\n\n    # wait for all process\n    for p in processes:\n        p.join()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
wavenet_vocoder/bin/feature_extract.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport logging\nimport multiprocessing as mp\nimport os\nimport sys\n\nfrom distutils.util import strtobool\n\nimport librosa\nimport numpy as np\nimport pysptk\n\nfrom scipy.interpolate import interp1d\nfrom scipy.io import wavfile\nfrom scipy.signal import firwin\nfrom scipy.signal import get_window\nfrom scipy.signal import lfilter\nfrom sprocket.speech.feature_extractor import FeatureExtractor\n\nfrom wavenet_vocoder.utils import find_files\nfrom wavenet_vocoder.utils import read_txt\nfrom wavenet_vocoder.utils import write_hdf5\n\nEPS = 1e-10\n\n\ndef low_cut_filter(x, fs, cutoff=70):\n    """"""APPLY LOW CUT FILTER.\n\n    Args:\n        x (ndarray): Waveform sequence.\n        fs (int): Sampling frequency.\n        cutoff (float): Cutoff frequency of low cut filter.\n\n    Return:\n        ndarray: Low cut filtered waveform sequence.\n\n    """"""\n    nyquist = fs // 2\n    norm_cutoff = cutoff / nyquist\n\n    # low cut filter\n    fil = firwin(255, norm_cutoff, pass_zero=False)\n    lcf_x = lfilter(fil, 1, x)\n\n    return lcf_x\n\n\ndef low_pass_filter(x, fs, cutoff=70, padding=True):\n    """"""APPLY LOW PASS FILTER.\n\n    Args:\n        x (ndarray): Waveform sequence.\n        fs (int): Sampling frequency.\n        cutoff (float): Cutoff frequency of low pass filter.\n\n    Returns:\n        ndarray: Low pass filtered waveform sequence\n\n    """"""\n    nyquist = fs // 2\n    norm_cutoff = cutoff / nyquist\n\n    # low cut filter\n    numtaps = 255\n    fil = firwin(numtaps, norm_cutoff)\n    x_pad = np.pad(x, (numtaps, numtaps), \'edge\')\n    lpf_x = lfilter(fil, 1, x_pad)\n    lpf_x = lpf_x[numtaps + numtaps // 2: -numtaps // 2]\n\n    return lpf_x\n\n\ndef convert_to_continuos_f0(f0):\n    """"""CONVERT F0 TO CONTINUOUS F0.\n\n    Args:\n        f0 (ndarray): original f0 sequence with the shape (T,).\n\n    Returns:\n        ndarray: continuous f0 with the shape (T,).\n\n    """"""\n    # get uv information as binary\n    uv = np.float32(f0 != 0)\n\n    # get start and end of f0\n    if (f0 == 0).all():\n        logging.warning(""all of the f0 values are 0."")\n        return uv, f0\n    start_f0 = f0[f0 != 0][0]\n    end_f0 = f0[f0 != 0][-1]\n\n    # padding start and end of f0 sequence\n    start_idx = np.where(f0 == start_f0)[0][0]\n    end_idx = np.where(f0 == end_f0)[0][-1]\n    f0[:start_idx] = start_f0\n    f0[end_idx:] = end_f0\n\n    # get non-zero frame index\n    nz_frames = np.where(f0 != 0)[0]\n\n    # perform linear interpolation\n    f = interp1d(nz_frames, f0[nz_frames])\n    cont_f0 = f(np.arange(0, f0.shape[0]))\n\n    return uv, cont_f0\n\n\ndef stft_mcep(x, fftl=512, shiftl=256, dim=25, alpha=0.41, window=""hamming"", is_padding=False):\n    """"""EXTRACT STFT-BASED MEL-CEPSTRUM.\n\n    Args:\n        x (ndarray): Numpy double array with the size (T,).\n        fftl (int): FFT length in point (default=512).\n        shiftl (int): Shift length in point (default=256).\n        dim (int): Dimension of mel-cepstrum (default=25).\n        alpha (float): All pass filter coefficient (default=0.41).\n        window (str): Analysis window type (default=""hamming"").\n        is_padding (bool): Whether to pad the end of signal (default=False).\n\n    Returns:\n        ndarray: Mel-cepstrum with the size (N, n_fft).\n\n    """"""\n    # perform padding\n    if is_padding:\n        n_pad = fftl - (len(x) - fftl) % shiftl\n        x = np.pad(x, (0, n_pad), \'reflect\')\n\n    # get number of frames\n    n_frame = (len(x) - fftl) // shiftl + 1\n\n    # get window function\n    win = get_window(window, fftl)\n\n    # calculate spectrogram\n    mcep = [pysptk.mcep(x[shiftl * i: shiftl * i + fftl] * win,\n                        dim, alpha, eps=EPS, etype=1)\n            for i in range(n_frame)]\n\n    return np.stack(mcep)\n\n\ndef world_feature_extract(wav_list, args):\n    """"""EXTRACT WORLD FEATURE VECTOR.""""""\n    # define feature extractor\n    feature_extractor = FeatureExtractor(\n        analyzer=""world"",\n        fs=args.fs,\n        shiftms=args.shiftms,\n        minf0=args.minf0,\n        maxf0=args.maxf0,\n        fftl=args.fftl)\n\n    for i, wav_name in enumerate(wav_list):\n        logging.info(""now processing %s (%d/%d)"" % (wav_name, i + 1, len(wav_list)))\n\n        # load wavfile and apply low cut filter\n        fs, x = wavfile.read(wav_name)\n        if x.dtype != np.int16:\n            logging.warning(""wav file format is not 16 bit PCM."")\n        x = np.array(x, dtype=np.float64)\n        if args.highpass_cutoff != 0:\n            x = low_cut_filter(x, fs, cutoff=args.highpass_cutoff)\n\n        # check sampling frequency\n        if not fs == args.fs:\n            logging.error(""sampling frequency is not matched."")\n            sys.exit(1)\n\n        # extract features\n        f0, _, _ = feature_extractor.analyze(x)\n        uv, cont_f0 = convert_to_continuos_f0(f0)\n        cont_f0_lpf = low_pass_filter(cont_f0, int(1.0 / (args.shiftms * 0.001)), cutoff=20)\n        codeap = feature_extractor.codeap()\n        mcep = feature_extractor.mcep(dim=args.mcep_dim, alpha=args.mcep_alpha)\n\n        # concatenate\n        cont_f0_lpf = np.expand_dims(cont_f0_lpf, axis=-1)\n        uv = np.expand_dims(uv, axis=-1)\n        feats = np.concatenate([uv, cont_f0_lpf, mcep, codeap], axis=1)\n\n        # save to hdf5\n        hdf5name = args.hdf5dir + ""/"" + os.path.basename(wav_name).replace("".wav"", "".h5"")\n        write_hdf5(hdf5name, ""/world"", feats)\n\n        # overwrite wav file\n        if args.highpass_cutoff != 0 and args.save_wav:\n            wavfile.write(args.wavdir + ""/"" + os.path.basename(wav_name), fs, np.int16(x))\n\n\ndef melspectrogram_extract(wav_list, args):\n    """"""EXTRACT MEL SPECTROGRAM.""""""\n    # define feature extractor\n    for i, wav_name in enumerate(wav_list):\n        logging.info(""now processing %s (%d/%d)"" % (wav_name, i + 1, len(wav_list)))\n\n        # load wavfile and apply low cut filter\n        fs, x = wavfile.read(wav_name)\n        if x.dtype != np.int16:\n            logging.warning(""wav file format is not 16 bit PCM."")\n        x = np.array(x, dtype=np.float64)\n        if args.highpass_cutoff != 0:\n            x = low_cut_filter(x, fs, cutoff=args.highpass_cutoff)\n\n        # check sampling frequency\n        if not fs == args.fs:\n            logging.error(""sampling frequency is not matched."")\n            sys.exit(1)\n\n        # extract features\n        x_norm = x / (np.iinfo(np.int16).max + 1)\n        shiftl = int(args.shiftms * fs * 0.001)\n        mspc = librosa.feature.melspectrogram(\n            x_norm, fs,\n            n_fft=args.fftl,\n            hop_length=shiftl,\n            n_mels=args.mspc_dim,\n            fmin=args.fmin if args.fmin is not None else 0,\n            fmax=args.fmax if args.fmax is not None else fs // 2,\n            power=1.0)\n        mspc = np.log10(np.maximum(EPS, mspc.T))\n\n        # save to hdf5\n        hdf5name = args.hdf5dir + ""/"" + os.path.basename(wav_name).replace("".wav"", "".h5"")\n        write_hdf5(hdf5name, ""/melspc"", np.float32(mspc))\n\n        # overwrite wav file\n        if args.highpass_cutoff != 0 and args.save_wav:\n            wavfile.write(args.wavdir + ""/"" + os.path.basename(wav_name), fs, np.int16(x))\n\n\ndef melcepstrum_extract(wav_list, args):\n    """"""EXTRACT MEL CEPSTRUM.""""""\n    # define feature extractor\n    for i, wav_name in enumerate(wav_list):\n        logging.info(""now processing %s (%d/%d)"" % (wav_name, i + 1, len(wav_list)))\n\n        # load wavfile and apply low cut filter\n        fs, x = wavfile.read(wav_name)\n        if x.dtype != np.int16:\n            logging.warning(""wav file format is not 16 bit PCM."")\n        x = np.array(x, dtype=np.float64)\n        if args.highpass_cutoff != 0:\n            x = low_cut_filter(x, fs, cutoff=args.highpass_cutoff)\n\n        # check sampling frequency\n        if not fs == args.fs:\n            logging.error(""sampling frequency is not matched."")\n            sys.exit(1)\n\n        # extract features\n        shiftl = int(args.shiftms * fs * 0.001)\n        mcep = stft_mcep(x, args.fftl, shiftl, args.mcep_dim, args.mcep_alpha)\n\n        # save to hdf5\n        hdf5name = args.hdf5dir + ""/"" + os.path.basename(wav_name).replace("".wav"", "".h5"")\n        write_hdf5(hdf5name, ""/mcep"", np.float32(mcep))\n\n        # overwrite wav file\n        if args.highpass_cutoff != 0 and args.save_wav:\n            wavfile.write(args.wavdir + ""/"" + os.path.basename(wav_name), fs, np.int16(x))\n\n\ndef main():\n    """"""RUN FEATURE EXTRACTION IN PARALLEL.""""""\n    parser = argparse.ArgumentParser(\n        description=""making feature file argsurations."")\n\n    parser.add_argument(\n        ""--waveforms"", default=None,\n        help=""directory or list of filename of input wavfile"")\n    parser.add_argument(\n        ""--hdf5dir"", default=None,\n        help=""directory to save hdf5"")\n    parser.add_argument(\n        ""--wavdir"", default=None,\n        help=""directory to save of preprocessed wav file"")\n    parser.add_argument(\n        ""--fs"", default=16000,\n        type=int, help=""Sampling frequency"")\n    parser.add_argument(\n        ""--shiftms"", default=5,\n        type=float, help=""Frame shift in msec"")\n    parser.add_argument(\n        ""--feature_type"", default=""world"", choices=[""world"", ""melspc"", ""mcep""],\n        type=str, help=""feature type"")\n    parser.add_argument(\n        ""--mspc_dim"", default=80,\n        type=int, help=""Dimension of mel spectrogram"")\n    parser.add_argument(\n        ""--minf0"", default=40,\n        type=int, help=""minimum f0 for world analysis"")\n    parser.add_argument(\n        ""--maxf0"", default=400,\n        type=int, help=""maximum f0 for world analysis"")\n    parser.add_argument(\n        ""--fmin"", default=None, nargs=""?"",\n        type=int, help=""minimum frequency for melspc"")\n    parser.add_argument(\n        ""--fmax"", default=None, nargs=""?"",\n        type=int, help=""maximum frequency for melspc"")\n    parser.add_argument(\n        ""--mcep_dim"", default=24,\n        type=int, help=""Dimension of mel cepstrum"")\n    parser.add_argument(\n        ""--mcep_alpha"", default=0.41,\n        type=float, help=""Alpha of mel cepstrum"")\n    parser.add_argument(\n        ""--fftl"", default=1024,\n        type=int, help=""FFT length"")\n    parser.add_argument(\n        ""--highpass_cutoff"", default=70,\n        type=int, help=""Cut off frequency in lowpass filter"")\n    parser.add_argument(\n        ""--save_wav"", default=True,\n        type=strtobool, help=""Whether to save filtered wav file"")\n    parser.add_argument(\n        ""--n_jobs"", default=10,\n        type=int, help=""number of parallel jobs"")\n    parser.add_argument(\n        ""--verbose"", default=1,\n        type=int, help=""log message level"")\n\n    args = parser.parse_args()\n\n    # set log level\n    if args.verbose == 1:\n        logging.basicConfig(level=logging.INFO,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n    elif args.verbose > 1:\n        logging.basicConfig(level=logging.DEBUG,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n    else:\n        logging.basicConfig(level=logging.WARNING,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n        logging.warning(""logging is disabled."")\n\n    # show arguments\n    for key, value in vars(args).items():\n        logging.info(""%s = %s"" % (key, str(value)))\n\n    # read list\n    if os.path.isdir(args.waveforms):\n        file_list = sorted(find_files(args.waveforms, ""*.wav""))\n    else:\n        file_list = read_txt(args.waveforms)\n    logging.info(""number of utterances = %d"" % len(file_list))\n\n    # check directory existence\n    if not os.path.exists(args.wavdir) and args.highpass_cutoff != 0 and args.save_wav:\n        os.makedirs(args.wavdir)\n    if not os.path.exists(args.hdf5dir):\n        os.makedirs(args.hdf5dir)\n\n    # divide list\n    file_lists = np.array_split(file_list, args.n_jobs)\n    file_lists = [f_list.tolist() for f_list in file_lists]\n\n    # multi processing\n    processes = []\n    if args.feature_type == ""world"":\n        target_fn = world_feature_extract\n    elif args.feature_type == ""melspc"":\n        target_fn = melspectrogram_extract\n    else:\n        target_fn = melcepstrum_extract\n    for f in file_lists:\n        p = mp.Process(target=target_fn, args=(f, args,))\n        p.start()\n        processes.append(p)\n\n    # wait for all process\n    for p in processes:\n        p.join()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
wavenet_vocoder/bin/noise_shaping.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport logging\nimport multiprocessing as mp\nimport os\nimport sys\n\nfrom distutils.util import strtobool\n\nimport numpy as np\nimport pysptk\n\nfrom scipy.io import wavfile\n\nfrom wavenet_vocoder.utils import check_hdf5\nfrom wavenet_vocoder.utils import find_files\nfrom wavenet_vocoder.utils import read_hdf5\nfrom wavenet_vocoder.utils import read_txt\nfrom wavenet_vocoder.utils import write_hdf5\n\n\ndef convert_mcep_to_mlsa_coef(avg_mcep, mag, alpha):\n    """"""CONVERT AVERAGE MEL-CEPTSRUM TO MLSA FILTER COEFFICIENT.\n\n    Args:\n        avg_mcep (ndarray): Averaged Mel-cepstrum (D,).\n        mag (float): Magnification of noise shaping.\n        alpha (float): All pass constant value.\n\n    Return:\n        ndarray: MLSA filter coefficient (D,).\n\n    """"""\n    avg_mcep *= mag\n    avg_mcep[0] = 0.0\n    coef = pysptk.mc2b(avg_mcep.astype(np.float64), alpha)\n    assert np.isfinite(coef).all()\n    return coef\n\n\ndef noise_shaping(wav_list, args):\n    """"""APPLY NOISE SHAPING BASED ON MLSA FILTER.""""""\n    # load coefficient of filter\n    if check_hdf5(args.stats, ""/mlsa/coef""):\n        mlsa_coef = read_hdf5(args.stats, ""/mlsa/coef"")\n        alpha = read_hdf5(args.stats, ""/mlsa/alpha"")\n    else:\n        raise KeyError(""\\""/mlsa/coef\\"" is not found in %s."" % (args.stats))\n    if args.inv:\n        mlsa_coef *= -1.0\n\n    # define synthesizer\n    shiftl = int(args.fs / 1000 * args.shiftms)\n    synthesizer = pysptk.synthesis.Synthesizer(\n        pysptk.synthesis.MLSADF(\n            order=mlsa_coef.shape[0] - 1,\n            alpha=alpha),\n        hopsize=shiftl\n    )\n\n    for i, wav_name in enumerate(wav_list):\n        logging.info(""now processing %s (%d/%d)"" % (wav_name, i + 1, len(wav_list)))\n\n        # load wavfile and apply low cut filter\n        fs, x = wavfile.read(wav_name)\n        if x.dtype != np.int16:\n            logging.warning(""wav file format is not 16 bit PCM."")\n        x = np.float64(x)\n\n        # check sampling frequency\n        if not fs == args.fs:\n            logging.error(""sampling frequency is not matched."")\n            sys.exit(1)\n\n        # replicate coef for time-invariant filtering\n        num_frames = int(len(x) / shiftl) + 1\n        mlsa_coefs = np.float64(np.tile(mlsa_coef, [num_frames, 1]))\n\n        # synthesis and write\n        x_ns = synthesizer.synthesis(x, mlsa_coefs)\n        write_name = args.outdir + ""/"" + os.path.basename(wav_name)\n        wavfile.write(write_name, args.fs, np.int16(x_ns))\n\n\ndef main():\n    """"""RUN NOISE SHAPING IN PARALLEL.""""""\n    parser = argparse.ArgumentParser(\n        description=""making feature file argsurations."")\n\n    parser.add_argument(\n        ""--waveforms"", default=None,\n        help=""directory or list of filename of input wavfile"")\n    parser.add_argument(\n        ""--stats"", default=None,\n        help=""filename of hdf5 format"")\n    parser.add_argument(\n        ""--outdir"", default=None,\n        help=""directory to save preprocessed wav file"")\n    parser.add_argument(\n        ""--fs"", default=16000,\n        type=int, help=""Sampling frequency"")\n    parser.add_argument(\n        ""--shiftms"", default=5,\n        type=float, help=""Frame shift in msec"")\n    parser.add_argument(\n        ""--feature_type"", default=""world"", choices=[""world"", ""mcep"", ""melspc""],\n        type=str, help=""feature type"")\n    parser.add_argument(\n        ""--mcep_dim_start"", default=2,\n        type=int, help=""Start index of mel cepstrum"")\n    parser.add_argument(\n        ""--mcep_dim_end"", default=27,\n        type=int, help=""End index of mel cepstrum"")\n    parser.add_argument(\n        ""--mcep_alpha"", default=0.41,\n        type=float, help=""Alpha of mel cepstrum"")\n    parser.add_argument(\n        ""--mag"", default=0.5,\n        type=float, help=""magnification of noise shaping"")\n    parser.add_argument(\n        ""--verbose"", default=1,\n        type=int, help=""log message level"")\n    parser.add_argument(\n        \'--n_jobs\', default=10,\n        type=int, help=""number of parallel jobs"")\n    parser.add_argument(\n        \'--inv\', default=False, type=strtobool,\n        help=""if True, inverse filtering will be performed"")\n\n    args = parser.parse_args()\n\n    # set log level\n    if args.verbose == 1:\n        logging.basicConfig(level=logging.INFO,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n    elif args.verbose > 1:\n        logging.basicConfig(level=logging.DEBUG,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n    else:\n        logging.basicConfig(level=logging.WARNING,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n        logging.warning(""logging is disabled."")\n\n    # show arguments\n    for key, value in vars(args).items():\n        logging.info(""%s = %s"" % (key, str(value)))\n\n    # read list\n    if os.path.isdir(args.waveforms):\n        file_list = sorted(find_files(args.waveforms, ""*.wav""))\n    else:\n        file_list = read_txt(args.waveforms)\n    logging.info(""number of utterances = %d"" % len(file_list))\n\n    # check directory existence\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n\n    # divide list\n    file_lists = np.array_split(file_list, args.n_jobs)\n    file_lists = [f_list.tolist() for f_list in file_lists]\n\n    # calculate MLSA coef ans save it\n    if not check_hdf5(args.stats, ""/mlsa/coef""):\n        avg_mcep = read_hdf5(args.stats, args.feature_type + ""/mean"")\n        if args.feature_type == ""world"":\n            avg_mcep = avg_mcep[args.mcep_dim_start:args.mcep_dim_end]\n        mlsa_coef = convert_mcep_to_mlsa_coef(avg_mcep, args.mag, args.mcep_alpha)\n        write_hdf5(args.stats, ""/mlsa/coef"", mlsa_coef)\n        write_hdf5(args.stats, ""/mlsa/alpha"", args.mcep_alpha)\n\n    # multi processing\n    processes = []\n    if args.feature_type == ""melspc"":\n        # TODO(kan-bayashi): implement noise shaping using melspectrogram\n        raise NotImplementedError(""currently, support only world and mcep."")\n    for f in file_lists:\n        p = mp.Process(target=noise_shaping, args=(f, args,))\n        p.start()\n        processes.append(p)\n\n    # wait for all process\n    for p in processes:\n        p.join()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
wavenet_vocoder/bin/train.py,31,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport logging\nimport os\nimport sys\nimport time\n\nfrom dateutil.relativedelta import relativedelta\nfrom distutils.util import strtobool\n\nimport numpy as np\nimport six\nimport soundfile as sf\nimport torch\n\nfrom sklearn.preprocessing import StandardScaler\nfrom torch import nn\nfrom torchvision import transforms\n\nfrom wavenet_vocoder.nets import encode_mu_law\nfrom wavenet_vocoder.nets import initialize\nfrom wavenet_vocoder.nets import WaveNet\nfrom wavenet_vocoder.utils import background\nfrom wavenet_vocoder.utils import extend_time\nfrom wavenet_vocoder.utils import find_files\nfrom wavenet_vocoder.utils import read_hdf5\nfrom wavenet_vocoder.utils import read_txt\n\n\ndef validate_length(x, y, upsampling_factor=None):\n    """"""VALIDATE LENGTH.\n\n    Args:\n        x (ndarray): ndarray with x.shape[0] = len_x.\n        y (ndarray): ndarray with y.shape[0] = len_y.\n        upsampling_factor (int): Upsampling factor.\n\n    Returns:\n        ndarray: Length adjusted x with same length y.\n        ndarray: Length adjusted y with same length x.\n\n    """"""\n    if upsampling_factor is None:\n        if x.shape[0] < y.shape[0]:\n            y = y[:x.shape[0]]\n        if x.shape[0] > y.shape[0]:\n            x = x[:y.shape[0]]\n        assert len(x) == len(y)\n    else:\n        if x.shape[0] > y.shape[0] * upsampling_factor:\n            x = x[:y.shape[0] * upsampling_factor]\n        if x.shape[0] < y.shape[0] * upsampling_factor:\n            mod_y = y.shape[0] * upsampling_factor - x.shape[0]\n            mod_y_frame = mod_y // upsampling_factor + 1\n            y = y[:-mod_y_frame]\n            x = x[:y.shape[0] * upsampling_factor]\n        assert len(x) == len(y) * upsampling_factor\n\n    return x, y\n\n\n@background(max_prefetch=16)\ndef train_generator(wav_list, feat_list, receptive_field,\n                    batch_length=None,\n                    batch_size=1,\n                    feature_type=""world"",\n                    wav_transform=None,\n                    feat_transform=None,\n                    shuffle=True,\n                    upsampling_factor=80,\n                    use_upsampling_layer=True,\n                    use_speaker_code=False):\n    """"""GENERATE TRAINING BATCH.\n\n    Args:\n        wav_list (list): List of wav files.\n        feat_list (list): List of feat files.\n        receptive_field (int): Size of receptive filed.\n        batch_length (int): Batch length (if set None, utterance batch will be used.).\n        batch_size (int): Batch size (if batch_length = None, batch_size will be 1.).\n        feature_type (str): Auxiliary feature type.\n        wav_transform (func): Preprocessing function for waveform.\n        feat_transform (func): Preprocessing function for aux feats.\n        shuffle (bool): Whether to shuffle the file list.\n        upsampling_factor (int): Upsampling factor.\n        use_upsampling_layer (bool): Whether to use upsampling layer.\n        use_speaker_code (bool): Whether to use speaker code.\n\n    Returns:\n        generator: Generator instance.\n\n    """"""\n    # shuffle list\n    if shuffle:\n        n_files = len(wav_list)\n        idx = np.random.permutation(n_files)\n        wav_list = [wav_list[i] for i in idx]\n        feat_list = [feat_list[i] for i in idx]\n\n    # check batch_length\n    if batch_length is not None and use_upsampling_layer:\n        batch_mod = (receptive_field + batch_length) % upsampling_factor\n        logging.warning(""batch length is decreased due to upsampling (%d -> %d)"" % (\n            batch_length, batch_length - batch_mod))\n        batch_length -= batch_mod\n\n    # show warning\n    if batch_length is None and batch_size > 1:\n        logging.warning(""in utterance batch mode, batchsize will be 1."")\n\n    while True:\n        batch_x, batch_h, batch_t = [], [], []\n        # process over all of files\n        for wavfile, featfile in zip(wav_list, feat_list):\n            # load waveform and aux feature\n            x, fs = sf.read(wavfile, dtype=np.float32)\n            h = read_hdf5(featfile, ""/"" + feature_type)\n            if not use_upsampling_layer:\n                h = extend_time(h, upsampling_factor)\n            if use_speaker_code:\n                sc = read_hdf5(featfile, ""/speaker_code"")\n                sc = np.tile(sc, [h.shape[0], 1])\n                h = np.concatenate([h, sc], axis=1)\n\n            # check both lengths are same\n            logging.debug(""before x length = %d"" % x.shape[0])\n            logging.debug(""before h length = %d"" % h.shape[0])\n            if use_upsampling_layer:\n                x, h = validate_length(x, h, upsampling_factor)\n            else:\n                x, h = validate_length(x, h)\n            logging.debug(""after x length = %d"" % x.shape[0])\n            logging.debug(""after h length = %d"" % h.shape[0])\n\n            # ---------------------------------------\n            # use mini batch without upsampling layer\n            # ---------------------------------------\n            if batch_length is not None and not use_upsampling_layer:\n                # make buffer array\n                if ""x_buffer"" not in locals():\n                    x_buffer = np.empty((0), dtype=np.float32)\n                    h_buffer = np.empty((0, h.shape[1]), dtype=np.float32)\n                x_buffer = np.concatenate([x_buffer, x], axis=0)\n                h_buffer = np.concatenate([h_buffer, h], axis=0)\n\n                while len(x_buffer) > receptive_field + batch_length:\n                    # get pieces\n                    x_ = x_buffer[:receptive_field + batch_length]\n                    h_ = h_buffer[:receptive_field + batch_length]\n\n                    # perform pre-processing\n                    if wav_transform is not None:\n                        x_ = wav_transform(x_)\n                    if feat_transform is not None:\n                        h_ = feat_transform(h_)\n\n                    # convert to torch variable\n                    x_ = torch.from_numpy(x_).long()\n                    h_ = torch.from_numpy(h_).float()\n\n                    # remove the last and first sample for training\n                    batch_x += [x_[:-1]]  # (T)\n                    batch_h += [h_[:-1].transpose(0, 1)]  # (D x T)\n                    batch_t += [x_[1:]]  # (T)\n\n                    # update buffer\n                    x_buffer = x_buffer[batch_length:]\n                    h_buffer = h_buffer[batch_length:]\n\n                    # return mini batch\n                    if len(batch_x) == batch_size:\n                        batch_x = torch.stack(batch_x)\n                        batch_h = torch.stack(batch_h)\n                        batch_t = torch.stack(batch_t)\n\n                        # send to cuda\n                        if torch.cuda.is_available():\n                            batch_x = batch_x.cuda()\n                            batch_h = batch_h.cuda()\n                            batch_t = batch_t.cuda()\n\n                        yield (batch_x, batch_h), batch_t\n\n                        batch_x, batch_h, batch_t = [], [], []\n\n            # ------------------------------------\n            # use mini batch with upsampling layer\n            # ------------------------------------\n            elif batch_length is not None and use_upsampling_layer:\n                # make buffer array\n                if ""x_buffer"" not in locals():\n                    x_buffer = np.empty((0), dtype=np.float32)\n                    h_buffer = np.empty((0, h.shape[1]), dtype=np.float32)\n                x_buffer = np.concatenate([x_buffer, x], axis=0)\n                h_buffer = np.concatenate([h_buffer, h], axis=0)\n\n                while len(h_buffer) > (receptive_field + batch_length) // upsampling_factor:\n                    # set batch size\n                    h_bs = (receptive_field + batch_length) // upsampling_factor\n                    x_bs = h_bs * upsampling_factor + 1\n\n                    # get pieces\n                    h_ = h_buffer[:h_bs]\n                    x_ = x_buffer[:x_bs]\n\n                    # perform pre-processing\n                    if wav_transform is not None:\n                        x_ = wav_transform(x_)\n                    if feat_transform is not None:\n                        h_ = feat_transform(h_)\n\n                    # convert to torch variable\n                    x_ = torch.from_numpy(x_).long()\n                    h_ = torch.from_numpy(h_).float()\n\n                    # remove the last and first sample for training\n                    batch_h += [h_.transpose(0, 1)]  # (D x T)\n                    batch_x += [x_[:-1]]  # (T)\n                    batch_t += [x_[1:]]  # (T)\n\n                    # set shift size\n                    h_ss = batch_length // upsampling_factor\n                    x_ss = h_ss * upsampling_factor\n\n                    # update buffer\n                    h_buffer = h_buffer[h_ss:]\n                    x_buffer = x_buffer[x_ss:]\n\n                    # return mini batch\n                    if len(batch_x) == batch_size:\n                        batch_x = torch.stack(batch_x)\n                        batch_h = torch.stack(batch_h)\n                        batch_t = torch.stack(batch_t)\n\n                        # send to cuda\n                        if torch.cuda.is_available():\n                            batch_x = batch_x.cuda()\n                            batch_h = batch_h.cuda()\n                            batch_t = batch_t.cuda()\n\n                        yield (batch_x, batch_h), batch_t\n\n                        batch_x, batch_h, batch_t = [], [], []\n\n            # --------------------------------------------\n            # use utterance batch without upsampling layer\n            # --------------------------------------------\n            elif batch_length is None and not use_upsampling_layer:\n                # perform pre-processing\n                if wav_transform is not None:\n                    x = wav_transform(x)\n                if feat_transform is not None:\n                    h = feat_transform(h)\n\n                # convert to torch variable\n                x = torch.from_numpy(x).long()\n                h = torch.from_numpy(h).float()\n\n                # remove the last and first sample for training\n                batch_x = x[:-1].unsqueeze(0)  # (1 x T)\n                batch_h = h[:-1].transpose(0, 1).unsqueeze(0)  # (1 x D x T)\n                batch_t = x[1:].unsqueeze(0)  # (1 x T)\n\n                # send to cuda\n                if torch.cuda.is_available():\n                    batch_x = batch_x.cuda()\n                    batch_h = batch_h.cuda()\n                    batch_t = batch_t.cuda()\n\n                yield (batch_x, batch_h), batch_t\n\n            # -----------------------------------------\n            # use utterance batch with upsampling layer\n            # -----------------------------------------\n            else:\n                # remove last frame\n                h = h[:-1]\n                x = x[:-upsampling_factor + 1]\n\n                # perform pre-processing\n                if wav_transform is not None:\n                    x = wav_transform(x)\n                if feat_transform is not None:\n                    h = feat_transform(h)\n\n                # convert to torch variable\n                x = torch.from_numpy(x).long()\n                h = torch.from_numpy(h).float()\n\n                # remove the last and first sample for training\n                batch_h = h.transpose(0, 1).unsqueeze(0)  # (1 x D x T\')\n                batch_x = x[:-1].unsqueeze(0)  # (1 x T)\n                batch_t = x[1:].unsqueeze(0)  # (1 x T)\n\n                # send to cuda\n                if torch.cuda.is_available():\n                    batch_x = batch_x.cuda()\n                    batch_h = batch_h.cuda()\n                    batch_t = batch_t.cuda()\n\n                yield (batch_x, batch_h), batch_t\n\n        # re-shuffle\n        if shuffle:\n            idx = np.random.permutation(n_files)\n            wav_list = [wav_list[i] for i in idx]\n            feat_list = [feat_list[i] for i in idx]\n\n\ndef save_checkpoint(checkpoint_dir, model, optimizer, iterations):\n    """"""SAVE CHECKPOINT.\n\n    Args:\n        checkpoint_dir (str): Directory to save checkpoint.\n        model (torch.nn.Module): Pytorch model instance.\n        optimizer (torch.optim.optimizer): Pytorch optimizer instance.\n        iterations (int): Number of current iterations.\n\n    """"""\n    checkpoint = {\n        ""model"": model.state_dict(),\n        ""optimizer"": optimizer.state_dict(),\n        ""iterations"": iterations}\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n    torch.save(checkpoint, checkpoint_dir + ""/checkpoint-%d.pkl"" % iterations)\n    logging.info(""%d-iter checkpoint created."" % iterations)\n\n\ndef main():\n    """"""RUN TRAINING.""""""\n    parser = argparse.ArgumentParser()\n    # path setting\n    parser.add_argument(""--waveforms"", required=True,\n                        type=str, help=""directory or list of wav files"")\n    parser.add_argument(""--feats"", required=True,\n                        type=str, help=""directory or list of aux feat files"")\n    parser.add_argument(""--stats"", required=True,\n                        type=str, help=""hdf5 file including statistics"")\n    parser.add_argument(""--expdir"", required=True,\n                        type=str, help=""directory to save the model"")\n    parser.add_argument(""--feature_type"", default=""world"", choices=[""world"", ""melspc""],\n                        type=str, help=""feature type"")\n    # network structure setting\n    parser.add_argument(""--n_quantize"", default=256,\n                        type=int, help=""number of quantization"")\n    parser.add_argument(""--n_aux"", default=28,\n                        type=int, help=""number of dimension of aux feats"")\n    parser.add_argument(""--n_resch"", default=512,\n                        type=int, help=""number of channels of residual output"")\n    parser.add_argument(""--n_skipch"", default=256,\n                        type=int, help=""number of channels of skip output"")\n    parser.add_argument(""--dilation_depth"", default=10,\n                        type=int, help=""depth of dilation"")\n    parser.add_argument(""--dilation_repeat"", default=1,\n                        type=int, help=""number of repeating of dilation"")\n    parser.add_argument(""--kernel_size"", default=2,\n                        type=int, help=""kernel size of dilated causal convolution"")\n    parser.add_argument(""--upsampling_factor"", default=80,\n                        type=int, help=""upsampling factor of aux features"")\n    parser.add_argument(""--use_upsampling_layer"", default=True,\n                        type=strtobool, help=""flag to use upsampling layer"")\n    parser.add_argument(""--use_speaker_code"", default=False,\n                        type=strtobool, help=""flag to use speaker code"")\n    # network training setting\n    parser.add_argument(""--lr"", default=1e-4,\n                        type=float, help=""learning rate"")\n    parser.add_argument(""--weight_decay"", default=0.0,\n                        type=float, help=""weight decay coefficient"")\n    parser.add_argument(""--batch_length"", default=20000,\n                        type=int, help=""batch length (if set 0, utterance batch will be used)"")\n    parser.add_argument(""--batch_size"", default=1,\n                        type=int, help=""batch size (if use utterance batch, batch_size will be 1."")\n    parser.add_argument(""--iters"", default=200000,\n                        type=int, help=""number of iterations"")\n    # other setting\n    parser.add_argument(""--checkpoint_interval"", default=10000,\n                        type=int, help=""how frequent saving model"")\n    parser.add_argument(""--intervals"", default=100,\n                        type=int, help=""log interval"")\n    parser.add_argument(""--seed"", default=1,\n                        type=int, help=""seed number"")\n    parser.add_argument(""--resume"", default=None, nargs=""?"",\n                        type=str, help=""model path to restart training"")\n    parser.add_argument(""--n_gpus"", default=1,\n                        type=int, help=""number of gpus"")\n    parser.add_argument(""--verbose"", default=1,\n                        type=int, help=""log level"")\n    args = parser.parse_args()\n\n    # set log level\n    if args.verbose == 1:\n        logging.basicConfig(level=logging.INFO,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n    elif args.verbose > 1:\n        logging.basicConfig(level=logging.DEBUG,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n    else:\n        logging.basicConfig(level=logging.WARNING,\n                            format=\'%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\',\n                            datefmt=\'%m/%d/%Y %I:%M:%S\')\n        logging.warning(""logging is disabled."")\n\n    # show arguments\n    for key, value in vars(args).items():\n        logging.info(""%s = %s"" % (key, str(value)))\n\n    # make experimental directory\n    if not os.path.exists(args.expdir):\n        os.makedirs(args.expdir)\n\n    # fix seed\n    os.environ[\'PYTHONHASHSEED\'] = str(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    # fix slow computation of dilated conv\n    # https://github.com/pytorch/pytorch/issues/15054#issuecomment-450191923\n    torch.backends.cudnn.benchmark = True\n\n    # save args as conf\n    torch.save(args, args.expdir + ""/model.conf"")\n\n    # define network\n    if args.use_upsampling_layer:\n        upsampling_factor = args.upsampling_factor\n    else:\n        upsampling_factor = 0\n    model = WaveNet(\n        n_quantize=args.n_quantize,\n        n_aux=args.n_aux,\n        n_resch=args.n_resch,\n        n_skipch=args.n_skipch,\n        dilation_depth=args.dilation_depth,\n        dilation_repeat=args.dilation_repeat,\n        kernel_size=args.kernel_size,\n        upsampling_factor=upsampling_factor)\n    logging.info(model)\n    model.apply(initialize)\n    model.train()\n\n    if args.n_gpus > 1:\n        device_ids = range(args.n_gpus)\n        model = torch.nn.DataParallel(model, device_ids)\n        model.receptive_field = model.module.receptive_field\n        if args.n_gpus > args.batch_size:\n            logging.warning(""batch size is less than number of gpus."")\n\n    # define optimizer and loss\n    optimizer = torch.optim.Adam(\n        model.parameters(),\n        lr=args.lr,\n        weight_decay=args.weight_decay)\n    criterion = nn.CrossEntropyLoss()\n\n    # define transforms\n    scaler = StandardScaler()\n    scaler.mean_ = read_hdf5(args.stats, ""/"" + args.feature_type + ""/mean"")\n    scaler.scale_ = read_hdf5(args.stats, ""/"" + args.feature_type + ""/scale"")\n    wav_transform = transforms.Compose([\n        lambda x: encode_mu_law(x, args.n_quantize)])\n    feat_transform = transforms.Compose([\n        lambda x: scaler.transform(x)])\n\n    # define generator\n    if os.path.isdir(args.waveforms):\n        filenames = sorted(find_files(args.waveforms, ""*.wav"", use_dir_name=False))\n        wav_list = [args.waveforms + ""/"" + filename for filename in filenames]\n        feat_list = [args.feats + ""/"" + filename.replace("".wav"", "".h5"") for filename in filenames]\n    elif os.path.isfile(args.waveforms):\n        wav_list = read_txt(args.waveforms)\n        feat_list = read_txt(args.feats)\n    else:\n        logging.error(""--waveforms should be directory or list."")\n        sys.exit(1)\n    assert len(wav_list) == len(feat_list)\n    logging.info(""number of training data = %d."" % len(wav_list))\n    generator = train_generator(\n        wav_list, feat_list,\n        receptive_field=model.receptive_field,\n        batch_length=args.batch_length,\n        batch_size=args.batch_size,\n        feature_type=args.feature_type,\n        wav_transform=wav_transform,\n        feat_transform=feat_transform,\n        shuffle=True,\n        upsampling_factor=args.upsampling_factor,\n        use_upsampling_layer=args.use_upsampling_layer,\n        use_speaker_code=args.use_speaker_code)\n\n    # charge minibatch in queue\n    while not generator.queue.full():\n        time.sleep(0.1)\n\n    # resume model and optimizer\n    if args.resume is not None and len(args.resume) != 0:\n        checkpoint = torch.load(args.resume, map_location=lambda storage, loc: storage)\n        iterations = checkpoint[""iterations""]\n        if args.n_gpus > 1:\n            model.module.load_state_dict(checkpoint[""model""])\n        else:\n            model.load_state_dict(checkpoint[""model""])\n        optimizer.load_state_dict(checkpoint[""optimizer""])\n        logging.info(""restored from %d-iter checkpoint."" % iterations)\n    else:\n        iterations = 0\n\n    # check gpu and then send to gpu\n    if torch.cuda.is_available():\n        model.cuda()\n        criterion.cuda()\n        for state in optimizer.state.values():\n            for key, value in state.items():\n                if torch.is_tensor(value):\n                    state[key] = value.cuda()\n    else:\n        logging.error(""gpu is not available. please check the setting."")\n        sys.exit(1)\n\n    # train\n    loss = 0\n    total = 0\n    for i in six.moves.range(iterations, args.iters):\n        start = time.time()\n        (batch_x, batch_h), batch_t = generator.next()\n        batch_output = model(batch_x, batch_h)\n        batch_loss = criterion(\n            batch_output[:, model.receptive_field:].contiguous().view(-1, args.n_quantize),\n            batch_t[:, model.receptive_field:].contiguous().view(-1))\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n        loss += batch_loss.item()\n        total += time.time() - start\n        logging.debug(""batch loss = %.3f (%.3f sec / batch)"" % (\n            batch_loss.item(), time.time() - start))\n\n        # report progress\n        if (i + 1) % args.intervals == 0:\n            logging.info(""(iter:%d) average loss = %.6f (%.3f sec / batch)"" % (\n                i + 1, loss / args.intervals, total / args.intervals))\n            logging.info(""estimated required time = ""\n                         ""{0.days:02}:{0.hours:02}:{0.minutes:02}:{0.seconds:02}""\n                         .format(relativedelta(\n                             seconds=int((args.iters - (i + 1)) * (total / args.intervals)))))\n            loss = 0\n            total = 0\n\n        # save intermidiate model\n        if (i + 1) % args.checkpoint_interval == 0:\n            if args.n_gpus > 1:\n                save_checkpoint(args.expdir, model.module, optimizer, i + 1)\n            else:\n                save_checkpoint(args.expdir, model, optimizer, i + 1)\n\n    # save final model\n    if args.n_gpus > 1:\n        torch.save({""model"": model.module.state_dict()}, args.expdir + ""/checkpoint-final.pkl"")\n    else:\n        torch.save({""model"": model.state_dict()}, args.expdir + ""/checkpoint-final.pkl"")\n    logging.info(""final checkpoint created."")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
wavenet_vocoder/nets/__init__.py,0,b'from .wavenet import *  # NOQA\n'
wavenet_vocoder/nets/wavenet.py,15,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport logging\nimport sys\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom torch import nn\n\n\ndef encode_mu_law(x, mu=256):\n    """"""PERFORM MU-LAW ENCODING.\n\n    Args:\n        x (ndarray): Audio signal with the range from -1 to 1.\n        mu (int): Quantized level.\n\n    Returns:\n        ndarray: Quantized audio signal with the range from 0 to mu - 1.\n\n    """"""\n    mu = mu - 1\n    fx = np.sign(x) * np.log(1 + mu * np.abs(x)) / np.log(1 + mu)\n    return np.floor((fx + 1) / 2 * mu + 0.5).astype(np.int64)\n\n\ndef decode_mu_law(y, mu=256):\n    """"""PERFORM MU-LAW DECODING.\n\n    Args:\n        x (ndarray): Quantized audio signal with the range from 0 to mu - 1.\n        mu (int): Quantized level.\n\n    Returns:\n        ndarray: Audio signal with the range from -1 to 1.\n\n    """"""\n    mu = mu - 1\n    fx = (y - 0.5) / mu * 2 - 1\n    x = np.sign(fx) / mu * ((1 + mu) ** np.abs(fx) - 1)\n    return x\n\n\ndef initialize(m):\n    """"""INITILIZE CONV WITH XAVIER.\n\n    Arg:\n        m (torch.nn.Module): Pytorch nn module instance.\n\n    """"""\n    if isinstance(m, nn.Conv1d):\n        nn.init.xavier_uniform_(m.weight)\n        nn.init.constant_(m.bias, 0.0)\n\n    if isinstance(m, nn.ConvTranspose2d):\n        nn.init.constant_(m.weight, 1.0)\n        nn.init.constant_(m.bias, 0.0)\n\n\nclass OneHot(nn.Module):\n    """"""CONVERT TO ONE-HOT VECTOR.\n\n    Args:\n        depth (int): Dimension of one-hot vector\n\n    """"""\n\n    def __init__(self, depth):\n        super(OneHot, self).__init__()\n        self.depth = depth\n\n    def forward(self, x):\n        """"""FORWARD CALCULATION.\n\n        Arg:\n            x (LongTensor): Long tensor variable with the shape (B, T).\n\n        Returns:\n            Tensor: Float tensor variable with the shape (B, depth, T).\n\n        """"""\n        x = x % self.depth\n        x = torch.unsqueeze(x, 2)\n        x_onehot = x.new_zeros(x.size(0), x.size(1), self.depth).float()\n\n        return x_onehot.scatter_(2, x, 1)\n\n\nclass CausalConv1d(nn.Module):\n    """"""1D DILATED CAUSAL CONVOLUTION.""""""\n\n    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, bias=True):\n        super(CausalConv1d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n        self.padding = padding = (kernel_size - 1) * dilation\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,\n                              padding=padding, dilation=dilation, bias=bias)\n\n    def forward(self, x):\n        """"""FORWARD CALCULATION.\n\n        Args:\n            x (Tensor): Float tensor variable with the shape  (B, C, T).\n\n        Returns:\n            Tensor: Float tensor variable with the shape (B, C, T).\n\n        """"""\n        x = self.conv(x)\n        if self.padding != 0:\n            x = x[:, :, :-self.padding]\n        return x\n\n\nclass UpSampling(nn.Module):\n    """"""UPSAMPLING LAYER WITH DECONVOLUTION.\n\n    Args:\n        upsampling_factor (int): Upsampling factor.\n\n    """"""\n\n    def __init__(self, upsampling_factor, bias=True):\n        super(UpSampling, self).__init__()\n        self.upsampling_factor = upsampling_factor\n        self.bias = bias\n        self.conv = nn.ConvTranspose2d(1, 1,\n                                       kernel_size=(1, self.upsampling_factor),\n                                       stride=(1, self.upsampling_factor),\n                                       bias=self.bias)\n\n    def forward(self, x):\n        """"""FORWARD CALCULATION.\n\n        Args:\n            x (Tensor): Float tensor variable with the shape (B, C, T).\n\n        Returns:\n            Tensor: Float tensor variable with the shape (B, C, T\'),\n                where T\' = T * upsampling_factor.\n\n        """"""\n        x = x.unsqueeze(1)  # B x 1 x C x T\n        x = self.conv(x)  # B x 1 x C x T\'\n        return x.squeeze(1)\n\n\nclass WaveNet(nn.Module):\n    """"""CONDITIONAL WAVENET.\n\n    Args:\n        n_quantize (int): Number of quantization.\n        n_aux (int): Number of aux feature dimension.\n        n_resch (int): Number of filter channels for residual block.\n        n_skipch (int): Number of filter channels for skip connection.\n        dilation_depth (int): Number of dilation depth (e.g. if set 10, max dilation = 2^(10-1)).\n        dilation_repeat (int): Number of dilation repeat.\n        kernel_size (int): Filter size of dilated causal convolution.\n        upsampling_factor (int): Upsampling factor.\n\n    """"""\n\n    def __init__(self, n_quantize=256, n_aux=28, n_resch=512, n_skipch=256,\n                 dilation_depth=10, dilation_repeat=3, kernel_size=2, upsampling_factor=0):\n        super(WaveNet, self).__init__()\n        self.n_aux = n_aux\n        self.n_quantize = n_quantize\n        self.n_resch = n_resch\n        self.n_skipch = n_skipch\n        self.kernel_size = kernel_size\n        self.dilation_depth = dilation_depth\n        self.dilation_repeat = dilation_repeat\n        self.upsampling_factor = upsampling_factor\n\n        self.dilations = [2 ** i for i in range(self.dilation_depth)] * self.dilation_repeat\n        self.receptive_field = (self.kernel_size - 1) * sum(self.dilations) + 1\n\n        # for preprocessing\n        self.onehot = OneHot(self.n_quantize)\n        self.causal = CausalConv1d(self.n_quantize, self.n_resch, self.kernel_size)\n        if self.upsampling_factor > 0:\n            self.upsampling = UpSampling(self.upsampling_factor)\n\n        # for residual blocks\n        self.dil_sigmoid = nn.ModuleList()\n        self.dil_tanh = nn.ModuleList()\n        self.aux_1x1_sigmoid = nn.ModuleList()\n        self.aux_1x1_tanh = nn.ModuleList()\n        self.skip_1x1 = nn.ModuleList()\n        self.res_1x1 = nn.ModuleList()\n        for d in self.dilations:\n            self.dil_sigmoid += [CausalConv1d(self.n_resch, self.n_resch, self.kernel_size, d)]\n            self.dil_tanh += [CausalConv1d(self.n_resch, self.n_resch, self.kernel_size, d)]\n            self.aux_1x1_sigmoid += [nn.Conv1d(self.n_aux, self.n_resch, 1)]\n            self.aux_1x1_tanh += [nn.Conv1d(self.n_aux, self.n_resch, 1)]\n            self.skip_1x1 += [nn.Conv1d(self.n_resch, self.n_skipch, 1)]\n            self.res_1x1 += [nn.Conv1d(self.n_resch, self.n_resch, 1)]\n\n        # for postprocessing\n        self.conv_post_1 = nn.Conv1d(self.n_skipch, self.n_skipch, 1)\n        self.conv_post_2 = nn.Conv1d(self.n_skipch, self.n_quantize, 1)\n\n    def forward(self, x, h):\n        """"""FORWARD CALCULATION.\n\n        Args:\n            x (Tensor): Long tensor variable with the shape (B, T).\n            h (Tensor): Float tensor variable with the shape (B, n_aux, T),\n\n        Returns:\n            Tensor: Float tensor variable with the shape (B, T, n_quantize).\n\n        """"""\n        # preprocess\n        output = self._preprocess(x)\n        if self.upsampling_factor > 0:\n            h = self.upsampling(h)\n\n        # residual block\n        skip_connections = []\n        for l in range(len(self.dilations)):\n            output, skip = self._residual_forward(\n                output, h, self.dil_sigmoid[l], self.dil_tanh[l],\n                self.aux_1x1_sigmoid[l], self.aux_1x1_tanh[l],\n                self.skip_1x1[l], self.res_1x1[l])\n            skip_connections.append(skip)\n\n        # skip-connection part\n        output = sum(skip_connections)\n        output = self._postprocess(output)\n\n        return output\n\n    def generate(self, x, h, n_samples, intervals=None, mode=""sampling""):\n        """"""GENERATE WAVEFORM WITH NAIVE CALCULATION.\n\n        Args:\n            x (Tensor): Long tensor variable with the shape (1, T).\n            h (Tensor): Float tensor variable with the shape (1, n_aux, n_samples + T).\n            n_samples (int): Number of samples to be generated.\n            intervals (int): Log interval.\n            mode (str): ""sampling"" or ""argmax"".\n\n        Returns:\n            ndarray: Generated quantized wavenform (n_samples,).\n\n        """"""\n        # upsampling\n        if self.upsampling_factor > 0:\n            h = self.upsampling(h)\n\n        # padding if the length less than receptive field size\n        n_pad = self.receptive_field - x.size(1)\n        if n_pad > 0:\n            x = F.pad(x, (n_pad, 0), ""constant"", self.n_quantize // 2)\n            h = F.pad(h, (n_pad, 0), ""replicate"")\n\n        # generate\n        samples = x[0].tolist()\n        start = time.time()\n        for i in range(n_samples):\n            current_idx = len(samples)\n            x = torch.tensor(samples[-self.receptive_field:]).long().view(1, -1)\n            h_ = h[:, :, current_idx - self.receptive_field: current_idx]\n\n            # calculate output\n            output = self._preprocess(x)\n            skip_connections = []\n            for l in range(len(self.dilations)):\n                output, skip = self._residual_forward(\n                    output, h_, self.dil_sigmoid[l], self.dil_tanh[l],\n                    self.aux_1x1_sigmoid[l], self.aux_1x1_tanh[l],\n                    self.skip_1x1[l], self.res_1x1[l])\n                skip_connections.append(skip)\n            output = sum(skip_connections)\n            output = self._postprocess(output)[0]  # T x n_quantize\n\n            # get waveform\n            if mode == ""sampling"":\n                posterior = F.softmax(output[-1], dim=0)\n                dist = torch.distributions.Categorical(posterior)\n                sample = dist.sample()\n            elif mode == ""argmax"":\n                sample = output[-1].argmax()\n            else:\n                logging.error(""mode should be sampling or argmax"")\n                sys.exit(1)\n            samples.append(sample)\n\n            # show progress\n            if intervals is not None and (i + 1) % intervals == 0:\n                logging.info(""%d/%d estimated time = %.3f sec (%.3f sec / sample)"" % (\n                    i + 1, n_samples,\n                    (n_samples - i - 1) * ((time.time() - start) / intervals),\n                    (time.time() - start) / intervals))\n                start = time.time()\n\n        return np.array(samples[-n_samples:])\n\n    def fast_generate(self, x, h, n_samples, intervals=None, mode=""sampling""):\n        """"""GENERATE WAVEFORM WITH FAST ALGORITHM.\n\n        Args:\n            x (tensor): Long tensor variable with the shape  (1, T).\n            h (tensor): Float tensor variable with the shape  (1, n_aux, n_samples + T).\n            n_samples (int): Number of samples to be generated.\n            intervals (int): Log interval.\n            mode (str): ""sampling"" or ""argmax"".\n\n        Returns:\n            ndarray: Generated quantized wavenform (n_samples,).\n\n        References:\n            Fast Wavenet Generation Algorithm: https://arxiv.org/abs/1611.09482\n\n        """"""\n        # upsampling\n        if self.upsampling_factor > 0:\n            h = self.upsampling(h)\n\n        # padding if the length less than\n        n_pad = self.receptive_field - x.size(1)\n        if n_pad > 0:\n            x = F.pad(x, (n_pad, 0), ""constant"", self.n_quantize // 2)\n            h = F.pad(h, (n_pad, 0), ""replicate"")\n\n        # prepare buffer\n        output = self._preprocess(x)\n        h_ = h[:, :, :x.size(1)]\n        output_buffer = []\n        buffer_size = []\n        for l, d in enumerate(self.dilations):\n            output, _ = self._residual_forward(\n                output, h_, self.dil_sigmoid[l], self.dil_tanh[l],\n                self.aux_1x1_sigmoid[l], self.aux_1x1_tanh[l],\n                self.skip_1x1[l], self.res_1x1[l])\n            if d == 2 ** (self.dilation_depth - 1):\n                buffer_size.append(self.kernel_size - 1)\n            else:\n                buffer_size.append(d * 2 * (self.kernel_size - 1))\n            output_buffer.append(output[:, :, -buffer_size[l] - 1: -1])\n\n        # generate\n        samples = x[0]\n        start = time.time()\n        for i in range(n_samples):\n            output = samples[-self.kernel_size * 2 + 1:].unsqueeze(0)\n            output = self._preprocess(output)\n            h_ = h[:, :, samples.size(0) - 1].contiguous().view(1, self.n_aux, 1)\n            output_buffer_next = []\n            skip_connections = []\n            for l, d in enumerate(self.dilations):\n                output, skip = self._generate_residual_forward(\n                    output, h_, self.dil_sigmoid[l], self.dil_tanh[l],\n                    self.aux_1x1_sigmoid[l], self.aux_1x1_tanh[l],\n                    self.skip_1x1[l], self.res_1x1[l])\n                output = torch.cat([output_buffer[l], output], dim=2)\n                output_buffer_next.append(output[:, :, -buffer_size[l]:])\n                skip_connections.append(skip)\n\n            # update buffer\n            output_buffer = output_buffer_next\n\n            # get predicted sample\n            output = sum(skip_connections)\n            output = self._postprocess(output)[0]\n            if mode == ""sampling"":\n                posterior = F.softmax(output[-1], dim=0)\n                dist = torch.distributions.Categorical(posterior)\n                sample = dist.sample().unsqueeze(0)\n            elif mode == ""argmax"":\n                sample = output.argmax(-1)\n            else:\n                logging.error(""mode should be sampling or argmax"")\n                sys.exit(1)\n            samples = torch.cat([samples, sample], dim=0)\n\n            # show progress\n            if intervals is not None and (i + 1) % intervals == 0:\n                logging.info(""%d/%d estimated time = %.3f sec (%.3f sec / sample)"" % (\n                    i + 1, n_samples,\n                    (n_samples - i - 1) * ((time.time() - start) / intervals),\n                    (time.time() - start) / intervals))\n                start = time.time()\n\n        return samples[-n_samples:].cpu().numpy()\n\n    def batch_fast_generate(self, x, h, n_samples_list, intervals=None, mode=""sampling""):\n        """"""GENERATE WAVEFORM WITH FAST ALGORITHM IN BATCH MODE.\n\n        Args:\n            x (tensor): Long tensor variable with the shape (B, T).\n            h (tensor): Float tensor variable with the shape (B, n_aux, max(n_samples_list) + T).\n            n_samples_list (list): List of number of samples to be generated (B,).\n            intervals (int): Log interval.\n            mode (str): ""sampling"" or ""argmax"".\n\n        Returns:\n            list: List of ndarray which is generated quantized wavenform.\n\n        """"""\n        # get min max length\n        max_n_samples = max(n_samples_list)\n        min_n_samples = min(n_samples_list)\n        min_idx = np.argmin(n_samples_list)\n\n        # upsampling\n        if self.upsampling_factor > 0:\n            h = self.upsampling(h)\n\n        # padding if the length less than\n        n_pad = self.receptive_field - x.size(1)\n        if n_pad > 0:\n            x = F.pad(x, (n_pad, 0), ""constant"", self.n_quantize // 2)\n            h = F.pad(h, (n_pad, 0), ""replicate"")\n\n        # prepare buffer\n        output = self._preprocess(x)\n        h_ = h[:, :, :x.size(1)]\n        output_buffer = []\n        buffer_size = []\n        for l, d in enumerate(self.dilations):\n            output, _ = self._residual_forward(\n                output, h_, self.dil_sigmoid[l], self.dil_tanh[l],\n                self.aux_1x1_sigmoid[l], self.aux_1x1_tanh[l],\n                self.skip_1x1[l], self.res_1x1[l])\n            if d == 2 ** (self.dilation_depth - 1):\n                buffer_size.append(self.kernel_size - 1)\n            else:\n                buffer_size.append(d * 2 * (self.kernel_size - 1))\n            output_buffer.append(output[:, :, -buffer_size[l] - 1: -1])\n\n        # generate\n        samples = x  # B x T\n        end_samples = []\n        start = time.time()\n        for i in range(max_n_samples):\n            output = samples[:, -self.kernel_size * 2 + 1:]\n            output = self._preprocess(output)  # B x C x T\n            h_ = h[:, :, samples.size(-1) - 1].contiguous().unsqueeze(-1)  # B x C x 1\n            output_buffer_next = []\n            skip_connections = []\n            for l, d in enumerate(self.dilations):\n                output, skip = self._generate_residual_forward(\n                    output, h_, self.dil_sigmoid[l], self.dil_tanh[l],\n                    self.aux_1x1_sigmoid[l], self.aux_1x1_tanh[l],\n                    self.skip_1x1[l], self.res_1x1[l])\n                output = torch.cat([output_buffer[l], output], dim=2)\n                output_buffer_next.append(output[:, :, -buffer_size[l]:])\n                skip_connections.append(skip)\n\n            # update buffer\n            output_buffer = output_buffer_next\n\n            # get predicted sample\n            output = sum(skip_connections)\n            output = self._postprocess(output)[:, -1]  # B x n_quantize\n            if mode == ""sampling"":\n                posterior = F.softmax(output, dim=-1)\n                dist = torch.distributions.Categorical(posterior)\n                sample = dist.sample()  # B\n            elif mode == ""argmax"":\n                sample = output.argmax(-1)  # B\n            else:\n                logging.error(""mode should be sampling or argmax"")\n                sys.exit(1)\n            samples = torch.cat([samples, sample.view(-1, 1)], dim=1)\n\n            # show progress\n            if intervals is not None and (i + 1) % intervals == 0:\n                logging.info(""%d/%d estimated time = %.3f sec (%.3f sec / sample)"" % (\n                    i + 1, max_n_samples,\n                    (max_n_samples - i - 1) * ((time.time() - start) / intervals),\n                    (time.time() - start) / intervals))\n                start = time.time()\n\n            # check length\n            if (i + 1) == min_n_samples:\n                while True:\n                    # get finished sample\n                    end_samples += [samples[min_idx, -min_n_samples:].cpu().numpy()]\n                    # get index of unfinished samples\n                    idx_list = [idx for idx in range(len(n_samples_list)) if idx != min_idx]\n                    if len(idx_list) == 0:\n                        # break when all of samples are finished\n                        break\n                    else:\n                        # remove finished sample\n                        samples = samples[idx_list]\n                        h = h[idx_list]\n                        output_buffer = [out_[idx_list] for out_ in output_buffer]\n                        del n_samples_list[min_idx]\n                        # update min length\n                        prev_min_n_samples = min_n_samples\n                        min_n_samples = min(n_samples_list)\n                        min_idx = np.argmin(n_samples_list)\n\n                    # break when there is no same length samples\n                    if min_n_samples != prev_min_n_samples:\n                        break\n\n        return end_samples\n\n    def _preprocess(self, x):\n        x = self.onehot(x).transpose(1, 2)\n        output = self.causal(x)\n        return output\n\n    def _postprocess(self, x):\n        output = F.relu(x)\n        output = self.conv_post_1(output)\n        output = F.relu(output)  # B x C x T\n        output = self.conv_post_2(output).transpose(1, 2)  # B x T x C\n        return output\n\n    def _residual_forward(self, x, h, dil_sigmoid, dil_tanh,\n                          aux_1x1_sigmoid, aux_1x1_tanh, skip_1x1, res_1x1):\n        output_sigmoid = dil_sigmoid(x)\n        output_tanh = dil_tanh(x)\n        aux_output_sigmoid = aux_1x1_sigmoid(h)\n        aux_output_tanh = aux_1x1_tanh(h)\n        output = torch.sigmoid(output_sigmoid + aux_output_sigmoid) * \\\n            torch.tanh(output_tanh + aux_output_tanh)\n        skip = skip_1x1(output)\n        output = res_1x1(output)\n        output = output + x\n        return output, skip\n\n    def _generate_residual_forward(self, x, h, dil_sigmoid, dil_tanh,\n                                   aux_1x1_sigmoid, aux_1x1_tanh, skip_1x1, res_1x1):\n        output_sigmoid = dil_sigmoid(x)[:, :, -1:]\n        output_tanh = dil_tanh(x)[:, :, -1:]\n        aux_output_sigmoid = aux_1x1_sigmoid(h)\n        aux_output_tanh = aux_1x1_tanh(h)\n        output = torch.sigmoid(output_sigmoid + aux_output_sigmoid) * \\\n            torch.tanh(output_tanh + aux_output_tanh)\n        skip = skip_1x1(output)\n        output = res_1x1(output)\n        output = output + x[:, :, -1:]  # B x C x 1\n        return output, skip\n'"
wavenet_vocoder/utils/__init__.py,0,b'from .utils import *  # NOQA\n'
wavenet_vocoder/utils/utils.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport fnmatch\nimport logging\nimport os\nimport sys\nimport threading\n\nimport h5py\nimport numpy as np\n\nfrom numpy.matlib import repmat\n\n\ndef check_hdf5(hdf5_name, hdf5_path):\n    """"""CHECK HDF5 EXISTENCE.\n\n    Args:\n        hdf5_name (str): Filename of hdf5 file.\n        hdf5_path (str): Dataset name in hdf5 file.\n\n    Returns:\n        bool: Dataset exists then return True.\n\n    """"""\n    if not os.path.exists(hdf5_name):\n        return False\n    else:\n        with h5py.File(hdf5_name, ""r"") as f:\n            if hdf5_path in f:\n                return True\n            else:\n                return False\n\n\ndef read_hdf5(hdf5_name, hdf5_path):\n    """"""READ HDF5 DATASET.\n\n    Args:\n        hdf5_name (str): Filename of hdf5 file.\n        hdf5_path (str): Dataset name in hdf5 file.\n\n    Return:\n        any: Dataset values.\n\n    """"""\n    if not os.path.exists(hdf5_name):\n        logging.error(""there is no such a hdf5 file (%s)."" % hdf5_name)\n        sys.exit(1)\n\n    hdf5_file = h5py.File(hdf5_name, ""r"")\n\n    if hdf5_path not in hdf5_file:\n        logging.error(""there is no such a data in hdf5 file. (%s)"" % hdf5_path)\n        sys.exit(1)\n\n    hdf5_data = hdf5_file[hdf5_path][()]\n    hdf5_file.close()\n\n    return hdf5_data\n\n\ndef shape_hdf5(hdf5_name, hdf5_path):\n    """"""GET HDF5 DATASET SHAPE.\n\n    Args:\n        hdf5_name (str): Filename of hdf5 file.\n        hdf5_path (str): Dataset name in hdf5 file.\n\n    Returns:\n        (tuple): Shape of dataset.\n\n    """"""\n    if check_hdf5(hdf5_name, hdf5_path):\n        with h5py.File(hdf5_name, ""r"") as f:\n            hdf5_shape = f[hdf5_path].shape\n        return hdf5_shape\n    else:\n        logging.error(""there is no such a file or dataset"")\n        sys.exit(1)\n\n\ndef write_hdf5(hdf5_name, hdf5_path, write_data, is_overwrite=True):\n    """"""WRITE DATASET TO HDF5.\n\n    Args:\n        hdf5_name (str): Hdf5 dataset filename.\n        hdf5_path (str): Dataset path in hdf5.\n        write_data (ndarray): Data to write.\n        is_overwrite (bool): Whether to overwrite dataset.\n\n    """"""\n    # convert to numpy array\n    write_data = np.array(write_data)\n\n    # check folder existence\n    folder_name, _ = os.path.split(hdf5_name)\n    if not os.path.exists(folder_name) and len(folder_name) != 0:\n        os.makedirs(folder_name)\n\n    # check hdf5 existence\n    if os.path.exists(hdf5_name):\n        # if already exists, open with r+ mode\n        hdf5_file = h5py.File(hdf5_name, ""r+"")\n        # check dataset existence\n        if hdf5_path in hdf5_file:\n            if is_overwrite:\n                logging.warning(""dataset in hdf5 file already exists."")\n                logging.warning(""recreate dataset in hdf5."")\n                hdf5_file.__delitem__(hdf5_path)\n            else:\n                logging.error(""dataset in hdf5 file already exists."")\n                logging.error(""if you want to overwrite, please set is_overwrite = True."")\n                hdf5_file.close()\n                sys.exit(1)\n    else:\n        # if not exists, open with w mode\n        hdf5_file = h5py.File(hdf5_name, ""w"")\n\n    # write data to hdf5\n    hdf5_file.create_dataset(hdf5_path, data=write_data)\n    hdf5_file.flush()\n    hdf5_file.close()\n\n\ndef find_files(directory, pattern=""*.wav"", use_dir_name=True):\n    """"""FIND FILES RECURSIVELY.\n\n    Args:\n        directory (str): Root directory to find.\n        pattern (str): Query to find.\n        use_dir_name (bool): If False, directory name is not included.\n\n    Returns:\n        list: List of found filenames.\n\n    """"""\n    files = []\n    for root, dirnames, filenames in os.walk(directory, followlinks=True):\n        for filename in fnmatch.filter(filenames, pattern):\n            files.append(os.path.join(root, filename))\n    if not use_dir_name:\n        files = [file_.replace(directory + ""/"", """") for file_ in files]\n    return files\n\n\ndef read_txt(file_list):\n    """"""READ TXT FILE.\n\n    Args:\n        file_list (str): TXT file filename.\n\n    Returns:\n        list: List of read lines.\n\n    """"""\n    with open(file_list, ""r"") as f:\n        filenames = f.readlines()\n    return [filename.replace(""\\n"", """") for filename in filenames]\n\n\nclass BackgroundGenerator(threading.Thread):\n    """"""BACKGROUND GENERATOR.\n\n    Args:\n        generator (object): Generator instance.\n        max_prefetch (int): Max number of prefetch.\n\n    References:\n        https://stackoverflow.com/questions/7323664/python-generator-pre-fetch\n\n    """"""\n\n    def __init__(self, generator, max_prefetch=1):\n        threading.Thread.__init__(self)\n        if sys.version_info.major == 2:\n            from Queue import Queue\n        else:\n            from queue import Queue\n        self.queue = Queue(max_prefetch)\n        self.generator = generator\n        self.daemon = True\n        self.start()\n\n    def run(self):\n        """"""STORE ITEMS IN QUEUE.""""""\n        for item in self.generator:\n            self.queue.put(item)\n        self.queue.put(None)\n\n    def next(self):\n        """"""GET ITEM IN THE QUEUE.""""""\n        next_item = self.queue.get()\n        if next_item is None:\n            raise StopIteration\n        return next_item\n\n    def __next__(self):\n        return self.next()\n\n    def __iter__(self):\n        return self\n\n\nclass background(object):\n    """"""BACKGROUND GENERATOR DECORATOR.""""""\n\n    def __init__(self, max_prefetch=1):\n        self.max_prefetch = max_prefetch\n\n    def __call__(self, gen):\n        def bg_generator(*args, **kwargs):\n            return BackgroundGenerator(gen(*args, **kwargs))\n        return bg_generator\n\n\ndef extend_time(feats, upsampling_factor):\n    """"""EXTEND TIME RESOLUTION.\n\n    Args:\n        feats (ndarray): Feature vector with the shape (T, D).\n        upsampling_factor (int): Upsampling_factor.\n\n    Returns:\n        (ndarray): Extended feats with the shape (upsampling_factor * T, D).\n\n    """"""\n    # get number\n    n_frames = feats.shape[0]\n    n_dims = feats.shape[1]\n\n    # extend time\n    feats_extended = np.zeros((n_frames * upsampling_factor, n_dims))\n    for j in range(n_frames):\n        start_idx = j * upsampling_factor\n        end_idx = (j + 1) * upsampling_factor\n        feats_extended[start_idx: end_idx] = repmat(feats[j, :], upsampling_factor, 1)\n\n    return feats_extended\n'"
