file_path,api_count,code
classification.py,11,"b'#You can write your own classification file to use the module\nfrom attention.model import StructuredSelfAttention\nfrom attention.train import train,get_activation_wts,evaluate\nfrom utils.pretrained_glove_embeddings import load_glove_embeddings\nfrom utils.data_loader import load_data_set\nfrom visualization.attention_visualization import createHTML\nimport torch\nimport numpy as np\nfrom torch.autograd import Variable\nfrom keras.preprocessing.sequence import pad_sequences\nimport torch.nn.functional as F\nimport torch.utils.data as data_utils\nimport os,sys\nimport json\n \nclassified = False\nclassification_type = sys.argv[1]\n \ndef json_to_dict(json_set):\n    for k,v in json_set.items():\n        if v == \'False\':\n            json_set[k] = False\n        elif v == \'True\':\n            json_set[k] = True\n        else:\n            json_set[k] = v\n    return json_set\n \n \nwith open(\'config.json\', \'r\') as f:\n    params_set = json.load(f)\n \nwith open(\'model_params.json\', \'r\') as f:\n    model_params = json.load(f)\n \nparams_set = json_to_dict(params_set)\nmodel_params = json_to_dict(model_params)\n \nprint(""Using settings:"",params_set)\nprint(""Using model settings"",model_params)\n \ndef visualize_attention(wts,x_test_pad,word_to_id,filename):\n    wts_add = torch.sum(wts,1)\n    wts_add_np = wts_add.data.numpy()\n    wts_add_list = wts_add_np.tolist()\n    id_to_word = {v:k for k,v in word_to_id.items()}\n    text= []\n    for test in x_test_pad:\n        text.append("" "".join([id_to_word.get(i) for i in test]))\n    createHTML(text, wts_add_list, filename)\n    print(""Attention visualization created for {} samples"".format(len(x_test_pad)))\n    return\n \ndef binary_classfication(attention_model,train_loader,epochs=5,use_regularization=True,C=1.0,clip=True):\n    loss = torch.nn.BCELoss()\n    optimizer = torch.optim.RMSprop(attention_model.parameters())\n    train(attention_model,train_loader,loss,optimizer,epochs,use_regularization,C,clip)\n \ndef multiclass_classification(attention_model,train_loader,epochs=5,use_regularization=True,C=1.0,clip=True):\n    loss = torch.nn.NLLLoss()\n    optimizer = torch.optim.RMSprop(attention_model.parameters())\n    train(attention_model,train_loader,loss,optimizer,epochs,use_regularization,C,clip)\n \n \n \nMAXLENGTH = model_params[\'timesteps\']\nif classification_type ==\'binary\':\n \n    train_loader,x_test_pad,y_test,word_to_id = load_data_set(0,MAXLENGTH,model_params[""vocab_size""],model_params[\'batch_size\']) #loading imdb dataset\n \n \n    if params_set[""use_embeddings""]:\n        embeddings = load_glove_embeddings(""glove/glove.6B.50d.txt"",word_to_id,50)\n    else:\n        embeddings = None\n    #Can use pretrained embeddings by passing in the embeddings and setting the use_pretrained_embeddings=True\n    attention_model = StructuredSelfAttention(batch_size=train_loader.batch_size,lstm_hid_dim=model_params[\'lstm_hidden_dimension\'],d_a = model_params[""d_a""],r=params_set[""attention_hops""],vocab_size=len(word_to_id),max_len=MAXLENGTH,type=0,n_classes=1,use_pretrained_embeddings=params_set[""use_embeddings""],embeddings=embeddings)\n \n    #Can set use_regularization=True for penalization and clip=True for gradient clipping\n    binary_classfication(attention_model,train_loader=train_loader,epochs=params_set[""epochs""],use_regularization=params_set[""use_regularization""],C=params_set[""C""],clip=params_set[""clip""])\n    classified = True\n    #wts = get_activation_wts(binary_attention_model,Variable(torch.from_numpy(x_test_pad[:]).type(torch.LongTensor)))\n    #print(""Attention weights for the testing data in binary classification are:"",wts)\n \n \nif classification_type == \'multiclass\':\n    train_loader,train_set,test_set,x_test_pad,word_to_id = load_data_set(1,MAXLENGTH,model_params[""vocab_size""],model_params[\'batch_size\']) #load the reuters dataset\n    #Using pretrained embeddings\n    if params_set[""use_embeddings""]:\n        embeddings = load_glove_embeddings(""glove/glove.6B.50d.txt"",word_to_id,50)\n    else:\n        embeddings = None\n    attention_model = StructuredSelfAttention(batch_size=train_loader.batch_size,lstm_hid_dim=model_params[\'lstm_hidden_dimension\'],d_a = model_params[""d_a""],r=params_set[""attention_hops""],vocab_size=len(word_to_id),max_len=MAXLENGTH,type=1,n_classes=46,use_pretrained_embeddings=params_set[""use_embeddings""],embeddings=embeddings)\n \n    #Using regularization and gradient clipping at 0.5 (currently unparameterized)\n    multiclass_classification(attention_model,train_loader,epochs=params_set[""epochs""],use_regularization=params_set[""use_regularization""],C=params_set[""C""],clip=params_set[""clip""])\n    classified=True\n    #wts = get_activation_wts(multiclass_attention_model,Variable(torch.from_numpy(x_test_pad[:]).type(torch.LongTensor)))\n    #print(""Attention weights for the data in multiclass classification are:"",wts)\nif classified:\n    test_last_idx = 100\n    wts = get_activation_wts(attention_model,Variable(torch.from_numpy(x_test_pad[:test_last_idx]).type(torch.LongTensor)))\n    print(wts.size())\n    visualize_attention(wts,x_test_pad[:test_last_idx],word_to_id,filename=\'attention.html\')'"
attention/model.py,15,"b'import torch,keras\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.utils.data as data_utils\n \nclass StructuredSelfAttention(torch.nn.Module):\n    """"""\n    The class is an implementation of the paper A Structured Self-Attentive Sentence Embedding including regularization\n    and without pruning. Slight modifications have been done for speedup\n    """"""\n   \n    def __init__(self,batch_size,lstm_hid_dim,d_a,r,max_len,emb_dim=100,vocab_size=None,use_pretrained_embeddings = False,embeddings=None,type=0,n_classes = 1):\n        """"""\n        Initializes parameters suggested in paper\n \n        Args:\n            batch_size  : {int} batch_size used for training\n            lstm_hid_dim: {int} hidden dimension for lstm\n            d_a         : {int} hidden dimension for the dense layer\n            r           : {int} attention-hops or attention heads\n            max_len     : {int} number of lstm timesteps\n            emb_dim     : {int} embeddings dimension\n            vocab_size  : {int} size of the vocabulary\n            use_pretrained_embeddings: {bool} use or train your own embeddings\n            embeddings  : {torch.FloatTensor} loaded pretrained embeddings\n            type        : [0,1] 0-->binary_classification 1-->multiclass classification\n            n_classes   : {int} number of classes\n \n        Returns:\n            self\n \n        Raises:\n            Exception\n        """"""\n        super(StructuredSelfAttention,self).__init__()\n       \n        self.embeddings,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n        self.lstm = torch.nn.LSTM(emb_dim,lstm_hid_dim,1,batch_first=True)\n        self.linear_first = torch.nn.Linear(lstm_hid_dim,d_a)\n        self.linear_first.bias.data.fill_(0)\n        self.linear_second = torch.nn.Linear(d_a,r)\n        self.linear_second.bias.data.fill_(0)\n        self.n_classes = n_classes\n        self.linear_final = torch.nn.Linear(lstm_hid_dim,self.n_classes)\n        self.batch_size = batch_size       \n        self.max_len = max_len\n        self.lstm_hid_dim = lstm_hid_dim\n        self.hidden_state = self.init_hidden()\n        self.r = r\n        self.type = type\n                 \n    def _load_embeddings(self,use_pretrained_embeddings,embeddings,vocab_size,emb_dim):\n        """"""Load the embeddings based on flag""""""\n       \n        if use_pretrained_embeddings is True and embeddings is None:\n            raise Exception(""Send a pretrained word embedding as an argument"")\n           \n        if not use_pretrained_embeddings and vocab_size is None:\n            raise Exception(""Vocab size cannot be empty"")\n   \n        if not use_pretrained_embeddings:\n            word_embeddings = torch.nn.Embedding(vocab_size,emb_dim,padding_idx=0)\n            \n        elif use_pretrained_embeddings:\n            word_embeddings = torch.nn.Embedding(embeddings.size(0), embeddings.size(1))\n            word_embeddings.weight = torch.nn.Parameter(embeddings)\n            emb_dim = embeddings.size(1)\n            \n        return word_embeddings,emb_dim\n       \n        \n    def softmax(self,input, axis=1):\n        """"""\n        Softmax applied to axis=n\n \n        Args:\n           input: {Tensor,Variable} input on which softmax is to be applied\n           axis : {int} axis on which softmax is to be applied\n \n        Returns:\n            softmaxed tensors\n \n       \n        """"""\n \n        input_size = input.size()\n        trans_input = input.transpose(axis, len(input_size)-1)\n        trans_size = trans_input.size()\n        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n        soft_max_2d = F.softmax(input_2d)\n        soft_max_nd = soft_max_2d.view(*trans_size)\n        return soft_max_nd.transpose(axis, len(input_size)-1)\n       \n        \n    def init_hidden(self):\n        return (Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)),Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)))\n       \n        \n    def forward(self,x):\n        embeddings = self.embeddings(x)       \n        outputs, self.hidden_state = self.lstm(embeddings.view(self.batch_size,self.max_len,-1),self.hidden_state)       \n        x = F.tanh(self.linear_first(outputs))       \n        x = self.linear_second(x)       \n        x = self.softmax(x,1)       \n        attention = x.transpose(1,2)       \n        sentence_embeddings = attention@outputs       \n        avg_sentence_embeddings = torch.sum(sentence_embeddings,1)/self.r\n       \n        if not bool(self.type):\n            output = F.sigmoid(self.linear_final(avg_sentence_embeddings))\n           \n            return output,attention\n        else:\n            return F.log_softmax(self.linear_final(avg_sentence_embeddings)),attention\n       \n\t   \n\t#Regularization\n    def l2_matrix_norm(self,m):\n        """"""\n        Frobenius norm calculation\n \n        Args:\n           m: {Variable} ||AAT - I||\n \n        Returns:\n            regularized value\n \n       \n        """"""\n        return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)'"
attention/train.py,15,"b'import torch\nfrom torch.autograd import Variable\n \ndef train(attention_model,train_loader,criterion,optimizer,epochs = 5,use_regularization = False,C=0,clip=False):\n    """"""\n        Training code\n \n        Args:\n            attention_model : {object} model\n            train_loader    : {DataLoader} training data loaded into a dataloader\n            optimizer       :  optimizer\n            criterion       :  loss function. Must be BCELoss for binary_classification and NLLLoss for multiclass\n            epochs          : {int} number of epochs\n            use_regularizer : {bool} use penalization or not\n            C               : {int} penalization coeff\n            clip            : {bool} use gradient clipping or not\n       \n        Returns:\n            accuracy and losses of the model\n \n      \n        """"""\n    losses = []\n    accuracy = []\n    for i in range(epochs):\n        print(""Running EPOCH"",i+1)\n        total_loss = 0\n        n_batches = 0\n        correct = 0\n       \n        for batch_idx,train in enumerate(train_loader):\n \n            attention_model.hidden_state = attention_model.init_hidden()\n            x,y = Variable(train[0]),Variable(train[1])\n            y_pred,att = attention_model(x)\n           \n            #penalization AAT - I\n            if use_regularization:\n                attT = att.transpose(1,2)\n                identity = torch.eye(att.size(1))\n                identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,att.size(1),att.size(1)))\n                penal = attention_model.l2_matrix_norm(att@attT - identity)\n           \n            \n            if not bool(attention_model.type) :\n                #binary classification\n                #Adding a very small value to prevent BCELoss from outputting NaN\'s\n                correct+=torch.eq(torch.round(y_pred.type(torch.DoubleTensor).squeeze(1)),y).data.sum()\n                if use_regularization:\n                    try:\n                        loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1)+1e-8,y) + C * penal/train_loader.batch_size\n                       \n                    except RuntimeError:\n                        raise Exception(""BCELoss gets nan values on regularization. Either remove regularization or add very small values"")\n                else:\n                    loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1),y)\n                \n            \n            else:\n                \n                correct+=torch.eq(torch.max(y_pred,1)[1],y.type(torch.LongTensor)).data.sum()\n                if use_regularization:\n                    loss = criterion(y_pred,y) + (C * penal/train_loader.batch_size).type(torch.FloatTensor)\n                else:\n                    loss = criterion(y_pred,y)\n               \n \n            total_loss+=loss.data\n            optimizer.zero_grad()\n            loss.backward()\n           \n            #gradient clipping\n            if clip:\n                torch.nn.utils.clip_grad_norm(attention_model.parameters(),0.5)\n            optimizer.step()\n            n_batches+=1\n           \n        print(""avg_loss is"",total_loss/n_batches)\n        print(""Accuracy of the model"",correct/(n_batches*train_loader.batch_size))\n        losses.append(total_loss/n_batches)\n        accuracy.append(correct/(n_batches*train_loader.batch_size))\n    return losses,accuracy\n \n \ndef evaluate(attention_model,x_test,y_test):\n    """"""\n        cv results\n \n        Args:\n            attention_model : {object} model\n            x_test          : {nplist} x_test\n            y_test          : {nplist} y_test\n       \n        Returns:\n            cv-accuracy\n \n      \n    """"""\n   \n    attention_model.batch_size = x_test.shape[0]\n    attention_model.hidden_state = attention_model.init_hidden()\n    x_test_var = Variable(torch.from_numpy(x_test).type(torch.LongTensor))\n    y_test_pred,_ = attention_model(x_test_var)\n    if bool(attention_model.type):\n        y_preds = torch.max(y_test_pred,1)[1]\n        y_test_var = Variable(torch.from_numpy(y_test).type(torch.LongTensor))\n       \n    else:\n        y_preds = torch.round(y_test_pred.type(torch.DoubleTensor).squeeze(1))\n        y_test_var = Variable(torch.from_numpy(y_test).type(torch.DoubleTensor))\n       \n    return torch.eq(y_preds,y_test_var).data.sum()/x_test_var.size(0)\n \ndef get_activation_wts(attention_model,x):\n    """"""\n        Get r attention heads\n \n        Args:\n            attention_model : {object} model\n            x               : {torch.Variable} input whose weights we want\n       \n        Returns:\n            r different attention weights\n \n      \n    """"""\n    attention_model.batch_size = x.size(0)\n    attention_model.hidden_state = attention_model.init_hidden()\n    _,wts = attention_model(x)\n    return wts'"
utils/data_loader.py,4,"b'#Please create your own dataloader for new datasets of the following type\n\nimport torch\nimport numpy as np\nfrom keras.datasets import imdb\nfrom keras.preprocessing.sequence import pad_sequences\nimport torch.utils.data as data_utils\n \ndef load_data_set(type,max_len,vocab_size,batch_size):\n    """"""\n        Loads the dataset. Keras Imdb dataset for binary classifcation. Keras reuters dataset for multiclass classification\n \n        Args:\n            type   : {bool} 0 for binary classification returns imdb dataset. 1 for multiclass classfication return reuters set\n            max_len: {int} timesteps used for padding\n\t\t\tvocab_size: {int} size of the vocabulary\n\t\t\tbatch_size: batch_size\n        Returns:\n            train_loader: {torch.Dataloader} train dataloader\n            x_test_pad  : padded tokenized test_data for cross validating\n\t\t\ty_test      : y_test\n            word_to_id  : {dict} words mapped to indices\n \n      \n        """"""\n   \n    INDEX_FROM=3\n    if not bool(type):\n        NUM_WORDS=vocab_size # only use top 1000 words\n           # word index offset\n \n        train_set,test_set = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n        x_train,y_train = train_set[0],train_set[1]\n        x_test,y_test = test_set[0],test_set[1]\n        word_to_id = imdb.get_word_index()\n        word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n        word_to_id[""<PAD>""] = 0\n        word_to_id[""<START>""] = 1\n        word_to_id[""<UNK>""] = 2\n \n        id_to_word = {value:key for key,value in word_to_id.items()}\n        x = np.concatenate([x_train, x_test])\n        y = np.concatenate([y_train, y_test])\n        n_train = x.shape[0] - 1000\n        n_valid = 1000\n \n        x_train = x[:n_train]\n        y_train = y[:n_train]\n        x_test = x[n_train:n_train+n_valid]\n        y_test = y[n_train:n_train+n_valid]\n \n \n        #embeddings = load_glove_embeddings(""../../GloVe/glove.6B.50d.txt"",word_to_id,50)\n        x_train_pad = pad_sequences(x_train,maxlen=max_len)\n        x_test_pad = pad_sequences(x_test,maxlen=max_len)\n \n \n        train_data = data_utils.TensorDataset(torch.from_numpy(x_train_pad).type(torch.LongTensor),torch.from_numpy(y_train).type(torch.DoubleTensor))\n        train_loader = data_utils.DataLoader(train_data,batch_size=batch_size,drop_last=True)\n        return train_loader,x_test_pad,y_test,word_to_id\n       \n    else:\n        from keras.datasets import reuters\n \n        train_set,test_set = reuters.load_data(path=""reuters.npz"",num_words=vocab_size,skip_top=0,index_from=INDEX_FROM)\n        x_train,y_train = train_set[0],train_set[1]\n        x_test,y_test = test_set[0],test_set[1]\n        word_to_id = reuters.get_word_index(path=""reuters_word_index.json"")\n        word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n        word_to_id[""<PAD>""] = 0\n        word_to_id[""<START>""] = 1\n        word_to_id[""<UNK>""] = 2\n        word_to_id[\'<EOS>\'] = 3\n        id_to_word = {value:key for key,value in word_to_id.items()}\n        x_train_pad = pad_sequences(x_train,maxlen=max_len)\n        x_test_pad = pad_sequences(x_test,maxlen=max_len)\n \n \n        train_data = data_utils.TensorDataset(torch.from_numpy(x_train_pad).type(torch.LongTensor),torch.from_numpy(y_train).type(torch.LongTensor))\n        train_loader = data_utils.DataLoader(train_data,batch_size=batch_size,drop_last=True)\n        return train_loader,train_set,test_set,x_test_pad,word_to_id'"
utils/pretrained_glove_embeddings.py,3,"b'from nltk import word_tokenize\nimport numpy as np\nimport torch\n \nvocab,word2idx = None,{}\n \n \ndef load_glove_embeddings(path, word2idx, embedding_dim):\n    """"""Loading the glove embeddings""""""\n    with open(path) as f:\n        embeddings = np.zeros((len(word2idx), embedding_dim))\n        for line in f.readlines():\n            values = line.split()\n            word = values[0]\n            index = word2idx.get(word)\n            if index:\n                vector = np.array(values[1:], dtype=\'float32\')\n                if vector.shape[-1] != embedding_dim:\n                    raise Exception(\'Dimension not matching.\')\n                embeddings[index] = vector\n        return torch.from_numpy(embeddings).float()\ndef get_vocab():\n    """"""\n    Get vocabulary. Must run get_embeddings first\n   \n    Returns:\n        vocabulary\n   \n    """"""\n    if vocab is None:\n        raise Exception(""Run get_embedding first to create vocabulary"")\n    return vocab\ndef get_word_idx():\n    """"""\n    Get word2idx. Must run get_embeddings first\n   \n    Returns:\n        word2idx\n   \n    """"""\n    if not word2idx:\n        raise Exception(""Run get_embedding first to create vocabulary"")\n    return word2idx\n \n \n \ndef get_embeddings(emb_path,corpus_tokens,emb_dim,add_eos=False,add_sos=False,add_unk=False,add_pad=False):\n    """"""\n        Method to get the embeddings\n \n        Args:\n            emb_path     : {path} path of glove embeddings\n            corpus_tokens: {list} list of tokens from the corpus. Use keras or spaCy tokenizer to tokenize\n            emb_dim      : {int} embeddings dimension\n            add_eos      : {bool} add <EOS> tag to vocab\n            add_sos      : {bool} add <SOS> tag to vocab\n            add_unk      : {bool} add <UNK> tag to vocab\n            add_pad      : {bool} add <PAD> tag to vocab\n \n        Returns:\n            gloVe Embeddings\n \n        \n    """"""\n    global vocab,word2idx\n    vocab = set(corpus_tokens)\n    addons = int(add_eos)+int(add_unk)+int(add_sos)+int(add_pad)\n    word2idx = {word: (idx+addons) for idx, word in enumerate(vocab)}\n   \n    if add_pad:\n        word2idx[\'<PAD>\'] = 0\n    if add_eos:\n        word2idx[\'<EOS>\'] = 1\n    if add_sos:\n        word2idx[\'<SOS>\'] = 2\n    if add_unk:\n        word2idx[\'<UNK>\'] = 3\n   \n    \n    #print(word2idx)\n    # create word index\n    word_embeddings = load_glove_embeddings(emb_path, word2idx,emb_dim)\n    word_embedding = torch.nn.Embedding(word_embeddings.size(0), word_embeddings.size(1))\n    word_embedding.weight = torch.nn.Parameter(word_embeddings)\n    return word_embedding'"
visualization/attention_visualization.py,0,"b'#Credits to Lin Zhouhan(@hantek) for the complete visualization code\nimport random, os, numpy, scipy\nfrom codecs import open\ndef createHTML(texts, weights, fileName):\n    """"""\n    Creates a html file with text heat.\n\tweights: attention weights for visualizing\n\ttexts: text on which attention weights are to be visualized\n    """"""\n    fileName = ""visualization/""+fileName\n    fOut = open(fileName, ""w"", encoding=""utf-8"")\n    part1 = """"""\n    <html lang=""en"">\n    <head>\n    <meta http-equiv=""content-type"" content=""text/html; charset=utf-8"">\n    <style>\n    body {\n    font-family: Sans-Serif;\n    }\n    </style>\n    </head>\n    <body>\n    <h3>\n    Heatmaps\n    </h3>\n    </body>\n    <script>\n    """"""\n    part2 = """"""\n    var color = ""255,0,0"";\n    var ngram_length = 3;\n    var half_ngram = 1;\n    for (var k=0; k < any_text.length; k++) {\n    var tokens = any_text[k].split("" "");\n    var intensity = new Array(tokens.length);\n    var max_intensity = Number.MIN_SAFE_INTEGER;\n    var min_intensity = Number.MAX_SAFE_INTEGER;\n    for (var i = 0; i < intensity.length; i++) {\n    intensity[i] = 0.0;\n    for (var j = -half_ngram; j < ngram_length-half_ngram; j++) {\n    if (i+j < intensity.length && i+j > -1) {\n    intensity[i] += trigram_weights[k][i + j];\n    }\n    }\n    if (i == 0 || i == intensity.length-1) {\n    intensity[i] /= 2.0;\n    } else {\n    intensity[i] /= 3.0;\n    }\n    if (intensity[i] > max_intensity) {\n    max_intensity = intensity[i];\n    }\n    if (intensity[i] < min_intensity) {\n    min_intensity = intensity[i];\n    }\n    }\n    var denominator = max_intensity - min_intensity;\n    for (var i = 0; i < intensity.length; i++) {\n    intensity[i] = (intensity[i] - min_intensity) / denominator;\n    }\n    if (k%2 == 0) {\n    var heat_text = ""<p><br><b>Example:</b><br>"";\n    } else {\n    var heat_text = ""<b>Example:</b><br>"";\n    }\n    var space = """";\n    for (var i = 0; i < tokens.length; i++) {\n    heat_text += ""<span style=\'background-color:rgba("" + color + "","" + intensity[i] + "")\'>"" + space + tokens[i] + ""</span>"";\n    if (space == """") {\n    space = "" "";\n    }\n    }\n    //heat_text += ""<p>"";\n    document.body.innerHTML += heat_text;\n    }\n    </script>\n    </html>""""""\n    putQuote = lambda x: ""\\""%s\\""""%x\n    textsString = ""var any_text = [%s];\\n""%("","".join(map(putQuote, texts)))\n    weightsString = ""var trigram_weights = [%s];\\n""%("","".join(map(str,weights)))\n    fOut.write(part1)\n    fOut.write(textsString)\n    fOut.write(weightsString)\n    fOut.write(part2)\n    fOut.close()\n  \n    return\n'"
