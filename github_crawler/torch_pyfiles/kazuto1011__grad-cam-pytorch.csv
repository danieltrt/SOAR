file_path,api_count,code
grad_cam.py,9,"b'#!/usr/bin/env python\n# coding: utf-8\n#\n# Author:   Kazuto Nakashima\n# URL:      http://kazuto1011.github.io\n# Created:  2017-05-26\n\nfrom collections import Sequence\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\n\n\nclass _BaseWrapper(object):\n    def __init__(self, model):\n        super(_BaseWrapper, self).__init__()\n        self.device = next(model.parameters()).device\n        self.model = model\n        self.handlers = []  # a set of hook function handlers\n\n    def _encode_one_hot(self, ids):\n        one_hot = torch.zeros_like(self.logits).to(self.device)\n        one_hot.scatter_(1, ids, 1.0)\n        return one_hot\n\n    def forward(self, image):\n        self.image_shape = image.shape[2:]\n        self.logits = self.model(image)\n        self.probs = F.softmax(self.logits, dim=1)\n        return self.probs.sort(dim=1, descending=True)  # ordered results\n\n    def backward(self, ids):\n        """"""\n        Class-specific backpropagation\n        """"""\n        one_hot = self._encode_one_hot(ids)\n        self.model.zero_grad()\n        self.logits.backward(gradient=one_hot, retain_graph=True)\n\n    def generate(self):\n        raise NotImplementedError\n\n    def remove_hook(self):\n        """"""\n        Remove all the forward/backward hook functions\n        """"""\n        for handle in self.handlers:\n            handle.remove()\n\n\nclass BackPropagation(_BaseWrapper):\n    def forward(self, image):\n        self.image = image.requires_grad_()\n        return super(BackPropagation, self).forward(self.image)\n\n    def generate(self):\n        gradient = self.image.grad.clone()\n        self.image.grad.zero_()\n        return gradient\n\n\nclass GuidedBackPropagation(BackPropagation):\n    """"""\n    ""Striving for Simplicity: the All Convolutional Net""\n    https://arxiv.org/pdf/1412.6806.pdf\n    Look at Figure 1 on page 8.\n    """"""\n\n    def __init__(self, model):\n        super(GuidedBackPropagation, self).__init__(model)\n\n        def backward_hook(module, grad_in, grad_out):\n            # Cut off negative gradients\n            if isinstance(module, nn.ReLU):\n                return (F.relu(grad_in[0]),)\n\n        for module in self.model.named_modules():\n            self.handlers.append(module[1].register_backward_hook(backward_hook))\n\n\nclass Deconvnet(BackPropagation):\n    """"""\n    ""Striving for Simplicity: the All Convolutional Net""\n    https://arxiv.org/pdf/1412.6806.pdf\n    Look at Figure 1 on page 8.\n    """"""\n\n    def __init__(self, model):\n        super(Deconvnet, self).__init__(model)\n\n        def backward_hook(module, grad_in, grad_out):\n            # Cut off negative gradients and ignore ReLU\n            if isinstance(module, nn.ReLU):\n                return (F.relu(grad_out[0]),)\n\n        for module in self.model.named_modules():\n            self.handlers.append(module[1].register_backward_hook(backward_hook))\n\n\nclass GradCAM(_BaseWrapper):\n    """"""\n    ""Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization""\n    https://arxiv.org/pdf/1610.02391.pdf\n    Look at Figure 2 on page 4\n    """"""\n\n    def __init__(self, model, candidate_layers=None):\n        super(GradCAM, self).__init__(model)\n        self.fmap_pool = {}\n        self.grad_pool = {}\n        self.candidate_layers = candidate_layers  # list\n\n        def save_fmaps(key):\n            def forward_hook(module, input, output):\n                self.fmap_pool[key] = output.detach()\n\n            return forward_hook\n\n        def save_grads(key):\n            def backward_hook(module, grad_in, grad_out):\n                self.grad_pool[key] = grad_out[0].detach()\n\n            return backward_hook\n\n        # If any candidates are not specified, the hook is registered to all the layers.\n        for name, module in self.model.named_modules():\n            if self.candidate_layers is None or name in self.candidate_layers:\n                self.handlers.append(module.register_forward_hook(save_fmaps(name)))\n                self.handlers.append(module.register_backward_hook(save_grads(name)))\n\n    def _find(self, pool, target_layer):\n        if target_layer in pool.keys():\n            return pool[target_layer]\n        else:\n            raise ValueError(""Invalid layer name: {}"".format(target_layer))\n\n    def generate(self, target_layer):\n        fmaps = self._find(self.fmap_pool, target_layer)\n        grads = self._find(self.grad_pool, target_layer)\n        weights = F.adaptive_avg_pool2d(grads, 1)\n\n        gcam = torch.mul(fmaps, weights).sum(dim=1, keepdim=True)\n        gcam = F.relu(gcam)\n        gcam = F.interpolate(\n            gcam, self.image_shape, mode=""bilinear"", align_corners=False\n        )\n\n        B, C, H, W = gcam.shape\n        gcam = gcam.view(B, -1)\n        gcam -= gcam.min(dim=1, keepdim=True)[0]\n        gcam /= gcam.max(dim=1, keepdim=True)[0]\n        gcam = gcam.view(B, C, H, W)\n\n        return gcam\n\n\ndef occlusion_sensitivity(\n    model, images, ids, mean=None, patch=35, stride=1, n_batches=128\n):\n    """"""\n    ""Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization""\n    https://arxiv.org/pdf/1610.02391.pdf\n    Look at Figure A5 on page 17\n\n    Originally proposed in:\n    ""Visualizing and Understanding Convolutional Networks""\n    https://arxiv.org/abs/1311.2901\n    """"""\n\n    torch.set_grad_enabled(False)\n    model.eval()\n    mean = mean if mean else 0\n    patch_H, patch_W = patch if isinstance(patch, Sequence) else (patch, patch)\n    pad_H, pad_W = patch_H // 2, patch_W // 2\n\n    # Padded image\n    images = F.pad(images, (pad_W, pad_W, pad_H, pad_H), value=mean)\n    B, _, H, W = images.shape\n    new_H = (H - patch_H) // stride + 1\n    new_W = (W - patch_W) // stride + 1\n\n    # Prepare sampling grids\n    anchors = []\n    grid_h = 0\n    while grid_h <= H - patch_H:\n        grid_w = 0\n        while grid_w <= W - patch_W:\n            grid_w += stride\n            anchors.append((grid_h, grid_w))\n        grid_h += stride\n\n    # Baseline score without occlusion\n    baseline = model(images).detach().gather(1, ids)\n\n    # Compute per-pixel logits\n    scoremaps = []\n    for i in tqdm(range(0, len(anchors), n_batches), leave=False):\n        batch_images = []\n        batch_ids = []\n        for grid_h, grid_w in anchors[i : i + n_batches]:\n            images_ = images.clone()\n            images_[..., grid_h : grid_h + patch_H, grid_w : grid_w + patch_W] = mean\n            batch_images.append(images_)\n            batch_ids.append(ids)\n        batch_images = torch.cat(batch_images, dim=0)\n        batch_ids = torch.cat(batch_ids, dim=0)\n        scores = model(batch_images).detach().gather(1, batch_ids)\n        scoremaps += list(torch.split(scores, B))\n\n    diffmaps = torch.cat(scoremaps, dim=1) - baseline\n    diffmaps = diffmaps.view(B, new_H, new_W)\n\n    return diffmaps\n'"
main.py,12,"b'#!/usr/bin/env python\n# coding: utf-8\n#\n# Author:   Kazuto Nakashima\n# URL:      http://kazuto1011.github.io\n# Created:  2017-05-18\n\nfrom __future__ import print_function\n\nimport copy\nimport os.path as osp\n\nimport click\nimport cv2\nimport matplotlib.cm as cm\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import models, transforms\n\nfrom grad_cam import (\n    BackPropagation,\n    Deconvnet,\n    GradCAM,\n    GuidedBackPropagation,\n    occlusion_sensitivity,\n)\n\n# if a model includes LSTM, such as in image captioning,\n# torch.backends.cudnn.enabled = False\n\n\ndef get_device(cuda):\n    cuda = cuda and torch.cuda.is_available()\n    device = torch.device(""cuda"" if cuda else ""cpu"")\n    if cuda:\n        current_device = torch.cuda.current_device()\n        print(""Device:"", torch.cuda.get_device_name(current_device))\n    else:\n        print(""Device: CPU"")\n    return device\n\n\ndef load_images(image_paths):\n    images = []\n    raw_images = []\n    print(""Images:"")\n    for i, image_path in enumerate(image_paths):\n        print(""\\t#{}: {}"".format(i, image_path))\n        image, raw_image = preprocess(image_path)\n        images.append(image)\n        raw_images.append(raw_image)\n    return images, raw_images\n\n\ndef get_classtable():\n    classes = []\n    with open(""samples/synset_words.txt"") as lines:\n        for line in lines:\n            line = line.strip().split("" "", 1)[1]\n            line = line.split("", "", 1)[0].replace("" "", ""_"")\n            classes.append(line)\n    return classes\n\n\ndef preprocess(image_path):\n    raw_image = cv2.imread(image_path)\n    raw_image = cv2.resize(raw_image, (224,) * 2)\n    image = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )(raw_image[..., ::-1].copy())\n    return image, raw_image\n\n\ndef save_gradient(filename, gradient):\n    gradient = gradient.cpu().numpy().transpose(1, 2, 0)\n    gradient -= gradient.min()\n    gradient /= gradient.max()\n    gradient *= 255.0\n    cv2.imwrite(filename, np.uint8(gradient))\n\n\ndef save_gradcam(filename, gcam, raw_image, paper_cmap=False):\n    gcam = gcam.cpu().numpy()\n    cmap = cm.jet_r(gcam)[..., :3] * 255.0\n    if paper_cmap:\n        alpha = gcam[..., None]\n        gcam = alpha * cmap + (1 - alpha) * raw_image\n    else:\n        gcam = (cmap.astype(np.float) + raw_image.astype(np.float)) / 2\n    cv2.imwrite(filename, np.uint8(gcam))\n\n\ndef save_sensitivity(filename, maps):\n    maps = maps.cpu().numpy()\n    scale = max(maps[maps > 0].max(), -maps[maps <= 0].min())\n    maps = maps / scale * 0.5\n    maps += 0.5\n    maps = cm.bwr_r(maps)[..., :3]\n    maps = np.uint8(maps * 255.0)\n    maps = cv2.resize(maps, (224, 224), interpolation=cv2.INTER_NEAREST)\n    cv2.imwrite(filename, maps)\n\n\n# torchvision models\nmodel_names = sorted(\n    name\n    for name in models.__dict__\n    if name.islower() and not name.startswith(""__"") and callable(models.__dict__[name])\n)\n\n\n@click.group()\n@click.pass_context\ndef main(ctx):\n    print(""Mode:"", ctx.invoked_subcommand)\n\n\n@main.command()\n@click.option(""-i"", ""--image-paths"", type=str, multiple=True, required=True)\n@click.option(""-a"", ""--arch"", type=click.Choice(model_names), required=True)\n@click.option(""-t"", ""--target-layer"", type=str, required=True)\n@click.option(""-k"", ""--topk"", type=int, default=3)\n@click.option(""-o"", ""--output-dir"", type=str, default=""./results"")\n@click.option(""--cuda/--cpu"", default=True)\ndef demo1(image_paths, target_layer, arch, topk, output_dir, cuda):\n    """"""\n    Visualize model responses given multiple images\n    """"""\n\n    device = get_device(cuda)\n\n    # Synset words\n    classes = get_classtable()\n\n    # Model from torchvision\n    model = models.__dict__[arch](pretrained=True)\n    model.to(device)\n    model.eval()\n\n    # Images\n    images, raw_images = load_images(image_paths)\n    images = torch.stack(images).to(device)\n\n    """"""\n    Common usage:\n    1. Wrap your model with visualization classes defined in grad_cam.py\n    2. Run forward() with images\n    3. Run backward() with a list of specific classes\n    4. Run generate() to export results\n    """"""\n\n    # =========================================================================\n    print(""Vanilla Backpropagation:"")\n\n    bp = BackPropagation(model=model)\n    probs, ids = bp.forward(images)  # sorted\n\n    for i in range(topk):\n        bp.backward(ids=ids[:, [i]])\n        gradients = bp.generate()\n\n        # Save results as image files\n        for j in range(len(images)):\n            print(""\\t#{}: {} ({:.5f})"".format(j, classes[ids[j, i]], probs[j, i]))\n\n            save_gradient(\n                filename=osp.join(\n                    output_dir,\n                    ""{}-{}-vanilla-{}.png"".format(j, arch, classes[ids[j, i]]),\n                ),\n                gradient=gradients[j],\n            )\n\n    # Remove all the hook function in the ""model""\n    bp.remove_hook()\n\n    # =========================================================================\n    print(""Deconvolution:"")\n\n    deconv = Deconvnet(model=model)\n    _ = deconv.forward(images)\n\n    for i in range(topk):\n        deconv.backward(ids=ids[:, [i]])\n        gradients = deconv.generate()\n\n        for j in range(len(images)):\n            print(""\\t#{}: {} ({:.5f})"".format(j, classes[ids[j, i]], probs[j, i]))\n\n            save_gradient(\n                filename=osp.join(\n                    output_dir,\n                    ""{}-{}-deconvnet-{}.png"".format(j, arch, classes[ids[j, i]]),\n                ),\n                gradient=gradients[j],\n            )\n\n    deconv.remove_hook()\n\n    # =========================================================================\n    print(""Grad-CAM/Guided Backpropagation/Guided Grad-CAM:"")\n\n    gcam = GradCAM(model=model)\n    _ = gcam.forward(images)\n\n    gbp = GuidedBackPropagation(model=model)\n    _ = gbp.forward(images)\n\n    for i in range(topk):\n        # Guided Backpropagation\n        gbp.backward(ids=ids[:, [i]])\n        gradients = gbp.generate()\n\n        # Grad-CAM\n        gcam.backward(ids=ids[:, [i]])\n        regions = gcam.generate(target_layer=target_layer)\n\n        for j in range(len(images)):\n            print(""\\t#{}: {} ({:.5f})"".format(j, classes[ids[j, i]], probs[j, i]))\n\n            # Guided Backpropagation\n            save_gradient(\n                filename=osp.join(\n                    output_dir,\n                    ""{}-{}-guided-{}.png"".format(j, arch, classes[ids[j, i]]),\n                ),\n                gradient=gradients[j],\n            )\n\n            # Grad-CAM\n            save_gradcam(\n                filename=osp.join(\n                    output_dir,\n                    ""{}-{}-gradcam-{}-{}.png"".format(\n                        j, arch, target_layer, classes[ids[j, i]]\n                    ),\n                ),\n                gcam=regions[j, 0],\n                raw_image=raw_images[j],\n            )\n\n            # Guided Grad-CAM\n            save_gradient(\n                filename=osp.join(\n                    output_dir,\n                    ""{}-{}-guided_gradcam-{}-{}.png"".format(\n                        j, arch, target_layer, classes[ids[j, i]]\n                    ),\n                ),\n                gradient=torch.mul(regions, gradients)[j],\n            )\n\n\n@main.command()\n@click.option(""-i"", ""--image-paths"", type=str, multiple=True, required=True)\n@click.option(""-o"", ""--output-dir"", type=str, default=""./results"")\n@click.option(""--cuda/--cpu"", default=True)\ndef demo2(image_paths, output_dir, cuda):\n    """"""\n    Generate Grad-CAM at different layers of ResNet-152\n    """"""\n\n    device = get_device(cuda)\n\n    # Synset words\n    classes = get_classtable()\n\n    # Model\n    model = models.resnet152(pretrained=True)\n    model.to(device)\n    model.eval()\n\n    # The four residual layers\n    target_layers = [""relu"", ""layer1"", ""layer2"", ""layer3"", ""layer4""]\n    target_class = 243  # ""bull mastif""\n\n    # Images\n    images, raw_images = load_images(image_paths)\n    images = torch.stack(images).to(device)\n\n    gcam = GradCAM(model=model)\n    probs, ids = gcam.forward(images)\n    ids_ = torch.LongTensor([[target_class]] * len(images)).to(device)\n    gcam.backward(ids=ids_)\n\n    for target_layer in target_layers:\n        print(""Generating Grad-CAM @{}"".format(target_layer))\n\n        # Grad-CAM\n        regions = gcam.generate(target_layer=target_layer)\n\n        for j in range(len(images)):\n            print(\n                ""\\t#{}: {} ({:.5f})"".format(\n                    j, classes[target_class], float(probs[ids == target_class])\n                )\n            )\n\n            save_gradcam(\n                filename=osp.join(\n                    output_dir,\n                    ""{}-{}-gradcam-{}-{}.png"".format(\n                        j, ""resnet152"", target_layer, classes[target_class]\n                    ),\n                ),\n                gcam=regions[j, 0],\n                raw_image=raw_images[j],\n            )\n\n\n@main.command()\n@click.option(""-i"", ""--image-paths"", type=str, multiple=True, required=True)\n@click.option(""-a"", ""--arch"", type=click.Choice(model_names), required=True)\n@click.option(""-k"", ""--topk"", type=int, default=3)\n@click.option(""-s"", ""--stride"", type=int, default=1)\n@click.option(""-b"", ""--n-batches"", type=int, default=128)\n@click.option(""-o"", ""--output-dir"", type=str, default=""./results"")\n@click.option(""--cuda/--cpu"", default=True)\ndef demo3(image_paths, arch, topk, stride, n_batches, output_dir, cuda):\n    """"""\n    Generate occlusion sensitivity maps\n    """"""\n\n    device = get_device(cuda)\n\n    # Synset words\n    classes = get_classtable()\n\n    # Model from torchvision\n    model = models.__dict__[arch](pretrained=True)\n    model = torch.nn.DataParallel(model)\n    model.to(device)\n    model.eval()\n\n    # Images\n    images, _ = load_images(image_paths)\n    images = torch.stack(images).to(device)\n\n    print(""Occlusion Sensitivity:"")\n\n    patche_sizes = [10, 15, 25, 35, 45, 90]\n\n    logits = model(images)\n    probs = F.softmax(logits, dim=1)\n    probs, ids = probs.sort(dim=1, descending=True)\n\n    for i in range(topk):\n        for p in patche_sizes:\n            print(""Patch:"", p)\n            sensitivity = occlusion_sensitivity(\n                model, images, ids[:, [i]], patch=p, stride=stride, n_batches=n_batches\n            )\n\n            # Save results as image files\n            for j in range(len(images)):\n                print(""\\t#{}: {} ({:.5f})"".format(j, classes[ids[j, i]], probs[j, i]))\n\n                save_sensitivity(\n                    filename=osp.join(\n                        output_dir,\n                        ""{}-{}-sensitivity-{}-{}.png"".format(\n                            j, arch, p, classes[ids[j, i]]\n                        ),\n                    ),\n                    maps=sensitivity[j],\n                )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
