file_path,api_count,code
__init__.py,0,b'from . import nn_layers\nfrom . import utility\n'
sample.py,0,"b""import argparse\nimport os\n\nimport numpy as np\nimport torch as t\n\nfrom utils.batch_loader import BatchLoader\nfrom utils.parameters import Parameters\nfrom model.rvae import RVAE\n\nif __name__ == '__main__':\n\n    assert os.path.exists('trained_RVAE'), \\\n        'trained model not found'\n\n    parser = argparse.ArgumentParser(description='Sampler')\n    parser.add_argument('--use-cuda', type=bool, default=True, metavar='CUDA',\n                        help='use cuda (default: True)')\n    parser.add_argument('--num-sample', type=int, default=10, metavar='NS',\n                        help='num samplings (default: 10)')\n\n    args = parser.parse_args()\n\n    batch_loader = BatchLoader('')\n    parameters = Parameters(batch_loader.max_word_len,\n                            batch_loader.max_seq_len,\n                            batch_loader.words_vocab_size,\n                            batch_loader.chars_vocab_size)\n\n    rvae = RVAE(parameters)\n    rvae.load_state_dict(t.load('trained_RVAE'))\n    if args.use_cuda:\n        rvae = rvae.cuda()\n\n    for iteration in range(args.num_sample):\n        seed = np.random.normal(size=[1, parameters.latent_variable_size])\n        result = rvae.sample(batch_loader, 50, seed, args.use_cuda)\n        print(result)\n        print()"""
train.py,1,"b'import argparse\nimport os\n\nimport numpy as np\nimport torch as t\nfrom torch.optim import Adam\n\nfrom utils.batch_loader import BatchLoader\nfrom utils.parameters import Parameters\nfrom model.rvae import RVAE\n\nif __name__ == ""__main__"":\n\n    if not os.path.exists(\'data/word_embeddings.npy\'):\n        raise FileNotFoundError(""word embeddings file was\'t found"")\n\n    parser = argparse.ArgumentParser(description=\'RVAE\')\n    parser.add_argument(\'--num-iterations\', type=int, default=120000, metavar=\'NI\',\n                        help=\'num iterations (default: 120000)\')\n    parser.add_argument(\'--batch-size\', type=int, default=32, metavar=\'BS\',\n                        help=\'batch size (default: 32)\')\n    parser.add_argument(\'--use-cuda\', type=bool, default=True, metavar=\'CUDA\',\n                        help=\'use cuda (default: True)\')\n    parser.add_argument(\'--learning-rate\', type=float, default=0.00005, metavar=\'LR\',\n                        help=\'learning rate (default: 0.00005)\')\n    parser.add_argument(\'--dropout\', type=float, default=0.3, metavar=\'DR\',\n                        help=\'dropout (default: 0.3)\')\n    parser.add_argument(\'--use-trained\', type=bool, default=False, metavar=\'UT\',\n                        help=\'load pretrained model (default: False)\')\n    parser.add_argument(\'--ce-result\', default=\'\', metavar=\'CE\',\n                        help=\'ce result path (default: \'\')\')\n    parser.add_argument(\'--kld-result\', default=\'\', metavar=\'KLD\',\n                        help=\'ce result path (default: \'\')\')\n\n    args = parser.parse_args()\n\n    batch_loader = BatchLoader(\'\')\n    parameters = Parameters(batch_loader.max_word_len,\n                            batch_loader.max_seq_len,\n                            batch_loader.words_vocab_size,\n                            batch_loader.chars_vocab_size)\n\n    rvae = RVAE(parameters)\n    if args.use_trained:\n        rvae.load_state_dict(t.load(\'trained_RVAE\'))\n    if args.use_cuda:\n        rvae = rvae.cuda()\n\n    optimizer = Adam(rvae.learnable_parameters(), args.learning_rate)\n\n    train_step = rvae.trainer(optimizer, batch_loader)\n    validate = rvae.validater(batch_loader)\n\n    ce_result = []\n    kld_result = []\n\n    for iteration in range(args.num_iterations):\n\n        cross_entropy, kld, coef = train_step(iteration, args.batch_size, args.use_cuda, args.dropout)\n\n        if iteration % 5 == 0:\n            print(\'\\n\')\n            print(\'------------TRAIN-------------\')\n            print(\'----------ITERATION-----------\')\n            print(iteration)\n            print(\'--------CROSS-ENTROPY---------\')\n            print(cross_entropy.data.cpu().numpy()[0])\n            print(\'-------------KLD--------------\')\n            print(kld.data.cpu().numpy()[0])\n            print(\'-----------KLD-coef-----------\')\n            print(coef)\n            print(\'------------------------------\')\n\n        if iteration % 10 == 0:\n            cross_entropy, kld = validate(args.batch_size, args.use_cuda)\n\n            cross_entropy = cross_entropy.data.cpu().numpy()[0]\n            kld = kld.data.cpu().numpy()[0]\n\n            print(\'\\n\')\n            print(\'------------VALID-------------\')\n            print(\'--------CROSS-ENTROPY---------\')\n            print(cross_entropy)\n            print(\'-------------KLD--------------\')\n            print(kld)\n            print(\'------------------------------\')\n\n            ce_result += [cross_entropy]\n            kld_result += [kld]\n\n        if iteration % 20 == 0:\n            seed = np.random.normal(size=[1, parameters.latent_variable_size])\n\n            sample = rvae.sample(batch_loader, 50, seed, args.use_cuda)\n\n            print(\'\\n\')\n            print(\'------------SAMPLE------------\')\n            print(\'------------------------------\')\n            print(sample)\n            print(\'------------------------------\')\n\n    t.save(rvae.state_dict(), \'trained_RVAE\')\n\n    np.save(\'ce_result_{}.npy\'.format(args.ce_result), np.array(ce_result))\n    np.save(\'kld_result_npy_{}\'.format(args.kld_result), np.array(kld_result))\n'"
train_word_embeddings.py,2,"b""import argparse\n\nimport numpy as np\nimport torch as t\nfrom torch.autograd import Variable\nfrom torch.optim import SGD\n\nfrom utils.batch_loader import BatchLoader\nfrom utils.parameters import Parameters\nfrom selfModules.neg import NEG_loss\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description='word2vec')\n    parser.add_argument('--num-iterations', type=int, default=1000000, metavar='NI',\n                        help='num iterations (default: 1000000)')\n    parser.add_argument('--batch-size', type=int, default=10, metavar='BS',\n                        help='batch size (default: 10)')\n    parser.add_argument('--num-sample', type=int, default=5, metavar='NS',\n                        help='num sample (default: 5)')\n    parser.add_argument('--use-cuda', type=bool, default=True, metavar='CUDA',\n                        help='use cuda (default: True)')\n    args = parser.parse_args()\n\n    batch_loader = BatchLoader('')\n    params = Parameters(batch_loader.max_word_len,\n                        batch_loader.max_seq_len,\n                        batch_loader.words_vocab_size,\n                        batch_loader.chars_vocab_size)\n\n    neg_loss = NEG_loss(params.word_vocab_size, params.word_embed_size)\n    if args.use_cuda:\n        neg_loss = neg_loss.cuda()\n\n    # NEG_loss is defined over two embedding matrixes with shape of [params.word_vocab_size, params.word_embed_size]\n    optimizer = SGD(neg_loss.parameters(), 0.1)\n\n    for iteration in range(args.num_iterations):\n\n        input_idx, target_idx = batch_loader.next_embedding_seq(args.batch_size)\n\n        input = Variable(t.from_numpy(input_idx).long())\n        target = Variable(t.from_numpy(target_idx).long())\n        if args.use_cuda:\n            input, target = input.cuda(), target.cuda()\n\n        out = neg_loss(input, target, args.num_sample).mean()\n\n        optimizer.zero_grad()\n        out.backward()\n        optimizer.step()\n\n        if iteration % 500 == 0:\n            out = out.cpu().data.numpy()[0]\n            print('iteration = {}, loss = {}'.format(iteration, out))\n\n    word_embeddings = neg_loss.input_embeddings()\n    np.save('data/word_embeddings.npy', word_embeddings)\n"""
model/__init__.py,0,b''
model/decoder.py,2,"b'import torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom utils.functional import parameters_allocation_check\n\n\nclass Decoder(nn.Module):\n    def __init__(self, params):\n        super(Decoder, self).__init__()\n\n        self.params = params\n\n        self.rnn = nn.LSTM(input_size=self.params.latent_variable_size + self.params.word_embed_size,\n                           hidden_size=self.params.decoder_rnn_size,\n                           num_layers=self.params.decoder_num_layers,\n                           batch_first=True)\n\n        self.fc = nn.Linear(self.params.decoder_rnn_size, self.params.word_vocab_size)\n\n    def forward(self, decoder_input, z, drop_prob, initial_state=None):\n        """"""\n        :param decoder_input: tensor with shape of [batch_size, seq_len, embed_size]\n        :param z: sequence context with shape of [batch_size, latent_variable_size]\n        :param drop_prob: probability of an element of decoder input to be zeroed in sense of dropout\n        :param initial_state: initial state of decoder rnn\n\n        :return: unnormalized logits of sentense words distribution probabilities\n                    with shape of [batch_size, seq_len, word_vocab_size]\n                 final rnn state with shape of [num_layers, batch_size, decoder_rnn_size]\n        """"""\n\n        assert parameters_allocation_check(self), \\\n            \'Invalid CUDA options. Parameters should be allocated in the same memory\'\n\n        [batch_size, seq_len, _] = decoder_input.size()\n\n        \'\'\'\n            decoder rnn is conditioned on context via additional bias = W_cond * z to every input token\n        \'\'\'\n        decoder_input = F.dropout(decoder_input, drop_prob)\n\n        z = t.cat([z] * seq_len, 1).view(batch_size, seq_len, self.params.latent_variable_size)\n        decoder_input = t.cat([decoder_input, z], 2)\n\n        rnn_out, final_state = self.rnn(decoder_input, initial_state)\n\n        rnn_out = rnn_out.contiguous().view(-1, self.params.decoder_rnn_size)\n        result = self.fc(rnn_out)\n        result = result.view(batch_size, seq_len, self.params.word_vocab_size)\n\n        return result, final_state\n'"
model/encoder.py,2,"b'import torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom selfModules.highway import Highway\nfrom utils.functional import parameters_allocation_check\n\n\nclass Encoder(nn.Module):\n    def __init__(self, params):\n        super(Encoder, self).__init__()\n\n        self.params = params\n\n        self.hw1 = Highway(self.params.sum_depth + self.params.word_embed_size, 2, F.relu)\n\n        self.rnn = nn.LSTM(input_size=self.params.word_embed_size + self.params.sum_depth,\n                           hidden_size=self.params.encoder_rnn_size,\n                           num_layers=self.params.encoder_num_layers,\n                           batch_first=True,\n                           bidirectional=True)\n\n    def forward(self, input):\n        """"""\n        :param input: [batch_size, seq_len, embed_size] tensor\n        :return: context of input sentenses with shape of [batch_size, latent_variable_size]\n        """"""\n\n        [batch_size, seq_len, embed_size] = input.size()\n\n        input = input.view(-1, embed_size)\n        input = self.hw1(input)\n        input = input.view(batch_size, seq_len, embed_size)\n\n        assert parameters_allocation_check(self), \\\n            \'Invalid CUDA options. Parameters should be allocated in the same memory\'\n\n        \'\'\' Unfold rnn with zero initial state and get its final state from the last layer\n        \'\'\'\n        _, (_, final_state) = self.rnn(input)\n\n        final_state = final_state.view(self.params.encoder_num_layers, 2, batch_size, self.params.encoder_rnn_size)\n        final_state = final_state[-1]\n        h_1, h_2 = final_state[0], final_state[1]\n        final_state = t.cat([h_1, h_2], 1)\n\n        return final_state\n'"
model/rvae.py,3,"b'import numpy as np\nimport torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom .decoder import Decoder\nfrom .encoder import Encoder\n\nfrom selfModules.embedding import Embedding\n\nfrom utils.functional import kld_coef, parameters_allocation_check, fold\n\n\nclass RVAE(nn.Module):\n    def __init__(self, params):\n        super(RVAE, self).__init__()\n\n        self.params = params\n\n        self.embedding = Embedding(self.params, \'\')\n\n        self.encoder = Encoder(self.params)\n\n        self.context_to_mu = nn.Linear(self.params.encoder_rnn_size * 2, self.params.latent_variable_size)\n        self.context_to_logvar = nn.Linear(self.params.encoder_rnn_size * 2, self.params.latent_variable_size)\n\n        self.decoder = Decoder(self.params)\n\n    def forward(self, drop_prob,\n                encoder_word_input=None, encoder_character_input=None,\n                decoder_word_input=None, decoder_character_input=None,\n                z=None, initial_state=None):\n        """"""\n        :param encoder_word_input: An tensor with shape of [batch_size, seq_len] of Long type\n        :param encoder_character_input: An tensor with shape of [batch_size, seq_len, max_word_len] of Long type\n        :param decoder_word_input: An tensor with shape of [batch_size, max_seq_len + 1] of Long type\n        :param initial_state: initial state of decoder rnn in order to perform sampling\n\n        :param drop_prob: probability of an element of decoder input to be zeroed in sense of dropout\n\n        :param z: context if sampling is performing\n\n        :return: unnormalized logits of sentence words distribution probabilities\n                    with shape of [batch_size, seq_len, word_vocab_size]\n                 final rnn state with shape of [num_layers, batch_size, decoder_rnn_size]\n        """"""\n\n        assert parameters_allocation_check(self), \\\n            \'Invalid CUDA options. Parameters should be allocated in the same memory\'\n        use_cuda = self.embedding.word_embed.weight.is_cuda\n\n        assert z is None and fold(lambda acc, parameter: acc and parameter is not None,\n                                  [encoder_word_input, encoder_character_input, decoder_word_input],\n                                  True) \\\n            or (z is not None and decoder_word_input is not None), \\\n            ""Invalid input. If z is None then encoder and decoder inputs should be passed as arguments""\n\n        if z is None:\n            \'\'\' Get context from encoder and sample z ~ N(mu, std)\n            \'\'\'\n            [batch_size, _] = encoder_word_input.size()\n\n            encoder_input = self.embedding(encoder_word_input, encoder_character_input)\n\n            context = self.encoder(encoder_input)\n\n            mu = self.context_to_mu(context)\n            logvar = self.context_to_logvar(context)\n            std = t.exp(0.5 * logvar)\n\n            z = Variable(t.randn([batch_size, self.params.latent_variable_size]))\n            if use_cuda:\n                z = z.cuda()\n\n            z = z * std + mu\n\n            kld = (-0.5 * t.sum(logvar - t.pow(mu, 2) - t.exp(logvar) + 1, 1)).mean().squeeze()\n        else:\n            kld = None\n\n        decoder_input = self.embedding.word_embed(decoder_word_input)\n        out, final_state = self.decoder(decoder_input, z, drop_prob, initial_state)\n\n        return out, final_state, kld\n\n    def learnable_parameters(self):\n\n        # word_embedding is constant parameter thus it must be dropped from list of parameters for optimizer\n        return [p for p in self.parameters() if p.requires_grad]\n\n    def trainer(self, optimizer, batch_loader):\n        def train(i, batch_size, use_cuda, dropout):\n            input = batch_loader.next_batch(batch_size, \'train\')\n            input = [Variable(t.from_numpy(var)) for var in input]\n            input = [var.long() for var in input]\n            input = [var.cuda() if use_cuda else var for var in input]\n\n            [encoder_word_input, encoder_character_input, decoder_word_input, decoder_character_input, target] = input\n\n            logits, _, kld = self(dropout,\n                                  encoder_word_input, encoder_character_input,\n                                  decoder_word_input, decoder_character_input,\n                                  z=None)\n\n            logits = logits.view(-1, self.params.word_vocab_size)\n            target = target.view(-1)\n            cross_entropy = F.cross_entropy(logits, target)\n\n            loss = 79 * cross_entropy + kld_coef(i) * kld\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            return cross_entropy, kld, kld_coef(i)\n\n        return train\n\n    def validater(self, batch_loader):\n        def validate(batch_size, use_cuda):\n            input = batch_loader.next_batch(batch_size, \'valid\')\n            input = [Variable(t.from_numpy(var)) for var in input]\n            input = [var.long() for var in input]\n            input = [var.cuda() if use_cuda else var for var in input]\n\n            [encoder_word_input, encoder_character_input, decoder_word_input, decoder_character_input, target] = input\n\n            logits, _, kld = self(0.,\n                                  encoder_word_input, encoder_character_input,\n                                  decoder_word_input, decoder_character_input,\n                                  z=None)\n\n            logits = logits.view(-1, self.params.word_vocab_size)\n            target = target.view(-1)\n\n            cross_entropy = F.cross_entropy(logits, target)\n\n            return cross_entropy, kld\n\n        return validate\n\n    def sample(self, batch_loader, seq_len, seed, use_cuda):\n        seed = Variable(t.from_numpy(seed).float())\n        if use_cuda:\n            seed = seed.cuda()\n\n        decoder_word_input_np, decoder_character_input_np = batch_loader.go_input(1)\n\n        decoder_word_input = Variable(t.from_numpy(decoder_word_input_np).long())\n        decoder_character_input = Variable(t.from_numpy(decoder_character_input_np).long())\n\n        if use_cuda:\n            decoder_word_input, decoder_character_input = decoder_word_input.cuda(), decoder_character_input.cuda()\n\n        result = \'\'\n\n        initial_state = None\n\n        for i in range(seq_len):\n            logits, initial_state, _ = self(0., None, None,\n                                            decoder_word_input, decoder_character_input,\n                                            seed, initial_state)\n\n            logits = logits.view(-1, self.params.word_vocab_size)\n            prediction = F.softmax(logits)\n\n            word = batch_loader.sample_word_from_distribution(prediction.data.cpu().numpy()[-1])\n\n            if word == batch_loader.end_token:\n                break\n\n            result += \' \' + word\n\n            decoder_word_input_np = np.array([[batch_loader.word_to_idx[word]]])\n            decoder_character_input_np = np.array([[batch_loader.encode_characters(word)]])\n\n            decoder_word_input = Variable(t.from_numpy(decoder_word_input_np).long())\n            decoder_character_input = Variable(t.from_numpy(decoder_character_input_np).long())\n\n            if use_cuda:\n                decoder_word_input, decoder_character_input = decoder_word_input.cuda(), decoder_character_input.cuda()\n\n        return result\n'"
selfModules/__init__.py,0,b''
selfModules/embedding.py,2,"b'import numpy as np\nimport torch as t\nimport torch.nn as nn\nfrom torch.nn import Parameter\n\nfrom .tdnn import TDNN\n\n\nclass Embedding(nn.Module):\n    def __init__(self, params, path=\'../../../\'):\n        super(Embedding, self).__init__()\n\n        self.params = params\n\n        word_embed = np.load(path + \'data/word_embeddings.npy\')\n\n        self.word_embed = nn.Embedding(self.params.word_vocab_size, self.params.word_embed_size)\n        self.char_embed = nn.Embedding(self.params.char_vocab_size, self.params.char_embed_size)\n        self.word_embed.weight = Parameter(t.from_numpy(word_embed).float(), requires_grad=False)\n        self.char_embed.weight = Parameter(\n            t.Tensor(self.params.char_vocab_size, self.params.char_embed_size).uniform_(-1, 1))\n\n        self.TDNN = TDNN(self.params)\n\n    def forward(self, word_input, character_input):\n        """"""\n        :param word_input: [batch_size, seq_len] tensor of Long type\n        :param character_input: [batch_size, seq_len, max_word_len] tensor of Long type\n        :return: input embedding with shape of [batch_size, seq_len, word_embed_size + sum_depth]\n        """"""\n\n        assert word_input.size()[:2] == character_input.size()[:2], \\\n            \'Word input and character input must have the same sizes, but {} and {} found\'.format(\n                word_input.size(), character_input.size())\n\n        [batch_size, seq_len] = word_input.size()\n\n        word_input = self.word_embed(word_input)\n\n        character_input = character_input.view(-1, self.params.max_word_len)\n        character_input = self.char_embed(character_input)\n        character_input = character_input.view(batch_size,\n                                               seq_len,\n                                               self.params.max_word_len,\n                                               self.params.char_embed_size)\n\n        character_input = self.TDNN(character_input)\n\n        result = t.cat([word_input, character_input], 2)\n\n        return result\n'"
selfModules/highway.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Highway(nn.Module):\n    def __init__(self, size, num_layers, f):\n\n        super(Highway, self).__init__()\n\n        self.num_layers = num_layers\n\n        self.nonlinear = [nn.Linear(size, size) for _ in range(num_layers)]\n        for i, module in enumerate(self.nonlinear):\n            self._add_to_parameters(module.parameters(), \'nonlinear_module_{}\'.format(i))\n\n        self.linear = [nn.Linear(size, size) for _ in range(num_layers)]\n        for i, module in enumerate(self.linear):\n            self._add_to_parameters(module.parameters(), \'linear_module_{}\'.format(i))\n\n        self.gate = [nn.Linear(size, size) for _ in range(num_layers)]\n        for i, module in enumerate(self.gate):\n            self._add_to_parameters(module.parameters(), \'gate_module_{}\'.format(i))\n\n        self.f = f\n\n    def forward(self, x):\n        """"""\n        :param x: tensor with shape of [batch_size, size]\n\n        :return: tensor with shape of [batch_size, size]\n\n        applies \xcf\x83(x) \xe2\xa8\x80 (f(G(x))) + (1 - \xcf\x83(x)) \xe2\xa8\x80 (Q(x)) transformation | G and Q is affine transformation,\n            f is non-linear transformation, \xcf\x83(x) is affine transformation with sigmoid non-linearition\n            and \xe2\xa8\x80 is element-wise multiplication\n        """"""\n\n        for layer in range(self.num_layers):\n            gate = F.sigmoid(self.gate[layer](x))\n\n            nonlinear = self.f(self.nonlinear[layer](x))\n            linear = self.linear[layer](x)\n\n            x = gate * nonlinear + (1 - gate) * linear\n\n        return x\n\n    def _add_to_parameters(self, parameters, name):\n        for i, parameter in enumerate(parameters):\n            self.register_parameter(name=\'{}-{}\'.format(name, i), param=parameter)\n'"
selfModules/neg.py,3,"b'import torch as t\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn import Parameter\n\nfrom utils.functional import *\n\n\nclass NEG_loss(nn.Module):\n    def __init__(self, num_classes, embed_size):\n        """"""\n        :param num_classes: An int. The number of possible classes.\n        :param embed_size: An int. Embedding size\n        """"""\n\n        super(NEG_loss, self).__init__()\n\n        self.num_classes = num_classes\n        self.embed_size = embed_size\n\n        self.out_embed = nn.Embedding(self.num_classes, self.embed_size)\n        self.out_embed.weight = Parameter(t.FloatTensor(self.num_classes, self.embed_size).uniform_(-1, 1))\n\n        self.in_embed = nn.Embedding(self.num_classes, self.embed_size)\n        self.in_embed.weight = Parameter(t.FloatTensor(self.num_classes, self.embed_size).uniform_(-1, 1))\n\n    def forward(self, input_labes, out_labels, num_sampled):\n        """"""\n        :param input_labes: Tensor with shape of [batch_size] of Long type\n        :param out_labels: Tensor with shape of [batch_size] of Long type\n        :param num_sampled: An int. The number of sampled from noise examples\n\n        :return: Loss estimation with shape of [batch_size]\n            loss defined in Mikolov et al. Distributed Representations of Words and Phrases and their Compositionality\n            papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n        """"""\n\n        assert parameters_allocation_check(self), \\\n            """"""\n            Invalid CUDA options. out_embed and in_embed parameters both should be stored in the same memory\n            got out_embed.is_cuda = {}, in_embed.is_cuda = {}\n            """""".format(self.out_embed.weight.is_cuda, self.in_embed.weight.is_cuda)\n\n        use_cuda = self.out_embed.weight.is_cuda\n\n        [batch_size] = input_labes.size()\n\n        input = self.in_embed(input_labes)\n        output = self.out_embed(out_labels)\n\n        noise = Variable(t.Tensor(batch_size, num_sampled).uniform_(0, self.num_classes - 1).long())\n        if use_cuda:\n            noise = noise.cuda()\n        noise = self.out_embed(noise).neg()\n\n        log_target = (input * output).sum(1).squeeze().sigmoid().log()\n\n        \'\'\' \xe2\x88\x91[batch_size, num_sampled, embed_size] * [batch_size, embed_size, 1] ->\n            \xe2\x88\x91[batch_size, num_sampled] -> [batch_size] \'\'\'\n        sum_log_sampled = t.bmm(noise, input.unsqueeze(2)).sigmoid().log().sum(1).squeeze()\n\n        loss = log_target + sum_log_sampled\n\n        return -loss\n\n    def input_embeddings(self):\n        return self.in_embed.weight.data.cpu().numpy()\n'"
selfModules/tdnn.py,3,"b'import torch as t\nfrom torch.nn import Parameter\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass TDNN(nn.Module):\n    def __init__(self, params):\n        super(TDNN, self).__init__()\n\n        self.params = params\n\n        self.kernels = [Parameter(t.Tensor(out_dim, self.params.char_embed_size, kW).uniform_(-1, 1))\n                        for kW, out_dim in params.kernels]\n        self._add_to_parameters(self.kernels, \'TDNN_kernel\')\n\n    def forward(self, x):\n        """"""\n        :param x: tensor with shape [batch_size, max_seq_len, max_word_len, char_embed_size]\n\n        :return: tensor with shape [batch_size, max_seq_len, depth_sum]\n\n        applies multikenrel 1d-conv layer along every word in input with max-over-time pooling\n            to emit fixed-size output\n        """"""\n\n        input_size = x.size()\n        input_size_len = len(input_size)\n\n        assert input_size_len == 4, \\\n            \'Wrong input rang, must be equal to 4, but {} found\'.format(input_size_len)\n\n        [batch_size, seq_len, _, embed_size] = input_size\n\n        assert embed_size == self.params.char_embed_size, \\\n            \'Wrong embedding size, must be equal to {}, but {} found\'.format(self.params.char_embed_size, embed_size)\n\n        # leaps with shape\n        x = x.view(-1, self.params.max_word_len, self.params.char_embed_size).transpose(1, 2).contiguous()\n\n        xs = [F.tanh(F.conv1d(x, kernel)) for kernel in self.kernels]\n        xs = [x.max(2)[0].squeeze(2) for x in xs]\n\n        x = t.cat(xs, 1)\n        x = x.view(batch_size, seq_len, -1)\n\n        return x\n\n    def _add_to_parameters(self, parameters, name):\n        for i, parameter in enumerate(parameters):\n            self.register_parameter(name=\'{}-{}\'.format(name, i), param=parameter)\n'"
utils/__init__.py,0,b''
utils/batch_loader.py,0,"b'import collections\nimport os\nimport re\n\nimport numpy as np\nfrom six.moves import cPickle\n\nfrom .functional import *\n\n\nclass BatchLoader:\n    def __init__(self, path=\'../../\'):\n\n        \'\'\'\n            :properties\n\n                data_files - array containing paths to data sources\n\n                idx_files - array of paths to vocabulury files\n\n                tensor_files - matrix with shape of [2, target_num] containing paths to files\n                    with data represented as tensors\n                    where first index in shape corresponds to types of representation of data,\n                    i.e. word representation and character-aware representation\n\n                blind_symbol - special symbol to fill spaces in every word in character-aware representation\n                    to make all words be the same lenght\n                pad_token - the same special symbol as blind_symbol, but in case of lines of words\n                go_token - start of sequence symbol\n                end_token - end of sequence symbol\n\n                chars_vocab_size - number of unique characters\n                idx_to_char - array of shape [chars_vocab_size] containing ordered list of inique characters\n                char_to_idx - dictionary of shape [chars_vocab_size]\n                    such that idx_to_char[char_to_idx[some_char]] = some_char\n                    where some_char is such that idx_to_char contains it\n\n                words_vocab_size, idx_to_word, word_to_idx - same as for characters\n\n                max_word_len - maximum word length\n                max_seq_len - maximum sequence length\n                num_lines - num of lines in data with shape [target_num]\n\n                word_tensor -  tensor of shape [target_num, num_lines, line_lenght] c\n                    ontains word\'s indexes instead of words itself\n\n                character_tensor - tensor of shape [target_num, num_lines, line_lenght, max_word_len].\n                    Rows contain character indexes for every word in data\n\n            :methods\n\n                build_character_vocab(self, data) -> chars_vocab_size, idx_to_char, char_to_idx\n                    chars_vocab_size - size of unique characters in corpus\n                    idx_to_char - array of shape [chars_vocab_size] containing ordered list of inique characters\n                    char_to_idx - dictionary of shape [chars_vocab_size]\n                        such that idx_to_char[char_to_idx[some_char]] = some_char\n                        where some_char is such that idx_to_char contains it\n\n                build_word_vocab(self, sentences) -> words_vocab_size, idx_to_word, word_to_idx\n                    same as for characters\n\n                preprocess(self, data_files, idx_files, tensor_files) -> Void\n                    preprocessed and initialized properties and then save them\n\n                load_preprocessed(self, data_files, idx_files, tensor_files) -> Void\n                    load and and initialized properties\n\n                next_batch(self, batch_size, target_str) -> encoder_word_input, encoder_character_input, input_seq_len,\n                        decoder_input, decoder_output\n                    randomly sampled batch_size num of sequences for target from target_str.\n                    fills sequences with pad tokens to made them the same lenght.\n                    encoder_word_input and encoder_character_input have reversed order of the words\n                        in case of performance\n        \'\'\'\n\n        self.data_files = [path + \'data/train.txt\',\n                           path + \'data/test.txt\']\n\n        self.idx_files = [path + \'data/words_vocab.pkl\',\n                          path + \'data/characters_vocab.pkl\']\n\n        self.tensor_files = [[path + \'data/train_word_tensor.npy\',\n                              path + \'data/valid_word_tensor.npy\'],\n                             [path + \'data/train_character_tensor.npy\',\n                              path + \'data/valid_character_tensor.npy\']]\n\n        self.blind_symbol = \'\'\n        self.pad_token = \'_\'\n        self.go_token = \'>\'\n        self.end_token = \'|\'\n\n        idx_exists = fold(f_and,\n                          [os.path.exists(file) for file in self.idx_files],\n                          True)\n\n        tensors_exists = fold(f_and,\n                              [os.path.exists(file) for target in self.tensor_files\n                               for file in target],\n                              True)\n\n        if idx_exists and tensors_exists:\n            self.load_preprocessed(self.data_files,\n                                   self.idx_files,\n                                   self.tensor_files)\n            print(\'preprocessed data was found and loaded\')\n        else:\n            self.preprocess(self.data_files,\n                            self.idx_files,\n                            self.tensor_files)\n            print(\'data have preprocessed\')\n\n        self.word_embedding_index = 0\n\n    def clean_whole_data(self, string):\n        string = re.sub(\'^[\\d\\:]+ \', \'\', string, 0, re.M)\n        string = re.sub(\'\\n\\s{11}\', \' \', string, 0, re.M)\n        string = re.sub(\'\\n{2}\', \'\\n\', string, 0, re.M)\n\n        return string.lower()\n\n    def clean_str(self, string):\n        \'\'\'\n            Tokenization/string cleaning for all datasets except for SST.\n            Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data\n        \'\'\'\n\n        string = re.sub(r""[^\xea\xb0\x80-\xed\x9e\xa3A-Za-z0-9(),!?:;.\\\'\\`]"", "" "", string)\n        string = re.sub(r""\\\'s"", "" \\\'s"", string)\n        string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n        string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n        string = re.sub(r""\\\'re"", "" \\\'re"", string)\n        string = re.sub(r""\\\'d"", "" \\\'d"", string)\n        string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n        string = re.sub(r""\\."", "" . "", string)\n        string = re.sub(r"","", "" , "", string)\n        string = re.sub(r"":"", "" : "", string)\n        string = re.sub(r"";"", "" ; "", string)\n        string = re.sub(r""!"", "" ! "", string)\n        string = re.sub(r""\\("", "" ( "", string)\n        string = re.sub(r""\\)"", "" ) "", string)\n        string = re.sub(r""\\?"", "" ? "", string)\n        string = re.sub(r""\\s{2,}"", "" "", string)\n        return string.strip()\n\n    def build_character_vocab(self, data):\n\n        # unique characters with blind symbol\n        chars = list(set(data)) + [self.blind_symbol, self.pad_token, self.go_token, self.end_token]\n        chars_vocab_size = len(chars)\n\n        # mappings itself\n        idx_to_char = chars\n        char_to_idx = {x: i for i, x in enumerate(idx_to_char)}\n\n        return chars_vocab_size, idx_to_char, char_to_idx\n\n    def build_word_vocab(self, sentences):\n\n        # Build vocabulary\n        word_counts = collections.Counter(sentences)\n\n        # Mapping from index to word\n        idx_to_word = [x[0] for x in word_counts.most_common()]\n        idx_to_word = list(sorted(idx_to_word)) + [self.pad_token, self.go_token, self.end_token]\n\n        words_vocab_size = len(idx_to_word)\n\n        # Mapping from word to index\n        word_to_idx = {x: i for i, x in enumerate(idx_to_word)}\n\n        return words_vocab_size, idx_to_word, word_to_idx\n\n    def preprocess(self, data_files, idx_files, tensor_files):\n\n        data = [open(file, ""r"").read() for file in data_files]\n        merged_data = data[0] + \'\\n\' + data[1]\n\n        self.chars_vocab_size, self.idx_to_char, self.char_to_idx = self.build_character_vocab(merged_data)\n\n        with open(idx_files[1], \'wb\') as f:\n            cPickle.dump(self.idx_to_char, f)\n\n        data_words = [[line.split() for line in target.split(\'\\n\')] for target in data]\n        merged_data_words = merged_data.split()\n\n        self.words_vocab_size, self.idx_to_word, self.word_to_idx = self.build_word_vocab(merged_data_words)\n        self.max_word_len = np.amax([len(word) for word in self.idx_to_word])\n        self.max_seq_len = np.amax([len(line) for target in data_words for line in target])\n        self.num_lines = [len(target) for target in data_words]\n\n        with open(idx_files[0], \'wb\') as f:\n            cPickle.dump(self.idx_to_word, f)\n\n        self.word_tensor = np.array(\n            [[list(map(self.word_to_idx.get, line)) for line in target] for target in data_words])\n        print(self.word_tensor.shape)\n        for i, path in enumerate(tensor_files[0]):\n            np.save(path, self.word_tensor[i])\n\n        self.character_tensor = np.array(\n            [[list(map(self.encode_characters, line)) for line in target] for target in data_words])\n        for i, path in enumerate(tensor_files[1]):\n            np.save(path, self.character_tensor[i])\n\n        self.just_words = [word for line in self.word_tensor[0] for word in line]\n\n    def load_preprocessed(self, data_files, idx_files, tensor_files):\n\n        data = [open(file, ""r"").read() for file in data_files]\n        data_words = [[line.split() for line in target.split(\'\\n\')] for target in data]\n        self.max_seq_len = np.amax([len(line) for target in data_words for line in target])\n        self.num_lines = [len(target) for target in data_words]\n\n        [self.idx_to_word, self.idx_to_char] = [cPickle.load(open(file, ""rb"")) for file in idx_files]\n\n        [self.words_vocab_size, self.chars_vocab_size] = [len(idx) for idx in [self.idx_to_word, self.idx_to_char]]\n\n        [self.word_to_idx, self.char_to_idx] = [dict(zip(idx, range(len(idx)))) for idx in\n                                                [self.idx_to_word, self.idx_to_char]]\n\n        self.max_word_len = np.amax([len(word) for word in self.idx_to_word])\n\n        [self.word_tensor, self.character_tensor] = [np.array([np.load(target) for target in input_type])\n                                                     for input_type in tensor_files]\n\n        self.just_words = [word for line in self.word_tensor[0] for word in line]\n\n    def next_batch(self, batch_size, target_str):\n        target = 0 if target_str == \'train\' else 1\n\n        indexes = np.array(np.random.randint(self.num_lines[target], size=batch_size))\n\n        encoder_word_input = [self.word_tensor[target][index] for index in indexes]\n        encoder_character_input = [self.character_tensor[target][index] for index in indexes]\n        input_seq_len = [len(line) for line in encoder_word_input]\n        max_input_seq_len = np.amax(input_seq_len)\n\n        encoded_words = [[idx for idx in line] for line in encoder_word_input]\n        decoder_word_input = [[self.word_to_idx[self.go_token]] + line for line in encoder_word_input]\n        decoder_character_input = [[self.encode_characters(self.go_token)] + line for line in encoder_character_input]\n        decoder_output = [line + [self.word_to_idx[self.end_token]] for line in encoded_words]\n\n        # sorry\n        for i, line in enumerate(decoder_word_input):\n            line_len = input_seq_len[i]\n            to_add = max_input_seq_len - line_len\n            decoder_word_input[i] = line + [self.word_to_idx[self.pad_token]] * to_add\n\n        for i, line in enumerate(decoder_character_input):\n            line_len = input_seq_len[i]\n            to_add = max_input_seq_len - line_len\n            decoder_character_input[i] = line + [self.encode_characters(self.pad_token)] * to_add\n\n        for i, line in enumerate(decoder_output):\n            line_len = input_seq_len[i]\n            to_add = max_input_seq_len - line_len\n            decoder_output[i] = line + [self.word_to_idx[self.pad_token]] * to_add\n\n        for i, line in enumerate(encoder_word_input):\n            line_len = input_seq_len[i]\n            to_add = max_input_seq_len - line_len\n            encoder_word_input[i] = [self.word_to_idx[self.pad_token]] * to_add + line[::-1]\n\n        for i, line in enumerate(encoder_character_input):\n            line_len = input_seq_len[i]\n            to_add = max_input_seq_len - line_len\n            encoder_character_input[i] = [self.encode_characters(self.pad_token)] * to_add + line[::-1]\n\n        return np.array(encoder_word_input), np.array(encoder_character_input), \\\n               np.array(decoder_word_input), np.array(decoder_character_input), np.array(decoder_output)\n\n    def next_embedding_seq(self, seq_len):\n        """"""\n        :return:\n            tuple of input and output for word embedding learning,\n            where input = [b, b, c, c, d, d, e, e]\n            and output  = [a, c, b, d, d, e, d, g]\n            for line [a, b, c, d, e, g] at index i\n        """"""\n\n        words_len = len(self.just_words)\n        seq = [self.just_words[i % words_len]\n               for i in np.arange(self.word_embedding_index, self.word_embedding_index + seq_len)]\n\n        result = []\n        for i in range(seq_len - 2):\n            result.append([seq[i + 1], seq[i]])\n            result.append([seq[i + 1], seq[i + 2]])\n\n        self.word_embedding_index = (self.word_embedding_index + seq_len) % words_len - 2\n\n        # input and target\n        result = np.array(result)\n\n        return result[:, 0], result[:, 1]\n\n    def go_input(self, batch_size):\n        go_word_input = [[self.word_to_idx[self.go_token]] for _ in range(batch_size)]\n        go_character_input = [[self.encode_characters(self.go_token)] for _ in range(batch_size)]\n\n        return np.array(go_word_input), np.array(go_character_input)\n\n    def encode_word(self, idx):\n        result = np.zeros(self.words_vocab_size)\n        result[idx] = 1\n        return result\n\n    def decode_word(self, word_idx):\n        word = self.idx_to_word[word_idx]\n        return word\n\n    def sample_word_from_distribution(self, distribution):\n        ix = np.random.choice(range(self.words_vocab_size), p=distribution.ravel())\n        x = np.zeros((self.words_vocab_size, 1))\n        x[ix] = 1\n        return self.idx_to_word[np.argmax(x)]\n\n    def encode_characters(self, characters):\n        word_len = len(characters)\n        to_add = self.max_word_len - word_len\n        characters_idx = [self.char_to_idx[i] for i in characters] + to_add * [self.char_to_idx[\'\']]\n        return characters_idx\n\n    def decode_characters(self, characters_idx):\n        characters = [self.idx_to_char[i] for i in characters_idx]\n        return \'\'.join(characters)\n'"
utils/functional.py,1,"b'def fold(f, l, a):\n    return a if (len(l) == 0) else fold(f, l[1:], f(a, l[0]))\n\n\ndef f_and(x, y):\n    return x and y\n\n\ndef f_or(x, y):\n    return x or y\n\n\ndef parameters_allocation_check(module):\n    parameters = list(module.parameters())\n    return fold(f_and, parameters, True) or not fold(f_or, parameters, False)\n\n\ndef handle_inputs(inputs, use_cuda):\n    import torch as t\n    from torch.autograd import Variable\n\n    result = [Variable(t.from_numpy(var)) for var in inputs]\n    result = [var.cuda() if use_cuda else var for var in result]\n\n    return result\n\n\ndef kld_coef(i):\n    import math\n    return (math.tanh((i - 3500)/1000) + 1)/2\n\n'"
utils/parameters.py,0,"b'from .functional import *\n\n\nclass Parameters:\n    def __init__(self, max_word_len, max_seq_len, word_vocab_size, char_vocab_size):\n        self.max_word_len = int(max_word_len)\n        self.max_seq_len = int(max_seq_len) + 1  # go or eos token\n\n        self.word_vocab_size = int(word_vocab_size)\n        self.char_vocab_size = int(char_vocab_size)\n\n        self.word_embed_size = 300\n        self.char_embed_size = 15\n\n        self.kernels = [(1, 25), (2, 50), (3, 75), (4, 100), (5, 125), (6, 150)]\n        self.sum_depth = fold(lambda x, y: x + y, [depth for _, depth in self.kernels], 0)\n\n        self.encoder_rnn_size = 600\n        self.encoder_num_layers = 1\n\n        self.latent_variable_size = 1100\n\n        self.decoder_rnn_size = 800\n        self.decoder_num_layers = 2\n'"
utils/visualize_word_embeddings.py,0,"b'import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nfrom utils.batch_loader import BatchLoader\n\nif __name__ == ""__main__"":\n    if not os.path.exists(\'../../data/word_embeddings.npy\'):\n        raise FileNotFoundError(""word embeddings file was\'t found"")\n\n    pca = PCA(n_components=2)\n    word_embeddings = np.load(\'../../data/word_embeddings.npy\')\n    word_embeddings_pca = pca.fit_transform(word_embeddings)\n\n    batch_loader = BatchLoader()\n    words = batch_loader.idx_to_word\n\n    fig, ax = plt.subplots()\n    fig.set_size_inches(150, 150)\n    x = word_embeddings_pca[:, 0]\n    y = word_embeddings_pca[:, 1]\n    ax.scatter(x, y)\n\n    for i, word in enumerate(words):\n        ax.annotate(word, (x[i], y[i]))\n\n    fig.savefig(\'word_embedding.png\', dpi=100)\n'"
