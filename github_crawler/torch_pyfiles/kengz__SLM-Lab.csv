file_path,api_count,code
run_lab.py,1,"b""# The SLM Lab entrypoint\nfrom glob import glob\nfrom slm_lab import EVAL_MODES, TRAIN_MODES\nfrom slm_lab.experiment import search\nfrom slm_lab.experiment.control import Session, Trial, Experiment\nfrom slm_lab.lib import logger, util\nfrom slm_lab.spec import spec_util\nimport os\nimport sys\nimport torch.multiprocessing as mp\n\n\ndebug_modules = [\n    # 'algorithm',\n]\ndebug_level = 'DEBUG'\nlogger.toggle_debug(debug_modules, debug_level)\nlogger = logger.get_logger(__name__)\n\n\ndef get_spec(spec_file, spec_name, lab_mode, pre_):\n    '''Get spec using args processed from inputs'''\n    if lab_mode in TRAIN_MODES:\n        if pre_ is None:  # new train trial\n            spec = spec_util.get(spec_file, spec_name)\n        else:\n            # for resuming with train@{predir}\n            # e.g. train@latest (fill find the latest predir)\n            # e.g. train@data/reinforce_cartpole_2020_04_13_232521\n            predir = pre_\n            if predir == 'latest':\n                predir = sorted(glob(f'data/{spec_name}*/'))[-1]  # get the latest predir with spec_name\n            _, _, _, _, experiment_ts = util.prepath_split(predir)  # get experiment_ts to resume train spec\n            logger.info(f'Resolved to train@{predir}')\n            spec = spec_util.get(spec_file, spec_name, experiment_ts)\n    elif lab_mode == 'enjoy':\n        # for enjoy@{session_spec_file}\n        # e.g. enjoy@data/reinforce_cartpole_2020_04_13_232521/reinforce_cartpole_t0_s0_spec.json\n        session_spec_file = pre_\n        assert session_spec_file is not None, 'enjoy mode must specify a `enjoy@{session_spec_file}`'\n        spec = util.read(f'{session_spec_file}')\n    else:\n        raise ValueError(f'Unrecognizable lab_mode not of {TRAIN_MODES} or {EVAL_MODES}')\n    return spec\n\n\ndef run_spec(spec, lab_mode):\n    '''Run a spec in lab_mode'''\n    os.environ['lab_mode'] = lab_mode  # set lab_mode\n    spec = spec_util.override_spec(spec, lab_mode)  # conditionally override spec\n    if lab_mode in TRAIN_MODES:\n        spec_util.save(spec)  # first save the new spec\n        if lab_mode == 'search':\n            spec_util.tick(spec, 'experiment')\n            Experiment(spec).run()\n        else:\n            spec_util.tick(spec, 'trial')\n            Trial(spec).run()\n    elif lab_mode in EVAL_MODES:\n        Session(spec).run()\n    else:\n        raise ValueError(f'Unrecognizable lab_mode not of {TRAIN_MODES} or {EVAL_MODES}')\n\n\ndef get_spec_and_run(spec_file, spec_name, lab_mode):\n    '''Read a spec and run it in lab mode'''\n    logger.info(f'Running lab spec_file:{spec_file} spec_name:{spec_name} in mode:{lab_mode}')\n    if '@' in lab_mode:  # process lab_mode@{predir/prename}\n        lab_mode, pre_ = lab_mode.split('@')\n    else:\n        pre_ = None\n    spec = get_spec(spec_file, spec_name, lab_mode, pre_)\n\n    if 'spec_params' not in spec:\n        run_spec(spec, lab_mode)\n    else:  # spec is parametrized; run them in parallel using ray\n        param_specs = spec_util.get_param_specs(spec)\n        search.run_param_specs(param_specs)\n\n\ndef main():\n    '''Main method to run jobs from scheduler or from a spec directly'''\n    args = sys.argv[1:]\n    if len(args) <= 1:  # use scheduler\n        job_file = args[0] if len(args) == 1 else 'job/experiments.json'\n        for spec_file, spec_and_mode in util.read(job_file).items():\n            for spec_name, lab_mode in spec_and_mode.items():\n                get_spec_and_run(spec_file, spec_name, lab_mode)\n    else:  # run single spec\n        assert len(args) == 3, f'To use sys args, specify spec_file, spec_name, lab_mode'\n        get_spec_and_run(*args)\n\n\nif __name__ == '__main__':\n    try:\n        mp.set_start_method('spawn')  # for distributed pytorch to work\n    except RuntimeError:\n        pass\n    main()\n"""
setup.py,0,"b""import os\nimport sys\nfrom setuptools import setup\nfrom setuptools.command.test import test as TestCommand\n\ntest_args = [\n    '--verbose',\n    '--capture=sys',\n    '--log-level=INFO',\n    '--log-cli-level=INFO',\n    '--log-file-level=INFO',\n    '--no-flaky-report',\n    '--timeout=300',\n    '--cov-report=html',\n    '--cov-report=term',\n    '--cov-report=xml',\n    '--cov=slm_lab',\n    '--ignore=test/spec/test_dist_spec.py',\n    'test',\n]\n\n\nclass PyTest(TestCommand):\n    user_options = [('pytest-args=', 'a', 'Arguments to pass to py.test')]\n\n    def initialize_options(self):\n        os.environ['PY_ENV'] = 'test'\n        TestCommand.initialize_options(self)\n        self.pytest_args = test_args\n\n    def run_tests(self):\n        import pytest\n        errno = pytest.main(self.pytest_args)\n        sys.exit(errno)\n\n\nsetup(\n    name='slm_lab',\n    version='4.0.1',\n    description='Modular Deep Reinforcement Learning framework in PyTorch.',\n    long_description='https://github.com/kengz/slm_lab',\n    keywords='SLM Lab',\n    url='https://github.com/kengz/slm_lab',\n    author='kengz,lgraesser',\n    author_email='kengzwl@gmail.com',\n    license='MIT',\n    packages=['slm_lab'],\n    # NOTE: use the optimized conda dependencies\n    install_requires=[],\n    zip_safe=False,\n    include_package_data=True,\n    dependency_links=[],\n    extras_require={\n        'dev': [],\n        'docs': [],\n        'testing': []\n    },\n    classifiers=[],\n    test_suite='test',\n    cmdclass={'test': PyTest},\n)\n"""
bin/plot_benchmark.py,0,"b'# Script to generate latex and markdown graphs and tables\n# NOTE: add this to viz.plot_multi_trial method before saving fig:\n# fig.layout.update(dict(\n#     font=dict(size=18),\n#     yaxis=dict(rangemode=\'tozero\', title=None),\n#     xaxis=dict(title=None),\n# ))\nfrom copy import deepcopy\nfrom glob import glob\nfrom slm_lab.lib import logger, util, viz\nimport numpy as np\nimport pydash as ps\n\n\n# declare file patterns\ntrial_metrics_scalar_path = \'*trial_metrics_scalar.json\'\ntrial_metrics_path = \'*t0_trial_metrics.pkl\'\nenv_name_map = {\n    \'lunar\': \'LunarLander\',\n    \'reakout\': \'Breakout\',\n    \'ong\': \'Pong\',\n    \'bert\': \'Qbert\',\n    \'eaquest\': \'Seaquest\',\n    \'humanoid\': \'RoboschoolHumanoid\',\n    \'humanoidflagrun\': \'RoboschoolHumanoidFlagrun\',\n    \'humanoidflagrunharder\': \'RoboschoolHumanoidFlagrunHarder\',\n}\nmaster_legend_list = [\n    \'DQN\',\n    \'DDQN+PER\',\n    \'A2C (GAE)\',\n    \'A2C (n-step)\',\n    \'PPO\',\n    \'SAC\',\n]\nmaster_palette_dict = dict(zip(master_legend_list, viz.get_palette(len(master_legend_list))))\nmaster_palette_dict[\'Async SAC\'] = master_palette_dict[\'SAC\']\n\n\ndef guard_env_name(env):\n    env = env.strip(\'_\').strip(\'-\')\n    if env in env_name_map:\n        return env_name_map[env]\n    else:\n        return env\n\n\ndef get_trial_metrics_scalar(algo, env, data_folder):\n    try:\n        filepaths = glob(f\'{data_folder}/{algo}*{env}*/{trial_metrics_scalar_path}\')\n        assert len(filepaths) == 1, f\'{algo}, {env}, {filepaths}\'\n        filepath = filepaths[0]\n        return util.read(filepath)\n    except Exception as e:\n        # blank fill\n        return {\'final_return_ma\': \'\'}\n\n\ndef get_latex_row(algos, env, data_folder):\n    \'\'\'\n    Get an environment\'s latex row where each column cell is an algorithm\'s reward.\n    Max value in a row is formatted with textbf\n    \'\'\'\n    env_ret_ma_list = [get_trial_metrics_scalar(algo, env, data_folder)[\'final_return_ma\'] for algo in algos]\n    try:\n        max_val = ps.max_([k for k in env_ret_ma_list if isinstance(k, (int, float))])\n    except Exception as e:\n        print(env, env_ret_ma_list)\n        raise\n    ret_ma_str_list = []\n    for ret_ma in env_ret_ma_list:\n        if isinstance(ret_ma, str):\n            ret_ma_str = str(ret_ma)\n        else:\n            if abs(ret_ma) < 100:\n                ret_ma_str = str(round(ret_ma, 2))\n            else:\n                ret_ma_str = str(round(ret_ma))\n        if ret_ma and ret_ma == max_val:\n            ret_ma_str = f\'\\\\textbf{{{ret_ma_str}}}\'\n        ret_ma_str_list.append(ret_ma_str)\n    env = env.split(\'-\')[0]\n    env = guard_env_name(env)\n    latex_row = f\'{env} & {"" & "".join(ret_ma_str_list)} \\\\\\\\\'\n    return latex_row\n\n\ndef get_latex_body(algos, envs, data_folder):\n    \'\'\'Get the benchmark table latex body (without header)\'\'\'\n    latex_rows = [get_latex_row(algos, env, data_folder) for env in envs]\n    latex_body = \'\\n\'.join(latex_rows)\n    return latex_body\n\n\ndef get_latex_im_body(envs):\n    latex_ims = []\n    for env in envs:\n        env = guard_env_name(env)\n        latex_im = f\'\\subfloat{{\\includegraphics[width=1.22in]{{images/{env}_multi_trial_graph_mean_returns_ma_vs_frames.png}}}}\'\n        latex_ims.append(latex_im)\n\n    im_matrix = ps.chunk(latex_ims, 4)\n    latex_im_body = \'\\\\\\\\\\n\'.join([\' & \\n\'.join(row) for row in im_matrix])\n    return latex_im_body\n\n\ndef get_trial_metrics_path(algo, env, data_folder):\n    filepaths = glob(f\'{data_folder}/{algo}*{env}*/info/{trial_metrics_path}\')\n    assert len(filepaths) == 1, f\'{algo}, {env}, {filepaths}\'\n    return filepaths[0]\n\n\ndef plot_env(algos, env, data_folder, legend_list=None, frame_scales=None, showlegend=False):\n    legend_list = deepcopy(legend_list)\n    trial_metrics_path_list = []\n    for idx, algo in enumerate(algos):\n        try:\n            trial_metrics_path_list.append(get_trial_metrics_path(algo, env, data_folder))\n        except Exception as e:\n            if legend_list is not None:\n                del legend_list[idx]\n            logger.warning(f\'Nothing to plot for algo: {algo}, env: {env}\')\n    env = guard_env_name(env)\n    title = env\n    if showlegend:\n        graph_prepath = f\'{data_folder}/{env}-legend\'\n    else:\n        graph_prepath = f\'{data_folder}/{env}\'\n    palette = [master_palette_dict[k] for k in legend_list]\n    viz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath, ma=True, name_time_pairs=[(\'mean_returns\', \'frames\')], frame_scales=frame_scales, palette=palette, showlegend=showlegend)\n\n\ndef plot_envs(algos, envs, data_folder, legend_list, frame_scales=None):\n    for idx, env in enumerate(envs):\n        try:\n            plot_env(algos, env, data_folder, legend_list=legend_list, frame_scales=frame_scales, showlegend=False)\n            if idx == len(envs) - 1:\n                # plot extra to crop legend out\n                plot_env(algos, env, data_folder, legend_list=legend_list, frame_scales=frame_scales, showlegend=True)\n        except Exception as e:\n            logger.warning(f\'Cant plot for env: {env}. Error: {e}\')\n\n\n# Discrete\n# LunarLander + Small Atari + Unity\ndata_folder = util.smart_path(\'../Desktop/benchmark/discrete\')\n\nalgos = [\n    \'dqn\',\n    \'ddqn_per\',\n    \'a2c_gae\',\n    \'a2c_nstep\',\n    \'ppo\',\n    \'*sac\',\n]\nlegend_list = [\n    \'DQN\',\n    \'DDQN+PER\',\n    \'A2C (GAE)\',\n    \'A2C (n-step)\',\n    \'PPO\',\n    \'SAC\',\n]\nenvs = [\n    \'reakout\',\n    \'ong\',\n    \'eaquest\',\n    \'bert\',\n    \'lunar\',\n    \'UnityHallway\',\n    \'UnityPushBlock\',\n]\n\nlatex_body = get_latex_body(algos, envs, data_folder)\nprint(latex_body)\nlatex_im_body = get_latex_im_body(envs)\nprint(latex_im_body)\n\n\n# plot normal\nenvs = [\n    # \'Breakout\',\n    # \'Seaquest\',\n    \'lunar\',\n    \'UnityHallway\',\n    \'UnityPushBlock\',\n]\nplot_envs(algos, envs, data_folder, legend_list)\n\n# Replot Pong and Qbert for Async SAC\nenvs = [\n    \'reakout\',\n    \'ong\',\n    \'eaquest\',\n]\nplot_envs(algos, envs, data_folder, legend_list, frame_scales=[(-1, 6)])\n\nenvs = [\n    \'bert\',\n]\nplot_envs(algos, envs, data_folder, legend_list, frame_scales=[(-1, 8)])\n\n\n# Continuous\n# Roboschool + Unity\ndata_folder = util.smart_path(\'../Desktop/benchmark/cont\')\n\nalgos = [\n    \'a2c_gae\',\n    \'a2c_nstep\',\n    \'ppo\',\n    \'*sac\',\n]\nlegend_list = [\n    \'A2C (GAE)\',\n    \'A2C (n-step)\',\n    \'PPO\',\n    \'SAC\',\n]\nenvs = [\n    \'RoboschoolAnt\',\n    \'RoboschoolAtlasForwardWalk\',\n    \'RoboschoolHalfCheetah\',\n    \'RoboschoolHopper\',\n    \'RoboschoolInvertedDoublePendulum\',\n    \'RoboschoolInvertedPendulum\',\n    \'RoboschoolReacher\',\n    \'RoboschoolWalker2d\',\n    \'humanoid_\',\n    \'humanoidflagrun_\',\n    \'humanoidflagrunharder\',\n    \'Unity3DBall-\',\n    \'Unity3DBallHard\',\n    # \'UnityCrawlerDynamic\',\n    # \'UnityCrawlerStatic\',\n    # \'UnityReacher\',\n    # \'UnityWalker\',\n]\n\nlatex_body = get_latex_body(algos, envs, data_folder)\nprint(latex_body)\nlatex_im_body = get_latex_im_body(envs)\nprint(latex_im_body)\n\n\n# plot simple\nenvs = [\n    \'RoboschoolAnt\',\n    \'RoboschoolAtlasForwardWalk\',\n    \'RoboschoolHalfCheetah\',\n    \'RoboschoolHopper\',\n    \'RoboschoolInvertedDoublePendulum\',\n    \'RoboschoolInvertedPendulum\',\n    \'RoboschoolReacher\',\n    \'RoboschoolWalker2d\',\n    \'Unity3DBall-\',\n    \'Unity3DBallHard\',\n    # \'UnityCrawlerDynamic\',\n    \'UnityCrawlerStatic\',\n    \'UnityReacher\',\n    # \'UnityWalker\',\n]\nplot_envs(algos, envs, data_folder, legend_list)\n\n\nalgos = [\n    \'a2c_gae\',\n    \'a2c_nstep\',\n    \'ppo\',\n    \'*sac\',\n]\nlegend_list = [\n    \'A2C (GAE)\',\n    \'A2C (n-step)\',\n    \'PPO\',\n    \'Async SAC\',\n]\n# plot humanoids with async sac\nenvs = [\n    \'humanoid_\',\n]\nplot_envs(algos, envs, data_folder, legend_list, frame_scales=[(-1, 16)])\n\nenvs = [\n    \'humanoidflagrun_\',\n]\nplot_envs(algos, envs, data_folder, legend_list, frame_scales=[(-1, 32)])\n\nenvs = [\n    \'humanoidflagrunharder\',\n]\nplot_envs(algos, envs, data_folder, legend_list, frame_scales=[(-1, 32)])\n\n\n# Atari full\ndata_folder = util.smart_path(\'../Desktop/benchmark/atari\')\n\nalgos = [\n    \'dqn_atari\',\n    \'ddqn_per\',\n    \'a2c_gae\',\n    \'a2c_nstep\',\n    \'ppo\',\n]\nlegend_list = [\n    \'DQN\',\n    \'DDQN+PER\',\n    \'A2C (GAE)\',\n    \'A2C (n-step)\',\n    \'PPO\',\n]\nenvs = [\n    ""Adventure"", ""AirRaid"", ""Alien"", ""Amidar"", ""Assault"", ""Asterix"", ""Asteroids"", ""Atlantis"", ""BankHeist"", ""BattleZone"", ""BeamRider"", ""Berzerk"", ""Bowling"", ""Boxing"", ""Breakout"", ""Carnival"", ""Centipede"", ""ChopperCommand"", ""CrazyClimber"", ""Defender"", ""DemonAttack"", ""DoubleDunk"", ""ElevatorAction"", ""Enduro"", ""FishingDerby"", ""Freeway"", ""Frostbite"", ""Gopher"", ""Gravitar"", ""Hero"", ""IceHockey"", ""Jamesbond"", ""JourneyEscape"", ""Kangaroo"", ""Krull"", ""KungFuMaster"", ""MontezumaRevenge"", ""MsPacman"", ""NameThisGame"", ""Phoenix"", ""Pitfall"", ""Pong"", ""Pooyan"", ""PrivateEye"", ""Qbert"", ""Riverraid"", ""RoadRunner"", ""Robotank"", ""Seaquest"", ""Skiing"", ""Solaris"", ""SpaceInvaders"", ""StarGunner"", ""Tennis"", ""TimePilot"", ""Tutankham"", ""UpNDown"", ""Venture"", ""VideoPinball"", ""WizardOfWor"", ""YarsRevenge"", ""Zaxxon""\n]\n\nlatex_body = get_latex_body(algos, envs, data_folder)\nprint(latex_body)\nlatex_im_body = get_latex_im_body(envs)\nprint(latex_im_body)\nplot_envs(algos, envs, data_folder, legend_list)\n'"
bin/plot_script.py,0,"b""# Script to plot graphs from data/\nfrom slm_lab.lib import logger, util, viz\nimport numpy as np\n\n# Atari\ntrial_metrics_path_list = [\n    'data/dqn_atari_PongNoFrameskip-v4_2019_07_28_142154/info/dqn_atari_PongNoFrameskip-v4_t0_trial_metrics.pkl',  # DQN\n    'data/ddqn_per_atari_PongNoFrameskip-v4_2019_07_30_000958/info/ddqn_per_atari_PongNoFrameskip-v4_t0_trial_metrics.pkl',  # DDQN PER\n    'data/a2c_nstep_atari_PongNoFrameskip-v4_2019_07_28_020953/info/a2c_nstep_atari_PongNoFrameskip-v4_t0_trial_metrics.pkl',  # A2C Nstep\n    'data/a2c_gae_atari_PongNoFrameskip-v4_2019_07_28_084758/info/a2c_gae_atari_PongNoFrameskip-v4_t0_trial_metrics.pkl',  # A2C GAE\n    'data/ppo_atari_PongNoFrameskip-v4_2019_07_29_042926/info/ppo_atari_PongNoFrameskip-v4_t0_trial_metrics.pkl',  # PPO\n]\nlegend_list = [\n    'DQN',\n    'DoubleDQN+PER',\n    'A2C (n-step)',\n    'A2C (GAE)',\n    'PPO',\n]\ntitle = f'multi trial graph: Pong'\ngraph_prepath = 'data/benchmark_pong'\nviz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath)\nviz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath, ma=True)\n\n\n# Roboschool\nenv_list = [\n    'RoboschoolAnt',\n    'RoboschoolAtlasForwardWalk',\n    'RoboschoolHalfCheetah',\n    'RoboschoolHopper',\n    'RoboschoolInvertedDoublePendulum',\n    'RoboschoolInvertedPendulum',\n    'RoboschoolReacher',\n    'RoboschoolWalker2d',\n]\n\nfor env in env_list:\n    trial_metrics_path_list = [\n        f'data/a2c_gae_roboschool_{env}-v1_2019_08_27_135211/info/a2c_gae_roboschool_{env}-v1_t0_trial_metrics.pkl',\n        f'data/a2c_nstep_roboschool_{env}-v1_2019_08_27_075653/info/a2c_nstep_roboschool_{env}-v1_t0_trial_metrics.pkl',\n        f'data/ppo_roboschool_{env}-v1_2019_08_27_182010/info/ppo_roboschool_{env}-v1_t0_trial_metrics.pkl',\n        f'data/sac_roboschool_{env}-v1_2019_08_29_021001/info/sac_roboschool_{env}-v1_t0_trial_metrics.pkl',\n    ]\n    legend_list = [\n        'A2C (GAE)',\n        'A2C (n-step)',\n        'PPO',\n        'SAC',\n    ]\n    title = f'multi trial graph: {env}'\n    graph_prepath = f'data/almanac_{env}'\n    viz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath)\n    viz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath, ma=True)\n\n\n# Humanoid\n\nenv = 'humanoid'\ntrial_metrics_path_list = [\n    f'data/a2c_gae_{env}_2019_08_27_170720/info/a2c_gae_{env}_t0_trial_metrics.pkl',\n    f'data/a2c_nstep_{env}_2019_08_27_170715/info/a2c_nstep_{env}_t0_trial_metrics.pkl',\n    f'data/ppo_{env}_2019_08_27_170710/info/ppo_{env}_t0_trial_metrics.pkl',\n    f'data/async_sac_{env}_2019_08_29_164833/info/async_sac_{env}_t0_trial_metrics.pkl',\n]\nlegend_list = [\n    'A2C (GAE)',\n    'A2C (n-step)',\n    'PPO',\n    'SAC',\n]\ntitle = f'multi trial graph: {env}'\ngraph_prepath = f'data/almanac_{env}'\nviz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath)\nviz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath, ma=True)\n\nenv = 'humanoidflagrun'\ntrial_metrics_path_list = [\n    f'data/a2c_gae_{env}_2019_08_28_041224/info/a2c_gae_{env}_t0_trial_metrics.pkl',\n    f'data/a2c_nstep_{env}_2019_08_28_041251/info/a2c_nstep_{env}_t0_trial_metrics.pkl',\n    f'data/ppo_{env}_2019_08_28_041328/info/ppo_{env}_t0_trial_metrics.pkl',\n    f'data/async_sac_{env}_2019_08_29_164843/info/async_sac_{env}_t0_trial_metrics.pkl',\n]\nlegend_list = [\n    'A2C (GAE)',\n    'A2C (n-step)',\n    'PPO',\n    'SAC',\n]\ntitle = f'multi trial graph: {env}'\ngraph_prepath = f'data/almanac_{env}'\nviz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath)\nviz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath, ma=True)\n\nenv = 'humanoidflagrunharder'\ntrial_metrics_path_list = [\n    f'data/a2c_gae_{env}_2019_08_28_041503/info/a2c_gae_{env}_t0_trial_metrics.pkl',\n    f'data/a2c_nstep_{env}_2019_08_28_041525/info/a2c_nstep_{env}_t0_trial_metrics.pkl',\n    f'data/ppo_{env}_2019_08_28_041447/info/ppo_{env}_t0_trial_metrics.pkl',\n    f'data/async_sac_{env}_2019_08_29_164837/info/async_sac_{env}_t0_trial_metrics.pkl',\n]\nlegend_list = [\n    'A2C (GAE)',\n    'A2C (n-step)',\n    'PPO',\n    'SAC',\n]\ntitle = f'multi trial graph: {env}'\ngraph_prepath = f'data/almanac_{env}'\nviz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath)\nviz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath, ma=True)\n"""
slm_lab/__init__.py,0,"b""import os\n\nos.environ['PY_ENV'] = os.environ.get('PY_ENV') or 'development'\nROOT_DIR = os.path.normpath(os.path.join(os.path.dirname(__file__), '..'))\n\n# valid lab_mode in SLM Lab\nEVAL_MODES = ('enjoy', 'eval')\nTRAIN_MODES = ('search', 'train', 'dev')\n"""
test/__init__.py,0,b''
test/conftest.py,0,"b""from slm_lab.experiment.control import make_agent_env\nfrom slm_lab.spec import spec_util\nimport numpy as np\nimport pandas as pd\nimport pytest\n\n\n@pytest.fixture(scope='session')\ndef test_spec():\n    spec = spec_util.get('experimental/misc/base.json', 'base_case_openai')\n    spec_util.tick(spec, 'trial')\n    spec = spec_util.override_spec(spec, 'test')\n    return spec\n\n\n@pytest.fixture\ndef test_df():\n    data = pd.DataFrame({\n        'integer': [1, 2, 3],\n        'square': [1, 4, 9],\n        'letter': ['a', 'b', 'c'],\n    })\n    assert isinstance(data, pd.DataFrame)\n    return data\n\n\n@pytest.fixture\ndef test_dict():\n    data = {\n        'a': 1,\n        'b': 2,\n        'c': 3,\n    }\n    assert isinstance(data, dict)\n    return data\n\n\n@pytest.fixture\ndef test_list():\n    data = [1, 2, 3]\n    assert isinstance(data, list)\n    return data\n\n\n@pytest.fixture\ndef test_obj():\n    class Foo:\n        bar = 'bar'\n    return Foo()\n\n\n@pytest.fixture\ndef test_str():\n    data = 'lorem ipsum dolor'\n    assert isinstance(data, str)\n    return data\n\n\n@pytest.fixture(scope='session', params=[\n    (\n        2,\n        [\n            [np.asarray([1, 1, 1, 1]), 1, 1, np.asarray([2, 2, 2, 2]), 1],\n            [np.asarray([2, 2, 2, 2]), 1, 2, np.asarray([3, 3, 3, 3]), 2],\n            [np.asarray([3, 3, 3, 3]), 1, 3, np.asarray([4, 4, 4, 4]), 3],\n            [np.asarray([4, 4, 4, 4]), 1, 4, np.asarray([5, 5, 5, 5]), 4],\n            [np.asarray([5, 5, 5, 5]), 1, 5, np.asarray([6, 6, 6, 6]), 5],\n            [np.asarray([6, 6, 6, 6]), 1, 6, np.asarray([7, 7, 7, 7]), 6],\n            [np.asarray([7, 7, 7, 7]), 1, 7, np.asarray([8, 8, 8, 8]), 7],\n            [np.asarray([8, 8, 8, 8]), 1, 8, np.asarray([9, 9, 9, 9]), 8],\n        ]\n    ),\n])\ndef test_memory(request):\n    spec = spec_util.get('experimental/misc/base.json', 'base_memory')\n    spec_util.tick(spec, 'trial')\n    agent, env = make_agent_env(spec)\n    res = (agent.body.memory, ) + request.param\n    return res\n\n\n@pytest.fixture(scope='session', params=[\n    (\n        2,\n        [\n            [np.asarray([1, 1, 1, 1]), 1, 1, np.asarray([2, 2, 2, 2]), 0],\n            [np.asarray([2, 2, 2, 2]), 1, 2, np.asarray([3, 3, 3, 3]), 0],\n            [np.asarray([3, 3, 3, 3]), 1, 3, np.asarray([4, 4, 4, 4]), 0],\n            [np.asarray([4, 4, 4, 4]), 1, 4, np.asarray([5, 5, 5, 5]), 0],\n            [np.asarray([5, 5, 5, 5]), 1, 5, np.asarray([6, 6, 6, 6]), 0],\n            [np.asarray([6, 6, 6, 6]), 1, 6, np.asarray([7, 7, 7, 7]), 0],\n            [np.asarray([7, 7, 7, 7]), 1, 7, np.asarray([8, 8, 8, 8]), 0],\n            [np.asarray([8, 8, 8, 8]), 1, 8, np.asarray([9, 9, 9, 9]), 1],\n        ]\n    ),\n])\ndef test_on_policy_episodic_memory(request):\n    spec = spec_util.get('experimental/misc/base.json', 'base_on_policy_memory')\n    spec_util.tick(spec, 'trial')\n    agent, env = make_agent_env(spec)\n    res = (agent.body.memory, ) + request.param\n    return res\n\n\n@pytest.fixture(scope='session', params=[\n    (\n        4,\n        [\n            [np.asarray([1, 1, 1, 1]), 1, 1, np.asarray([2, 2, 2, 2]), 0],\n            [np.asarray([2, 2, 2, 2]), 1, 2, np.asarray([3, 3, 3, 3]), 0],\n            [np.asarray([3, 3, 3, 3]), 1, 3, np.asarray([4, 4, 4, 4]), 0],\n            [np.asarray([4, 4, 4, 4]), 1, 4, np.asarray([5, 5, 5, 5]), 0],\n            [np.asarray([5, 5, 5, 5]), 1, 5, np.asarray([6, 6, 6, 6]), 0],\n            [np.asarray([6, 6, 6, 6]), 1, 6, np.asarray([7, 7, 7, 7]), 0],\n            [np.asarray([7, 7, 7, 7]), 1, 7, np.asarray([8, 8, 8, 8]), 0],\n            [np.asarray([8, 8, 8, 8]), 1, 8, np.asarray([9, 9, 9, 9]), 1],\n        ]\n    ),\n])\ndef test_on_policy_batch_memory(request):\n    spec = spec_util.get('experimental/misc/base.json', 'base_on_policy_batch_memory')\n    spec_util.tick(spec, 'trial')\n    agent, env = make_agent_env(spec)\n    res = (agent.body.memory, ) + request.param\n    return res\n\n\n@pytest.fixture(scope='session', params=[\n    (\n        4,\n        [\n            [np.asarray([1, 1, 1, 1]), 1, 1, np.asarray([2, 2, 2, 2]), 0, 1000],\n            [np.asarray([2, 2, 2, 2]), 1, 2, np.asarray([3, 3, 3, 3]), 0, 0],\n            [np.asarray([3, 3, 3, 3]), 1, 3, np.asarray([4, 4, 4, 4]), 0, 0],\n            [np.asarray([4, 4, 4, 4]), 1, 4, np.asarray([5, 5, 5, 5]), 0, 0],\n            [np.asarray([5, 5, 5, 5]), 1, 5, np.asarray([6, 6, 6, 6]), 0, 1000],\n            [np.asarray([6, 6, 6, 6]), 1, 6, np.asarray([7, 7, 7, 7]), 0, 0],\n            [np.asarray([7, 7, 7, 7]), 1, 7, np.asarray([8, 8, 8, 8]), 0, 0],\n            [np.asarray([8, 8, 8, 8]), 1, 8, np.asarray([9, 9, 9, 9]), 1, 1000],\n        ]\n    ),\n])\ndef test_prioritized_replay_memory(request):\n    spec = spec_util.get('experimental/misc/base.json', 'base_prioritized_replay_memory')\n    spec_util.tick(spec, 'trial')\n    agent, env = make_agent_env(spec)\n    res = (agent.body.memory, ) + request.param\n    return res\n"""
slm_lab/agent/__init__.py,3,"b""# The agent module\nfrom slm_lab.agent import algorithm, memory\nfrom slm_lab.agent.algorithm import policy_util\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.lib import logger, util, viz\nfrom slm_lab.lib.decorator import lab_api\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimport os\nimport pandas as pd\nimport pydash as ps\nimport torch\nimport warnings\n\nlogger = logger.get_logger(__name__)\n\n\nclass Agent:\n    '''\n    Agent abstraction; implements the API to interface with Env in SLM Lab\n    Contains algorithm, memory, body\n    '''\n\n    def __init__(self, spec, body, global_nets=None):\n        self.spec = spec\n        self.agent_spec = spec['agent'][0]  # idx 0 for single-agent\n        self.name = self.agent_spec['name']\n        assert not ps.is_list(global_nets), f'single agent global_nets must be a dict, got {global_nets}'\n        # set components\n        self.body = body\n        body.agent = self\n        MemoryClass = getattr(memory, ps.get(self.agent_spec, 'memory.name'))\n        self.body.memory = MemoryClass(self.agent_spec['memory'], self.body)\n        AlgorithmClass = getattr(algorithm, ps.get(self.agent_spec, 'algorithm.name'))\n        self.algorithm = AlgorithmClass(self, global_nets)\n\n        logger.info(util.self_desc(self))\n\n    @lab_api\n    def act(self, state):\n        '''Standard act method from algorithm.'''\n        with torch.no_grad():  # for efficiency, only calc grad in algorithm.train\n            action = self.algorithm.act(state)\n        return action\n\n    @lab_api\n    def update(self, state, action, reward, next_state, done):\n        '''Update per timestep after env transitions, e.g. memory, algorithm, update agent params, train net'''\n        self.body.update(state, action, reward, next_state, done)\n        if util.in_eval_lab_mode():  # eval does not update agent for training\n            return\n        self.body.memory.update(state, action, reward, next_state, done)\n        loss = self.algorithm.train()\n        if not np.isnan(loss):  # set for log_summary()\n            self.body.loss = loss\n        explore_var = self.algorithm.update()\n        return loss, explore_var\n\n    @lab_api\n    def save(self, ckpt=None):\n        '''Save agent'''\n        if util.in_eval_lab_mode():  # eval does not save new models\n            return\n        self.algorithm.save(ckpt=ckpt)\n\n    @lab_api\n    def close(self):\n        '''Close and cleanup agent at the end of a session, e.g. save model'''\n        self.save()\n\n\nclass Body:\n    '''\n    Body of an agent inside an environment, it:\n    - enables the automatic dimension inference for constructing network input/output\n    - acts as reference bridge between agent and environment (useful for multi-agent, multi-env)\n    - acts as non-gradient variable storage for monitoring and analysis\n    '''\n\n    def __init__(self, env, spec, aeb=(0, 0, 0)):\n        # essential reference variables\n        self.agent = None  # set later\n        self.env = env\n        self.spec = spec\n        # agent, env, body index for multi-agent-env\n        self.a, self.e, self.b = self.aeb = aeb\n\n        # variables set during init_algorithm_params\n        self.explore_var = np.nan  # action exploration: epsilon or tau\n        self.entropy_coef = np.nan  # entropy for exploration\n\n        # debugging/logging variables, set in train or loss function\n        self.loss = np.nan\n        self.mean_entropy = np.nan\n        self.mean_grad_norm = np.nan\n\n        # total_reward_ma from eval for model checkpoint saves\n        self.best_total_reward_ma = -np.inf\n        self.total_reward_ma = np.nan\n\n        # dataframes to track data for analysis.analyze_session\n        # track training data per episode\n        self.train_df = pd.DataFrame(columns=[\n            'epi', 't', 'wall_t', 'opt_step', 'frame', 'fps', 'total_reward', 'total_reward_ma', 'loss', 'lr',\n            'explore_var', 'entropy_coef', 'entropy', 'grad_norm'])\n\n        # in train@ mode, override from saved train_df if exists\n        if util.in_train_lab_mode() and self.spec['meta']['resume']:\n            train_df_filepath = util.get_session_df_path(self.spec, 'train')\n            if os.path.exists(train_df_filepath):\n                self.train_df = util.read(train_df_filepath)\n                self.env.clock.load(self.train_df)\n\n        # track eval data within run_eval. the same as train_df except for reward\n        if self.spec['meta']['rigorous_eval']:\n            self.eval_df = self.train_df.copy()\n        else:\n            self.eval_df = self.train_df\n\n        # the specific agent-env interface variables for a body\n        self.observation_space = self.env.observation_space\n        self.action_space = self.env.action_space\n        self.observable_dim = self.env.observable_dim\n        self.state_dim = self.observable_dim['state']\n        self.action_dim = self.env.action_dim\n        self.is_discrete = self.env.is_discrete\n        # set the ActionPD class for sampling action\n        self.action_type = policy_util.get_action_type(self.action_space)\n        self.action_pdtype = ps.get(spec, f'agent.{self.a}.algorithm.action_pdtype')\n        if self.action_pdtype in (None, 'default'):\n            self.action_pdtype = policy_util.ACTION_PDS[self.action_type][0]\n        self.ActionPD = policy_util.get_action_pd_cls(self.action_pdtype, self.action_type)\n\n    def update(self, state, action, reward, next_state, done):\n        '''Interface update method for body at agent.update()'''\n        if util.get_lab_mode() == 'dev':  # log tensorboard only on dev mode\n            self.track_tensorboard(action)\n\n    def __str__(self):\n        class_attr = util.get_class_attr(self)\n        class_attr.pop('spec')\n        return f'body: {util.to_json(class_attr)}'\n\n    def calc_df_row(self, env):\n        '''Calculate a row for updating train_df or eval_df.'''\n        frame = self.env.clock.frame\n        wall_t = self.env.clock.wall_t\n        fps = 0 if wall_t == 0 else frame / wall_t\n        with warnings.catch_warnings():  # mute np.nanmean warning\n            warnings.filterwarnings('ignore')\n            total_reward = np.nanmean(env.total_reward)  # guard for vec env\n\n        # update debugging variables\n        if net_util.to_check_train_step():\n            grad_norms = net_util.get_grad_norms(self.agent.algorithm)\n            self.mean_grad_norm = np.nan if ps.is_empty(grad_norms) else np.mean(grad_norms)\n\n        row = pd.Series({\n            # epi and frame are always measured from training env\n            'epi': self.env.clock.epi,\n            # t and reward are measured from a given env or eval_env\n            't': env.clock.t,\n            'wall_t': wall_t,\n            'opt_step': self.env.clock.opt_step,\n            'frame': frame,\n            'fps': fps,\n            'total_reward': total_reward,\n            'total_reward_ma': np.nan,  # update outside\n            'loss': self.loss,\n            'lr': self.get_mean_lr(),\n            'explore_var': self.explore_var,\n            'entropy_coef': self.entropy_coef if hasattr(self, 'entropy_coef') else np.nan,\n            'entropy': self.mean_entropy,\n            'grad_norm': self.mean_grad_norm,\n        }, dtype=np.float32)\n        assert all(col in self.train_df.columns for col in row.index), f'Mismatched row keys: {row.index} vs df columns {self.train_df.columns}'\n        return row\n\n    def ckpt(self, env, df_mode):\n        '''\n        Checkpoint to update body.train_df or eval_df data\n        @param OpenAIEnv:env self.env or self.eval_env\n        @param str:df_mode 'train' or 'eval'\n        '''\n        row = self.calc_df_row(env)\n        df = getattr(self, f'{df_mode}_df')\n        df.loc[len(df)] = row  # append efficiently to df\n        df.iloc[-1]['total_reward_ma'] = total_reward_ma = df[-viz.PLOT_MA_WINDOW:]['total_reward'].mean()\n        df.drop_duplicates('frame', inplace=True)  # remove any duplicates by the same frame\n        self.total_reward_ma = total_reward_ma\n\n    def get_mean_lr(self):\n        '''Gets the average current learning rate of the algorithm's nets.'''\n        if not hasattr(self.agent.algorithm, 'net_names'):\n            return np.nan\n        lrs = []\n        for attr, obj in self.agent.algorithm.__dict__.items():\n            if attr.endswith('lr_scheduler'):\n                lrs.append(obj.get_lr())\n        return np.mean(lrs)\n\n    def get_log_prefix(self):\n        '''Get the prefix for logging'''\n        spec_name = self.spec['name']\n        trial_index = self.spec['meta']['trial']\n        session_index = self.spec['meta']['session']\n        prefix = f'Trial {trial_index} session {session_index} {spec_name}_t{trial_index}_s{session_index}'\n        return prefix\n\n    def log_metrics(self, metrics, df_mode):\n        '''Log session metrics'''\n        prefix = self.get_log_prefix()\n        row_str = '  '.join([f'{k}: {v:g}' for k, v in metrics.items()])\n        msg = f'{prefix} [{df_mode}_df metrics] {row_str}'\n        logger.info(msg)\n\n    def log_summary(self, df_mode):\n        '''\n        Log the summary for this body when its environment is done\n        @param str:df_mode 'train' or 'eval'\n        '''\n        prefix = self.get_log_prefix()\n        df = getattr(self, f'{df_mode}_df')\n        last_row = df.iloc[-1]\n        row_str = '  '.join([f'{k}: {v:g}' for k, v in last_row.items()])\n        msg = f'{prefix} [{df_mode}_df] {row_str}'\n        logger.info(msg)\n        if util.get_lab_mode() == 'dev' and df_mode == 'train':  # log tensorboard only on dev mode and train df data\n            self.log_tensorboard()\n\n    def log_tensorboard(self):\n        '''\n        Log summary and useful info to TensorBoard.\n        NOTE this logging is comprehensive and memory-intensive, hence it is used in dev mode only\n        '''\n        # initialize TensorBoard writer\n        if not hasattr(self, 'tb_writer'):\n            log_prepath = self.spec['meta']['log_prepath']\n            self.tb_writer = SummaryWriter(os.path.dirname(log_prepath), filename_suffix=os.path.basename(log_prepath))\n            self.tb_actions = []  # store actions for tensorboard\n            logger.info(f'Using TensorBoard logging for dev mode. Run `tensorboard --logdir={log_prepath}` to start TensorBoard.')\n\n        trial_index = self.spec['meta']['trial']\n        session_index = self.spec['meta']['session']\n        if session_index != 0:  # log only session 0\n            return\n        idx_suffix = f'trial{trial_index}_session{session_index}'\n        frame = self.env.clock.frame\n        # add main graph\n        if False and self.env.clock.frame == 0 and hasattr(self.agent.algorithm, 'net'):\n            # can only log 1 net to tb now, and 8 is a good common length for stacked and rnn inputs\n            net = self.agent.algorithm.net\n            self.tb_writer.add_graph(net, torch.rand(ps.flatten([8, net.in_dim])))\n        # add summary variables\n        last_row = self.train_df.iloc[-1]\n        for k, v in last_row.items():\n            self.tb_writer.add_scalar(f'{k}/{idx_suffix}', v, frame)\n        # add network parameters\n        for net_name in self.agent.algorithm.net_names:\n            if net_name.startswith('global_') or net_name.startswith('target_'):\n                continue\n            net = getattr(self.agent.algorithm, net_name)\n            for name, params in net.named_parameters():\n                self.tb_writer.add_histogram(f'{net_name}.{name}/{idx_suffix}', params, frame)\n        # add action histogram and flush\n        if not ps.is_empty(self.tb_actions):\n            actions = np.array(self.tb_actions)\n            if len(actions.shape) == 1:\n                self.tb_writer.add_histogram(f'action/{idx_suffix}', actions, frame)\n            else:  # multi-action\n                for idx, subactions in enumerate(actions.T):\n                    self.tb_writer.add_histogram(f'action.{idx}/{idx_suffix}', subactions, frame)\n            self.tb_actions = []\n\n    def track_tensorboard(self, action):\n        '''Helper to track variables for tensorboard logging'''\n        if self.env.is_venv:\n            self.tb_actions.extend(action.tolist())\n        else:\n            self.tb_actions.append(action)\n"""
slm_lab/env/__init__.py,0,b'# the environment module\n\n\ndef make_env(spec):\n    from slm_lab.env.openai import OpenAIEnv\n    env = OpenAIEnv(spec)\n    return env\n'
slm_lab/env/base.py,0,"b'from abc import ABC, abstractmethod\nfrom gym import spaces\nfrom slm_lab.lib import logger, util\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\nimport pydash as ps\nimport time\n\nlogger = logger.get_logger(__name__)\n\n\ndef set_gym_space_attr(gym_space):\n    \'\'\'Set missing gym space attributes for standardization\'\'\'\n    if isinstance(gym_space, spaces.Box):\n        setattr(gym_space, \'is_discrete\', False)\n    elif isinstance(gym_space, spaces.Discrete):\n        setattr(gym_space, \'is_discrete\', True)\n        setattr(gym_space, \'low\', 0)\n        setattr(gym_space, \'high\', gym_space.n)\n    elif isinstance(gym_space, spaces.MultiBinary):\n        setattr(gym_space, \'is_discrete\', True)\n        setattr(gym_space, \'low\', np.full(gym_space.n, 0))\n        setattr(gym_space, \'high\', np.full(gym_space.n, 2))\n    elif isinstance(gym_space, spaces.MultiDiscrete):\n        setattr(gym_space, \'is_discrete\', True)\n        setattr(gym_space, \'low\', np.zeros_like(gym_space.nvec))\n        setattr(gym_space, \'high\', np.array(gym_space.nvec))\n    else:\n        raise ValueError(\'gym_space not recognized\')\n\n\nclass Clock:\n    \'\'\'Clock class for each env and space to keep track of relative time. Ticking and control loop is such that reset is at t=0 and epi=0\'\'\'\n\n    def __init__(self, max_frame=int(1e7), clock_speed=1):\n        self.max_frame = max_frame\n        self.clock_speed = int(clock_speed)\n        self.reset()\n\n    def reset(self):\n        self.t = 0\n        self.frame = 0  # i.e. total_t\n        self.epi = 0\n        self.start_wall_t = time.time()\n        self.wall_t = 0\n        self.batch_size = 1  # multiplier to accurately count opt steps\n        self.opt_step = 0  # count the number of optimizer updates\n\n    def load(self, train_df):\n        \'\'\'Load clock from the last row of body.train_df\'\'\'\n        last_row = train_df.iloc[-1]\n        last_clock_vals = ps.pick(last_row, *[\'epi\', \'t\', \'wall_t\', \'opt_step\', \'frame\'])\n        util.set_attr(self, last_clock_vals)\n        self.start_wall_t -= self.wall_t  # offset elapsed wall_t\n\n    def get(self, unit=\'frame\'):\n        return getattr(self, unit)\n\n    def get_elapsed_wall_t(self):\n        \'\'\'Calculate the elapsed wall time (int seconds) since self.start_wall_t\'\'\'\n        return int(time.time() - self.start_wall_t)\n\n    def set_batch_size(self, batch_size):\n        self.batch_size = batch_size\n\n    def tick(self, unit=\'t\'):\n        if unit == \'t\':  # timestep\n            self.t += self.clock_speed\n            self.frame += self.clock_speed\n            self.wall_t = self.get_elapsed_wall_t()\n        elif unit == \'epi\':  # episode, reset timestep\n            self.epi += 1\n            self.t = 0\n        elif unit == \'opt_step\':\n            self.opt_step += self.batch_size\n        else:\n            raise KeyError\n\n\nclass BaseEnv(ABC):\n    \'\'\'\n    The base Env class with API and helper methods. Use this to implement your env class that is compatible with the Lab APIs\n\n    e.g. env_spec\n    ""env"": [{\n        ""name"": ""PongNoFrameskip-v4"",\n        ""frame_op"": ""concat"",\n        ""frame_op_len"": 4,\n        ""normalize_state"": false,\n        ""reward_scale"": ""sign"",\n        ""num_envs"": 8,\n        ""max_t"": null,\n        ""max_frame"": 1e7\n    }],\n    \'\'\'\n\n    def __init__(self, spec):\n        self.env_spec = spec[\'env\'][0]  # idx 0 for single-env\n        # set default\n        util.set_attr(self, dict(\n            eval_frequency=10000,\n            log_frequency=10000,\n            frame_op=None,\n            frame_op_len=None,\n            image_downsize=(84, 84),\n            normalize_state=False,\n            reward_scale=None,\n            num_envs=1,\n        ))\n        util.set_attr(self, spec[\'meta\'], [\n            \'eval_frequency\',\n            \'log_frequency\',\n        ])\n        util.set_attr(self, self.env_spec, [\n            \'name\',\n            \'frame_op\',\n            \'frame_op_len\',\n            \'image_downsize\',\n            \'normalize_state\',\n            \'reward_scale\',\n            \'num_envs\',\n            \'max_t\',\n            \'max_frame\',\n        ])\n        if util.get_lab_mode() == \'eval\':  # override if env is for eval\n            self.num_envs = ps.get(spec, \'meta.rigorous_eval\')\n        self.to_render = util.to_render()\n        self._infer_frame_attr(spec)\n        self._infer_venv_attr()\n        self._set_clock()\n        self.done = False\n        self.total_reward = np.nan\n\n    def _get_spaces(self, u_env):\n        \'\'\'Helper to set the extra attributes to, and get, observation and action spaces\'\'\'\n        observation_space = u_env.observation_space\n        action_space = u_env.action_space\n        set_gym_space_attr(observation_space)\n        set_gym_space_attr(action_space)\n        return observation_space, action_space\n\n    def _get_observable_dim(self, observation_space):\n        \'\'\'Get the observable dim for an agent in env\'\'\'\n        state_dim = observation_space.shape\n        if len(state_dim) == 1:\n            state_dim = state_dim[0]\n        return {\'state\': state_dim}\n\n    def _get_action_dim(self, action_space):\n        \'\'\'Get the action dim for an action_space for agent to use\'\'\'\n        if isinstance(action_space, spaces.Box):\n            assert len(action_space.shape) == 1\n            action_dim = action_space.shape[0]\n        elif isinstance(action_space, (spaces.Discrete, spaces.MultiBinary)):\n            action_dim = action_space.n\n        elif isinstance(action_space, spaces.MultiDiscrete):\n            action_dim = action_space.nvec.tolist()\n        else:\n            raise ValueError(\'action_space not recognized\')\n        return action_dim\n\n    def _infer_frame_attr(self, spec):\n        \'\'\'Infer frame attributes\'\'\'\n        seq_len = ps.get(spec, \'agent.0.net.seq_len\')\n        if seq_len is not None:  # infer if using RNN\n            self.frame_op = \'stack\'\n            self.frame_op_len = seq_len\n        if spec[\'meta\'][\'distributed\'] != False:  # divide max_frame for distributed\n            self.max_frame = int(self.max_frame / spec[\'meta\'][\'max_session\'])\n\n    def _infer_venv_attr(self):\n        \'\'\'Infer vectorized env attributes\'\'\'\n        self.is_venv = (self.num_envs is not None and self.num_envs > 1)\n\n    def _is_discrete(self, action_space):\n        \'\'\'Check if an action space is discrete\'\'\'\n        return util.get_class_name(action_space) != \'Box\'\n\n    def _set_clock(self):\n        self.clock_speed = 1 * (self.num_envs or 1)  # tick with a multiple of num_envs to properly count frames\n        self.clock = Clock(self.max_frame, self.clock_speed)\n\n    def _set_attr_from_u_env(self, u_env):\n        \'\'\'Set the observation, action dimensions and action type from u_env\'\'\'\n        self.observation_space, self.action_space = self._get_spaces(u_env)\n        self.observable_dim = self._get_observable_dim(self.observation_space)\n        self.action_dim = self._get_action_dim(self.action_space)\n        self.is_discrete = self._is_discrete(self.action_space)\n\n    def _update_total_reward(self, info):\n        \'\'\'Extract total_reward from info (set in wrapper) into self.total_reward for single and vec env\'\'\'\n        if isinstance(info, dict):\n            self.total_reward = info[\'total_reward\']\n        else:  # vec env tuple of infos\n            self.total_reward = np.array([i[\'total_reward\'] for i in info])\n\n    @abstractmethod\n    @lab_api\n    def reset(self):\n        \'\'\'Reset method, return state\'\'\'\n        raise NotImplementedError\n\n    @abstractmethod\n    @lab_api\n    def step(self, action):\n        \'\'\'Step method, return state, reward, done, info\'\'\'\n        raise NotImplementedError\n\n    @abstractmethod\n    @lab_api\n    def close(self):\n        \'\'\'Method to close and cleanup env\'\'\'\n        raise NotImplementedError\n'"
slm_lab/env/openai.py,0,"b'from slm_lab.env.base import BaseEnv\nfrom slm_lab.env.wrapper import make_gym_env\nfrom slm_lab.env.vec_env import make_gym_venv\nfrom slm_lab.env.registration import try_register_env\nfrom slm_lab.lib import logger, util\nfrom slm_lab.lib.decorator import lab_api\nimport gym\nimport numpy as np\nimport pydash as ps\nimport roboschool\n\n\nlogger = logger.get_logger(__name__)\n\n\nclass OpenAIEnv(BaseEnv):\n    \'\'\'\n    Wrapper for OpenAI Gym env to work with the Lab.\n\n    e.g. env_spec\n    ""env"": [{\n        ""name"": ""PongNoFrameskip-v4"",\n        ""frame_op"": ""concat"",\n        ""frame_op_len"": 4,\n        ""normalize_state"": false,\n        ""reward_scale"": ""sign"",\n        ""num_envs"": 8,\n        ""max_t"": null,\n        ""max_frame"": 1e7\n    }],\n    \'\'\'\n\n    def __init__(self, spec):\n        super().__init__(spec)\n        try_register_env(spec)  # register if it\'s a custom gym env\n        seed = ps.get(spec, \'meta.random_seed\')\n        episode_life = util.in_train_lab_mode()\n        if self.is_venv:  # make vector environment\n            self.u_env = make_gym_venv(name=self.name, num_envs=self.num_envs, seed=seed, frame_op=self.frame_op, frame_op_len=self.frame_op_len, image_downsize=self.image_downsize, reward_scale=self.reward_scale, normalize_state=self.normalize_state, episode_life=episode_life)\n        else:\n            self.u_env = make_gym_env(name=self.name, seed=seed, frame_op=self.frame_op, frame_op_len=self.frame_op_len, image_downsize=self.image_downsize, reward_scale=self.reward_scale, normalize_state=self.normalize_state, episode_life=episode_life)\n        if self.name.startswith(\'Unity\'):\n            # Unity is always initialized as singleton gym env, but the Unity runtime can be vec_env\n            self.num_envs = self.u_env.num_envs\n            # update variables dependent on num_envs\n            self._infer_venv_attr()\n            self._set_clock()\n        self._set_attr_from_u_env(self.u_env)\n        self.max_t = self.max_t or self.u_env.spec.max_episode_steps\n        assert self.max_t is not None\n        logger.info(util.self_desc(self))\n\n    def seed(self, seed):\n        self.u_env.seed(seed)\n\n    @lab_api\n    def reset(self):\n        self.done = False\n        state = self.u_env.reset()\n        if self.to_render:\n            self.u_env.render()\n        return state\n\n    @lab_api\n    def step(self, action):\n        if not self.is_discrete and self.action_dim == 1:  # guard for continuous with action_dim 1, make array\n            action = np.expand_dims(action, axis=-1)\n        state, reward, done, info = self.u_env.step(action)\n        self._update_total_reward(info)\n        if self.to_render:\n            self.u_env.render()\n        if not self.is_venv and self.clock.t > self.max_t:\n            done = True\n        self.done = done\n        return state, reward, done, info\n\n    @lab_api\n    def close(self):\n        self.u_env.close()\n'"
slm_lab/env/registration.py,0,"b""# module to register and mange multiple environment offerings\nfrom gym.envs.registration import register\nfrom slm_lab.lib import logger, util\nimport gym\nimport os\n\n\ndef get_env_path(env_name):\n    '''Get the path to Unity env binaries distributed via npm'''\n    env_path = util.smart_path(f'slm_lab/env/SLM-Env/build/{env_name}')\n    env_dir = os.path.dirname(env_path)\n    assert os.path.exists(env_dir), f'Missing {env_path}. See README to install from yarn.'\n    return env_path\n\n\ndef try_register_env(spec):\n    '''Try to additional environments for OpenAI gym.'''\n    try:\n        env_name = spec['env'][0]['name']\n        if env_name == 'vizdoom-v0':\n            assert 'cfg_name' in spec['env'][0].keys(), 'Environment config name must be defined for vizdoom.'\n            cfg_name = spec['env'][0]['cfg_name']\n            register(\n                id=env_name,\n                entry_point='slm_lab.env.vizdoom.vizdoom_env:VizDoomEnv',\n                kwargs={'cfg_name': cfg_name})\n        elif env_name.startswith('Unity'):\n            # NOTE: do not specify max_episode_steps, will cause shape inconsistency in done\n            register(\n                id=env_name,\n                entry_point='slm_lab.env.unity:GymUnityEnv',\n                kwargs={'name': env_name})\n    except Exception as e:\n        logger.exception(e)\n"""
slm_lab/env/unity.py,0,"b""from gym_unity.envs import UnityEnv\nfrom slm_lab.env.registration import get_env_path\nfrom slm_lab.lib import util\nimport numpy as np\nimport os\nimport pydash as ps\n\n# NOTE: stack-frames used in ml-agents:\n# 3DBallHard 9\n# Hallways 3\n# PushBlock 3\n# Walker 5\n\n\nclass GymUnityEnv(UnityEnv):\n    '''Wrapper to make UnityEnv register-able under gym'''\n    spec = None\n\n    def __init__(self, name):\n        worker_id = int(f'{os.getpid()}{int(ps.unique_id())}'[-4:])\n        super().__init__(get_env_path(name), worker_id, no_graphics=not util.to_render(), multiagent=True)\n        self.num_envs = self.number_agents\n\n    def reset(self):\n        state = super().reset()\n        # Unity returns list, we need array\n        return np.array(state)\n\n    def step(self, action):\n        # Unity wants list instead of numpy\n        action = list(action)\n        state, reward, done, info = super().step(action)\n        # Unity returns list, we need array\n        state = np.array(state)\n        reward = np.array(reward)\n        done = np.array(done)\n        return state, reward, done, info\n\n    def close(self):\n        try:  # guard repeated call to close()\n            super().close()\n        except Exception as e:\n            pass\n"""
slm_lab/env/vec_env.py,1,"b""# Wrappers for parallel vector environments.\n# Adapted from OpenAI Baselines (MIT) https://github.com/openai/baselines/tree/master/baselines/common/vec_env\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom functools import partial\nfrom gym import spaces\nfrom slm_lab.env.wrapper import make_gym_env\nfrom slm_lab.lib import logger\nimport contextlib\nimport ctypes\nimport gym\nimport numpy as np\nimport os\nimport torch.multiprocessing as mp\n\n\n_NP_TO_CT = {\n    np.float32: ctypes.c_float,\n    np.int32: ctypes.c_int32,\n    np.int8: ctypes.c_int8,\n    np.uint8: ctypes.c_char,\n    np.bool: ctypes.c_bool,\n}\n\n\n# helper methods\n\n@contextlib.contextmanager\ndef clear_mpi_env_vars():\n    '''\n    from mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\n    This context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing Processes.\n    '''\n    removed_environment = {}\n    for k, v in list(os.environ.items()):\n        for prefix in ['OMPI_', 'PMI_']:\n            if k.startswith(prefix):\n                removed_environment[k] = v\n                del os.environ[k]\n    try:\n        yield\n    finally:\n        os.environ.update(removed_environment)\n\n\ndef copy_obs_dict(obs):\n    '''Deep-copy an observation dict.'''\n    return {k: np.copy(v) for k, v in obs.items()}\n\n\ndef dict_to_obs(obs_dict):\n    '''Convert an observation dict into a raw array if the original observation space was not a Dict space.'''\n    if set(obs_dict.keys()) == {None}:\n        return obs_dict[None]\n    return obs_dict\n\n\ndef obs_to_dict(obs):\n    '''Convert an observation into a dict.'''\n    if isinstance(obs, dict):\n        return obs\n    return {None: obs}\n\n\ndef obs_space_info(obs_space):\n    '''\n    Get dict-structured information about a gym.Space.\n    @returns (keys, shapes, dtypes)\n    - keys: a list of dict keys.\n    - shapes: a dict mapping keys to shapes.\n    - dtypes: a dict mapping keys to dtypes.\n    '''\n    if isinstance(obs_space, gym.spaces.Dict):\n        assert isinstance(obs_space.spaces, OrderedDict)\n        subspaces = obs_space.spaces\n    else:\n        subspaces = {None: obs_space}\n    keys = []\n    shapes = {}\n    dtypes = {}\n    for key, box in subspaces.items():\n        keys.append(key)\n        shapes[key] = box.shape\n        dtypes[key] = box.dtype\n    return keys, shapes, dtypes\n\n\ndef tile_images(img_nhwc):\n    '''\n    Tile N images into a rectangular grid for rendering\n\n    @param img_nhwc list or array of images, with shape (batch, h, w, c)\n    @returns bigim_HWc ndarray with shape (h',w',c)\n    '''\n    img_nhwc = np.asarray(img_nhwc)\n    N, h, w, c = img_nhwc.shape\n    H = int(np.ceil(np.sqrt(N)))\n    W = int(np.ceil(float(N) / H))\n    img_nhwc = np.array(list(img_nhwc) + [img_nhwc[0] * 0 for _ in range(N, H * W)])\n    img_HWhwc = img_nhwc.reshape(H, W, h, w, c)\n    img_HhWwc = img_HWhwc.transpose(0, 2, 1, 3, 4)\n    img_Hh_Ww_c = img_HhWwc.reshape(H * h, W * w, c)\n    return img_Hh_Ww_c\n\n\ndef subproc_worker(\n        pipe, parent_pipe, env_fn_wrapper,\n        obs_bufs, obs_shapes, obs_dtypes, keys):\n    '''\n    Control a single environment instance using IPC and shared memory. Used by ShmemVecEnv.\n    '''\n    def _write_obs(maybe_dict_obs):\n        flatdict = obs_to_dict(maybe_dict_obs)\n        for k in keys:\n            dst = obs_bufs[k].get_obj()\n            dst_np = np.frombuffer(dst, dtype=obs_dtypes[k]).reshape(obs_shapes[k])\n            np.copyto(dst_np, flatdict[k])\n\n    env = env_fn_wrapper.x()\n    parent_pipe.close()\n    try:\n        while True:\n            cmd, data = pipe.recv()\n            if cmd == 'reset':\n                pipe.send(_write_obs(env.reset()))\n            elif cmd == 'step':\n                obs, reward, done, info = env.step(data)\n                if done:\n                    obs = env.reset()\n                pipe.send((_write_obs(obs), reward, done, info))\n            elif cmd == 'render':\n                pipe.send(env.render(mode='rgb_array'))\n            elif cmd == 'close':\n                pipe.send(None)\n                break\n            else:\n                raise RuntimeError(f'Got unrecognized cmd {cmd}')\n    except KeyboardInterrupt:\n        logger.exception('ShmemVecEnv worker: got KeyboardInterrupt')\n    finally:\n        env.close()\n\n\n# vector environment wrappers\n\n\nclass CloudpickleWrapper(object):\n    '''\n    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n    '''\n\n    def __init__(self, x):\n        self.x = x\n\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n\n\nclass VecEnv(ABC):\n    '''\n    An abstract asynchronous, vectorized environment.\n    Used to batch data from multiple copies of an environment, so that each observation becomes an batch of observations, and expected action is a batch of actions to be applied per-environment.\n    '''\n    closed = False\n    viewer = None\n\n    metadata = {\n        'render.modes': ['human', 'rgb_array']\n    }\n\n    def __init__(self, num_envs, observation_space, action_space):\n        self.num_envs = num_envs\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    @abstractmethod\n    def reset(self):\n        '''\n        Reset all the environments and return an array of observations, or a dict of observation arrays.\n\n        If step_async is still doing work, that work will be cancelled and step_wait() should not be called until step_async() is invoked again.\n        '''\n        pass\n\n    @abstractmethod\n    def step_async(self, actions):\n        '''\n        Tell all the environments to start taking a step with the given actions.\n        Call step_wait() to get the results of the step.\n\n        You should not call this if a step_async run is already pending.\n        '''\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        '''\n        Wait for the step taken with step_async().\n\n        @returns (obs, rews, dones, infos)\n         - obs: an array of observations, or a dict of arrays of observations.\n         - rews: an array of rewards\n         - dones: an array of 'episode done' booleans\n         - infos: a sequence of info objects\n        '''\n        pass\n\n    def close_extras(self):\n        '''\n        Clean up the extra resources, beyond what's in this base class.\n        Only runs when not self.closed.\n        '''\n        pass\n\n    def close(self):\n        if self.closed:\n            return\n        if self.viewer is not None:\n            self.viewer.close()\n        self.close_extras()\n        self.closed = True\n\n    def step(self, actions):\n        '''\n        Step the environments synchronously.\n\n        This is available for backwards compatibility.\n        '''\n        self.step_async(actions)\n        return self.step_wait()\n\n    def render(self, mode='human'):\n        imgs = self.get_images()\n        bigimg = tile_images(imgs)\n        if mode == 'human':\n            self.get_viewer().imshow(bigimg)\n            return self.get_viewer().isopen\n        elif mode == 'rgb_array':\n            return bigimg\n        else:\n            raise NotImplementedError\n\n    def get_images(self):\n        '''Return RGB images from each environment'''\n        raise NotImplementedError\n\n    @property\n    def unwrapped(self):\n        if isinstance(self, VecEnvWrapper):\n            return self.venv.unwrapped\n        else:\n            return self\n\n    def get_viewer(self):\n        if self.viewer is None:\n            from gym.envs.classic_control import rendering\n            self.viewer = rendering.SimpleImageViewer()\n        return self.viewer\n\n\nclass DummyVecEnv(VecEnv):\n    '''\n    VecEnv that does runs multiple environments sequentially, that is, the step and reset commands are send to one environment at a time.\n    Useful when debugging and when num_envs == 1 (in the latter case, avoids communication overhead)\n    '''\n\n    def __init__(self, env_fns):\n        '''\n        @param env_fns iterable of functions that build environments\n        '''\n        self.envs = [fn() for fn in env_fns]\n        env = self.envs[0]\n        VecEnv.__init__(self, len(env_fns), env.observation_space, env.action_space)\n        obs_space = env.observation_space\n        self.keys, shapes, dtypes = obs_space_info(obs_space)\n\n        self.buf_obs = {k: np.zeros((self.num_envs,) + tuple(shapes[k]), dtype=dtypes[k]) for k in self.keys}\n        self.buf_dones = np.zeros((self.num_envs,), dtype=np.bool)\n        self.buf_rews = np.zeros((self.num_envs,), dtype=np.float32)\n        self.buf_infos = [{} for _ in range(self.num_envs)]\n        self.actions = None\n        self.spec = self.envs[0].spec\n\n    def step_async(self, actions):\n        listify = True\n        try:\n            if len(actions) == self.num_envs:\n                listify = False\n        except TypeError:\n            pass\n\n        if not listify:\n            self.actions = actions\n        else:\n            assert self.num_envs == 1, f'actions {actions} is either not a list or has a wrong size - cannot match to {self.num_envs} environments'\n            self.actions = [actions]\n\n    def step_wait(self):\n        for e in range(self.num_envs):\n            action = self.actions[e]\n\n            obs, self.buf_rews[e], self.buf_dones[e], self.buf_infos[e] = self.envs[e].step(action)\n            if self.buf_dones[e]:\n                obs = self.envs[e].reset()\n            self._save_obs(e, obs)\n        return (self._obs_from_buf(), np.copy(self.buf_rews), np.copy(self.buf_dones),\n                self.buf_infos.copy())\n\n    def reset(self):\n        for e in range(self.num_envs):\n            obs = self.envs[e].reset()\n            self._save_obs(e, obs)\n        return self._obs_from_buf()\n\n    def _save_obs(self, e, obs):\n        for k in self.keys:\n            if k is None:\n                self.buf_obs[k][e] = obs\n            else:\n                self.buf_obs[k][e] = obs[k]\n\n    def _obs_from_buf(self):\n        return dict_to_obs(copy_obs_dict(self.buf_obs))\n\n    def get_images(self):\n        return [env.render(mode='rgb_array') for env in self.envs]\n\n    def render(self, mode='human'):\n        if self.num_envs == 1:\n            return self.envs[0].render(mode=mode)\n        else:\n            return super().render(mode=mode)\n\n\nclass VecEnvWrapper(VecEnv):\n    '''\n    An environment wrapper that applies to an entire batch of environments at once.\n    '''\n\n    def __init__(self, venv, observation_space=None, action_space=None):\n        self.venv = venv\n        observation_space = observation_space or venv.observation_space\n        action_space = action_space or venv.action_space\n        VecEnv.__init__(self, venv.num_envs, observation_space, action_space)\n\n    def step_async(self, actions):\n        self.venv.step_async(actions)\n\n    @abstractmethod\n    def reset(self):\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        pass\n\n    def close(self):\n        return self.venv.close()\n\n    def render(self, mode='human'):\n        return self.venv.render(mode=mode)\n\n    def get_images(self):\n        return self.venv.get_images()\n\n\nclass ShmemVecEnv(VecEnv):\n    '''\n    Optimized version of SubprocVecEnv that uses shared variables to communicate observations.\n    '''\n\n    def __init__(self, env_fns, context='spawn'):\n        ctx = mp.get_context(context)\n        dummy = env_fns[0]()\n        observation_space, action_space = dummy.observation_space, dummy.action_space\n        self.spec = dummy.spec\n        dummy.close()\n        del dummy\n        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n        self.obs_keys, self.obs_shapes, self.obs_dtypes = obs_space_info(observation_space)\n        self.obs_bufs = [\n            {k: ctx.Array(_NP_TO_CT[self.obs_dtypes[k].type], int(np.prod(self.obs_shapes[k]))) for k in self.obs_keys}\n            for _ in env_fns]\n        self.parent_pipes = []\n        self.procs = []\n        with clear_mpi_env_vars():\n            for env_fn, obs_buf in zip(env_fns, self.obs_bufs):\n                wrapped_fn = CloudpickleWrapper(env_fn)\n                parent_pipe, child_pipe = ctx.Pipe()\n                proc = ctx.Process(\n                    target=subproc_worker,\n                    args=(child_pipe, parent_pipe, wrapped_fn, obs_buf, self.obs_shapes, self.obs_dtypes, self.obs_keys))\n                proc.daemon = True\n                self.procs.append(proc)\n                self.parent_pipes.append(parent_pipe)\n                proc.start()\n                child_pipe.close()\n        self.waiting_step = False\n        self.viewer = None\n\n    def reset(self):\n        if self.waiting_step:\n            logger.warning('Called reset() while waiting for the step to complete')\n            self.step_wait()\n        for pipe in self.parent_pipes:\n            pipe.send(('reset', None))\n        return self._decode_obses([pipe.recv() for pipe in self.parent_pipes])\n\n    def step_async(self, actions):\n        assert len(actions) == len(self.parent_pipes)\n        for pipe, act in zip(self.parent_pipes, actions):\n            pipe.send(('step', act))\n\n    def step_wait(self):\n        outs = [pipe.recv() for pipe in self.parent_pipes]\n        obs, rews, dones, infos = zip(*outs)\n        return self._decode_obses(obs), np.array(rews), np.array(dones), infos\n\n    def close_extras(self):\n        if self.waiting_step:\n            self.step_wait()\n        for pipe in self.parent_pipes:\n            pipe.send(('close', None))\n        for pipe in self.parent_pipes:\n            pipe.recv()\n            pipe.close()\n        for proc in self.procs:\n            proc.join()\n\n    def get_images(self, mode='human'):\n        for pipe in self.parent_pipes:\n            pipe.send(('render', None))\n        return [pipe.recv() for pipe in self.parent_pipes]\n\n    def _decode_obses(self, obs):\n        result = {}\n        for k in self.obs_keys:\n            bufs = [b[k] for b in self.obs_bufs]\n            o = [np.frombuffer(b.get_obj(), dtype=self.obs_dtypes[k]).reshape(self.obs_shapes[k]) for b in bufs]\n            result[k] = np.array(o)\n        return dict_to_obs(result)\n\n\nclass VecFrameStack(VecEnvWrapper):\n    '''Frame stack wrapper for vector environment'''\n\n    def __init__(self, venv, frame_op, frame_op_len):\n        self.venv = venv\n        assert frame_op in ('concat', 'stack'), 'Invalid frame_op mode'\n        self.is_stack = frame_op == 'stack'\n        self.frame_op_len = frame_op_len\n        self.spec = venv.spec\n        wos = venv.observation_space  # wrapped ob space\n        if self.is_stack:\n            self.shape_dim0 = 1\n            low = np.repeat(np.expand_dims(wos.low, axis=0), self.frame_op_len, axis=0)\n            high = np.repeat(np.expand_dims(wos.high, axis=0), self.frame_op_len, axis=0)\n        else:  # concat\n            self.shape_dim0 = wos.shape[0]\n            low = np.repeat(wos.low, self.frame_op_len, axis=0)\n            high = np.repeat(wos.high, self.frame_op_len, axis=0)\n        self.stackedobs = np.zeros((venv.num_envs,) + low.shape, low.dtype)\n        observation_space = spaces.Box(low=low, high=high, dtype=venv.observation_space.dtype)\n        VecEnvWrapper.__init__(self, venv, observation_space=observation_space)\n\n    def step_wait(self):\n        obs, rews, news, infos = self.venv.step_wait()\n        self.stackedobs[:, :-self.shape_dim0] = self.stackedobs[:, self.shape_dim0:]\n        for (i, new) in enumerate(news):\n            if new:\n                self.stackedobs[i] = 0\n        if self.is_stack:\n            obs = np.expand_dims(obs, axis=1)\n        self.stackedobs[:, -self.shape_dim0:] = obs\n        return self.stackedobs.copy(), rews, news, infos\n\n    def reset(self):\n        obs = self.venv.reset()\n        self.stackedobs[...] = 0\n        if self.is_stack:\n            obs = np.expand_dims(obs, axis=1)\n        self.stackedobs[:, -self.shape_dim0:] = obs\n        return self.stackedobs.copy()\n\n\ndef make_gym_venv(name, num_envs=4, seed=0, frame_op=None, frame_op_len=None, image_downsize=None, reward_scale=None, normalize_state=False, episode_life=True):\n    '''General method to create any parallel vectorized Gym env; auto wraps Atari'''\n    venv = [\n        # don't concat frame or clip reward on individual env; do that at vector level\n        partial(make_gym_env, name, seed + i, frame_op=None, frame_op_len=None, image_downsize=image_downsize, reward_scale=reward_scale, normalize_state=normalize_state, episode_life=episode_life)\n        for i in range(num_envs)\n    ]\n    if len(venv) > 1:\n        venv = ShmemVecEnv(venv, context='fork')\n    else:\n        venv = DummyVecEnv(venv)\n    if frame_op is not None:\n        venv = VecFrameStack(venv, frame_op, frame_op_len)\n    return venv\n"""
slm_lab/env/wrapper.py,0,"b'# Generic env wrappers, including for Atari/images\n# They don\'t come with Gym but are crucial for Atari to work\n# Many were adapted from OpenAI Baselines (MIT) https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\nfrom collections import deque\nfrom gym import spaces\nfrom slm_lab.lib import util\nimport gym\nimport numpy as np\n\n\ndef try_scale_reward(cls, reward):\n    \'\'\'Env class to scale reward\'\'\'\n    if util.in_eval_lab_mode():  # only trigger on training\n        return reward\n    if cls.reward_scale is not None:\n        if cls.sign_reward:\n            reward = np.sign(reward)\n        else:\n            reward *= cls.reward_scale\n    return reward\n\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        \'\'\'\n        Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        \'\'\'\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n    def reset(self, **kwargs):\n        \'\'\'Do no-op action for a number of steps in [1, noop_max].\'\'\'\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    \'\'\'OpenAI max-skipframe wrapper used for a NoFrameskip env\'\'\'\n\n    def __init__(self, env, skip=4):\n        \'\'\'Return only every `skip`-th frame\'\'\'\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n        self._skip = skip\n\n    def step(self, action):\n        \'\'\'Repeat action, sum reward, and max over last observations.\'\'\'\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2:\n                self._obs_buffer[0] = obs\n            if i == self._skip - 1:\n                self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        \'\'\'\n        Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        \'\'\'\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = info[\'was_real_done\'] = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n            # so its important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        \'\'\'\n        Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        \'\'\'\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        \'\'\'Take action on reset for environments that are fixed until firing.\'\'\'\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass PreprocessImage(gym.ObservationWrapper):\n    def __init__(self, env, w_h):\n        \'\'\'\n        Apply image preprocessing:\n        - grayscale\n        - downsize to 84x84\n        - transpose shape from h,w,c to PyTorch format c,h,w\n        \'\'\'\n        gym.ObservationWrapper.__init__(self, env)\n        w_h = w_h or (84, 84)\n        self.width, self.height = w_h\n        self.observation_space = spaces.Box(\n            low=0, high=255, shape=(1, self.width, self.height), dtype=np.uint8)\n\n    def observation(self, frame):\n        return util.preprocess_image(frame, (self.width, self.height))\n\n\nclass LazyFrames(object):\n    def __init__(self, frames, frame_op=\'stack\'):\n        \'\'\'\n        Wrapper to stack or concat frames by keeping unique soft reference insted of copies of data.\n        So this should only be converted to numpy array before being passed to the model.\n        It exists purely to optimize memory usage which can be huge for DQN\'s 1M frames replay buffers.\n        @param str:frame_op \'stack\' or \'concat\'\n        \'\'\'\n        self._frames = frames\n        self._out = None\n        if frame_op == \'stack\':\n            self._frame_op = np.stack\n        elif frame_op == \'concat\':\n            self._frame_op = np.concatenate\n        else:\n            raise ValueError(\'frame_op not recognized for LazyFrames. Choose from ""stack"", ""concat""\')\n\n    def _force(self):\n        if self._out is None:\n            self._out = self._frame_op(self._frames, axis=0)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[i]\n\n    def astype(self, dtype):\n        \'\'\'To prevent state.astype(np.float16) breaking on LazyFrames\'\'\'\n        return self\n\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, frame_op, frame_op_len):\n        \'\'\'\n        Stack/concat last k frames. Returns lazy array, which is much more memory efficient.\n        @param str:frame_op \'concat\' or \'stack\'. Note: use concat for image since the shape is (1, 84, 84) concat-able.\n        @param int:frame_op_len The number of frames to keep for frame_op\n        \'\'\'\n        gym.Wrapper.__init__(self, env)\n        self.frame_op = frame_op\n        self.frame_op_len = frame_op_len\n        self.frames = deque([], maxlen=self.frame_op_len)\n        old_shape = env.observation_space.shape\n        if self.frame_op == \'concat\':  # concat multiplies first dim\n            shape = (self.frame_op_len * old_shape[0],) + old_shape[1:]\n        elif self.frame_op == \'stack\':  # stack creates new dim\n            shape = (self.frame_op_len,) + old_shape\n        else:\n            raise ValueError(\'frame_op not recognized for FrameStack. Choose from ""stack"", ""concat"".\')\n        self.observation_space = spaces.Box(\n            low=np.min(env.observation_space.low),\n            high=np.max(env.observation_space.high),\n            shape=shape, dtype=env.observation_space.dtype)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.frame_op_len):\n            self.frames.append(ob.astype(np.float16))\n        return self._get_ob()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob.astype(np.float16))\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.frame_op_len\n        return LazyFrames(list(self.frames), self.frame_op)\n\n\nclass UnityVecFrameStack(gym.Wrapper):\n    \'\'\'Frame stack wrapper for Unity vector environment\'\'\'\n\n    def __init__(self, env, frame_op, frame_op_len):\n        self.env = env\n        assert frame_op in (\'concat\', \'stack\'), \'Invalid frame_op mode\'\n        self.is_stack = frame_op == \'stack\'\n        self.frame_op_len = frame_op_len\n        self.spec = env.spec\n        wos = env.observation_space  # wrapped ob space\n        if self.is_stack:\n            self.shape_dim0 = 1\n            low = np.repeat(np.expand_dims(wos.low, axis=0), self.frame_op_len, axis=0)\n            high = np.repeat(np.expand_dims(wos.high, axis=0), self.frame_op_len, axis=0)\n        else:  # concat\n            self.shape_dim0 = wos.shape[0]\n            low = np.repeat(wos.low, self.frame_op_len, axis=0)\n            high = np.repeat(wos.high, self.frame_op_len, axis=0)\n        self.stackedobs = np.zeros((env.num_envs,) + low.shape, low.dtype)\n        self.observation_space = spaces.Box(low=low, high=high, dtype=env.observation_space.dtype)\n        self.action_space = env.action_space\n\n    def step(self, action):\n        obs, rews, news, infos = self.env.step(action)\n        self.stackedobs[:, :-self.shape_dim0] = self.stackedobs[:, self.shape_dim0:]\n        for (i, new) in enumerate(news):\n            if new:\n                self.stackedobs[i] = 0\n        if self.is_stack:\n            obs = np.expand_dims(obs, axis=1)\n        self.stackedobs[:, -self.shape_dim0:] = obs\n        return self.stackedobs.copy(), rews, news, infos\n\n    def reset(self):\n        obs = self.env.reset()\n        self.stackedobs[...] = 0\n        if self.is_stack:\n            obs = np.expand_dims(obs, axis=1)\n        self.stackedobs[:, -self.shape_dim0:] = obs\n        return self.stackedobs.copy()\n\n\nclass NormalizeStateEnv(gym.ObservationWrapper):\n    def __init__(self, env=None):\n        \'\'\'\n        Normalize observations on-line\n        Adapted from https://github.com/ikostrikov/pytorch-a3c/blob/e898f7514a03de73a2bf01e7b0f17a6f93963389/envs.py (MIT)\n        \'\'\'\n        super().__init__(env)\n        self.state_mean = 0\n        self.state_std = 0\n        self.alpha = 0.9999\n        self.num_steps = 0\n\n    def observation(self, observation):\n        self.num_steps += 1\n        self.state_mean = self.state_mean * self.alpha + \\\n            observation.mean() * (1 - self.alpha)\n        self.state_std = self.state_std * self.alpha + \\\n            observation.std() * (1 - self.alpha)\n\n        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n\n        return (observation - unbiased_mean) / (unbiased_std + 1e-8)\n\n\nclass ScaleRewardEnv(gym.RewardWrapper):\n    def __init__(self, env, reward_scale):\n        \'\'\'\n        Rescale reward\n        @param (str,float):reward_scale If \'sign\', use np.sign, else multiply with the specified float scale\n        \'\'\'\n        gym.Wrapper.__init__(self, env)\n        self.reward_scale = reward_scale\n        self.sign_reward = self.reward_scale == \'sign\'\n\n    def reward(self, reward):\n        return try_scale_reward(self, reward)\n\n\nclass TrackReward(gym.Wrapper):\n    def __init__(self, env):\n        \'\'\'\n        Self-tracking as a simple solution to total reward tracking\n        Tracks the latest episodic rewards\n        \'\'\'\n        gym.Wrapper.__init__(self, env)\n        self.tracked_reward = 0\n        self.total_reward = np.nan\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.tracked_reward += reward\n        # fix shape by inferring from reward\n        if np.isscalar(self.total_reward) and not np.isscalar(reward):\n            self.total_reward = np.full_like(reward, self.total_reward)\n        # use self.was_real_done from EpisodicLifeEnv, or plain done\n        real_done = info.get(\'was_real_done\', False) or done\n        not_real_done = (1 - real_done)\n        # if isnan and at done, reset total_reward from nan to 0 so it can be updated with tracked_reward\n        if np.isnan(self.total_reward).any():\n            if np.isscalar(self.total_reward):\n                if np.isnan(self.total_reward) and real_done:\n                    self.total_reward = 0.0\n            else:\n                replace_locs = np.logical_and(np.isnan(self.total_reward), real_done)\n                self.total_reward[replace_locs] = 0.0\n        # update total_reward\n        self.total_reward = self.total_reward * not_real_done + self.tracked_reward * real_done\n        # reset to 0 on real_done, i.e. multiply with not_real_done\n        self.tracked_reward = self.tracked_reward * not_real_done\n        info.update({\'total_reward\': self.total_reward})\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        self.tracked_reward = 0\n        return self.env.reset(**kwargs)\n\n\ndef wrap_atari(env):\n    \'\'\'Apply a common set of wrappers for Atari games\'\'\'\n    assert \'NoFrameskip\' in env.spec.id\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    return env\n\n\ndef wrap_deepmind(env, episode_life=True, stack_len=None, image_downsize=None):\n    \'\'\'Wrap Atari environment DeepMind-style\'\'\'\n    if episode_life:\n        env = EpisodicLifeEnv(env)\n    if \'FIRE\' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = PreprocessImage(env, image_downsize)\n    if stack_len is not None:  # use concat for image (1, 84, 84)\n        env = FrameStack(env, \'concat\', stack_len)\n    return env\n\n\ndef make_gym_env(name, seed=None, frame_op=None, frame_op_len=None, image_downsize=None, reward_scale=None, normalize_state=False, episode_life=True):\n    \'\'\'General method to create any Gym env; auto wraps Atari\'\'\'\n    env = gym.make(name)\n    if seed is not None:\n        env.seed(seed)\n    if \'NoFrameskip\' in env.spec.id:  # Atari\n        env = wrap_atari(env)\n        # no reward clipping to allow monitoring; Atari memory clips it\n        env = wrap_deepmind(env, episode_life, frame_op_len, image_downsize)\n    elif len(env.observation_space.shape) == 3:  # image-state env\n        env = PreprocessImage(env, image_downsize)\n        if normalize_state:\n            env = NormalizeStateEnv(env)\n        if frame_op_len is not None:  # use concat for image (1, 84, 84)\n            env = FrameStack(env, \'concat\', frame_op_len)\n    else:  # vector-state env\n        if normalize_state:\n            env = NormalizeStateEnv(env)\n        if frame_op is not None:\n            Stacker = UnityVecFrameStack if name.startswith(\'Unity\') else FrameStack\n            env = Stacker(env, frame_op, frame_op_len)\n    env = TrackReward(env)  # auto-track total reward\n    if reward_scale is not None:\n        env = ScaleRewardEnv(env, reward_scale)\n    return env\n'"
slm_lab/experiment/__init__.py,0,"b'# The experiment module\n# Handles experimentation logic: control, analysis\n'"
slm_lab/experiment/analysis.py,1,"b'from slm_lab.lib import logger, util, viz\nfrom slm_lab.spec import random_baseline\nimport numpy as np\nimport pandas as pd\nimport pydash as ps\nimport shutil\nimport torch\nimport warnings\n\n\nMETRICS_COLS = [\n    \'final_return_ma\',\n    \'strength\', \'max_strength\', \'final_strength\',\n    \'sample_efficiency\', \'training_efficiency\',\n    \'stability\', \'consistency\',\n]\n\nlogger = logger.get_logger(__name__)\n\n\n# methods to generate returns (total rewards)\n\ndef gen_return(agent, env):\n    \'\'\'Generate return for an agent and an env in eval mode. eval_env should be a vec env with NUM_EVAL instances\'\'\'\n    vec_dones = False  # done check for single and vec env\n    # swap ref to allow inference based on body.env\n    body_env = agent.body.env\n    agent.body.env = env\n    # start eval loop\n    state = env.reset()\n    while not np.all(vec_dones):\n        action = agent.act(state)\n        state, reward, done, info = env.step(action)\n        vec_dones = np.logical_or(vec_dones, done)  # wait till every vec slot done turns True\n    agent.body.env = body_env  # restore swapped ref\n    return np.mean(env.total_reward)\n\n\ndef gen_avg_return(agent, env):\n    \'\'\'Generate average return for agent and an env\'\'\'\n    with util.ctx_lab_mode(\'eval\'):  # enter eval context\n        agent.algorithm.update()  # set explore_var etc. to end_val under ctx\n    with torch.no_grad():\n        ret = gen_return(agent, env)\n    # exit eval context, restore variables simply by updating\n    agent.algorithm.update()\n    return ret\n\n\n# metrics calculation methods\n\ndef calc_strength(mean_returns, mean_rand_returns):\n    \'\'\'\n    Calculate strength for metric\n    str &= \\frac{1}{N} \\sum_{i=0}^N \\overline{R}_i - \\overline{R}_{rand}\n    @param Series:mean_returns A series of mean returns from each checkpoint\n    @param float:mean_rand_returns The random baseline\n    @returns float:str, Series:local_strs\n    \'\'\'\n    local_strs = mean_returns - mean_rand_returns\n    str_ = local_strs.mean()\n    return str_, local_strs\n\n\ndef calc_efficiency(local_strs, ts):\n    \'\'\'\n    Calculate efficiency for metric\n    e &= \\frac{\\sum_{i=0}^N \\frac{1}{t_i} str_i}{\\sum_{i=0}^N \\frac{1}{t_i}}\n    @param Series:local_strs A series of local strengths\n    @param Series:ts A series of times units (frame or opt_steps)\n    @returns float:eff, Series:local_effs\n    \'\'\'\n    # drop inf from when first t is 0\n    str_t_ratios = (local_strs / ts).replace([np.inf, -np.inf], np.nan).dropna()\n    eff = str_t_ratios.sum() / local_strs.sum()\n    local_effs = str_t_ratios.cumsum() / local_strs.cumsum()\n    return eff, local_effs\n\n\ndef calc_stability(local_strs):\n    \'\'\'\n    Calculate stability for metric\n    sta &= 1 - \\left| \\frac{\\sum_{i=0}^{N-1} \\min(str_{i+1} - str_i, 0)}{\\sum_{i=0}^{N-1} str_i} \\right|\n    @param Series:local_strs A series of local strengths\n    @returns float:sta, Series:local_stas\n    \'\'\'\n    # shift to keep indices for division\n    drops = local_strs.diff().shift(-1).iloc[:-1].clip(upper=0.0)\n    denoms = local_strs.iloc[:-1]\n    local_stas = 1 - (drops / denoms).abs()\n    sum_drops = drops.sum()\n    sum_denom = denoms.sum()\n    sta = 1 - np.abs(sum_drops / sum_denom)\n    return sta, local_stas\n\n\ndef calc_consistency(local_strs_list):\n    \'\'\'\n    Calculate consistency for metric\n    con &= 1 - \\frac{\\sum_{i=0}^N 2 stdev_j(str_{i,j})}{\\sum_{i=0}^N avg_j(str_{i,j})}\n    @param Series:local_strs_list A list of multiple series of local strengths from different sessions\n    @returns float:con, Series:local_cons\n    \'\'\'\n    mean_local_strs, std_local_strs = util.calc_srs_mean_std(local_strs_list)\n    local_cons = 1 - 2 * std_local_strs / mean_local_strs\n    con = 1 - 2 * std_local_strs.sum() / mean_local_strs.sum()\n    return con, local_cons\n\n\ndef calc_session_metrics(session_df, env_name, info_prepath=None, df_mode=None):\n    \'\'\'\n    Calculate the session metrics: strength, efficiency, stability\n    @param DataFrame:session_df Dataframe containing reward, frame, opt_step\n    @param str:env_name Name of the environment to get its random baseline\n    @param str:info_prepath Optional info_prepath to auto-save the output to\n    @param str:df_mode Optional df_mode to save with info_prepath\n    @returns dict:metrics Consists of scalar metrics and series local metrics\n    \'\'\'\n    rand_bl = random_baseline.get_random_baseline(env_name)\n    if rand_bl is None:\n        mean_rand_returns = 0.0\n        logger.warn(\'Random baseline unavailable for environment. Please generate separately.\')\n    else:\n        mean_rand_returns = rand_bl[\'mean\']\n    mean_returns = session_df[\'total_reward\']\n    frames = session_df[\'frame\']\n    opt_steps = session_df[\'opt_step\']\n\n    final_return_ma = mean_returns[-viz.PLOT_MA_WINDOW:].mean()\n    str_, local_strs = calc_strength(mean_returns, mean_rand_returns)\n    max_str, final_str = local_strs.max(), local_strs.iloc[-1]\n    with warnings.catch_warnings():  # mute np.nanmean warning\n        warnings.filterwarnings(\'ignore\')\n        sample_eff, local_sample_effs = calc_efficiency(local_strs, frames)\n        train_eff, local_train_effs = calc_efficiency(local_strs, opt_steps)\n        sta, local_stas = calc_stability(local_strs)\n\n    # all the scalar session metrics\n    scalar = {\n        \'final_return_ma\': final_return_ma,\n        \'strength\': str_,\n        \'max_strength\': max_str,\n        \'final_strength\': final_str,\n        \'sample_efficiency\': sample_eff,\n        \'training_efficiency\': train_eff,\n        \'stability\': sta,\n    }\n    # all the session local metrics\n    local = {\n        \'mean_returns\': mean_returns,\n        \'strengths\': local_strs,\n        \'sample_efficiencies\': local_sample_effs,\n        \'training_efficiencies\': local_train_effs,\n        \'stabilities\': local_stas,\n        \'frames\': frames,\n        \'opt_steps\': opt_steps,\n    }\n    metrics = {\n        \'scalar\': scalar,\n        \'local\': local,\n    }\n    if info_prepath is not None:  # auto-save if info_prepath is given\n        util.write(metrics, f\'{info_prepath}_session_metrics_{df_mode}.pkl\')\n        util.write(scalar, f\'{info_prepath}_session_metrics_scalar_{df_mode}.json\')\n        # save important metrics in info_prepath directly\n        util.write(scalar, f\'{info_prepath.replace(""info/"", """")}_session_metrics_scalar_{df_mode}.json\')\n    return metrics\n\n\ndef calc_trial_metrics(session_metrics_list, info_prepath=None):\n    \'\'\'\n    Calculate the trial metrics: mean(strength), mean(efficiency), mean(stability), consistency\n    @param list:session_metrics_list The metrics collected from each session; format: {session_index: {\'scalar\': {...}, \'local\': {...}}}\n    @param str:info_prepath Optional info_prepath to auto-save the output to\n    @returns dict:metrics Consists of scalar metrics and series local metrics\n    \'\'\'\n    # calculate mean of session metrics\n    scalar_list = [sm[\'scalar\'] for sm in session_metrics_list]\n    mean_scalar = pd.DataFrame(scalar_list).mean().to_dict()\n\n    mean_returns_list = [sm[\'local\'][\'mean_returns\'] for sm in session_metrics_list]\n    local_strs_list = [sm[\'local\'][\'strengths\'] for sm in session_metrics_list]\n    local_se_list = [sm[\'local\'][\'sample_efficiencies\'] for sm in session_metrics_list]\n    local_te_list = [sm[\'local\'][\'training_efficiencies\'] for sm in session_metrics_list]\n    local_sta_list = [sm[\'local\'][\'stabilities\'] for sm in session_metrics_list]\n    frames = session_metrics_list[0][\'local\'][\'frames\']\n    opt_steps = session_metrics_list[0][\'local\'][\'opt_steps\']\n    # calculate consistency\n    con, local_cons = calc_consistency(local_strs_list)\n\n    # all the scalar trial metrics\n    scalar = {\n        \'final_return_ma\': mean_scalar[\'final_return_ma\'],\n        \'strength\': mean_scalar[\'strength\'],\n        \'max_strength\': mean_scalar[\'max_strength\'],\n        \'final_strength\': mean_scalar[\'final_strength\'],\n        \'sample_efficiency\': mean_scalar[\'sample_efficiency\'],\n        \'training_efficiency\': mean_scalar[\'training_efficiency\'],\n        \'stability\': mean_scalar[\'stability\'],\n        \'consistency\': con,\n    }\n    assert set(scalar.keys()) == set(METRICS_COLS)\n    # for plotting: gather all local series of sessions\n    local = {\n        \'mean_returns\': mean_returns_list,\n        \'strengths\': local_strs_list,\n        \'sample_efficiencies\': local_se_list,\n        \'training_efficiencies\': local_te_list,\n        \'stabilities\': local_sta_list,\n        \'consistencies\': local_cons,  # this is a list\n        \'frames\': frames,\n        \'opt_steps\': opt_steps,\n    }\n    metrics = {\n        \'scalar\': scalar,\n        \'local\': local,\n    }\n    if info_prepath is not None:  # auto-save if info_prepath is given\n        util.write(metrics, f\'{info_prepath}_trial_metrics.pkl\')\n        util.write(scalar, f\'{info_prepath}_trial_metrics_scalar.json\')\n        # save important metrics in info_prepath directly\n        util.write(scalar, f\'{info_prepath.replace(""info/"", """")}_trial_metrics_scalar.json\')\n    return metrics\n\n\ndef calc_experiment_df(trial_data_dict, info_prepath=None):\n    \'\'\'Collect all trial data (metrics and config) from trials into a dataframe\'\'\'\n    experiment_df = pd.DataFrame(trial_data_dict).transpose()\n    cols = METRICS_COLS\n    config_cols = sorted(ps.difference(experiment_df.columns.tolist(), cols))\n    sorted_cols = config_cols + cols\n    experiment_df = experiment_df.reindex(sorted_cols, axis=1)\n    experiment_df.sort_values(by=[\'strength\'], ascending=False, inplace=True)\n    # insert trial index\n    experiment_df.insert(0, \'trial\', experiment_df.index.astype(np.int))\n    if info_prepath is not None:\n        util.write(experiment_df, f\'{info_prepath}_experiment_df.csv\')\n        # save important metrics in info_prepath directly\n        util.write(experiment_df, f\'{info_prepath.replace(""info/"", """")}_experiment_df.csv\')\n    return experiment_df\n\n\n# interface analyze methods\n\ndef analyze_session(session_spec, session_df, df_mode, plot=True):\n    \'\'\'Analyze session and save data, then return metrics. Note there are 2 types of session_df: body.eval_df and body.train_df\'\'\'\n    info_prepath = session_spec[\'meta\'][\'info_prepath\']\n    session_df = session_df.copy()  # prevent modification\n    assert len(session_df) > 2, f\'Need more than 2 datapoint to calculate metrics\'  # first datapoint at frame 0 is empty\n    util.write(session_df, util.get_session_df_path(session_spec, df_mode))\n    # calculate metrics\n    session_metrics = calc_session_metrics(session_df, ps.get(session_spec, \'env.0.name\'), info_prepath, df_mode)\n    if plot:\n        # plot graph\n        viz.plot_session(session_spec, session_metrics, session_df, df_mode)\n        viz.plot_session(session_spec, session_metrics, session_df, df_mode, ma=True)\n    # manually shut down orca server to avoid zombie processes\n    viz.pio.orca.shutdown_server()\n    return session_metrics\n\n\ndef analyze_trial(trial_spec, session_metrics_list):\n    \'\'\'Analyze trial and save data, then return metrics\'\'\'\n    info_prepath = trial_spec[\'meta\'][\'info_prepath\']\n    # calculate metrics\n    trial_metrics = calc_trial_metrics(session_metrics_list, info_prepath)\n    # plot graphs\n    viz.plot_trial(trial_spec, trial_metrics)\n    viz.plot_trial(trial_spec, trial_metrics, ma=True)\n    # manually shut down orca server to avoid zombie processes\n    viz.pio.orca.shutdown_server()\n    # zip files\n    if util.get_lab_mode() == \'train\':\n        predir, _, _, _, _ = util.prepath_split(info_prepath)\n        zipdir = util.smart_path(predir)\n        shutil.make_archive(zipdir, \'zip\', zipdir)\n        logger.info(f\'All trial data zipped to {predir}.zip\')\n    return trial_metrics\n\n\ndef analyze_experiment(spec, trial_data_dict):\n    \'\'\'Analyze experiment and save data\'\'\'\n    info_prepath = spec[\'meta\'][\'info_prepath\']\n    util.write(trial_data_dict, f\'{info_prepath}_trial_data_dict.json\')\n    # calculate experiment df\n    experiment_df = calc_experiment_df(trial_data_dict, info_prepath)\n    # plot graph\n    viz.plot_experiment(spec, experiment_df, METRICS_COLS)\n    viz.plot_experiment_trials(spec, experiment_df, METRICS_COLS)\n    # manually shut down orca server to avoid zombie processes\n    viz.pio.orca.shutdown_server()\n    # zip files\n    predir, _, _, _, _ = util.prepath_split(info_prepath)\n    zipdir = util.smart_path(predir)\n    shutil.make_archive(zipdir, \'zip\', zipdir)\n    logger.info(f\'All experiment data zipped to {predir}.zip\')\n    return experiment_df\n'"
slm_lab/experiment/control.py,3,"b'# The control module\n# Creates and runs control loops at levels: Experiment, Trial, Session\nfrom copy import deepcopy\nfrom slm_lab.agent import Agent, Body\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.env import make_env\nfrom slm_lab.experiment import analysis, search\nfrom slm_lab.lib import logger, util\nfrom slm_lab.spec import spec_util\nimport pydash as ps\nimport torch\nimport torch.multiprocessing as mp\n\n\ndef make_agent_env(spec, global_nets=None):\n    \'\'\'Helper to create agent and env given spec\'\'\'\n    env = make_env(spec)\n    body = Body(env, spec)\n    agent = Agent(spec, body=body, global_nets=global_nets)\n    return agent, env\n\n\ndef mp_run_session(spec, global_nets, mp_dict):\n    \'\'\'Wrap for multiprocessing with shared variable\'\'\'\n    session = Session(spec, global_nets)\n    metrics = session.run()\n    mp_dict[session.index] = metrics\n\n\nclass Session:\n    \'\'\'\n    The base lab unit to run a RL session for a spec.\n    Given a spec, it creates the agent and env, runs the RL loop,\n    then gather data and analyze it to produce session data.\n    \'\'\'\n\n    def __init__(self, spec, global_nets=None):\n        self.spec = spec\n        self.index = self.spec[\'meta\'][\'session\']\n        util.set_random_seed(self.spec)\n        util.set_cuda_id(self.spec)\n        util.set_logger(self.spec, logger, \'session\')\n        spec_util.save(spec, unit=\'session\')\n\n        self.agent, self.env = make_agent_env(self.spec, global_nets)\n        if ps.get(self.spec, \'meta.rigorous_eval\'):\n            with util.ctx_lab_mode(\'eval\'):\n                self.eval_env = make_env(self.spec)\n        else:\n            self.eval_env = self.env\n        logger.info(util.self_desc(self))\n\n    def to_ckpt(self, env, mode=\'eval\'):\n        \'\'\'Check with clock whether to run log/eval ckpt: at the start, save_freq, and the end\'\'\'\n        if mode == \'eval\' and util.in_eval_lab_mode():  # avoid double-eval: eval-ckpt in eval mode\n            return False\n        clock = env.clock\n        frame = clock.get()\n        frequency = env.eval_frequency if mode == \'eval\' else env.log_frequency\n        to_ckpt = util.frame_mod(frame, frequency, env.num_envs) or frame == clock.max_frame\n        return to_ckpt\n\n    def try_ckpt(self, agent, env):\n        \'\'\'Check then run checkpoint log/eval\'\'\'\n        body = agent.body\n        if self.to_ckpt(env, \'log\'):\n            body.ckpt(self.env, \'train\')\n            body.log_summary(\'train\')\n            agent.save()  # save the latest ckpt\n            if body.total_reward_ma >= body.best_total_reward_ma:\n                body.best_total_reward_ma = body.total_reward_ma\n                agent.save(ckpt=\'best\')\n            if len(body.train_df) > 2:  # need more rows to calculate metrics\n                metrics = analysis.analyze_session(self.spec, body.train_df, \'train\', plot=False)\n                body.log_metrics(metrics[\'scalar\'], \'train\')\n\n        if ps.get(self.spec, \'meta.rigorous_eval\') and self.to_ckpt(env, \'eval\'):\n            logger.info(\'Running eval ckpt\')\n            analysis.gen_avg_return(agent, self.eval_env)\n            body.ckpt(self.eval_env, \'eval\')\n            body.log_summary(\'eval\')\n            if len(body.eval_df) > 2:  # need more rows to calculate metrics\n                metrics = analysis.analyze_session(self.spec, body.eval_df, \'eval\', plot=False)\n                body.log_metrics(metrics[\'scalar\'], \'eval\')\n\n    def run_rl(self):\n        \'\'\'Run the main RL loop until clock.max_frame\'\'\'\n        logger.info(f\'Running RL loop for trial {self.spec[""meta""][""trial""]} session {self.index}\')\n        clock = self.env.clock\n        state = self.env.reset()\n        done = False\n        while True:\n            if util.epi_done(done):  # before starting another episode\n                self.try_ckpt(self.agent, self.env)\n                if clock.get() < clock.max_frame:  # reset and continue\n                    clock.tick(\'epi\')\n                    state = self.env.reset()\n                    done = False\n            self.try_ckpt(self.agent, self.env)\n            if clock.get() >= clock.max_frame:  # finish\n                break\n            clock.tick(\'t\')\n            with torch.no_grad():\n                action = self.agent.act(state)\n            next_state, reward, done, info = self.env.step(action)\n            self.agent.update(state, action, reward, next_state, done)\n            state = next_state\n\n    def close(self):\n        \'\'\'Close session and clean up. Save agent, close env.\'\'\'\n        self.agent.close()\n        self.env.close()\n        self.eval_env.close()\n        torch.cuda.empty_cache()\n        logger.info(f\'Session {self.index} done\')\n\n    def run(self):\n        self.run_rl()\n        metrics = analysis.analyze_session(self.spec, self.agent.body.eval_df, \'eval\')\n        self.agent.body.log_metrics(metrics[\'scalar\'], \'eval\')\n        self.close()\n        return metrics\n\n\nclass Trial:\n    \'\'\'\n    The lab unit which runs repeated sessions for a same spec, i.e. a trial\n    Given a spec and number s, trial creates and runs s sessions,\n    then gathers session data and analyze it to produce trial data.\n    \'\'\'\n\n    def __init__(self, spec):\n        self.spec = spec\n        self.index = self.spec[\'meta\'][\'trial\']\n        util.set_logger(self.spec, logger, \'trial\')\n        spec_util.save(spec, unit=\'trial\')\n\n    def parallelize_sessions(self, global_nets=None):\n        mp_dict = mp.Manager().dict()\n        workers = []\n        spec = deepcopy(self.spec)\n        for _s in range(spec[\'meta\'][\'max_session\']):\n            spec_util.tick(spec, \'session\')\n            w = mp.Process(target=mp_run_session, args=(spec, global_nets, mp_dict))\n            w.start()\n            workers.append(w)\n        for w in workers:\n            w.join()\n        session_metrics_list = [mp_dict[idx] for idx in sorted(mp_dict.keys())]\n        return session_metrics_list\n\n    def run_sessions(self):\n        logger.info(\'Running sessions\')\n        if self.spec[\'meta\'][\'max_session\'] == 1:\n            spec = deepcopy(self.spec)\n            spec_util.tick(spec, \'session\')\n            session_metrics_list = [Session(spec).run()]\n        else:\n            session_metrics_list = self.parallelize_sessions()\n        return session_metrics_list\n\n    def init_global_nets(self):\n        session = Session(deepcopy(self.spec))\n        session.env.close()  # safety\n        global_nets = net_util.init_global_nets(session.agent.algorithm)\n        return global_nets\n\n    def run_distributed_sessions(self):\n        logger.info(\'Running distributed sessions\')\n        global_nets = self.init_global_nets()\n        session_metrics_list = self.parallelize_sessions(global_nets)\n        return session_metrics_list\n\n    def close(self):\n        logger.info(f\'Trial {self.index} done\')\n\n    def run(self):\n        if self.spec[\'meta\'].get(\'distributed\') == False:\n            session_metrics_list = self.run_sessions()\n        else:\n            session_metrics_list = self.run_distributed_sessions()\n        metrics = analysis.analyze_trial(self.spec, session_metrics_list)\n        self.close()\n        return metrics[\'scalar\']\n\n\nclass Experiment:\n    \'\'\'\n    The lab unit to run experiments.\n    It generates a list of specs to search over, then run each as a trial with s repeated session,\n    then gathers trial data and analyze it to produce experiment data.\n    \'\'\'\n\n    def __init__(self, spec):\n        self.spec = spec\n        self.index = self.spec[\'meta\'][\'experiment\']\n        util.set_logger(self.spec, logger, \'trial\')\n        spec_util.save(spec, unit=\'experiment\')\n\n    def close(self):\n        logger.info(\'Experiment done\')\n\n    def run(self):\n        trial_data_dict = search.run_ray_search(self.spec)\n        experiment_df = analysis.analyze_experiment(self.spec, trial_data_dict)\n        self.close()\n        return experiment_df\n'"
slm_lab/experiment/retro_analysis.py,0,"b""# The retro analysis module\n# Runs analysis post-hoc using existing data files\n# example: yarn retro_analyze data/reinforce_cartpole_2018_01_22_211751/\nfrom glob import glob\nfrom slm_lab.experiment import analysis\nfrom slm_lab.lib import logger, util\nimport os\nimport pydash as ps\n\nlogger = logger.get_logger(__name__)\n\n\ndef retro_analyze_sessions(predir):\n    '''Retro analyze all sessions'''\n    logger.info('Running retro_analyze_sessions')\n    session_spec_paths = glob(f'{predir}/*_s*_spec.json')\n    for p in session_spec_paths:\n        _retro_analyze_session(p)\n\n\ndef _retro_analyze_session(session_spec_path):\n    '''Method to retro analyze a single session given only a path to its spec'''\n    session_spec = util.read(session_spec_path)\n    info_prepath = session_spec['meta']['info_prepath']\n    for df_mode in ('eval', 'train'):\n        session_df = util.read(f'{info_prepath}_session_df_{df_mode}.csv')\n        analysis.analyze_session(session_spec, session_df, df_mode)\n\n\ndef retro_analyze_trials(predir):\n    '''Retro analyze all trials'''\n    logger.info('Running retro_analyze_trials')\n    session_spec_paths = glob(f'{predir}/*_s*_spec.json')\n    # remove session spec paths\n    trial_spec_paths = ps.difference(glob(f'{predir}/*_t*_spec.json'), session_spec_paths)\n    for p in trial_spec_paths:\n        _retro_analyze_trial(p)\n\n\ndef _retro_analyze_trial(trial_spec_path):\n    '''Method to retro analyze a single trial given only a path to its spec'''\n    trial_spec = util.read(trial_spec_path)\n    meta_spec = trial_spec['meta']\n    info_prepath = meta_spec['info_prepath']\n    session_metrics_list = [util.read(f'{info_prepath}_s{s}_session_metrics_eval.pkl') for s in range(meta_spec['max_session'])]\n    analysis.analyze_trial(trial_spec, session_metrics_list)\n\n\ndef retro_analyze_experiment(predir):\n    '''Retro analyze an experiment'''\n    logger.info('Running retro_analyze_experiment')\n    if ps.is_empty(glob(f'{predir}/info/*_trial_data_dict.json')):\n        logger.info('Skipping retro_analyze_experiment since no experiment was ran.')\n        return  # only run analysis if experiment had been ran\n    trial_spec_paths = glob(f'{predir}/*_t*_spec.json')\n    # remove trial and session spec paths\n    experiment_spec_paths = ps.difference(glob(f'{predir}/*_spec.json'), trial_spec_paths)\n    experiment_spec_path = experiment_spec_paths[0]\n    spec = util.read(experiment_spec_path)\n    info_prepath = spec['meta'].get('info_prepath')\n    trial_data_dict = util.read(f'{info_prepath}_trial_data_dict.json')\n    analysis.analyze_experiment(spec, trial_data_dict)\n\n\ndef retro_analyze(predir):\n    '''\n    Method to analyze experiment/trial from files after it ran.\n    @example\n\n    yarn retro_analyze data/reinforce_cartpole_2018_01_22_211751/\n    '''\n    predir = predir.strip('/')  # sanitary\n    os.environ['LOG_PREPATH'] = f'{predir}/log/retro_analyze'  # to prevent overwriting log file\n    logger.info(f'Running retro-analysis on {predir}')\n    retro_analyze_sessions(predir)\n    retro_analyze_trials(predir)\n    retro_analyze_experiment(predir)\n    logger.info('Finished retro-analysis')\n"""
slm_lab/experiment/search.py,1,"b'from copy import deepcopy\nfrom slm_lab.lib import logger, util\nfrom slm_lab.spec import spec_util\nimport numpy as np\nimport pydash as ps\nimport random\nimport ray\nimport ray.tune as tune\nimport torch\n\nlogger = logger.get_logger(__name__)\n\n\ndef build_config_space(spec):\n    \'\'\'\n    Build ray config space from flattened spec.search\n    Specify a config space in spec using `""{key}__{space_type}"": {v}`.\n    Where `{space_type}` is `grid_search` of `ray.tune`, or any function name of `np.random`:\n    - `grid_search`: str/int/float. v = list of choices\n    - `choice`: str/int/float. v = list of choices\n    - `randint`: int. v = [low, high)\n    - `uniform`: float. v = [low, high)\n    - `normal`: float. v = [mean, stdev)\n\n    For example:\n    - `""explore_anneal_epi__randint"": [10, 60],` will sample integers uniformly from 10 to 60 for `explore_anneal_epi`,\n    - `""lr__uniform"": [0.001, 0.1]`, and it will sample `lr` using `np.random.uniform(0.001, 0.1)`\n\n    If any key uses `grid_search`, it will be combined exhaustively in combination with other random sampling.\n    \'\'\'\n    space_types = (\'grid_search\', \'choice\', \'randint\', \'uniform\', \'normal\')\n    config_space = {}\n    for k, v in util.flatten_dict(spec[\'search\']).items():\n        key, space_type = k.split(\'__\')\n        assert space_type in space_types, f\'Please specify your search variable as {key}__<space_type> in one of {space_types}\'\n        if space_type == \'grid_search\':\n            config_space[key] = tune.grid_search(v)\n        elif space_type == \'choice\':\n            config_space[key] = tune.sample_from(lambda spec, v=v: random.choice(v))\n        else:\n            np_fn = getattr(np.random, space_type)\n            config_space[key] = tune.sample_from(lambda spec, v=v: np_fn(*v))\n    return config_space\n\n\ndef infer_trial_resources(spec):\n    \'\'\'Infer the resources_per_trial for ray from spec\'\'\'\n    meta_spec = spec[\'meta\']\n    cpu_per_session = meta_spec.get(\'num_cpus\') or 1\n    requested_cpu = cpu_per_session * meta_spec[\'max_session\']\n    num_cpus = min(util.NUM_CPUS, requested_cpu)\n\n    use_gpu = any(agent_spec[\'net\'].get(\'gpu\') for agent_spec in spec[\'agent\'])\n    gpu_per_session = meta_spec.get(\'num_gpus\') or 1\n    requested_gpu = gpu_per_session * meta_spec[\'max_session\'] if use_gpu else 0\n    gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    num_gpus = min(gpu_count, requested_gpu)\n    resources_per_trial = {\'cpu\': num_cpus, \'gpu\': num_gpus}\n    return resources_per_trial\n\n\ndef inject_config(spec, config):\n    \'\'\'Inject flattened config into SLM Lab spec.\'\'\'\n    spec = deepcopy(spec)\n    spec.pop(\'search\', None)\n    for k, v in config.items():\n        ps.set_(spec, k, v)\n    return spec\n\n\ndef ray_trainable(config, reporter):\n    \'\'\'\n    Create an instance of a trainable function for ray: https://ray.readthedocs.io/en/latest/tune-usage.html#training-api\n    Lab needs a spec and a trial_index to be carried through config, pass them with config in ray.run() like so:\n    config = {\n        \'spec\': spec,\n        \'trial_index\': tune.sample_from(lambda spec: gen_trial_index()),\n        ... # normal ray config with sample, grid search etc.\n    }\n    \'\'\'\n    import os\n    os.environ.pop(\'CUDA_VISIBLE_DEVICES\', None)  # remove CUDA id restriction from ray\n    from slm_lab.experiment.control import Trial\n    # restore data carried from ray.run() config\n    spec = config.pop(\'spec\')\n    spec = inject_config(spec, config)\n    # tick trial_index with proper offset\n    trial_index = config.pop(\'trial_index\')\n    spec[\'meta\'][\'trial\'] = trial_index - 1\n    spec_util.tick(spec, \'trial\')\n    # run SLM Lab trial\n    metrics = Trial(spec).run()\n    metrics.update(config)  # carry config for analysis too\n    # ray report to carry data in ray trial.last_result\n    reporter(trial_data={trial_index: metrics})\n\n\ndef run_ray_search(spec):\n    \'\'\'\n    Method to run ray search from experiment. Uses RandomSearch now.\n    TODO support for other ray search algorithms: https://ray.readthedocs.io/en/latest/tune-searchalg.html\n    \'\'\'\n    logger.info(f\'Running ray search for spec {spec[""name""]}\')\n    # generate trial index to pass into Lab Trial\n    global trial_index  # make gen_trial_index passable into ray.run\n    trial_index = -1\n\n    def gen_trial_index():\n        global trial_index\n        trial_index += 1\n        return trial_index\n\n    ray.init()\n\n    ray_trials = tune.run(\n        ray_trainable,\n        name=spec[\'name\'],\n        config={\n            \'spec\': spec,\n            \'trial_index\': tune.sample_from(lambda spec: gen_trial_index()),\n            **build_config_space(spec)\n        },\n        resources_per_trial=infer_trial_resources(spec),\n        num_samples=spec[\'meta\'][\'max_trial\'],\n        reuse_actors=False,\n        server_port=util.get_port(),\n    )\n    trial_data_dict = {}  # data for Lab Experiment to analyze\n    for ray_trial in ray_trials:\n        ray_trial_data = ray_trial.last_result[\'trial_data\']\n        trial_data_dict.update(ray_trial_data)\n\n    ray.shutdown()\n    return trial_data_dict\n\n\ndef run_param_specs(param_specs):\n    \'\'\'Run the given param_specs in parallel trials using ray. Used for benchmarking.\'\'\'\n    ray.init()\n    ray_trials = tune.run(\n        ray_trainable,\n        name=\'param_specs\',\n        config={\n            \'spec\': tune.grid_search(param_specs),\n            \'trial_index\': 0,\n        },\n        resources_per_trial=infer_trial_resources(param_specs[0]),\n        num_samples=1,\n        reuse_actors=False,\n        server_port=util.get_port(),\n    )\n    ray.shutdown()\n'"
slm_lab/lib/__init__.py,0,b''
slm_lab/lib/decorator.py,0,"b""from functools import wraps\nfrom slm_lab.lib import logger\nimport time\n\nlogger = logger.get_logger(__name__)\n\n\ndef lab_api(fn):\n    '''\n    Function decorator to label and check Lab API methods\n    @example\n\n    from slm_lab.lib.decorator import lab_api\n    @lab_api\n    def foo():\n        print('foo')\n    '''\n    return fn\n\n\ndef timeit(fn):\n    '''\n    Function decorator to measure execution time\n    @example\n\n    from slm_lab.lib.decorator import timeit\n    @timeit\n    def foo(sec):\n        time.sleep(sec)\n        print('foo')\n\n    foo(1)\n    # => foo\n    # => Timed: foo 1000.9971ms\n    '''\n    @wraps(fn)\n    def time_fn(*args, **kwargs):\n        start = time.time()\n        output = fn(*args, **kwargs)\n        end = time.time()\n        logger.debug(f'Timed: {fn.__name__} {round((end - start) * 1000, 4)}ms')\n        return output\n    return time_fn\n"""
slm_lab/lib/distribution.py,16,"b""# Custom PyTorch distribution classes to be registered in policy_util.py\n# Mainly used by policy_util action distribution\nfrom torch import distributions\nimport torch\nimport torch.nn.functional as F\n\n\nclass Argmax(distributions.Categorical):\n    '''\n    Special distribution class for argmax sampling, where probability is always 1 for the argmax.\n    NOTE although argmax is not a sampling distribution, this implementation is for API consistency.\n    '''\n\n    def __init__(self, probs=None, logits=None, validate_args=None):\n        if probs is not None:\n            new_probs = torch.zeros_like(probs, dtype=torch.float)\n            new_probs[probs == probs.max(dim=-1, keepdim=True)[0]] = 1.0\n            probs = new_probs\n        elif logits is not None:\n            new_logits = torch.full_like(logits, -1e8, dtype=torch.float)\n            new_logits[logits == logits.max(dim=-1, keepdim=True)[0]] = 1.0\n            logits = new_logits\n\n        super().__init__(probs=probs, logits=logits, validate_args=validate_args)\n\n\nclass GumbelSoftmax(distributions.RelaxedOneHotCategorical):\n    '''\n    A differentiable Categorical distribution using reparametrization trick with Gumbel-Softmax\n    Explanation http://amid.fish/assets/gumbel.html\n    NOTE: use this in place PyTorch's RelaxedOneHotCategorical distribution since its log_prob is not working right (returns positive values)\n    Papers:\n    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017)\n    [2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)\n    '''\n\n    def sample(self, sample_shape=torch.Size()):\n        '''Gumbel-softmax sampling. Note rsample is inherited from RelaxedOneHotCategorical'''\n        u = torch.empty(self.logits.size(), device=self.logits.device, dtype=self.logits.dtype).uniform_(0, 1)\n        noisy_logits = self.logits - torch.log(-torch.log(u))\n        return torch.argmax(noisy_logits, dim=-1)\n\n    def rsample(self, sample_shape=torch.Size()):\n        '''\n        Gumbel-softmax resampling using the Straight-Through trick.\n        Credit to Ian Temple for bringing this to our attention. To see standalone code of how this works, refer to https://gist.github.com/yzh119/fd2146d2aeb329d067568a493b20172f\n        '''\n        rout = super().rsample(sample_shape)  # differentiable\n        out = F.one_hot(torch.argmax(rout, dim=-1), self.logits.shape[-1]).float()\n        return (out - rout).detach() + rout\n\n    def log_prob(self, value):\n        '''value is one-hot or relaxed'''\n        if value.shape != self.logits.shape:\n            value = F.one_hot(value.long(), self.logits.shape[-1]).float()\n            assert value.shape == self.logits.shape\n        return - torch.sum(- value * F.log_softmax(self.logits, -1), -1)\n\n\nclass MultiCategorical(distributions.Categorical):\n    '''MultiCategorical as collection of Categoricals'''\n\n    def __init__(self, probs=None, logits=None, validate_args=None):\n        self.categoricals = []\n        if probs is None:\n            probs = [None] * len(logits)\n        elif logits is None:\n            logits = [None] * len(probs)\n        else:\n            raise ValueError('Either probs or logits must be None')\n\n        for sub_probs, sub_logits in zip(probs, logits):\n            categorical = distributions.Categorical(probs=sub_probs, logits=sub_logits, validate_args=validate_args)\n            self.categoricals.append(categorical)\n\n    @property\n    def logits(self):\n        return [cat.logits for cat in self.categoricals]\n\n    @property\n    def probs(self):\n        return [cat.probs for cat in self.categoricals]\n\n    @property\n    def param_shape(self):\n        return [cat.param_shape for cat in self.categoricals]\n\n    @property\n    def mean(self):\n        return torch.stack([cat.mean for cat in self.categoricals])\n\n    @property\n    def variance(self):\n        return torch.stack([cat.variance for cat in self.categoricals])\n\n    def sample(self, sample_shape=torch.Size()):\n        return torch.stack([cat.sample(sample_shape=sample_shape) for cat in self.categoricals])\n\n    def log_prob(self, value):\n        value_t = value.transpose(0, 1)\n        return torch.stack([cat.log_prob(value_t[idx]) for idx, cat in enumerate(self.categoricals)])\n\n    def entropy(self):\n        return torch.stack([cat.entropy() for cat in self.categoricals])\n\n    def enumerate_support(self):\n        return [cat.enumerate_support() for cat in self.categoricals]\n"""
slm_lab/lib/logger.py,0,"b""from slm_lab.lib import util\nimport colorlog\nimport logging\nimport os\nimport pandas as pd\nimport sys\nimport warnings\n\n\nclass FixedList(list):\n    '''fixed-list to restrict addition to root logger handler'''\n\n    def append(self, e):\n        pass\n\n\nLOG_FORMAT = '[%(asctime)s PID:%(process)d %(levelname)s %(filename)s %(funcName)s] %(message)s'\ncolor_formatter = colorlog.ColoredFormatter('%(log_color)s[%(asctime)s PID:%(process)d %(levelname)s %(filename)s %(funcName)s]%(reset)s %(message)s')\nsh = logging.StreamHandler(sys.stdout)\nsh.setFormatter(color_formatter)\nlab_logger = logging.getLogger()\nlab_logger.handlers = FixedList([sh])\nlogging.getLogger('ray').propagate = False  # hack to mute poorly designed ray TF warning log\n\n# this will trigger from Experiment init on reload(logger)\nif os.environ.get('LOG_PREPATH') is not None:\n    warnings.filterwarnings('ignore', category=pd.io.pytables.PerformanceWarning)\n\n    log_filepath = os.environ['LOG_PREPATH'] + '.log'\n    os.makedirs(os.path.dirname(log_filepath), exist_ok=True)\n    # create file handler\n    formatter = logging.Formatter(LOG_FORMAT)\n    fh = logging.FileHandler(log_filepath)\n    fh.setFormatter(formatter)\n    # add stream and file handler\n    lab_logger.handlers = FixedList([sh, fh])\n\nif os.environ.get('LOG_LEVEL'):\n    lab_logger.setLevel(os.environ['LOG_LEVEL'])\nelse:\n    lab_logger.setLevel('INFO')\n\n\ndef set_level(lvl):\n    lab_logger.setLevel(lvl)\n    os.environ['LOG_LEVEL'] = lvl\n\n\ndef critical(msg, *args, **kwargs):\n    return lab_logger.critical(msg, *args, **kwargs)\n\n\ndef debug(msg, *args, **kwargs):\n    return lab_logger.debug(msg, *args, **kwargs)\n\n\ndef error(msg, *args, **kwargs):\n    return lab_logger.error(msg, *args, **kwargs)\n\n\ndef exception(msg, *args, **kwargs):\n    return lab_logger.exception(msg, *args, **kwargs)\n\n\ndef info(msg, *args, **kwargs):\n    return lab_logger.info(msg, *args, **kwargs)\n\n\ndef warning(msg, *args, **kwargs):\n    return lab_logger.warning(msg, *args, **kwargs)\n\n\ndef get_logger(__name__):\n    '''Create a child logger specific to a module'''\n    return logging.getLogger(__name__)\n\n\ndef toggle_debug(modules, level='DEBUG'):\n    '''Turn on module-specific debugging using their names, e.g. algorithm, actor_critic, at the desired debug level.'''\n    logger_names = list(logging.Logger.manager.loggerDict.keys())\n    for module in modules:\n        name = module.strip()\n        for logger_name in logger_names:\n            if name in logger_name.split('.'):\n                module_logger = logging.getLogger(logger_name)\n                module_logger.setLevel(getattr(logging, level))\n"""
slm_lab/lib/math_util.py,5,"b""# Various math calculations used by algorithms\nimport numpy as np\nimport torch\n\n\n# general math methods\n\ndef center_mean(v):\n    '''Center an array by its mean'''\n    return v - v.mean()\n\n\ndef normalize(v):\n    '''Method to normalize a rank-1 np array'''\n    v_min = v.min()\n    v_max = v.max()\n    v_range = v_max - v_min\n    v_range += 1e-08  # division guard\n    v_norm = (v - v_min) / v_range\n    return v_norm\n\n\ndef standardize(v):\n    '''Method to standardize a rank-1 np array'''\n    assert len(v) > 1, 'Cannot standardize vector of size 1'\n    v_std = (v - v.mean()) / (v.std() + 1e-08)\n    return v_std\n\n\ndef to_one_hot(data, max_val):\n    '''Convert an int list of data into one-hot vectors'''\n    return np.eye(max_val)[np.array(data)]\n\n\ndef venv_pack(batch_tensor, num_envs):\n    '''Apply the reverse of venv_unpack to pack a batch tensor from (b*num_envs, *shape) to (b, num_envs, *shape)'''\n    shape = list(batch_tensor.shape)\n    if len(shape) < 2:  # scalar data (b, num_envs,)\n        return batch_tensor.view(-1, num_envs)\n    else:  # non-scalar data (b, num_envs, *shape)\n        pack_shape = [-1, num_envs] + shape[1:]\n        return batch_tensor.view(pack_shape)\n\n\ndef venv_unpack(batch_tensor):\n    '''\n    Unpack a sampled vec env batch tensor\n    e.g. for a state with original shape (4, ), vec env should return vec state with shape (num_envs, 4) to store in memory\n    When sampled with batch_size b, we should get shape (b, num_envs, 4). But we need to unpack the num_envs dimension to get (b * num_envs, 4) for passing to a network. This method does that.\n    '''\n    shape = list(batch_tensor.shape)\n    if len(shape) < 3:  # scalar data (b, num_envs,)\n        return batch_tensor.view(-1)\n    else:  # non-scalar data (b, num_envs, *shape)\n        unpack_shape = [-1] + shape[2:]\n        return batch_tensor.view(unpack_shape)\n\n\n# Policy Gradient calc\n# advantage functions\n\ndef calc_returns(rewards, dones, gamma):\n    '''\n    Calculate the simple returns (full rollout) i.e. sum discounted rewards up till termination\n    '''\n    T = len(rewards)\n    rets = torch.zeros_like(rewards)\n    future_ret = torch.tensor(0.0, dtype=rewards.dtype)\n    not_dones = 1 - dones\n    for t in reversed(range(T)):\n        rets[t] = future_ret = rewards[t] + gamma * future_ret * not_dones[t]\n    return rets\n\n\ndef calc_nstep_returns(rewards, dones, next_v_pred, gamma, n):\n    '''\n    Estimate the advantages using n-step returns. Ref: http://www-anw.cs.umass.edu/~barto/courses/cs687/Chapter%207.pdf\n    Also see Algorithm S3 from A3C paper https://arxiv.org/pdf/1602.01783.pdf for the calculation used below\n    R^(n)_t = r_{t} + gamma r_{t+1} + ... + gamma^(n-1) r_{t+n-1} + gamma^(n) V(s_{t+n})\n    '''\n    rets = torch.zeros_like(rewards)\n    future_ret = next_v_pred\n    not_dones = 1 - dones\n    for t in reversed(range(n)):\n        rets[t] = future_ret = rewards[t] + gamma * future_ret * not_dones[t]\n    return rets\n\n\ndef calc_gaes(rewards, dones, v_preds, gamma, lam):\n    '''\n    Estimate the advantages using GAE from Schulman et al. https://arxiv.org/pdf/1506.02438.pdf\n    v_preds are values predicted for current states, with one last element as the final next_state\n    delta is defined as r + gamma * V(s') - V(s) in eqn 10\n    GAE is defined in eqn 16\n    This method computes in torch tensor to prevent unnecessary moves between devices (e.g. GPU tensor to CPU numpy)\n    NOTE any standardization is done outside of this method\n    '''\n    T = len(rewards)\n    assert T + 1 == len(v_preds), f'T+1: {T+1} v.s. v_preds.shape: {v_preds.shape}'  # v_preds runs into t+1\n    gaes = torch.zeros_like(rewards)\n    future_gae = torch.tensor(0.0, dtype=rewards.dtype)\n    not_dones = 1 - dones  # to reset at episode boundary by multiplying 0\n    deltas = rewards + gamma * v_preds[1:] * not_dones - v_preds[:-1]\n    coef = gamma * lam\n    for t in reversed(range(T)):\n        gaes[t] = future_gae = deltas[t] + coef * not_dones[t] * future_gae\n    return gaes\n\n\ndef calc_q_value_logits(state_value, raw_advantages):\n    mean_adv = raw_advantages.mean(dim=-1).unsqueeze(dim=-1)\n    return state_value + raw_advantages - mean_adv\n\n\n# generic variable decay methods\n\ndef no_decay(start_val, end_val, start_step, end_step, step):\n    '''dummy method for API consistency'''\n    return start_val\n\n\ndef linear_decay(start_val, end_val, start_step, end_step, step):\n    '''Simple linear decay with annealing'''\n    if step < start_step:\n        return start_val\n    slope = (end_val - start_val) / (end_step - start_step)\n    val = max(slope * (step - start_step) + start_val, end_val)\n    return val\n\n\ndef rate_decay(start_val, end_val, start_step, end_step, step, decay_rate=0.9, frequency=20.):\n    '''Compounding rate decay that anneals in 20 decay iterations until end_step'''\n    if step < start_step:\n        return start_val\n    if step >= end_step:\n        return end_val\n    step_per_decay = (end_step - start_step) / frequency\n    decay_step = (step - start_step) / step_per_decay\n    val = max(np.power(decay_rate, decay_step) * start_val, end_val)\n    return val\n\n\ndef periodic_decay(start_val, end_val, start_step, end_step, step, frequency=60.):\n    '''\n    Linearly decaying sinusoid that decays in roughly 10 iterations until explore_anneal_epi\n    Plot the equation below to see the pattern\n    suppose sinusoidal decay, start_val = 1, end_val = 0.2, stop after 60 unscaled x steps\n    then we get 0.2+0.5*(1-0.2)(1 + cos x)*(1-x/60)\n    '''\n    if step < start_step:\n        return start_val\n    if step >= end_step:\n        return end_val\n    x_freq = frequency\n    step_per_decay = (end_step - start_step) / x_freq\n    x = (step - start_step) / step_per_decay\n    unit = start_val - end_val\n    val = end_val * 0.5 * unit * (1 + np.cos(x) * (1 - x / x_freq))\n    val = max(val, end_val)\n    return val\n"""
slm_lab/lib/optimizer.py,7,"b""# Custom PyTorch optimizer classes, to be registered in net_util.py\nfrom torch.optim.optimizer import Optimizer\nimport itertools as it\nimport math\nimport torch\n\n\nclass GlobalAdam(torch.optim.Adam):\n    '''\n    Global Adam algorithm with shared states for Hogwild.\n    Adapted from https://github.com/ikostrikov/pytorch-a3c/blob/master/my_optim.py (MIT)\n    '''\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        super().__init__(params, lr, betas, eps, weight_decay)\n\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                state['step'] = torch.zeros(1)\n                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_()\n                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_()\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                state['step'].share_memory_()\n                state['exp_avg'].share_memory_()\n                state['exp_avg_sq'].share_memory_()\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n                state['step'] += 1\n                if group['weight_decay'] != 0:\n                    grad = grad.add(group['weight_decay'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                denom = exp_avg_sq.sqrt().add_(group['eps'])\n                bias_correction1 = 1 - beta1 ** state['step'].item()\n                bias_correction2 = 1 - beta2 ** state['step'].item()\n                step_size = group['lr'] * math.sqrt(\n                    bias_correction2) / bias_correction1\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n        return loss\n\n\nclass GlobalRMSprop(torch.optim.RMSprop):\n    '''\n    Global RMSprop algorithm with shared states for Hogwild.\n    Adapted from https://github.com/jingweiz/pytorch-rl/blob/master/optims/sharedRMSprop.py (MIT)\n    '''\n\n    def __init__(self, params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0):\n        super().__init__(params, lr=lr, alpha=alpha, eps=eps, weight_decay=weight_decay, momentum=0, centered=False)\n\n        # State initialisation (must be done before step, else will not be shared between threads)\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                state['step'] = p.data.new().resize_(1).zero_()\n                state['square_avg'] = p.data.new().resize_as_(p.data).zero_()\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                state['step'].share_memory_()\n                state['square_avg'].share_memory_()\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n                square_avg = state['square_avg']\n                alpha = group['alpha']\n                state['step'] += 1\n                if group['weight_decay'] != 0:\n                    grad = grad.add(group['weight_decay'], p.data)\n\n                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n                avg = square_avg.sqrt().add_(group['eps'])\n                p.data.addcdiv_(-group['lr'], grad, avg)\n        return loss\n\n\nclass Lookahead(Optimizer):\n    '''\n    Lookahead Optimizer: k steps forward, 1 step back\n    https://arxiv.org/abs/1907.08610\n    Implementation modified from https://github.com/lonePatient/lookahead_pytorch; reference from https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d\n    '''\n\n    def __init__(self, params, alpha=0.5, k=5, optimizer='RAdam', **optimizer_kwargs):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        # construct base optimizer\n        OptimClass = getattr(torch.optim, optimizer)\n        self.optimizer = OptimClass(params, **optimizer_kwargs)\n        self.param_groups = self.optimizer.param_groups\n        self.state = self.optimizer.state\n        # create and use defaults to track params to retain them in multiprocessing spawn\n        self.defaults = self.optimizer.defaults\n        self.defaults['alpha'] = alpha\n        self.defaults['k'] = k\n        for group in self.param_groups:\n            group['step_counter'] = 0\n        self.defaults['slow_weights'] = [[\n            p.clone().detach() for p in group['params']]\n            for group in self.param_groups]\n\n        for w in it.chain(*self.defaults['slow_weights']):\n            w.requires_grad = False\n\n    def share_memory(self):\n        self.optimizer.share_memory()\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        loss = self.optimizer.step()\n        for group, slow_weights in zip(self.param_groups, self.defaults['slow_weights']):\n            group['step_counter'] += 1\n            if group['step_counter'] % self.defaults['k'] != 0:\n                continue\n            for p, q in zip(group['params'], slow_weights):\n                if p.grad is None:\n                    continue\n                q.data.add_(self.defaults['alpha'], p.data - q.data)\n                p.data.copy_(q.data)\n        return loss\n\n\nclass RAdam(Optimizer):\n    '''\n    RAdam optimizer which stabilizes training vs. different learning rates.\n    paper: On the Variance of the Adaptive Learning Rate and Beyond https://arxiv.org/abs/1908.03265\n    Adapted from https://github.com/LiyuanLucasLiu/RAdam/blob/master/radam.py (Apache-2.0)\n    '''\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=torch.zeros((10, 3)))\n        super(RAdam, self).__init__(params, defaults)\n\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                state['step'] = torch.zeros(1)\n                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_()\n                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_()\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                state['step'].share_memory_()\n                state['exp_avg'].share_memory_()\n                state['exp_avg_sq'].share_memory_()\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n                state = self.state[p]\n                state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = self.defaults['buffer'][int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = 1.0 / (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                adap_lr = (-step_size * group['lr']).squeeze(dim=0).item()\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(adap_lr, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(adap_lr, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n"""
slm_lab/lib/util.py,7,"b'from collections import deque\nfrom contextlib import contextmanager\nfrom datetime import datetime\nfrom importlib import reload\nfrom pprint import pformat\nfrom slm_lab import ROOT_DIR, EVAL_MODES, TRAIN_MODES\nimport cv2\nimport json\nimport numpy as np\nimport operator\nimport os\nimport pandas as pd\nimport pickle\nimport pydash as ps\nimport regex as re\nimport subprocess\nimport sys\nimport time\nimport torch\nimport torch.multiprocessing as mp\nimport ujson\nimport yaml\n\nNUM_CPUS = mp.cpu_count()\nFILE_TS_FORMAT = \'%Y_%m_%d_%H%M%S\'\nRE_FILE_TS = re.compile(r\'(\\d{4}_\\d{2}_\\d{2}_\\d{6})\')\n\n\nclass LabJsonEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, (np.ndarray, pd.Series)):\n            return obj.tolist()\n        else:\n            return str(obj)\n\n\ndef batch_get(arr, idxs):\n    \'\'\'Get multi-idxs from an array depending if it\'s a python list or np.array\'\'\'\n    if isinstance(arr, (list, deque)):\n        return np.array(operator.itemgetter(*idxs)(arr))\n    else:\n        return arr[idxs]\n\n\ndef calc_srs_mean_std(sr_list):\n    \'\'\'Given a list of series, calculate their mean and std\'\'\'\n    cat_df = pd.DataFrame(dict(enumerate(sr_list)))\n    mean_sr = cat_df.mean(axis=1)\n    std_sr = cat_df.std(axis=1)\n    return mean_sr, std_sr\n\n\ndef calc_ts_diff(ts2, ts1):\n    \'\'\'\n    Calculate the time from tss ts1 to ts2\n    @param {str} ts2 Later ts in the FILE_TS_FORMAT\n    @param {str} ts1 Earlier ts in the FILE_TS_FORMAT\n    @returns {str} delta_t in %H:%M:%S format\n    @example\n\n    ts1 = \'2017_10_17_084739\'\n    ts2 = \'2017_10_17_084740\'\n    ts_diff = util.calc_ts_diff(ts2, ts1)\n    # => \'0:00:01\'\n    \'\'\'\n    delta_t = datetime.strptime(ts2, FILE_TS_FORMAT) - datetime.strptime(ts1, FILE_TS_FORMAT)\n    return str(delta_t)\n\n\ndef cast_df(val):\n    \'\'\'missing pydash method to cast value as DataFrame\'\'\'\n    if isinstance(val, pd.DataFrame):\n        return val\n    return pd.DataFrame(val)\n\n\ndef cast_list(val):\n    \'\'\'missing pydash method to cast value as list\'\'\'\n    if ps.is_list(val):\n        return val\n    else:\n        return [val]\n\n\ndef concat_batches(batches):\n    \'\'\'\n    Concat batch objects from body.memory.sample() into one batch, when all bodies experience similar envs\n    Also concat any nested epi sub-batches into flat batch\n    {k: arr1} + {k: arr2} = {k: arr1 + arr2}\n    \'\'\'\n    # if is nested, then is episodic\n    is_episodic = isinstance(batches[0][\'dones\'][0], (list, np.ndarray))\n    concat_batch = {}\n    for k in batches[0]:\n        datas = []\n        for batch in batches:\n            data = batch[k]\n            if is_episodic:  # make into plain batch instead of nested\n                data = np.concatenate(data)\n            datas.append(data)\n        concat_batch[k] = np.concatenate(datas)\n    return concat_batch\n\n\ndef downcast_float32(df):\n    \'\'\'Downcast any float64 col to float32 to allow safer pandas comparison\'\'\'\n    for col in df.columns:\n        if df[col].dtype == \'float\':\n            df[col] = df[col].astype(\'float32\')\n    return df\n\n\ndef epi_done(done):\n    \'\'\'\n    General method to check if episode is done for both single and vectorized env\n    Only return True for singleton done since vectorized env does not have a natural episode boundary\n    \'\'\'\n    return np.isscalar(done) and done\n\n\ndef frame_mod(frame, frequency, num_envs):\n    \'\'\'\n    Generic mod for (frame % frequency == 0) for when num_envs is 1 or more,\n    since frame will increase multiple ticks for vector env, use the remainder\'\'\'\n    remainder = num_envs or 1\n    return (frame % frequency < remainder)\n\n\ndef flatten_dict(obj, delim=\'.\'):\n    \'\'\'Missing pydash method to flatten dict\'\'\'\n    nobj = {}\n    for key, val in obj.items():\n        if ps.is_dict(val) and not ps.is_empty(val):\n            strip = flatten_dict(val, delim)\n            for k, v in strip.items():\n                nobj[key + delim + k] = v\n        elif ps.is_list(val) and not ps.is_empty(val) and ps.is_dict(val[0]):\n            for idx, v in enumerate(val):\n                nobj[key + delim + str(idx)] = v\n                if ps.is_object(v):\n                    nobj = flatten_dict(nobj, delim)\n        else:\n            nobj[key] = val\n    return nobj\n\n\ndef get_class_name(obj, lower=False):\n    \'\'\'Get the class name of an object\'\'\'\n    class_name = obj.__class__.__name__\n    if lower:\n        class_name = class_name.lower()\n    return class_name\n\n\ndef get_class_attr(obj):\n    \'\'\'Get the class attr of an object as dict\'\'\'\n    attr_dict = {}\n    for k, v in obj.__dict__.items():\n        if hasattr(v, \'__dict__\') or ps.is_tuple(v):\n            val = str(v)\n        else:\n            val = v\n        attr_dict[k] = val\n    return attr_dict\n\n\ndef get_file_ext(data_path):\n    \'\'\'get the `.ext` of file.ext\'\'\'\n    return os.path.splitext(data_path)[-1]\n\n\ndef get_fn_list(a_cls):\n    \'\'\'\n    Get the callable, non-private functions of a class\n    @returns {[*str]} A list of strings of fn names\n    \'\'\'\n    fn_list = ps.filter_(dir(a_cls), lambda fn: not fn.endswith(\'__\') and callable(getattr(a_cls, fn)))\n    return fn_list\n\n\ndef get_git_sha():\n    return subprocess.check_output([\'git\', \'rev-parse\', \'HEAD\'], close_fds=True, cwd=ROOT_DIR).decode().strip()\n\n\ndef get_lab_mode():\n    return os.environ.get(\'lab_mode\')\n\n\ndef get_port():\n    \'\'\'Get a unique port number for a run time as 4xxx, where xxx is the last 3 digits from the PID, front-padded with 0\'\'\'\n    # get 3 digits from pid\n    xxx = ps.pad_start(str(os.getpid())[-3:], 3, 0)\n    port = int(f\'4{xxx}\')\n    return port\n\n\ndef get_prepath(spec, unit=\'experiment\'):\n    spec_name = spec[\'name\']\n    meta_spec = spec[\'meta\']\n    predir = f\'data/{spec_name}_{meta_spec[""experiment_ts""]}\'\n    prename = f\'{spec_name}\'\n    trial_index = meta_spec[\'trial\']\n    session_index = meta_spec[\'session\']\n    t_str = \'\' if trial_index is None else f\'_t{trial_index}\'\n    s_str = \'\' if session_index is None else f\'_s{session_index}\'\n    if unit == \'trial\':\n        prename += t_str\n    elif unit == \'session\':\n        prename += f\'{t_str}{s_str}\'\n    prepath = f\'{predir}/{prename}\'\n    return prepath\n\n\ndef get_session_df_path(session_spec, df_mode):\n    \'\'\'Method to return standard filepath for session_df (agent.body.train_df/eval_df) for saving and loading\'\'\'\n    info_prepath = session_spec[\'meta\'][\'info_prepath\']\n    return f\'{info_prepath}_session_df_{df_mode}.csv\'\n\n\ndef get_ts(pattern=FILE_TS_FORMAT):\n    \'\'\'\n    Get current ts, defaults to format used for filename\n    @param {str} pattern To format the ts\n    @returns {str} ts\n    @example\n\n    util.get_ts()\n    # => \'2017_10_17_084739\'\n    \'\'\'\n    ts_obj = datetime.now()\n    ts = ts_obj.strftime(pattern)\n    assert RE_FILE_TS.search(ts)\n    return ts\n\n\ndef insert_folder(prepath, folder):\n    \'\'\'Insert a folder into prepath\'\'\'\n    split_path = prepath.split(\'/\')\n    prename = split_path.pop()\n    split_path += [folder, prename]\n    return \'/\'.join(split_path)\n\n\ndef in_eval_lab_mode():\n    \'\'\'Check if lab_mode is one of EVAL_MODES\'\'\'\n    return get_lab_mode() in EVAL_MODES\n\n\ndef in_train_lab_mode():\n    \'\'\'Check if lab_mode is one of TRAIN_MODES\'\'\'\n    return get_lab_mode() in TRAIN_MODES\n\n\ndef is_jupyter():\n    \'\'\'Check if process is in Jupyter kernel\'\'\'\n    try:\n        get_ipython().config\n        return True\n    except NameError:\n        return False\n    return False\n\n\n@contextmanager\ndef ctx_lab_mode(lab_mode):\n    \'\'\'\n    Creates context to run method with a specific lab_mode\n    @example\n    with util.ctx_lab_mode(\'eval\'):\n        foo()\n\n    @util.ctx_lab_mode(\'eval\')\n    def foo():\n        ...\n    \'\'\'\n    prev_lab_mode = os.environ.get(\'lab_mode\')\n    os.environ[\'lab_mode\'] = lab_mode\n    yield\n    if prev_lab_mode is None:\n        del os.environ[\'lab_mode\']\n    else:\n        os.environ[\'lab_mode\'] = prev_lab_mode\n\n\ndef monkey_patch(base_cls, extend_cls):\n    \'\'\'Monkey patch a base class with methods from extend_cls\'\'\'\n    ext_fn_list = get_fn_list(extend_cls)\n    for fn in ext_fn_list:\n        setattr(base_cls, fn, getattr(extend_cls, fn))\n\n\ndef parallelize(fn, args, num_cpus=NUM_CPUS):\n    \'\'\'\n    Parallelize a method fn, args and return results with order preserved per args.\n    args should be a list of tuples.\n    @returns {list} results Order preserved output from fn.\n    \'\'\'\n    pool = mp.Pool(num_cpus, maxtasksperchild=1)\n    results = pool.starmap(fn, args)\n    pool.close()\n    pool.join()\n    return results\n\n\ndef prepath_split(prepath):\n    \'\'\'\n    Split prepath into useful names. Works with predir (prename will be None)\n    prepath: data/dqn_pong_2018_12_02_082510/dqn_pong_t0_s0\n    predir: data/dqn_pong_2018_12_02_082510\n    prefolder: dqn_pong_2018_12_02_082510\n    prename: dqn_pong_t0_s0\n    spec_name: dqn_pong\n    experiment_ts: 2018_12_02_082510\n    \'\'\'\n    prepath = prepath.strip(\'_\')\n    tail = prepath.split(\'data/\')[-1]\n    if \'/\' in tail:  # tail = prefolder/prename\n        prefolder, prename = tail.split(\'/\', 1)\n    else:\n        prefolder, prename = tail, None\n    predir = f\'data/{prefolder}\'\n    spec_name = RE_FILE_TS.sub(\'\', prefolder).strip(\'_\')\n    experiment_ts = RE_FILE_TS.findall(prefolder)[0]\n    return predir, prefolder, prename, spec_name, experiment_ts\n\n\ndef prepath_to_idxs(prepath):\n    \'\'\'Extract trial index and session index from prepath if available\'\'\'\n    tidxs = re.findall(r\'_t(\\d+)\', prepath)\n    trial_index = int(tidxs[0]) if tidxs else None\n    sidxs = re.findall(r\'_s(\\d+)\', prepath)\n    session_index = int(sidxs[0]) if sidxs else None\n    return trial_index, session_index\n\n\ndef read(data_path, **kwargs):\n    \'\'\'\n    Universal data reading method with smart data parsing\n    - {.csv} to DataFrame\n    - {.json} to dict, list\n    - {.yml} to dict\n    - {*} to str\n    @param {str} data_path The data path to read from\n    @returns {data} The read data in sensible format\n    @example\n\n    data_df = util.read(\'test/fixture/lib/util/test_df.csv\')\n    # => <DataFrame>\n\n    data_dict = util.read(\'test/fixture/lib/util/test_dict.json\')\n    data_dict = util.read(\'test/fixture/lib/util/test_dict.yml\')\n    # => <dict>\n\n    data_list = util.read(\'test/fixture/lib/util/test_list.json\')\n    # => <list>\n\n    data_str = util.read(\'test/fixture/lib/util/test_str.txt\')\n    # => <str>\n    \'\'\'\n    data_path = smart_path(data_path)\n    try:\n        assert os.path.isfile(data_path)\n    except AssertionError:\n        raise FileNotFoundError(data_path)\n    ext = get_file_ext(data_path)\n    if ext == \'.csv\':\n        data = read_as_df(data_path, **kwargs)\n    elif ext == \'.pkl\':\n        data = read_as_pickle(data_path, **kwargs)\n    else:\n        data = read_as_plain(data_path, **kwargs)\n    return data\n\n\ndef read_as_df(data_path, **kwargs):\n    \'\'\'Submethod to read data as DataFrame\'\'\'\n    data = pd.read_csv(data_path, **kwargs)\n    return data\n\n\ndef read_as_pickle(data_path, **kwargs):\n    \'\'\'Submethod to read data as pickle\'\'\'\n    with open(data_path, \'rb\') as f:\n        data = pickle.load(f)\n    return data\n\n\ndef read_as_plain(data_path, **kwargs):\n    \'\'\'Submethod to read data as plain type\'\'\'\n    open_file = open(data_path, \'r\')\n    ext = get_file_ext(data_path)\n    if ext == \'.json\':\n        data = ujson.load(open_file, **kwargs)\n    elif ext == \'.yml\':\n        data = yaml.load(open_file, **kwargs)\n    else:\n        data = open_file.read()\n    open_file.close()\n    return data\n\n\ndef self_desc(cls, omit=None):\n    \'\'\'Method to get self description, used at init.\'\'\'\n    desc_list = [f\'{get_class_name(cls)}:\']\n    omit_list = ps.compact(cast_list(omit))\n    for k, v in get_class_attr(cls).items():\n        if k in omit_list:\n            continue\n        if k == \'spec\':  # spec components are described at their object level; for session, only desc spec.meta\n            desc_v = pformat(v[\'meta\'])\n        elif ps.is_dict(v) or ps.is_dict(ps.head(v)):\n            desc_v = pformat(v)\n        else:\n            desc_v = v\n        desc_list.append(f\'- {k} = {desc_v}\')\n    desc = \'\\n\'.join(desc_list)\n    return desc\n\n\ndef set_attr(obj, attr_dict, keys=None):\n    \'\'\'Set attribute of an object from a dict\'\'\'\n    if keys is not None:\n        attr_dict = ps.pick(attr_dict, keys)\n    for attr, val in attr_dict.items():\n        setattr(obj, attr, val)\n    return obj\n\n\ndef set_cuda_id(spec):\n    \'\'\'Use trial and session id to hash and modulo cuda device count for a cuda_id to maximize device usage. Sets the net_spec for the base Net class to pick up.\'\'\'\n    # Don\'t trigger any cuda call if not using GPU. Otherwise will break multiprocessing on machines with CUDA.\n    # see issues https://github.com/pytorch/pytorch/issues/334 https://github.com/pytorch/pytorch/issues/3491 https://github.com/pytorch/pytorch/issues/9996\n    for agent_spec in spec[\'agent\']:\n        if not agent_spec[\'net\'].get(\'gpu\'):\n            return\n    meta_spec = spec[\'meta\']\n    trial_idx = meta_spec[\'trial\'] or 0\n    session_idx = meta_spec[\'session\'] or 0\n    if meta_spec[\'distributed\'] == \'shared\':  # shared hogwild uses only global networks, offset them to idx 0\n        session_idx = 0\n    job_idx = trial_idx * meta_spec[\'max_session\'] + session_idx\n    job_idx += meta_spec[\'cuda_offset\']\n    device_count = torch.cuda.device_count()\n    cuda_id = job_idx % device_count if torch.cuda.is_available() else None\n\n    for agent_spec in spec[\'agent\']:\n        agent_spec[\'net\'][\'cuda_id\'] = cuda_id\n\n\ndef set_logger(spec, logger, unit=None):\n    \'\'\'Set the logger for a lab unit give its spec\'\'\'\n    os.environ[\'LOG_PREPATH\'] = insert_folder(get_prepath(spec, unit=unit), \'log\')\n    reload(logger)  # to set session-specific logger\n\n\ndef set_random_seed(spec):\n    \'\'\'Generate and set random seed for relevant modules, and record it in spec.meta.random_seed\'\'\'\n    trial = spec[\'meta\'][\'trial\']\n    session = spec[\'meta\'][\'session\']\n    random_seed = int(1e5 * (trial or 0) + 1e3 * (session or 0) + time.time())\n    torch.cuda.manual_seed_all(random_seed)\n    torch.manual_seed(random_seed)\n    np.random.seed(random_seed)\n    spec[\'meta\'][\'random_seed\'] = random_seed\n    return random_seed\n\n\ndef _sizeof(obj, seen=None):\n    \'\'\'Recursively finds size of objects\'\'\'\n    size = sys.getsizeof(obj)\n    if seen is None:\n        seen = set()\n    obj_id = id(obj)\n    if obj_id in seen:\n        return 0\n    # Important mark as seen *before* entering recursion to gracefully handle\n    # self-referential objects\n    seen.add(obj_id)\n    if isinstance(obj, dict):\n        size += sum([_sizeof(v, seen) for v in obj.values()])\n        size += sum([_sizeof(k, seen) for k in obj.keys()])\n    elif hasattr(obj, \'__dict__\'):\n        size += _sizeof(obj.__dict__, seen)\n    elif hasattr(obj, \'__iter__\') and not isinstance(obj, (str, bytes, bytearray)):\n        size += sum([_sizeof(i, seen) for i in obj])\n    return size\n\n\ndef sizeof(obj, divisor=1e6):\n    \'\'\'Return the size of object, in MB by default\'\'\'\n    return _sizeof(obj) / divisor\n\n\ndef smart_path(data_path, as_dir=False):\n    \'\'\'\n    Resolve data_path into abspath with fallback to join from ROOT_DIR\n    @param {str} data_path The input data path to resolve\n    @param {bool} as_dir Whether to return as dirname\n    @returns {str} The normalized absolute data_path\n    @example\n\n    util.smart_path(\'slm_lab/lib\')\n    # => \'/Users/ANON/Documents/slm_lab/slm_lab/lib\'\n\n    util.smart_path(\'/tmp\')\n    # => \'/tmp\'\n    \'\'\'\n    if not os.path.isabs(data_path):\n        data_path = os.path.join(ROOT_DIR, data_path)\n    if as_dir:\n        data_path = os.path.dirname(data_path)\n    return os.path.normpath(data_path)\n\n\ndef split_minibatch(batch, mb_size):\n    \'\'\'Split a batch into minibatches of mb_size or smaller, without replacement\'\'\'\n    size = len(batch[\'rewards\'])\n    assert mb_size < size, f\'Minibatch size {mb_size} must be < batch size {size}\'\n    idxs = np.arange(size)\n    np.random.shuffle(idxs)\n    chunks = int(size / mb_size)\n    nested_idxs = np.array_split(idxs[:chunks * mb_size], chunks)\n    if size % mb_size != 0:  # append leftover from split\n        nested_idxs += [idxs[chunks * mb_size:]]\n    mini_batches = []\n    for minibatch_idxs in nested_idxs:\n        minibatch = {k: v[minibatch_idxs] for k, v in batch.items()}\n        mini_batches.append(minibatch)\n    return mini_batches\n\n\ndef to_json(d, indent=2):\n    \'\'\'Shorthand method for stringify JSON with indent\'\'\'\n    return json.dumps(d, indent=indent, cls=LabJsonEncoder)\n\n\ndef to_render():\n    return os.environ.get(\'RENDER\', \'false\') == \'true\' or (get_lab_mode() in (\'dev\', \'enjoy\') and os.environ.get(\'RENDER\', \'true\') == \'true\')\n\n\ndef to_torch_batch(batch, device, is_episodic):\n    \'\'\'Mutate a batch (dict) to make its values from numpy into PyTorch tensor\'\'\'\n    for k in batch:\n        if is_episodic:  # for episodic format\n            batch[k] = np.concatenate(batch[k])\n        elif ps.is_list(batch[k]):\n            batch[k] = np.array(batch[k])\n        batch[k] = torch.from_numpy(batch[k].astype(np.float32)).to(device)\n    return batch\n\n\ndef write(data, data_path):\n    \'\'\'\n    Universal data writing method with smart data parsing\n    - {.csv} from DataFrame\n    - {.json} from dict, list\n    - {.yml} from dict\n    - {*} from str(*)\n    @param {*} data The data to write\n    @param {str} data_path The data path to write to\n    @returns {data_path} The data path written to\n    @example\n\n    data_path = util.write(data_df, \'test/fixture/lib/util/test_df.csv\')\n\n    data_path = util.write(data_dict, \'test/fixture/lib/util/test_dict.json\')\n    data_path = util.write(data_dict, \'test/fixture/lib/util/test_dict.yml\')\n\n    data_path = util.write(data_list, \'test/fixture/lib/util/test_list.json\')\n\n    data_path = util.write(data_str, \'test/fixture/lib/util/test_str.txt\')\n    \'\'\'\n    data_path = smart_path(data_path)\n    data_dir = os.path.dirname(data_path)\n    os.makedirs(data_dir, exist_ok=True)\n    ext = get_file_ext(data_path)\n    if ext == \'.csv\':\n        write_as_df(data, data_path)\n    elif ext == \'.pkl\':\n        write_as_pickle(data, data_path)\n    else:\n        write_as_plain(data, data_path)\n    return data_path\n\n\ndef write_as_df(data, data_path):\n    \'\'\'Submethod to write data as DataFrame\'\'\'\n    df = cast_df(data)\n    df.to_csv(data_path, index=False)\n    return data_path\n\n\ndef write_as_pickle(data, data_path):\n    \'\'\'Submethod to write data as pickle\'\'\'\n    with open(data_path, \'wb\') as f:\n        pickle.dump(data, f)\n    return data_path\n\n\ndef write_as_plain(data, data_path):\n    \'\'\'Submethod to write data as plain type\'\'\'\n    open_file = open(data_path, \'w\')\n    ext = get_file_ext(data_path)\n    if ext == \'.json\':\n        json.dump(data, open_file, indent=2, cls=LabJsonEncoder)\n    elif ext == \'.yml\':\n        yaml.dump(data, open_file)\n    else:\n        open_file.write(str(data))\n    open_file.close()\n    return data_path\n\n\n# Atari image preprocessing\n\n\ndef to_opencv_image(im):\n    \'\'\'Convert to OpenCV image shape h,w,c\'\'\'\n    shape = im.shape\n    if len(shape) == 3 and shape[0] < shape[-1]:\n        return im.transpose(1, 2, 0)\n    else:\n        return im\n\n\ndef to_pytorch_image(im):\n    \'\'\'Convert to PyTorch image shape c,h,w\'\'\'\n    shape = im.shape\n    if len(shape) == 3 and shape[-1] < shape[0]:\n        return im.transpose(2, 0, 1)\n    else:\n        return im\n\n\ndef grayscale_image(im):\n    return cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n\n\ndef resize_image(im, w_h):\n    return cv2.resize(im, w_h, interpolation=cv2.INTER_AREA)\n\n\ndef normalize_image(im):\n    \'\'\'Normalizing image by dividing max value 255\'\'\'\n    # NOTE: beware in its application, may cause loss to be 255 times lower due to smaller input values\n    return np.divide(im, 255.0)\n\n\ndef preprocess_image(im, w_h=(84, 84)):\n    \'\'\'\n    Image preprocessing using OpenAI Baselines method: grayscale, resize\n    This resize uses stretching instead of cropping\n    \'\'\'\n    im = to_opencv_image(im)\n    im = grayscale_image(im)\n    im = resize_image(im, w_h)\n    im = np.expand_dims(im, 0)\n    return im\n\n\ndef debug_image(im):\n    \'\'\'\n    Renders an image for debugging; pauses process until key press\n    Handles tensor/numpy and conventions among libraries\n    \'\'\'\n    if torch.is_tensor(im):  # if PyTorch tensor, get numpy\n        im = im.cpu().numpy()\n    im = to_opencv_image(im)\n    im = im.astype(np.uint8)  # typecast guard\n    if im.shape[0] == 3:  # RGB image\n        # accommodate from RGB (numpy) to BGR (cv2)\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    cv2.imshow(\'debug image\', im)\n    cv2.waitKey(0)\n'"
slm_lab/lib/viz.py,0,"b'# The data visualization module\n# Defines plotting methods for analysis\nfrom glob import glob\nfrom plotly import graph_objs as go, io as pio, subplots\nfrom plotly.offline import init_notebook_mode, iplot\nfrom slm_lab.lib import logger, util\nimport colorlover as cl\nimport os\nimport pydash as ps\n\nlogger = logger.get_logger(__name__)\npio.templates.default = \'none\'  # set default white background for plots\n# moving-average window size for plotting\nPLOT_MA_WINDOW = 100\n# warn orca failure only once\norca_warn_once = ps.once(lambda e: logger.warning(f\'Failed to generate graph. Run retro-analysis to generate graphs later. {e}\\nIf running on a headless server, prepend your Python command with `xvfb-run -a `, for example `xvfb-run -a python run_lab.py`\'))\nif util.is_jupyter():\n    init_notebook_mode(connected=True)\n\n\ndef calc_sr_ma(sr):\n    \'\'\'Calculate the moving-average of a series to be plotted\'\'\'\n    return sr.rolling(PLOT_MA_WINDOW, min_periods=1).mean()\n\n\ndef create_label(y_col, x_col, title=None, y_title=None, x_title=None, legend_name=None):\n    \'\'\'Create label dict for go.Layout with smart resolution\'\'\'\n    legend_name = legend_name or y_col\n    y_col_list, x_col_list, legend_name_list = ps.map_(\n        [y_col, x_col, legend_name], util.cast_list)\n    y_title = str(y_title or \',\'.join(y_col_list))\n    x_title = str(x_title or \',\'.join(x_col_list))\n    title = title or f\'{y_title} vs {x_title}\'\n\n    label = {\n        \'y_title\': y_title,\n        \'x_title\': x_title,\n        \'title\': title,\n        \'y_col_list\': y_col_list,\n        \'x_col_list\': x_col_list,\n        \'legend_name_list\': legend_name_list,\n    }\n    return label\n\n\ndef create_layout(title, y_title, x_title, x_type=None, width=500, height=500, layout_kwargs=None):\n    \'\'\'simplified method to generate Layout\'\'\'\n    layout = go.Layout(\n        title=title,\n        legend=dict(x=0.0, y=-0.25, orientation=\'h\'),\n        yaxis=dict(rangemode=\'tozero\', title=y_title),\n        xaxis=dict(type=x_type, title=x_title),\n        width=width, height=height,\n        margin=go.layout.Margin(l=60, r=30, t=60, b=60),\n    )\n    layout.update(layout_kwargs)\n    return layout\n\n\ndef get_palette(size):\n    \'\'\'Get the suitable palette of a certain size\'\'\'\n    if size <= 8:\n        palette = cl.scales[str(max(3, size))][\'qual\'][\'Set2\']\n    else:\n        palette = cl.interp(cl.scales[\'8\'][\'qual\'][\'Set2\'], size)\n    return palette\n\n\ndef lower_opacity(rgb, opacity):\n    return rgb.replace(\'rgb(\', \'rgba(\').replace(\'hsl(\', \'hsla(\').replace(\')\', f\', {opacity})\')\n\n\ndef plot(*args, **kwargs):\n    if util.is_jupyter():\n        return iplot(*args, **kwargs)\n\n\ndef plot_sr(sr, time_sr, title, y_title, x_title, color=None):\n    \'\'\'Plot a series\'\'\'\n    x = time_sr.tolist()\n    color = color or get_palette(1)[0]\n    main_trace = go.Scatter(\n        x=x, y=sr, mode=\'lines\', showlegend=False,\n        line={\'color\': color, \'width\': 1},\n    )\n    data = [main_trace]\n    layout = create_layout(title=title, y_title=y_title, x_title=x_title)\n    fig = go.Figure(data, layout)\n    plot(fig)\n    return fig\n\n\ndef plot_mean_sr(sr_list, time_sr, title, y_title, x_title, color=None):\n    \'\'\'Plot a list of series using its mean, with error bar using std\'\'\'\n    mean_sr, std_sr = util.calc_srs_mean_std(sr_list)\n    max_sr = mean_sr + std_sr\n    min_sr = mean_sr - std_sr\n    max_y = max_sr.tolist()\n    min_y = min_sr.tolist()\n    x = time_sr.tolist()\n    color = color or get_palette(1)[0]\n    main_trace = go.Scatter(\n        x=x, y=mean_sr, mode=\'lines\', showlegend=False,\n        line={\'color\': color, \'width\': 1},\n    )\n    envelope_trace = go.Scatter(\n        x=x + x[::-1], y=max_y + min_y[::-1], showlegend=False,\n        line={\'color\': \'rgba(0, 0, 0, 0)\'},\n        fill=\'tozerox\', fillcolor=lower_opacity(color, 0.15),\n    )\n    data = [main_trace, envelope_trace]\n    layout = create_layout(title=title, y_title=y_title, x_title=x_title)\n    fig = go.Figure(data, layout)\n    return fig\n\n\ndef save_image(figure, filepath):\n    if os.environ[\'PY_ENV\'] == \'test\':\n        return\n    filepath = util.smart_path(filepath)\n    try:\n        pio.write_image(figure, filepath, scale=2)\n    except Exception as e:\n        orca_warn_once(e)\n\n\n# analysis plot methods\n\ndef plot_session(session_spec, session_metrics, session_df, df_mode=\'eval\', ma=False):\n    \'\'\'\n    Plot the session graphs:\n    - mean_returns, strengths, sample_efficiencies, training_efficiencies, stabilities (with error bar)\n    - additional plots from session_df: losses, exploration variable, entropy\n    \'\'\'\n    meta_spec = session_spec[\'meta\']\n    prepath = meta_spec[\'prepath\']\n    graph_prepath = meta_spec[\'graph_prepath\']\n    title = f\'session graph: {session_spec[""name""]} t{meta_spec[""trial""]} s{meta_spec[""session""]}\'\n\n    local_metrics = session_metrics[\'local\']\n    name_time_pairs = [\n        (\'mean_returns\', \'frames\'),\n        (\'strengths\', \'frames\'),\n        (\'sample_efficiencies\', \'frames\'),\n        (\'training_efficiencies\', \'opt_steps\'),\n        (\'stabilities\', \'frames\'),\n    ]\n    for name, time in name_time_pairs:\n        sr = local_metrics[name]\n        if ma:\n            sr = calc_sr_ma(sr)\n            name = f\'{name}_ma\'  # for labeling\n        fig = plot_sr(\n            sr, local_metrics[time], title, name, time)\n        save_image(fig, f\'{graph_prepath}_session_graph_{df_mode}_{name}_vs_{time}.png\')\n        if name in (\'mean_returns\', \'mean_returns_ma\'):  # save important graphs in prepath directly\n            save_image(fig, f\'{prepath}_session_graph_{df_mode}_{name}_vs_{time}.png\')\n\n    if ma:\n        return\n    # training plots from session_df\n    name_time_pairs = [\n        (\'loss\', \'frame\'),\n        (\'explore_var\', \'frame\'),\n        (\'entropy\', \'frame\'),\n    ]\n    for name, time in name_time_pairs:\n        fig = plot_sr(\n            session_df[name], session_df[time], title, name, time)\n        save_image(fig, f\'{graph_prepath}_session_graph_{df_mode}_{name}_vs_{time}.png\')\n\n\ndef plot_trial(trial_spec, trial_metrics, ma=False):\n    \'\'\'\n    Plot the trial graphs:\n    - mean_returns, strengths, sample_efficiencies, training_efficiencies, stabilities (with error bar)\n    - consistencies (no error bar)\n    \'\'\'\n    meta_spec = trial_spec[\'meta\']\n    prepath = meta_spec[\'prepath\']\n    graph_prepath = meta_spec[\'graph_prepath\']\n    title = f\'trial graph: {trial_spec[""name""]} t{meta_spec[""trial""]} {meta_spec[""max_session""]} sessions\'\n\n    local_metrics = trial_metrics[\'local\']\n    name_time_pairs = [\n        (\'mean_returns\', \'frames\'),\n        (\'strengths\', \'frames\'),\n        (\'sample_efficiencies\', \'frames\'),\n        (\'training_efficiencies\', \'opt_steps\'),\n        (\'stabilities\', \'frames\'),\n        (\'consistencies\', \'frames\'),\n    ]\n    for name, time in name_time_pairs:\n        if name == \'consistencies\':\n            sr = local_metrics[name]\n            if ma:\n                sr = calc_sr_ma(sr)\n                name = f\'{name}_ma\'  # for labeling\n            fig = plot_sr(\n                sr, local_metrics[time], title, name, time)\n        else:\n            sr_list = local_metrics[name]\n            if ma:\n                sr_list = [calc_sr_ma(sr) for sr in sr_list]\n                name = f\'{name}_ma\'  # for labeling\n            fig = plot_mean_sr(\n                sr_list, local_metrics[time], title, name, time)\n        save_image(fig, f\'{graph_prepath}_trial_graph_{name}_vs_{time}.png\')\n        if name in (\'mean_returns\', \'mean_returns_ma\'):  # save important graphs in prepath directly\n            save_image(fig, f\'{prepath}_trial_graph_{name}_vs_{time}.png\')\n\n\ndef plot_experiment(experiment_spec, experiment_df, metrics_cols):\n    \'\'\'\n    Plot the metrics vs. specs parameters of an experiment, where each point is a trial.\n    ref colors: https://plot.ly/python/heatmaps-contours-and-2dhistograms-tutorial/#plotlys-predefined-color-scales\n    \'\'\'\n    y_cols = metrics_cols\n    x_cols = ps.difference(experiment_df.columns.tolist(), y_cols + [\'trial\'])\n    fig = subplots.make_subplots(rows=len(y_cols), cols=len(x_cols), shared_xaxes=True, shared_yaxes=True, print_grid=False)\n    strength_sr = experiment_df[\'strength\']\n    min_strength, max_strength = strength_sr.min(), strength_sr.max()\n    for row_idx, y in enumerate(y_cols):\n        for col_idx, x in enumerate(x_cols):\n            x_sr = experiment_df[x]\n            guard_cat_x = x_sr.astype(str) if x_sr.dtype == \'object\' else x_sr\n            trace = go.Scatter(\n                y=experiment_df[y], yaxis=f\'y{row_idx+1}\',\n                x=guard_cat_x, xaxis=f\'x{col_idx+1}\',\n                showlegend=False, mode=\'markers\',\n                marker={\n                    \'symbol\': \'circle-open-dot\', \'color\': strength_sr, \'opacity\': 0.5,\n                    # dump first portion of colorscale that is too bright\n                    \'cmin\': min_strength - 0.5 * (max_strength - min_strength), \'cmax\': max_strength,\n                    \'colorscale\': \'YlGnBu\', \'reversescale\': False\n                },\n            )\n            fig.add_trace(trace, row_idx + 1, col_idx + 1)\n            fig.update_xaxes(title_text=\'<br>\'.join(ps.chunk(x, 20)), zerolinewidth=1, categoryarray=sorted(guard_cat_x.unique()), row=len(y_cols), col=col_idx+1)\n        fig.update_yaxes(title_text=y, rangemode=\'tozero\', row=row_idx+1, col=1)\n    fig.layout.update(\n        title=f\'experiment graph: {experiment_spec[""name""]}\',\n        width=100 + 300 * len(x_cols), height=200 + 300 * len(y_cols))\n    plot(fig)\n    graph_prepath = experiment_spec[\'meta\'][\'graph_prepath\']\n    save_image(fig, f\'{graph_prepath}_experiment_graph.png\')\n    # save important graphs in prepath directly\n    prepath = experiment_spec[\'meta\'][\'prepath\']\n    save_image(fig, f\'{prepath}_experiment_graph.png\')\n    return fig\n\n\ndef plot_multi_local_metrics(local_metrics_list, legend_list, name, time, title, palette=None, showlegend=True):\n    \'\'\'Method to plot list local_metrics gathered from multiple trials, with ability to specify custom legend and title. Used by plot_multi_trial\'\'\'\n    palette = palette or get_palette(len(local_metrics_list))\n    all_data = []\n    for idx, local_metrics in enumerate(local_metrics_list):\n        fig = plot_mean_sr(\n            local_metrics[name], local_metrics[time], \'\', name, time, color=palette[idx])\n        if legend_list is not None:\n            # update legend for the main trace\n            fig.data[0].update({\'showlegend\': showlegend, \'name\': legend_list[idx]})\n        all_data += list(fig.data)\n    layout = create_layout(title, name, time)\n    fig = go.Figure(all_data, layout)\n    return fig\n\n\ndef plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath, ma=False, name_time_pairs=None, frame_scales=None, palette=None, showlegend=True):\n    \'\'\'\n    Plot multiple trial graphs together\n    This method can be used in analysis and also custom plotting by specifying the arguments manually\n    @example\n\n    trial_metrics_path_list = [\n        \'data/dqn_cartpole_2019_06_11_092512/info/dqn_cartpole_t0_trial_metrics.pkl\',\n        \'data/dqn_cartpole_2019_06_11_092512/info/dqn_cartpole_t1_trial_metrics.pkl\',\n    ]\n    legend_list = [\n        \'0\',\n        \'1\',\n    ]\n    title = f\'Multi trial trial graphs\'\n    graph_prepath = \'data/my_exp\'\n    viz.plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath)\n    \'\'\'\n    local_metrics_list = [util.read(path)[\'local\'] for path in trial_metrics_path_list]\n    # for plotting with async runs to adjust frame scale\n    if frame_scales is not None:\n        for idx, scale in frame_scales:\n            local_metrics_list[idx][\'frames\'] = local_metrics_list[idx][\'frames\'] * scale\n    name_time_pairs = name_time_pairs or [\n        (\'mean_returns\', \'frames\'),\n        (\'strengths\', \'frames\'),\n        (\'sample_efficiencies\', \'frames\'),\n        (\'training_efficiencies\', \'opt_steps\'),\n        (\'stabilities\', \'frames\')\n    ]\n    for name, time in name_time_pairs:\n        if ma:\n            for local_metrics in local_metrics_list:\n                sr_list = local_metrics[name]\n                sr_list = [calc_sr_ma(sr) for sr in sr_list]\n                local_metrics[f\'{name}_ma\'] = sr_list\n            name = f\'{name}_ma\'  # for labeling\n        fig = plot_multi_local_metrics(local_metrics_list, legend_list, name, time, title, palette, showlegend)\n        save_image(fig, f\'{graph_prepath}_multi_trial_graph_{name}_vs_{time}.png\')\n        if name in (\'mean_returns\', \'mean_returns_ma\'):  # save important graphs in prepath directly\n            prepath = graph_prepath.replace(\'/graph/\', \'/\')\n            save_image(fig, f\'{prepath}_multi_trial_graph_{name}_vs_{time}.png\')\n\n\ndef get_trial_legends(experiment_df, trial_idxs, metrics_cols):\n    \'\'\'Format trial variables in experiment_df into legend strings\'\'\'\n    var_df = experiment_df.drop(metrics_cols, axis=1).set_index(\'trial\')\n    trial_legends = []\n    for trial_idx in trial_idxs:\n        trial_vars = var_df.loc[trial_idx].to_dict()\n        var_list = []\n        for k, v in trial_vars.items():\n            if hasattr(v, \'__round__\'):\n                v = round(v, 8)  # prevent long float digits in formatting\n            var_list.append(f\'{k.split(""."").pop()} {v}\')\n        var_str = \' \'.join(var_list)\n        legend = f\'t{trial_idx}: {var_str}\'\n        trial_legends.append(legend)\n    trial_legends\n    return trial_legends\n\n\ndef plot_experiment_trials(experiment_spec, experiment_df, metrics_cols):\n    meta_spec = experiment_spec[\'meta\']\n    info_prepath = meta_spec[\'info_prepath\']\n    trial_metrics_path_list = glob(f\'{info_prepath}*_trial_metrics.pkl\')\n    # sort by trial id\n    trial_metrics_path_list = list(sorted(trial_metrics_path_list, key=lambda k: util.prepath_to_idxs(k)[0]))\n\n    # get trial indices to build legends\n    trial_idxs = [util.prepath_to_idxs(prepath)[0] for prepath in trial_metrics_path_list]\n    legend_list = get_trial_legends(experiment_df, trial_idxs, metrics_cols)\n\n    title = f\'multi trial graph: {experiment_spec[""name""]}\'\n    graph_prepath = meta_spec[\'graph_prepath\']\n    plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath)\n    plot_multi_trial(trial_metrics_path_list, legend_list, title, graph_prepath, ma=True)\n'"
slm_lab/spec/__init__.py,0,b''
slm_lab/spec/random_baseline.py,0,"b""# Module to generate random baselines\n# Run as: python slm_lab/spec/random_baseline.py\nfrom slm_lab.lib import logger, util\nimport gym\nimport numpy as np\nimport pydash as ps\nimport roboschool\n\n\nFILEPATH = 'slm_lab/spec/_random_baseline.json'\nNUM_EVAL = 100\n# extra envs to include\nINCLUDE_ENVS = [\n    'vizdoom-v0',\n]\nEXCLUDE_ENVS = [\n    'CarRacing-v0',  # window bug\n    'Reacher-v2',  # exclude mujoco\n    'Pusher-v2',\n    'Thrower-v2',\n    'Striker-v2',\n    'InvertedPendulum-v2',\n    'InvertedDoublePendulum-v2',\n    'HalfCheetah-v3',\n    'Hopper-v3',\n    'Swimmer-v3',\n    'Walker2d-v3',\n    'Ant-v3',\n    'Humanoid-v3',\n    'HumanoidStandup-v2',\n    'FetchSlide-v1',\n    'FetchPickAndPlace-v1',\n    'FetchReach-v1',\n    'FetchPush-v1',\n    'HandReach-v0',\n    'HandManipulateBlockRotateZ-v0',\n    'HandManipulateBlockRotateParallel-v0',\n    'HandManipulateBlockRotateXYZ-v0',\n    'HandManipulateBlockFull-v0',\n    'HandManipulateBlock-v0',\n    'HandManipulateBlockTouchSensors-v0',\n    'HandManipulateEggRotate-v0',\n    'HandManipulateEggFull-v0',\n    'HandManipulateEgg-v0',\n    'HandManipulateEggTouchSensors-v0',\n    'HandManipulatePenRotate-v0',\n    'HandManipulatePenFull-v0',\n    'HandManipulatePen-v0',\n    'HandManipulatePenTouchSensors-v0',\n    'FetchSlideDense-v1',\n    'FetchPickAndPlaceDense-v1',\n    'FetchReachDense-v1',\n    'FetchPushDense-v1',\n    'HandReachDense-v0',\n    'HandManipulateBlockRotateZDense-v0',\n    'HandManipulateBlockRotateParallelDense-v0',\n    'HandManipulateBlockRotateXYZDense-v0',\n    'HandManipulateBlockFullDense-v0',\n    'HandManipulateBlockDense-v0',\n    'HandManipulateBlockTouchSensorsDense-v0',\n    'HandManipulateEggRotateDense-v0',\n    'HandManipulateEggFullDense-v0',\n    'HandManipulateEggDense-v0',\n    'HandManipulateEggTouchSensorsDense-v0',\n    'HandManipulatePenRotateDense-v0',\n    'HandManipulatePenFullDense-v0',\n    'HandManipulatePenDense-v0',\n    'HandManipulatePenTouchSensorsDense-v0',\n]\n\n\ndef enum_envs():\n    '''Enumerate all the env names of the latest version'''\n    envs = [es.id for es in gym.envs.registration.registry.all()]\n    def get_name(s): return s.split('-')[0]\n    # filter out the old stuff\n    envs = ps.reverse(ps.uniq_by(ps.reverse(envs), get_name))\n    # filter out the excluded envs\n    envs = ps.difference_by(envs, EXCLUDE_ENVS, get_name)\n    envs += INCLUDE_ENVS\n    return envs\n\n\ndef gen_random_return(env_name, seed):\n    '''Generate a single-episode random policy return for an environment'''\n    # TODO generalize for unity too once it has a gym wrapper\n    env = gym.make(env_name)\n    env.seed(seed)\n    env.reset()\n    done = False\n    total_reward = 0\n    while not done:\n        _, reward, done, _ = env.step(env.action_space.sample())\n        total_reward += reward\n    return total_reward\n\n\ndef gen_random_baseline(env_name, num_eval=NUM_EVAL):\n    '''Generate the random baseline for an environment by averaging over num_eval episodes'''\n    returns = util.parallelize(gen_random_return, [(env_name, i) for i in range(num_eval)])\n    mean_rand_ret = np.mean(returns)\n    std_rand_ret = np.std(returns)\n    return {'mean': mean_rand_ret, 'std': std_rand_ret}\n\n\ndef get_random_baseline(env_name):\n    '''Get a single random baseline for env; if does not exist in file, generate live and update the file'''\n    random_baseline = util.read(FILEPATH)\n    if env_name in random_baseline:\n        baseline = random_baseline[env_name]\n    else:\n        try:\n            logger.info(f'Generating random baseline for {env_name}')\n            baseline = gen_random_baseline(env_name, NUM_EVAL)\n        except Exception as e:\n            logger.warning(f'Cannot start env: {env_name}, skipping random baseline generation')\n            baseline = None\n        # update immediately\n        logger.info(f'Updating new random baseline in {FILEPATH}')\n        random_baseline[env_name] = baseline\n        util.write(random_baseline, FILEPATH)\n    return baseline\n\n\ndef main():\n    '''\n    Main method to generate all random baselines and write to file.\n    Run as: python slm_lab/spec/random_baseline.py\n    '''\n    envs = enum_envs()\n    for idx, env_name in enumerate(envs):\n        logger.info(f'Generating random baseline for {env_name}: {idx + 1}/{len(envs)}')\n        get_random_baseline(env_name)\n    logger.info(f'Done, random baseline updated in {FILEPATH}')\n\n\nif __name__ == '__main__':\n    main()\n"""
slm_lab/spec/spec_util.py,0,"b'# The spec module\n# Manages specification to run things in lab\nfrom slm_lab import ROOT_DIR\nfrom slm_lab.lib import logger, util\nfrom string import Template\nimport itertools\nimport json\nimport os\nimport pydash as ps\n\n\nSPEC_DIR = \'slm_lab/spec\'\n\'\'\'\nAll spec values are already param, inferred automatically.\nTo change from a value into param range, e.g.\n- single: ""explore_anneal_epi"": 50\n- continuous param: ""explore_anneal_epi"": {""min"": 50, ""max"": 100, ""dist"": ""uniform""}\n- discrete range: ""explore_anneal_epi"": {""values"": [50, 75, 100]}\n\'\'\'\nSPEC_FORMAT = {\n    ""agent"": [{\n        ""name"": str,\n        ""algorithm"": dict,\n        ""memory"": dict,\n        ""net"": dict,\n    }],\n    ""env"": [{\n        ""name"": str,\n        ""max_t"": (type(None), int, float),\n        ""max_frame"": (int, float),\n    }],\n    ""body"": {\n        ""product"": [""outer"", ""inner"", ""custom""],\n        ""num"": (int, list),\n    },\n    ""meta"": {\n        ""max_session"": int,\n        ""max_trial"": (type(None), int),\n    },\n    ""name"": str,\n}\nlogger = logger.get_logger(__name__)\n\n\ndef check_comp_spec(comp_spec, comp_spec_format):\n    \'\'\'Base method to check component spec\'\'\'\n    for spec_k, spec_format_v in comp_spec_format.items():\n        comp_spec_v = comp_spec[spec_k]\n        if ps.is_list(spec_format_v):\n            v_set = spec_format_v\n            assert comp_spec_v in v_set, f\'Component spec value {ps.pick(comp_spec, spec_k)} needs to be one of {util.to_json(v_set)}\'\n        else:\n            v_type = spec_format_v\n            assert isinstance(comp_spec_v, v_type), f\'Component spec {ps.pick(comp_spec, spec_k)} needs to be of type: {v_type}\'\n            if isinstance(v_type, tuple) and int in v_type and isinstance(comp_spec_v, float):\n                # cast if it can be int\n                comp_spec[spec_k] = int(comp_spec_v)\n\n\ndef check_body_spec(spec):\n    \'\'\'Base method to check body spec for multi-agent multi-env\'\'\'\n    ae_product = ps.get(spec, \'body.product\')\n    body_num = ps.get(spec, \'body.num\')\n    if ae_product == \'outer\':\n        pass\n    elif ae_product == \'inner\':\n        agent_num = len(spec[\'agent\'])\n        env_num = len(spec[\'env\'])\n        assert agent_num == env_num, \'Agent and Env spec length must be equal for body `inner` product. Given {agent_num}, {env_num}\'\n    else:  # custom\n        assert ps.is_list(body_num)\n\n\ndef check_compatibility(spec):\n    \'\'\'Check compatibility among spec setups\'\'\'\n    # TODO expand to be more comprehensive\n    if spec[\'meta\'].get(\'distributed\') == \'synced\':\n        assert ps.get(spec, \'agent.0.net.gpu\') == False, f\'Distributed mode ""synced"" works with CPU only. Set gpu: false.\'\n\n\ndef check(spec):\n    \'\'\'Check a single spec for validity\'\'\'\n    try:\n        spec_name = spec.get(\'name\')\n        assert set(spec.keys()) >= set(SPEC_FORMAT.keys()), f\'Spec needs to follow spec.SPEC_FORMAT. Given \\n {spec_name}: {util.to_json(spec)}\'\n        for agent_spec in spec[\'agent\']:\n            check_comp_spec(agent_spec, SPEC_FORMAT[\'agent\'][0])\n        for env_spec in spec[\'env\']:\n            check_comp_spec(env_spec, SPEC_FORMAT[\'env\'][0])\n        check_comp_spec(spec[\'body\'], SPEC_FORMAT[\'body\'])\n        check_comp_spec(spec[\'meta\'], SPEC_FORMAT[\'meta\'])\n        # check_body_spec(spec)\n        check_compatibility(spec)\n    except Exception as e:\n        logger.exception(f\'spec {spec_name} fails spec check\')\n        raise e\n    return True\n\n\ndef check_all():\n    \'\'\'Check all spec files, all specs.\'\'\'\n    spec_files = ps.filter_(os.listdir(SPEC_DIR), lambda f: f.endswith(\'.json\') and not f.startswith(\'_\'))\n    for spec_file in spec_files:\n        spec_dict = util.read(f\'{SPEC_DIR}/{spec_file}\')\n        for spec_name, spec in spec_dict.items():\n            # fill-in info at runtime\n            spec[\'name\'] = spec_name\n            spec = extend_meta_spec(spec)\n            try:\n                check(spec)\n            except Exception as e:\n                logger.exception(f\'spec_file {spec_file} fails spec check\')\n                raise e\n    logger.info(f\'Checked all specs from: {ps.join(spec_files, "","")}\')\n    return True\n\n\ndef extend_meta_spec(spec, experiment_ts=None):\n    \'\'\'\n    Extend meta spec with information for lab functions\n    @param dict:spec\n    @param str:experiment_ts Use this experiment_ts if given; used for resuming training\n    \'\'\'\n    extended_meta_spec = {\n        \'rigorous_eval\': ps.get(spec, \'meta.rigorous_eval\', 0),\n        # reset lab indices to -1 so that they tick to 0\n        \'experiment\': -1,\n        \'trial\': -1,\n        \'session\': -1,\n        \'cuda_offset\': int(os.environ.get(\'CUDA_OFFSET\', 0)),\n        \'resume\': experiment_ts is not None,\n        \'experiment_ts\': experiment_ts or util.get_ts(),\n        \'prepath\': None,\n        \'git_sha\': util.get_git_sha(),\n        \'random_seed\': None,\n    }\n    spec[\'meta\'].update(extended_meta_spec)\n    return spec\n\n\ndef get(spec_file, spec_name, experiment_ts=None):\n    \'\'\'\n    Get an experiment spec from spec_file, spec_name.\n    Auto-check spec.\n    @param str:spec_file\n    @param str:spec_name\n    @param str:experiment_ts Use this experiment_ts if given; used for resuming training\n    @example\n\n    spec = spec_util.get(\'demo.json\', \'dqn_cartpole\')\n    \'\'\'\n    spec_file = spec_file.replace(SPEC_DIR, \'\')  # guard\n    spec_file = f\'{SPEC_DIR}/{spec_file}\'  # allow direct filename\n    spec_dict = util.read(spec_file)\n    assert spec_name in spec_dict, f\'spec_name {spec_name} is not in spec_file {spec_file}. Choose from:\\n {ps.join(spec_dict.keys(), "","")}\'\n    spec = spec_dict[spec_name]\n    # fill-in info at runtime\n    spec[\'name\'] = spec_name\n    spec = extend_meta_spec(spec, experiment_ts)\n    check(spec)\n    return spec\n\n\ndef get_param_specs(spec):\n    \'\'\'Return a list of specs with substituted spec_params\'\'\'\n    assert \'spec_params\' in spec, \'Parametrized spec needs a spec_params key\'\n    spec_params = spec.pop(\'spec_params\')\n    spec_template = Template(json.dumps(spec))\n    keys = spec_params.keys()\n    specs = []\n    for idx, vals in enumerate(itertools.product(*spec_params.values())):\n        spec_str = spec_template.substitute(dict(zip(keys, vals)))\n        spec = json.loads(spec_str)\n        spec[\'name\'] += f\'_{""_"".join(vals)}\'\n        # offset to prevent parallel-run GPU competition, to mod in util.set_cuda_id\n        spec[\'meta\'][\'cuda_offset\'] += idx * spec[\'meta\'][\'max_session\']\n        specs.append(spec)\n    return specs\n\n\ndef _override_dev_spec(spec):\n    spec[\'meta\'][\'max_session\'] = 1\n    spec[\'meta\'][\'max_trial\'] = 2\n    return spec\n\n\ndef _override_enjoy_spec(spec):\n    spec[\'meta\'][\'max_session\'] = 1\n    return spec\n\n\ndef _override_test_spec(spec):\n    for agent_spec in spec[\'agent\']:\n        # onpolicy freq is episodic\n        freq = 1 if agent_spec[\'memory\'][\'name\'] == \'OnPolicyReplay\' else 8\n        agent_spec[\'algorithm\'][\'training_frequency\'] = freq\n        agent_spec[\'algorithm\'][\'time_horizon\'] = freq\n        agent_spec[\'algorithm\'][\'training_start_step\'] = 1\n        agent_spec[\'algorithm\'][\'training_iter\'] = 1\n        agent_spec[\'algorithm\'][\'training_batch_iter\'] = 1\n    for env_spec in spec[\'env\']:\n        env_spec[\'max_frame\'] = 40\n        if env_spec.get(\'num_envs\', 1) > 1:\n            env_spec[\'num_envs\'] = 2\n        env_spec[\'max_t\'] = 12\n    spec[\'meta\'][\'log_frequency\'] = 10\n    spec[\'meta\'][\'eval_frequency\'] = 10\n    spec[\'meta\'][\'max_session\'] = 1\n    spec[\'meta\'][\'max_trial\'] = 2\n    return spec\n\n\ndef override_spec(spec, mode):\n    \'\'\'Override spec based on the (lab_)mode, do nothing otherwise.\'\'\'\n    overrider = {\n        \'dev\': _override_dev_spec,\n        \'enjoy\': _override_enjoy_spec,\n        \'test\': _override_test_spec,\n    }.get(mode)\n    if overrider is not None:\n        overrider(spec)\n    return spec\n\n\ndef save(spec, unit=\'experiment\'):\n    \'\'\'Save spec to proper path. Called at Experiment or Trial init.\'\'\'\n    prepath = util.get_prepath(spec, unit)\n    util.write(spec, f\'{prepath}_spec.json\')\n\n\ndef tick(spec, unit):\n    \'\'\'\n    Method to tick lab unit (experiment, trial, session) in meta spec to advance their indices\n    Reset lower lab indices to -1 so that they tick to 0\n    spec_util.tick(spec, \'session\')\n    session = Session(spec)\n    \'\'\'\n    if util.get_lab_mode() == \'enjoy\':  # don\'t tick in enjoy mode\n        return spec\n\n    meta_spec = spec[\'meta\']\n    if unit == \'experiment\':\n        meta_spec[\'experiment_ts\'] = util.get_ts()\n        meta_spec[\'experiment\'] += 1\n        meta_spec[\'trial\'] = -1\n        meta_spec[\'session\'] = -1\n    elif unit == \'trial\':\n        if meta_spec[\'experiment\'] == -1:\n            meta_spec[\'experiment\'] += 1\n        meta_spec[\'trial\'] += 1\n        meta_spec[\'session\'] = -1\n    elif unit == \'session\':\n        if meta_spec[\'experiment\'] == -1:\n            meta_spec[\'experiment\'] += 1\n        if meta_spec[\'trial\'] == -1:\n            meta_spec[\'trial\'] += 1\n        meta_spec[\'session\'] += 1\n    else:\n        raise ValueError(f\'Unrecognized lab unit to tick: {unit}\')\n    # set prepath since it is determined at this point\n    meta_spec[\'prepath\'] = prepath = util.get_prepath(spec, unit)\n    for folder in (\'graph\', \'info\', \'log\', \'model\'):\n        folder_prepath = util.insert_folder(prepath, folder)\n        folder_predir = os.path.dirname(f\'{ROOT_DIR}/{folder_prepath}\')\n        os.makedirs(folder_predir, exist_ok=True)\n        assert os.path.exists(folder_predir)\n        meta_spec[f\'{folder}_prepath\'] = folder_prepath\n    return spec\n'"
test/env/test_registration.py,0,"b'from slm_lab.env.registration import get_env_path\nimport pytest\n\n\n@pytest.mark.skip(reason=""Not implemented yet"")\ndef test_get_env_path():\n    assert \'node_modules/slm-env-3dball/build/3dball\' in get_env_path(\n        \'3dball\')\n'"
test/env/test_vec_env.py,0,"b""from slm_lab.env.vec_env import make_gym_venv\nimport numpy as np\nimport pytest\n\n\n@pytest.mark.parametrize('name,state_shape,reward_scale', [\n    ('PongNoFrameskip-v4', (1, 84, 84), 'sign'),\n    ('LunarLander-v2', (8,), None),\n    ('CartPole-v0', (4,), None),\n])\n@pytest.mark.parametrize('num_envs', (1, 4))\ndef test_make_gym_venv_nostack(name, num_envs, state_shape, reward_scale):\n    seed = 0\n    frame_op = None\n    frame_op_len = None\n    venv = make_gym_venv(name, num_envs, seed, frame_op=frame_op, frame_op_len=frame_op_len, reward_scale=reward_scale)\n    venv.reset()\n    for i in range(5):\n        state, reward, done, info = venv.step([venv.action_space.sample()] * num_envs)\n\n    assert isinstance(state, np.ndarray)\n    assert state.shape == (num_envs,) + state_shape\n    assert isinstance(reward, np.ndarray)\n    assert reward.shape == (num_envs,)\n    assert isinstance(done, np.ndarray)\n    assert done.shape == (num_envs,)\n    assert len(info) == num_envs\n    venv.close()\n\n\n@pytest.mark.parametrize('name,state_shape, reward_scale', [\n    ('PongNoFrameskip-v4', (1, 84, 84), 'sign'),\n    ('LunarLander-v2', (8,), None),\n    ('CartPole-v0', (4,), None),\n])\n@pytest.mark.parametrize('num_envs', (1, 4))\ndef test_make_gym_concat(name, num_envs, state_shape, reward_scale):\n    seed = 0\n    frame_op = 'concat'  # used for image, or for concat vector\n    frame_op_len = 4\n    venv = make_gym_venv(name, num_envs, seed, frame_op=frame_op, frame_op_len=frame_op_len, reward_scale=reward_scale)\n    venv.reset()\n    for i in range(5):\n        state, reward, done, info = venv.step([venv.action_space.sample()] * num_envs)\n\n    assert isinstance(state, np.ndarray)\n    stack_shape = (num_envs, frame_op_len * state_shape[0],) + state_shape[1:]\n    assert state.shape == stack_shape\n    assert isinstance(reward, np.ndarray)\n    assert reward.shape == (num_envs,)\n    assert isinstance(done, np.ndarray)\n    assert done.shape == (num_envs,)\n    assert len(info) == num_envs\n    venv.close()\n\n\n@pytest.mark.skip(reason='Not implemented yet')\n@pytest.mark.parametrize('name,state_shape,reward_scale', [\n    ('LunarLander-v2', (8,), None),\n    ('CartPole-v0', (4,), None),\n])\n@pytest.mark.parametrize('num_envs', (1, 4))\ndef test_make_gym_stack(name, num_envs, state_shape, reward_scale):\n    seed = 0\n    frame_op = 'stack'  # used for rnn\n    frame_op_len = 4\n    venv = make_gym_venv(name, num_envs, seed, frame_op=frame_op, frame_op_len=frame_op_len, reward_scale=reward_scale)\n    venv.reset()\n    for i in range(5):\n        state, reward, done, info = venv.step([venv.action_space.sample()] * num_envs)\n\n    assert isinstance(state, np.ndarray)\n    stack_shape = (num_envs, frame_op_len,) + state_shape\n    assert state.shape == stack_shape\n    assert isinstance(reward, np.ndarray)\n    assert reward.shape == (num_envs,)\n    assert isinstance(done, np.ndarray)\n    assert done.shape == (num_envs,)\n    assert len(info) == num_envs\n    venv.close()\n\n\n@pytest.mark.parametrize('name,state_shape,image_downsize', [\n    ('PongNoFrameskip-v4', (1, 84, 84), (84, 84)),\n    ('PongNoFrameskip-v4', (1, 64, 64), (64, 64)),\n])\n@pytest.mark.parametrize('num_envs', (1, 4))\ndef test_make_gym_venv_downsize(name, num_envs, state_shape, image_downsize):\n    seed = 0\n    frame_op = None\n    frame_op_len = None\n    venv = make_gym_venv(name, num_envs, seed, frame_op=frame_op, frame_op_len=frame_op_len, image_downsize=image_downsize)\n    venv.reset()\n    for i in range(5):\n        state, reward, done, info = venv.step([venv.action_space.sample()] * num_envs)\n\n    assert isinstance(state, np.ndarray)\n    assert state.shape == (num_envs,) + state_shape\n    assert isinstance(reward, np.ndarray)\n    assert reward.shape == (num_envs,)\n    assert isinstance(done, np.ndarray)\n    assert done.shape == (num_envs,)\n    assert len(info) == num_envs\n    venv.close()\n"""
test/env/test_wrapper.py,0,"b""from slm_lab.env.wrapper import make_gym_env, LazyFrames\nimport numpy as np\nimport pytest\n\n\n@pytest.mark.parametrize('name,state_shape,reward_scale', [\n    ('PongNoFrameskip-v4', (1, 84, 84), 'sign'),\n    ('LunarLander-v2', (8,), None),\n    ('CartPole-v0', (4,), None),\n])\ndef test_make_gym_env_nostack(name, state_shape, reward_scale):\n    seed = 0\n    frame_op = None\n    frame_op_len = None\n    env = make_gym_env(name, seed, frame_op=frame_op, frame_op_len=frame_op_len, reward_scale=reward_scale)\n    env.reset()\n    for i in range(5):\n        state, reward, done, info = env.step(env.action_space.sample())\n\n    assert isinstance(state, np.ndarray)\n    assert state.shape == state_shape\n    assert state.shape == env.observation_space.shape\n    assert isinstance(reward, float)\n    assert isinstance(done, bool)\n    assert isinstance(info, dict)\n    env.close()\n\n\n@pytest.mark.parametrize('name,state_shape,reward_scale', [\n    ('PongNoFrameskip-v4', (1, 84, 84), 'sign'),\n    ('LunarLander-v2', (8,), None),\n    ('CartPole-v0', (4,), None),\n])\ndef test_make_gym_env_concat(name, state_shape, reward_scale):\n    seed = 0\n    frame_op = 'concat'  # used for image, or for concat vector\n    frame_op_len = 4\n    env = make_gym_env(name, seed, frame_op=frame_op, frame_op_len=frame_op_len, reward_scale=reward_scale)\n    env.reset()\n    for i in range(5):\n        state, reward, done, info = env.step(env.action_space.sample())\n\n    assert isinstance(state, LazyFrames)\n    state = state.__array__()  # realize data\n    assert isinstance(state, np.ndarray)\n    # concat multiplies first dim\n    stack_shape = (frame_op_len * state_shape[0],) + state_shape[1:]\n    assert state.shape == stack_shape\n    assert state.shape == env.observation_space.shape\n    assert isinstance(reward, float)\n    assert isinstance(done, bool)\n    assert isinstance(info, dict)\n    env.close()\n\n\n@pytest.mark.parametrize('name,state_shape, reward_scale', [\n    ('LunarLander-v2', (8,), None),\n    ('CartPole-v0', (4,), None),\n])\ndef test_make_gym_env_stack(name, state_shape, reward_scale):\n    seed = 0\n    frame_op = 'stack'  # used for rnn\n    frame_op_len = 4\n    env = make_gym_env(name, seed, frame_op=frame_op, frame_op_len=frame_op_len, reward_scale=reward_scale)\n    env.reset()\n    for i in range(5):\n        state, reward, done, info = env.step(env.action_space.sample())\n\n    assert isinstance(state, LazyFrames)\n    state = state.__array__()  # realize data\n    assert isinstance(state, np.ndarray)\n    # stack creates new dim\n    stack_shape = (frame_op_len, ) + state_shape\n    assert state.shape == stack_shape\n    assert state.shape == env.observation_space.shape\n    assert isinstance(reward, float)\n    assert isinstance(done, bool)\n    assert isinstance(info, dict)\n    env.close()\n\n\n@pytest.mark.parametrize('name,state_shape,image_downsize', [\n    ('PongNoFrameskip-v4', (1, 84, 84), (84, 84)),\n    ('PongNoFrameskip-v4', (1, 64, 64), (64, 64)),\n])\ndef test_make_gym_env_downsize(name, state_shape, image_downsize):\n    seed = 0\n    frame_op = None\n    frame_op_len = None\n    env = make_gym_env(name, seed, frame_op=frame_op, frame_op_len=frame_op_len, image_downsize=image_downsize)\n    env.reset()\n    for i in range(5):\n        state, reward, done, info = env.step(env.action_space.sample())\n\n    assert isinstance(state, np.ndarray)\n    assert state.shape == state_shape\n    assert state.shape == env.observation_space.shape\n    assert isinstance(reward, float)\n    assert isinstance(done, bool)\n    assert isinstance(info, dict)\n    env.close()\n"""
test/experiment/test_control.py,0,"b'from copy import deepcopy\nfrom flaky import flaky\nfrom slm_lab.experiment import analysis\nfrom slm_lab.experiment.control import Session, Trial, Experiment\nfrom slm_lab.spec import spec_util\nimport pandas as pd\nimport pytest\n\n\ndef test_session(test_spec):\n    spec_util.tick(test_spec, \'trial\')\n    spec_util.tick(test_spec, \'session\')\n    spec_util.save(test_spec, unit=\'trial\')\n    session = Session(test_spec)\n    session_metrics = session.run()\n    assert isinstance(session_metrics, dict)\n\n\ndef test_trial(test_spec):\n    spec_util.tick(test_spec, \'trial\')\n    spec_util.save(test_spec, unit=\'trial\')\n    trial = Trial(test_spec)\n    trial_metrics = trial.run()\n    assert isinstance(trial_metrics, dict)\n\n\ndef test_trial_demo():\n    spec = spec_util.get(\'demo.json\', \'dqn_cartpole\')\n    spec_util.save(spec, unit=\'experiment\')\n    spec = spec_util.override_spec(spec, \'test\')\n    spec_util.tick(spec, \'trial\')\n    trial_metrics = Trial(spec).run()\n    assert isinstance(trial_metrics, dict)\n\n\n@pytest.mark.skip(reason=""Unstable"")\n@flaky\ndef test_demo_performance():\n    spec = spec_util.get(\'demo.json\', \'dqn_cartpole\')\n    spec_util.save(spec, unit=\'experiment\')\n    for env_spec in spec[\'env\']:\n        env_spec[\'max_frame\'] = 2000\n    spec_util.tick(spec, \'trial\')\n    trial = Trial(spec)\n    spec_util.tick(spec, \'session\')\n    session = Session(spec)\n    session.run()\n    last_reward = session.agent.body.train_df.iloc[-1][\'total_reward\']\n    assert last_reward > 50, f\'last_reward is too low: {last_reward}\'\n\n\n@pytest.mark.skip(reason=""Cant run on CI"")\ndef test_experiment():\n    spec = spec_util.get(\'demo.json\', \'dqn_cartpole\')\n    spec_util.save(spec, unit=\'experiment\')\n    spec = spec_util.override_spec(spec, \'test\')\n    spec_util.tick(spec, \'experiment\')\n    experiment_df = Experiment(spec).run()\n    assert isinstance(experiment_df, pd.DataFrame)\n'"
test/experiment/test_monitor.py,0,b'import pytest\n\n# TODO add these tests\n\n\ndef test_clock():\n    return\n\n\ndef test_body():\n    return\n'
test/lib/test_distribution.py,12,"b""from flaky import flaky\nfrom slm_lab.lib import distribution\nimport pytest\nimport torch\n\n\n@pytest.mark.parametrize('pdparam_type', [\n    'probs', 'logits'\n])\ndef test_argmax(pdparam_type):\n    pdparam = torch.tensor([1.1, 10.0, 2.1])\n    # test both probs or logits\n    pd = distribution.Argmax(**{pdparam_type: pdparam})\n    for _ in range(10):\n        assert pd.sample().item() == 1\n    assert torch.equal(pd.probs, torch.tensor([0., 1., 0.]))\n\n\n@flaky\n@pytest.mark.parametrize('pdparam_type', [\n    'probs', 'logits'\n])\ndef test_gumbel_categorical(pdparam_type):\n    pdparam = torch.tensor([1.1, 10.0, 2.1])\n    pd = distribution.GumbelSoftmax(**{pdparam_type: pdparam, 'temperature': torch.tensor(1.0)})\n    for _ in range(10):\n        assert torch.is_tensor(pd.sample())\n\n\n@pytest.mark.parametrize('pdparam_type', [\n    'probs', 'logits'\n])\ndef test_multicategorical(pdparam_type):\n    pdparam0 = torch.tensor([10.0, 0.0, 0.0])\n    pdparam1 = torch.tensor([0.0, 10.0, 0.0])\n    pdparam2 = torch.tensor([0.0, 0.0, 10.0])\n    pdparams = [pdparam0, pdparam1, pdparam2]\n    # use a probs\n    pd = distribution.MultiCategorical(**{pdparam_type: pdparams})\n    assert isinstance(pd.probs, list)\n    # test probs only since if init from logits, probs will be close but not precise\n    if pdparam_type == 'probs':\n        assert torch.equal(pd.probs[0], torch.tensor([1., 0., 0.]))\n        assert torch.equal(pd.probs[1], torch.tensor([0., 1., 0.]))\n        assert torch.equal(pd.probs[2], torch.tensor([0., 0., 1.]))\n    for _ in range(10):\n        assert torch.equal(pd.sample(), torch.tensor([0, 1, 2]))\n"""
test/lib/test_logger.py,0,b'from slm_lab.lib import logger\n\n\ndef test_logger(test_str):\n    logger.critical(test_str)\n    logger.debug(test_str)\n    logger.error(test_str)\n    logger.exception(test_str)\n    logger.info(test_str)\n    logger.warning(test_str)\n'
test/lib/test_math_util.py,11,"b""from slm_lab.lib import math_util\nimport numpy as np\nimport pytest\nimport torch\n\n\n@pytest.mark.parametrize('base_shape', [\n    [],  # scalar\n    [2],  # vector\n    [4, 84, 84],  # image\n])\ndef test_venv_pack(base_shape):\n    batch_size = 5\n    num_envs = 4\n    batch_arr = torch.zeros([batch_size, num_envs] + base_shape)\n    unpacked_arr = math_util.venv_unpack(batch_arr)\n    packed_arr = math_util.venv_pack(unpacked_arr, num_envs)\n    assert list(packed_arr.shape) == [batch_size, num_envs] + base_shape\n\n\n@pytest.mark.parametrize('base_shape', [\n    [],  # scalar\n    [2],  # vector\n    [4, 84, 84],  # image\n])\ndef test_venv_unpack(base_shape):\n    batch_size = 5\n    num_envs = 4\n    batch_arr = torch.zeros([batch_size, num_envs] + base_shape)\n    unpacked_arr = math_util.venv_unpack(batch_arr)\n    assert list(unpacked_arr.shape) == [batch_size * num_envs] + base_shape\n\n\ndef test_calc_gaes():\n    rewards = torch.tensor([1., 0., 1., 1., 0., 1., 1., 1.])\n    dones = torch.tensor([0., 0., 1., 1., 0., 0., 0., 0.])\n    v_preds = torch.tensor([1.1, 0.1, 1.1, 1.1, 0.1, 1.1, 1.1, 1.1, 1.1])\n    assert len(v_preds) == len(rewards) + 1  # includes last state\n    gamma = 0.99\n    lam = 0.95\n    gaes = math_util.calc_gaes(rewards, dones, v_preds, gamma, lam)\n    res = torch.tensor([0.84070045, 0.89495, -0.1, -0.1, 3.616724, 2.7939649, 1.9191545, 0.989])\n    # use allclose instead of equal to account for atol\n    assert torch.allclose(gaes, res)\n\n\n@pytest.mark.parametrize('start_val, end_val, start_step, end_step, step, correct', [\n    (0.1, 0.0, 0, 100, 0, 0.1),\n    (0.1, 0.0, 0, 100, 50, 0.05),\n    (0.1, 0.0, 0, 100, 100, 0.0),\n    (0.1, 0.0, 0, 100, 150, 0.0),\n    (0.1, 0.0, 100, 200, 50, 0.1),\n    (0.1, 0.0, 100, 200, 100, 0.1),\n    (0.1, 0.0, 100, 200, 150, 0.05),\n    (0.1, 0.0, 100, 200, 200, 0.0),\n    (0.1, 0.0, 100, 200, 250, 0.0),\n])\ndef test_linear_decay(start_val, end_val, start_step, end_step, step, correct):\n    assert math_util.linear_decay(start_val, end_val, start_step, end_step, step) == correct\n\n\n@pytest.mark.parametrize('start_val, end_val, start_step, end_step, step, correct', [\n    (1.0, 0.0, 0, 100, 0, 1.0),\n    (1.0, 0.0, 0, 100, 5, 0.9),\n    (1.0, 0.0, 0, 100, 10, 0.81),\n    (1.0, 0.0, 0, 100, 25, 0.59049),\n    (1.0, 0.0, 0, 100, 50, 0.3486784401),\n    (1.0, 0.0, 0, 100, 100, 0.0),\n    (1.0, 0.0, 0, 100, 150, 0.0),\n    (1.0, 0.0, 100, 200, 0, 1.0),\n    (1.0, 0.0, 100, 200, 50, 1.0),\n    (1.0, 0.0, 100, 200, 100, 1.0),\n    (1.0, 0.0, 100, 200, 105, 0.9),\n    (1.0, 0.0, 100, 200, 125, 0.59049),\n    (1.0, 0.0, 100, 200, 200, 0.0),\n    (1.0, 0.0, 100, 200, 250, 0.0),\n])\ndef test_rate_decay(start_val, end_val, start_step, end_step, step, correct):\n    np.testing.assert_almost_equal(math_util.rate_decay(start_val, end_val, start_step, end_step, step), correct)\n\ndef test_calc_q_value_logits():\n    state_value = torch.tensor([[1.], [2.], [3.]])\n    advantages = torch.tensor([\n        [0., 1.],\n        [1., 1.],\n        [1., 0.]])\n    result = torch.tensor([\n        [0.5, 1.5],\n        [2.0, 2.0],\n        [3.5, 2.5]])\n    out = math_util.calc_q_value_logits(state_value, advantages)\n    assert torch.allclose(out, result)\n"""
test/lib/test_util.py,0,"b""from slm_lab.agent import Agent\nfrom slm_lab.lib import util\nimport numpy as np\nimport os\nimport pandas as pd\nimport pydash as ps\nimport pytest\n\n\ndef test_calc_ts_diff():\n    ts1 = '2017_10_17_084739'\n    ts2 = '2017_10_17_084740'\n    ts_diff = util.calc_ts_diff(ts2, ts1)\n    assert ts_diff == '0:00:01'\n\n\ndef test_cast_df(test_df, test_list):\n    assert isinstance(test_df, pd.DataFrame)\n    assert isinstance(util.cast_df(test_df), pd.DataFrame)\n\n    assert not isinstance(test_list, pd.DataFrame)\n    assert isinstance(util.cast_df(test_list), pd.DataFrame)\n\n\ndef test_cast_list(test_list, test_str):\n    assert ps.is_list(test_list)\n    assert ps.is_list(util.cast_list(test_list))\n\n    assert not ps.is_list(test_str)\n    assert ps.is_list(util.cast_list(test_str))\n\n\n@pytest.mark.parametrize('d,flat_d', [\n    ({'a': 1}, {'a': 1}),\n    ({'a': {'b': 1}}, {'a.b': 1}),\n    ({\n        'level1': {\n            'level2': {\n                'level3': 0,\n                'level3b': 1\n            },\n            'level2b': {\n                'level3': [2, 3]\n            }\n        }\n    }, {'level1.level2.level3': 0,\n        'level1.level2.level3b': 1,\n        'level1.level2b.level3': [2, 3]}),\n    ({\n        'level1': {\n            'level2': [\n                  {'level3': 0},\n                  {'level3b': 1}\n            ],\n            'level2b': {\n                'level3': [2, 3]\n            }\n        }\n    }, {'level1.level2.0.level3': 0,\n        'level1.level2.1.level3b': 1,\n        'level1.level2b.level3': [2, 3]}),\n])\ndef test_flatten_dict(d, flat_d):\n    assert util.flatten_dict(d) == flat_d\n\n\ndef test_get_fn_list():\n    fn_list = util.get_fn_list(Agent)\n    assert 'act' in fn_list\n    assert 'update' in fn_list\n\n\ndef test_get_ts():\n    ts = util.get_ts()\n    assert ps.is_string(ts)\n    assert util.RE_FILE_TS.match(ts)\n\n\ndef test_insert_folder():\n    assert util.insert_folder('data/dqn_pong_2018_12_02_082510/dqn_pong_t0_s0', 'model') == 'data/dqn_pong_2018_12_02_082510/model/dqn_pong_t0_s0'\n\n\ndef test_is_jupyter():\n    assert not util.is_jupyter()\n\n\ndef test_prepath_split():\n    prepath = 'data/dqn_pong_2018_12_02_082510/dqn_pong_t0_s0'\n    predir, prefolder, prename, spec_name, experiment_ts = util.prepath_split(prepath)\n    assert predir == 'data/dqn_pong_2018_12_02_082510'\n    assert prefolder == 'dqn_pong_2018_12_02_082510'\n    assert prename == 'dqn_pong_t0_s0'\n    assert spec_name == 'dqn_pong'\n    assert experiment_ts == '2018_12_02_082510'\n\n\ndef test_set_attr():\n    class Foo:\n        bar = 0\n    foo = Foo()\n    util.set_attr(foo, {'bar': 1, 'baz': 2})\n    assert foo.bar == 1\n    assert foo.baz == 2\n\n\ndef test_smart_path():\n    rel_path = 'test/lib/test_util.py'\n    fake_rel_path = 'test/lib/test_util.py_fake'\n    abs_path = os.path.abspath(__file__)\n    assert util.smart_path(rel_path) == abs_path\n    assert util.smart_path(fake_rel_path) == abs_path + '_fake'\n    assert util.smart_path(abs_path) == abs_path\n    assert util.smart_path(abs_path, as_dir=True) == os.path.dirname(abs_path)\n\n\n@pytest.mark.parametrize('filename,dtype', [\n    ('test_df.csv', pd.DataFrame),\n])\ndef test_write_read_as_df(test_df, filename, dtype):\n    data_path = f'test/fixture/lib/util/{filename}'\n    util.write(test_df, util.smart_path(data_path))\n    assert os.path.exists(data_path)\n    data_df = util.read(util.smart_path(data_path))\n    assert isinstance(data_df, dtype)\n\n\n@pytest.mark.parametrize('filename,dtype', [\n    ('test_dict.json', dict),\n    ('test_dict.yml', dict),\n])\ndef test_write_read_as_plain_dict(test_dict, filename, dtype):\n    data_path = f'test/fixture/lib/util/{filename}'\n    util.write(test_dict, util.smart_path(data_path))\n    assert os.path.exists(data_path)\n    data_dict = util.read(util.smart_path(data_path))\n    assert isinstance(data_dict, dtype)\n\n\n@pytest.mark.parametrize('filename,dtype', [\n    ('test_list.json', list),\n])\ndef test_write_read_as_plain_list(test_list, filename, dtype):\n    data_path = f'test/fixture/lib/util/{filename}'\n    util.write(test_list, util.smart_path(data_path))\n    assert os.path.exists(data_path)\n    data_dict = util.read(util.smart_path(data_path))\n    assert isinstance(data_dict, dtype)\n\n\n@pytest.mark.parametrize('filename,dtype', [\n    ('test_str.txt', str),\n])\ndef test_write_read_as_plain_list(test_str, filename, dtype):\n    data_path = f'test/fixture/lib/util/{filename}'\n    util.write(test_str, util.smart_path(data_path))\n    assert os.path.exists(data_path)\n    data_dict = util.read(util.smart_path(data_path))\n    assert isinstance(data_dict, dtype)\n\n\ndef test_read_file_not_found():\n    fake_rel_path = 'test/lib/test_util.py_fake'\n    with pytest.raises(FileNotFoundError) as excinfo:\n        util.read(fake_rel_path)\n\n\ndef test_to_opencv_image():\n    im = np.zeros((80, 100, 3))\n    assert util.to_opencv_image(im).shape == (80, 100, 3)\n\n    im = np.zeros((3, 80, 100))\n    assert util.to_opencv_image(im).shape == (80, 100, 3)\n\n\ndef test_to_pytorch_image():\n    im = np.zeros((80, 100, 3))\n    assert util.to_pytorch_image(im).shape == (3, 80, 100)\n\n    im = np.zeros((3, 80, 100))\n    assert util.to_pytorch_image(im).shape == (3, 80, 100)\n"""
test/spec/test_dist_spec.py,0,"b""from flaky import flaky\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.experiment import analysis\nfrom slm_lab.experiment.control import Trial\nfrom slm_lab.lib import util\nfrom slm_lab.spec import spec_util\nimport os\nimport pydash as ps\nimport pytest\n\n\n# helper method to run all tests in test_spec\ndef run_trial_test_dist(spec_file, spec_name=False):\n    spec = spec_util.get(spec_file, spec_name)\n    spec = spec_util.override_spec(spec, 'test')\n    spec_util.tick(spec, 'trial')\n    spec['meta']['distributed'] = 'synced'\n    spec['meta']['max_session'] = 2\n\n    trial = Trial(spec)\n    # manually run the logic to obtain global nets for testing to ensure global net gets updated\n    global_nets = trial.init_global_nets()\n    # only test first network\n    if ps.is_list(global_nets):  # multiagent only test first\n        net = list(global_nets[0].values())[0]\n    else:\n        net = list(global_nets.values())[0]\n    session_metrics_list = trial.parallelize_sessions(global_nets)\n    trial_metrics = analysis.analyze_trial(spec, session_metrics_list)\n    trial.close()\n    assert isinstance(trial_metrics, dict)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/reinforce.json', 'reinforce_cartpole'),\n])\ndef test_reinforce_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/reinforce.json', 'reinforce_pendulum'),\n])\ndef test_reinforce_cont_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/a3c.json', 'a3c_gae_mlp_shared_cartpole'),\n    ('experimental/a3c.json', 'a3c_gae_mlp_separate_cartpole'),\n    ('experimental/a3c.json', 'a3c_gae_rnn_shared_cartpole'),\n    ('experimental/a3c.json', 'a3c_gae_rnn_separate_cartpole'),\n    # ('experimental/a3c.json', 'a3c_gae_conv_shared_breakout'),\n    # ('experimental/a3c.json', 'a3c_gae_conv_separate_breakout'),\n])\ndef test_a3c_gae_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/a3c.json', 'a3c_gae_mlp_shared_pendulum'),\n    ('experimental/a3c.json', 'a3c_gae_mlp_separate_pendulum'),\n    ('experimental/a3c.json', 'a3c_gae_rnn_shared_pendulum'),\n    ('experimental/a3c.json', 'a3c_gae_rnn_separate_pendulum'),\n])\ndef test_a3c_gae_cont_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/dppo.json', 'dppo_mlp_shared_cartpole'),\n    ('experimental/dppo.json', 'dppo_mlp_separate_cartpole'),\n    ('experimental/dppo.json', 'dppo_rnn_shared_cartpole'),\n    ('experimental/dppo.json', 'dppo_rnn_separate_cartpole'),\n    # ('experimental/dppo.json', 'dppo_conv_shared_breakout'),\n    # ('experimental/dppo.json', 'dppo_conv_separate_breakout'),\n])\ndef test_dppo_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/ppo.json', 'ppo_mlp_shared_pendulum'),\n    ('experimental/ppo.json', 'ppo_mlp_separate_pendulum'),\n    ('experimental/ppo.json', 'ppo_rnn_shared_pendulum'),\n    ('experimental/ppo.json', 'ppo_rnn_separate_pendulum'),\n])\ndef test_ppo_cont_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@flaky\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/ppo_sil.json', 'ppo_sil_mlp_shared_cartpole'),\n    ('experimental/ppo_sil.json', 'ppo_sil_mlp_separate_cartpole'),\n    ('experimental/ppo_sil.json', 'ppo_sil_rnn_shared_cartpole'),\n    ('experimental/ppo_sil.json', 'ppo_sil_rnn_separate_cartpole'),\n])\ndef test_ppo_sil_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@flaky\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/ppo_sil.json', 'ppo_sil_mlp_shared_pendulum'),\n    ('experimental/ppo_sil.json', 'ppo_sil_mlp_separate_pendulum'),\n    ('experimental/ppo_sil.json', 'ppo_sil_rnn_shared_pendulum'),\n    ('experimental/ppo_sil.json', 'ppo_sil_rnn_separate_pendulum'),\n])\ndef test_ppo_sil_cont_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@flaky\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/sil.json', 'sil_mlp_shared_cartpole'),\n    ('experimental/sil.json', 'sil_mlp_separate_cartpole'),\n    ('experimental/sil.json', 'sil_rnn_shared_cartpole'),\n    ('experimental/sil.json', 'sil_rnn_separate_cartpole'),\n    # ('experimental/sil.json', 'sil_conv_shared_breakout'),\n    # ('experimental/sil.json', 'sil_conv_separate_breakout'),\n])\ndef test_sil_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@flaky\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/sil.json', 'sil_mlp_shared_pendulum'),\n    ('experimental/sil.json', 'sil_mlp_separate_pendulum'),\n    ('experimental/sil.json', 'sil_rnn_shared_pendulum'),\n    ('experimental/sil.json', 'sil_rnn_separate_pendulum'),\n])\ndef test_sil_cont_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/sarsa.json', 'sarsa_epsilon_greedy_cartpole'),\n    ('experimental/sarsa.json', 'sarsa_boltzmann_cartpole'),\n])\ndef test_sarsa_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/dqn.json', 'vanilla_dqn_cartpole'),\n    ('experimental/dqn.json', 'dqn_boltzmann_cartpole'),\n    ('experimental/dqn.json', 'dqn_epsilon_greedy_cartpole'),\n    ('experimental/dqn.json', 'drqn_boltzmann_cartpole'),\n    ('experimental/dqn.json', 'drqn_epsilon_greedy_cartpole'),\n    # ('experimental/dqn.json', 'dqn_boltzmann_breakout'),\n    # ('experimental/dqn.json', 'dqn_epsilon_greedy_breakout'),\n    ('experimental/dqn.json', 'dqn_stack_epsilon_greedy_lunar'),\n])\ndef test_dqn_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/ddqn.json', 'ddqn_boltzmann_cartpole'),\n    ('experimental/ddqn.json', 'ddqn_epsilon_greedy_cartpole'),\n    ('experimental/ddqn.json', 'ddrqn_boltzmann_cartpole'),\n    ('experimental/ddqn.json', 'ddrqn_epsilon_greedy_cartpole'),\n    # ('experimental/ddqn.json', 'ddqn_boltzmann_breakout'),\n    # ('experimental/ddqn.json', 'ddqn_epsilon_greedy_breakout'),\n])\ndef test_ddqn_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/dueling_dqn.json', 'dueling_dqn_boltzmann_cartpole'),\n    ('experimental/dueling_dqn.json', 'dueling_dqn_epsilon_greedy_cartpole'),\n    # ('experimental/dueling_dqn.json', 'dueling_dqn_boltzmann_breakout'),\n    # ('experimental/dueling_dqn.json', 'dueling_dqn_epsilon_greedy_breakout'),\n])\ndef test_dueling_dqn_dist(spec_file, spec_name):\n    run_trial_test_dist(spec_file, spec_name)\n"""
test/spec/test_spec.py,0,"b""from flaky import flaky\nfrom slm_lab.experiment.control import Trial\nfrom slm_lab.spec import spec_util\nimport pytest\n\n\n# helper method to run all tests in test_spec\ndef run_trial_test(spec_file, spec_name=False):\n    spec = spec_util.get(spec_file, spec_name)\n    spec = spec_util.override_spec(spec, 'test')\n    spec_util.tick(spec, 'trial')\n    trial = Trial(spec)\n    trial_metrics = trial.run()\n    assert isinstance(trial_metrics, dict)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/reinforce/reinforce_cartpole.json', 'reinforce_cartpole'),\n])\ndef test_reinforce(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/reinforce/reinforce_pendulum.json', 'reinforce_pendulum'),\n])\ndef test_reinforce_cont(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/sarsa/sarsa_cartpole.json', 'sarsa_epsilon_greedy_cartpole'),\n    ('experimental/sarsa/sarsa_cartpole.json', 'sarsa_boltzmann_cartpole'),\n])\ndef test_sarsa(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/a2c/a2c_cartpole.json', 'a2c_shared_cartpole'),\n    ('experimental/a2c/a2c_cartpole.json', 'a2c_separate_cartpole'),\n    ('experimental/a2c/a2c_cartpole.json', 'a2c_concat_cartpole'),\n    ('experimental/a2c/a2c_cartpole.json', 'a2c_rnn_shared_cartpole'),\n    ('experimental/a2c/a2c_cartpole.json', 'a2c_rnn_separate_cartpole'),\n])\ndef test_a2c(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/a2c/a2c_pendulum.json', 'a2c_shared_pendulum'),\n    ('experimental/a2c/a2c_pendulum.json', 'a2c_separate_pendulum'),\n    ('experimental/a2c/a2c_pendulum.json', 'a2c_concat_pendulum'),\n    ('experimental/a2c/a2c_pendulum.json', 'a2c_rnn_shared_pendulum'),\n    ('experimental/a2c/a2c_pendulum.json', 'a2c_rnn_separate_pendulum'),\n])\ndef test_a2c_cont(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/ppo/ppo_cartpole.json', 'ppo_shared_cartpole'),\n    ('experimental/ppo/ppo_cartpole.json', 'ppo_separate_cartpole'),\n    ('experimental/ppo/ppo_cartpole.json', 'ppo_rnn_shared_cartpole'),\n    ('experimental/ppo/ppo_cartpole.json', 'ppo_rnn_separate_cartpole'),\n])\ndef test_ppo(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/ppo/ppo_pendulum.json', 'ppo_shared_pendulum'),\n    ('experimental/ppo/ppo_pendulum.json', 'ppo_separate_pendulum'),\n    ('experimental/ppo/ppo_pendulum.json', 'ppo_rnn_shared_pendulum'),\n    ('experimental/ppo/ppo_pendulum.json', 'ppo_rnn_separate_pendulum'),\n])\ndef test_ppo_cont(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@flaky\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/sil/sil_cartpole.json', 'sil_shared_cartpole'),\n    ('experimental/sil/sil_cartpole.json', 'sil_separate_cartpole'),\n    ('experimental/sil/sil_cartpole.json', 'sil_rnn_shared_cartpole'),\n    ('experimental/sil/sil_cartpole.json', 'sil_rnn_separate_cartpole'),\n])\ndef test_sil(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@flaky\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/sil/ppo_sil_cartpole.json', 'ppo_sil_shared_cartpole'),\n    ('experimental/sil/ppo_sil_cartpole.json', 'ppo_sil_separate_cartpole'),\n    ('experimental/sil/ppo_sil_cartpole.json', 'ppo_sil_rnn_shared_cartpole'),\n    ('experimental/sil/ppo_sil_cartpole.json', 'ppo_sil_rnn_separate_cartpole'),\n])\ndef test_ppo_sil(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/sac/sac_cartpole.json', 'sac_cartpole'),\n    ('benchmark/sac/sac_halfcheetah.json', 'sac_halfcheetah'),\n])\ndef test_sac_cont(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/dqn/dqn_cartpole.json', 'vanilla_dqn_cartpole'),\n    ('experimental/dqn/dqn_cartpole.json', 'dqn_boltzmann_cartpole'),\n    ('experimental/dqn/dqn_cartpole.json', 'dqn_epsilon_greedy_cartpole'),\n    ('experimental/dqn/dqn_cartpole.json', 'drqn_boltzmann_cartpole'),\n    ('experimental/dqn/dqn_cartpole.json', 'drqn_epsilon_greedy_cartpole'),\n    ('benchmark/dqn/dqn_lunar.json', 'dqn_concat_lunar'),\n])\ndef test_dqn(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/dqn/ddqn_cartpole.json', 'ddqn_boltzmann_cartpole'),\n    ('experimental/dqn/ddqn_cartpole.json', 'ddqn_epsilon_greedy_cartpole'),\n    ('experimental/dqn/ddqn_cartpole.json', 'ddrqn_boltzmann_cartpole'),\n    ('experimental/dqn/ddqn_cartpole.json', 'ddrqn_epsilon_greedy_cartpole'),\n    ('experimental/dqn/ddqn_lunar.json', 'ddqn_concat_lunar'),\n])\ndef test_ddqn(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('experimental/dqn/dueling_dqn_cartpole.json', 'dueling_dqn_boltzmann_cartpole'),\n    ('experimental/dqn/dueling_dqn_cartpole.json', 'dueling_dqn_epsilon_greedy_cartpole'),\n])\ndef test_dueling_dqn(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@flaky\n@pytest.mark.parametrize('spec_file,spec_name', [\n    ('benchmark/dqn/dqn_pong.json', 'dqn_pong'),\n    ('benchmark/a2c/a2c_gae_pong.json', 'a2c_gae_pong'),\n])\ndef test_atari(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n\n\n@pytest.mark.parametrize('spec_file,spec_name', [\n    # ('experimental/misc/base.json', 'base_case_unity'),\n    ('experimental/misc/base.json', 'base_case_openai'),\n    ('experimental/misc/random.json', 'random_cartpole'),\n    # ('experimental/misc/random.json', 'random_pendulum'),  # mp EOF error\n])\ndef test_base(spec_file, spec_name):\n    run_trial_test(spec_file, spec_name)\n"""
test/spec/test_spec_util.py,0,"b""from slm_lab.spec import spec_util\nimport numpy as np\nimport pytest\n\n\ndef test_check():\n    spec = spec_util.get('experimental/misc/base.json', 'base_case_openai')\n    assert spec_util.check(spec)\n\n\ndef test_check_all():\n    assert spec_util.check_all()\n\n\ndef test_get():\n    spec = spec_util.get('experimental/misc/base.json', 'base_case_openai')\n    assert spec is not None\n"""
slm_lab/agent/algorithm/__init__.py,0,b'# The algorithm module\n# Contains implementations of reinforcement learning algorithms.\n# Uses the nets module to build neural networks as the relevant function approximators\nfrom .actor_critic import *\nfrom .dqn import *\nfrom .ppo import *\nfrom .random import *\nfrom .reinforce import *\nfrom .sac import *\nfrom .sarsa import *\nfrom .sil import *\n'
slm_lab/agent/algorithm/actor_critic.py,3,"b'from slm_lab.agent import net\nfrom slm_lab.agent.algorithm import policy_util\nfrom slm_lab.agent.algorithm.reinforce import Reinforce\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.lib import logger, math_util, util\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\nimport pydash as ps\nimport torch\n\nlogger = logger.get_logger(__name__)\n\n\nclass ActorCritic(Reinforce):\n    \'\'\'\n    Implementation of single threaded Advantage Actor Critic\n    Original paper: ""Asynchronous Methods for Deep Reinforcement Learning""\n    https://arxiv.org/abs/1602.01783\n    Algorithm specific spec param:\n    memory.name: batch (through OnPolicyBatchReplay memory class) or episodic through (OnPolicyReplay memory class)\n    lam: if not null, used as the lambda value of generalized advantage estimation (GAE) introduced in ""High-Dimensional Continuous Control Using Generalized Advantage Estimation https://arxiv.org/abs/1506.02438. This lambda controls the bias variance tradeoff for GAE. Floating point value between 0 and 1. Lower values correspond to more bias, less variance. Higher values to more variance, less bias. Algorithm becomes A2C(GAE).\n    num_step_returns: if lam is null and this is not null, specifies the number of steps for N-step returns from ""Asynchronous Methods for Deep Reinforcement Learning"". The algorithm becomes A2C(Nstep).\n    If both lam and num_step_returns are null, use the default TD error. Then the algorithm stays as AC.\n    net.type: whether the actor and critic should share params (e.g. through \'MLPNetShared\') or have separate params (e.g. through \'MLPNetSeparate\'). If param sharing is used then there is also the option to control the weight given to the policy and value components of the loss function through \'policy_loss_coef\' and \'val_loss_coef\'\n    Algorithm - separate actor and critic:\n        Repeat:\n            1. Collect k examples\n            2. Train the critic network using these examples\n            3. Calculate the advantage of each example using the critic\n            4. Multiply the advantage by the negative of log probability of the action taken, and sum all the values. This is the policy loss.\n            5. Calculate the gradient the parameters of the actor network with respect to the policy loss\n            6. Update the actor network parameters using the gradient\n    Algorithm - shared parameters:\n        Repeat:\n            1. Collect k examples\n            2. Calculate the target for each example for the critic\n            3. Compute current estimate of state-value for each example using the critic\n            4. Calculate the critic loss using a regression loss (e.g. square loss) between the target and estimate of the state-value for each example\n            5. Calculate the advantage of each example using the rewards and critic\n            6. Multiply the advantage by the negative of log probability of the action taken, and sum all the values. This is the policy loss.\n            7. Compute the total loss by summing the value and policy lossses\n            8. Calculate the gradient of the parameters of shared network with respect to the total loss\n            9. Update the shared network parameters using the gradient\n\n    e.g. algorithm_spec\n    ""algorithm"": {\n        ""name"": ""ActorCritic"",\n        ""action_pdtype"": ""default"",\n        ""action_policy"": ""default"",\n        ""explore_var_spec"": null,\n        ""gamma"": 0.99,\n        ""lam"": 0.95,\n        ""num_step_returns"": 100,\n        ""entropy_coef_spec"": {\n          ""name"": ""linear_decay"",\n          ""start_val"": 0.01,\n          ""end_val"": 0.001,\n          ""start_step"": 100,\n          ""end_step"": 5000,\n        },\n        ""policy_loss_coef"": 1.0,\n        ""val_loss_coef"": 0.01,\n        ""training_frequency"": 1,\n    }\n\n    e.g. special net_spec param ""shared"" to share/separate Actor/Critic\n    ""net"": {\n        ""type"": ""MLPNet"",\n        ""shared"": true,\n        ...\n    \'\'\'\n\n    @lab_api\n    def init_algorithm_params(self):\n        \'\'\'Initialize other algorithm parameters\'\'\'\n        # set default\n        util.set_attr(self, dict(\n            action_pdtype=\'default\',\n            action_policy=\'default\',\n            explore_var_spec=None,\n            entropy_coef_spec=None,\n            policy_loss_coef=1.0,\n            val_loss_coef=1.0,\n        ))\n        util.set_attr(self, self.algorithm_spec, [\n            \'action_pdtype\',\n            \'action_policy\',\n            # theoretically, AC does not have policy update; but in this implementation we have such option\n            \'explore_var_spec\',\n            \'gamma\',  # the discount factor\n            \'lam\',\n            \'num_step_returns\',\n            \'entropy_coef_spec\',\n            \'policy_loss_coef\',\n            \'val_loss_coef\',\n            \'training_frequency\',\n        ])\n        self.to_train = 0\n        self.action_policy = getattr(policy_util, self.action_policy)\n        self.explore_var_scheduler = policy_util.VarScheduler(self.explore_var_spec)\n        self.body.explore_var = self.explore_var_scheduler.start_val\n        if self.entropy_coef_spec is not None:\n            self.entropy_coef_scheduler = policy_util.VarScheduler(self.entropy_coef_spec)\n            self.body.entropy_coef = self.entropy_coef_scheduler.start_val\n        # Select appropriate methods to calculate advs and v_targets for training\n        if self.lam is not None:\n            self.calc_advs_v_targets = self.calc_gae_advs_v_targets\n        elif self.num_step_returns is not None:\n            # need to override training_frequency for nstep to be the same\n            self.training_frequency = self.num_step_returns\n            self.calc_advs_v_targets = self.calc_nstep_advs_v_targets\n        else:\n            self.calc_advs_v_targets = self.calc_ret_advs_v_targets\n\n    @lab_api\n    def init_nets(self, global_nets=None):\n        \'\'\'\n        Initialize the neural networks used to learn the actor and critic from the spec\n        Below we automatically select an appropriate net based on two different conditions\n        1. If the action space is discrete or continuous action\n            - Networks for continuous action spaces have two heads and return two values, the first is a tensor containing the mean of the action policy, the second is a tensor containing the std deviation of the action policy. The distribution is assumed to be a Gaussian (Normal) distribution.\n            - Networks for discrete action spaces have a single head and return the logits for a categorical probability distribution over the discrete actions\n        2. If the actor and critic are separate or share weights\n            - If the networks share weights then the single network returns a list.\n            - Continuous action spaces: The return list contains 3 elements: The first element contains the mean output for the actor (policy), the second element the std dev of the policy, and the third element is the state-value estimated by the network.\n            - Discrete action spaces: The return list contains 2 element. The first element is a tensor containing the logits for a categorical probability distribution over the actions. The second element contains the state-value estimated by the network.\n        3. If the network type is feedforward, convolutional, or recurrent\n            - Feedforward and convolutional networks take a single state as input and require an OnPolicyReplay or OnPolicyBatchReplay memory\n            - Recurrent networks take n states as input and require env spec ""frame_op"": ""concat"", ""frame_op_len"": seq_len\n        \'\'\'\n        assert \'shared\' in self.net_spec, \'Specify ""shared"" for ActorCritic network in net_spec\'\n        self.shared = self.net_spec[\'shared\']\n\n        # create actor/critic specific specs\n        actor_net_spec = self.net_spec.copy()\n        critic_net_spec = self.net_spec.copy()\n        for k in self.net_spec:\n            if \'actor_\' in k:\n                actor_net_spec[k.replace(\'actor_\', \'\')] = actor_net_spec.pop(k)\n                critic_net_spec.pop(k)\n            if \'critic_\' in k:\n                critic_net_spec[k.replace(\'critic_\', \'\')] = critic_net_spec.pop(k)\n                actor_net_spec.pop(k)\n        if critic_net_spec[\'use_same_optim\']:\n            critic_net_spec = actor_net_spec\n\n        in_dim = self.body.state_dim\n        out_dim = net_util.get_out_dim(self.body, add_critic=self.shared)\n        # main actor network, may contain out_dim self.shared == True\n        NetClass = getattr(net, actor_net_spec[\'type\'])\n        self.net = NetClass(actor_net_spec, in_dim, out_dim)\n        self.net_names = [\'net\']\n        if not self.shared:  # add separate network for critic\n            critic_out_dim = 1\n            CriticNetClass = getattr(net, critic_net_spec[\'type\'])\n            self.critic_net = CriticNetClass(critic_net_spec, in_dim, critic_out_dim)\n            self.net_names.append(\'critic_net\')\n        # init net optimizer and its lr scheduler\n        self.optim = net_util.get_optim(self.net, self.net.optim_spec)\n        self.lr_scheduler = net_util.get_lr_scheduler(self.optim, self.net.lr_scheduler_spec)\n        if not self.shared:\n            self.critic_optim = net_util.get_optim(self.critic_net, self.critic_net.optim_spec)\n            self.critic_lr_scheduler = net_util.get_lr_scheduler(self.critic_optim, self.critic_net.lr_scheduler_spec)\n        net_util.set_global_nets(self, global_nets)\n        self.end_init_nets()\n\n    @lab_api\n    def calc_pdparam(self, x, net=None):\n        \'\'\'\n        The pdparam will be the logits for discrete prob. dist., or the mean and std for continuous prob. dist.\n        \'\'\'\n        out = super().calc_pdparam(x, net=net)\n        if self.shared:\n            assert ps.is_list(out), f\'Shared output should be a list [pdparam, v]\'\n            if len(out) == 2:  # single policy\n                pdparam = out[0]\n            else:  # multiple-task policies, still assumes 1 value\n                pdparam = out[:-1]\n            self.v_pred = out[-1].view(-1)  # cache for loss calc to prevent double-pass\n        else:  # out is pdparam\n            pdparam = out\n        return pdparam\n\n    def calc_v(self, x, net=None, use_cache=True):\n        \'\'\'\n        Forward-pass to calculate the predicted state-value from critic_net.\n        \'\'\'\n        if self.shared:  # output: policy, value\n            if use_cache:  # uses cache from calc_pdparam to prevent double-pass\n                v_pred = self.v_pred\n            else:\n                net = self.net if net is None else net\n                v_pred = net(x)[-1].view(-1)\n        else:\n            net = self.critic_net if net is None else net\n            v_pred = net(x).view(-1)\n        return v_pred\n\n    def calc_pdparam_v(self, batch):\n        \'\'\'Efficiently forward to get pdparam and v by batch for loss computation\'\'\'\n        states = batch[\'states\']\n        if self.body.env.is_venv:\n            states = math_util.venv_unpack(states)\n        pdparam = self.calc_pdparam(states)\n        v_pred = self.calc_v(states)  # uses self.v_pred from calc_pdparam if self.shared\n        return pdparam, v_pred\n\n    def calc_ret_advs_v_targets(self, batch, v_preds):\n        \'\'\'Calculate plain returns, and advs = rets - v_preds, v_targets = rets\'\'\'\n        v_preds = v_preds.detach()  # adv does not accumulate grad\n        if self.body.env.is_venv:\n            v_preds = math_util.venv_pack(v_preds, self.body.env.num_envs)\n        rets = math_util.calc_returns(batch[\'rewards\'], batch[\'dones\'], self.gamma)\n        advs = rets - v_preds\n        v_targets = rets\n        if self.body.env.is_venv:\n            advs = math_util.venv_unpack(advs)\n            v_targets = math_util.venv_unpack(v_targets)\n        logger.debug(f\'advs: {advs}\\nv_targets: {v_targets}\')\n        return advs, v_targets\n\n    def calc_nstep_advs_v_targets(self, batch, v_preds):\n        \'\'\'\n        Calculate N-step returns, and advs = nstep_rets - v_preds, v_targets = nstep_rets\n        See n-step advantage under http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf.pdf\n        \'\'\'\n        next_states = batch[\'next_states\'][-1]\n        if not self.body.env.is_venv:\n            next_states = next_states.unsqueeze(dim=0)\n        with torch.no_grad():\n            next_v_pred = self.calc_v(next_states, use_cache=False)\n        v_preds = v_preds.detach()  # adv does not accumulate grad\n        if self.body.env.is_venv:\n            v_preds = math_util.venv_pack(v_preds, self.body.env.num_envs)\n        nstep_rets = math_util.calc_nstep_returns(batch[\'rewards\'], batch[\'dones\'], next_v_pred, self.gamma, self.num_step_returns)\n        advs = nstep_rets - v_preds\n        v_targets = nstep_rets\n        if self.body.env.is_venv:\n            advs = math_util.venv_unpack(advs)\n            v_targets = math_util.venv_unpack(v_targets)\n        logger.debug(f\'advs: {advs}\\nv_targets: {v_targets}\')\n        return advs, v_targets\n\n    def calc_gae_advs_v_targets(self, batch, v_preds):\n        \'\'\'\n        Calculate GAE, and advs = GAE, v_targets = advs + v_preds\n        See GAE from Schulman et al. https://arxiv.org/pdf/1506.02438.pdf\n        \'\'\'\n        next_states = batch[\'next_states\'][-1]\n        if not self.body.env.is_venv:\n            next_states = next_states.unsqueeze(dim=0)\n        with torch.no_grad():\n            next_v_pred = self.calc_v(next_states, use_cache=False)\n        v_preds = v_preds.detach()  # adv does not accumulate grad\n        if self.body.env.is_venv:\n            v_preds = math_util.venv_pack(v_preds, self.body.env.num_envs)\n            next_v_pred = next_v_pred.unsqueeze(dim=0)\n        v_preds_all = torch.cat((v_preds, next_v_pred), dim=0)\n        advs = math_util.calc_gaes(batch[\'rewards\'], batch[\'dones\'], v_preds_all, self.gamma, self.lam)\n        v_targets = advs + v_preds\n        advs = math_util.standardize(advs)  # standardize only for advs, not v_targets\n        if self.body.env.is_venv:\n            advs = math_util.venv_unpack(advs)\n            v_targets = math_util.venv_unpack(v_targets)\n        logger.debug(f\'advs: {advs}\\nv_targets: {v_targets}\')\n        return advs, v_targets\n\n    def calc_policy_loss(self, batch, pdparams, advs):\n        \'\'\'Calculate the actor\'s policy loss\'\'\'\n        return super().calc_policy_loss(batch, pdparams, advs)\n\n    def calc_val_loss(self, v_preds, v_targets):\n        \'\'\'Calculate the critic\'s value loss\'\'\'\n        assert v_preds.shape == v_targets.shape, f\'{v_preds.shape} != {v_targets.shape}\'\n        val_loss = self.val_loss_coef * self.net.loss_fn(v_preds, v_targets)\n        logger.debug(f\'Critic value loss: {val_loss:g}\')\n        return val_loss\n\n    def train(self):\n        \'\'\'Train actor critic by computing the loss in batch efficiently\'\'\'\n        clock = self.body.env.clock\n        if self.to_train == 1:\n            batch = self.sample()\n            clock.set_batch_size(len(batch))\n            pdparams, v_preds = self.calc_pdparam_v(batch)\n            advs, v_targets = self.calc_advs_v_targets(batch, v_preds)\n            policy_loss = self.calc_policy_loss(batch, pdparams, advs)  # from actor\n            val_loss = self.calc_val_loss(v_preds, v_targets)  # from critic\n            if self.shared:  # shared network\n                loss = policy_loss + val_loss\n                self.net.train_step(loss, self.optim, self.lr_scheduler, clock=clock, global_net=self.global_net)\n            else:\n                self.net.train_step(policy_loss, self.optim, self.lr_scheduler, clock=clock, global_net=self.global_net)\n                self.critic_net.train_step(val_loss, self.critic_optim, self.critic_lr_scheduler, clock=clock, global_net=self.global_critic_net)\n                loss = policy_loss + val_loss\n            # reset\n            self.to_train = 0\n            logger.debug(f\'Trained {self.name} at epi: {clock.epi}, frame: {clock.frame}, t: {clock.t}, total_reward so far: {self.body.env.total_reward}, loss: {loss:g}\')\n            return loss.item()\n        else:\n            return np.nan\n\n    @lab_api\n    def update(self):\n        self.body.explore_var = self.explore_var_scheduler.update(self, self.body.env.clock)\n        if self.entropy_coef_spec is not None:\n            self.body.entropy_coef = self.entropy_coef_scheduler.update(self, self.body.env.clock)\n        return self.body.explore_var\n'"
slm_lab/agent/algorithm/base.py,0,"b'from abc import ABC, abstractmethod\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.lib import logger, util\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\n\nlogger = logger.get_logger(__name__)\n\n\nclass Algorithm(ABC):\n    \'\'\'Abstract Algorithm class to define the API methods\'\'\'\n\n    def __init__(self, agent, global_nets=None):\n        \'\'\'\n        @param {*} agent is the container for algorithm and related components, and interfaces with env.\n        \'\'\'\n        self.agent = agent\n        self.algorithm_spec = agent.agent_spec[\'algorithm\']\n        self.name = self.algorithm_spec[\'name\']\n        self.memory_spec = agent.agent_spec[\'memory\']\n        self.net_spec = agent.agent_spec[\'net\']\n        self.body = self.agent.body\n        self.init_algorithm_params()\n        self.init_nets(global_nets)\n        logger.info(util.self_desc(self, omit=[\'algorithm_spec\', \'name\', \'memory_spec\', \'net_spec\', \'body\']))\n\n    @abstractmethod\n    @lab_api\n    def init_algorithm_params(self):\n        \'\'\'Initialize other algorithm parameters\'\'\'\n        raise NotImplementedError\n\n    @abstractmethod\n    @lab_api\n    def init_nets(self, global_nets=None):\n        \'\'\'Initialize the neural network from the spec\'\'\'\n        raise NotImplementedError\n\n    @lab_api\n    def end_init_nets(self):\n        \'\'\'Checkers and conditional loaders called at the end of init_nets()\'\'\'\n        # check all nets naming\n        assert hasattr(self, \'net_names\')\n        for net_name in self.net_names:\n            assert net_name.endswith(\'net\'), f\'Naming convention: net_name must end with ""net""; got {net_name}\'\n\n        # load algorithm if is in train@ resume or enjoy mode\n        lab_mode = util.get_lab_mode()\n        if self.agent.spec[\'meta\'][\'resume\'] or lab_mode == \'enjoy\':\n            self.load()\n            logger.info(f\'Loaded algorithm models for lab_mode: {lab_mode}\')\n        else:\n            logger.info(f\'Initialized algorithm models for lab_mode: {lab_mode}\')\n\n    @lab_api\n    def calc_pdparam(self, x, net=None):\n        \'\'\'\n        To get the pdparam for action policy sampling, do a forward pass of the appropriate net, and pick the correct outputs.\n        The pdparam will be the logits for discrete prob. dist., or the mean and std for continuous prob. dist.\n        \'\'\'\n        raise NotImplementedError\n\n    @lab_api\n    def act(self, state):\n        \'\'\'Standard act method.\'\'\'\n        raise NotImplementedError\n        return action\n\n    @abstractmethod\n    @lab_api\n    def sample(self):\n        \'\'\'Samples a batch from memory\'\'\'\n        raise NotImplementedError\n        return batch\n\n    @abstractmethod\n    @lab_api\n    def train(self):\n        \'\'\'Implement algorithm train, or throw NotImplementedError\'\'\'\n        raise NotImplementedError\n\n    @abstractmethod\n    @lab_api\n    def update(self):\n        \'\'\'Implement algorithm update, or throw NotImplementedError\'\'\'\n        raise NotImplementedError\n\n    @lab_api\n    def save(self, ckpt=None):\n        \'\'\'Save net models for algorithm given the required property self.net_names\'\'\'\n        if not hasattr(self, \'net_names\'):\n            logger.info(\'No net declared in self.net_names in init_nets(); no models to save.\')\n        else:\n            net_util.save_algorithm(self, ckpt=ckpt)\n\n    @lab_api\n    def load(self):\n        \'\'\'Load net models for algorithm given the required property self.net_names\'\'\'\n        if not hasattr(self, \'net_names\'):\n            logger.info(\'No net declared in self.net_names in init_nets(); no models to load.\')\n        else:\n            net_util.load_algorithm(self)\n        # set decayable variables to final values\n        for k, v in vars(self).items():\n            if k.endswith(\'_scheduler\') and hasattr(v, \'end_val\'):\n                var_name = k.replace(\'_scheduler\', \'\')\n                setattr(self.body, var_name, v.end_val)\n'"
slm_lab/agent/algorithm/dqn.py,3,"b'from slm_lab.agent import net\nfrom slm_lab.agent.algorithm.sarsa import SARSA\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.lib import logger, util\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\nimport torch\n\nlogger = logger.get_logger(__name__)\n\n\nclass VanillaDQN(SARSA):\n    \'\'\'\n    Implementation of a simple DQN algorithm.\n    Algorithm:\n        1. Collect some examples by acting in the environment and store them in a replay memory\n        2. Every K steps sample N examples from replay memory\n        3. For each example calculate the target (bootstrapped estimate of the discounted value of the state and action taken), y, using a neural network to approximate the Q function. s\' is the next state following the action actually taken.\n                y_t = r_t + gamma * argmax_a Q(s_t\', a)\n        4. For each example calculate the current estimate of the discounted value of the state and action taken\n                x_t = Q(s_t, a_t)\n        5. Calculate L(x, y) where L is a regression loss (eg. mse)\n        6. Calculate the gradient of L with respect to all the parameters in the network and update the network parameters using the gradient\n        7. Repeat steps 3 - 6 M times\n        8. Repeat steps 2 - 7 Z times\n        9. Repeat steps 1 - 8\n\n    For more information on Q-Learning see Sergey Levine\'s lectures 6 and 7 from CS294-112 Fall 2017\n    https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3\n\n    e.g. algorithm_spec\n    ""algorithm"": {\n        ""name"": ""VanillaDQN"",\n        ""action_pdtype"": ""Argmax"",\n        ""action_policy"": ""epsilon_greedy"",\n        ""explore_var_spec"": {\n            ""name"": ""linear_decay"",\n            ""start_val"": 1.0,\n            ""end_val"": 0.1,\n            ""start_step"": 10,\n            ""end_step"": 1000,\n        },\n        ""gamma"": 0.99,\n        ""training_batch_iter"": 8,\n        ""training_iter"": 4,\n        ""training_frequency"": 10,\n        ""training_start_step"": 10,\n    }\n    \'\'\'\n\n    @lab_api\n    def init_algorithm_params(self):\n        # set default\n        util.set_attr(self, dict(\n            action_pdtype=\'Argmax\',\n            action_policy=\'epsilon_greedy\',\n            explore_var_spec=None,\n            training_start_step=self.body.memory.batch_size,\n        ))\n        util.set_attr(self, self.algorithm_spec, [\n            \'action_pdtype\',\n            \'action_policy\',\n            # explore_var is epsilon, tau or etc. depending on the action policy\n            # these control the trade off between exploration and exploitaton\n            \'explore_var_spec\',\n            \'gamma\',  # the discount factor\n            \'training_batch_iter\',  # how many gradient updates per batch\n            \'training_iter\',  # how many batches to train each time\n            \'training_frequency\',  # how often to train (once a few timesteps)\n            \'training_start_step\',  # how long before starting training\n        ])\n        super().init_algorithm_params()\n\n    @lab_api\n    def init_nets(self, global_nets=None):\n        \'\'\'Initialize the neural network used to learn the Q function from the spec\'\'\'\n        if self.algorithm_spec[\'name\'] == \'VanillaDQN\':\n            assert all(k not in self.net_spec for k in [\'update_type\', \'update_frequency\', \'polyak_coef\']), \'Network update not available for VanillaDQN; use DQN.\'\n        in_dim = self.body.state_dim\n        out_dim = net_util.get_out_dim(self.body)\n        NetClass = getattr(net, self.net_spec[\'type\'])\n        self.net = NetClass(self.net_spec, in_dim, out_dim)\n        self.net_names = [\'net\']\n        # init net optimizer and its lr scheduler\n        self.optim = net_util.get_optim(self.net, self.net.optim_spec)\n        self.lr_scheduler = net_util.get_lr_scheduler(self.optim, self.net.lr_scheduler_spec)\n        net_util.set_global_nets(self, global_nets)\n        self.end_init_nets()\n\n    def calc_q_loss(self, batch):\n        \'\'\'Compute the Q value loss using predicted and target Q values from the appropriate networks\'\'\'\n        states = batch[\'states\']\n        next_states = batch[\'next_states\']\n        q_preds = self.net(states)\n        with torch.no_grad():\n            next_q_preds = self.net(next_states)\n        act_q_preds = q_preds.gather(-1, batch[\'actions\'].long().unsqueeze(-1)).squeeze(-1)\n        # Bellman equation: compute max_q_targets using reward and max estimated Q values (0 if no next_state)\n        max_next_q_preds, _ = next_q_preds.max(dim=-1, keepdim=False)\n        max_q_targets = batch[\'rewards\'] + self.gamma * (1 - batch[\'dones\']) * max_next_q_preds\n        logger.debug(f\'act_q_preds: {act_q_preds}\\nmax_q_targets: {max_q_targets}\')\n        q_loss = self.net.loss_fn(act_q_preds, max_q_targets)\n\n        # TODO use the same loss_fn but do not reduce yet\n        if \'Prioritized\' in util.get_class_name(self.body.memory):  # PER\n            errors = (max_q_targets - act_q_preds.detach()).abs().cpu().numpy()\n            self.body.memory.update_priorities(errors)\n        return q_loss\n\n    @lab_api\n    def act(self, state):\n        \'\'\'Selects and returns a discrete action for body using the action policy\'\'\'\n        return super().act(state)\n\n    @lab_api\n    def sample(self):\n        \'\'\'Samples a batch from memory of size self.memory_spec[\'batch_size\']\'\'\'\n        batch = self.body.memory.sample()\n        batch = util.to_torch_batch(batch, self.net.device, self.body.memory.is_episodic)\n        return batch\n\n    @lab_api\n    def train(self):\n        \'\'\'\n        Completes one training step for the agent if it is time to train.\n        i.e. the environment timestep is greater than the minimum training timestep and a multiple of the training_frequency.\n        Each training step consists of sampling n batches from the agent\'s memory.\n        For each of the batches, the target Q values (q_targets) are computed and a single training step is taken k times\n        Otherwise this function does nothing.\n        \'\'\'\n        clock = self.body.env.clock\n        if self.to_train == 1:\n            total_loss = torch.tensor(0.0)\n            for _ in range(self.training_iter):\n                batch = self.sample()\n                clock.set_batch_size(len(batch))\n                for _ in range(self.training_batch_iter):\n                    loss = self.calc_q_loss(batch)\n                    self.net.train_step(loss, self.optim, self.lr_scheduler, clock=clock, global_net=self.global_net)\n                    total_loss += loss\n            loss = total_loss / (self.training_iter * self.training_batch_iter)\n            # reset\n            self.to_train = 0\n            logger.debug(f\'Trained {self.name} at epi: {clock.epi}, frame: {clock.frame}, t: {clock.t}, total_reward so far: {self.body.env.total_reward}, loss: {loss:g}\')\n            return loss.item()\n        else:\n            return np.nan\n\n    @lab_api\n    def update(self):\n        \'\'\'Update the agent after training\'\'\'\n        return super().update()\n\n\nclass DQNBase(VanillaDQN):\n    \'\'\'\n    Implementation of the base DQN algorithm.\n    The algorithm follows the same general approach as VanillaDQN but is more general since it allows\n    for two different networks (through self.net and self.target_net).\n\n    self.net is used to act, and is the network trained.\n    self.target_net is used to estimate the maximum value of the Q-function in the next state when calculating the target (see VanillaDQN comments).\n    self.target_net is updated periodically to either match self.net (self.net.update_type = ""replace"") or to be a weighted average of self.net and the previous self.target_net (self.net.update_type = ""polyak"")\n    If desired, self.target_net can be updated slowly, and this can help to stabilize learning.\n\n    It also allows for different nets to be used to select the action in the next state and to evaluate the value of that action through self.online_net and self.eval_net. This can help reduce the tendency of DQN\'s to overestimate the value of the Q-function. Following this approach leads to the DoubleDQN algorithm.\n\n    Setting all nets to self.net reduces to the VanillaDQN case.\n    \'\'\'\n\n    @lab_api\n    def init_nets(self, global_nets=None):\n        \'\'\'Initialize networks\'\'\'\n        if self.algorithm_spec[\'name\'] == \'DQNBase\':\n            assert all(k not in self.net_spec for k in [\'update_type\', \'update_frequency\', \'polyak_coef\']), \'Network update not available for DQNBase; use DQN.\'\n        in_dim = self.body.state_dim\n        out_dim = net_util.get_out_dim(self.body)\n        NetClass = getattr(net, self.net_spec[\'type\'])\n        self.net = NetClass(self.net_spec, in_dim, out_dim)\n        self.target_net = NetClass(self.net_spec, in_dim, out_dim)\n        self.net_names = [\'net\', \'target_net\']\n        # init net optimizer and its lr scheduler\n        self.optim = net_util.get_optim(self.net, self.net.optim_spec)\n        self.lr_scheduler = net_util.get_lr_scheduler(self.optim, self.net.lr_scheduler_spec)\n        net_util.set_global_nets(self, global_nets)\n        self.end_init_nets()\n        self.online_net = self.target_net\n        self.eval_net = self.target_net\n\n    def calc_q_loss(self, batch):\n        \'\'\'Compute the Q value loss using predicted and target Q values from the appropriate networks\'\'\'\n        states = batch[\'states\']\n        next_states = batch[\'next_states\']\n        q_preds = self.net(states)\n        with torch.no_grad():\n            # Use online_net to select actions in next state\n            online_next_q_preds = self.online_net(next_states)\n            # Use eval_net to calculate next_q_preds for actions chosen by online_net\n            next_q_preds = self.eval_net(next_states)\n        act_q_preds = q_preds.gather(-1, batch[\'actions\'].long().unsqueeze(-1)).squeeze(-1)\n        online_actions = online_next_q_preds.argmax(dim=-1, keepdim=True)\n        max_next_q_preds = next_q_preds.gather(-1, online_actions).squeeze(-1)\n        max_q_targets = batch[\'rewards\'] + self.gamma * (1 - batch[\'dones\']) * max_next_q_preds\n        logger.debug(f\'act_q_preds: {act_q_preds}\\nmax_q_targets: {max_q_targets}\')\n        q_loss = self.net.loss_fn(act_q_preds, max_q_targets)\n\n        # TODO use the same loss_fn but do not reduce yet\n        if \'Prioritized\' in util.get_class_name(self.body.memory):  # PER\n            errors = (max_q_targets - act_q_preds.detach()).abs().cpu().numpy()\n            self.body.memory.update_priorities(errors)\n        return q_loss\n\n    def update_nets(self):\n        if util.frame_mod(self.body.env.clock.frame, self.net.update_frequency, self.body.env.num_envs):\n            if self.net.update_type == \'replace\':\n                net_util.copy(self.net, self.target_net)\n            elif self.net.update_type == \'polyak\':\n                net_util.polyak_update(self.net, self.target_net, self.net.polyak_coef)\n            else:\n                raise ValueError(\'Unknown net.update_type. Should be ""replace"" or ""polyak"". Exiting.\')\n\n    @lab_api\n    def update(self):\n        \'\'\'Updates self.target_net and the explore variables\'\'\'\n        self.update_nets()\n        return super().update()\n\n\nclass DQN(DQNBase):\n    \'\'\'\n    DQN class\n\n    e.g. algorithm_spec\n    ""algorithm"": {\n        ""name"": ""DQN"",\n        ""action_pdtype"": ""Argmax"",\n        ""action_policy"": ""epsilon_greedy"",\n        ""explore_var_spec"": {\n            ""name"": ""linear_decay"",\n            ""start_val"": 1.0,\n            ""end_val"": 0.1,\n            ""start_step"": 10,\n            ""end_step"": 1000,\n        },\n        ""gamma"": 0.99,\n        ""training_batch_iter"": 8,\n        ""training_iter"": 4,\n        ""training_frequency"": 10,\n        ""training_start_step"": 10\n    }\n    \'\'\'\n    @lab_api\n    def init_nets(self, global_nets=None):\n        super().init_nets(global_nets)\n\n\nclass DoubleDQN(DQN):\n    \'\'\'\n    Double-DQN (DDQN) class\n\n    e.g. algorithm_spec\n    ""algorithm"": {\n        ""name"": ""DDQN"",\n        ""action_pdtype"": ""Argmax"",\n        ""action_policy"": ""epsilon_greedy"",\n        ""explore_var_spec"": {\n            ""name"": ""linear_decay"",\n            ""start_val"": 1.0,\n            ""end_val"": 0.1,\n            ""start_step"": 10,\n            ""end_step"": 1000,\n        },\n        ""gamma"": 0.99,\n        ""training_batch_iter"": 8,\n        ""training_iter"": 4,\n        ""training_frequency"": 10,\n        ""training_start_step"": 10\n    }\n    \'\'\'\n    @lab_api\n    def init_nets(self, global_nets=None):\n        super().init_nets(global_nets)\n        self.online_net = self.net\n        self.eval_net = self.target_net\n'"
slm_lab/agent/algorithm/policy_util.py,10,"b'# Action policy module\n# Constructs action probability distribution used by agent to sample action and calculate log_prob, entropy, etc.\nfrom gym import spaces\nfrom slm_lab.env.wrapper import LazyFrames\nfrom slm_lab.lib import distribution, logger, math_util, util\nfrom torch import distributions\nimport numpy as np\nimport torch\n\nlogger = logger.get_logger(__name__)\n\n# register custom distributions\nsetattr(distributions, \'Argmax\', distribution.Argmax)\nsetattr(distributions, \'GumbelSoftmax\', distribution.GumbelSoftmax)\nsetattr(distributions, \'MultiCategorical\', distribution.MultiCategorical)\n# probability distributions constraints for different action types; the first in the list is the default\nACTION_PDS = {\n    \'continuous\': [\'Normal\', \'Beta\', \'Gumbel\', \'LogNormal\'],\n    \'multi_continuous\': [\'MultivariateNormal\'],\n    \'discrete\': [\'Categorical\', \'Argmax\', \'GumbelSoftmax\'],\n    \'multi_discrete\': [\'MultiCategorical\'],\n    \'multi_binary\': [\'Bernoulli\'],\n}\n\n\ndef get_action_type(action_space):\n    \'\'\'Method to get the action type to choose prob. dist. to sample actions from NN logits output\'\'\'\n    if isinstance(action_space, spaces.Box):\n        shape = action_space.shape\n        assert len(shape) == 1\n        if shape[0] == 1:\n            return \'continuous\'\n        else:\n            return \'multi_continuous\'\n    elif isinstance(action_space, spaces.Discrete):\n        return \'discrete\'\n    elif isinstance(action_space, spaces.MultiDiscrete):\n        return \'multi_discrete\'\n    elif isinstance(action_space, spaces.MultiBinary):\n        return \'multi_binary\'\n    else:\n        raise NotImplementedError\n\n\n# action_policy base methods\n\ndef get_action_pd_cls(action_pdtype, action_type):\n    \'\'\'\n    Verify and get the action prob. distribution class for construction\n    Called by body at init to set its own ActionPD\n    \'\'\'\n    pdtypes = ACTION_PDS[action_type]\n    assert action_pdtype in pdtypes, f\'Pdtype {action_pdtype} is not compatible/supported with action_type {action_type}. Options are: {pdtypes}\'\n    ActionPD = getattr(distributions, action_pdtype)\n    return ActionPD\n\n\ndef guard_tensor(state, body):\n    \'\'\'Guard-cast tensor before being input to network\'\'\'\n    if isinstance(state, LazyFrames):\n        state = state.__array__()  # realize data\n    state = torch.from_numpy(state.astype(np.float32))\n    if not body.env.is_venv:\n        # singleton state, unsqueeze as minibatch for net input\n        state = state.unsqueeze(dim=0)\n    return state\n\n\ndef calc_pdparam(state, algorithm, body):\n    \'\'\'\n    Prepare the state and run algorithm.calc_pdparam to get pdparam for action_pd\n    @param tensor:state For pdparam = net(state)\n    @param algorithm The algorithm containing self.net\n    @param body Body which links algorithm to the env which the action is for\n    @returns tensor:pdparam\n    @example\n\n    pdparam = calc_pdparam(state, algorithm, body)\n    action_pd = ActionPD(logits=pdparam)  # e.g. ActionPD is Categorical\n    action = action_pd.sample()\n    \'\'\'\n    if not torch.is_tensor(state):  # dont need to cast from numpy\n        state = guard_tensor(state, body)\n        state = state.to(algorithm.net.device)\n    pdparam = algorithm.calc_pdparam(state)\n    return pdparam\n\n\ndef init_action_pd(ActionPD, pdparam):\n    \'\'\'\n    Initialize the action_pd for discrete or continuous actions:\n    - discrete: action_pd = ActionPD(logits)\n    - continuous: action_pd = ActionPD(loc, scale)\n    \'\'\'\n    args = ActionPD.arg_constraints\n    if \'logits\' in args:  # discrete\n        # for relaxed discrete dist. with reparametrizable discrete actions\n        pd_kwargs = {\'temperature\': torch.tensor(1.0)} if hasattr(ActionPD, \'temperature\') else {}\n        action_pd = ActionPD(logits=pdparam, **pd_kwargs)\n    else:  # continuous, args = loc and scale\n        if isinstance(pdparam, list):  # split output\n            loc, scale = pdparam\n        else:\n            loc, scale = pdparam.transpose(0, 1)\n        # scale (stdev) must be > 0, log-clamp-exp\n        scale = torch.clamp(scale, min=-20, max=2).exp()\n        if \'covariance_matrix\' in args:  # split output\n            # construct covars from a batched scale tensor\n            covars = torch.diag_embed(scale)\n            action_pd = ActionPD(loc=loc, covariance_matrix=covars)\n        else:\n            action_pd = ActionPD(loc=loc, scale=scale)\n    return action_pd\n\n\ndef sample_action(ActionPD, pdparam):\n    \'\'\'\n    Convenience method to sample action(s) from action_pd = ActionPD(pdparam)\n    Works with batched pdparam too\n    @returns tensor:action Sampled action(s)\n    @example\n\n    # policy contains:\n    pdparam = calc_pdparam(state, algorithm, body)\n    action = sample_action(body.ActionPD, pdparam)\n    \'\'\'\n    action_pd = init_action_pd(ActionPD, pdparam)\n    action = action_pd.sample()\n    return action\n\n\n# action_policy used by agent\n\n\ndef default(state, algorithm, body):\n    \'\'\'Plain policy by direct sampling from a default action probability defined by body.ActionPD\'\'\'\n    pdparam = calc_pdparam(state, algorithm, body)\n    action = sample_action(body.ActionPD, pdparam)\n    return action\n\n\ndef random(state, algorithm, body):\n    \'\'\'Random action using gym.action_space.sample(), with the same format as default()\'\'\'\n    if body.env.is_venv:\n        _action = [body.action_space.sample() for _ in range(body.env.num_envs)]\n    else:\n        _action = [body.action_space.sample()]\n    action = torch.tensor(_action)\n    return action\n\n\ndef epsilon_greedy(state, algorithm, body):\n    \'\'\'Epsilon-greedy policy: with probability epsilon, do random action, otherwise do default sampling.\'\'\'\n    epsilon = body.explore_var\n    if epsilon > np.random.rand():\n        return random(state, algorithm, body)\n    else:\n        return default(state, algorithm, body)\n\n\ndef boltzmann(state, algorithm, body):\n    \'\'\'\n    Boltzmann policy: adjust pdparam with temperature tau; the higher the more randomness/noise in action.\n    \'\'\'\n    tau = body.explore_var\n    pdparam = calc_pdparam(state, algorithm, body)\n    pdparam /= tau\n    action = sample_action(body.ActionPD, pdparam)\n    return action\n\n\n# multi-body/multi-env action_policy used by agent\n# TODO rework\n\ndef multi_default(states, algorithm, body_list, pdparam):\n    \'\'\'\n    Apply default policy body-wise\n    Note, for efficiency, do a single forward pass to calculate pdparam, then call this policy like:\n    @example\n\n    pdparam = self.calc_pdparam(state)\n    action_a = self.action_policy(pdparam, self, body_list)\n    \'\'\'\n    # assert pdparam has been chunked\n    assert pdparam.dim() > 1 and len(pdparam) == len(body_list), f\'pdparam shape: {pdparam.shape}, bodies: {len(body_list)}\'\n    action_list = []\n    for idx, sub_pdparam in enumerate(pdparam):\n        body = body_list[idx]\n        guard_tensor(states[idx], body)  # for consistency with singleton inner logic\n        action = sample_action(body.ActionPD, sub_pdparam)\n        action_list.append(action)\n    action_a = torch.tensor(action_list, device=algorithm.net.device).unsqueeze(dim=1)\n    return action_a\n\n\ndef multi_random(states, algorithm, body_list, pdparam):\n    \'\'\'Apply random policy body-wise.\'\'\'\n    action_list = []\n    for idx, body in body_list:\n        action = random(states[idx], algorithm, body)\n        action_list.append(action)\n    action_a = torch.tensor(action_list, device=algorithm.net.device).unsqueeze(dim=1)\n    return action_a\n\n\ndef multi_epsilon_greedy(states, algorithm, body_list, pdparam):\n    \'\'\'Apply epsilon-greedy policy body-wise\'\'\'\n    assert len(pdparam) > 1 and len(pdparam) == len(body_list), f\'pdparam shape: {pdparam.shape}, bodies: {len(body_list)}\'\n    action_list = []\n    for idx, sub_pdparam in enumerate(pdparam):\n        body = body_list[idx]\n        epsilon = body.explore_var\n        if epsilon > np.random.rand():\n            action = random(states[idx], algorithm, body)\n        else:\n            guard_tensor(states[idx], body)  # for consistency with singleton inner logic\n            action = sample_action(body.ActionPD, sub_pdparam)\n        action_list.append(action)\n    action_a = torch.tensor(action_list, device=algorithm.net.device).unsqueeze(dim=1)\n    return action_a\n\n\ndef multi_boltzmann(states, algorithm, body_list, pdparam):\n    \'\'\'Apply Boltzmann policy body-wise\'\'\'\n    assert len(pdparam) > 1 and len(pdparam) == len(body_list), f\'pdparam shape: {pdparam.shape}, bodies: {len(body_list)}\'\n    action_list = []\n    for idx, sub_pdparam in enumerate(pdparam):\n        body = body_list[idx]\n        guard_tensor(states[idx], body)  # for consistency with singleton inner logic\n        tau = body.explore_var\n        sub_pdparam /= tau\n        action = sample_action(body.ActionPD, sub_pdparam)\n        action_list.append(action)\n    action_a = torch.tensor(action_list, device=algorithm.net.device).unsqueeze(dim=1)\n    return action_a\n\n\n# action policy update methods\n\nclass VarScheduler:\n    \'\'\'\n    Variable scheduler for decaying variables such as explore_var (epsilon, tau) and entropy\n\n    e.g. spec\n    ""explore_var_spec"": {\n        ""name"": ""linear_decay"",\n        ""start_val"": 1.0,\n        ""end_val"": 0.1,\n        ""start_step"": 0,\n        ""end_step"": 800,\n    },\n    \'\'\'\n\n    def __init__(self, var_decay_spec=None):\n        self._updater_name = \'no_decay\' if var_decay_spec is None else var_decay_spec[\'name\']\n        self._updater = getattr(math_util, self._updater_name)\n        util.set_attr(self, dict(\n            start_val=np.nan,\n        ))\n        util.set_attr(self, var_decay_spec, [\n            \'start_val\',\n            \'end_val\',\n            \'start_step\',\n            \'end_step\',\n        ])\n        if not getattr(self, \'end_val\', None):\n            self.end_val = self.start_val\n\n    def update(self, algorithm, clock):\n        \'\'\'Get an updated value for var\'\'\'\n        if (util.in_eval_lab_mode()) or self._updater_name == \'no_decay\':\n            return self.end_val\n        step = clock.get()\n        val = self._updater(self.start_val, self.end_val, self.start_step, self.end_step, step)\n        return val\n'"
slm_lab/agent/algorithm/ppo.py,8,"b'from copy import deepcopy\nfrom slm_lab.agent.algorithm import policy_util\nfrom slm_lab.agent.algorithm.actor_critic import ActorCritic\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.lib import logger, math_util, util\nfrom slm_lab.lib.decorator import lab_api\nimport math\nimport numpy as np\nimport torch\n\nlogger = logger.get_logger(__name__)\n\n\nclass PPO(ActorCritic):\n    \'\'\'\n    Implementation of PPO\n    This is actually just ActorCritic with a custom loss function\n    Original paper: ""Proximal Policy Optimization Algorithms""\n    https://arxiv.org/pdf/1707.06347.pdf\n\n    Adapted from OpenAI baselines, CPU version https://github.com/openai/baselines/tree/master/baselines/ppo1\n    Algorithm:\n    for iteration = 1, 2, 3, ... do\n        for actor = 1, 2, 3, ..., N do\n            run policy pi_old in env for T timesteps\n            compute advantage A_1, ..., A_T\n        end for\n        optimize surrogate L wrt theta, with K epochs and minibatch size M <= NT\n    end for\n\n    e.g. algorithm_spec\n    ""algorithm"": {\n        ""name"": ""PPO"",\n        ""action_pdtype"": ""default"",\n        ""action_policy"": ""default"",\n        ""explore_var_spec"": null,\n        ""gamma"": 0.99,\n        ""lam"": 0.95,\n        ""clip_eps_spec"": {\n          ""name"": ""linear_decay"",\n          ""start_val"": 0.01,\n          ""end_val"": 0.001,\n          ""start_step"": 100,\n          ""end_step"": 5000,\n        },\n        ""entropy_coef_spec"": {\n          ""name"": ""linear_decay"",\n          ""start_val"": 0.01,\n          ""end_val"": 0.001,\n          ""start_step"": 100,\n          ""end_step"": 5000,\n        },\n        ""minibatch_size"": 256,\n        ""time_horizon"": 32,\n        ""training_epoch"": 8,\n    }\n\n    e.g. special net_spec param ""shared"" to share/separate Actor/Critic\n    ""net"": {\n        ""type"": ""MLPNet"",\n        ""shared"": true,\n        ...\n    \'\'\'\n\n    @lab_api\n    def init_algorithm_params(self):\n        \'\'\'Initialize other algorithm parameters\'\'\'\n        # set default\n        util.set_attr(self, dict(\n            action_pdtype=\'default\',\n            action_policy=\'default\',\n            explore_var_spec=None,\n            entropy_coef_spec=None,\n            minibatch_size=4,\n            val_loss_coef=1.0,\n        ))\n        util.set_attr(self, self.algorithm_spec, [\n            \'action_pdtype\',\n            \'action_policy\',\n            # theoretically, PPO does not have policy update; but in this implementation we have such option\n            \'explore_var_spec\',\n            \'gamma\',\n            \'lam\',\n            \'clip_eps_spec\',\n            \'entropy_coef_spec\',\n            \'val_loss_coef\',\n            \'minibatch_size\',\n            \'time_horizon\',  # training_frequency = actor * horizon\n            \'training_epoch\',\n        ])\n        self.to_train = 0\n        # guard\n        num_envs = self.body.env.num_envs\n        if self.minibatch_size % num_envs != 0 or self.time_horizon % num_envs != 0:\n            self.minibatch_size = math.ceil(self.minibatch_size / num_envs) * num_envs\n            self.time_horizon = math.ceil(self.time_horizon / num_envs) * num_envs\n            logger.info(f\'minibatch_size and time_horizon needs to be multiples of num_envs; autocorrected values: minibatch_size: {self.minibatch_size}  time_horizon {self.time_horizon}\')\n        self.training_frequency = self.time_horizon  # since all memories stores num_envs by batch in list\n        assert self.memory_spec[\'name\'] == \'OnPolicyBatchReplay\', f\'PPO only works with OnPolicyBatchReplay, but got {self.memory_spec[""name""]}\'\n        self.action_policy = getattr(policy_util, self.action_policy)\n        self.explore_var_scheduler = policy_util.VarScheduler(self.explore_var_spec)\n        self.body.explore_var = self.explore_var_scheduler.start_val\n        # extra variable decays for PPO\n        self.clip_eps_scheduler = policy_util.VarScheduler(self.clip_eps_spec)\n        self.body.clip_eps = self.clip_eps_scheduler.start_val\n        if self.entropy_coef_spec is not None:\n            self.entropy_coef_scheduler = policy_util.VarScheduler(self.entropy_coef_spec)\n            self.body.entropy_coef = self.entropy_coef_scheduler.start_val\n        # PPO uses GAE\n        self.calc_advs_v_targets = self.calc_gae_advs_v_targets\n\n    @lab_api\n    def init_nets(self, global_nets=None):\n        \'\'\'PPO uses old and new to calculate ratio for loss\'\'\'\n        super().init_nets(global_nets)\n        # create old net to calculate ratio\n        self.old_net = deepcopy(self.net)\n        assert id(self.old_net) != id(self.net)\n\n    def calc_policy_loss(self, batch, pdparams, advs):\n        \'\'\'\n        The PPO loss function (subscript t is omitted)\n        L^{CLIP+VF+S} = E[ L^CLIP - c1 * L^VF + c2 * H[pi](s) ]\n\n        Breakdown piecewise,\n        1. L^CLIP = E[ min(ratio * A, clip(ratio, 1-eps, 1+eps) * A) ]\n        where ratio = pi(a|s) / pi_old(a|s)\n\n        2. L^VF = E[ mse(V(s_t), V^target) ]\n\n        3. H = E[ entropy ]\n        \'\'\'\n        clip_eps = self.body.clip_eps\n        action_pd = policy_util.init_action_pd(self.body.ActionPD, pdparams)\n        states = batch[\'states\']\n        actions = batch[\'actions\']\n        if self.body.env.is_venv:\n            states = math_util.venv_unpack(states)\n            actions = math_util.venv_unpack(actions)\n\n        # L^CLIP\n        log_probs = action_pd.log_prob(actions)\n        with torch.no_grad():\n            old_pdparams = self.calc_pdparam(states, net=self.old_net)\n            old_action_pd = policy_util.init_action_pd(self.body.ActionPD, old_pdparams)\n            old_log_probs = old_action_pd.log_prob(actions)\n        assert log_probs.shape == old_log_probs.shape\n        ratios = torch.exp(log_probs - old_log_probs)\n        logger.debug(f\'ratios: {ratios}\')\n        sur_1 = ratios * advs\n        sur_2 = torch.clamp(ratios, 1.0 - clip_eps, 1.0 + clip_eps) * advs\n        # flip sign because need to maximize\n        clip_loss = -torch.min(sur_1, sur_2).mean()\n        logger.debug(f\'clip_loss: {clip_loss}\')\n\n        # L^VF (inherit from ActorCritic)\n\n        # H entropy regularization\n        entropy = action_pd.entropy().mean()\n        self.body.mean_entropy = entropy  # update logging variable\n        ent_penalty = -self.body.entropy_coef * entropy\n        logger.debug(f\'ent_penalty: {ent_penalty}\')\n\n        policy_loss = clip_loss + ent_penalty\n        logger.debug(f\'PPO Actor policy loss: {policy_loss:g}\')\n        return policy_loss\n\n    def train(self):\n        clock = self.body.env.clock\n        if self.to_train == 1:\n            net_util.copy(self.net, self.old_net)  # update old net\n            batch = self.sample()\n            clock.set_batch_size(len(batch))\n            with torch.no_grad():\n                states = batch[\'states\']\n                if self.body.env.is_venv:\n                    states = math_util.venv_unpack(states)\n                # NOTE states is massive with batch_size = time_horizon * num_envs. Chunk up so forward pass can fit into device esp. GPU\n                num_chunks = int(len(states) / self.minibatch_size)\n                v_preds_chunks = [self.calc_v(states_chunk, use_cache=False) for states_chunk in torch.chunk(states, num_chunks)]\n                v_preds = torch.cat(v_preds_chunks)\n                advs, v_targets = self.calc_advs_v_targets(batch, v_preds)\n            # piggy back on batch, but remember to not pack or unpack\n            batch[\'advs\'], batch[\'v_targets\'] = advs, v_targets\n            if self.body.env.is_venv:  # unpack if venv for minibatch sampling\n                for k, v in batch.items():\n                    if k not in (\'advs\', \'v_targets\'):\n                        batch[k] = math_util.venv_unpack(v)\n            total_loss = torch.tensor(0.0)\n            for _ in range(self.training_epoch):\n                minibatches = util.split_minibatch(batch, self.minibatch_size)\n                for minibatch in minibatches:\n                    if self.body.env.is_venv:  # re-pack to restore proper shape\n                        for k, v in minibatch.items():\n                            if k not in (\'advs\', \'v_targets\'):\n                                minibatch[k] = math_util.venv_pack(v, self.body.env.num_envs)\n                    advs, v_targets = minibatch[\'advs\'], minibatch[\'v_targets\']\n                    pdparams, v_preds = self.calc_pdparam_v(minibatch)\n                    policy_loss = self.calc_policy_loss(minibatch, pdparams, advs)  # from actor\n                    val_loss = self.calc_val_loss(v_preds, v_targets)  # from critic\n                    if self.shared:  # shared network\n                        loss = policy_loss + val_loss\n                        self.net.train_step(loss, self.optim, self.lr_scheduler, clock=clock, global_net=self.global_net)\n                    else:\n                        self.net.train_step(policy_loss, self.optim, self.lr_scheduler, clock=clock, global_net=self.global_net)\n                        self.critic_net.train_step(val_loss, self.critic_optim, self.critic_lr_scheduler, clock=clock, global_net=self.global_critic_net)\n                        loss = policy_loss + val_loss\n                    total_loss += loss\n            loss = total_loss / self.training_epoch / len(minibatches)\n            # reset\n            self.to_train = 0\n            logger.debug(f\'Trained {self.name} at epi: {clock.epi}, frame: {clock.frame}, t: {clock.t}, total_reward so far: {self.body.env.total_reward}, loss: {loss:g}\')\n            return loss.item()\n        else:\n            return np.nan\n\n    @lab_api\n    def update(self):\n        self.body.explore_var = self.explore_var_scheduler.update(self, self.body.env.clock)\n        if self.entropy_coef_spec is not None:\n            self.body.entropy_coef = self.entropy_coef_scheduler.update(self, self.body.env.clock)\n        self.body.clip_eps = self.clip_eps_scheduler.update(self, self.body.env.clock)\n        return self.body.explore_var\n'"
slm_lab/agent/algorithm/random.py,0,"b""# The random agent algorithm\n# For basic dev purpose\nfrom slm_lab.agent.algorithm.base import Algorithm\nfrom slm_lab.lib import logger\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\n\nlogger = logger.get_logger(__name__)\n\n\nclass Random(Algorithm):\n    '''\n    Example Random agent that works in both discrete and continuous envs\n    '''\n\n    @lab_api\n    def init_algorithm_params(self):\n        '''Initialize other algorithm parameters'''\n        self.to_train = 0\n        self.training_frequency = 1\n        self.training_start_step = 0\n\n    @lab_api\n    def init_nets(self, global_nets=None):\n        '''Initialize the neural network from the spec'''\n        self.net_names = []\n\n    @lab_api\n    def act(self, state):\n        '''Random action'''\n        body = self.body\n        if body.env.is_venv:\n            action = np.array([body.action_space.sample() for _ in range(body.env.num_envs)])\n        else:\n            action = body.action_space.sample()\n        return action\n\n    @lab_api\n    def sample(self):\n        self.body.memory.sample()\n        batch = np.nan\n        return batch\n\n    @lab_api\n    def train(self):\n        self.sample()\n        self.body.env.clock.tick('opt_step')  # to simulate metrics calc\n        loss = np.nan\n        return loss\n\n    @lab_api\n    def update(self):\n        self.body.explore_var = np.nan\n        return self.body.explore_var\n"""
slm_lab/agent/algorithm/reinforce.py,0,"b'from slm_lab.agent import net\nfrom slm_lab.agent.algorithm import policy_util\nfrom slm_lab.agent.algorithm.base import Algorithm\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.lib import logger, math_util, util\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\n\nlogger = logger.get_logger(__name__)\n\n\nclass Reinforce(Algorithm):\n    \'\'\'\n    Implementation of REINFORCE (Williams, 1992) with baseline for discrete or continuous actions http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf\n    Adapted from https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py\n    Algorithm:\n        0. Collect n episodes of data\n        1. At each timestep in an episode\n            - Calculate the advantage of that timestep\n            - Multiply the advantage by the negative of the log probability of the action taken\n        2. Sum all the values above.\n        3. Calculate the gradient of this value with respect to all of the parameters of the network\n        4. Update the network parameters using the gradient\n\n    e.g. algorithm_spec:\n    ""algorithm"": {\n        ""name"": ""Reinforce"",\n        ""action_pdtype"": ""default"",\n        ""action_policy"": ""default"",\n        ""explore_var_spec"": null,\n        ""gamma"": 0.99,\n        ""entropy_coef_spec"": {\n          ""name"": ""linear_decay"",\n          ""start_val"": 0.01,\n          ""end_val"": 0.001,\n          ""start_step"": 100,\n          ""end_step"": 5000,\n        },\n        ""training_frequency"": 1,\n    }\n    \'\'\'\n\n    @lab_api\n    def init_algorithm_params(self):\n        \'\'\'Initialize other algorithm parameters\'\'\'\n        # set default\n        util.set_attr(self, dict(\n            action_pdtype=\'default\',\n            action_policy=\'default\',\n            center_return=False,\n            explore_var_spec=None,\n            entropy_coef_spec=None,\n            policy_loss_coef=1.0,\n        ))\n        util.set_attr(self, self.algorithm_spec, [\n            \'action_pdtype\',\n            \'action_policy\',\n            \'center_return\',  # center by the mean\n            \'explore_var_spec\',\n            \'gamma\',  # the discount factor\n            \'entropy_coef_spec\',\n            \'policy_loss_coef\',\n            \'training_frequency\',\n        ])\n        self.to_train = 0\n        self.action_policy = getattr(policy_util, self.action_policy)\n        self.explore_var_scheduler = policy_util.VarScheduler(self.explore_var_spec)\n        self.body.explore_var = self.explore_var_scheduler.start_val\n        if self.entropy_coef_spec is not None:\n            self.entropy_coef_scheduler = policy_util.VarScheduler(self.entropy_coef_spec)\n            self.body.entropy_coef = self.entropy_coef_scheduler.start_val\n\n    @lab_api\n    def init_nets(self, global_nets=None):\n        \'\'\'\n        Initialize the neural network used to learn the policy function from the spec\n        Below we automatically select an appropriate net for a discrete or continuous action space if the setting is of the form \'MLPNet\'. Otherwise the correct type of network is assumed to be specified in the spec.\n        Networks for continuous action spaces have two heads and return two values, the first is a tensor containing the mean of the action policy, the second is a tensor containing the std deviation of the action policy. The distribution is assumed to be a Gaussian (Normal) distribution.\n        Networks for discrete action spaces have a single head and return the logits for a categorical probability distribution over the discrete actions\n        \'\'\'\n        in_dim = self.body.state_dim\n        out_dim = net_util.get_out_dim(self.body)\n        NetClass = getattr(net, self.net_spec[\'type\'])\n        self.net = NetClass(self.net_spec, in_dim, out_dim)\n        self.net_names = [\'net\']\n        # init net optimizer and its lr scheduler\n        self.optim = net_util.get_optim(self.net, self.net.optim_spec)\n        self.lr_scheduler = net_util.get_lr_scheduler(self.optim, self.net.lr_scheduler_spec)\n        net_util.set_global_nets(self, global_nets)\n        self.end_init_nets()\n\n    @lab_api\n    def calc_pdparam(self, x, net=None):\n        \'\'\'The pdparam will be the logits for discrete prob. dist., or the mean and std for continuous prob. dist.\'\'\'\n        net = self.net if net is None else net\n        pdparam = net(x)\n        return pdparam\n\n    @lab_api\n    def act(self, state):\n        body = self.body\n        action = self.action_policy(state, self, body)\n        return action.cpu().squeeze().numpy()  # squeeze to handle scalar\n\n    @lab_api\n    def sample(self):\n        \'\'\'Samples a batch from memory\'\'\'\n        batch = self.body.memory.sample()\n        batch = util.to_torch_batch(batch, self.net.device, self.body.memory.is_episodic)\n        return batch\n\n    def calc_pdparam_batch(self, batch):\n        \'\'\'Efficiently forward to get pdparam and by batch for loss computation\'\'\'\n        states = batch[\'states\']\n        if self.body.env.is_venv:\n            states = math_util.venv_unpack(states)\n        pdparam = self.calc_pdparam(states)\n        return pdparam\n\n    def calc_ret_advs(self, batch):\n        \'\'\'Calculate plain returns; which is generalized to advantage in ActorCritic\'\'\'\n        rets = math_util.calc_returns(batch[\'rewards\'], batch[\'dones\'], self.gamma)\n        if self.center_return:\n            rets = math_util.center_mean(rets)\n        advs = rets\n        if self.body.env.is_venv:\n            advs = math_util.venv_unpack(advs)\n        logger.debug(f\'advs: {advs}\')\n        return advs\n\n    def calc_policy_loss(self, batch, pdparams, advs):\n        \'\'\'Calculate the actor\'s policy loss\'\'\'\n        action_pd = policy_util.init_action_pd(self.body.ActionPD, pdparams)\n        actions = batch[\'actions\']\n        if self.body.env.is_venv:\n            actions = math_util.venv_unpack(actions)\n        log_probs = action_pd.log_prob(actions)\n        policy_loss = - self.policy_loss_coef * (log_probs * advs).mean()\n        if self.entropy_coef_spec:\n            entropy = action_pd.entropy().mean()\n            self.body.mean_entropy = entropy  # update logging variable\n            policy_loss += (-self.body.entropy_coef * entropy)\n        logger.debug(f\'Actor policy loss: {policy_loss:g}\')\n        return policy_loss\n\n    @lab_api\n    def train(self):\n        clock = self.body.env.clock\n        if self.to_train == 1:\n            batch = self.sample()\n            clock.set_batch_size(len(batch))\n            pdparams = self.calc_pdparam_batch(batch)\n            advs = self.calc_ret_advs(batch)\n            loss = self.calc_policy_loss(batch, pdparams, advs)\n            self.net.train_step(loss, self.optim, self.lr_scheduler, clock=clock, global_net=self.global_net)\n            # reset\n            self.to_train = 0\n            logger.debug(f\'Trained {self.name} at epi: {clock.epi}, frame: {clock.frame}, t: {clock.t}, total_reward so far: {self.body.env.total_reward}, loss: {loss:g}\')\n            return loss.item()\n        else:\n            return np.nan\n\n    @lab_api\n    def update(self):\n        self.body.explore_var = self.explore_var_scheduler.update(self, self.body.env.clock)\n        if self.entropy_coef_spec is not None:\n            self.body.entropy_coef = self.entropy_coef_scheduler.update(self, self.body.env.clock)\n        return self.body.explore_var\n'"
slm_lab/agent/algorithm/sac.py,11,"b'from slm_lab.agent import net\nfrom slm_lab.agent.algorithm import policy_util\nfrom slm_lab.agent.algorithm.actor_critic import ActorCritic\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.lib import logger, util\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nlogger = logger.get_logger(__name__)\n\n\nclass SoftActorCritic(ActorCritic):\n    \'\'\'\n    Implementation of Soft Actor-Critic (SAC)\n    Original paper: ""Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor""\n    https://arxiv.org/abs/1801.01290\n    Improvement of SAC paper: ""Soft Actor-Critic Algorithms and Applications""\n    https://arxiv.org/abs/1812.05905\n\n    e.g. algorithm_spec\n    ""algorithm"": {\n        ""name"": ""SoftActorCritic"",\n        ""action_pdtype"": ""default"",\n        ""action_policy"": ""default"",\n        ""gamma"": 0.99,\n        ""training_frequency"": 1,\n    }\n    \'\'\'\n    @lab_api\n    def init_algorithm_params(self):\n        \'\'\'Initialize other algorithm parameters\'\'\'\n        # set default\n        util.set_attr(self, dict(\n            action_pdtype=\'default\',\n            action_policy=\'default\',\n            training_iter=self.body.env.num_envs,\n            training_start_step=self.body.memory.batch_size,\n        ))\n        util.set_attr(self, self.algorithm_spec, [\n            \'action_pdtype\',\n            \'action_policy\',\n            \'gamma\',  # the discount factor\n            \'training_iter\',\n            \'training_frequency\',\n            \'training_start_step\',\n        ])\n        if self.body.is_discrete:\n            assert self.action_pdtype == \'GumbelSoftmax\'\n        self.to_train = 0\n        self.action_policy = getattr(policy_util, self.action_policy)\n\n    @lab_api\n    def init_nets(self, global_nets=None):\n        \'\'\'\n        Networks: net(actor/policy), q1_net, target_q1_net, q2_net, target_q2_net\n        All networks are separate, and have the same hidden layer architectures and optim specs, so tuning is minimal\n        \'\'\'\n        self.shared = False  # SAC does not share networks\n        NetClass = getattr(net, self.net_spec[\'type\'])\n        # main actor network\n        self.net = NetClass(self.net_spec, self.body.state_dim, net_util.get_out_dim(self.body))\n        self.net_names = [\'net\']\n        # two critic Q-networks to mitigate positive bias in q_loss and speed up training, uses q_net.py with prefix Q\n        QNetClass = getattr(net, \'Q\' + self.net_spec[\'type\'])\n        q_in_dim = [self.body.state_dim, self.body.action_dim]\n        self.q1_net = QNetClass(self.net_spec, q_in_dim, 1)\n        self.target_q1_net = QNetClass(self.net_spec, q_in_dim, 1)\n        self.q2_net = QNetClass(self.net_spec, q_in_dim, 1)\n        self.target_q2_net = QNetClass(self.net_spec, q_in_dim, 1)\n        self.net_names += [\'q1_net\', \'target_q1_net\', \'q2_net\', \'target_q2_net\']\n        net_util.copy(self.q1_net, self.target_q1_net)\n        net_util.copy(self.q2_net, self.target_q2_net)\n        # temperature variable to be learned, and its target entropy\n        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.net.device)\n        self.alpha = self.log_alpha.detach().exp()\n        if self.body.is_discrete:\n            self.target_entropy = - self.body.action_space.n\n        else:\n            self.target_entropy = - np.product(self.body.action_space.shape)\n\n        # init net optimizer and its lr scheduler\n        self.optim = net_util.get_optim(self.net, self.net.optim_spec)\n        self.lr_scheduler = net_util.get_lr_scheduler(self.optim, self.net.lr_scheduler_spec)\n        self.q1_optim = net_util.get_optim(self.q1_net, self.q1_net.optim_spec)\n        self.q1_lr_scheduler = net_util.get_lr_scheduler(self.q1_optim, self.q1_net.lr_scheduler_spec)\n        self.q2_optim = net_util.get_optim(self.q2_net, self.q2_net.optim_spec)\n        self.q2_lr_scheduler = net_util.get_lr_scheduler(self.q2_optim, self.q2_net.lr_scheduler_spec)\n        self.alpha_optim = net_util.get_optim(self.log_alpha, self.net.optim_spec)\n        self.alpha_lr_scheduler = net_util.get_lr_scheduler(self.alpha_optim, self.net.lr_scheduler_spec)\n        net_util.set_global_nets(self, global_nets)\n        self.end_init_nets()\n\n    @lab_api\n    def act(self, state):\n        if self.body.env.clock.frame < self.training_start_step:\n            return policy_util.random(state, self, self.body).cpu().squeeze().numpy()\n        else:\n            action = self.action_policy(state, self, self.body)\n            if not self.body.is_discrete:\n                action = self.scale_action(torch.tanh(action))  # continuous action bound\n            return action.cpu().squeeze().numpy()\n\n    def scale_action(self, action):\n        \'\'\'Scale continuous actions from tanh range\'\'\'\n        action_space = self.body.action_space\n        low, high = torch.from_numpy(action_space.low), torch.from_numpy(action_space.high)\n        return action * (high - low) / 2 + (low + high) / 2\n\n    def guard_q_actions(self, actions):\n        \'\'\'Guard to convert actions to one-hot for input to Q-network\'\'\'\n        if self.body.is_discrete:\n            # TODO support multi-discrete actions\n            actions = F.one_hot(actions.long(), self.body.action_dim).float()\n        return actions\n\n    def calc_log_prob_action(self, action_pd, reparam=False):\n        \'\'\'Calculate log_probs and actions with option to reparametrize from paper eq. 11\'\'\'\n        samples = action_pd.rsample() if reparam else action_pd.sample()\n        if self.body.is_discrete:  # this is straightforward using GumbelSoftmax\n            actions = samples\n            log_probs = action_pd.log_prob(actions)\n        else:\n            mus = samples\n            actions = self.scale_action(torch.tanh(mus))\n            if actions.dim() == 1:  # handle shape consistency for single actions\n                actions = actions.unsqueeze(dim=-1)\n            # paper Appendix C. Enforcing Action Bounds for continuous actions\n            log_probs = (action_pd.log_prob(mus) - torch.log(1 - actions.pow(2) + 1e-6).sum(1))\n        return log_probs, actions\n\n    def calc_q(self, state, action, net):\n        \'\'\'Forward-pass to calculate the predicted state-action-value from q1_net.\'\'\'\n        if not self.body.is_discrete and action.dim() == 1:  # handle shape consistency for single continuous action\n            action = action.unsqueeze(dim=-1)\n        q_pred = net(state, action).view(-1)\n        return q_pred\n\n    def calc_q_targets(self, batch):\n        \'\'\'Q_tar = r + gamma * (target_Q(s\', a\') - alpha * log pi(a\'|s\'))\'\'\'\n        next_states = batch[\'next_states\']\n        with torch.no_grad():\n            pdparams = self.calc_pdparam(next_states)\n            action_pd = policy_util.init_action_pd(self.body.ActionPD, pdparams)\n            next_log_probs, next_actions = self.calc_log_prob_action(action_pd)\n            next_actions = self.guard_q_actions(next_actions)  # non-reparam discrete actions need to be converted into one-hot\n\n            next_target_q1_preds = self.calc_q(next_states, next_actions, self.target_q1_net)\n            next_target_q2_preds = self.calc_q(next_states, next_actions, self.target_q2_net)\n            next_target_q_preds = torch.min(next_target_q1_preds, next_target_q2_preds)\n            q_targets = batch[\'rewards\'] + self.gamma * (1 - batch[\'dones\']) * (next_target_q_preds - self.alpha * next_log_probs)\n        return q_targets\n\n    def calc_reg_loss(self, preds, targets):\n        \'\'\'Calculate the regression loss for V and Q values, using the same loss function from net_spec\'\'\'\n        assert preds.shape == targets.shape, f\'{preds.shape} != {targets.shape}\'\n        reg_loss = self.net.loss_fn(preds, targets)\n        return reg_loss\n\n    def calc_policy_loss(self, batch, log_probs, reparam_actions):\n        \'\'\'policy_loss = alpha * log pi(f(a)|s) - Q1(s, f(a)), where f(a) = reparametrized action\'\'\'\n        states = batch[\'states\']\n        q1_preds = self.calc_q(states, reparam_actions, self.q1_net)\n        q2_preds = self.calc_q(states, reparam_actions, self.q2_net)\n        q_preds = torch.min(q1_preds, q2_preds)\n        policy_loss = (self.alpha * log_probs - q_preds).mean()\n        return policy_loss\n\n    def calc_alpha_loss(self, log_probs):\n        alpha_loss = - (self.log_alpha * (log_probs.detach() + self.target_entropy)).mean()\n        return alpha_loss\n\n    def try_update_per(self, q_preds, q_targets):\n        if \'Prioritized\' in util.get_class_name(self.body.memory):  # PER\n            with torch.no_grad():\n                errors = (q_preds - q_targets).abs().cpu().numpy()\n            self.body.memory.update_priorities(errors)\n\n    def train_alpha(self, alpha_loss):\n        \'\'\'Custom method to train the alpha variable\'\'\'\n        self.alpha_lr_scheduler.step(epoch=self.body.env.clock.frame)\n        self.alpha_optim.zero_grad()\n        alpha_loss.backward()\n        self.alpha_optim.step()\n        self.alpha = self.log_alpha.detach().exp()\n\n    def train(self):\n        \'\'\'Train actor critic by computing the loss in batch efficiently\'\'\'\n        clock = self.body.env.clock\n        if self.to_train == 1:\n            for _ in range(self.training_iter):\n                batch = self.sample()\n                clock.set_batch_size(len(batch))\n\n                states = batch[\'states\']\n                actions = self.guard_q_actions(batch[\'actions\'])\n                q_targets = self.calc_q_targets(batch)\n                # Q-value loss for both Q nets\n                q1_preds = self.calc_q(states, actions, self.q1_net)\n                q1_loss = self.calc_reg_loss(q1_preds, q_targets)\n                self.q1_net.train_step(q1_loss, self.q1_optim, self.q1_lr_scheduler, clock=clock, global_net=self.global_q1_net)\n\n                q2_preds = self.calc_q(states, actions, self.q2_net)\n                q2_loss = self.calc_reg_loss(q2_preds, q_targets)\n                self.q2_net.train_step(q2_loss, self.q2_optim, self.q2_lr_scheduler, clock=clock, global_net=self.global_q2_net)\n\n                # policy loss\n                action_pd = policy_util.init_action_pd(self.body.ActionPD, self.calc_pdparam(states))\n                log_probs, reparam_actions = self.calc_log_prob_action(action_pd, reparam=True)\n                policy_loss = self.calc_policy_loss(batch, log_probs, reparam_actions)\n                self.net.train_step(policy_loss, self.optim, self.lr_scheduler, clock=clock, global_net=self.global_net)\n\n                # alpha loss\n                alpha_loss = self.calc_alpha_loss(log_probs)\n                self.train_alpha(alpha_loss)\n\n                loss = q1_loss + q2_loss + policy_loss + alpha_loss\n                # update target networks\n                self.update_nets()\n                # update PER priorities if availalbe\n                self.try_update_per(torch.min(q1_preds, q2_preds), q_targets)\n\n            # reset\n            self.to_train = 0\n            logger.debug(f\'Trained {self.name} at epi: {clock.epi}, frame: {clock.frame}, t: {clock.t}, total_reward so far: {self.body.env.total_reward}, loss: {loss:g}\')\n            return loss.item()\n        else:\n            return np.nan\n\n    def update_nets(self):\n        \'\'\'Update target networks\'\'\'\n        if util.frame_mod(self.body.env.clock.frame, self.q1_net.update_frequency, self.body.env.num_envs):\n            if self.q1_net.update_type == \'replace\':\n                net_util.copy(self.q1_net, self.target_q1_net)\n                net_util.copy(self.q2_net, self.target_q2_net)\n            elif self.q1_net.update_type == \'polyak\':\n                net_util.polyak_update(self.q1_net, self.target_q1_net, self.q1_net.polyak_coef)\n                net_util.polyak_update(self.q2_net, self.target_q2_net, self.q2_net.polyak_coef)\n            else:\n                raise ValueError(\'Unknown q1_net.update_type. Should be ""replace"" or ""polyak"". Exiting.\')\n\n    @lab_api\n    def update(self):\n        \'\'\'Override parent method to do nothing\'\'\'\n        return self.body.explore_var\n'"
slm_lab/agent/algorithm/sarsa.py,1,"b'from slm_lab.agent import net\nfrom slm_lab.agent.algorithm import policy_util\nfrom slm_lab.agent.algorithm.base import Algorithm\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.lib import logger, math_util, util\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\nimport torch\n\nlogger = logger.get_logger(__name__)\n\n\nclass SARSA(Algorithm):\n    \'\'\'\n    Implementation of SARSA.\n\n    Algorithm:\n    Repeat:\n        1. Collect some examples by acting in the environment and store them in an on policy replay memory (either batch or episodic)\n        2. For each example calculate the target (bootstrapped estimate of the discounted value of the state and action taken), y, using a neural network to approximate the Q function. s_t\' is the next state following the action actually taken, a_t. a_t\' is the action actually taken in the next state s_t\'.\n                y_t = r_t + gamma * Q(s_t\', a_t\')\n        4. For each example calculate the current estimate of the discounted value of the state and action taken\n                x_t = Q(s_t, a_t)\n        5. Calculate L(x, y) where L is a regression loss (eg. mse)\n        6. Calculate the gradient of L with respect to all the parameters in the network and update the network parameters using the gradient\n\n    e.g. algorithm_spec\n    ""algorithm"": {\n        ""name"": ""SARSA"",\n        ""action_pdtype"": ""default"",\n        ""action_policy"": ""boltzmann"",\n        ""explore_var_spec"": {\n            ""name"": ""linear_decay"",\n            ""start_val"": 1.0,\n            ""end_val"": 0.1,\n            ""start_step"": 10,\n            ""end_step"": 1000,\n        },\n        ""gamma"": 0.99,\n        ""training_frequency"": 10,\n    }\n    \'\'\'\n\n    @lab_api\n    def init_algorithm_params(self):\n        \'\'\'Initialize other algorithm parameters.\'\'\'\n        # set default\n        util.set_attr(self, dict(\n            action_pdtype=\'default\',\n            action_policy=\'default\',\n            explore_var_spec=None,\n        ))\n        util.set_attr(self, self.algorithm_spec, [\n            \'action_pdtype\',\n            \'action_policy\',\n            # explore_var is epsilon, tau or etc. depending on the action policy\n            # these control the trade off between exploration and exploitaton\n            \'explore_var_spec\',\n            \'gamma\',  # the discount factor\n            \'training_frequency\',  # how often to train for batch training (once each training_frequency time steps)\n        ])\n        self.to_train = 0\n        self.action_policy = getattr(policy_util, self.action_policy)\n        self.explore_var_scheduler = policy_util.VarScheduler(self.explore_var_spec)\n        self.body.explore_var = self.explore_var_scheduler.start_val\n\n    @lab_api\n    def init_nets(self, global_nets=None):\n        \'\'\'Initialize the neural network used to learn the Q function from the spec\'\'\'\n        if \'Recurrent\' in self.net_spec[\'type\']:\n            self.net_spec.update(seq_len=self.net_spec[\'seq_len\'])\n        in_dim = self.body.state_dim\n        out_dim = net_util.get_out_dim(self.body)\n        NetClass = getattr(net, self.net_spec[\'type\'])\n        self.net = NetClass(self.net_spec, in_dim, out_dim)\n        self.net_names = [\'net\']\n        # init net optimizer and its lr scheduler\n        self.optim = net_util.get_optim(self.net, self.net.optim_spec)\n        self.lr_scheduler = net_util.get_lr_scheduler(self.optim, self.net.lr_scheduler_spec)\n        net_util.set_global_nets(self, global_nets)\n        self.end_init_nets()\n\n    @lab_api\n    def calc_pdparam(self, x, net=None):\n        \'\'\'\n        To get the pdparam for action policy sampling, do a forward pass of the appropriate net, and pick the correct outputs.\n        The pdparam will be the logits for discrete prob. dist., or the mean and std for continuous prob. dist.\n        \'\'\'\n        net = self.net if net is None else net\n        pdparam = net(x)\n        return pdparam\n\n    @lab_api\n    def act(self, state):\n        \'\'\'Note, SARSA is discrete-only\'\'\'\n        body = self.body\n        action = self.action_policy(state, self, body)\n        return action.cpu().squeeze().numpy()  # squeeze to handle scalar\n\n    @lab_api\n    def sample(self):\n        \'\'\'Samples a batch from memory\'\'\'\n        batch = self.body.memory.sample()\n        # this is safe for next_action at done since the calculated act_next_q_preds will be multiplied by (1 - batch[\'dones\'])\n        batch[\'next_actions\'] = np.zeros_like(batch[\'actions\'])\n        batch[\'next_actions\'][:-1] = batch[\'actions\'][1:]\n        batch = util.to_torch_batch(batch, self.net.device, self.body.memory.is_episodic)\n        return batch\n\n    def calc_q_loss(self, batch):\n        \'\'\'Compute the Q value loss using predicted and target Q values from the appropriate networks\'\'\'\n        states = batch[\'states\']\n        next_states = batch[\'next_states\']\n        if self.body.env.is_venv:\n            states = math_util.venv_unpack(states)\n            next_states = math_util.venv_unpack(next_states)\n        q_preds = self.net(states)\n        with torch.no_grad():\n            next_q_preds = self.net(next_states)\n        if self.body.env.is_venv:\n            q_preds = math_util.venv_pack(q_preds, self.body.env.num_envs)\n            next_q_preds = math_util.venv_pack(next_q_preds, self.body.env.num_envs)\n        act_q_preds = q_preds.gather(-1, batch[\'actions\'].long().unsqueeze(-1)).squeeze(-1)\n        act_next_q_preds = next_q_preds.gather(-1, batch[\'next_actions\'].long().unsqueeze(-1)).squeeze(-1)\n        act_q_targets = batch[\'rewards\'] + self.gamma * (1 - batch[\'dones\']) * act_next_q_preds\n        logger.debug(f\'act_q_preds: {act_q_preds}\\nact_q_targets: {act_q_targets}\')\n        q_loss = self.net.loss_fn(act_q_preds, act_q_targets)\n        return q_loss\n\n    @lab_api\n    def train(self):\n        \'\'\'\n        Completes one training step for the agent if it is time to train.\n        Otherwise this function does nothing.\n        \'\'\'\n        clock = self.body.env.clock\n        if self.to_train == 1:\n            batch = self.sample()\n            clock.set_batch_size(len(batch))\n            loss = self.calc_q_loss(batch)\n            self.net.train_step(loss, self.optim, self.lr_scheduler, clock=clock, global_net=self.global_net)\n            # reset\n            self.to_train = 0\n            logger.debug(f\'Trained {self.name} at epi: {clock.epi}, frame: {clock.frame}, t: {clock.t}, total_reward so far: {self.body.env.total_reward}, loss: {loss:g}\')\n            return loss.item()\n        else:\n            return np.nan\n\n    @lab_api\n    def update(self):\n        \'\'\'Update the agent after training\'\'\'\n        self.body.explore_var = self.explore_var_scheduler.update(self, self.body.env.clock)\n        return self.body.explore_var\n'"
slm_lab/agent/algorithm/sil.py,2,"b'from slm_lab.agent import net, memory\nfrom slm_lab.agent.algorithm import policy_util\nfrom slm_lab.agent.algorithm.actor_critic import ActorCritic\nfrom slm_lab.agent.algorithm.ppo import PPO\nfrom slm_lab.lib import logger, math_util, util\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\nimport pydash as ps\nimport torch\n\nlogger = logger.get_logger(__name__)\n\n\nclass SIL(ActorCritic):\n    \'\'\'\n    Implementation of Self-Imitation Learning (SIL) https://arxiv.org/abs/1806.05635\n    This is actually just A2C with an extra SIL loss function\n\n    e.g. algorithm_spec\n    ""algorithm"": {\n        ""name"": ""SIL"",\n        ""action_pdtype"": ""default"",\n        ""action_policy"": ""default"",\n        ""explore_var_spec"": null,\n        ""gamma"": 0.99,\n        ""lam"": 0.95,\n        ""num_step_returns"": 100,\n        ""entropy_coef_spec"": {\n          ""name"": ""linear_decay"",\n          ""start_val"": 0.01,\n          ""end_val"": 0.001,\n          ""start_step"": 100,\n          ""end_step"": 5000,\n        },\n        ""policy_loss_coef"": 1.0,\n        ""val_loss_coef"": 0.01,\n        ""sil_policy_loss_coef"": 1.0,\n        ""sil_val_loss_coef"": 0.01,\n        ""training_batch_iter"": 8,\n        ""training_frequency"": 1,\n        ""training_iter"": 8,\n    }\n\n    e.g. special memory_spec\n    ""memory"": {\n        ""name"": ""OnPolicyReplay"",\n        ""sil_replay_name"": ""Replay"",\n        ""batch_size"": 32,\n        ""max_size"": 10000,\n        ""use_cer"": true\n    }\n    \'\'\'\n\n    def __init__(self, agent, global_nets=None):\n        super().__init__(agent, global_nets)\n        # create the extra replay memory for SIL\n        MemoryClass = getattr(memory, self.memory_spec[\'sil_replay_name\'])\n        self.body.replay_memory = MemoryClass(self.memory_spec, self.body)\n\n    @lab_api\n    def init_algorithm_params(self):\n        \'\'\'Initialize other algorithm parameters\'\'\'\n        # set default\n        util.set_attr(self, dict(\n            action_pdtype=\'default\',\n            action_policy=\'default\',\n            explore_var_spec=None,\n            entropy_coef_spec=None,\n            policy_loss_coef=1.0,\n            val_loss_coef=1.0,\n        ))\n        util.set_attr(self, self.algorithm_spec, [\n            \'action_pdtype\',\n            \'action_policy\',\n            # theoretically, AC does not have policy update; but in this implementation we have such option\n            \'explore_var_spec\',\n            \'gamma\',  # the discount factor\n            \'lam\',\n            \'num_step_returns\',\n            \'entropy_coef_spec\',\n            \'policy_loss_coef\',\n            \'val_loss_coef\',\n            \'sil_policy_loss_coef\',\n            \'sil_val_loss_coef\',\n            \'training_frequency\',\n            \'training_batch_iter\',\n            \'training_iter\',\n        ])\n        super().init_algorithm_params()\n\n    def sample(self):\n        \'\'\'Modify the onpolicy sample to also append to replay\'\'\'\n        batch = self.body.memory.sample()\n        if self.body.memory.is_episodic:\n            batch = {k: np.concatenate(v) for k, v in batch.items()}  # concat episodic memory\n        for idx in range(len(batch[\'dones\'])):\n            tuples = [batch[k][idx] for k in self.body.replay_memory.data_keys]\n            self.body.replay_memory.add_experience(*tuples)\n        batch = util.to_torch_batch(batch, self.net.device, self.body.replay_memory.is_episodic)\n        return batch\n\n    def replay_sample(self):\n        \'\'\'Samples a batch from memory\'\'\'\n        batch = self.body.replay_memory.sample()\n        batch = util.to_torch_batch(batch, self.net.device, self.body.replay_memory.is_episodic)\n        return batch\n\n    def calc_sil_policy_val_loss(self, batch, pdparams):\n        \'\'\'\n        Calculate the SIL policy losses for actor and critic\n        sil_policy_loss = -log_prob * max(R - v_pred, 0)\n        sil_val_loss = (max(R - v_pred, 0)^2) / 2\n        This is called on a randomly-sample batch from experience replay\n        \'\'\'\n        v_preds = self.calc_v(batch[\'states\'], use_cache=False)\n        rets = math_util.calc_returns(batch[\'rewards\'], batch[\'dones\'], self.gamma)\n        clipped_advs = torch.clamp(rets - v_preds, min=0.0)\n\n        action_pd = policy_util.init_action_pd(self.body.ActionPD, pdparams)\n        actions = batch[\'actions\']\n        if self.body.env.is_venv:\n            actions = math_util.venv_unpack(actions)\n        log_probs = action_pd.log_prob(actions)\n\n        sil_policy_loss = - self.sil_policy_loss_coef * (log_probs * clipped_advs).mean()\n        sil_val_loss = self.sil_val_loss_coef * clipped_advs.pow(2).mean() / 2\n        logger.debug(f\'SIL actor policy loss: {sil_policy_loss:g}\')\n        logger.debug(f\'SIL critic value loss: {sil_val_loss:g}\')\n        return sil_policy_loss, sil_val_loss\n\n    def train(self):\n        clock = self.body.env.clock\n        if self.to_train == 1:\n            # onpolicy update\n            super_loss = super().train()\n            # offpolicy sil update with random minibatch\n            total_sil_loss = torch.tensor(0.0)\n            for _ in range(self.training_iter):\n                batch = self.replay_sample()\n                for _ in range(self.training_batch_iter):\n                    pdparams, _v_preds = self.calc_pdparam_v(batch)\n                    sil_policy_loss, sil_val_loss = self.calc_sil_policy_val_loss(batch, pdparams)\n                    sil_loss = sil_policy_loss + sil_val_loss\n                    self.net.train_step(sil_loss, self.optim, self.lr_scheduler, clock=clock, global_net=self.global_net)\n                    total_sil_loss += sil_loss\n            sil_loss = total_sil_loss / self.training_iter\n            loss = super_loss + sil_loss\n            logger.debug(f\'Trained {self.name} at epi: {clock.epi}, frame: {clock.frame}, t: {clock.t}, total_reward so far: {self.body.env.total_reward}, loss: {loss:g}\')\n            return loss.item()\n        else:\n            return np.nan\n\n\nclass PPOSIL(SIL, PPO):\n    \'\'\'\n    SIL extended from PPO. This will call the SIL methods and use PPO as super().\n\n    e.g. algorithm_spec\n    ""algorithm"": {\n        ""name"": ""PPOSIL"",\n        ""action_pdtype"": ""default"",\n        ""action_policy"": ""default"",\n        ""explore_var_spec"": null,\n        ""gamma"": 0.99,\n        ""lam"": 0.95,\n        ""clip_eps_spec"": {\n          ""name"": ""linear_decay"",\n          ""start_val"": 0.01,\n          ""end_val"": 0.001,\n          ""start_step"": 100,\n          ""end_step"": 5000,\n        },\n        ""entropy_coef_spec"": {\n          ""name"": ""linear_decay"",\n          ""start_val"": 0.01,\n          ""end_val"": 0.001,\n          ""start_step"": 100,\n          ""end_step"": 5000,\n        },\n        ""sil_policy_loss_coef"": 1.0,\n        ""sil_val_loss_coef"": 0.01,\n        ""time_horizon"": 32,\n        ""training_batch_iter"": 8,\n        ""training_iter"": 8,\n        ""training_epoch"": 8,\n    }\n\n    e.g. special memory_spec\n    ""memory"": {\n        ""name"": ""OnPolicyReplay"",\n        ""sil_replay_name"": ""Replay"",\n        ""batch_size"": 32,\n        ""max_size"": 10000,\n        ""use_cer"": true\n    }\n    \'\'\'\n    pass\n'"
slm_lab/agent/memory/__init__.py,0,b'# The memory module\n# Implements various methods for memory storage\nfrom .replay import *\nfrom .onpolicy import *\nfrom .prioritized import *\n'
slm_lab/agent/memory/base.py,0,"b""from abc import ABC, abstractmethod\nfrom collections import deque\nfrom slm_lab.lib import logger, util\nimport numpy as np\nimport pydash as ps\n\nlogger = logger.get_logger(__name__)\n\n\nclass Memory(ABC):\n    '''Abstract Memory class to define the API methods'''\n\n    def __init__(self, memory_spec, body):\n        '''\n        @param {*} body is the unit that stores its experience in this memory. Each body has a distinct memory.\n        '''\n        self.memory_spec = memory_spec\n        self.body = body\n        # declare what data keys to store\n        self.data_keys = ['states', 'actions', 'rewards', 'next_states', 'dones', 'priorities']\n\n    @abstractmethod\n    def reset(self):\n        '''Method to fully reset the memory storage and related variables'''\n        raise NotImplementedError\n\n    @abstractmethod\n    def update(self, state, action, reward, next_state, done):\n        '''Implement memory update given the full info from the latest timestep. NOTE: guard for np.nan reward and done when individual env resets.'''\n        raise NotImplementedError\n\n    @abstractmethod\n    def sample(self):\n        '''Implement memory sampling mechanism'''\n        raise NotImplementedError\n"""
slm_lab/agent/memory/onpolicy.py,0,"b'from collections import deque\nfrom copy import deepcopy\nfrom slm_lab.agent.memory.base import Memory\nfrom slm_lab.lib import logger, util\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\nimport pydash as ps\n\nlogger = logger.get_logger(__name__)\n\n\nclass OnPolicyReplay(Memory):\n    \'\'\'\n    Stores agent experiences and returns them in a batch for agent training.\n\n    An experience consists of\n        - state: representation of a state\n        - action: action taken\n        - reward: scalar value\n        - next state: representation of next state (should be same as state)\n        - done: 0 / 1 representing if the current state is the last in an episode\n\n    The memory does not have a fixed size. Instead the memory stores data from N episodes, where N is determined by the user. After N episodes, all of the examples are returned to the agent to learn from.\n\n    When the examples are returned to the agent, the memory is cleared to prevent the agent from learning from off policy experiences. This memory is intended for on policy algorithms.\n\n    Differences vs. Replay memory:\n        - Experiences are nested into episodes. In Replay experiences are flat, and episode is not tracked\n        - The entire memory constitues a batch. In Replay batches are sampled from memory.\n        - The memory is cleared automatically when a batch is given to the agent.\n\n    e.g. memory_spec\n    ""memory"": {\n        ""name"": ""OnPolicyReplay""\n    }\n    \'\'\'\n\n    def __init__(self, memory_spec, body):\n        super().__init__(memory_spec, body)\n        # NOTE for OnPolicy replay, frequency = episode; for other classes below frequency = frames\n        # Don\'t want total experiences reset when memory is\n        self.is_episodic = True\n        self.size = 0  # total experiences stored\n        self.seen_size = 0  # total experiences seen cumulatively\n        # declare what data keys to store\n        self.data_keys = [\'states\', \'actions\', \'rewards\', \'next_states\', \'dones\']\n        self.reset()\n\n    @lab_api\n    def reset(self):\n        \'\'\'Resets the memory. Also used to initialize memory vars\'\'\'\n        for k in self.data_keys:\n            setattr(self, k, [])\n        self.cur_epi_data = {k: [] for k in self.data_keys}\n        self.most_recent = (None,) * len(self.data_keys)\n        self.size = 0\n\n    @lab_api\n    def update(self, state, action, reward, next_state, done):\n        \'\'\'Interface method to update memory\'\'\'\n        self.add_experience(state, action, reward, next_state, done)\n\n    def add_experience(self, state, action, reward, next_state, done):\n        \'\'\'Interface helper method for update() to add experience to memory\'\'\'\n        self.most_recent = (state, action, reward, next_state, done)\n        for idx, k in enumerate(self.data_keys):\n            self.cur_epi_data[k].append(self.most_recent[idx])\n        # If episode ended, add to memory and clear cur_epi_data\n        if util.epi_done(done):\n            for k in self.data_keys:\n                getattr(self, k).append(self.cur_epi_data[k])\n            self.cur_epi_data = {k: [] for k in self.data_keys}\n            # If agent has collected the desired number of episodes, it is ready to train\n            # length is num of epis due to nested structure\n            if len(self.states) == self.body.agent.algorithm.training_frequency:\n                self.body.agent.algorithm.to_train = 1\n        # Track memory size and num experiences\n        self.size += 1\n        self.seen_size += 1\n\n    def sample(self):\n        \'\'\'\n        Returns all the examples from memory in a single batch. Batch is stored as a dict.\n        Keys are the names of the different elements of an experience. Values are nested lists of the corresponding sampled elements. Elements are nested into episodes\n        e.g.\n        batch = {\n            \'states\'     : [[s_epi1], [s_epi2], ...],\n            \'actions\'    : [[a_epi1], [a_epi2], ...],\n            \'rewards\'    : [[r_epi1], [r_epi2], ...],\n            \'next_states\': [[ns_epi1], [ns_epi2], ...],\n            \'dones\'      : [[d_epi1], [d_epi2], ...]}\n        \'\'\'\n        batch = {k: getattr(self, k) for k in self.data_keys}\n        self.reset()\n        return batch\n\n\nclass OnPolicyBatchReplay(OnPolicyReplay):\n    \'\'\'\n    Same as OnPolicyReplay Memory with the following difference.\n\n    The memory does not have a fixed size. Instead the memory stores data from N experiences, where N is determined by the user. After N experiences or if an episode has ended, all of the examples are returned to the agent to learn from.\n\n    In contrast, OnPolicyReplay stores entire episodes and stores them in a nested structure. OnPolicyBatchReplay stores experiences in a flat structure.\n\n    e.g. memory_spec\n    ""memory"": {\n        ""name"": ""OnPolicyBatchReplay""\n    }\n    * batch_size is training_frequency provided by algorithm_spec\n    \'\'\'\n\n    def __init__(self, memory_spec, body):\n        super().__init__(memory_spec, body)\n        self.is_episodic = False\n\n    def add_experience(self, state, action, reward, next_state, done):\n        \'\'\'Interface helper method for update() to add experience to memory\'\'\'\n        self.most_recent = [state, action, reward, next_state, done]\n        for idx, k in enumerate(self.data_keys):\n            getattr(self, k).append(self.most_recent[idx])\n        # Track memory size and num experiences\n        self.size += 1\n        self.seen_size += 1\n        # Decide if agent is to train\n        if len(self.states) == self.body.agent.algorithm.training_frequency:\n            self.body.agent.algorithm.to_train = 1\n\n    def sample(self):\n        \'\'\'\n        Returns all the examples from memory in a single batch. Batch is stored as a dict.\n        Keys are the names of the different elements of an experience. Values are a list of the corresponding sampled elements\n        e.g.\n        batch = {\n            \'states\'     : states,\n            \'actions\'    : actions,\n            \'rewards\'    : rewards,\n            \'next_states\': next_states,\n            \'dones\'      : dones}\n        \'\'\'\n        return super().sample()\n\n\nclass OnPolicyCrossEntropy(OnPolicyReplay):\n    \'\'\'\n    Same as OnPolicyReplay with the addition of the cross entropy method.\n\n    We collect a bach of episode with respect to the training_frequency argument and then we keep only the top cross_entropy\n    percent of episodes with respect to the accumulated reward.\n\n    e.g. memory_spec\n    ""memory"": {\n        ""name"": ""OnPolicyCrossEntropy"",\n        ""cross_entropy"" : 1.0,\n    }\n\n    See: Kroese, Dirk P., et al. ""Cross-entropy method."" Encyclopedia of Operations Research and Management Science (2013): 326-333.\n    http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.399.7005&rep=rep1&type=pdf (section 2)\n    \'\'\'\n\n    def __init__(self, memory_spec, body):\n        # set default\n        util.set_attr(self, dict(\n            cross_entropy=1.0,\n        ))\n        util.set_attr(self, memory_spec, [\n            \'cross_entropy\',\n        ])\n        super().__init__(memory_spec, body)\n\n    def filter_episodes(self, batch, cross_entropy):\n        \'\'\'Filter the episodes for the cross_entropy method\'\'\'\n        accumulated_reward = [sum(rewards) for rewards in batch[\'rewards\']]\n        percentile = cross_entropy * 100\n        reward_bound = np.percentile(accumulated_reward, percentile)\n        # we save the batch with reward above the bound\n        result = {k: [] for k in self.data_keys}\n        episode_kept = 0\n        for i in range(len(accumulated_reward)):\n            if accumulated_reward[i] >= reward_bound:\n                for k in self.data_keys:\n                    result[k].append(batch[k][i])\n                episode_kept += 1\n        return result\n\n    def sample(self):\n        \'\'\'\n        Refer to the parent methods for documentation\n        If the cross entropy parameter is activated, we filter the collected episodes\n        \'\'\'\n        batch = {k: getattr(self, k) for k in self.data_keys}\n        self.reset()\n        # we remove the episodes below the cross_entropy percentage\n        if self.cross_entropy < 1.0:\n            batch = self.filter_episodes(batch, self.cross_entropy)\n        return batch\n'"
slm_lab/agent/memory/prioritized.py,0,"b'from slm_lab.agent.memory.replay import Replay\nfrom slm_lab.lib import util\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\nimport random\n\n\nclass SumTree:\n    \'\'\'\n    Helper class for PrioritizedReplay\n\n    This implementation is, with minor adaptations, Jarom\xc3\xadr Janisch\'s. The license is reproduced below.\n    For more information see his excellent blog series ""Let\'s make a DQN"" https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/\n\n    MIT License\n\n    Copyright (c) 2018 Jarom\xc3\xadr Janisch\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the ""Software""), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n    \'\'\'\n    write = 0\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.tree = np.zeros(2 * capacity - 1)  # Stores the priorities and sums of priorities\n        self.indices = np.zeros(capacity)  # Stores the indices of the experiences\n\n    def _propagate(self, idx, change):\n        parent = (idx - 1) // 2\n\n        self.tree[parent] += change\n\n        if parent != 0:\n            self._propagate(parent, change)\n\n    def _retrieve(self, idx, s):\n        left = 2 * idx + 1\n        right = left + 1\n\n        if left >= len(self.tree):\n            return idx\n\n        if s <= self.tree[left]:\n            return self._retrieve(left, s)\n        else:\n            return self._retrieve(right, s - self.tree[left])\n\n    def total(self):\n        return self.tree[0]\n\n    def add(self, p, index):\n        idx = self.write + self.capacity - 1\n\n        self.indices[self.write] = index\n        self.update(idx, p)\n\n        self.write += 1\n        if self.write >= self.capacity:\n            self.write = 0\n\n    def update(self, idx, p):\n        change = p - self.tree[idx]\n\n        self.tree[idx] = p\n        self._propagate(idx, change)\n\n    def get(self, s):\n        assert s <= self.total()\n        idx = self._retrieve(0, s)\n        indexIdx = idx - self.capacity + 1\n\n        return (idx, self.tree[idx], self.indices[indexIdx])\n\n    def print_tree(self):\n        for i in range(len(self.indices)):\n            j = i + self.capacity - 1\n            print(f\'Idx: {i}, Data idx: {self.indices[i]}, Prio: {self.tree[j]}\')\n\n\nclass PrioritizedReplay(Replay):\n    \'\'\'\n    Prioritized Experience Replay\n\n    Implementation follows the approach in the paper ""Prioritized Experience Replay"", Schaul et al 2015"" https://arxiv.org/pdf/1511.05952.pdf and is Jarom\xc3\xadr Janisch\'s with minor adaptations.\n    See memory_util.py for the license and link to Jarom\xc3\xadr\'s excellent blog\n\n    Stores agent experiences and samples from them for agent training according to each experience\'s priority\n\n    The memory has the same behaviour and storage structure as Replay memory with the addition of a SumTree to store and sample the priorities.\n\n    e.g. memory_spec\n    ""memory"": {\n        ""name"": ""PrioritizedReplay"",\n        ""alpha"": 1,\n        ""epsilon"": 0,\n        ""batch_size"": 32,\n        ""max_size"": 10000,\n        ""use_cer"": true\n    }\n    \'\'\'\n\n    def __init__(self, memory_spec, body):\n        util.set_attr(self, memory_spec, [\n            \'alpha\',\n            \'epsilon\',\n            \'batch_size\',\n            \'max_size\',\n            \'use_cer\',\n        ])\n        super().__init__(memory_spec, body)\n\n        self.epsilon = np.full((1,), self.epsilon)\n        self.alpha = np.full((1,), self.alpha)\n        # adds a \'priorities\' scalar to the data_keys and call reset again\n        self.data_keys = [\'states\', \'actions\', \'rewards\', \'next_states\', \'dones\', \'priorities\']\n        self.reset()\n\n    def reset(self):\n        super().reset()\n        self.tree = SumTree(self.max_size)\n\n    def add_experience(self, state, action, reward, next_state, done, error=100000):\n        \'\'\'\n        Implementation for update() to add experience to memory, expanding the memory size if necessary.\n        All experiences are added with a high priority to increase the likelihood that they are sampled at least once.\n        \'\'\'\n        super().add_experience(state, action, reward, next_state, done)\n        priority = self.get_priority(error)\n        self.priorities[self.head] = priority\n        self.tree.add(priority, self.head)\n\n    def get_priority(self, error):\n        \'\'\'Takes in the error of one or more examples and returns the proportional priority\'\'\'\n        return np.power(error + self.epsilon, self.alpha).squeeze()\n\n    def sample_idxs(self, batch_size):\n        \'\'\'Samples batch_size indices from memory in proportional to their priority.\'\'\'\n        batch_idxs = np.zeros(batch_size)\n        tree_idxs = np.zeros(batch_size, dtype=np.int)\n\n        for i in range(batch_size):\n            s = random.uniform(0, self.tree.total())\n            (tree_idx, p, idx) = self.tree.get(s)\n            batch_idxs[i] = idx\n            tree_idxs[i] = tree_idx\n\n        batch_idxs = np.asarray(batch_idxs).astype(int)\n        self.tree_idxs = tree_idxs\n        if self.use_cer:  # add the latest sample\n            batch_idxs[-1] = self.head\n        return batch_idxs\n\n    def update_priorities(self, errors):\n        \'\'\'\n        Updates the priorities from the most recent batch\n        Assumes the relevant batch indices are stored in self.batch_idxs\n        \'\'\'\n        priorities = self.get_priority(errors)\n        assert len(priorities) == self.batch_idxs.size\n        for idx, p in zip(self.batch_idxs, priorities):\n            self.priorities[idx] = p\n        for p, i in zip(priorities, self.tree_idxs):\n            self.tree.update(i, p)\n'"
slm_lab/agent/memory/replay.py,0,"b'from collections import deque\nfrom copy import deepcopy\nfrom slm_lab.agent.memory.base import Memory\nfrom slm_lab.lib import logger, math_util, util\nfrom slm_lab.lib.decorator import lab_api\nimport numpy as np\nimport pydash as ps\n\nlogger = logger.get_logger(__name__)\n\n\ndef sample_next_states(head, max_size, ns_idx_offset, batch_idxs, states, ns_buffer):\n    \'\'\'Method to sample next_states from states, with proper guard for next_state idx being out of bound\'\'\'\n    # idxs for next state is state idxs with offset, modded\n    ns_batch_idxs = (batch_idxs + ns_idx_offset) % max_size\n    # if head < ns_idx <= head + ns_idx_offset, ns is stored in ns_buffer\n    ns_batch_idxs = ns_batch_idxs % max_size\n    buffer_ns_locs = np.argwhere(\n        (head < ns_batch_idxs) & (ns_batch_idxs <= head + ns_idx_offset)).flatten()\n    # find if there is any idxs to get from buffer\n    to_replace = buffer_ns_locs.size != 0\n    if to_replace:\n        # extract the buffer_idxs first for replacement later\n        # given head < ns_idx <= head + offset, and valid buffer idx is [0, offset)\n        # get 0 < ns_idx - head <= offset, or equiv.\n        # get -1 < ns_idx - head - 1 <= offset - 1, i.e.\n        # get 0 <= ns_idx - head - 1 < offset, hence:\n        buffer_idxs = ns_batch_idxs[buffer_ns_locs] - head - 1\n        # set them to 0 first to allow sampling, then replace later with buffer\n        ns_batch_idxs[buffer_ns_locs] = 0\n    # guard all against overrun idxs from offset\n    ns_batch_idxs = ns_batch_idxs % max_size\n    next_states = util.batch_get(states, ns_batch_idxs)\n    if to_replace:\n        # now replace using buffer_idxs and ns_buffer\n        buffer_ns = util.batch_get(ns_buffer, buffer_idxs)\n        next_states[buffer_ns_locs] = buffer_ns\n    return next_states\n\n\nclass Replay(Memory):\n    \'\'\'\n    Stores agent experiences and samples from them for agent training\n\n    An experience consists of\n        - state: representation of a state\n        - action: action taken\n        - reward: scalar value\n        - next state: representation of next state (should be same as state)\n        - done: 0 / 1 representing if the current state is the last in an episode\n\n    The memory has a size of N. When capacity is reached, the oldest experience\n    is deleted to make space for the lastest experience.\n        - This is implemented as a circular buffer so that inserting experiences are O(1)\n        - Each element of an experience is stored as a separate array of size N * element dim\n\n    When a batch of experiences is requested, K experiences are sampled according to a random uniform distribution.\n\n    If \'use_cer\', sampling will add the latest experience.\n\n    e.g. memory_spec\n    ""memory"": {\n        ""name"": ""Replay"",\n        ""batch_size"": 32,\n        ""max_size"": 10000,\n        ""use_cer"": true\n    }\n    \'\'\'\n\n    def __init__(self, memory_spec, body):\n        super().__init__(memory_spec, body)\n        util.set_attr(self, self.memory_spec, [\n            \'batch_size\',\n            \'max_size\',\n            \'use_cer\',\n        ])\n        self.is_episodic = False\n        self.batch_idxs = None\n        self.size = 0  # total experiences stored\n        self.seen_size = 0  # total experiences seen cumulatively\n        self.head = -1  # index of most recent experience\n        # generic next_state buffer to store last next_states (allow for multiple for venv)\n        self.ns_idx_offset = self.body.env.num_envs if body.env.is_venv else 1\n        self.ns_buffer = deque(maxlen=self.ns_idx_offset)\n        # declare what data keys to store\n        self.data_keys = [\'states\', \'actions\', \'rewards\', \'next_states\', \'dones\']\n        self.reset()\n\n    def reset(self):\n        \'\'\'Initializes the memory arrays, size and head pointer\'\'\'\n        # set self.states, self.actions, ...\n        for k in self.data_keys:\n            if k != \'next_states\':  # reuse self.states\n                # list add/sample is over 10x faster than np, also simpler to handle\n                setattr(self, k, [None] * self.max_size)\n        self.size = 0\n        self.head = -1\n        self.ns_buffer.clear()\n\n    @lab_api\n    def update(self, state, action, reward, next_state, done):\n        \'\'\'Interface method to update memory\'\'\'\n        if self.body.env.is_venv:\n            for sarsd in zip(state, action, reward, next_state, done):\n                self.add_experience(*sarsd)\n        else:\n            self.add_experience(state, action, reward, next_state, done)\n\n    def add_experience(self, state, action, reward, next_state, done):\n        \'\'\'Implementation for update() to add experience to memory, expanding the memory size if necessary\'\'\'\n        # Move head pointer. Wrap around if necessary\n        self.head = (self.head + 1) % self.max_size\n        self.states[self.head] = state.astype(np.float16)\n        self.actions[self.head] = action\n        self.rewards[self.head] = reward\n        self.ns_buffer.append(next_state.astype(np.float16))\n        self.dones[self.head] = done\n        # Actually occupied size of memory\n        if self.size < self.max_size:\n            self.size += 1\n        self.seen_size += 1\n        # set to_train using memory counters head, seen_size instead of tick since clock will step by num_envs when on venv; to_train will be set to 0 after training step\n        algorithm = self.body.agent.algorithm\n        algorithm.to_train = algorithm.to_train or (self.seen_size > algorithm.training_start_step and self.head % algorithm.training_frequency == 0)\n\n    @lab_api\n    def sample(self):\n        \'\'\'\n        Returns a batch of batch_size samples. Batch is stored as a dict.\n        Keys are the names of the different elements of an experience. Values are an array of the corresponding sampled elements\n        e.g.\n        batch = {\n            \'states\'     : states,\n            \'actions\'    : actions,\n            \'rewards\'    : rewards,\n            \'next_states\': next_states,\n            \'dones\'      : dones}\n        \'\'\'\n        self.batch_idxs = self.sample_idxs(self.batch_size)\n        batch = {}\n        for k in self.data_keys:\n            if k == \'next_states\':\n                batch[k] = sample_next_states(self.head, self.max_size, self.ns_idx_offset, self.batch_idxs, self.states, self.ns_buffer)\n            else:\n                batch[k] = util.batch_get(getattr(self, k), self.batch_idxs)\n        return batch\n\n    def sample_idxs(self, batch_size):\n        \'\'\'Batch indices a sampled random uniformly\'\'\'\n        batch_idxs = np.random.randint(self.size, size=batch_size)\n        if self.use_cer:  # add the latest sample\n            batch_idxs[-1] = self.head\n        return batch_idxs\n'"
slm_lab/agent/net/__init__.py,0,b'# The nets module\n# Implements differents types of neural network\nfrom slm_lab.agent.net.conv import *\nfrom slm_lab.agent.net.mlp import *\nfrom slm_lab.agent.net.recurrent import *\nfrom slm_lab.agent.net.q_net import *\n'
slm_lab/agent/net/base.py,2,"b'from abc import ABC, abstractmethod\nfrom slm_lab.agent.net import net_util\nimport pydash as ps\nimport torch\nimport torch.nn as nn\n\n\nclass Net(ABC):\n    \'\'\'Abstract Net class to define the API methods\'\'\'\n\n    def __init__(self, net_spec, in_dim, out_dim):\n        \'\'\'\n        @param {dict} net_spec is the spec for the net\n        @param {int|list} in_dim is the input dimension(s) for the network. Usually use in_dim=body.state_dim\n        @param {int|list} out_dim is the output dimension(s) for the network. Usually use out_dim=body.action_dim\n        \'\'\'\n        self.net_spec = net_spec\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.grad_norms = None  # for debugging\n        if self.net_spec.get(\'gpu\'):\n            if torch.cuda.device_count():\n                self.device = f\'cuda:{net_spec.get(""cuda_id"", 0)}\'\n            else:\n                self.device = \'cpu\'\n        else:\n            self.device = \'cpu\'\n\n    @abstractmethod\n    def forward(self):\n        \'\'\'The forward step for a specific network architecture\'\'\'\n        raise NotImplementedError\n\n    @net_util.dev_check_train_step\n    def train_step(self, loss, optim, lr_scheduler=None, clock=None, global_net=None):\n        if lr_scheduler is not None:\n            lr_scheduler.step(epoch=ps.get(clock, \'frame\'))\n        optim.zero_grad()\n        loss.backward()\n        if self.clip_grad_val is not None:\n            nn.utils.clip_grad_norm_(self.parameters(), self.clip_grad_val)\n        if global_net is not None:\n            net_util.push_global_grads(self, global_net)\n        optim.step()\n        if global_net is not None:\n            net_util.copy(global_net, self)\n        if clock is not None:\n            clock.tick(\'opt_step\')\n        return loss\n\n    def store_grad_norms(self):\n        \'\'\'Stores the gradient norms for debugging.\'\'\'\n        norms = [param.grad.norm().item() for param in self.parameters()]\n        self.grad_norms = norms\n'"
slm_lab/agent/net/conv.py,4,"b'from slm_lab.agent.net import net_util\nfrom slm_lab.agent.net.base import Net\nfrom slm_lab.lib import math_util, util\nimport pydash as ps\nimport torch\nimport torch.nn as nn\n\n\nclass ConvNet(Net, nn.Module):\n    \'\'\'\n    Class for generating arbitrary sized convolutional neural network,\n    with optional batch normalization\n\n    Assumes that a single input example is organized into a 3D tensor.\n    The entire model consists of three parts:\n        1. self.conv_model\n        2. self.fc_model\n        3. self.model_tails\n\n    e.g. net_spec\n    ""net"": {\n        ""type"": ""ConvNet"",\n        ""shared"": true,\n        ""conv_hid_layers"": [\n            [32, 8, 4, 0, 1],\n            [64, 4, 2, 0, 1],\n            [64, 3, 1, 0, 1]\n        ],\n        ""fc_hid_layers"": [512],\n        ""hid_layers_activation"": ""relu"",\n        ""out_layer_activation"": ""tanh"",\n        ""init_fn"": null,\n        ""normalize"": false,\n        ""batch_norm"": false,\n        ""clip_grad_val"": 1.0,\n        ""loss_spec"": {\n          ""name"": ""SmoothL1Loss""\n        },\n        ""optim_spec"": {\n          ""name"": ""Adam"",\n          ""lr"": 0.02\n        },\n        ""lr_scheduler_spec"": {\n            ""name"": ""StepLR"",\n            ""step_size"": 30,\n            ""gamma"": 0.1\n        },\n        ""update_type"": ""replace"",\n        ""update_frequency"": 10000,\n        ""polyak_coef"": 0.9,\n        ""gpu"": true\n    }\n    \'\'\'\n\n    def __init__(self, net_spec, in_dim, out_dim):\n        \'\'\'\n        net_spec:\n        conv_hid_layers: list containing dimensions of the convolutional hidden layers, each is a list representing hid_layer = out_d, kernel, stride, padding, dilation.\n            Asssumed to all come before the flat layers.\n            Note: a convolutional layer should specify the in_channel, out_channels, kernel_size, stride (of kernel steps), padding, and dilation (spacing between kernel points) E.g. [3, 16, (5, 5), 1, 0, (2, 2)]\n            For more details, see http://pytorch.org/docs/master/nn.html#conv2d and https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n        fc_hid_layers: list of fc layers following the convolutional layers\n        hid_layers_activation: activation function for the hidden layers\n        out_layer_activation: activation function for the output layer, same shape as out_dim\n        init_fn: weight initialization function\n        normalize: whether to divide by 255.0 to normalize image input\n        batch_norm: whether to add batch normalization after each convolutional layer, excluding the input layer.\n        clip_grad_val: clip gradient norm if value is not None\n        loss_spec: measure of error between model predictions and correct outputs\n        optim_spec: parameters for initializing the optimizer\n        lr_scheduler_spec: Pytorch optim.lr_scheduler\n        update_type: method to update network weights: \'replace\' or \'polyak\'\n        update_frequency: how many total timesteps per update\n        polyak_coef: ratio of polyak weight update\n        gpu: whether to train using a GPU. Note this will only work if a GPU is available, othewise setting gpu=True does nothing\n        \'\'\'\n        assert len(in_dim) == 3  # image shape (c,w,h)\n        nn.Module.__init__(self)\n        super().__init__(net_spec, in_dim, out_dim)\n        # set default\n        util.set_attr(self, dict(\n            out_layer_activation=None,\n            init_fn=None,\n            normalize=False,\n            batch_norm=True,\n            clip_grad_val=None,\n            loss_spec={\'name\': \'MSELoss\'},\n            optim_spec={\'name\': \'Adam\'},\n            lr_scheduler_spec=None,\n            update_type=\'replace\',\n            update_frequency=1,\n            polyak_coef=0.0,\n            gpu=False,\n        ))\n        util.set_attr(self, self.net_spec, [\n            \'conv_hid_layers\',\n            \'fc_hid_layers\',\n            \'hid_layers_activation\',\n            \'out_layer_activation\',\n            \'init_fn\',\n            \'normalize\',\n            \'batch_norm\',\n            \'clip_grad_val\',\n            \'loss_spec\',\n            \'optim_spec\',\n            \'lr_scheduler_spec\',\n            \'update_type\',\n            \'update_frequency\',\n            \'polyak_coef\',\n            \'gpu\',\n        ])\n\n        # conv body\n        self.conv_model = self.build_conv_layers(self.conv_hid_layers)\n        self.conv_out_dim = self.get_conv_output_size()\n\n        # fc body\n        if ps.is_empty(self.fc_hid_layers):\n            tail_in_dim = self.conv_out_dim\n        else:\n            # fc body from flattened conv\n            self.fc_model = net_util.build_fc_model([self.conv_out_dim] + self.fc_hid_layers, self.hid_layers_activation)\n            tail_in_dim = self.fc_hid_layers[-1]\n\n        # tails. avoid list for single-tail for compute speed\n        if ps.is_integer(self.out_dim):\n            self.model_tail = net_util.build_fc_model([tail_in_dim, self.out_dim], self.out_layer_activation)\n        else:\n            if not ps.is_list(self.out_layer_activation):\n                self.out_layer_activation = [self.out_layer_activation] * len(out_dim)\n            assert len(self.out_layer_activation) == len(self.out_dim)\n            tails = []\n            for out_d, out_activ in zip(self.out_dim, self.out_layer_activation):\n                tail = net_util.build_fc_model([tail_in_dim, out_d], out_activ)\n                tails.append(tail)\n            self.model_tails = nn.ModuleList(tails)\n\n        net_util.init_layers(self, self.init_fn)\n        self.loss_fn = net_util.get_loss_fn(self, self.loss_spec)\n        self.to(self.device)\n        self.train()\n\n    def get_conv_output_size(self):\n        \'\'\'Helper function to calculate the size of the flattened features after the final convolutional layer\'\'\'\n        with torch.no_grad():\n            x = torch.ones(1, *self.in_dim)\n            x = self.conv_model(x)\n            return x.numel()\n\n    def build_conv_layers(self, conv_hid_layers):\n        \'\'\'\n        Builds all of the convolutional layers in the network and store in a Sequential model\n        \'\'\'\n        conv_layers = []\n        in_d = self.in_dim[0]  # input channel\n        for i, hid_layer in enumerate(conv_hid_layers):\n            hid_layer = [tuple(e) if ps.is_list(e) else e for e in hid_layer]  # guard list-to-tuple\n            # hid_layer = out_d, kernel, stride, padding, dilation\n            conv_layers.append(nn.Conv2d(in_d, *hid_layer))\n            if self.hid_layers_activation is not None:\n                conv_layers.append(net_util.get_activation_fn(self.hid_layers_activation))\n            # Don\'t include batch norm in the first layer\n            if self.batch_norm and i != 0:\n                conv_layers.append(nn.BatchNorm2d(hid_layer[0]))\n            in_d = hid_layer[0]  # update to out_d\n        conv_model = nn.Sequential(*conv_layers)\n        return conv_model\n\n    def forward(self, x):\n        \'\'\'\n        The feedforward step\n        Note that PyTorch takes (c,h,w) but gym provides (h,w,c), so preprocessing must be done before passing to network\n        \'\'\'\n        if self.normalize:\n            x = x / 255.0\n        x = self.conv_model(x)\n        x = x.view(x.size(0), -1)  # to (batch_size, -1)\n        if hasattr(self, \'fc_model\'):\n            x = self.fc_model(x)\n        # return tensor if single tail, else list of tail tensors\n        if hasattr(self, \'model_tails\'):\n            outs = []\n            for model_tail in self.model_tails:\n                outs.append(model_tail(x))\n            return outs\n        else:\n            return self.model_tail(x)\n\n\nclass DuelingConvNet(ConvNet):\n    \'\'\'\n    Class for generating arbitrary sized convolutional neural network,\n    with optional batch normalization, and with dueling heads. Intended for Q-Learning algorithms only.\n    Implementation based on ""Dueling Network Architectures for Deep Reinforcement Learning"" http://proceedings.mlr.press/v48/wangf16.pdf\n\n    Assumes that a single input example is organized into a 3D tensor.\n    The entire model consists of three parts:\n        1. self.conv_model\n        2. self.fc_model\n        3. self.model_tails\n\n    e.g. net_spec\n    ""net"": {\n        ""type"": ""DuelingConvNet"",\n        ""shared"": true,\n        ""conv_hid_layers"": [\n            [32, 8, 4, 0, 1],\n            [64, 4, 2, 0, 1],\n            [64, 3, 1, 0, 1]\n        ],\n        ""fc_hid_layers"": [512],\n        ""hid_layers_activation"": ""relu"",\n        ""init_fn"": ""xavier_uniform_"",\n        ""normalize"": false,\n        ""batch_norm"": false,\n        ""clip_grad_val"": 1.0,\n        ""loss_spec"": {\n          ""name"": ""SmoothL1Loss""\n        },\n        ""optim_spec"": {\n          ""name"": ""Adam"",\n          ""lr"": 0.02\n        },\n        ""lr_scheduler_spec"": {\n            ""name"": ""StepLR"",\n            ""step_size"": 30,\n            ""gamma"": 0.1\n        },\n        ""update_type"": ""replace"",\n        ""update_frequency"": 10000,\n        ""polyak_coef"": 0.9,\n        ""gpu"": true\n    }\n    \'\'\'\n\n    def __init__(self, net_spec, in_dim, out_dim):\n        assert len(in_dim) == 3  # image shape (c,w,h)\n        nn.Module.__init__(self)\n        Net.__init__(self, net_spec, in_dim, out_dim)\n        # set default\n        util.set_attr(self, dict(\n            init_fn=None,\n            normalize=False,\n            batch_norm=False,\n            clip_grad_val=None,\n            loss_spec={\'name\': \'MSELoss\'},\n            optim_spec={\'name\': \'Adam\'},\n            lr_scheduler_spec=None,\n            update_type=\'replace\',\n            update_frequency=1,\n            polyak_coef=0.0,\n            gpu=False,\n        ))\n        util.set_attr(self, self.net_spec, [\n            \'conv_hid_layers\',\n            \'fc_hid_layers\',\n            \'hid_layers_activation\',\n            \'init_fn\',\n            \'normalize\',\n            \'batch_norm\',\n            \'clip_grad_val\',\n            \'loss_spec\',\n            \'optim_spec\',\n            \'lr_scheduler_spec\',\n            \'update_type\',\n            \'update_frequency\',\n            \'polyak_coef\',\n            \'gpu\',\n        ])\n\n        # Guard against inappropriate algorithms and environments\n        assert isinstance(out_dim, int)\n\n        # conv body\n        self.conv_model = self.build_conv_layers(self.conv_hid_layers)\n        self.conv_out_dim = self.get_conv_output_size()\n\n        # fc body\n        if ps.is_empty(self.fc_hid_layers):\n            tail_in_dim = self.conv_out_dim\n        else:\n            # fc layer from flattened conv\n            self.fc_model = net_util.build_fc_model([self.conv_out_dim] + self.fc_hid_layers, self.hid_layers_activation)\n            tail_in_dim = self.fc_hid_layers[-1]\n\n        # tails. avoid list for single-tail for compute speed\n        self.v = nn.Linear(tail_in_dim, 1)  # state value\n        self.adv = nn.Linear(tail_in_dim, out_dim)  # action dependent raw advantage\n        self.model_tails = nn.ModuleList([self.v, self.adv])\n\n        net_util.init_layers(self, self.init_fn)\n        self.loss_fn = net_util.get_loss_fn(self, self.loss_spec)\n        self.to(self.device)\n        self.train()\n\n    def forward(self, x):\n        \'\'\'The feedforward step\'\'\'\n        if self.normalize:\n            x = x / 255.0\n        x = self.conv_model(x)\n        x = x.view(x.size(0), -1)  # to (batch_size, -1)\n        if hasattr(self, \'fc_model\'):\n            x = self.fc_model(x)\n        state_value = self.v(x)\n        raw_advantages = self.adv(x)\n        out = math_util.calc_q_value_logits(state_value, raw_advantages)\n        return out\n'"
slm_lab/agent/net/mlp.py,2,"b'from slm_lab.agent.net import net_util\nfrom slm_lab.agent.net.base import Net\nfrom slm_lab.lib import math_util, util\nimport numpy as np\nimport pydash as ps\nimport torch\nimport torch.nn as nn\n\n\nclass MLPNet(Net, nn.Module):\n    \'\'\'\n    Class for generating arbitrary sized feedforward neural network\n    If more than 1 output tensors, will create a self.model_tails instead of making last layer part of self.model\n\n    e.g. net_spec\n    ""net"": {\n        ""type"": ""MLPNet"",\n        ""shared"": true,\n        ""hid_layers"": [32],\n        ""hid_layers_activation"": ""relu"",\n        ""out_layer_activation"": null,\n        ""init_fn"": ""xavier_uniform_"",\n        ""clip_grad_val"": 1.0,\n        ""loss_spec"": {\n          ""name"": ""MSELoss""\n        },\n        ""optim_spec"": {\n          ""name"": ""Adam"",\n          ""lr"": 0.02\n        },\n        ""lr_scheduler_spec"": {\n            ""name"": ""StepLR"",\n            ""step_size"": 30,\n            ""gamma"": 0.1\n        },\n        ""update_type"": ""replace"",\n        ""update_frequency"": 1,\n        ""polyak_coef"": 0.9,\n        ""gpu"": true\n    }\n    \'\'\'\n\n    def __init__(self, net_spec, in_dim, out_dim):\n        \'\'\'\n        net_spec:\n        hid_layers: list containing dimensions of the hidden layers\n        hid_layers_activation: activation function for the hidden layers\n        out_layer_activation: activation function for the output layer, same shape as out_dim\n        init_fn: weight initialization function\n        clip_grad_val: clip gradient norm if value is not None\n        loss_spec: measure of error between model predictions and correct outputs\n        optim_spec: parameters for initializing the optimizer\n        lr_scheduler_spec: Pytorch optim.lr_scheduler\n        update_type: method to update network weights: \'replace\' or \'polyak\'\n        update_frequency: how many total timesteps per update\n        polyak_coef: ratio of polyak weight update\n        gpu: whether to train using a GPU. Note this will only work if a GPU is available, othewise setting gpu=True does nothing\n        \'\'\'\n        nn.Module.__init__(self)\n        super().__init__(net_spec, in_dim, out_dim)\n        # set default\n        util.set_attr(self, dict(\n            out_layer_activation=None,\n            init_fn=None,\n            clip_grad_val=None,\n            loss_spec={\'name\': \'MSELoss\'},\n            optim_spec={\'name\': \'Adam\'},\n            lr_scheduler_spec=None,\n            update_type=\'replace\',\n            update_frequency=1,\n            polyak_coef=0.0,\n            gpu=False,\n        ))\n        util.set_attr(self, self.net_spec, [\n            \'shared\',\n            \'hid_layers\',\n            \'hid_layers_activation\',\n            \'out_layer_activation\',\n            \'init_fn\',\n            \'clip_grad_val\',\n            \'loss_spec\',\n            \'optim_spec\',\n            \'lr_scheduler_spec\',\n            \'update_type\',\n            \'update_frequency\',\n            \'polyak_coef\',\n            \'gpu\',\n        ])\n\n        dims = [self.in_dim] + self.hid_layers\n        self.model = net_util.build_fc_model(dims, self.hid_layers_activation)\n        # add last layer with no activation\n        # tails. avoid list for single-tail for compute speed\n        if ps.is_integer(self.out_dim):\n            self.model_tail = net_util.build_fc_model([dims[-1], self.out_dim], self.out_layer_activation)\n        else:\n            if not ps.is_list(self.out_layer_activation):\n                self.out_layer_activation = [self.out_layer_activation] * len(out_dim)\n            assert len(self.out_layer_activation) == len(self.out_dim)\n            tails = []\n            for out_d, out_activ in zip(self.out_dim, self.out_layer_activation):\n                tail = net_util.build_fc_model([dims[-1], out_d], out_activ)\n                tails.append(tail)\n            self.model_tails = nn.ModuleList(tails)\n\n        net_util.init_layers(self, self.init_fn)\n        self.loss_fn = net_util.get_loss_fn(self, self.loss_spec)\n        self.to(self.device)\n        self.train()\n\n    def forward(self, x):\n        \'\'\'The feedforward step\'\'\'\n        x = self.model(x)\n        if hasattr(self, \'model_tails\'):\n            outs = []\n            for model_tail in self.model_tails:\n                outs.append(model_tail(x))\n            return outs\n        else:\n            return self.model_tail(x)\n\n\nclass HydraMLPNet(Net, nn.Module):\n    \'\'\'\n    Class for generating arbitrary sized feedforward neural network with multiple state and action heads, and a single shared body.\n\n    e.g. net_spec\n    ""net"": {\n        ""type"": ""HydraMLPNet"",\n        ""shared"": true,\n        ""hid_layers"": [\n            [[32],[32]], # 2 heads with hidden layers\n            [64], # body\n            [] # tail, no hidden layers\n        ],\n        ""hid_layers_activation"": ""relu"",\n        ""out_layer_activation"": null,\n        ""init_fn"": ""xavier_uniform_"",\n        ""clip_grad_val"": 1.0,\n        ""loss_spec"": {\n          ""name"": ""MSELoss""\n        },\n        ""optim_spec"": {\n          ""name"": ""Adam"",\n          ""lr"": 0.02\n        },\n        ""lr_scheduler_spec"": {\n            ""name"": ""StepLR"",\n            ""step_size"": 30,\n            ""gamma"": 0.1\n        },\n        ""update_type"": ""replace"",\n        ""update_frequency"": 1,\n        ""polyak_coef"": 0.9,\n        ""gpu"": true\n    }\n    \'\'\'\n\n    def __init__(self, net_spec, in_dim, out_dim):\n        \'\'\'\n        Multi state processing heads, single shared body, and multi action tails.\n        There is one state and action head per body/environment\n        Example:\n\n          env 1 state       env 2 state\n         _______|______    _______|______\n        |    head 1    |  |    head 2    |\n        |______________|  |______________|\n                |                  |\n                |__________________|\n         ________________|_______________\n        |          Shared body           |\n        |________________________________|\n                         |\n                 ________|_______\n                |                |\n         _______|______    ______|_______\n        |    tail 1    |  |    tail 2    |\n        |______________|  |______________|\n                |                |\n           env 1 action      env 2 action\n        \'\'\'\n        nn.Module.__init__(self)\n        super().__init__(net_spec, in_dim, out_dim)\n        # set default\n        util.set_attr(self, dict(\n            out_layer_activation=None,\n            init_fn=None,\n            clip_grad_val=None,\n            loss_spec={\'name\': \'MSELoss\'},\n            optim_spec={\'name\': \'Adam\'},\n            lr_scheduler_spec=None,\n            update_type=\'replace\',\n            update_frequency=1,\n            polyak_coef=0.0,\n            gpu=False,\n        ))\n        util.set_attr(self, self.net_spec, [\n            \'hid_layers\',\n            \'hid_layers_activation\',\n            \'out_layer_activation\',\n            \'init_fn\',\n            \'clip_grad_val\',\n            \'loss_spec\',\n            \'optim_spec\',\n            \'lr_scheduler_spec\',\n            \'update_type\',\n            \'update_frequency\',\n            \'polyak_coef\',\n            \'gpu\',\n        ])\n        assert len(self.hid_layers) == 3, \'Your hidden layers must specify [*heads], [body], [*tails]. If not, use MLPNet\'\n        assert isinstance(self.in_dim, list), \'Hydra network needs in_dim as list\'\n        assert isinstance(self.out_dim, list), \'Hydra network needs out_dim as list\'\n        self.head_hid_layers = self.hid_layers[0]\n        self.body_hid_layers = self.hid_layers[1]\n        self.tail_hid_layers = self.hid_layers[2]\n        if len(self.head_hid_layers) == 1:\n            self.head_hid_layers = self.head_hid_layers * len(self.in_dim)\n        if len(self.tail_hid_layers) == 1:\n            self.tail_hid_layers = self.tail_hid_layers * len(self.out_dim)\n\n        self.model_heads = self.build_model_heads(in_dim)\n        heads_out_dim = np.sum([head_hid_layers[-1] for head_hid_layers in self.head_hid_layers])\n        dims = [heads_out_dim] + self.body_hid_layers\n        self.model_body = net_util.build_fc_model(dims, self.hid_layers_activation)\n        self.model_tails = self.build_model_tails(self.out_dim, self.out_layer_activation)\n\n        net_util.init_layers(self, self.init_fn)\n        self.loss_fn = net_util.get_loss_fn(self, self.loss_spec)\n        self.to(self.device)\n        self.train()\n\n    def build_model_heads(self, in_dim):\n        \'\'\'Build each model_head. These are stored as Sequential models in model_heads\'\'\'\n        assert len(self.head_hid_layers) == len(in_dim), \'Hydra head hid_params inconsistent with number in dims\'\n        model_heads = nn.ModuleList()\n        for in_d, hid_layers in zip(in_dim, self.head_hid_layers):\n            dims = [in_d] + hid_layers\n            model_head = net_util.build_fc_model(dims, self.hid_layers_activation)\n            model_heads.append(model_head)\n        return model_heads\n\n    def build_model_tails(self, out_dim, out_layer_activation):\n        \'\'\'Build each model_tail. These are stored as Sequential models in model_tails\'\'\'\n        if not ps.is_list(out_layer_activation):\n            out_layer_activation = [out_layer_activation] * len(out_dim)\n        model_tails = nn.ModuleList()\n        if ps.is_empty(self.tail_hid_layers):\n            for out_d, out_activ in zip(out_dim, out_layer_activation):\n                tail = net_util.build_fc_model([self.body_hid_layers[-1], out_d], out_activ)\n                model_tails.append(tail)\n        else:\n            assert len(self.tail_hid_layers) == len(out_dim), \'Hydra tail hid_params inconsistent with number out dims\'\n            for out_d, out_activ, hid_layers in zip(out_dim, out_layer_activation, self.tail_hid_layers):\n                dims = hid_layers\n                model_tail = net_util.build_fc_model(dims, self.hid_layers_activation)\n                tail_out = net_util.build_fc_model([dims[-1], out_d], out_activ)\n                model_tail.add_module(str(len(model_tail)), tail_out)\n                model_tails.append(model_tail)\n        return model_tails\n\n    def forward(self, xs):\n        \'\'\'The feedforward step\'\'\'\n        head_xs = []\n        for model_head, x in zip(self.model_heads, xs):\n            head_xs.append(model_head(x))\n        head_xs = torch.cat(head_xs, dim=-1)\n        body_x = self.model_body(head_xs)\n        outs = []\n        for model_tail in self.model_tails:\n            outs.append(model_tail(body_x))\n        return outs\n\n\nclass DuelingMLPNet(MLPNet):\n    \'\'\'\n    Class for generating arbitrary sized feedforward neural network, with dueling heads. Intended for Q-Learning algorithms only.\n    Implementation based on ""Dueling Network Architectures for Deep Reinforcement Learning"" http://proceedings.mlr.press/v48/wangf16.pdf\n\n    e.g. net_spec\n    ""net"": {\n        ""type"": ""DuelingMLPNet"",\n        ""shared"": true,\n        ""hid_layers"": [32],\n        ""hid_layers_activation"": ""relu"",\n        ""init_fn"": ""xavier_uniform_"",\n        ""clip_grad_val"": 1.0,\n        ""loss_spec"": {\n          ""name"": ""MSELoss""\n        },\n        ""optim_spec"": {\n          ""name"": ""Adam"",\n          ""lr"": 0.02\n        },\n        ""lr_scheduler_spec"": {\n            ""name"": ""StepLR"",\n            ""step_size"": 30,\n            ""gamma"": 0.1\n        },\n        ""update_type"": ""replace"",\n        ""update_frequency"": 1,\n        ""polyak_coef"": 0.9,\n        ""gpu"": true\n    }\n    \'\'\'\n\n    def __init__(self, net_spec, in_dim, out_dim):\n        nn.Module.__init__(self)\n        Net.__init__(self, net_spec, in_dim, out_dim)\n        # set default\n        util.set_attr(self, dict(\n            init_fn=None,\n            clip_grad_val=None,\n            loss_spec={\'name\': \'MSELoss\'},\n            optim_spec={\'name\': \'Adam\'},\n            lr_scheduler_spec=None,\n            update_type=\'replace\',\n            update_frequency=1,\n            polyak_coef=0.0,\n            gpu=False,\n        ))\n        util.set_attr(self, self.net_spec, [\n            \'shared\',\n            \'hid_layers\',\n            \'hid_layers_activation\',\n            \'init_fn\',\n            \'clip_grad_val\',\n            \'loss_spec\',\n            \'optim_spec\',\n            \'lr_scheduler_spec\',\n            \'update_type\',\n            \'update_frequency\',\n            \'polyak_coef\',\n            \'gpu\',\n        ])\n\n        # Guard against inappropriate algorithms and environments\n        # Build model body\n        dims = [self.in_dim] + self.hid_layers\n        self.model_body = net_util.build_fc_model(dims, self.hid_layers_activation)\n        # output layers\n        self.v = nn.Linear(dims[-1], 1)  # state value\n        self.adv = nn.Linear(dims[-1], out_dim)  # action dependent raw advantage\n        self.model_tails = nn.ModuleList([self.v, self.adv])\n        net_util.init_layers(self, self.init_fn)\n        self.loss_fn = net_util.get_loss_fn(self, self.loss_spec)\n        self.to(self.device)\n\n    def forward(self, x):\n        \'\'\'The feedforward step\'\'\'\n        x = self.model_body(x)\n        state_value = self.v(x)\n        raw_advantages = self.adv(x)\n        out = math_util.calc_q_value_logits(state_value, raw_advantages)\n        return out\n'"
slm_lab/agent/net/net_util.py,15,"b""from functools import partial, wraps\nfrom slm_lab.lib import logger, optimizer, util\nimport os\nimport pydash as ps\nimport torch\nimport torch.nn as nn\n\nlogger = logger.get_logger(__name__)\n\n# register custom torch.optim\nsetattr(torch.optim, 'GlobalAdam', optimizer.GlobalAdam)\nsetattr(torch.optim, 'GlobalRMSprop', optimizer.GlobalRMSprop)\nsetattr(torch.optim, 'Lookahead', optimizer.Lookahead)\nsetattr(torch.optim, 'RAdam', optimizer.RAdam)\n\n\nclass NoOpLRScheduler:\n    '''Symbolic LRScheduler class for API consistency'''\n\n    def __init__(self, optim):\n        self.optim = optim\n\n    def step(self, epoch=None):\n        pass\n\n    def get_lr(self):\n        if hasattr(self.optim, 'defaults'):\n            return self.optim.defaults['lr']\n        else:  # TODO retrieve lr more generally\n            return self.optim.param_groups[0]['lr']\n\n\ndef build_fc_model(dims, activation=None):\n    '''Build a full-connected model by interleaving nn.Linear and activation_fn'''\n    assert len(dims) >= 2, 'dims need to at least contain input, output'\n    # shift dims and make pairs of (in, out) dims per layer\n    dim_pairs = list(zip(dims[:-1], dims[1:]))\n    layers = []\n    for in_d, out_d in dim_pairs:\n        layers.append(nn.Linear(in_d, out_d))\n        if activation is not None:\n            layers.append(get_activation_fn(activation))\n    model = nn.Sequential(*layers)\n    return model\n\n\ndef get_nn_name(uncased_name):\n    '''Helper to get the proper name in PyTorch nn given a case-insensitive name'''\n    for nn_name in nn.__dict__:\n        if uncased_name.lower() == nn_name.lower():\n            return nn_name\n    raise ValueError(f'Name {uncased_name} not found in {nn.__dict__}')\n\n\ndef get_activation_fn(activation):\n    '''Helper to generate activation function layers for net'''\n    ActivationClass = getattr(nn, get_nn_name(activation))\n    return ActivationClass()\n\n\ndef get_loss_fn(cls, loss_spec):\n    '''Helper to parse loss param and construct loss_fn for net'''\n    LossClass = getattr(nn, get_nn_name(loss_spec['name']))\n    loss_spec = ps.omit(loss_spec, 'name')\n    loss_fn = LossClass(**loss_spec)\n    return loss_fn\n\n\ndef get_lr_scheduler(optim, lr_scheduler_spec):\n    '''Helper to parse lr_scheduler param and construct Pytorch optim.lr_scheduler'''\n    if ps.is_empty(lr_scheduler_spec):\n        lr_scheduler = NoOpLRScheduler(optim)\n    elif lr_scheduler_spec['name'] == 'LinearToZero':\n        LRSchedulerClass = getattr(torch.optim.lr_scheduler, 'LambdaLR')\n        frame = float(lr_scheduler_spec['frame'])\n        lr_scheduler = LRSchedulerClass(optim, lr_lambda=lambda x: 1 - x / frame)\n    else:\n        LRSchedulerClass = getattr(torch.optim.lr_scheduler, lr_scheduler_spec['name'])\n        lr_scheduler_spec = ps.omit(lr_scheduler_spec, 'name')\n        lr_scheduler = LRSchedulerClass(optim, **lr_scheduler_spec)\n    return lr_scheduler\n\n\ndef get_optim(net, optim_spec):\n    '''Helper to parse optim param and construct optim for net'''\n    OptimClass = getattr(torch.optim, optim_spec['name'])\n    optim_spec = ps.omit(optim_spec, 'name')\n    if torch.is_tensor(net):  # for non-net tensor variable\n        optim = OptimClass([net], **optim_spec)\n    else:\n        optim = OptimClass(net.parameters(), **optim_spec)\n    return optim\n\n\ndef get_policy_out_dim(body):\n    '''Helper method to construct the policy network out_dim for a body according to is_discrete, action_type'''\n    action_dim = body.action_dim\n    if body.is_discrete:\n        if body.action_type == 'multi_discrete':\n            assert ps.is_list(action_dim), action_dim\n            policy_out_dim = action_dim\n        else:\n            assert ps.is_integer(action_dim), action_dim\n            policy_out_dim = action_dim\n    else:\n        assert ps.is_integer(action_dim), action_dim\n        if action_dim == 1:  # single action, use [loc, scale]\n            policy_out_dim = 2\n        else:  # multi-action, use [locs], [scales]\n            policy_out_dim = [action_dim, action_dim]\n    return policy_out_dim\n\n\ndef get_out_dim(body, add_critic=False):\n    '''Construct the NetClass out_dim for a body according to is_discrete, action_type, and whether to add a critic unit'''\n    policy_out_dim = get_policy_out_dim(body)\n    if add_critic:\n        if ps.is_list(policy_out_dim):\n            out_dim = policy_out_dim + [1]\n        else:\n            out_dim = [policy_out_dim, 1]\n    else:\n        out_dim = policy_out_dim\n    return out_dim\n\n\ndef init_layers(net, init_fn_name):\n    '''Primary method to initialize the weights of the layers of a network'''\n    if init_fn_name is None:\n        return\n\n    # get nonlinearity\n    nonlinearity = get_nn_name(net.hid_layers_activation).lower()\n    if nonlinearity == 'leakyrelu':\n        nonlinearity = 'leaky_relu'  # guard name\n\n    # get init_fn and add arguments depending on nonlinearity\n    init_fn = getattr(nn.init, init_fn_name)\n    if 'kaiming' in init_fn_name:  # has 'nonlinearity' as arg\n        assert nonlinearity in ['relu', 'leaky_relu'], f'Kaiming initialization not supported for {nonlinearity}'\n        init_fn = partial(init_fn, nonlinearity=nonlinearity)\n    elif 'orthogonal' in init_fn_name or 'xavier' in init_fn_name:  # has 'gain' as arg\n        gain = nn.init.calculate_gain(nonlinearity)\n        init_fn = partial(init_fn, gain=gain)\n    else:\n        pass\n\n    # finally, apply init_params to each layer in its modules\n    net.apply(partial(init_params, init_fn=init_fn))\n\n\ndef init_params(module, init_fn):\n    '''Initialize module's weights using init_fn, and biases to 0.0'''\n    bias_init = 0.0\n    classname = util.get_class_name(module)\n    if 'Net' in classname:  # skip if it's a net, not pytorch layer\n        pass\n    elif classname == 'BatchNorm2d':\n        pass  # can't init BatchNorm2d\n    elif any(k in classname for k in ('Conv', 'Linear')):\n        init_fn(module.weight)\n        nn.init.constant_(module.bias, bias_init)\n    elif 'GRU' in classname:\n        for name, param in module.named_parameters():\n            if 'weight' in name:\n                init_fn(param)\n            elif 'bias' in name:\n                nn.init.constant_(param, bias_init)\n    else:\n        pass\n\n\n# params methods\n\n\ndef save(net, model_path):\n    '''Save model weights to path'''\n    torch.save(net.state_dict(), util.smart_path(model_path))\n\n\ndef save_algorithm(algorithm, ckpt=None):\n    '''Save all the nets for an algorithm'''\n    agent = algorithm.agent\n    net_names = algorithm.net_names\n    model_prepath = agent.spec['meta']['model_prepath']\n    if ckpt is not None:\n        model_prepath += f'_ckpt-{ckpt}'\n    for net_name in net_names:\n        net = getattr(algorithm, net_name)\n        model_path = f'{model_prepath}_{net_name}_model.pt'\n        save(net, model_path)\n        optim_name = net_name.replace('net', 'optim')\n        optim = getattr(algorithm, optim_name, None)\n        if optim is not None:  # only trainable net has optim\n            optim_path = f'{model_prepath}_{net_name}_optim.pt'\n            save(optim, optim_path)\n    logger.debug(f'Saved algorithm {util.get_class_name(algorithm)} nets {net_names} to {model_prepath}_*.pt')\n\n\ndef load(net, model_path):\n    '''Save model weights from a path into a net module'''\n    device = None if torch.cuda.is_available() else 'cpu'\n    net.load_state_dict(torch.load(util.smart_path(model_path), map_location=device))\n\n\ndef load_algorithm(algorithm):\n    '''Save all the nets for an algorithm'''\n    agent = algorithm.agent\n    net_names = algorithm.net_names\n    model_prepath = agent.spec['meta']['model_prepath']\n    if util.get_lab_mode() == 'enjoy':\n        model_prepath += '_ckpt-best'\n    logger.info(f'Loading algorithm {util.get_class_name(algorithm)} nets {net_names} from {model_prepath}_*.pt')\n    for net_name in net_names:\n        net = getattr(algorithm, net_name)\n        model_path = f'{model_prepath}_{net_name}_model.pt'\n        load(net, model_path)\n        optim_name = net_name.replace('net', 'optim')\n        optim = getattr(algorithm, optim_name, None)\n        if optim is not None:  # only trainable net has optim\n            optim_path = f'{model_prepath}_{net_name}_optim.pt'\n            load(optim, optim_path)\n\n\ndef copy(src_net, tar_net):\n    '''Copy model weights from src to target'''\n    tar_net.load_state_dict(src_net.state_dict())\n\n\ndef polyak_update(src_net, tar_net, old_ratio=0.5):\n    '''\n    Polyak weight update to update a target tar_net, retain old weights by its ratio, i.e.\n    target <- old_ratio * source + (1 - old_ratio) * target\n    '''\n    for src_param, tar_param in zip(src_net.parameters(), tar_net.parameters()):\n        tar_param.data.copy_(old_ratio * src_param.data + (1.0 - old_ratio) * tar_param.data)\n\n\ndef to_check_train_step():\n    '''Condition for running assert_trained'''\n    return os.environ.get('PY_ENV') == 'test' or util.get_lab_mode() == 'dev'\n\n\ndef dev_check_train_step(fn):\n    '''\n    Decorator to check if net.train_step actually updates the network weights properly\n    Triggers only if to_check_train_step is True (dev/test mode)\n    @example\n\n    @net_util.dev_check_train_step\n    def train_step(self, ...):\n        ...\n    '''\n    @wraps(fn)\n    def check_fn(*args, **kwargs):\n        if not to_check_train_step():\n            return fn(*args, **kwargs)\n\n        net = args[0]  # first arg self\n        # get pre-update parameters to compare\n        pre_params = [param.clone() for param in net.parameters()]\n\n        # run train_step, get loss\n        loss = fn(*args, **kwargs)\n        assert not torch.isnan(loss).any(), loss\n\n        # get post-update parameters to compare\n        post_params = [param.clone() for param in net.parameters()]\n        if loss == 0.0:\n            # if loss is 0, there should be no updates\n            # TODO if without momentum, parameters should not change too\n            for p_name, param in net.named_parameters():\n                assert param.grad.norm() == 0\n        else:\n            # check parameter updates\n            try:\n                assert not all(torch.equal(w1, w2) for w1, w2 in zip(pre_params, post_params)), f'Model parameter is not updated in train_step(), check if your tensor is detached from graph. Loss: {loss:g}'\n            except Exception as e:\n                logger.error(e)\n                if os.environ.get('PY_ENV') == 'test':\n                    # raise error if in unit test\n                    raise(e)\n\n            # check grad norms\n            min_norm, max_norm = 0.0, 1e5\n            for p_name, param in net.named_parameters():\n                try:\n                    grad_norm = param.grad.norm()\n                    assert min_norm < grad_norm < max_norm, f'Gradient norm for {p_name} is {grad_norm:g}, fails the extreme value check {min_norm} < grad_norm < {max_norm}. Loss: {loss:g}. Check your network and loss computation.'\n                except Exception as e:\n                    logger.warning(e)\n        logger.debug('Passed network parameter update check.')\n        # store grad norms for debugging\n        net.store_grad_norms()\n        return loss\n    return check_fn\n\n\ndef get_grad_norms(algorithm):\n    '''Gather all the net's grad norms of an algorithm for debugging'''\n    grad_norms = []\n    for net_name in algorithm.net_names:\n        net = getattr(algorithm, net_name)\n        if net.grad_norms is not None:\n            grad_norms.extend(net.grad_norms)\n    return grad_norms\n\n\ndef init_global_nets(algorithm):\n    '''\n    Initialize global_nets for Hogwild using an identical instance of an algorithm from an isolated Session\n    in spec.meta.distributed, specify either:\n    - 'shared': global network parameter is shared all the time. In this mode, algorithm local network will be replaced directly by global_net via overriding by identify attribute name\n    - 'synced': global network parameter is periodically synced to local network after each gradient push. In this mode, algorithm will keep a separate reference to `global_{net}` for each of its network\n    '''\n    dist_mode = algorithm.agent.spec['meta']['distributed']\n    assert dist_mode in ('shared', 'synced'), f'Unrecognized distributed mode'\n    global_nets = {}\n    for net_name in algorithm.net_names:\n        optim_name = net_name.replace('net', 'optim')\n        if not hasattr(algorithm, optim_name):  # only for trainable network, i.e. has an optim\n            continue\n        g_net = getattr(algorithm, net_name)\n        g_net.share_memory()  # make net global\n        if dist_mode == 'shared':  # use the same name to override the local net\n            global_nets[net_name] = g_net\n        else:  # keep a separate reference for syncing\n            global_nets[f'global_{net_name}'] = g_net\n        # if optim is Global, set to override the local optim and its scheduler\n        optim = getattr(algorithm, optim_name)\n        if hasattr(optim, 'share_memory'):\n            optim.share_memory()  # make optim global\n            global_nets[optim_name] = optim\n            if hasattr(optim, 'optimizer'):  # for Lookahead with an inner optimizer\n                global_nets[f'{optim_name}_optimizer'] = optim.optimizer\n            lr_scheduler_name = net_name.replace('net', 'lr_scheduler')\n            lr_scheduler = getattr(algorithm, lr_scheduler_name)\n            global_nets[lr_scheduler_name] = lr_scheduler\n    logger.info(f'Initialized global_nets attr {list(global_nets.keys())} for Hogwild')\n    return global_nets\n\n\ndef set_global_nets(algorithm, global_nets):\n    '''For Hogwild, set attr built in init_global_nets above. Use in algorithm init.'''\n    # set attr first so algorithm always has self.global_{net} to pass into train_step\n    for net_name in algorithm.net_names:\n        setattr(algorithm, f'global_{net_name}', None)\n    # set attr created in init_global_nets\n    if global_nets is not None:\n        # handle inner-optimizer recovery\n        inner_opt_keys = [k for k in global_nets if k.endswith('_optimizer')]\n        for inner_opt_key in inner_opt_keys:\n            opt = global_nets[inner_opt_key.replace('_optimizer', '')]  # optimizer which has a inner optimizer\n            setattr(opt, 'optimizer', global_nets.pop(inner_opt_key))\n        # set global nets and optims\n        util.set_attr(algorithm, global_nets)\n        logger.info(f'Set global_nets attr {list(global_nets.keys())} for Hogwild')\n\n\ndef push_global_grads(net, global_net):\n    '''Push gradients to global_net, call inside train_step between loss.backward() and optim.step()'''\n    for param, global_param in zip(net.parameters(), global_net.parameters()):\n        if global_param.grad is not None:\n            return  # quick skip\n        global_param._grad = param.grad\n"""
slm_lab/agent/net/q_net.py,3,"b""# special module for Q-networks, Q(s, a) -> q\nfrom slm_lab.agent.net.base import Net\nfrom slm_lab.agent.net.conv import ConvNet\nfrom slm_lab.agent.net.mlp import MLPNet\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.lib import util\nimport pydash as ps\nimport torch\nimport torch.nn as nn\n\n\nclass QMLPNet(MLPNet):\n    def __init__(self, net_spec, in_dim, out_dim):\n        state_dim, action_dim = in_dim\n        nn.Module.__init__(self)\n        Net.__init__(self, net_spec, in_dim, out_dim)\n        # set default\n        util.set_attr(self, dict(\n            out_layer_activation=None,\n            init_fn=None,\n            clip_grad_val=None,\n            loss_spec={'name': 'MSELoss'},\n            optim_spec={'name': 'Adam'},\n            lr_scheduler_spec=None,\n            update_type='replace',\n            update_frequency=1,\n            polyak_coef=0.0,\n            gpu=False,\n        ))\n        util.set_attr(self, self.net_spec, [\n            'shared',\n            'hid_layers',\n            'hid_layers_activation',\n            'out_layer_activation',\n            'init_fn',\n            'clip_grad_val',\n            'loss_spec',\n            'optim_spec',\n            'lr_scheduler_spec',\n            'update_type',\n            'update_frequency',\n            'polyak_coef',\n            'gpu',\n        ])\n        dims = [state_dim + action_dim] + self.hid_layers\n        self.model = net_util.build_fc_model(dims, self.hid_layers_activation)\n        # add last layer with no activation\n        self.model_tail = net_util.build_fc_model([dims[-1], self.out_dim], self.out_layer_activation)\n\n        net_util.init_layers(self, self.init_fn)\n        self.loss_fn = net_util.get_loss_fn(self, self.loss_spec)\n        self.to(self.device)\n        self.train()\n\n    def forward(self, state, action):\n        s_a = torch.cat((state, action), dim=-1)\n        s_a = self.model(s_a)\n        return self.model_tail(s_a)\n\n\nclass QConvNet(ConvNet):\n\n    def __init__(self, net_spec, in_dim, out_dim):\n        state_dim, action_dim = in_dim\n        assert len(state_dim) == 3  # image shape (c,w,h)\n        # conv body\n        nn.Module.__init__(self)\n        Net.__init__(self, net_spec, state_dim, out_dim)\n        # set default\n        util.set_attr(self, dict(\n            out_layer_activation=None,\n            init_fn=None,\n            normalize=False,\n            batch_norm=True,\n            clip_grad_val=None,\n            loss_spec={'name': 'MSELoss'},\n            optim_spec={'name': 'Adam'},\n            lr_scheduler_spec=None,\n            update_type='replace',\n            update_frequency=1,\n            polyak_coef=0.0,\n            gpu=False,\n        ))\n        util.set_attr(self, self.net_spec, [\n            'conv_hid_layers',\n            'fc_hid_layers',\n            'hid_layers_activation',\n            'out_layer_activation',\n            'init_fn',\n            'normalize',\n            'batch_norm',\n            'clip_grad_val',\n            'loss_spec',\n            'optim_spec',\n            'lr_scheduler_spec',\n            'update_type',\n            'update_frequency',\n            'polyak_coef',\n            'gpu',\n        ])\n        # state conv model\n        self.conv_model = self.build_conv_layers(self.conv_hid_layers)\n        self.conv_out_dim = self.get_conv_output_size()\n\n        # state fc model\n        self.fc_model = net_util.build_fc_model([self.conv_out_dim + action_dim] + self.fc_hid_layers, self.hid_layers_activation)\n\n        # affine transformation applied to\n        tail_in_dim = self.fc_hid_layers[-1]\n        self.model_tail = net_util.build_fc_model([tail_in_dim, self.out_dim], self.out_layer_activation)\n\n        net_util.init_layers(self, self.init_fn)\n        self.loss_fn = net_util.get_loss_fn(self, self.loss_spec)\n        self.to(self.device)\n        self.train()\n\n    def forward(self, state, action):\n        if self.normalize:\n            state = state / 255.0\n        state = self.conv_model(state)\n        state = state.view(state.size(0), -1)  # to (batch_size, -1)\n        s_a = torch.cat((state, action), dim=-1)\n        s_a = self.fc_model(s_a)\n        return self.model_tail(s_a)\n\n\nclass FiLMQConvNet(ConvNet):\n\n    def __init__(self, net_spec, in_dim, out_dim):\n        state_dim, action_dim = in_dim\n        assert len(state_dim) == 3  # image shape (c,w,h)\n        # conv body\n        nn.Module.__init__(self)\n        Net.__init__(self, net_spec, state_dim, out_dim)\n        # set default\n        util.set_attr(self, dict(\n            out_layer_activation=None,\n            init_fn=None,\n            normalize=False,\n            batch_norm=True,\n            clip_grad_val=None,\n            loss_spec={'name': 'MSELoss'},\n            optim_spec={'name': 'Adam'},\n            lr_scheduler_spec=None,\n            update_type='replace',\n            update_frequency=1,\n            polyak_coef=0.0,\n            gpu=False,\n        ))\n        util.set_attr(self, self.net_spec, [\n            'conv_hid_layers',\n            'fc_hid_layers',\n            'hid_layers_activation',\n            'out_layer_activation',\n            'init_fn',\n            'normalize',\n            'batch_norm',\n            'clip_grad_val',\n            'loss_spec',\n            'optim_spec',\n            'lr_scheduler_spec',\n            'update_type',\n            'update_frequency',\n            'polyak_coef',\n            'gpu',\n        ])\n        # state conv model\n        self.conv_model = self.build_conv_layers(self.conv_hid_layers)\n        self.conv_out_dim = self.get_conv_output_size()\n\n        # state fc model\n        self.state_fc_model = net_util.build_fc_model([self.conv_out_dim] + self.fc_hid_layers, 'sigmoid')\n\n        # use Feature-wise Linear Modulation applied to the outputs of the last state_fc_model hid_layers\n        # https://arxiv.org/pdf/1709.07871.pdf\n        state_fc_out_dim = self.fc_hid_layers[-1]\n        # self.action_conv_scale = net_util.build_fc_model([action_dim, self.conv_out_dim], 'sigmoid')\n        # self.action_conv_shift = net_util.build_fc_model([action_dim, self.conv_out_dim], 'sigmoid')\n        self.action_fc_scale = net_util.build_fc_model([action_dim, state_fc_out_dim], 'sigmoid')\n        self.action_fc_shift = net_util.build_fc_model([action_dim, state_fc_out_dim], 'sigmoid')\n\n        # affine transformation applied to\n        tail_in_dim = self.fc_hid_layers[-1]\n        self.model_tail = net_util.build_fc_model([tail_in_dim, self.out_dim], self.out_layer_activation)\n\n        net_util.init_layers(self, self.init_fn)\n        self.loss_fn = net_util.get_loss_fn(self, self.loss_spec)\n        self.to(self.device)\n        self.train()\n\n    def forward(self, state, action):\n        if self.normalize:\n            state = state / 255.0\n        state = self.conv_model(state)\n        state = state.view(state.size(0), -1)  # to (batch_size, -1)\n        # action_conv_scale = self.action_conv_scale(action)\n        # action_conv_shift = self.action_conv_shift(action)\n        # state = state * action_conv_scale + action_conv_shift\n        state = self.state_fc_model(state)\n        action_fc_scale = self.action_fc_scale(action)\n        action_fc_shift = self.action_fc_shift(action)\n        s_a = state * action_fc_scale + action_fc_shift\n        return self.model_tail(s_a)\n"""
slm_lab/agent/net/recurrent.py,1,"b'from slm_lab.agent.net import net_util\nfrom slm_lab.agent.net.base import Net\nfrom slm_lab.lib import util\nimport pydash as ps\nimport torch.nn as nn\n\n\nclass RecurrentNet(Net, nn.Module):\n    \'\'\'\n    Class for generating arbitrary sized recurrent neural networks which take a sequence of states as input.\n\n    Assumes that a single input example is organized into a 3D tensor\n    batch_size x seq_len x state_dim\n    The entire model consists of three parts:\n        1. self.fc_model (state processing)\n        2. self.rnn_model\n        3. self.model_tails\n\n    e.g. net_spec\n    ""net"": {\n        ""type"": ""RecurrentNet"",\n        ""shared"": true,\n        ""cell_type"": ""GRU"",\n        ""fc_hid_layers"": [],\n        ""hid_layers_activation"": ""relu"",\n        ""out_layer_activation"": null,\n        ""rnn_hidden_size"": 32,\n        ""rnn_num_layers"": 1,\n        ""bidirectional"": False,\n        ""seq_len"": 4,\n        ""init_fn"": ""xavier_uniform_"",\n        ""clip_grad_val"": 1.0,\n        ""loss_spec"": {\n          ""name"": ""MSELoss""\n        },\n        ""optim_spec"": {\n          ""name"": ""Adam"",\n          ""lr"": 0.01\n        },\n        ""lr_scheduler_spec"": {\n            ""name"": ""StepLR"",\n            ""step_size"": 30,\n            ""gamma"": 0.1\n        },\n        ""update_type"": ""replace"",\n        ""update_frequency"": 1,\n        ""polyak_coef"": 0.9,\n        ""gpu"": true\n    }\n    \'\'\'\n\n    def __init__(self, net_spec, in_dim, out_dim):\n        \'\'\'\n        net_spec:\n        cell_type: any of RNN, LSTM, GRU\n        fc_hid_layers: list of fc layers preceeding the RNN layers\n        hid_layers_activation: activation function for the fc hidden layers\n        out_layer_activation: activation function for the output layer, same shape as out_dim\n        rnn_hidden_size: rnn hidden_size\n        rnn_num_layers: number of recurrent layers\n        bidirectional: if RNN should be bidirectional\n        seq_len: length of the history of being passed to the net\n        init_fn: weight initialization function\n        clip_grad_val: clip gradient norm if value is not None\n        loss_spec: measure of error between model predictions and correct outputs\n        optim_spec: parameters for initializing the optimizer\n        lr_scheduler_spec: Pytorch optim.lr_scheduler\n        update_type: method to update network weights: \'replace\' or \'polyak\'\n        update_frequency: how many total timesteps per update\n        polyak_coef: ratio of polyak weight update\n        gpu: whether to train using a GPU. Note this will only work if a GPU is available, othewise setting gpu=True does nothing\n        \'\'\'\n        nn.Module.__init__(self)\n        super().__init__(net_spec, in_dim, out_dim)\n        # set default\n        util.set_attr(self, dict(\n            out_layer_activation=None,\n            cell_type=\'GRU\',\n            rnn_num_layers=1,\n            bidirectional=False,\n            init_fn=None,\n            clip_grad_val=None,\n            loss_spec={\'name\': \'MSELoss\'},\n            optim_spec={\'name\': \'Adam\'},\n            lr_scheduler_spec=None,\n            update_type=\'replace\',\n            update_frequency=1,\n            polyak_coef=0.0,\n            gpu=False,\n        ))\n        util.set_attr(self, self.net_spec, [\n            \'cell_type\',\n            \'fc_hid_layers\',\n            \'hid_layers_activation\',\n            \'out_layer_activation\',\n            \'rnn_hidden_size\',\n            \'rnn_num_layers\',\n            \'bidirectional\',\n            \'seq_len\',\n            \'init_fn\',\n            \'clip_grad_val\',\n            \'loss_spec\',\n            \'optim_spec\',\n            \'lr_scheduler_spec\',\n            \'update_type\',\n            \'update_frequency\',\n            \'polyak_coef\',\n            \'gpu\',\n        ])\n        # restore proper in_dim from env stacked state_dim (stack_len, *raw_state_dim)\n        self.in_dim = in_dim[1:] if len(in_dim) > 2 else in_dim[1]\n        # fc body: state processing model\n        if ps.is_empty(self.fc_hid_layers):\n            self.rnn_input_dim = self.in_dim\n        else:\n            fc_dims = [self.in_dim] + self.fc_hid_layers\n            self.fc_model = net_util.build_fc_model(fc_dims, self.hid_layers_activation)\n            self.rnn_input_dim = fc_dims[-1]\n\n        # RNN model\n        self.rnn_model = getattr(nn, net_util.get_nn_name(self.cell_type))(\n            input_size=self.rnn_input_dim,\n            hidden_size=self.rnn_hidden_size,\n            num_layers=self.rnn_num_layers,\n            batch_first=True, bidirectional=self.bidirectional)\n\n        # tails. avoid list for single-tail for compute speed\n        if ps.is_integer(self.out_dim):\n            self.model_tail = net_util.build_fc_model([self.rnn_hidden_size, self.out_dim], self.out_layer_activation)\n        else:\n            if not ps.is_list(self.out_layer_activation):\n                self.out_layer_activation = [self.out_layer_activation] * len(out_dim)\n            assert len(self.out_layer_activation) == len(self.out_dim)\n            tails = []\n            for out_d, out_activ in zip(self.out_dim, self.out_layer_activation):\n                tail = net_util.build_fc_model([self.rnn_hidden_size, out_d], out_activ)\n                tails.append(tail)\n            self.model_tails = nn.ModuleList(tails)\n\n        net_util.init_layers(self, self.init_fn)\n        self.loss_fn = net_util.get_loss_fn(self, self.loss_spec)\n        self.to(self.device)\n        self.train()\n\n    def forward(self, x):\n        \'\'\'The feedforward step. Input is batch_size x seq_len x state_dim\'\'\'\n        # Unstack input to (batch_size x seq_len) x state_dim in order to transform all state inputs\n        batch_size = x.size(0)\n        x = x.view(-1, self.in_dim)\n        if hasattr(self, \'fc_model\'):\n            x = self.fc_model(x)\n        # Restack to batch_size x seq_len x rnn_input_dim\n        x = x.view(-1, self.seq_len, self.rnn_input_dim)\n        if self.cell_type == \'LSTM\':\n            _output, (h_n, c_n) = self.rnn_model(x)\n        else:\n            _output, h_n = self.rnn_model(x)\n        hid_x = h_n[-1]  # get final time-layer\n        # return tensor if single tail, else list of tail tensors\n        if hasattr(self, \'model_tails\'):\n            outs = []\n            for model_tail in self.model_tails:\n                outs.append(model_tail(hid_x))\n            return outs\n        else:\n            return self.model_tail(hid_x)\n'"
slm_lab/env/vizdoom/__init__.py,0,b'from .vizdoom_env import VizDoomEnv\n'
slm_lab/env/vizdoom/vizdoom_env.py,0,"b""# inspired by nsavinov/gym-vizdoom and ppaquette/gym-doom\nfrom gym import Env\nfrom gym.envs.classic_control import rendering\nfrom slm_lab.lib import util\nfrom vizdoom import DoomGame\nimport gym.spaces as spaces\nimport numpy as np\n\n\nclass VizDoomEnv(Env):\n    '''\n    Wrapper for vizdoom to use as an OpenAI gym environment.\n    '''\n    metadata = {'render.modes': ['human', 'rgb_array']}\n\n    def __init__(self, cfg_name, repeat=1):\n        super().__init__()\n        self.game = DoomGame()\n        self.game.load_config(f'./slm_lab/env/vizdoom/cfgs/{cfg_name}.cfg')\n        self._viewer = None\n        self.repeat = 1\n        # TODO In future, need to update action to handle (continuous) DELTA buttons using gym's Box space\n        self.action_space = spaces.MultiDiscrete([2] * self.game.get_available_buttons_size())\n        self.action_space.dtype = 'uint8'\n        output_shape = (self.game.get_screen_channels(), self.game.get_screen_height(), self.game.get_screen_width())\n        self.observation_space = spaces.Box(low=0, high=255, shape=output_shape, dtype='uint8')\n        self.game.init()\n\n    def close(self):\n        self.game.close()\n        if self._viewer is not None:\n            self._viewer.close()\n            self._viewer = None\n\n    def seed(self, seed=None):\n        self.game.set_seed(seed)\n\n    def step(self, action):\n        reward = self.game.make_action(list(action), self.repeat)\n        state = self.game.get_state()\n        done = self.game.is_episode_finished()\n        # info = self._get_game_variables(state.game_variables)\n        info = {}\n        if state is not None:\n            observation = state.screen_buffer\n        else:\n            observation = np.zeros(shape=self.observation_space.shape, dtype=np.uint8)\n        return observation, reward, done, info\n\n    def reset(self):\n        self.game.new_episode()\n        return self.game.get_state().screen_buffer\n\n    def render(self, mode='human', close=False):\n        if close:\n            if self._viewer is not None:\n                self._viewer.close()\n                self._viewer = None\n            return\n        img = None\n        state = self.game.get_state()\n        if state is not None:\n            img = state.screen_buffer\n        if img is None:\n            # at the end of the episode\n            img = np.zeros(shape=self.observation_space.shape, dtype=np.uint8)\n        if mode == 'rgb_array':\n            return img\n        elif mode is 'human':\n            if self._viewer is None:\n                self._viewer = rendering.SimpleImageViewer()\n            self._viewer.imshow(util.to_opencv_image(img))\n\n    def _get_game_variables(self, state_variables):\n        info = {}\n        if state_variables is not None:\n            info['KILLCOUNT'] = state_variables[0]\n            info['ITEMCOUNT'] = state_variables[1]\n            info['SECRETCOUNT'] = state_variables[2]\n            info['FRAGCOUNT'] = state_variables[3]\n            info['HEALTH'] = state_variables[4]\n            info['ARMOR'] = state_variables[5]\n            info['DEAD'] = state_variables[6]\n            info['ON_GROUND'] = state_variables[7]\n            info['ATTACK_READY'] = state_variables[8]\n            info['ALTATTACK_READY'] = state_variables[9]\n            info['SELECTED_WEAPON'] = state_variables[10]\n            info['SELECTED_WEAPON_AMMO'] = state_variables[11]\n            info['AMMO1'] = state_variables[12]\n            info['AMMO2'] = state_variables[13]\n            info['AMMO3'] = state_variables[14]\n            info['AMMO4'] = state_variables[15]\n            info['AMMO5'] = state_variables[16]\n            info['AMMO6'] = state_variables[17]\n            info['AMMO7'] = state_variables[18]\n            info['AMMO8'] = state_variables[19]\n            info['AMMO9'] = state_variables[20]\n            info['AMMO0'] = state_variables[21]\n        return info\n"""
test/agent/memory/test_onpolicy_memory.py,0,"b""from collections import Counter\nfrom flaky import flaky\nimport numpy as np\nimport pytest\n\n\ndef memory_init_util(memory):\n    assert memory.size == 0\n    assert memory.seen_size == 0\n    return True\n\n\ndef memory_reset_util(memory, experiences):\n    memory.reset()\n    for i in range(2):\n        e = experiences[i]\n        memory.add_experience(*e)\n    memory.reset()\n    assert memory.size == 0\n    assert np.sum(memory.states) == 0\n    assert np.sum(memory.actions) == 0\n    assert np.sum(memory.rewards) == 0\n    assert np.sum(memory.next_states) == 0\n    assert np.sum(memory.dones) == 0\n    return True\n\n\nclass TestOnPolicyBatchMemory:\n    '''\n    Class for unit testing OnPolicyBatchReplay memory\n    Note: each test examples from test_memory consists of\n          a tuple containing three elements:\n          (memory, batch_size, experiences)\n    '''\n\n    def test_memory_init(self, test_on_policy_batch_memory):\n        memory = test_on_policy_batch_memory[0]\n        assert memory_init_util(memory)\n\n    def test_add_experience(self, test_on_policy_batch_memory):\n        '''Adds an experience to the memory.\n        Checks that memory size = 1, and checks that the experience values are equal to the experience added'''\n        memory = test_on_policy_batch_memory[0]\n        memory.reset()\n        experiences = test_on_policy_batch_memory[2]\n        exp = experiences[0]\n        memory.add_experience(*exp)\n        assert memory.size == 1\n        assert len(memory.states) == 1\n        # Handle states and actions with multiple dimensions\n        assert np.array_equal(memory.states[-1], exp[0])\n        assert memory.rewards[-1] == exp[1]\n        assert memory.actions[-1] == exp[2]\n        assert np.array_equal(memory.next_states[-1], exp[3])\n        assert memory.dones[-1] == exp[4]\n\n    def test_sample(self, test_on_policy_batch_memory):\n        '''Tests that a sample of batch size is returned with the correct dimensions'''\n        memory = test_on_policy_batch_memory[0]\n        memory.reset()\n        batch_size = test_on_policy_batch_memory[1]\n        experiences = test_on_policy_batch_memory[2]\n        size = len(experiences)\n        for e in experiences:\n            memory.add_experience(*e)\n        batch = memory.sample()\n        assert len(batch['states']) == size\n        assert len(batch['rewards']) == size\n        assert len(batch['next_states']) == size\n        assert len(batch['actions']) == size\n        assert len(batch['dones']) == size\n        assert len(memory.states) == 0\n\n    def test_batch_size(self, test_on_policy_batch_memory):\n        '''Tests that memory sets agent training flag correctly'''\n        memory = test_on_policy_batch_memory[0]\n        memory.reset()\n        memory.body.agent.algorithm.to_train = 0\n        batch_size = test_on_policy_batch_memory[1]\n        experiences = test_on_policy_batch_memory[2]\n        size = len(experiences)\n        for i, e in enumerate(experiences):\n            if i == batch_size:\n                break\n            else:\n                memory.add_experience(*e)\n        assert memory.body.agent.algorithm.to_train == 1\n\n    def test_reset(self, test_on_policy_batch_memory):\n        '''Tests memory reset.\n        Adds 2 experiences, then resets the memory and checks if all appropriate values have been zeroed'''\n        memory = test_on_policy_batch_memory[0]\n        experiences = test_on_policy_batch_memory[2]\n        assert memory_reset_util(memory, experiences)\n\n\nclass TestOnPolicyMemory:\n    '''\n    Class for unit testing OnPolicyReplay memory\n    Note: each test examples from test_memory consists of\n          a tuple containing three elements:\n          (memory, batch_size, experiences)\n    '''\n\n    def test_memory_init(self, test_on_policy_episodic_memory):\n        memory = test_on_policy_episodic_memory[0]\n        assert memory_init_util(memory)\n\n    def test_add_experience(self, test_on_policy_episodic_memory):\n        '''Adds an experience to the memory.\n        Checks that memory size = 1, and checks that the experience values are equal to the experience added'''\n        memory = test_on_policy_episodic_memory[0]\n        memory.reset()\n        experiences = test_on_policy_episodic_memory[2]\n        exp = experiences[0]\n        memory.add_experience(*exp)\n        assert memory.size == 1\n        assert len(memory.states) == 0\n        # Handle states and actions with multiple dimensions\n        assert np.array_equal(memory.cur_epi_data['states'][-1], exp[0])\n        assert memory.cur_epi_data['rewards'][-1] == exp[1]\n        assert memory.cur_epi_data['actions'][-1] == exp[2]\n        assert np.array_equal(memory.cur_epi_data['next_states'][-1], exp[3])\n        assert memory.cur_epi_data['dones'][-1] == exp[4]\n\n    def test_sample(self, test_on_policy_episodic_memory):\n        '''Tests that a sample of batch size is returned with the correct dimensions'''\n        memory = test_on_policy_episodic_memory[0]\n        memory.reset()\n        batch_size = test_on_policy_episodic_memory[1]\n        experiences = test_on_policy_episodic_memory[2]\n        size = len(experiences)\n        for e in experiences:\n            memory.add_experience(*e)\n        batch = memory.sample()\n        assert len(batch['states'][0]) == size\n        assert len(batch['rewards'][0]) == size\n        assert len(batch['next_states'][0]) == size\n        assert len(batch['actions'][0]) == size\n        assert len(batch['dones'][0]) == size\n        assert len(memory.states) == 0\n\n    def test_batch_size(self, test_on_policy_episodic_memory):\n        '''Tests that memory sets agent training flag correctly'''\n        memory = test_on_policy_episodic_memory[0]\n        memory.reset()\n        memory.body.agent.algorithm.to_train = 0\n        batch_size = test_on_policy_episodic_memory[1]\n        experiences = test_on_policy_episodic_memory[2]\n        size = len(experiences)\n        for e in experiences:\n            assert memory.body.agent.algorithm.to_train == 0\n            memory.add_experience(*e)\n        assert memory.body.agent.algorithm.to_train == 1\n\n    def test_multiple_epis_samples(self, test_on_policy_episodic_memory):\n        '''Tests that a sample of batch size is returned with the correct number of episodes'''\n        memory = test_on_policy_episodic_memory[0]\n        memory.reset()\n        batch_size = test_on_policy_episodic_memory[1]\n        experiences = test_on_policy_episodic_memory[2]\n        size = len(experiences)\n        for i in range(3):\n            for e in experiences:\n                memory.add_experience(*e)\n        batch = memory.sample()\n        assert len(batch['states']) == 3\n        assert len(batch['rewards']) == 3\n        assert len(batch['next_states']) == 3\n        assert len(batch['actions']) == 3\n        assert len(batch['dones']) == 3\n        assert len(batch['states'][0]) == size\n        assert len(batch['states'][1]) == size\n        assert len(batch['states'][2]) == size\n        assert len(memory.states) == 0\n\n    def test_reset(self, test_on_policy_episodic_memory):\n        '''Tests memory reset.\n        Adds 2 experiences, then resets the memory and checks if all appropriate values have been zeroed'''\n        memory = test_on_policy_episodic_memory[0]\n        experiences = test_on_policy_episodic_memory[2]\n        assert memory_reset_util(memory, experiences)\n"""
test/agent/memory/test_per_memory.py,0,"b""from collections import Counter\nfrom flaky import flaky\nimport numpy as np\nimport pytest\n\n\n@flaky\nclass TestPERMemory:\n    '''\n    Class for unit testing prioritized replay memory\n    Note: each test examples from test_prioritized_replay_memory consists of\n          a tuple containing three elements:\n          (memory, batch_size, experiences)\n    '''\n\n    def test_prioritized_replay_memory_init(self, test_prioritized_replay_memory):\n        memory = test_prioritized_replay_memory[0]\n        memory.reset()\n        assert memory.size == 0\n        assert len(memory.states) == memory.max_size\n        assert len(memory.actions) == memory.max_size\n        assert len(memory.rewards) == memory.max_size\n        assert len(memory.dones) == memory.max_size\n        assert len(memory.priorities) == memory.max_size\n        assert memory.tree.write == 0\n        assert memory.tree.total() == 0\n        assert memory.epsilon[0] == 0\n        assert memory.alpha[0] == 1\n\n    def test_add_experience(self, test_prioritized_replay_memory):\n        '''Adds an experience to the memory. Checks that memory size = 1, and checks that the experience values are equal to the experience added'''\n        memory = test_prioritized_replay_memory[0]\n        memory.reset()\n        experiences = test_prioritized_replay_memory[2]\n        exp = experiences[0]\n        memory.add_experience(*exp)\n        assert memory.size == 1\n        assert memory.head == 0\n        # Handle states and actions with multiple dimensions\n        assert np.array_equal(memory.states[memory.head], exp[0])\n        assert memory.actions[memory.head] == exp[1]\n        assert memory.rewards[memory.head] == exp[2]\n        assert np.array_equal(memory.ns_buffer[0], exp[3])\n        assert memory.dones[memory.head] == exp[4]\n        assert memory.priorities[memory.head] == 1000\n\n    def test_wrap(self, test_prioritized_replay_memory):\n        '''Tests that the memory wraps round when it is at capacity'''\n        memory = test_prioritized_replay_memory[0]\n        memory.reset()\n        experiences = test_prioritized_replay_memory[2]\n        num_added = 0\n        for e in experiences:\n            memory.add_experience(*e)\n            num_added += 1\n            assert memory.size == min(memory.max_size, num_added)\n            assert memory.head == (num_added - 1) % memory.max_size\n            write = (num_added - 1) % memory.max_size + 1\n            if write == memory.max_size:\n                write = 0\n            assert memory.tree.write == write\n\n    def test_sample(self, test_prioritized_replay_memory):\n        '''Tests that a sample of batch size is returned with the correct dimensions'''\n        memory = test_prioritized_replay_memory[0]\n        memory.reset()\n        batch_size = test_prioritized_replay_memory[1]\n        experiences = test_prioritized_replay_memory[2]\n        for e in experiences:\n            memory.add_experience(*e)\n        batch = memory.sample()\n        assert batch['states'].shape == (batch_size, memory.body.state_dim)\n        assert batch['actions'].shape == (batch_size,)\n        assert batch['rewards'].shape == (batch_size,)\n        assert batch['next_states'].shape == (batch_size, memory.body.state_dim)\n        assert batch['dones'].shape == (batch_size,)\n        assert batch['priorities'].shape == (batch_size,)\n\n    def test_sample_distribution(self, test_prioritized_replay_memory):\n        '''Tests if batch conforms to prioritized distribution'''\n        memory = test_prioritized_replay_memory[0]\n        memory.reset()\n        batch_size = test_prioritized_replay_memory[1]\n        experiences = test_prioritized_replay_memory[2]\n        for e in experiences:\n            memory.add_experience(*e)\n        batch = memory.sample()\n        assert 0 in memory.batch_idxs\n        assert 3 in memory.batch_idxs\n        assert 1 not in memory.batch_idxs\n        assert 2 not in memory.batch_idxs\n\n    def test_reset(self, test_prioritized_replay_memory):\n        '''Tests memory reset. Adds 2 experiences, then resets the memory and checks if all appropriate values have been zeroed'''\n        memory = test_prioritized_replay_memory[0]\n        memory.reset()\n        experiences = test_prioritized_replay_memory[2]\n        for i in range(2):\n            e = experiences[i]\n            memory.add_experience(*e)\n        memory.reset()\n        assert memory.head == -1\n        assert memory.size == 0\n        assert memory.states[0] is None\n        assert memory.actions[0] is None\n        assert memory.rewards[0] is None\n        assert memory.dones[0] is None\n        assert memory.priorities[0] is None\n        assert len(memory.ns_buffer) == 0\n        assert memory.tree.write == 0\n        assert memory.tree.total() == 0\n\n    def test_update_priorities(self, test_prioritized_replay_memory):\n        '''Samples from memory, and updates priorities twice. Each time checks that the priorities are updated'''\n        memory = test_prioritized_replay_memory[0]\n        memory.reset()\n        batch_size = test_prioritized_replay_memory[1]\n        experiences = test_prioritized_replay_memory[2]\n        for e in experiences:\n            memory.add_experience(*e)\n        print(f'memory.priorities: {memory.priorities}')\n        batch = memory.sample()\n        # First update\n        # Manually change tree idxs and batch idxs\n        memory.batch_idxs = np.asarray([0, 1, 2, 3]).astype(int)\n        memory.tree_idxs = [3, 4, 5, 6]\n        print(f'batch_size: {batch_size}, batch_idxs: {memory.batch_idxs}, tree_idxs: {memory.tree_idxs}')\n        new_errors = np.array([0, 10, 10, 20], dtype=np.float32)\n        print(f'new_errors: {new_errors}')\n        memory.update_priorities(new_errors)\n        memory.tree.print_tree()\n        print(f'memory.priorities: {memory.priorities}')\n        assert memory.priorities[0] == 0\n        assert memory.priorities[1] == 10\n        assert memory.priorities[2] == 10\n        assert memory.priorities[3] == 20\n        # Second update\n        new_errors = np.array([90, 0, 30, 0], dtype=np.float32)\n        # Manually change tree idxs and batch idxs\n        memory.batch_idxs = np.asarray([0, 1, 2, 3]).astype(int)\n        memory.tree_idxs = [3, 4, 5, 6]\n        print(f'new_errors: {new_errors}')\n        memory.update_priorities(new_errors)\n        memory.tree.print_tree()\n        print(f'memory.priorities: {memory.priorities}')\n        assert memory.priorities[0] == 90\n        assert memory.priorities[1] == 0\n        assert memory.priorities[2] == 30\n        assert memory.priorities[3] == 0\n"""
test/agent/memory/test_replay_memory.py,0,"b'from collections import deque\nfrom copy import deepcopy\nfrom flaky import flaky\nfrom slm_lab.agent.memory.replay import sample_next_states\nimport numpy as np\nimport pytest\n\n\ndef test_sample_next_states():\n    # for each state, its next state is itself + 10\n    head = 1\n    max_size = 9\n    ns_idx_offset = 3\n    batch_idxs = np.arange(max_size)\n    states = [31, 32, 10, 11, 12, 20, 21, 22, 30]\n    ns_buffer = deque([40, 41, 42], maxlen=ns_idx_offset)\n    ns = sample_next_states(head, max_size, ns_idx_offset, batch_idxs, states, ns_buffer)\n    res = np.array([41, 42, 20, 21, 22, 30, 31, 32, 40])\n    assert np.array_equal(ns, res)\n\n\n@flaky\nclass TestMemory:\n    \'\'\'\n    Base class for unit testing replay memory\n    Note: each test examples from test_memory consists of\n          a tuple containing three elements:\n          (memory, batch_size, experiences)\n    \'\'\'\n\n    def test_memory_init(self, test_memory):\n        memory = test_memory[0]\n        memory.reset()\n        assert memory.size == 0\n        assert len(memory.states) == memory.max_size\n        assert len(memory.actions) == memory.max_size\n        assert len(memory.rewards) == memory.max_size\n        assert len(memory.dones) == memory.max_size\n\n    def test_add_experience(self, test_memory):\n        \'\'\'Adds an experience to the memory. Checks that memory size = 1, and checks that the experience values are equal to the experience added\'\'\'\n        memory = test_memory[0]\n        memory.reset()\n        experiences = test_memory[2]\n        exp = experiences[0]\n        memory.add_experience(*exp)\n        assert memory.size == 1\n        assert memory.head == 0\n        # Handle states and actions with multiple dimensions\n        assert np.array_equal(memory.states[memory.head], exp[0])\n        assert memory.actions[memory.head] == exp[1]\n        assert memory.rewards[memory.head] == exp[2]\n        assert np.array_equal(memory.ns_buffer[0], exp[3])\n        assert memory.dones[memory.head] == exp[4]\n\n    def test_wrap(self, test_memory):\n        \'\'\'Tests that the memory wraps round when it is at capacity\'\'\'\n        memory = test_memory[0]\n        memory.reset()\n        experiences = test_memory[2]\n        num_added = 0\n        for e in experiences:\n            memory.add_experience(*e)\n            num_added += 1\n            assert memory.size == min(memory.max_size, num_added)\n            assert memory.head == (num_added - 1) % memory.max_size\n\n    def test_sample(self, test_memory):\n        \'\'\'Tests that a sample of batch size is returned with the correct dimensions\'\'\'\n        memory = test_memory[0]\n        memory.reset()\n        batch_size = test_memory[1]\n        experiences = test_memory[2]\n        for e in experiences:\n            memory.add_experience(*e)\n        memory.batch_size = batch_size\n        batch = memory.sample()\n        assert batch[\'states\'].shape == (batch_size, memory.body.state_dim)\n        assert batch[\'actions\'].shape == (batch_size,)\n        assert batch[\'rewards\'].shape == (batch_size,)\n        assert batch[\'next_states\'].shape == (batch_size, memory.body.state_dim)\n        assert batch[\'dones\'].shape == (batch_size,)\n\n    @flaky(max_runs=10)\n    def test_sample_changes(self, test_memory):\n        \'\'\'Tests if memory.current_batch_indices changes from sample to sample\'\'\'\n        memory = test_memory[0]\n        memory.reset()\n        batch_size = test_memory[1]\n        experiences = test_memory[2]\n        for e in experiences:\n            memory.add_experience(*e)\n        memory.batch_size = batch_size\n        _batch = memory.sample()\n        old_idx = deepcopy(memory.batch_idxs).tolist()\n        for i in range(5):\n            _batch = memory.sample()\n            new_idx = memory.batch_idxs.tolist()\n            assert old_idx != new_idx\n            old_idx = deepcopy(memory.batch_idxs).tolist()\n\n    def test_sample_next_states(self, test_memory):\n        memory = test_memory[0]\n        experiences = test_memory[2]\n        for e in experiences:\n            memory.add_experience(*e)\n        idxs = np.arange(memory.size)  # for any self.head\n        next_states = sample_next_states(memory.head, memory.max_size, memory.ns_idx_offset, idxs, memory.states, memory.ns_buffer)\n        # check self.head actually samples from ns_buffer\n        assert np.array_equal(next_states[memory.head], memory.ns_buffer[0])\n\n    def test_reset(self, test_memory):\n        \'\'\'Tests memory reset. Adds 2 experiences, then resets the memory and checks if all appropriate values have been zeroed\'\'\'\n        memory = test_memory[0]\n        memory.reset()\n        experiences = test_memory[2]\n        for i in range(2):\n            e = experiences[i]\n            memory.add_experience(*e)\n        memory.reset()\n        assert memory.head == -1\n        assert memory.size == 0\n        assert memory.states[0] is None\n        assert memory.actions[0] is None\n        assert memory.rewards[0] is None\n        assert memory.dones[0] is None\n        assert len(memory.ns_buffer) == 0\n\n    @pytest.mark.skip(reason=""Not implemented yet"")\n    def test_sample_dist(self, test_memory):\n        \'\'\'Samples 100 times from memory. Accumulates the indices sampled and checks for significant deviation from a uniform distribution\'\'\'\n        # TODO test_sample_dist\n        assert None is None\n'"
test/agent/net/test_conv.py,3,"b'from copy import deepcopy\nfrom slm_lab.env.base import Clock\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.agent.net.conv import ConvNet\nimport torch\nimport torch.nn as nn\n\nnet_spec = {\n    ""type"": ""ConvNet"",\n    ""shared"": True,\n    ""conv_hid_layers"": [\n        [32, 8, 4, 0, 1],\n        [64, 4, 2, 0, 1],\n        [64, 3, 1, 0, 1]\n    ],\n    ""fc_hid_layers"": [512],\n    ""hid_layers_activation"": ""relu"",\n    ""init_fn"": ""xavier_uniform_"",\n    ""batch_norm"": False,\n    ""clip_grad_val"": 1.0,\n    ""loss_spec"": {\n        ""name"": ""SmoothL1Loss""\n    },\n    ""optim_spec"": {\n        ""name"": ""Adam"",\n        ""lr"": 0.02\n    },\n    ""lr_scheduler_spec"": {\n        ""name"": ""StepLR"",\n        ""step_size"": 30,\n        ""gamma"": 0.1\n    },\n    ""gpu"": True\n}\nin_dim = (4, 84, 84)\nout_dim = 3\nbatch_size = 16\nnet = ConvNet(net_spec, in_dim, out_dim)\n# init net optimizer and its lr scheduler\noptim = net_util.get_optim(net, net.optim_spec)\nlr_scheduler = net_util.get_lr_scheduler(optim, net.lr_scheduler_spec)\nx = torch.rand((batch_size,) + in_dim)\n\n\ndef test_init():\n    net = ConvNet(net_spec, in_dim, out_dim)\n    assert isinstance(net, nn.Module)\n    assert hasattr(net, \'conv_model\')\n    assert hasattr(net, \'fc_model\')\n    assert hasattr(net, \'model_tail\')\n    assert not hasattr(net, \'model_tails\')\n\n\ndef test_forward():\n    y = net.forward(x)\n    assert y.shape == (batch_size, out_dim)\n\n\ndef test_train_step():\n    y = torch.rand((batch_size, out_dim))\n    clock = Clock(100, 1)\n    loss = net.loss_fn(net.forward(x), y)\n    net.train_step(loss, optim, lr_scheduler, clock=clock)\n    assert loss != 0.0\n\n\ndef test_no_fc():\n    no_fc_net_spec = deepcopy(net_spec)\n    no_fc_net_spec[\'fc_hid_layers\'] = []\n    net = ConvNet(no_fc_net_spec, in_dim, out_dim)\n    assert isinstance(net, nn.Module)\n    assert hasattr(net, \'conv_model\')\n    assert not hasattr(net, \'fc_model\')\n    assert hasattr(net, \'model_tail\')\n    assert not hasattr(net, \'model_tails\')\n\n    y = net.forward(x)\n    assert y.shape == (batch_size, out_dim)\n\n\ndef test_multitails():\n    net = ConvNet(net_spec, in_dim, [3, 4])\n    assert isinstance(net, nn.Module)\n    assert hasattr(net, \'conv_model\')\n    assert hasattr(net, \'fc_model\')\n    assert not hasattr(net, \'model_tail\')\n    assert hasattr(net, \'model_tails\')\n    assert len(net.model_tails) == 2\n\n    y = net.forward(x)\n    assert len(y) == 2\n    assert y[0].shape == (batch_size, 3)\n    assert y[1].shape == (batch_size, 4)\n'"
test/agent/net/test_mlp.py,3,"b'from copy import deepcopy\nfrom slm_lab.env.base import Clock\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.agent.net.mlp import MLPNet\nimport torch\nimport torch.nn as nn\n\nnet_spec = {\n    ""type"": ""MLPNet"",\n    ""shared"": True,\n    ""hid_layers"": [32],\n    ""hid_layers_activation"": ""relu"",\n    ""init_fn"": ""xavier_uniform_"",\n    ""clip_grad_val"": 1.0,\n    ""loss_spec"": {\n        ""name"": ""MSELoss""\n    },\n    ""optim_spec"": {\n        ""name"": ""Adam"",\n        ""lr"": 0.02\n    },\n    ""lr_scheduler_spec"": {\n        ""name"": ""StepLR"",\n        ""step_size"": 30,\n        ""gamma"": 0.1\n    },\n    ""update_type"": ""replace"",\n    ""update_frequency"": 1,\n    ""polyak_coef"": 0.9,\n    ""gpu"": True\n}\nin_dim = 10\nout_dim = 3\nbatch_size = 16\nnet = MLPNet(net_spec, in_dim, out_dim)\n# init net optimizer and its lr scheduler\noptim = net_util.get_optim(net, net.optim_spec)\nlr_scheduler = net_util.get_lr_scheduler(optim, net.lr_scheduler_spec)\nx = torch.rand((batch_size, in_dim))\n\n\ndef test_init():\n    net = MLPNet(net_spec, in_dim, out_dim)\n    assert isinstance(net, nn.Module)\n    assert hasattr(net, \'model\')\n    assert hasattr(net, \'model_tail\')\n    assert not hasattr(net, \'model_tails\')\n\n\ndef test_forward():\n    y = net.forward(x)\n    assert y.shape == (batch_size, out_dim)\n\n\ndef test_train_step():\n    y = torch.rand((batch_size, out_dim))\n    clock = Clock(100, 1)\n    loss = net.loss_fn(net.forward(x), y)\n    net.train_step(loss, optim, lr_scheduler, clock=clock)\n    assert loss != 0.0\n\n\ndef test_no_lr_scheduler():\n    nopo_lrs_net_spec = deepcopy(net_spec)\n    nopo_lrs_net_spec[\'lr_scheduler_spec\'] = None\n    net = MLPNet(nopo_lrs_net_spec, in_dim, out_dim)\n    assert isinstance(net, nn.Module)\n    assert hasattr(net, \'model\')\n    assert hasattr(net, \'model_tail\')\n    assert not hasattr(net, \'model_tails\')\n\n    y = net.forward(x)\n    assert y.shape == (batch_size, out_dim)\n\n\ndef test_multitails():\n    net = MLPNet(net_spec, in_dim, [3, 4])\n    assert isinstance(net, nn.Module)\n    assert hasattr(net, \'model\')\n    assert not hasattr(net, \'model_tail\')\n    assert hasattr(net, \'model_tails\')\n    assert len(net.model_tails) == 2\n\n    y = net.forward(x)\n    assert len(y) == 2\n    assert y[0].shape == (batch_size, 3)\n    assert y[1].shape == (batch_size, 4)\n'"
test/agent/net/test_recurrent.py,3,"b'from copy import deepcopy\nfrom slm_lab.env.base import Clock\nfrom slm_lab.agent.net import net_util\nfrom slm_lab.agent.net.recurrent import RecurrentNet\nimport pytest\nimport torch\nimport torch.nn as nn\n\nnet_spec = {\n    ""type"": ""RecurrentNet"",\n    ""shared"": True,\n    ""cell_type"": ""GRU"",\n    ""fc_hid_layers"": [10],\n    ""hid_layers_activation"": ""relu"",\n    ""rnn_hidden_size"": 64,\n    ""rnn_num_layers"": 2,\n    ""bidirectional"": False,\n    ""seq_len"": 4,\n    ""init_fn"": ""xavier_uniform_"",\n    ""clip_grad_val"": 1.0,\n    ""loss_spec"": {\n        ""name"": ""SmoothL1Loss""\n    },\n    ""optim_spec"": {\n        ""name"": ""Adam"",\n        ""lr"": 0.02\n    },\n    ""lr_scheduler_spec"": {\n        ""name"": ""StepLR"",\n        ""step_size"": 30,\n        ""gamma"": 0.1\n    },\n    ""gpu"": True\n}\nstate_dim = 10\nout_dim = 3\nbatch_size = 16\nseq_len = net_spec[\'seq_len\']\nin_dim = (seq_len, state_dim)\nnet = RecurrentNet(net_spec, in_dim, out_dim)\n# init net optimizer and its lr scheduler\noptim = net_util.get_optim(net, net.optim_spec)\nlr_scheduler = net_util.get_lr_scheduler(optim, net.lr_scheduler_spec)\nx = torch.rand((batch_size, seq_len, state_dim))\n\n\ndef test_init():\n    net = RecurrentNet(net_spec, in_dim, out_dim)\n    assert isinstance(net, nn.Module)\n    assert hasattr(net, \'fc_model\')\n    assert hasattr(net, \'rnn_model\')\n    assert hasattr(net, \'model_tail\')\n    assert not hasattr(net, \'model_tails\')\n    assert net.rnn_model.bidirectional == False\n\n\ndef test_forward():\n    y = net.forward(x)\n    assert y.shape == (batch_size, out_dim)\n\n\ndef test_train_step():\n    y = torch.rand((batch_size, out_dim))\n    clock = Clock(100, 1)\n    loss = net.loss_fn(net.forward(x), y)\n    net.train_step(loss, optim, lr_scheduler, clock=clock)\n    assert loss != 0.0\n\n\n@pytest.mark.parametrize(\'bidirectional\', (False, True))\n@pytest.mark.parametrize(\'cell_type\', (\'RNN\', \'LSTM\', \'GRU\'))\ndef test_variant(bidirectional, cell_type):\n    var_net_spec = deepcopy(net_spec)\n    var_net_spec[\'bidirectional\'] = bidirectional\n    var_net_spec[\'cell_type\'] = cell_type\n    net = RecurrentNet(var_net_spec, in_dim, out_dim)\n    assert isinstance(net, nn.Module)\n    assert hasattr(net, \'fc_model\')\n    assert hasattr(net, \'rnn_model\')\n    assert hasattr(net, \'model_tail\')\n    assert not hasattr(net, \'model_tails\')\n    assert net.rnn_model.bidirectional == bidirectional\n\n    y = net.forward(x)\n    assert y.shape == (batch_size, out_dim)\n\n\ndef test_no_fc():\n    no_fc_net_spec = deepcopy(net_spec)\n    no_fc_net_spec[\'fc_hid_layers\'] = []\n    net = RecurrentNet(no_fc_net_spec, in_dim, out_dim)\n    assert isinstance(net, nn.Module)\n    assert not hasattr(net, \'fc_model\')\n    assert hasattr(net, \'rnn_model\')\n    assert hasattr(net, \'model_tail\')\n    assert not hasattr(net, \'model_tails\')\n\n    y = net.forward(x)\n    assert y.shape == (batch_size, out_dim)\n\n\ndef test_multitails():\n    net = RecurrentNet(net_spec, in_dim, [3, 4])\n    assert isinstance(net, nn.Module)\n    assert hasattr(net, \'fc_model\')\n    assert hasattr(net, \'rnn_model\')\n    assert not hasattr(net, \'model_tail\')\n    assert hasattr(net, \'model_tails\')\n    assert len(net.model_tails) == 2\n\n    y = net.forward(x)\n    assert len(y) == 2\n    assert y[0].shape == (batch_size, 3)\n    assert y[1].shape == (batch_size, 4)\n'"
slm_lab/env/vizdoom/cfgs/__init__.py,0,b''
