file_path,api_count,code
dataset.py,0,"b""from torchvision import get_image_backend\n\nfrom datasets.videodataset import VideoDataset\nfrom datasets.videodataset_multiclips import (VideoDatasetMultiClips,\n                                              collate_fn)\nfrom datasets.activitynet import ActivityNet\nfrom datasets.loader import VideoLoader, VideoLoaderHDF5, VideoLoaderFlowHDF5\n\n\ndef image_name_formatter(x):\n    return f'image_{x:05d}.jpg'\n\n\ndef get_training_data(video_path,\n                      annotation_path,\n                      dataset_name,\n                      input_type,\n                      file_type,\n                      spatial_transform=None,\n                      temporal_transform=None,\n                      target_transform=None):\n    assert dataset_name in [\n        'kinetics', 'activitynet', 'ucf101', 'hmdb51', 'mit'\n    ]\n    assert input_type in ['rgb', 'flow']\n    assert file_type in ['jpg', 'hdf5']\n\n    if file_type == 'jpg':\n        assert input_type == 'rgb', 'flow input is supported only when input type is hdf5.'\n\n        if get_image_backend() == 'accimage':\n            from datasets.loader import ImageLoaderAccImage\n            loader = VideoLoader(image_name_formatter, ImageLoaderAccImage())\n        else:\n            loader = VideoLoader(image_name_formatter)\n\n        video_path_formatter = (\n            lambda root_path, label, video_id: root_path / label / video_id)\n    else:\n        if input_type == 'rgb':\n            loader = VideoLoaderHDF5()\n        else:\n            loader = VideoLoaderFlowHDF5()\n        video_path_formatter = (lambda root_path, label, video_id: root_path /\n                                label / f'{video_id}.hdf5')\n\n    if dataset_name == 'activitynet':\n        training_data = ActivityNet(video_path,\n                                    annotation_path,\n                                    'training',\n                                    spatial_transform=spatial_transform,\n                                    temporal_transform=temporal_transform,\n                                    target_transform=target_transform,\n                                    video_loader=loader,\n                                    video_path_formatter=video_path_formatter)\n    else:\n        training_data = VideoDataset(video_path,\n                                     annotation_path,\n                                     'training',\n                                     spatial_transform=spatial_transform,\n                                     temporal_transform=temporal_transform,\n                                     target_transform=target_transform,\n                                     video_loader=loader,\n                                     video_path_formatter=video_path_formatter)\n\n    return training_data\n\n\ndef get_validation_data(video_path,\n                        annotation_path,\n                        dataset_name,\n                        input_type,\n                        file_type,\n                        spatial_transform=None,\n                        temporal_transform=None,\n                        target_transform=None):\n    assert dataset_name in [\n        'kinetics', 'activitynet', 'ucf101', 'hmdb51', 'mit'\n    ]\n    assert input_type in ['rgb', 'flow']\n    assert file_type in ['jpg', 'hdf5']\n\n    if file_type == 'jpg':\n        assert input_type == 'rgb', 'flow input is supported only when input type is hdf5.'\n\n        if get_image_backend() == 'accimage':\n            from datasets.loader import ImageLoaderAccImage\n            loader = VideoLoader(image_name_formatter, ImageLoaderAccImage())\n        else:\n            loader = VideoLoader(image_name_formatter)\n\n        video_path_formatter = (\n            lambda root_path, label, video_id: root_path / label / video_id)\n    else:\n        if input_type == 'rgb':\n            loader = VideoLoaderHDF5()\n        else:\n            loader = VideoLoaderFlowHDF5()\n        video_path_formatter = (lambda root_path, label, video_id: root_path /\n                                label / f'{video_id}.hdf5')\n\n    if dataset_name == 'activitynet':\n        validation_data = ActivityNet(video_path,\n                                      annotation_path,\n                                      'validation',\n                                      spatial_transform=spatial_transform,\n                                      temporal_transform=temporal_transform,\n                                      target_transform=target_transform,\n                                      video_loader=loader,\n                                      video_path_formatter=video_path_formatter)\n    else:\n        validation_data = VideoDatasetMultiClips(\n            video_path,\n            annotation_path,\n            'validation',\n            spatial_transform=spatial_transform,\n            temporal_transform=temporal_transform,\n            target_transform=target_transform,\n            video_loader=loader,\n            video_path_formatter=video_path_formatter)\n\n    return validation_data, collate_fn\n\n\ndef get_inference_data(video_path,\n                       annotation_path,\n                       dataset_name,\n                       input_type,\n                       file_type,\n                       inference_subset,\n                       spatial_transform=None,\n                       temporal_transform=None,\n                       target_transform=None):\n    assert dataset_name in [\n        'kinetics', 'activitynet', 'ucf101', 'hmdb51', 'mit'\n    ]\n    assert input_type in ['rgb', 'flow']\n    assert file_type in ['jpg', 'hdf5']\n    assert inference_subset in ['train', 'val', 'test']\n\n    if file_type == 'jpg':\n        assert input_type == 'rgb', 'flow input is supported only when input type is hdf5.'\n\n        if get_image_backend() == 'accimage':\n            from datasets.loader import ImageLoaderAccImage\n            loader = VideoLoader(image_name_formatter, ImageLoaderAccImage())\n        else:\n            loader = VideoLoader(image_name_formatter)\n\n        video_path_formatter = (\n            lambda root_path, label, video_id: root_path / label / video_id)\n    else:\n        if input_type == 'rgb':\n            loader = VideoLoaderHDF5()\n        else:\n            loader = VideoLoaderFlowHDF5()\n        video_path_formatter = (lambda root_path, label, video_id: root_path /\n                                label / f'{video_id}.hdf5')\n\n    if inference_subset == 'train':\n        subset = 'training'\n    elif inference_subset == 'val':\n        subset = 'validation'\n    elif inference_subset == 'test':\n        subset = 'testing'\n    if dataset_name == 'activitynet':\n        inference_data = ActivityNet(video_path,\n                                     annotation_path,\n                                     subset,\n                                     spatial_transform=spatial_transform,\n                                     temporal_transform=temporal_transform,\n                                     target_transform=target_transform,\n                                     video_loader=loader,\n                                     video_path_formatter=video_path_formatter,\n                                     is_untrimmed_setting=True)\n    else:\n        inference_data = VideoDatasetMultiClips(\n            video_path,\n            annotation_path,\n            subset,\n            spatial_transform=spatial_transform,\n            temporal_transform=temporal_transform,\n            target_transform=target_transform,\n            video_loader=loader,\n            video_path_formatter=video_path_formatter,\n            target_type=['video_id', 'segment'])\n\n    return inference_data, collate_fn"""
inference.py,5,"b""import time\nimport json\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn.functional as F\n\nfrom utils import AverageMeter\n\n\ndef get_video_results(outputs, class_names, output_topk):\n    sorted_scores, locs = torch.topk(outputs,\n                                     k=min(output_topk, len(class_names)))\n\n    video_results = []\n    for i in range(sorted_scores.size(0)):\n        video_results.append({\n            'label': class_names[locs[i].item()],\n            'score': sorted_scores[i].item()\n        })\n\n    return video_results\n\n\ndef inference(data_loader, model, result_path, class_names, no_average,\n              output_topk):\n    print('inference')\n\n    model.eval()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    results = {'results': defaultdict(list)}\n\n    end_time = time.time()\n\n    with torch.no_grad():\n        for i, (inputs, targets) in enumerate(data_loader):\n            data_time.update(time.time() - end_time)\n\n            video_ids, segments = zip(*targets)\n            outputs = model(inputs)\n            outputs = F.softmax(outputs, dim=1).cpu()\n\n            for j in range(outputs.size(0)):\n                results['results'][video_ids[j]].append({\n                    'segment': segments[j],\n                    'output': outputs[j]\n                })\n\n            batch_time.update(time.time() - end_time)\n            end_time = time.time()\n\n            print('[{}/{}]\\t'\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'.format(\n                      i + 1,\n                      len(data_loader),\n                      batch_time=batch_time,\n                      data_time=data_time))\n\n    inference_results = {'results': {}}\n    if not no_average:\n        for video_id, video_results in results['results'].items():\n            video_outputs = [\n                segment_result['output'] for segment_result in video_results\n            ]\n            video_outputs = torch.stack(video_outputs)\n            average_scores = torch.mean(video_outputs, dim=0)\n            inference_results['results'][video_id] = get_video_results(\n                average_scores, class_names, output_topk)\n    else:\n        for video_id, video_results in results['results'].items():\n            inference_results['results'][video_id] = []\n            for segment_result in video_results:\n                segment = segment_result['segment']\n                result = get_video_results(segment_result['output'],\n                                           class_names, output_topk)\n                inference_results['results'][video_id].append({\n                    'segment': segment,\n                    'result': result\n                })\n\n    with result_path.open('w') as f:\n        json.dump(inference_results, f)\n"""
main.py,19,"b'from pathlib import Path\nimport json\nimport random\nimport os\n\nimport numpy as np\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import SGD, lr_scheduler\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.backends import cudnn\nimport torchvision\n\nfrom opts import parse_opts\nfrom model import (generate_model, load_pretrained_model, make_data_parallel,\n                   get_fine_tuning_parameters)\nfrom mean import get_mean_std\nfrom spatial_transforms import (Compose, Normalize, Resize, CenterCrop,\n                                CornerCrop, MultiScaleCornerCrop,\n                                RandomResizedCrop, RandomHorizontalFlip,\n                                ToTensor, ScaleValue, ColorJitter,\n                                PickFirstChannels)\nfrom temporal_transforms import (LoopPadding, TemporalRandomCrop,\n                                 TemporalCenterCrop, TemporalEvenCrop,\n                                 SlidingWindow, TemporalSubsampling)\nfrom temporal_transforms import Compose as TemporalCompose\nfrom dataset import get_training_data, get_validation_data, get_inference_data\nfrom utils import Logger, worker_init_fn, get_lr\nfrom training import train_epoch\nfrom validation import val_epoch\nimport inference\n\n\ndef json_serial(obj):\n    if isinstance(obj, Path):\n        return str(obj)\n\n\ndef get_opt():\n    opt = parse_opts()\n\n    if opt.root_path is not None:\n        opt.video_path = opt.root_path / opt.video_path\n        opt.annotation_path = opt.root_path / opt.annotation_path\n        opt.result_path = opt.root_path / opt.result_path\n        if opt.resume_path is not None:\n            opt.resume_path = opt.root_path / opt.resume_path\n        if opt.pretrain_path is not None:\n            opt.pretrain_path = opt.root_path / opt.pretrain_path\n\n    if opt.pretrain_path is not None:\n        opt.n_finetune_classes = opt.n_classes\n        opt.n_classes = opt.n_pretrain_classes\n\n    if opt.output_topk <= 0:\n        opt.output_topk = opt.n_classes\n\n    if opt.inference_batch_size == 0:\n        opt.inference_batch_size = opt.batch_size\n\n    opt.arch = \'{}-{}\'.format(opt.model, opt.model_depth)\n    opt.begin_epoch = 1\n    opt.mean, opt.std = get_mean_std(opt.value_scale, dataset=opt.mean_dataset)\n    opt.n_input_channels = 3\n    if opt.input_type == \'flow\':\n        opt.n_input_channels = 2\n        opt.mean = opt.mean[:2]\n        opt.std = opt.std[:2]\n\n    if opt.distributed:\n        opt.dist_rank = int(os.environ[""OMPI_COMM_WORLD_RANK""])\n\n        if opt.dist_rank == 0:\n            print(opt)\n            with (opt.result_path / \'opts.json\').open(\'w\') as opt_file:\n                json.dump(vars(opt), opt_file, default=json_serial)\n    else:\n        print(opt)\n        with (opt.result_path / \'opts.json\').open(\'w\') as opt_file:\n            json.dump(vars(opt), opt_file, default=json_serial)\n\n    return opt\n\n\ndef resume_model(resume_path, arch, model):\n    print(\'loading checkpoint {} model\'.format(resume_path))\n    checkpoint = torch.load(resume_path, map_location=\'cpu\')\n    assert arch == checkpoint[\'arch\']\n\n    if hasattr(model, \'module\'):\n        model.module.load_state_dict(checkpoint[\'state_dict\'])\n    else:\n        model.load_state_dict(checkpoint[\'state_dict\'])\n\n    return model\n\n\ndef resume_train_utils(resume_path, begin_epoch, optimizer, scheduler):\n    print(\'loading checkpoint {} train utils\'.format(resume_path))\n    checkpoint = torch.load(resume_path, map_location=\'cpu\')\n\n    begin_epoch = checkpoint[\'epoch\'] + 1\n    if optimizer is not None and \'optimizer\' in checkpoint:\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n    if scheduler is not None and \'scheduler\' in checkpoint:\n        scheduler.load_state_dict(checkpoint[\'scheduler\'])\n\n    return begin_epoch, optimizer, scheduler\n\n\ndef get_normalize_method(mean, std, no_mean_norm, no_std_norm):\n    if no_mean_norm:\n        if no_std_norm:\n            return Normalize([0, 0, 0], [1, 1, 1])\n        else:\n            return Normalize([0, 0, 0], std)\n    else:\n        if no_std_norm:\n            return Normalize(mean, [1, 1, 1])\n        else:\n            return Normalize(mean, std)\n\n\ndef get_train_utils(opt, model_parameters):\n    assert opt.train_crop in [\'random\', \'corner\', \'center\']\n    spatial_transform = []\n    if opt.train_crop == \'random\':\n        spatial_transform.append(\n            RandomResizedCrop(\n                opt.sample_size, (opt.train_crop_min_scale, 1.0),\n                (opt.train_crop_min_ratio, 1.0 / opt.train_crop_min_ratio)))\n    elif opt.train_crop == \'corner\':\n        scales = [1.0]\n        scale_step = 1 / (2**(1 / 4))\n        for _ in range(1, 5):\n            scales.append(scales[-1] * scale_step)\n        spatial_transform.append(MultiScaleCornerCrop(opt.sample_size, scales))\n    elif opt.train_crop == \'center\':\n        spatial_transform.append(Resize(opt.sample_size))\n        spatial_transform.append(CenterCrop(opt.sample_size))\n    normalize = get_normalize_method(opt.mean, opt.std, opt.no_mean_norm,\n                                     opt.no_std_norm)\n    if not opt.no_hflip:\n        spatial_transform.append(RandomHorizontalFlip())\n    if opt.colorjitter:\n        spatial_transform.append(ColorJitter())\n    spatial_transform.append(ToTensor())\n    if opt.input_type == \'flow\':\n        spatial_transform.append(PickFirstChannels(n=2))\n    spatial_transform.append(ScaleValue(opt.value_scale))\n    spatial_transform.append(normalize)\n    spatial_transform = Compose(spatial_transform)\n\n    assert opt.train_t_crop in [\'random\', \'center\']\n    temporal_transform = []\n    if opt.sample_t_stride > 1:\n        temporal_transform.append(TemporalSubsampling(opt.sample_t_stride))\n    if opt.train_t_crop == \'random\':\n        temporal_transform.append(TemporalRandomCrop(opt.sample_duration))\n    elif opt.train_t_crop == \'center\':\n        temporal_transform.append(TemporalCenterCrop(opt.sample_duration))\n    temporal_transform = TemporalCompose(temporal_transform)\n\n    train_data = get_training_data(opt.video_path, opt.annotation_path,\n                                   opt.dataset, opt.input_type, opt.file_type,\n                                   spatial_transform, temporal_transform)\n    if opt.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(\n            train_data)\n    else:\n        train_sampler = None\n    train_loader = torch.utils.data.DataLoader(train_data,\n                                               batch_size=opt.batch_size,\n                                               shuffle=(train_sampler is None),\n                                               num_workers=opt.n_threads,\n                                               pin_memory=True,\n                                               sampler=train_sampler,\n                                               worker_init_fn=worker_init_fn)\n\n    if opt.is_master_node:\n        train_logger = Logger(opt.result_path / \'train.log\',\n                              [\'epoch\', \'loss\', \'acc\', \'lr\'])\n        train_batch_logger = Logger(\n            opt.result_path / \'train_batch.log\',\n            [\'epoch\', \'batch\', \'iter\', \'loss\', \'acc\', \'lr\'])\n    else:\n        train_logger = None\n        train_batch_logger = None\n\n    if opt.nesterov:\n        dampening = 0\n    else:\n        dampening = opt.dampening\n    optimizer = SGD(model_parameters,\n                    lr=opt.learning_rate,\n                    momentum=opt.momentum,\n                    dampening=dampening,\n                    weight_decay=opt.weight_decay,\n                    nesterov=opt.nesterov)\n\n    assert opt.lr_scheduler in [\'plateau\', \'multistep\']\n    assert not (opt.lr_scheduler == \'plateau\' and opt.no_val)\n    if opt.lr_scheduler == \'plateau\':\n        scheduler = lr_scheduler.ReduceLROnPlateau(\n            optimizer, \'min\', patience=opt.plateau_patience)\n    else:\n        scheduler = lr_scheduler.MultiStepLR(optimizer,\n                                             opt.multistep_milestones)\n\n    return (train_loader, train_sampler, train_logger, train_batch_logger,\n            optimizer, scheduler)\n\n\ndef get_val_utils(opt):\n    normalize = get_normalize_method(opt.mean, opt.std, opt.no_mean_norm,\n                                     opt.no_std_norm)\n    spatial_transform = [\n        Resize(opt.sample_size),\n        CenterCrop(opt.sample_size),\n        ToTensor()\n    ]\n    if opt.input_type == \'flow\':\n        spatial_transform.append(PickFirstChannels(n=2))\n    spatial_transform.extend([ScaleValue(opt.value_scale), normalize])\n    spatial_transform = Compose(spatial_transform)\n\n    temporal_transform = []\n    if opt.sample_t_stride > 1:\n        temporal_transform.append(TemporalSubsampling(opt.sample_t_stride))\n    temporal_transform.append(\n        TemporalEvenCrop(opt.sample_duration, opt.n_val_samples))\n    temporal_transform = TemporalCompose(temporal_transform)\n\n    val_data, collate_fn = get_validation_data(opt.video_path,\n                                               opt.annotation_path, opt.dataset,\n                                               opt.input_type, opt.file_type,\n                                               spatial_transform,\n                                               temporal_transform)\n    if opt.distributed:\n        val_sampler = torch.utils.data.distributed.DistributedSampler(\n            val_data, shuffle=False)\n    else:\n        val_sampler = None\n    val_loader = torch.utils.data.DataLoader(val_data,\n                                             batch_size=(opt.batch_size //\n                                                         opt.n_val_samples),\n                                             shuffle=False,\n                                             num_workers=opt.n_threads,\n                                             pin_memory=True,\n                                             sampler=val_sampler,\n                                             worker_init_fn=worker_init_fn,\n                                             collate_fn=collate_fn)\n\n    if opt.is_master_node:\n        val_logger = Logger(opt.result_path / \'val.log\',\n                            [\'epoch\', \'loss\', \'acc\'])\n    else:\n        val_logger = None\n\n    return val_loader, val_logger\n\n\ndef get_inference_utils(opt):\n    assert opt.inference_crop in [\'center\', \'nocrop\']\n\n    normalize = get_normalize_method(opt.mean, opt.std, opt.no_mean_norm,\n                                     opt.no_std_norm)\n\n    spatial_transform = [Resize(opt.sample_size)]\n    if opt.inference_crop == \'center\':\n        spatial_transform.append(CenterCrop(opt.sample_size))\n    spatial_transform.append(ToTensor())\n    if opt.input_type == \'flow\':\n        spatial_transform.append(PickFirstChannels(n=2))\n    spatial_transform.extend([ScaleValue(opt.value_scale), normalize])\n    spatial_transform = Compose(spatial_transform)\n\n    temporal_transform = []\n    if opt.sample_t_stride > 1:\n        temporal_transform.append(TemporalSubsampling(opt.sample_t_stride))\n    temporal_transform.append(\n        SlidingWindow(opt.sample_duration, opt.inference_stride))\n    temporal_transform = TemporalCompose(temporal_transform)\n\n    inference_data, collate_fn = get_inference_data(\n        opt.video_path, opt.annotation_path, opt.dataset, opt.input_type,\n        opt.file_type, opt.inference_subset, spatial_transform,\n        temporal_transform)\n\n    inference_loader = torch.utils.data.DataLoader(\n        inference_data,\n        batch_size=opt.inference_batch_size,\n        shuffle=False,\n        num_workers=opt.n_threads,\n        pin_memory=True,\n        worker_init_fn=worker_init_fn,\n        collate_fn=collate_fn)\n\n    return inference_loader, inference_data.class_names\n\n\ndef save_checkpoint(save_file_path, epoch, arch, model, optimizer, scheduler):\n    if hasattr(model, \'module\'):\n        model_state_dict = model.module.state_dict()\n    else:\n        model_state_dict = model.state_dict()\n    save_states = {\n        \'epoch\': epoch,\n        \'arch\': arch,\n        \'state_dict\': model_state_dict,\n        \'optimizer\': optimizer.state_dict(),\n        \'scheduler\': scheduler.state_dict()\n    }\n    torch.save(save_states, save_file_path)\n\n\ndef main_worker(index, opt):\n    random.seed(opt.manual_seed)\n    np.random.seed(opt.manual_seed)\n    torch.manual_seed(opt.manual_seed)\n\n    if index >= 0 and opt.device.type == \'cuda\':\n        opt.device = torch.device(f\'cuda:{index}\')\n\n    if opt.distributed:\n        opt.dist_rank = opt.dist_rank * opt.ngpus_per_node + index\n        dist.init_process_group(backend=\'nccl\',\n                                init_method=opt.dist_url,\n                                world_size=opt.world_size,\n                                rank=opt.dist_rank)\n        opt.batch_size = int(opt.batch_size / opt.ngpus_per_node)\n        opt.n_threads = int(\n            (opt.n_threads + opt.ngpus_per_node - 1) / opt.ngpus_per_node)\n    opt.is_master_node = not opt.distributed or opt.dist_rank == 0\n\n    model = generate_model(opt)\n    if opt.batchnorm_sync:\n        assert opt.distributed, \'SyncBatchNorm only supports DistributedDataParallel.\'\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n    if opt.pretrain_path:\n        model = load_pretrained_model(model, opt.pretrain_path, opt.model,\n                                      opt.n_finetune_classes)\n    if opt.resume_path is not None:\n        model = resume_model(opt.resume_path, opt.arch, model)\n    model = make_data_parallel(model, opt.distributed, opt.device)\n\n    if opt.pretrain_path:\n        parameters = get_fine_tuning_parameters(model, opt.ft_begin_module)\n    else:\n        parameters = model.parameters()\n\n    if opt.is_master_node:\n        print(model)\n\n    criterion = CrossEntropyLoss().to(opt.device)\n\n    if not opt.no_train:\n        (train_loader, train_sampler, train_logger, train_batch_logger,\n         optimizer, scheduler) = get_train_utils(opt, parameters)\n        if opt.resume_path is not None:\n            opt.begin_epoch, optimizer, scheduler = resume_train_utils(\n                opt.resume_path, opt.begin_epoch, optimizer, scheduler)\n            if opt.overwrite_milestones:\n                scheduler.milestones = opt.multistep_milestones\n    if not opt.no_val:\n        val_loader, val_logger = get_val_utils(opt)\n\n    if opt.tensorboard and opt.is_master_node:\n        from torch.utils.tensorboard import SummaryWriter\n        if opt.begin_epoch == 1:\n            tb_writer = SummaryWriter(log_dir=opt.result_path)\n        else:\n            tb_writer = SummaryWriter(log_dir=opt.result_path,\n                                      purge_step=opt.begin_epoch)\n    else:\n        tb_writer = None\n\n    prev_val_loss = None\n    for i in range(opt.begin_epoch, opt.n_epochs + 1):\n        if not opt.no_train:\n            if opt.distributed:\n                train_sampler.set_epoch(i)\n            current_lr = get_lr(optimizer)\n            train_epoch(i, train_loader, model, criterion, optimizer,\n                        opt.device, current_lr, train_logger,\n                        train_batch_logger, tb_writer, opt.distributed)\n\n            if i % opt.checkpoint == 0 and opt.is_master_node:\n                save_file_path = opt.result_path / \'save_{}.pth\'.format(i)\n                save_checkpoint(save_file_path, i, opt.arch, model, optimizer,\n                                scheduler)\n\n        if not opt.no_val:\n            prev_val_loss = val_epoch(i, val_loader, model, criterion,\n                                      opt.device, val_logger, tb_writer,\n                                      opt.distributed)\n\n        if not opt.no_train and opt.lr_scheduler == \'multistep\':\n            scheduler.step()\n        elif not opt.no_train and opt.lr_scheduler == \'plateau\':\n            scheduler.step(prev_val_loss)\n\n    if opt.inference:\n        inference_loader, inference_class_names = get_inference_utils(opt)\n        inference_result_path = opt.result_path / \'{}.json\'.format(\n            opt.inference_subset)\n\n        inference.inference(inference_loader, model, inference_result_path,\n                            inference_class_names, opt.inference_no_average,\n                            opt.output_topk)\n\n\nif __name__ == \'__main__\':\n    opt = get_opt()\n\n    opt.device = torch.device(\'cpu\' if opt.no_cuda else \'cuda\')\n    if not opt.no_cuda:\n        cudnn.benchmark = True\n    if opt.accimage:\n        torchvision.set_image_backend(\'accimage\')\n\n    opt.ngpus_per_node = torch.cuda.device_count()\n    if opt.distributed:\n        opt.world_size = opt.ngpus_per_node * opt.world_size\n        mp.spawn(main_worker, nprocs=opt.ngpus_per_node, args=(opt,))\n    else:\n        main_worker(-1, opt)'"
mean.py,0,"b""def get_mean_std(value_scale, dataset):\n    assert dataset in ['activitynet', 'kinetics', '0.5']\n\n    if dataset == 'activitynet':\n        mean = [0.4477, 0.4209, 0.3906]\n        std = [0.2767, 0.2695, 0.2714]\n    elif dataset == 'kinetics':\n        mean = [0.4345, 0.4051, 0.3775]\n        std = [0.2768, 0.2713, 0.2737]\n    elif dataset == '0.5':\n        mean = [0.5, 0.5, 0.5]\n        std = [0.5, 0.5, 0.5]\n\n    mean = [x * value_scale for x in mean]\n    std = [x * value_scale for x in std]\n\n    return mean, std"""
model.py,2,"b""import torch\nfrom torch import nn\n\nfrom models import resnet, resnet2p1d, pre_act_resnet, wide_resnet, resnext, densenet\n\n\ndef get_module_name(name):\n    name = name.split('.')\n    if name[0] == 'module':\n        i = 1\n    else:\n        i = 0\n    if name[i] == 'features':\n        i += 1\n\n    return name[i]\n\n\ndef get_fine_tuning_parameters(model, ft_begin_module):\n    if not ft_begin_module:\n        return model.parameters()\n\n    parameters = []\n    add_flag = False\n    for k, v in model.named_parameters():\n        if ft_begin_module == get_module_name(k):\n            add_flag = True\n\n        if add_flag:\n            parameters.append({'params': v})\n\n    return parameters\n\n\ndef generate_model(opt):\n    assert opt.model in [\n        'resnet', 'resnet2p1d', 'preresnet', 'wideresnet', 'resnext', 'densenet'\n    ]\n\n    if opt.model == 'resnet':\n        model = resnet.generate_model(model_depth=opt.model_depth,\n                                      n_classes=opt.n_classes,\n                                      n_input_channels=opt.n_input_channels,\n                                      shortcut_type=opt.resnet_shortcut,\n                                      conv1_t_size=opt.conv1_t_size,\n                                      conv1_t_stride=opt.conv1_t_stride,\n                                      no_max_pool=opt.no_max_pool,\n                                      widen_factor=opt.resnet_widen_factor)\n    elif opt.model == 'resnet2p1d':\n        model = resnet2p1d.generate_model(model_depth=opt.model_depth,\n                                          n_classes=opt.n_classes,\n                                          n_input_channels=opt.n_input_channels,\n                                          shortcut_type=opt.resnet_shortcut,\n                                          conv1_t_size=opt.conv1_t_size,\n                                          conv1_t_stride=opt.conv1_t_stride,\n                                          no_max_pool=opt.no_max_pool,\n                                          widen_factor=opt.resnet_widen_factor)\n    elif opt.model == 'wideresnet':\n        model = wide_resnet.generate_model(\n            model_depth=opt.model_depth,\n            k=opt.wide_resnet_k,\n            n_classes=opt.n_classes,\n            n_input_channels=opt.n_input_channels,\n            shortcut_type=opt.resnet_shortcut,\n            conv1_t_size=opt.conv1_t_size,\n            conv1_t_stride=opt.conv1_t_stride,\n            no_max_pool=opt.no_max_pool)\n    elif opt.model == 'resnext':\n        model = resnext.generate_model(model_depth=opt.model_depth,\n                                       cardinality=opt.resnext_cardinality,\n                                       n_classes=opt.n_classes,\n                                       n_input_channels=opt.n_input_channels,\n                                       shortcut_type=opt.resnet_shortcut,\n                                       conv1_t_size=opt.conv1_t_size,\n                                       conv1_t_stride=opt.conv1_t_stride,\n                                       no_max_pool=opt.no_max_pool)\n    elif opt.model == 'preresnet':\n        model = pre_act_resnet.generate_model(\n            model_depth=opt.model_depth,\n            n_classes=opt.n_classes,\n            n_input_channels=opt.n_input_channels,\n            shortcut_type=opt.resnet_shortcut,\n            conv1_t_size=opt.conv1_t_size,\n            conv1_t_stride=opt.conv1_t_stride,\n            no_max_pool=opt.no_max_pool)\n    elif opt.model == 'densenet':\n        model = densenet.generate_model(model_depth=opt.model_depth,\n                                        n_classes=opt.n_classes,\n                                        n_input_channels=opt.n_input_channels,\n                                        conv1_t_size=opt.conv1_t_size,\n                                        conv1_t_stride=opt.conv1_t_stride,\n                                        no_max_pool=opt.no_max_pool)\n\n    return model\n\n\ndef load_pretrained_model(model, pretrain_path, model_name, n_finetune_classes):\n    if pretrain_path:\n        print('loading pretrained model {}'.format(pretrain_path))\n        pretrain = torch.load(pretrain_path, map_location='cpu')\n\n        model.load_state_dict(pretrain['state_dict'])\n        tmp_model = model\n        if model_name == 'densenet':\n            tmp_model.classifier = nn.Linear(tmp_model.classifier.in_features,\n                                             n_finetune_classes)\n        else:\n            tmp_model.fc = nn.Linear(tmp_model.fc.in_features,\n                                     n_finetune_classes)\n\n    return model\n\n\ndef make_data_parallel(model, is_distributed, device):\n    if is_distributed:\n        if device.type == 'cuda' and device.index is not None:\n            torch.cuda.set_device(device)\n            model.to(device)\n\n            model = nn.parallel.DistributedDataParallel(model,\n                                                        device_ids=[device])\n        else:\n            model.to(device)\n            model = nn.parallel.DistributedDataParallel(model)\n    elif device.type == 'cuda':\n        model = nn.DataParallel(model, device_ids=None).cuda()\n\n    return model\n"""
opts.py,0,"b""import argparse\nfrom pathlib import Path\n\n\ndef parse_opts():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--root_path',\n                        default=None,\n                        type=Path,\n                        help='Root directory path')\n    parser.add_argument('--video_path',\n                        default=None,\n                        type=Path,\n                        help='Directory path of videos')\n    parser.add_argument('--annotation_path',\n                        default=None,\n                        type=Path,\n                        help='Annotation file path')\n    parser.add_argument('--result_path',\n                        default=None,\n                        type=Path,\n                        help='Result directory path')\n    parser.add_argument(\n        '--dataset',\n        default='kinetics',\n        type=str,\n        help='Used dataset (activitynet | kinetics | ucf101 | hmdb51)')\n    parser.add_argument(\n        '--n_classes',\n        default=400,\n        type=int,\n        help=\n        'Number of classes (activitynet: 200, kinetics: 400 or 600, ucf101: 101, hmdb51: 51)'\n    )\n    parser.add_argument('--n_pretrain_classes',\n                        default=0,\n                        type=int,\n                        help=('Number of classes of pretraining task.'\n                              'When using --pretrain_path, this must be set.'))\n    parser.add_argument('--pretrain_path',\n                        default=None,\n                        type=Path,\n                        help='Pretrained model path (.pth).')\n    parser.add_argument(\n        '--ft_begin_module',\n        default='',\n        type=str,\n        help=('Module name of beginning of fine-tuning'\n              '(conv1, layer1, fc, denseblock1, classifier, ...).'\n              'The default means all layers are fine-tuned.'))\n    parser.add_argument('--sample_size',\n                        default=112,\n                        type=int,\n                        help='Height and width of inputs')\n    parser.add_argument('--sample_duration',\n                        default=16,\n                        type=int,\n                        help='Temporal duration of inputs')\n    parser.add_argument(\n        '--sample_t_stride',\n        default=1,\n        type=int,\n        help='If larger than 1, input frames are subsampled with the stride.')\n    parser.add_argument(\n        '--train_crop',\n        default='random',\n        type=str,\n        help=('Spatial cropping method in training. '\n              'random is uniform. '\n              'corner is selection from 4 corners and 1 center. '\n              '(random | corner | center)'))\n    parser.add_argument('--train_crop_min_scale',\n                        default=0.25,\n                        type=float,\n                        help='Min scale for random cropping in training')\n    parser.add_argument('--train_crop_min_ratio',\n                        default=0.75,\n                        type=float,\n                        help='Min aspect ratio for random cropping in training')\n    parser.add_argument('--no_hflip',\n                        action='store_true',\n                        help='If true holizontal flipping is not performed.')\n    parser.add_argument('--colorjitter',\n                        action='store_true',\n                        help='If true colorjitter is performed.')\n    parser.add_argument('--train_t_crop',\n                        default='random',\n                        type=str,\n                        help=('Temporal cropping method in training. '\n                              'random is uniform. '\n                              '(random | center)'))\n    parser.add_argument('--learning_rate',\n                        default=0.1,\n                        type=float,\n                        help=('Initial learning rate'\n                              '(divided by 10 while training by lr scheduler)'))\n    parser.add_argument('--momentum', default=0.9, type=float, help='Momentum')\n    parser.add_argument('--dampening',\n                        default=0.0,\n                        type=float,\n                        help='dampening of SGD')\n    parser.add_argument('--weight_decay',\n                        default=1e-3,\n                        type=float,\n                        help='Weight Decay')\n    parser.add_argument('--mean_dataset',\n                        default='kinetics',\n                        type=str,\n                        help=('dataset for mean values of mean subtraction'\n                              '(activitynet | kinetics | 0.5)'))\n    parser.add_argument('--no_mean_norm',\n                        action='store_true',\n                        help='If true, inputs are not normalized by mean.')\n    parser.add_argument(\n        '--no_std_norm',\n        action='store_true',\n        help='If true, inputs are not normalized by standard deviation.')\n    parser.add_argument(\n        '--value_scale',\n        default=1,\n        type=int,\n        help=\n        'If 1, range of inputs is [0-1]. If 255, range of inputs is [0-255].')\n    parser.add_argument('--nesterov',\n                        action='store_true',\n                        help='Nesterov momentum')\n    parser.add_argument('--optimizer',\n                        default='sgd',\n                        type=str,\n                        help='Currently only support SGD')\n    parser.add_argument('--lr_scheduler',\n                        default='multistep',\n                        type=str,\n                        help='Type of LR scheduler (multistep | plateau)')\n    parser.add_argument(\n        '--multistep_milestones',\n        default=[50, 100, 150],\n        type=int,\n        nargs='+',\n        help='Milestones of LR scheduler. See documentation of MultistepLR.')\n    parser.add_argument(\n        '--overwrite_milestones',\n        action='store_true',\n        help='If true, overwriting multistep_milestones when resuming training.'\n    )\n    parser.add_argument(\n        '--plateau_patience',\n        default=10,\n        type=int,\n        help='Patience of LR scheduler. See documentation of ReduceLROnPlateau.'\n    )\n    parser.add_argument('--batch_size',\n                        default=128,\n                        type=int,\n                        help='Batch Size')\n    parser.add_argument(\n        '--inference_batch_size',\n        default=0,\n        type=int,\n        help='Batch Size for inference. 0 means this is the same as batch_size.'\n    )\n    parser.add_argument(\n        '--batchnorm_sync',\n        action='store_true',\n        help='If true, SyncBatchNorm is used instead of BatchNorm.')\n    parser.add_argument('--n_epochs',\n                        default=200,\n                        type=int,\n                        help='Number of total epochs to run')\n    parser.add_argument('--n_val_samples',\n                        default=3,\n                        type=int,\n                        help='Number of validation samples for each activity')\n    parser.add_argument('--resume_path',\n                        default=None,\n                        type=Path,\n                        help='Save data (.pth) of previous training')\n    parser.add_argument('--no_train',\n                        action='store_true',\n                        help='If true, training is not performed.')\n    parser.add_argument('--no_val',\n                        action='store_true',\n                        help='If true, validation is not performed.')\n    parser.add_argument('--inference',\n                        action='store_true',\n                        help='If true, inference is performed.')\n    parser.add_argument('--inference_subset',\n                        default='val',\n                        type=str,\n                        help='Used subset in inference (train | val | test)')\n    parser.add_argument('--inference_stride',\n                        default=16,\n                        type=int,\n                        help='Stride of sliding window in inference.')\n    parser.add_argument(\n        '--inference_crop',\n        default='center',\n        type=str,\n        help=('Cropping method in inference. (center | nocrop)'\n              'When nocrop, fully convolutional inference is performed,'\n              'and mini-batch consists of clips of one video.'))\n    parser.add_argument(\n        '--inference_no_average',\n        action='store_true',\n        help='If true, outputs for segments in a video are not averaged.')\n    parser.add_argument('--no_cuda',\n                        action='store_true',\n                        help='If true, cuda is not used.')\n    parser.add_argument('--n_threads',\n                        default=4,\n                        type=int,\n                        help='Number of threads for multi-thread loading')\n    parser.add_argument('--checkpoint',\n                        default=10,\n                        type=int,\n                        help='Trained model is saved at every this epochs.')\n    parser.add_argument(\n        '--model',\n        default='resnet',\n        type=str,\n        help=\n        '(resnet | resnet2p1d | preresnet | wideresnet | resnext | densenet | ')\n    parser.add_argument('--model_depth',\n                        default=18,\n                        type=int,\n                        help='Depth of resnet (10 | 18 | 34 | 50 | 101)')\n    parser.add_argument('--conv1_t_size',\n                        default=7,\n                        type=int,\n                        help='Kernel size in t dim of conv1.')\n    parser.add_argument('--conv1_t_stride',\n                        default=1,\n                        type=int,\n                        help='Stride in t dim of conv1.')\n    parser.add_argument('--no_max_pool',\n                        action='store_true',\n                        help='If true, the max pooling after conv1 is removed.')\n    parser.add_argument('--resnet_shortcut',\n                        default='B',\n                        type=str,\n                        help='Shortcut type of resnet (A | B)')\n    parser.add_argument(\n        '--resnet_widen_factor',\n        default=1.0,\n        type=float,\n        help='The number of feature maps of resnet is multiplied by this value')\n    parser.add_argument('--wide_resnet_k',\n                        default=2,\n                        type=int,\n                        help='Wide resnet k')\n    parser.add_argument('--resnext_cardinality',\n                        default=32,\n                        type=int,\n                        help='ResNeXt cardinality')\n    parser.add_argument('--input_type',\n                        default='rgb',\n                        type=str,\n                        help='(rgb | flow)')\n    parser.add_argument('--manual_seed',\n                        default=1,\n                        type=int,\n                        help='Manually set random seed')\n    parser.add_argument('--accimage',\n                        action='store_true',\n                        help='If true, accimage is used to load images.')\n    parser.add_argument('--output_topk',\n                        default=5,\n                        type=int,\n                        help='Top-k scores are saved in json file.')\n    parser.add_argument('--file_type',\n                        default='jpg',\n                        type=str,\n                        help='(jpg | hdf5)')\n    parser.add_argument('--tensorboard',\n                        action='store_true',\n                        help='If true, output tensorboard log file.')\n    parser.add_argument(\n        '--distributed',\n        action='store_true',\n        help='Use multi-processing distributed training to launch '\n        'N processes per node, which has N GPUs.')\n    parser.add_argument('--dist_url',\n                        default='tcp://127.0.0.1:23456',\n                        type=str,\n                        help='url used to set up distributed training')\n    parser.add_argument('--world_size',\n                        default=-1,\n                        type=int,\n                        help='number of nodes for distributed training')\n\n    args = parser.parse_args()\n\n    return args\n"""
spatial_transforms.py,0,"b'import random\n\nfrom torchvision.transforms import transforms\nfrom torchvision.transforms import functional as F\nfrom PIL import Image\n\n\nclass Compose(transforms.Compose):\n\n    def randomize_parameters(self):\n        for t in self.transforms:\n            t.randomize_parameters()\n\n\nclass ToTensor(transforms.ToTensor):\n\n    def randomize_parameters(self):\n        pass\n\n\nclass Normalize(transforms.Normalize):\n\n    def randomize_parameters(self):\n        pass\n\n\nclass ScaleValue(object):\n\n    def __init__(self, s):\n        self.s = s\n\n    def __call__(self, tensor):\n        tensor *= self.s\n        return tensor\n\n    def randomize_parameters(self):\n        pass\n\n\nclass Resize(transforms.Resize):\n\n    def randomize_parameters(self):\n        pass\n\n\nclass Scale(transforms.Scale):\n\n    def randomize_parameters(self):\n        pass\n\n\nclass CenterCrop(transforms.CenterCrop):\n\n    def randomize_parameters(self):\n        pass\n\n\nclass CornerCrop(object):\n\n    def __init__(self,\n                 size,\n                 crop_position=None,\n                 crop_positions=[\'c\', \'tl\', \'tr\', \'bl\', \'br\']):\n        self.size = size\n        self.crop_position = crop_position\n        self.crop_positions = crop_positions\n\n        if crop_position is None:\n            self.randomize = True\n        else:\n            self.randomize = False\n        self.randomize_parameters()\n\n    def __call__(self, img):\n        image_width = img.size[0]\n        image_height = img.size[1]\n\n        h, w = (self.size, self.size)\n        if self.crop_position == \'c\':\n            i = int(round((image_height - h) / 2.))\n            j = int(round((image_width - w) / 2.))\n        elif self.crop_position == \'tl\':\n            i = 0\n            j = 0\n        elif self.crop_position == \'tr\':\n            i = 0\n            j = image_width - self.size\n        elif self.crop_position == \'bl\':\n            i = image_height - self.size\n            j = 0\n        elif self.crop_position == \'br\':\n            i = image_height - self.size\n            j = image_width - self.size\n\n        img = F.crop(img, i, j, h, w)\n\n        return img\n\n    def randomize_parameters(self):\n        if self.randomize:\n            self.crop_position = self.crop_positions[random.randint(\n                0,\n                len(self.crop_positions) - 1)]\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(size={0}, crop_position={1}, randomize={2})\'.format(\n            self.size, self.crop_position, self.randomize)\n\n\nclass RandomHorizontalFlip(transforms.RandomHorizontalFlip):\n\n    def __init__(self, p=0.5):\n        super().__init__(p)\n        self.randomize_parameters()\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL.Image): Image to be flipped.\n        Returns:\n            PIL.Image: Randomly flipped image.\n        """"""\n        if self.random_p < self.p:\n            return F.hflip(img)\n        return img\n\n    def randomize_parameters(self):\n        self.random_p = random.random()\n\n\nclass MultiScaleCornerCrop(object):\n\n    def __init__(self,\n                 size,\n                 scales,\n                 crop_positions=[\'c\', \'tl\', \'tr\', \'bl\', \'br\'],\n                 interpolation=Image.BILINEAR):\n        self.size = size\n        self.scales = scales\n        self.interpolation = interpolation\n        self.crop_positions = crop_positions\n\n        self.randomize_parameters()\n\n    def __call__(self, img):\n        short_side = min(img.size[0], img.size[1])\n        crop_size = int(short_side * self.scale)\n        self.corner_crop.size = crop_size\n\n        img = self.corner_crop(img)\n        return img.resize((self.size, self.size), self.interpolation)\n\n    def randomize_parameters(self):\n        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n        crop_position = self.crop_positions[random.randint(\n            0,\n            len(self.crop_positions) - 1)]\n\n        self.corner_crop = CornerCrop(None, crop_position)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(size={0}, scales={1}, interpolation={2})\'.format(\n            self.size, self.scales, self.interpolation)\n\n\nclass RandomResizedCrop(transforms.RandomResizedCrop):\n\n    def __init__(self,\n                 size,\n                 scale=(0.08, 1.0),\n                 ratio=(3. / 4., 4. / 3.),\n                 interpolation=Image.BILINEAR):\n        super().__init__(size, scale, ratio, interpolation)\n        self.randomize_parameters()\n\n    def __call__(self, img):\n        if self.randomize:\n            self.random_crop = self.get_params(img, self.scale, self.ratio)\n            self.randomize = False\n\n        i, j, h, w = self.random_crop\n        return F.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n\n    def randomize_parameters(self):\n        self.randomize = True\n\n\nclass ColorJitter(transforms.ColorJitter):\n\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n        super().__init__(brightness, contrast, saturation, hue)\n        self.randomize_parameters()\n\n    def __call__(self, img):\n        if self.randomize:\n            self.transform = self.get_params(self.brightness, self.contrast,\n                                             self.saturation, self.hue)\n            self.randomize = False\n\n        return self.transform(img)\n\n    def randomize_parameters(self):\n        self.randomize = True\n\n\nclass PickFirstChannels(object):\n\n    def __init__(self, n):\n        self.n = n\n\n    def __call__(self, tensor):\n        return tensor[:self.n, :, :]\n\n    def randomize_parameters(self):\n        pass'"
temporal_transforms.py,0,"b'import random\nimport math\n\n\nclass Compose(object):\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, frame_indices):\n        for i, t in enumerate(self.transforms):\n            if isinstance(frame_indices[0], list):\n                next_transforms = Compose(self.transforms[i:])\n                dst_frame_indices = [\n                    next_transforms(clip_frame_indices)\n                    for clip_frame_indices in frame_indices\n                ]\n\n                return dst_frame_indices\n            else:\n                frame_indices = t(frame_indices)\n        return frame_indices\n\n\nclass LoopPadding(object):\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n        out = frame_indices\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n\n\nclass TemporalBeginCrop(object):\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n        out = frame_indices[:self.size]\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n\n\nclass TemporalCenterCrop(object):\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n\n        center_index = len(frame_indices) // 2\n        begin_index = max(0, center_index - (self.size // 2))\n        end_index = min(begin_index + self.size, len(frame_indices))\n\n        out = frame_indices[begin_index:end_index]\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n\n\nclass TemporalRandomCrop(object):\n\n    def __init__(self, size):\n        self.size = size\n        self.loop = LoopPadding(size)\n\n    def __call__(self, frame_indices):\n\n        rand_end = max(0, len(frame_indices) - self.size - 1)\n        begin_index = random.randint(0, rand_end)\n        end_index = min(begin_index + self.size, len(frame_indices))\n\n        out = frame_indices[begin_index:end_index]\n\n        if len(out) < self.size:\n            out = self.loop(out)\n\n        return out\n\n\nclass TemporalEvenCrop(object):\n\n    def __init__(self, size, n_samples=1):\n        self.size = size\n        self.n_samples = n_samples\n        self.loop = LoopPadding(size)\n\n    def __call__(self, frame_indices):\n        n_frames = len(frame_indices)\n        stride = max(\n            1, math.ceil((n_frames - 1 - self.size) / (self.n_samples - 1)))\n\n        out = []\n        for begin_index in frame_indices[::stride]:\n            if len(out) >= self.n_samples:\n                break\n            end_index = min(frame_indices[-1] + 1, begin_index + self.size)\n            sample = list(range(begin_index, end_index))\n\n            if len(sample) < self.size:\n                out.append(self.loop(sample))\n                break\n            else:\n                out.append(sample)\n\n        return out\n\n\nclass SlidingWindow(object):\n\n    def __init__(self, size, stride=0):\n        self.size = size\n        if stride == 0:\n            self.stride = self.size\n        else:\n            self.stride = stride\n        self.loop = LoopPadding(size)\n\n    def __call__(self, frame_indices):\n        out = []\n        for begin_index in frame_indices[::self.stride]:\n            end_index = min(frame_indices[-1] + 1, begin_index + self.size)\n            sample = list(range(begin_index, end_index))\n\n            if len(sample) < self.size:\n                out.append(self.loop(sample))\n                break\n            else:\n                out.append(sample)\n\n        return out\n\n\nclass TemporalSubsampling(object):\n\n    def __init__(self, stride):\n        self.stride = stride\n\n    def __call__(self, frame_indices):\n        return frame_indices[::self.stride]\n\n\nclass Shuffle(object):\n\n    def __init__(self, block_size):\n        self.block_size = block_size\n\n    def __call__(self, frame_indices):\n        frame_indices = [\n            frame_indices[i:(i + self.block_size)]\n            for i in range(0, len(frame_indices), self.block_size)\n        ]\n        random.shuffle(frame_indices)\n        frame_indices = [t for block in frame_indices for t in block]\n        return frame_indices'"
training.py,9,"b""import torch\nimport time\nimport os\nimport sys\n\nimport torch\nimport torch.distributed as dist\n\nfrom utils import AverageMeter, calculate_accuracy\n\n\ndef train_epoch(epoch,\n                data_loader,\n                model,\n                criterion,\n                optimizer,\n                device,\n                current_lr,\n                epoch_logger,\n                batch_logger,\n                tb_writer=None,\n                distributed=False):\n    print('train at epoch {}'.format(epoch))\n\n    model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n\n    end_time = time.time()\n    for i, (inputs, targets) in enumerate(data_loader):\n        data_time.update(time.time() - end_time)\n\n        targets = targets.to(device, non_blocking=True)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        acc = calculate_accuracy(outputs, targets)\n\n        losses.update(loss.item(), inputs.size(0))\n        accuracies.update(acc, inputs.size(0))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        batch_time.update(time.time() - end_time)\n        end_time = time.time()\n\n        if batch_logger is not None:\n            batch_logger.log({\n                'epoch': epoch,\n                'batch': i + 1,\n                'iter': (epoch - 1) * len(data_loader) + (i + 1),\n                'loss': losses.val,\n                'acc': accuracies.val,\n                'lr': current_lr\n            })\n\n        print('Epoch: [{0}][{1}/{2}]\\t'\n              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n              'Acc {acc.val:.3f} ({acc.avg:.3f})'.format(epoch,\n                                                         i + 1,\n                                                         len(data_loader),\n                                                         batch_time=batch_time,\n                                                         data_time=data_time,\n                                                         loss=losses,\n                                                         acc=accuracies))\n\n    if distributed:\n        loss_sum = torch.tensor([losses.sum],\n                                dtype=torch.float32,\n                                device=device)\n        loss_count = torch.tensor([losses.count],\n                                  dtype=torch.float32,\n                                  device=device)\n        acc_sum = torch.tensor([accuracies.sum],\n                               dtype=torch.float32,\n                               device=device)\n        acc_count = torch.tensor([accuracies.count],\n                                 dtype=torch.float32,\n                                 device=device)\n\n        dist.all_reduce(loss_sum, op=dist.ReduceOp.SUM)\n        dist.all_reduce(loss_count, op=dist.ReduceOp.SUM)\n        dist.all_reduce(acc_sum, op=dist.ReduceOp.SUM)\n        dist.all_reduce(acc_count, op=dist.ReduceOp.SUM)\n\n        losses.avg = loss_sum.item() / loss_count.item()\n        accuracies.avg = acc_sum.item() / acc_count.item()\n\n    if epoch_logger is not None:\n        epoch_logger.log({\n            'epoch': epoch,\n            'loss': losses.avg,\n            'acc': accuracies.avg,\n            'lr': current_lr\n        })\n\n    if tb_writer is not None:\n        tb_writer.add_scalar('train/loss', losses.avg, epoch)\n        tb_writer.add_scalar('train/acc', accuracies.avg, epoch)\n        tb_writer.add_scalar('train/lr', accuracies.avg, epoch)\n"""
utils.py,3,"b'import csv\nimport random\nfrom functools import partialmethod\n\nimport torch\nimport numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass Logger(object):\n\n    def __init__(self, path, header):\n        self.log_file = path.open(\'w\')\n        self.logger = csv.writer(self.log_file, delimiter=\'\\t\')\n\n        self.logger.writerow(header)\n        self.header = header\n\n    def __del(self):\n        self.log_file.close()\n\n    def log(self, values):\n        write_values = []\n        for col in self.header:\n            assert col in values\n            write_values.append(values[col])\n\n        self.logger.writerow(write_values)\n        self.log_file.flush()\n\n\ndef calculate_accuracy(outputs, targets):\n    with torch.no_grad():\n        batch_size = targets.size(0)\n\n        _, pred = outputs.topk(1, 1, largest=True, sorted=True)\n        pred = pred.t()\n        correct = pred.eq(targets.view(1, -1))\n        n_correct_elems = correct.float().sum().item()\n\n        return n_correct_elems / batch_size\n\n\ndef calculate_precision_and_recall(outputs, targets, pos_label=1):\n    with torch.no_grad():\n        _, pred = outputs.topk(1, 1, largest=True, sorted=True)\n        precision, recall, _, _ = precision_recall_fscore_support(\n            targets.view(-1, 1).cpu().numpy(),\n            pred.cpu().numpy())\n\n        return precision[pos_label], recall[pos_label]\n\n\ndef worker_init_fn(worker_id):\n    torch_seed = torch.initial_seed()\n\n    random.seed(torch_seed + worker_id)\n\n    if torch_seed >= 2**32:\n        torch_seed = torch_seed % 2**32\n    np.random.seed(torch_seed + worker_id)\n\n\ndef get_lr(optimizer):\n    lrs = []\n    for param_group in optimizer.param_groups:\n        lr = float(param_group[\'lr\'])\n        lrs.append(lr)\n\n    return max(lrs)\n\n\ndef partialclass(cls, *args, **kwargs):\n\n    class PartialClass(cls):\n        __init__ = partialmethod(cls.__init__, *args, **kwargs)\n\n    return PartialClass'"
validation.py,10,"b""import torch\nimport time\nimport sys\n\nimport torch\nimport torch.distributed as dist\n\nfrom utils import AverageMeter, calculate_accuracy\n\n\ndef val_epoch(epoch,\n              data_loader,\n              model,\n              criterion,\n              device,\n              logger,\n              tb_writer=None,\n              distributed=False):\n    print('validation at epoch {}'.format(epoch))\n\n    model.eval()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n\n    end_time = time.time()\n\n    with torch.no_grad():\n        for i, (inputs, targets) in enumerate(data_loader):\n            data_time.update(time.time() - end_time)\n\n            targets = targets.to(device, non_blocking=True)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            acc = calculate_accuracy(outputs, targets)\n\n            losses.update(loss.item(), inputs.size(0))\n            accuracies.update(acc, inputs.size(0))\n\n            batch_time.update(time.time() - end_time)\n            end_time = time.time()\n\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Acc {acc.val:.3f} ({acc.avg:.3f})'.format(\n                      epoch,\n                      i + 1,\n                      len(data_loader),\n                      batch_time=batch_time,\n                      data_time=data_time,\n                      loss=losses,\n                      acc=accuracies))\n\n    if distributed:\n        loss_sum = torch.tensor([losses.sum],\n                                dtype=torch.float32,\n                                device=device)\n        loss_count = torch.tensor([losses.count],\n                                  dtype=torch.float32,\n                                  device=device)\n        acc_sum = torch.tensor([accuracies.sum],\n                               dtype=torch.float32,\n                               device=device)\n        acc_count = torch.tensor([accuracies.count],\n                                 dtype=torch.float32,\n                                 device=device)\n\n        dist.all_reduce(loss_sum, op=dist.ReduceOp.SUM)\n        dist.all_reduce(loss_count, op=dist.ReduceOp.SUM)\n        dist.all_reduce(acc_sum, op=dist.ReduceOp.SUM)\n        dist.all_reduce(acc_count, op=dist.ReduceOp.SUM)\n\n        losses.avg = loss_sum.item() / loss_count.item()\n        accuracies.avg = acc_sum.item() / acc_count.item()\n\n    if logger is not None:\n        logger.log({'epoch': epoch, 'loss': losses.avg, 'acc': accuracies.avg})\n\n    if tb_writer is not None:\n        tb_writer.add_scalar('val/loss', losses.avg, epoch)\n        tb_writer.add_scalar('val/acc', accuracies.avg, epoch)\n\n    return losses.avg\n"""
datasets/__init__.py,0,b''
datasets/activitynet.py,1,"b""import math\nimport json\n\nimport torch\nimport torch.utils.data as data\n\nfrom .loader import VideoLoader\nfrom .videodataset import VideoDataset\n\n\ndef get_n_frames(video_path):\n    return len([\n        x for x in video_path.iterdir()\n        if 'image' in x.name and x.name[0] != '.'\n    ])\n\n\ndef get_class_labels(data):\n    class_names = []\n    for node1 in data['taxonomy']:\n        is_leaf = True\n        for node2 in data['taxonomy']:\n            if node2['parentId'] == node1['nodeId']:\n                is_leaf = False\n                break\n        if is_leaf:\n            class_names.append(node1['nodeName'])\n\n    class_labels_map = {}\n\n    for i, class_name in enumerate(class_names):\n        class_labels_map[class_name] = i\n\n    return class_labels_map\n\n\ndef get_video_ids_annotations_and_fps(data, subset):\n    video_ids = []\n    annotations = []\n    fps_values = []\n\n    for key, value in data['database'].items():\n        this_subset = value['subset']\n        if this_subset == subset:\n            video_ids.append(key)\n            annotations.append(value['annotations'])\n            fps_values.append(value['fps'])\n\n    return video_ids, annotations, fps_values\n\n\nclass ActivityNet(VideoDataset):\n\n    def __init__(\n            self,\n            root_path,\n            annotation_path,\n            subset,\n            spatial_transform=None,\n            temporal_transform=None,\n            target_transform=None,\n            video_loader=None,\n            video_path_formatter=(\n                lambda root_path, label, video_id: root_path / f'v_{video_id}'),\n            image_name_formatter=lambda x: f'image_{x:05d}.jpg',\n            is_untrimmed_setting=False):\n        if is_untrimmed_setting:\n            self.data, self.class_names = self.__make_untrimmed_dataset(\n                root_path, annotation_path, subset, video_path_formatter)\n        else:\n            self.data, self.class_names = self.__make_dataset(\n                root_path, annotation_path, subset, video_path_formatter)\n\n        self.spatial_transform = spatial_transform\n        self.temporal_transform = temporal_transform\n        self.target_transform = target_transform\n\n        if video_loader is None:\n            self.loader = VideoLoader(image_name_formatter)\n        else:\n            self.loader = video_loader\n\n    def __make_dataset(self, root_path, annotation_path, subset,\n                       video_path_formatter):\n        with annotation_path.open('r') as f:\n            data = json.load(f)\n        video_ids, annotations, fps_values = get_video_ids_annotations_and_fps(\n            data, subset)\n        class_to_idx = get_class_labels(data)\n        idx_to_class = {}\n        for name, label in class_to_idx.items():\n            idx_to_class[label] = name\n\n        dataset = []\n        for i in range(len(video_ids)):\n            if i % 1000 == 0:\n                print('dataset loading [{}/{}]'.format(i, len(video_ids)))\n\n            video_path = video_path_formatter(root_path, label, video_ids[i])\n            if not video_path.exists():\n                continue\n\n            fps = fps_values[i]\n\n            for annotation in annotations[i]:\n                t_begin = math.floor(annotation['segment'][0] * fps) + 1\n                t_end = math.floor(annotation['segment'][1] * fps) + 1\n                n_video_frames = get_n_frames(video_path)\n                t_end = min(t_end, n_video_frames)\n                frame_indices = list(range(t_begin, t_end))\n\n                sample = {\n                    'video': video_path,\n                    'segment': (frame_indices[0], frame_indices[-1] + 1),\n                    'frame_indices': frame_indices,\n                    'fps': fps,\n                    'video_id': video_ids[i]\n                }\n                if annotations is not None:\n                    sample['label'] = class_to_idx[annotation['label']]\n                else:\n                    sample['label'] = -1\n\n                if len(sample['frame_indices']) < 8:\n                    continue\n                dataset.append(sample)\n\n        return dataset, idx_to_class\n\n    def __make_untrimmed_dataset(self, root_path, annotation_path, subset,\n                                 video_path_formatter):\n        with annotation_path.open('r') as f:\n            data = json.load(f)\n        video_ids, annotations, fps_values = get_video_ids_annotations_and_fps(\n            data, subset)\n        class_to_idx = get_class_labels(data)\n        idx_to_class = {}\n        for name, label in class_to_idx.items():\n            idx_to_class[label] = name\n\n        dataset = []\n        for i in range(len(video_ids)):\n            if i % 1000 == 0:\n                print('dataset loading [{}/{}]'.format(i, len(video_ids)))\n\n            video_path = video_path_formatter(root_path, label, video_ids[i])\n            if not video_path.exists():\n                continue\n\n            fps = fps_values[i]\n\n            t_begin = 1\n            t_end = get_n_frames(video_path) + 1\n            frame_indices = list(range(t_begin, t_end))\n\n            sample = {\n                'video': video_path,\n                'segment': (frame_indices[0], frame_indices[-1] + 1),\n                'frame_indices': frame_indices,\n                'fps': fps,\n                'video_id': video_ids[i]\n            }\n            dataset.append(sample)\n\n        return dataset, idx_to_class"""
datasets/loader.py,0,"b""import io\n\nimport h5py\nfrom PIL import Image\n\n\nclass ImageLoaderPIL(object):\n\n    def __call__(self, path):\n        # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n        with path.open('rb') as f:\n            with Image.open(f) as img:\n                return img.convert('RGB')\n\n\nclass ImageLoaderAccImage(object):\n\n    def __call__(self, path):\n        import accimage\n        return accimage.Image(str(path))\n\n\nclass VideoLoader(object):\n\n    def __init__(self, image_name_formatter, image_loader=None):\n        self.image_name_formatter = image_name_formatter\n        if image_loader is None:\n            self.image_loader = ImageLoaderPIL()\n        else:\n            self.image_loader = image_loader\n\n    def __call__(self, video_path, frame_indices):\n        video = []\n        for i in frame_indices:\n            image_path = video_path / self.image_name_formatter(i)\n            if image_path.exists():\n                video.append(self.image_loader(image_path))\n\n        return video\n\n\nclass VideoLoaderHDF5(object):\n\n    def __call__(self, video_path, frame_indices):\n        with h5py.File(video_path, 'r') as f:\n            video_data = f['video']\n\n            video = []\n            for i in frame_indices:\n                if i < len(video_data):\n                    video.append(Image.open(io.BytesIO(video_data[i])))\n                else:\n                    return video\n\n        return video\n\n\nclass VideoLoaderFlowHDF5(object):\n\n    def __init__(self):\n        self.flows = ['u', 'v']\n\n    def __call__(self, video_path, frame_indices):\n        with h5py.File(video_path, 'r') as f:\n\n            flow_data = []\n            for flow in self.flows:\n                flow_data.append(f[f'video_{flow}'])\n\n            video = []\n            for i in frame_indices:\n                if i < len(flow_data[0]):\n                    frame = [\n                        Image.open(io.BytesIO(video_data[i]))\n                        for video_data in flow_data\n                    ]\n                    frame.append(frame[-1])  # add dummy data into third channel\n                    video.append(Image.merge('RGB', frame))\n\n        return video"""
datasets/videodataset.py,2,"b""import json\nfrom pathlib import Path\n\nimport torch\nimport torch.utils.data as data\n\nfrom .loader import VideoLoader\n\n\ndef get_class_labels(data):\n    class_labels_map = {}\n    index = 0\n    for class_label in data['labels']:\n        class_labels_map[class_label] = index\n        index += 1\n    return class_labels_map\n\n\ndef get_database(data, subset, root_path, video_path_formatter):\n    video_ids = []\n    video_paths = []\n    annotations = []\n\n    for key, value in data['database'].items():\n        this_subset = value['subset']\n        if this_subset == subset:\n            video_ids.append(key)\n            annotations.append(value['annotations'])\n            if 'video_path' in value:\n                video_paths.append(Path(value['video_path']))\n            else:\n                label = value['annotations']['label']\n                video_paths.append(video_path_formatter(root_path, label, key))\n\n    return video_ids, video_paths, annotations\n\n\nclass VideoDataset(data.Dataset):\n\n    def __init__(self,\n                 root_path,\n                 annotation_path,\n                 subset,\n                 spatial_transform=None,\n                 temporal_transform=None,\n                 target_transform=None,\n                 video_loader=None,\n                 video_path_formatter=(lambda root_path, label, video_id:\n                                       root_path / label / video_id),\n                 image_name_formatter=lambda x: f'image_{x:05d}.jpg',\n                 target_type='label'):\n        self.data, self.class_names = self.__make_dataset(\n            root_path, annotation_path, subset, video_path_formatter)\n\n        self.spatial_transform = spatial_transform\n        self.temporal_transform = temporal_transform\n        self.target_transform = target_transform\n\n        if video_loader is None:\n            self.loader = VideoLoader(image_name_formatter)\n        else:\n            self.loader = video_loader\n\n        self.target_type = target_type\n\n    def __make_dataset(self, root_path, annotation_path, subset,\n                       video_path_formatter):\n        with annotation_path.open('r') as f:\n            data = json.load(f)\n        video_ids, video_paths, annotations = get_database(\n            data, subset, root_path, video_path_formatter)\n        class_to_idx = get_class_labels(data)\n        idx_to_class = {}\n        for name, label in class_to_idx.items():\n            idx_to_class[label] = name\n\n        n_videos = len(video_ids)\n        dataset = []\n        for i in range(n_videos):\n            if i % (n_videos // 5) == 0:\n                print('dataset loading [{}/{}]'.format(i, len(video_ids)))\n\n            if 'label' in annotations[i]:\n                label = annotations[i]['label']\n                label_id = class_to_idx[label]\n            else:\n                label = 'test'\n                label_id = -1\n\n            video_path = video_paths[i]\n            if not video_path.exists():\n                continue\n\n            segment = annotations[i]['segment']\n            if segment[1] == 1:\n                continue\n\n            frame_indices = list(range(segment[0], segment[1]))\n            sample = {\n                'video': video_path,\n                'segment': segment,\n                'frame_indices': frame_indices,\n                'video_id': video_ids[i],\n                'label': label_id\n            }\n            dataset.append(sample)\n\n        return dataset, idx_to_class\n\n    def __loading(self, path, frame_indices):\n        clip = self.loader(path, frame_indices)\n        if self.spatial_transform is not None:\n            self.spatial_transform.randomize_parameters()\n            clip = [self.spatial_transform(img) for img in clip]\n        clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n\n        return clip\n\n    def __getitem__(self, index):\n        path = self.data[index]['video']\n        if isinstance(self.target_type, list):\n            target = [self.data[index][t] for t in self.target_type]\n        else:\n            target = self.data[index][self.target_type]\n\n        frame_indices = self.data[index]['frame_indices']\n        if self.temporal_transform is not None:\n            frame_indices = self.temporal_transform(frame_indices)\n\n        clip = self.__loading(path, frame_indices)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return clip, target\n\n    def __len__(self):\n        return len(self.data)"""
datasets/videodataset_multiclips.py,2,"b""import json\nimport copy\nimport functools\n\nimport torch\nfrom torch.utils.data.dataloader import default_collate\n\nfrom .videodataset import VideoDataset\n\n\ndef collate_fn(batch):\n    batch_clips, batch_targets = zip(*batch)\n\n    batch_clips = [clip for multi_clips in batch_clips for clip in multi_clips]\n    batch_targets = [\n        target for multi_targets in batch_targets for target in multi_targets\n    ]\n\n    target_element = batch_targets[0]\n    if isinstance(target_element, int) or isinstance(target_element, str):\n        return default_collate(batch_clips), default_collate(batch_targets)\n    else:\n        return default_collate(batch_clips), batch_targets\n\n\nclass VideoDatasetMultiClips(VideoDataset):\n\n    def __loading(self, path, video_frame_indices):\n        clips = []\n        segments = []\n        for clip_frame_indices in video_frame_indices:\n            clip = self.loader(path, clip_frame_indices)\n            if self.spatial_transform is not None:\n                self.spatial_transform.randomize_parameters()\n                clip = [self.spatial_transform(img) for img in clip]\n            clips.append(torch.stack(clip, 0).permute(1, 0, 2, 3))\n            segments.append(\n                [min(clip_frame_indices),\n                 max(clip_frame_indices) + 1])\n\n        return clips, segments\n\n    def __getitem__(self, index):\n        path = self.data[index]['video']\n\n        video_frame_indices = self.data[index]['frame_indices']\n        if self.temporal_transform is not None:\n            video_frame_indices = self.temporal_transform(video_frame_indices)\n\n        clips, segments = self.__loading(path, video_frame_indices)\n\n        if isinstance(self.target_type, list):\n            target = [self.data[index][t] for t in self.target_type]\n        else:\n            target = self.data[index][self.target_type]\n\n        if 'segment' in self.target_type:\n            if isinstance(self.target_type, list):\n                segment_index = self.target_type.index('segment')\n                targets = []\n                for s in segments:\n                    targets.append(copy.deepcopy(target))\n                    targets[-1][segment_index] = s\n            else:\n                targets = segments\n        else:\n            targets = [target for _ in range(len(segments))]\n\n        return clips, targets"""
models/__init__.py,0,b''
models/densenet.py,3,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\n\nclass _DenseLayer(nn.Sequential):\n\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super().__init__()\n        self.add_module(\'norm1\', nn.BatchNorm3d(num_input_features))\n        self.add_module(\'relu1\', nn.ReLU(inplace=True))\n        self.add_module(\n            \'conv1\',\n            nn.Conv3d(num_input_features,\n                      bn_size * growth_rate,\n                      kernel_size=1,\n                      stride=1,\n                      bias=False))\n        self.add_module(\'norm2\', nn.BatchNorm3d(bn_size * growth_rate))\n        self.add_module(\'relu2\', nn.ReLU(inplace=True))\n        self.add_module(\n            \'conv2\',\n            nn.Conv3d(bn_size * growth_rate,\n                      growth_rate,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1,\n                      bias=False))\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super().forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features,\n                                     p=self.drop_rate,\n                                     training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate,\n                 drop_rate):\n        super().__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate,\n                                growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer{}\'.format(i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n\n    def __init__(self, num_input_features, num_output_features):\n        super().__init__()\n        self.add_module(\'norm\', nn.BatchNorm3d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\n            \'conv\',\n            nn.Conv3d(num_input_features,\n                      num_output_features,\n                      kernel_size=1,\n                      stride=1,\n                      bias=False))\n        self.add_module(\'pool\', nn.AvgPool3d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    """"""Densenet-BC model class\n    Args:\n        growth_rate (int) - how many filters to add each layer (k in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    """"""\n\n    def __init__(self,\n                 n_input_channels=3,\n                 conv1_t_size=7,\n                 conv1_t_stride=1,\n                 no_max_pool=False,\n                 growth_rate=32,\n                 block_config=(6, 12, 24, 16),\n                 num_init_features=64,\n                 bn_size=4,\n                 drop_rate=0,\n                 num_classes=1000):\n\n        super().__init__()\n\n        # First convolution\n        self.features = [(\'conv1\',\n                          nn.Conv3d(n_input_channels,\n                                    num_init_features,\n                                    kernel_size=(conv1_t_size, 7, 7),\n                                    stride=(conv1_t_stride, 2, 2),\n                                    padding=(conv1_t_size // 2, 3, 3),\n                                    bias=False)),\n                         (\'norm1\', nn.BatchNorm3d(num_init_features)),\n                         (\'relu1\', nn.ReLU(inplace=True))]\n        if not no_max_pool:\n            self.features.append(\n                (\'pool1\', nn.MaxPool3d(kernel_size=3, stride=2, padding=1)))\n        self.features = nn.Sequential(OrderedDict(self.features))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers,\n                                num_input_features=num_features,\n                                bn_size=bn_size,\n                                growth_rate=growth_rate,\n                                drop_rate=drop_rate)\n            self.features.add_module(\'denseblock{}\'.format(i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features,\n                                    num_output_features=num_features // 2)\n                self.features.add_module(\'transition{}\'.format(i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm3d(num_features))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode=\'fan_out\')\n            elif isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight,\n                                        mode=\'fan_out\',\n                                        nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.adaptive_avg_pool3d(out,\n                                    output_size=(1, 1,\n                                                 1)).view(features.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n\ndef generate_model(model_depth, **kwargs):\n    assert model_depth in [121, 169, 201, 264]\n\n    if model_depth == 121:\n        model = DenseNet(num_init_features=64,\n                         growth_rate=32,\n                         block_config=(6, 12, 24, 16),\n                         **kwargs)\n    elif model_depth == 169:\n        model = DenseNet(num_init_features=64,\n                         growth_rate=32,\n                         block_config=(6, 12, 32, 32),\n                         **kwargs)\n    elif model_depth == 201:\n        model = DenseNet(num_init_features=64,\n                         growth_rate=32,\n                         block_config=(6, 12, 48, 32),\n                         **kwargs)\n    elif model_depth == 264:\n        model = DenseNet(num_init_features=64,\n                         growth_rate=32,\n                         block_config=(6, 12, 64, 48),\n                         **kwargs)\n\n    return model'"
models/pre_act_resnet.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .resnet import conv3x3x3, conv1x1x1, get_inplanes, ResNet\n\n\nclass PreActivationBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super().__init__()\n\n        self.bn1 = nn.BatchNorm3d(inplanes)\n        self.conv1 = conv3x3x3(inplanes, planes, stride)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass PreActivationBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super().__init__()\n\n        self.bn1 = nn.BatchNorm3d(inplanes)\n        self.conv1 = conv1x1x1(inplanes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv2 = conv3x3x3(planes, planes, stride)\n        self.bn3 = nn.BatchNorm3d(planes)\n        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\ndef generate_model(model_depth, **kwargs):\n    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n\n    if model_depth == 10:\n        model = ResNet(PreActivationBasicBlock, [1, 1, 1, 1], get_inplanes(),\n                       **kwargs)\n    elif model_depth == 18:\n        model = ResNet(PreActivationBasicBlock, [2, 2, 2, 2], get_inplanes(),\n                       **kwargs)\n    elif model_depth == 34:\n        model = ResNet(PreActivationBasicBlock, [3, 4, 6, 3], get_inplanes(),\n                       **kwargs)\n    elif model_depth == 50:\n        model = ResNet(PreActivationBottleneck, [3, 4, 6, 3], get_inplanes(),\n                       **kwargs)\n    elif model_depth == 101:\n        model = ResNet(PreActivationBottleneck, [3, 4, 23, 3], get_inplanes(),\n                       **kwargs)\n    elif model_depth == 152:\n        model = ResNet(PreActivationBottleneck, [3, 8, 36, 3], get_inplanes(),\n                       **kwargs)\n    elif model_depth == 200:\n        model = ResNet(PreActivationBottleneck, [3, 24, 36, 3], get_inplanes(),\n                       **kwargs)\n\n    return model\n'"
models/resnet.py,5,"b""import math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef get_inplanes():\n    return [64, 128, 256, 512]\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    return nn.Conv3d(in_planes,\n                     out_planes,\n                     kernel_size=3,\n                     stride=stride,\n                     padding=1,\n                     bias=False)\n\n\ndef conv1x1x1(in_planes, out_planes, stride=1):\n    return nn.Conv3d(in_planes,\n                     out_planes,\n                     kernel_size=1,\n                     stride=stride,\n                     bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, downsample=None):\n        super().__init__()\n\n        self.conv1 = conv3x3x3(in_planes, planes, stride)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1, downsample=None):\n        super().__init__()\n\n        self.conv1 = conv1x1x1(in_planes, planes)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = conv3x3x3(planes, planes, stride)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self,\n                 block,\n                 layers,\n                 block_inplanes,\n                 n_input_channels=3,\n                 conv1_t_size=7,\n                 conv1_t_stride=1,\n                 no_max_pool=False,\n                 shortcut_type='B',\n                 widen_factor=1.0,\n                 n_classes=400):\n        super().__init__()\n\n        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n\n        self.in_planes = block_inplanes[0]\n        self.no_max_pool = no_max_pool\n\n        self.conv1 = nn.Conv3d(n_input_channels,\n                               self.in_planes,\n                               kernel_size=(conv1_t_size, 7, 7),\n                               stride=(conv1_t_stride, 2, 2),\n                               padding=(conv1_t_size // 2, 3, 3),\n                               bias=False)\n        self.bn1 = nn.BatchNorm3d(self.in_planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n                                       shortcut_type)\n        self.layer2 = self._make_layer(block,\n                                       block_inplanes[1],\n                                       layers[1],\n                                       shortcut_type,\n                                       stride=2)\n        self.layer3 = self._make_layer(block,\n                                       block_inplanes[2],\n                                       layers[2],\n                                       shortcut_type,\n                                       stride=2)\n        self.layer4 = self._make_layer(block,\n                                       block_inplanes[3],\n                                       layers[3],\n                                       shortcut_type,\n                                       stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight,\n                                        mode='fan_out',\n                                        nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _downsample_basic_block(self, x, planes, stride):\n        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n                                out.size(3), out.size(4))\n        if isinstance(out.data, torch.cuda.FloatTensor):\n            zero_pads = zero_pads.cuda()\n\n        out = torch.cat([out.data, zero_pads], dim=1)\n\n        return out\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.in_planes != planes * block.expansion:\n            if shortcut_type == 'A':\n                downsample = partial(self._downsample_basic_block,\n                                     planes=planes * block.expansion,\n                                     stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n                    nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(\n            block(in_planes=self.in_planes,\n                  planes=planes,\n                  stride=stride,\n                  downsample=downsample))\n        self.in_planes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.in_planes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        if not self.no_max_pool:\n            x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef generate_model(model_depth, **kwargs):\n    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n\n    if model_depth == 10:\n        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n    elif model_depth == 18:\n        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n    elif model_depth == 34:\n        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n    elif model_depth == 50:\n        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n    elif model_depth == 101:\n        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n    elif model_depth == 152:\n        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n    elif model_depth == 200:\n        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n\n    return model\n"""
models/resnet2p1d.py,5,"b""import math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef get_inplanes():\n    return [64, 128, 256, 512]\n\n\ndef conv1x3x3(in_planes, mid_planes, stride=1):\n    return nn.Conv3d(in_planes,\n                     mid_planes,\n                     kernel_size=(1, 3, 3),\n                     stride=(1, stride, stride),\n                     padding=(0, 1, 1),\n                     bias=False)\n\n\ndef conv3x1x1(mid_planes, planes, stride=1):\n    return nn.Conv3d(mid_planes,\n                     planes,\n                     kernel_size=(3, 1, 1),\n                     stride=(stride, 1, 1),\n                     padding=(1, 0, 0),\n                     bias=False)\n\n\ndef conv1x1x1(in_planes, out_planes, stride=1):\n    return nn.Conv3d(in_planes,\n                     out_planes,\n                     kernel_size=1,\n                     stride=stride,\n                     bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, downsample=None):\n        super().__init__()\n\n        n_3d_parameters1 = in_planes * planes * 3 * 3 * 3\n        n_2p1d_parameters1 = in_planes * 3 * 3 + 3 * planes\n        mid_planes1 = n_3d_parameters1 // n_2p1d_parameters1\n        self.conv1_s = conv1x3x3(in_planes, mid_planes1, stride)\n        self.bn1_s = nn.BatchNorm3d(mid_planes1)\n        self.conv1_t = conv3x1x1(mid_planes1, planes, stride)\n        self.bn1_t = nn.BatchNorm3d(planes)\n\n        n_3d_parameters2 = planes * planes * 3 * 3 * 3\n        n_2p1d_parameters2 = planes * 3 * 3 + 3 * planes\n        mid_planes2 = n_3d_parameters2 // n_2p1d_parameters2\n        self.conv2_s = conv1x3x3(planes, mid_planes2)\n        self.bn2_s = nn.BatchNorm3d(mid_planes2)\n        self.conv2_t = conv3x1x1(mid_planes2, planes)\n        self.bn2_t = nn.BatchNorm3d(planes)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1_s(x)\n        out = self.bn1_s(out)\n        out = self.relu(out)\n        out = self.conv1_t(out)\n        out = self.bn1_t(out)\n        out = self.relu(out)\n\n        out = self.conv2_s(out)\n        out = self.bn2_s(out)\n        out = self.relu(out)\n        out = self.conv2_t(out)\n        out = self.bn2_t(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1, downsample=None):\n        super().__init__()\n\n        self.conv1 = conv1x1x1(in_planes, planes)\n        self.bn1 = nn.BatchNorm3d(planes)\n\n        n_3d_parameters = planes * planes * 3 * 3 * 3\n        n_2p1d_parameters = planes * 3 * 3 + 3 * planes\n        mid_planes = n_3d_parameters // n_2p1d_parameters\n        self.conv2_s = conv1x3x3(planes, mid_planes, stride)\n        self.bn2_s = nn.BatchNorm3d(mid_planes)\n        self.conv2_t = conv3x1x1(mid_planes, planes, stride)\n        self.bn2_t = nn.BatchNorm3d(planes)\n\n        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2_s(out)\n        out = self.bn2_s(out)\n        out = self.relu(out)\n        out = self.conv2_t(out)\n        out = self.bn2_t(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self,\n                 block,\n                 layers,\n                 block_inplanes,\n                 n_input_channels=3,\n                 conv1_t_size=7,\n                 conv1_t_stride=1,\n                 no_max_pool=False,\n                 shortcut_type='B',\n                 widen_factor=1.0,\n                 n_classes=400):\n        super().__init__()\n\n        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n\n        self.in_planes = block_inplanes[0]\n        self.no_max_pool = no_max_pool\n\n        n_3d_parameters = 3 * self.in_planes * conv1_t_size * 7 * 7\n        n_2p1d_parameters = 3 * 7 * 7 + conv1_t_size * self.in_planes\n        mid_planes = n_3d_parameters // n_2p1d_parameters\n        self.conv1_s = nn.Conv3d(n_input_channels,\n                                 mid_planes,\n                                 kernel_size=(1, 7, 7),\n                                 stride=(1, 2, 2),\n                                 padding=(0, 3, 3),\n                                 bias=False)\n        self.bn1_s = nn.BatchNorm3d(mid_planes)\n        self.conv1_t = nn.Conv3d(mid_planes,\n                                 self.in_planes,\n                                 kernel_size=(conv1_t_size, 1, 1),\n                                 stride=(conv1_t_stride, 1, 1),\n                                 padding=(conv1_t_size // 2, 0, 0),\n                                 bias=False)\n        self.bn1_t = nn.BatchNorm3d(self.in_planes)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n                                       shortcut_type)\n        self.layer2 = self._make_layer(block,\n                                       block_inplanes[1],\n                                       layers[1],\n                                       shortcut_type,\n                                       stride=2)\n        self.layer3 = self._make_layer(block,\n                                       block_inplanes[2],\n                                       layers[2],\n                                       shortcut_type,\n                                       stride=2)\n        self.layer4 = self._make_layer(block,\n                                       block_inplanes[3],\n                                       layers[3],\n                                       shortcut_type,\n                                       stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight,\n                                        mode='fan_out',\n                                        nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _downsample_basic_block(self, x, planes, stride):\n        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n                                out.size(3), out.size(4))\n        if isinstance(out.data, torch.cuda.FloatTensor):\n            zero_pads = zero_pads.cuda()\n\n        out = torch.cat([out.data, zero_pads], dim=1)\n\n        return out\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.in_planes != planes * block.expansion:\n            if shortcut_type == 'A':\n                downsample = partial(self._downsample_basic_block,\n                                     planes=planes * block.expansion,\n                                     stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n                    nn.BatchNorm3d(planes * block.expansion))\n\n        layers = []\n        layers.append(\n            block(in_planes=self.in_planes,\n                  planes=planes,\n                  stride=stride,\n                  downsample=downsample))\n        self.in_planes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.in_planes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1_s(x)\n        x = self.bn1_s(x)\n        x = self.relu(x)\n        x = self.conv1_t(x)\n        x = self.bn1_t(x)\n        x = self.relu(x)\n\n        if not self.no_max_pool:\n            x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef generate_model(model_depth, **kwargs):\n    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n\n    if model_depth == 10:\n        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n    elif model_depth == 18:\n        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n    elif model_depth == 34:\n        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n    elif model_depth == 50:\n        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n    elif model_depth == 101:\n        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n    elif model_depth == 152:\n        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n    elif model_depth == 200:\n        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n\n    return model"""
models/resnext.py,2,"b""import math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .resnet import conv1x1x1, Bottleneck, ResNet\nfrom utils import partialclass\n\n\ndef get_inplanes():\n    return [128, 256, 512, 1024]\n\n\nclass ResNeXtBottleneck(Bottleneck):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, cardinality, stride=1,\n                 downsample=None):\n        super().__init__(inplanes, planes, stride, downsample)\n\n        mid_planes = cardinality * planes // 32\n        self.conv1 = conv1x1x1(inplanes, mid_planes)\n        self.bn1 = nn.BatchNorm3d(mid_planes)\n        self.conv2 = nn.Conv3d(mid_planes,\n                               mid_planes,\n                               kernel_size=3,\n                               stride=stride,\n                               padding=1,\n                               groups=cardinality,\n                               bias=False)\n        self.bn2 = nn.BatchNorm3d(mid_planes)\n        self.conv3 = conv1x1x1(mid_planes, planes * self.expansion)\n\n\nclass ResNeXt(ResNet):\n\n    def __init__(self,\n                 block,\n                 layers,\n                 block_inplanes,\n                 n_input_channels=3,\n                 conv1_t_size=7,\n                 conv1_t_stride=1,\n                 no_max_pool=False,\n                 shortcut_type='B',\n                 cardinality=32,\n                 n_classes=400):\n        block = partialclass(block, cardinality=cardinality)\n        super().__init__(block, layers, block_inplanes, n_input_channels,\n                         conv1_t_size, conv1_t_stride, no_max_pool,\n                         shortcut_type, n_classes)\n\n        self.fc = nn.Linear(cardinality * 32 * block.expansion, n_classes)\n\n\ndef generate_model(model_depth, **kwargs):\n    assert model_depth in [50, 101, 152, 200]\n\n    if model_depth == 50:\n        model = ResNeXt(ResNeXtBottleneck, [3, 4, 6, 3], get_inplanes(),\n                        **kwargs)\n    elif model_depth == 101:\n        model = ResNeXt(ResNeXtBottleneck, [3, 4, 23, 3], get_inplanes(),\n                        **kwargs)\n    elif model_depth == 152:\n        model = ResNeXt(ResNeXtBottleneck, [3, 8, 36, 3], get_inplanes(),\n                        **kwargs)\n    elif model_depth == 200:\n        model = ResNeXt(ResNeXtBottleneck, [3, 24, 36, 3], get_inplanes(),\n                        **kwargs)\n\n    return model\n"""
models/wide_resnet.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom . import resnet\n\n\nclass WideBottleneck(resnet.Bottleneck):\n    expansion = 2\n\n\ndef generate_model(model_depth, k, **kwargs):\n    assert model_depth in [50, 101, 152, 200]\n\n    inplanes = [x * k for x in resnet.get_inplanes()]\n    if model_depth == 50:\n        model = resnet.ResNet(WideBottleneck, [3, 4, 6, 3], inplanes, **kwargs)\n    elif model_depth == 101:\n        model = resnet.ResNet(WideBottleneck, [3, 4, 23, 3], inplanes, **kwargs)\n    elif model_depth == 152:\n        model = resnet.ResNet(WideBottleneck, [3, 8, 36, 3], inplanes, **kwargs)\n    elif model_depth == 200:\n        model = resnet.ResNet(WideBottleneck, [3, 24, 36, 3], inplanes,\n                              **kwargs)\n\n    return model\n'"
util_scripts/__init__.py,0,b''
util_scripts/add_fps_into_activitynet_json.py,0,"b""import sys\nimport json\nimport subprocess\nfrom pathlib import Path\n\nif __name__ == '__main__':\n    video_dir_path = Path(sys.argv[1])\n    json_path = Path(sys.argv[2])\n    if len(sys.argv) > 3:\n        dst_json_path = Path(sys.argv[3])\n    else:\n        dst_json_path = json_path\n\n    with json_path.open('r') as f:\n        json_data = json.load(f)\n\n    for video_file_path in sorted(video_dir_path.iterdir()):\n        file_name = video_file_path.name\n        if '.mp4' not in file_name:\n            continue\n        name = video_file_path.stem\n\n        ffprobe_cmd = ['ffprobe', str(video_file_path)]\n        p = subprocess.Popen(\n            ffprobe_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        res = p.communicate()[1].decode('utf-8')\n\n        fps = float([x for x in res.split(',') if 'fps' in x][0].rstrip('fps'))\n        json_data['database'][name[2:]]['fps'] = fps\n\n    with dst_json_path.open('w') as f:\n        json.dump(json_data, f)"""
util_scripts/eval_accuracy.py,0,"b""import json\nimport argparse\nfrom pathlib import Path\n\n\ndef get_class_labels(data):\n    class_labels_map = {}\n    index = 0\n    for class_label in data['labels']:\n        class_labels_map[class_label] = index\n        index += 1\n    return class_labels_map\n\n\ndef load_ground_truth(ground_truth_path, subset):\n    with ground_truth_path.open('r') as f:\n        data = json.load(f)\n\n    class_labels_map = get_class_labels(data)\n\n    ground_truth = []\n    for video_id, v in data['database'].items():\n        if subset != v['subset']:\n            continue\n        this_label = v['annotations']['label']\n        ground_truth.append((video_id, class_labels_map[this_label]))\n\n    return ground_truth, class_labels_map\n\n\ndef load_result(result_path, top_k, class_labels_map):\n    with result_path.open('r') as f:\n        data = json.load(f)\n\n    result = {}\n    for video_id, v in data['results'].items():\n        labels_and_scores = []\n        for this_result in v:\n            label = class_labels_map[this_result['label']]\n            score = this_result['score']\n            labels_and_scores.append((label, score))\n        labels_and_scores.sort(key=lambda x: x[1], reverse=True)\n        result[video_id] = list(zip(*labels_and_scores[:top_k]))[0]\n    return result\n\n\ndef remove_nonexistent_ground_truth(ground_truth, result):\n    exist_ground_truth = [line for line in ground_truth if line[0] in result]\n\n    return exist_ground_truth\n\n\ndef evaluate(ground_truth_path, result_path, subset, top_k, ignore):\n    print('load ground truth')\n    ground_truth, class_labels_map = load_ground_truth(ground_truth_path,\n                                                       subset)\n    print('number of ground truth: {}'.format(len(ground_truth)))\n\n    print('load result')\n    result = load_result(result_path, top_k, class_labels_map)\n    print('number of result: {}'.format(len(result)))\n\n    n_ground_truth = len(ground_truth)\n    ground_truth = remove_nonexistent_ground_truth(ground_truth, result)\n    if ignore:\n        n_ground_truth = len(ground_truth)\n\n    print('calculate top-{} accuracy'.format(top_k))\n    correct = [1 if line[1] in result[line[0]] else 0 for line in ground_truth]\n    accuracy = sum(correct) / n_ground_truth\n\n    print('top-{} accuracy: {}'.format(top_k, accuracy))\n    return accuracy\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('ground_truth_path', type=Path)\n    parser.add_argument('result_path', type=Path)\n    parser.add_argument('-k', type=int, default=1)\n    parser.add_argument('--subset', type=str, default='validation')\n    parser.add_argument('--save', action='store_true')\n    parser.add_argument(\n        '--ignore',\n        action='store_true',\n        help='ignore nonexistent videos in result')\n\n    args = parser.parse_args()\n\n    accuracy = evaluate(args.ground_truth_path, args.result_path, args.subset,\n                        args.k, args.ignore)\n\n    if args.save:\n        with (args.result_path.parent / 'top{}.txt'.format(\n                args.k)).open('w') as f:\n            f.write(str(accuracy))\n"""
util_scripts/generate_video_hdf5.py,0,"b""import subprocess\nimport argparse\nfrom pathlib import Path\n\nfrom joblib import Parallel, delayed\nimport h5py\nimport numpy as np\n\n\ndef video_process(video_file_path, dst_root_path, ext, fps=-1, size=240):\n    if ext != video_file_path.suffix:\n        return\n\n    ffprobe_cmd = ('ffprobe -v error -select_streams v:0 '\n                   '-of default=noprint_wrappers=1:nokey=1 -show_entries '\n                   'stream=width,height,avg_frame_rate,duration').split()\n    ffprobe_cmd.append(str(video_file_path))\n\n    p = subprocess.run(ffprobe_cmd, capture_output=True)\n    res = p.stdout.decode('utf-8').splitlines()\n    if len(res) < 4:\n        return\n\n    name = video_file_path.stem\n    dst_dir_path = dst_root_path / name\n    dst_dir_path.mkdir(exist_ok=True)\n\n    width = int(res[0])\n    height = int(res[1])\n\n    if width > height:\n        vf_param = f'scale=-1:{size}'\n    else:\n        vf_param = f'scale={size}:-1'\n\n    if fps > 0:\n        vf_param += f',minterpolate={fps}'\n\n    ffmpeg_cmd = ['ffmpeg', '-i', str(video_file_path), '-vf', vf_param]\n    ffmpeg_cmd += ['-threads', '1', f'{dst_dir_path}/image_%05d.jpg']\n    print(ffmpeg_cmd)\n    subprocess.run(ffmpeg_cmd)\n\n    hdf5_path = dst_dir_path.parent / f'{dst_dir_path.name}.hdf5'\n    try:\n        with h5py.File(hdf5_path, 'w') as f:\n            dtype = h5py.special_dtype(vlen='uint8')\n            video = f.create_dataset('video',\n                                     (len(list(dst_dir_path.glob('*.jpg'))),),\n                                     dtype=dtype)\n    except OSError as exc:\n        if 'errno = 36' in exc.args[0]:\n            hdf5_path = dst_dir_path.parent / f'{dst_dir_path.name[:250]}.hdf5'\n            with h5py.File(hdf5_path, 'w') as f:\n                dtype = h5py.special_dtype(vlen='uint8')\n                video = f.create_dataset(\n                    'video', (len(list(dst_dir_path.glob('*.jpg'))),),\n                    dtype=dtype)\n        else:\n            raise\n\n    for i, file_path in enumerate(sorted(dst_dir_path.glob('*.jpg'))):\n        with file_path.open('rb') as f:\n            data = f.read()\n        with h5py.File(hdf5_path, 'r+') as f:\n            video = f['video']\n            video[i] = np.frombuffer(data, dtype='uint8')\n\n    for file_path in dst_dir_path.glob('*.jpg'):\n        file_path.unlink()\n    dst_dir_path.rmdir()\n\n\ndef class_process(class_dir_path, dst_root_path, ext, fps=-1, size=240):\n    if not class_dir_path.is_dir():\n        return\n\n    dst_class_path = dst_root_path / class_dir_path.name\n    dst_class_path.mkdir(exist_ok=True)\n\n    for video_file_path in sorted(class_dir_path.iterdir()):\n        video_process(video_file_path, dst_class_path, ext, fps, size)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('dir_path',\n                        default=None,\n                        type=Path,\n                        help='Directory path of videos')\n    parser.add_argument('dst_path',\n                        default=None,\n                        type=Path,\n                        help='Directory path of jpg videos')\n    parser.add_argument(\n        'dataset',\n        default='',\n        type=str,\n        help='Dataset name (kinetics | mit | ucf101 | hmdb51 | activitynet)')\n    parser.add_argument('--n_jobs',\n                        default=-1,\n                        type=int,\n                        help='Number of parallel jobs')\n    parser.add_argument('--fps',\n                        default=-1,\n                        type=int,\n                        help=('Frame rates of output videos. '\n                              '-1 means original frame rates.'))\n    parser.add_argument('--size',\n                        default=240,\n                        type=int,\n                        help='Frame size of output videos.')\n    args = parser.parse_args()\n\n    if args.dataset in ['kinetics', 'mit', 'activitynet']:\n        ext = '.mp4'\n    else:\n        ext = '.avi'\n\n    if args.dataset == 'activitynet':\n        video_file_paths = [x for x in sorted(args.dir_path.iterdir())]\n        status_list = Parallel(n_jobs=args.n_jobs, backend='threading')(\n            delayed(video_process)(video_file_path, args.dst_path, ext,\n                                   args.fps, args.size)\n            for video_file_path in video_file_paths)\n    else:\n        class_dir_paths = [x for x in sorted(args.dir_path.iterdir())]\n        test_set_video_path = args.dir_path / 'test'\n        if test_set_video_path.exists():\n            class_dir_paths.append(test_set_video_path)\n\n        status_list = Parallel(n_jobs=args.n_jobs, backend='threading')(\n            delayed(class_process)(class_dir_path, args.dst_path, ext, args.fps,\n                                   args.size)\n            for class_dir_path in class_dir_paths)\n"""
util_scripts/generate_video_jpgs.py,0,"b""import subprocess\nimport argparse\nfrom pathlib import Path\n\nfrom joblib import Parallel, delayed\n\n\ndef video_process(video_file_path, dst_root_path, ext, fps=-1, size=240):\n    if ext != video_file_path.suffix:\n        return\n\n    ffprobe_cmd = ('ffprobe -v error -select_streams v:0 '\n                   '-of default=noprint_wrappers=1:nokey=1 -show_entries '\n                   'stream=width,height,avg_frame_rate,duration').split()\n    ffprobe_cmd.append(str(video_file_path))\n\n    p = subprocess.run(ffprobe_cmd, capture_output=True)\n    res = p.stdout.decode('utf-8').splitlines()\n    if len(res) < 4:\n        return\n\n    frame_rate = [float(r) for r in res[2].split('/')]\n    frame_rate = frame_rate[0] / frame_rate[1]\n    duration = float(res[3])\n    n_frames = int(frame_rate * duration)\n\n    name = video_file_path.stem\n    dst_dir_path = dst_root_path / name\n    dst_dir_path.mkdir(exist_ok=True)\n    n_exist_frames = len([\n        x for x in dst_dir_path.iterdir()\n        if x.suffix == '.jpg' and x.name[0] != '.'\n    ])\n\n    if n_exist_frames >= n_frames:\n        return\n\n    width = int(res[0])\n    height = int(res[1])\n\n    if width > height:\n        vf_param = 'scale=-1:{}'.format(size)\n    else:\n        vf_param = 'scale={}:-1'.format(size)\n\n    if fps > 0:\n        vf_param += ',minterpolate={}'.format(fps)\n\n    ffmpeg_cmd = ['ffmpeg', '-i', str(video_file_path), '-vf', vf_param]\n    ffmpeg_cmd += ['-threads', '1', '{}/image_%05d.jpg'.format(dst_dir_path)]\n    print(ffmpeg_cmd)\n    subprocess.run(ffmpeg_cmd)\n    print('\\n')\n\n\ndef class_process(class_dir_path, dst_root_path, ext, fps=-1, size=240):\n    if not class_dir_path.is_dir():\n        return\n\n    dst_class_path = dst_root_path / class_dir_path.name\n    dst_class_path.mkdir(exist_ok=True)\n\n    for video_file_path in sorted(class_dir_path.iterdir()):\n        video_process(video_file_path, dst_class_path, ext, fps, size)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'dir_path', default=None, type=Path, help='Directory path of videos')\n    parser.add_argument(\n        'dst_path',\n        default=None,\n        type=Path,\n        help='Directory path of jpg videos')\n    parser.add_argument(\n        'dataset',\n        default='',\n        type=str,\n        help='Dataset name (kinetics | mit | ucf101 | hmdb51 | activitynet)')\n    parser.add_argument(\n        '--n_jobs', default=-1, type=int, help='Number of parallel jobs')\n    parser.add_argument(\n        '--fps',\n        default=-1,\n        type=int,\n        help=('Frame rates of output videos. '\n              '-1 means original frame rates.'))\n    parser.add_argument(\n        '--size', default=240, type=int, help='Frame size of output videos.')\n    args = parser.parse_args()\n\n    if args.dataset in ['kinetics', 'mit', 'activitynet']:\n        ext = '.mp4'\n    else:\n        ext = '.avi'\n\n    if args.dataset == 'activitynet':\n        video_file_paths = [x for x in sorted(args.dir_path.iterdir())]\n        status_list = Parallel(\n            n_jobs=args.n_jobs,\n            backend='threading')(delayed(video_process)(\n                video_file_path, args.dst_path, ext, args.fps, args.size)\n                                 for video_file_path in video_file_paths)\n    else:\n        class_dir_paths = [x for x in sorted(args.dir_path.iterdir())]\n        test_set_video_path = args.dir_path / 'test'\n        if test_set_video_path.exists():\n            class_dir_paths.append(test_set_video_path)\n\n        status_list = Parallel(\n            n_jobs=args.n_jobs,\n            backend='threading')(delayed(class_process)(\n                class_dir_path, args.dst_path, ext, args.fps, args.size)\n                                 for class_dir_path in class_dir_paths)\n"""
util_scripts/hmdb51_json.py,0,"b""import argparse\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom .utils import get_n_frames\n\n\ndef convert_csv_to_dict(csv_dir_path, split_index):\n    database = {}\n    for file_path in csv_dir_path.iterdir():\n        filename = file_path.name\n        if 'split{}'.format(split_index) not in filename:\n            continue\n\n        data = pd.read_csv(csv_dir_path / filename, delimiter=' ', header=None)\n        keys = []\n        subsets = []\n        for i in range(data.shape[0]):\n            row = data.iloc[i, :]\n            if row[1] == 0:\n                continue\n            elif row[1] == 1:\n                subset = 'training'\n            elif row[1] == 2:\n                subset = 'validation'\n\n            keys.append(row[0].split('.')[0])\n            subsets.append(subset)\n\n        for i in range(len(keys)):\n            key = keys[i]\n            database[key] = {}\n            database[key]['subset'] = subsets[i]\n            label = '_'.join(filename.split('_')[:-2])\n            database[key]['annotations'] = {'label': label}\n\n    return database\n\n\ndef get_labels(csv_dir_path):\n    labels = []\n    for file_path in csv_dir_path.iterdir():\n        labels.append('_'.join(file_path.name.split('_')[:-2]))\n    return sorted(list(set(labels)))\n\n\ndef convert_hmdb51_csv_to_json(csv_dir_path, split_index, video_dir_path,\n                               dst_json_path):\n    labels = get_labels(csv_dir_path)\n    database = convert_csv_to_dict(csv_dir_path, split_index)\n\n    dst_data = {}\n    dst_data['labels'] = labels\n    dst_data['database'] = {}\n    dst_data['database'].update(database)\n\n    for k, v in dst_data['database'].items():\n        if v['annotations'] is not None:\n            label = v['annotations']['label']\n        else:\n            label = 'test'\n\n        video_path = video_dir_path / label / k\n        n_frames = get_n_frames(video_path)\n        v['annotations']['segment'] = (1, n_frames + 1)\n\n    with dst_json_path.open('w') as dst_file:\n        json.dump(dst_data, dst_file)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('dir_path',\n                        default=None,\n                        type=Path,\n                        help='Directory path of HMDB51 annotation files.')\n    parser.add_argument('video_path',\n                        default=None,\n                        type=Path,\n                        help=('Path of video directory (jpg).'\n                              'Using to get n_frames of each video.'))\n    parser.add_argument('dst_dir_path',\n                        default=None,\n                        type=Path,\n                        help='Directory path of dst json file.')\n\n    args = parser.parse_args()\n\n    for split_index in range(1, 4):\n        dst_json_path = args.dst_dir_path / 'hmdb51_{}.json'.format(split_index)\n        convert_hmdb51_csv_to_json(args.dir_path, split_index, args.video_path,\n                                   dst_json_path)\n"""
util_scripts/kinetics_json.py,0,"b""import argparse\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom .utils import get_n_frames, get_n_frames_hdf5\n\n\ndef convert_csv_to_dict(csv_path, subset):\n    data = pd.read_csv(csv_path)\n    keys = []\n    key_labels = []\n    for i in range(data.shape[0]):\n        row = data.iloc[i, :]\n        basename = '%s_%s_%s' % (row['youtube_id'], '%06d' % row['time_start'],\n                                 '%06d' % row['time_end'])\n        keys.append(basename)\n        if subset != 'testing':\n            key_labels.append(row['label'])\n\n    database = {}\n    for i in range(len(keys)):\n        key = keys[i]\n        database[key] = {}\n        database[key]['subset'] = subset\n        if subset != 'testing':\n            label = key_labels[i]\n            database[key]['annotations'] = {'label': label}\n        else:\n            database[key]['annotations'] = {}\n\n    return database\n\n\ndef load_labels(train_csv_path):\n    data = pd.read_csv(train_csv_path)\n    return data['label'].unique().tolist()\n\n\ndef convert_kinetics_csv_to_json(train_csv_path, val_csv_path, test_csv_path,\n                                 video_dir_path, video_type, dst_json_path):\n    labels = load_labels(train_csv_path)\n    train_database = convert_csv_to_dict(train_csv_path, 'training')\n    val_database = convert_csv_to_dict(val_csv_path, 'validation')\n    if test_csv_path.exists():\n        test_database = convert_csv_to_dict(test_csv_path, 'testing')\n\n    dst_data = {}\n    dst_data['labels'] = labels\n    dst_data['database'] = {}\n    dst_data['database'].update(train_database)\n    dst_data['database'].update(val_database)\n    if test_csv_path.exists():\n        dst_data['database'].update(test_database)\n\n    for k, v in dst_data['database'].items():\n        if 'label' in v['annotations']:\n            label = v['annotations']['label']\n        else:\n            label = 'test'\n\n        if video_type == 'jpg':\n            video_path = video_dir_path / label / k\n            if video_path.exists():\n                n_frames = get_n_frames(video_path)\n                v['annotations']['segment'] = (1, n_frames + 1)\n        else:\n            video_path = video_dir_path / label / f'{k}.hdf5'\n            if video_path.exists():\n                n_frames = get_n_frames_hdf5(video_path)\n                v['annotations']['segment'] = (0, n_frames)\n\n    with dst_json_path.open('w') as dst_file:\n        json.dump(dst_data, dst_file)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('dir_path',\n                        default=None,\n                        type=Path,\n                        help=('Directory path including '\n                              'kinetics_train.csv, kinetics_val.csv, '\n                              '(kinetics_test.csv (optional))'))\n    parser.add_argument(\n        'n_classes',\n        default=700,\n        type=int,\n        help='400, 600, or 700 (Kinetics-400, Kinetics-600, or Kinetics-700)')\n    parser.add_argument('video_path',\n                        default=None,\n                        type=Path,\n                        help=('Path of video directory (jpg or hdf5).'\n                              'Using to get n_frames of each video.'))\n    parser.add_argument('video_type',\n                        default='jpg',\n                        type=str,\n                        help=('jpg or hdf5'))\n    parser.add_argument('dst_path',\n                        default=None,\n                        type=Path,\n                        help='Path of dst json file.')\n\n    args = parser.parse_args()\n\n    assert args.video_type in ['jpg', 'hdf5']\n\n    train_csv_path = (args.dir_path /\n                      'kinetics-{}_train.csv'.format(args.n_classes))\n    val_csv_path = (args.dir_path /\n                    'kinetics-{}_val.csv'.format(args.n_classes))\n    test_csv_path = (args.dir_path /\n                     'kinetics-{}_test.csv'.format(args.n_classes))\n\n    convert_kinetics_csv_to_json(train_csv_path, val_csv_path, test_csv_path,\n                                 args.video_path, args.video_type,\n                                 args.dst_path)\n"""
util_scripts/mit_json.py,0,"b""import argparse\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom .utils import get_n_frames\n\n\ndef convert_csv_to_dict(csv_path, subset):\n    data = pd.read_csv(csv_path, header=None)\n    keys = []\n    key_labels = []\n    if subset == 'testing':\n        for i in range(data.shape[0]):\n            basename = data.iloc[i, 0].split('/')\n            assert len(basename) == 1\n            basename = Path(basename[0]).stem\n\n            keys.append(basename)\n    else:\n        for i in range(data.shape[0]):\n            basename = data.iloc[i, 0].split('/')\n            assert len(basename) == 2\n            basename = Path(basename[1]).stem\n\n            keys.append(basename)\n            key_labels.append(data.iloc[i, 1])\n\n    database = {}\n    for i in range(len(keys)):\n        key = keys[i]\n        database[key] = {}\n        database[key]['subset'] = subset\n        if subset != 'testing':\n            label = key_labels[i]\n            database[key]['annotations'] = {'label': label}\n        else:\n            database[key]['annotations'] = {}\n\n    return database\n\n\ndef load_labels(train_csv_path):\n    data = pd.read_csv(train_csv_path, header=None)\n    return data.iloc[:, 0].tolist()\n\n\ndef convert_mit_csv_to_json(class_file_path, train_csv_path, val_csv_path,\n                            test_csv_path, video_dir_path, dst_json_path):\n    labels = load_labels(class_file_path)\n    train_database = convert_csv_to_dict(train_csv_path, 'training')\n    val_database = convert_csv_to_dict(val_csv_path, 'validation')\n    if test_csv_path.exists():\n        test_database = convert_csv_to_dict(test_csv_path, 'testing')\n\n    dst_data = {}\n    dst_data['labels'] = labels\n    dst_data['database'] = {}\n    dst_data['database'].update(train_database)\n    dst_data['database'].update(val_database)\n    if test_csv_path.exists():\n        dst_data['database'].update(test_database)\n\n    for k, v in dst_data['database'].items():\n        if 'label' in v['annotations']:\n            label = v['annotations']['label']\n        else:\n            label = 'test'\n\n        video_path = video_dir_path / label / k\n        n_frames = get_n_frames(video_path)\n        v['annotations']['segment'] = (1, n_frames + 1)\n\n    with dst_json_path.open('w') as dst_file:\n        json.dump(dst_data, dst_file)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'dir_path',\n        default=None,\n        type=Path,\n        help=('Directory path including moments_categories.txt, '\n              'trainingSet.csv, validationSet.csv, '\n              '(testingSet.csv (optional))'))\n    parser.add_argument('video_path',\n                        default=None,\n                        type=Path,\n                        help=('Path of video directory (jpg).'\n                              'Using to get n_frames of each video.'))\n    parser.add_argument('dst_path',\n                        default=None,\n                        type=Path,\n                        help='Path of dst json file.')\n\n    args = parser.parse_args()\n\n    class_file_path = args.dir_path / 'moments_categories.txt'\n    train_csv_path = args.dir_path / 'trainingSet.csv'\n    val_csv_path = args.dir_path / 'validationSet.csv'\n    test_csv_path = args.dir_path / 'testingSet.csv'\n\n    convert_mit_csv_to_json(class_file_path, train_csv_path, val_csv_path,\n                            test_csv_path, args.video_path, args.dst_path)\n"""
util_scripts/remove_dataparallel.py,2,"b""import argparse\nfrom collections import OrderedDict\n\nimport torch\n\nparser = argparse.ArgumentParser()\nparser.add_argument('file_path', type=str)\nparser.add_argument('--dst_file_path', default=None, type=str)\nargs = parser.parse_args()\n\nif args.dst_file_path is None:\n    args.dst_file_path = args.file_path\n\nx = torch.load(args.file_path)\nstate_dict = x['state_dict']\nnew_state_dict = OrderedDict()\n\nfor k, v in state_dict.items():\n    new_k = '.'.join(k.split('.')[1:])\n    new_state_dict[new_k] = v\n\nx['state_dict'] = new_state_dict\n\ntorch.save(x, args.dst_file_path)"""
util_scripts/ucf101_json.py,0,"b""import argparse\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom .utils import get_n_frames\n\n\ndef convert_csv_to_dict(csv_path, subset):\n    data = pd.read_csv(csv_path, delimiter=' ', header=None)\n    keys = []\n    key_labels = []\n    for i in range(data.shape[0]):\n        row = data.iloc[i, :]\n        slash_rows = data.iloc[i, 0].split('/')\n        class_name = slash_rows[0]\n        basename = slash_rows[1].split('.')[0]\n\n        keys.append(basename)\n        key_labels.append(class_name)\n\n    database = {}\n    for i in range(len(keys)):\n        key = keys[i]\n        database[key] = {}\n        database[key]['subset'] = subset\n        label = key_labels[i]\n        database[key]['annotations'] = {'label': label}\n\n    return database\n\n\ndef load_labels(label_csv_path):\n    data = pd.read_csv(label_csv_path, delimiter=' ', header=None)\n    labels = []\n    for i in range(data.shape[0]):\n        labels.append(data.iloc[i, 1])\n    return labels\n\n\ndef convert_ucf101_csv_to_json(label_csv_path, train_csv_path, val_csv_path,\n                               video_dir_path, dst_json_path):\n    labels = load_labels(label_csv_path)\n    train_database = convert_csv_to_dict(train_csv_path, 'training')\n    val_database = convert_csv_to_dict(val_csv_path, 'validation')\n\n    dst_data = {}\n    dst_data['labels'] = labels\n    dst_data['database'] = {}\n    dst_data['database'].update(train_database)\n    dst_data['database'].update(val_database)\n\n    for k, v in dst_data['database'].items():\n        if v['annotations'] is not None:\n            label = v['annotations']['label']\n        else:\n            label = 'test'\n\n        video_path = video_dir_path / label / k\n        n_frames = get_n_frames(video_path)\n        v['annotations']['segment'] = (1, n_frames + 1)\n\n    with dst_json_path.open('w') as dst_file:\n        json.dump(dst_data, dst_file)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('dir_path',\n                        default=None,\n                        type=Path,\n                        help=('Directory path including classInd.txt, '\n                              'trainlist0-.txt, testlist0-.txt'))\n    parser.add_argument('video_path',\n                        default=None,\n                        type=Path,\n                        help=('Path of video directory (jpg).'\n                              'Using to get n_frames of each video.'))\n    parser.add_argument('dst_path',\n                        default=None,\n                        type=Path,\n                        help='Directory path of dst json file.')\n\n    args = parser.parse_args()\n\n    for split_index in range(1, 4):\n        label_csv_path = args.dir_path / 'classInd.txt'\n        train_csv_path = args.dir_path / 'trainlist0{}.txt'.format(split_index)\n        val_csv_path = args.dir_path / 'testlist0{}.txt'.format(split_index)\n        dst_json_path = args.dst_path / 'ucf101_0{}.json'.format(split_index)\n\n        convert_ucf101_csv_to_json(label_csv_path, train_csv_path, val_csv_path,\n                                   args.video_path, dst_json_path)\n"""
util_scripts/utils.py,0,"b""import h5py\n\n\ndef get_n_frames(video_path):\n    return len([\n        x for x in video_path.iterdir()\n        if 'image' in x.name and x.name[0] != '.'\n    ])\n\n\ndef get_n_frames_hdf5(video_path):\n    with h5py.File(video_path, 'r') as f:\n        video_data = f['video']\n        return len(video_data)"""
