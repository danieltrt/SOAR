file_path,api_count,code
classify.py,4,"b""import torch\nfrom torch.autograd import Variable\n\nfrom dataset import Video\nfrom spatial_transforms import (Compose, Normalize, Scale, CenterCrop, ToTensor)\nfrom temporal_transforms import LoopPadding\n\ndef classify_video(video_dir, video_name, class_names, model, opt):\n    assert opt.mode in ['score', 'feature']\n\n    spatial_transform = Compose([Scale(opt.sample_size),\n                                 CenterCrop(opt.sample_size),\n                                 ToTensor(),\n                                 Normalize(opt.mean, [1, 1, 1])])\n    temporal_transform = LoopPadding(opt.sample_duration)\n    data = Video(video_dir, spatial_transform=spatial_transform,\n                 temporal_transform=temporal_transform,\n                 sample_duration=opt.sample_duration)\n    data_loader = torch.utils.data.DataLoader(data, batch_size=opt.batch_size,\n                                              shuffle=False, num_workers=opt.n_threads, pin_memory=True)\n\n    video_outputs = []\n    video_segments = []\n    for i, (inputs, segments) in enumerate(data_loader):\n        inputs = Variable(inputs, volatile=True)\n        outputs = model(inputs)\n\n        video_outputs.append(outputs.cpu().data)\n        video_segments.append(segments)\n\n    video_outputs = torch.cat(video_outputs)\n    video_segments = torch.cat(video_segments)\n    results = {\n        'video': video_name,\n        'clips': []\n    }\n\n    _, max_indices = video_outputs.max(dim=1)\n    for i in range(video_outputs.size(0)):\n        clip_results = {\n            'segment': video_segments[i].tolist(),\n        }\n\n        if opt.mode == 'score':\n            clip_results['label'] = class_names[max_indices[i]]\n            clip_results['scores'] = video_outputs[i].tolist()\n        elif opt.mode == 'feature':\n            clip_results['features'] = video_outputs[i].tolist()\n\n        results['clips'].append(clip_results)\n\n    return results\n"""
dataset.py,3,"b'import torch\nimport torch.utils.data as data\nfrom PIL import Image\nimport os\nimport math\nimport functools\nimport copy\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        with Image.open(f) as img:\n            return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef get_default_image_loader():\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        import accimage\n        return accimage_loader\n    else:\n        return pil_loader\n\n\ndef video_loader(video_dir_path, frame_indices, image_loader):\n    video = []\n    for i in frame_indices:\n        image_path = os.path.join(video_dir_path, \'image_{:05d}.jpg\'.format(i))\n        if os.path.exists(image_path):\n            video.append(image_loader(image_path))\n        else:\n            return video\n\n    return video\n\n\ndef get_default_video_loader():\n    image_loader = get_default_image_loader()\n    return functools.partial(video_loader, image_loader=image_loader)\n\n\ndef load_annotation_data(data_file_path):\n    with open(data_file_path, \'r\') as data_file:\n        return json.load(data_file)\n\n\ndef get_class_labels(data):\n    class_labels_map = {}\n    index = 0\n    for class_label in data[\'labels\']:\n        class_labels_map[class_label] = index\n        index += 1\n    return class_labels_map\n\n\ndef get_video_names_and_annotations(data, subset):\n    video_names = []\n    annotations = []\n\n    for key, value in data[\'database\'].items():\n        this_subset = value[\'subset\']\n        if this_subset == subset:\n            if subset == \'testing\':\n                video_names.append(\'test/{}\'.format(key))\n            else:\n                label = value[\'annotations\'][\'label\']\n                video_names.append(\'{}/{}\'.format(label, key))\n                annotations.append(value[\'annotations\'])\n\n    return video_names, annotations\n\n\ndef make_dataset(video_path, sample_duration):\n    dataset = []\n\n    n_frames = len(os.listdir(video_path))\n\n    begin_t = 1\n    end_t = n_frames\n    sample = {\n        \'video\': video_path,\n        \'segment\': [begin_t, end_t],\n        \'n_frames\': n_frames,\n    }\n\n    step = sample_duration\n    for i in range(1, (n_frames - sample_duration + 1), step):\n        sample_i = copy.deepcopy(sample)\n        sample_i[\'frame_indices\'] = list(range(i, i + sample_duration))\n        sample_i[\'segment\'] = torch.IntTensor([i, i + sample_duration - 1])\n        dataset.append(sample_i)\n\n    return dataset\n\n\nclass Video(data.Dataset):\n    def __init__(self, video_path,\n                 spatial_transform=None, temporal_transform=None,\n                 sample_duration=16, get_loader=get_default_video_loader):\n        self.data = make_dataset(video_path, sample_duration)\n\n        self.spatial_transform = spatial_transform\n        self.temporal_transform = temporal_transform\n        self.loader = get_loader()\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is class_index of the target class.\n        """"""\n        path = self.data[index][\'video\']\n\n        frame_indices = self.data[index][\'frame_indices\']\n        if self.temporal_transform is not None:\n            frame_indices = self.temporal_transform(frame_indices)\n        clip = self.loader(path, frame_indices)\n        if self.spatial_transform is not None:\n            clip = [self.spatial_transform(img) for img in clip]\n        clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n\n        target = self.data[index][\'segment\']\n\n        return clip, target\n\n    def __len__(self):\n        return len(self.data)\n'"
main.py,1,"b'import os\nimport sys\nimport json\nimport subprocess\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom opts import parse_opts\nfrom model import generate_model\nfrom mean import get_mean\nfrom classify import classify_video\n\nif __name__==""__main__"":\n    opt = parse_opts()\n    opt.mean = get_mean()\n    opt.arch = \'{}-{}\'.format(opt.model_name, opt.model_depth)\n    opt.sample_size = 112\n    opt.sample_duration = 16\n    opt.n_classes = 400\n\n    model = generate_model(opt)\n    print(\'loading model {}\'.format(opt.model))\n    model_data = torch.load(opt.model)\n    assert opt.arch == model_data[\'arch\']\n    model.load_state_dict(model_data[\'state_dict\'])\n    model.eval()\n    if opt.verbose:\n        print(model)\n\n    input_files = []\n    with open(opt.input, \'r\') as f:\n        for row in f:\n            input_files.append(row[:-1])\n\n    class_names = []\n    with open(\'class_names_list\') as f:\n        for row in f:\n            class_names.append(row[:-1])\n\n    ffmpeg_loglevel = \'quiet\'\n    if opt.verbose:\n        ffmpeg_loglevel = \'info\'\n\n    if os.path.exists(\'tmp\'):\n        subprocess.call(\'rm -rf tmp\', shell=True)\n\n    outputs = []\n    for input_file in input_files:\n        video_path = os.path.join(opt.video_root, input_file)\n        if os.path.exists(video_path):\n            print(video_path)\n            subprocess.call(\'mkdir tmp\', shell=True)\n            subprocess.call(\'ffmpeg -i {} tmp/image_%05d.jpg\'.format(video_path),\n                            shell=True)\n\n            result = classify_video(\'tmp\', input_file, class_names, model, opt)\n            outputs.append(result)\n\n            subprocess.call(\'rm -rf tmp\', shell=True)\n        else:\n            print(\'{} does not exist\'.format(input_file))\n\n    if os.path.exists(\'tmp\'):\n        subprocess.call(\'rm -rf tmp\', shell=True)\n\n    with open(opt.output, \'w\') as f:\n        json.dump(outputs, f)\n'"
mean.py,0,"b'def get_mean():\n    return [114.7748, 107.7354, 99.4750]\n'"
model.py,0,"b""import torch\nfrom torch import nn\n\nfrom models import resnet, pre_act_resnet, wide_resnet, resnext, densenet\n\n\ndef generate_model(opt):\n    assert opt.mode in ['score', 'feature']\n    if opt.mode == 'score':\n        last_fc = True\n    elif opt.mode == 'feature':\n        last_fc = False\n\n    assert opt.model_name in ['resnet', 'preresnet', 'wideresnet', 'resnext', 'densenet']\n\n    if opt.model_name == 'resnet':\n        assert opt.model_depth in [10, 18, 34, 50, 101, 152, 200]\n\n        if opt.model_depth == 10:\n            model = resnet.resnet10(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                    sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                    last_fc=last_fc)\n        elif opt.model_depth == 18:\n            model = resnet.resnet18(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                    sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                    last_fc=last_fc)\n        elif opt.model_depth == 34:\n            model = resnet.resnet34(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                    sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                    last_fc=last_fc)\n        elif opt.model_depth == 50:\n            model = resnet.resnet50(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                    sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                    last_fc=last_fc)\n        elif opt.model_depth == 101:\n            model = resnet.resnet101(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                     sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                     last_fc=last_fc)\n        elif opt.model_depth == 152:\n            model = resnet.resnet152(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                     sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                     last_fc=last_fc)\n        elif opt.model_depth == 200:\n            model = resnet.resnet200(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                     sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                     last_fc=last_fc)\n    elif opt.model_name == 'wideresnet':\n        assert opt.model_depth in [50]\n\n        if opt.model_depth == 50:\n            model = wide_resnet.resnet50(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut, k=opt.wide_resnet_k,\n                                         sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                         last_fc=last_fc)\n    elif opt.model_name == 'resnext':\n        assert opt.model_depth in [50, 101, 152]\n\n        if opt.model_depth == 50:\n            model = resnext.resnet50(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut, cardinality=opt.resnext_cardinality,\n                                     sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                     last_fc=last_fc)\n        elif opt.model_depth == 101:\n            model = resnext.resnet101(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut, cardinality=opt.resnext_cardinality,\n                                      sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                      last_fc=last_fc)\n        elif opt.model_depth == 152:\n            model = resnext.resnet152(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut, cardinality=opt.resnext_cardinality,\n                                      sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                      last_fc=last_fc)\n    elif opt.model_name == 'preresnet':\n        assert opt.model_depth in [18, 34, 50, 101, 152, 200]\n\n        if opt.model_depth == 18:\n            model = pre_act_resnet.resnet18(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                            sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                            last_fc=last_fc)\n        elif opt.model_depth == 34:\n            model = pre_act_resnet.resnet34(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                            sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                            last_fc=last_fc)\n        elif opt.model_depth == 50:\n            model = pre_act_resnet.resnet50(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                            sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                            last_fc=last_fc)\n        elif opt.model_depth == 101:\n            model = pre_act_resnet.resnet101(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                             sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                             last_fc=last_fc)\n        elif opt.model_depth == 152:\n            model = pre_act_resnet.resnet152(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                             sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                             last_fc=last_fc)\n        elif opt.model_depth == 200:\n            model = pre_act_resnet.resnet200(num_classes=opt.n_classes, shortcut_type=opt.resnet_shortcut,\n                                             sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                             last_fc=last_fc)\n    elif opt.model_name == 'densenet':\n        assert opt.model_depth in [121, 169, 201, 264]\n\n        if opt.model_depth == 121:\n            model = densenet.densenet121(num_classes=opt.n_classes,\n                                         sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                         last_fc=last_fc)\n        elif opt.model_depth == 169:\n            model = densenet.densenet169(num_classes=opt.n_classes,\n                                         sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                         last_fc=last_fc)\n        elif opt.model_depth == 201:\n            model = densenet.densenet201(num_classes=opt.n_classes,\n                                         sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                         last_fc=last_fc)\n        elif opt.model_depth == 264:\n            model = densenet.densenet264(num_classes=opt.n_classes,\n                                         sample_size=opt.sample_size, sample_duration=opt.sample_duration,\n                                         last_fc=last_fc)\n\n    if not opt.no_cuda:\n        model = model.cuda()\n        model = nn.DataParallel(model, device_ids=None)\n\n    return model\n"""
opts.py,0,"b""import argparse\n\ndef parse_opts():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input', default='input', type=str, help='Input file path')\n    parser.add_argument('--video_root', default='', type=str, help='Root path of input videos')\n    parser.add_argument('--model', default='', type=str, help='Model file path')\n    parser.add_argument('--output', default='output.json', type=str, help='Output file path')\n    parser.add_argument('--mode', default='score', type=str, help='Mode (score | feature). score outputs class scores. feature outputs features (after global average pooling).')\n    parser.add_argument('--batch_size', default=32, type=int, help='Batch Size')\n    parser.add_argument('--n_threads', default=4, type=int, help='Number of threads for multi-thread loading')\n    parser.add_argument('--model_name', default='resnet', type=str, help='Currently only support resnet')\n    parser.add_argument('--model_depth', default=34, type=int, help='Depth of resnet (10 | 18 | 34 | 50 | 101)')\n    parser.add_argument('--resnet_shortcut', default='A', type=str, help='Shortcut type of resnet (A | B)')\n    parser.add_argument('--wide_resnet_k', default=2, type=int, help='Wide resnet k')\n    parser.add_argument('--resnext_cardinality', default=32, type=int, help='ResNeXt cardinality')\n    parser.add_argument('--no_cuda', action='store_true', help='If true, cuda is not used.')\n    parser.set_defaults(verbose=False)\n    parser.add_argument('--verbose', action='store_true', help='')\n    parser.set_defaults(verbose=False)\n\n    args = parser.parse_args()\n\n    return args\n"""
spatial_transforms.py,8,"b'import random\nimport math\nimport numbers\nimport collections\nimport numpy as np\nimport torch\nfrom PIL import Image, ImageOps\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\n\n\nclass Compose(object):\n    """"""Composes several transforms together.\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n    Example:\n        >>> transforms.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n\nclass ToTensor(object):\n    """"""Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n\n    def __call__(self, pic):\n        """"""\n        Args:\n            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n        Returns:\n            Tensor: Converted image.\n        """"""\n        if isinstance(pic, np.ndarray):\n            # handle numpy array\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            # backward compatibility\n            return img.float()\n\n        if accimage is not None and isinstance(pic, accimage.Image):\n            nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n            pic.copyto(nppic)\n            return torch.from_numpy(nppic)\n\n        # handle PIL Image\n        if pic.mode == \'I\':\n            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n        elif pic.mode == \'I;16\':\n            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n        else:\n            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == \'YCbCr\':\n            nchannel = 3\n        elif pic.mode == \'I;16\':\n            nchannel = 1\n        else:\n            nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n        # put it from HWC to CHW format\n        # yikes, this transpose takes 80% of the loading time/CPU\n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        if isinstance(img, torch.ByteTensor):\n            return img.float()\n        else:\n            return img\n\n\nclass Normalize(object):\n    """"""Normalize an tensor image with mean and standard deviation.\n    Given mean: (R, G, B) and std: (R, G, B),\n    will normalize each channel of the torch.*Tensor, i.e.\n    channel = (channel - mean) / std\n    Args:\n        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n        std (sequence): Sequence of standard deviations for R, G, B channels\n            respecitvely.\n    """"""\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        """"""\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        """"""\n        # TODO: make efficient\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.sub_(m).div_(s)\n        return tensor\n\n\nclass Scale(object):\n    """"""Rescale the input PIL.Image to the given size.\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (w, h), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL.Image): Image to be scaled.\n        Returns:\n            PIL.Image: Rescaled image.\n        """"""\n        if isinstance(self.size, int):\n            w, h = img.size\n            if (w <= h and w == self.size) or (h <= w and h == self.size):\n                return img\n            if w < h:\n                ow = self.size\n                oh = int(self.size * h / w)\n                return img.resize((ow, oh), self.interpolation)\n            else:\n                oh = self.size\n                ow = int(self.size * w / h)\n                return img.resize((ow, oh), self.interpolation)\n        else:\n            return img.resize(self.size, self.interpolation)\n\n\nclass CenterCrop(object):\n    """"""Crops the given PIL.Image at the center.\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    """"""\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL.Image): Image to be cropped.\n        Returns:\n            PIL.Image: Cropped image.\n        """"""\n        w, h = img.size\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        return img.crop((x1, y1, x1 + tw, y1 + th))\n'"
temporal_transforms.py,0,"b'import random\nimport math\n\n\nclass LoopPadding(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n        out = frame_indices\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n\n\nclass TemporalCenterCrop(object):\n    """"""Temporally crop the given frame indices at a center.\n\n    If the number of frames is less than the size,\n    loop the indices as many times as necessary to satisfy the size.\n\n    Args:\n        size (int): Desired output size of the crop.\n    """"""\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frame_indices):\n        """"""\n        Args:\n            frame_indices (list): frame indices to be cropped.\n        Returns:\n            list: Cropped frame indices.\n        """"""\n\n        center_index = len(frame_indices) // 2\n        begin_index = max(0, center_index - (self.size // 2))\n        end_index = min(begin_index + self.size, len(frame_indices))\n\n        out = frame_indices[begin_index:end_index]\n\n        for index in out:\n            if len(out) >= self.size:\n                break\n            out.append(index)\n\n        return out\n'"
test.py,4,"b""import torch\nfrom torch.autograd import Variable\nimport time\nimport os\nimport sys\nimport json\n\nfrom utils import AverageMeter\n\n\ndef calculate_video_results(output_buffer, video_id, test_results, class_names):\n    video_outputs = torch.stack(output_buffer)\n    average_scores = torch.mean(video_outputs, dim=0)\n    sorted_scores, locs = torch.topk(average_scores, k=10)\n\n    video_results = []\n    for i in range(sorted_scores.size(0)):\n        video_results.append({'label': class_names[locs[i]], 'score': sorted_scores[i]})\n\n    test_results['results'][video_id] = video_results\n\n\ndef test(data_loader, model, opt, class_names):\n    print('test')\n\n    model.eval()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n\n    end_time = time.time()\n    output_buffer = []\n    previous_video_id = ''\n    test_results = {'results': {}}\n    for i, (inputs, targets) in enumerate(data_loader):\n        data_time.update(time.time() - end_time)\n\n        inputs = Variable(inputs, volatile=True)\n        outputs = model(inputs)\n\n        for j in range(outputs.size(0)):\n            if not (i == 0 and j == 0) and targets[j] != previous_video_id:\n                calculate_video_results(output_buffer, previous_video_id,\n                                        test_results, class_names)\n                output_buffer = []\n            output_buffer.append(outputs[j].data.cpu())\n            previous_video_id = targets[j]\n\n        if (i % 100) == 0:\n            with open(os.path.join(opt.result_path,\n                                   '{}.json'.format(opt.test_subset)),\n                      'w') as f:\n                json.dump(test_results, f)\n\n        batch_time.update(time.time() - end_time)\n        end_time = time.time()\n\n        print('[{}/{}]\\t'\n              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'.format(\n                  i + 1, len(data_loader), batch_time=batch_time, data_time=data_time))\n    with open(os.path.join(opt.result_path,\n                           '{}.json'.format(opt.test_subset)),\n              'w') as f:\n        json.dump(test_results, f)\n"""
train.py,2,"b""import torch\nfrom torch.autograd import Variable\nimport time\nimport os\nimport sys\n\nfrom utils import AverageMeter, calculate_accuracy\n\n\ndef train_epoch(epoch, data_loader, model, criterion, optimizer, opt,\n                epoch_logger, batch_logger):\n    print('train at epoch {}'.format(epoch))\n\n    model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n\n    end_time = time.time()\n    for i, (inputs, targets) in enumerate(data_loader):\n        data_time.update(time.time() - end_time)\n\n        if not opt.no_cuda:\n            targets = targets.cuda(async=True)\n        inputs = Variable(inputs)\n        targets = Variable(targets)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        acc = calculate_accuracy(outputs, targets)\n\n        losses.update(loss.data[0], inputs.size(0))\n        accuracies.update(acc, inputs.size(0))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        batch_time.update(time.time() - end_time)\n        end_time = time.time()\n\n        batch_logger.log({\n            'epoch': epoch,\n            'batch': i + 1,\n            'iter': (epoch - 1) * len(data_loader) + (i + 1),\n            'loss': losses.val,\n            'acc': accuracies.val,\n            'lr': optimizer.param_groups[0]['lr']\n        })\n\n        print('Epoch: [{0}][{1}/{2}]\\t'\n              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n              'Acc {acc.val:.3f} ({acc.avg:.3f})'.format(\n                  epoch, i + 1, len(data_loader), batch_time=batch_time,\n                  data_time=data_time, loss=losses, acc=accuracies))\n\n    epoch_logger.log({\n        'epoch': epoch,\n        'loss': losses.avg,\n        'acc': accuracies.avg,\n        'lr': optimizer.param_groups[0]['lr']\n    })\n\n    if epoch % opt.checkpoint == 0:\n        save_file_path = os.path.join(opt.result_path, 'save_{}.pth'.format(epoch))\n        states = {\n            'epoch': epoch + 1,\n            'arch': opt.arch,\n            'state_dict': model.state_dict(),\n            'optimizer' : optimizer.state_dict(),\n        }\n        torch.save(states, save_file_path)\n"""
validation.py,1,"b""import torch\nfrom torch.autograd import Variable\nimport time\nimport sys\n\nfrom utils import AverageMeter, calculate_accuracy\n\n\ndef val_epoch(epoch, data_loader, model, criterion, opt, logger):\n    print('validation at epoch {}'.format(epoch))\n\n    model.eval()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n\n    end_time = time.time()\n    for i, (inputs, targets) in enumerate(data_loader):\n        data_time.update(time.time() - end_time)\n\n        if not opt.no_cuda:\n            targets = targets.cuda(async=True)\n        inputs = Variable(inputs, volatile=True)\n        targets = Variable(targets, volatile=True)\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        acc = calculate_accuracy(outputs, targets)\n\n        losses.update(loss.data[0], inputs.size(0))\n        accuracies.update(acc, inputs.size(0))\n\n        batch_time.update(time.time() - end_time)\n        end_time = time.time()\n\n        print('Epoch: [{0}][{1}/{2}]\\t'\n              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n              'Acc {acc.val:.3f} ({acc.avg:.3f})'.format(\n                  epoch, i + 1, len(data_loader), batch_time=batch_time,\n                  data_time=data_time, loss=losses, acc=accuracies))\n\n    logger.log({\n        'epoch': epoch,\n        'loss': losses.avg,\n        'acc': accuracies.avg\n    })\n\n    return losses.avg\n"""
generate_result_video/generate_result_video.py,0,"b""import os\nimport sys\nimport json\nimport subprocess\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\n\n\ndef get_fps(video_file_path, frames_directory_path):\n    p = subprocess.Popen('ffprobe {}'.format(video_file_path),\n                         shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    _, res = p.communicate()\n    res = res.decode('utf-8')\n\n    duration_index = res.find('Duration:')\n    duration_str = res[(duration_index + 10):(duration_index + 21)]\n    hour = float(duration_str[0:2])\n    minute = float(duration_str[3:5])\n    sec = float(duration_str[6:10])\n    total_sec = hour * 3600 + minute * 60 + sec\n\n    n_frames = len(os.listdir(frames_directory_path))\n    fps = round(n_frames / total_sec, 2)\n    return fps\n\n\nif __name__ == '__main__':\n    result_json_path = sys.argv[1]\n    video_root_path = sys.argv[2]\n    dst_directory_path = sys.argv[3]\n    if not os.path.exists(dst_directory_path):\n        subprocess.call('mkdir -p {}'.format(dst_directory_path), shell=True)\n    class_name_path = sys.argv[4]\n    temporal_unit = int(sys.argv[5])\n\n    with open(result_json_path, 'r') as f:\n        results = json.load(f)\n\n    with open(class_name_path, 'r') as f:\n        class_names = []\n        for row in f:\n            class_names.append(row[:-1])\n\n    for index in range(len(results)):\n        video_path = os.path.join(video_root_path, results[index]['video'])\n        print(video_path)\n\n        clips = results[index]['clips']\n        unit_classes = []\n        unit_segments = []\n        if temporal_unit == 0:\n            unit = len(clips)\n        else:\n            unit = temporal_unit\n        for i in range(0, len(clips), unit):\n            n_elements = min(unit, len(clips) - i)\n            scores = np.array(clips[i]['scores'])\n            for j in range(i, min(i + unit, len(clips))):\n                scores += np.array(clips[i]['scores'])\n            scores /= n_elements\n            unit_classes.append(class_names[np.argmax(scores)])\n            unit_segments.append([clips[i]['segment'][0],\n                                  clips[i + n_elements - 1]['segment'][1]])\n\n        if os.path.exists('tmp'):\n            subprocess.call('rm -rf tmp', shell=True)\n        subprocess.call('mkdir tmp', shell=True)\n\n        subprocess.call('ffmpeg -i {} tmp/image_%05d.jpg'.format(video_path), shell=True)\n\n        fps = get_fps(video_path, 'tmp')\n\n        for i in range(len(unit_classes)):\n            for j in range(unit_segments[i][0], unit_segments[i][1] + 1):\n                image = Image.open('tmp/image_{:05}.jpg'.format(j)).convert('RGB')\n                min_length = min(image.size)\n                font_size = int(min_length * 0.05)\n                font = ImageFont.truetype(os.path.join(os.path.dirname(__file__),\n                                                       'SourceSansPro-Regular.ttf'),\n                                          font_size)\n                d = ImageDraw.Draw(image)\n                textsize = d.textsize(unit_classes[i], font=font)\n                x = int(font_size * 0.5)\n                y = int(font_size * 0.25)\n                x_offset = x\n                y_offset = y\n                rect_position = (x, y, x + textsize[0] + x_offset * 2,\n                                 y + textsize[1] + y_offset * 2)\n                d.rectangle(rect_position, fill=(30, 30, 30))\n                d.text((x + x_offset, y + y_offset), unit_classes[i],\n                       font=font, fill=(235, 235, 235))\n                image.save('tmp/image_{:05}_pred.jpg'.format(j))\n\n        dst_file_path = os.path.join(dst_directory_path, video_path.split('/')[-1])\n        subprocess.call('ffmpeg -y -r {} -i tmp/image_%05d_pred.jpg -b:v 1000k {}'.format(fps, dst_file_path),\n                        shell=True)\n\n        if os.path.exists('tmp'):\n            subprocess.call('rm -rf tmp', shell=True)\n"""
models/densenet.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport math\n\n__all__ = [\'DenseNet\', \'densenet121\', \'densenet169\', \'densenet201\', \'densenet264\']\n\n\ndef densenet121(**kwargs):\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n                     **kwargs)\n    return model\n\n\ndef densenet169(**kwargs):\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32),\n                     **kwargs)\n    return model\n\n\ndef densenet201(**kwargs):\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32),\n                     **kwargs)\n    return model\n\n\ndef densenet264(**kwargs):\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 64, 48),\n                     **kwargs)\n    return model\n\n\ndef get_fine_tuning_parameters(model, ft_begin_index):\n    if ft_begin_index == 0:\n        return model.parameters()\n\n    ft_module_names = []\n    for i in range(ft_begin_index, 5):\n        ft_module_names.append(\'denseblock{}\'.format(ft_begin_index))\n        ft_module_names.append(\'transition{}\'.format(ft_begin_index))\n    ft_module_names.append(\'norm5\')\n    ft_module_names.append(\'classifier\')\n\n    parameters = []\n    for k, v in model.named_parameters():\n        for ft_module in ft_module_names:\n            if ft_module in k:\n                parameters.append({\'params\': v})\n                break\n        else:\n            parameters.append({\'params\': v, \'lr\': 0.0})\n\n    return parameters\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm.1\', nn.BatchNorm3d(num_input_features))\n        self.add_module(\'relu.1\', nn.ReLU(inplace=True))\n        self.add_module(\'conv.1\', nn.Conv3d(num_input_features, bn_size * growth_rate,\n                                            kernel_size=1, stride=1, bias=False))\n        self.add_module(\'norm.2\', nn.BatchNorm3d(bn_size * growth_rate))\n        self.add_module(\'relu.2\', nn.ReLU(inplace=True))\n        self.add_module(\'conv.2\', nn.Conv3d(bn_size * growth_rate, growth_rate,\n                                            kernel_size=3, stride=1, padding=1, bias=False))\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm3d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv3d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool3d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    """"""Densenet-BC model class\n    Args:\n        growth_rate (int) - how many filters to add each layer (k in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    """"""\n    def __init__(self, sample_size, sample_duration, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, last_fc=True):\n\n        super(DenseNet, self).__init__()\n\n        self.last_fc = last_fc\n\n        self.sample_size = sample_size\n        self.sample_duration = sample_duration\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv3d(3, num_init_features, kernel_size=7,\n                                stride=(1, 2, 2), padding=(3, 3, 3), bias=False)),\n            (\'norm0\', nn.BatchNorm3d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool3d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        last_duration = math.ceil(self.sample_duration / 16)\n        last_size = math.floor(self.sample_size / 32)\n        out = F.avg_pool3d(out, kernel_size=(last_duration, last_size, last_size)).view(features.size(0), -1)\n        if self.last_fc:\n            out = self.classifier(out)\n        return out\n'"
models/pre_act_resnet.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\n\n__all__ = [\'PreActivationResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'resnet152\', \'resnet200\']\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(in_planes, out_planes, kernel_size=3,\n                     stride=stride, padding=1, bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(out.size(0), planes - out.size(1),\n                             out.size(2), out.size(3),\n                             out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass PreActivationBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(PreActivationBasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm3d(inplanes)\n        self.conv1 = conv3x3x3(inplanes, planes, stride)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass PreActivationBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(PreActivationBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm3d(inplanes)\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass PreActivationResNet(nn.Module):\n\n    def __init__(self, block, layers, sample_size, sample_duration, shortcut_type=\'B\', num_classes=400, last_fc=True):\n        self.last_fc = last_fc\n\n        self.inplanes = 64\n        super(PreActivationResNet, self).__init__()\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2),\n                               padding=(3, 3, 3), bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n        self.layer2 = self._make_layer(block, 128, layers[1], shortcut_type, stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], shortcut_type, stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], shortcut_type, stride=2)\n        last_duration = math.ceil(sample_duration / 16)\n        last_size = math.ceil(sample_size / 32)\n        self.avgpool = nn.AvgPool3d((last_duration, last_size, last_size), stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == \'A\':\n                downsample = partial(downsample_basic_block,\n                                     planes=planes * block.expansion,\n                                     stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(self.inplanes, planes * block.expansion,\n                              kernel_size=1, stride=stride, bias=False),\n                    nn.BatchNorm3d(planes * block.expansion)\n                )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        if self.last_fc:\n            x = self.fc(x)\n\n        return x\n\ndef get_fine_tuning_parameters(model, ft_begin_index):\n    if ft_begin_index == 0:\n        return model.parameters()\n\n    ft_module_names = []\n    for i in range(ft_begin_index, 5):\n        ft_module_names.append(\'layer{}\'.format(ft_begin_index))\n    ft_module_names.append(\'fc\')\n\n    parameters = []\n    for k, v in model.named_parameters():\n        for ft_module in ft_module_names:\n            if ft_module in k:\n                parameters.append({\'params\': v})\n                break\n        else:\n            parameters.append({\'params\': v, \'lr\': 0.0})\n\n    return parameters\n\ndef resnet18(**kwargs):\n    """"""Constructs a ResNet-18 model.\n    """"""\n    model = PreActivationResNet(PreActivationBasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\ndef resnet34(**kwargs):\n    """"""Constructs a ResNet-34 model.\n    """"""\n    model = PreActivationResNet(PreActivationBasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\n\ndef resnet50(**kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = PreActivationResNet(PreActivationBottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\ndef resnet101(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = PreActivationResNet(PreActivationBottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\ndef resnet152(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = PreActivationResNet(PreActivationBottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n\ndef resnet200(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = PreActivationResNet(PreActivationBottleneck, [3, 24, 36, 3], **kwargs)\n    return model\n'"
models/resnet.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\n\n__all__ = [\'ResNet\', \'resnet10\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'resnet152\', \'resnet200\']\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(in_planes, out_planes, kernel_size=3,\n                     stride=stride, padding=1, bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(out.size(0), planes - out.size(1),\n                             out.size(2), out.size(3),\n                             out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, sample_size, sample_duration, shortcut_type=\'B\', num_classes=400, last_fc=True):\n        self.last_fc = last_fc\n\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2),\n                               padding=(3, 3, 3), bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n        self.layer2 = self._make_layer(block, 128, layers[1], shortcut_type, stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], shortcut_type, stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], shortcut_type, stride=2)\n        last_duration = math.ceil(sample_duration / 16)\n        last_size = math.ceil(sample_size / 32)\n        self.avgpool = nn.AvgPool3d((last_duration, last_size, last_size), stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == \'A\':\n                downsample = partial(downsample_basic_block,\n                                     planes=planes * block.expansion,\n                                     stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(self.inplanes, planes * block.expansion,\n                              kernel_size=1, stride=stride, bias=False),\n                    nn.BatchNorm3d(planes * block.expansion)\n                )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        if self.last_fc:\n            x = self.fc(x)\n\n        return x\n\n\ndef get_fine_tuning_parameters(model, ft_begin_index):\n    if ft_begin_index == 0:\n        return model.parameters()\n\n    ft_module_names = []\n    for i in range(ft_begin_index, 5):\n        ft_module_names.append(\'layer{}\'.format(ft_begin_index))\n    ft_module_names.append(\'fc\')\n\n    parameters = []\n    for k, v in model.named_parameters():\n        for ft_module in ft_module_names:\n            if ft_module in k:\n                parameters.append({\'params\': v})\n                break\n        else:\n            parameters.append({\'params\': v, \'lr\': 0.0})\n\n    return parameters\n\n\ndef resnet10(**kwargs):\n    """"""Constructs a ResNet-18 model.\n    """"""\n    model = ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\n    return model\n\ndef resnet18(**kwargs):\n    """"""Constructs a ResNet-18 model.\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\ndef resnet34(**kwargs):\n    """"""Constructs a ResNet-34 model.\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n\ndef resnet50(**kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\ndef resnet101(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\ndef resnet152(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n\ndef resnet200(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)\n    return model\n'"
models/resnext.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\n\n__all__ = [\'ResNeXt\', \'resnet50\', \'resnet101\']\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(in_planes, out_planes, kernel_size=3,\n                     stride=stride, padding=1, bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(out.size(0), planes - out.size(1),\n                             out.size(2), out.size(3),\n                             out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass ResNeXtBottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, cardinality, stride=1, downsample=None):\n        super(ResNeXtBottleneck, self).__init__()\n        mid_planes = cardinality * int(planes / 32)\n        self.conv1 = nn.Conv3d(inplanes, mid_planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(mid_planes)\n        self.conv2 = nn.Conv3d(mid_planes, mid_planes, kernel_size=3, stride=stride,\n                               padding=1, groups=cardinality, bias=False)\n        self.bn2 = nn.BatchNorm3d(mid_planes)\n        self.conv3 = nn.Conv3d(mid_planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNeXt(nn.Module):\n\n    def __init__(self, block, layers, sample_size, sample_duration, shortcut_type=\'B\', cardinality=32, num_classes=400, last_fc=True):\n        self.last_fc = last_fc\n\n        self.inplanes = 64\n        super(ResNeXt, self).__init__()\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2),\n                               padding=(3, 3, 3), bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 128, layers[0], shortcut_type, cardinality)\n        self.layer2 = self._make_layer(block, 256, layers[1], shortcut_type, cardinality, stride=2)\n        self.layer3 = self._make_layer(block, 512, layers[2], shortcut_type, cardinality, stride=2)\n        self.layer4 = self._make_layer(block, 1024, layers[3], shortcut_type, cardinality, stride=2)\n        last_duration = math.ceil(sample_duration / 16)\n        last_size = math.ceil(sample_size / 32)\n        self.avgpool = nn.AvgPool3d((last_duration, last_size, last_size), stride=1)\n        self.fc = nn.Linear(cardinality * 32 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, cardinality, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == \'A\':\n                downsample = partial(downsample_basic_block,\n                                     planes=planes * block.expansion,\n                                     stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(self.inplanes, planes * block.expansion,\n                              kernel_size=1, stride=stride, bias=False),\n                    nn.BatchNorm3d(planes * block.expansion)\n                )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, cardinality, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, cardinality))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        if self.last_fc:\n            x = self.fc(x)\n\n        return x\n\ndef get_fine_tuning_parameters(model, ft_begin_index):\n    if ft_begin_index == 0:\n        return model.parameters()\n\n    ft_module_names = []\n    for i in range(ft_begin_index, 5):\n        ft_module_names.append(\'layer{}\'.format(ft_begin_index))\n    ft_module_names.append(\'fc\')\n\n    parameters = []\n    for k, v in model.named_parameters():\n        for ft_module in ft_module_names:\n            if ft_module in k:\n                parameters.append({\'params\': v})\n                break\n        else:\n            parameters.append({\'params\': v, \'lr\': 0.0})\n\n    return parameters\n\ndef resnet50(**kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = ResNeXt(ResNeXtBottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n\ndef resnet101(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNeXt(ResNeXtBottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n\ndef resnet152(**kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNeXt(ResNeXtBottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n'"
models/wide_resnet.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom functools import partial\n\n__all__ = [\'WideResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\']\n\n\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(in_planes, out_planes, kernel_size=3,\n                     stride=stride, padding=1, bias=False)\n\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(out.size(0), planes - out.size(1),\n                             out.size(2), out.size(3),\n                             out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n\n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n\n    return out\n\n\nclass WideBottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(WideBottleneck, self).__init__()\n        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass WideResNet(nn.Module):\n\n    def __init__(self, block, layers, sample_size, sample_duration, k=1, shortcut_type=\'B\', num_classes=400, last_fc=True):\n        self.last_fc = last_fc\n\n        self.inplanes = 64\n        super(WideResNet, self).__init__()\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2),\n                               padding=(3, 3, 3), bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64 * k, layers[0], shortcut_type)\n        self.layer2 = self._make_layer(block, 128 * k, layers[1], shortcut_type, stride=2)\n        self.layer3 = self._make_layer(block, 256 * k, layers[2], shortcut_type, stride=2)\n        self.layer4 = self._make_layer(block, 512 * k, layers[3], shortcut_type, stride=2)\n        last_duration = math.ceil(sample_duration / 16)\n        last_size = math.ceil(sample_size / 32)\n        self.avgpool = nn.AvgPool3d((last_duration, last_size, last_size), stride=1)\n        self.fc = nn.Linear(512 * k * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if shortcut_type == \'A\':\n                downsample = partial(downsample_basic_block,\n                                     planes=planes * block.expansion,\n                                     stride=stride)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv3d(self.inplanes, planes * block.expansion,\n                              kernel_size=1, stride=stride, bias=False),\n                    nn.BatchNorm3d(planes * block.expansion)\n                )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n\n        x = x.view(x.size(0), -1)\n        if self.last_fc:\n            x = self.fc(x)\n\n        return x\n\ndef get_fine_tuning_parameters(model, ft_begin_index):\n    if ft_begin_index == 0:\n        return model.parameters()\n\n    ft_module_names = []\n    for i in range(ft_begin_index, 5):\n        ft_module_names.append(\'layer{}\'.format(ft_begin_index))\n    ft_module_names.append(\'fc\')\n\n    parameters = []\n    for k, v in model.named_parameters():\n        for ft_module in ft_module_names:\n            if ft_module in k:\n                parameters.append({\'params\': v})\n                break\n        else:\n            parameters.append({\'params\': v, \'lr\': 0.0})\n\n    return parameters\n\ndef resnet50(**kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = WideResNet(WideBottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n'"
