file_path,api_count,code
failures.py,8,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.functional as F\n\nfrom models import MLP\n\nTRAIN_RANGE = [-5, 5]\nTEST_RANGE = [-20, 20]\nLEARNING_RATE = 1e-2\nNUM_ITERS = int(1e4)\nNON_LINEARITIES = [\n    \'hardtanh\', \'sigmoid\',\n    \'relu6\', \'tanh\',\n    \'tanhshrink\', \'hardshrink\',\n    \'leakyrelu\', \'softshrink\',\n    \'softsign\', \'relu\',\n    \'prelu\', \'softplus\',\n    \'elu\', \'selu\',\n]\n\n\ndef train(model, optimizer, data, num_iters):\n    for i in range(num_iters):\n        out = model(data)\n        loss = F.mse_loss(out, data)\n        mea = torch.mean(torch.abs(data - out))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if i % 1000 == 0:\n            print(""\\t{}/{}: loss: {:.3f} - mea: {:.3f}"".format(\n                i+1, num_iters, loss.item(), mea.item())\n            )\n\n\ndef test(model, data):\n    with torch.no_grad():\n        out = model(data)\n        return torch.abs(data - out)\n\n\ndef main():\n    save_dir = \'./imgs/\'\n\n    TRAIN_RANGE[-1] += 1\n    TEST_RANGE[-1] += 1\n\n    # datasets\n    train_data = torch.arange(*TRAIN_RANGE).unsqueeze_(1).float()\n    test_data = torch.arange(*TEST_RANGE).unsqueeze_(1).float()\n\n    # train\n    all_mses = []\n    for non_lin in NON_LINEARITIES:\n        print(""Working with {}..."".format(non_lin))\n        mses = []\n        for i in range(100):\n            net = MLP(4, 1, 8, 1, non_lin)\n            optim = torch.optim.RMSprop(net.parameters(), lr=LEARNING_RATE)\n            train(net, optim, train_data, NUM_ITERS)\n            mses.append(test(net, test_data))\n        all_mses.append(torch.cat(mses, dim=1).mean(dim=1))\n    all_mses = [x.numpy().flatten() for x in all_mses]\n\n    # plot\n    fig, ax = plt.subplots(figsize=(8, 7))\n    x_axis = np.arange(-20, 21)\n    for i, non_lin in enumerate(NON_LINEARITIES):\n        ax.plot(x_axis, all_mses[i], label=non_lin)\n    plt.grid()\n    plt.legend(loc=\'best\')\n    plt.ylabel(\'Mean Absolute Error\')\n    plt.savefig(save_dir + \'extrapolation.png\', format=\'png\', dpi=300)\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
function_learning.py,11,"b'import math\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom models import MLP, NAC, NALU\n\nNORMALIZE = True\nNUM_LAYERS = 2\nHIDDEN_DIM = 2\nLEARNING_RATE = 1e-2\nNUM_ITERS = int(1e5)\nRANGE = [5, 10]\nARITHMETIC_FUNCTIONS = {\n    \'add\': lambda x, y: x + y,\n    \'sub\': lambda x, y: x - y,\n    \'mul\': lambda x, y: x * y,\n    \'div\': lambda x, y: x / y,\n    \'squared\': lambda x, y: torch.pow(x, 2),\n    \'root\': lambda x, y: torch.sqrt(x),\n}\n\n\ndef generate_data(num_train, num_test, dim, num_sum, fn, support):\n    data = torch.FloatTensor(dim).uniform_(*support).unsqueeze_(1)\n    X, y = [], []\n    for i in range(num_train + num_test):\n        idx_a = random.sample(range(dim), num_sum)\n        idx_b = random.sample([x for x in range(dim) if x not in idx_a], num_sum)\n        a, b = data[idx_a].sum(), data[idx_b].sum()\n        X.append([a, b])\n        y.append(fn(a, b))\n    X = torch.FloatTensor(X)\n    y = torch.FloatTensor(y).unsqueeze_(1)\n    indices = list(range(num_train + num_test))\n    np.random.shuffle(indices)\n    X_train, y_train = X[indices[num_test:]], y[indices[num_test:]]\n    X_test, y_test = X[indices[:num_test]], y[indices[:num_test]]\n    return X_train, y_train, X_test, y_test\n\n\ndef train(model, optimizer, data, target, num_iters):\n    for i in range(num_iters):\n        out = model(data)\n        loss = F.mse_loss(out, target)\n        mea = torch.mean(torch.abs(target - out))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if i % 1000 == 0:\n            print(""\\t{}/{}: loss: {:.7f} - mea: {:.7f}"".format(\n                i+1, num_iters, loss.item(), mea.item())\n            )\n\n\ndef test(model, data, target):\n    with torch.no_grad():\n        out = model(data)\n        return torch.abs(target - out)\n\n\n\ndef main():\n    save_dir = \'./results/\'\n\n    models = [\n        MLP(\n            num_layers=NUM_LAYERS,\n            in_dim=2,\n            hidden_dim=HIDDEN_DIM,\n            out_dim=1,\n            activation=\'relu6\',\n        ),\n        MLP(\n            num_layers=NUM_LAYERS,\n            in_dim=2,\n            hidden_dim=HIDDEN_DIM,\n            out_dim=1,\n            activation=\'none\',\n        ),\n        NAC(\n            num_layers=NUM_LAYERS,\n            in_dim=2,\n            hidden_dim=HIDDEN_DIM,\n            out_dim=1,\n        ),\n        NALU(\n            num_layers=NUM_LAYERS,\n            in_dim=2,\n            hidden_dim=HIDDEN_DIM,\n            out_dim=1\n        ),\n    ]\n\n    results = {}\n    for fn_str, fn in ARITHMETIC_FUNCTIONS.items():\n        print(\'[*] Testing function: {}\'.format(fn_str))\n        results[fn_str] = []\n\n        # dataset\n        X_train, y_train, X_test, y_test = generate_data(\n            num_train=500, num_test=50,\n            dim=100, num_sum=5, fn=fn,\n            support=RANGE,\n        )\n\n        # random model\n        random_mse = []\n        for i in range(100):\n            net = MLP(\n                num_layers=NUM_LAYERS, in_dim=2,\n                hidden_dim=HIDDEN_DIM, out_dim=1,\n                activation=\'relu6\',\n            )\n            mse = test(net, X_test, y_test)\n            random_mse.append(mse.mean().item())\n        results[fn_str].append(np.mean(random_mse))\n\n        # others\n        for net in models:\n            print(""\\tTraining {}..."".format(net.__str__().split(""("")[0]))\n            optim = torch.optim.RMSprop(net.parameters(), lr=LEARNING_RATE)\n            train(net, optim, X_train, y_train, NUM_ITERS)\n            mse = test(net, X_test, y_test).mean().item()\n            results[fn_str].append(mse)\n\n    with open(save_dir + ""interpolation.txt"", ""w"") as f:\n        f.write(""Relu6\\tNone\\tNAC\\tNALU\\n"")\n        for k, v in results.items():\n            rand = results[k][0]\n            mses = [100.0*x/rand for x in results[k][1:]]\n            if NORMALIZE:\n                f.write(""{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\n"".format(*mses))\n            else:\n                f.write(""{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\n"".format(*results[k][1:]))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
models/__init__.py,0,"b'from .mlp import MLP\nfrom .nac import NeuralAccumulatorCell, NAC\nfrom .nalu import NeuralArithmeticLogicUnitCell, NALU\n'"
models/mlp.py,1,"b'import math\nimport torch.nn as nn\n\nfrom .utils import str2act\n\n\nclass MLP(nn.Module):\n    """"""A Multi-Layer Perceptron (MLP).\n\n    Also known as a Fully-Connected Network (FCN). This\n    implementation assumes that all hidden layers have\n    the same hidden size and the same activation function.\n\n    Attributes:\n        num_layers: the number of layers in the network.\n        in_dim: the size of the input sample.\n        hidden_dim: the size of the hidden layers.\n        out_dim: the size of the output.\n        activation: the activation function.\n    """"""\n    def __init__(self, num_layers, in_dim, hidden_dim, out_dim, activation=\'relu\'):\n        super().__init__()\n        self.num_layers = num_layers\n        self.in_dim = in_dim\n        self.hidden_dim = hidden_dim\n        self.out_dim = out_dim\n        self.activation = str2act(activation)\n\n        nonlin = True\n        if self.activation is None:\n            nonlin = False\n\n        layers = []\n        for i in range(num_layers - 1):\n            layers.extend(\n                self._layer(\n                    hidden_dim if i > 0 else in_dim,\n                    hidden_dim,\n                    nonlin,\n                )\n            )\n        layers.extend(self._layer(hidden_dim, out_dim, False))\n\n        self.model = nn.Sequential(*layers)\n\n        # init\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                bound = 1 / math.sqrt(fan_in)\n                nn.init.uniform_(m.bias, -bound, bound)\n\n    def _layer(self, in_dim, out_dim, activation=True):\n        if activation:\n            return [\n                nn.Linear(in_dim, out_dim),\n                self.activation,\n            ]\n        else:\n            return [\n                nn.Linear(in_dim, out_dim),\n            ]\n\n    def forward(self, x):\n        out = self.model(x)\n        return out\n'"
models/nac.py,7,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nfrom torch.nn.parameter import Parameter\n\n\nclass NeuralAccumulatorCell(nn.Module):\n    """"""A Neural Accumulator (NAC) cell [1].\n\n    Attributes:\n        in_dim: size of the input sample.\n        out_dim: size of the output sample.\n\n    Sources:\n        [1]: https://arxiv.org/abs/1808.00508\n    """"""\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n\n        self.W_hat = Parameter(torch.Tensor(out_dim, in_dim))\n        self.M_hat = Parameter(torch.Tensor(out_dim, in_dim))\n\n        self.register_parameter(\'W_hat\', self.W_hat)\n        self.register_parameter(\'M_hat\', self.M_hat)\n        self.register_parameter(\'bias\', None)\n\n        self._reset_params()\n\n    def _reset_params(self):\n        init.kaiming_uniform_(self.W_hat)\n        init.kaiming_uniform_(self.M_hat)\n\n    def forward(self, input):\n        W = torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat)\n        return F.linear(input, W, self.bias)\n\n    def extra_repr(self):\n        return \'in_dim={}, out_dim={}\'.format(\n            self.in_dim, self.out_dim\n        )\n\n\nclass NAC(nn.Module):\n    """"""A stack of NAC layers.\n\n    Attributes:\n        num_layers: the number of NAC layers.\n        in_dim: the size of the input sample.\n        hidden_dim: the size of the hidden layers.\n        out_dim: the size of the output.\n    """"""\n    def __init__(self, num_layers, in_dim, hidden_dim, out_dim):\n        super().__init__()\n        self.num_layers = num_layers\n        self.in_dim = in_dim\n        self.hidden_dim = hidden_dim\n        self.out_dim = out_dim\n\n        layers = []\n        for i in range(num_layers):\n            layers.append(\n                NeuralAccumulatorCell(\n                    hidden_dim if i > 0 else in_dim,\n                    hidden_dim if i < num_layers - 1 else out_dim,\n                )\n            )\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.model(x)\n        return out\n'"
models/nalu.py,8,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nfrom .nac import NeuralAccumulatorCell\nfrom torch.nn.parameter import Parameter\n\n\nclass NeuralArithmeticLogicUnitCell(nn.Module):\n    """"""A Neural Arithmetic Logic Unit (NALU) cell [1].\n\n    Attributes:\n        in_dim: size of the input sample.\n        out_dim: size of the output sample.\n\n    Sources:\n        [1]: https://arxiv.org/abs/1808.00508\n    """"""\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.eps = 1e-10\n\n        self.G = Parameter(torch.Tensor(out_dim, in_dim))\n        self.nac = NeuralAccumulatorCell(in_dim, out_dim)\n        self.register_parameter(\'bias\', None)\n\n        init.kaiming_uniform_(self.G, a=math.sqrt(5))\n\n    def forward(self, input):\n        a = self.nac(input)\n        g = torch.sigmoid(F.linear(input, self.G, self.bias))\n        add_sub = g * a\n        log_input = torch.log(torch.abs(input) + self.eps)\n        m = torch.exp(self.nac(log_input))\n        mul_div = (1 - g) * m\n        y = add_sub + mul_div\n        return y\n\n    def extra_repr(self):\n        return \'in_dim={}, out_dim={}\'.format(\n            self.in_dim, self.out_dim\n        )\n\n\nclass NALU(nn.Module):\n    """"""A stack of NAC layers.\n\n    Attributes:\n        num_layers: the number of NAC layers.\n        in_dim: the size of the input sample.\n        hidden_dim: the size of the hidden layers.\n        out_dim: the size of the output.\n    """"""\n    def __init__(self, num_layers, in_dim, hidden_dim, out_dim):\n        super().__init__()\n        self.num_layers = num_layers\n        self.in_dim = in_dim\n        self.hidden_dim = hidden_dim\n        self.out_dim = out_dim\n\n        layers = []\n        for i in range(num_layers):\n            layers.append(\n                NeuralArithmeticLogicUnitCell(\n                    hidden_dim if i > 0 else in_dim,\n                    hidden_dim if i < num_layers - 1 else out_dim,\n                )\n            )\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.model(x)\n        return out\n'"
models/utils.py,1,"b'import torch\nimport torch.nn as nn\n\n\ndef str2act(s):\n    if s is \'none\':\n        return None\n    elif s is \'hardtanh\':\n        return nn.Hardtanh()\n    elif s is \'sigmoid\':\n        return nn.Sigmoid()\n    elif s is \'relu6\':\n        return nn.ReLU6()\n    elif s is \'tanh\':\n        return nn.Tanh()\n    elif s is \'tanhshrink\':\n        return nn.Tanhshrink()\n    elif s is \'hardshrink\':\n        return nn.Hardshrink()\n    elif s is \'leakyrelu\':\n        return nn.LeakyReLU()\n    elif s is \'softshrink\':\n        return nn.Softshrink()\n    elif s is \'softsign\':\n        return nn.Softsign()\n    elif s is \'relu\':\n        return nn.ReLU()\n    elif s is \'prelu\':\n        return nn.PReLU()\n    elif s is \'softplus\':\n        return nn.Softplus()\n    elif s is \'elu\':\n        return nn.ELU()\n    elif s is \'selu\':\n        return nn.SELU()\n    else:\n        raise ValueError(""[!] Invalid activation function."")\n'"
