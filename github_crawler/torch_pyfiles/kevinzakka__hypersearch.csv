file_path,api_count,code
config.py,0,"b""import argparse\n\narg_lists = []\nparser = argparse.ArgumentParser(description='HyperSearch')\n\n\ndef str2bool(v):\n    return v.lower() in ('true', '1')\n\n\ndef add_argument_group(name):\n    arg = parser.add_argument_group(name)\n    arg_lists.append(arg)\n    return arg\n\n\n# hyperband params\nhyper_arg = add_argument_group('Hyperband Params')\nhyper_arg.add_argument('--max_iter', type=int, default=10,\n                       help='Maximum # of iters allocated to a given config')\nhyper_arg.add_argument('--eta', type=int, default=3,\n                       help='Proportion of configs discarded in each round of SH')\nhyper_arg.add_argument('--epoch_scale', type=str2bool, default=True,\n                       help='Compute `max_iter` in terms of epochs or mini-batch iters')\n\n# data params\ndata_arg = add_argument_group('Data Params')\ndata_arg.add_argument('--name', type=str, default='mnist',\n                      help='Dataset name to train and validate on')\ndata_arg.add_argument('--valid_size', type=float, default=0.1,\n                      help='Proportion of training set used for validation')\ndata_arg.add_argument('--batch_size', type=int, default=64,\n                      help='# of images in each batch of data')\ndata_arg.add_argument('--num_workers', type=int, default=4,\n                      help='# of subprocesses to use for data loading')\ndata_arg.add_argument('--shuffle', type=str2bool, default=False,\n                      help='Whether to shuffle the train and valid indices')\n\n# optim params\ntrain_arg = add_argument_group('Optim Params')\ntrain_arg.add_argument('--def_lr', type=float, default=1e-3,\n                       help='Default lr')\ntrain_arg.add_argument('--def_optim', type=str, default='adam',\n                       help='Default optimizer')\ntrain_arg.add_argument('--patience', type=int, default=5,\n                       help='# of epochs to wait before early stopping')\n\n# misc params\nmisc_arg = add_argument_group('Misc.')\nmisc_arg.add_argument('--num_gpu', type=int, default=0,\n                      help='0 for cpu, greater for gpu')\nmisc_arg.add_argument('--data_dir', type=str, default='./data/',\n                      help='Directory in which data is stored')\nmisc_arg.add_argument('--ckpt_dir', type=str, default='./ckpt/',\n                      help='Directory in which to save model checkpoints')\nmisc_arg.add_argument('--print_freq', type=int, default=10,\n                      help='How frequently to print training details')\n\n\ndef get_args():\n    args, unparsed = parser.parse_known_args()\n    return args, unparsed\n"""
data_loader.py,3,"b'import numpy as np\n\nimport torch\n\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n\ndef get_train_valid_loader(data_dir,\n                           name,\n                           batch_size,\n                           valid_size=0.1,\n                           shuffle=True,\n                           num_workers=4,\n                           pin_memory=False):\n    """"""\n    Utility function for loading and returning train and valid\n    multi-process iterators over the desired dataset.\n\n    Params\n    ------\n    - data_dir: path directory to the dataset.\n    - name: string specifying which dataset to load. Can be `mnist`,\n      `cifar10`, `cifar100`.\n    - batch_size: how many samples per batch to load.\n    - valid_size: percentage split of the training set used for\n      the validation set. Should be a float in the range [0, 1].\n      In the paper, this number is set to 0.1.\n    - shuffle: whether to shuffle the train/validation indices.\n    - num_workers: number of subprocesses to use when loading the dataset.\n    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n      True if using GPU.\n\n    Returns\n    -------\n    - train_loader: training set iterator.\n    - valid_loader: validation set iterator.\n    """"""\n    error_msg1 = ""[!] valid_size should be in the range [0, 1].""\n    error_msg2 = ""[!] Invalid dataset name.""\n    assert ((valid_size >= 0) and (valid_size <= 1)), error_msg1\n    assert name in [\'mnist\', \'cifar10\', \'cifar100\'], error_msg2\n\n    # define transforms\n    if name == \'mnist\':\n        normalize = transforms.Normalize(\n            mean=(0.1307,),\n            std=(0.3081,)\n        )\n    else:\n        normalize = transforms.Normalize(\n            mean=[0.4914, 0.4822, 0.4465],\n            std=[0.2023, 0.1994, 0.2010],\n        )\n\n    train_trans = transforms.Compose([\n        transforms.ToTensor(),\n        normalize,\n    ])\n    valid_trans = transforms.Compose([\n            transforms.ToTensor(),\n            normalize,\n    ])\n\n    # load the dataset\n    if name == \'mnist\':\n        train_dataset = datasets.MNIST(\n            root=data_dir, train=True,\n            download=True, transform=train_trans,\n        )\n        valid_dataset = datasets.MNIST(\n            root=data_dir, train=True,\n            download=True, transform=valid_trans,\n        )\n    elif name == \'cifar10\':\n        train_dataset = datasets.CIFAR10(\n            root=data_dir, train=True,\n            download=True, transform=train_trans,\n        )\n        valid_dataset = datasets.CIFAR10(\n            root=data_dir, train=True,\n            download=True, transform=valid_trans,\n        )\n    else:\n        train_dataset = datasets.CIFAR100(\n            root=data_dir, train=True,\n            download=True, transform=train_trans,\n        )\n        valid_dataset = datasets.CIFAR100(\n            root=data_dir, train=True,\n            download=True, transform=valid_trans,\n        )\n\n    # create dataloaders\n    num_train = len(train_dataset)\n    indices = list(range(num_train))\n    split = int(np.floor(valid_size * num_train))\n\n    if shuffle:\n        seed = 786427186\n        np.random.seed(seed)\n        np.random.shuffle(indices)\n\n    train_idx, valid_idx = indices[split:], indices[:split]\n\n    train_sampler = SubsetRandomSampler(train_idx)\n    valid_sampler = SubsetRandomSampler(valid_idx)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, sampler=train_sampler,\n        num_workers=num_workers, pin_memory=pin_memory,\n    )\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=batch_size, sampler=valid_sampler,\n        num_workers=num_workers, pin_memory=pin_memory,\n    )\n\n    return (train_loader, valid_loader)\n'"
hyperband.py,11,"b'import os\nimport uuid\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom data_loader import get_train_valid_loader\nfrom utils import find_key, sample_from, str2act\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.optim import SGD, Adam\nfrom torch.autograd import Variable\n\n\nclass Hyperband(object):\n    """"""\n    Hyperband is a bandit-based configuration\n    evaluation for hyperparameter optimization [1].\n\n    Hyperband is a principled early-stoppping method\n    that adaptively allocates resources to randomly\n    sampled configurations, quickly eliminating poor\n    ones, until a single configuration remains.\n\n    References\n    ----------\n    - [1]: Li et. al., https://arxiv.org/abs/1603.06560\n    """"""\n    def __init__(self, args, model, params):\n        """"""\n        Initialize the Hyperband object.\n\n        Args\n        ----\n        - args: object containing command line arguments.\n        - model: the `Sequential()` model you wish to tune.\n        - params: a dictionary where the key is the hyperparameter\n          to tune, and the value is the space from which to randomly\n          sample it.\n        """"""\n        self.args = args\n        self.model = model\n        self._parse_params(params)\n\n        # hyperband params\n        self.epoch_scale = args.epoch_scale\n        self.max_iter = args.max_iter\n        self.eta = args.eta\n        self.s_max = int(np.log(self.max_iter) / np.log(self.eta))\n        self.B = (self.s_max + 1) * self.max_iter\n\n        print(\n            ""[*] max_iter: {}, eta: {}, B: {}"".format(\n                self.max_iter, self.eta, self.B\n            )\n        )\n\n        # misc params\n        self.data_dir = args.data_dir\n        self.ckpt_dir = args.ckpt_dir\n        self.num_gpu = args.num_gpu\n        self.print_freq = args.print_freq\n\n        # device\n        self.device = torch.device(""cuda"" if self.num_gpu > 0 else ""cpu"")\n\n        # data params\n        self.data_loader = None\n        self.kwargs = {}\n        if self.num_gpu > 0:\n            self.kwargs = {\'num_workers\': 1, \'pin_memory\': True}\n        if \'batch_size\' not in self.optim_params:\n            self.batch_hyper = False\n            self.data_loader = get_train_valid_loader(\n                args.data_dir, args.name, args.batch_size,\n                args.valid_size, args.shuffle, **self.kwargs\n            )\n\n        # optim params\n        self.def_optim = args.def_optim\n        self.def_lr = args.def_lr\n        self.patience = args.patience\n\n    def _parse_params(self, params):\n        """"""\n        Split the user-defined params dictionary\n        into its different components.\n        """"""\n        self.size_params = {}\n        self.net_params = {}\n        self.optim_params = {}\n        self.reg_params = {}\n\n        size_filter = [""hidden""]\n        net_filter = [""act"", ""dropout"", ""batchnorm""]\n        optim_filter = [""lr"", ""optim"", ""batch_size""]\n        reg_filter = [""l2"", ""l1""]\n\n        for k, v in params.items():\n            if any(s in k for s in size_filter):\n                self.size_params[k] = v\n            elif any(s in k for s in net_filter):\n                self.net_params[k] = v\n            elif any(s in k for s in optim_filter):\n                self.optim_params[k] = v\n            elif any(s in k for s in reg_filter):\n                self.reg_params[k] = v\n            else:\n                raise ValueError(""[!] key not supported."")\n\n    def tune(self):\n        """"""\n        Tune the hyperparameters of the pytorch model\n        using Hyperband.\n        """"""\n        best_configs = []\n        results = {}\n\n        # finite horizon outerloop\n        for s in reversed(range(self.s_max + 1)):\n            # initial number of configs\n            n = int(\n                np.ceil(\n                    int(self.B / self.max_iter / (s + 1)) * self.eta ** s\n                )\n            )\n            # initial number of iterations to run the n configs for\n            r = self.max_iter * self.eta ** (-s)\n\n            # finite horizon SH with (n, r)\n            T = [self.get_random_config() for i in range(n)]\n\n            tqdm.write(""s: {}"".format(s))\n\n            for i in range(s + 1):\n                n_i = int(n * self.eta ** (-i))\n                r_i = int(r * self.eta ** (i))\n\n                tqdm.write(\n                    ""[*] {}/{} - running {} configs for {} iters each"".format(\n                        i+1, s+1, len(T), r_i)\n                )\n\n                # Todo: add condition for all models early stopping\n\n                # run each of the n_i configs for r_i iterations\n                val_losses = []\n                with tqdm(total=len(T)) as pbar:\n                    for t in T:\n                        val_loss = self.run_config(t, r_i)\n                        val_losses.append(val_loss)\n                        pbar.update(1)\n\n                # remove early stopped configs and keep the best n_i / eta\n                if i < s - 1:\n                    sort_loss_idx = np.argsort(\n                        val_losses\n                    )[0:int(n_i / self.eta)]\n                    T = [T[k] for k in sort_loss_idx if not T[k].early_stopped]\n                    tqdm.write(""Left with: {}"".format(len(T)))\n\n            best_idx = np.argmin(val_losses)\n            best_configs.append([T[best_idx], val_losses[best_idx]])\n\n        best_idx = np.argmin([b[1] for b in best_configs])\n        best_model = best_configs[best_idx]\n        results[""val_loss""] = best_model[1]\n        results[""params""] = best_model[0].new_params\n        results[""str""] = best_model[0].__str__()\n        return results\n\n    def get_random_config(self):\n        """"""\n        Build a mutated version of the user\'s model that\n        incorporates the new hyperparameters settings defined\n        by `hyperparams`.\n        """"""\n        self.all_batchnorm = False\n        self.all_drop = False\n        new_params = {}\n\n        if not self.net_params:\n            mutated = self.model\n        else:\n            layers = []\n            used_acts = []\n            all_act = False\n            all_drop = False\n            all_batchnorm = False\n            num_layers = len(self.model)\n\n            i = 0\n            used_acts.append(self.model[1].__str__())\n            for layer_hp in self.net_params.keys():\n                layer, hp = layer_hp.split(\'_\', 1)\n                if layer.isdigit():\n                    layer_num = int(layer)\n                    diff = layer_num - i\n                    if diff > 0:\n                        for j in range(diff + 1):\n                            layers.append(self.model[i+j])\n                        i += diff\n                        if hp == \'act\':\n                            space = find_key(\n                                self.net_params, \'{}_act\'.format(layer_num)\n                            )\n                            hyperp = sample_from(space)\n                            new_params[""act""] = hyperp\n                            new_act = str2act(hyperp)\n                            used_acts.append(new_act.__str__())\n                            layers.append(new_act)\n                            i += 1\n                        elif hp == \'dropout\':\n                            layers.append(self.model[i])\n                            space = find_key(\n                                self.net_params, \'{}_drop\'.format(layer_num)\n                            )\n                            hyperp = sample_from(space)\n                            new_params[""drop""] = hyperp\n                            layers.append(nn.Dropout(p=hyperp))\n                        else:\n                            pass\n                    elif diff == 0:\n                        layers.append(self.model[i])\n                        if hp == \'act\':\n                            space = find_key(\n                                self.net_params, \'{}_act\'.format(layer_num)\n                            )\n                            hyperp = sample_from(space)\n                            new_params[""act""] = hyperp\n                            new_act = str2act(hyperp)\n                            used_acts.append(new_act.__str__())\n                            layers.append(new_act)\n                            i += 1\n                        elif hp == \'dropout\':\n                            i += 1\n                            layers.append(self.model[i])\n                            space = find_key(\n                                self.net_params, \'{}_drop\'.format(layer_num)\n                            )\n                            hyperp = sample_from(space)\n                            new_params[""drop""] = hyperp\n                            layers.append(nn.Dropout(p=hyperp))\n                        else:\n                            pass\n                    else:\n                        if hp == \'act\':\n                            space = find_key(\n                                self.net_params, \'{}_act\'.format(layer_num)\n                            )\n                            hyperp = sample_from(space)\n                            new_params[""act""] = hyperp\n                            new_act = str2act(hyperp)\n                            used_acts.append(new_act.__str__())\n                            layers[i] = new_act\n                        elif hp == \'dropout\':\n                            space = find_key(\n                                self.net_params, \'{}_drop\'.format(layer_num)\n                            )\n                            hyperp = sample_from(space)\n                            new_params[""drop""] = hyperp\n                            layers.append(nn.Dropout(p=hyperp))\n                            layers.append(self.model[i])\n                        else:\n                            pass\n                    i += 1\n                else:\n                    if (i < num_layers) and (len(layers) < num_layers):\n                        for j in range(num_layers-i):\n                            layers.append(self.model[i+j])\n                        i += 1\n                    if layer == ""all"":\n                        if hp == ""act"":\n                            space = self.net_params[\'all_act\']\n                            hyperp = sample_from(space)\n                            all_act = False if hyperp == [0] else True\n                        elif hp == ""dropout"":\n                            space = self.net_params[\'all_dropout\']\n                            hyperp = sample_from(space)\n                            all_drop = False if hyperp == [0] else True\n                        elif hp == ""batchnorm"":\n                            space = self.net_params[\'all_batchnorm\']\n                            hyperp = sample_from(space)\n                            all_batchnorm = True if hyperp == 1 else False\n                        else:\n                            pass\n\n            used_acts = sorted(set(used_acts), key=used_acts.index)\n\n            if all_act:\n                old_act = used_acts[0]\n                space = self.net_params[\'all_act\'][1][1]\n                hyperp = sample_from(space)\n                new_params[""all_act""] = hyperp\n                new_act = str2act(hyperp)\n                used_acts.append(new_act.__str__())\n                for i, l in enumerate(layers):\n                    if l.__str__() == old_act:\n                        layers[i] = new_act\n            if all_batchnorm:\n                self.all_batchnorm = True\n                new_params[""all_batch""] = True\n                target_acts = used_acts if not all_act else used_acts[1:]\n                for i, l in enumerate(layers):\n                    if l.__str__() in target_acts:\n                        if \'Linear\' in layers[i-1].__str__():\n                            bn = nn.BatchNorm2d(layers[i-1].out_features)\n                        else:\n                            bn = nn.BatchNorm2d(layers[i-1].out_channels)\n                        layers.insert(i+1, bn)\n                if \'Linear\' in layers[-2].__str__():\n                    bn = nn.BatchNorm2d(layers[i-1].out_features)\n                else:\n                    bn = nn.BatchNorm2d(layers[i-1].out_channels)\n                layers.insert(-1, bn)\n            if all_drop:\n                self.all_drop = True\n                new_params[""all_drop""] = True\n                target_acts = used_acts if not all_act else used_acts[1:]\n                space = self.net_params[\'all_dropout\'][1][1]\n                hyperp = sample_from(space)\n                for i, l in enumerate(layers):\n                    if l.__str__() in target_acts:\n                        layers.insert(i + 1 + all_batchnorm, nn.Dropout(p=hyperp))\n\n            sizes = {}\n            for k, v in self.size_params.items():\n                layer_num = int(k.split(""_"", 1)[0])\n                layer_num += (layer_num // 2) * (\n                    self.all_batchnorm + self.all_drop\n                )\n                hyperp = sample_from(v)\n                new_params[""{}_hidden_size"".format(layer_num)] = hyperp\n                sizes[layer_num] = hyperp\n\n            for layer, size in sizes.items():\n                in_dim = layers[layer].in_features\n                layers[layer] = nn.Linear(in_dim, size)\n                if self.all_batchnorm:\n                    layers[layer + 2] = nn.BatchNorm2d(size)\n                next_layer = layer + (\n                    2 + self.all_batchnorm + self.all_drop\n                )\n                out_dim = layers[next_layer].out_features\n                layers[next_layer] = nn.Linear(size, out_dim)\n\n            mutated = nn.Sequential(*layers)\n\n        self._init_weights_biases(mutated)\n        mutated.ckpt_name = str(uuid.uuid4().hex)\n        mutated.new_params = new_params\n        mutated.early_stopped = False\n        return mutated\n\n    def _init_weights_biases(self, model):\n        # figure out if model contains mix of layers => all glorot init)\n        glorot_all = False\n        for key in self.net_params.keys():\n            layer, hp = key.split(\'_\', 1)\n            if layer.isdigit() and hp == ""act"":\n                glorot_all = True\n\n        # figure out the activation function\n        for i, m in enumerate(model):\n            if not isinstance(\n                m, (nn.Linear, nn.BatchNorm2d, nn.Dropout)\n            ):\n                if i < len(model) - 1:\n                    act = model[i].__str__()\n                    act = act.lower().split(""("")[0]\n                    break\n\n        if glorot_all:\n            for m in model:\n                if isinstance(m, nn.Linear):\n                    nn.init.xavier_normal_(m.weight)\n                elif isinstance(m, nn.BatchNorm2d):\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n        else:\n            if act == ""relu"":\n                for m in model:\n                    if isinstance(m, nn.Linear):\n                        nn.init.kaiming_normal_(m.weight)\n                    elif isinstance(m, nn.BatchNorm2d):\n                        nn.init.constant_(m.weight, 1)\n                        nn.init.constant_(m.bias, 0)\n            elif act == ""selu"":\n                for m in model:\n                    if isinstance(m, nn.Linear):\n                        n = m.out_features\n                        nn.init.normal_(\n                            m.weight, mean=0, std=np.sqrt(1./n)\n                        )\n                    elif isinstance(m, nn.BatchNorm2d):\n                        nn.init.constant_(m.weight, 1)\n                        nn.init.constant_(m.bias, 0)\n            else:\n                for m in model:\n                    if isinstance(m, nn.Linear):\n                        nn.init.xavier_normal_(m.weight)\n                    elif isinstance(m, nn.BatchNorm2d):\n                        nn.init.constant_(m.weight, 1)\n                        nn.init.constant_(m.bias, 0)\n\n    def _check_bn_drop(self, model):\n        names = []\n        count = 0\n        for layer in model.named_children():\n            names.append(layer[1].__str__().split(""("")[0])\n        names = list(set(names))\n        if any(""Dropout"" in s for s in names):\n            count += 1\n        if any(""BatchNorm"" in s for s in names):\n            count += 1\n        return count\n\n    def _add_reg(self, model):\n        offset = self._check_bn_drop(model)\n        reg_layers = []\n        for k in self.reg_params.keys():\n            if k in [""all_l2"", ""all_l1""]:\n                l2_reg = False\n                if k == ""all_l2"":\n                    l2_reg = True\n                num_lin_layers = int(\n                    ((len(self.model) - 2) / 2) + 1\n                )\n                j = 0\n                for i in range(num_lin_layers):\n                    space = self.reg_params[k]\n                    hyperp = sample_from(space)\n                    reg_layers.append((j, hyperp, l2_reg))\n                    j += 2 + offset\n            elif k.split(\'_\', 1)[1] in [""l2"", ""l1""]:\n                layer_num = int(k.split(\'_\', 1)[0])\n                layer_num += (layer_num // 2) * (offset)\n                l2_reg = True\n                if k.split(\'_\', 1)[1] == ""l1"":\n                    l2_reg = False\n                space = self.reg_params[k]\n                hyperp = sample_from(space)\n                reg_layers.append((layer_num, hyperp, l2_reg))\n            else:\n                pass\n        model.new_params[""reg_layers""] = reg_layers\n        return reg_layers\n\n    def _get_reg_loss(self, model, reg_layers):\n        dtype = torch.FloatTensor if self.num_gpu == 0 else torch.cuda.FloatTensor\n        reg_loss = Variable(torch.zeros(1), requires_grad=True).type(dtype)\n        for layer_num, scale, l2 in reg_layers:\n            l1_loss = Variable(torch.zeros(1), requires_grad=True).type(dtype)\n            l2_loss = Variable(torch.zeros(1), requires_grad=True).type(dtype)\n            if l2:\n                for W in model[layer_num].parameters():\n                    l2_loss = l2_loss + (W.norm(2) ** 2)\n                l2_loss = l2_loss.sqrt()\n            else:\n                for W in model[layer_num].parameters():\n                    l1_loss = l1_loss + W.norm(1)\n                l1_loss = l1_loss / 2\n            reg_loss = reg_loss + ((l1_loss + l2_loss) * scale)\n        return reg_loss\n\n    def _get_optimizer(self, model):\n        lr = self.def_lr\n        name = self.def_optim\n        if ""optim"" in self.optim_params:\n            space = self.optim_params[\'optim\']\n            name = sample_from(space)\n        if ""lr"" in self.optim_params:\n            space = self.optim_params[\'lr\']\n            lr = sample_from(space)\n        if name == ""sgd"":\n            opt = SGD\n        elif name == ""adam"":\n            opt = Adam\n        model.new_params[""optim""] = name\n        model.new_params[""lr""] = lr\n        optim = opt(model.parameters(), lr=lr)\n        return optim\n\n    def run_config(self, model, num_iters):\n        """"""\n        Train a particular hyperparameter configuration for a\n        given number of iterations and evaluate the loss on the\n        validation set.\n\n        For hyperparameters that have previously been evaluated,\n        resume from a previous checkpoint.\n\n        Args\n        ----\n        - model: the mutated model to train.\n        - num_iters: an int indicating the number of iterations\n          to train the model for.\n\n        Returns\n        -------\n        - val_loss: the lowest validaton loss achieved.\n        """"""\n        try:\n            ckpt = self._load_checkpoint(model.ckpt_name)\n            model.load_state_dict(ckpt[\'state_dict\'])\n        except FileNotFoundError:\n            pass\n\n        model = model.to(self.device)\n\n        # parse reg params\n        reg_layers = self._add_reg(model)\n\n        # setup train loader\n        if self.data_loader is None:\n            self.batch_hyper = True\n            space = self.optim_params[\'batch_size\']\n            batch_size = sample_from(space)\n            tqdm.write(""batch size: {}"".format(batch_size))\n            self.data_loader = get_train_valid_loader(\n                self.data_dir, self.args.name,\n                batch_size, self.args.valid_size,\n                self.args.shuffle, **self.kwargs\n            )\n\n        # training logic\n        min_val_loss = 999999\n        counter = 0\n        num_epochs = int(num_iters) if self.epoch_scale else 1\n        num_passes = None if self.epoch_scale else num_iters\n        for epoch in range(num_epochs):\n            self._train_one_epoch(model, num_passes, reg_layers)\n            val_loss = self._validate_one_epoch(model)\n            if val_loss < min_val_loss:\n                min_val_loss = val_loss\n                counter = 0\n            else:\n                counter += 1\n            if counter > self.patience:\n                tqdm.write(""[!] early stopped!!"")\n                model.early_stopped = True\n                return min_val_loss\n        if self.batch_hyper:\n            self.data_loader = None\n        state = {\n            \'state_dict\': model.state_dict(),\n            \'min_val_loss\': min_val_loss,\n        }\n        self._save_checkpoint(state, model.ckpt_name)\n        return min_val_loss\n\n    def _train_one_epoch(self, model, num_passes, reg_layers):\n        model.train()\n        optim = self._get_optimizer(model)\n        train_loader = self.data_loader[0]\n        for i, (x, y) in enumerate(train_loader):\n            if num_passes is not None:\n                if i > num_passes:\n                    return\n            x, y = x.to(self.device), y.to(self.device)\n            batch_size = x.shape[0]\n            x = x.view(batch_size, -1)\n            x, y = Variable(x), Variable(y)\n            optim.zero_grad()\n            output = model(x)\n            loss = F.nll_loss(output, y)\n            reg_loss = self._get_reg_loss(model, reg_layers)\n            comp_loss = loss + reg_loss\n            comp_loss.backward()\n            optim.step()\n\n    def _validate_one_epoch(self, model):\n        model.eval()\n        val_loader = self.data_loader[1]\n        num_valid = len(val_loader.sampler.indices)\n        val_loss = 0.\n        for i, (x, y) in enumerate(val_loader):\n            x, y = x.to(self.device), y.to(self.device)\n            x = x.view(x.size(0), -1)\n            x, y = Variable(x), Variable(y)\n            output = model(x)\n            val_loss += F.nll_loss(output, y, size_average=False).item()\n        val_loss /= num_valid\n        return val_loss\n\n    def _save_checkpoint(self, state, name):\n        filename = name + \'.pth.tar\'\n        ckpt_path = os.path.join(self.ckpt_dir, filename)\n        torch.save(state, ckpt_path)\n\n    def _load_checkpoint(self, name):\n        filename = name + \'.pth.tar\'\n        ckpt_path = os.path.join(self.ckpt_dir, filename)\n        ckpt = torch.load(ckpt_path)\n        return ckpt\n'"
main.py,0,"b'import config\n\nfrom hyperband import Hyperband\nfrom model import get_base_model\nfrom utils import prepare_dirs, save_results\n\n\ndef main(args):\n\n    # ensure directories are setup\n    dirs = [args.data_dir, args.ckpt_dir]\n    prepare_dirs(dirs)\n\n    # create base model\n    model = get_base_model()\n\n    # define params\n    params = {\n        # \'0_dropout\': [\'uniform\', 0.1, 0.5],\n        # \'0_act\': [\'choice\', [\'relu\', \'selu\', \'elu\', \'tanh\', \'sigmoid\']],\n        # \'0_l2\': [\'log_uniform\', 1e-1, 2],\n        # \'2_act\': [\'choice\', [\'selu\', \'elu\', \'tanh\', \'sigmoid\']],\n        # \'2_l1\': [\'log_uniform\', 1e-1, 2],\n        # \'2_hidden\': [\'quniform\', 512, 1000, 1],\n        # \'4_hidden\': [\'quniform\', 128, 512, 1],\n        # \'all_act\': [\'choice\', [[0], [\'choice\', [\'selu\', \'elu\', \'tanh\']]]],\n        \'all_dropout\': [\'choice\', [[0], [\'uniform\', 0.1, 0.5]]],\n        # \'all_batchnorm\': [\'choice\', [0, 1]],\n        \'all_l2\': [\'uniform\', 1e-8, 1e-5],\n        # \'optim\': [\'choice\', [""adam"", ""sgd""]],\n        # \'lr\': [\'uniform\', 1e-3, 8e-3],\n        # \'batch_size\': [\'quniform\', 32, 128, 1]\n    }\n\n    # instantiate hyperband object\n    hyperband = Hyperband(args, model, params)\n\n    # tune\n    results = hyperband.tune()\n\n    # dump results\n    save_results(results)\n\n\nif __name__ == \'__main__\':\n    args, unparsed = config.get_args()\n    main(args)\n'"
model.py,1,"b'import torch.nn as nn\n\n\ndef get_base_model():\n    layers = []\n    layers.append(nn.Linear(784, 512))\n    layers.append(nn.ReLU())\n    layers.append(nn.Linear(512, 256))\n    layers.append(nn.ReLU())\n    layers.append(nn.Linear(256, 128))\n    layers.append(nn.ReLU())\n    layers.append(nn.Linear(128, 10))\n    layers.append(nn.LogSoftmax(dim=1))\n    model = nn.Sequential(*layers)\n    return model\n'"
utils.py,1,"b'import os\nimport time\nimport json\nimport numpy as np\n\nimport torch.nn as nn\n\nfrom numpy.random import uniform, normal, randint, choice\n\n\ndef save_results(r):\n    date = time.strftime(""%Y%m%d_%H%M%S"")\n    filename = date + \'_results.json\'\n    param_path = os.path.join(\'./results/\', filename)\n    with open(param_path, \'w\') as fp:\n        json.dump(r, fp, indent=4, sort_keys=True)\n\n\ndef find_key(params, partial_key):\n    return next(v for k, v in params.items() if partial_key in k)\n\n\ndef sample_from(space):\n    """"""\n    Sample a hyperparameter value from a distribution\n    defined and parametrized in the list `space`.\n    """"""\n    distrs = {\n        \'choice\': choice,\n        \'randint\': randint,\n        \'uniform\': uniform,\n        \'normal\': normal,\n    }\n    s = space[0]\n\n    np.random.seed(int(time.time() + np.random.randint(0, 300)))\n\n    log = s.startswith(\'log_\')\n    s = s[len(\'log_\'):] if log else s\n\n    quantized = s.startswith(\'q\')\n    s = s[1:] if quantized else s\n\n    distr = distrs[s]\n    if s == \'choice\':\n        return distr(space[1])\n    samp = distr(space[1], space[2])\n    if log:\n        samp = np.exp(samp)\n    if quantized:\n        samp = round((samp / space[3]) * space[3])\n    return samp\n\n\ndef str2act(a):\n    if a == \'relu\':\n        return nn.ReLU()\n    elif a == \'selu\':\n        return nn.SELU()\n    elif a == \'elu\':\n        return nn.ELU()\n    elif a == \'tanh\':\n        return nn.Tanh()\n    elif a == \'sigmoid\':\n        return nn.Sigmoid()\n    else:\n        raise ValueError(\'[!] Unsupported activation.\')\n\n\ndef prepare_dirs(dirs):\n    for path in dirs:\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n\nclass Reshape(nn.Module):\n    def __init__(self, *args):\n        super(Reshape, self).__init__()\n        self.shape = args\n\n    def forward(self, x):\n        return x.view(self.shape, -1)\n\n\nclass AverageMeter(object):\n    """"""\n    Computes and stores the average and\n    current value.\n    """"""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n'"
