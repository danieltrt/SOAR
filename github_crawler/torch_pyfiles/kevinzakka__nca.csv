file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nwith open(""README.md"", ""r"") as fh:\n  long_description = fh.read()\n\nsetup(\n  name=\'torchnca\',\n  version=\'0.1.1\',\n  description=\'Neighbourhood Components Analysis in PyTorch.\',\n  long_description=long_description,\n  long_description_content_type=\'text/markdown\',\n  url=\'https://github.com/kevinzakka/nca\',\n  author=\'Kevin Zakka\',\n  author_email=\'kevinarmandzakka@gmail.com\',\n  license=\'MIT\',\n  classifiers=[\n    \'Programming Language :: Python :: 3\',\n    \'License :: OSI Approved :: MIT License\',\n    \'Operating System :: OS Independent\',\n  ],\n  keywords=\'ai metric learning nearest neighbours dimensionality reduction\',\n  packages=find_packages(exclude=[\'examples\']),\n  install_requires=[\n    \'numpy>=1.0.0,<2.0.0\',\n    \'torch>=1.0.0,<=1.4.0\',\n  ],\n  python_requires=\'>=3.5\',\n)\n'"
examples/dim_reduct.py,7,"b'""""""NCA for linear dimensionality reduction.\n""""""\n\nimport argparse\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom torchnca import NCA\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\ndef make_circle(r, num_samples):\n  t = np.linspace(0, 2*np.pi, num_samples)\n  xc, yc = 0, 0  # circle center coordinates\n  x = r*np.cos(t) + 0.2*np.random.randn(num_samples) + xc\n  y = r*np.sin(t) + 0.2*np.random.randn(num_samples) + yc\n  return x, y\n\n\ndef gen_data(num_samples, num_classes, mean, std):\n  """"""Generates the data.\n  """"""\n  num_samples_per = num_samples // num_classes\n  X = []\n  y = []\n  for i, r in enumerate(range(num_classes)):\n    # first two dimensions are that of a circle\n    x1, x2 = make_circle(r+1.5, num_samples_per)\n    # third dimension is Gaussian noise\n    x3 = std*np.random.randn(num_samples_per) + mean\n    X.append(np.stack([x1, x2, x3]))\n    y.append(np.repeat(i, num_samples_per))\n  X = np.concatenate(X, axis=1)\n  y = np.concatenate(y)\n  indices = list(range(X.shape[1]))\n  np.random.shuffle(indices)\n  X = X[:, indices]\n  y = y[indices]\n  X = X.T  # make it (N, D)\n  return X, y\n\n\ndef plot(Xs, y, labels, save=None):\n  fig, axes = plt.subplots(1, len(labels), figsize=(18, 4))\n  for ax, X, lab in zip(axes, Xs, labels):\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n    ax.title.set_text(lab)\n  if save is not None:\n    filename = ""./assets/{}"".format(save)\n    plt.savefig(filename, format=""png"", dpi=300, bbox_inches=\'tight\')\n  plt.show()\n\n\ndef main(args):\n  np.random.seed(args.seed)\n  if args.cuda and torch.cuda.is_available():\n    torch.cuda.manual_seed(args.seed)\n    device = torch.device(""cuda"")\n  else:\n    print(""[*] Using cpu."")\n    torch.manual_seed(args.seed)\n    device = torch.device(""cpu"")\n\n  num_samples = 500\n  X, y = gen_data(num_samples, 5, 0, args.sigma)\n  print(""data"", X.shape)\n\n  # plot first two dimensions of original data\n  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n  plt.show()\n\n  # fit PCA\n  pipeline = Pipeline([(\'scaling\', StandardScaler()), (\'pca\', PCA(n_components=2))])\n  X_pca = pipeline.fit_transform(X)\n\n  # fit LDA\n  X_lda = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\n\n  # fit NCA\n  X = torch.from_numpy(X).float().to(device)\n  y = torch.from_numpy(y).long().to(device)\n  nca = NCA(dim=2, init=args.init, max_iters=1000, tol=1e-5)\n  nca.train(X, y, batch_size=None, weight_decay=20)\n  X_nca = nca(X).detach().cpu().numpy()\n\n  # plot PCA vs NCA\n  y = y.detach().cpu().numpy()\n  X = X.detach().cpu().numpy()\n  plot([X, X_nca, X_pca, X_lda], y, [""original"", ""torchnca"", ""pca"", ""lda""])\n\n  A = nca.A.detach().cpu().numpy()\n  print(""\\nSolution: \\n"", A)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--seed"", type=int, default=0, help=""The rng seed."")\n  parser.add_argument(""--sigma"", type=float, default=5, help=""The standard deviation of the Gaussian noise."")\n  parser.add_argument(""--init"", type=str, default=""identity"", help=""Which initialization to use."")\n  parser.add_argument(""--cuda"", type=lambda x: x.lower() in [\'true\', \'1\'], default=False, help=""Whether to show GUI."")\n  args, unparsed = parser.parse_known_args()\n  main(args)\n'"
examples/knn.py,5,"b'""""""NCA + kNN vs. vanilla kNN.\n\nTODO:\n  1. Tune the value of `dim` in NCA.\n  2. Tune the number of neighbours `k` in kNN.\n""""""\n\nimport argparse\nimport numpy as np\nimport time\nimport torch\n\nfrom torchvision import datasets, transforms\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom torchnca import NCA\n\n\ndef main(args):\n  np.random.seed(args.seed)\n  use_cuda = args.cuda and torch.cuda.is_available()\n  if use_cuda:\n    torch.cuda.manual_seed(args.seed)\n    device = torch.device(""cuda"")\n  else:\n    print(""[*] Using cpu."")\n    torch.manual_seed(args.seed)\n    device = torch.device(""cpu"")\n\n  # load the MNIST dataset\n  transform = transforms.Compose([\n    transforms.ToTensor(),\n  ])\n  mnist_data = datasets.MNIST(\'./data\', train=True, transform=transform)\n\n  # split into train and test\n  X_train = mnist_data.data[:50000]\n  y_train = mnist_data.targets[:50000]\n  X_test = mnist_data.data[50000:]\n  y_test = mnist_data.targets[50000:]\n\n  # flatten to (N, D)\n  X_train = X_train.view(X_train.shape[0], -1).float() / 255.\n  X_test = X_test.view(X_test.shape[0], -1).float() / 255.\n  X_test = X_test.cpu().numpy()\n  y_test = y_test.cpu().numpy()\n\n  # NCA + kNN\n  nca = NCA(dim=32, init=args.init, max_iters=70, tol=1e-5)\n  nca.train(X_train, y_train, batch_size=512, weight_decay=10, lr=1e-4, normalize=False)\n  A = nca.A.detach().cpu().numpy()\n  X_train = X_train.cpu().numpy()\n  y_train = y_train.cpu().numpy()\n  X_train_embed = X_train @ A.T\n  X_test_embed = X_test @ A.T\n  knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n  knn.fit(X_train_embed, y_train)\n  tic = time.time()\n  predictions = knn.predict(X_test_embed)\n  toc = time.time()\n  nca_time = toc - tic\n  nca_error = 1 - accuracy_score(predictions, y_test)\n  nca_bytes = X_train_embed.size * X_train_embed.itemsize\n  print(""torchnca knn - time: {:.2f} - error: {:.2f} - storage: {:.2f} Mb"".format(\n    nca_time, 100 * nca_error, nca_bytes*1e-6))\n\n  # raw kNN\n  knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n  knn.fit(X_train, y_train)\n  tic = time.time()\n  predictions = knn.predict(X_test)\n  toc = time.time()\n  vanilla_time = toc - tic\n  vanilla_error = 1 - accuracy_score(predictions, y_test)\n  vanilla_bytes = X_train.size * X_train.itemsize\n  print(""vanilla knn - time: {:.2f} - error: {:.2f} - storage: {:.2f} Mb"".format(\n    vanilla_time, 100*vanilla_error, vanilla_bytes*1e-6))\n\n  speedup = vanilla_time / nca_time\n  print(""speedup: {:.2f}x"".format(speedup))\n\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--seed"", type=int, default=0, help=""The rng seed."")\n  parser.add_argument(""--init"", type=str, default=""random"", help=""Which initialization to use."")\n  parser.add_argument(""--cuda"", type=lambda x: x.lower() in [\'true\', \'1\'], default=False, help=""Whether to show GUI."")\n  args, unparsed = parser.parse_known_args()\n  main(args)\n'"
torchnca/__init__.py,0,"b'""""""A PyTorch implementation of Neighbourhood Components Analysis.\n""""""\n\nfrom torchnca.nca import NCA\n'"
torchnca/nca.py,20,"b'import numpy as np\nimport torch\n\n\nclass NCA:\n  """"""Neighbourhood Components Analysis [1].\n\n  References:\n    [1]: https://www.cs.toronto.edu/~hinton/absps/nca.pdf\n  """"""\n  def __init__(self, dim=None, init=""random"", max_iters=500, tol=1e-5):\n    """"""Constructor.\n\n    Args:\n      dim (int): The dimension of the the learned\n        linear map A. If no dimension is provided,\n        we assume a square matrix A of same dimension\n        as the input data. For small values of `dim`\n        (i.e. 2, 3), NCA will perform dimensionality\n        reduction.\n      init (str): The type of initialization to use for\n        the matrix A.\n          - `random`: A = N(0, I)\n          - `identity`: A = I\n      max_iters (int): The maximum number of iterations\n        to run the optimization for.\n      tol (float): The tolerance for convergence. If the\n        difference between consecutive solutions is within\n        this number, optimization is terminated.\n    """"""\n    self.dim = dim\n    self.init = init\n    self.max_iters = max_iters\n    self.tol = tol\n    self._mean = None\n    self._stddev = None\n    self._losses = None\n\n  def __call__(self, X):\n    """"""Apply the learned linear map to the input.\n    """"""\n    if self._mean is not None and self._stddev is not None:\n      X = (X - self._mean) / self._stddev\n    return torch.mm(X, torch.t(self.A))\n\n  def _init_transformation(self):\n    """"""Initialize the linear transformation A.\n    """"""\n    if self.dim is None:\n      self.dim = self.num_dims\n    if self.init == ""random"":\n      a = torch.randn(self.dim, self.num_dims, device=self.device) * 0.01\n      self.A = torch.nn.Parameter(a)\n    elif self.init == ""identity"":\n      a = torch.eye(self.dim, self.num_dims, device=self.device)\n      self.A = torch.nn.Parameter(a)\n    else:\n      raise ValueError(""[!] {} initialization is not supported."".format(self.init))\n\n  @staticmethod\n  def _pairwise_l2_sq(x):\n    """"""Compute pairwise squared Euclidean distances.\n    """"""\n    dot = torch.mm(x.double(), torch.t(x.double()))\n    norm_sq = torch.diag(dot)\n    dist = norm_sq[None, :] - 2*dot + norm_sq[:, None]\n    dist = torch.clamp(dist, min=0)  # replace negative values with 0\n    return dist.float()\n\n  @staticmethod\n  def _softmax(x):\n    """"""Compute row-wise softmax.\n\n    Notes:\n      Since the input to this softmax is the negative of the\n      pairwise L2 distances, we don\'t need to do the classical\n      numerical stability trick.\n    """"""\n    exp = torch.exp(x)\n    return exp / exp.sum(dim=1)\n\n  @property\n  def mean(self):\n    if self._mean is None:\n      raise ValueError(\'No mean was computed. Make sure normalize is set to True.\')\n    return self._mean\n\n  @property\n  def stddev(self):\n    if self._stddev is None:\n      raise ValueError(\'No stddev was computed. Make sure normalize is set to True.\')\n    return self._stddev\n\n  @property\n  def losses(self):\n    if self._losses is None:\n      raise ValueError(\'There are no losses to report. You must call train first.\')\n    return self._losses\n\n  def loss(self, X, y_mask):\n    # compute pairwise squared Euclidean distances\n    # in transformed space\n    embedding = torch.mm(X, torch.t(self.A))\n    distances = self._pairwise_l2_sq(embedding)\n\n    # fill diagonal values such that exponentiating them\n    # makes them equal to 0\n    distances.diagonal().copy_(np.inf*torch.ones(len(distances)))\n\n    # compute pairwise probability matrix p_ij\n    # defined by a softmax over negative squared\n    # distances in the transformed space.\n    # since we are dealing with negative values\n    # with the largest value being 0, we need\n    # not worry about numerical instabilities\n    # in the softmax function\n    p_ij = self._softmax(-distances)\n\n    # for each p_i, zero out any p_ij that\n    # is not of the same class label as i\n    p_ij_mask = p_ij * y_mask.float()\n\n    # sum over js to compute p_i\n    p_i = p_ij_mask.sum(dim=1)\n\n    # compute expected number of points\n    # correctly classified by summing\n    # over all p_i\'s.\n    # to maximize the above expectation\n    # we can negate it and feed it to\n    # a minimizer\n    # for numerical stability, we only\n    # log_sum over non-zero values\n    classification_loss = -torch.log(torch.masked_select(p_i, p_i != 0)).sum()\n\n    # to prevent the embeddings of different\n    # classes from collapsing to the same\n    # point, we add a hinge loss penalty\n    distances.diagonal().copy_(torch.zeros(len(distances)))\n    diff_class_distances = distances * (~y_mask).float()\n    margin_diff = 1 - diff_class_distances\n    hinge_loss = torch.clamp(margin_diff, min=0).pow(2).sum(1).mean()\n\n    # sum both loss terms and return\n    loss = classification_loss + hinge_loss\n    return loss\n\n  def train(\n    self,\n    X,\n    y,\n    batch_size=None,\n    lr=1e-4,\n    momentum=0.9,\n    weight_decay=10,\n    normalize=True,\n  ):\n    """"""Trains NCA until convergence.\n\n    Specifically, we maximize the expected number of points\n    correctly classified under a stochastic selection rule.\n    This rule is defined using a softmax over Euclidean distances\n    in the transformed space.\n\n    Args:\n      X (torch.FloatTensor): The dataset of shape (N, D) where\n        `D` is the dimension of the feature space and `N`\n        is the number of training examples.\n      y (torch.LongTensor): The class labels of shape (N,).\n      batch_size (int): How many data samples to use in an SGD\n        update step.\n      lr (float): The learning rate.\n      weight_decay (float): The strength of the L2 regularization\n        on the learned transformation A.\n      normalize (bool): Whether to whiten the input, i.e. to\n        subtract the feature-wise mean and divide by the\n        feature-wise standard deviation.\n    """"""\n    self._losses = []\n    self.num_train, self.num_dims = X.shape\n    self.device = torch.device(""cuda"" if X.is_cuda else ""cpu"")\n    if batch_size is None:\n      batch_size = self.num_train\n    batch_size = min(batch_size, self.num_train)\n\n    # initialize the linear transformation matrix A\n    self._init_transformation()\n\n    # zero-mean the input data\n    if normalize:\n      self._mean = X.mean(dim=0)\n      self._stddev = X.std(dim=0)\n      X = (X - self._mean) / self._stddev\n\n    optim_args = {\n      \'lr\': lr,\n      \'weight_decay\': weight_decay,\n      \'momentum\': momentum,\n    }\n    optimizer = torch.optim.SGD([self.A], **optim_args)\n    iters_per_epoch = int(np.ceil(self.num_train / batch_size))\n    i_global = 0\n    for epoch in range(self.max_iters):\n      rand_idxs = torch.randperm(len(y))  # shuffle dataset\n      X = X[rand_idxs]\n      y = y[rand_idxs]\n      A_prev = optimizer.param_groups[0][\'params\'][0].clone()\n      for i in range(iters_per_epoch):\n        # grab batch\n        X_batch = X[i*batch_size:(i+1)*batch_size]\n        y_batch = y[i*batch_size:(i+1)*batch_size]\n\n        # compute pairwise boolean class matrix\n        y_mask = y_batch[:, None] == y_batch[None, :]\n\n        # compute loss and take gradient step\n        optimizer.zero_grad()\n        loss = self.loss(X_batch, y_mask)\n        self._losses.append(loss.item())\n        loss.backward()\n        optimizer.step()\n\n        i_global += 1\n        if not i_global % 25:\n          print(""epoch: {} - loss: {:.5f}"".format(epoch+1, loss.item()))\n\n      # check if within convergence\n      A_curr = optimizer.param_groups[0][\'params\'][0]\n      if torch.all(torch.abs(A_prev - A_curr) <= self.tol):\n        print(""[*] Optimization has converged in {} mini batch iterations."".format(i_global))\n        break\n'"
