file_path,api_count,code
config.py,0,"b'import argparse\n\narg_lists = []\nparser = argparse.ArgumentParser(description=""RAM"")\n\n\ndef str2bool(v):\n    return v.lower() in (""true"", ""1"")\n\n\ndef add_argument_group(name):\n    arg = parser.add_argument_group(name)\n    arg_lists.append(arg)\n    return arg\n\n\n# glimpse network params\nglimpse_arg = add_argument_group(""Glimpse Network Params"")\nglimpse_arg.add_argument(\n    ""--patch_size"", type=int, default=8, help=""size of extracted patch at highest res""\n)\nglimpse_arg.add_argument(\n    ""--glimpse_scale"", type=int, default=1, help=""scale of successive patches""\n)\nglimpse_arg.add_argument(\n    ""--num_patches"", type=int, default=1, help=""# of downscaled patches per glimpse""\n)\nglimpse_arg.add_argument(\n    ""--loc_hidden"", type=int, default=128, help=""hidden size of loc fc""\n)\nglimpse_arg.add_argument(\n    ""--glimpse_hidden"", type=int, default=128, help=""hidden size of glimpse fc""\n)\n\n\n# core network params\ncore_arg = add_argument_group(""Core Network Params"")\ncore_arg.add_argument(\n    ""--num_glimpses"", type=int, default=6, help=""# of glimpses, i.e. BPTT iterations""\n)\ncore_arg.add_argument(""--hidden_size"", type=int, default=256, help=""hidden size of rnn"")\n\n\n# reinforce params\nreinforce_arg = add_argument_group(""Reinforce Params"")\nreinforce_arg.add_argument(\n    ""--std"", type=float, default=0.05, help=""gaussian policy standard deviation""\n)\nreinforce_arg.add_argument(\n    ""--M"", type=int, default=1, help=""Monte Carlo sampling for valid and test sets""\n)\n\n\n# data params\ndata_arg = add_argument_group(""Data Params"")\ndata_arg.add_argument(\n    ""--valid_size"",\n    type=float,\n    default=0.1,\n    help=""Proportion of training set used for validation"",\n)\ndata_arg.add_argument(\n    ""--batch_size"", type=int, default=128, help=""# of images in each batch of data""\n)\ndata_arg.add_argument(\n    ""--num_workers"",\n    type=int,\n    default=4,\n    help=""# of subprocesses to use for data loading"",\n)\ndata_arg.add_argument(\n    ""--shuffle"",\n    type=str2bool,\n    default=True,\n    help=""Whether to shuffle the train and valid indices"",\n)\ndata_arg.add_argument(\n    ""--show_sample"",\n    type=str2bool,\n    default=False,\n    help=""Whether to visualize a sample grid of the data"",\n)\n\n\n# training params\ntrain_arg = add_argument_group(""Training Params"")\ntrain_arg.add_argument(\n    ""--is_train"", type=str2bool, default=True, help=""Whether to train or test the model""\n)\ntrain_arg.add_argument(\n    ""--momentum"", type=float, default=0.5, help=""Nesterov momentum value""\n)\ntrain_arg.add_argument(\n    ""--epochs"", type=int, default=200, help=""# of epochs to train for""\n)\ntrain_arg.add_argument(\n    ""--init_lr"", type=float, default=3e-4, help=""Initial learning rate value""\n)\ntrain_arg.add_argument(\n    ""--lr_patience"",\n    type=int,\n    default=20,\n    help=""Number of epochs to wait before reducing lr"",\n)\ntrain_arg.add_argument(\n    ""--train_patience"",\n    type=int,\n    default=50,\n    help=""Number of epochs to wait before stopping train"",\n)\n\n\n# other params\nmisc_arg = add_argument_group(""Misc."")\nmisc_arg.add_argument(\n    ""--use_gpu"", type=str2bool, default=True, help=""Whether to run on the GPU""\n)\nmisc_arg.add_argument(\n    ""--best"",\n    type=str2bool,\n    default=True,\n    help=""Load best model or most recent for testing"",\n)\nmisc_arg.add_argument(\n    ""--random_seed"", type=int, default=1, help=""Seed to ensure reproducibility""\n)\nmisc_arg.add_argument(\n    ""--data_dir"", type=str, default=""./data"", help=""Directory in which data is stored""\n)\nmisc_arg.add_argument(\n    ""--ckpt_dir"",\n    type=str,\n    default=""./ckpt"",\n    help=""Directory in which to save model checkpoints"",\n)\nmisc_arg.add_argument(\n    ""--logs_dir"",\n    type=str,\n    default=""./logs/"",\n    help=""Directory in which Tensorboard logs wil be stored"",\n)\nmisc_arg.add_argument(\n    ""--use_tensorboard"",\n    type=str2bool,\n    default=False,\n    help=""Whether to use tensorboard for visualization"",\n)\nmisc_arg.add_argument(\n    ""--resume"",\n    type=str2bool,\n    default=False,\n    help=""Whether to resume training from checkpoint"",\n)\nmisc_arg.add_argument(\n    ""--print_freq"",\n    type=int,\n    default=10,\n    help=""How frequently to print training details"",\n)\nmisc_arg.add_argument(\n    ""--plot_freq"", type=int, default=1, help=""How frequently to plot glimpses""\n)\n\n\ndef get_config():\n    config, unparsed = parser.parse_known_args()\n    return config, unparsed\n'"
data_loader.py,5,"b'import numpy as np\nfrom utils import plot_images\n\nimport torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n\ndef get_train_valid_loader(\n    data_dir,\n    batch_size,\n    random_seed,\n    valid_size=0.1,\n    shuffle=True,\n    show_sample=False,\n    num_workers=4,\n    pin_memory=False,\n):\n    """"""Train and validation data loaders.\n\n    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n\n    Args:\n        data_dir: path directory to the dataset.\n        batch_size: how many samples per batch to load.\n        random_seed: fix seed for reproducibility.\n        valid_size: percentage split of the training set used for\n            the validation set. Should be a float in the range [0, 1].\n            In the paper, this number is set to 0.1.\n        shuffle: whether to shuffle the train/validation indices.\n        show_sample: plot 9x9 sample grid of the dataset.\n        num_workers: number of subprocesses to use when loading the dataset.\n        pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n            True if using GPU.\n    """"""\n    error_msg = ""[!] valid_size should be in the range [0, 1].""\n    assert (valid_size >= 0) and (valid_size <= 1), error_msg\n\n    # define transforms\n    normalize = transforms.Normalize((0.1307,), (0.3081,))\n    trans = transforms.Compose([transforms.ToTensor(), normalize])\n\n    # load dataset\n    dataset = datasets.MNIST(data_dir, train=True, download=True, transform=trans)\n\n    num_train = len(dataset)\n    indices = list(range(num_train))\n    split = int(np.floor(valid_size * num_train))\n\n    if shuffle:\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n\n    train_idx, valid_idx = indices[split:], indices[:split]\n\n    train_sampler = SubsetRandomSampler(train_idx)\n    valid_sampler = SubsetRandomSampler(valid_idx)\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        sampler=train_sampler,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n\n    valid_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        sampler=valid_sampler,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n\n    # visualize some images\n    if show_sample:\n        sample_loader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=9,\n            shuffle=shuffle,\n            num_workers=num_workers,\n            pin_memory=pin_memory,\n        )\n        data_iter = iter(sample_loader)\n        images, labels = data_iter.next()\n        X = images.numpy()\n        X = np.transpose(X, [0, 2, 3, 1])\n        plot_images(X, labels)\n\n    return (train_loader, valid_loader)\n\n\ndef get_test_loader(data_dir, batch_size, num_workers=4, pin_memory=False):\n    """"""Test datalaoder.\n\n    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n\n    Args:\n        data_dir: path directory to the dataset.\n        batch_size: how many samples per batch to load.\n        num_workers: number of subprocesses to use when loading the dataset.\n        pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n            True if using GPU.\n    """"""\n    # define transforms\n    normalize = transforms.Normalize((0.1307,), (0.3081,))\n    trans = transforms.Compose([transforms.ToTensor(), normalize])\n\n    # load dataset\n    dataset = datasets.MNIST(data_dir, train=False, download=True, transform=trans)\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n\n    return data_loader\n'"
main.py,2,"b'import torch\n\nimport utils\nimport data_loader\n\nfrom trainer import Trainer\nfrom config import get_config\n\n\ndef main(config):\n    utils.prepare_dirs(config)\n\n    # ensure reproducibility\n    torch.manual_seed(config.random_seed)\n    kwargs = {}\n    if config.use_gpu:\n        torch.cuda.manual_seed(config.random_seed)\n        kwargs = {""num_workers"": 1, ""pin_memory"": True}\n\n    # instantiate data loaders\n    if config.is_train:\n        dloader = data_loader.get_train_valid_loader(\n            config.data_dir,\n            config.batch_size,\n            config.random_seed,\n            config.valid_size,\n            config.shuffle,\n            config.show_sample,\n            **kwargs,\n        )\n    else:\n        dloader = data_loader.get_test_loader(\n            config.data_dir, config.batch_size, **kwargs,\n        )\n\n    trainer = Trainer(config, dloader)\n\n    # either train\n    if config.is_train:\n        utils.save_config(config)\n        trainer.train()\n    # or load a pretrained model and test\n    else:\n        trainer.test()\n\n\nif __name__ == ""__main__"":\n    config, unparsed = get_config()\n    main(config)\n'"
model.py,1,"b'import torch.nn as nn\n\nimport modules\n\n\nclass RecurrentAttention(nn.Module):\n    """"""A Recurrent Model of Visual Attention (RAM) [1].\n\n    RAM is a recurrent neural network that processes\n    inputs sequentially, attending to different locations\n    within the image one at a time, and incrementally\n    combining information from these fixations to build\n    up a dynamic internal representation of the image.\n\n    References:\n      [1]: Minh et. al., https://arxiv.org/abs/1406.6247\n    """"""\n\n    def __init__(\n        self, g, k, s, c, h_g, h_l, std, hidden_size, num_classes,\n    ):\n        """"""Constructor.\n\n        Args:\n          g: size of the square patches in the glimpses extracted by the retina.\n          k: number of patches to extract per glimpse.\n          s: scaling factor that controls the size of successive patches.\n          c: number of channels in each image.\n          h_g: hidden layer size of the fc layer for `phi`.\n          h_l: hidden layer size of the fc layer for `l`.\n          std: standard deviation of the Gaussian policy.\n          hidden_size: hidden size of the rnn.\n          num_classes: number of classes in the dataset.\n          num_glimpses: number of glimpses to take per image,\n            i.e. number of BPTT steps.\n        """"""\n        super().__init__()\n\n        self.std = std\n\n        self.sensor = modules.GlimpseNetwork(h_g, h_l, g, k, s, c)\n        self.rnn = modules.CoreNetwork(hidden_size, hidden_size)\n        self.locator = modules.LocationNetwork(hidden_size, 2, std)\n        self.classifier = modules.ActionNetwork(hidden_size, num_classes)\n        self.baseliner = modules.BaselineNetwork(hidden_size, 1)\n\n    def forward(self, x, l_t_prev, h_t_prev, last=False):\n        """"""Run RAM for one timestep on a minibatch of images.\n\n        Args:\n            x: a 4D Tensor of shape (B, H, W, C). The minibatch\n                of images.\n            l_t_prev: a 2D tensor of shape (B, 2). The location vector\n                containing the glimpse coordinates [x, y] for the previous\n                timestep `t-1`.\n            h_t_prev: a 2D tensor of shape (B, hidden_size). The hidden\n                state vector for the previous timestep `t-1`.\n            last: a bool indicating whether this is the last timestep.\n                If True, the action network returns an output probability\n                vector over the classes and the baseline `b_t` for the\n                current timestep `t`. Else, the core network returns the\n                hidden state vector for the next timestep `t+1` and the\n                location vector for the next timestep `t+1`.\n\n        Returns:\n            h_t: a 2D tensor of shape (B, hidden_size). The hidden\n                state vector for the current timestep `t`.\n            mu: a 2D tensor of shape (B, 2). The mean that parametrizes\n                the Gaussian policy.\n            l_t: a 2D tensor of shape (B, 2). The location vector\n                containing the glimpse coordinates [x, y] for the\n                current timestep `t`.\n            b_t: a vector of length (B,). The baseline for the\n                current time step `t`.\n            log_probas: a 2D tensor of shape (B, num_classes). The\n                output log probability vector over the classes.\n            log_pi: a vector of length (B,).\n        """"""\n        g_t = self.sensor(x, l_t_prev)\n        h_t = self.rnn(g_t, h_t_prev)\n\n        log_pi, l_t = self.locator(h_t)\n        b_t = self.baseliner(h_t).squeeze()\n\n        if last:\n            log_probas = self.classifier(h_t)\n            return h_t, l_t, b_t, log_probas, log_pi\n\n        return h_t, l_t, b_t, log_pi\n'"
modules.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.distributions import Normal\n\n\nclass Retina:\n    """"""A visual retina.\n\n    Extracts a foveated glimpse `phi` around location `l`\n    from an image `x`.\n\n    Concretely, encodes the region around `l` at a\n    high-resolution but uses a progressively lower\n    resolution for pixels further from `l`, resulting\n    in a compressed representation of the original\n    image `x`.\n\n    Args:\n        x: a 4D Tensor of shape (B, H, W, C). The minibatch\n            of images.\n        l: a 2D Tensor of shape (B, 2). Contains normalized\n            coordinates in the range [-1, 1].\n        g: size of the first square patch.\n        k: number of patches to extract in the glimpse.\n        s: scaling factor that controls the size of\n            successive patches.\n\n    Returns:\n        phi: a 5D tensor of shape (B, k, g, g, C). The\n            foveated glimpse of the image.\n    """"""\n\n    def __init__(self, g, k, s):\n        self.g = g\n        self.k = k\n        self.s = s\n\n    def foveate(self, x, l):\n        """"""Extract `k` square patches of size `g`, centered\n        at location `l`. The initial patch is a square of\n        size `g`, and each subsequent patch is a square\n        whose side is `s` times the size of the previous\n        patch.\n\n        The `k` patches are finally resized to (g, g) and\n        concatenated into a tensor of shape (B, k, g, g, C).\n        """"""\n        phi = []\n        size = self.g\n\n        # extract k patches of increasing size\n        for i in range(self.k):\n            phi.append(self.extract_patch(x, l, size))\n            size = int(self.s * size)\n\n        # resize the patches to squares of size g\n        for i in range(1, len(phi)):\n            k = phi[i].shape[-1] // self.g\n            phi[i] = F.avg_pool2d(phi[i], k)\n\n        # concatenate into a single tensor and flatten\n        phi = torch.cat(phi, 1)\n        phi = phi.view(phi.shape[0], -1)\n\n        return phi\n\n    def extract_patch(self, x, l, size):\n        """"""Extract a single patch for each image in `x`.\n\n        Args:\n        x: a 4D Tensor of shape (B, H, W, C). The minibatch\n            of images.\n        l: a 2D Tensor of shape (B, 2).\n        size: a scalar defining the size of the extracted patch.\n\n        Returns:\n            patch: a 4D Tensor of shape (B, size, size, C)\n        """"""\n        B, C, H, W = x.shape\n\n        start = self.denormalize(H, l)\n        end = start + size\n\n        # pad with zeros\n        x = F.pad(x, (size // 2, size // 2, size // 2, size // 2))\n\n        # loop through mini-batch and extract patches\n        patch = []\n        for i in range(B):\n            patch.append(x[i, :, start[i, 1] : end[i, 1], start[i, 0] : end[i, 0]])\n        return torch.stack(patch)\n\n    def denormalize(self, T, coords):\n        """"""Convert coordinates in the range [-1, 1] to\n        coordinates in the range [0, T] where `T` is\n        the size of the image.\n        """"""\n        return (0.5 * ((coords + 1.0) * T)).long()\n\n    def exceeds(self, from_x, to_x, from_y, to_y, T):\n        """"""Check whether the extracted patch will exceed\n        the boundaries of the image of size `T`.\n        """"""\n        if (from_x < 0) or (from_y < 0) or (to_x > T) or (to_y > T):\n            return True\n        return False\n\n\nclass GlimpseNetwork(nn.Module):\n    """"""The glimpse network.\n\n    Combines the ""what"" and the ""where"" into a glimpse\n    feature vector `g_t`.\n\n    - ""what"": glimpse extracted from the retina.\n    - ""where"": location tuple where glimpse was extracted.\n\n    Concretely, feeds the output of the retina `phi` to\n    a fc layer and the glimpse location vector `l_t_prev`\n    to a fc layer. Finally, these outputs are fed each\n    through a fc layer and their sum is rectified.\n\n    In other words:\n\n        `g_t = relu( fc( fc(l) ) + fc( fc(phi) ) )`\n\n    Args:\n        h_g: hidden layer size of the fc layer for `phi`.\n        h_l: hidden layer size of the fc layer for `l`.\n        g: size of the square patches in the glimpses extracted\n        by the retina.\n        k: number of patches to extract per glimpse.\n        s: scaling factor that controls the size of successive patches.\n        c: number of channels in each image.\n        x: a 4D Tensor of shape (B, H, W, C). The minibatch\n            of images.\n        l_t_prev: a 2D tensor of shape (B, 2). Contains the glimpse\n            coordinates [x, y] for the previous timestep `t-1`.\n\n    Returns:\n        g_t: a 2D tensor of shape (B, hidden_size).\n            The glimpse representation returned by\n            the glimpse network for the current\n            timestep `t`.\n    """"""\n\n    def __init__(self, h_g, h_l, g, k, s, c):\n        super().__init__()\n\n        self.retina = Retina(g, k, s)\n\n        # glimpse layer\n        D_in = k * g * g * c\n        self.fc1 = nn.Linear(D_in, h_g)\n\n        # location layer\n        D_in = 2\n        self.fc2 = nn.Linear(D_in, h_l)\n\n        self.fc3 = nn.Linear(h_g, h_g + h_l)\n        self.fc4 = nn.Linear(h_l, h_g + h_l)\n\n    def forward(self, x, l_t_prev):\n        # generate glimpse phi from image x\n        phi = self.retina.foveate(x, l_t_prev)\n\n        # flatten location vector\n        l_t_prev = l_t_prev.view(l_t_prev.size(0), -1)\n\n        # feed phi and l to respective fc layers\n        phi_out = F.relu(self.fc1(phi))\n        l_out = F.relu(self.fc2(l_t_prev))\n\n        what = self.fc3(phi_out)\n        where = self.fc4(l_out)\n\n        # feed to fc layer\n        g_t = F.relu(what + where)\n\n        return g_t\n\n\nclass CoreNetwork(nn.Module):\n    """"""The core network.\n\n    An RNN that maintains an internal state by integrating\n    information extracted from the history of past observations.\n    It encodes the agent\'s knowledge of the environment through\n    a state vector `h_t` that gets updated at every time step `t`.\n\n    Concretely, it takes the glimpse representation `g_t` as input,\n    and combines it with its internal state `h_t_prev` at the previous\n    time step, to produce the new internal state `h_t` at the current\n    time step.\n\n    In other words:\n\n        `h_t = relu( fc(h_t_prev) + fc(g_t) )`\n\n    Args:\n        input_size: input size of the rnn.\n        hidden_size: hidden size of the rnn.\n        g_t: a 2D tensor of shape (B, hidden_size). The glimpse\n            representation returned by the glimpse network for the\n            current timestep `t`.\n        h_t_prev: a 2D tensor of shape (B, hidden_size). The\n            hidden state vector for the previous timestep `t-1`.\n\n    Returns:\n        h_t: a 2D tensor of shape (B, hidden_size). The hidden\n            state vector for the current timestep `t`.\n    """"""\n\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        self.i2h = nn.Linear(input_size, hidden_size)\n        self.h2h = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, g_t, h_t_prev):\n        h1 = self.i2h(g_t)\n        h2 = self.h2h(h_t_prev)\n        h_t = F.relu(h1 + h2)\n        return h_t\n\n\nclass ActionNetwork(nn.Module):\n    """"""The action network.\n\n    Uses the internal state `h_t` of the core network to\n    produce the final output classification.\n\n    Concretely, feeds the hidden state `h_t` through a fc\n    layer followed by a softmax to create a vector of\n    output probabilities over the possible classes.\n\n    Hence, the environment action `a_t` is drawn from a\n    distribution conditioned on an affine transformation\n    of the hidden state vector `h_t`, or in other words,\n    the action network is simply a linear softmax classifier.\n\n    Args:\n        input_size: input size of the fc layer.\n        output_size: output size of the fc layer.\n        h_t: the hidden state vector of the core network\n            for the current time step `t`.\n\n    Returns:\n        a_t: output probability vector over the classes.\n    """"""\n\n    def __init__(self, input_size, output_size):\n        super().__init__()\n\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, h_t):\n        a_t = F.log_softmax(self.fc(h_t), dim=1)\n        return a_t\n\n\nclass LocationNetwork(nn.Module):\n    """"""The location network.\n\n    Uses the internal state `h_t` of the core network to\n    produce the location coordinates `l_t` for the next\n    time step.\n\n    Concretely, feeds the hidden state `h_t` through a fc\n    layer followed by a tanh to clamp the output beween\n    [-1, 1]. This produces a 2D vector of means used to\n    parametrize a two-component Gaussian with a fixed\n    variance from which the location coordinates `l_t`\n    for the next time step are sampled.\n\n    Hence, the location `l_t` is chosen stochastically\n    from a distribution conditioned on an affine\n    transformation of the hidden state vector `h_t`.\n\n    Args:\n        input_size: input size of the fc layer.\n        output_size: output size of the fc layer.\n        std: standard deviation of the normal distribution.\n        h_t: the hidden state vector of the core network for\n            the current time step `t`.\n\n    Returns:\n        mu: a 2D vector of shape (B, 2).\n        l_t: a 2D vector of shape (B, 2).\n    """"""\n\n    def __init__(self, input_size, output_size, std):\n        super().__init__()\n\n        self.std = std\n\n        hid_size = input_size // 2\n        self.fc = nn.Linear(input_size, hid_size)\n        self.fc_lt = nn.Linear(hid_size, output_size)\n\n    def forward(self, h_t):\n        # compute mean\n        feat = F.relu(self.fc(h_t.detach()))\n        mu = torch.tanh(self.fc_lt(feat))\n\n        # reparametrization trick\n        l_t = torch.distributions.Normal(mu, self.std).rsample()\n        l_t = l_t.detach()\n        log_pi = Normal(mu, self.std).log_prob(l_t)\n\n        # we assume both dimensions are independent\n        # 1. pdf of the joint is the product of the pdfs\n        # 2. log of the product is the sum of the logs\n        log_pi = torch.sum(log_pi, dim=1)\n\n        # bound between [-1, 1]\n        l_t = torch.clamp(l_t, -1, 1)\n\n        return log_pi, l_t\n\n\nclass BaselineNetwork(nn.Module):\n    """"""The baseline network.\n\n    This network regresses the baseline in the\n    reward function to reduce the variance of\n    the gradient update.\n\n    Args:\n        input_size: input size of the fc layer.\n        output_size: output size of the fc layer.\n        h_t: the hidden state vector of the core network\n            for the current time step `t`.\n\n    Returns:\n        b_t: a 2D vector of shape (B, 1). The baseline\n            for the current time step `t`.\n    """"""\n\n    def __init__(self, input_size, output_size):\n        super().__init__()\n\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, h_t):\n        b_t = self.fc(h_t.detach())\n        return b_t\n'"
plot_glimpses.py,0,"b'import pickle\nimport argparse\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nfrom utils import denormalize, bounding_box\n\n\ndef parse_arguments():\n    arg = argparse.ArgumentParser()\n    arg.add_argument(\n        ""--plot_dir"",\n        type=str,\n        required=True,\n        help=""path to directory containing pickle dumps"",\n    )\n    arg.add_argument(""--epoch"", type=int, required=True, help=""epoch of desired plot"")\n    args = vars(arg.parse_args())\n    return args[""plot_dir""], args[""epoch""]\n\n\ndef main(plot_dir, epoch):\n    # read in pickle files\n    glimpses = pickle.load(open(plot_dir + ""g_{}.p"".format(epoch), ""rb""))\n    locations = pickle.load(open(plot_dir + ""l_{}.p"".format(epoch), ""rb""))\n\n    from ipdb import set_trace\n\n    set_trace()\n\n    glimpses = np.concatenate(glimpses)\n\n    # grab useful params\n    size = int(plot_dir.split(""_"")[2][0])\n    num_anims = len(locations)\n    num_cols = glimpses.shape[0]\n    img_shape = glimpses.shape[1]\n\n    # denormalize coordinates\n    coords = [denormalize(img_shape, l) for l in locations]\n\n    fig, axs = plt.subplots(nrows=1, ncols=num_cols)\n    # fig.set_dpi(100)\n\n    # plot base image\n    for j, ax in enumerate(axs.flat):\n        ax.imshow(glimpses[j], cmap=""Greys_r"")\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\n    def updateData(i):\n        color = ""r""\n        co = coords[i]\n        for j, ax in enumerate(axs.flat):\n            for p in ax.patches:\n                p.remove()\n            c = co[j]\n            rect = bounding_box(c[0], c[1], size, color)\n            ax.add_patch(rect)\n\n    # animate\n    anim = animation.FuncAnimation(\n        fig, updateData, frames=num_anims, interval=500, repeat=True\n    )\n\n    # save as mp4\n    name = plot_dir + ""epoch_{}.mp4"".format(epoch)\n    anim.save(name, extra_args=[""-vcodec"", ""h264"", ""-pix_fmt"", ""yuv420p""])\n\n\nif __name__ == ""__main__"":\n    args = parse_arguments()\n    main(*args)\n'"
trainer.py,27,"b'import os\nimport time\nimport shutil\nimport pickle\n\nimport torch\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tensorboard_logger import configure, log_value\n\nfrom model import RecurrentAttention\nfrom utils import AverageMeter\n\n\nclass Trainer:\n    """"""A Recurrent Attention Model trainer.\n\n    All hyperparameters are provided by the user in the\n    config file.\n    """"""\n\n    def __init__(self, config, data_loader):\n        """"""\n        Construct a new Trainer instance.\n\n        Args:\n            config: object containing command line arguments.\n            data_loader: A data iterator.\n        """"""\n        self.config = config\n\n        if config.use_gpu and torch.cuda.is_available():\n            self.device = torch.device(""cuda"")\n        else:\n            self.device = torch.device(""cpu"")\n\n        # glimpse network params\n        self.patch_size = config.patch_size\n        self.glimpse_scale = config.glimpse_scale\n        self.num_patches = config.num_patches\n        self.loc_hidden = config.loc_hidden\n        self.glimpse_hidden = config.glimpse_hidden\n\n        # core network params\n        self.num_glimpses = config.num_glimpses\n        self.hidden_size = config.hidden_size\n\n        # reinforce params\n        self.std = config.std\n        self.M = config.M\n\n        # data params\n        if config.is_train:\n            self.train_loader = data_loader[0]\n            self.valid_loader = data_loader[1]\n            self.num_train = len(self.train_loader.sampler.indices)\n            self.num_valid = len(self.valid_loader.sampler.indices)\n        else:\n            self.test_loader = data_loader\n            self.num_test = len(self.test_loader.dataset)\n        self.num_classes = 10\n        self.num_channels = 1\n\n        # training params\n        self.epochs = config.epochs\n        self.start_epoch = 0\n        self.momentum = config.momentum\n        self.lr = config.init_lr\n\n        # misc params\n        self.best = config.best\n        self.ckpt_dir = config.ckpt_dir\n        self.logs_dir = config.logs_dir\n        self.best_valid_acc = 0.0\n        self.counter = 0\n        self.lr_patience = config.lr_patience\n        self.train_patience = config.train_patience\n        self.use_tensorboard = config.use_tensorboard\n        self.resume = config.resume\n        self.print_freq = config.print_freq\n        self.plot_freq = config.plot_freq\n        self.model_name = ""ram_{}_{}x{}_{}"".format(\n            config.num_glimpses,\n            config.patch_size,\n            config.patch_size,\n            config.glimpse_scale,\n        )\n\n        self.plot_dir = ""./plots/"" + self.model_name + ""/""\n        if not os.path.exists(self.plot_dir):\n            os.makedirs(self.plot_dir)\n\n        # configure tensorboard logging\n        if self.use_tensorboard:\n            tensorboard_dir = self.logs_dir + self.model_name\n            print(""[*] Saving tensorboard logs to {}"".format(tensorboard_dir))\n            if not os.path.exists(tensorboard_dir):\n                os.makedirs(tensorboard_dir)\n            configure(tensorboard_dir)\n\n        # build RAM model\n        self.model = RecurrentAttention(\n            self.patch_size,\n            self.num_patches,\n            self.glimpse_scale,\n            self.num_channels,\n            self.loc_hidden,\n            self.glimpse_hidden,\n            self.std,\n            self.hidden_size,\n            self.num_classes,\n        )\n        self.model.to(self.device)\n\n        # initialize optimizer and scheduler\n        self.optimizer = torch.optim.Adam(\n            self.model.parameters(), lr=self.config.init_lr\n        )\n        self.scheduler = ReduceLROnPlateau(\n            self.optimizer, ""min"", patience=self.lr_patience\n        )\n\n    def reset(self):\n        h_t = torch.zeros(\n            self.batch_size,\n            self.hidden_size,\n            dtype=torch.float,\n            device=self.device,\n            requires_grad=True,\n        )\n        l_t = torch.FloatTensor(self.batch_size, 2).uniform_(-1, 1).to(self.device)\n        l_t.requires_grad = True\n\n        return h_t, l_t\n\n    def train(self):\n        """"""Train the model on the training set.\n\n        A checkpoint of the model is saved after each epoch\n        and if the validation accuracy is improved upon,\n        a separate ckpt is created for use on the test set.\n        """"""\n        # load the most recent checkpoint\n        if self.resume:\n            self.load_checkpoint(best=False)\n\n        print(\n            ""\\n[*] Train on {} samples, validate on {} samples"".format(\n                self.num_train, self.num_valid\n            )\n        )\n\n        for epoch in range(self.start_epoch, self.epochs):\n\n            print(\n                ""\\nEpoch: {}/{} - LR: {:.6f}"".format(\n                    epoch + 1, self.epochs, self.optimizer.param_groups[0][""lr""]\n                )\n            )\n\n            # train for 1 epoch\n            train_loss, train_acc = self.train_one_epoch(epoch)\n\n            # evaluate on validation set\n            valid_loss, valid_acc = self.validate(epoch)\n\n            # # reduce lr if validation loss plateaus\n            self.scheduler.step(-valid_acc)\n\n            is_best = valid_acc > self.best_valid_acc\n            msg1 = ""train loss: {:.3f} - train acc: {:.3f} ""\n            msg2 = ""- val loss: {:.3f} - val acc: {:.3f} - val err: {:.3f}""\n            if is_best:\n                self.counter = 0\n                msg2 += "" [*]""\n            msg = msg1 + msg2\n            print(\n                msg.format(\n                    train_loss, train_acc, valid_loss, valid_acc, 100 - valid_acc\n                )\n            )\n\n            # check for improvement\n            if not is_best:\n                self.counter += 1\n            if self.counter > self.train_patience:\n                print(""[!] No improvement in a while, stopping training."")\n                return\n            self.best_valid_acc = max(valid_acc, self.best_valid_acc)\n            self.save_checkpoint(\n                {\n                    ""epoch"": epoch + 1,\n                    ""model_state"": self.model.state_dict(),\n                    ""optim_state"": self.optimizer.state_dict(),\n                    ""best_valid_acc"": self.best_valid_acc,\n                },\n                is_best,\n            )\n\n    def train_one_epoch(self, epoch):\n        """"""\n        Train the model for 1 epoch of the training set.\n\n        An epoch corresponds to one full pass through the entire\n        training set in successive mini-batches.\n\n        This is used by train() and should not be called manually.\n        """"""\n        self.model.train()\n        batch_time = AverageMeter()\n        losses = AverageMeter()\n        accs = AverageMeter()\n\n        tic = time.time()\n        with tqdm(total=self.num_train) as pbar:\n            for i, (x, y) in enumerate(self.train_loader):\n                self.optimizer.zero_grad()\n\n                x, y = x.to(self.device), y.to(self.device)\n\n                plot = False\n                if (epoch % self.plot_freq == 0) and (i == 0):\n                    plot = True\n\n                # initialize location vector and hidden state\n                self.batch_size = x.shape[0]\n                h_t, l_t = self.reset()\n\n                # save images\n                imgs = []\n                imgs.append(x[0:9])\n\n                # extract the glimpses\n                locs = []\n                log_pi = []\n                baselines = []\n                for t in range(self.num_glimpses - 1):\n                    # forward pass through model\n                    h_t, l_t, b_t, p = self.model(x, l_t, h_t)\n\n                    # store\n                    locs.append(l_t[0:9])\n                    baselines.append(b_t)\n                    log_pi.append(p)\n\n                # last iteration\n                h_t, l_t, b_t, log_probas, p = self.model(x, l_t, h_t, last=True)\n                log_pi.append(p)\n                baselines.append(b_t)\n                locs.append(l_t[0:9])\n\n                # convert list to tensors and reshape\n                baselines = torch.stack(baselines).transpose(1, 0)\n                log_pi = torch.stack(log_pi).transpose(1, 0)\n\n                # calculate reward\n                predicted = torch.max(log_probas, 1)[1]\n                R = (predicted.detach() == y).float()\n                R = R.unsqueeze(1).repeat(1, self.num_glimpses)\n\n                # compute losses for differentiable modules\n                loss_action = F.nll_loss(log_probas, y)\n                loss_baseline = F.mse_loss(baselines, R)\n\n                # compute reinforce loss\n                # summed over timesteps and averaged across batch\n                adjusted_reward = R - baselines.detach()\n                loss_reinforce = torch.sum(-log_pi * adjusted_reward, dim=1)\n                loss_reinforce = torch.mean(loss_reinforce, dim=0)\n\n                # sum up into a hybrid loss\n                loss = loss_action + loss_baseline + loss_reinforce * 0.01\n\n                # compute accuracy\n                correct = (predicted == y).float()\n                acc = 100 * (correct.sum() / len(y))\n\n                # store\n                losses.update(loss.item(), x.size()[0])\n                accs.update(acc.item(), x.size()[0])\n\n                # compute gradients and update SGD\n                loss.backward()\n                self.optimizer.step()\n\n                # measure elapsed time\n                toc = time.time()\n                batch_time.update(toc - tic)\n\n                pbar.set_description(\n                    (\n                        ""{:.1f}s - loss: {:.3f} - acc: {:.3f}"".format(\n                            (toc - tic), loss.item(), acc.item()\n                        )\n                    )\n                )\n                pbar.update(self.batch_size)\n\n                # dump the glimpses and locs\n                if plot:\n                    imgs = [g.cpu().data.numpy().squeeze() for g in imgs]\n                    locs = [l.cpu().data.numpy() for l in locs]\n                    pickle.dump(\n                        imgs, open(self.plot_dir + ""g_{}.p"".format(epoch + 1), ""wb"")\n                    )\n                    pickle.dump(\n                        locs, open(self.plot_dir + ""l_{}.p"".format(epoch + 1), ""wb"")\n                    )\n\n                # log to tensorboard\n                if self.use_tensorboard:\n                    iteration = epoch * len(self.train_loader) + i\n                    log_value(""train_loss"", losses.avg, iteration)\n                    log_value(""train_acc"", accs.avg, iteration)\n\n            return losses.avg, accs.avg\n\n    @torch.no_grad()\n    def validate(self, epoch):\n        """"""Evaluate the RAM model on the validation set.\n        """"""\n        losses = AverageMeter()\n        accs = AverageMeter()\n\n        for i, (x, y) in enumerate(self.valid_loader):\n            x, y = x.to(self.device), y.to(self.device)\n\n            # duplicate M times\n            x = x.repeat(self.M, 1, 1, 1)\n\n            # initialize location vector and hidden state\n            self.batch_size = x.shape[0]\n            h_t, l_t = self.reset()\n\n            # extract the glimpses\n            log_pi = []\n            baselines = []\n            for t in range(self.num_glimpses - 1):\n                # forward pass through model\n                h_t, l_t, b_t, p = self.model(x, l_t, h_t)\n\n                # store\n                baselines.append(b_t)\n                log_pi.append(p)\n\n            # last iteration\n            h_t, l_t, b_t, log_probas, p = self.model(x, l_t, h_t, last=True)\n            log_pi.append(p)\n            baselines.append(b_t)\n\n            # convert list to tensors and reshape\n            baselines = torch.stack(baselines).transpose(1, 0)\n            log_pi = torch.stack(log_pi).transpose(1, 0)\n\n            # average\n            log_probas = log_probas.view(self.M, -1, log_probas.shape[-1])\n            log_probas = torch.mean(log_probas, dim=0)\n\n            baselines = baselines.contiguous().view(self.M, -1, baselines.shape[-1])\n            baselines = torch.mean(baselines, dim=0)\n\n            log_pi = log_pi.contiguous().view(self.M, -1, log_pi.shape[-1])\n            log_pi = torch.mean(log_pi, dim=0)\n\n            # calculate reward\n            predicted = torch.max(log_probas, 1)[1]\n            R = (predicted.detach() == y).float()\n            R = R.unsqueeze(1).repeat(1, self.num_glimpses)\n\n            # compute losses for differentiable modules\n            loss_action = F.nll_loss(log_probas, y)\n            loss_baseline = F.mse_loss(baselines, R)\n\n            # compute reinforce loss\n            adjusted_reward = R - baselines.detach()\n            loss_reinforce = torch.sum(-log_pi * adjusted_reward, dim=1)\n            loss_reinforce = torch.mean(loss_reinforce, dim=0)\n\n            # sum up into a hybrid loss\n            loss = loss_action + loss_baseline + loss_reinforce * 0.01\n\n            # compute accuracy\n            correct = (predicted == y).float()\n            acc = 100 * (correct.sum() / len(y))\n\n            # store\n            losses.update(loss.item(), x.size()[0])\n            accs.update(acc.item(), x.size()[0])\n\n            # log to tensorboard\n            if self.use_tensorboard:\n                iteration = epoch * len(self.valid_loader) + i\n                log_value(""valid_loss"", losses.avg, iteration)\n                log_value(""valid_acc"", accs.avg, iteration)\n\n        return losses.avg, accs.avg\n\n    @torch.no_grad()\n    def test(self):\n        """"""Test the RAM model.\n\n        This function should only be called at the very\n        end once the model has finished training.\n        """"""\n        correct = 0\n\n        # load the best checkpoint\n        self.load_checkpoint(best=self.best)\n\n        for i, (x, y) in enumerate(self.test_loader):\n            x, y = x.to(self.device), y.to(self.device)\n\n            # duplicate M times\n            x = x.repeat(self.M, 1, 1, 1)\n\n            # initialize location vector and hidden state\n            self.batch_size = x.shape[0]\n            h_t, l_t = self.reset()\n\n            # extract the glimpses\n            for t in range(self.num_glimpses - 1):\n                # forward pass through model\n                h_t, l_t, b_t, p = self.model(x, l_t, h_t)\n\n            # last iteration\n            h_t, l_t, b_t, log_probas, p = self.model(x, l_t, h_t, last=True)\n\n            log_probas = log_probas.view(self.M, -1, log_probas.shape[-1])\n            log_probas = torch.mean(log_probas, dim=0)\n\n            pred = log_probas.data.max(1, keepdim=True)[1]\n            correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n\n        perc = (100.0 * correct) / (self.num_test)\n        error = 100 - perc\n        print(\n            ""[*] Test Acc: {}/{} ({:.2f}% - {:.2f}%)"".format(\n                correct, self.num_test, perc, error\n            )\n        )\n\n    def save_checkpoint(self, state, is_best):\n        """"""Saves a checkpoint of the model.\n\n        If this model has reached the best validation accuracy thus\n        far, a seperate file with the suffix `best` is created.\n        """"""\n        filename = self.model_name + ""_ckpt.pth.tar""\n        ckpt_path = os.path.join(self.ckpt_dir, filename)\n        torch.save(state, ckpt_path)\n        if is_best:\n            filename = self.model_name + ""_model_best.pth.tar""\n            shutil.copyfile(ckpt_path, os.path.join(self.ckpt_dir, filename))\n\n    def load_checkpoint(self, best=False):\n        """"""Load the best copy of a model.\n\n        This is useful for 2 cases:\n        - Resuming training with the most recent model checkpoint.\n        - Loading the best validation model to evaluate on the test data.\n\n        Args:\n            best: if set to True, loads the best model.\n                Use this if you want to evaluate your model\n                on the test data. Else, set to False in which\n                case the most recent version of the checkpoint\n                is used.\n        """"""\n        print(""[*] Loading model from {}"".format(self.ckpt_dir))\n\n        filename = self.model_name + ""_ckpt.pth.tar""\n        if best:\n            filename = self.model_name + ""_model_best.pth.tar""\n        ckpt_path = os.path.join(self.ckpt_dir, filename)\n        ckpt = torch.load(ckpt_path)\n\n        # load variables from checkpoint\n        self.start_epoch = ckpt[""epoch""]\n        self.best_valid_acc = ckpt[""best_valid_acc""]\n        self.model.load_state_dict(ckpt[""model_state""])\n        self.optimizer.load_state_dict(ckpt[""optim_state""])\n\n        if best:\n            print(\n                ""[*] Loaded {} checkpoint @ epoch {} ""\n                ""with best valid acc of {:.3f}"".format(\n                    filename, ckpt[""epoch""], ckpt[""best_valid_acc""]\n                )\n            )\n        else:\n            print(""[*] Loaded {} checkpoint @ epoch {}"".format(filename, ckpt[""epoch""]))\n'"
utils.py,0,"b'import os\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom PIL import Image\n\n\ndef denormalize(T, coords):\n    return 0.5 * ((coords + 1.0) * T)\n\n\ndef bounding_box(x, y, size, color=""w""):\n    x = int(x - (size / 2))\n    y = int(y - (size / 2))\n    rect = patches.Rectangle(\n        (x, y), size, size, linewidth=1, edgecolor=color, fill=False\n    )\n    return rect\n\n\n# https://github.com/pytorch/examples/blob/master/imagenet/main.py\nclass AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef resize_array(x, size):\n    # 3D and 4D tensors allowed only\n    assert x.ndim in [3, 4], ""Only 3D and 4D Tensors allowed!""\n\n    # 4D Tensor\n    if x.ndim == 4:\n        res = []\n        for i in range(x.shape[0]):\n            img = array2img(x[i])\n            img = img.resize((size, size))\n            img = np.asarray(img, dtype=""float32"")\n            img = np.expand_dims(img, axis=0)\n            img /= 255.0\n            res.append(img)\n        res = np.concatenate(res)\n        res = np.expand_dims(res, axis=1)\n        return res\n\n    # 3D Tensor\n    img = array2img(x)\n    img = img.resize((size, size))\n    res = np.asarray(img, dtype=""float32"")\n    res = np.expand_dims(res, axis=0)\n    res /= 255.0\n    return res\n\n\ndef img2array(data_path, desired_size=None, expand=False, view=False):\n    """"""\n    Util function for loading RGB image into a numpy array.\n\n    Returns array of shape (1, H, W, C).\n    """"""\n    img = Image.open(data_path)\n    img = img.convert(""RGB"")\n    if desired_size:\n        img = img.resize((desired_size[1], desired_size[0]))\n    if view:\n        img.show()\n    x = np.asarray(img, dtype=""float32"")\n    if expand:\n        x = np.expand_dims(x, axis=0)\n    x /= 255.0\n    return x\n\n\ndef array2img(x):\n    """"""\n    Util function for converting anumpy array to a PIL img.\n\n    Returns PIL RGB img.\n    """"""\n    x = np.asarray(x)\n    x = x + max(-np.min(x), 0)\n    x_max = np.max(x)\n    if x_max != 0:\n        x /= x_max\n    x *= 255\n    return Image.fromarray(x.astype(""uint8""), ""RGB"")\n\n\ndef plot_images(images, gd_truth):\n\n    images = images.squeeze()\n    assert len(images) == len(gd_truth) == 9\n\n    # Create figure with sub-plots.\n    fig, axes = plt.subplots(3, 3)\n\n    for i, ax in enumerate(axes.flat):\n        # plot the image\n        ax.imshow(images[i], cmap=""Greys_r"")\n\n        xlabel = ""{}"".format(gd_truth[i])\n        ax.set_xlabel(xlabel)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    plt.show()\n\n\ndef prepare_dirs(config):\n    for path in [config.data_dir, config.ckpt_dir, config.logs_dir]:\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n\ndef save_config(config):\n    model_name = ""ram_{}_{}x{}_{}"".format(\n        config.num_glimpses, config.patch_size, config.patch_size, config.glimpse_scale\n    )\n    filename = model_name + ""_params.json""\n    param_path = os.path.join(config.ckpt_dir, filename)\n\n    print(""[*] Model Checkpoint Dir: {}"".format(config.ckpt_dir))\n    print(""[*] Param Path: {}"".format(param_path))\n\n    with open(param_path, ""w"") as fp:\n        json.dump(config.__dict__, fp, indent=4, sort_keys=True)\n'"
tests/test_model.py,4,"b'import sys\nsys.path.append("".."")\n\nimport torch\n\nimport model\nimport utils\n\n\nif __name__ == ""__main__"":\n    # paths\n    plot_dir = ""../plots/""\n    data_dir = ""../data/""\n\n    # load images\n    imgs = []\n    paths = [data_dir + ""./lenna.jpg"", data_dir + ""./cat.jpg""]\n    for i in range(len(paths)):\n        img = utils.img2array(paths[i], desired_size=[512, 512], expand=True)\n        imgs.append(torch.from_numpy(img))\n    imgs = torch.cat(imgs).permute((0, 3, 1, 2))\n\n    B, C, H, W = imgs.shape\n    l_t_prev = torch.FloatTensor(B, 2).uniform_(-1, 1)\n    h_t_prev = torch.zeros(B, 256)\n\n    ram = model.RecurrentAttention(64, 3, 2, C, 128, 128, 0.11, 256, 10)\n    h_t, l_t, _, _ = ram(imgs, l_t_prev, h_t_prev)\n\n    assert h_t.shape == (B, 256)\n    assert l_t.shape == (B, 2)\n'"
tests/test_modules.py,4,"b'import sys\nsys.path.append("".."")\n\nimport torch\n\nimport modules\nimport utils\n\n\nif __name__ == ""__main__"":\n    # paths\n    plot_dir = ""../plots/""\n    data_dir = ""../data/""\n\n    # load images\n    imgs = []\n    paths = [data_dir + ""./lenna.jpg"", data_dir + ""./cat.jpg""]\n    for i in range(len(paths)):\n        img = utils.img2array(paths[i], desired_size=[512, 512], expand=True)\n        imgs.append(torch.from_numpy(img))\n    imgs = torch.cat(imgs).permute((0, 3, 1, 2))\n    B, C, H, W = imgs.shape\n\n    loc = torch.Tensor([[-1.0, 1.0], [-1.0, 1.0]])\n    sensor = modules.GlimpseNetwork(h_g=128, h_l=128, g=64, k=3, s=2, c=3)\n    g_t = sensor(imgs, loc)\n    assert g_t.shape == (B, 256)\n\n    rnn = modules.CoreNetwork(input_size=256, hidden_size=256)\n    h_t = torch.zeros(g_t.shape[0], 256)\n    h_t = rnn(g_t, h_t)\n    assert h_t.shape == (B, 256)\n\n    classifier = modules.ActionNetwork(256, 10)\n    a_t = classifier(h_t)\n    assert a_t.shape == (B, 10)\n\n    loc_net = modules.LocationNetwork(256, 2, 0.11)\n    mu, l_t = loc_net(h_t)\n    assert l_t.shape == (B, 2)\n\n    base = modules.BaselineNetwork(256, 1)\n    b_t = base(h_t)\n    assert b_t.shape == (B, 1)\n'"
tests/test_retina.py,4,"b'import sys\nsys.path.append("".."")\n\nfrom functools import reduce\n\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom PIL import Image\n\nfrom modules import Retina\nfrom utils import array2img, img2array\n\n\n\ndef bounding_box(x, y, size, color=""w""):\n    x = int(x - (size / 2))\n    y = int(y - (size / 2))\n    rect = patches.Rectangle(\n        (x, y), size, size, linewidth=1, edgecolor=color, fill=False\n    )\n    return rect\n\n\n# https://stackoverflow.com/questions/10657383/stitching-photos-together\ndef merge_images(image1, image2):\n    """"""Merge two images into one, displayed side by side.\n    """"""\n    (width1, height1) = image1.size\n    (width2, height2) = image2.size\n\n    result_width = width1 + width2\n    result_height = max(height1, height2)\n\n    result = Image.new(""RGB"", (result_width, result_height))\n    result.paste(im=image1, box=(0, 0))\n    result.paste(im=image2, box=(width1, 0))\n    return result\n\n\ndef main():\n    # paths\n    data_dir = ""../data/""\n\n    # load images\n    imgs = []\n    paths = [data_dir + ""./lenna.jpg"", data_dir + ""./cat.jpg""]\n    for i in range(len(paths)):\n        img = img2array(paths[i], desired_size=[512, 512], expand=True)\n        imgs.append(torch.from_numpy(img))\n    imgs = torch.cat(imgs).permute(0, 3, 1, 2)\n\n    # loc = torch.Tensor(2, 2).uniform_(-1, 1)\n    loc = torch.from_numpy(np.array([[0.0, 0.0], [0.0, 0.0]]))\n\n    num_patches = 5\n    scale = 2\n    patch_size = 10\n\n    ret = Retina(g=patch_size, k=num_patches, s=scale)\n    glimpse = ret.foveate(imgs, loc).data.numpy()\n\n    glimpse = np.reshape(glimpse, [2, num_patches, 3, patch_size, patch_size])\n    glimpse = np.transpose(glimpse, [0, 1, 3, 4, 2])\n\n    merged = []\n    for i in range(len(glimpse)):\n        g = glimpse[i]\n        g = list(g)\n        g = [array2img(l) for l in g]\n        res = reduce(merge_images, list(g))\n        merged.append(res)\n\n    merged = [np.asarray(l, dtype=""float32"") / 255.0 for l in merged]\n\n    fig, axs = plt.subplots(nrows=2, ncols=1)\n    for i, ax in enumerate(axs.flat):\n        axs[i].imshow(merged[i])\n        axs[i].get_xaxis().set_visible(False)\n        axs[i].get_yaxis().set_visible(False)\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
