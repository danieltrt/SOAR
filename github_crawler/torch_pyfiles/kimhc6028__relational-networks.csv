file_path,api_count,code
main.py,12,"b'""""""""""""""""""\nPytorch implementation of ""A simple neural network module for relational reasoning\nCode is based on pytorch/examples/mnist (https://github.com/pytorch/examples/tree/master/mnist)\n""""""""""""""""""\nfrom __future__ import print_function\nimport argparse\nimport os\n#import cPickle as pickle\nimport pickle\nimport random\nimport numpy as np\nimport csv\n\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.autograd import Variable\n\nfrom model import RN, CNN_MLP\n\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch Relational-Network sort-of-CLVR Example\')\nparser.add_argument(\'--model\', type=str, choices=[\'RN\', \'CNN_MLP\'], default=\'RN\', \n                    help=\'resume from model stored\')\nparser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\n                    help=\'input batch size for training (default: 64)\')\nparser.add_argument(\'--epochs\', type=int, default=20, metavar=\'N\',\n                    help=\'number of epochs to train (default: 20)\')\nparser.add_argument(\'--lr\', type=float, default=0.0001, metavar=\'LR\',\n                    help=\'learning rate (default: 0.0001)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nparser.add_argument(\'--resume\', type=str,\n                    help=\'resume from model stored\')\nparser.add_argument(\'--relation-type\', type=str, default=\'binary\',\n                    help=\'what kind of relations to learn. options: binary, ternary (default: binary)\')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\nsummary_writer = SummaryWriter()\n\nif args.model==\'CNN_MLP\': \n  model = CNN_MLP(args)\nelse:\n  model = RN(args)\n  \nmodel_dirs = \'./model\'\nbs = args.batch_size\ninput_img = torch.FloatTensor(bs, 3, 75, 75)\ninput_qst = torch.FloatTensor(bs, 18)\nlabel = torch.LongTensor(bs)\n\nif args.cuda:\n    model.cuda()\n    input_img = input_img.cuda()\n    input_qst = input_qst.cuda()\n    label = label.cuda()\n\ninput_img = Variable(input_img)\ninput_qst = Variable(input_qst)\nlabel = Variable(label)\n\ndef tensor_data(data, i):\n    img = torch.from_numpy(np.asarray(data[0][bs*i:bs*(i+1)]))\n    qst = torch.from_numpy(np.asarray(data[1][bs*i:bs*(i+1)]))\n    ans = torch.from_numpy(np.asarray(data[2][bs*i:bs*(i+1)]))\n\n    input_img.data.resize_(img.size()).copy_(img)\n    input_qst.data.resize_(qst.size()).copy_(qst)\n    label.data.resize_(ans.size()).copy_(ans)\n\n\ndef cvt_data_axis(data):\n    img = [e[0] for e in data]\n    qst = [e[1] for e in data]\n    ans = [e[2] for e in data]\n    return (img,qst,ans)\n\n    \ndef train(epoch, ternary, rel, norel):\n    model.train()\n\n    if not len(rel[0]) == len(norel[0]):\n        print(\'Not equal length for relation dataset and non-relation dataset.\')\n        return\n    \n    random.shuffle(ternary)\n    random.shuffle(rel)\n    random.shuffle(norel)\n\n    ternary = cvt_data_axis(ternary)\n    rel = cvt_data_axis(rel)\n    norel = cvt_data_axis(norel)\n\n    acc_ternary = []\n    acc_rels = []\n    acc_norels = []\n\n    l_ternary = []\n    l_binary = []\n    l_unary = []\n\n    for batch_idx in range(len(rel[0]) // bs):\n        tensor_data(ternary, batch_idx)\n        accuracy_ternary, loss_ternary = model.train_(input_img, input_qst, label)\n        acc_ternary.append(accuracy_ternary.item())\n        l_ternary.append(loss_ternary.item())\n\n        tensor_data(rel, batch_idx)\n        accuracy_rel, loss_binary = model.train_(input_img, input_qst, label)\n        acc_rels.append(accuracy_rel.item())\n        l_binary.append(loss_binary.item())\n\n        tensor_data(norel, batch_idx)\n        accuracy_norel, loss_unary = model.train_(input_img, input_qst, label)\n        acc_norels.append(accuracy_norel.item())\n        l_unary.append(loss_unary.item())\n\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)] \'\n                  \'Ternary accuracy: {:.0f}% | Relations accuracy: {:.0f}% | Non-relations accuracy: {:.0f}%\'.format(\n                   epoch,\n                   batch_idx * bs * 2,\n                   len(rel[0]) * 2,\n                   100. * batch_idx * bs / len(rel[0]),\n                   accuracy_ternary,\n                   accuracy_rel,\n                   accuracy_norel))\n        \n    avg_acc_ternary = sum(acc_ternary) / len(acc_ternary)\n    avg_acc_binary = sum(acc_rels) / len(acc_rels)\n    avg_acc_unary = sum(acc_norels) / len(acc_norels)\n\n    summary_writer.add_scalars(\'Accuracy/train\', {\n        \'ternary\': avg_acc_ternary,\n        \'binary\': avg_acc_binary,\n        \'unary\': avg_acc_unary\n    }, epoch)\n\n    avg_loss_ternary = sum(l_ternary) / len(l_ternary)\n    avg_loss_binary = sum(l_binary) / len(l_binary)\n    avg_loss_unary = sum(l_unary) / len(l_unary)\n\n    summary_writer.add_scalars(\'Loss/train\', {\n        \'ternary\': avg_loss_ternary,\n        \'binary\': avg_loss_binary,\n        \'unary\': avg_loss_unary\n    }, epoch)\n\n    # return average accuracy\n    return avg_acc_ternary, avg_acc_binary, avg_acc_unary\n\ndef test(epoch, ternary, rel, norel):\n    model.eval()\n    if not len(rel[0]) == len(norel[0]):\n        print(\'Not equal length for relation dataset and non-relation dataset.\')\n        return\n    \n    ternary = cvt_data_axis(ternary)\n    rel = cvt_data_axis(rel)\n    norel = cvt_data_axis(norel)\n\n    accuracy_ternary = []\n    accuracy_rels = []\n    accuracy_norels = []\n\n    loss_ternary = []\n    loss_binary = []\n    loss_unary = []\n\n    for batch_idx in range(len(rel[0]) // bs):\n        tensor_data(ternary, batch_idx)\n        acc_ter, l_ter = model.test_(input_img, input_qst, label)\n        accuracy_ternary.append(acc_ter.item())\n        loss_ternary.append(l_ter.item())\n\n        tensor_data(rel, batch_idx)\n        acc_bin, l_bin = model.test_(input_img, input_qst, label)\n        accuracy_rels.append(acc_bin.item())\n        loss_binary.append(l_bin.item())\n\n        tensor_data(norel, batch_idx)\n        acc_un, l_un = model.test_(input_img, input_qst, label)\n        accuracy_norels.append(acc_un.item())\n        loss_unary.append(l_un.item())\n\n    accuracy_ternary = sum(accuracy_ternary) / len(accuracy_ternary)\n    accuracy_rel = sum(accuracy_rels) / len(accuracy_rels)\n    accuracy_norel = sum(accuracy_norels) / len(accuracy_norels)\n    print(\'\\n Test set: Ternary accuracy: {:.0f}% Binary accuracy: {:.0f}% | Unary accuracy: {:.0f}%\\n\'.format(\n        accuracy_ternary, accuracy_rel, accuracy_norel))\n\n    summary_writer.add_scalars(\'Accuracy/test\', {\n        \'ternary\': accuracy_ternary,\n        \'binary\': accuracy_rel,\n        \'unary\': accuracy_norel\n    }, epoch)\n\n    loss_ternary = sum(loss_ternary) / len(loss_ternary)\n    loss_binary = sum(loss_binary) / len(loss_binary)\n    loss_unary = sum(loss_unary) / len(loss_unary)\n\n    summary_writer.add_scalars(\'Loss/test\', {\n        \'ternary\': loss_ternary,\n        \'binary\': loss_binary,\n        \'unary\': loss_unary\n    }, epoch)\n\n    return accuracy_ternary, accuracy_rel, accuracy_norel\n\n    \ndef load_data():\n    print(\'loading data...\')\n    dirs = \'./data\'\n    filename = os.path.join(dirs,\'sort-of-clevr.pickle\')\n    with open(filename, \'rb\') as f:\n      train_datasets, test_datasets = pickle.load(f)\n    ternary_train = []\n    ternary_test = []\n    rel_train = []\n    rel_test = []\n    norel_train = []\n    norel_test = []\n    print(\'processing data...\')\n\n    for img, ternary, relations, norelations in train_datasets:\n        img = np.swapaxes(img, 0, 2)\n        for qst, ans in zip(ternary[0], ternary[1]):\n            ternary_train.append((img,qst,ans))\n        for qst,ans in zip(relations[0], relations[1]):\n            rel_train.append((img,qst,ans))\n        for qst,ans in zip(norelations[0], norelations[1]):\n            norel_train.append((img,qst,ans))\n\n    for img, ternary, relations, norelations in test_datasets:\n        img = np.swapaxes(img, 0, 2)\n        for qst, ans in zip(ternary[0], ternary[1]):\n            ternary_test.append((img, qst, ans))\n        for qst,ans in zip(relations[0], relations[1]):\n            rel_test.append((img,qst,ans))\n        for qst,ans in zip(norelations[0], norelations[1]):\n            norel_test.append((img,qst,ans))\n    \n    return (ternary_train, ternary_test, rel_train, rel_test, norel_train, norel_test)\n    \n\nternary_train, ternary_test, rel_train, rel_test, norel_train, norel_test = load_data()\n\ntry:\n    os.makedirs(model_dirs)\nexcept:\n    print(\'directory {} already exists\'.format(model_dirs))\n\nif args.resume:\n    filename = os.path.join(model_dirs, args.resume)\n    if os.path.isfile(filename):\n        print(\'==> loading checkpoint {}\'.format(filename))\n        checkpoint = torch.load(filename)\n        model.load_state_dict(checkpoint)\n        print(\'==> loaded checkpoint {}\'.format(filename))\n\nwith open(f\'./{args.model}_{args.seed}_log.csv\', \'w\') as log_file:\n    csv_writer = csv.writer(log_file, delimiter=\',\')\n    csv_writer.writerow([\'epoch\', \'train_acc_ternary\', \'train_acc_rel\',\n                     \'train_acc_norel\', \'train_acc_ternary\', \'test_acc_rel\', \'test_acc_norel\'])\n\n    print(f""Training {args.model} {f\'({args.relation_type})\' if args.model == \'RN\' else \'\'} model..."")\n\n    for epoch in range(1, args.epochs + 1):\n        train_acc_ternary, train_acc_binary, train_acc_unary = train(\n            epoch, ternary_train, rel_train, norel_train)\n        test_acc_ternary, test_acc_binary, test_acc_unary = test(\n            epoch, ternary_test, rel_test, norel_test)\n\n        csv_writer.writerow([epoch, train_acc_ternary, train_acc_binary,\n                         train_acc_unary, test_acc_ternary, test_acc_binary, test_acc_unary])\n        model.save_model(epoch)\n'"
model.py,28,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\n\nclass ConvInputModel(nn.Module):\n    def __init__(self):\n        super(ConvInputModel, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 24, 3, stride=2, padding=1)\n        self.batchNorm1 = nn.BatchNorm2d(24)\n        self.conv2 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n        self.batchNorm2 = nn.BatchNorm2d(24)\n        self.conv3 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n        self.batchNorm3 = nn.BatchNorm2d(24)\n        self.conv4 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n        self.batchNorm4 = nn.BatchNorm2d(24)\n\n        \n    def forward(self, img):\n        """"""convolution""""""\n        x = self.conv1(img)\n        x = F.relu(x)\n        x = self.batchNorm1(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.batchNorm2(x)\n        x = self.conv3(x)\n        x = F.relu(x)\n        x = self.batchNorm3(x)\n        x = self.conv4(x)\n        x = F.relu(x)\n        x = self.batchNorm4(x)\n        return x\n\n  \nclass FCOutputModel(nn.Module):\n    def __init__(self):\n        super(FCOutputModel, self).__init__()\n\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 10)\n\n    def forward(self, x):\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = F.dropout(x)\n        x = self.fc3(x)\n        return F.log_softmax(x, dim=1)\n\nclass BasicModel(nn.Module):\n    def __init__(self, args, name):\n        super(BasicModel, self).__init__()\n        self.name=name\n\n    def train_(self, input_img, input_qst, label):\n        self.optimizer.zero_grad()\n        output = self(input_img, input_qst)\n        loss = F.nll_loss(output, label)\n        loss.backward()\n        self.optimizer.step()\n        pred = output.data.max(1)[1]\n        correct = pred.eq(label.data).cpu().sum()\n        accuracy = correct * 100. / len(label)\n        return accuracy, loss\n        \n    def test_(self, input_img, input_qst, label):\n        output = self(input_img, input_qst)\n        loss = F.nll_loss(output, label)\n        pred = output.data.max(1)[1]\n        correct = pred.eq(label.data).cpu().sum()\n        accuracy = correct * 100. / len(label)\n        return accuracy, loss\n\n    def save_model(self, epoch):\n        torch.save(self.state_dict(), \'model/epoch_{}_{:02d}.pth\'.format(self.name, epoch))\n\n\nclass RN(BasicModel):\n    def __init__(self, args):\n        super(RN, self).__init__(args, \'RN\')\n        \n        self.conv = ConvInputModel()\n        \n        self.relation_type = args.relation_type\n        \n        if self.relation_type == \'ternary\':\n            ##(number of filters per object+coordinate of object)*3+question vector\n            self.g_fc1 = nn.Linear((24+2)*3+18, 256)\n        else:\n            ##(number of filters per object+coordinate of object)*2+question vector\n            self.g_fc1 = nn.Linear((24+2)*2+18, 256)\n\n        self.g_fc2 = nn.Linear(256, 256)\n        self.g_fc3 = nn.Linear(256, 256)\n        self.g_fc4 = nn.Linear(256, 256)\n\n        self.f_fc1 = nn.Linear(256, 256)\n\n        self.coord_oi = torch.FloatTensor(args.batch_size, 2)\n        self.coord_oj = torch.FloatTensor(args.batch_size, 2)\n        if args.cuda:\n            self.coord_oi = self.coord_oi.cuda()\n            self.coord_oj = self.coord_oj.cuda()\n        self.coord_oi = Variable(self.coord_oi)\n        self.coord_oj = Variable(self.coord_oj)\n\n        # prepare coord tensor\n        def cvt_coord(i):\n            return [(i/5-2)/2., (i%5-2)/2.]\n        \n        self.coord_tensor = torch.FloatTensor(args.batch_size, 25, 2)\n        if args.cuda:\n            self.coord_tensor = self.coord_tensor.cuda()\n        self.coord_tensor = Variable(self.coord_tensor)\n        np_coord_tensor = np.zeros((args.batch_size, 25, 2))\n        for i in range(25):\n            np_coord_tensor[:,i,:] = np.array( cvt_coord(i) )\n        self.coord_tensor.data.copy_(torch.from_numpy(np_coord_tensor))\n\n\n        self.fcout = FCOutputModel()\n        \n        self.optimizer = optim.Adam(self.parameters(), lr=args.lr)\n\n\n    def forward(self, img, qst):\n        x = self.conv(img) ## x = (64 x 24 x 5 x 5)\n        \n        """"""g""""""\n        mb = x.size()[0]\n        n_channels = x.size()[1]\n        d = x.size()[2]\n        # x_flat = (64 x 25 x 24)\n        x_flat = x.view(mb,n_channels,d*d).permute(0,2,1)\n        \n        # add coordinates\n        x_flat = torch.cat([x_flat, self.coord_tensor],2)\n        \n\n        if self.relation_type == \'ternary\':\n            # add question everywhere\n            qst = torch.unsqueeze(qst, 1) # (64x1x18)\n            qst = qst.repeat(1, 25, 1) # (64x25x18)\n            qst = torch.unsqueeze(qst, 1)  # (64x1x25x18)\n            qst = torch.unsqueeze(qst, 1)  # (64x1x1x25x18)\n\n            # cast all triples against each other\n            x_i = torch.unsqueeze(x_flat, 1)  # (64x1x25x26)\n            x_i = torch.unsqueeze(x_i, 3)  # (64x1x25x1x26)\n            x_i = x_i.repeat(1, 25, 1, 25, 1)  # (64x25x25x25x26)\n            \n            x_j = torch.unsqueeze(x_flat, 2)  # (64x25x1x26)\n            x_j = torch.unsqueeze(x_j, 2)  # (64x25x1x1x26)\n            x_j = x_j.repeat(1, 1, 25, 25, 1)  # (64x25x25x25x26)\n\n            x_k = torch.unsqueeze(x_flat, 1)  # (64x1x25x26)\n            x_k = torch.unsqueeze(x_k, 1)  # (64x1x1x25x26)\n            x_k = torch.cat([x_k, qst], 4)  # (64x1x1x25x26+18)\n            x_k = x_k.repeat(1, 25, 25, 1, 1)  # (64x25x25x25x26+18)\n\n            # concatenate all together\n            x_full = torch.cat([x_i, x_j, x_k], 4)  # (64x25x25x25x3*26+18)\n\n            # reshape for passing through network\n            x_ = x_full.view(mb * (d * d) * (d * d) * (d * d), 96)  # (64*25*25*25x3*26+18) = (1.000.000, 96)\n        else:\n            # add question everywhere\n            qst = torch.unsqueeze(qst, 1)\n            qst = qst.repeat(1, 25, 1)\n            qst = torch.unsqueeze(qst, 2)\n\n            # cast all pairs against each other\n            x_i = torch.unsqueeze(x_flat, 1)  # (64x1x25x26+18)\n            x_i = x_i.repeat(1, 25, 1, 1)  # (64x25x25x26+18)\n            x_j = torch.unsqueeze(x_flat, 2)  # (64x25x1x26+18)\n            x_j = torch.cat([x_j, qst], 3)\n            x_j = x_j.repeat(1, 1, 25, 1)  # (64x25x25x26+18)\n            \n            # concatenate all together\n            x_full = torch.cat([x_i,x_j],3) # (64x25x25x2*26+18)\n        \n            # reshape for passing through network\n            x_ = x_full.view(mb * (d * d) * (d * d), 70)  # (64*25*25x2*26*18) = (40.000, 70)\n            \n        x_ = self.g_fc1(x_)\n        x_ = F.relu(x_)\n        x_ = self.g_fc2(x_)\n        x_ = F.relu(x_)\n        x_ = self.g_fc3(x_)\n        x_ = F.relu(x_)\n        x_ = self.g_fc4(x_)\n        x_ = F.relu(x_)\n        \n        # reshape again and sum\n        if self.relation_type == \'ternary\':\n            x_g = x_.view(mb, (d * d) * (d * d) * (d * d), 256)\n        else:\n            x_g = x_.view(mb, (d * d) * (d * d), 256)\n\n        x_g = x_g.sum(1).squeeze()\n        \n        """"""f""""""\n        x_f = self.f_fc1(x_g)\n        x_f = F.relu(x_f)\n        \n        return self.fcout(x_f)\n\n\nclass CNN_MLP(BasicModel):\n    def __init__(self, args):\n        super(CNN_MLP, self).__init__(args, \'CNNMLP\')\n\n        self.conv  = ConvInputModel()\n        self.fc1   = nn.Linear(5*5*24 + 18, 256)  # question concatenated to all\n        self.fcout = FCOutputModel()\n\n        self.optimizer = optim.Adam(self.parameters(), lr=args.lr)\n        #print([ a for a in self.parameters() ] )\n  \n    def forward(self, img, qst):\n        x = self.conv(img) ## x = (64 x 24 x 5 x 5)\n\n        """"""fully connected layers""""""\n        x = x.view(x.size(0), -1)\n        \n        x_ = torch.cat((x, qst), 1)  # Concat question\n        \n        x_ = self.fc1(x_)\n        x_ = F.relu(x_)\n        \n        return self.fcout(x_)\n\n'"
sort_of_clevr_generator.py,0,"b'import cv2\nimport os\nimport numpy as np\nimport random\n#import cPickle as pickle\nimport pickle\nimport warnings\nimport argparse\n\nparser = argparse.ArgumentParser(description=\'Sort-of-CLEVR dataset generator\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--t-subtype\', type=int, default=-1,\n                    help=\'Force ternary questions to be of a given type\')\nargs = parser.parse_args()\n\nrandom.seed(args.seed)\nnp.random.seed(args.seed)\n\ntrain_size = 9800\ntest_size = 200\nimg_size = 75\nsize = 5\nquestion_size = 18  ## 2 x (6 for one-hot vector of color), 3 for question type, 3 for question subtype\nq_type_idx = 12\nsub_q_type_idx = 15\n""""""Answer : [yes, no, rectangle, circle, r, g, b, o, k, y]""""""\n\nnb_questions = 10\ndirs = \'./data\'\n\ncolors = [\n    (0,0,255),##r\n    (0,255,0),##g\n    (255,0,0),##b\n    (0,156,255),##o\n    (128,128,128),##k\n    (0,255,255)##y\n]\n\n\ntry:\n    os.makedirs(dirs)\nexcept:\n    print(\'directory {} already exists\'.format(dirs))\n\ndef center_generate(objects):\n    while True:\n        pas = True\n        center = np.random.randint(0+size, img_size - size, 2)        \n        if len(objects) > 0:\n            for name,c,shape in objects:\n                if ((center - c) ** 2).sum() < ((size * 2) ** 2):\n                    pas = False\n        if pas:\n            return center\n\n\n\ndef build_dataset():\n    objects = []\n    img = np.ones((img_size,img_size,3)) * 255\n    for color_id,color in enumerate(colors):  \n        center = center_generate(objects)\n        if random.random()<0.5:\n            start = (center[0]-size, center[1]-size)\n            end = (center[0]+size, center[1]+size)\n            cv2.rectangle(img, start, end, color, -1)\n            objects.append((color_id,center,\'r\'))\n        else:\n            center_ = (center[0], center[1])\n            cv2.circle(img, center_, size, color, -1)\n            objects.append((color_id,center,\'c\'))\n\n\n    ternary_questions = []\n    binary_questions = []\n    norel_questions = []\n    ternary_answers = []\n    binary_answers = []\n    norel_answers = []\n    """"""Non-relational questions""""""\n    for _ in range(nb_questions):\n        question = np.zeros((question_size))\n        color = random.randint(0,5)\n        question[color] = 1\n        question[q_type_idx] = 1\n        subtype = random.randint(0,2)\n        question[subtype+sub_q_type_idx] = 1\n        norel_questions.append(question)\n        """"""Answer : [yes, no, rectangle, circle, r, g, b, o, k, y]""""""\n        if subtype == 0:\n            """"""query shape->rectangle/circle""""""\n            if objects[color][2] == \'r\':\n                answer = 2\n            else:\n                answer = 3\n\n        elif subtype == 1:\n            """"""query horizontal position->yes/no""""""\n            if objects[color][1][0] < img_size / 2:\n                answer = 0\n            else:\n                answer = 1\n\n        elif subtype == 2:\n            """"""query vertical position->yes/no""""""\n            if objects[color][1][1] < img_size / 2:\n                answer = 0\n            else:\n                answer = 1\n        norel_answers.append(answer)\n    \n    """"""Binary Relational questions""""""\n    for _ in range(nb_questions):\n        question = np.zeros((question_size))\n        color = random.randint(0,5)\n        question[color] = 1\n        question[q_type_idx+1] = 1\n        subtype = random.randint(0,2)\n        question[subtype+sub_q_type_idx] = 1\n        binary_questions.append(question)\n\n        if subtype == 0:\n            """"""closest-to->rectangle/circle""""""\n            my_obj = objects[color][1]\n            dist_list = [((my_obj - obj[1]) ** 2).sum() for obj in objects]\n            dist_list[dist_list.index(0)] = 999\n            closest = dist_list.index(min(dist_list))\n            if objects[closest][2] == \'r\':\n                answer = 2\n            else:\n                answer = 3\n                \n        elif subtype == 1:\n            """"""furthest-from->rectangle/circle""""""\n            my_obj = objects[color][1]\n            dist_list = [((my_obj - obj[1]) ** 2).sum() for obj in objects]\n            furthest = dist_list.index(max(dist_list))\n            if objects[furthest][2] == \'r\':\n                answer = 2\n            else:\n                answer = 3\n\n        elif subtype == 2:\n            """"""count->1~6""""""\n            my_obj = objects[color][2]\n            count = -1\n            for obj in objects:\n                if obj[2] == my_obj:\n                    count +=1 \n            answer = count+4\n\n        binary_answers.append(answer)\n\n    """"""Ternary Relational questions""""""\n    for _ in range(nb_questions):\n        question = np.zeros((question_size))\n        rnd_colors = np.random.permutation(np.arange(5))\n        # 1st object\n        color1 = rnd_colors[0]\n        question[color1] = 1\n        # 2nd object\n        color2 = rnd_colors[1]\n        question[6 + color2] = 1\n\n        question[q_type_idx + 2] = 1\n        \n        if args.t_subtype >= 0 and args.t_subtype < 3:\n            subtype = args.t_subtype\n        else:\n            subtype = random.randint(0, 2)\n\n        question[subtype+sub_q_type_idx] = 1\n        ternary_questions.append(question)\n\n        # get coordiantes of object from question\n        A = objects[color1][1]\n        B = objects[color2][1]\n\n        if subtype == 0:\n            """"""between->1~4""""""\n\n            between_count = 0 \n            # check is any objects lies inside the box\n            for other_obj in objects:\n                # skip object A and B\n                if (other_obj[0] == color1) or (other_obj[0] == color2):\n                    continue\n\n                # Get x and y coordinate of third object\n                other_objx = other_obj[1][0]\n                other_objy = other_obj[1][1]\n\n                if (A[0] <= other_objx <= B[0] and A[1] <= other_objy <= B[1]) or \\\n                   (A[0] <= other_objx <= B[0] and B[1] <= other_objy <= A[1]) or \\\n                   (B[0] <= other_objx <= A[0] and B[1] <= other_objy <= A[1]) or \\\n                   (B[0] <= other_objx <= A[0] and A[1] <= other_objy <= B[1]):\n                    between_count += 1\n\n            answer = between_count + 4\n        elif subtype == 1:\n            """"""is-on-band->yes/no""""""\n            \n            grace_threshold = 12  # half of the size of objects\n            epsilon = 1e-10  \n            m = (B[1]-A[1])/((B[0]-A[0]) + epsilon ) # add epsilon to prevent dividing by zero\n            c = A[1] - (m*A[0])\n\n            answer = 1  # default answer is \'no\'\n\n            # check if any object lies on/close the line between object A and object B\n            for other_obj in objects:\n                # skip object A and B\n                if (other_obj[0] == color1) or (other_obj[0] == color2):\n                    continue\n\n                other_obj_pos = other_obj[1]\n                \n                # y = mx + c\n                y = (m*other_obj_pos[0]) + c\n                if (y - grace_threshold)  <= other_obj_pos[1] <= (y + grace_threshold):\n                    answer = 0\n        elif subtype == 2:\n            """"""count-obtuse-triangles->1~6""""""\n\n            obtuse_count = 0\n\n            # disable warnings\n            # the angle computation may fail if the points are on a line\n            warnings.filterwarnings(""ignore"")\n            for other_obj in objects:\n                # skip object A and B\n                if (other_obj[0] == color1) or (other_obj[0] == color2):\n                    continue\n\n                # get position of 3rd object\n                C = other_obj[1]\n                # edge length\n                a = np.linalg.norm(B - C)\n                b = np.linalg.norm(C - A)\n                c = np.linalg.norm(A - B)\n                # angles by law of cosine\n                alpha = np.rad2deg(np.arccos((b ** 2 + c ** 2 - a ** 2) / (2 * b * c)))\n                beta = np.rad2deg(np.arccos((a ** 2 + c ** 2 - b ** 2) / (2 * a * c)))\n                gamma = np.rad2deg(np.arccos((a ** 2 + b ** 2 - c ** 2) / (2 * a * b)))\n                max_angle = max(alpha, beta, gamma)\n                if max_angle >= 90 and max_angle < 180:\n                    obtuse_count += 1\n\n            warnings.filterwarnings(""default"")\n            answer = obtuse_count + 4\n\n        ternary_answers.append(answer)\n\n    ternary_relations = (ternary_questions, ternary_answers)\n    binary_relations = (binary_questions, binary_answers)\n    norelations = (norel_questions, norel_answers)\n    \n    img = img/255.\n    dataset = (img, ternary_relations, binary_relations, norelations)\n    return dataset\n\n\nprint(\'building test datasets...\')\ntest_datasets = [build_dataset() for _ in range(test_size)]\nprint(\'building train datasets...\')\ntrain_datasets = [build_dataset() for _ in range(train_size)]\n\n\n#img_count = 0\n#cv2.imwrite(os.path.join(dirs,\'{}.png\'.format(img_count)), cv2.resize(train_datasets[0][0]*255, (512,512)))\n\n\nprint(\'saving datasets...\')\nfilename = os.path.join(dirs,\'sort-of-clevr.pickle\')\nwith  open(filename, \'wb\') as f:\n    pickle.dump((train_datasets, test_datasets), f)\nprint(\'datasets saved at {}\'.format(filename))\n'"
translator.py,0,"b""import cv2\ndef translate(dataset):\n    img, (rel_questions, rel_answers), (norel_questions, norel_answers) = dataset\n    colors = ['red ', 'green ', 'blue ', 'orange ', 'gray ', 'yellow ']\n    answer_sheet = ['yes', 'no', 'rectangle', 'circle', '1', '2', '3', '4', '5', '6']\n    questions = rel_questions + norel_questions\n    answers = rel_answers + norel_answers\n\n    print rel_questions\n    print rel_answers\n\n\n    for question,answer in zip(questions,answers):\n        query = ''\n        query += colors[question.tolist()[0:6].index(1)]\n\n        if question[6] == 1:\n            if question[8] == 1:\n                query += 'shape?'\n            if question[9] == 1:\n                query += 'left?'\n            if question[10] == 1:\n                query += 'up?'\n        if question[7] == 1:\n            if question[8] == 1:\n                query += 'closest shape?'\n            if question[9] == 1:\n                query += 'furthest shape?'\n            if question[10] == 1:\n                query += 'count?'\n\n        ans = answer_sheet[answer]\n        print query,'==>', ans\n    #cv2.imwrite('sample.jpg',(img*255).astype(np.int32))\n    cv2.imshow('img',cv2.resize(img,(512,512)))\n    cv2.waitKey(0)\n"""
