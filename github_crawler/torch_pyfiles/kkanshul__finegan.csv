file_path,api_count,code
code/datasets.py,3,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nimport sys\n\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport PIL\nimport os\nimport os.path\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nfrom miscc.config import cfg\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport os\nimport os.path\nimport six\nimport string\nimport sys\nimport torch\nfrom copy import deepcopy\nif sys.version_info[0] == 2:\n    import cPickle as pickle\nelse:\n    import pickle\n\nIMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG',\n                  '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP']\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef get_imgs(img_path, imsize, bbox=None,\n             transform=None, normalize=None):\n    img = Image.open(img_path).convert('RGB')\n    width, height = img.size\n    if bbox is not None:\n        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n        y1 = np.maximum(0, center_y - r)\n        y2 = np.minimum(height, center_y + r)\n        x1 = np.maximum(0, center_x - r)\n        x2 = np.minimum(width, center_x + r)\n\tfimg = deepcopy(img)\n\tfimg_arr = np.array(fimg)\n\tfimg = Image.fromarray(fimg_arr)\n        cimg = img.crop([x1, y1, x2, y2])\n\n    if transform is not None:\n\tcimg = transform(cimg)\n\n    \n    retf = []\n    retc = []\n    re_cimg = transforms.Scale(imsize[1])(cimg)\n    retc.append(normalize(re_cimg))\n\n    # We use full image to get background patches\n\n    # We resize the full image to be 126 X 126 (instead of 128 X 128)  for the full coverage of the input (full) image by \n    # the receptive fields of the final convolution layer of background discriminator\n    \n    my_crop_width = 126\t\n    re_fimg = transforms.Scale(int(my_crop_width * 76 / 64))(fimg)\n    re_width, re_height = re_fimg.size\n\n    # random cropping\n    x_crop_range = re_width-my_crop_width\n    y_crop_range = re_height-my_crop_width\n\n    crop_start_x = np.random.randint(x_crop_range)\t\n    crop_start_y = np.random.randint(y_crop_range)\n\n    crop_re_fimg = re_fimg.crop([crop_start_x, crop_start_y, crop_start_x + my_crop_width, crop_start_y + my_crop_width])\n    warped_x1 = bbox[0] * re_width / width\n    warped_y1 = bbox[1] * re_height / height\n    warped_x2 = warped_x1 + (bbox[2] * re_width / width)\n    warped_y2 = warped_y1 + (bbox[3] * re_height / height)\n\n    warped_x1 =min(max(0, warped_x1 - crop_start_x), my_crop_width)\n    warped_y1 =min(max(0, warped_y1 - crop_start_y), my_crop_width)\n    warped_x2 =max(min(my_crop_width, warped_x2 - crop_start_x),0)\n    warped_y2 =max(min(my_crop_width, warped_y2 - crop_start_y),0)\n\n    # random flipping\n    random_flag=np.random.randint(2)\n    if(random_flag == 0):\n\tcrop_re_fimg = crop_re_fimg.transpose(Image.FLIP_LEFT_RIGHT)\n\tflipped_x1 = my_crop_width - warped_x2\n\tflipped_x2 = my_crop_width - warped_x1\n\twarped_x1 = flipped_x1\n\twarped_x2 = flipped_x2\n\n    retf.append(normalize(crop_re_fimg))\n\t\t\n    warped_bbox = []\n    warped_bbox.append(warped_y1)\n    warped_bbox.append(warped_x1)\n    warped_bbox.append(warped_y2)\n    warped_bbox.append(warped_x2)\n\n    return retf, retc, warped_bbox\n\n\n\nclass Dataset(data.Dataset):\n    def __init__(self, data_dir, base_size=64, transform = None):\n\n        self.transform = transform\n        self.norm = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n        self.imsize = []\n        for i in range(cfg.TREE.BRANCH_NUM):\n            self.imsize.append(base_size)\n            base_size = base_size * 2\n\n        self.data = []\n        self.data_dir = data_dir\n        self.bbox = self.load_bbox()\n        self.filenames = self.load_filenames(data_dir)\n        if cfg.TRAIN.FLAG:\n            self.iterator = self.prepair_training_pairs\n        else:\n            self.iterator = self.prepair_test_pairs\n\n        \n    # only used in background stage\n    def load_bbox(self):\n        # Returns a dictionary with image filename as 'key' and its bounding box coordinates as 'value'\n\n        data_dir = self.data_dir\n        bbox_path = os.path.join(data_dir, 'bounding_boxes.txt')\n        df_bounding_boxes = pd.read_csv(bbox_path,\n                                        delim_whitespace=True,\n                                        header=None).astype(int)\n        filepath = os.path.join(data_dir, 'images.txt')\n        df_filenames = \\\n            pd.read_csv(filepath, delim_whitespace=True, header=None)\n        filenames = df_filenames[1].tolist()\n        print('Total filenames: ', len(filenames), filenames[0])\n        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n        numImgs = len(filenames)\n        for i in xrange(0, numImgs):\n            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n            key = filenames[i][:-4]\n            filename_bbox[key] = bbox\n        return filename_bbox\n\n\n    def load_filenames(self, data_dir):\n        filepath = os.path.join(data_dir, 'images.txt')\n        df_filenames = \\\n            pd.read_csv(filepath, delim_whitespace=True, header=None)\n        filenames = df_filenames[1].tolist()\n        filenames =  [fname[:-4] for fname in filenames];\n        print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n        return filenames\n\n\n    def prepair_training_pairs(self, index):\n        key = self.filenames[index]\n        if self.bbox is not None:\n            bbox = self.bbox[key]\n        else:\n            bbox = None\n        data_dir = self.data_dir\n        img_name = '%s/images/%s.jpg' % (data_dir, key)\n        fimgs, cimgs, warped_bbox = get_imgs(img_name, self.imsize,\n                        bbox, self.transform, normalize=self.norm)\n\n        rand_class= random.sample(range(cfg.FINE_GRAINED_CATEGORIES),1); # Randomly generating child code during training \n\tc_code = torch.zeros([cfg.FINE_GRAINED_CATEGORIES,])\n\tc_code[rand_class] = 1\n\n        return fimgs, cimgs, c_code, key, warped_bbox \n\n    def prepair_test_pairs(self, index):\n        key = self.filenames[index]\n        if self.bbox is not None:\n            bbox = self.bbox[key]\n        else:\n            bbox = None\n        data_dir = self.data_dir\n        c_code = self.c_code[index, :, :]\n        img_name = '%s/images/%s.jpg' % (data_dir, key)\n        _, imgs, _ = get_imgs(img_name, self.imsize,\n                        bbox, self.transform, normalize=self.norm)\n\n        return imgs, c_code, key \n\n    def __getitem__(self, index):\n        return self.iterator(index)\n\n    def __len__(self):\n        return len(self.filenames)\n"""
code/inception.py,12,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom miscc.config import cfg\n\n\n__all__ = [\'Inception3\', \'inception_v3\']\n\n\nmodel_urls = {\n    # Inception v3 ported from TensorFlow\n    \'inception_v3_google\': \'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\',\n}\n\n\n\ndef inception_v3(pretrained=True, **kwargs):\n    r""""""Inception v3 model architecture from\n    `""Rethinking the Inception Architecture for Computer Vision"" <http://arxiv.org/abs/1512.00567>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        if \'transform_input\' not in kwargs:\n            kwargs[\'transform_input\'] = True\n        model = Inception3(**kwargs)\n        pretrained_dict = model_zoo.load_url(model_urls[\'inception_v3_google\'])\n        model_dict = model.state_dict()\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        model_dict.update(pretrained_dict) \n        model.load_state_dict(model_dict)\n\tprint (""Inception pretrained on IMAGENET loaded"")\n        return model\n\n    return Inception3(**kwargs)\n\n\nclass Inception3(nn.Module):\n\n    def __init__(self, num_classes=200, aux_logits=True, transform_input=False):\n        super(Inception3, self).__init__()\n        self.aux_logits = aux_logits\n        self.transform_input = transform_input\n        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)\n        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n        self.Mixed_5b = InceptionA(192, pool_features=32)\n        self.Mixed_5c = InceptionA(256, pool_features=64)\n        self.Mixed_5d = InceptionA(288, pool_features=64)\n        self.Mixed_6a = InceptionB(288)\n        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n        if aux_logits:\n            self.AuxLogits = InceptionAux(768, num_classes)\n        self.Mixed_7a = InceptionD(768)\n        self.Mixed_7b = InceptionE(1280)\n        self.Mixed_7c = InceptionE(2048)\n        self.fc_new = nn.Linear(2048, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                import scipy.stats as stats\n                stddev = m.stddev if hasattr(m, \'stddev\') else 0.1\n                X = stats.truncnorm(-2, 2, scale=stddev)\n                values = torch.Tensor(X.rvs(m.weight.numel()))\n                values = values.view(m.weight.size())\n                m.weight.data.copy_(values)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n\n        #No preprocessing being done right now\n\n        # 299 x 299 x 3\n        x = self.Conv2d_1a_3x3(x)\n        # 149 x 149 x 32\n        x = self.Conv2d_2a_3x3(x)\n        # 147 x 147 x 32\n        x = self.Conv2d_2b_3x3(x)\n        # 147 x 147 x 64\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 73 x 73 x 64\n        x = self.Conv2d_3b_1x1(x)\n        # 73 x 73 x 80\n        x = self.Conv2d_4a_3x3(x)\n        # 71 x 71 x 192\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 35 x 35 x 192\n        x = self.Mixed_5b(x)\n        # 35 x 35 x 256\n        x = self.Mixed_5c(x)\n        # 35 x 35 x 288\n        x = self.Mixed_5d(x)\n        # 35 x 35 x 288\n        x = self.Mixed_6a(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6b(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6c(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6d(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6e(x)\n        # 17 x 17 x 768\n        if self.training and self.aux_logits:\n            aux = self.AuxLogits(x)\n        # 17 x 17 x 768\n        x = self.Mixed_7a(x)\n        # 8 x 8 x 1280\n        x = self.Mixed_7b(x)\n        # 8 x 8 x 2048\n        x = self.Mixed_7c(x)\n        # 8 x 8 x 2048\n        x = F.avg_pool2d(x, kernel_size=8)\n        # 1 x 1 x 2048\n        x = F.dropout(x, training=self.training)\n        # 1 x 1 x 2048\n        x = x.view(x.size(0), -1)\n        # 2048\n        x = self.fc_new(x)\n        # 1000 (num_classes)\n        if self.training and self.aux_logits:\n            return x, aux\n        return x\n\n\nclass InceptionA(nn.Module):\n\n    def __init__(self, in_channels, pool_features):\n        super(InceptionA, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n\n        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n\n        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionB(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionB, self).__init__()\n        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2)\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        branch3x3 = self.branch3x3(x)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n\n        outputs = [branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionC(nn.Module):\n\n    def __init__(self, in_channels, channels_7x7):\n        super(InceptionC, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)\n\n        c7 = channels_7x7\n        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n\n        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n\n        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionD(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionD, self).__init__()\n        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2)\n\n        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = self.branch3x3_2(branch3x3)\n\n        branch7x7x3 = self.branch7x7x3_1(x)\n        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n        outputs = [branch3x3, branch7x7x3, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionE(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionE, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)\n\n        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)\n        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionAux(nn.Module):\n\n    def __init__(self, in_channels, num_classes):\n        super(InceptionAux, self).__init__()\n        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)\n        self.conv1 = BasicConv2d(128, 768, kernel_size=5)\n        self.conv1.stddev = 0.01\n        self.fc_new = nn.Linear(768, num_classes)\n        self.fc_new.stddev = 0.001\n\n    def forward(self, x):\n        # 17 x 17 x 768\n        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n        # 5 x 5 x 768\n        x = self.conv0(x)\n        # 5 x 5 x 128\n        x = self.conv1(x)\n        # 1 x 1 x 768\n        x = x.view(x.size(0), -1)\n        # 768\n        x = self.fc_new(x)\n        # 1000\n        return x\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)\n\n\n'"
code/main.py,3,"b'from __future__ import print_function\nimport torch\nimport torchvision.transforms as transforms\n\nimport argparse\nimport os\nimport random\nimport sys\nimport pprint\nimport datetime\nimport dateutil.tz\nimport time\nimport pickle\n\ndir_path = (os.path.abspath(os.path.join(os.path.realpath(__file__), \'./.\')))\nsys.path.append(dir_path)\n\n\nfrom miscc.config import cfg, cfg_from_file\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Train a GAN network\')\n    parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                        help=\'optional config file\',\n                        default=\'cfg/birds_proGAN.yml\', type=str)\n    parser.add_argument(\'--gpu\', dest=\'gpu_id\', type=str, default=\'-1\')\n    parser.add_argument(\'--data_dir\', dest=\'data_dir\', type=str, default=\'\')\n    parser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\n    #parser.add_argument(\'--config_key\',dest=\'config_key\', type=str, help=\'configuration name\', default = \'finegan_birds\')\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    if args.cfg_file is not None:\n        cfg_from_file(args.cfg_file)\n\n    if args.gpu_id != \'-1\':\n        cfg.GPU_ID = args.gpu_id\n    else:\n        cfg.CUDA = False\n\n    if args.data_dir != \'\':\n        cfg.DATA_DIR = args.data_dir\n    if cfg.TRAIN.FLAG:\n        print(\'Using config:\')\n        pprint.pprint(cfg)\n\n    if not cfg.TRAIN.FLAG:\n        args.manualSeed = 45   # Change this to have different random seed during evaluation\n\n    elif args.manualSeed is None:\n        args.manualSeed = random.randint(1, 10000)\n    random.seed(args.manualSeed)\n    torch.manual_seed(args.manualSeed)\n    if cfg.CUDA:\n        torch.cuda.manual_seed_all(args.manualSeed)\n    \n    # Evaluation part\n    if not cfg.TRAIN.FLAG:\n        from trainer import FineGAN_evaluator as evaluator\n        algo = evaluator()\n        algo.evaluate_finegan()\n\n    # Training part\n    else:\n        now = datetime.datetime.now(dateutil.tz.tzlocal())\n        timestamp = now.strftime(\'%Y_%m_%d_%H_%M_%S\')\n        output_dir = \'../output/%s_%s\' % \\\n            (cfg.DATASET_NAME, timestamp)\n        pkl_filename = \'cfg.pickle\'\n\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        with open(os.path.join(output_dir, pkl_filename), \'wb\') as pk:\n            pickle.dump(cfg, pk, protocol=pickle.HIGHEST_PROTOCOL)\n\n        bshuffle = True\n\n        # Get data loader\n        imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n        image_transform = transforms.Compose([\n            transforms.Scale(int(imsize * 76 / 64)),\n            transforms.RandomCrop(imsize),\n            transforms.RandomHorizontalFlip()])\n\n\n        from datasets import Dataset\n        dataset = Dataset(cfg.DATA_DIR,\n                              base_size=cfg.TREE.BASE_SIZE,\n                              transform=image_transform)\n        assert dataset\n        num_gpu = len(cfg.GPU_ID.split(\',\'))\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=cfg.TRAIN.BATCH_SIZE * num_gpu,\n            drop_last=True, shuffle=bshuffle, num_workers=int(cfg.WORKERS))\n\n\n        from trainer import FineGAN_trainer as trainer\n        algo = trainer(output_dir, dataloader, imsize)\n\n        start_t = time.time()\n        algo.train()\n        end_t = time.time()\n        print(\'Total time for training:\', end_t - start_t)\n'"
code/model.py,15,"b'import sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom miscc.config import cfg\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.nn import Upsample\n\n\nclass GLU(nn.Module):\n    def __init__(self):\n        super(GLU, self).__init__()\n\n    def forward(self, x):\n        nc = x.size(1)\n        assert nc % 2 == 0, \'channels dont divide 2!\'\n        nc = int(nc/2)\n        return x[:, :nc] * F.sigmoid(x[:, nc:])\n\n\ndef conv3x3(in_planes, out_planes):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n                     padding=1, bias=False)\n\n\ndef convlxl(in_planes, out_planes):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=13, stride=1,\n                     padding=1, bias=False)\n\n\ndef child_to_parent(child_c_code, classes_child, classes_parent):\n    \n    ratio = classes_child / classes_parent\n    arg_parent = torch.argmax(child_c_code,  dim = 1) / ratio\n    parent_c_code = torch.zeros([child_c_code.size(0), classes_parent]).cuda()\n    for i in range(child_c_code.size(0)):\n\tparent_c_code[i][arg_parent[i]] = 1\t\n    return parent_c_code\t\n\n\n# ############## G networks ################################################\n# Upsale the spatial size by a factor of 2\ndef upBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        nn.Upsample(scale_factor=2, mode=\'nearest\'),\n        conv3x3(in_planes, out_planes * 2),\n        nn.BatchNorm2d(out_planes * 2),\n        GLU()\n    )\n    return block\n\ndef sameBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        conv3x3(in_planes, out_planes * 2),\n        nn.BatchNorm2d(out_planes * 2),\n        GLU()\n    )\n    return block\n\n# Keep the spatial size\ndef Block3x3_relu(in_planes, out_planes):\n    block = nn.Sequential(\n        conv3x3(in_planes, out_planes * 2),\n        nn.BatchNorm2d(out_planes * 2),\n        GLU()\n    )\n    return block\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, channel_num):\n        super(ResBlock, self).__init__()\n        self.block = nn.Sequential(\n            conv3x3(channel_num, channel_num * 2),\n            nn.BatchNorm2d(channel_num * 2),\n            GLU(),\n            conv3x3(channel_num, channel_num),\n            nn.BatchNorm2d(channel_num)\n        )\n\n\n    def forward(self, x):\n        residual = x\n        out = self.block(x)\n        out += residual\n        return out\n\n\nclass INIT_STAGE_G(nn.Module):\n    def __init__(self, ngf, c_flag):\n        super(INIT_STAGE_G, self).__init__()\n        self.gf_dim = ngf\n\tself.c_flag= c_flag\n\n        if self.c_flag==1 :\n            \tself.in_dim = cfg.GAN.Z_DIM + cfg.SUPER_CATEGORIES\n\telif self.c_flag==2:\n\t\tself.in_dim = cfg.GAN.Z_DIM + cfg.FINE_GRAINED_CATEGORIES \n\n        self.define_module()\n\n    def define_module(self):\n        in_dim = self.in_dim\n        ngf = self.gf_dim\n        self.fc = nn.Sequential(\n            nn.Linear(in_dim, ngf * 4 * 4 * 2, bias=False),\n            nn.BatchNorm1d(ngf * 4 * 4 * 2),\n            GLU())\n\n        self.upsample1 = upBlock(ngf, ngf // 2)\n        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n        self.upsample5 = upBlock(ngf // 16, ngf // 16)\n\n\n    def forward(self, z_code, code):\n\n        in_code = torch.cat((code, z_code), 1)\n        out_code = self.fc(in_code)\n        out_code = out_code.view(-1, self.gf_dim, 4, 4)\n        out_code = self.upsample1(out_code)\n        out_code = self.upsample2(out_code)\n        out_code = self.upsample3(out_code)\n        out_code = self.upsample4(out_code)\n\tout_code = self.upsample5(out_code)\n\n        return out_code\n\n\nclass NEXT_STAGE_G(nn.Module):\n    def __init__(self, ngf, use_hrc = 1, num_residual=cfg.GAN.R_NUM):\n        super(NEXT_STAGE_G, self).__init__()\n        self.gf_dim = ngf\n        if use_hrc == 1: # For parent stage\n            self.ef_dim = cfg.SUPER_CATEGORIES\n\n        else:            # For child stage\t\n            self.ef_dim = cfg.FINE_GRAINED_CATEGORIES\n\n        self.num_residual = num_residual\n        self.define_module()\n\n    def _make_layer(self, block, channel_num):\n        layers = []\n        for i in range(self.num_residual):\n            layers.append(block(channel_num))\n        return nn.Sequential(*layers)\n\n    def define_module(self):\n        ngf = self.gf_dim\n        efg = self.ef_dim\n        self.jointConv = Block3x3_relu(ngf + efg, ngf)\n        self.residual = self._make_layer(ResBlock, ngf)\n        self.samesample = sameBlock(ngf, ngf // 2)\n\n    def forward(self, h_code, code):\n        s_size = h_code.size(2)\n        code = code.view(-1, self.ef_dim, 1, 1)\n        code = code.repeat(1, 1, s_size, s_size)\n        h_c_code = torch.cat((code, h_code), 1)\n        out_code = self.jointConv(h_c_code)\n        out_code = self.residual(out_code)\n        out_code = self.samesample(out_code)\n        return out_code\n\n\nclass GET_IMAGE_G(nn.Module):\n    def __init__(self, ngf):\n        super(GET_IMAGE_G, self).__init__()\n        self.gf_dim = ngf\n        self.img = nn.Sequential(\n            conv3x3(ngf, 3),\n            nn.Tanh()\n        )\n\n    def forward(self, h_code):\n        out_img = self.img(h_code)\n        return out_img\n\n\n\nclass GET_MASK_G(nn.Module):\n    def __init__(self, ngf):\n        super(GET_MASK_G, self).__init__()\n        self.gf_dim = ngf\n        self.img = nn.Sequential(\n            conv3x3(ngf, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, h_code):\n        out_img = self.img(h_code)\n\treturn out_img\n\n\nclass G_NET(nn.Module):\n    def __init__(self):\n        super(G_NET, self).__init__()\n        self.gf_dim = cfg.GAN.GF_DIM\n        self.define_module()\n\tself.upsampling = Upsample(scale_factor = 2, mode = \'bilinear\')\n\tself.scale_fimg = nn.UpsamplingBilinear2d(size = [126, 126])\n\n    def define_module(self):\n\n        #Background stage\n        self.h_net1_bg = INIT_STAGE_G(self.gf_dim * 16, 2)\n        self.img_net1_bg = GET_IMAGE_G(self.gf_dim) # Background generation network\n        \n        # Parent stage networks\n        self.h_net1 = INIT_STAGE_G(self.gf_dim * 16, 1)\n        self.h_net2 = NEXT_STAGE_G(self.gf_dim, use_hrc = 1) \n        self.img_net2 = GET_IMAGE_G(self.gf_dim // 2)  # Parent foreground generation network \n        self.img_net2_mask= GET_MASK_G(self.gf_dim // 2) # Parent mask generation network \n        \n        # Child stage networks\n        self.h_net3 = NEXT_STAGE_G(self.gf_dim // 2, use_hrc = 0)  \n        self.img_net3 = GET_IMAGE_G(self.gf_dim // 4) # Child foreground generation network\n        self.img_net3_mask = GET_MASK_G(self.gf_dim // 4) # Child mask generation network\n\n    def forward(self, z_code, c_code, p_code = None, bg_code = None):\n\n        fake_imgs = [] # Will contain [background image, parent image, child image]\n\tfg_imgs = [] # Will contain [parent foreground, child foreground]\n\tmk_imgs = [] # Will contain [parent mask, child mask]\n\tfg_mk = [] # Will contain [masked parent foreground, masked child foreground]\n\n        if cfg.TIED_CODES:\n\t    p_code = child_to_parent(c_code, cfg.FINE_GRAINED_CATEGORIES, cfg.SUPER_CATEGORIES) # Obtaining the parent code from child code\n            bg_code = c_code\n\n\t#Background stage\t\n        h_code1_bg = self.h_net1_bg(z_code, bg_code)\t    \t\t\n        fake_img1 = self.img_net1_bg(h_code1_bg) # Background image\n        fake_img1_126 = self.scale_fimg(fake_img1) # Resizing fake background image from 128x128 to the resolution which background discriminator expects: 126 x 126.\t\n        fake_imgs.append(fake_img1_126)\n\n\t#Parent stage    \t\n        h_code1 = self.h_net1(z_code, p_code)\n        h_code2 = self.h_net2(h_code1, p_code)  \n        fake_img2_foreground = self.img_net2(h_code2) # Parent foreground\n        fake_img2_mask = self.img_net2_mask(h_code2) # Parent mask \n        ones_mask_p = torch.ones_like(fake_img2_mask)\n        opp_mask_p = ones_mask_p - fake_img2_mask\n        fg_masked2 = torch.mul(fake_img2_foreground, fake_img2_mask)\n        fg_mk.append(fg_masked2)\n        bg_masked2 = torch.mul(fake_img1, opp_mask_p)\t \t\n        fake_img2_final = fg_masked2 + bg_masked2 # Parent image\n        fake_imgs.append(fake_img2_final) \n        fg_imgs.append(fake_img2_foreground)\n        mk_imgs.append(fake_img2_mask)\n\n\t#Child stage\n        h_code3 = self.h_net3(h_code2, c_code)\n        fake_img3_foreground = self.img_net3(h_code3) # Child foreground  \n        fake_img3_mask = self.img_net3_mask(h_code3) # Child mask\t\n        ones_mask_c = torch.ones_like(fake_img3_mask)\n        opp_mask_c = ones_mask_c - fake_img3_mask\n        fg_masked3 = torch.mul(fake_img3_foreground, fake_img3_mask)\n        fg_mk.append(fg_masked3)\n        bg_masked3 = torch.mul(fake_img2_final, opp_mask_c)\t\n        fake_img3_final = fg_masked3 + bg_masked3  # Child image\n        fake_imgs.append(fake_img3_final)\n        fg_imgs.append(fake_img3_foreground)\n        mk_imgs.append(fake_img3_mask)\n\n        return fake_imgs, fg_imgs, mk_imgs, fg_mk\n\n\n# ############## D networks ################################################\ndef Block3x3_leakRelu(in_planes, out_planes):\n    block = nn.Sequential(\n        conv3x3(in_planes, out_planes),\n        nn.BatchNorm2d(out_planes),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return block\n\n\n# Downsale the spatial size by a factor of 2\ndef downBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(out_planes),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return block\n\n\n\ndef encode_parent_and_child_img(ndf): # Defines the encoder network used for parent and child image\n    encode_img = nn.Sequential(\n        nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 2),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 4),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 8),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return encode_img\n\n\ndef encode_background_img(ndf): # Defines the encoder network used for background image\n    encode_img = nn.Sequential(\n        nn.Conv2d(3, ndf, 4, 2, 0, bias=False),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf, ndf * 2, 4, 2, 0, bias=False),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(ndf * 2, ndf * 4, 4, 1, 0, bias=False),\n        nn.LeakyReLU(0.2, inplace=True),\n    )\n    return encode_img\n\n\nclass D_NET(nn.Module):\n    def __init__(self, stg_no):\n        super(D_NET, self).__init__()\n        self.df_dim = cfg.GAN.DF_DIM\n        self.stg_no = stg_no\n\n\tif self.stg_no  == 0:\n\t\tself.ef_dim = 1\n\telif self.stg_no == 1:\n        \tself.ef_dim = cfg.SUPER_CATEGORIES\n        elif self.stg_no == 2:\n        \tself.ef_dim = cfg.FINE_GRAINED_CATEGORIES\n        else:\n                print (""Invalid stage number. Set stage number as follows:"")\n                print (""0 - for background stage"")\n                print (""1 - for parent stage"")\n                print (""2 - for child stage"")\n                print (""...Exiting now"")\n                sys.exit(0)\n        self.define_module()\n\n    def define_module(self):\n        ndf = self.df_dim\n        efg = self.ef_dim\n\n        if self.stg_no == 0:\n\n        \tself.patchgan_img_code_s16 = encode_background_img(ndf)\n                self.uncond_logits1 = nn.Sequential(\n                nn.Conv2d(ndf * 4, 1, kernel_size=4, stride=1),\n                nn.Sigmoid())\n                self.uncond_logits2 = nn.Sequential(\n                nn.Conv2d(ndf * 4, 1, kernel_size=4, stride=1),\n                nn.Sigmoid())\n\n        else:\n        \tself.img_code_s16 = encode_parent_and_child_img(ndf)\n\t\tself.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n\t\tself.img_code_s32_1 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n\n\t\tself.logits = nn.Sequential(\n\t\t    nn.Conv2d(ndf * 8, efg, kernel_size=4, stride=4))\n\n                self.jointConv = Block3x3_leakRelu(ndf * 8, ndf * 8)\n                self.uncond_logits = nn.Sequential(\n                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n                nn.Sigmoid())\n\n\n    def forward(self, x_var):\n\n\tif self.stg_no == 0: \n        \tx_code = self.patchgan_img_code_s16(x_var)\n            \tclassi_score = self.uncond_logits1(x_code) # Background vs Foreground classification score (0 - background and 1 - foreground) \n        \trf_score = self.uncond_logits2(x_code) # Real/Fake score for the background image\n\t\treturn [classi_score, rf_score]\n\n\telif self.stg_no > 0:\n        \tx_code = self.img_code_s16(x_var)\n        \tx_code = self.img_code_s32(x_code)\n        \tx_code = self.img_code_s32_1(x_code)\n                h_c_code = self.jointConv(x_code)\n                code_pred = self.logits(h_c_code) # Predicts the parent code and child code in parent and child stage respectively\n                rf_score = self.uncond_logits(x_code) # This score is not used in parent stage while training\n            \treturn [code_pred.view(-1, self.ef_dim), rf_score.view(-1)]\n\n\n\n'"
code/trainer.py,48,"b'from __future__ import print_function\nfrom six.moves import range\nimport sys\nimport numpy as np\nimport os\nimport random\nimport time\nfrom PIL import Image\nfrom copy import deepcopy\n\nimport torch.backends.cudnn as cudnn\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torchvision.utils as vutils\nfrom torch.nn.functional import softmax, log_softmax\nfrom torch.nn.functional import cosine_similarity\nfrom tensorboardX import summary\nfrom tensorboardX import FileWriter\n\nfrom miscc.config import cfg\nfrom miscc.utils import mkdir_p\n\nfrom model import G_NET, D_NET\n\n\n# ################## Shared functions ###################\n\ndef child_to_parent(child_c_code, classes_child, classes_parent):\n\n    ratio = classes_child / classes_parent\n    arg_parent = torch.argmax(child_c_code,  dim = 1) / ratio\n    parent_c_code = torch.zeros([child_c_code.size(0), classes_parent]).cuda()\n    for i in range(child_c_code.size(0)):\n        parent_c_code[i][arg_parent[i]] = 1\n    return parent_c_code\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        nn.init.orthogonal(m.weight.data, 1.0)\n    elif classname.find(\'BatchNorm\') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n    elif classname.find(\'Linear\') != -1:\n        nn.init.orthogonal(m.weight.data, 1.0)\n        if m.bias is not None:\n            m.bias.data.fill_(0.0)\n\n\ndef load_params(model, new_param):\n    for p, new_p in zip(model.parameters(), new_param):\n        p.data.copy_(new_p)\n\n\ndef copy_G_params(model):\n    flatten = deepcopy(list(p.data for p in model.parameters()))\n    return flatten\n\ndef load_network(gpus):\n    netG = G_NET()\n    netG.apply(weights_init)\n    netG = torch.nn.DataParallel(netG, device_ids=gpus)\n    print(netG)\n\n    netsD = []\n    for i in range(3): # 3 discriminators for background, parent and child stage\n        netsD.append(D_NET(i))\n\n    for i in range(len(netsD)):\n        netsD[i].apply(weights_init)\n        netsD[i] = torch.nn.DataParallel(netsD[i], device_ids=gpus)\n\n    count = 0\n\n    if cfg.TRAIN.NET_G != \'\':\n        state_dict = torch.load(cfg.TRAIN.NET_G)\n        netG.load_state_dict(state_dict)\n        print(\'Load \', cfg.TRAIN.NET_G)\n\n        istart = cfg.TRAIN.NET_G.rfind(\'_\') + 1\n        iend = cfg.TRAIN.NET_G.rfind(\'.\')\n        count = cfg.TRAIN.NET_G[istart:iend]\n        count = int(count) + 1\n\n    if cfg.TRAIN.NET_D != \'\':\n        for i in range(len(netsD)):\n            print(\'Load %s_%d.pth\' % (cfg.TRAIN.NET_D, i))\n            state_dict = torch.load(\'%s_%d.pth\' % (cfg.TRAIN.NET_D, i))\n            netsD[i].load_state_dict(state_dict)\n\n    if cfg.CUDA:\n        netG.cuda()\n        for i in range(len(netsD)):\n            netsD[i].cuda()\n\n    return netG, netsD, len(netsD), count\n\n\ndef define_optimizers(netG, netsD):\n    optimizersD = []\n    num_Ds = len(netsD)\n    for i in range(num_Ds):      \n        opt = optim.Adam(netsD[i].parameters(),\n                         lr=cfg.TRAIN.DISCRIMINATOR_LR,\n                         betas=(0.5, 0.999))\n        optimizersD.append(opt)\n\n    optimizerG = []\n    optimizerG.append(optim.Adam(netG.parameters(),\n                            lr=cfg.TRAIN.GENERATOR_LR,\n                            betas=(0.5, 0.999)))\n\n    for i in range(num_Ds):\n        if i==1:   \n                opt = optim.Adam(netsD[i].parameters(),\n                lr=cfg.TRAIN.GENERATOR_LR,\n                betas=(0.5, 0.999))\n                optimizerG.append(opt)\n        elif i==2:\n                opt = optim.Adam([{\'params\':netsD[i].module.jointConv.parameters()},{\'params\':netsD[i].module.logits.parameters()}],\n                lr=cfg.TRAIN.GENERATOR_LR,\n                betas=(0.5, 0.999))\n                optimizerG.append(opt)\n\n    return optimizerG, optimizersD\n\n\ndef save_model(netG, avg_param_G, netsD, epoch, model_dir):\n    load_params(netG, avg_param_G)\n    torch.save(\n        netG.state_dict(),\n        \'%s/netG_%d.pth\' % (model_dir, epoch))\n    for i in range(len(netsD)):\n        netD = netsD[i]\n        torch.save(\n            netD.state_dict(),\n            \'%s/netD%d.pth\' % (model_dir, i))\n    print(\'Save G/Ds models.\')\n\n\ndef save_img_results(imgs_tcpu, fake_imgs, num_imgs,\n                     count, image_dir, summary_writer):\n    num = cfg.TRAIN.VIS_COUNT\n\n    real_img = imgs_tcpu[-1][0:num]\n    vutils.save_image(\n        real_img, \'%s/real_samples%09d.png\' % (image_dir,count),\n        normalize=True)\n    real_img_set = vutils.make_grid(real_img).numpy()\n    real_img_set = np.transpose(real_img_set, (1, 2, 0))\n    real_img_set = real_img_set * 255\n    real_img_set = real_img_set.astype(np.uint8)\n\n    for i in range(len(fake_imgs)):\n        fake_img = fake_imgs[i][0:num]\n\n        vutils.save_image(\n            fake_img.data, \'%s/count_%09d_fake_samples%d.png\' %\n            (image_dir, count, i), normalize=True)\n\n        fake_img_set = vutils.make_grid(fake_img.data).cpu().numpy()\n\n        fake_img_set = np.transpose(fake_img_set, (1, 2, 0))\n        fake_img_set = (fake_img_set + 1) * 255 / 2\n        fake_img_set = fake_img_set.astype(np.uint8)\n        summary_writer.flush()\n\n\n\nclass FineGAN_trainer(object):\n    def __init__(self, output_dir, data_loader, imsize):\n        if cfg.TRAIN.FLAG:\n            self.model_dir = os.path.join(output_dir, \'Model\')\n            self.image_dir = os.path.join(output_dir, \'Image\')\n            self.log_dir = os.path.join(output_dir, \'Log\')\n            mkdir_p(self.model_dir)\n            mkdir_p(self.image_dir)\n            mkdir_p(self.log_dir)\n            self.summary_writer = FileWriter(self.log_dir)\n\n        s_gpus = cfg.GPU_ID.split(\',\')\n        self.gpus = [int(ix) for ix in s_gpus]\n        self.num_gpus = len(self.gpus)\n        torch.cuda.set_device(self.gpus[0])\n        cudnn.benchmark = True\n\n        self.batch_size = cfg.TRAIN.BATCH_SIZE * self.num_gpus\n        self.max_epoch = cfg.TRAIN.MAX_EPOCH\n        self.snapshot_interval = cfg.TRAIN.SNAPSHOT_INTERVAL\n\n        self.data_loader = data_loader\n        self.num_batches = len(self.data_loader)\n\n\n\n    def prepare_data(self, data):\n        fimgs, cimgs, c_code, _, warped_bbox = data\n\n        real_vfimgs, real_vcimgs = [], []\n        if cfg.CUDA:\n            vc_code = Variable(c_code).cuda()\n\t    for i in range(len(warped_bbox)):\n\t\twarped_bbox[i] = Variable(warped_bbox[i]).float().cuda()\n\n        else:\n            vc_code = Variable(c_code)\n\t    for i in range(len(warped_bbox)):\n\t\twarped_bbox[i] = Variable(warped_bbox[i])\n\n        if cfg.CUDA:\n            real_vfimgs.append(Variable(fimgs[0]).cuda())\n            real_vcimgs.append(Variable(cimgs[0]).cuda())\n        else:\n            real_vfimgs.append(Variable(fimgs[0]))\n            real_vcimgs.append(Variable(cimgs[0]))\n\n        return fimgs, real_vfimgs, real_vcimgs, vc_code, warped_bbox\n\n    def train_Dnet(self, idx, count):\n      if idx == 0 or idx == 2: # Discriminator is only trained in background and child stage. (NOT in parent stage)\n        flag = count % 100\n        batch_size = self.real_fimgs[0].size(0)\n        criterion, criterion_one = self.criterion, self.criterion_one\n\n        netD, optD = self.netsD[idx], self.optimizersD[idx] \n\tif idx == 0:\n        \treal_imgs = self.real_fimgs[0]\n\t\n\telif idx == 2:\n\t\treal_imgs = self.real_cimgs[0]\n       \n        fake_imgs = self.fake_imgs[idx]\n        netD.zero_grad()        \n        real_logits = netD(real_imgs)\n\n\tif idx == 2:\n\t\tfake_labels = torch.zeros_like(real_logits[1])\n\t\treal_labels = torch.ones_like(real_logits[1])\n\telif idx == 0:\n\n\t\tfake_labels = torch.zeros_like(real_logits[1])\n\t\text, output = real_logits\n\t\tweights_real = torch.ones_like(output)\n\t\treal_labels = torch.ones_like(output)\n            \n                for i in range(batch_size):\n                        x1 =  self.warped_bbox[0][i]\n                        x2 =  self.warped_bbox[2][i]\n                        y1 =  self.warped_bbox[1][i]\n                        y2 =  self.warped_bbox[3][i]\n\n                        a1 = max(torch.tensor(0).float().cuda(), torch.ceil((x1 - self.recp_field)/self.patch_stride))\n                        a2 = min(torch.tensor(self.n_out - 1).float().cuda(), torch.floor((self.n_out - 1) - ((126 - self.recp_field) - x2)/self.patch_stride)) + 1\n                        b1 = max(torch.tensor(0).float().cuda(), torch.ceil((y1 - self.recp_field)/self.patch_stride))\n                        b2 = min(torch.tensor(self.n_out - 1).float().cuda(), torch.floor((self.n_out - 1) - ((126 - self.recp_field) - y2)/self.patch_stride)) + 1\n\n\t\t\tif (x1 != x2 and y1 != y2):\n                        \tweights_real[i, :, a1.type(torch.int) : a2.type(torch.int) , b1.type(torch.int) : b2.type(torch.int)] = 0.0\n\n                norm_fact_real = weights_real.sum()\n                norm_fact_fake = weights_real.shape[0]*weights_real.shape[1]*weights_real.shape[2]*weights_real.shape[3]\n\t\treal_logits = ext, output\n\n        fake_logits = netD(fake_imgs.detach())\n\n\n\t  \n\tif idx == 0: # Background stage\n\n            errD_real_uncond = criterion(real_logits[1], real_labels)  # Real/Fake loss for \'real background\' (on patch level)\n\t    errD_real_uncond = torch.mul(errD_real_uncond, weights_real)  # Masking output units which correspond to receptive fields which lie within the boundin box\n\t    errD_real_uncond = errD_real_uncond.mean()\n\n            errD_real_uncond_classi = criterion(real_logits[0], weights_real)  # Background/foreground classification loss\n\t    errD_real_uncond_classi = errD_real_uncond_classi.mean()\n\t   \n            errD_fake_uncond = criterion(fake_logits[1], fake_labels)  # Real/Fake loss for \'fake background\' (on patch level)\n\t    errD_fake_uncond = errD_fake_uncond.mean()\n\n            if (norm_fact_real > 0):    # Normalizing the real/fake loss for background after accounting the number of masked members in the output.\n            \terrD_real = errD_real_uncond * ((norm_fact_fake * 1.0) /(norm_fact_real * 1.0))\n\t    else:\n\t\terrD_real = errD_real_uncond\n\n            errD_fake = errD_fake_uncond\n\t    errD = ((errD_real + errD_fake) * cfg.TRAIN.BG_LOSS_WT) + errD_real_uncond_classi\n\n        if idx == 2:\n\t\n            errD_real = criterion_one(real_logits[1], real_labels) # Real/Fake loss for the real image\n            errD_fake = criterion_one(fake_logits[1], fake_labels) # Real/Fake loss for the fake image   \n            errD = errD_real + errD_fake\n\n        if (idx == 0 or idx == 2):\n              errD.backward()\n              optD.step()\n\n        if (flag == 0):\n            summary_D = summary.scalar(\'D_loss%d\' % idx, errD.data[0])\n            self.summary_writer.add_summary(summary_D, count)\n            summary_D_real = summary.scalar(\'D_loss_real_%d\' % idx, errD_real.data[0])\n            self.summary_writer.add_summary(summary_D_real, count)\n            summary_D_fake = summary.scalar(\'D_loss_fake_%d\' % idx, errD_fake.data[0])\n            self.summary_writer.add_summary(summary_D_fake, count)\n\n        return errD\n\n    def train_Gnet(self, count):\n        self.netG.zero_grad()\n        for myit in range(len(self.netsD)): \n             self.netsD[myit].zero_grad()\n\n        errG_total = 0\n        flag = count % 100\n        batch_size = self.real_fimgs[0].size(0)\n        criterion_one, criterion_class, c_code, p_code = self.criterion_one, self.criterion_class, self.c_code, self.p_code\n\n        for i in range(self.num_Ds):\n\t  \n\t    outputs = self.netsD[i](self.fake_imgs[i]) \t\n\n            if i == 0 or i == 2:  # real/fake loss for background (0) and child (2) stage\n\t\treal_labels = torch.ones_like(outputs[1])\n\t    \terrG = criterion_one(outputs[1], real_labels) \n\t\tif i==0:\n\t\t\terrG = errG * cfg.TRAIN.BG_LOSS_WT\n\t\t\terrG_classi = criterion_one(outputs[0], real_labels) # Background/Foreground classification loss for the fake background image (on patch level)\n\t\t\terrG = errG + errG_classi\n\t    \terrG_total = errG_total + errG\t\n\n            if i == 1: # Mutual information loss for the parent stage (1)\n                    pred_p = self.netsD[i](self.fg_mk[i-1])\n                    errG_info = criterion_class(pred_p[0], torch.nonzero(p_code.long())[:,1])\n            elif i == 2: # Mutual information loss for the child stage (2)\n                    pred_c = self.netsD[i](self.fg_mk[i-1])\n                    errG_info = criterion_class(pred_c[0], torch.nonzero(c_code.long())[:,1])\n\n            if(i>0):\n                errG_total = errG_total + errG_info\n\n            if flag == 0:\n\t\tif i>0:\n                  summary_D_class = summary.scalar(\'Information_loss_%d\' % i, errG_info.data[0])\n                  self.summary_writer.add_summary(summary_D_class, count)\n\n\t\tif i == 0 or i == 2:\n                  summary_D = summary.scalar(\'G_loss%d\' % i, errG.data[0])\n                  self.summary_writer.add_summary(summary_D, count) \n\n        errG_total.backward()\n        for myit in range(len(self.netsD)): \n        \tself.optimizerG[myit].step()\n        return errG_total\n\n    def train(self):\n        self.netG, self.netsD, self.num_Ds, start_count = load_network(self.gpus)\n        avg_param_G = copy_G_params(self.netG)\n\n        self.optimizerG, self.optimizersD = \\\n            define_optimizers(self.netG, self.netsD)\n\n        self.criterion = nn.BCELoss(reduce=False)\n\tself.criterion_one = nn.BCELoss()\n        self.criterion_class = nn.CrossEntropyLoss()\n\n        self.real_labels = \\\n            Variable(torch.FloatTensor(self.batch_size).fill_(1))\n        self.fake_labels = \\\n            Variable(torch.FloatTensor(self.batch_size).fill_(0))\n\t\n        nz = cfg.GAN.Z_DIM\n        noise = Variable(torch.FloatTensor(self.batch_size, nz))\n        fixed_noise = \\\n            Variable(torch.FloatTensor(self.batch_size, nz).normal_(0, 1))\n        hard_noise = \\\n            Variable(torch.FloatTensor(self.batch_size, nz).normal_(0, 1)).cuda()\n\t\n\tself.patch_stride = float(4)    # Receptive field stride given the current discriminator architecture for background stage \n\tself.n_out = 24                 # Output size of the discriminator at the background stage; N X N where N = 24\n\tself.recp_field = 34            # Receptive field of each of the member of N X N\n\n\n        if cfg.CUDA:\n            self.criterion.cuda()\n            self.criterion_one.cuda()\n            self.criterion_class.cuda()\n            self.real_labels = self.real_labels.cuda()\n            self.fake_labels = self.fake_labels.cuda()\n            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n        \n        print (""Starting normal FineGAN training.."") \n        count = start_count\n        start_epoch = start_count // (self.num_batches)\n\n        for epoch in range(start_epoch, self.max_epoch):\n            start_t = time.time()\n            \n            for step, data in enumerate(self.data_loader, 0):\n               \n                self.imgs_tcpu, self.real_fimgs, self.real_cimgs, \\\n                    self.c_code, self.warped_bbox = self.prepare_data(data)\n\n                # Feedforward through Generator. Obtain stagewise fake images\n                noise.data.normal_(0, 1)\n                self.fake_imgs, self.fg_imgs, self.mk_imgs, self.fg_mk = \\\n                    self.netG(noise, self.c_code)\n\n                # Obtain the parent code given the child code\n\t\tself.p_code = child_to_parent(self.c_code, cfg.FINE_GRAINED_CATEGORIES, cfg.SUPER_CATEGORIES) \n\n                # Update Discriminator networks \n                errD_total = 0\n                for i in range(self.num_Ds):\n\t\t  if i == 0 or i == 2: # only at parent and child stage\n                    errD = self.train_Dnet(i, count)\n                    errD_total += errD\n\n                # Update the Generator networks\n                errG_total = self.train_Gnet(count)\n                for p, avg_p in zip(self.netG.parameters(), avg_param_G):\n                    avg_p.mul_(0.999).add_(0.001, p.data)\n\n                count = count + 1\n\n                if count % cfg.TRAIN.SNAPSHOT_INTERVAL == 0:\n                    backup_para = copy_G_params(self.netG)\n                    save_model(self.netG, avg_param_G, self.netsD, count, self.model_dir)\n                    # Save images\n                    load_params(self.netG, avg_param_G)\n                    self.netG.eval()\n\t\t    with torch.set_grad_enabled(False):\n                    \tself.fake_imgs, self.fg_imgs, self.mk_imgs, self.fg_mk = \\\n                        \tself.netG(fixed_noise, self.c_code)\n                    save_img_results(self.imgs_tcpu, (self.fake_imgs + self.fg_imgs + self.mk_imgs + self.fg_mk), self.num_Ds,\n                                     count, self.image_dir, self.summary_writer)\n                    self.netG.train()\n                    load_params(self.netG, backup_para)\n\n            end_t = time.time()\n            print(\'\'\'[%d/%d][%d]\n                         Loss_D: %.2f Loss_G: %.2f Time: %.2fs\n                      \'\'\'  \n                  % (epoch, self.max_epoch, self.num_batches,\n                     errD_total.data[0], errG_total.data[0],\n                     end_t - start_t))\n\n        save_model(self.netG, avg_param_G, self.netsD, count, self.model_dir)\n\n\tprint (""Done with the normal training. Now performing hard negative training.."")\n        count = 0\n        start_t = time.time()\n        for step, data in enumerate(self.data_loader, 0):\n\n            self.imgs_tcpu, self.real_fimgs, self.real_cimgs, \\\n                self.c_code, self.warped_bbox = self.prepare_data(data)\n\n            if (count % 2) == 0: # Train on normal batch of images\n\n                    # Feedforward through Generator. Obtain stagewise fake images\n                    noise.data.normal_(0, 1)\n                    self.fake_imgs, self.fg_imgs, self.mk_imgs, self.fg_mk = \\\n                        self.netG(noise, self.c_code)\n\n                    self.p_code = child_to_parent(self.c_code, cfg.FINE_GRAINED_CATEGORIES, cfg.SUPER_CATEGORIES)\n\n                    # Update discriminator networks\n                    errD_total = 0\n                    for i in range(self.num_Ds):\n                            if i == 0 or i == 2:\n                                    errD = self.train_Dnet(i, count)\n                                    errD_total += errD\n\n\n                    # Update the generator network\n                    errG_total = self.train_Gnet(count)\n\n            else: # Train on degenerate images\n                    repeat_times=10\n                    all_hard_z = Variable(torch.zeros(self.batch_size * repeat_times, nz)).cuda()\n                    all_hard_class = Variable(torch.zeros(self.batch_size * repeat_times, cfg.FINE_GRAINED_CATEGORIES)).cuda()\n                    all_logits = Variable(torch.zeros(self.batch_size * repeat_times,)).cuda()\n                    \n                    for hard_it in range(repeat_times):\n                            hard_noise = hard_noise.data.normal_(0,1)\n                            hard_class = Variable(torch.zeros([self.batch_size, cfg.FINE_GRAINED_CATEGORIES])).cuda()\n                            my_rand_id=[]\n\n                            for c_it in range(self.batch_size):\n                                    rand_class = random.sample(range(cfg.FINE_GRAINED_CATEGORIES),1);\n                                    hard_class[c_it][rand_class] = 1\n                                    my_rand_id.append(rand_class)\n\n                            all_hard_z[self.batch_size * hard_it : self.batch_size * (hard_it + 1)] = hard_noise.data\n                            all_hard_class[self.batch_size * hard_it : self.batch_size * (hard_it + 1)] = hard_class.data\n                            self.fake_imgs, self.fg_imgs, self.mk_imgs, self.fg_mk = self.netG(hard_noise.detach(), hard_class.detach())\n\n                            fake_logits = self.netsD[2](self.fg_mk[1].detach())\n                            smax_class = softmax(fake_logits[0], dim = 1)\n\n                            for b_it in range(self.batch_size):\t\n                                    all_logits[(self.batch_size * hard_it) + b_it] = smax_class[b_it][my_rand_id[b_it]]\n                            \n                    sorted_val, indices_hard = torch.sort(all_logits)\n                    noise = all_hard_z[indices_hard[0 : self.batch_size]]\n                    self.c_code = all_hard_class[indices_hard[0 : self.batch_size]]\n\n                    self.fake_imgs, self.fg_imgs, self.mk_imgs, self.fg_mk = \\\n                        self.netG(noise, self.c_code)\n\n                    self.p_code = child_to_parent(self.c_code, cfg.FINE_GRAINED_CATEGORIES, cfg.SUPER_CATEGORIES)\n\n                    # Update Discriminator networks\n                    errD_total = 0\n                    for i in range(self.num_Ds):\n                            if i == 0 or i == 2:\n                                    errD = self.train_Dnet(i, count)\n                                    errD_total += errD\n\n                    # Update generator network\n                    errG_total = self.train_Gnet(count)\n\n            for p, avg_p in zip(self.netG.parameters(), avg_param_G):\n                        avg_p.mul_(0.999).add_(0.001, p.data)\n            count = count + 1\n\n            if count % cfg.TRAIN.SNAPSHOT_INTERVAL_HARDNEG == 0:\n\t\tbackup_para = copy_G_params(self.netG)\n                save_model(self.netG, avg_param_G, self.netsD, count+500000, self.model_dir)\n                load_params(self.netG, avg_param_G)\n\t\tself.netG.eval()\n\t\twith torch.set_grad_enabled(False):\n                \tself.fake_imgs, self.fg_imgs, self.mk_imgs, self.fg_mk = \\\n                    \t\tself.netG(fixed_noise, self.c_code)\n                save_img_results(self.imgs_tcpu, (self.fake_imgs + self.fg_imgs + self.mk_imgs + self.fg_mk), self.num_Ds,\n                                 count, self.image_dir, self.summary_writer)\n                self.netG.train()\n                load_params(self.netG, backup_para)\n\n            end_t = time.time()\n\n            if (count % 100) == 0:\n                print(\'\'\'[%d/%d][%d]\n                             Loss_D: %.2f Loss_G: %.2f Time: %.2fs\n                          \'\'\'  \n                      % (count, cfg.TRAIN.HARDNEG_MAX_ITER, self.num_batches,\n                         errD_total.data[0], errG_total.data[0],\n                         end_t - start_t))\n\n            if (count == cfg.TRAIN.HARDNEG_MAX_ITER): # Hard negative training complete\n                    break\n\n        save_model(self.netG, avg_param_G, self.netsD, count, self.model_dir)\n        self.summary_writer.close()\n\n\n\nclass FineGAN_evaluator(object):\n\n    def __init__(self):\n\n        self.save_dir = os.path.join(cfg.SAVE_DIR, \'images\')\n        mkdir_p(self.save_dir)\n        s_gpus = cfg.GPU_ID.split(\',\')\n        self.gpus = [int(ix) for ix in s_gpus]\n        self.num_gpus = len(self.gpus)\n        torch.cuda.set_device(self.gpus[0])\n        cudnn.benchmark = True\n        self.batch_size = cfg.TRAIN.BATCH_SIZE * self.num_gpus\n\n\n    def evaluate_finegan(self):\n        if cfg.TRAIN.NET_G == \'\':\n            print(\'Error: the path for model not found!\')\n        else:\n            # Build and load the generator\n            netG = G_NET()\n            netG.apply(weights_init)\n            netG = torch.nn.DataParallel(netG, device_ids=self.gpus)\n            model_dict = netG.state_dict()\n\n            state_dict = \\\n                torch.load(cfg.TRAIN.NET_G,\n                           map_location=lambda storage, loc: storage)\n\n            state_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n\n            model_dict.update(state_dict)\n            netG.load_state_dict(model_dict)\n            print(\'Load \', cfg.TRAIN.NET_G)\n\n            # Uncomment this to print Generator layers\n            # print(netG)\n            \n            nz = cfg.GAN.Z_DIM\n            noise = torch.FloatTensor(self.batch_size, nz)\n            noise.data.normal_(0, 1)\n\n            if cfg.CUDA:\n                netG.cuda()\n                noise = noise.cuda()\n\n            netG.eval()\n\n            background_class = cfg.TEST_BACKGROUND_CLASS \n            parent_class = cfg.TEST_PARENT_CLASS \n            child_class = cfg.TEST_CHILD_CLASS\n            bg_code = torch.zeros([self.batch_size, cfg.FINE_GRAINED_CATEGORIES])\n            p_code = torch.zeros([self.batch_size, cfg.SUPER_CATEGORIES])\n            c_code = torch.zeros([self.batch_size, cfg.FINE_GRAINED_CATEGORIES])\n\n            for j in range(self.batch_size):\n                bg_code[j][background_class] = 1\n                p_code[j][parent_class] = 1\n                c_code[j][child_class] = 1\n\n            fake_imgs, fg_imgs, mk_imgs, fgmk_imgs = netG(noise, c_code, p_code, bg_code) # Forward pass through the generator\n\n            self.save_image(fake_imgs[0][0], self.save_dir, \'background\')\n            self.save_image(fake_imgs[1][0], self.save_dir, \'parent_final\')\n            self.save_image(fake_imgs[2][0], self.save_dir, \'child_final\')\n            self.save_image(fg_imgs[0][0], self.save_dir, \'parent_foreground\')\n            self.save_image(fg_imgs[1][0], self.save_dir, \'child_foreground\')\n            self.save_image(mk_imgs[0][0], self.save_dir, \'parent_mask\')\n            self.save_image(mk_imgs[1][0], self.save_dir, \'child_mask\')\n            self.save_image(fgmk_imgs[0][0], self.save_dir, \'parent_foreground_masked\')\n            self.save_image(fgmk_imgs[1][0], self.save_dir, \'child_foreground_masked\')\n\n\n    def save_image(self, images, save_dir, iname):\n        \n        img_name = \'%s.png\' % (iname)\n        full_path = os.path.join(save_dir, img_name)\n        \n        if (iname.find(\'mask\') == -1) or (iname.find(\'foreground\') != -1):\n            img = images.add(1).div(2).mul(255).clamp(0, 255).byte()\n            ndarr = img.permute(1, 2, 0).data.cpu().numpy()\n            im = Image.fromarray(ndarr)\n            im.save(full_path)\n\n        else:\n            img = images.mul(255).clamp(0, 255).byte()\n            ndarr = img.data.cpu().numpy()\n            ndarr = np.reshape(ndarr, (ndarr.shape[-1], ndarr.shape[-1], 1))\n            ndarr = np.repeat(ndarr, 3, axis=2)\n            im = Image.fromarray(ndarr)\n            im.save(full_path)\n\n\n'"
code/miscc/__init__.py,0,b'from __future__ import division\nfrom __future__ import print_function\n'
code/miscc/config.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport os.path as osp\nimport numpy as np\nfrom easydict import EasyDict as edict\n\n\n__C = edict()\ncfg = __C\n\n# Dataset name: flowers, birds\n__C.DATASET_NAME = \'birds\'\n__C.CONFIG_NAME = \'\'\n__C.DATA_DIR = \'\'\n__C.SAVE_DIR = \'\'\n__C.GPU_ID = \'0\'\n__C.CUDA = True\n\n__C.WORKERS = 6\n\n__C.TREE = edict()\n__C.TREE.BRANCH_NUM = 3\n__C.TREE.BASE_SIZE = 64\n__C.SUPER_CATEGORIES = 20\n__C.FINE_GRAINED_CATEGORIES = 200\n__C.TEST_CHILD_CLASS = 0\n__C.TEST_PARENT_CLASS = 0\n__C.TEST_BACKGROUND_CLASS = 0\n__C.TIED_CODES = True\n\n# Test options\n__C.TEST = edict()\n\n# Training options\n__C.TRAIN = edict()\n__C.TRAIN.BATCH_SIZE = 64\n__C.TRAIN.BG_LOSS_WT = 10\n__C.TRAIN.VIS_COUNT = 64\n__C.TRAIN.MAX_EPOCH = 600\n__C.TRAIN.HARDNEG_MAX_ITER = 1500\n__C.TRAIN.SNAPSHOT_INTERVAL = 2000\n__C.TRAIN.SNAPSHOT_INTERVAL_HARDNEG = 500\n__C.TRAIN.DISCRIMINATOR_LR = 2e-4\n__C.TRAIN.GENERATOR_LR = 2e-4\n__C.TRAIN.FLAG = True\n__C.TRAIN.NET_G = \'\'\n__C.TRAIN.NET_D = \'\'\n\n\n# Modal options\n__C.GAN = edict()\n__C.GAN.DF_DIM = 64\n__C.GAN.GF_DIM = 64\n__C.GAN.Z_DIM = 100\n__C.GAN.NETWORK_TYPE = \'default\'\n__C.GAN.R_NUM = 2\n\n\n\n\ndef _merge_a_into_b(a, b):\n    """"""Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    """"""\n    if type(a) is not edict:\n        return\n\n    for k, v in a.iteritems():\n        # a must specify keys that are in b\n        if not b.has_key(k):\n            raise KeyError(\'{} is not a valid config key\'.format(k))\n\n        # the types must match, too\n        old_type = type(b[k])\n        if old_type is not type(v):\n            if isinstance(b[k], np.ndarray):\n                v = np.array(v, dtype=b[k].dtype)\n            else:\n                raise ValueError((\'Type mismatch ({} vs. {}) \'\n                                  \'for config key: {}\').format(type(b[k]),\n                                                               type(v), k))\n\n        # recursively merge dicts\n        if type(v) is edict:\n            try:\n                _merge_a_into_b(a[k], b[k])\n            except:\n                print(\'Error under config key: {}\'.format(k))\n                raise\n        else:\n            b[k] = v\n\n\ndef cfg_from_file(filename):\n    """"""Load a config file and merge it into the default options.""""""\n    import yaml\n    with open(filename, \'r\') as f:\n        yaml_cfg = edict(yaml.load(f))\n\n    _merge_a_into_b(yaml_cfg, __C)\n'"
code/miscc/utils.py,0,b'import os\nimport errno\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n'
