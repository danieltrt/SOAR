file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\nwith open(path.join(here, 'README.md'), encoding='utf-8') as f:\n    long_description = f.read()\n\nsetup(\n    name='torchnlp',\n    version='0.1.0',\n    description='Easy to use NLP library built on PyTorch and TorchText',\n    long_description=long_description,\n    url='https://github.com/kolloldas/torchnlp',\n    author='Kollol Das',\n    packages=find_packages(exclude=['tests'])\n)\n\n"""
torchnlp/__init__.py,0,b''
torchnlp/chunk.py,0,"b'""""""\nChunking (Shallow parsing)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\n\nfrom .common.hparams import HParams\nfrom .data.conll import conll2000_dataset\nfrom .tasks.sequence_tagging import TransformerTagger\nfrom .tasks.sequence_tagging import BiLSTMTagger\nfrom .tasks.sequence_tagging import hparams_tagging_base\nfrom .tasks.sequence_tagging import train, evaluate, infer, interactive\n\nfrom .common.prefs import PREFS\nfrom .common.info import Info\n\nimport sys, os\nimport logging\nfrom functools import partial\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\nInfo(__doc__).models(TransformerTagger, BiLSTMTagger).datasets(conll2000_dataset)\n\nPREFS.defaults(\n    data_root=\'./.data/conll2000\',\n    data_train=\'train.txt\',\n    data_test=\'test.txt\',\n    early_stopping=\'highest_5_F1\'\n)\n\n\n# Default dataset is Conll2000\nconll2000 = partial(conll2000_dataset, \n                                    root=PREFS.data_root,\n                                    train_file=PREFS.data_train,\n                                    test_file=PREFS.data_test)\n\n# Hyperparameter configuration for Chunking tasks\n\ndef hparams_transformer_chunk():\n    hparams = hparams_tagging_base()\n    return hparams.update(\n        embedding_size_char=16,\n        embedding_size_char_per_word=100,\n        num_hidden_layers=2,\n        num_heads=4,\n        attention_key_channels=0, # Take hidden size\n        attention_value_channels=0, # Take hidden size\n        filter_size = 128,\n        filter_size_char = 64,\n        input_dropout=0.2,\n        attention_dropout=0.2,\n        relu_dropout=0.2,\n        learning_rate_decay=\'noam_step\',\n        learning_rate_warmup_steps=500,\n        use_crf=True\n    )\n\ndef hparams_lstm_chunk():\n    hparams = hparams_tagging_base()\n\n    return hparams.update(\n        embedding_size_char=25,\n        embedding_size_char_per_word=25,\n        num_hidden_layers=2,\n        hidden_size=100,\n        learning_rate=0.05,\n        learning_rate_decay=\'noam_step\',\n        learning_rate_warmup_steps=100,\n        dropout=0.5,\n        use_crf=True\n    )\n\nhparams_map = {\n    TransformerTagger: hparams_transformer_chunk(),\n    BiLSTMTagger: hparams_lstm_chunk()\n}\n\n# Add HParams mapping\ntrain = partial(train, hparams_map=hparams_map)\n'"
torchnlp/ner.py,0,"b'""""""\nNamed Entity Recognition\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\n\nfrom .common.hparams import HParams\nfrom .data.conll import conll2003_dataset\nfrom .data.nyt import nyt_ingredients_ner_dataset\nfrom .tasks.sequence_tagging import TransformerTagger\nfrom .tasks.sequence_tagging import BiLSTMTagger\nfrom .tasks.sequence_tagging import hparams_tagging_base\nfrom .tasks.sequence_tagging import train, evaluate, infer, interactive\n\nfrom .common.prefs import PREFS\nfrom .common.info import Info\n\nimport sys, os\nimport logging\nfrom functools import partial\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\nInfo(__doc__).models(TransformerTagger, BiLSTMTagger).datasets(conll2003_dataset, nyt_ingredients_ner_dataset)\n\nPREFS.defaults(\n    data_root=\'./.data/conll2003\',\n    data_train=\'eng.train.txt\',\n    data_validation=\'eng.testa.txt\',\n    data_test=\'eng.testb.txt\',\n    early_stopping=\'highest_5_F1\'\n)\n\n\n# Default dataset is Conll2003\nconll2003 = partial(conll2003_dataset, \'ner\', \n                                    root=PREFS.data_root,\n                                    train_file=PREFS.data_train,\n                                    validation_file=PREFS.data_validation,\n                                    test_file=PREFS.data_test)\n\nnyt_ingredients_ner = partial(nyt_ingredients_ner_dataset)                                  \n\n# Hyperparameter configuration for NER tasks\n\ndef hparams_transformer_ner():\n    hparams = hparams_tagging_base()\n    return hparams.update(\n        embedding_size_char=16,\n        embedding_size_char_per_word=100,\n        num_heads=4,\n        attention_key_channels=0, # Take hidden size\n        attention_value_channels=0, # Take hidden size\n        filter_size = 128,\n        filter_size_char = 64,\n        input_dropout=0.2,\n        attention_dropout=0.2,\n        relu_dropout=0.2,\n        learning_rate_decay=\'noam_step\',\n        learning_rate_warmup_steps=500,\n        use_crf=True\n    )\n\ndef hparams_lstm_ner():\n    hparams = hparams_tagging_base()\n\n    return hparams.update(\n        embedding_size_char=25,\n        embedding_size_char_per_word=25,\n        hidden_size=100,\n        learning_rate=0.05,\n        learning_rate_decay=\'noam_step\',\n        learning_rate_warmup_steps=100,\n        num_hidden_layers=1,\n        dropout=0.5,\n        use_crf=True\n    )\n\nhparams_map = {\n    TransformerTagger: hparams_transformer_ner(),\n    BiLSTMTagger: hparams_lstm_ner()\n}\n\n# Add HParams mapping\ntrain = partial(train, hparams_map=hparams_map)\n'"
tests/common/test_evaluation.py,13,"b""from torchnlp.common.evaluation import convert_iob_to_segments\nfrom torchnlp.common.evaluation import Evaluator, Metrics, BasicMetrics, IOBMetrics\nfrom torchnlp.common.model import Model\nfrom torchnlp.common.hparams import HParams\n\nimport torch\n\n\nclass DummyModel(Model):\n    def loss(self, batch, compute_predictions=False):\n        return torch.FloatTensor([1]), 10\n\nclass DummyMetrics(Metrics):\n    def __init__(self, mul):\n        self.mul = mul\n\n    def reset(self):\n        self.total = 0\n        self.count = 0\n        self.batches = 0\n\n    def evaluate(self, batch, loss, predictions):\n        self.total += predictions*self.mul\n        self.count += 1\n        self.batches += batch\n\n    def results(self, total_loss):\n        return {\n            'test_{}'.format(self.mul): self.total/self.count,\n            'batches': self.batches\n        }\n\nclass DummyIter(object):\n    def __init__(self, tot):\n        self.tot = tot\n\n    def init_epoch(self):\n        pass\n\n    def __len__(self):\n        return self.tot\n\n    def __iter__(self):\n        self.count = 0\n        return self\n    \n    def __next__(self):\n        if self.count < self.tot:\n            self.count += 1\n            return self.count\n        else:\n            raise StopIteration\n    next = __next__\n\ndef test_convert_iob_to_segments():\n    tags = ['B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n           'B-LOC', 'I-LOC', 'O', 'B-WE-IRD', 'I-WE-IRD',\n           '<SOS>', '<EOS>', '<UNK>', '<PAD>']\n    tag_mapping = {t:i for i,t in enumerate(tags)}\n\n    pairs = [(['<SOS>', '<EOS>'], \n             set([])),\n\n             (['<SOS>', 'B-PER', '<EOS>'], \n             set([('PER', 1, 1)])),\n\n             (['<SOS>', 'B-PER', 'O', 'B-PER', '<EOS>'], \n             set([('PER', 1, 1), ('PER', 3, 3)])),\n\n             (['<SOS>', 'B-PER', 'B-PER', '<EOS>'], \n             set([('PER', 1, 1), ('PER', 2, 2)])),\n\n             (['<SOS>', 'B-PER', 'I-PER', 'I-PER', '<EOS>', '<PAD>'], \n             set([('PER', 1, 3)])),\n\n             (['<SOS>', 'O', 'O', 'B-LOC', 'I-ORG', '<EOS>'], \n             set([('LOC', 3, 3), ('ORG', 4, 4)])),\n\n             (['<SOS>', 'B-PER', 'I-PER', '<UNK>', 'B-LOC', 'I-LOC', '<EOS>'], \n             set([('PER', 1, 2), ('LOC', 4, 5)])),\n\n             (['<SOS>', 'I-PER', 'I-ORG', 'I-LOC', '<EOS>'], \n             set([('PER', 1, 1), ('ORG', 2, 2), ('LOC', 3, 3)])),\n             \n             (['<SOS>', 'O', 'B-WE-IRD', 'I-WE-IRD', '<EOS>'], \n             set([('WE-IRD', 2, 3)]))]\n\n    for sequence, true_segment in pairs:\n        seq_ids = map(lambda item: tag_mapping[item], sequence)\n        gen_segment = convert_iob_to_segments(seq_ids, tags)\n        assert true_segment == gen_segment\n\n    \ndef test_evaluator():\n    model = DummyModel(hparams=HParams(test=1))\n    data_iter = DummyIter(2)\n    metric_1, metric_2 = DummyMetrics(1), DummyMetrics(2)\n\n    evaluator = Evaluator(data_iter, metric_1, metric_2)\n    result = evaluator.evaluate(model)\n\n    assert isinstance(result, dict)\n    assert result['loss'] == 1\n    assert result['test_1'] == 10\n    assert result['test_2'] == 20\n    assert result['batches'] == 3\n\n    result = evaluator.evaluate(model)\n    assert result['loss'] == 1\n    assert result['test_1'] == 10\n    assert result['test_2'] == 20\n    assert result['batches'] == 3\n    \ndef test_basic_metrics():\n    class Vocab(object):\n        def __init__(self):\n            self.stoi = {'<pad>': 2, '<unk>': 3}\n\n    class Batch(object):\n        def __init__(self):\n            self.labels = torch.LongTensor([[0, 1, 2], [0, 0, 3], \n                                                     [0, 0, 0], [1, 1, 1]])\n\n    predictions_1 = torch.LongTensor([[0, 1, 2], [0, 0, 3], \n                                               [0, 0, 0], [1, 1, 1]])\n\n    loss = torch.FloatTensor([0])\n\n    basic = BasicMetrics(Vocab())\n    basic.reset()\n    basic.evaluate(Batch(), loss, predictions_1)\n    result = basic.results(0)\n\n    assert isinstance(result['acc'], float)\n    assert result['acc'] == 1\n    assert result['acc-seq'] == 1\n\n    predictions_2 = torch.LongTensor([[0, 1, 0], [0, 0, 0], \n                                               [0, 0, 0], [1, 1, 1]])\n    basic.reset()\n    basic.evaluate(Batch(), loss, predictions_2)\n    result = basic.results(0)\n\n    assert isinstance(result['acc'], float)\n    assert result['acc'] == 1\n    assert result['acc-seq'] == 0.5\n\n    predictions_3 = torch.LongTensor([[0, 0, 0], [0, 0, 0], \n                                    [1, 0, 0], [1, 1, 1]])\n    basic.reset()\n    basic.evaluate(Batch(), loss, predictions_3)\n    result = basic.results(0)\n\n    assert isinstance(result['acc'], float)\n    assert result['acc'] == 0.8\n    assert result['acc-seq'] == 0.25\n\ndef test_iob_metrics():\n    class Vocab(object):\n        def __init__(self):\n            self.itos = ['B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n                        'B-LOC', 'I-LOC', 'O',\n                        '<SOS>', '<EOS>', '<UNK>', '<PAD>']\n\n    class Batch(object):\n        def __init__(self):\n            self.labels = torch.LongTensor([[0, 1, 6], [0, 0, 1], \n                                            [0, 2, 3], [6, 6, 6]])\n\n    # All Correct\n    predictions_1 = torch.LongTensor([[0, 1, 6], [0, 0, 1], \n                                    [0, 2, 3], [6, 6, 6]])\n\n    loss = torch.FloatTensor([0])\n\n    iob = IOBMetrics(Vocab())\n    iob.reset()\n    iob.evaluate(Batch(), loss, predictions_1)\n    result = iob.results(0)\n\n    assert isinstance(result['precision'], float)\n    assert isinstance(result['recall'], float)\n    assert isinstance(result['F1'], float)\n    assert result['precision'] == 1\n    assert result['recall'] == 1\n    assert result['F1'] == 1\n\n    # Ignore anything not B/I\n    predictions_2 = torch.LongTensor([[0, 1, 6], [0, 0, 1], \n                                    [0, 2, 3], [9, 9, 9]])\n\n    loss = torch.FloatTensor([0])\n\n    iob = IOBMetrics(Vocab())\n    iob.reset()\n    iob.evaluate(Batch(), loss, predictions_2)\n    result = iob.results(0)\n\n    assert result['precision'] == 1\n    assert result['recall'] == 1\n    assert result['F1'] == 1\n\n    # Lower precision/F1\n    predictions_3 = torch.LongTensor([[0, 0, 10], [0, 0, 1], \n                                        [0, 2, 3], [0, 0, 9]])\n\n    loss = torch.FloatTensor([0])\n\n    iob = IOBMetrics(Vocab())\n    iob.reset()\n    iob.evaluate(Batch(), loss, predictions_3)\n    result = iob.results(0)\n\n    assert result['precision'] == 0.5\n    assert result['recall'] == 0.8\n    assert result['F1'] - 0.6153 < 1e-4\n"""
tests/common/test_model.py,6,"b""from torchnlp.common.model import gen_model_dir, prepare_model_dir\nfrom torchnlp.common.model import Model, HYPERPARAMS_FILE, CHECKPOINT_FILE\n\nfrom torchnlp.common.hparams import HParams\n\nimport torch\nimport torch.nn as nn\nimport os\nfrom time import sleep\n\nclass DummyModel(Model):\n    def __init__(self, hparams=None, extra=None):\n        super(DummyModel, self).__init__(hparams)\n        self.extra = extra\n        self.param = nn.Parameter(torch.LongTensor([0]), requires_grad=False)\n\n    def loss(self, batch):\n        return -1\n\ndef test_gen_model_dir(tmpdir):\n    tmpdir.chdir()\n    model_dir = gen_model_dir('test.Task', Model)\n    assert tmpdir.join('test.Task-Model').fnmatch(model_dir)\n    assert os.path.exists(model_dir)\n\ndef test_prepare_model_dir(tmpdir):\n    tmpdir.chdir()\n    sub = tmpdir.mkdir('model')\n\n    # Test clearing\n    sub.join('dummy.pt').write('x')\n    prepare_model_dir(str(sub), True)\n    assert len(sub.listdir()) == 0\n\n    # Test rename\n    tmpdir.mkdir('model-1')\n    sub.join('dummy.pt').write('x')\n    prepare_model_dir(str(sub), False)\n    assert sub.check()\n    assert len(sub.listdir()) == 0\n    \n    assert tmpdir.join('model-1').check()\n    assert len(tmpdir.join('model-1').listdir()) == 0\n\n    assert tmpdir.join('model-2').check()\n    assert tmpdir.join('model-2').join('dummy.pt').check()\n\ndef test_create_model(tmpdir):\n    tmpdir.chdir()\n\n    model = DummyModel.create('test.Task', HParams(test=21), extra=111)\n    assert isinstance(model, DummyModel)\n    assert hasattr(model, 'hparams')\n    assert model.hparams.test == 21\n    assert model.extra == 111\n\ndef test_load_model(tmpdir):\n    tmpdir.chdir()\n    sub = tmpdir.mkdir('test.Task-DummyModel')\n\n    torch.save(HParams(test=22), str(sub.join(HYPERPARAMS_FILE)))\n    assert sub.join(HYPERPARAMS_FILE).check()\n\n    torch.save(DummyModel(HParams(test=20)).state_dict(), str(sub.join(CHECKPOINT_FILE.format(1))))\n    assert sub.join(CHECKPOINT_FILE.format(1)).check()\n\n    sleep(1) # To ensure different file mtimes\n\n    dummy_model = DummyModel(HParams(test=21))\n    dummy_model.param += 1\n    torch.save(dummy_model.state_dict(), str(sub.join(CHECKPOINT_FILE.format(2))))\n    assert sub.join(CHECKPOINT_FILE.format(2)).check()\n\n    model, _ = DummyModel.load('test.Task', checkpoint=-1, extra=111)\n    assert isinstance(model, DummyModel)\n    assert hasattr(model, 'hparams')\n    assert model.hparams.test == 22\n    assert model.extra == 111\n    assert int(model.param) == 1\n\ndef test_save_model(tmpdir):\n    tmpdir.chdir()\n\n    dummy_model = DummyModel.create('test.Task', HParams(test=21))\n    dummy_model.param += 1\n    dummy_model.iterations += 100\n\n    dummy_model.save('test.Task')\n    sub = tmpdir.join('test.Task-DummyModel')\n    assert sub.check()\n    assert sub.join(CHECKPOINT_FILE.format(100)).check()\n    assert sub.join(HYPERPARAMS_FILE).check()\n\n    hparams = torch.load(str(sub.join(HYPERPARAMS_FILE)))\n    assert isinstance(hparams, HParams)\n    assert hparams.test == 21\n"""
tests/common/test_train.py,2,"b""from torchnlp.common.train import Trainer\nfrom torchnlp.common.model import Model, CHECKPOINT_FILE\nfrom torchnlp.common.hparams import HParams, hparams_basic\n\nimport torch\nimport torch.nn as nn\n\nclass DummyModel(Model):\n    def __init__(self, hparams=None):\n        super(DummyModel, self).__init__(hparams)\n        self.dummy_param = nn.Parameter(torch.FloatTensor([1]))\n\n    def loss(self, batch):\n        return self.dummy_param, 0\n\nclass DummyIter(object):\n    def __init__(self, tot):\n        self.tot = tot\n\n    def init_epoch(self):\n        pass\n\n    def __len__(self):\n        return self.tot\n\n    def __iter__(self):\n        self.count = 0\n        return self\n    \n    def __next__(self):\n        if self.count < self.tot:\n            self.count += 1\n            return self.count\n        else:\n            raise StopIteration\n    next = __next__\n\ndef test_get_early_stopping_criteria(tmpdir):\n    tmpdir.chdir()\n    model = DummyModel(hparams=hparams_basic())\n    trainer = Trainer('test.Task', model, model.hparams, DummyIter(1), None)\n    best_fn, window, metric = trainer._get_early_stopping_criteria('lowest_3_loss')\n\n    assert window == 3\n    assert metric == 'loss'\n    assert best_fn([(1, 2), (3, 4)])[1] == 2\n\n\ndef test_early_stopping(tmpdir):\n    tmpdir.chdir()\n    model = DummyModel(hparams=hparams_basic())\n\n    class Evaluator(object):\n        def __init__(self):\n            self.losses = [\n                100, 50, 20, 15, 14, 13, 12, 10, 10,\n                11, 9, 8, 9, 10, 10, 12, 9, 9, 11\n            ]\n            self.count = -1\n\n        def evaluate(self, model):\n            self.count += 1\n            return {'loss': self.losses[self.count]}\n\n    trainer = Trainer('test.Task', model, model.hparams, DummyIter(2), Evaluator())\n    best_iteration, _ = trainer.train(20, early_stopping='lowest_5_loss')\n    \n    assert best_iteration == 24\n    assert tmpdir.join('test.Task-DummyModel').join(CHECKPOINT_FILE.format(best_iteration)).check()"""
tests/modules/test_crf.py,21,"b'from torchnlp.modules.crf import CRF\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\ndef test_sequence_score():\n    crf = CRF(3)\n    crf.transitions = nn.Parameter(torch.Tensor([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]]))\n\n    crf.start_transitions = nn.Parameter(torch.Tensor([\n        2, 0, 0\n    ]))\n\n    crf.stop_transitions = nn.Parameter(torch.Tensor([\n        1, 1, 1\n    ]))\n\n    feats = torch.Tensor([\n        [\n            [0, 0, 1],\n            [1, 1, 1],\n            [2, 3, 2]\n        ],\n        [\n            [0, 0, 2],\n            [0, 1, 0],\n            [1, 1, 1]\n        ]])\n    tags = torch.LongTensor([\n        [0, 1, 2],\n        [0, 2, 1]\n    ])\n\n    scores = crf._sequence_score(feats, tags)\n    assert scores.shape[0] == 2\n\n    feat_score = torch.Tensor([0+1+2, 0+0+1])\n    trans_score = torch.Tensor([2+6, 3+8])\n    start_score = torch.Tensor([2, 2])\n    stop_score = torch.Tensor([1, 1])\n\n    assert list(scores.data) == list((feat_score + trans_score + start_score + stop_score).data)\n\ndef test_log_sum_exp():\n    logits = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n    crf = CRF(3)\n    lse = crf._log_sum_exp(logits, -1)\n    assert lse.shape[0] == 2\n    assert (lse == logits.exp().sum(-1).log()).all()\n\ndef test_partition_function():\n    crf = CRF(3)\n    crf.transitions = nn.Parameter(torch.Tensor([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]]))\n\n    crf.start_transitions = nn.Parameter(torch.Tensor([\n        2, 0, 0\n    ]))\n\n    crf.stop_transitions = nn.Parameter(torch.Tensor([\n        1, 1, 1\n    ]))\n\n    feats = torch.Tensor([\n        [\n            [0, 0, 1],\n            [1, 1, 1],\n            [2, 3, 2]\n        ],\n        [\n            [0, 0, 2],\n            [0, 1, 0],\n            [1, 1, 1]\n        ]])\n    \n    # Monkey patch _log_sum_exp\n    crf._log_sum_exp = lambda logits, dim: logits.sum(dim)\n    scores = crf._partition_function(feats)\n    """"""\n    2, 0, 1\n    3+12+3, 3+15+3, 3+18+3 = 18, 21, 24\n    63+12+6, 63+15+9, 63+18+6 = 81, 87, 87\n    82+88+88 = 258\n\n    2, 0, 2\n    4+12+0, 4+15+3, 4+18+0 = 16, 22, 22\n    60+12+3, 60+15+3, 60+18+3 = 75, 78, 81\n    76+79+82 = 237\n    """"""\n    assert scores.shape[0] == 2\n    print(scores.data)\n    assert (scores.data == torch.Tensor([258, 237])).all()\n    \ndef test_viterbi():\n    crf = CRF(3)\n    crf.transitions = nn.Parameter(torch.Tensor([\n        [1, 2, 9],\n        [4, 8, 6],\n        [7, 5, 3]]))\n\n    crf.start_transitions = nn.Parameter(torch.Tensor([\n        2, 0, 0\n    ]))\n\n    crf.stop_transitions = nn.Parameter(torch.Tensor([\n        1, 1, 1\n    ]))\n\n    feats = torch.Tensor([\n        [\n            [0, 0, 1],\n            [1, 1, 1],\n            [2, 3, 2]\n        ],\n        [\n            [0, 0, 2],\n            [0, 1, 0],\n            [1, 1, 1]\n        ]])\n\n    tags = crf._viterbi(feats)\n    """"""\n    2, 0, 1\n    8+1, 8+1, 11+1 | 2, 1, 0 = 9, 9, 12\n    19+2, 17+3, 18+2 | 2, 1, 0 = 21, 20, 20\n    22, 21, 21 | 0 <- 2 <- 0\n    \n    2, 0, 2\n    9+0, 8+1, 11+0 | 2, 1, 0 = 9, 9, 11\n    18+1, 17+1, 18+1 | 2, 1, 0 = 19, 18, 19\n    20, 19, 20 | 0 <- 2 <- 0\n    \n    """"""\n    assert tags.shape == (2, 3)\n    print(tags.data)\n    assert (tags.data == torch.LongTensor([[0, 2, 0],[0, 2, 0]])).all()'"
tests/modules/test_outputs.py,4,"b'from torchnlp.modules.outputs import SoftmaxOutputLayer, CRFOutputLayer\n\nimport torch\nimport numpy as np\n\ndef test_softmax_output_layer():\n    hidden = torch.randn(2, 3, 4)\n    labels = torch.ones(2, 3).long()\n    softmax = SoftmaxOutputLayer(4, 8)\n    loss = softmax.loss(hidden, labels)\n    assert loss > 0\n    pred = softmax(hidden)\n    assert pred.shape == (2, 3)\n\ndef test_crf_output_layer():\n    hidden = torch.randn(2, 3, 4)\n    labels = torch.ones(2, 3).long()\n    crf = CRFOutputLayer(4, 8)\n    loss = crf.loss(hidden, labels)\n    assert loss > 0\n    pred = crf(hidden)\n    assert pred.shape == (2, 3)'"
tests/modules/test_transformer.py,10,"b""from torchnlp.modules.transformer import Encoder, Decoder\nfrom torchnlp.modules.transformer.layers import EncoderLayer, DecoderLayer\nfrom torchnlp.modules.transformer.sublayers import MultiHeadAttention, PositionwiseFeedForward\n\nimport torch\nimport numpy as np\n\ndef test_multi_head_attention():\n    mask = (torch.from_numpy(np.triu(np.ones([10, 10])*float('-Inf'), 1))\n        .type(torch.FloatTensor)\n        .unsqueeze(0).unsqueeze(1))\n    mha = MultiHeadAttention(32, 64, 32, 32, 4, bias_mask=mask)\n    t = torch.randn(10, 5, 32)\n    output = mha(t, t, t)\n    assert output.shape == t.shape\n\n\ndef test_positionwise_feed_forward():\n    pwff = PositionwiseFeedForward(32, 64, 16, layer_config='cl')\n    assert len(pwff.layers) == 2\n    t = torch.randn(10, 5, 32)\n    output = pwff(t)\n    assert output.shape == (10, 5, 16)\n\ndef test_encoder_layer():\n    el = EncoderLayer(32, 64, 32, 64, 4)\n    t = torch.randn(10, 5, 32)\n    output = el(t)\n    assert t.shape == output.shape\n\ndef test_decoder_layer():\n    el = EncoderLayer(32, 64, 32, 64, 4)\n    te = torch.randn(10, 5, 32)\n    enc_output = el(te)\n\n    dl = DecoderLayer(32, 64, 32, 64, 4, None)\n    td = torch.randn(10, 5, 32)\n    dec_output, _ = dl((td, enc_output))\n\n    assert td.shape == dec_output.shape\n\n\ndef test_encoder():\n    e = Encoder(16, 32, 2, 2, 0, 0, 16)\n    te = torch.randn(10, 6, 16)\n    enc_output = e(te)\n\n    assert len(e.enc) == 2\n    assert enc_output.shape == (10, 6, 32)\n\ndef test_decoder():\n    e = Encoder(16, 32, 2, 2, 0, 0, 16)\n    te = torch.randn(10, 6, 16)\n    enc_output = e(te)\n\n    d = Decoder(8, 32, 3, 4, 0, 0, 8)\n    td = torch.randn(10, 5, 8)\n    dec_output = d(td, enc_output)\n\n    assert len(d.dec) == 3\n    assert dec_output.shape == (10, 5, 32)"""
tests/tasks/test_sequence_tagging.py,3,"b'from torchnlp.tasks.sequence_tagging import Tagger, hparams_tagging_base, VOCABS_FILE\n\nimport torch\nimport torch.nn as nn\n\nimport torchtext\nfrom torchtext import data\nfrom torchtext import datasets\n\nimport pytest\n\ndef udpos_dataset(batch_size):\n    # Setup fields with batch dimension first\n    inputs = data.Field(init_token=""<bos>"", eos_token=""<eos>"", batch_first=True)\n    tags = data.Field(init_token=""<bos>"", eos_token=""<eos>"", batch_first=True)\n    \n    # Download and the load default data.\n    train, val, test = datasets.UDPOS.splits(\n    fields=((\'inputs_word\', inputs), (\'labels\', tags), (None, None)))\n    \n    # Build vocab\n    inputs.build_vocab(train.inputs)\n    tags.build_vocab(train.tags)\n    \n    # Get iterators\n    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n                            (train, val, test), batch_size=batch_size, \n                            device=torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu""))\n    train_iter.repeat = False\n    return train_iter, val_iter, test_iter, inputs, tags\n\nclass DummyTagger(Tagger):\n    def __init__(self, hparams, **kwargs):\n        super(DummyTagger, self).__init__(hparams=hparams, **kwargs)\n        self.linear = nn.Linear(hparams.embedding_size_word, \n                                hparams.hidden_size)\n\n    def compute(self, inputs_word_emb, inputs_char_emb):\n        return self.linear(inputs_word_emb)\n\n@pytest.mark.slow\ndef test_tagger(tmpdir):\n    tmpdir.chdir()\n    hparams = hparams_tagging_base()\n    train_iter, val_iter, test_iter, inputs, tags = udpos_dataset(hparams.batch_size)\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    tagger = DummyTagger(hparams=hparams, vocabs=(inputs.vocab, None, tags.vocab)).to(device)\n    assert tagger.embedding_word.weight.shape == (len(inputs.vocab), hparams.embedding_size_word)\n    assert tagger.output_layer.output_projection.weight.shape == (len(tags.vocab), hparams.hidden_size)\n\n    batch = next(iter(val_iter))\n    loss, preds = tagger.loss(batch, compute_predictions=True)\n\n    assert loss > 0\n    assert preds.data.shape == batch.labels.data.shape\n\n@pytest.mark.slow\ndef test_tagger_create(tmpdir):\n    tmpdir.chdir()\n    hparams = hparams_tagging_base()\n    train_iter, val_iter, test_iter, inputs, tags = udpos_dataset(hparams.batch_size)\n\n    tagger = DummyTagger.create(\'test.Task\', hparams=hparams, vocabs=(inputs.vocab, None, tags.vocab))\n    assert isinstance(tagger, DummyTagger)\n    assert tmpdir.join(\'test.Task-DummyTagger\').join(VOCABS_FILE).check()\n\n@pytest.mark.slow\ndef test_tagger_load(tmpdir):\n    tmpdir.chdir()\n    hparams = hparams_tagging_base()\n    train_iter, val_iter, test_iter, inputs, tags = udpos_dataset(hparams.batch_size)\n\n    tagger = DummyTagger.create(\'test.Task\', hparams=hparams, vocabs=(inputs.vocab, None, tags.vocab))\n    tagger.iterations += 10\n    tagger.save(\'test.Task\')\n\n    tagger_load, _ = DummyTagger.load(\'test.Task\', checkpoint=-1)\n    assert isinstance(tagger_load.vocab_tags, torchtext.vocab.Vocab)'"
torchnlp/common/__init__.py,0,b''
torchnlp/common/cmd.py,0,"b'import sys\nimport re\n\ndef run_cmd(default=None, **kwargs):\n    """"""\n    Simple command runner\n    """"""\n    if len(sys.argv) > 1:\n        m = re.match(r\'^--(.+)\', sys.argv[1])\n        if m and m.group(1) in kwargs:\n            kwargs[m.group(1)]()\n    elif default is not None:\n        default()'"
torchnlp/common/evaluation.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nfrom functools import reduce\nfrom tqdm import tqdm\n\nclass Evaluator(object):\n    """"""\n    A class to evaluate a dataset on multiple\n    metrics. Uses generator tricks to pass batch predictions\n    to all the metric functions without recomputing\n    """"""\n    def __init__(self, data_iter, *metrics):\n        """"""\n        Parameters:\n            data_iter: Dataset iterator derived from torchtext.data.Iterator\n            metric_fns: List of metric objects extending the Metric class\n        """"""\n        self.data_iter = data_iter\n        self.metrics = metrics or []\n\n    def evaluate(self, model):\n        """"""\n        Evaluates the model on the given metrics for a dataset.\n        Returns: A dict containing all the metrics\n        """"""\n        self.data_iter.init_epoch()\n        model.eval()\n        for m in self.metrics:\n            m.reset()\n\n        total_loss = 0\n\n        prog_iter = tqdm(self.data_iter, leave=False)\n        with torch.no_grad():\n            for batch in prog_iter:\n                loss, predictions = model.loss(batch, compute_predictions=True)\n                for m in self.metrics:\n                    m.evaluate(batch, loss, predictions)\n                total_loss += float(loss)\n                \n                prog_iter.set_description(\'Evaluating\')\n            \n            results = {\'loss\': total_loss/len(self.data_iter)}\n            for m in self.metrics:\n                r = m.results(total_loss)\n                if not isinstance(r, dict):\n                    raise ValueError(\'{}.results() should return a dict containing metrics\'.format(m.__class__.__name__))\n                results.update(r)\n\n        return results\n\nclass Metrics(object):\n    """"""\n    Base metrics class that Evaluator calls to evaluate a metric on a dataset.\n    Derived classes should implement the reset(), evaluate() and results() methods\n    """"""\n    def reset(self):\n        """"""\n        Called to reset counters, accumulators etc\n        """"""\n        raise NotImplementedError(\'{} must implement reset()\'.format(self.__class__.__name__))\n\n    def evaluate(self, batch, loss, prediction):\n        """"""\n        Called to evaluate metrics for one batch\n        Parameters:\n            batch: A single minibatch. Instance of torchtext.data.Batch\n            loss: Loss Tensor\n            prediction: Model prediction\n        """"""\n        raise NotImplementedError(\'{} must implement evaluate()\'.format(self.__class__.__name__))\n\n\n    def results(self, total_loss):\n        """"""\n        Called to retrieve the final metrics.\n        Parameters:\n            total_loss: Total loss on the dataset\n        Return:\n            Must return a dict mapping metric names to values\n        """"""\n\nclass BasicMetrics(Metrics):\n    """"""\n    A basic metrics class that computes symbol\n    and sequence accuracy. It is meant to be passed to the Evaluator\n    """"""\n    def __init__(self, output_vocab, ignore_symbols=[\'<unk>\', \'<pad>\']):\n        """"""\n        Parameters:\n            output_vocab: Instance of torchtext.Vocab\n            ignore_symbols: List of symbols to ignore while calculating symbol accuracy\n                            e.g. padding, unknown etc\n        """"""\n        self.ignore_ids = [output_vocab.stoi[s] for s in (ignore_symbols or [])]\n    \n    def reset(self):\n        self.total_correct_examples = 0\n        self.total_correct_symbols = 0\n        self.total_symbols = 0\n        self.total_examples = 0\n\n    def evaluate(self, batch, loss, predictions):\n        eq = torch.eq(predictions, batch.labels).type_as(loss)\n        self.total_correct_examples += int(eq.prod(1).sum())\n        self.total_examples += batch.labels.shape[0]\n\n        # Mark elements to discard as 1\n        discard = reduce(lambda x, y: x + y, \n                         map(lambda ignore: ignore == batch.labels, self.ignore_ids))\n        # Elements which stayed zero are the ones we want\n        mask = (discard == 0).float()\n\n        # Mask correct predictions to remove ignored ones\n        self.total_correct_symbols += int((eq * mask).sum())\n        # Also track total symbols\n        self.total_symbols += int(mask.sum())\n        \n\n    def results(self, total_loss):\n        """"""\n        Returns: A dict containing the following:\n            loss: Total loss on the dataset\n            acc: Symbol accuracy\n            acc-seq: Sequence accuracy\n        """"""\n        return {\n            \'acc\': self.total_correct_symbols/self.total_symbols,\n            \'acc-seq\': self.total_correct_examples/self.total_examples\n        }\n\n\ndef convert_iob_to_segments(sequence, mapping, begin_tag=\'B\', inside_tag=\'I\'):\n    """"""\n    Convert an IOB tag sequence to segments (O tags are excluded). Used\n    by iob_metrics_fn\n    Parameters:\n        sequence: List of tag ids\n        mapping: Maps ids to tags (B/I prefix + O etc.)\n        begin_tag: Label for begin tag (default \'B\')\n        inside_tag: Label for inside tag (default \'I\')\n\n    Returns:\n        A set of segment tuples of the form \n        (segment value, segment start index, segment end index)\n    """"""\n    segments = []\n    segment, segment_start, segment_end = None, None, None\n    relevant_tags = set([begin_tag, inside_tag])\n\n    for i, tok in enumerate(map(lambda s: mapping[s].split(\'-\', 1), sequence)):\n        if tok[0] in relevant_tags:\n            if segment is None:\n                segment, segment_start = tok[1], i\n            elif tok[0] == begin_tag or tok[1] != segment:\n                segments.append((segment, segment_start, segment_end))\n                segment, segment_start = tok[1], i\n            segment_end = i\n\n    if segment is not None:\n        segments.append((segment, segment_start, segment_end))\n\n    return set(segments)\n\nclass IOBMetrics(Metrics):\n    """"""\n    Compute precision, recall, F1\n    for sequence tagging tasks following IOB schema. Meant\n    to be passed to the Evaluator only\n    """"""\n    def __init__(self, tag_vocab):\n        """"""\n        Parameters:\n            tag_vocab: Instance of torchtext.Vocab\n        """"""\n        self.tag_mappings = tag_vocab.itos\n    \n    def reset(self):\n        self.correct_seg_count = 0 \n        self.pred_seg_count = 0 \n        self.total_seg_count = 0\n\n    def evaluate(self, batch, loss, predictions):\n        for j in range(predictions.shape[0]):\n            pred_indices = list(predictions[j, :].data)\n            target_indices = list(batch.labels[j, :].data)\n            \n            pred_segs = convert_iob_to_segments(pred_indices, self.tag_mappings)\n            target_segs = convert_iob_to_segments(target_indices, self.tag_mappings)\n\n            correct_segs = pred_segs & target_segs\n\n            self.correct_seg_count += len(correct_segs)\n            self.pred_seg_count += len(pred_segs)\n            self.total_seg_count += len(target_segs)\n\n    def results(self, total_loss):\n        """"""\n        Returns: A dict containing the following:\n            precision: Tag precision\n            recall: Tag recall\n            F1: Tag F1\n        """"""\n        precision, recall, f1 = 0, 0, 0\n        if self.correct_seg_count > 0:\n            precision = self.correct_seg_count / self.pred_seg_count\n            recall = self.correct_seg_count / self.total_seg_count\n            f1 = 2 * precision * recall / (precision + recall)\n\n        return {\n            \'precision\': precision,\n            \'recall\': recall,\n            \'F1\': f1\n        }\n'"
torchnlp/common/hparams.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport torch.nn as nn\nimport numpy as np\n\nclass HParams(object):\n    """"""\n    Holds arbitrary hyperparameters. Converts dict key-values to objects\n    """"""\n    def __init__(self, **kwargs):\n        self.__dict__ = kwargs\n        \n    def add(self, **kwargs):\n        self.__dict__.update(kwargs)\n        \n    def update(self, **kwargs):\n        self.__dict__.update(kwargs)\n        return self\n\n    def save(self, path):\n        with open(path, \'w\') as f:\n            json.dump(self.__dict__, f)\n        return self\n\n    def __repr__(self):\n        return \'\\nHyperparameters:\\n\' + \'\\n\'.join([\' {}={}\'.format(k, v) for k,v in self.__dict__.items()])\n\n    @classmethod\n    def load(cls, path):\n        with open(path, \'r\') as f:\n            return cls(**json.load(f))\n\n\ndef hparams_basic():\n    return HParams(\n        batch_size=100,\n        learning_rate=0.2,\n        learning_rate_decay=None,\n        optimizer_adam_beta1=0.9,\n        optimizer_adam_beta2=0.98,\n    )'"
torchnlp/common/info.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nclass Info(object):\n    """"""\n    Displays information about models and datasets.\n    Currently a stub\n    """"""\n    def __init__(self, docstring):\n        print(\'\\nTask:\', docstring.strip())\n\n    def models(self, *args):\n        print(\'\\nAvailable models:\')\n        print(\'-------------------\')\n        print(\'\\n\'.join(\'{}\\n{}\'.format(m.__name__, m.__doc__) for m in args))\n        \n        return self\n\n    def datasets(self, *args):\n        print(\'\\nAvailable datasets:\')\n        print(\'-------------------\')\n        print(\'\\n\'.join(d.__doc__.split(\'\\n\')[1] for d in args if d.__doc__))\n        print()\n        return self\n'"
torchnlp/common/model.py,8,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\n\nimport os\nimport glob\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nHYPERPARAMS_FILE = \'hyperparams.pt\'\nCHECKPOINT_FILE = \'checkpoint-{}.pt\'\nCHECKPOINT_GLOB = \'checkpoint-*.pt\'\n\ndef gen_model_dir(task_name, model_cls):\n    """"""\n    Generate the model directory from the task name and model class.\n    Creat if not exists. \n    Parameters:\n        task_name: Name of the task. Gets prefixed to model directory\n        model_cls: The models class (derived from Model)\n    """"""\n    model_dir = os.path.join(os.getcwd(), \'%s-%s\'%(task_name, model_cls.__name__))\n    if not os.path.exists(model_dir):\n        os.mkdir(model_dir)\n    return model_dir\n\ndef prepare_model_dir(model_dir, clear=False):\n    """"""\n    Prepares the model directory. If clear is set to True, deletes all files, else\n    renames existing directory and creates a fresh one\n    Parameters:\n        model_dir: Absolute path of the model directory to prepare\n    """"""\n    p = list(os.walk(model_dir))\n\n    # Check if directory is not empty (ignore subdirectories)\n    if clear:\n        for file in os.listdir(model_dir):\n            # Delete all files\n            path = os.path.join(model_dir, file)\n            if os.path.isfile(path):\n                try:\n                    os.remove(path)\n                except:\n                    logger.warning(\'WARNING: Failed to delete {}\'.format(path))\n    elif len(p[0][2]) > 0:\n        # Rename to available directory\n        # Max index is 10\n        for i in range(1, 10):\n            try:\n                rename_dir = \'{}-{}\'.format(model_dir, i)\n                if not os.path.exists(rename_dir):\n                    os.rename(model_dir, rename_dir)\n                    os.mkdir(model_dir)\n                    break\n            except:\n                pass\n        \n\ndef xavier_uniform_init(m):\n    """"""\n    Xavier initializer to be used with model.apply\n    """"""\n    if type(m) == nn.Linear: # TODO: Add Embeddings?\n        nn.init.xavier_uniform_(m.weight.data)\n\nclass Model(nn.Module):\n    """"""\n    Abstract base class that defines a loss() function\n    """"""\n    def __init__(self, hparams=None):\n        super(Model, self).__init__()\n        \n        if hparams is None:\n            raise ValueError(\'Must provide hparams\')\n        \n        self.hparams = hparams\n\n        # Track total iterations\n        self.iterations = nn.Parameter(torch.LongTensor([0]), requires_grad=False)\n\n    def loss(self, batch, compute_predictions=False):\n        """"""\n        Called by train.Trainer to compute negative log likelihood\n        Parameters:\n            batch: A minibatch, instance of torchtext.data.Batch\n            compute_predictions: If true compute and provide predictions, else None\n        Returns:\n            Tuple of loss and predictions\n        """"""\n        raise NotImplementedError(""Must implement loss()"")\n\n    @classmethod\n    def create(cls, task_name, hparams, overwrite=False, **kwargs):\n        """"""\n        Create a new instance of this class. Prepares the model directory\n        and saves hyperparams. Derived classes should override this function\n        to save other dependencies (e.g. vocabs)\n        """"""\n        logger.info(hparams)\n        model_dir = gen_model_dir(task_name, cls)\n        model = cls(hparams, **kwargs)\n        # Xavier initialization\n        model.apply(xavier_uniform_init)\n\n        if torch.cuda.is_available():\n            model = model.cuda()\n\n        prepare_model_dir(model_dir, overwrite)\n\n        #Save hyperparams\n        torch.save(hparams, os.path.join(model_dir, HYPERPARAMS_FILE))\n\n        return model\n\n    @classmethod\n    def load(cls, task_name, checkpoint, **kwargs):\n        """"""\n        Loads a model from a checkpoint. Also loads hyperparams\n        Parameters:\n            task_name: Name of the task. Needed to determine model dir\n            checkpoint: Number indicating the checkpoint. -1 to load latest\n            **kwargs: Additional key-value args passed to constructor\n        """"""\n        model_dir = gen_model_dir(task_name, cls)\n        hparams_path = os.path.join(model_dir, HYPERPARAMS_FILE)\n\n        if not os.path.exists(hparams_path):\n            raise OSError(\'HParams file not found\')\n\n        hparams = torch.load(hparams_path)\n        logger.info(\'Hyperparameters: {}\'.format(str(hparams)))\n\n        model = cls(hparams, **kwargs)\n        if torch.cuda.is_available():\n            model = model.cuda()\n\n        if checkpoint == -1:\n            # Find latest checkpoint file\n            files = glob.glob(os.path.join(model_dir, CHECKPOINT_GLOB))\n            if not files:\n                raise OSError(\'Checkpoint files not found\')\n            files.sort(key=os.path.getmtime, reverse=True)\n            checkpoint_path = files[0]\n        else:\n            checkpoint_path = os.path.join(model_dir, CHECKPOINT_FILE.format(checkpoint))\n            if not os.path.exists(checkpoint_path):\n                raise OSError(\'File not found: {}\'.format(checkpoint_path))\n\n        logger.info(\'Loading from {}\'.format(checkpoint_path))\n        # Load the model\n        model.load_state_dict(torch.load(checkpoint_path))\n\n        return model, hparams\n\n    def save(self, task_name):\n        """"""\n        Save the model. Directory is determined by the task name and model class name\n        """"""\n        model_dir = gen_model_dir(task_name, self.__class__)\n        checkpoint_path = os.path.join(model_dir, CHECKPOINT_FILE.format(int(self.iterations)))\n        torch.save(self.state_dict(), checkpoint_path)\n        logger.info(\'-------------- Saved checkpoint {} --------------\'.format(int(self.iterations)))\n\n    def set_latest(self, task_name, iteration):\n        """"""\n        Set the modified time of the checkpoint to latest. Used to set the best\n        checkpoint\n        """"""\n        model_dir = gen_model_dir(task_name, self.__class__)\n        checkpoint_path = os.path.join(model_dir, CHECKPOINT_FILE.format(int(iteration)))\n        os.utime(checkpoint_path, None)\n'"
torchnlp/common/prefs.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport json\n\nclass Prefs(object):\n    """"""\n    Stores auto-saving preferences\n    """"""\n    path = \'./prefs.json\'\n    def __init__(self, **kwargs):\n        if os.path.exists(Prefs.path):\n            with open(Prefs.path, \'r\')  as f:\n                super(Prefs, self).__setattr__(\'prefs\', json.load(f))\n        else:\n            super(Prefs, self).__setattr__(\'prefs\', kwargs)\n            \n    def defaults(self, **kwargs):\n        self.prefs.update(kwargs)\n\n    def __repr__(self):\n        return \'\\n\'.join(\'{}={}\'.format(k, v) for k,v in self.prefs.items())\n\n    def __getattr__(self, name):\n        return self.prefs[name]\n    \n    def __setattr__(self, name, value):\n        \n        self.prefs[name] = value\n        \n        # Serialize\n        with open(Prefs.path, \'w\') as f:\n            json.dump(self.prefs, f)\n\n\n# Add default preferences here\nPREFS = Prefs(\n    overwrite_model_dir = False\n)'"
torchnlp/common/train.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.optim as optim\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom .model import gen_model_dir\n\nimport os\nfrom functools import partial\nfrom collections import deque, defaultdict\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nOPTIMIZER_FILE = ""optimizer.pt""\n\n# Decay functions to be used with lr_scheduler\ndef lr_decay_noam(hparams):\n    return lambda t: (\n        10.0 * hparams.hidden_size**-0.5 * min(\n        (t + 1) * hparams.learning_rate_warmup_steps**-1.5, (t + 1)**-0.5))\n\ndef lr_decay_exp(hparams):\n    return lambda t: hparams.learning_rate_falloff ** t\n\n\n# Map names to lr decay functions\nlr_decay_map = {\n    \'noam\': lr_decay_noam,\n    \'exp\': lr_decay_exp\n}\n\n        \ndef compute_num_params(model):\n    """"""\n    Computes number of trainable and non-trainable parameters\n    """"""\n    sizes = [(np.array(p.data.size()).prod(), int(p.requires_grad)) for p in model.parameters()]\n    return sum(map(lambda t: t[0]*t[1],sizes)), sum(map(lambda t: t[0]*(1 - t[1]),sizes))\n\nclass Trainer(object):\n    """"""\n    Class to handle training in a task-agnostic way\n    """"""\n    def __init__(self, task_name, model, hparams, train_iter, evaluator):\n        """"""\n        Parameters:\n            task_name: Name of the task\n            model: Model instance (derived from model.Model)\n            hparams: Instance of HParams\n            train_iter: An instance of torchtext.data.Iterator\n            evaluator: Instance of evalutation.Evaluator that will\n                        run metrics on the validation dataset\n        """"""\n\n        self.task_name = task_name\n        self.model = model\n        self.hparams = hparams\n        self.evaluator = evaluator\n\n        self.train_iter = train_iter\n        # Disable repetitions\n        self.train_iter.repeat = False\n\n        model_params = filter(lambda p: p.requires_grad, model.parameters())\n        \n        # TODO: Add support for other optimizers\n        self.optimizer = optim.Adam(\n                            model_params,\n                            betas=(hparams.optimizer_adam_beta1, hparams.optimizer_adam_beta2), \n                            lr=hparams.learning_rate)\n\n        self.opt_path = os.path.join(gen_model_dir(task_name, model.__class__), \n                                    OPTIMIZER_FILE)\n\n        # If model is loaded from a checkpoint restore optimizer also\n        if int(model.iterations) > 0:\n            self.optimizer.load_state_dict(torch.load(self.opt_path))\n\n        self.lr_scheduler_step = self.lr_scheduler_epoch = None\n\n        # Set up learing rate decay scheme\n        if hparams.learning_rate_decay is not None:\n            if \'_\' not in hparams.learning_rate_decay:\n                raise ValueError(""Malformed learning_rate_decay"")\n            lrd_scheme, lrd_range = hparams.learning_rate_decay.split(\'_\')\n\n            if lrd_scheme not in lr_decay_map:\n                raise ValueError(""Unknown lr decay scheme {}"".format(lrd_scheme))\n            \n            lrd_func = lr_decay_map[lrd_scheme]            \n            lr_scheduler = optim.lr_scheduler.LambdaLR(\n                                            self.optimizer, \n                                            lrd_func(hparams),\n                                            last_epoch=int(self.model.iterations) or -1\n                                        )\n            # For each scheme, decay can happen every step or every epoch\n            if lrd_range == \'epoch\':\n                self.lr_scheduler_epoch = lr_scheduler\n            elif lrd_range == \'step\':\n                self.lr_scheduler_step = lr_scheduler\n            else:\n                raise ValueError(""Unknown lr decay range {}"".format(lrd_range))\n\n        # Display number of parameters\n        logger.info(\'Parameters: {}(trainable), {}(non-trainable)\'.format(*compute_num_params(self.model)))\n    \n    def _get_early_stopping_criteria(self, early_stopping):\n        es = early_stopping.split(\'_\')\n        if len(es) != 3:\n            raise ValueError(\'Malformed early stopping criteria\')\n        best_type, window, metric = es\n        logger.info(\'Early stopping for {} value of validation {} after {} epochs\'\n                    .format(best_type, metric, window))\n        \n        if best_type == \'lowest\':\n            best_fn = partial(min, key=lambda item: item[0])\n        elif best_type == \'highest\':\n            best_fn = partial(max, key=lambda item: item[0])\n        else:\n            raise ValueError(\'Unknown best type {}\'.format(best_type))\n\n        return best_fn, int(window), metric\n\n    def train(self, num_epochs, early_stopping=None, save=True):\n        """"""\n        Run the training loop for given number of epochs. The model\n        is evaluated at the end of every epoch and saved as well\n        Parameters:\n            num_epochs: Total number of epochs to run\n            early_stopping: A string indicating how to perform early stopping\n                Should be of the form lowest/highest_n_metric where:\n                    lowest/highest: Track lowest or highest values\n                    n: The window size within which to track best\n                    metric: Name of the metric to track. Should be available\n                        in the dict returned by evaluator\n            save: Save model every epoch if true\n        Returns:\n            Tuple of best checkpoint number and metrics array (for plotting etc)\n        """"""\n\n        all_metrics = defaultdict(list)\n        best_iteration = 0\n\n        if early_stopping:\n            if not save:\n                raise ValueError(\'save should be True for early stopping\')\n            if self.evaluator is None:\n                raise ValueError(\'early stopping requires an eval function\')\n\n            best_fn, best_window, best_metric_name = self._get_early_stopping_criteria(early_stopping)\n            tracking = deque([], best_window + 1)\n\n\n        for epoch in range(num_epochs):\n            self.train_iter.init_epoch()\n            epoch_loss = 0\n            count = 0\n\n            logger.info(\'Epoch %d (%d)\'%(epoch + 1, int(self.model.iterations)))\n\n            prog_iter = tqdm(self.train_iter, leave=False)\n            for batch in prog_iter:\n                # Train mode\n                self.model.train()\n\n                self.optimizer.zero_grad()\n                loss, _ = self.model.loss(batch)\n                loss.backward()\n                self.optimizer.step()\n\n                if self.lr_scheduler_step:\n                    self.lr_scheduler_step.step()\n\n                epoch_loss += loss.item()\n                count += 1\n                self.model.iterations += 1\n\n                # Display loss\n                prog_iter.set_description(\'Training\')\n                prog_iter.set_postfix(loss=(epoch_loss/count))\n\n            \n            if self.lr_scheduler_epoch:\n                self.lr_scheduler_epoch.step()\n\n            train_loss = epoch_loss/count\n            all_metrics[\'train_loss\'].append(train_loss)\n            logger.info(\'Train Loss: {:3.5f}\'.format(train_loss))\n\n            best_iteration = int(self.model.iterations)\n\n            # Run evaluation\n            if self.evaluator:\n                eval_metrics = self.evaluator.evaluate(self.model)\n                if not isinstance(eval_metrics, dict):\n                    raise ValueError(\'eval_fn should return a dict of metrics\')\n\n                # Display eval metrics\n                logger.info(\'Validation metrics: \')\n                logger.info(\', \'.join([\'{}={:3.5f}\'.format(k, v) for k,v in eval_metrics.items()]))\n\n                # Append metrics\n                for k, v in eval_metrics.items():\n                    all_metrics[k].append(v)\n\n                # Handle early stopping\n                tracking.append((eval_metrics[best_metric_name], int(self.model.iterations), epoch))\n                logger.debug(\'Epoch {} Tracking: {}\'.format(epoch, tracking))\n                \n                if epoch >= best_window:\n                    # Get the best value of metric in the window\n                    best_metric, best_iteration, best_epoch = best_fn(tracking)\n                    if tracking[0][1] == best_iteration:\n                        # The best value has gone outside the desired window\n                        # hence stop\n                        logger.info(\'Early stopping at iteration {}, epoch {}, {}={:3.5f}\'\n                                    .format(best_iteration, best_epoch, best_metric_name, best_metric))\n                        # Update the file time of that checkpoint file to latest\n                        self.model.set_latest(self.task_name, best_iteration)\n                        break\n                \n            if save:\n                self.model.save(self.task_name)\n                torch.save(self.optimizer.state_dict(), self.opt_path)\n\n        return best_iteration, all_metrics            \n\n            \n                '"
torchnlp/data/__init__.py,0,b''
torchnlp/data/conll.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nfrom torchtext import data\nfrom torchtext.datasets import SequenceTaggingDataset, CoNLL2000Chunking\nfrom torchtext.vocab import Vectors, GloVe, CharNGram\n\nimport numpy as np\nimport random\nimport logging\nlogger = logging.getLogger(__name__)\n\n\ndef conll2003_dataset(tag_type, batch_size, root=\'./conll2003\', \n                          train_file=\'eng.train.txt\', \n                          validation_file=\'eng.testa.txt\',\n                          test_file=\'eng.testb.txt\',\n                          convert_digits=True):\n    """"""\n    conll2003: Conll 2003 (Parser only. You must place the files)\n    Extract Conll2003 dataset using torchtext. Applies GloVe 6B.200d and Char N-gram\n    pretrained vectors. Also sets up per word character Field\n    Parameters:\n        tag_type: Type of tag to pick as task [pos, chunk, ner]\n        batch_size: Batch size to return from iterator\n        root: Dataset root directory\n        train_file: Train filename\n        validation_file: Validation filename\n        test_file: Test filename\n        convert_digits: If True will convert numbers to single 0\'s\n\n    Returns:\n        A dict containing:\n            task: \'conll2003.\' + tag_type\n            iters: (train iter, validation iter, test iter)\n            vocabs: (Inputs word vocabulary, Inputs character vocabulary, \n                    Tag vocabulary )\n    """"""\n    \n    # Setup fields with batch dimension first\n    inputs_word = data.Field(init_token=""<bos>"", eos_token=""<eos>"", batch_first=True, lower=True,\n                                preprocessing=data.Pipeline(\n                                    lambda w: \'0\' if convert_digits and w.isdigit() else w ))\n\n    inputs_char_nesting = data.Field(tokenize=list, init_token=""<bos>"", eos_token=""<eos>"", \n                                    batch_first=True)\n\n    inputs_char = data.NestedField(inputs_char_nesting, \n                                    init_token=""<bos>"", eos_token=""<eos>"")\n                        \n\n    labels = data.Field(init_token=""<bos>"", eos_token=""<eos>"", batch_first=True)\n\n    fields = ([((\'inputs_word\', \'inputs_char\'), (inputs_word, inputs_char))] + \n                [(\'labels\', labels) if label == tag_type else (None, None) \n                for label in [\'pos\', \'chunk\', \'ner\']])\n\n    # Load the data\n    train, val, test = SequenceTaggingDataset.splits(\n                                path=root, \n                                train=train_file, \n                                validation=validation_file, \n                                test=test_file,\n                                separator=\' \',\n                                fields=tuple(fields))\n\n    logger.info(\'---------- CONLL 2003 %s ---------\'%tag_type)\n    logger.info(\'Train size: %d\'%(len(train)))\n    logger.info(\'Validation size: %d\'%(len(val)))\n    logger.info(\'Test size: %d\'%(len(test)))\n    \n    # Build vocab\n    inputs_char.build_vocab(train.inputs_char, val.inputs_char, test.inputs_char)\n    inputs_word.build_vocab(train.inputs_word, val.inputs_word, test.inputs_word, max_size=50000,\n                        vectors=[GloVe(name=\'6B\', dim=\'200\'), CharNGram()])\n    \n    labels.build_vocab(train.labels)\n    logger.info(\'Input vocab size:%d\'%(len(inputs_word.vocab)))\n    logger.info(\'Tagset size: %d\'%(len(labels.vocab)))\n\n    # Get iterators\n    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n                            (train, val, test), batch_size=batch_size, \n                            device=torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu""))\n    train_iter.repeat = False\n    \n    return {\n        \'task\': \'conll2003.%s\'%tag_type,\n        \'iters\': (train_iter, val_iter, test_iter), \n        \'vocabs\': (inputs_word.vocab, inputs_char.vocab, labels.vocab) \n        }\n    \n\ndef conll2000_dataset(batch_size, use_local=False, root=\'.data/conll2000\',\n                            train_file=\'train.txt\',\n                            test_file=\'test.txt\',\n                            validation_frac=0.1,\n                            convert_digits=True):\n    """"""\n    conll2000: Conll 2000 (Chunking)\n    Extract Conll2000 Chunking dataset using torchtext. By default will fetch\n    data files from online repository.\n    Applies GloVe 6B.200d and Char N-gram pretrained vectors. Also sets \n    up per word character Field\n    Parameters:\n        batch_size: Batch size to return from iterator\n        use_local: If True use local provided files (default False)\n        root (optional): Dataset root directory (needed only if use_local is True)\n        train_file (optional): Train filename (needed only if use_local is True)\n        test_file (optional): Test filename (needed only if use_local is True)\n        validation_frac (optional): Fraction of train dataset to use for validation\n        convert_digits (optional): If True will convert numbers to single 0\'s\n    NOTE: Since there is only a train and test set we use 10% of the train set as\n        validation\n    Returns:\n        A dict containing:\n            task: \'conll2000.\' + tag_type\n            iters: (train iter, validation iter, None)\n            vocabs: (Inputs word vocabulary, Inputs character vocabulary, \n                    Tag vocabulary )\n    """"""\n    \n    # Setup fields with batch dimension first\n    inputs_word = data.Field(init_token=""<bos>"", eos_token=""<eos>"", batch_first=True, lower=True,\n                                preprocessing=data.Pipeline(\n                                    lambda w: \'0\' if convert_digits and w.isdigit() else w ))\n\n    inputs_char_nesting = data.Field(tokenize=list, init_token=""<bos>"", eos_token=""<eos>"", \n                                    batch_first=True)\n\n    inputs_char = data.NestedField(inputs_char_nesting, \n                                    init_token=""<bos>"", eos_token=""<eos>"")\n                        \n\n    labels = data.Field(init_token=""<bos>"", eos_token=""<eos>"", batch_first=True)\n\n    fields = [((\'inputs_word\', \'inputs_char\'), (inputs_word, inputs_char)),\n                (None, None), (\'labels\', labels)]\n\n    if use_local:\n        # Load the data\n        train, test = SequenceTaggingDataset.splits(\n                                    path=root, \n                                    train=train_file,\n                                    test=test_file,\n                                    fields=tuple(fields))\n\n        # HACK: Saving the sort key function as the split() call removes it\n        sort_key = train.sort_key\n        # To make the split deterministic\n        random.seed(0)\n        train, val = train.split(1 - validation_frac, random_state=random.getstate())\n        # Reset the seed\n        random.seed()\n\n        # HACK: Set the sort key\n        train.sort_key = sort_key\n        val.sort_key = sort_key\n    else:\n        train, val, test = CoNLL2000Chunking.splits(fields=tuple(fields), \n                                                    validation_frac=validation_frac)\n\n    logger.info(\'---------- CONLL 2000 Chunking ---------\')\n    logger.info(\'Train size: %d\'%(len(train)))\n    logger.info(\'Validation size: %d\'%(len(val)))\n    logger.info(\'Test size: %d\'%(len(test)))\n    \n    # Build vocab\n    inputs_char.build_vocab(train.inputs_char, val.inputs_char, test.inputs_char)\n    inputs_word.build_vocab(train.inputs_word, val.inputs_word, test.inputs_word, max_size=50000,\n                        vectors=[GloVe(name=\'6B\', dim=\'200\'), CharNGram()])\n    \n    labels.build_vocab(train.labels)\n    logger.info(\'Input vocab size:%d\'%(len(inputs_word.vocab)))\n    logger.info(\'Tagset size: %d\'%(len(labels.vocab)))\n\n    # Get iterators\n    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n                            (train, val, test), batch_size=batch_size, \n                            device=torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu""))\n    train_iter.repeat = False\n    \n    return {\n        \'task\': \'conll2000.chunk\',\n        \'iters\': (train_iter, val_iter, test_iter), \n        \'vocabs\': (inputs_word.vocab, inputs_char.vocab, labels.vocab) \n        }'"
torchnlp/data/inputs.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nfrom torchtext import data\n\nimport numpy as np\nimport logging\nlogger = logging.getLogger(__name__)\n\ndef get_input_processor_words(vocab_word, vocab_char=None, convert_digits=True):\n    """"""\n    Returns a function that converts text into a processed batch. Required duing\n    inference.\n    Parameters:\n        vocab_word: Instance of torchtext.Vocab for input word vocabulary\n        vocab_char[optional]: Instance of torchtext.Vocab for input per-word \n                              character vocabulary\n        convert_digits: If True will convert numbers to single 0\'s\n    """"""\n    inputs_word = data.Field(init_token=""<bos>"", eos_token=""<eos>"", batch_first=True, lower=True,\n                                preprocessing=data.Pipeline(\n                                    lambda w: \'0\' if convert_digits and w.isdigit() else w ))\n    # Set the vocab object manually without building from training dataset\n    inputs_word.vocab = vocab_word\n\n    if vocab_char is not None:\n        inputs_char_nesting = data.Field(tokenize=list, init_token=""<bos>"", eos_token=""<eos>"", \n                                        batch_first=True)\n\n        inputs_char = data.NestedField(inputs_char_nesting, \n                                        init_token=""<bos>"", eos_token=""<eos>"")\n        # Set the vocab object manually without building from training dataset\n        inputs_char.vocab = inputs_char_nesting.vocab = vocab_char\n        \n        fields = [((\'inputs_word\', \'inputs_char\'), (inputs_word, inputs_char))]\n    else:\n        fields = [(\'inputs_word\', inputs_word)]\n\n\n    def input_processor_fn(inputs):\n        if not isinstance(inputs, list):\n            inputs = [inputs]\n\n        examples = []\n        for line in inputs:\n            examples.append(data.Example.fromlist([line], fields))\n        \n        dataset = data.Dataset(examples, fields)\n        # Entire input in one batch\n        return data.Batch(data=dataset, \n                          dataset=dataset,\n                          device=torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu""))\n\n    return input_processor_fn'"
torchnlp/data/nyt.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nfrom torchtext import data\nfrom torchtext.datasets import SequenceTaggingDataset\nfrom torchtext.vocab import Vectors, GloVe, CharNGram\n\nimport numpy as np\nimport random\nimport logging\nlogger = logging.getLogger(__name__)\n\nclass Ingredients(SequenceTaggingDataset):\n\n    # New York Times ingredients dataset\n    # Download original at https://github.com/NYTimes/ingredient-phrase-tagger\n    \n    urls = [\'https://raw.githubusercontent.com/kolloldas/torchnlp/master/data/nyt/nyt_ingredients_ner.zip\']\n    dirname = \'\'\n    name = \'nyt_ingredients_ner\'\n\n    @classmethod\n    def splits(cls, fields, root="".data"", train=""train.txt"",\n               validation=""valid.txt"",\n               test=""test.txt"", **kwargs):\n        """"""Downloads and loads the NYT ingredients NER data in CoNLL format\n        """"""\n\n        return super(Ingredients, cls).splits(\n            fields=fields, root=root, train=train, validation=validation,\n            test=test, **kwargs)\n\n\ndef nyt_ingredients_ner_dataset(batch_size, use_local=False, root=\'.data/nyt_ingredients_ner\', \n                          train_file=\'train.txt\', \n                          validation_file=\'valid.txt\',\n                          test_file=\'test.txt\',\n                          convert_digits=True):\n    """"""\n    nyt_ingredients_ner: New York Times Ingredient tagging dataset\n    Extract NYT ingredients dataset using torchtext. Applies GloVe 6B.200d and Char N-gram\n    pretrained vectors. Also sets up per word character Field\n    Parameters:\n        batch_size: Batch size to return from iterator\n        use_local: If True use local provided files (default False)\n        root: Dataset root directory\n        train_file: Train filename\n        validation_file: Validation filename\n        test_file: Test filename\n        convert_digits: If True will convert numbers to single 0\'s\n\n    Returns:\n        A dict containing:\n            task: \'nyt_ingredients.ner\'\n            iters: (train iter, validation iter, test iter)\n            vocabs: (Inputs word vocabulary, Inputs character vocabulary, \n                    Tag vocabulary )\n    """"""\n    \n    # Setup fields with batch dimension first\n    inputs_word = data.Field(init_token=""<bos>"", eos_token=""<eos>"", batch_first=True, lower=True,\n                                preprocessing=data.Pipeline(\n                                    lambda w: \'0\' if convert_digits and w.isdigit() else w ))\n\n    inputs_char_nesting = data.Field(tokenize=list, init_token=""<bos>"", eos_token=""<eos>"", \n                                    batch_first=True)\n\n    inputs_char = data.NestedField(inputs_char_nesting, \n                                    init_token=""<bos>"", eos_token=""<eos>"")\n                        \n\n    labels = data.Field(init_token=""<bos>"", eos_token=""<eos>"", batch_first=True)\n\n    fields = ([((\'inputs_word\', \'inputs_char\'), (inputs_word, inputs_char)), \n                (\'labels\', labels)])\n\n    # Load the data\n    if use_local:\n        train, val, test = SequenceTaggingDataset.splits(\n                                    path=root, \n                                    train=train_file, \n                                    validation=validation_file, \n                                    test=test_file,\n                                    fields=tuple(fields))\n    else:\n        train, val, test = Ingredients.splits(fields=tuple(fields))\n\n    logger.info(\'---------- NYT INGREDIENTS NER ---------\')\n    logger.info(\'Train size: %d\'%(len(train)))\n    logger.info(\'Validation size: %d\'%(len(val)))\n    logger.info(\'Test size: %d\'%(len(test)))\n    \n    # Build vocab\n    inputs_char.build_vocab(train.inputs_char, val.inputs_char, test.inputs_char)\n    inputs_word.build_vocab(train.inputs_word, val.inputs_word, test.inputs_word, max_size=50000,\n                        vectors=[GloVe(name=\'6B\', dim=\'200\'), CharNGram()])\n    \n    labels.build_vocab(train.labels)\n    logger.info(\'Input vocab size:%d\'%(len(inputs_word.vocab)))\n    logger.info(\'Tagset size: %d\'%(len(labels.vocab)))\n\n    # Get iterators\n    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n                            (train, val, test), batch_size=batch_size, \n                            device=torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu""))\n    train_iter.repeat = False\n    \n    return {\n        \'task\': \'nyt_ingredients.ner\',\n        \'iters\': (train_iter, val_iter, test_iter), \n        \'vocabs\': (inputs_word.vocab, inputs_char.vocab, labels.vocab) \n        }\n    '"
torchnlp/modules/__init__.py,0,b'from . import transformer'
torchnlp/modules/crf.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CRF(nn.Module):\n    """"""\n    Implements Conditional Random Fields that can be trained via\n    backpropagation. \n    """"""\n    def __init__(self, num_tags):\n        super(CRF, self).__init__()\n        \n        self.num_tags = num_tags\n        self.transitions = nn.Parameter(torch.Tensor(num_tags, num_tags))\n        self.start_transitions = nn.Parameter(torch.randn(num_tags))\n        self.stop_transitions = nn.Parameter(torch.randn(num_tags))\n\n        nn.init.xavier_normal_(self.transitions)\n\n    def forward(self, feats):\n        # Shape checks\n        if len(feats.shape) != 3:\n            raise ValueError(""feats must be 3-d got {}-d"".format(feats.shape))\n\n        return self._viterbi(feats)\n\n    def loss(self, feats, tags):\n        """"""\n        Computes negative log likelihood between features and tags.\n        Essentially difference between individual sequence scores and \n        sum of all possible sequence scores (partition function)\n        Parameters:\n            feats: Input features [batch size, sequence length, number of tags]\n            tags: Target tag indices [batch size, sequence length]. Should be between\n                    0 and num_tags\n        Returns:\n            Negative log likelihood [a scalar] \n        """"""\n        # Shape checks\n        if len(feats.shape) != 3:\n            raise ValueError(""feats must be 3-d got {}-d"".format(feats.shape))\n\n        if len(tags.shape) != 2:\n            raise ValueError(\'tags must be 2-d but got {}-d\'.format(tags.shape))\n\n        if feats.shape[:2] != tags.shape:\n            raise ValueError(\'First two dimensions of feats and tags must match\')\n\n        sequence_score = self._sequence_score(feats, tags)\n        partition_function = self._partition_function(feats)\n        log_probability = sequence_score - partition_function\n\n        # -ve of l()\n        # Average across batch\n        return -log_probability.mean()\n\n    def _sequence_score(self, feats, tags):\n        """"""\n        Parameters:\n            feats: Input features [batch size, sequence length, number of tags]\n            tags: Target tag indices [batch size, sequence length]. Should be between\n                    0 and num_tags\n        Returns: Sequence score of shape [batch size]\n        """"""\n\n        batch_size = feats.shape[0]\n\n        # Compute feature scores\n        feat_score = feats.gather(2, tags.unsqueeze(-1)).squeeze(-1).sum(dim=-1)\n\n        # Compute transition scores\n        # Unfold to get [from, to] tag index pairs\n        tags_pairs = tags.unfold(1, 2, 1)\n\n        # Use advanced indexing to pull out required transition scores\n        indices = tags_pairs.permute(2, 0, 1).chunk(2)\n        trans_score = self.transitions[indices].squeeze(0).sum(dim=-1)\n\n        # Compute start and stop scores\n        start_score = self.start_transitions[tags[:, 0]]\n        stop_score = self.stop_transitions[tags[:, -1]]\n\n        return feat_score + start_score + trans_score + stop_score\n\n    def _partition_function(self, feats):\n        """"""\n        Computes the partitition function for CRF using the forward algorithm.\n        Basically calculate scores for all possible tag sequences for \n        the given feature vector sequence\n        Parameters:\n            feats: Input features [batch size, sequence length, number of tags]\n        Returns:\n            Total scores of shape [batch size]\n        """"""\n        _, seq_size, num_tags = feats.shape\n\n        if self.num_tags != num_tags:\n            raise ValueError(\'num_tags should be {} but got {}\'.format(self.num_tags, num_tags))\n\n        a = feats[:, 0] + self.start_transitions.unsqueeze(0) # [batch_size, num_tags]\n        transitions = self.transitions.unsqueeze(0) # [1, num_tags, num_tags] from -> to\n\n        for i in range(1, seq_size):\n            feat = feats[:, i].unsqueeze(1) # [batch_size, 1, num_tags]\n            a = self._log_sum_exp(a.unsqueeze(-1) + transitions + feat, 1) # [batch_size, num_tags]\n\n        return self._log_sum_exp(a + self.stop_transitions.unsqueeze(0), 1) # [batch_size]\n\n    def _viterbi(self, feats):\n        """"""\n        Uses Viterbi algorithm to predict the best sequence\n        Parameters:\n            feats: Input features [batch size, sequence length, number of tags]\n        Returns: Best tag sequence [batch size, sequence length]\n        """"""\n        _, seq_size, num_tags = feats.shape\n\n        if self.num_tags != num_tags:\n            raise ValueError(\'num_tags should be {} but got {}\'.format(self.num_tags, num_tags))\n        \n        v = feats[:, 0] + self.start_transitions.unsqueeze(0) # [batch_size, num_tags]\n        transitions = self.transitions.unsqueeze(0) # [1, num_tags, num_tags] from -> to\n        paths = []\n\n        for i in range(1, seq_size):\n            feat = feats[:, i] # [batch_size, num_tags]\n            v, idx = (v.unsqueeze(-1) + transitions).max(1) # [batch_size, num_tags], [batch_size, num_tags]\n            \n            paths.append(idx)\n            v = (v + feat) # [batch_size, num_tags]\n\n        \n        v, tag = (v + self.stop_transitions.unsqueeze(0)).max(1, True)\n\n        # Backtrack\n        tags = [tag]\n        for idx in reversed(paths):\n            tag = idx.gather(1, tag)\n            tags.append(tag)\n\n        tags.reverse()\n        return torch.cat(tags, 1)\n\n    \n    def _log_sum_exp(self, logits, dim):\n        """"""\n        Computes log-sum-exp in a stable way\n        """"""\n        max_val, _ = logits.max(dim)\n        return max_val + (logits - max_val.unsqueeze(dim)).exp().sum(dim).log()\n        '"
torchnlp/modules/normalization.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\n\nclass LayerNorm(nn.Module):\n    # Borrowed from jekbradbury\n    # https://github.com/pytorch/pytorch/issues/1959\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta'"
torchnlp/modules/outputs.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .crf import CRF\n\nclass OutputLayer(nn.Module):\n    """"""\n    Abstract base class for output layer. \n    Handles projection to output labels\n    """"""\n    def __init__(self, hidden_size, output_size):\n        super(OutputLayer, self).__init__()\n        self.output_size = output_size\n        self.output_projection = nn.Linear(hidden_size, output_size)\n\n    def loss(self, hidden, labels):\n        raise NotImplementedError(\'Must implement {}.loss\'.format(self.__class__.__name__))\n\n\nclass SoftmaxOutputLayer(OutputLayer):\n    """"""\n    Implements a softmax based output layer\n    """"""\n    def forward(self, hidden):\n        logits = self.output_projection(hidden)\n        probs = F.softmax(logits, -1)\n        _, predictions = torch.max(probs, dim=-1)\n\n        return predictions\n\n    def loss(self, hidden, labels):\n        logits = self.output_projection(hidden)\n        log_probs = F.log_softmax(logits, -1)\n        return F.nll_loss(log_probs.view(-1, self.output_size), labels.view(-1))\n\nclass CRFOutputLayer(OutputLayer):\n    """"""\n    Implements a CRF based output layer\n    """"""\n    def __init__(self, hidden_size, output_size):\n        super(CRFOutputLayer, self).__init__(hidden_size, output_size)\n        self.crf = CRF(output_size)\n\n    def forward(self, hidden):\n        feats = self.output_projection(hidden)\n        return self.crf(feats)\n\n    def loss(self, hidden, labels):\n        feats = self.output_projection(hidden)\n        return self.crf.loss(feats, labels)'"
torchnlp/tasks/__init__.py,0,b'from . import sequence_tagging'
torchnlp/modules/transformer/__init__.py,0,"b'from .main import Encoder, Decoder'"
torchnlp/modules/transformer/layers.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\nfrom .sublayers import MultiHeadAttention, PositionwiseFeedForward\nfrom ..normalization import LayerNorm\n\nclass EncoderLayer(nn.Module):\n    """"""\n    Represents one Encoder layer of the Transformer Encoder\n    Refer Fig. 1 in https://arxiv.org/pdf/1706.03762.pdf\n    NOTE: The layer normalization step has been moved to the input as per latest version of T2T\n    """"""\n    def __init__(self, hidden_size, total_key_depth, total_value_depth, filter_size, num_heads,\n                 bias_mask=None, layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0):\n        """"""\n        Parameters:\n            hidden_size: Hidden size\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            filter_size: Hidden size of the middle layer in FFN\n            num_heads: Number of attention heads\n            bias_mask: Masking tensor to prevent connections to future elements\n            layer_dropout: Dropout for this layer\n            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n        """"""\n        \n        super(EncoderLayer, self).__init__()\n        \n        self.multi_head_attention = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth, \n                                                       hidden_size, num_heads, bias_mask, attention_dropout)\n        \n        self.positionwise_feed_forward = PositionwiseFeedForward(hidden_size, filter_size, hidden_size,\n                                                                 layer_config=\'cc\', padding = \'both\', \n                                                                 dropout=relu_dropout)\n        self.dropout = nn.Dropout(layer_dropout)\n        self.layer_norm_mha = LayerNorm(hidden_size)\n        self.layer_norm_ffn = LayerNorm(hidden_size)\n        \n    def forward(self, inputs):\n        x = inputs\n        \n        # Layer Normalization\n        x_norm = self.layer_norm_mha(x)\n        \n        # Multi-head attention\n        y = self.multi_head_attention(x_norm, x_norm, x_norm)\n        \n        # Dropout and residual\n        x = self.dropout(x + y)\n        \n        # Layer Normalization\n        x_norm = self.layer_norm_ffn(x)\n        \n        # Positionwise Feedforward\n        y = self.positionwise_feed_forward(x_norm)\n        \n        # Dropout and residual\n        y = self.dropout(x + y)\n        \n        return y\n\nclass DecoderLayer(nn.Module):\n    """"""\n    Represents one Decoder layer of the Transformer Decoder\n    Refer Fig. 1 in https://arxiv.org/pdf/1706.03762.pdf\n    NOTE: The layer normalization step has been moved to the input as per latest version of T2T\n    """"""\n    def __init__(self, hidden_size, total_key_depth, total_value_depth, filter_size, num_heads,\n                 bias_mask, layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0):\n        """"""\n        Parameters:\n            hidden_size: Hidden size\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            filter_size: Hidden size of the middle layer in FFN\n            num_heads: Number of attention heads\n            bias_mask: Masking tensor to prevent connections to future elements\n            layer_dropout: Dropout for this layer\n            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n        """"""\n        \n        super(DecoderLayer, self).__init__()\n        \n        self.multi_head_attention_dec = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth, \n                                                       hidden_size, num_heads, bias_mask, attention_dropout)\n\n        self.multi_head_attention_enc_dec = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth, \n                                                       hidden_size, num_heads, None, attention_dropout)\n        \n        self.positionwise_feed_forward = PositionwiseFeedForward(hidden_size, filter_size, hidden_size,\n                                                                 layer_config=\'cc\', padding = \'left\', \n                                                                 dropout=relu_dropout)\n        self.dropout = nn.Dropout(layer_dropout)\n        self.layer_norm_mha_dec = LayerNorm(hidden_size)\n        self.layer_norm_mha_enc = LayerNorm(hidden_size)\n        self.layer_norm_ffn = LayerNorm(hidden_size)\n        \n    def forward(self, inputs):\n        """"""\n        NOTE: Inputs is a tuple consisting of decoder inputs and encoder output\n        """"""\n        x, encoder_outputs = inputs\n        \n        # Layer Normalization before decoder self attention\n        x_norm = self.layer_norm_mha_dec(x)\n        \n        # Masked Multi-head attention\n        y = self.multi_head_attention_dec(x_norm, x_norm, x_norm)\n        \n        # Dropout and residual after self-attention\n        x = self.dropout(x + y)\n\n        # Layer Normalization before encoder-decoder attention\n        x_norm = self.layer_norm_mha_enc(x)\n\n        # Multi-head encoder-decoder attention\n        y = self.multi_head_attention_enc_dec(x_norm, encoder_outputs, encoder_outputs)\n        \n        # Dropout and residual after encoder-decoder attention\n        x = self.dropout(x + y)\n        \n        # Layer Normalization\n        x_norm = self.layer_norm_ffn(x)\n        \n        # Positionwise Feedforward\n        y = self.positionwise_feed_forward(x_norm)\n        \n        # Dropout and residual after positionwise feed forward layer\n        y = self.dropout(x + y)\n        \n        # Return encoder outputs as well to work with nn.Sequential\n        return y, encoder_outputs\n'"
torchnlp/modules/transformer/main.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport math\n\nfrom .layers import EncoderLayer, DecoderLayer\nfrom ..normalization import LayerNorm\n\ndef _gen_bias_mask(max_length):\n    """"""\n    Generates bias values (-Inf) to mask future timesteps during attention\n    """"""\n    np_mask = np.triu(np.full([max_length, max_length], -np.inf), 1)\n    torch_mask = torch.from_numpy(np_mask).type(torch.FloatTensor)\n    \n    return torch_mask.unsqueeze(0).unsqueeze(1)\n\ndef _gen_timing_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n    """"""\n    Generates a [1, length, channels] timing signal consisting of sinusoids\n    Adapted from:\n    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n    """"""\n    position = np.arange(length)\n    num_timescales = channels // 2\n    log_timescale_increment = (\n                    math.log(float(max_timescale) / float(min_timescale)) /\n                    (float(num_timescales) - 1))\n    inv_timescales = min_timescale * np.exp(\n                    np.arange(num_timescales).astype(np.float) * -log_timescale_increment)\n    scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)\n\n\n    signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)\n    signal = np.pad(signal, [[0, 0], [0, channels % 2]], \n                    \'constant\', constant_values=[0.0, 0.0])\n    signal =  signal.reshape([1, length, channels])\n\n    return torch.from_numpy(signal).type(torch.FloatTensor)\n\nclass Encoder(nn.Module):\n    """"""\n    A Transformer Encoder module. \n    Inputs should be in the shape [batch_size, length, hidden_size]\n    Outputs will have the shape [batch_size, length, hidden_size]\n    Refer Fig.1 in https://arxiv.org/pdf/1706.03762.pdf\n    """"""\n    def __init__(self, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n                 filter_size, max_length=100, input_dropout=0.0, layer_dropout=0.0, \n                 attention_dropout=0.0, relu_dropout=0.0, use_mask=False):\n        """"""\n        Parameters:\n            embedding_size: Size of embeddings\n            hidden_size: Hidden size\n            num_layers: Total layers in the Encoder\n            num_heads: Number of attention heads\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            filter_size: Hidden size of the middle layer in FFN\n            max_length: Max sequence length (required for timing signal)\n            input_dropout: Dropout just after embedding\n            layer_dropout: Dropout for each layer\n            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n            use_mask: Set to True to turn on future value masking\n        """"""\n        \n        super(Encoder, self).__init__()\n        \n        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n        \n        params =(hidden_size, \n                 total_key_depth or hidden_size,\n                 total_value_depth or hidden_size,\n                 filter_size, \n                 num_heads, \n                 _gen_bias_mask(max_length) if use_mask else None,\n                 layer_dropout, \n                 attention_dropout, \n                 relu_dropout)\n        \n        self.embedding_proj = nn.Linear(embedding_size, hidden_size, bias=False)\n        self.enc = nn.Sequential(*[EncoderLayer(*params) for l in range(num_layers)])\n        \n        self.layer_norm = LayerNorm(hidden_size)\n        self.input_dropout = nn.Dropout(input_dropout)\n        \n    \n    def forward(self, inputs):\n        #Add input dropout\n        x = self.input_dropout(inputs)\n        \n        # Project to hidden size\n        x = self.embedding_proj(x)\n        \n        # Add timing signal\n        x += self.timing_signal[:, :inputs.shape[1], :].type_as(inputs.data)\n        \n        y = self.enc(x)\n        \n        y = self.layer_norm(y)\n        return y\n\nclass Decoder(nn.Module):\n    """"""\n    A Transformer Decoder module. \n    Inputs should be in the shape [batch_size, length, hidden_size]\n    Outputs will have the shape [batch_size, length, hidden_size]\n    Refer Fig.1 in https://arxiv.org/pdf/1706.03762.pdf\n    """"""\n    def __init__(self, embedding_size, hidden_size, num_layers, num_heads, total_key_depth, total_value_depth,\n                 filter_size, max_length=100, input_dropout=0.0, layer_dropout=0.0, \n                 attention_dropout=0.0, relu_dropout=0.0):\n        """"""\n        Parameters:\n            embedding_size: Size of embeddings\n            hidden_size: Hidden size\n            num_layers: Total layers in the Encoder\n            num_heads: Number of attention heads\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            filter_size: Hidden size of the middle layer in FFN\n            max_length: Max sequence length (required for timing signal)\n            input_dropout: Dropout just after embedding\n            layer_dropout: Dropout for each layer\n            attention_dropout: Dropout probability after attention (Should be non-zero only during training)\n            relu_dropout: Dropout probability after relu in FFN (Should be non-zero only during training)\n        """"""\n        \n        super(Decoder, self).__init__()\n        \n        self.timing_signal = _gen_timing_signal(max_length, hidden_size)\n        \n        params =(hidden_size, \n                 total_key_depth or hidden_size,\n                 total_value_depth or hidden_size,\n                 filter_size, \n                 num_heads, \n                 _gen_bias_mask(max_length), # mandatory\n                 layer_dropout, \n                 attention_dropout, \n                 relu_dropout)\n        \n        self.embedding_proj = nn.Linear(embedding_size, hidden_size, bias=False)\n        self.dec = nn.Sequential(*[DecoderLayer(*params) for l in range(num_layers)])\n        \n        self.layer_norm = LayerNorm(hidden_size)\n        self.input_dropout = nn.Dropout(input_dropout)\n        \n    \n    def forward(self, inputs, encoder_output):\n        #Add input dropout\n        x = self.input_dropout(inputs)\n        \n        # Project to hidden size\n        x = self.embedding_proj(x)\n        \n        # Add timing signal\n        x += self.timing_signal[:, :inputs.shape[1], :].type_as(inputs.data)\n        \n        # Run decoder\n        y, _ = self.dec((x, encoder_output))\n        \n        # Final layer normalization\n        y = self.layer_norm(y)\n        return y\n'"
torchnlp/modules/transformer/sublayers.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\nclass MultiHeadAttention(nn.Module):\n    """"""\n    Multi-head attention as per https://arxiv.org/pdf/1706.03762.pdf\n    Refer Figure 2\n    """"""\n    def __init__(self, input_depth, total_key_depth, total_value_depth, output_depth, \n                 num_heads, bias_mask=None, dropout=0.0):\n        """"""\n        Parameters:\n            input_depth: Size of last dimension of input\n            total_key_depth: Size of last dimension of keys. Must be divisible by num_head\n            total_value_depth: Size of last dimension of values. Must be divisible by num_head\n            output_depth: Size last dimension of the final output\n            num_heads: Number of attention heads\n            bias_mask: Masking tensor to prevent connections to future elements\n            dropout: Dropout probability (Should be non-zero only during training)\n        """"""\n        super(MultiHeadAttention, self).__init__()\n        # Checks borrowed from \n        # https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n        if total_key_depth % num_heads != 0:\n            raise ValueError(""Key depth (%d) must be divisible by the number of ""\n                             ""attention heads (%d)."" % (total_key_depth, num_heads))\n        if total_value_depth % num_heads != 0:\n            raise ValueError(""Value depth (%d) must be divisible by the number of ""\n                             ""attention heads (%d)."" % (total_value_depth, num_heads))\n            \n        self.num_heads = num_heads\n        self.query_scale = (total_key_depth//num_heads)**-0.5\n        self.bias_mask = bias_mask\n        \n        # Key and query depth will be same\n        self.query_linear = nn.Linear(input_depth, total_key_depth, bias=False)\n        self.key_linear = nn.Linear(input_depth, total_key_depth, bias=False)\n        self.value_linear = nn.Linear(input_depth, total_value_depth, bias=False)\n        self.output_linear = nn.Linear(total_value_depth, output_depth, bias=False)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def _split_heads(self, x):\n        """"""\n        Split x such to add an extra num_heads dimension\n        Input:\n            x: a Tensor with shape [batch_size, seq_length, depth]\n        Returns:\n            A Tensor with shape [batch_size, num_heads, seq_length, depth/num_heads]\n        """"""\n        if len(x.shape) != 3:\n            raise ValueError(""x must have rank 3"")\n        shape = x.shape\n        return x.view(shape[0], shape[1], self.num_heads, shape[2]//self.num_heads).permute(0, 2, 1, 3)\n    \n    def _merge_heads(self, x):\n        """"""\n        Merge the extra num_heads into the last dimension\n        Input:\n            x: a Tensor with shape [batch_size, num_heads, seq_length, depth/num_heads]\n        Returns:\n            A Tensor with shape [batch_size, seq_length, depth]\n        """"""\n        if len(x.shape) != 4:\n            raise ValueError(""x must have rank 4"")\n        shape = x.shape\n        return x.permute(0, 2, 1, 3).contiguous().view(shape[0], shape[2], shape[3]*self.num_heads)\n        \n    def forward(self, queries, keys, values):\n        \n        # Do a linear for each component\n        queries = self.query_linear(queries)\n        keys = self.key_linear(keys)\n        values = self.value_linear(values)\n        \n        # Split into multiple heads\n        queries = self._split_heads(queries)\n        keys = self._split_heads(keys)\n        values = self._split_heads(values)\n        \n        # Scale queries\n        queries *= self.query_scale\n        \n        # Combine queries and keys\n        logits = torch.matmul(queries, keys.permute(0, 1, 3, 2))\n        \n        # Add bias to mask future values\n        if self.bias_mask is not None:\n            logits += self.bias_mask[:, :, :logits.shape[-2], :logits.shape[-1]].type_as(logits.data)\n        \n        # Convert to probabilites\n        weights = nn.functional.softmax(logits, dim=-1)\n        \n        # Dropout\n        weights = self.dropout(weights)\n        \n        # Combine with values to get context\n        contexts = torch.matmul(weights, values)\n        \n        # Merge heads\n        contexts = self._merge_heads(contexts)\n        #contexts = torch.tanh(contexts)\n        \n        # Linear to get output\n        outputs = self.output_linear(contexts)\n        \n        return outputs\n\nclass Conv(nn.Module):\n    """"""\n    Convenience class that does padding and convolution for inputs in the format\n    [batch_size, sequence length, hidden size]\n    """"""\n    def __init__(self, input_size, output_size, kernel_size, pad_type):\n        """"""\n        Parameters:\n            input_size: Input feature size\n            output_size: Output feature size\n            kernel_size: Kernel width\n            pad_type: left -> pad on the left side (to mask future data), \n                      both -> pad on both sides\n        """"""\n        super(Conv, self).__init__()\n        padding = (kernel_size - 1, 0) if pad_type == \'left\' else (kernel_size//2, (kernel_size - 1)//2)\n        self.pad = nn.ConstantPad1d(padding, 0)\n        self.conv = nn.Conv1d(input_size, output_size, kernel_size=kernel_size, padding=0)\n\n    def forward(self, inputs):\n        inputs = self.pad(inputs.permute(0, 2, 1))\n        outputs = self.conv(inputs).permute(0, 2, 1)\n\n        return outputs\n\n\nclass PositionwiseFeedForward(nn.Module):\n    """"""\n    Does a Linear + RELU + Linear on each of the timesteps\n    """"""\n    def __init__(self, input_depth, filter_size, output_depth, layer_config=\'ll\', padding=\'left\', dropout=0.0):\n        """"""\n        Parameters:\n            input_depth: Size of last dimension of input\n            filter_size: Hidden size of the middle layer\n            output_depth: Size last dimension of the final output\n            layer_config: ll -> linear + ReLU + linear\n                          cc -> conv + ReLU + conv etc.\n            padding: left -> pad on the left side (to mask future data), \n                     both -> pad on both sides\n            dropout: Dropout probability (Should be non-zero only during training)\n        """"""\n        super(PositionwiseFeedForward, self).__init__()\n        \n        layers = []\n        sizes = ([(input_depth, filter_size)] + \n                 [(filter_size, filter_size)]*(len(layer_config)-2) + \n                 [(filter_size, output_depth)])\n\n        for lc, s in zip(list(layer_config), sizes):\n            if lc == \'l\':\n                layers.append(nn.Linear(*s))\n            elif lc == \'c\':\n                layers.append(Conv(*s, kernel_size=3, pad_type=padding))\n            else:\n                raise ValueError(""Unknown layer type {}"".format(lc))\n\n        self.layers = nn.ModuleList(layers)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, inputs):\n        x = inputs\n        for i, layer in enumerate(self.layers):\n            x = layer(x)\n            if i < len(self.layers):\n                x = self.relu(x)\n                x = self.dropout(x)\n\n        return x\n\n'"
torchnlp/tasks/sequence_tagging/__init__.py,0,"b'from .main import train, evaluate, infer, interactive\nfrom .transformer_tagger import TransformerTagger\nfrom .bilstm_tagger import BiLSTMTagger\nfrom .tagger import Tagger, hparams_tagging_base, VOCABS_FILE'"
torchnlp/tasks/sequence_tagging/bilstm_tagger.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchnlp.common.hparams import HParams\n\nfrom .tagger import Tagger, hparams_tagging_base\n\nclass BiLSTMTagger(Tagger):\n    """"""\n    Sequence tagger using bidirectional LSTM. For character embeddings per word\n    uses (unidirectional) LSTM\n    """"""\n    def __init__(self, hparams=None, **kwargs):\n        """"""\n        No additional parameters\n        """"""\n        super(BiLSTMTagger, self).__init__(hparams=hparams, **kwargs)\n\n        embedding_size = hparams.embedding_size_word\n        if hparams.embedding_size_char > 0:\n            embedding_size += hparams.embedding_size_char_per_word\n            self.lstm_char = nn.LSTM(hparams.embedding_size_char, \n                                     hparams.embedding_size_char_per_word,\n                                     num_layers=1, \n                                     batch_first=True, \n                                     bidirectional=False)\n\n\n        self.lstm = nn.LSTM(embedding_size, \n                            hparams.hidden_size, \n                            num_layers=hparams.num_hidden_layers,\n                            batch_first=True,\n                            bidirectional=True\n                           )\n\n        self.dropout = nn.Dropout(hparams.dropout)\n        self.extra_layer = nn.Linear(2*hparams.hidden_size, hparams.hidden_size)\n\n    def compute(self, inputs_word_emb, inputs_char_emb):\n         \n        if inputs_char_emb is not None:\n            seq_len = inputs_word_emb.shape[1]\n\n            # Compute per word character embeddings using unidirectional LSTM\n            _, (h_n, _) = self.lstm_char(inputs_char_emb)\n        \n            inputs_char_emb = h_n.view(-1, seq_len, \n                                  self.hparams.embedding_size_char_per_word)\n\n            # Combine embeddings\n            inputs_word_emb = torch.cat([inputs_word_emb, inputs_char_emb], -1)\n        \n        hidden, _ = self.lstm(self.dropout(inputs_word_emb))\n        hidden_extra = F.tanh(self.extra_layer(hidden))\n\n        return hidden_extra\n'"
torchnlp/tasks/sequence_tagging/main.py,0,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\n\nfrom torchnlp.common.hparams import HParams\nfrom torchnlp.common.evaluation import Evaluator, BasicMetrics, IOBMetrics\nfrom torchnlp.common.train import Trainer\nfrom torchnlp.data.inputs import get_input_processor_words\nfrom torchnlp.common.prefs import PREFS\n\nimport sys, os\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n# Globals\nSPECIALS = set([\'<unk>\', \'<pad>\', \'<bos>\', \'<eos>\'])\n\ndef train(task_name, model_cls, dataset_fn, hparams=None, hparams_map=None,\n            num_epochs=100, checkpoint=None,\n            use_iob_metrics=True, early_stopping=None):\n    """"""\n    Train a model on a dataset. Saves model and hyperparams so that training\n    can be easily resumed\n    Parameters:\n        task_name: Name of the task like NER1 etc. More like a unique identifier\n        model_cls: Model class name. Should inherit from sequence_tagging.Tagger class\n        dataset_fn: Dataset function. Must return a dict. See data/conll.py\n        hparams: Hyperparameters defined for the particular model. Not required when\n                resuming training\n        hparams_map: mapping between model classes and hparams. Only used of hparams is\n                None\n        num_epochs: Set to 100. With early stopping set number of epochs will usually\n                be lower. Set PREFS.early_stopping to desired criteria\n        checkpoint: Saved checkpoint number (train iteration). Used to resume training\n        use_iob_metrics: If True then adds IOB metrics (precision/recall/F1) to Evaluator\n        early_stopping: Early stopping criteria. Should be of the form\n                        [lowest/highest]_[window_Size]_[metric] where metric is one of\n                        loss, acc, acc-seq, precision, recall, F1. Default is picked\n                        from PREFS.early_stopping\n    """"""\n    early_stopping=early_stopping or PREFS.early_stopping\n\n    # Create or load the model\n    if checkpoint is None:\n        if hparams is None:\n            hparams = hparams_map[model_cls]\n        dataset = dataset_fn(hparams.batch_size)\n        model = model_cls.create(task_name, hparams, vocabs=dataset[\'vocabs\'], \n                                overwrite=PREFS.overwrite_model_dir)\n    else:\n        model, hparams_loaded = model_cls.load(task_name, checkpoint)\n        if hparams is None:\n            hparams = hparams_loaded\n        dataset = dataset_fn(hparams.batch_size)\n    \n    train_iter, validation_iter, _ = dataset[\'iters\']\n    _, _, tag_vocab = dataset[\'vocabs\']\n\n    metrics = [BasicMetrics(output_vocab=tag_vocab)]\n    if use_iob_metrics:\n        metrics += [IOBMetrics(tag_vocab=tag_vocab)]\n\n    # Setup evaluator on the validation dataset\n    evaluator = Evaluator(validation_iter, *metrics)\n\n    # Setup trainer\n    trainer = Trainer(task_name, model, hparams, train_iter, evaluator)\n\n    # Start training\n    best_cp, _ = trainer.train(num_epochs, early_stopping=early_stopping)\n\n    return best_cp\n\n\ndef evaluate(task_name, model_cls, dataset_fn, split, checkpoint=-1, use_iob_metrics=True):\n    """"""\n    Evaluate the model on a specific dataset split [train, validation, test].\n    Evaluated for loss, accuracy, precision, recall, F1. Model hyperparameters\n    are loaded automatically\n    Parameters:\n        task_name: Name of the task like NER1 etc. You must use the same name\n                    that was provided during training\n        model_cls: Model class name. Must be same as used during training\n        dataset_fn: Dataset function. Must return a dict. See data/conll.py\n        split: One of the strings train/validation/test\n        checkpoint: Checkpoint number (train iteration) to load from. -1 for\n                    best/latest checkpoint\n        use_iob_metrics: If True then adds IOB metrics (precision/recall/F1) \n                    to Evaluator\n    """"""\n    iter_map = {\'train\': 0, \'validation\': 1, \'test\': 2}\n    dataset = dataset_fn()\n    data_iter = dataset[\'iters\'][iter_map[split]]\n\n    model, hparams = model_cls.load(task_name, checkpoint)\n\n    metrics = [BasicMetrics(output_vocab=model.vocab_tags)]\n    if use_iob_metrics:\n        metrics += [IOBMetrics(tag_vocab=model.vocab_tags)]\n\n    # Setup evaluator on the given dataset\n    evaluator = Evaluator(data_iter, *metrics)\n    results = evaluator.evaluate(model)\n\n    print(\'{} set evaluation: {}-{}\'.format(split, task_name, model.__class__.__name__))\n    print(\', \'.join([\'{}={:3.5f}\'.format(k, v) for k,v in results.items()]))\n\ndef _run_model_loaded(model, batch):\n    predictions = model(batch)\n    results = []\n    for j in range(predictions.shape[0]):\n        pred_indices = list(predictions[j, :].data)\n        tags = map(lambda id: model.vocab_tags.itos[id], pred_indices)\n        tags = filter(lambda item: item not in SPECIALS, tags)\n        results.append(list(tags))\n\n    return results\n\ndef infer(task_name, inputs, model_cls, checkpoint=-1):\n    """"""\n    Run inference on an input sentence. \n    Parameters:\n        task_name: Name of the task as provided during training\n        inputs: Input sentence Either a string or a list of strings\n        model_cls: Model class name. Must be same as used during training\n        checkpoint: Checkpoint number (train iteration) to load from. -1 for\n                    best/latest checkpoint\n    """"""\n    task_name = task_name or PREFS.cur_task\n    model = model_cls.load(task_name, checkpoint)\n    \n    vocab_word, vocab_char, _ = model.vocabs\n    input_fn = get_input_processor_words(vocab_word, vocab_char)\n\n    batch = input_fn(inputs)\n    \n    return _run_model_loaded(model, batch)\n\ndef interactive(task_name, model_cls, checkpoint=-1):\n    """"""\n    Run inference interactively\n    Parameters:\n        task_name: Name of the task as provided during training\n        model_cls: Model class name. Must be same as used during training\n        checkpoint: Checkpoint number (train iteration) to load from. -1 for\n                    best/latest checkpoint\n    """"""\n    model, hparams = model_cls.load(task_name, checkpoint)\n\n    vocab_word, vocab_char, _ = model.vocabs\n    input_fn = get_input_processor_words(vocab_word, vocab_char)\n\n    print(\'Ctrl+C to quit\')\n    try:\n        while True:\n            inputs = input(\'> \')\n            batch = input_fn(inputs)\n            output = _run_model_loaded(model, batch)\n            print(\' \'.join(output[0]))\n    except:\n        pass\n    \n\n'"
torchnlp/tasks/sequence_tagging/tagger.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchnlp.common.hparams import HParams\nfrom torchnlp.common.model import Model, gen_model_dir\nfrom torchnlp.modules import outputs\n\nimport os\n\nVOCABS_FILE = \'vocabs.pt\'\n\nclass Tagger(Model):\n    """"""\n    Abstract base class that adds the following boilerplate for\n    sequence tagging tasks:\n    - Word Embeddings\n    - Character Embeddings\n    - Tag projection\n    - CRF\n    Derived classes implement the compute() method and not forward(). \n    This is so that projection and other layers can be added\n    """"""\n    def __init__(self, hparams=None, vocabs=None):\n        """"""\n        Parameters:\n            hparams: Instance of HParams class\n            num_tags: Number of output tags\n            vocabs: tuple of (word vocab, char vocab, tags vocab). Each is an\n                    instance of torchtext.vocab.Vocab.\n            NOTE: If word_vocab.vectors is available it will initialize the embeddings\n            and with word_vocab.vectors make it non-trainable\n        """"""\n        super(Tagger, self).__init__(hparams)\n\n        if vocabs is None or not isinstance(vocabs, tuple) or len(vocabs) != 3:\n            raise ValueError(\'Must provide vocabs 3-tuple\')\n\n        vocab_word, vocab_char, vocab_tags = vocabs\n        if vocab_word is None:\n            raise ValueError(\'Must provide vocab_word\')\n        if vocab_tags is None:\n            raise ValueError(\'Must provide vocab_word\')\n\n        self.vocabs = vocabs\n        self.vocab_tags = vocab_tags # Needed during eval and prediction\n        self.embedding_word = nn.Embedding(len(vocab_word), hparams.embedding_size_word)\n        self.embedding_char = None\n\n        if vocab_char is not None and hparams.embedding_size_char > 0:\n            self.embedding_char = nn.Embedding(len(vocab_char), hparams.embedding_size_char)\n        \n        if vocab_word.vectors is not None:\n            if hparams.embedding_size_word != vocab_word.vectors.shape[1]:\n                raise ValueError(\'embedding_size should be {} but got {}\'\n                                .format(vocab_word.vectors.shape[1], \n                                        hparams.embedding_size_word))\n            self.embedding_word.weight.data.copy_(vocab_word.vectors)\n            self.embedding_word.weight.requires_grad = False\n        \n\n        if hparams.use_crf:\n            self.output_layer = outputs.CRFOutputLayer(hparams.hidden_size, len(vocab_tags))\n        else:\n            self.output_layer = outputs.SoftmaxOutputLayer(hparams.hidden_size, len(vocab_tags))\n\n    def _embed_compute(self, batch):\n        inputs_word_emb = self.embedding_word(batch.inputs_word)\n        inputs_char_emb = None\n        if self.embedding_char is not None:\n            inputs_char_emb = self.embedding_char(batch.inputs_char.view(-1, \n                                                  batch.inputs_char.shape[-1]))\n\n        return self.compute(inputs_word_emb, inputs_char_emb)\n\n    def forward(self, batch):\n        """"""\n        NOTE: batch must have the following attributes:\n            inputs_word, inputs_char, labels\n        """"""\n        with torch.no_grad():\n            hidden = self._embed_compute(batch)\n            output = self.output_layer(hidden)\n\n        return output\n\n        # TODO: Add beam search somewhere :)\n        \n\n    def loss(self, batch, compute_predictions=False):\n        """"""\n        NOTE: batch must have the following attributes:\n            inputs_word, inputs_char, labels\n        """"""\n        hidden = self._embed_compute(batch)\n        predictions = None\n        if compute_predictions:\n            predictions = self.output_layer(hidden)\n\n        loss_val = self.output_layer.loss(hidden, batch.labels)\n\n        return loss_val, predictions\n\n    def compute(self, inputs_word_emb, inputs_char_emb):\n        """"""\n        Abstract method that is called to compute the final model\n        hidden state. Derived classes implement the method to take\n        input embeddings and provide the final hidden state\n\n        Parameters:\n            inputs_word_emb: Input word embeddings of shape\n                                [batch, sequence-length, word-embedding-size]\n            inputs_char_emb[optional]: Input character embeddings of shape\n                                [batch x sequence-length, word-length, char-embedding-size]\n\n        Returns:\n            Final hidden state in the shape [batch, sequence-length, hidden-size]\n        """"""\n        raise NotImplementedError(""Must implement compute()"")\n\n    @classmethod\n    def create(cls, task_name, hparams, vocabs, **kwargs):\n        """"""\n        Saves the vocab files\n        """"""\n        model = super(Tagger, cls).create(task_name, hparams, vocabs=vocabs, **kwargs)\n        model_dir = gen_model_dir(task_name, cls)\n        torch.save(vocabs, os.path.join(model_dir, VOCABS_FILE))\n\n        return model\n\n    @classmethod\n    def load(cls, task_name, checkpoint, **kwargs):\n        model_dir = gen_model_dir(task_name, cls)\n        vocabs_path = os.path.join(model_dir, VOCABS_FILE)\n        if not os.path.exists(vocabs_path):\n            raise OSError(\'Vocabs file not found\')\n        vocabs = torch.load(vocabs_path)\n        return super(Tagger, cls).load(task_name, checkpoint, vocabs=vocabs, **kwargs)\n\n\ndef hparams_tagging_base():\n    return HParams(\n        batch_size=100,\n        embedding_size_word=300,\n        embedding_size_char=0, # No char embeddings\n        embedding_size_char_per_word=100,\n        embedding_size_tags=100,\n        hidden_size=128,\n        learning_rate=0.2,\n        learning_rate_decay=None,\n        max_length=256,\n        num_hidden_layers=1,\n        dropout=0.2,\n        optimizer_adam_beta1=0.9,\n        optimizer_adam_beta2=0.98,\n        use_crf=False\n    )'"
torchnlp/tasks/sequence_tagging/transformer_tagger.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchnlp.common.hparams import HParams\nfrom torchnlp.modules import transformer\n\nfrom .tagger import Tagger, hparams_tagging_base\n\nclass TransformerTagger(Tagger):\n    """"""\n    Sequence tagger using the Transformer network (https://arxiv.org/pdf/1706.03762.pdf)\n    Specifically it uses the Encoder module. For character embeddings (per word) it uses\n    the same Encoder module above which an additive (Bahdanau) self-attention layer is added\n    """"""\n    def __init__(self, hparams=None, **kwargs):\n        """"""\n        No additional parameters\n        """"""\n        super(TransformerTagger, self).__init__(hparams=hparams, **kwargs)\n\n        embedding_size = hparams.embedding_size_word\n        if hparams.embedding_size_char > 0:\n            embedding_size += hparams.embedding_size_char_per_word\n            self.transformer_char = transformer.Encoder(\n                                        hparams.embedding_size_char,\n                                        hparams.embedding_size_char_per_word,\n                                        1,\n                                        4,\n                                        hparams.attention_key_channels,\n                                        hparams.attention_value_channels,\n                                        hparams.filter_size_char,\n                                        hparams.max_length,\n                                        hparams.input_dropout,\n                                        hparams.dropout,\n                                        hparams.attention_dropout,\n                                        hparams.relu_dropout,\n                                        use_mask=False\n                                    ) \n            self.char_linear = nn.Linear(hparams.embedding_size_char_per_word, 1)\n\n        self.transformer_enc = transformer.Encoder(\n                                    embedding_size,\n                                    hparams.hidden_size,\n                                    hparams.num_hidden_layers,\n                                    hparams.num_heads,\n                                    hparams.attention_key_channels,\n                                    hparams.attention_value_channels,\n                                    hparams.filter_size,\n                                    hparams.max_length,\n                                    hparams.input_dropout,\n                                    hparams.dropout,\n                                    hparams.attention_dropout,\n                                    hparams.relu_dropout,\n                                    use_mask=False\n                                )\n\n    def compute(self, inputs_word_emb, inputs_char_emb):\n\n        if inputs_char_emb is not None:\n            seq_len = inputs_word_emb.shape[1]\n\n            # Process character embeddings to get per word embeddings\n            inputs_char_emb = self.transformer_char(inputs_char_emb)\n\n            # Apply additive self-attention to combine outputs\n            mask = self.char_linear(inputs_char_emb)\n            mask = F.softmax(mask, dim=-1)\n            inputs_emb_char = (torch.matmul(mask.permute(0, 2, 1), inputs_char_emb).contiguous()\n                            .view(-1, seq_len, self.hparams.embedding_size_char_per_word))\n\n            # Combine embeddings\n            inputs_word_emb = torch.cat([inputs_word_emb, inputs_emb_char], -1)\n\n        # Apply Transformer Encoder\n        enc_out = self.transformer_enc(inputs_word_emb)\n\n        return enc_out\n\n'"
