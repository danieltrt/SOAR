file_path,api_count,code
conftest.py,16,"b'from itertools import product\nfrom typing import Dict\n\nimport pytest\nimport torch\n\n\ndef get_test_devices() -> Dict[str, torch.device]:\n    """"""Creates a dictionary with the devices to test the source code.\n    CUDA devices will be test only in case the current hardware supports it.\n\n    Return:\n        dict(str, torch.device): list with devices names.\n    """"""\n    devices: Dict[str, torch.device] = {}\n    devices[""cpu""] = torch.device(""cpu"")\n    if torch.cuda.is_available():\n        devices[""cuda""] = torch.device(""cuda:0"")\n    return devices\n\n\ndef get_test_dtypes() -> Dict[str, torch.dtype]:\n    """"""Creates a dictionary with the dtypes the source code.\n\n    Return:\n        dict(str, torch.dtype): list with dtype names.\n    """"""\n    dtypes: Dict[str, torch.dtype] = {}\n    dtypes[""float16""] = torch.float16\n    dtypes[""float32""] = torch.float32\n    dtypes[""float64""] = torch.float64\n    return dtypes\n\n\n# setup the devices to test the source code\n\nTEST_DEVICES: Dict[str, torch.device] = get_test_devices()\nTEST_DTYPES: Dict[str, torch.dtype] = get_test_dtypes()\n\n# Combinations of device and dtype to be excluded from testing.\nDEVICE_DTYPE_BLACKLIST = {(\'cpu\', \'float16\')}\n\n\n@pytest.fixture()\ndef device(device_name) -> torch.device:\n    if device_name not in TEST_DEVICES:\n        pytest.skip(f""Unsupported device type: {device_name}"")\n    return TEST_DEVICES[device_name]\n\n\n@pytest.fixture()\ndef dtype(dtype_name) -> torch.dtype:\n    return TEST_DTYPES[dtype_name]\n\n\ndef pytest_generate_tests(metafunc):\n    device_names = None\n    dtype_names = None\n    if \'device_name\' in metafunc.fixturenames:\n        raw_value = metafunc.config.getoption(\'--device\')\n        if raw_value == \'all\':\n            device_names = list(TEST_DEVICES.keys())\n        else:\n            device_names = raw_value.split(\',\')\n    if \'dtype_name\' in metafunc.fixturenames:\n        raw_value = metafunc.config.getoption(\'--dtype\')\n        if raw_value == \'all\':\n            dtype_names = list(TEST_DTYPES.keys())\n        else:\n            dtype_names = raw_value.split(\',\')\n    if device_names is not None and dtype_names is not None:\n        # Exclude any blacklisted device/dtype combinations.\n        params = [combo for combo in product(device_names, dtype_names)\n                  if combo not in DEVICE_DTYPE_BLACKLIST]\n        metafunc.parametrize(\'device_name,dtype_name\', params)\n    elif device_names is not None:\n        metafunc.parametrize(\'device_name\', device_names)\n    elif dtype_names is not None:\n        metafunc.parametrize(\'dtype_name\', dtype_names)\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\'--device\', action=""store"", default=""cpu"")\n    parser.addoption(\'--dtype\', action=""store"", default=""float32"")\n'"
setup.py,0,"b'# Welcome to the Kornia setup.py.\n#\n\nimport os\nfrom setuptools import setup, find_packages\nimport subprocess\nimport distutils.command.clean\n\n\n################\n# The variables below define the current version under\n# development and the current pytorch supported verions.\n# WARNING: Becareful and do not touch those variables,\n# unless you are a maintainer. Otherwise, could brake\n# the package backward compatibility.\n\n# NOTE(maintainers): modify this variable each time you do a release\n\nversion = \'0.3.2\'\n\n#################################\n\nsha = \'Unknown\'\npackage_name = \'kornia\'\n\ncwd = os.path.dirname(os.path.abspath(__file__))\n\ntry:\n    sha = subprocess.check_output([\'git\', \'rev-parse\', \'HEAD\'], cwd=cwd).decode(\'ascii\').strip()\nexcept Exception:\n    pass\n\nif os.getenv(\'KORNIA_BUILD_VERSION\'):\n    version = os.getenv(\'KORNIA_BUILD_VERSION\')\nelif sha != \'Unknown\':\n    version += \'+\' + sha[:7]\nprint(""Building wheel {}-{}"".format(package_name, version))\n\n\ndef write_version_file():\n    version_path = os.path.join(cwd, \'kornia\', \'version.py\')\n    with open(version_path, \'w\') as f:\n        f.write(""__version__ = \'{}\'\\n"".format(version))\n        f.write(""git_version = {}\\n"".format(repr(sha)))\n\n\ndef read(*names, **kwargs):\n    with io.open(\n        os.path.join(os.path.dirname(__file__), *names),\n        encoding=kwargs.get(""encoding"", ""utf8"")\n    ) as fp:\n        return fp.read()\n\n\n# open readme file and remove logo\nreadme = open(\'README.rst\').read()\nlong_description = \'\\n\'.join(readme.split(\'\\n\')[7:])\n\n\nclass clean(distutils.command.clean.clean):\n    def run(self):\n        with open(\'.gitignore\', \'r\') as f:\n            ignores = f.read()\n            for wildcard in filter(None, ignores.split(\'\\n\')):\n                for filename in glob.glob(wildcard):\n                    try:\n                        os.remove(filename)\n                    except OSError:\n                        shutil.rmtree(filename, ignore_errors=True)\n\n        # It\'s an old-style class in Python 2.7...\n        distutils.command.clean.clean.run(self)\n    # remove compiled and temporary files\n    subprocess.call([\'rm -rf dist/ build/ kornia.egg*\'], shell=True)\n\n\npytorch_dep = \'torch\'\nif os.getenv(\'PYTORCH_VERSION\'):\n    pytorch_dep += ""=="" + os.getenv(\'PYTORCH_VERSION\')\n\nrequirements = [\n    \'numpy\',\n    pytorch_dep,\n]\n\n\nif __name__ == \'__main__\':\n    write_version_file()\n    setup(\n        # Metadata\n        name=package_name,\n        version=version,\n        author=\'Edgar Riba\',\n        author_email=\'contact@kornia.org\',\n        url=\'https://github.com/kornia/kornia\',\n        description=\'Open Source Differentiable Computer Vision Library for PyTorch\',\n        long_description=long_description,\n        license=\'Apache License 2.0\',\n        python_requires=\'>=3.6\',\n\n        # Test\n        setup_requires=[\'pytest-runner\'],\n        tests_require=[\'pytest\'],\n\n        # Package info\n        packages=find_packages(exclude=(\'docs\', \'test\', \'examples\',)),\n\n        zip_safe=True,\n        install_requires=requirements,\n        classifiers=[\n            \'Intended Audience :: Developers\',\n            \'Intended Audience :: Education\',\n            \'Intended Audience :: Science/Research\',\n            \'Operating System :: POSIX :: Linux\',\n            \'Programming Language :: Python :: 3 :: Only\',\n            \'License :: OSI Approved :: Apache Software License\',\n            \'Topic :: Scientific/Engineering :: Image Recognition\',\n            \'Topic :: Software Development :: Libraries\',\n        ],\n    )\n'"
kornia/__init__.py,0,"b'# Make sure that kornia is running on Python 3.6.0 or later\n# (to avoid running into this bug: https://bugs.python.org/issue29246)\nimport sys\nif sys.version_info < (3, 6, 0):\n    raise RuntimeError(""Kornia requires Python 3.6.0 or later"")\n\ntry:\n    from .version import __version__  # noqa: F401\nexcept ImportError:\n    pass\n\nfrom kornia import augmentation\nfrom kornia import color\nfrom kornia import contrib\nfrom kornia import feature\nfrom kornia import filters\nfrom kornia import geometry\nfrom kornia import jit\nfrom kornia import losses\nfrom kornia import utils\n\n# Exposes package functional to top level\n\nfrom kornia.augmentation.functional import *\nfrom kornia.color import (\n    rgb_to_grayscale,\n    bgr_to_grayscale,\n    bgr_to_rgb,\n    rgb_to_bgr,\n    rgb_to_rgba,\n    bgr_to_rgba,\n    rgba_to_rgb,\n    rgba_to_bgr,\n    rgb_to_hsv,\n    hsv_to_rgb,\n    rgb_to_hls,\n    rgb_to_yuv,\n    yuv_to_rgb,\n    hls_to_rgb,\n    rgb_to_ycbcr,\n    ycbcr_to_rgb,\n    rgb_to_xyz,\n    xyz_to_rgb,\n    rgb_to_luv,\n    luv_to_rgb,\n    normalize,\n    denormalize,\n    zca_mean,\n    zca_whiten,\n    linear_transform,\n    adjust_brightness,\n    adjust_contrast,\n    adjust_gamma,\n    adjust_hue,\n    adjust_saturation,\n)\nfrom kornia.contrib import (\n    extract_tensor_patches,\n    max_blur_pool2d,\n)\nfrom kornia.feature import (\n    nms2d,\n    harris_response,\n    hessian_response,\n    gftt_response,\n    SIFTDescriptor\n)\nfrom kornia.filters import (\n    get_gaussian_kernel1d,\n    get_gaussian_kernel2d,\n    get_laplacian_kernel1d,\n    get_laplacian_kernel2d,\n    get_motion_kernel2d,\n    gaussian_blur2d,\n    laplacian,\n    sobel,\n    spatial_gradient,\n    box_blur,\n    median_blur,\n    motion_blur,\n    filter2D,\n)\nfrom kornia.losses import (\n    ssim,\n    dice_loss,\n    tversky_loss,\n    inverse_depth_smoothness_loss,\n    total_variation,\n    psnr_loss,\n    kl_div_loss_2d,\n    js_div_loss_2d,\n)\nfrom kornia.utils import (\n    one_hot,\n    create_meshgrid,\n    tensor_to_image,\n    image_to_tensor,\n    save_pointcloud_ply,\n    load_pointcloud_ply,\n)\n\nfrom kornia.geometry import *\nfrom kornia.constants import pi\n'"
kornia/constants.py,1,"b""from typing import Union, TypeVar\nfrom enum import Enum\n\nimport torch\n\npi = torch.tensor(3.14159265358979323846)\nT = TypeVar('T', bound='Resample')\nU = TypeVar('U', bound='BorderType')\n\n\nclass Resample(Enum):\n    NEAREST = 0\n    BILINEAR = 1\n    BICUBIC = 2\n\n    @classmethod\n    def get(cls, value: Union[str, int, T]) -> T:  # type: ignore\n        if type(value) == str:\n            return cls[value.upper()]  # type: ignore\n        if type(value) == int:\n            return cls(value)  # type: ignore\n        if type(value) == cls:\n            return value  # type: ignore\n        raise TypeError()\n\n\nclass BorderType(Enum):\n    CONSTANT = 0\n    REFLECT = 1\n    REPLICATE = 2\n    CIRCULAR = 3\n\n    @classmethod\n    def get(cls, value: Union[str, int, U]) -> U:  # type: ignore\n        if type(value) == str:\n            return cls[value.upper()]  # type: ignore\n        if type(value) == int:\n            return cls(value)  # type: ignore\n        if type(value) == cls:\n            return value  # type: ignore\n        raise TypeError()\n"""
test/__init__.py,0,b''
test/smoke_test.py,1,"b'import pytest\n\nimport torch\nimport kornia\n\n\n@pytest.mark.parametrize(""batch_size"", [1, 2, 5])\ndef test_smoke(batch_size):\n    x = torch.rand(batch_size, 2, 3)\n    assert x.shape == (batch_size, 2, 3), x.shape\n'"
test/test_contrib.py,22,"b'import pytest\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\n\nclass TestMaxBlurPool2d:\n    def test_shape(self, device):\n        input = torch.rand(1, 2, 4, 6).to(device)\n        pool = kornia.contrib.MaxBlurPool2d(kernel_size=3)\n        assert pool(input).shape == (1, 2, 2, 3)\n\n    def test_shape_batch(self, device):\n        input = torch.rand(3, 2, 6, 10).to(device)\n        pool = kornia.contrib.MaxBlurPool2d(kernel_size=5)\n        assert pool(input).shape == (3, 2, 3, 5)\n\n    def test_gradcheck(self, device):\n        input = torch.rand(2, 3, 4, 4).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.contrib.max_blur_pool2d,\n                         (input, 3,), raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input: torch.Tensor, kernel_size: int) -> torch.Tensor:\n            return kornia.contrib.max_blur_pool2d(input, kernel_size)\n        img = torch.rand(2, 3, 4, 5).to(device)\n        actual = op_script(img, kernel_size=3)\n        expected = kornia.contrib.max_blur_pool2d(img, kernel_size=3)\n        assert_allclose(actual, expected)\n\n\nclass TestExtractTensorPatches:\n    def test_smoke(self, device):\n        input = torch.arange(16.).view(1, 1, 4, 4).to(device)\n        m = kornia.contrib.ExtractTensorPatches(3)\n        assert m(input).shape == (1, 4, 1, 3, 3)\n\n    def test_b1_ch1_h4w4_ws3(self, device):\n        input = torch.arange(16.).view(1, 1, 4, 4).to(device)\n        m = kornia.contrib.ExtractTensorPatches(3)\n        patches = m(input)\n        assert patches.shape == (1, 4, 1, 3, 3)\n        assert_allclose(input[0, :, :3, :3], patches[0, 0])\n        assert_allclose(input[0, :, :3, 1:], patches[0, 1])\n        assert_allclose(input[0, :, 1:, :3], patches[0, 2])\n        assert_allclose(input[0, :, 1:, 1:], patches[0, 3])\n\n    def test_b1_ch2_h4w4_ws3(self, device):\n        input = torch.arange(16.).view(1, 1, 4, 4).to(device)\n        input = input.expand(-1, 2, -1, -1)  # copy all channels\n        m = kornia.contrib.ExtractTensorPatches(3)\n        patches = m(input)\n        assert patches.shape == (1, 4, 2, 3, 3)\n        assert_allclose(input[0, :, :3, :3], patches[0, 0])\n        assert_allclose(input[0, :, :3, 1:], patches[0, 1])\n        assert_allclose(input[0, :, 1:, :3], patches[0, 2])\n        assert_allclose(input[0, :, 1:, 1:], patches[0, 3])\n\n    def test_b1_ch1_h4w4_ws2(self, device):\n        input = torch.arange(16.).view(1, 1, 4, 4).to(device)\n        m = kornia.contrib.ExtractTensorPatches(2)\n        patches = m(input)\n        assert patches.shape == (1, 9, 1, 2, 2)\n        assert_allclose(input[0, :, 0:2, 1:3], patches[0, 1])\n        assert_allclose(input[0, :, 0:2, 2:4], patches[0, 2])\n        assert_allclose(input[0, :, 1:3, 1:3], patches[0, 4])\n        assert_allclose(input[0, :, 2:4, 1:3], patches[0, 7])\n\n    def test_b1_ch1_h4w4_ws2_stride2(self, device):\n        input = torch.arange(16.).view(1, 1, 4, 4).to(device)\n        m = kornia.contrib.ExtractTensorPatches(2, stride=2)\n        patches = m(input)\n        assert patches.shape == (1, 4, 1, 2, 2)\n        assert_allclose(input[0, :, 0:2, 0:2], patches[0, 0])\n        assert_allclose(input[0, :, 0:2, 2:4], patches[0, 1])\n        assert_allclose(input[0, :, 2:4, 0:2], patches[0, 2])\n        assert_allclose(input[0, :, 2:4, 2:4], patches[0, 3])\n\n    def test_b1_ch1_h4w4_ws2_stride21(self, device):\n        input = torch.arange(16.).view(1, 1, 4, 4).to(device)\n        m = kornia.contrib.ExtractTensorPatches(2, stride=(2, 1))\n        patches = m(input)\n        assert patches.shape == (1, 6, 1, 2, 2)\n        assert_allclose(input[0, :, 0:2, 1:3], patches[0, 1])\n        assert_allclose(input[0, :, 0:2, 2:4], patches[0, 2])\n        assert_allclose(input[0, :, 2:4, 0:2], patches[0, 3])\n        assert_allclose(input[0, :, 2:4, 2:4], patches[0, 5])\n\n    def test_b1_ch1_h3w3_ws2_stride1_padding1(self, device):\n        input = torch.arange(9.).view(1, 1, 3, 3).to(device)\n        m = kornia.contrib.ExtractTensorPatches(2, stride=1, padding=1)\n        patches = m(input)\n        assert patches.shape == (1, 16, 1, 2, 2)\n        assert_allclose(input[0, :, 0:2, 0:2], patches[0, 5])\n        assert_allclose(input[0, :, 0:2, 1:3], patches[0, 6])\n        assert_allclose(input[0, :, 1:3, 0:2], patches[0, 9])\n        assert_allclose(input[0, :, 1:3, 1:3], patches[0, 10])\n\n    def test_b2_ch1_h3w3_ws2_stride1_padding1(self, device):\n        batch_size = 2\n        input = torch.arange(9.).view(1, 1, 3, 3).to(device)\n        input = input.expand(batch_size, -1, -1, -1)\n        m = kornia.contrib.ExtractTensorPatches(2, stride=1, padding=1)\n        patches = m(input)\n        assert patches.shape == (batch_size, 16, 1, 2, 2)\n        for i in range(batch_size):\n            assert_allclose(\n                input[i, :, 0:2, 0:2], patches[i, 5])\n            assert_allclose(\n                input[i, :, 0:2, 1:3], patches[i, 6])\n            assert_allclose(\n                input[i, :, 1:3, 0:2], patches[i, 9])\n            assert_allclose(\n                input[i, :, 1:3, 1:3], patches[i, 10])\n\n    def test_b1_ch1_h3w3_ws23(self, device):\n        input = torch.arange(9.).view(1, 1, 3, 3).to(device)\n        m = kornia.contrib.ExtractTensorPatches((2, 3))\n        patches = m(input)\n        assert patches.shape == (1, 2, 1, 2, 3)\n        assert_allclose(input[0, :, 0:2, 0:3], patches[0, 0])\n        assert_allclose(input[0, :, 1:3, 0:3], patches[0, 1])\n\n    def test_b1_ch1_h3w4_ws23(self, device):\n        input = torch.arange(12.).view(1, 1, 3, 4).to(device)\n        m = kornia.contrib.ExtractTensorPatches((2, 3))\n        patches = m(input)\n        assert patches.shape == (1, 4, 1, 2, 3)\n        assert_allclose(input[0, :, 0:2, 0:3], patches[0, 0])\n        assert_allclose(input[0, :, 0:2, 1:4], patches[0, 1])\n        assert_allclose(input[0, :, 1:3, 0:3], patches[0, 2])\n        assert_allclose(input[0, :, 1:3, 1:4], patches[0, 3])\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input: torch.Tensor, height: int,\n                      width: int) -> torch.Tensor:\n            return kornia.denormalize_pixel_coordinates(input, height, width)\n        height, width = 3, 4\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=True).to(device)\n\n        actual = op_script(grid, height, width)\n        expected = kornia.denormalize_pixel_coordinates(\n            grid, height, width)\n\n        assert_allclose(actual, expected)\n\n    def test_gradcheck(self, device):\n        input = torch.rand(2, 3, 4, 4).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.contrib.extract_tensor_patches,\n                         (input, 3,), raise_exception=True)\n'"
test/test_losses.py,87,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport math\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestFocalLoss:\n    def test_smoke_none(self, device):\n        num_classes = 3\n        logits = torch.rand(2, num_classes, 3, 2).to(device)\n        labels = torch.rand(2, 3, 2) * num_classes\n        labels = labels.to(device).long()\n\n        assert kornia.losses.focal_loss(\n            logits, labels, alpha=0.5, gamma=2.0, reduction=""none""\n        ).shape == (2, 3, 2)\n\n    def test_smoke_sum(self, device):\n        num_classes = 3\n        logits = torch.rand(2, num_classes, 3, 2).to(device)\n        labels = torch.rand(2, 3, 2) * num_classes\n        labels = labels.to(device).long()\n\n        assert (\n            kornia.losses.focal_loss(\n                logits, labels, alpha=0.5, gamma=2.0, reduction=""sum""\n            ).shape == ()\n        )\n\n    def test_smoke_mean(self, device):\n        num_classes = 3\n        logits = torch.rand(2, num_classes, 3, 2).to(device)\n        labels = torch.rand(2, 3, 2) * num_classes\n        labels = labels.to(device).long()\n\n        assert (\n            kornia.losses.focal_loss(\n                logits, labels, alpha=0.5, gamma=2.0, reduction=""mean""\n            ).shape == ()\n        )\n\n    def test_smoke_mean_flat(self, device):\n        num_classes = 3\n        logits = torch.rand(2, num_classes).to(device)\n        labels = torch.rand(2) * num_classes\n        labels = labels.to(device).long()\n        assert (\n            kornia.losses.focal_loss(\n                logits, labels, alpha=0.5, gamma=2.0, reduction=""mean""\n            ).shape == ()\n        )\n\n    # TODO: implement me\n    def test_jit(self, device):\n        pass\n\n    def test_gradcheck(self, device):\n        num_classes = 3\n        alpha, gamma = 0.5, 2.0  # for focal loss\n        logits = torch.rand(2, num_classes, 3, 2).to(device)\n        labels = torch.rand(2, 3, 2) * num_classes\n        labels = labels.to(device).long()\n\n        logits = utils.tensor_to_gradcheck_var(logits)  # to var\n        assert gradcheck(\n            kornia.losses.focal_loss,\n            (logits, labels, alpha, gamma),\n            raise_exception=True,\n        )\n\n\nclass TestTverskyLoss:\n    def test_smoke(self, device):\n        num_classes = 3\n        logits = torch.rand(2, num_classes, 3, 2).to(device)\n        labels = torch.rand(2, 3, 2) * num_classes\n        labels = labels.to(device).long()\n\n        criterion = kornia.losses.TverskyLoss(alpha=0.5, beta=0.5)\n        loss = criterion(logits, labels)\n\n    def test_all_zeros(self, device):\n        num_classes = 3\n        logits = torch.zeros(2, num_classes, 1, 2).to(device)\n        logits[:, 0] = 10.0\n        logits[:, 1] = 1.0\n        logits[:, 2] = 1.0\n        labels = torch.zeros(2, 1, 2, dtype=torch.int64).to(device)\n\n        criterion = kornia.losses.TverskyLoss(alpha=0.5, beta=0.5)\n        loss = criterion(logits, labels)\n        assert pytest.approx(loss.item(), 0.0)\n\n    # TODO: implement me\n    def test_jit(self, device):\n        pass\n\n    def test_gradcheck(self, device):\n        num_classes = 3\n        alpha, beta = 0.5, 0.5  # for tversky loss\n        logits = torch.rand(2, num_classes, 3, 2).to(device)\n        labels = torch.rand(2, 3, 2) * num_classes\n        labels = labels.to(device).long()\n\n        logits = utils.tensor_to_gradcheck_var(logits)  # to var\n        assert gradcheck(\n            kornia.losses.tversky_loss,\n            (logits, labels, alpha, beta),\n            raise_exception=True,\n        )\n\n\nclass TestDiceLoss:\n    def test_smoke(self, device):\n        num_classes = 3\n        logits = torch.rand(2, num_classes, 3, 2).to(device)\n        labels = torch.rand(2, 3, 2) * num_classes\n        labels = labels.to(device).long()\n\n        criterion = kornia.losses.DiceLoss()\n        loss = criterion(logits, labels)\n\n    def test_all_zeros(self, device):\n        num_classes = 3\n        logits = torch.zeros(2, num_classes, 1, 2).to(device)\n        logits[:, 0] = 10.0\n        logits[:, 1] = 1.0\n        logits[:, 2] = 1.0\n        labels = torch.zeros(2, 1, 2, dtype=torch.int64).to(device)\n\n        criterion = kornia.losses.DiceLoss()\n        loss = criterion(logits, labels)\n        assert pytest.approx(loss.item(), 0.0)\n\n    # TODO: implement me\n    def test_jit(self, device):\n        pass\n\n    def test_gradcheck(self, device):\n        num_classes = 3\n        logits = torch.rand(2, num_classes, 3, 2).to(device)\n        labels = torch.rand(2, 3, 2) * num_classes\n        labels = labels.to(device).long()\n\n        logits = utils.tensor_to_gradcheck_var(logits)  # to var\n        assert gradcheck(\n            kornia.losses.dice_loss, (logits, labels), raise_exception=True\n        )\n\n\nclass TestDepthSmoothnessLoss:\n    @pytest.mark.parametrize(""data_shape"", [(1, 1, 10, 16), (2, 4, 8, 15)])\n    def test_smoke(self, device, data_shape):\n        image = torch.rand(data_shape).to(device)\n        depth = torch.rand(data_shape).to(device)\n\n        criterion = kornia.losses.InverseDepthSmoothnessLoss()\n        loss = criterion(depth, image)\n\n    # TODO: implement me\n    def test_1(self, device):\n        pass\n\n    # TODO: implement me\n    def test_jit(self, device):\n        pass\n\n    def test_gradcheck(self, device):\n        image = torch.rand(1, 1, 10, 16).to(device)\n        depth = torch.rand(1, 1, 10, 16).to(device)\n        depth = utils.tensor_to_gradcheck_var(depth)  # to var\n        image = utils.tensor_to_gradcheck_var(image)  # to var\n        assert gradcheck(\n            kornia.losses.inverse_depth_smoothness_loss,\n            (depth, image),\n            raise_exception=True,\n        )\n\n\nclass TestSSIMLoss:\n\n    def test_ssim_equal_none(self, device):\n        # input data\n        img1 = torch.rand(1, 1, 10, 16).to(device)\n        img2 = torch.rand(1, 1, 10, 16).to(device)\n\n        ssim1 = kornia.ssim(img1, img1, window_size=5, reduction=""none"")\n        ssim2 = kornia.ssim(img2, img2, window_size=5, reduction=""none"")\n\n        assert_allclose(ssim1, torch.zeros_like(img1))\n        assert_allclose(ssim2, torch.zeros_like(img2))\n\n    @pytest.mark.parametrize(""window_size"", [5, 11])\n    @pytest.mark.parametrize(""reduction_type"", [""mean"", ""sum""])\n    @pytest.mark.parametrize(""batch_shape"", [(1, 1, 10, 16), (2, 4, 8, 15)])\n    def test_ssim(self, device, batch_shape, window_size, reduction_type):\n        # input data\n        img = torch.rand(batch_shape).to(device)\n\n        ssim = kornia.losses.SSIM(window_size, reduction_type)\n        assert_allclose(ssim(img, img).item(), 0.0)\n\n    def test_gradcheck(self, device):\n        # input data\n        window_size = 3\n        img1 = torch.rand(1, 1, 10, 16).to(device)\n        img2 = torch.rand(1, 1, 10, 16).to(device)\n\n        # evaluate function gradient\n        img1 = utils.tensor_to_gradcheck_var(img1)  # to var\n        img2 = utils.tensor_to_gradcheck_var(img2, requires_grad=False)  # to var\n        assert gradcheck(kornia.ssim, (img1, img2, window_size), raise_exception=True)\n\n\nclass TestDivergenceLoss:\n    @pytest.mark.parametrize(\'input,target,expected\', [\n        (torch.full((1, 1, 2, 4), 0.125), torch.full((1, 1, 2, 4), 0.125), 0.0),\n        (torch.full((1, 7, 2, 4), 0.125), torch.full((1, 7, 2, 4), 0.125), 0.0),\n        (torch.full((1, 7, 2, 4), 0.125), torch.zeros((1, 7, 2, 4)), 0.346574),\n        (torch.zeros((1, 7, 2, 4)), torch.full((1, 7, 2, 4), 0.125), 0.346574),\n    ])\n    def test_js_div_loss_2d(self, device, input, target, expected):\n        actual = kornia.losses.js_div_loss_2d(input.to(device), target.to(device)).item()\n        assert_allclose(actual, expected)\n\n    @pytest.mark.parametrize(\'input,target,expected\', [\n        (torch.full((1, 1, 2, 4), 0.125), torch.full((1, 1, 2, 4), 0.125), 0.0),\n        (torch.full((1, 7, 2, 4), 0.125), torch.full((1, 7, 2, 4), 0.125), 0.0),\n        (torch.full((1, 7, 2, 4), 0.125), torch.zeros((1, 7, 2, 4)), 0.0),\n        (torch.zeros((1, 7, 2, 4)), torch.full((1, 7, 2, 4), 0.125), math.inf),\n    ])\n    def test_kl_div_loss_2d(self, device, input, target, expected):\n        actual = kornia.losses.kl_div_loss_2d(input.to(device), target.to(device)).item()\n        assert_allclose(actual, expected)\n\n    @pytest.mark.parametrize(\'input,target,expected\', [\n        (torch.full((1, 1, 2, 4), 0.125),\n         torch.full((1, 1, 2, 4), 0.125),\n         torch.full((1, 1), 0.0)),\n        (torch.full((1, 7, 2, 4), 0.125),\n         torch.full((1, 7, 2, 4), 0.125),\n         torch.full((1, 7), 0.0)),\n        (torch.full((1, 7, 2, 4), 0.125),\n         torch.zeros((1, 7, 2, 4)),\n         torch.full((1, 7), 0.0)),\n        (torch.zeros((1, 7, 2, 4)),\n         torch.full((1, 7, 2, 4), 0.125),\n         torch.full((1, 7), math.inf)),\n    ])\n    def test_kl_div_loss_2d_without_reduction(self, device, input, target, expected):\n        actual = kornia.losses.kl_div_loss_2d(input.to(device), target.to(device), reduction=\'none\')\n        assert_allclose(actual, expected.to(device))\n\n    def test_gradcheck_kl(self, device):\n        input = torch.rand(1, 1, 10, 16).to(device)\n        target = torch.rand(1, 1, 10, 16).to(device)\n\n        # evaluate function gradient\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        target = utils.tensor_to_gradcheck_var(target)  # to var\n        assert gradcheck(kornia.losses.kl_div_loss_2d, (input, target),\n                         raise_exception=True)\n\n    def test_gradcheck_js(self, device):\n        input = torch.rand(1, 1, 10, 16).to(device)\n        target = torch.rand(1, 1, 10, 16).to(device)\n\n        # evaluate function gradient\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        target = utils.tensor_to_gradcheck_var(target)  # to var\n        assert gradcheck(kornia.losses.js_div_loss_2d, (input, target),\n                         raise_exception=True)\n\n    def test_jit_trace_kl(self, device, dtype):\n        input = torch.randn((2, 4, 10, 16), dtype=dtype, device=device)\n        target = torch.randn((2, 4, 10, 16), dtype=dtype, device=device)\n        args = (input, target)\n        op = kornia.losses.kl_div_loss_2d\n        op_jit = torch.jit.trace(op, args)\n        assert_allclose(op(*args), op_jit(*args), rtol=0, atol=1e-5)\n\n    def test_jit_trace_js(self, device, dtype):\n        input = torch.randn((2, 4, 10, 16), dtype=dtype, device=device)\n        target = torch.randn((2, 4, 10, 16), dtype=dtype, device=device)\n        args = (input, target)\n        op = kornia.losses.js_div_loss_2d\n        op_jit = torch.jit.trace(op, args)\n        assert_allclose(op(*args), op_jit(*args), rtol=0, atol=1e-5)\n\n\nclass TestTotalVariation:\n    # Total variation of constant vectors is 0\n    @pytest.mark.parametrize(\'input,expected\', [\n        (torch.ones(3, 4, 5), torch.zeros(())),\n        (2 * torch.ones(2, 3, 4, 5), torch.zeros(2)),\n    ])\n    def test_tv_on_constant(self, device, input, expected):\n        actual = kornia.losses.total_variation(input.to(device))\n        assert_allclose(actual, expected.to(device))\n\n    # Total variation for 3D tensors\n    @pytest.mark.parametrize(\'input,expected\', [\n        (torch.tensor([[[0.11747694, 0.5717714, 0.89223915, 0.2929412, 0.63556224],\n                        [0.5371079, 0.13416398, 0.7782737, 0.21392655, 0.1757018],\n                        [0.62360305, 0.8563448, 0.25304103, 0.68539226, 0.6956515],\n                        [0.9350611, 0.01694632, 0.78724295, 0.4760313, 0.73099905]],\n\n                       [[0.4788819, 0.45253807, 0.932798, 0.5721999, 0.7612051],\n                        [0.5455887, 0.8836531, 0.79551977, 0.6677338, 0.74293613],\n                        [0.4830376, 0.16420758, 0.15784949, 0.21445751, 0.34168917],\n                        [0.8675162, 0.5468113, 0.6117004, 0.01305223, 0.17554593]],\n\n                       [[0.6423703, 0.5561105, 0.54304767, 0.20339686, 0.8553698],\n                        [0.98024786, 0.31562763, 0.10122144, 0.17686582, 0.26260805],\n                        [0.20522952, 0.14523649, 0.8601968, 0.02593213, 0.7382898],\n                        [0.71935296, 0.9625162, 0.42287344, 0.07979459, 0.9149871]]]), torch.tensor(33.001236)),\n        (torch.tensor([[[0.09094203, 0.32630223, 0.8066123],\n                        [0.10921168, 0.09534764, 0.48588026]]]), torch.tensor(1.6900232)),\n    ])\n    def test_tv_on_3d(self, device, input, expected):\n        assert_allclose(kornia.losses.total_variation(input.to(device)), expected.to(device))\n\n    # Total variation for 4D tensors\n    @pytest.mark.parametrize(\'input,expected\', [\n        (torch.tensor([[[[0.8756, 0.0920],\n                         [0.8034, 0.3107]],\n                        [[0.3069, 0.2981],\n                         [0.9399, 0.7944]],\n                        [[0.6269, 0.1494],\n                         [0.2493, 0.8490]]],\n                       [[[0.3256, 0.9923],\n                         [0.2856, 0.9104]],\n                        [[0.4107, 0.4387],\n                           [0.2742, 0.0095]],\n                        [[0.7064, 0.3674],\n                           [0.6139, 0.2487]]]]), torch.tensor([5.0054283, 3.1870906])),\n        (torch.tensor([[[[0.1104, 0.2284, 0.4371],\n                         [0.4569, 0.1906, 0.8035]]],\n                       [[[0.0552, 0.6831, 0.8310],\n                         [0.3589, 0.5044, 0.0802]]],\n                       [[[0.5078, 0.5703, 0.9110],\n                         [0.4765, 0.8401, 0.2754]]]]), torch.tensor([1.9565653, 2.5786452, 2.2681699])),\n    ])\n    def test_tv_on_4d(self, device, input, expected):\n        assert_allclose(kornia.losses.total_variation(input.to(device)), expected.to(device))\n\n    # Expect ValueError to be raised when tensors of ndim != 3 or 4 are passed\n    @pytest.mark.parametrize(\'input\', [\n        torch.rand(2, 3, 4, 5, 3),\n        torch.rand(3, 1),\n    ])\n    def test_tv_on_invalid_dims(self, device, input):\n        with pytest.raises(ValueError) as ex_info:\n            kornia.losses.total_variation(input.to(device))\n\n    # Expect TypeError to be raised when non-torch tensors are passed\n    @pytest.mark.parametrize(\'input\', [\n        1,\n        [1, 2],\n    ])\n    def test_tv_on_invalid_types(self, input):\n        with pytest.raises(TypeError) as ex_info:\n            kornia.losses.total_variation(input)\n\n\nclass TestPSNRLoss:\n    def test_smoke(self, device):\n        input = torch.rand(2, 3, 3, 2).to(device)\n        target = torch.rand(2, 3, 3, 2).to(device)\n\n        criterion = kornia.losses.PSNRLoss(1.0)\n        loss = criterion(input, target)\n\n        assert loss.shape == tuple()\n\n    def test_same_input(self, device):\n        input = torch.rand(2, 3, 3, 2).to(device)\n        target = input.clone()\n\n        criterion = kornia.losses.PSNRLoss(1.0)\n        loss = criterion(input, target)\n\n        assert_allclose(loss, torch.tensor(float(\'inf\')).to(device))\n\n    def test_type(self):\n        # Expecting an exception\n        # since we pass integers instead of torch tensors\n        criterion = kornia.losses.PSNRLoss(1.0)\n        with pytest.raises(Exception) as e:\n            criterion(1, 2)\n\n    def test_shape(self):\n        # Expecting an exception\n        # since we pass tensors of different shapes\n        criterion = kornia.losses.PSNRLoss(1.0)\n        with pytest.raises(Exception) as e:\n            criterion(torch.rand(2, 3, 3, 2), torch.rand(2, 3, 3))\n\n    def test_simple(self, device):\n        assert_allclose(\n            kornia.losses.psnr_loss(\n                torch.ones(1).to(device),\n                1.2 * torch.ones(1).to(device),\n                2),\n            torch.tensor(20.0).to(device))\n\n    @pytest.mark.skip(reason=""TODO: implement me"")\n    def test_jit(self):\n        pass\n\n    def test_gradcheck(self, device):\n        input = torch.rand(2, 3, 3, 2).to(device)\n        target = torch.rand(2, 3, 3, 2).to(device)\n\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        target = utils.tensor_to_gradcheck_var(target)  # to var\n        assert gradcheck(\n            kornia.losses.psnr_loss, (input, target, 1.0), raise_exception=True\n        )\n'"
tutorials/color_adjust.py,9,"b'""""""\nColor adjustment\n================\n\nIn this tutorial we are going to learn how to adjust the color in image batches.\n\n""""""\n\nfrom matplotlib import pyplot as plt\nimport cv2\nimport numpy as np\n\nimport torch\nimport kornia\nimport torchvision\n\n#############################\n# We use OpenCV to load an image to memory represented in a numpy.ndarray\nimg_bgr: np.ndarray = cv2.imread(\'./data/ninja_turtles.jpg\', cv2.IMREAD_COLOR)\n\n#############################\n# Convert the numpy array to torch\nx_bgr: torch.Tensor = kornia.image_to_tensor(img_bgr)\nx_rgb: torch.Tensor = kornia.bgr_to_rgb(x_bgr)\n\n#############################\n# Create batch and normalize\nx_rgb = x_rgb.expand(4, -1, -1, -1)  # 4xCxHxW\nx_rgb = x_rgb.float() / 255.\n\n\ndef imshow(input: torch.Tensor):\n    out: torch.Tensor = torchvision.utils.make_grid(input, nrow=2, padding=5)\n    out_np: np.ndarray = kornia.tensor_to_image(out)\n    plt.imshow(out_np)\n    plt.axis(\'off\')\n    plt.show()\n\n#############################\n# Show original\nimshow(x_rgb)\n\n#############################\n# Adjust Brightness\nx_brightness: torch.Tensor = kornia.adjust_brightness(x_rgb, 0.6)\nimshow(x_brightness)\n\n#############################\n# Adjust Contrast\nx_contrast: torch.Tensor = kornia.adjust_contrast(x_rgb, 0.2)\nimshow(x_contrast)\n\n#############################\n# Adjust Gamma\nx_gamma: torch.Tensor = kornia.adjust_gamma(x_rgb, gamma=3., gain=1.5)\nimshow(x_gamma)\n\n#############################\n# Adjust Saturation\nx_saturated: torch.Tensor = kornia.adjust_saturation(x_rgb, 0.2)\nimshow(x_saturated)\n\n#############################\n# Adjust Hue\nx_hue: torch.Tensor = kornia.adjust_hue(x_rgb, 0.5)\nimshow(x_hue)\n'"
tutorials/color_conversions.py,12,"b'""""""\nColor space conversions\n=======================\n\nIn this tutorial we are going to learn how to convert image from different image spaces using `kornia.color`.\n\n""""""\n\nfrom matplotlib import pyplot as plt\nimport cv2\nimport numpy as np\n\nimport torch\nimport kornia\nimport torchvision\n\n#############################\n# We use OpenCV to load an image to memory represented in a numpy.ndarray\nimg_bgr: np.ndarray = cv2.imread(\'./data/simba.png\', cv2.IMREAD_COLOR)\n\n#############################\n# Convert the numpy array to torch\nx_bgr: torch.Tensor = kornia.image_to_tensor(img_bgr, keepdim=False)\n\n#############################\n# Using `kornia` we easily perform color transformation in batch mode.\n\n\ndef hflip(input: torch.Tensor) -> torch.Tensor:\n    return torch.flip(input, [-1])\n\n\ndef vflip(input: torch.Tensor) -> torch.Tensor:\n    return torch.flip(input, [-2])\n\n\ndef rot180(input: torch.Tensor) -> torch.Tensor:\n    return torch.flip(input, [-2, -1])\n\n\ndef imshow(input: torch.Tensor):\n    out: torch.Tensor = torchvision.utils.make_grid(input, nrow=2, padding=5)\n    out_np: np.ndarray = kornia.tensor_to_image(out)\n    plt.imshow(out_np)\n    plt.axis(\'off\')\n    plt.show()\n\n#############################\n# Create a batch of images\nxb_bgr = torch.cat([x_bgr, hflip(x_bgr), vflip(x_bgr), rot180(x_bgr)])\nimshow(xb_bgr)\n\n#############################\n# Convert BGR to RGB\nxb_rgb = kornia.bgr_to_rgb(xb_bgr)\nimshow(xb_rgb)\n\n#############################\n# Convert RGB to grayscale\n# NOTE: image comes in torch.uint8, and kornia assumes floating point type\nxb_gray = kornia.rgb_to_grayscale(xb_rgb.float() / 255.)\nimshow(xb_gray)\n\n#############################\n# Convert RGB to HSV\nxb_hsv = kornia.rgb_to_hsv(xb_rgb.float() / 255.)\nimshow(xb_hsv[:, 2:3])\n\n#############################\n# Convert RGB to YUV\n# NOTE: image comes in torch.uint8, and kornia assumes floating point type\nyuv = kornia.rgb_to_yuv(xb_rgb.float() / 255.)\ny_channel = torchvision.utils.make_grid(yuv, nrow=2)[0, :, :]\nplt.imshow(y_channel, cmap=\'gray\', vmin=0, vmax=1)  # Displaying only y channel\nplt.axis(\'off\')\nplt.show()\n'"
tutorials/data_augmentation.py,6,"b'""""""\nData augmentation on the GPU\n============================\n\nIn this data you learn how to use `kornia` modules in order to perform the data augmentatio on the GPU in batch mode.\n""""""\n\n################################\n# 1. Create a dummy data loader\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# from: https://gist.github.com/edgarriba/a781de516c508826f79568d08598efdb\n\n\nclass DummyDataset(Dataset):\n    def __init__(self, data_root=None):\n        self.data_root = data_root\n        self.data_index = self.build_index(self.data_root)\n\n    def build_index(self, data_root):\n        return range(10)\n\n    def __len__(self):\n        return len(self.data_index)\n\n    def __getitem__(self, idx):\n        # get data sample\n        sample = self.data_index[idx]\n\n        # load data, NOTE: modify by cv2.imread(...)\n        image = torch.rand(3, 240, 320)\n        label = torch.rand(1, 240, 320)\n        return dict(images=image, labels=label)\n\n################################\n# 2. Define the data augmentation operations\n# Thanks to the `kornia` design all the operators can be placed inside inside a `nn.Sequential`.\n\nimport kornia\n\ntransform = nn.Sequential(\n    kornia.color.AdjustBrightness(0.5),\n    kornia.color.AdjustGamma(gamma=2.),\n    kornia.color.AdjustContrast(0.7),\n)\n\n################################\n# 3. Run the dataset and perform the data augmentation\n\n# NOTE: change device to \'cuda\'\ndevice = torch.device(\'cpu\')\nprint(f""Running with device: {device}"")\n\n# create the dataloader\ndataset = DummyDataset()\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n\n# get samples and perform the data augmentation\nfor i_batch, sample_batched in enumerate(dataloader):\n    images = sample_batched[\'images\'].to(device)\n    labels = sample_batched[\'labels\'].to(device)\n\n    # perform the transforms\n    images = transform(images)\n    labels = transform(labels)\n\n    print(f""Iteration: {i_batch} Image shape: {images.shape}"")\n'"
tutorials/filter_blurring.py,7,"b'""""""\nFiltering Operations\n====================\n\nIn this tutorial you are going to learn how to use the different filtering operations found in `kornia.filters`.\n\n""""""\n\nfrom matplotlib import pyplot as plt\nimport cv2\nimport numpy as np\n\nimport torch\nimport kornia\nimport torchvision\n\n#############################\n# We use OpenCV to load an image to memory represented in a numpy.ndarray\nimg_bgr: np.ndarray = cv2.imread(\'./data/drslump.jpg\', cv2.IMREAD_COLOR)\n\n#############################\n# Convert the numpy array to torch\nx_bgr: torch.Tensor = kornia.image_to_tensor(img_bgr)\nx_rgb: torch.Tensor = kornia.bgr_to_rgb(x_bgr)\n\n#############################\n# Create batch and normalize\nx_rgb = x_rgb.expand(2, -1, -1, -1)  # 4xCxHxW\nx_rgb = x_rgb.float() / 255.\n\n\ndef imshow(input: torch.Tensor):\n    out: torch.Tensor = torchvision.utils.make_grid(input, nrow=2, padding=1)\n    out_np: np.ndarray = kornia.tensor_to_image(out)\n    plt.imshow(out_np)\n    plt.axis(\'off\')\n\n#############################\n# Show original\nimshow(x_rgb)\n\n#############################\n# Box Blur\nx_blur: torch.Tensor = kornia.box_blur(x_rgb, (9, 9))\nimshow(x_blur)\n\n#############################\n# Median Blur\nx_blur: torch.Tensor = kornia.median_blur(x_rgb, (5, 5))\nimshow(x_blur)\n\n#############################\n# Gaussian Blur\nx_blur: torch.Tensor = kornia.gaussian_blur2d(x_rgb, (11, 11), (11., 11.))\nimshow(x_blur)\n'"
tutorials/filter_edges.py,8,"b'""""""\nCompute filter edges\n====================\n\nIn this tutorial we are going to learn how to compute the first order and second\norder derivatives of an image using `kornia.filters`.\n""""""\n\nfrom matplotlib import pyplot as plt\nimport cv2\nimport numpy as np\n\nimport torch\nimport kornia\nimport torchvision\n\n#############################\n# We use OpenCV to load an image to memory represented in a numpy.ndarray\nimg_bgr: np.ndarray = cv2.imread(\'./data/doraemon.png\', cv2.IMREAD_COLOR)\n\n#############################\n# Convert the numpy array to torch\nx_bgr: torch.Tensor = kornia.image_to_tensor(img_bgr)\nx_rgb: torch.Tensor = kornia.bgr_to_rgb(x_bgr)\n\n#############################\n# Create batch and normalize\nx_rgb = x_rgb.expand(2, -1, -1, -1)  # 4xCxHxW\nx_gray = kornia.rgb_to_grayscale(x_rgb.float() / 255.)\n\n\ndef imshow(input: torch.Tensor):\n    out: torch.Tensor = torchvision.utils.make_grid(input, nrow=2, padding=1)\n    out_np: np.ndarray = kornia.tensor_to_image(out)\n    plt.imshow(out_np)\n    plt.axis(\'off\')\n\n#############################\n# Show original\nimshow(x_rgb)\n\n#################################\n# Compute the 1st order derivates\ngrads: torch.Tensor = kornia.spatial_gradient(x_gray, order=1)  # BxCx2xHxW\ngrads_x = grads[:, :, 0]\ngrads_y = grads[:, :, 1]\n\n#################################\n# Show first derivatives in x\nimshow(grads_x)\n\n#################################\n# Show first derivatives in y\nimshow(grads_y)\n\n#################################\n# Sobel Edges\n# Once with the gradients in the two directions we can computet the Sobel edges.\n# However, in kornia we already have it implemented.\nx_sobel: torch.Tensor = kornia.sobel(x_gray)\nimshow(x_sobel)\n\n#################################\n# Compute the 2nd order derivates\ngrads: torch.Tensor = kornia.spatial_gradient(x_gray, order=2)  # BxCx2xHxW\ngrads_x = grads[:, :, 0]\ngrads_y = grads[:, :, 1]\n\n#################################\n# Show second derivatives in x\nimshow(grads_x)\n\n#################################\n# Show second derivatives in y\nimshow(grads_y)\n\n#################################\n# Comute Laplacian Edges\nx_laplacian: torch.Tensor = kornia.laplacian(x_gray, kernel_size=5)\nimshow(x_laplacian)\n'"
tutorials/gaussian_blur.py,2,"b'""""""\n\nBlur image using GaussianBlur operator\n======================================\n\n""""""\n\nimport torch\nimport kornia\nimport cv2\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n# read the image with OpenCV\nimg: np.ndarray = cv2.imread(\'./data/lena.jpg\')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# convert to torch tensor\ndata: torch.tensor = kornia.image_to_tensor(img, keepdim=False)  # BxCxHxW\n\n# create the operator\ngauss = kornia.filters.GaussianBlur2d((11, 11), (10.5, 10.5))\n\n# blur the image\nx_blur: torch.tensor = gauss(data.float())\n\n# convert back to numpy\nimg_blur: np.ndarray = kornia.tensor_to_image(x_blur.byte())\n\n# Create the plot\nfig, axs = plt.subplots(1, 2, figsize=(16, 10))\naxs = axs.ravel()\n\naxs[0].axis(\'off\')\naxs[0].set_title(\'image source\')\naxs[0].imshow(img)\n\naxs[1].axis(\'off\')\naxs[1].set_title(\'image blurred\')\naxs[1].imshow(img_blur)\n'"
tutorials/hello_world.py,2,"b'""""""\n\nHello world: Planet Kornia\n==========================\n\nWelcome to Planet Kornia: a set of tutorial to learn about Computer Vision in PyTorch.\n\nThis is the first tutorial showing how one can simply load an image and convert from BGR to RGB using Kornia.\n\n""""""\n\nimport torch\nimport kornia\nimport cv2\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n#############################\n# We use OpenCV to load an image to memory represented in a numpy.ndarray\nimg_bgr: np.ndarray = cv2.imread(\'./data/arturito.jpeg\')  # HxWxC\n\n#############################\n# The image is convert to a 4D torch tensor\nx_bgr: torch.tensor = kornia.image_to_tensor(img_bgr)  # 1xCxHxW\n\n#############################\n# Once with a torch tensor we can use any Kornia operator\nx_rgb: torch.tensor = kornia.bgr_to_rgb(x_bgr)  # 1xCxHxW\n\n#############################\n# Convert back to numpy to visualize\nimg_rgb: np.ndarray = kornia.tensor_to_image(x_rgb.byte())  # HxWxC\n\n#############################\n# We use Matplotlib to visualize de results\nfig, axs = plt.subplots(1, 2, figsize=(32, 16))\naxs = axs.ravel()\n\naxs[0].axis(\'off\')\naxs[0].imshow(img_bgr)\n\naxs[1].axis(\'off\')\naxs[1].imshow(img_rgb)\n'"
tutorials/total_variation_denoising.py,5,"b'""""""\n\nDenoise image using total variation\n======================================\n\n""""""\n\nimport torch\nimport kornia\nimport cv2\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n# read the image with OpenCV\nimg: np.ndarray = cv2.imread(\'./data/doraemon.png\')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\nimg = img + np.random.normal(loc=0.0, scale=0.1, size=img.shape)\nimg = np.clip(img, 0.0, 1.0)\n\n# convert to torch tensor\nnoisy_image: torch.tensor = kornia.image_to_tensor(img).squeeze()  # CxHxW\n\n\n# define the total variation denoising network\nclass TVDenoise(torch.nn.Module):\n    def __init__(self, noisy_image):\n        super(TVDenoise, self).__init__()\n        self.l2_term = torch.nn.MSELoss(reduction=\'mean\')\n        self.regularization_term = kornia.losses.TotalVariation()\n        # create the variable which will be optimized to produce the noise free image\n        self.clean_image = torch.nn.Parameter(data=noisy_image.clone(), requires_grad=True)\n        self.noisy_image = noisy_image\n\n    def forward(self):\n        return self.l2_term(self.clean_image, self.noisy_image) + 0.0001 * self.regularization_term(self.clean_image)\n\n    def get_clean_image(self):\n        return self.clean_image\n\ntv_denoiser = TVDenoise(noisy_image)\n\n# define the optimizer to optimize the 1 parameter of tv_denoiser\noptimizer = torch.optim.SGD(tv_denoiser.parameters(), lr=0.1, momentum=0.9)\n\n# run the optimization loop\nnum_iters = 500\nfor i in range(num_iters):\n    optimizer.zero_grad()\n    loss = tv_denoiser()\n    if i % 25 == 0:\n        print(""Loss in iteration {} of {}: {:.3f}"".format(i, num_iters, loss.item()))\n    loss.backward()\n    optimizer.step()\n\n# convert back to numpy\nimg_clean: np.ndarray = kornia.tensor_to_image(tv_denoiser.get_clean_image())\n\n# Create the plot\nfig, axs = plt.subplots(1, 2, figsize=(16, 10))\naxs = axs.ravel()\n\naxs[0].axis(\'off\')\naxs[0].set_title(\'Noisy image\')\naxs[0].imshow(img)\n\naxs[1].axis(\'off\')\naxs[1].set_title(\'Cleaned image\')\naxs[1].imshow(img_clean)\n\nplt.show()\n'"
tutorials/warp_affine.py,6,"b'""""""\n\nRotate image using warp affine transform\n========================================\n\n""""""\n\nimport torch\nimport kornia\nimport cv2\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n# read the image with OpenCV\nimg: np.ndarray = cv2.imread(\'./data/bennett_aden.png\')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# convert to torch tensor\ndata: torch.tensor = kornia.image_to_tensor(img, keepdim=False)  # BxCxHxW\n\n# create transformation (rotation)\nalpha: float = 45.0  # in degrees\nangle: torch.tensor = torch.ones(1) * alpha\n\n# define the rotation center\ncenter: torch.tensor = torch.ones(1, 2)\ncenter[..., 0] = data.shape[3] / 2  # x\ncenter[..., 1] = data.shape[2] / 2  # y\n\n# define the scale factor\nscale: torch.tensor = torch.ones(1)\n\n# compute the transformation matrix\nM: torch.tensor = kornia.get_rotation_matrix2d(center, angle, scale)\n\n# apply the transformation to original image\n_, _, h, w = data.shape\ndata_warped: torch.tensor = kornia.warp_affine(data.float(), M, dsize=(h, w))\n\n# convert back to numpy\nimg_warped: np.ndarray = kornia.tensor_to_image(data_warped.byte()[0])\n\n# create the plot\nfig, axs = plt.subplots(1, 2, figsize=(16, 10))\naxs = axs.ravel()\n\naxs[0].axis(\'off\')\naxs[0].set_title(\'image source\')\naxs[0].imshow(img)\n\naxs[1].axis(\'off\')\naxs[1].set_title(\'image warped\')\naxs[1].imshow(img_warped)\n'"
tutorials/warp_perspective.py,5,"b'""""""\n\nWarp image using perspective transform\n======================================\n\n""""""\n\nimport torch\nimport kornia\nimport cv2\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n# read the image with OpenCV\nimg: np.ndarray = cv2.imread(\'./data/bruce.png\')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# convert to torch tensor\ndata: torch.tensor = kornia.image_to_tensor(img, keepdim=False)  # BxCxHxW\n\n# the source points are the region to crop corners\npoints_src = torch.tensor([[\n    [125., 150.], [562., 40.], [562., 282.], [54., 328.],\n]])\n\n# the destination points are the image vertexes\nh, w = 64, 128  # destination size\npoints_dst = torch.tensor([[\n    [0., 0.], [w - 1., 0.], [w - 1., h - 1.], [0., h - 1.],\n]])\n\n# compute perspective transform\nM: torch.tensor = kornia.get_perspective_transform(points_src, points_dst)\n\n# warp the original image by the found transform\ndata_warp: torch.tensor = kornia.warp_perspective(data.float(), M, dsize=(h, w))\n\n# convert back to numpy\nimg_warp: np.ndarray = kornia.tensor_to_image(data_warp.byte())\n\n# draw points into original image\nfor i in range(4):\n    center = tuple(points_src[0, i].long().numpy())\n    img = cv2.circle(img.copy(), center, 5, (0, 255, 0), -1)\n\n# create the plot\nfig, axs = plt.subplots(1, 2, figsize=(16, 10))\naxs = axs.ravel()\n\naxs[0].axis(\'off\')\naxs[0].set_title(\'image source\')\naxs[0].imshow(img)\n\naxs[1].axis(\'off\')\naxs[1].set_title(\'image destination\')\naxs[1].imshow(img_warp)\n'"
docs/source/conf.py,3,"b'import os\nimport sys\n\nimport torch\nimport kornia\n\nimport sphinx_gallery\nimport sphinx_rtd_theme\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\ncurrent_path = os.path.abspath(os.path.join(__file__, "".."", "".."", ""..""))\nsys.path.append(current_path)\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'nbsphinx\',\n    \'sphinxcontrib.bibtex\',\n    \'sphinx_gallery.gen_gallery\',\n]\n\nnapoleon_use_ivar = True\n\ngoogleanalytics_id = \'UA-90545585-1\'\ngoogleanalytics_enabled = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = [\'.rst\', \'.ipynb\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Kornia\'\nauthor = u\'%s developers\' % project\ncopyright = u\'2019, %s\' % author\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n\n# version = \'master (\' + kornia.__version__ + \' )\'\nversion = \'\'\n\nif \'READTHEDOCS\' not in os.environ:\n    # if developing locally, use pyro.__version__ as version\n    from kornia import __version__  # noqaE402\n    version = __version__\n\n# release = \'master\'\nrelease = version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'.ipynb_checkpoints\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n# do not prepend module name to functions\nadd_module_names = False\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': True,\n}\n\nhtml_logo = \'_static/img/kornia_logo.svg\'\nhtml_favicon = \'_static/img/kornia_logo_mini.png\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# html_style_path = \'css/pytorch_theme.css\'\nhtml_context = {\n    \'css_files\': [\n        \'https://fonts.googleapis.com/css?family=Lato\',\n        \'_static/css/pytorch_theme.css\'\n    ],\n}\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'Kornia\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'kornia.tex\', u\'Kornia\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'Kornia\', u\'Kornia Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'kornia\', \'Kornia Documentation\',\n     author, \'Kornia\', \'Differentiable Computer Vision in Pytorch.\',\n     \'Miscellaneous\'),\n]\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n    \'torch\': (\'http://pytorch.org/docs/master/\', None),\n}\n\nexamples_dir = os.path.join(current_path, ""tutorials"")\nsphinx_gallery_conf = {\n    \'doc_module\': \'kornia\',\n    \'examples_dirs\': [examples_dir],   # path to your example scripts\n    \'gallery_dirs\': [\'tutorials\'],  # path where to save gallery generated output\n    \'filename_pattern\': \'./\',\n}\n\n\n# -- A patch that prevents Sphinx from cross-referencing ivar tags -------\n# See http://stackoverflow.com/a/41184353/3343043\n\nfrom docutils import nodes\nfrom sphinx.util.docfields import TypedField\nfrom sphinx import addnodes\n\n\ndef patched_make_field(self, types, domain, items, **kw):\n    # `kw` catches `env=None` needed for newer sphinx while maintaining\n    #  backwards compatibility when passed along further down!\n\n    # type: (List, unicode, Tuple) -> nodes.field\n    def handle_item(fieldarg, content):\n        par = nodes.paragraph()\n        par += addnodes.literal_strong(\'\', fieldarg)  # Patch: this line added\n        # par.extend(self.make_xrefs(self.rolename, domain, fieldarg,\n        #                           addnodes.literal_strong))\n        if fieldarg in types:\n            par += nodes.Text(\' (\')\n            # NOTE: using .pop() here to prevent a single type node to be\n            # inserted twice into the doctree, which leads to\n            # inconsistencies later when references are resolved\n            fieldtype = types.pop(fieldarg)\n            if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):\n                typename = u\'\'.join(n.astext() for n in fieldtype)\n                typename = typename.replace(\'int\', \'python:int\')\n                typename = typename.replace(\'long\', \'python:long\')\n                typename = typename.replace(\'float\', \'python:float\')\n                typename = typename.replace(\'type\', \'python:type\')\n                par.extend(self.make_xrefs(self.typerolename, domain, typename,\n                                           addnodes.literal_emphasis, **kw))\n            else:\n                par += fieldtype\n            par += nodes.Text(\')\')\n        par += nodes.Text(\' -- \')\n        par += content\n        return par\n\n    fieldname = nodes.field_name(\'\', self.label)\n    if len(items) == 1 and self.can_collapse:\n        fieldarg, content = items[0]\n        bodynode = handle_item(fieldarg, content)\n    else:\n        bodynode = self.list_type()\n        for fieldarg, content in items:\n            bodynode += nodes.list_item(\'\', handle_item(fieldarg, content))\n    fieldbody = nodes.field_body(\'\', bodynode)\n    return nodes.field(\'\', fieldname, fieldbody)\n\n\nTypedField.make_field = patched_make_field\n\n\n# @jpchen\'s hack to get rtd builder to install latest pytorch\nif \'READTHEDOCS\' in os.environ:\n    os.system(\'pip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp27-cp27mu-linux_x86_64.whl\')\n'"
examples/depth_regression/main.py,13,"b'import argparse\nimport os\nimport cv2\nimport sys\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport kornia as tgm\n\n\ndef load_data(root_path, sequence_name, frame_id):\n    # index paths\n    file_name = \'frame_%04d\' % (frame_id)\n    image_file = os.path.join(root_path, \'clean\', sequence_name,\n                              file_name + \'.png\')\n    depth_file = os.path.join(root_path, \'depth\', sequence_name,\n                              file_name + \'.dpt\')\n    camera_file = os.path.join(root_path, \'camdata_left\', sequence_name,\n                               file_name + \'.cam\')\n    # load the actual data\n    image_tensor = load_image(image_file)\n    depth = load_depth(depth_file)\n    # load camera data and create pinhole\n    height, width = image_tensor.shape[-2:]\n    intrinsics, extrinsics = load_camera_data(camera_file)\n    camera = tgm.utils.create_pinhole(intrinsics, extrinsics, height, width)\n    return image_tensor, depth, camera\n\n\ndef load_depth(file_name):\n    """"""Loads the depth using the sintel SDK and converts to torch.Tensor\n    """"""\n    assert os.path.isfile(file_name), ""Invalid file {}"".format(file_name)\n    import sintel_io\n    depth = sintel_io.depth_read(file_name)\n    return torch.from_numpy(depth).view(1, 1, *depth.shape).float()\n\n\ndef load_camera_data(file_name):\n    """"""Loads the camera data using the sintel SDK and converts to torch.Tensor.\n    """"""\n    assert os.path.isfile(file_name), ""Invalid file {}"".format(file_name)\n    import sintel_io\n    intrinsic, extrinsic = sintel_io.cam_read(file_name)\n    return intrinsic, extrinsic\n\n\ndef load_image(file_name):\n    """"""Loads the image with OpenCV and converts to torch.Tensor\n    """"""\n    assert os.path.isfile(file_name), ""Invalid file {}"".format(file_name)\n\n    # load image with OpenCV\n    img = cv2.imread(file_name, cv2.IMREAD_COLOR)\n\n    # convert image to torch tensor\n    tensor = tgm.utils.image_to_tensor(img).float() / 255.\n    return tensor.view(1, *tensor.shape)  # 1xCxHxW\n\n\ndef clip_and_convert_tensor(tensor):\n    """"""convert the input torch.Tensor to OpenCV image,clip it to be between\n    [0, 255] and convert it to unit\n    """"""\n    img = tgm.utils.tensor_to_image(255. * tensor)  # convert tensor to numpy\n    img_cliped = np.clip(img, 0, 255)  # clip and reorder the channels\n    img = img_cliped.astype(\'uint8\')  # convert to uint\n    return img\n\n\nclass InvDepth(nn.Module):\n    def __init__(self, height, width, min_depth=0.50, max_depth=25.):\n        super(InvDepth, self).__init__()\n        self._min_range = 1. / max_depth\n        self._max_range = 1. / min_depth\n\n        self.w = nn.Parameter(self._init_weights(height, width))\n\n    def _init_weights(self, height, width):\n        r1 = self._min_range\n        r2 = self._min_range + (self._max_range - self._min_range) * 0.1\n        w_init = (r1 - r2) * torch.rand(1, 1, height, width) + r2\n        return w_init\n\n    def forward(self):\n        return self.w.clamp(min=self._min_range, max=self._max_range)\n\n\ndef DepthRegressionApp():\n    # data settings\n    parser = argparse.ArgumentParser(\n        description=\'Depth Regression with photometric loss.\')\n    parser.add_argument(\'--input-dir\', type=str, required=True,\n                        help=\'the path to the directory with the input data.\')\n    parser.add_argument(\'--output-dir\', type=str, required=True,\n                        help=\'the path to output the results.\')\n    parser.add_argument(\'--num-iterations\', type=int, default=1000, metavar=\'N\',\n                        help=\'number of training iterations (default: 1000)\')\n    parser.add_argument(\'--sequence-name\', type=str, default=\'alley_1\',\n                        help=\'the name of the sequence.\')\n    parser.add_argument(\'--frame-ref-id\', type=int, default=1,\n                        help=\'the id for the reference image in the sequence.\')\n    parser.add_argument(\n        \'--frame-i-id\',\n        type=int,\n        default=2,\n        help=\'the id for the image i in the sequence.\')\n    # optimization parameters\n    parser.add_argument(\'--lr\', type=float, default=1e-3, metavar=\'LR\',\n                        help=\'learning rate (default: 1e-3)\')\n    # device parameters\n    parser.add_argument(\'--cuda\', action=\'store_true\', default=False,\n                        help=\'enables CUDA training\')\n    parser.add_argument(\'--seed\', type=int, default=666, metavar=\'S\',\n                        help=\'random seed (default: 666)\')\n    parser.add_argument(\n        \'--log-interval\',\n        type=int,\n        default=10,\n        metavar=\'N\',\n        help=\'how many batches to wait before logging training status\')\n    parser.add_argument(\n        \'--log-interval-vis\',\n        type=int,\n        default=100,\n        metavar=\'N\',\n        help=\'how many batches to wait before visual logging training status\')\n    args = parser.parse_args()\n\n    # define the device to use for inference\n    use_cuda = args.cuda and torch.cuda.is_available()\n    device = torch.device(\'cuda\' if use_cuda else \'cpu\')\n\n    torch.manual_seed(args.seed)\n\n    # configure sintel SDK path\n    root_path = os.path.abspath(args.input_dir)\n    sys.path.append(os.path.join(root_path, \'sdk/python\'))\n\n    # load the data\n    root_dir = os.path.join(root_path, \'training\')\n    img_ref, depth_ref, cam_ref = load_data(root_dir, args.sequence_name,\n                                            args.frame_ref_id)\n    img_i, _, cam_i = load_data(root_dir, args.sequence_name, args.frame_i_id)\n\n    # instantiate the depth warper from `kornia`\n    warper = tgm.DepthWarper(cam_i)\n    warper.compute_homographies(cam_ref)\n\n    # create the inverse depth as a parameter to be optimized\n    height, width = img_ref.shape[-2:]\n    inv_depth_ref = InvDepth(height, width).to(device)\n\n    # create optimizer\n    optimizer = optim.Adam(inv_depth_ref.parameters(), lr=args.lr)\n\n    # send data to device\n    img_ref, img_i = img_ref.to(device), img_i.to(device)\n\n    # main training loop\n\n    for iter_idx in range(args.num_iterations):\n        # compute the inverse depth and warp the source image\n        img_i_to_ref = warper(inv_depth_ref(), img_i)\n\n        # compute the photometric loss\n        loss = F.l1_loss(img_i_to_ref, img_ref, reduction=\'none\')\n        loss = torch.mean(loss)\n\n        # compute gradient and update optimizer parameters\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if iter_idx % args.log_interval == 0 or \\\n           iter_idx == args.num_iterations - 1:\n            print(\'Train iteration: {}/{}\\tLoss: {:.6}\'.format(\n                  iter_idx, args.num_iterations, loss.item()))\n\n            if iter_idx % args.log_interval_vis == 0:\n                # merge warped and target image for  visualization\n                img_i_to_ref = warper(inv_depth_ref(), img_i)\n                img_both_vis = 0.5 * (img_i_to_ref + img_ref)\n\n                img_both_vis = clip_and_convert_tensor(img_both_vis)\n                img_i_to_ref_vis = clip_and_convert_tensor(img_i_to_ref)\n                inv_depth_ref_vis = tgm.utils.tensor_to_image(\n                    inv_depth_ref() / (inv_depth_ref().max() + 1e-6)).squeeze()\n                inv_depth_ref_vis = np.clip(255. * inv_depth_ref_vis, 0, 255)\n                inv_depth_ref_vis = inv_depth_ref_vis.astype(\'uint8\')\n\n                # save warped image and depth to disk\n                def file_name(x):\n                    return os.path.join(args.output_dir,\n                                        ""{0}_{1}.png"".format(x, iter_idx))\n                cv2.imwrite(file_name(""warped""), img_i_to_ref_vis)\n                cv2.imwrite(file_name(""warped_both""), img_both_vis)\n                cv2.imwrite(file_name(""inv_depth_ref""), inv_depth_ref_vis)\n\n\nif __name__ == ""__main__"":\n    DepthRegressionApp()\n'"
examples/depth_warper/main.py,10,"b'import argparse\nimport os\nimport cv2\nimport sys\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport kornia as dgm\n\n\ndef load_depth(file_name):\n    """"""Loads the depth using the syntel SDK and converts to torch.Tensor\n    """"""\n    assert os.path.isfile(file_name), ""Invalid file {}"".format(file_name)\n    import sintel_io\n    depth = sintel_io.depth_read(file_name)\n    return torch.from_numpy(depth).view(1, 1, *depth.shape).float()\n\n\ndef load_camera_data(file_name):\n    """"""Loads the camera data using the syntel SDK and converts to torch.Tensor.\n    """"""\n    assert os.path.isfile(file_name), ""Invalid file {}"".format(file_name)\n    import sintel_io\n    intrinsic, extrinsic = sintel_io.cam_read(file_name)\n    return intrinsic, extrinsic\n\n\ndef load_image(file_name):\n    """"""Loads the image with OpenCV and converts to torch.Tensor\n    """"""\n    assert os.path.isfile(file_name), ""Invalid file {}"".format(file_name)\n\n    # load image with OpenCV\n    img = cv2.imread(file_name, cv2.IMREAD_COLOR)\n\n    # convert image to torch tensor\n    tensor = dgm.utils.image_to_tensor(img).float() / 255.\n    return tensor.view(1, *tensor.shape)  # 1xCxHxW\n\n\ndef load_data(root_path, sequence_name, frame_id):\n    # index paths\n    file_name = \'frame_%04d\' % (frame_id)\n    image_file = os.path.join(root_path, \'clean\', sequence_name,\n                              file_name + \'.png\')\n    depth_file = os.path.join(root_path, \'depth\', sequence_name,\n                              file_name + \'.dpt\')\n    camera_file = os.path.join(root_path, \'camdata_left\', sequence_name,\n                               file_name + \'.cam\')\n    # load the actual data\n    image = load_image(image_file)\n    depth = load_depth(depth_file)\n    # load camera data and create pinhole\n    height, width = image.shape[-2:]\n    intrinsics, extrinsics = load_camera_data(camera_file)\n    camera = create_pinhole(intrinsics, extrinsics, height, width)\n    return image, depth, camera\n\n\ndef DepthWarperApp():\n    parser = argparse.ArgumentParser(\n        description=\'Warp images by depth application.\')\n    # data parameters\n    parser.add_argument(\'--input-dir\', type=str, required=True,\n                        help=\'the path to the directory with the input data.\')\n    parser.add_argument(\'--output-dir\', type=str, required=True,\n                        help=\'the path to output the results.\')\n    parser.add_argument(\'--sequence-name\', type=str, default=\'alley_1\',\n                        help=\'the name of the sequence.\')\n    parser.add_argument(\'--frame-ref-id\', type=int, default=1,\n                        help=\'the id for the reference image in the sequence.\')\n    parser.add_argument(\n        \'--frame-i-id\',\n        type=int,\n        default=2,\n        help=\'the id for the image i in the sequence.\')\n    # device parameters\n    parser.add_argument(\'--cuda\', action=\'store_true\', default=False,\n                        help=\'enables CUDA training\')\n    parser.add_argument(\'--seed\', type=int, default=666, metavar=\'S\',\n                        help=\'random seed (default: 666)\')\n    args = parser.parse_args()\n\n    # define the device to use for inference\n    use_cuda = args.cuda and torch.cuda.is_available()\n    device = torch.device(\'cuda\' if use_cuda else \'cpu\')\n\n    torch.manual_seed(args.seed)\n\n    # configure syntel SDK path\n    root_path = os.path.abspath(args.input_dir)\n    sys.path.append(os.path.join(root_path, \'sdk/python\'))\n\n    # load the data\n    root_dir = os.path.join(root_path, \'training\')\n    img_ref, depth_ref, cam_ref = load_data(root_dir, args.sequence_name,\n                                            args.frame_ref_id)\n    img_i, depth_i, cam_i = load_data(root_dir, args.sequence_name,\n                                      args.frame_i_id)\n\n    # instantiate the homography warper from `kornia`\n    warper = dgm.DepthWarper(cam_i)\n    warper.compute_homographies(cam_ref)\n\n    # compute the inverse depth and warp the source image\n    inv_depth_ref = 1. / depth_ref\n    img_i_to_ref = warper(inv_depth_ref, img_i)\n\n    # generate occlusion mask\n    mask = ((img_ref - img_i_to_ref).mean(1) < 1e-1).float()\n\n    img_vis_warped = 0.5 * img_i_to_ref + img_ref\n    img_vis_warped_masked = mask * (0.5 * img_i_to_ref + img_ref)\n\n    # save warped image to disk\n    file_name = os.path.join(\n        args.output_dir,\n        \'warped_{0}_to_{1}.png\'.format(\n            args.frame_i_id, args.frame_ref_id))\n    cv2.imwrite(file_name, dgm.utils.tensor_to_image(255. * img_vis_warped))\n    cv2.imwrite(file_name + \'mask.png\', dgm.utils.tensor_to_image(255. * mask))\n    cv2.imwrite(file_name + \'warpedmask.png\',\n                dgm.utils.tensor_to_image(255. * img_vis_warped_masked))\n\n\nif __name__ == ""__main__"":\n    DepthWarperApp()\n'"
examples/homography_regression/main.py,14,"b'import argparse\nimport os\nimport cv2\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport kornia as dgm\n\n\ndef load_homography(file_name):\n    """"""Loads an homography from text file.\n    """"""\n    assert os.path.isfile(file_name), ""Invalid file {}"".format(file_name)\n    return torch.from_numpy(np.loadtxt(file_name)).float()\n\n\ndef load_image(file_name):\n    """"""Loads the image with OpenCV and converts to torch.Tensor\n    """"""\n    assert os.path.isfile(file_name), ""Invalid file {}"".format(file_name)\n\n    # load image with OpenCV\n    img = cv2.imread(file_name, cv2.IMREAD_COLOR)\n\n    # convert image to torch tensor\n    tensor = dgm.utils.image_to_tensor(img).float() / 255.\n    tensor = tensor.view(1, *tensor.shape)  # 1xCxHxW\n\n    return tensor, img\n\n\nclass MyHomography(nn.Module):\n    def __init__(self):\n        super(MyHomography, self).__init__()\n        self.homo = nn.Parameter(torch.Tensor(3, 3))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.eye_(self.homo)\n\n    def forward(self):\n        return torch.unsqueeze(self.homo, dim=0)  # 1x3x3\n\n\ndef HomographyRegressionApp():\n    # Training settings\n    parser = argparse.ArgumentParser(\n        description=\'Homography Regression with photometric loss.\')\n    parser.add_argument(\'--input-dir\', type=str, required=True,\n                        help=\'the path to the directory with the input data.\')\n    parser.add_argument(\'--output-dir\', type=str, required=True,\n                        help=\'the path to output the results.\')\n    parser.add_argument(\'--num-iterations\', type=int, default=1000, metavar=\'N\',\n                        help=\'number of training iterations (default: 1000)\')\n    parser.add_argument(\'--lr\', type=float, default=1e-3, metavar=\'LR\',\n                        help=\'learning rate (default: 1e-3)\')\n    parser.add_argument(\'--cuda\', action=\'store_true\', default=False,\n                        help=\'enables CUDA training\')\n    parser.add_argument(\'--seed\', type=int, default=666, metavar=\'S\',\n                        help=\'random seed (default: 666)\')\n    parser.add_argument(\n        \'--log-interval\',\n        type=int,\n        default=10,\n        metavar=\'N\',\n        help=\'how many batches to wait before logging training status\')\n    parser.add_argument(\n        \'--log-interval-vis\',\n        type=int,\n        default=100,\n        metavar=\'N\',\n        help=\'how many batches to wait before visual logging training status\')\n    args = parser.parse_args()\n\n    # define the device to use for inference\n    use_cuda = args.cuda and torch.cuda.is_available()\n    device = torch.device(\'cuda\' if use_cuda else \'cpu\')\n\n    torch.manual_seed(args.seed)\n\n    # load the data\n    img_src, _ = load_image(os.path.join(args.input_dir, \'img1.ppm\'))\n    img_dst, _ = load_image(os.path.join(args.input_dir, \'img2.ppm\'))\n    dst_homo_src_gt = load_homography(os.path.join(args.input_dir, \'H1to2p\'))\n\n    # instantiate the homography warper from `kornia`\n    height, width = img_src.shape[-2:]\n    warper = dgm.HomographyWarper(height, width)\n\n    # create the homography as the parameter to be optimized\n    dst_homo_src = MyHomography().to(device)\n\n    # create optimizer\n    optimizer = optim.Adam(dst_homo_src.parameters(), lr=args.lr)\n\n    # main training loop\n\n    for iter_idx in range(args.num_iterations):\n        # send data to device\n        img_src, img_dst = img_src.to(device), img_dst.to(device)\n\n        # warp the reference image to the destiny with current homography\n        img_src_to_dst = warper(img_src, dst_homo_src())\n\n        # compute the photometric loss\n        loss = F.l1_loss(img_src_to_dst, img_dst, reduction=\'none\')\n\n        # propagate the error just for a fixed window\n        w_size = 100  # window size\n        h_2, w_2 = height // 2, width // 2\n        loss = loss[..., h_2 - w_size:h_2 + w_size, w_2 - w_size:w_2 + w_size]\n        loss = torch.mean(loss)\n\n        # compute gradient and update optimizer parameters\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if iter_idx % args.log_interval == 0:\n            print(\'Train iteration: {}/{}\\tLoss: {:.6}\'.format(\n                  iter_idx, args.num_iterations, loss.item()))\n            print(dst_homo_src.homo)\n\n        def draw_rectangle(image, dst_homo_src):\n            height, width = image.shape[:2]\n            pts_src = torch.FloatTensor([[\n                [-1, -1],  # top-left\n                [1, -1],  # bottom-left\n                [1, 1],  # bottom-right\n                [-1, 1],  # top-right\n            ]]).to(dst_homo_src.device)\n            # transform points\n            pts_dst = dgm.transform_points(torch.inverse(dst_homo_src), pts_src)\n\n            def compute_factor(size):\n                return 1.0 * size / 2\n\n            def convert_coordinates_to_pixel(coordinates, factor):\n                return factor * (coordinates + 1.0)\n            # compute convertion factor\n            x_factor = compute_factor(width - 1)\n            y_factor = compute_factor(height - 1)\n            pts_dst = pts_dst.cpu().squeeze().detach().numpy()\n            pts_dst[..., 0] = convert_coordinates_to_pixel(\n                pts_dst[..., 0], x_factor)\n            pts_dst[..., 1] = convert_coordinates_to_pixel(\n                pts_dst[..., 1], y_factor)\n\n            # do the actual drawing\n            for i in range(4):\n                pt_i, pt_ii = tuple(pts_dst[i % 4]), tuple(pts_dst[(i + 1) % 4])\n                image = cv2.line(image, pt_i, pt_ii, (255, 0, 0), 3)\n            return image\n\n        if iter_idx % args.log_interval_vis == 0:\n            # merge warped and target image for visualization\n            img_src_to_dst = warper(img_src, dst_homo_src())\n            img_vis = 255. * 0.5 * (img_src_to_dst + img_dst)\n            img_vis_np = dgm.utils.tensor_to_image(img_vis)\n            image_draw = draw_rectangle(img_vis_np, dst_homo_src())\n            # save warped image to disk\n            file_name = os.path.join(\n                args.output_dir, \'warped_{}.png\'.format(iter_idx))\n            cv2.imwrite(file_name, image_draw)\n\n\nif __name__ == ""__main__"":\n    HomographyRegressionApp()\n'"
kornia/augmentation/__init__.py,0,"b'from .augmentation import *\nfrom kornia.color.normalize import (\n    Normalize,\n    Denormalize\n)\n\n__all__ = [\n    ""RandomAffine"",\n    ""RandomCrop"",\n    ""RandomErasing"",\n    ""RandomGrayscale"",\n    ""RandomHorizontalFlip"",\n    ""RandomVerticalFlip"",\n    ""RandomPerspective"",\n    ""RandomResizedCrop"",\n    ""RandomRotation"",\n    ""CenterCrop"",\n    ""ColorJitter"",\n    ""RandomSolarize"",\n    ""RandomPosterize"",\n    ""RandomSharpness"",\n    ""RandomEqualize"",\n    ""Normalize"",\n    ""Denormalize""\n]\n'"
kornia/augmentation/augmentation.py,91,"b'from typing import Callable, Tuple, Union, List, Optional, Dict, cast\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import pad\n\nfrom kornia.constants import Resample, BorderType\nfrom . import functional as F\nfrom . import random_generator as rg\nfrom .utils import (\n    _adapted_uniform,\n    _infer_batch_shape\n)\nfrom .types import (\n    TupleFloat,\n    UnionFloat,\n    UnionType,\n    FloatUnionType,\n    BoarderUnionType\n)\n\n\nclass AugmentationBase(nn.Module):\n    r""""""AugmentationBase base class for customized augmentation implementations. For any augmentation,\n    the implementation of ""generate_parameters"" and ""apply_transform"" are required while the\n    ""compute_transformation"" is only required when passing ""return_transform"" as True.\n\n    Args:\n        return_transform (bool): if ``True`` return the matrix describing the geometric transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated.\n\n    """"""\n    def __init__(self, return_transform: bool = False) -> None:\n        super(AugmentationBase, self).__init__()\n        self.return_transform = return_transform\n\n    def infer_batch_shape(self, input: UnionType) -> torch.Size:\n        return _infer_batch_shape(input)\n\n    def generate_parameters(self, input_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        raise NotImplementedError\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        raise NotImplementedError\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        raise NotImplementedError\n\n    def forward(self, input: UnionType, params: Optional[Dict[str, torch.Tensor]] = None,  # type: ignore\n                return_transform: Optional[bool] = None) -> UnionType:  # type: ignore\n        if return_transform is None:\n            return_transform = self.return_transform\n        if params is None:\n            batch_shape = self.infer_batch_shape(input)\n            self._params = self.generate_parameters(batch_shape)\n        else:\n            self._params = params\n\n        if isinstance(input, tuple):\n            output = self.apply_transform(input[0], self._params)\n            transformation_matrix = self.compute_transformation(input[0], self._params)\n            if return_transform:\n                return output, input[1] @ transformation_matrix\n            else:\n                return output, input[1]\n\n        output = self.apply_transform(input, self._params)\n        if return_transform:\n            transformation_matrix = self.compute_transformation(input, self._params)\n            return output, transformation_matrix\n        return output\n\n\nclass RandomHorizontalFlip(AugmentationBase):\n\n    r""""""Horizontally flip a tensor image or a batch of tensor images randomly with a given probability.\n    Input should be a tensor of shape (C, H, W) or a batch of tensors :math:`(B, C, H, W)`.\n    If Input is a tuple it is assumed that the first element contains the aforementioned tensors and the second,\n    the corresponding transformation matrix that has been applied to them. In this case the module\n    will Horizontally flip the tensors and concatenate the corresponding transformation matrix to the\n    previous one. This is especially useful when using this functionality as part of an ``nn.Sequential`` module.\n\n    Args:\n        p (float): probability of the image being flipped. Default value is 0.5\n        return_transform (bool): if ``True`` return the matrix describing the transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Examples:\n        >>> input = torch.tensor([[[[0., 0., 0.],\n        ...                         [0., 0., 0.],\n        ...                         [0., 1., 1.]]]])\n        >>> seq = nn.Sequential(RandomHorizontalFlip(p=1.0, return_transform=True),\n        ...                     RandomHorizontalFlip(p=1.0, return_transform=True))\n        >>> seq(input)\n        (tensor([[[[0., 0., 0.],\n                  [0., 0., 0.],\n                  [0., 1., 1.]]]]), tensor([[[1., 0., 0.],\n                 [0., 1., 0.],\n                 [0., 0., 1.]]]))\n\n    """"""\n\n    def __init__(self, p: float = 0.5, return_transform: bool = False, same_on_batch: bool = False,\n                 align_corners: bool = False) -> None:\n        super(RandomHorizontalFlip, self).__init__(return_transform)\n        self.p: float = p\n        self.same_on_batch = same_on_batch\n        self.align_corners = align_corners\n\n    def __repr__(self) -> str:\n        repr = f""(p={self.p}, return_transform={self.return_transform}, same_on_batch={self.same_on_batch})""\n        return self.__class__.__name__ + repr\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_prob_generator(batch_shape[0], self.p, self.same_on_batch)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_hflip_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_hflip(input, params)\n\n\nclass RandomVerticalFlip(AugmentationBase):\n\n    r""""""Vertically flip a tensor image or a batch of tensor images randomly with a given probability.\n    Input should be a tensor of shape (C, H, W) or a batch of tensors :math:`(B, C, H, W)`.\n    If Input is a tuple it is assumed that the first element contains the aforementioned tensors and the second,\n    the corresponding transformation matrix that has been applied to them. In this case the module\n    will Vertically flip the tensors and concatenate the corresponding transformation matrix to the\n    previous one. This is especially useful when using this functionality as part of an ``nn.Sequential`` module.\n\n    Args:\n        p (float): probability of the image being flipped. Default value is 0.5\n        return_transform (bool): if ``True`` return the matrix describing the transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Examples:\n        >>> input = torch.tensor([[[[0., 0., 0.],\n        ...                         [0., 0., 0.],\n        ...                         [0., 1., 1.]]]])\n        >>> seq = RandomVerticalFlip(p=1.0, return_transform=True)\n        >>> seq(input)\n        (tensor([[[[0., 1., 1.],\n                  [0., 0., 0.],\n                  [0., 0., 0.]]]]), tensor([[[ 1.,  0.,  0.],\n                 [ 0., -1.,  3.],\n                 [ 0.,  0.,  1.]]]))\n\n    """"""\n\n    def __init__(self, p: float = 0.5, return_transform: bool = False, same_on_batch: bool = False) -> None:\n        super(RandomVerticalFlip, self).__init__(return_transform)\n        self.p: float = p\n        self.same_on_batch = same_on_batch\n\n    def __repr__(self) -> str:\n        repr = f""(p={self.p}, return_transform={self.return_transform}, same_on_batch={self.same_on_batch})""\n        return self.__class__.__name__ + repr\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_prob_generator(batch_shape[0], self.p, self.same_on_batch)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_vflip_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_vflip(input, params)\n\n\nclass ColorJitter(AugmentationBase):\n\n    r""""""Change the brightness, contrast, saturation and hue randomly given tensor image or a batch of tensor images.\n    Input should be a tensor of shape (C, H, W) or a batch of tensors :math:`(B, C, H, W)`.\n\n    Args:\n        brightness (float or tuple): Default value is 0\n        contrast (float or tuple): Default value is 0\n        saturation (float or tuple): Default value is 0\n        hue (float or tuple): Default value is 0\n        return_transform (bool): if ``True`` return the matrix describing the transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Examples:\n        >>> rng = torch.manual_seed(0)\n        >>> inputs = torch.ones(1, 3, 3, 3)\n        >>> aug = ColorJitter(0.1, 0.1, 0.1, 0.1)\n        >>> aug(inputs)\n        tensor([[[[0.9993, 0.9993, 0.9993],\n                  [0.9993, 0.9993, 0.9993],\n                  [0.9993, 0.9993, 0.9993]],\n        <BLANKLINE>\n                 [[0.9993, 0.9993, 0.9993],\n                  [0.9993, 0.9993, 0.9993],\n                  [0.9993, 0.9993, 0.9993]],\n        <BLANKLINE>\n                 [[0.9993, 0.9993, 0.9993],\n                  [0.9993, 0.9993, 0.9993],\n                  [0.9993, 0.9993, 0.9993]]]])\n    """"""\n\n    def __init__(\n        self, brightness: FloatUnionType = 0., contrast: FloatUnionType = 0., saturation: FloatUnionType = 0.,\n        hue: FloatUnionType = 0., return_transform: bool = False, same_on_batch: bool = False\n    ) -> None:\n        super(ColorJitter, self).__init__(return_transform)\n        self.brightness: FloatUnionType = brightness\n        self.contrast: FloatUnionType = contrast\n        self.saturation: FloatUnionType = saturation\n        self.hue: FloatUnionType = hue\n        self.same_on_batch = same_on_batch\n\n    def __repr__(self) -> str:\n        repr = f""(brightness={self.brightness}, contrast={self.contrast}, saturation={self.saturation},\\\n            hue={self.hue}, return_transform={self.return_transform}, same_on_batch={self.same_on_batch})""\n        return self.__class__.__name__ + repr\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_color_jitter_generator(\n            batch_shape[0], self.brightness, self.contrast, self.saturation, self.hue, self.same_on_batch)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_intensity_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_color_jitter(input, params)\n\n\nclass RandomGrayscale(AugmentationBase):\n    r""""""Random Grayscale transformation according to a probability p value\n\n    Args:\n        p (float): probability of the image to be transformed to grayscale. Default value is 0.1\n        return_transform (bool): if ``True`` return the matrix describing the transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Examples:\n        >>> rng = torch.manual_seed(0)\n        >>> inputs = torch.randn((1, 3, 3, 3))\n        >>> rec_er = RandomGrayscale(p=1.0)\n        >>> rec_er(inputs)\n        tensor([[[[-1.1344, -0.1330,  0.1517],\n                  [-0.0791,  0.6711, -0.1413],\n                  [-0.1717, -0.9023,  0.0819]],\n        <BLANKLINE>\n                 [[-1.1344, -0.1330,  0.1517],\n                  [-0.0791,  0.6711, -0.1413],\n                  [-0.1717, -0.9023,  0.0819]],\n        <BLANKLINE>\n                 [[-1.1344, -0.1330,  0.1517],\n                  [-0.0791,  0.6711, -0.1413],\n                  [-0.1717, -0.9023,  0.0819]]]])\n    """"""\n\n    def __init__(self, p: float = 0.1, return_transform: bool = False, same_on_batch: bool = False) -> None:\n        super(RandomGrayscale, self).__init__(return_transform)\n        self.p = p\n        self.same_on_batch = same_on_batch\n\n    def __repr__(self) -> str:\n        repr = f""(p={self.p}, return_transform={self.return_transform}, same_on_batch={self.same_on_batch})""\n        return self.__class__.__name__ + repr\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_prob_generator(batch_shape[0], self.p, self.same_on_batch)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_intensity_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_grayscale(input, params)\n\n\nclass RandomErasing(AugmentationBase):\n    r""""""\n    Erases a random selected rectangle for each image in the batch, putting the value to zero.\n    The rectangle will have an area equal to the original image area multiplied by a value uniformly\n    sampled between the range [scale[0], scale[1]) and an aspect ratio sampled\n    between [ratio[0], ratio[1])\n\n    Args:\n        p (float): probability that the random erasing operation will be performed.\n        scale (Tuple[float, float]): range of proportion of erased area against input image.\n        ratio (Tuple[float, float]): range of aspect ratio of erased area.\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Examples:\n        >>> rng = torch.manual_seed(0)\n        >>> inputs = torch.ones(1, 1, 3, 3)\n        >>> rec_er = RandomErasing(1.0, (.4, .8), (.3, 1/.3))\n        >>> rec_er(inputs)\n        tensor([[[[1., 0., 0.],\n                  [1., 0., 0.],\n                  [1., 0., 0.]]]])\n    """"""\n    # Note: Extra params, inplace=False in Torchvision.\n\n    def __init__(\n            self, p: float = 0.5, scale: Tuple[float, float] = (0.02, 0.33), ratio: Tuple[float, float] = (0.3, 3.3),\n            value: float = 0., return_transform: bool = False, same_on_batch: bool = False\n    ) -> None:\n        super(RandomErasing, self).__init__(return_transform)\n        self.p = p\n        self.scale: Tuple[float, float] = scale\n        self.ratio: Tuple[float, float] = ratio\n        self.value: float = value\n        self.same_on_batch = same_on_batch\n\n    def __repr__(self) -> str:\n        repr = f""(scale={self.scale}, ratio={self.ratio}, value={self.value}, ""\n        f""return_transform={self.return_transform}, same_on_batch={self.same_on_batch})""\n        return self.__class__.__name__ + repr\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_rectangles_params_generator(\n            batch_shape[0], batch_shape[-2], batch_shape[-1], p=self.p, scale=self.scale, ratio=self.ratio,\n            value=self.value, same_on_batch=self.same_on_batch)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_intensity_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_erase_rectangles(input, params)\n\n\nclass RandomPerspective(AugmentationBase):\n    r""""""Performs Perspective transformation of the given torch.Tensor randomly with a given probability.\n\n    Args:\n        p (float): probability of the image being perspectively transformed. Default value is 0.5\n        distortion_scale(float): it controls the degree of distortion and ranges from 0 to 1. Default value is 0.5.\n        interpolation (int, str or kornia.Resample): Default: Resample.BILINEAR\n        return_transform (bool): if ``True`` return the matrix describing the transformation\n                                 applied to each. Default: False.\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Examples:\n        >>> rng = torch.manual_seed(0)\n        >>> inputs= torch.tensor([[[[1., 0., 0.],\n        ...                         [0., 1., 0.],\n        ...                         [0., 0., 1.]]]])\n        >>> aug = RandomPerspective(0.5, 1.0)\n        >>> aug(inputs)\n        tensor([[[[0.0000, 0.2289, 0.0000],\n                  [0.0000, 0.4800, 0.0000],\n                  [0.0000, 0.0000, 0.0000]]]])\n    """"""\n\n    def __init__(\n        self, distortion_scale: float = 0.5, p: float = 0.5,\n        interpolation: Union[str, int, Resample] = Resample.BILINEAR.name,\n        return_transform: bool = False, same_on_batch: bool = False,\n        align_corners: bool = False\n    ) -> None:\n        super(RandomPerspective, self).__init__(return_transform)\n        self.p: float = p\n        self.distortion_scale: float = distortion_scale\n        self.interpolation: Resample = Resample.get(interpolation)\n        self.same_on_batch = same_on_batch\n        self.align_corners = align_corners\n\n    def __repr__(self) -> str:\n        repr = f""(distortion_scale={self.distortion_scale}, p={self.p}, interpolation={self.interpolation.name}, ""\n        f""return_transform={self.return_transform}, same_on_batch={self.same_on_batch})""\n        return self.__class__.__name__ + repr\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_perspective_generator(\n            batch_shape[0], batch_shape[-2], batch_shape[-1], self.p, self.distortion_scale,\n            self.interpolation, self.same_on_batch, self.align_corners)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_perspective_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_perspective(input, params)\n\n\nclass RandomAffine(AugmentationBase):\n    r""""""Random affine transformation of the image keeping center invariant.\n\n    Args:\n        degrees (float or tuple): Range of degrees to select from.\n            If degrees is a number instead of sequence like (min, max), the range of degrees\n            will be (-degrees, +degrees). Set to 0 to deactivate rotations.\n        translate (tuple, optional): tuple of maximum absolute fraction for horizontal\n            and vertical translations. For example translate=(a, b), then horizontal shift\n            is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is\n            randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default.\n        scale (tuple, optional): scaling factor interval, e.g (a, b), then scale is\n            randomly sampled from the range a <= scale <= b. Will keep original scale by default.\n        shear (sequence or float, optional): Range of degrees to select from.\n            If shear is a number, a shear parallel to the x axis in the range (-shear, +shear)\n            will be apllied. Else if shear is a tuple or list of 2 values a shear parallel to the x axis in the\n            range (shear[0], shear[1]) will be applied. Else if shear is a tuple or list of 4 values,\n            a x-axis shear in (shear[0], shear[1]) and y-axis shear in (shear[2], shear[3]) will be applied.\n            Will not apply shear by default\n        resample (int, str or kornia.Resample): Default: Resample.BILINEAR\n        return_transform (bool): if ``True`` return the matrix describing the transformation\n            applied to each. Default: False.\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Examples:\n        >>> rng = torch.manual_seed(0)\n        >>> input = torch.rand(1, 1, 3, 3)\n        >>> aug = RandomAffine((-15., 20.), return_transform=True)\n        >>> aug(input)\n        (tensor([[[[0.3961, 0.7310, 0.1574],\n                  [0.1781, 0.3074, 0.5648],\n                  [0.4804, 0.8379, 0.4234]]]]), tensor([[[ 0.9923, -0.1241,  0.1319],\n                 [ 0.1241,  0.9923, -0.1164],\n                 [ 0.0000,  0.0000,  1.0000]]]))\n    """"""\n\n    def __init__(\n        self, degrees: UnionFloat, translate: Optional[TupleFloat] = None, scale: Optional[TupleFloat] = None,\n        shear: Optional[UnionFloat] = None, resample: Union[str, int, Resample] = Resample.BILINEAR.name,\n        return_transform: bool = False, same_on_batch: bool = False,\n        align_corners: bool = False\n    ) -> None:\n        super(RandomAffine, self).__init__(return_transform)\n        self.degrees = degrees\n        self.translate = translate\n        self.scale = scale\n        self.shear = shear\n        self.resample: Resample = Resample.get(resample)\n        self.same_on_batch = same_on_batch\n        self.align_corners = align_corners\n\n    def __repr__(self) -> str:\n        repr = f""(degrees={self.degrees}, translate={self.translate}, scale={self.scale}, shear={self.shear}, ""\n        f""resample={self.resample.name}, return_transform={self.return_transform}, same_on_batch={self.same_on_batch}""\n        return self.__class__.__name__ + repr\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_affine_generator(\n            batch_shape[0], batch_shape[-2], batch_shape[-1], self.degrees, self.translate, self.scale, self.shear,\n            self.resample, self.same_on_batch, self.align_corners)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_affine_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_affine(input, params)\n\n\nclass CenterCrop(AugmentationBase):\n    r""""""Crops the given torch.Tensor at the center.\n\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n        return_transform (bool): if ``True`` return the matrix describing the transformation\n            applied to each. Default: False.\n\n    Examples:\n        >>> rng = torch.manual_seed(0)\n        >>> inputs = torch.randn(1, 1, 3, 3)\n        >>> aug = CenterCrop(2)\n        >>> aug(inputs)\n        tensor([[[[-0.1425, -1.1266],\n                  [-0.0373, -0.6562]]]])\n    """"""\n\n    def __init__(self, size: Union[int, Tuple[int, int]], return_transform: bool = False) -> None:\n        # same_on_batch is always True for CenterCrop\n        super(CenterCrop, self).__init__(return_transform)\n        self.size = size\n\n    def __repr__(self) -> str:\n        repr = f""(size={self.size}, return_transform={self.return_transform}""\n        return self.__class__.__name__ + repr\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        if isinstance(self.size, tuple):\n            size_param = (self.size[0], self.size[1])\n        elif isinstance(self.size, int):\n            size_param = (self.size, self.size)\n        else:\n            raise Exception(f""Invalid size type. Expected (int, tuple(int, int). ""\n                            f""Got: {type(self.size)}."")\n        return rg.center_crop_params_generator(batch_shape[0], batch_shape[-2], batch_shape[-1], size_param)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_crop_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_crop(input, params)\n\n\nclass RandomRotation(AugmentationBase):\n\n    r""""""Rotate a tensor image or a batch of tensor images a random amount of degrees.\n    Input should be a tensor of shape (C, H, W) or a batch of tensors :math:`(B, C, H, W)`.\n    If Input is a tuple it is assumed that the first element contains the aforementioned tensors and the second,\n    the corresponding transformation matrix that has been applied to them. In this case the module\n    will rotate the tensors and concatenate the corresponding transformation matrix to the\n    previous one. This is especially useful when using this functionality as part of an ``nn.Sequential`` module.\n\n    Args:\n        degrees (sequence or float or tensor): range of degrees to select from. If degrees is a number the\n        range of degrees to select from will be (-degrees, +degrees)\n        interpolation (int, str or kornia.Resample): Default: Resample.BILINEAR\n        return_transform (bool): if ``True`` return the matrix describing the transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Examples:\n        >>> rng = torch.manual_seed(0)\n        >>> input = torch.tensor([[1., 0., 0., 2.],\n        ...                       [0., 0., 0., 0.],\n        ...                       [0., 1., 2., 0.],\n        ...                       [0., 0., 1., 2.]])\n        >>> seq = RandomRotation(degrees=45.0, return_transform=True)\n        >>> seq(input)\n        (tensor([[[[0.9824, 0.0088, 0.0000, 1.9649],\n                  [0.0000, 0.0029, 0.0000, 0.0176],\n                  [0.0029, 1.0000, 1.9883, 0.0000],\n                  [0.0000, 0.0088, 1.0117, 1.9649]]]]), tensor([[[ 1.0000, -0.0059,  0.0088],\n                 [ 0.0059,  1.0000, -0.0088],\n                 [ 0.0000,  0.0000,  1.0000]]]))\n    """"""\n    # Note: Extra params, center=None, fill=0 in TorchVision\n\n    def __init__(\n        self, degrees: FloatUnionType, interpolation: Union[str, int, Resample] = Resample.BILINEAR.name,\n        return_transform: bool = False, same_on_batch: bool = False, align_corners: bool = False\n    ) -> None:\n        super(RandomRotation, self).__init__(return_transform)\n        self.degrees = degrees\n        self.interpolation: Resample = Resample.get(interpolation)\n        self.same_on_batch = same_on_batch\n        self.align_corners = align_corners\n\n    def __repr__(self) -> str:\n        repr = f""(degrees={self.degrees}, interpolation={self.interpolation.name}, ""\n        f""return_transform={self.return_transform}, same_on_batch={self.same_on_batch})""\n        return self.__class__.__name__ + repr\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_rotation_generator(batch_shape[0], self.degrees, self.interpolation,\n                                            self.same_on_batch, self.align_corners)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_rotate_tranformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_rotation(input, params)\n\n\nclass RandomCrop(AugmentationBase):\n    r""""""Random Crop on given size.\n\n    Args:\n        size (tuple): Desired output size of the crop, like (h, w).\n        padding (int or sequence, optional): Optional padding on each border\n            of the image. Default is None, i.e no padding. If a sequence of length\n            4 is provided, it is used to pad left, top, right, bottom borders\n            respectively. If a sequence of length 2 is provided, it is used to\n            pad left/right, top/bottom borders, respectively.\n        pad_if_needed (boolean): It will pad the image if smaller than the\n            desired size to avoid raising an exception. Since cropping is done\n            after padding, the padding seems to be done at a random offset.\n        fill: Pixel fill value for constant fill. Default is 0. If a tuple of\n            length 3, it is used to fill R, G, B channels respectively.\n            This value is only used when the padding_mode is constant\n        padding_mode: Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.\n        return_transform (bool): if ``True`` return the matrix describing the transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Examples:\n        >>> rng = torch.manual_seed(0)\n        >>> inputs = torch.randn(1, 1, 3, 3)\n        >>> aug = RandomCrop((2, 2))\n        >>> aug(inputs)\n        tensor([[[[-0.6562, -1.0009],\n                  [ 0.2223, -0.5507]]]])\n    """"""\n\n    def __init__(\n        self, size: Tuple[int, int], padding: Optional[BoarderUnionType] = None, pad_if_needed: Optional[bool] = False,\n        fill: int = 0, padding_mode: str = \'constant\', return_transform: bool = False, same_on_batch: bool = False,\n        align_corners: bool = False\n    ) -> None:\n        super(RandomCrop, self).__init__(return_transform)\n        self.size = size\n        self.padding = padding\n        self.pad_if_needed = pad_if_needed\n        self.fill = fill\n        self.padding_mode = padding_mode\n        self.same_on_batch = same_on_batch\n        self.align_corners = align_corners\n\n    def __repr__(self) -> str:\n        repr = f""(crop_size={self.size}, padding={self.padding}, fill={self.fill}, ""\n        f""pad_if_needed={self.pad_if_needed}, padding_mode=${self.padding_mode}, ""\n        f""return_transform={self.return_transform}, same_on_batch={self.same_on_batch})""\n        return self.__class__.__name__ + repr\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_crop_generator(batch_shape[0], (batch_shape[-2], batch_shape[-1]), self.size,\n                                        same_on_batch=self.same_on_batch, align_corners=self.align_corners)\n\n    def precrop_padding(self, input: torch.Tensor) -> torch.Tensor:\n        if self.padding is not None:\n            if isinstance(self.padding, int):\n                padding = [self.padding, self.padding, self.padding, self.padding]\n            elif isinstance(self.padding, tuple) and len(self.padding) == 2:\n                padding = [self.padding[1], self.padding[1], self.padding[0], self.padding[0]]\n            elif isinstance(self.padding, tuple) and len(self.padding) == 4:\n                padding = [self.padding[3], self.padding[2], self.padding[1], self.padding[0]]  # type:ignore\n            input = pad(input, padding, value=self.fill, mode=self.padding_mode)\n\n        if self.pad_if_needed and input.shape[-2] < self.size[0]:\n            padding = [0, 0, (self.size[0] - input.shape[-2]), self.size[0] - input.shape[-2]]\n            input = pad(input, padding, value=self.fill, mode=self.padding_mode)\n\n        if self.pad_if_needed and input.shape[-1] < self.size[1]:\n            padding = [self.size[1] - input.shape[-1], self.size[1] - input.shape[-1], 0, 0]\n            input = pad(input, padding, value=self.fill, mode=self.padding_mode)\n\n        return input\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_crop_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_crop(input, params)\n\n    def forward(self, input: UnionType, params: Optional[Dict[str, torch.Tensor]] = None,  # type: ignore\n                return_transform: Optional[bool] = None) -> UnionType:  # type: ignore\n        if type(input) == tuple:\n            input = (self.precrop_padding(input[0]), input[1])\n        else:\n            input = self.precrop_padding(input)  # type:ignore\n        return super().forward(input, params, return_transform)\n\n\nclass RandomResizedCrop(AugmentationBase):\n    r""""""Random Crop on given size and resizing the cropped patch to another.\n\n    Args:\n        size (Tuple[int, int]): expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped\n        interpolation (int, str or kornia.Resample): Default: Resample.BILINEAR\n        return_transform (bool): if ``True`` return the matrix describing the transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Example:\n        >>> rng = torch.manual_seed(0)\n        >>> inputs = torch.tensor([[[0., 1., 2.],\n        ...                         [3., 4., 5.],\n        ...                         [6., 7., 8.]]])\n        >>> aug = RandomResizedCrop(size=(3, 3), scale=(3., 3.), ratio=(2., 2.))\n        >>> aug(inputs)\n        tensor([[[[3.7500, 4.7500, 5.7500],\n                  [5.2500, 6.2500, 7.2500],\n                  [4.5000, 5.2500, 6.0000]]]])\n    """"""\n\n    def __init__(\n        self, size: Tuple[int, int], scale: Tuple[float, float] = (0.08, 1.0),\n        ratio: Tuple[float, float] = (3. / 4., 4. / 3.),\n        interpolation: Union[str, int, Resample] = Resample.BILINEAR.name,\n        return_transform: bool = False, same_on_batch: bool = False,\n        align_corners: bool = False\n    ) -> None:\n        super(RandomResizedCrop, self).__init__(return_transform)\n        self.size = size\n        self.scale = scale\n        self.ratio = ratio\n        self.interpolation: Resample = Resample.get(interpolation)\n        self.same_on_batch = same_on_batch\n        self.align_corners = align_corners\n\n    def __repr__(self) -> str:\n        repr = f""(size={self.size}, resize_to={self.scale}, resize_to={self.ratio}, ""\n        f""interpolation={self.interpolation.name}, return_transform={self.return_transform}, ""\n        f""same_on_batch={self.same_on_batch})""\n        return self.__class__.__name__ + repr\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        target_size = rg.random_crop_size_generator(self.size, self.scale, self.ratio)\n        _target_size = (int(target_size[0].data.item()), int(target_size[1].data.item()))\n        return rg.random_crop_generator(batch_shape[0], (batch_shape[-2], batch_shape[-1]), _target_size,\n                                        resize_to=self.size, same_on_batch=self.same_on_batch,\n                                        align_corners=self.align_corners)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_crop_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_crop(input, params)\n\n\nclass RandomMotionBlur(AugmentationBase):\n    r""""""Blurs a tensor using the motion filter. Same transformation happens across batches.\n\n    Args:\n        kernel_size (int or Tuple[int, int]): motion kernel width and height (odd and positive).\n            If int, the kernel will have a fixed size.\n            If Tuple[int, int], it will randomly generate the value from the range.\n        angle (float or Tuple[float, float]): angle of the motion blur in degrees (anti-clockwise rotation).\n            If float, it will generate the value from (-angle, angle).\n        direction (float or Tuple[float, float]): forward/backward direction of the motion blur.\n            Lower values towards -1.0 will point the motion blur towards the back (with angle provided via angle),\n            while higher values towards 1.0 will point the motion blur forward. A value of 0.0 leads to a\n            uniformly (but still angled) motion blur.\n            If float, it will generate the value from (-direction, direction).\n            If Tuple[int, int], it will randomly generate the value from the range.\n        border_type (int, str or kornia.BorderType): the padding mode to be applied before convolving.\n            CONSTANT = 0, REFLECT = 1, REPLICATE = 2, CIRCULAR = 3. Default: BorderType.CONSTANT.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H, W)`\n\n    Examples::\n        >>> rng = torch.manual_seed(0)\n        >>> input = torch.rand(1, 1, 5, 5)\n        >>> motion_blur = RandomMotionBlur(3, 35., 0.5)\n        >>> motion_blur(input)\n        tensor([[[[0.2761, 0.5200, 0.3753, 0.2423, 0.2193],\n                  [0.3275, 0.5502, 0.5738, 0.5400, 0.3883],\n                  [0.2132, 0.3857, 0.3056, 0.2520, 0.1890],\n                  [0.3016, 0.6172, 0.6487, 0.4331, 0.2770],\n                  [0.3865, 0.6221, 0.5538, 0.4862, 0.4206]]]])\n    """"""\n\n    def __init__(\n            self, kernel_size: Union[int, Tuple[int, int]],\n            angle: Union[float, Tuple[float, float]],\n            direction: Union[float, Tuple[float, float]],\n            border_type: Union[int, str, BorderType] = BorderType.CONSTANT.name,\n            return_transform: bool = False\n    ) -> None:\n        super(RandomMotionBlur, self).__init__(return_transform)\n        self.kernel_size: Union[int, Tuple[int, int]] = kernel_size\n        self.angle: Union[float, Tuple[float, float]] = angle\n        self.direction: Union[float, Tuple[float, float]] = direction\n        self.border_type: BorderType = BorderType.get(border_type)\n\n    def __repr__(self) -> str:\n        return f""{self.__class__.__name__}(kernel_size={self.kernel_size}, angle={self.angle}, "" \\\n            f""direction={self.direction}, border_type=\'{self.border_type.name.lower()}\', "" \\\n            f""return_transform={self.return_transform})""\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        # TODO: Enable batch mode\n        return rg.random_motion_blur_generator(1, self.kernel_size, self.angle, self.direction, self.border_type)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_intensity_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_motion_blur(input, params)\n\n\nclass RandomSolarize(AugmentationBase):\n    r"""""" Solarize given tensor image or a batch of tensor images randomly.\n\n    Args:\n        thresholds (float or tuple): Default value is 0.1.\n            If float x, threshold will be generated from (0.5 - x, 0.5 + x).\n            If tuple (x, y), threshold will be generated from (x, y).\n        additions (float or tuple): Default value is 0.1.\n            If float x, addition will be generated from (-x, x).\n            If tuple (x, y), addition will be generated from (x, y).\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n        return_transform (bool): if ``True`` return the matrix describing the transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H, W)`\n\n    Examples:\n        >>> rng = torch.manual_seed(0)\n        >>> input = torch.rand(1, 1, 5, 5)\n        >>> solarize = RandomSolarize(0.1, 0.1)\n        >>> solarize(input)\n        tensor([[[[0.4132, 0.1412, 0.1790, 0.2226, 0.3980],\n                  [0.2754, 0.4194, 0.0130, 0.4538, 0.2771],\n                  [0.4394, 0.4923, 0.1129, 0.2594, 0.3844],\n                  [0.3909, 0.2118, 0.1094, 0.2516, 0.3728],\n                  [0.2278, 0.0000, 0.4876, 0.0353, 0.5100]]]])\n    """"""\n\n    def __init__(\n        self, thresholds: FloatUnionType = 0.1, additions: FloatUnionType = 0.1,\n        same_on_batch: bool = False, return_transform: bool = False\n    ) -> None:\n        super(RandomSolarize, self).__init__(return_transform)\n        self.thresholds = thresholds\n        self.additions = additions\n        self.same_on_batch = same_on_batch\n\n    def __repr__(self) -> str:\n        return f""{self.__class__.__name__}(thresholds={self.thresholds}, additions={self.additions}, "" \\\n            f""same_on_batch={self.same_on_batch}, return_transform={self.return_transform})""\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_solarize_generator(batch_shape[0], self.thresholds, self.additions, self.same_on_batch)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_intensity_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_solarize(input, params)\n\n\nclass RandomPosterize(AugmentationBase):\n    r"""""" Posterize given tensor image or a batch of tensor images randomly.\n\n    Args:\n        bits (int or tuple): Integer that ranged from (0, 8], in which 0 gives black image and 8 gives the original.\n            If int x, bits will be generated from (x, 8).\n            If tuple (x, y), bits will be generated from (x, y).\n            Default value is 3.\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n        return_transform (bool): if ``True`` return the matrix describing the transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H, W)`\n\n    Examples::\n        >>> rng = torch.manual_seed(0)\n        >>> input = torch.rand(1, 1, 5, 5)\n        >>> posterize = RandomPosterize(3)\n        >>> posterize(input)\n        tensor([[[[0.4706, 0.7529, 0.0627, 0.1255, 0.2824],\n                  [0.6275, 0.4706, 0.8784, 0.4392, 0.6275],\n                  [0.3451, 0.3765, 0.0000, 0.1569, 0.2824],\n                  [0.5020, 0.6902, 0.7843, 0.1569, 0.2510],\n                  [0.6588, 0.9098, 0.3765, 0.8471, 0.4078]]]])\n    """"""\n\n    def __init__(\n        self, bits: Union[int, Tuple[int, int], torch.Tensor] = 3,\n        same_on_batch: bool = False, return_transform: bool = False\n    ) -> None:\n        super(RandomPosterize, self).__init__(return_transform)\n        self.bits = bits\n        self.same_on_batch = same_on_batch\n\n    def __repr__(self) -> str:\n        return f""{self.__class__.__name__}(bits={self.bits}, same_on_batch={self.same_on_batch}, "" \\\n            f""return_transform={self.return_transform})""\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_posterize_generator(batch_shape[0], self.bits, self.same_on_batch)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_intensity_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_posterize(input, params)\n\n\nclass RandomSharpness(AugmentationBase):\n    r"""""" Sharpen given tensor image or a batch of tensor images randomly.\n\n    Args:\n        sharpness (float or tuple): factor of sharpness strength. Must be above 0. Default value is 0.5.\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n        return_transform (bool): if ``True`` return the matrix describing the transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H, W)`\n\n    Examples::\n        >>> rng = torch.manual_seed(0)\n        >>> input = torch.rand(1, 1, 5, 5)\n        >>> sharpness = RandomSharpness(1.)\n        >>> sharpness(input)\n        tensor([[[[0.4963, 0.7682, 0.0885, 0.1320, 0.3074],\n                  [0.6341, 0.7720, 0.9537, 0.7566, 0.6323],\n                  [0.3489, 0.7325, 0.5629, 0.6284, 0.2939],\n                  [0.5185, 0.8648, 0.9106, 0.6249, 0.2823],\n                  [0.6816, 0.9152, 0.3971, 0.8742, 0.4194]],\n        <BLANKLINE>\n                 [[0.4963, 0.7682, 0.0885, 0.1320, 0.3074],\n                  [0.6341, 0.7720, 0.9537, 0.7566, 0.6323],\n                  [0.3489, 0.7325, 0.5629, 0.6284, 0.2939],\n                  [0.5185, 0.8648, 0.9106, 0.6249, 0.2823],\n                  [0.6816, 0.9152, 0.3971, 0.8742, 0.4194]],\n        <BLANKLINE>\n                 [[0.4963, 0.7682, 0.0885, 0.1320, 0.3074],\n                  [0.6341, 0.7720, 0.9537, 0.7566, 0.6323],\n                  [0.3489, 0.7325, 0.5629, 0.6284, 0.2939],\n                  [0.5185, 0.8648, 0.9106, 0.6249, 0.2823],\n                  [0.6816, 0.9152, 0.3971, 0.8742, 0.4194]]]])\n    """"""\n\n    def __init__(\n        self, sharpness: Union[float, Tuple[float, float], torch.Tensor] = 0.5, same_on_batch: bool = False,\n        return_transform: bool = False\n    ) -> None:\n        super(RandomSharpness, self).__init__(return_transform)\n        self.sharpness = sharpness\n        self.same_on_batch = same_on_batch\n\n    def __repr__(self) -> str:\n        return f""{self.__class__.__name__}(sharpness={self.sharpness}, return_transform={self.return_transform})""\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_sharpness_generator(batch_shape[0], self.sharpness, self.same_on_batch)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_intensity_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_sharpness(input, params)\n\n\nclass RandomEqualize(AugmentationBase):\n    r"""""" Equalize given tensor image or a batch of tensor images randomly.\n\n    Args:\n        p (float): Probability to equalize an image. Default value is 0.5\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n        return_transform (bool): if ``True`` return the matrix describing the transformation applied to each\n                                      input tensor. If ``False`` and the input is a tuple the applied transformation\n                                      wont be concatenated\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H, W)`\n\n    Examples::\n        >>> rng = torch.manual_seed(0)\n        >>> input = torch.rand(1, 1, 5, 5)\n        >>> equalize = RandomEqualize(1.)\n        >>> equalize(input)\n        tensor([[[[0.4963, 0.7682, 0.0885, 0.1320, 0.3074],\n                  [0.6341, 0.4901, 0.8964, 0.4556, 0.6323],\n                  [0.3489, 0.4017, 0.0223, 0.1689, 0.2939],\n                  [0.5185, 0.6977, 0.8000, 0.1610, 0.2823],\n                  [0.6816, 0.9152, 0.3971, 0.8742, 0.4194]]]])\n    """"""\n\n    def __init__(\n        self, p: float = 0.5, same_on_batch: bool = False, return_transform: bool = False\n    ) -> None:\n        super(RandomEqualize, self).__init__(return_transform)\n        self.p = p\n        self.same_on_batch = same_on_batch\n\n    def __repr__(self) -> str:\n        return f""{self.__class__.__name__}(p={self.p}, return_transform={self.return_transform})""\n\n    def generate_parameters(self, batch_shape: torch.Size) -> Dict[str, torch.Tensor]:\n        return rg.random_prob_generator(batch_shape[0], self.p, self.same_on_batch)\n\n    def compute_transformation(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.compute_intensity_transformation(input, params)\n\n    def apply_transform(self, input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return F.apply_equalize(input, params)\n'"
kornia/augmentation/functional.py,173,"b'from typing import Tuple, List, Union, Dict, cast, Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom kornia.constants import Resample, BorderType, pi\nfrom kornia.geometry import (\n    get_perspective_transform,\n    get_rotation_matrix2d,\n    get_affine_matrix2d,\n    warp_perspective,\n    rotate,\n    crop_by_boxes,\n    warp_affine,\n    hflip,\n    vflip,\n    deg2rad\n)\nfrom kornia.color import (\n    adjust_brightness,\n    adjust_contrast,\n    adjust_saturation,\n    adjust_hue,\n    adjust_gamma,\n    rgb_to_grayscale,\n    solarize,\n    equalize,\n    posterize,\n    sharpness\n)\nfrom kornia.filters import motion_blur\nfrom kornia.geometry.transform.affwarp import _compute_rotation_matrix, _compute_tensor_center\n\nfrom . import random_generator as rg\nfrom .utils import _transform_input, _validate_input_shape, _validate_input_dtype\nfrom .types import (\n    TupleFloat,\n    UnionFloat,\n    UnionType,\n    FloatUnionType\n)\n\n\ndef random_hflip(input: torch.Tensor, p: float = 0.5, return_transform: bool = False) -> UnionType:\n    r""""""Generate params and apply operation on input tensor.\n\n    See :func:`~kornia.augmentation.random_generator.random_prob_generator` for details.\n    See :func:`~kornia.augmentation.functional.apply_hflip` for details.\n    """"""\n    input = _transform_input(input)\n    batch_size, _, h, w = input.size()\n    params = rg.random_prob_generator(batch_size, p=p)\n    output = apply_hflip(input, params)\n    if return_transform:\n        return output, compute_hflip_transformation(input, params)\n    return output\n\n\ndef random_vflip(input: torch.Tensor, p: float = 0.5, return_transform: bool = False) -> UnionType:\n    r""""""Generate params and apply operation on input tensor.\n\n    See :func:`~kornia.augmentation.random_generator.random_prob_generator` for details.\n    See :func:`~kornia.augmentation.functional.apply_vflip` for details.\n    """"""\n    input = _transform_input(input)\n    batch_size, _, h, w = input.size()\n    params = rg.random_prob_generator(batch_size, p=p)\n    output = apply_vflip(input, params)\n    if return_transform:\n        return output, compute_vflip_transformation(input, params)\n    return output\n\n\ndef color_jitter(input: torch.Tensor, brightness: FloatUnionType = 0.,\n                 contrast: FloatUnionType = 0., saturation: FloatUnionType = 0.,\n                 hue: FloatUnionType = 0., return_transform: bool = False) -> UnionType:\n    r""""""Generate params and apply operation on input tensor.\n\n    See :func:`~kornia.augmentation.random_generator.random_color_jitter_generator` for details.\n    See :func:`~kornia.augmentation.functional.apply_color_jitter` for details.\n    """"""\n    input = _transform_input(input)\n    batch_size, _, h, w = input.size()\n    params = rg.random_color_jitter_generator(batch_size, brightness, contrast, saturation, hue)\n    output = apply_color_jitter(input, params)\n    if return_transform:\n        return output, compute_intensity_transformation(input, params)\n    return output\n\n\ndef random_grayscale(input: torch.Tensor, p: float = 0.5, return_transform: bool = False):\n    r""""""Generate params and apply operation on input tensor.\n\n    See :func:`~kornia.augmentation.random_generator.random_prob_generator` for details.\n    See :func:`~kornia.augmentation.functional.apply_grayscale` for details.\n    """"""\n    input = _transform_input(input)\n    batch_size, _, h, w = input.size()\n    params = rg.random_prob_generator(batch_size, p=p)\n\n    output = apply_grayscale(input, params)\n    if return_transform:\n        return output, compute_intensity_transformation(input, params)\n    return output\n\n\ndef random_perspective(input: torch.Tensor,\n                       distortion_scale: float = 0.5,\n                       p: float = 0.5,\n                       return_transform: bool = False) -> UnionType:\n    r""""""Generate params and apply operation on input tensor.\n\n    See :func:`~kornia.augmentation.random_generator.random_perspective_generator` for details.\n    See :func:`~kornia.augmentation.functional.apply_perspective` for details.\n    """"""\n\n    input = _transform_input(input)\n    batch_size, _, height, width = input.size()\n    params: Dict[str, torch.Tensor] = rg.random_perspective_generator(\n        batch_size, height, width, p, distortion_scale)\n    output = apply_perspective(input, params)\n    if return_transform:\n        transform = compute_perspective_transformation(input, params)\n        return output, transform\n    return output\n\n\ndef random_affine(input: torch.Tensor,\n                  degrees: UnionFloat,\n                  translate: Optional[TupleFloat] = None,\n                  scale: Optional[TupleFloat] = None,\n                  shear: Optional[UnionFloat] = None,\n                  resample: Union[str, int, Resample] = Resample.BILINEAR.name,\n                  return_transform: bool = False) -> UnionType:\n    r""""""Generate params and apply operation on input tensor.\n\n    See :func:`~kornia.augmentation.random_generator.random_affine_generator` for details.\n    See :func:`~kornia.augmentation.functional.apply_affine` for details.\n    """"""\n\n    input = _transform_input(input)\n    batch_size, _, height, width = input.size()\n    params: Dict[str, torch.Tensor] = rg.random_affine_generator(\n        batch_size, height, width, degrees, translate, scale, shear, resample)\n    output = apply_affine(input, params)\n    if return_transform:\n        transform = compute_affine_transformation(input, params)\n        return output, transform\n    return output\n\n\ndef random_rectangle_erase(\n        input: torch.Tensor,\n        p: float = 0.5,\n        scale: Tuple[float, float] = (0.02, 0.33),\n        ratio: Tuple[float, float] = (0.3, 3.3),\n        return_transform: bool = False\n) -> UnionType:\n    r""""""\n    Function that erases a random selected rectangle for each image in the batch, putting\n    the value to zero.\n    The rectangle will have an area equal to the original image area multiplied by a value uniformly\n    sampled between the range [scale[0], scale[1]) and an aspect ratio sampled\n    between [aspect_ratio_range[0], aspect_ratio_range[1])\n\n    Args:\n        input (torch.Tensor): input images.\n        scale (Tuple[float, float]): range of proportion of erased area against input image.\n        ratio (Tuple[float, float]): range of aspect ratio of erased area.\n\n    See :func:`~kornia.augmentation.random_generator.random_rectangles_params_generator` for details.\n    See :func:`~kornia.augmentation.functional.apply_erase_rectangles` for details.\n    """"""\n    input = _transform_input(input)\n    b, _, h, w = input.size()\n    params = rg.random_rectangles_params_generator(\n        b, h, w, p, scale, ratio\n    )\n    output = apply_erase_rectangles(input, params)\n    if return_transform:\n        return output, compute_intensity_transformation(input, params)\n    return output\n\n\ndef random_rotation(input: torch.Tensor, degrees: FloatUnionType, return_transform: bool = False) -> UnionType:\n    r""""""Generate params and apply operation on input tensor.\n\n    See :func:`~kornia.augmentation.random_generator.random_rotation_generator` for details.\n    See :func:`~kornia.augmentation.functional.apply_rotation` for details.\n    """"""\n    input = _transform_input(input)\n    batch_size, _, _, _ = input.size()\n    params = rg.random_rotation_generator(batch_size, degrees=degrees)\n    output = apply_rotation(input, params)\n    if return_transform:\n        return output, compute_rotate_tranformation(input, params)\n    return output\n\n\ndef apply_hflip(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Apply Horizontally flip on a tensor image or a batch of tensor images with given random parameters.\n    Input should be a tensor of shape (H, W), (C, H, W) or a batch of tensors :math:`(B, C, H, W)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'batch_prob\']: A boolean tensor thatindicating whether if to transform an image in a batch.\n\n    Returns:\n        torch.Tensor: The horizontally flipped input\n    """"""\n\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    flipped: torch.Tensor = input.clone()\n\n    to_flip = params[\'batch_prob\'].to(input.device)\n    flipped[to_flip] = hflip(input[to_flip])\n\n    return flipped\n\n\ndef compute_hflip_transformation(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Compute the applied transformation matrix :math: `(*, 3, 3)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'batch_prob\']: A boolean tensor thatindicating whether if to transform an image in a batch.\n\n    Returns:\n        torch.Tensor: The applied transformation matrix :math: `(*, 3, 3)`\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n    to_flip = params[\'batch_prob\'].to(input.device)\n    trans_mat: torch.Tensor = torch.eye(3, device=input.device, dtype=input.dtype).repeat(input.shape[0], 1, 1)\n    w: int = input.shape[-1]\n    flip_mat: torch.Tensor = torch.tensor([[-1, 0, w],\n                                           [0, 1, 0],\n                                           [0, 0, 1]])\n    trans_mat[to_flip] = flip_mat.type_as(input)\n\n    return trans_mat\n\n\ndef apply_vflip(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Apply vertically flip on a tensor image or a batch of tensor images with given random parameters.\n    Input should be a tensor of shape (H, W), (C, H, W) or a batch of tensors :math:`(B, C, H, W)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'batch_prob\']: A boolean tensor thatindicating whether if to transform an image in a batch.\n\n    Returns:\n        torch.Tensor: The vertically flipped input\n    """"""\n    # TODO: params validation\n\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    flipped: torch.Tensor = input.clone()\n    to_flip = params[\'batch_prob\'].to(input.device)\n    flipped[to_flip] = vflip(input[to_flip])\n\n    return flipped\n\n\ndef compute_vflip_transformation(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Compute the applied transformation matrix :math: `(*, 3, 3)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'batch_prob\']: A boolean tensor thatindicating whether if to transform an image in a batch.\n\n    Returns:\n        torch.Tensor: The applied transformation matrix :math: `(*, 3, 3)`\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n    to_flip = params[\'batch_prob\'].to(input.device)\n    trans_mat: torch.Tensor = torch.eye(3, device=input.device, dtype=input.dtype).repeat(input.shape[0], 1, 1)\n\n    h: int = input.shape[-2]\n    flip_mat: torch.Tensor = torch.tensor([[1, 0, 0],\n                                           [0, -1, h],\n                                           [0, 0, 1]])\n\n    trans_mat[to_flip] = flip_mat.type_as(input)\n\n    return trans_mat\n\n\ndef apply_color_jitter(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Apply Color Jitter on a tensor image or a batch of tensor images with given random parameters.\n    Input should be a tensor of shape (H, W), (C, H, W) or a batch of tensors :math:`(B, C, H, W)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'brightness_factor\']: The brightness factor.\n            - params[\'contrast_factor\']: The contrast factor.\n            - params[\'hue_factor\']: The hue factor.\n            - params[\'saturation_factor\']: The saturation factor.\n            - params[\'order\']: The order of applying color transforms.\n              0 is brightness, 1 is contrast, 2 is saturation, 4 is hue.\n\n    Returns:\n        torch.Tensor: The color jitterred input\n    """"""\n    # TODO: params validation\n\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    transforms = [\n        lambda img: apply_adjust_brightness(img, params),\n        lambda img: apply_adjust_contrast(img, params),\n        lambda img: apply_adjust_saturation(img, params),\n        lambda img: apply_adjust_hue(img, params)\n    ]\n\n    jittered = input\n    for idx in params[\'order\'].tolist():\n        t = transforms[idx]\n        jittered = t(jittered)\n\n    return jittered\n\n\ndef compute_intensity_transformation(input: torch.Tensor, params: Dict[str, torch.Tensor]):\n    r""""""Compute the applied transformation matrix :math: `(*, 3, 3)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'batch_prob\']: A boolean tensor that indicating whether if to transform an image in a batch.\n\n    Returns:\n        torch.Tensor: The applied transformation matrix :math: `(*, 3, 3)`. Returns identity transformations.\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n    identity: torch.Tensor = torch.eye(3, device=input.device, dtype=input.dtype).repeat(input.shape[0], 1, 1)\n    return identity\n\n\ndef apply_grayscale(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Apply Gray Scale on a tensor image or a batch of tensor images with given random parameters.\n    Input should be a tensor of shape (3, H, W) or a batch of tensors :math:`(*, 3, H, W)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'batch_prob\']: A boolean tensor that indicating whether if to transform an image in a batch.\n\n    Returns:\n        torch.Tensor: The grayscaled input\n    """"""\n    # TODO: params validation\n\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    if not _validate_input_shape(input, 1, 3):\n        raise ValueError(f""Input size must have a shape of (*, 3, H, W). Got {input.shape}"")\n\n    grayscale: torch.Tensor = input.clone()\n\n    to_gray = params[\'batch_prob\'].to(input.device)\n\n    grayscale[to_gray] = rgb_to_grayscale(input[to_gray])\n\n    return grayscale\n\n\ndef apply_perspective(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Perform perspective transform of the given torch.Tensor or batch of tensors.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'batch_prob\']: A boolean tensor thatindicating whether if to transform an image in a batch.\n            - params[\'start_points\']: Tensor containing [top-left, top-right, bottom-right,\n              bottom-left] of the orignal image with shape Bx4x2.\n            - params[\'end_points\']: Tensor containing [top-left, top-right, bottom-right,\n              bottom-left] of the transformed image with shape Bx4x2.\n            - params[\'interpolation\']: Integer tensor. NEAREST = 0, BILINEAR = 1.\n            - params[\'align_corners\']: Boolean tensor.\n\n    Returns:\n        torch.Tensor: Perspectively transformed tensor.\n    """"""\n\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    # arrange input data\n    x_data: torch.Tensor = input.view(-1, *input.shape[-3:])\n\n    _, _, height, width = x_data.shape\n\n    # compute the homography between the input points\n    transform: torch.Tensor = compute_perspective_transformation(input, params)\n\n    out_data: torch.Tensor = x_data.clone()\n\n    # process valid samples\n    mask: torch.Tensor = params[\'batch_prob\'].to(input.device)\n\n    # TODO: look for a workaround for this hack. In CUDA it fails when no elements found.\n    # TODO: this if statement is super weird and sum here is not the propeer way to check\n    # it\'s valid. In addition, \'interpolation\' shouldn\'t be a reason to get into the branch.\n\n    if bool(mask.sum() > 0) and (\'interpolation\' in params):\n        # apply the computed transform\n        height, width = x_data.shape[-2:]\n        resample_name: str = Resample(params[\'interpolation\'].item()).name.lower()\n        align_corners: bool = cast(bool, params[\'align_corners\'].item())\n\n        out_data[mask] = warp_perspective(\n            x_data[mask], transform[mask], (height, width),\n            flags=resample_name, align_corners=align_corners)\n\n    return out_data.view_as(input)\n\n\ndef compute_perspective_transformation(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Compute the applied transformation matrix :math: `(*, 3, 3)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'batch_prob\']: A boolean tensor thatindicating whether if to transform an image in a batch.\n            - params[\'start_points\']: Tensor containing [top-left, top-right, bottom-right,\n              bottom-left] of the orignal image with shape Bx4x2.\n            - params[\'end_points\']: Tensor containing [top-left, top-right, bottom-right,\n              bottom-left] of the transformed image with shape Bx4x2.\n\n    Returns:\n        torch.Tensor: The applied transformation matrix :math: `(*, 3, 3)`\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n    transform: torch.Tensor = get_perspective_transform(\n        params[\'start_points\'], params[\'end_points\']).type_as(input)\n    return transform\n\n\ndef apply_affine(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Random affine transformation of the image keeping center invariant.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'angle\']: Degrees of rotation.\n            - params[\'translations\']: Horizontal and vertical translations.\n            - params[\'center\']: Rotation center.\n            - params[\'scale\']: Scaling params.\n            - params[\'sx\']: Shear param toward x-axis.\n            - params[\'sy\']: Shear param toward y-axis.\n            - params[\'resample\']: Integer tensor. NEAREST = 0, BILINEAR = 1.\n            - params[\'align_corners\']: Boolean tensor.\n\n    Returns:\n        torch.Tensor: The transfromed input\n    """"""\n\n    if not torch.is_tensor(input):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    # arrange input data\n    x_data: torch.Tensor = input.view(-1, *input.shape[-3:])\n\n    height, width = x_data.shape[-2:]\n\n    # concatenate transforms\n    transform: torch.Tensor = compute_affine_transformation(input, params)\n\n    resample_name: str = Resample(params[\'resample\'].item()).name.lower()\n    align_corners: bool = cast(bool, params[\'align_corners\'].item())\n\n    out_data: torch.Tensor = warp_affine(x_data, transform[:, :2, :],\n                                         (height, width), resample_name,\n                                         align_corners=align_corners)\n    return out_data.view_as(input)\n\n\ndef compute_affine_transformation(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Compute the applied transformation matrix :math: `(*, 3, 3)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'angle\']: Degrees of rotation.\n            - params[\'translations\']: Horizontal and vertical translations.\n            - params[\'center\']: Rotation center.\n            - params[\'scale\']: Scaling params.\n            - params[\'sx\']: Shear param toward x-axis.\n            - params[\'sy\']: Shear param toward y-axis.\n            - params[\'resample\']: Integer tensor. NEAREST = 0, BILINEAR = 1.\n            - params[\'align_corners\']: Boolean tensor.\n\n    Returns:\n        torch.Tensor: The applied transformation matrix :math: `(*, 3, 3)`\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n    transform = get_affine_matrix2d(\n        params[\'translations\'], params[\'center\'], params[\'scale\'], params[\'angle\'],\n        deg2rad(params[\'sx\']), deg2rad(params[\'sy\'])\n    ).type_as(input)\n    return transform\n\n\ndef apply_rotation(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Rotate a tensor image or a batch of tensor images a random amount of degrees.\n    Input should be a tensor of shape (C, H, W) or a batch of tensors :math:`(B, C, H, W)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'degrees\']: degree to be applied.\n\n    Returns:\n        torch.Tensor: The cropped input\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n    angles: torch.Tensor = params[""degrees""].type_as(input)\n\n    resample_mode: str = Resample(params[\'interpolation\'].item()).name.lower()\n    align_corners: bool = cast(bool, params[\'align_corners\'].item())\n\n    transformed: torch.Tensor = rotate(input, angles, mode=resample_mode, align_corners=align_corners)\n\n    return transformed\n\n\ndef compute_rotate_tranformation(input: torch.Tensor, params: Dict[str, torch.Tensor]):\n    r""""""Compute the applied transformation matrix :math: `(*, 3, 3)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'degrees\']: degree to be applied.\n\n    Returns:\n        torch.Tensor: The applied transformation matrix :math: `(*, 3, 3)`\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n    angles: torch.Tensor = params[""degrees""].type_as(input)\n\n    # TODO: This part should be inferred from rotate directly\n    center: torch.Tensor = _compute_tensor_center(input)\n    rotation_mat: torch.Tensor = _compute_rotation_matrix(angles, center.expand(angles.shape[0], -1))\n\n    # rotation_mat is B x 2 x 3 and we need a B x 3 x 3 matrix\n    trans_mat: torch.Tensor = torch.eye(3, device=input.device, dtype=input.dtype).repeat(input.shape[0], 1, 1)\n    trans_mat[:, 0] = rotation_mat[:, 0]\n    trans_mat[:, 1] = rotation_mat[:, 1]\n\n    return trans_mat\n\n\ndef apply_crop(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Apply cropping by src bounding box and dst bounding box.\n    Order: top-left, top-right, bottom-right and bottom-left. The coordinates must be in the x, y order.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'src\']: The applied cropping src matrix :math: `(*, 4, 2)`.\n            - params[\'dst\']: The applied cropping dst matrix :math: `(*, 4, 2)`.\n            - params[\'interpolation\']: Integer tensor. NEAREST = 0, BILINEAR = 1.\n            - params[\'align_corners\']: Boolean tensor.\n\n    Returns:\n        torch.Tensor: The cropped input.\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    resample_mode: str = Resample.get(params[\'interpolation\'].item()).name.lower()  # type: ignore\n    align_corners: bool = cast(bool, params[\'align_corners\'].item())\n\n    return crop_by_boxes(\n        input, params[\'src\'], params[\'dst\'], resample_mode, align_corners=align_corners)\n\n\ndef compute_crop_transformation(input: torch.Tensor, params: Dict[str, torch.Tensor]):\n    r""""""Compute the applied transformation matrix :math: `(*, 3, 3)`.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'src\']: The applied cropping src matrix :math: `(*, 4, 2)`.\n            - params[\'dst\']: The applied cropping dst matrix :math: `(*, 4, 2)`.\n\n    Returns:\n        torch.Tensor: The applied transformation matrix :math: `(*, 3, 3)`\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n    transform: torch.Tensor = get_perspective_transform(params[\'src\'].to(input.dtype), params[\'dst\'].to(input.dtype))\n    transform = transform.expand(input.shape[0], -1, -1).type_as(input)\n    return transform\n\n\ndef apply_erase_rectangles(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""\n    Generate a {0, 1} mask with drawed rectangle having parameters defined by params\n    and size by input.size()\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'widths\']: widths tensor\n            - params[\'heights\']: heights tensor\n            - params[\'xs\']: x positions tensor\n            - params[\'ys\']: y positions tensor\n            - params[\'values\']: the value to fill in\n\n    Returns:\n        torch.Tensor: Erased image.\n    """"""\n    if not (params[\'widths\'].size() == params[\'heights\'].size() == params[\'xs\'].size() == params[\'ys\'].size()):\n        raise TypeError(\n            ""rectangle params components must have same shape. ""\n            f""Got ({params[\'widths\'].size()}, {params[\'heights\'].size()}) ""\n            f""and ({params[\'xs\'].size()}, {params[\'ys\'].size()})""\n        )\n\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    mask = torch.zeros(input.size()).type_as(input)\n    values = torch.zeros(input.size()).type_as(input)\n\n    widths = params[\'widths\']\n    heights = params[\'heights\']\n    xs = params[\'xs\']\n    ys = params[\'ys\']\n    vs = params[\'values\']\n    for i_elem in range(input.size()[0]):\n        h = widths[i_elem].item()\n        w = heights[i_elem].item()\n        y = ys[i_elem].item()\n        x = xs[i_elem].item()\n        v = vs[i_elem].item()\n        mask[i_elem, :, int(y):int(y + w), int(x):int(x + h)] = 1.\n        values[i_elem, :, int(y):int(y + w), int(x):int(x + h)] = v\n    transformed = torch.where(mask == 1., values, input)\n    return transformed\n\n\ndef apply_adjust_brightness(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    """""" Wrapper for adjust_brightness for Torchvision-like param settings.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'brightness_factor\']: Brightness adjust factor per element\n              in the batch. 0 gives a black image, 1 does not modify the input image and 2 gives a\n              white image, while any other number modify the brightness.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    transformed = adjust_brightness(input, params[\'brightness_factor\'].to(input.dtype) - 1)\n\n    return transformed\n\n\ndef apply_adjust_contrast(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    """"""Wrapper for adjust_contrast for Torchvision-like param settings.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'contrast_factor\']: Contrast adjust factor per element in the batch.\n              0 generates a compleatly black image, 1 does not modify the input image while any other\n              non-negative number modify the brightness by this factor.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    transformed = adjust_contrast(input, params[\'contrast_factor\'].to(input.dtype))\n\n    return transformed\n\n\ndef apply_adjust_saturation(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    """"""Wrapper for adjust_saturation for Torchvision-like param settings.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'saturation_factor\']:  How much to adjust the saturation. 0 will give a black\n              and white image, 1 will give the original image while 2 will enhance the saturation\n              by a factor of 2.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    transformed = adjust_saturation(input, params[\'saturation_factor\'].to(input.dtype))\n\n    return transformed\n\n\ndef apply_adjust_hue(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    """"""Wrapper for adjust_hue for Torchvision-like param settings.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'hue_factor\']: How much to shift the hue channel. Should be in [-0.5, 0.5].\n              0.5 and -0.5 give complete reversal of hue channel in HSV space in positive and negative\n              direction respectively. 0 means no shift. Therefore, both -0.5 and 0.5 will give an\n              image with complementary colors while 0 gives the original image.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    transformed = adjust_hue(input, params[\'hue_factor\'].to(input.dtype) * 2 * pi)\n\n    return transformed\n\n\ndef apply_adjust_gamma(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Perform gamma correction on an image.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'gamma_factor\']: Non negative real number, same as \xce\xb3\\gamma\xce\xb3 in the equation.\n              gamma larger than 1 make the shadows darker, while gamma smaller than 1 make\n              dark regions lighter.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    transformed = adjust_gamma(input, params[\'gamma_factor\'].to(input.dtype))\n\n    return transformed\n\n\ndef apply_motion_blur(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Perform motion blur on an image\n\n    The input image is expected to be in the range of [0, 1].\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'ksize_factor\']: motion kernel width and height (odd and positive).\n            - params[\'angle_factor\']: angle of the motion blur in degrees (anti-clockwise rotation).\n            - params[\'direction_factor\']: forward/backward direction of the motion blur.\n              Lower values towards -1.0 will point the motion blur towards the back (with\n              angle provided via angle), while higher values towards 1.0 will point the motion\n              blur forward. A value of 0.0 leads to a uniformly (but still angled) motion blur.\n            - params[\'border_type\']: the padding mode to be applied before convolving.\n              CONSTANT = 0, REFLECT = 1, REPLICATE = 2, CIRCULAR = 3. Default: BorderType.CONSTANT.\n\n    Returns:\n        torch.Tensor: Adjusted image with the shape as the inpute (\\*, C, H, W).\n\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    kernel_size: int = cast(int, params[\'ksize_factor\'].item())\n    # TODO: this params should be at some point, learnable tensors\n    angle: float = cast(float, params[\'angle_factor\'].item())\n    direction: float = cast(float, params[\'direction_factor\'].item())\n    border_type: str = cast(str, BorderType(params[\'border_type\'].item()).name.lower())\n\n    return motion_blur(input, kernel_size, angle, direction, border_type)\n\n\ndef apply_solarize(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Solarize an image.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'thresholds_factor\']: thresholds ranged from 0 ~ 1.\n            - params[\'additions_factor\']: additions to add on before solarizing.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    thresholds = params[\'thresholds_factor\']\n    additions: Optional[torch.Tensor]\n    if \'additions_factor\' in params:\n        additions = params[\'additions_factor\']\n    else:\n        additions = None\n    return solarize(input, thresholds, additions)\n\n\ndef apply_posterize(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Posterize an image.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'bits_factor\']: uint8 bits number ranged from 0 to 8.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    bits = params[\'bits_factor\']\n\n    return posterize(input, bits)\n\n\ndef apply_sharpness(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Sharpen an image.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'sharpness_factor\']: Sharpness strength. Must be above 0.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    factor = params[\'sharpness_factor\']\n\n    return sharpness(input, factor)\n\n\ndef apply_equalize(input: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n    r""""""Equalize an image.\n\n    Args:\n        input (torch.Tensor): Tensor to be transformed with shape (H, W), (C, H, W), (B, C, H, W).\n        params (Dict[str, torch.Tensor]):\n            - params[\'p\']: Probability.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n    input = _transform_input(input)\n    _validate_input_dtype(input, accepted_dtypes=[torch.float16, torch.float32, torch.float64])\n\n    res = []\n    for image, prob in zip(input, params[\'batch_prob\']):\n        res.append(equalize(image) if prob else image)\n    return torch.cat(res, dim=0)\n'"
kornia/augmentation/random_generator.py,108,"b'from typing import Tuple, List, Union, Dict, Optional, cast\nimport random\nimport math\n\nimport torch\n\nfrom kornia.constants import Resample, BorderType\nfrom kornia.augmentation.utils import (\n    _adapted_uniform,\n    _check_and_bound\n)\n\nfrom .types import (\n    TupleFloat,\n    UnionFloat,\n    UnionType,\n    FloatUnionType\n)\n\n\ndef random_color_jitter_generator(\n    batch_size: int,\n    brightness: FloatUnionType = 0.,\n    contrast: FloatUnionType = 0.,\n    saturation: FloatUnionType = 0.,\n    hue: FloatUnionType = 0.,\n    same_on_batch: bool = False\n) -> Dict[str, torch.Tensor]:\n    r""""""Generator random color jiter parameters for a batch of images.\n\n    Args:\n        batch_size (int): the number of images.\n        brightness (float or tuple): Default value is 0\n        contrast (float or tuple): Default value is 0\n        saturation (float or tuple): Default value is 0\n        hue (float or tuple): Default value is 0\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n    """"""\n\n    brightness_bound: torch.Tensor = _check_and_bound(\n        brightness, \'brightness\', center=1., bounds=(0, 2))\n\n    contrast_bound: torch.Tensor = _check_and_bound(\n        contrast, \'contrast\', center=1.)\n\n    saturation_bound: torch.Tensor = _check_and_bound(\n        saturation, \'saturation\', center=1.)\n\n    hue_bound: torch.Tensor = _check_and_bound(hue, \'hue\', bounds=(-0.5, 0.5))\n\n    brightness_factor = _adapted_uniform(\n        (batch_size,), brightness_bound[0], brightness_bound[1], same_on_batch)\n\n    contrast_factor = _adapted_uniform(\n        (batch_size,), contrast_bound[0], contrast_bound[1], same_on_batch)\n\n    hue_factor = _adapted_uniform(\n        (batch_size,), hue_bound[0], hue_bound[1], same_on_batch)\n\n    saturation_factor = _adapted_uniform(\n        (batch_size,), saturation_bound[0], saturation_bound[1], same_on_batch)\n\n    return dict(brightness_factor=brightness_factor,\n                contrast_factor=contrast_factor,\n                hue_factor=hue_factor,\n                saturation_factor=saturation_factor,\n                order=torch.randperm(4))\n\n\ndef random_prob_generator(\n        batch_size: int, p: float = 0.5, same_on_batch: bool = False) -> Dict[str, torch.Tensor]:\n    r""""""Generator random probabilities for a batch of inputs.\n\n    Args:\n        batch_size (int): the number of images.\n        p (float): probability of the image being flipped or grayscaled. Default value is 0.5\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n    """"""\n\n    if not isinstance(p, float):\n        raise TypeError(f""The probability should be a float number. Got {type(p)}"")\n\n    probs: torch.Tensor = _adapted_uniform((batch_size,), 0, 1, same_on_batch)\n\n    batch_prob: torch.Tensor = (probs < p)\n\n    return dict(batch_prob=batch_prob)\n\n\ndef _get_perspective_params(\n    batch_size: int,\n    width: int,\n    height: int,\n    distortion_scale: float,\n    same_on_batch: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    start_points: torch.Tensor = torch.tensor([[\n        [0., 0],\n        [width - 1, 0],\n        [width - 1, height - 1],\n        [0, height - 1],\n    ]]).expand(batch_size, -1, -1)\n\n    # generate random offset not larger than half of the image\n    fx: float = distortion_scale * width / 2\n    fy: float = distortion_scale * height / 2\n\n    factor = torch.tensor([fx, fy]).view(-1, 1, 2)\n\n    rand_val: torch.Tensor = _adapted_uniform((batch_size, 4, 2), 0, 1, same_on_batch)\n    pts_norm = torch.tensor([[\n        [1, 1],\n        [-1, 1],\n        [-1, -1],\n        [1, -1]\n    ]])\n    end_points = start_points + factor * rand_val * pts_norm\n\n    return start_points, end_points\n\n\ndef random_perspective_generator(\n    batch_size: int,\n    height: int,\n    width: int,\n    p: float,\n    distortion_scale: float,\n    interpolation: Union[str, int, Resample] = Resample.BILINEAR.name,\n    same_on_batch: bool = False,\n    align_corners: bool = False,\n) -> Dict[str, torch.Tensor]:\n    r""""""Get parameters for ``perspective`` for a random perspective transform.\n\n    Args:\n        batch_size (int): the tensor batch size.\n        height (int) : height of the image.\n        width (int): width of the image.\n        p (float): probability of the image being applied perspective.\n        distortion_scale (float): it controls the degree of distortion and ranges from 0 to 1. Default value is 0.5.\n        interpolation (int, str or kornia.Resample): Default: Resample.BILINEAR\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n\n\n    Returns:\n        params (Dict[str, torch.Tensor])\n    """"""\n    params: Dict[str, torch.Tensor] = random_prob_generator(batch_size, p)\n    start_points, end_points = (\n        _get_perspective_params(batch_size, width, height, distortion_scale, same_on_batch)\n    )\n    params[\'start_points\'] = start_points\n    params[\'end_points\'] = end_points\n    params[\'interpolation\'] = torch.tensor(Resample.get(interpolation).value)\n    params[\'align_corners\'] = torch.tensor(align_corners)\n    return params\n\n\ndef random_affine_generator(\n    batch_size: int,\n    height: int,\n    width: int,\n    degrees: UnionFloat,\n    translate: Optional[TupleFloat] = None,\n    scale: Optional[TupleFloat] = None,\n    shear: Optional[UnionFloat] = None,\n    resample: Union[str, int, Resample] = Resample.BILINEAR.name,\n    same_on_batch: bool = False,\n    align_corners: bool = False\n) -> Dict[str, torch.Tensor]:\n    r""""""Get parameters for ``affine`` for a random affine transform.\n\n    Args:\n        batch_size (int): the tensor batch size.\n        height (int) : height of the image.\n        width (int): width of the image.\n        degrees (float or tuple): Range of degrees to select from.\n            If degrees is a number instead of sequence like (min, max), the range of degrees\n            will be (-degrees, +degrees). Set to 0 to deactivate rotations.\n        translate (tuple, optional): tuple of maximum absolute fraction for horizontal\n            and vertical translations. For example translate=(a, b), then horizontal shift\n            is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is\n            randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default.\n        scale (tuple, optional): scaling factor interval, e.g (a, b), then scale is\n            randomly sampled from the range a <= scale <= b. Will keep original scale by default.\n        shear (sequence or float, optional): Range of degrees to select from.\n            If shear is a number, a shear parallel to the x axis in the range (-shear, +shear)\n            will be apllied. Else if shear is a tuple or list of 2 values a shear parallel to the x axis in the\n            range (shear[0], shear[1]) will be applied. Else if shear is a tuple or list of 4 values,\n            a x-axis shear in (shear[0], shear[1]) and y-axis shear in (shear[2], shear[3]) will be applied.\n            Will not apply shear by default\n        resample (int, str or kornia.Resample): Default: Resample.BILINEAR\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n        align_corners(bool): interpolation flag. Default: False.See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n    """"""\n    # check angle ranges\n    degrees_tmp: TupleFloat\n    if isinstance(degrees, (float, int,)):\n        if degrees < 0.:\n            raise ValueError(""If degrees is a single number, it must be positive."")\n        degrees_tmp = (-degrees, degrees)\n    else:\n        assert isinstance(degrees, (tuple, list)) and len(degrees) == 2, \\\n            ""degrees should be a list or tuple and it must be of length 2.""\n        degrees_tmp = degrees\n\n    # check translation range\n    if translate is not None:\n        assert isinstance(translate, (tuple, list)) and len(translate) == 2, \\\n            ""translate should be a list or tuple and it must be of length 2.""\n        for t in translate:\n            if not (0.0 <= t <= 1.0):\n                raise ValueError(""translation values should be between 0 and 1"")\n\n    # check scale range\n    if scale is not None:\n        assert isinstance(scale, (tuple, list)) and len(scale) == 2, \\\n            ""scale should be a list or tuple and it must be of length 2.""\n        for s in scale:\n            if s <= 0:\n                raise ValueError(""scale values should be positive"")\n\n    # check shear range\n    shear_tmp: Optional[TupleFloat]\n    if shear is not None:\n        if isinstance(shear, float):\n            if shear < 0:\n                raise ValueError(""If shear is a single number, it must be positive."")\n            shear_tmp = (-shear, shear)\n        else:\n            assert isinstance(shear, (tuple, list)) and len(shear) == 2, \\\n                ""shear should be a list or tuple and it must be of length 2.""\n            shear_tmp = shear\n    else:\n        shear_tmp = shear\n\n    return _get_random_affine_params(\n        batch_size, height, width, degrees_tmp, translate, scale, shear_tmp, resample, same_on_batch, align_corners)\n\n\ndef random_rotation_generator(\n    batch_size: int,\n    degrees: FloatUnionType,\n    interpolation: Union[str, int, Resample] = Resample.BILINEAR.name,\n    same_on_batch: bool = False,\n    align_corners: bool = False\n) -> Dict[str, torch.Tensor]:\n    r""""""Get parameters for ``rotate`` for a random rotate transform.\n\n    Args:\n        batch_size (int): the tensor batch size.\n        degrees (sequence or float or tensor): range of degrees to select from. If degrees is a number the\n        range of degrees to select from will be (-degrees, +degrees)\n        interpolation (int, str or kornia.Resample): Default: Resample.BILINEAR\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n        align_corners (bool): interpolation flag. Default: False.\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n    """"""\n    if not torch.is_tensor(degrees):\n        if isinstance(degrees, (float, int)):\n            if degrees < 0:\n                raise ValueError(f""If Degrees is only one number it must be a positive number. Got{degrees}"")\n            degrees = torch.tensor([-degrees, degrees]).to(torch.float32)\n\n        elif isinstance(degrees, (tuple, list)):\n            degrees = torch.tensor(degrees).to(torch.float32)\n\n        else:\n            raise TypeError(f""Degrees should be a float number a sequence or a tensor. Got {type(degrees)}"")\n\n    # https://mypy.readthedocs.io/en/latest/casts.html cast to please mypy gods\n    degrees = cast(torch.Tensor, degrees)\n\n    if degrees.numel() != 2:\n        raise ValueError(""If degrees is a sequence it must be of length 2"")\n\n    degrees = _adapted_uniform((batch_size,), degrees[0], degrees[1], same_on_batch)\n\n    return dict(degrees=degrees,\n                interpolation=torch.tensor(Resample.get(interpolation).value),\n                align_corners=torch.tensor(align_corners))\n\n\ndef _get_random_affine_params(\n    batch_size: int,\n    height: int,\n    width: int,\n    degrees: TupleFloat,\n    translate: Optional[TupleFloat],\n    scales: Optional[TupleFloat],\n    shears: Optional[TupleFloat],\n    resample: Union[str, int, Resample] = Resample.BILINEAR.name,\n    same_on_batch: bool = False,\n    align_corners: bool = False\n) -> Dict[str, torch.Tensor]:\n    r""""""Get parameters for ```affine``` transformation random affine transform.\n    The returned matrix is Bx3x3.\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n    """"""\n    angle = _adapted_uniform((batch_size,), degrees[0], degrees[1], same_on_batch)\n\n    # compute tensor ranges\n    if scales is not None:\n        scale = _adapted_uniform((batch_size,), scales[0], scales[1], same_on_batch)\n    else:\n        scale = torch.ones(batch_size)\n\n    if translate is not None:\n        max_dx: float = translate[0] * width\n        max_dy: float = translate[1] * height\n        translations = torch.stack([\n            _adapted_uniform((batch_size,), -max_dx, max_dx, same_on_batch),\n            _adapted_uniform((batch_size,), -max_dy, max_dy, same_on_batch)\n        ], dim=-1)\n    else:\n        translations = torch.zeros(batch_size, 2)\n\n    center: torch.Tensor = torch.tensor(\n        [width, height], dtype=torch.float32).view(1, 2) / 2. - 0.5\n    center = center.expand(batch_size, -1)\n\n    if shears is not None:\n        sx = _adapted_uniform((batch_size,), shears[0], shears[1], same_on_batch)\n        sy = _adapted_uniform((batch_size,), shears[0], shears[1], same_on_batch)\n    else:\n        sx = sy = torch.tensor([0] * batch_size)\n\n    return dict(translations=translations,\n                center=center,\n                scale=scale,\n                angle=angle,\n                sx=sx,\n                sy=sy,\n                resample=torch.tensor(Resample.get(resample).value),\n                align_corners=torch.tensor(align_corners))\n\n\ndef random_crop_generator(\n    batch_size: int,\n    input_size: Tuple[int, int],\n    size: Tuple[int, int],\n    resize_to: Optional[Tuple[int, int]] = None,\n    interpolation: Union[str, int, Resample] = Resample.BILINEAR.name,\n    same_on_batch: bool = False,\n    align_corners: bool = False\n) -> Dict[str, torch.Tensor]:\n    r""""""Get parameters for ```crop``` transformation for crop transform.\n\n    Args:\n        batch_size (int): the tensor batch size.\n        input_size (tuple): Input image shape, like (h, w).\n        size (tuple): Desired size of the crop operation, like (h, w).\n        resize_to (tuple): Desired output size of the crop, like (h, w). If None, no resize will be performed.\n        interpolation (int, str or kornia.Resample): Default: Resample.BILINEAR\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n        align_corners (bool): interpolation flag. Default: False.\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n     """"""\n    x_diff = input_size[1] - size[1]\n    y_diff = input_size[0] - size[0]\n\n    if x_diff < 0 or y_diff < 0:\n        raise ValueError(""input_size %s cannot be smaller than crop size %s in any dimension.""\n                         % (str(input_size), str(size)))\n\n    x_start = _adapted_uniform((batch_size,), 0, x_diff + 1, same_on_batch).long()\n    y_start = _adapted_uniform((batch_size,), 0, y_diff + 1, same_on_batch).long()\n\n    crop = torch.tensor([[\n        [0, 0],\n        [size[1] - 1, 0],\n        [size[1] - 1, size[0] - 1],\n        [0, size[0] - 1],\n    ]]).repeat(batch_size, 1, 1)\n\n    crop_src = crop.clone()\n    crop_src[:, :, 0] += x_start.unsqueeze(dim=0).reshape(batch_size, 1)\n    crop_src[:, :, 1] += y_start.unsqueeze(dim=0).reshape(batch_size, 1)\n\n    if resize_to is None:\n        crop_dst = crop\n    else:\n        crop_dst = torch.tensor([[\n            [0, 0],\n            [resize_to[1] - 1, 0],\n            [resize_to[1] - 1, resize_to[0] - 1],\n            [0, resize_to[0] - 1],\n        ]]).repeat(batch_size, 1, 1)\n\n    return dict(src=crop_src,\n                dst=crop_dst,\n                interpolation=torch.tensor(Resample.get(interpolation).value),\n                align_corners=torch.tensor(align_corners))\n\n\ndef random_crop_size_generator(\n    size: Tuple[int, int],\n    scale: Tuple[float, float],\n    ratio: Tuple[float, float],\n    same_on_batch: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    r""""""Get cropping heights and widths for ```crop``` transformation for resized crop transform.\n\n    Args:\n        size (Tuple[int, int]): expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n    """"""\n    area = _adapted_uniform(\n        (10,), scale[0] * size[0] * size[1], scale[1] * size[0] * size[1], same_on_batch)\n    log_ratio = _adapted_uniform(\n        (10,), math.log(ratio[0]), math.log(ratio[1]), same_on_batch)\n    aspect_ratio = torch.exp(log_ratio)\n\n    w = torch.sqrt(area * aspect_ratio).int()\n    h = torch.sqrt(area / aspect_ratio).int()\n\n    # Element-wise w, h condition\n    cond = ((0 < h) * (h < size[1]) * (0 < w) * (w < size[0])).int()\n    if torch.sum(cond) > 0:\n        return (h[torch.argmax(cond)], w[torch.argmax(cond)])\n\n    # Fallback to center crop\n    in_ratio = float(size[0]) / float(size[1])\n    if (in_ratio < min(ratio)):\n        w = torch.tensor(size[0])\n        h = torch.round(w / min(ratio))\n    elif (in_ratio > max(ratio)):\n        h = torch.tensor(size[1])\n        w = torch.round(h * max(ratio))\n    else:  # whole image\n        w = torch.tensor(size[0])\n        h = torch.tensor(size[1])\n    return (h, w)\n\n\ndef random_rectangles_params_generator(\n    batch_size: int,\n    height: int,\n    width: int,\n    p: float,\n    scale: Tuple[float, float],\n    ratio: Tuple[float, float],\n    value: float = 0.,\n    same_on_batch: bool = False\n) -> Dict[str, torch.Tensor]:\n    r""""""Get parameters for ```erasing``` transformation for erasing transform\n\n    Args:\n        batch_size (int): the tensor batch size.\n        height (int) : height of the image.\n        width (int): width of the image.\n        p (float): probability of applying random earaing.\n        scale ([int, int]): range of size of the origin size cropped\n        ratio ([int, int]): range of aspect ratio of the origin aspect ratio cropped\n        value (float): value to be filled in the erased area.\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n    """"""\n    if not (isinstance(scale[0], float) and isinstance(scale[1], float) and scale[0] > 0. and scale[1] > 0.):\n        raise TypeError(\n            f""\'erase_scale_range\' must be a Tuple[float, float] with positive values. Got {scale}.""\n        )\n    if not (isinstance(ratio[0], float) and isinstance(ratio[1], float) and ratio[0] > 0. and ratio[1] > 0.):\n        raise TypeError(\n            f""\'ratio\' must be a Tuple[float, float] with positive values. Got {ratio}.""\n        )\n\n    batch_prob = random_prob_generator(batch_size, p, same_on_batch)[\'batch_prob\']\n    zeros = torch.zeros((batch_size,))\n    images_area = height * width\n    target_areas = _adapted_uniform(\n        (batch_size,), scale[0], scale[1], same_on_batch) * images_area\n    if ratio[0] < 1. and ratio[1] > 1.:\n        aspect_ratios1 = _adapted_uniform((batch_size,), ratio[0], 1, same_on_batch)\n        aspect_ratios2 = _adapted_uniform((batch_size,), 1, ratio[1], same_on_batch)\n        rand_idxs = torch.round(torch.rand((batch_size,))).bool()\n        aspect_ratios = torch.where(rand_idxs, aspect_ratios1, aspect_ratios2)\n    else:\n        aspect_ratios = _adapted_uniform((batch_size,), ratio[0], ratio[1], same_on_batch)\n\n    # based on target areas and aspect ratios, rectangle params are computed\n    heights = torch.min(\n        torch.max(torch.round((target_areas * aspect_ratios) ** (1 / 2)),\n                  torch.tensor(1.)),\n        torch.tensor(float(height))\n    ).int()\n\n    widths = torch.min(\n        torch.max(torch.round((target_areas / aspect_ratios) ** (1 / 2)),\n                  torch.tensor(1.)),\n        torch.tensor(float(width))\n    ).int()\n\n    xs = (_adapted_uniform((batch_size,), 0, 1, same_on_batch) * (torch.tensor(width) - widths + 1).float()).int()\n    ys = (_adapted_uniform((batch_size,), 0, 1, same_on_batch) * (torch.tensor(height) - heights + 1).float()).int()\n\n    params: Dict[str, torch.Tensor] = {}\n    params[""widths""] = torch.where(batch_prob, widths, zeros.to(widths.dtype))\n    params[""heights""] = torch.where(batch_prob, heights, zeros.to(widths.dtype))\n    params[""xs""] = xs\n    params[""ys""] = ys\n    params[""values""] = torch.tensor([value] * batch_size)\n    return params\n\n\ndef center_crop_params_generator(\n    batch_size: int,\n    height: int,\n    width: int,\n    size: Tuple[int, int],\n    align_corners: bool = False\n) -> Dict[str, torch.Tensor]:\n    r""""""Get parameters for ```center_crop``` transformation for center crop transform.\n\n    Args:\n        batch_size (int): the tensor batch size.\n        height (int) : height of the image.\n        width (int): width of the image.\n        size (tuple): Desired output size of the crop, like (h, w).\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n    """"""\n\n    if not isinstance(size, (tuple, list,)) and len(size) == 2:\n        raise ValueError(""Input size must be a tuple/list of length 2. Got {}""\n                         .format(size))\n\n    # unpack input sizes\n    dst_h, dst_w = size\n    src_h, src_w = height, width\n\n    # compute start/end offsets\n    dst_h_half = dst_h / 2\n    dst_w_half = dst_w / 2\n    src_h_half = src_h / 2\n    src_w_half = src_w / 2\n\n    start_x = src_w_half - dst_w_half\n    start_y = src_h_half - dst_h_half\n\n    end_x = start_x + dst_w - 1\n    end_y = start_y + dst_h - 1\n\n    # [y, x] origin\n    # top-left, top-right, bottom-right, bottom-left\n    points_src: torch.Tensor = torch.tensor([[\n        [start_x, start_y],\n        [end_x, start_y],\n        [end_x, end_y],\n        [start_x, end_y],\n    ]])\n\n    # [y, x] destination\n    # top-left, top-right, bottom-right, bottom-left\n    points_dst: torch.Tensor = torch.tensor([[\n        [0, 0],\n        [dst_w - 1, 0],\n        [dst_w - 1, dst_h - 1],\n        [0, dst_h - 1],\n    ]]).expand(points_src.shape[0], -1, -1)\n    return dict(src=points_src,\n                dst=points_dst,\n                interpolation=torch.tensor(Resample.BILINEAR.value),\n                align_corners=torch.tensor(align_corners))\n\n\ndef random_motion_blur_generator(\n    batch_size: int,\n    kernel_size: Union[int, Tuple[int, int]],\n    angle: UnionFloat,\n    direction: UnionFloat,\n    border_type: Union[int, str, BorderType] = BorderType.CONSTANT.name,\n    same_on_batch: bool = True\n) -> Dict[str, torch.Tensor]:\n\n    angle_bound: torch.Tensor = _check_and_bound(angle, \'angle\', center=0.)\n    direction_bound: torch.Tensor = _check_and_bound(direction, \'direction\', center=0., bounds=(-1, 1))\n\n    if isinstance(kernel_size, int):\n        ksize_factor = torch.tensor([kernel_size] * batch_size)\n    elif isinstance(kernel_size, tuple):\n        ksize_x, ksize_y = kernel_size\n        ksize_factor = _adapted_uniform(\n            (batch_size,), ksize_x // 2, ksize_y // 2, same_on_batch).int() * 2 + 1\n    else:\n        raise TypeError(f""Unsupported type: {type(kernel_size)}"")\n\n    angle_factor = _adapted_uniform(\n        (batch_size,), angle_bound[0].float(), angle_bound[1].float(), same_on_batch)\n\n    direction_factor = _adapted_uniform(\n        (batch_size,), direction_bound[0].float(), direction_bound[1].float(), same_on_batch)\n\n    return dict(ksize_factor=ksize_factor,\n                angle_factor=angle_factor,\n                direction_factor=direction_factor,\n                border_type=torch.tensor(BorderType.get(border_type).value))\n\n\ndef random_solarize_generator(\n    batch_size: int,\n    thresholds: FloatUnionType = 0.1,\n    additions: FloatUnionType = 0.1,\n    same_on_batch: bool = False\n) -> Dict[str, torch.Tensor]:\n    r""""""Generator random solarize parameters for a batch of images. For each pixel in the image less than threshold,\n    we add \'addition\' amount to it and then clip the pixel value to be between 0 and 1.0\n\n    Args:\n        batch_size (int): the number of images.\n        thresholds (float or tuple): Pixels less than threshold will selected. Otherwise, subtract 1.0 from the pixel.\n            Default value is 0.1\n        additions (float or tuple): The value is between -0.5 and 0.5. Default value is 0.1\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n    """"""\n\n    thresholds_bound: torch.Tensor = _check_and_bound(\n        thresholds, \'thresholds\', center=0.5, bounds=(0., 1.))\n    additions_bound: torch.Tensor = _check_and_bound(additions, \'additions\', bounds=(-0.5, 0.5))\n\n    thresholds_factor = _adapted_uniform(\n        (batch_size,), thresholds_bound[0].float(), thresholds_bound[1].float(), same_on_batch)\n\n    additions_factor = _adapted_uniform(\n        (batch_size,), additions_bound[0].float(), additions_bound[1].float(), same_on_batch)\n\n    return dict(\n        thresholds_factor=thresholds_factor,\n        additions_factor=additions_factor\n    )\n\n\ndef random_posterize_generator(\n    batch_size: int,\n    bits: Union[int, Tuple[int, int], torch.Tensor] = 3,\n    same_on_batch: bool = False\n) -> Dict[str, torch.Tensor]:\n    r""""""Generator random posterize parameters for a batch of images.\n\n    Args:\n        batch_size (int): the number of images.\n        bits (int or tuple): Default value is 3. Integer that ranged from 0 ~ 8.\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n    """"""\n    if not isinstance(bits, torch.Tensor):\n        bits = torch.tensor(bits)\n\n    if len(bits.size()) == 0:\n        lower = bits\n        upper = torch.tensor(8)\n    elif len(bits.size()) == 1 and bits.size(0) == 2:\n        lower = bits[0]\n        upper = bits[1]\n    else:\n        raise ValueError(f""Expect float or tuple. Got {bits}."")\n\n    bits_factor = _adapted_uniform((batch_size,), lower.float(), upper.float(), same_on_batch).int()\n\n    return dict(\n        bits_factor=bits_factor\n    )\n\n\ndef random_sharpness_generator(\n    batch_size: int,\n    sharpness: Union[float, Tuple[float, float], torch.Tensor] = 1.,\n    same_on_batch: bool = False\n) -> Dict[str, torch.Tensor]:\n    r""""""Generator random sharpness parameters for a batch of images.\n\n    Args:\n        batch_size (int): the number of images.\n        sharpness (float or tuple): Default value is 1. Must be above 0.\n        same_on_batch (bool): apply the same transformation across the batch. Default: False\n\n    Returns:\n        params Dict[str, torch.Tensor]: parameters to be passed for transformation.\n    """"""\n    if not isinstance(sharpness, torch.Tensor):\n        sharpness = torch.tensor(sharpness)\n\n    if len(sharpness.size()) == 0:\n        lower = torch.tensor(0)\n        upper = sharpness\n    elif len(sharpness.size()) == 1 and sharpness.size(0) == 2:\n        lower = sharpness[0]\n        upper = sharpness[1]\n    else:\n        raise ValueError(f""Expect float or tuple. Got {sharpness}."")\n\n    sharpness_factor = _adapted_uniform((batch_size,), lower.float(), upper.float(), same_on_batch)\n\n    return dict(\n        sharpness_factor=sharpness_factor\n    )\n'"
kornia/augmentation/types.py,4,"b'from typing import Callable, Tuple, Union, List, Optional, Dict, cast\n\nimport torch\n\nTupleFloat = Tuple[float, float]\nUnionFloat = Union[float, TupleFloat]\nUnionType = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\nUnionShape = Union[Tuple, torch.Size]\n\nFloatUnionType = Union[torch.Tensor, float, Tuple[float, float], List[float]]\nIntUnionType = Union[torch.Tensor, int, Tuple[int, int], List[int]]\nBoarderUnionType = Union[int, Tuple[int, int], Tuple[int, int, int, int]]\n'"
kornia/augmentation/utils.py,23,"b'from typing import Tuple, Union, List\n\nimport torch\nfrom torch.distributions import Uniform\n\nfrom kornia.utils.image import _to_bchw\nfrom .types import (\n    FloatUnionType,\n    UnionType,\n    UnionShape\n)\n\n\ndef _infer_batch_shape(input: UnionType) -> torch.Size:\n    r""""""Infer input shape. Input may be either (tensor,) or (tensor, transform_matrix)\n    """"""\n    if isinstance(input, tuple):\n        tensor = _transform_input(input[0])\n    else:\n        tensor = _transform_input(input)\n    return tensor.shape\n\n\ndef _transform_input(input: torch.Tensor) -> torch.Tensor:\n    r""""""Reshape an input tensor to be (*, C, H, W). Accept either (H, W), (C, H, W) or (*, C, H, W).\n    Args:\n        input: torch.Tensor\n\n    Returns:\n        torch.Tensor\n    """"""\n\n    return _to_bchw(input)\n\n\ndef _validate_input_dtype(input: torch.Tensor, accepted_dtypes: List) -> None:\n    r""""""Check if the dtype of the input tensor is in the range of accepted_dtypes\n    Args:\n        input: torch.Tensor\n        accepted_dtypes: List. e.g. [torch.float32, torch.float64]\n    """"""\n    if input.dtype not in accepted_dtypes:\n        raise TypeError(f""Expected input of {accepted_dtypes}. Got {input.dtype}"")\n\n\ndef _validate_shape(shape: UnionShape, required_shapes: List[str] = [""BCHW""]) -> None:\n    r""""""Check if the dtype of the input tensor is in the range of accepted_dtypes\n    Args:\n        input: torch.Tensor\n        required_shapes: List. e.g. [""BCHW"", ""BCDHW""]\n    """"""\n    passed = False\n    for required_shape in required_shapes:\n        if len(shape) == len(required_shape):\n            passed = True\n            break\n    if not passed:\n        raise TypeError(f""Expected input shape in {required_shape}. Got {shape}."")\n\n\ndef _validate_input_shape(input: torch.Tensor, channel_index: int, number: int) -> bool:\n    r""""""Validate if an input has the right shape. e.g. to check if an input is channel first.\n    If channel first, the second channel of an RGB input shall be fixed to 3. To verify using:\n        _validate_input_shape(input, 1, 3)\n    Args:\n        input: torch.Tensor\n        channel_index: int\n        number: int\n    Returns:\n        bool\n    """"""\n    return input.shape[channel_index] == number\n\n\ndef _adapted_uniform(shape: Union[Tuple, torch.Size], low, high, same_on_batch=False) -> torch.Tensor:\n    r"""""" The uniform function that accepts \'same_on_batch\'.\n    If same_on_batch is True, all values generated will be exactly same given a batch_size (shape[0]).\n    By default, same_on_batch is set to False.\n    """"""\n    if not isinstance(low, torch.Tensor):\n        low = torch.tensor(low).float()\n    if not isinstance(high, torch.Tensor):\n        high = torch.tensor(high).float()\n    dist = Uniform(low, high)\n    if same_on_batch:\n        return dist.rsample((1, *shape[1:])).repeat(shape[0])\n    else:\n        return dist.rsample(shape)\n\n\ndef _check_and_bound(factor: FloatUnionType, name: str, center: float = 0.,\n                     bounds: Tuple[float, float] = (0, float(\'inf\'))) -> torch.Tensor:\n    r""""""Check inputs and compute the corresponding factor bounds\n    """"""\n    factor_bound: torch.Tensor\n    if not isinstance(factor, torch.Tensor):\n        factor = torch.tensor(factor, dtype=torch.float32)\n\n    if factor.dim() == 0:\n        _center = torch.tensor(center, dtype=torch.float32)\n\n        if factor < 0:\n            raise ValueError(f""If {name} is a single number number, it must be non negative. Got {factor.item()}"")\n\n        factor_bound = torch.tensor([_center - factor, _center + factor], dtype=torch.float32)\n        # Should be something other than clamp\n        # Currently, single value factor will not out of scope as long as the user provided it.\n        factor_bound = torch.clamp(factor_bound, bounds[0], bounds[1])\n\n    elif factor.shape[0] == 2 and factor.dim() == 1:\n\n        if not bounds[0] <= factor[0] or not bounds[1] >= factor[1]:\n            raise ValueError(f""{name} out of bounds. Expected inside {bounds}, got {factor}."")\n\n        if not bounds[0] <= factor[0] <= factor[1] <= bounds[1]:\n            raise ValueError(f""{name}[0] should be smaller than {name}[1] got {factor}"")\n\n        factor_bound = factor\n\n    else:\n\n        raise TypeError(\n            f""The {name} should be a float number or a tuple with length 2 whose values move between {bounds}."")\n\n    return factor_bound\n'"
kornia/color/__init__.py,0,"b'from .gray import rgb_to_grayscale, RgbToGrayscale\nfrom .gray import bgr_to_grayscale, BgrToGrayscale\nfrom .rgb import BgrToRgb, bgr_to_rgb\nfrom .rgb import RgbToBgr, rgb_to_bgr\nfrom .rgb import RgbToRgba, rgb_to_rgba\nfrom .rgb import BgrToRgba, bgr_to_rgba\nfrom .rgb import RgbaToRgb, rgba_to_rgb\nfrom .rgb import RgbaToBgr, rgba_to_bgr\nfrom .hsv import RgbToHsv, rgb_to_hsv\nfrom .hsv import HsvToRgb, hsv_to_rgb\nfrom .hls import RgbToHls, rgb_to_hls\nfrom .hls import HlsToRgb, hls_to_rgb\nfrom .ycbcr import RgbToYcbcr, rgb_to_ycbcr\nfrom .ycbcr import YcbcrToRgb, ycbcr_to_rgb\nfrom .yuv import RgbToYuv, YuvToRgb, rgb_to_yuv, yuv_to_rgb\nfrom .xyz import RgbToXyz, XyzToRgb, rgb_to_xyz, xyz_to_rgb\nfrom .luv import RgbToLuv, LuvToRgb, rgb_to_luv, luv_to_rgb\nfrom .normalize import Normalize, normalize, Denormalize, denormalize\nfrom .zca import zca_mean, ZCAWhitening, zca_whiten, linear_transform\nfrom .core import add_weighted, AddWeighted\nfrom .histogram import histogram, histogram2d\nfrom .adjust import (\n    AdjustBrightness, AdjustContrast, AdjustGamma, AdjustHue, AdjustSaturation,\n)\nfrom .adjust import (\n    adjust_brightness, adjust_contrast, adjust_gamma, adjust_hue, adjust_saturation,\n    adjust_hue_raw, adjust_saturation_raw, solarize, equalize, posterize, sharpness\n)\n\n\n__all__ = [\n    ""rgb_to_grayscale"",\n    ""bgr_to_grayscale"",\n    ""bgr_to_rgb"",\n    ""rgb_to_bgr"",\n    ""rgb_to_rgba"",\n    ""rgb_to_hsv"",\n    ""hsv_to_rgb"",\n    ""rgb_to_hls"",\n    ""hls_to_rgb"",\n    ""rgb_to_ycbcr"",\n    ""ycbcr_to_rgb"",\n    ""rgb_to_yuv"",\n    ""yuv_to_rgb"",\n    ""rgb_to_xyz"",\n    ""xyz_to_rgb"",\n    ""normalize"",\n    ""denormalize"",\n    ""zca_mean"",\n    ""zca_whiten"",\n    ""linear_transform"",\n    ""histogram"",\n    ""histogram2d"",\n    ""adjust_brightness"",\n    ""adjust_contrast"",\n    ""adjust_gamma"",\n    ""adjust_hue"",\n    ""adjust_saturation"",\n    ""adjust_hue_raw"",\n    ""adjust_saturation_raw"",\n    ""solarize"",\n    ""equalize"",\n    ""posterize"",\n    ""sharpness"",\n    ""add_weighted"",\n    ""AddWeighted"",\n    ""RgbToGrayscale"",\n    ""BgrToGrayscale"",\n    ""BgrToRgb"",\n    ""RgbToBgr"",\n    ""RgbToRgba"",\n    ""RgbToHsv"",\n    ""HsvToRgb"",\n    ""RgbToHls"",\n    ""HlsToRgb"",\n    ""RgbToYcbcr"",\n    ""YcbcrToRgb"",\n    ""RgbToYuv"",\n    ""YuvToRgb"",\n    ""RgbToXyz"",\n    ""XyzToRgb"",\n    ""RgbToLuv"",\n    ""LuvToRgb"",\n    ""ZCAWhitening"",\n    ""Normalize"",\n    ""Denormalize"",\n    ""AdjustBrightness"",\n    ""AdjustContrast"",\n    ""AdjustGamma"",\n    ""AdjustHue"",\n    ""AdjustSaturation"",\n]\n'"
kornia/color/adjust.py,165,"b'from typing import Union, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.color.hsv import rgb_to_hsv, hsv_to_rgb\nfrom kornia.utils.image import _to_bchw\nfrom kornia.constants import pi\n\n\ndef adjust_saturation_raw(input: torch.Tensor, saturation_factor: Union[float, torch.Tensor]) -> torch.Tensor:\n    r""""""Adjust color saturation of an image. Expecting input to be in hsv format already.\n\n    See :class:`~kornia.color.AdjustSaturation` for details.\n    """"""\n\n    if not torch.is_tensor(input):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    if not isinstance(saturation_factor, (float, torch.Tensor,)):\n        raise TypeError(f""The saturation_factor should be a float number or torch.Tensor.""\n                        f""Got {type(saturation_factor)}"")\n\n    if isinstance(saturation_factor, float):\n        saturation_factor = torch.tensor([saturation_factor])\n\n    saturation_factor = saturation_factor.to(input.device).to(input.dtype)\n\n    if (saturation_factor < 0).any():\n        raise ValueError(f""Saturation factor must be non-negative. Got {saturation_factor}"")\n\n    for _ in input.shape[1:]:\n        saturation_factor = torch.unsqueeze(saturation_factor, dim=-1)\n\n    # unpack the hsv values\n    h, s, v = torch.chunk(input, chunks=3, dim=-3)\n\n    # transform the hue value and appl module\n    s_out: torch.Tensor = torch.clamp(s * saturation_factor, min=0, max=1)\n\n    # pack back back the corrected hue\n    out: torch.Tensor = torch.cat([h, s_out, v], dim=-3)\n\n    return out\n\n\ndef adjust_saturation(input: torch.Tensor, saturation_factor: Union[float, torch.Tensor]) -> torch.Tensor:\n    r""""""Adjust color saturation of an image.\n\n    See :class:`~kornia.color.AdjustSaturation` for details.\n    """"""\n\n    # convert the rgb image to hsv\n    x_hsv: torch.Tensor = rgb_to_hsv(input)\n\n    # perform the conversion\n    x_adjusted: torch.Tensor = adjust_saturation_raw(x_hsv, saturation_factor)\n\n    # convert back to rgb\n    out: torch.Tensor = hsv_to_rgb(x_adjusted)\n\n    return out\n\n\ndef adjust_hue_raw(input: torch.Tensor, hue_factor: Union[float, torch.Tensor]) -> torch.Tensor:\n    r""""""Adjust hue of an image. Expecting input to be in hsv format already.\n\n    See :class:`~kornia.color.AdjustHue` for details.\n    """"""\n\n    if not torch.is_tensor(input):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    if not isinstance(hue_factor, (float, torch.Tensor)):\n        raise TypeError(f""The hue_factor should be a float number or torch.Tensor in the range between""\n                        f"" [-PI, PI]. Got {type(hue_factor)}"")\n\n    if isinstance(hue_factor, float):\n        hue_factor = torch.tensor([hue_factor])\n\n    hue_factor = hue_factor.to(input.device).to(input.dtype)\n\n    if ((hue_factor < -pi) | (hue_factor > pi)).any():\n        raise ValueError(f""Hue-factor must be in the range [-PI, PI]. Got {hue_factor}"")\n\n    for _ in input.shape[1:]:\n        hue_factor = torch.unsqueeze(hue_factor, dim=-1)\n\n    # unpack the hsv values\n    h, s, v = torch.chunk(input, chunks=3, dim=-3)\n\n    # transform the hue value and appl module\n    divisor: float = 2 * pi.item()\n    h_out: torch.Tensor = torch.fmod(h + hue_factor, divisor)\n\n    # pack back back the corrected hue\n    out: torch.Tensor = torch.cat([h_out, s, v], dim=-3)\n\n    return out\n\n\ndef adjust_hue(input: torch.Tensor, hue_factor: Union[float, torch.Tensor]) -> torch.Tensor:\n    r""""""Adjust hue of an image.\n\n    See :class:`~kornia.color.AdjustHue` for details.\n    """"""\n\n    # convert the rgb image to hsv\n    x_hsv: torch.Tensor = rgb_to_hsv(input)\n\n    # perform the conversion\n    x_adjusted: torch.Tensor = adjust_hue_raw(x_hsv, hue_factor)\n\n    # convert back to rgb\n    out: torch.Tensor = hsv_to_rgb(x_adjusted)\n\n    return out\n\n\ndef adjust_gamma(input: torch.Tensor, gamma: Union[float, torch.Tensor],\n                 gain: Union[float, torch.Tensor] = 1.) -> torch.Tensor:\n    r""""""Perform gamma correction on an image.\n\n    See :class:`~kornia.color.AdjustGamma` for details.\n    """"""\n\n    if not torch.is_tensor(input):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    if not isinstance(gamma, (float, torch.Tensor)):\n        raise TypeError(f""The gamma should be a positive float or torch.Tensor. Got {type(gamma)}"")\n\n    if not isinstance(gain, (float, torch.Tensor)):\n        raise TypeError(f""The gain should be a positive float or torch.Tensor. Got {type(gain)}"")\n\n    if isinstance(gamma, float):\n        gamma = torch.tensor([gamma])\n\n    if isinstance(gain, float):\n        gain = torch.tensor([gain])\n\n    gamma = gamma.to(input.device).to(input.dtype)\n    gain = gain.to(input.device).to(input.dtype)\n\n    if (gamma < 0.0).any():\n        raise ValueError(f""Gamma must be non-negative. Got {gamma}"")\n\n    if (gain < 0.0).any():\n        raise ValueError(f""Gain must be non-negative. Got {gain}"")\n\n    for _ in input.shape[1:]:\n        gamma = torch.unsqueeze(gamma, dim=-1)\n        gain = torch.unsqueeze(gain, dim=-1)\n\n    # Apply the gamma correction\n    x_adjust: torch.Tensor = gain * torch.pow(input, gamma)\n\n    # Truncate between pixel values\n    out: torch.Tensor = torch.clamp(x_adjust, 0.0, 1.0)\n\n    return out\n\n\ndef adjust_contrast(input: torch.Tensor,\n                    contrast_factor: Union[float, torch.Tensor]) -> torch.Tensor:\n    r""""""Adjust Contrast of an image.\n\n    See :class:`~kornia.color.AdjustContrast` for details.\n    """"""\n\n    if not torch.is_tensor(input):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    if not isinstance(contrast_factor, (float, torch.Tensor,)):\n        raise TypeError(f""The factor should be either a float or torch.Tensor. ""\n                        f""Got {type(contrast_factor)}"")\n\n    if isinstance(contrast_factor, float):\n        contrast_factor = torch.tensor([contrast_factor])\n\n    contrast_factor = contrast_factor.to(input.device).to(input.dtype)\n\n    if (contrast_factor < 0).any():\n        raise ValueError(f""Contrast factor must be non-negative. Got {contrast_factor}"")\n\n    for _ in input.shape[1:]:\n        contrast_factor = torch.unsqueeze(contrast_factor, dim=-1)\n\n    # Apply contrast factor to each channel\n    x_adjust: torch.Tensor = input * contrast_factor\n\n    # Truncate between pixel values\n    out: torch.Tensor = torch.clamp(x_adjust, 0.0, 1.0)\n\n    return out\n\n\ndef adjust_brightness(input: torch.Tensor,\n                      brightness_factor: Union[float, torch.Tensor]) -> torch.Tensor:\n    r""""""Adjust Brightness of an image.\n\n    See :class:`~kornia.color.AdjustBrightness` for details.\n    """"""\n\n    if not torch.is_tensor(input):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    if not isinstance(brightness_factor, (float, torch.Tensor,)):\n        raise TypeError(f""The factor should be either a float or torch.Tensor. ""\n                        f""Got {type(brightness_factor)}"")\n\n    if isinstance(brightness_factor, float):\n        brightness_factor = torch.tensor([brightness_factor])\n\n    brightness_factor = brightness_factor.to(input.device).to(input.dtype)\n\n    for _ in input.shape[1:]:\n        brightness_factor = torch.unsqueeze(brightness_factor, dim=-1)\n\n    # Apply brightness factor to each channel\n    x_adjust: torch.Tensor = input + brightness_factor\n\n    # Truncate between pixel values\n    out: torch.Tensor = torch.clamp(x_adjust, 0.0, 1.0)\n\n    return out\n\n\ndef _solarize(input: torch.Tensor, thresholds: Union[float, torch.Tensor] = 0.5) -> torch.Tensor:\n    r"""""" For each pixel in the image, select the pixel if the value is less than the threshold.\n    Otherwise, subtract 1.0 from the pixel.\n    Args:\n        input (torch.Tensor): image or batched images to solarize.\n        thresholds (float or torch.Tensor): solarize thresholds.\n            If int or one element tensor, input will be solarized across the whole batch.\n            If 1-d tensor, input will be solarized element-wise, len(thresholds) == len(input).\n    Returns:\n        torch.Tensor: Solarized images.\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    if not isinstance(thresholds, (float, torch.Tensor,)):\n        raise TypeError(f""The factor should be either a float or torch.Tensor. ""\n                        f""Got {type(thresholds)}"")\n\n    if isinstance(thresholds, torch.Tensor) and len(thresholds.shape) != 0:\n        assert input.size(0) == len(thresholds) and len(thresholds.shape) == 1, \\\n            f""threshholds must be a 1-d vector of shape ({input.size(0)},). Got {thresholds}""\n        # TODO: I am not happy about this line, but no easy to do batch-wise operation\n        thresholds = torch.stack([x.expand(*input.shape[1:]) for x in thresholds])\n\n    return torch.where(input < thresholds, input, 1.0 - input)\n\n\ndef solarize(input: torch.Tensor, thresholds: Union[float, torch.Tensor] = 0.5,\n             additions: Optional[Union[float, torch.Tensor]] = None) -> torch.Tensor:\n    r"""""" For each pixel in the image less than threshold, we add \'addition\' amount to it and then clip the\n    pixel value to be between 0 and 1.0. The value of \'addition\' is between -0.5 and 0.5.\n    Args:\n        input (torch.Tensor): image tensor with shapes like (C, H, W) or (B, C, H, W) to solarize.\n        thresholds (float or torch.Tensor): solarize thresholds.\n            If int or one element tensor, input will be solarized across the whole batch.\n            If 1-d tensor, input will be solarized element-wise, len(thresholds) == len(input).\n        additions (optional, float or torch.Tensor): between -0.5 and 0.5. Default None.\n            If None, no addition will be performed.\n            If int or one element tensor, same addition will be added across the whole batch.\n            If 1-d tensor, additions will be added element-wisely, len(additions) == len(input).\n    Returns:\n        torch.Tensor: Solarized images.\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    if not isinstance(thresholds, (float, torch.Tensor,)):\n        raise TypeError(f""The factor should be either a float or torch.Tensor. ""\n                        f""Got {type(thresholds)}"")\n\n    if isinstance(thresholds, float):\n        thresholds = torch.tensor(thresholds)\n\n    if additions is not None:\n        if not isinstance(additions, (float, torch.Tensor,)):\n            raise TypeError(f""The factor should be either a float or torch.Tensor. ""\n                            f""Got {type(additions)}"")\n\n        if isinstance(additions, float):\n            additions = torch.tensor(additions)\n\n        assert torch.all((additions < 0.5) * (additions > -0.5)), \\\n            f""The value of \'addition\' is between -0.5 and 0.5. Got {additions}.""\n\n        if isinstance(additions, torch.Tensor) and len(additions.shape) != 0:\n            assert input.size(0) == len(additions) and len(additions.shape) == 1, \\\n                f""additions must be a 1-d vector of shape ({input.size(0)},). Got {additions}""\n            # TODO: I am not happy about this line, but no easy to do batch-wise operation\n            additions = torch.stack([x.expand(*input.shape[1:]) for x in additions])\n\n        input = input + additions\n        input = input.clamp(0., 1.)\n\n    return _solarize(input, thresholds)\n\n\ndef posterize(input: torch.Tensor, bits: Union[int, torch.Tensor]) -> torch.Tensor:\n    r""""""Reduce the number of bits for each color channel. Non-differentiable function, uint8 involved.\n    Args:\n        input (torch.Tensor): image tensor with shapes like (C, H, W) or (B, C, H, W) to posterize.\n        bits (int or torch.Tensor): number of high bits. Must be in range [0, 8].\n            If int or one element tensor, input will be posterized by this bits.\n            If 1-d tensor, input will be posterized element-wisely, len(bits) == input.shape[1].\n            If n-d tensor, input will be posterized element-channel-wisely, bits.shape == input.shape[:len(bits.shape)]\n    Returns:\n        torch.Tensor: Image with reduced color channels.\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    if isinstance(bits, int):\n        bits = torch.tensor(bits)\n\n    if not torch.all((bits >= 0) * (bits <= 8)) and bits.dtype == torch.int:\n        raise ValueError(f""bits must be integers within range [0, 8]. Got {bits}."")\n\n    # TODO: Make a differentiable version\n    # Current version:\n    # Ref: https://github.com/open-mmlab/mmcv/pull/132/files#diff-309c9320c7f71bedffe89a70ccff7f3bR19\n    # Ref: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py#L222\n    # Potential approach: implementing kornia.LUT with floating points\n    # https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/functional.py#L472\n    def _left_shift(input: torch.Tensor, shift: torch.Tensor):\n        return ((input * 255).to(torch.uint8) * (2 ** shift)).to(input.dtype) / 255.\n\n    def _right_shift(input: torch.Tensor, shift: torch.Tensor):\n        return (input * 255).to(torch.uint8) / (2 ** shift).to(input.dtype) / 255.\n\n    def _posterize_one(input: torch.Tensor, bits: torch.Tensor):\n        # Single bits value condition\n        if bits == 0:\n            return torch.zeros_like(input)\n        if bits == 8:\n            return input.clone()\n        bits = 8 - bits\n        return _left_shift(_right_shift(input, bits), bits)\n\n    if len(bits.shape) == 0 or (len(bits.shape) == 1 and len(bits) == 1):\n        return _posterize_one(input, bits)\n\n    res = []\n    if len(bits.shape) == 1:\n        input = _to_bchw(input)\n\n        assert bits.shape[0] == input.shape[0], \\\n            f""Batch size must be equal between bits and input. Got {bits.shape[0]}, {input.shape[0]}.""\n\n        for i in range(input.shape[0]):\n            res.append(_posterize_one(input[i], bits[i]))\n        return torch.stack(res, dim=0)\n\n    assert bits.shape == input.shape[:len(bits.shape)], \\\n        f""Batch and channel must be equal between bits and input. Got {bits.shape}, {input.shape[:len(bits.shape)]}.""\n    _input = input.view(-1, *input.shape[len(bits.shape):])\n    _bits = bits.flatten()\n    for i in range(input.shape[0]):\n        res.append(_posterize_one(_input[i], _bits[i]))\n    return torch.stack(res, dim=0).reshape(*input.shape)\n\n\ndef sharpness(input: torch.Tensor, factor: Union[float, torch.Tensor]) -> torch.Tensor:\n    r""""""Implements Sharpness function from PIL using torch ops.\n    Args:\n        input (torch.Tensor): image tensor with shapes like (C, H, W) or (B, C, H, W) to sharpen.\n        factor (float or torch.Tensor): factor of sharpness strength. Must be above 0.\n            If float or one element tensor, input will be sharpened by the same factor across the whole batch.\n            If 1-d tensor, input will be sharpened element-wisely, len(factor) == len(input).\n    Returns:\n        torch.Tensor: Sharpened image or images.\n    """"""\n    input = _to_bchw(input)\n    if isinstance(factor, torch.Tensor):\n        factor = factor.squeeze()\n        if len(factor.size()) != 0:\n            assert input.size(0) == factor.size(0), \\\n                f""Input batch size shall match with factor size if 1d array. Got {input.size(0)} and {factor.size(0)}""\n    else:\n        factor = float(factor)\n    kernel = torch.tensor([\n        [1, 1, 1],\n        [1, 5, 1],\n        [1, 1, 1]\n    ], dtype=input.dtype).view(1, 1, 3, 3).repeat(3, 1, 1, 1)\n\n    # This shall be equivalent to depthwise conv2d:\n    # Ref: https://discuss.pytorch.org/t/depthwise-and-separable-convolutions-in-pytorch/7315/2\n    degenerate = torch.nn.functional.conv2d(input, kernel, bias=None, stride=1, groups=input.size(1))\n    degenerate = torch.clamp(degenerate, 0., 1.)\n\n    mask = torch.ones_like(degenerate)\n    padded_mask = torch.nn.functional.pad(mask, [1, 1, 1, 1])\n    padded_degenerate = torch.nn.functional.pad(degenerate, [1, 1, 1, 1])\n    result = torch.where(padded_mask == 1, padded_degenerate, input)\n\n    def _blend_one(input1: torch.Tensor, input2: torch.Tensor, factor: Union[float, torch.Tensor]) -> torch.Tensor:\n        if isinstance(factor, torch.Tensor):\n            factor = factor.squeeze()\n            assert len(factor.size()) == 0, f""Factor shall be a float or single element tensor. Got {factor}""\n        if factor == 0.:\n            return input1\n        if factor == 1.:\n            return input2\n        diff = (input2 - input1) * factor\n        res = input1 + diff\n        if factor > 0. and factor < 1.:\n            return res\n        return torch.clamp(res, 0, 1)\n    if isinstance(factor, (float)) or len(factor.size()) == 0:\n        return _blend_one(input, result, factor)\n    return torch.stack([_blend_one(input[i], result[i], factor[i]) for i in range(len(factor))])\n\n\ndef equalize(input: torch.Tensor) -> torch.Tensor:\n    """"""Implements Equalize function from PIL using PyTorch ops based on uint8 format:\n    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py#L352\n    Args:\n        input (torch.Tensor): image tensor with shapes like (C, H, W) or (B, C, H, W) to equalize.\n    Returns:\n        torch.Tensor: Sharpened image or images.\n    """"""\n    input = _to_bchw(input) * 255\n\n    # Code taken from: https://github.com/pytorch/vision/pull/796\n    def scale_channel(im, c):\n        """"""Scale the data in the channel to implement equalize.""""""\n        im = im[c, :, :]\n        # Compute the histogram of the image channel.\n        histo = torch.histc(im, bins=256, min=0, max=255)\n        # For the purposes of computing the step, filter out the nonzeros.\n        nonzero_histo = torch.reshape(histo[histo != 0], [-1])\n        step = (torch.sum(nonzero_histo) - nonzero_histo[-1]) // 255\n\n        def build_lut(histo, step):\n            # Compute the cumulative sum, shifting by step // 2\n            # and then normalization by step.\n            lut = (torch.cumsum(histo, 0) + (step // 2)) // step\n            # Shift lut, prepending with 0.\n            lut = torch.cat([torch.zeros(1), lut[:-1]])\n            # Clip the counts to be in range.  This is done\n            # in the C code for image.point.\n            return torch.clamp(lut, 0, 255)\n\n        # If step is zero, return the original image.  Otherwise, build\n        # lut from the full histogram and step and then index from it.\n        if step == 0:\n            result = im\n        else:\n            # can\'t index using 2d index. Have to flatten and then reshape\n            result = torch.gather(build_lut(histo, step), 0, im.flatten().long())\n            result = result.reshape_as(im)\n\n        return result / 255.\n\n    res = []\n    for image in input:\n        # Assumes RGB for now.  Scales each channel independently\n        # and then stacks the result.\n        scaled_image = torch.stack([scale_channel(image, i) for i in range(len(image))])\n        res.append(scaled_image)\n    return torch.stack(res)\n\n\nclass AdjustSaturation(nn.Module):\n    r""""""Adjust color saturation of an image.\n\n    The input image is expected to be an RGB image in the range of [0, 1].\n\n    Args:\n        input (torch.Tensor): Image/Tensor to be adjusted in the shape of (\\*, N).\n        saturation_factor (float):  How much to adjust the saturation. 0 will give a black\n        and white image, 1 will give the original image while 2 will enhance the saturation\n        by a factor of 2.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n\n    def __init__(self, saturation_factor: Union[float, torch.Tensor]) -> None:\n        super(AdjustSaturation, self).__init__()\n        self.saturation_factor: Union[float, torch.Tensor] = saturation_factor\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return adjust_saturation(input, self.saturation_factor)\n\n\nclass AdjustHue(nn.Module):\n    r""""""Adjust hue of an image.\n\n    The input image is expected to be an RGB image in the range of [0, 1].\n\n    Args:\n        input (torch.Tensor): Image/Tensor to be adjusted in the shape of (\\*, N).\n        hue_factor (float): How much to shift the hue channel. Should be in [-PI, PI]. PI\n          and -PI give complete reversal of hue channel in HSV space in positive and negative\n          direction respectively. 0 means no shift. Therefore, both -PI and PI will give an\n          image with complementary colors while 0 gives the original image.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n\n    def __init__(self, hue_factor: Union[float, torch.Tensor]) -> None:\n        super(AdjustHue, self).__init__()\n        self.hue_factor: Union[float, torch.Tensor] = hue_factor\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return adjust_hue(input, self.hue_factor)\n\n\nclass AdjustGamma(nn.Module):\n    r""""""Perform gamma correction on an image.\n\n    The input image is expected to be in the range of [0, 1].\n\n    Args:\n        input (torch.Tensor): Image/Tensor to be adjusted in the shape of (\\*, N).\n        gamma (float): Non negative real number, same as \xce\xb3\\gamma\xce\xb3 in the equation.\n          gamma larger than 1 make the shadows darker, while gamma smaller than 1 make\n          dark regions lighter.\n        gain (float, optional): The constant multiplier. Default 1.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n\n    def __init__(self, gamma: Union[float, torch.Tensor], gain: Union[float, torch.Tensor] = 1.) -> None:\n        super(AdjustGamma, self).__init__()\n        self.gamma: Union[float, torch.Tensor] = gamma\n        self.gain: Union[float, torch.Tensor] = gain\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return adjust_gamma(input, self.gamma, self.gain)\n\n\nclass AdjustContrast(nn.Module):\n    r""""""Adjust Contrast of an image. This implementation aligns OpenCV, not PIL. Hence,\n    the output differs from TorchVision.\n\n    The input image is expected to be in the range of [0, 1].\n\n    Args:\n        input (torch.Tensor): Image to be adjusted in the shape of (\\*, N).\n        contrast_factor (Union[float, torch.Tensor]): Contrast adjust factor per element\n          in the batch. 0 generates a compleatly black image, 1 does not modify\n          the input image while any other non-negative number modify the\n          brightness by this factor.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n\n    def __init__(self, contrast_factor: Union[float, torch.Tensor]) -> None:\n        super(AdjustContrast, self).__init__()\n        self.contrast_factor: Union[float, torch.Tensor] = contrast_factor\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return adjust_contrast(input, self.contrast_factor)\n\n\nclass AdjustBrightness(nn.Module):\n    r""""""Adjust Brightness of an image. This implementation aligns OpenCV, not PIL. Hence,\n    the output differs from TorchVision.\n\n    The input image is expected to be in the range of [0, 1].\n\n    Args:\n        input (torch.Tensor): Image/Input to be adjusted in the shape of (\\*, N).\n        brightness_factor (Union[float, torch.Tensor]): Brightness adjust factor per element\n          in the batch. 0 does not modify the input image while any other number modify the\n          brightness.\n\n    Returns:\n        torch.Tensor: Adjusted image.\n    """"""\n\n    def __init__(self, brightness_factor: Union[float, torch.Tensor]) -> None:\n        super(AdjustBrightness, self).__init__()\n        self.brightness_factor: Union[float, torch.Tensor] = brightness_factor\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return adjust_brightness(input, self.brightness_factor)\n'"
kornia/color/core.py,10,"b'import torch\nimport torch.nn as nn\n\n\ndef add_weighted(src1: torch.Tensor, alpha: float,\n                 src2: torch.Tensor, beta: float,\n                 gamma: float) -> torch.Tensor:\n    r""""""Blend two Tensors.\n\n    See :class:`~kornia.color.AddWeighted` for details.\n    """"""\n    if not isinstance(src1, torch.Tensor):\n        raise TypeError(""src1 should be a tensor. Got {}"".format(type(src1)))\n\n    if not isinstance(src2, torch.Tensor):\n        raise TypeError(""src2 should be a tensor. Got {}"".format(type(src2)))\n\n    if not isinstance(alpha, float):\n        raise TypeError(""alpha should be a float. Got {}"".format(type(alpha)))\n\n    if not isinstance(beta, float):\n        raise TypeError(""beta should be a float. Got {}"".format(type(beta)))\n\n    if not isinstance(gamma, float):\n        raise TypeError(""gamma should be a float. Got {}"".format(type(gamma)))\n\n    return src1 * alpha + src2 * beta + gamma\n\n\nclass AddWeighted(nn.Module):\n    r""""""Calculates the weighted sum of two Tensors.\n\n    The function calculates the weighted sum of two Tensors as follows:\n\n    .. math::\n        out = src1 * alpha + src2 * beta + gamma\n\n    Args:\n        src1 (torch.Tensor): Tensor.\n        alpha (float): weight of the src1 elements.\n        src2 (torch.Tensor): Tensor of same size and channel number as src1.\n        beta (float): weight of the src2 elements.\n        gamma (float): scalar added to each sum.\n\n    Returns:\n        torch.Tensor: Weighted Tensor.\n    """"""\n\n    def __init__(self, alpha: float, beta: float, gamma: float) -> None:\n        super(AddWeighted, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n\n    def forward(self, src1: torch.Tensor, src2: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return add_weighted(src1, self.alpha, src2, self.beta, self.gamma)\n'"
kornia/color/gray.py,22,"b'import torch\nimport torch.nn as nn\nfrom .rgb import bgr_to_rgb\n\n\nclass RgbToGrayscale(nn.Module):\n    r""""""convert RGB image to grayscale version of image.\n\n    the image data is assumed to be in the range of (0, 1).\n\n    args:\n        input (torch.Tensor): RGB image to be converted to grayscale.\n\n    returns:\n        torch.Tensor: grayscale version of the image.\n\n    shape:\n        - input: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 1, H, W)`\n\n    reference:\n        https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html\n\n    Examples::\n\n        >>> import torch\n        >>> import kornia\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> gray = kornia.color.RgbToGrayscale()\n        >>> output = gray(input)  # 2x1x4x5\n    """"""\n\n    def __init__(self) -> None:\n        super(RgbToGrayscale, self).__init__()\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rgb_to_grayscale(input)\n\n\ndef rgb_to_grayscale(input: torch.Tensor) -> torch.Tensor:\n    r""""""Convert a RGB image to grayscale.\n\n    See :class:`~kornia.color.RgbToGrayscale` for details.\n\n    Args:\n        input (torch.Tensor): RGB image to be converted to grayscale.\n\n    Returns:\n        torch.Tensor: Grayscale version of the image.\n    """"""\n    if not isinstance(input, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(input)))\n\n    if len(input.shape) < 3 and input.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(input.shape))\n\n    r, g, b = torch.chunk(input, chunks=3, dim=-3)\n    gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b\n    return gray\n\n\nclass BgrToGrayscale(nn.Module):\n    r""""""convert BGR image to grayscale version of image.\n\n    the image data is assumed to be in the range of (0, 1).\n\n    args:\n        input (torch.Tensor): BGR image to be converted to grayscale.\n\n    returns:\n        torch.Tensor: grayscale version of the image.\n\n    shape:\n        - input: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 1, H, W)`\n\n    reference:\n        https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html\n\n    Examples::\n\n        >>> import torch\n        >>> import kornia\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> gray = kornia.color.BgrToGrayscale()\n        >>> output = gray(input)  # 2x1x4x5\n    """"""\n\n    def __init__(self) -> None:\n        super(BgrToGrayscale, self).__init__()\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return bgr_to_grayscale(input)\n\n\ndef bgr_to_grayscale(input: torch.Tensor) -> torch.Tensor:\n    r""""""Convert a BGR image to grayscale.\n\n    See :class:`~kornia.color.BgrToGrayscale` for details.\n\n    Args:\n        input (torch.Tensor): BGR image to be converted to grayscale.\n\n    Returns:\n        torch.Tensor: Grayscale version of the image.\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(input)))\n\n    if len(input.shape) < 3 and input.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(input.shape))\n\n    input_rgb = bgr_to_rgb(input)\n    gray: torch.Tensor = rgb_to_grayscale(input_rgb)\n    return gray\n'"
kornia/color/histogram.py,42,"b'from typing import Tuple, List\nimport torch\nimport torch.nn as nn\n\n\ndef marginal_pdf(values: torch.Tensor, bins: torch.Tensor, sigma: torch.Tensor,\n                 epsilon: float = 1e-10) -> Tuple[torch.Tensor, torch.Tensor]:\n    """"""\n    Function that calculates the marginal probability distribution function of the input tensor based on the number\n    of histogram bins.\n    Args:\n                    values: (torch.Tensor), shape [BxNx1]\n                    bins: (torch.Tensor), shape [NUM_BINS]\n                    sigma: (torch.Tensor), shape [1], gaussian smoothing factor\n                    epsilon: (float), scalar, for numerical stability\n    Returns:\n                    pdf: (torch.Tensor), shape [BxN]\n                    kernel_values: (torch.Tensor), shape [BxNxNUM_BINS]""""""\n\n    if not torch.is_tensor(values):\n        raise TypeError(""Input values type is not a torch.Tensor. Got {}""\n                        .format(type(values)))\n    if not torch.is_tensor(bins):\n        raise TypeError(""Input bins type is not a torch.Tensor. Got {}""\n                        .format(type(bins)))\n    if not torch.is_tensor(sigma):\n        raise TypeError(""Input sigma type is not a torch.Tensor. Got {}""\n                        .format(type(sigma)))\n\n    if not values.dim() == 3:\n        raise ValueError(""Input values must be a of the shape BxNx1.""\n                         "" Got {}"".format(values.shape))\n    if not bins.dim() == 1:\n        raise ValueError(""Input bins must be a of the shape NUM_BINS""\n                         "" Got {}"".format(bins.shape))\n    if not sigma.dim() == 0:\n        raise ValueError(""Input sigma must be a of the shape 1""\n                         "" Got {}"".format(sigma.shape))\n\n    residuals = values - bins.unsqueeze(0).unsqueeze(0)\n    kernel_values = torch.exp(-0.5 * (residuals / sigma).pow(2))\n\n    pdf = torch.mean(kernel_values, dim=1)\n    normalization = torch.sum(pdf, dim=1).unsqueeze(1) + epsilon\n    pdf = pdf / normalization\n\n    return (pdf, kernel_values)\n\n\ndef joint_pdf(kernel_values1: torch.Tensor, kernel_values2: torch.Tensor, epsilon: float = 1e-10) -> torch.Tensor:\n    """"""\n    Function that calculates the joint probability distribution function of the input tensors based on the number\n    of histogram bins.\n    Args:\n                    kernel_values1: (torch.Tensor), shape [BxNxNUM_BINS]\n                    kernel_values2: (torch.Tensor), shape [BxNxNUM_BINS]\n                    epsilon: (float), scalar, for numerical stability\n    Returns:\n                    pdf: (torch.Tensor), shape [BxNUM_BINSxNUM_BINS]""""""\n\n    if not torch.is_tensor(kernel_values1):\n        raise TypeError(""Input kernel_values1 type is not a torch.Tensor. Got {}""\n                        .format(type(kernel_values1)))\n    if not torch.is_tensor(kernel_values2):\n        raise TypeError(""Input kernel_values2 type is not a torch.Tensor. Got {}""\n                        .format(type(kernel_values2)))\n\n    if not kernel_values1.dim() == 3:\n        raise ValueError(""Input kernel_values1 must be a of the shape BxN.""\n                         "" Got {}"".format(kernel_values1.shape))\n    if not kernel_values2.dim() == 3:\n        raise ValueError(""Input kernel_values2 must be a of the shape BxN.""\n                         "" Got {}"".format(kernel_values2.shape))\n    if kernel_values1.shape != kernel_values2.shape:\n        raise ValueError(""Inputs kernel_values1 and kernel_values2 must have the same shape.""\n                         "" Got {} and {}"".format(kernel_values1.shape, kernel_values2.shape))\n\n    joint_kernel_values = torch.matmul(kernel_values1.transpose(1, 2), kernel_values2)\n    normalization = torch.sum(joint_kernel_values, dim=(1, 2)).view(-1, 1, 1) + epsilon\n    pdf = joint_kernel_values / normalization\n\n    return pdf\n\n\ndef histogram(x: torch.Tensor, bins: torch.Tensor, bandwidth: torch.Tensor, epsilon: float = 1e-10) -> torch.Tensor:\n    """"""\n    Function that estimates the histogram of the input tensor. The calculation uses kernel\n    density estimation which requires a bandwidth (smoothing) parameter.\n    Args:\n                    x: (torch.Tensor), shape [BxN]\n                    bins: (torch.Tensor), shape [NUM_BINS]\n                    bandwidth: (torch.Tensor), shape [1], gaussian smoothing factor\n                    epsilon: (float), scalar, for numerical stability\n    Returns:\n                    pdf: (torch.Tensor), shape [BxNUM_BINS]""""""\n\n    pdf, _ = marginal_pdf(x.unsqueeze(2), bins, bandwidth, epsilon)\n\n    return pdf\n\n\ndef histogram2d(\n        x1: torch.Tensor,\n        x2: torch.Tensor,\n        bins: torch.Tensor,\n        bandwidth: torch.Tensor,\n        epsilon: float = 1e-10) -> torch.Tensor:\n    """"""\n    Function that estimates the histogram of the input tensor. The calculation uses kernel\n    density estimation which requires a bandwidth (smoothing) parameter.\n    Args:\n                    x1: (torch.Tensor), shape [BxN1]\n                    x2: (torch.Tensor), shape [BxN2]\n                    bins: (torch.Tensor), shape [NUM_BINS]\n                    bandwidth: (torch.Tensor), scalar, gaussian smoothing factor\n                    epsilon: (float), scalar, for numerical stability\n    Returns:\n                    pdf: (torch.Tensor), shape [BxNUM_BINSxNUM_BINS]""""""\n\n    pdf1, kernel_values1 = marginal_pdf(x1.unsqueeze(2), bins, bandwidth, epsilon)\n    pdf2, kernel_values2 = marginal_pdf(x2.unsqueeze(2), bins, bandwidth, epsilon)\n\n    pdf = joint_pdf(kernel_values1, kernel_values2)\n\n    return pdf\n'"
kornia/color/hls.py,45,"b'import torch\nimport torch.nn as nn\nimport kornia\nfrom kornia.constants import pi\n\n\nclass HlsToRgb(nn.Module):\n    r""""""Convert image from HLS to Rgb\n    The image data is assumed to be in the range of (0, 1).\n\n    args:\n        image (torch.Tensor): HLS image to be converted to RGB.\n\n    returns:\n        torch.tensor: RGB version of the image.\n\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    reference:\n        https://en.wikipedia.org/wiki/HSL_and_HSV\n\n    Examples::\n\n        >>> import torch\n        >>> import kornia\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> rgb = kornia.color.HlsToRgb()\n        >>> output = rgb(input)  # 2x3x4x5\n\n    """"""\n\n    def __init__(self) -> None:\n        super(HlsToRgb, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return hls_to_rgb(image)\n\n\ndef hls_to_rgb(image: torch.Tensor) -> torch.Tensor:\n    r""""""Convert an HLS image to RGB\n    The image data is assumed to be in the range of (0, 1).\n\n    Args:\n        input (torch.Tensor): HLS Image to be converted to RGB.\n\n\n    Returns:\n        torch.Tensor: RGB version of the image.\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(image.shape))\n\n    h: torch.Tensor = image[..., 0, :, :] * 360 / (2 * pi.to(image.device))\n    l: torch.Tensor = image[..., 1, :, :]\n    s: torch.Tensor = image[..., 2, :, :]\n\n    kr = (0 + h / 30) % 12\n    kg = (8 + h / 30) % 12\n    kb = (4 + h / 30) % 12\n    a = s * torch.min(l, torch.tensor(1.) - l)\n\n    ones_k = torch.ones_like(kr)\n\n    fr: torch.Tensor = l - a * torch.max(torch.min(torch.min(kr - torch.tensor(3.),\n                                                             torch.tensor(9.) - kr), ones_k), -1 * ones_k)\n    fg: torch.Tensor = l - a * torch.max(torch.min(torch.min(kg - torch.tensor(3.),\n                                                             torch.tensor(9.) - kg), ones_k), -1 * ones_k)\n    fb: torch.Tensor = l - a * torch.max(torch.min(torch.min(kb - torch.tensor(3.),\n                                                             torch.tensor(9.) - kb), ones_k), -1 * ones_k)\n\n    out: torch.Tensor = torch.stack([fr, fg, fb], dim=-3)\n\n    return out\n\n\nclass RgbToHls(nn.Module):\n    r""""""Convert image from RGB to HLS\n    The image data is assumed to be in the range of (0, 1).\n\n    args:\n        image (torch.Tensor): RGB image to be converted to HLS.\n\n    returns:\n        torch.tensor: HLS version of the image.\n\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples::\n\n        >>> import torch\n        >>> import kornia\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> hls = kornia.color.RgbToHls()\n        >>> output = hls(input)  # 2x3x4x5\n\n    """"""\n\n    def __init__(self) -> None:\n        super(RgbToHls, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rgb_to_hls(image)\n\n\ndef rgb_to_hls(image: torch.Tensor) -> torch.Tensor:\n    r""""""Convert an RGB image to HLS\n    The image data is assumed to be in the range of (0, 1).\n\n    Args:\n        input (torch.Tensor): RGB Image to be converted to HLS.\n\n\n    Returns:\n        torch.Tensor: HLS version of the image.\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(image.shape))\n\n    r: torch.Tensor = image[..., 0, :, :]\n    g: torch.Tensor = image[..., 1, :, :]\n    b: torch.Tensor = image[..., 2, :, :]\n\n    maxc: torch.Tensor = image.max(-3)[0]\n    minc: torch.Tensor = image.min(-3)[0]\n\n    imax: torch.Tensor = image.max(-3)[1]\n\n    l: torch.Tensor = (maxc + minc) / 2  # luminance\n\n    deltac: torch.Tensor = maxc - minc\n\n    s: torch.Tensor = torch.where(l < 0.5, deltac / (maxc + minc), deltac /\n                                  (torch.tensor(2.) - (maxc + minc)))  # saturation\n\n    hi: torch.Tensor = torch.zeros_like(deltac)\n\n    hi[imax == 0] = (((g - b) / deltac) % 6)[imax == 0]\n    hi[imax == 1] = (((b - r) / deltac) + 2)[imax == 1]\n    hi[imax == 2] = (((r - g) / deltac) + 4)[imax == 2]\n\n    h: torch.Tensor = 2. * pi.to(image.device) * (60. * hi) / 360.  # hue [0, 2*pi]\n\n    image_hls: torch.Tensor = torch.stack([h, l, s], dim=-3)\n\n    image_hls[torch.isnan(image_hls)] = 0.\n\n    return image_hls\n'"
kornia/color/hsv.py,53,"b'import torch\nimport torch.nn as nn\nfrom kornia.constants import pi\n\n\nclass HsvToRgb(nn.Module):\n    r""""""Convert image from HSV to Rgb\n    The image data is assumed to be in the range of (0, 1).\n\n    args:\n        image (torch.Tensor): HSV image to be converted to RGB.\n\n    returns:\n        torch.tensor: RGB version of the image.\n\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples::\n\n        >>> import torch\n        >>> import kornia\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> rgb = kornia.color.HsvToRgb()\n        >>> output = rgb(input)  # 2x3x4x5\n\n    """"""\n\n    def __init__(self) -> None:\n        super(HsvToRgb, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return hsv_to_rgb(image)\n\n\ndef hsv_to_rgb(image: torch.Tensor) -> torch.Tensor:\n    r""""""Convert an HSV image to RGB\n    The image data is assumed to be in the range of (0, 1).\n\n    Args:\n        input (torch.Tensor): HSV Image to be converted to RGB.\n\n\n    Returns:\n        torch.Tensor: RGB version of the image.\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(image.shape))\n\n    h: torch.Tensor = image[..., 0, :, :] / (2 * pi.to(image.device))\n    s: torch.Tensor = image[..., 1, :, :]\n    v: torch.Tensor = image[..., 2, :, :]\n\n    hi: torch.Tensor = torch.floor(h * 6) % 6\n    f: torch.Tensor = ((h * 6) % 6) - hi\n    one: torch.Tensor = torch.tensor(1.).to(image.device)\n    p: torch.Tensor = v * (one - s)\n    q: torch.Tensor = v * (one - f * s)\n    t: torch.Tensor = v * (one - (one - f) * s)\n\n    out: torch.Tensor = torch.stack([hi, hi, hi], dim=-3)\n\n    out[out == 0] = torch.stack((v, t, p), dim=-3)[out == 0]\n    out[out == 1] = torch.stack((q, v, p), dim=-3)[out == 1]\n    out[out == 2] = torch.stack((p, v, t), dim=-3)[out == 2]\n    out[out == 3] = torch.stack((p, q, v), dim=-3)[out == 3]\n    out[out == 4] = torch.stack((t, p, v), dim=-3)[out == 4]\n    out[out == 5] = torch.stack((v, p, q), dim=-3)[out == 5]\n\n    return out\n\n\nclass RgbToHsv(nn.Module):\n    r""""""Convert image from RGB to HSV.\n\n    The image data is assumed to be in the range of (0, 1).\n\n    args:\n        image (torch.Tensor): RGB image to be converted to HSV.\n\n    returns:\n        torch.tensor: HSV version of the image.\n\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples::\n\n        >>> import torch\n        >>> import kornia\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> hsv = kornia.color.RgbToHsv()\n        >>> output = hsv(input)  # 2x3x4x5\n\n    """"""\n\n    def __init__(self) -> None:\n        super(RgbToHsv, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rgb_to_hsv(image)\n\n\ndef rgb_to_hsv(image: torch.Tensor) -> torch.Tensor:\n    r""""""Convert an RGB image to HSV.\n\n    Args:\n        input (torch.Tensor): RGB Image to be converted to HSV.\n\n    Returns:\n        torch.Tensor: HSV version of the image.\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(image.shape))\n\n    r: torch.Tensor = image[..., 0, :, :]\n    g: torch.Tensor = image[..., 1, :, :]\n    b: torch.Tensor = image[..., 2, :, :]\n\n    maxc: torch.Tensor = image.max(-3)[0]\n    minc: torch.Tensor = image.min(-3)[0]\n\n    v: torch.Tensor = maxc  # brightness\n\n    deltac: torch.Tensor = maxc - minc\n    s: torch.Tensor = deltac / v\n\n    s[torch.isnan(s)] = 0.\n\n    # avoid division by zero\n    deltac = torch.where(\n        deltac == 0, torch.ones_like(deltac), deltac)\n\n    rc: torch.Tensor = (maxc - r) / deltac\n    gc: torch.Tensor = (maxc - g) / deltac\n    bc: torch.Tensor = (maxc - b) / deltac\n\n    maxg: torch.Tensor = g == maxc\n    maxr: torch.Tensor = r == maxc\n\n    h: torch.Tensor = 4.0 + gc - rc\n    h[maxg] = 2.0 + rc[maxg] - bc[maxg]\n    h[maxr] = bc[maxr] - gc[maxr]\n    h[minc == maxc] = 0.0\n\n    h = (h / 6.0) % 1.0\n\n    h = 2 * pi.to(image.device) * h\n    return torch.stack([h, s, v], dim=-3)\n'"
kornia/color/luv.py,56,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom .xyz import rgb_to_xyz, xyz_to_rgb\n\n""""""\nThe RGB to Luv color transformations were translated from scikit image\'s rgb2luv and luv2rgb\n\nhttps://github.com/scikit-image/scikit-image/blob/a48bf6774718c64dade4548153ae16065b595ca9/skimage/color/colorconv.py\n\n""""""\n\n\nclass RgbToLuv(nn.Module):\n    r""""""Converts an image from RGB to Luv\n\n    The image data is assumed to be in the range of :math:`[0, 1]`. Luv\n    color is computed using the D65 illuminant and Observer 2.\n\n    args:\n        image (torch.Tensor): RGB image to be converted to Luv.\n\n    returns:\n        torch.Tensor: Luv version of the image.\n\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples:\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> luv = kornia.color.RgbToLuv()\n        >>> output = luv(input)  # 2x3x4x5\n\n    Reference:\n        [1] https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html\n\n        [2] https://www.easyrgb.com/en/math.php\n\n        [3] http://www.poynton.com/ColorFAQ.html\n    """"""\n\n    def __init__(self) -> None:\n\n        super(RgbToLuv, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n\n        return rgb_to_luv(image)\n\n\nclass LuvToRgb(nn.Module):\n    r""""""Converts an image from Luv to RGB\n\n    args:\n        image (torch.Tensor): Luv image to be converted to RGB.\n\n    returns:\n        torch.Tensor: RGB version of the image.\n\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples:\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> rgb = kornia.color.LuvToRgb()\n        >>> output = rgb(input)  # 2x3x4x5\n\n    References:\n        [1] https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html\n\n        [2] https://www.easyrgb.com/en/math.php\n\n        [3] http://www.poynton.com/ColorFAQ.html\n    """"""\n\n    def __init__(self) -> None:\n\n        super(LuvToRgb, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n\n        return luv_to_rgb(image)\n\n\ndef rgb_to_luv(image: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n    r""""""Converts a RGB image to Luv.\n\n    See :class:`~kornia.color.RgbToLuv` for details.\n\n    Args:\n        image (torch.Tensor): RGB image\n        eps (float): for numerically stability when dividing. Default: 1e-8.\n\n    Returns:\n        torch.Tensor : Luv image\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(image.shape))\n\n    # Convert from Linear RGB to sRGB\n    r: torch.Tensor = image[..., 0, :, :]\n    g: torch.Tensor = image[..., 1, :, :]\n    b: torch.Tensor = image[..., 2, :, :]\n\n    rs: torch.Tensor = torch.where(r > 0.04045, torch.pow(((r + 0.055) / 1.055), 2.4), r / 12.92)\n    gs: torch.Tensor = torch.where(g > 0.04045, torch.pow(((g + 0.055) / 1.055), 2.4), g / 12.92)\n    bs: torch.Tensor = torch.where(b > 0.04045, torch.pow(((b + 0.055) / 1.055), 2.4), b / 12.92)\n\n    image_s = torch.stack((rs, gs, bs), dim=-3)\n\n    xyz_im: torch.Tensor = rgb_to_xyz(image_s)\n\n    x: torch.Tensor = xyz_im[..., 0, :, :]\n    y: torch.Tensor = xyz_im[..., 1, :, :]\n    z: torch.Tensor = xyz_im[..., 2, :, :]\n\n    L: torch.Tensor = torch.where(torch.gt(y, 0.008856),\n                                  116. * torch.pow(y, 1. / 3.) - 16.,\n                                  903.3 * y)\n\n    # Compute reference white point\n    xyz_ref_white: Tuple[float, float, float] = (.95047, 1., 1.08883)\n    u_w: float = (4 * xyz_ref_white[0]) / (xyz_ref_white[0] + 15 * xyz_ref_white[1] + 3 * xyz_ref_white[2])\n    v_w: float = (9 * xyz_ref_white[1]) / (xyz_ref_white[0] + 15 * xyz_ref_white[1] + 3 * xyz_ref_white[2])\n\n    u_p: torch.Tensor = (4 * x) / (x + 15 * y + 3 * z + eps)\n    v_p: torch.Tensor = (9 * y) / (x + 15 * y + 3 * z + eps)\n\n    u: torch.Tensor = 13 * L * (u_p - u_w)\n    v: torch.Tensor = 13 * L * (v_p - v_w)\n\n    out = torch.stack((L, u, v), dim=-3)\n\n    return out\n\n\ndef luv_to_rgb(image: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n    r""""""Converts a Luv image to RGB.\n\n    See :class:`~kornia.color.LuvToRgb` for details.\n\n    Args:\n        image (torch.Tensor): Luv image\n        eps (float): for numerically stability when dividing. Default: 1e-8.\n\n    Returns:\n        torch.Tensor : RGB image\n    """"""\n    if not torch.is_tensor(image):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(image.shape))\n\n    L: torch.Tensor = image[..., 0, :, :]\n    u: torch.Tensor = image[..., 1, :, :]\n    v: torch.Tensor = image[..., 2, :, :]\n    # Convert from Luv to XYZ\n    y: torch.Tensor = torch.where(L > 7.999625,\n                                  torch.pow((L + 16) / 116, 3.0),\n                                  L / 903.3)\n    # Compute white point\n    xyz_ref_white: Tuple[float, float, float] = (0.95047, 1., 1.08883)\n    u_w: float = (4 * xyz_ref_white[0]) / (xyz_ref_white[0] + 15 * xyz_ref_white[1] + 3 * xyz_ref_white[2])\n    v_w: float = (9 * xyz_ref_white[1]) / (xyz_ref_white[0] + 15 * xyz_ref_white[1] + 3 * xyz_ref_white[2])\n\n    a: torch.Tensor = u_w + u / (13 * L + eps)\n    d: torch.Tensor = v_w + v / (13 * L + eps)\n    c: torch.Tensor = 3 * y * (5 * d - 3)\n\n    z: torch.Tensor = ((a - 4) * c - 15 * a * d * y) / (12 * d + eps)\n    x: torch.Tensor = -(c / (d + eps) + 3. * z)\n\n    xyz_im: torch.Tensor = torch.stack((x, y, z), -3)\n\n    rgbs_im: torch.Tensor = xyz_to_rgb(xyz_im)\n    # Convert from sRGB to RGB Linear\n    rs: torch.Tensor = rgbs_im[..., 0, :, :]\n    gs: torch.Tensor = rgbs_im[..., 1, :, :]\n    bs: torch.Tensor = rgbs_im[..., 2, :, :]\n\n    r: torch.Tensor = torch.where(rs > 0.0031308, 1.055 * torch.pow(rs, 1 / 2.4) - 0.055, 12.92 * rs)\n    g: torch.Tensor = torch.where(gs > 0.0031308, 1.055 * torch.pow(gs, 1 / 2.4) - 0.055, 12.92 * gs)\n    b: torch.Tensor = torch.where(bs > 0.0031308, 1.055 * torch.pow(bs, 1 / 2.4) - 0.055, 12.92 * bs)\n\n    rgb_im: torch.Tensor = torch.stack((r, g, b), dim=-3)\n\n    return rgb_im\n'"
kornia/color/normalize.py,35,"b'""""""Module containing functionals for intensity normalisation.""""""\n\nfrom typing import Union\n\nimport torch\nimport torch.nn as nn\n\n\nclass Normalize(nn.Module):\n    r""""""Normalize a tensor image or a batch of tensor images with mean and standard deviation.\n\n    Input must be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.\n\n    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels,\n    this transform will normalize each channel of the input ``torch.Tensor``\n    i.e. ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n\n    Args:\n        mean (torch.Tensor or float): Mean for each channel.\n        std (torch.Tensor or float): Standard deviations for each channel.\n\n    """"""\n\n    def __init__(self, mean: Union[torch.Tensor, float], std: Union[torch.Tensor, float]) -> None:\n\n        super(Normalize, self).__init__()\n\n        self.mean = mean\n        self.std = std\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        """"""Normalises an input tensor by the mean and standard deviation.\n\n        Args:\n            input: image tensor of size (*, H, W).\n\n        Returns:\n            normalised tensor with same size as input (*, H, W).\n\n        """"""\n        return normalize(input, self.mean, self.std)\n\n    def __repr__(self):\n        repr = ""(mean={0}, std={1})"".format(self.mean, self.std)\n        return self.__class__.__name__ + repr\n\n\ndef normalize(\n    data: torch.Tensor, mean: Union[torch.Tensor, float], std: Union[torch.Tensor, float]\n) -> torch.Tensor:\n    r""""""Normalise the image with channel-wise mean and standard deviation.\n\n    See :class:`~kornia.color.Normalize` for details.\n\n    Args:\n        data (torch.Tensor): The image tensor to be normalised.\n        mean (torch.Tensor or float): Mean for each channel.\n        std (torch.Tensor or float): Standard deviations for each channel.\n\n    Returns:\n        torch.Tensor: The normalised image tensor.\n\n    """"""\n    if isinstance(mean, float):\n        mean = torch.tensor([mean])  # prevent 0 sized tensors\n\n    if isinstance(std, float):\n        std = torch.tensor([std])  # prevent 0 sized tensors\n\n    if not torch.is_tensor(data):\n        raise TypeError(""data should be a tensor. Got {}"".format(type(data)))\n\n    if not torch.is_tensor(mean):\n        raise TypeError(""mean should be a tensor or a float. Got {}"".format(type(mean)))\n\n    if not torch.is_tensor(std):\n        raise TypeError(""std should be a tensor or float. Got {}"".format(type(std)))\n\n    # Allow broadcast on channel dimension\n    if mean.shape and mean.shape[0] != 1:\n        if mean.shape[0] != data.shape[-3] and mean.shape[:2] != data.shape[:2]:\n            raise ValueError(""mean length and number of channels do not match"")\n\n    # Allow broadcast on channel dimension\n    if std.shape and std.shape[0] != 1:\n        if std.shape[0] != data.shape[-3] and std.shape[:2] != data.shape[:2]:\n            raise ValueError(""std length and number of channels do not match"")\n\n    if mean.shape:\n        mean = mean[..., :, None, None].to(data.device)\n    if std.shape:\n        std = std[..., :, None, None].to(data.device)\n\n    out: torch.Tensor = (data - mean) / std\n\n    return out\n\n\nclass Denormalize(nn.Module):\n    r""""""Denormalize a tensor image or a batch of tensor images.\n\n    Input must be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.\n\n    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels,\n    this transform will denormalize each channel of the input ``torch.Tensor``\n    i.e. ``input[channel] = (input[channel] * std[channel]) + mean[channel]``\n\n    Args:\n        mean (torch.Tensor or float): Mean for each channel.\n        std (torch.Tensor or float): Standard deviations for each channel.\n\n    """"""\n\n    def __init__(self, mean: Union[torch.Tensor, float], std: Union[torch.Tensor, float]) -> None:\n\n        super(Denormalize, self).__init__()\n\n        self.mean = mean\n        self.std = std\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        """"""Denormalises an input tensor by the mean and standard deviation.\n\n        Args:\n            input: image tensor of size (*, H, W).\n\n        Returns:\n            normalised tensor with same size as input (*, H, W).\n\n        """"""\n        return denormalize(input, self.mean, self.std)\n\n    def __repr__(self):\n        repr = ""(mean={0}, std={1})"".format(self.mean, self.std)\n        return self.__class__.__name__ + repr\n\n\ndef denormalize(\n    data: torch.Tensor, mean: Union[torch.Tensor, float], std: Union[torch.Tensor, float]\n) -> torch.Tensor:\n    r""""""Denormalize the image given channel-wise mean and standard deviation.\n\n    See :class:`~kornia.color.Normalize` for details.\n\n    Args:\n        data (torch.Tensor): The image tensor to be normalised.\n        mean (torch.Tensor or float): Mean for each channel.\n        std (torch.Tensor or float): Standard deviations for each channel.\n\n    Returns:\n        torch.Tensor: The normalised image tensor.\n\n    """"""\n    if isinstance(mean, float):\n        mean = torch.tensor([mean])  # prevent 0 sized tensors\n\n    if isinstance(std, float):\n        std = torch.tensor([std])  # prevent 0 sized tensors\n\n    if not torch.is_tensor(data):\n        raise TypeError(""data should be a tensor. Got {}"".format(type(data)))\n\n    if not torch.is_tensor(mean):\n        raise TypeError(""mean should be a tensor or a float. Got {}"".format(type(mean)))\n\n    if not torch.is_tensor(std):\n        raise TypeError(""std should be a tensor or float. Got {}"".format(type(std)))\n\n    # Allow broadcast on channel dimension\n    if mean.shape and mean.shape[0] != 1:\n        if mean.shape[0] != data.shape[-3] and mean.shape[:2] != data.shape[:2]:\n            raise ValueError(""mean length and number of channels do not match"")\n\n    # Allow broadcast on channel dimension\n    if std.shape and std.shape[0] != 1:\n        if std.shape[0] != data.shape[-3] and std.shape[:2] != data.shape[:2]:\n            raise ValueError(""std length and number of channels do not match"")\n\n    if mean.shape:\n        mean = mean[..., :, None, None].to(data.device)\n    if std.shape:\n        std = std[..., :, None, None].to(data.device)\n\n    out: torch.Tensor = (data * std) + mean\n\n    return out\n'"
kornia/color/rgb.py,70,"b'from typing import cast\n\nimport torch\nimport torch.nn as nn\nfrom typing import Union\n\n\ndef rgb_to_bgr(image: torch.Tensor) -> torch.Tensor:\n    r""""""Convert a RGB image to BGR.\n\n    See :class:`~kornia.color.RgbToBgr` for details.\n\n    Args:\n        image (torch.Tensor): RGB Image to be converted to BGR.\n\n    Returns:\n        torch.Tensor: BGR version of the image.\n    """"""\n\n    return bgr_to_rgb(image)\n\n\ndef bgr_to_rgb(image: torch.Tensor) -> torch.Tensor:\n    r""""""Convert a BGR image to RGB.\n\n    See :class:`~kornia.color.BgrToRgb` for details.\n\n    Args:\n        image (torch.Tensor): BGR Image to be converted to RGB.\n\n    Returns:\n        torch.Tensor: RGB version of the image.\n    """"""\n    if not torch.is_tensor(image):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W).Got {}""\n                         .format(image.shape))\n\n    # flip image channels\n    out: torch.Tensor = image.flip(-3)\n\n    return out\n\n\ndef rgb_to_rgba(image: torch.Tensor, alpha_val: Union[float, torch.Tensor]) -> torch.Tensor:\n    r""""""Convert image from RGB to RGBA.\n\n    See :class:`~kornia.color.RgbToRgba` for details.\n\n    Args:\n        image (torch.Tensor): RGB Image to be converted to RGBA.\n        alpha_val (float, torch.Tensor): A float number for the alpha value.\n\n    Returns:\n        torch.Tensor: RGBA version of the image.\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(image)}"")\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(f""Input size must have a shape of (*, 3, H, W).Got {image.shape}"")\n\n    if not isinstance(alpha_val, (float, torch.Tensor)):\n        raise TypeError(f""alpha_val type is not a float or torch.Tensor. Got {type(alpha_val)}"")\n\n    # add one channel\n    r, g, b = torch.chunk(image, image.shape[-3], dim=-3)\n\n    a: torch.Tensor = cast(torch.Tensor, alpha_val)\n\n    if isinstance(alpha_val, float):\n        a = torch.full_like(r, fill_value=float(alpha_val))\n\n    return torch.cat([r, g, b, a], dim=-3)\n\n\ndef bgr_to_rgba(image: torch.Tensor, alpha_val: Union[float, torch.Tensor]) -> torch.Tensor:\n    r""""""Convert image from BGR to RGBA.\n\n    See :class:`~kornia.color.BgrToRgba` for details.\n\n    Args:\n        image (torch.Tensor): BGR Image to be converted to RGBA.\n        alpha_val (float, torch.Tensor): A float number for the alpha value.\n\n    Returns:\n        torch.Tensor: RGBA version of the image.\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(image)}"")\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(f""Input size must have a shape of (*, 3, H, W).Got {image.shape}"")\n\n    if not isinstance(alpha_val, (float, torch.Tensor)):\n        raise TypeError(f""alpha_val type is not a float or torch.Tensor. Got {type(alpha_val)}"")\n    # convert first to RGB, then add alpha channel\n    x_rgb: torch.Tensor = bgr_to_rgb(image)\n    return rgb_to_rgba(x_rgb, alpha_val)\n\n\ndef rgba_to_rgb(image: torch.Tensor) -> torch.Tensor:\n    r""""""Convert image from RGBA to RGB.\n\n    See :class:`~kornia.color.RgbaToRgb` for details.\n\n    Args:\n        image (torch.Tensor): RGBA Image to be converted to RGB.\n\n    Returns:\n        torch.Tensor: RGB version of the image.\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(image)}"")\n\n    if len(image.shape) < 3 or image.shape[-3] != 4:\n        raise ValueError(f""Input size must have a shape of (*, 3, H, W).Got {image.shape}"")\n\n    # unpack channels\n    r, g, b, a = torch.chunk(image, image.shape[-3], dim=-3)\n\n    # compute new channels\n    a_one = torch.tensor(1.) - a\n    r_new: torch.Tensor = a_one * r + a * r\n    g_new: torch.Tensor = a_one * g + a * g\n    b_new: torch.Tensor = a_one * b + a * b\n\n    return torch.cat([r, g, b], dim=-3)\n\n\ndef rgba_to_bgr(image: torch.Tensor) -> torch.Tensor:\n    r""""""Convert image from RGBA to BGR.\n\n    See :class:`~kornia.color.RgbaToBgr` for details.\n\n    Args:\n        image (torch.Tensor): RGBA Image to be converted to BGR.\n\n    Returns:\n        torch.Tensor: BGR version of the image.\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(image)}"")\n\n    if len(image.shape) < 3 or image.shape[-3] != 4:\n        raise ValueError(f""Input size must have a shape of (*, 3, H, W).Got {image.shape}"")\n\n    # convert to RGB first, then to BGR\n    x_rgb: torch.Tensor = rgba_to_rgb(image)\n    return rgb_to_bgr(x_rgb)\n\n\nclass BgrToRgb(nn.Module):\n    r""""""Convert image from BGR to RGB.\n\n    The image data is assumed to be in the range of (0, 1).\n\n    Returns:\n        torch.Tensor: RGB version of the image.\n\n    Shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples::\n\n        >>> import torch\n        >>> import kornia\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> rgb = kornia.color.BgrToRgb()\n        >>> output = rgb(input)  # 2x3x4x5\n\n    """"""\n\n    def __init__(self) -> None:\n        super(BgrToRgb, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return bgr_to_rgb(image)\n\n\nclass RgbToBgr(nn.Module):\n    r""""""Convert image from RGB to BGR.\n\n    The image data is assumed to be in the range of (0, 1).\n\n    Returns:\n        torch.Tensor: BGR version of the image.\n\n    Shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples::\n\n        >>> import torch\n        >>> import kornia\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> bgr = kornia.color.RgbToBgr()\n        >>> output = bgr(input)  # 2x3x4x5\n\n    """"""\n\n    def __init__(self) -> None:\n        super(RgbToBgr, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rgb_to_bgr(image)\n\n\nclass RgbToRgba(nn.Module):\n    r""""""Convert image from RGB to RGBA.\n\n    Add an alpha channel to existing RGB image.\n\n    Args:\n        alpha_val (float, torch.Tensor): A float number for the alpha value.\n\n    Returns:\n        torch.Tensor: RGBA version of the image.\n\n    Shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 4, H, W)`\n\n    Examples::\n\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> rgba = kornia.color.RgbToRgba(1.)\n        >>> output = rgba(input)  # 2x4x4x5\n    """"""\n\n    def __init__(self, alpha_val: Union[float, torch.Tensor]) -> None:\n        super(RgbToRgba, self).__init__()\n        self.alpha_val = alpha_val\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rgb_to_rgba(image, self.alpha_val)\n\n\nclass BgrToRgba(nn.Module):\n    r""""""Convert image from BGR to RGBA.\n\n    Add an alpha channel to existing BGR image.\n\n    Args:\n        alpha_val (float, torch.Tensor): A float number for the alpha value.\n\n    Returns:\n        torch.Tensor: RGBA version of the image.\n\n    Shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 4, H, W)`\n\n    Examples::\n\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> rgba = kornia.color.BgrToRgba(1.)\n        >>> output = rgba(input)  # 2x4x4x5\n    """"""\n\n    def __init__(self, alpha_val: Union[float, torch.Tensor]) -> None:\n        super(BgrToRgba, self).__init__()\n        self.alpha_val = alpha_val\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rgb_to_rgba(image, self.alpha_val)\n\n\nclass RgbaToRgb(nn.Module):\n    r""""""Convert image from RGBA to RGB.\n\n    Remove an alpha channel from RGB image.\n\n    returns:\n        torch.Tensor: RGB version of the image.\n\n    shape:\n        - image: :math:`(*, 4, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples::\n\n        >>> input = torch.rand(2, 4, 4, 5)\n        >>> rgba = kornia.color.RgbaToRgb()\n        >>> output = rgba(input)  # 2x3x4x5\n    """"""\n\n    def __init__(self) -> None:\n        super(RgbaToRgb, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rgba_to_rgb(image)\n\n\nclass RgbaToBgr(nn.Module):\n    r""""""Convert image from RGBA to BGR.\n\n    Remove an alpha channel from BGR image.\n\n    returns:\n        torch.Tensor: BGR version of the image.\n\n    shape:\n        - image: :math:`(*, 4, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples::\n\n        >>> input = torch.rand(2, 4, 4, 5)\n        >>> rgba = kornia.color.RgbaToBgr()\n        >>> output = rgba(input)  # 2x3x4x5\n    """"""\n\n    def __init__(self) -> None:\n        super(RgbaToBgr, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rgba_to_bgr(image)\n'"
kornia/color/xyz.py,33,"b'from typing import Union\n\nimport torch\nimport torch.nn as nn\n\n\nclass RgbToXyz(nn.Module):\n    r""""""Converts an image from RGB to XYZ\n\n    The image data is assumed to be in the range of (0, 1).\n\n    args:\n        image (torch.Tensor): RGB image to be converted to XYZ.\n\n    returns:\n        torch.Tensor: XYZ version of the image.\n\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples:\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> xyz = kornia.color.RgbToXyz()\n        >>> output = xyz(input)  # 2x3x4x5\n\n    Reference:\n        [1] https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html\n    """"""\n\n    def __init__(self) -> None:\n        super(RgbToXyz, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rgb_to_xyz(image)\n\n\nclass XyzToRgb(nn.Module):\n    r""""""Converts an image from XYZ to RGB\n\n    args:\n        image (torch.Tensor): XYZ image to be converted to RGB.\n\n    returns:\n        torch.Tensor: RGB version of the image.\n\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples:\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> rgb = kornia.color.XyzToRgb()\n        >>> output = rgb(input)  # 2x3x4x5\n\n    Reference:\n        [1] https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html\n    """"""\n\n    def __init__(self) -> None:\n        super(XyzToRgb, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return xyz_to_rgb(image)\n\n\ndef rgb_to_xyz(image: torch.Tensor) -> torch.Tensor:\n    r""""""Converts a RGB image to XYZ.\n\n    See :class:`~kornia.color.RgbToXyz` for details.\n\n    Args:\n        image (torch.Tensor): RGB Image to be converted to XYZ.\n\n    Returns:\n        torch.Tensor: XYZ version of the image.\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(image.shape))\n\n    r: torch.Tensor = image[..., 0, :, :]\n    g: torch.Tensor = image[..., 1, :, :]\n    b: torch.Tensor = image[..., 2, :, :]\n\n    x: torch.Tensor = 0.412453 * r + 0.357580 * g + 0.180423 * b\n    y: torch.Tensor = 0.212671 * r + 0.715160 * g + 0.072169 * b\n    z: torch.Tensor = 0.019334 * r + 0.119193 * g + 0.950227 * b\n\n    out: torch.Tensor = torch.stack((x, y, z), -3)\n\n    return out\n\n\ndef xyz_to_rgb(image: torch.Tensor) -> torch.Tensor:\n    r""""""Converts a XYZ image to RGB.\n\n    See :class:`~kornia.color.XyzToRgb` for details.\n\n    Args:\n        image (torch.Tensor): XYZ Image to be converted to RGB.\n\n    Returns:\n        torch.Tensor: RGB version of the image.\n    """"""\n    if not torch.is_tensor(image):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(image.shape))\n\n    x: torch.Tensor = image[..., 0, :, :]\n    y: torch.Tensor = image[..., 1, :, :]\n    z: torch.Tensor = image[..., 2, :, :]\n\n    r: torch.Tensor = 3.2404813432005266 * x + -1.5371515162713185 * y + -0.4985363261688878 * z\n    g: torch.Tensor = -0.9692549499965682 * x + 1.8759900014898907 * y + 0.0415559265582928 * z\n    b: torch.Tensor = 0.0556466391351772 * x + -0.2040413383665112 * y + 1.0573110696453443 * z\n\n    out: torch.Tensor = torch.stack((r, g, b), dim=-3)\n\n    return out\n'"
kornia/color/ycbcr.py,35,"b'import torch\nimport torch.nn as nn\n\n\nclass YcbcrToRgb(nn.Module):\n    r""""""Convert image from YCbCr to Rgb\n    The image data is assumed to be in the range of (0, 1).\n\n    args:\n        image (torch.Tensor): YCbCr image to be converted to RGB.\n\n    returns:\n        torch.tensor: RGB version of the image.\n\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples::\n\n        >>> import torch\n        >>> import kornia\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> rgb = kornia.color.YcbcrToRgb()\n        >>> output = rgb(input)  # 2x3x4x5\n\n    """"""\n\n    def __init__(self) -> None:\n        super(YcbcrToRgb, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return ycbcr_to_rgb(image)\n\n\ndef ycbcr_to_rgb(image: torch.Tensor) -> torch.Tensor:\n    r""""""Convert an YCbCr image to RGB\n    The image data is assumed to be in the range of (0, 1).\n\n    Args:\n        image (torch.Tensor): YCbCr Image to be converted to RGB.\n\n\n    Returns:\n        torch.Tensor: RGB version of the image.\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(image.shape))\n\n    y: torch.Tensor = image[..., 0, :, :]\n    cb: torch.Tensor = image[..., 1, :, :]\n    cr: torch.Tensor = image[..., 2, :, :]\n\n    delta: float = .5\n    cb_shifted: torch.Tensor = cb - delta\n    cr_shifted: torch.Tensor = cr - delta\n\n    r: torch.Tensor = y + 1.403 * cr_shifted\n    g: torch.Tensor = y - .714 * cr_shifted - .344 * cb_shifted\n    b: torch.Tensor = y + 1.773 * cb_shifted\n    return torch.stack((r, g, b), -3)\n\n\nclass RgbToYcbcr(nn.Module):\n    r""""""Convert image from RGB to YCbCr\n    The image data is assumed to be in the range of (0, 1).\n\n    args:\n        image (torch.Tensor): RGB image to be converted to YCbCr.\n\n    returns:\n        torch.tensor: YCbCr version of the image.\n\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n\n    Examples::\n\n        >>> import torch\n        >>> import kornia\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> ycbcr = kornia.color.RgbToYcbcr()\n        >>> output = ycbcr(input)  # 2x3x4x5\n\n    """"""\n\n    def __init__(self) -> None:\n        super(RgbToYcbcr, self).__init__()\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rgb_to_ycbcr(image)\n\n\ndef rgb_to_ycbcr(image: torch.Tensor) -> torch.Tensor:\n    r""""""Convert an RGB image to YCbCr.\n\n    Args:\n        image (torch.Tensor): RGB Image to be converted to YCbCr.\n\n    Returns:\n        torch.Tensor: YCbCr version of the image.\n    """"""\n\n    if not torch.is_tensor(image):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) < 3 or image.shape[-3] != 3:\n        raise ValueError(""Input size must have a shape of (*, 3, H, W). Got {}""\n                         .format(image.shape))\n\n    r: torch.Tensor = image[..., 0, :, :]\n    g: torch.Tensor = image[..., 1, :, :]\n    b: torch.Tensor = image[..., 2, :, :]\n\n    delta = .5\n    y: torch.Tensor = .299 * r + .587 * g + .114 * b\n    cb: torch.Tensor = (b - y) * .564 + delta\n    cr: torch.Tensor = (r - y) * .713 + delta\n    return torch.stack((y, cb, cr), -3)\n'"
kornia/color/yuv.py,30,"b'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Tuple\n\n\nclass RgbToYuv(nn.Module):\n    r""""""Convert image from RGB to YUV\n    The image data is assumed to be in the range of (0, 1).\n\n    args:\n        image (torch.Tensor): RGB image to be converted to YUV.\n    returns:\n        torch.tensor: YUV version of the image.\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n    Examples::\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> yuv = kornia.color.RgbToYuv()\n        >>> output = yuv(input)  # 2x3x4x5\n    Reference::\n        [1] https://es.wikipedia.org/wiki/YUV#RGB_a_Y\'UV\n    """"""\n\n    def __init__(self) -> None:\n        super(RgbToYuv, self).__init__()\n\n    def forward(  # type: ignore\n            self, input: torch.Tensor) -> torch.Tensor:\n        return rgb_to_yuv(input)\n\n\ndef rgb_to_yuv(input: torch.Tensor) -> torch.Tensor:\n    r""""""Convert an RGB image to YUV\n    The image data is assumed to be in the range of (0, 1).\n\n    Args:\n        input (torch.Tensor): RGB Image to be converted to YUV.\n    Returns:\n        torch.Tensor: YUV version of the image.\n    See :class:`~kornia.color.RgbToYuv` for details.""""""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    if not(len(input.shape) == 3 or len(input.shape) == 4):\n        raise ValueError(f""Input size must have a shape of (*, 3, H, W) or (3, H, W). Got {input.shape}"")\n\n    if input.shape[-3] != 3:\n        raise ValueError(f""Expected input to have 3 channels, got {input.shape[-3]}"")\n\n    r, g, b = torch.chunk(input, chunks=3, dim=-3)\n    y: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b\n    u: torch.Tensor = -0.147 * r - 0.289 * g + 0.436 * b\n    v: torch.Tensor = 0.615 * r - 0.515 * g - 0.100 * b\n    yuv_img: torch.Tensor = torch.cat((y, u, v), -3)\n    return yuv_img\n\n\nclass YuvToRgb(nn.Module):\n    r""""""Convert image from YUV to RGB\n    The image data is assumed to be in the range of (0, 1).\n\n    args:\n        image (torch.Tensor): YUV image to be converted to RGB.\n    returns:\n        torch.tensor: RGB version of the image.\n    shape:\n        - image: :math:`(*, 3, H, W)`\n        - output: :math:`(*, 3, H, W)`\n    Examples::\n        >>> input = torch.rand(2, 3, 4, 5)\n        >>> rgb = kornia.color.YuvToRgb()\n        >>> output = rgb(input)  # 2x3x4x5\n    """"""\n\n    def __init__(self) -> None:\n        super(YuvToRgb, self).__init__()\n\n    def forward(  # type: ignore\n            self, input: torch.Tensor) -> torch.Tensor:\n        return yuv_to_rgb(input)\n\n\ndef yuv_to_rgb(input: torch.Tensor) -> torch.Tensor:\n    r""""""Convert an YUV image to RGB\n    The image data is assumed to be in the range of (0, 1).\n\n    Args:\n        input (torch.Tensor): YUV Image to be converted to RGB.\n    Returns:\n        torch.Tensor: RGB version of the image.\n    See :class:`~kornia.color.YuvToRgb` for details.""""""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    if not(len(input.shape) == 3 or len(input.shape) == 4):\n        raise ValueError(f""Input size must have a shape of (*, 3, H, W) or (3, H, W). Got {input.shape}"")\n\n    if input.shape[-3] != 3:\n        raise ValueError(f""Expected input to have 3 channels, got {input.shape[-3]}"")\n\n    y, u, v = torch.chunk(input, chunks=3, dim=-3)\n    r: torch.Tensor = y + 1.14 * v  # coefficient for g is 0\n    g: torch.Tensor = y + -0.396 * u - 0.581 * v\n    b: torch.Tensor = y + 2.029 * u  # coefficient for b is 0\n    rgb_img: torch.Tensor = torch.cat((r, g, b), -3)\n    return rgb_img\n'"
kornia/color/zca.py,59,"b'from typing import Tuple, Optional, Union, List\nfrom functools import reduce\n\nimport torch\nimport torch.nn as nn\n\n\nclass ZCAWhitening(nn.Module):\n    r""""""\n\n    Computes the ZCA whitening matrix transform and the mean vector and applies the transform\n    to the data. The data tensor is flattened, and the mean :math:`\\mathbf{\\mu}`\n    and covariance matrix :math:`\\mathbf{\\Sigma}` are computed from\n    the flattened data :math:`\\mathbf{X} \\in \\mathbb{R}^{N \\times D}`, where\n    :math:`N` is the sample size and :math:`D` is flattened dimensionality\n    (e.g. for a tensor with size 5x3x2x2 :math:`N = 5` and :math:`D = 12`). The ZCA whitening\n    transform is given by:\n\n    .. math::\n\n        \\mathbf{X}_{\\text{zca}} = (\\mathbf{X - \\mu})(US^{-\\frac{1}{2}}U^T)^T\n\n    where :math:`U` are the eigenvectors of :math:`\\Sigma` and :math:`S` contain the correpsonding\n    eigenvalues of :math:`\\Sigma`. After the transform is applied, the output is reshaped to same shape.\n\n    args:\n\n        dim (int): Determines the dimension that represents the samples axis. Default = 0\n        eps (float) : a small number used for numerial stablility. Default=1e-6\n        unbiased (bool): Whether to use the biased estimate of the covariance matrix. Default=False\n        compute_inv (bool): Compute the inverse transform matrix. Default=False\n        detach_transforms (bool): Detaches gradient from the ZCA fitting. Default=True\n\n    shape:\n        - x: :math:`(D_0,...,D_{\\text{dim}},...,D_N)` is a batch of N-D tensors.\n        - x_whiten: :math:`(D_0,...,D_{\\text{dim}},...,D_N)` same shape as input.\n\n\n    Examples:\n        >>> x = torch.tensor([[0,1],[1,0],[-1,0],[0,-1]], dtype = torch.float32)\n        >>> zca = kornia.color.ZCAWhitening().fit(x)\n        >>> x_whiten = zca(x)\n        >>> zca = kornia.color.ZCAWhitening()\n        >>> x_whiten = zca(x, include_fit = True) # Includes the fitting step\n        >>> x_whiten = zca(x) # Can run now without the fitting set\n        >>> # Enable backprop through ZCA fitting process\n        >>> zca = kornia.color.ZCAWhitening(detach_transforms = False)\n        >>> x_whiten = zca(x, include_fit = True) # Includes the fitting step\n\n    Note:\n\n        This implementation uses :py:meth:`~torch.svd` which yields NaNs in the backwards step\n        if the sigular values are not unique. See `here <https://pytorch.org/docs/stable/torch.html#torch.svd>`_ for\n        more information.\n\n    References:\n\n        [1] `Stanford PCA & ZCA whitening tutorial <http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/>`_\n    """"""\n\n    def __init__(self, dim: int = 0, eps: float = 1e-6,\n                 unbiased: bool = True, detach_transforms: bool = True,\n                 compute_inv: bool = False) -> None:\n\n        super(ZCAWhitening, self).__init__()\n\n        self.dim = dim\n        self.eps = eps\n        self.unbiased = unbiased\n        self.detach_transforms = detach_transforms\n        self.compute_inv = compute_inv\n\n        self.fitted = False\n\n    def fit(self, x: torch.Tensor):\n        r""""""\n\n        Fits ZCA whitening matrices to the data.\n\n        args:\n\n            x (torch.Tensor): Input data\n\n        returns:\n            ZCAWhiten: returns a fitted ZCAWhiten object instance.\n        """"""\n\n        T, mean, T_inv = zca_mean(x, self.dim, self.unbiased, self.eps, self.compute_inv)\n\n        self.mean_vector: torch.Tensor = mean\n        self.transform_matrix: torch.Tensor = T\n        if T_inv is None:\n            self.transform_inv: Optional[torch.Tensor] = torch.empty([0, ])\n        else:\n            self.transform_inv = T_inv\n\n        if self.detach_transforms:\n            self.mean_vector = self.mean_vector.detach()\n            self.transform_matrix = self.transform_matrix.detach()\n            self.transform_inv = self.transform_inv.detach()\n\n        self.fitted = True\n\n        return self\n\n    def forward(self, x: torch.Tensor, include_fit: bool = False) -> torch.Tensor:\n        r""""""\n\n        Applies the whitening transform to the data\n\n        args:\n\n            x (torch.Tensor): Input data\n            include_fit (bool): Indicates whether to fit the data as part of the forward pass\n\n        returns:\n\n            torch.Tensor : The transformed data\n\n        """"""\n\n        if include_fit:\n            self.fit(x)\n\n        if not self.fitted:\n            raise RuntimeError(""Needs to be fitted first before running. Please call fit or set include_fit to True."")\n\n        x_whiten = linear_transform(x, self.transform_matrix, self.mean_vector, self.dim)\n\n        return x_whiten\n\n    def inverse_transform(self, x: torch.Tensor) -> torch.Tensor:\n        r""""""\n\n        Applies the inverse transform to the whitened data.\n\n        args:\n            x (torch.Tensor): Whitened data\n\n        returns:\n            torch.Tensor: original data\n\n\n\n        """"""\n\n        if not self.fitted:\n            raise RuntimeError(""Needs to be fitted first before running. Please call fit or set include_fit to True."")\n\n        if not self.compute_inv:\n            raise RuntimeError(""Did not compute inverse ZCA. Please set compute_inv to True"")\n\n        mean_inv: torch.Tensor = -self.mean_vector.mm(self.transform_matrix)  # type: ignore\n\n        y = linear_transform(x, self.transform_inv, mean_inv)  # type: ignore\n\n        return y\n\n\ndef zca_mean(inp: torch.Tensor, dim: int = 0,\n             unbiased: bool = True, eps: float = 1e-6,\n             return_inverse: bool = False) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n    r""""""\n\n    Computes the ZCA whitening matrix and mean vector. The output can be used with\n    :py:meth:`~kornia.color.linear_transform`\n\n    See :class:`~kornia.color.ZCAWhitening` for details.\n\n\n    args:\n        inp (torch.Tensor) : input data tensor\n        dim (int): Specifies the dimension that serves as the samples dimension. Default = 0\n        unbiased (bool): Whether to use the unbiased estimate of the covariance matrix. Default = True\n        eps (float) : a small number used for numerical stability. Default = 0\n        return_inverse (bool): Whether to return the inverse ZCA transform.\n\n    shapes:\n        - inp: :math:`(D_0,...,D_{\\text{dim}},...,D_N)` is a batch of N-D tensors.\n        - transform_matrix: :math:`(\\Pi_{d=0,d\\neq \\text{dim}}^N D_d, \\Pi_{d=0,d\\neq \\text{dim}}^N D_d)`\n        - mean_vector: :math:`(1, \\Pi_{d=0,d\\neq \\text{dim}}^N D_d)`\n        - inv_transform: same shape as the transform matrix\n\n    returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        A tuple containing the ZCA matrix and the mean vector. If return_inverse is set to True,\n        then it returns the inverse ZCA matrix, otherwise it returns None.\n\n    Examples:\n        >>> from kornia.color import zca_mean\n        >>> x = torch.tensor([[0,1],[1,0],[-1,0],[0,-1]], dtype = torch.float32)\n        >>> transform_matrix, mean_vector,_ = zca_mean(x) # Returns transformation matrix and data mean\n        >>> x = torch.rand(3,20,2,2)\n        >>> transform_matrix, mean_vector, inv_transform = zca_mean(x, dim = 1, return_inverse = True)\n        >>> # transform_matrix.size() equals (12,12) and the mean vector.size equal (1,12)\n\n    """"""\n\n    if not isinstance(inp, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(inp)))\n\n    if not isinstance(eps, float):\n        raise TypeError(f""eps type is not a float. Got{type(eps)}"")\n\n    if not isinstance(unbiased, bool):\n        raise TypeError(f""unbiased type is not bool. Got{type(unbiased)}"")\n\n    if not isinstance(dim, int):\n        raise TypeError(""Argument \'dim\' must be of type int. Got {}"".format(type(dim)))\n\n    if not isinstance(return_inverse, bool):\n        raise TypeError(""Argument return_inverse must be of type bool {}"".format(type(return_inverse)))\n\n    inp_size = inp.size()\n\n    if dim >= len(inp_size) or dim < -len(inp_size):\n        raise IndexError(""Dimension out of range (expected to be in range of [{},{}], but got {}""\n                         .format(-len(inp_size), len(inp_size) - 1, dim))\n\n    if dim < 0:\n        dim = len(inp_size) + dim\n\n    feat_dims = torch.cat([torch.arange(0, dim), torch.arange(dim + 1, len(inp_size))])\n\n    new_order: List[int] = torch.cat([torch.tensor([dim]), feat_dims]).tolist()\n\n    inp_permute = inp.permute(new_order)\n\n    N = inp_size[dim]\n    feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1::])\n    num_features: int = int(torch.prod(feature_sizes).item())\n\n    mean: torch.Tensor = torch.mean(inp_permute, dim=0, keepdim=True)\n\n    mean = mean.reshape((1, num_features))\n\n    inp_center_flat: torch.Tensor = inp_permute.reshape((N, num_features)) - mean\n\n    cov = inp_center_flat.t().mm(inp_center_flat)\n\n    if unbiased:\n        cov = cov / float(N - 1)\n    else:\n        cov = cov / float(N)\n\n    U, S, _ = torch.svd(cov)\n\n    S = S.reshape(-1, 1)\n    S_inv_root: torch.Tensor = torch.rsqrt(S + eps)\n    T: torch.Tensor = (U).mm(S_inv_root * U.t())\n\n    T_inv: Optional[torch.Tensor] = None\n    if return_inverse:\n        T_inv = (U).mm(torch.sqrt(S) * U.t())\n\n    return T, mean, T_inv\n\n\ndef zca_whiten(inp: torch.Tensor, dim: int = 0,\n               unbiased: bool = True, eps: float = 1e-6) -> torch.Tensor:\n    r""""""\n\n    Applies ZCA whitening transform.\n\n    See :class:`~kornia.color.ZCAWhitening` for details.\n\n    args:\n        inp (torch.Tensor) : input data tensor\n        dim (int): Specifies the dimension that serves as the samples dimension. Default = 0\n        unbiased (bool): Whether to use the unbiased estimate of the covariance matrix. Default = True\n        eps (float) : a small number used for numerial stablility. Default = 0\n\n\n    returns:\n        torch.Tensor : Whiten Input data\n\n    Examples:\n        >>> import torch\n        >>> import kornia\n        >>> x = torch.tensor([[0,1],[1,0],[-1,0]], dtype = torch.float32)\n        >>> x_whiten = kornia.color.zca_whiten(x)\n\n    """"""\n\n    if not isinstance(inp, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(inp)))\n\n    if not isinstance(eps, float):\n        raise TypeError(f""eps type is not a float. Got{type(eps)}"")\n\n    if not isinstance(unbiased, bool):\n        raise TypeError(f""unbiased type is not bool. Got{type(unbiased)}"")\n\n    if not isinstance(dim, int):\n        raise TypeError(""Argument \'dim\' must be of type int. Got {}"".format(type(dim)))\n\n    transform, mean, _ = zca_mean(inp, dim, unbiased, eps, False)\n\n    inp_whiten = linear_transform(inp, transform, mean, dim)\n\n    return inp_whiten\n\n\ndef linear_transform(inp: torch.Tensor, transform_matrix: torch.Tensor,\n                     mean_vector: torch.Tensor, dim: int = 0) -> torch.Tensor:\n    r""""""\n\n    Given a transformation matrix and a mean vector, this function will flatten\n    the input tensor along the given dimension and subtract the mean vector\n    from it. Then the dot product with the transformation matrix will be computed\n    and then the resulting tensor is reshaped to the original input shape.\n\n    .. math::\n\n        \\mathbf{X}_{T} = (\\mathbf{X - \\mu})(T)\n\n    args:\n        inp (torch.Tensor): Input data :math:`X`\n        transform_matrix (torch.Tensor): Transform matrix :math:`T`\n        mean_vector (torch.Tensor): mean vector :math:`\\mu`\n        dim (int): Batch dimension. Default = 0\n\n    shapes:\n        - inp: :math:`(D_0,...,D_{\\text{dim}},...,D_N)` is a batch of N-D tensors.\n        - transform_matrix: :math:`(\\Pi_{d=0,d\\neq \\text{dim}}^N D_d, \\Pi_{d=0,d\\neq \\text{dim}}^N D_d)`\n        - mean_vector: :math:`(1, \\Pi_{d=0,d\\neq \\text{dim}}^N D_d)`\n\n    returns:\n        torch.Tensor : Transformed data\n\n    Example:\n        >>> # Example where dim = 3\n        >>> inp = torch.ones((10,3,4,5))\n        >>> transform_mat = torch.ones((10*3*4,10*3*4))\n        >>> mean = 2*torch.ones((1,10*3*4))\n        >>> out = kornia.color.linear_transform(inp, transform_mat, mean, 3)\n        >>> print(out) # Should a be (10,3,4,5) tensor of -120s\n        >>> # Example where dim = 0\n        >>> inp = torch.ones((10,2))\n        >>> transform_mat = torch.ones((2,2))\n        >>> mean = torch.zeros((1,2))\n        >>> out = kornia.color.linear_transform(inp, transform_mat, mean)\n        >>> print(out) # Should a be (10,3,4,5) tensor of 2s\n\n\n    """"""\n\n    inp_size = inp.size()\n\n    if dim >= len(inp_size) or dim < -len(inp_size):\n        raise IndexError(""Dimension out of range (expected to be in range of [{},{}], but got {}""\n                         .format(-len(inp_size), len(inp_size) - 1, dim))\n\n    if dim < 0:\n        dim = len(inp_size) + dim\n\n    feat_dims = torch.cat([torch.arange(0, dim), torch.arange(dim + 1, len(inp_size))])\n\n    perm = torch.cat([torch.tensor([dim]), feat_dims])\n    perm_inv = torch.argsort(perm)\n\n    new_order: List[int] = perm.tolist()\n    inv_order: List[int] = perm_inv.tolist()\n\n    N = inp_size[dim]\n    feature_sizes = torch.tensor(inp_size[0:dim] + inp_size[dim + 1::])\n    num_features: int = int(torch.prod(feature_sizes).item())\n\n    inp_permute = inp.permute(new_order)\n    inp_flat = inp_permute.reshape((-1, num_features))\n\n    inp_center = (inp_flat - mean_vector)\n    inp_transformed = inp_center.mm(transform_matrix)\n\n    inp_transformed = inp_transformed.reshape(inp_permute.size())\n\n    inp_transformed = inp_transformed.permute(inv_order)\n\n    return inp_transformed\n'"
kornia/contrib/__init__.py,0,"b'from .extract_patches import ExtractTensorPatches, extract_tensor_patches\nfrom .max_blur_pool import MaxBlurPool2d, max_blur_pool2d\n\n__all__ = [\n    ""extract_tensor_patches"",\n    ""max_blur_pool2d"",\n    ""ExtractTensorPatches"",\n    ""MaxBlurPool2d"",\n]\n'"
kornia/contrib/extract_patches.py,16,"b'from typing import Optional, Union, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.utils import _pair\n\n\nclass ExtractTensorPatches(nn.Module):\n    r""""""Module that extract patches from tensors and stack them.\n\n    Applies a 2D convolution over an input tensor to extract patches and stack\n    them in the depth axis of the output tensor. The function applies a\n    Depthwise Convolution by applying the same kernel for all the input planes.\n\n    In the simplest case, the output value of the operator with input size\n    :math:`(B, C, H, W)` is :math:`(B, N, C, H_{out}, W_{out})`.\n\n    where\n      - :math:`B` is the batch size.\n      - :math:`N` denotes the total number of extracted patches stacked in\n      - :math:`C` denotes the number of input channels.\n      - :math:`H`, :math:`W` the input height and width of the input in pixels.\n      - :math:`H_{out}`, :math:`W_{out}` denote to denote to the patch size\n        defined in the function signature.\n        left-right and top-bottom order.\n\n    * :attr:`window_size` is the size of the sliding window and controls the\n      shape of the output tensor and defines the shape of the output patch.\n    * :attr:`stride` controls the stride to apply to the sliding window and\n      regulates the overlapping between the extracted patches.\n    * :attr:`padding` controls the amount of implicit zeros-paddings on both\n      sizes at each dimension.\n\n    The parameters :attr:`window_size`, :attr:`stride` and :attr:`padding` can\n    be either:\n\n        - a single ``int`` -- in which case the same value is used for the\n          height and width dimension.\n        - a ``tuple`` of two ints -- in which case, the first `int` is used for\n          the height dimension, and the second `int` for the width dimension.\n\n    Arguments:\n        window_size (Union[int, Tuple[int, int]]): the size of the convolving\n          kernel and the output patch size.\n        stride (Optional[Union[int, Tuple[int, int]]]): stride of the\n          convolution. Default is 1.\n        padding (Optional[Union[int, Tuple[int, int]]]): Zero-padding added to\n          both side of the input. Default is 0.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, N, C, H_{out}, W_{out})`\n\n    Returns:\n        torch.Tensor: the tensor with the extracted patches.\n\n    Examples:\n        >>> input = torch.arange(9.).view(1, 1, 3, 3)\n        >>> patches = kornia.contrib.extract_tensor_patches(input, (2, 3))\n        >>> input\n        tensor([[[[0., 1., 2.],\n                  [3., 4., 5.],\n                  [6., 7., 8.]]]])\n        >>> patches[:, -1]\n        tensor([[[[3.0000, 4.0000, 5.0000],\n                  [6.0000, 7.0000, 8.0000]]]])\n    """"""\n\n    def __init__(\n            self,\n            window_size: Union[int, Tuple[int, int]],\n            stride: Optional[Union[int, Tuple[int, int]]] = 1,\n            padding: Optional[Union[int, Tuple[int, int]]] = 0) -> None:\n        super(ExtractTensorPatches, self).__init__()\n        self.window_size: Tuple[int, int] = _pair(window_size)\n        self.stride: Tuple[int, int] = _pair(stride)\n        self.padding: Tuple[int, int] = _pair(padding)\n\n        # create base kernel\n        self.kernel: torch.Tensor = self.create_kernel(self.window_size)\n\n    @staticmethod\n    def create_kernel(\n            window_size: Tuple[int, int]) -> torch.Tensor:\n        r""""""Creates a binary kernel to extract the patches. If the window size\n        is HxW will create a (H*W)xHxW kernel.\n        """"""\n        window_range: int = window_size[0] * window_size[1]\n        kernel: torch.Tensor = torch.zeros(window_range, window_range)\n        for i in range(window_range):\n            kernel[i, i] += 1.0\n        return kernel.view(window_range, 1, window_size[0], window_size[1])\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        if not torch.is_tensor(input):\n            raise TypeError(""Input input type is not a torch.Tensor. Got {}""\n                            .format(type(input)))\n        if not len(input.shape) == 4:\n            raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                             .format(input.shape))\n        # unpack shapes\n        batch_size, channels, height, width = input.shape\n\n        # run convolution 2d to extract patches\n        kernel: torch.Tensor = self.kernel.repeat(channels, 1, 1, 1)\n        kernel = kernel.to(input.device).to(input.dtype)\n        output_tmp: torch.Tensor = F.conv2d(\n            input,\n            kernel,\n            stride=self.stride,\n            padding=self.padding,\n            groups=channels)\n\n        # reshape the output tensor\n        output: torch.Tensor = output_tmp.view(\n            batch_size, channels, self.window_size[0], self.window_size[1], -1)\n        return output.permute(0, 4, 1, 2, 3)  # BxNxCxhxw\n\n\n######################\n# functional interface\n######################\n\n\ndef extract_tensor_patches(\n        input: torch.Tensor,\n        window_size: Union[int, Tuple[int, int]],\n        stride: Optional[Union[int, Tuple[int, int]]] = 1,\n        padding: Optional[Union[int, Tuple[int, int]]] = 0) -> torch.Tensor:\n    r""""""Function that extract patches from tensors and stack them.\n\n    See :class:`~kornia.contrib.ExtractTensorPatches` for details.\n    """"""\n    return ExtractTensorPatches(window_size, stride, padding)(input)\n'"
kornia/contrib/max_blur_pool.py,10,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.geometry.transform.pyramid import pyrdown\n\n__all__ = [\n    ""max_blur_pool2d"",\n    ""MaxBlurPool2d"",\n]\n\n\ndef _compute_zero_padding(kernel_size: Tuple[int, int]) -> Tuple[int, int]:\n    """"""Computes zero padding tuple.""""""\n    padding = [(k - 1) // 2 for k in kernel_size]\n    return padding[0], padding[1]\n\n\nclass MaxBlurPool2d(nn.Module):\n    r""""""Creates a module that computes pools and blurs and downsample a given\n    feature map.\n\n    See :cite:`zhang2019shiftinvar` for more details.\n\n    Args:\n        kernel_size (int): the kernel size for max pooling..\n        ceil_mode (bool): should be true to match output size of conv2d with same kernel size.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H / 2, W / 2)`\n\n    Returns:\n        torch.Tensor: the transformed tensor.\n\n    Examples:\n        >>> input = torch.rand(1, 4, 4, 8)\n        >>> pool = kornia.contrib.MaxBlurPool2d(kernel_size=3)\n        >>> output = pool(input)  # 1x4x2x4\n    """"""\n\n    def __init__(self, kernel_size: int, ceil_mode: bool = False) -> None:\n        super(MaxBlurPool2d, self).__init__()\n        self.ceil_mode: bool = ceil_mode\n        self.kernel_size: Tuple[int, int] = (kernel_size, kernel_size)\n        self.padding: Tuple[int, int] = _compute_zero_padding(self.kernel_size)\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        if not torch.is_tensor(input):\n            raise TypeError(""Input input type is not a torch.Tensor. Got {}""\n                            .format(type(input)))\n        if not len(input.shape) == 4:\n            raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                             .format(input.shape))\n        # compute local maxima\n        x_max: torch.Tensor = F.max_pool2d(\n            input, kernel_size=self.kernel_size,\n            padding=self.padding, stride=1, ceil_mode=self.ceil_mode)\n\n        # blur and downsample\n        x_down: torch.Tensor = pyrdown(x_max)\n        return x_down\n\n\n######################\n# functional interface\n######################\n\n\ndef max_blur_pool2d(input: torch.Tensor, kernel_size: int, ceil_mode: bool = False) -> torch.Tensor:\n    r""""""Creates a module that computes pools and blurs and downsample a given\n    feature map.\n\n    See :class:`~kornia.contrib.MaxBlurPool2d` for details.\n    """"""\n    return MaxBlurPool2d(kernel_size, ceil_mode)(input)\n'"
kornia/feature/__init__.py,0,"b'from .responses import (CornerHarris,\n                        CornerGFTT,\n                        BlobHessian,\n                        harris_response,\n                        gftt_response,\n                        hessian_response)\nfrom .nms import (NonMaximaSuppression2d,\n                  nms2d,\n                  NonMaximaSuppression3d,\n                  nms3d)\n\n# Backward compatibility\nfrom .nms import nms2d as non_maxima_suppression2d\nfrom .nms import nms3d as non_maxima_suppression3d\n\nfrom .laf import (extract_patches_from_pyramid,\n                  extract_patches_simple,\n                  normalize_laf,\n                  denormalize_laf,\n                  laf_to_boundary_points,\n                  ellipse_to_laf,\n                  make_upright,\n                  scale_laf,\n                  get_laf_scale,\n                  get_laf_center,\n                  get_laf_orientation,\n                  raise_error_if_laf_is_not_valid,\n                  laf_from_center_scale_ori,\n                  laf_is_inside_image,\n                  laf_to_three_points,\n                  laf_from_three_points)\nfrom .siftdesc import SIFTDescriptor\nfrom .hardnet import HardNet\nfrom .sosnet import SOSNet\nfrom .scale_space_detector import ScaleSpaceDetector, PassLAF\nfrom .affine_shape import LAFAffineShapeEstimator, PatchAffineShapeEstimator\nfrom .orientation import LAFOrienter, PatchDominantGradientOrientation\n\n__all__ = [\n    ""nms2d"",\n    ""nms3d"",\n    ""non_maxima_suppression2d"",\n    ""non_maxima_suppression3d"",\n    ""harris_response"",\n    ""gftt_response"",\n    ""hessian_response"",\n    ""NonMaximaSuppression2d"",\n    ""NonMaximaSuppression3d"",\n    ""CornerHarris"",\n    ""CornerGFTT"",\n    ""BlobHessian"",\n    ""extract_patches_from_pyramid"",\n    ""extract_patches_simple"",\n    ""normalize_laf"",\n    ""denormalize_laf"",\n    ""laf_to_boundary_points"",\n    ""ellipse_to_laf"",\n    ""make_upright"",\n    ""get_laf_scale"",\n    ""get_laf_center"",\n    ""get_laf_orientation"",\n    ""scale_laf"",\n    ""SIFTDescriptor"",\n    ""HardNet"",\n    ""PassLAF"",\n    ""ScaleSpaceDetector"",\n    ""LAFAffineShapeEstimator"",\n    ""PatchAffineShapeEstimator"",\n    ""LAFOrienter"",\n    ""PatchDominantGradientOrientation"",\n    ""raise_error_if_laf_is_not_valid"",\n    ""laf_is_inside_image"",\n    ""laf_from_center_scale_ori"",\n    ""laf_to_three_points"",\n    ""laf_from_three_points"",\n]\n'"
kornia/feature/affine_shape.py,20,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport math\nfrom kornia.filters import get_gaussian_kernel2d\nfrom kornia.filters import SpatialGradient\nfrom kornia.feature.laf import (ellipse_to_laf,\n                                get_laf_scale,\n                                raise_error_if_laf_is_not_valid,\n                                scale_laf, make_upright)\nfrom kornia.feature import extract_patches_from_pyramid\n\n\nclass PatchAffineShapeEstimator(nn.Module):\n    """"""Module, which estimates the second moment matrix of the patch gradients in order to determine the\n    affine shape of the local feature as in :cite:`baumberg2000`.\n\n    Args:\n        patch_size: int, default = 19\n        eps: float, for safe division, default is 1e-10""""""\n\n    def __init__(self, patch_size: int = 19, eps: float = 1e-10):\n        super(PatchAffineShapeEstimator, self).__init__()\n        self.patch_size: int = patch_size\n        self.gradient: nn.Module = SpatialGradient(\'sobel\', 1)\n        self.eps: float = eps\n        sigma: float = float(self.patch_size) / math.sqrt(2.0)\n        self.weighting: torch.Tensor = get_gaussian_kernel2d((self.patch_size, self.patch_size), (sigma, sigma), True)\n        return\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\'\\\n            \'patch_size=\' + str(self.patch_size) + \', \' + \\\n            \'eps=\' + str(self.eps) + \')\'\n\n    def forward(self, patch: torch.Tensor) -> torch.Tensor:   # type: ignore\n        """"""Args:\n            patch: (torch.Tensor) shape [Bx1xHxW]\n        Returns:\n            ellipse_shape: 3d tensor, shape [Bx1x5] """"""\n        if not torch.is_tensor(patch):\n            raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                            .format(type(patch)))\n        if not len(patch.shape) == 4:\n            raise ValueError(""Invalid input shape, we expect Bx1xHxW. Got: {}""\n                             .format(patch.shape))\n        B, CH, W, H = patch.size()\n        if (W != self.patch_size) or (H != self.patch_size) or (CH != 1):\n            raise TypeError(\n                ""input shape should be must be [Bx1x{}x{}]. ""\n                ""Got {}"".format(self.patch_size, self.patch_size, patch.size()))\n        self.weighting = self.weighting.to(patch.dtype).to(patch.device)\n        grads: torch.Tensor = self.gradient(patch) * self.weighting\n        # unpack the edges\n        gx: torch.Tensor = grads[:, :, 0]\n        gy: torch.Tensor = grads[:, :, 1]\n        # abc == 1st axis, mixture, 2nd axis. Ellipse_shape is a 2nd moment matrix.\n        ellipse_shape = torch.cat([gx.pow(2).mean(dim=2).mean(dim=2, keepdim=True),\n                                   (gx * gy).mean(dim=2).mean(dim=2, keepdim=True),\n                                   gy.pow(2).mean(dim=2).mean(dim=2, keepdim=True)], dim=2)\n\n        # Now lets detect degenerate cases: when 2 or 3 elements are close to zero (e.g. if patch is completely black\n        bad_mask = ((ellipse_shape < self.eps).float().sum(dim=2, keepdim=True) >= 2).to(ellipse_shape.dtype)\n        # We will replace degenerate shape with circular shapes.\n        circular_shape = torch.tensor([1.0, 0., 1.0]).to(ellipse_shape.device).to(ellipse_shape.dtype).view(1, 1, 3)\n        ellipse_shape = ellipse_shape * (1.0 - bad_mask) + circular_shape * bad_mask  # type: ignore\n        # normalization\n        ellipse_shape = ellipse_shape / ellipse_shape.max(dim=2, keepdim=True)[0]\n        return ellipse_shape\n\n\nclass LAFAffineShapeEstimator(nn.Module):\n    """"""Module, which extracts patches using input images and local affine frames (LAFs),\n    then runs :class:`~kornia.feature.PatchAffineShapeEstimator` on patches to estimate LAFs shape.\n    Then original LAF shape is replaced with estimated one. The original LAF orientation is not preserved,\n    so it is recommended to first run LAFAffineShapeEstimator and then LAFOrienter.\n\n    Args:\n            patch_size: int, default = 32""""""\n\n    def __init__(self,\n                 patch_size: int = 32) -> None:\n        super(LAFAffineShapeEstimator, self).__init__()\n        self.patch_size = patch_size\n        self.affine_shape_detector = PatchAffineShapeEstimator(self.patch_size)\n        return\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\'\\\n            \'patch_size=\' + str(self.patch_size) + \')\'\n\n    def forward(self, laf: torch.Tensor, img: torch.Tensor) -> torch.Tensor:  # type: ignore\n        """"""\n        Args:\n            laf: (torch.Tensor) shape [BxNx2x3]\n            img: (torch.Tensor) shape [Bx1xHxW]\n\n        Returns:\n            laf_out: (torch.Tensor) shape [BxNx2x3]""""""\n        raise_error_if_laf_is_not_valid(laf)\n        img_message: str = ""Invalid img shape, we expect BxCxHxW. Got: {}"".format(img.shape)\n        if not torch.is_tensor(img):\n            raise TypeError(""img type is not a torch.Tensor. Got {}""\n                            .format(type(img)))\n        if len(img.shape) != 4:\n            raise ValueError(img_message)\n        if laf.size(0) != img.size(0):\n            raise ValueError(""Batch size of laf and img should be the same. Got {}, {}""\n                             .format(img.size(0), laf.size(0)))\n        B, N = laf.shape[:2]\n        PS: int = self.patch_size\n        patches: torch.Tensor = extract_patches_from_pyramid(img,\n                                                             make_upright(laf),\n                                                             PS, True).view(-1, 1, PS, PS)\n        ellipse_shape: torch.Tensor = self.affine_shape_detector(patches)\n        ellipses = torch.cat([laf.view(-1, 2, 3)[..., 2].unsqueeze(1), ellipse_shape], dim=2).view(B, N, 5)\n        scale_orig = get_laf_scale(laf)\n        laf_out = ellipse_to_laf(ellipses)\n        ellipse_scale = get_laf_scale(laf_out)\n        laf_out = scale_laf(laf_out, scale_orig / ellipse_scale)\n        return laf_out\n'"
kornia/feature/hardnet.py,11,"b'from typing import Dict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nurls: Dict[str, str] = dict()\nurls[""hardnet++""] = ""https://github.com/DagnyT/hardnet/raw/master/pretrained/pretrained_all_datasets/HardNet++.pth""\nurls[""liberty_aug""] = ""https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth""  # noqa pylint: disable\n\n\nclass HardNet(nn.Module):\n    """"""\n    Module, which computes HardNet descriptors of given grayscale patches of 32x32.\n\n    This is based on the original code from paper ""Working hard to know your neighbor\'s\n    margins: Local descriptor learning loss"". See :cite:`HardNet2017` for more details.\n\n    Args:\n        pretrained: (bool) Download and set pretrained weights to the model. Default: false.\n\n    Returns:\n        torch.Tensor: HardeNet descriptor of the patches.\n\n    Shape:\n        - Input: (B, 1, 32, 32)\n        - Output: (B, 128)\n\n    Examples:\n        >>> input = torch.rand(16, 1, 32, 32)\n        >>> hardnet = kornia.feature.HardNet()\n        >>> descs = hardnet(input) # 16x128\n    """"""\n\n    def __init__(self, pretrained: bool = False) -> None:\n        super(HardNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Conv2d(128, 128, kernel_size=8, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n        )\n\n        # use torch.hub to load pretrained model\n        if pretrained:\n            pretrained_dict = torch.hub.load_state_dict_from_url(\n                urls[\'liberty_aug\'], map_location=lambda storage, loc: storage\n            )\n            self.load_state_dict(pretrained_dict[\'state_dict\'], strict=True)\n\n    @staticmethod\n    def _normalize_input(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n        ""Utility function that normalizes the input by batch.""""""\n        sp, mp = torch.std_mean(x, dim=(-3, -2, -1), keepdim=True)\n        # WARNING: we need to .detach() input, otherwise the gradients produced by\n        # the patches extractor with F.grid_sample are very noisy, making the detector\n        # training totally unstable.\n        return (x - mp.detach()) / (sp.detach() + eps)\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:   # type: ignore\n        x_norm: torch.Tensor = self._normalize_input(input)\n        x_features: torch.Tensor = self.features(x_norm)\n        x_out = x_features.view(x_features.size(0), -1)\n        return F.normalize(x_out, dim=1)\n'"
kornia/feature/laf.py,115,"b'from typing import Union\nimport kornia\nimport math\nimport torch\nimport torch.nn.functional as F\n\n\ndef raise_error_if_laf_is_not_valid(laf: torch.Tensor) -> None:\n    """"""Auxilary function, which verifies that input is a torch.tensor of [BxNx2x3] shape\n\n    Args:\n        laf\n    """"""\n    laf_message: str = ""Invalid laf shape, we expect BxNx2x3. Got: {}"".format(laf.shape)\n    if not torch.is_tensor(laf):\n        raise TypeError(""Laf type is not a torch.Tensor. Got {}""\n                        .format(type(laf)))\n    if len(laf.shape) != 4:\n        raise ValueError(laf_message)\n    if laf.size(2) != 2 or laf.size(3) != 3:\n        raise ValueError(laf_message)\n    return\n\n\ndef get_laf_scale(LAF: torch.Tensor) -> torch.Tensor:\n    """"""Returns a scale of the LAFs\n\n    Args:\n        LAF: (torch.Tensor): tensor [BxNx2x3] or [BxNx2x2].\n\n    Returns:\n        torch.Tensor: tensor  BxNx1x1 .\n\n    Shape:\n        - Input: :math: `(B, N, 2, 3)`\n        - Output: :math: `(B, N, 1, 1)`\n\n    Example:\n        >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3\n        >>> output = kornia.get_laf_scale(input)  # BxNx1x1\n    """"""\n    raise_error_if_laf_is_not_valid(LAF)\n    eps = 1e-10\n    out = LAF[..., 0:1, 0:1] * LAF[..., 1:2, 1:2] - LAF[..., 1:2, 0:1] * LAF[..., 0:1, 1:2] + eps\n    return out.abs().sqrt()\n\n\ndef get_laf_center(LAF: torch.Tensor) -> torch.Tensor:\n    """"""Returns a center (keypoint) of the LAFs\n\n    Args:\n        LAF: (torch.Tensor): tensor [BxNx2x3].\n\n    Returns:\n        torch.Tensor: tensor  BxNx2 .\n\n    Shape:\n        - Input: :math: `(B, N, 2, 3)`\n        - Output: :math: `(B, N, 2)`\n\n    Example:\n        >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3\n        >>> output = kornia.get_laf_center(input)  # BxNx2\n    """"""\n    raise_error_if_laf_is_not_valid(LAF)\n    out: torch.Tensor = LAF[..., 2]\n    return out\n\n\ndef get_laf_orientation(LAF: torch.Tensor) -> torch.Tensor:\n    """"""Returns orientation of the LAFs, in degrees.\n\n    Args:\n        LAF: (torch.Tensor): tensor [BxNx2x3].\n\n    Returns:\n        torch.Tensor: tensor  BxNx1 .\n\n    Shape:\n        - Input: :math: `(B, N, 2, 3)`\n        - Output: :math: `(B, N, 1)`\n\n    Example:\n        >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3\n        >>> output = kornia.get_laf_orientation(input)  # BxNx1\n    """"""\n    raise_error_if_laf_is_not_valid(LAF)\n    angle_rad: torch.Tensor = torch.atan2(LAF[..., 0, 1], LAF[..., 0, 0])\n    return kornia.rad2deg(angle_rad).unsqueeze(-1)\n\n\ndef laf_from_center_scale_ori(xy: torch.Tensor, scale: torch.Tensor, ori: torch.Tensor) -> torch.Tensor:\n    """"""Returns orientation of the LAFs, in radians. Useful to create kornia LAFs from OpenCV keypoints\n\n    Args:\n        xy: (torch.Tensor): tensor [BxNx2].\n        scale: (torch.Tensor): tensor [BxNx1x1].\n        ori: (torch.Tensor): tensor [BxNx1].\n\n    Returns:\n        torch.Tensor: tensor  BxNx2x3 .\n    """"""\n    names = [\'xy\', \'scale\', \'ori\']\n    for var_name, var, req_shape in zip(names,\n                                        [xy, scale, ori],\n                                        [(""B"", ""N"", 2), (""B"", ""N"", 1, 1), (""B"", ""N"", 1)]):\n        if not torch.is_tensor(var):\n            raise TypeError(""{} type is not a torch.Tensor. Got {}""\n                            .format(var_name, type(var)))\n        if len(var.shape) != len(req_shape):  # type: ignore  # because it does not like len(tensor.shape)\n            raise TypeError(\n                ""{} shape should be must be [{}]. ""\n                ""Got {}"".format(var_name, str(req_shape), var.size()))\n        for i, dim in enumerate(req_shape):  # type: ignore # because it wants typing for dim\n            if dim is not int:\n                continue\n            if var.size(i) != dim:\n                raise TypeError(\n                    ""{} shape should be must be [{}]. ""\n                    ""Got {}"".format(var_name, str(req_shape), var.size()))\n    unscaled_laf: torch.Tensor = torch.cat([kornia.angle_to_rotation_matrix(ori.squeeze(-1)),\n                                            xy.unsqueeze(-1)], dim=-1)\n    laf: torch.Tensor = scale_laf(unscaled_laf, scale)\n    return laf\n\n\ndef scale_laf(laf: torch.Tensor, scale_coef: Union[float, torch.Tensor]) -> torch.Tensor:\n    """"""\n    Multiplies region part of LAF ([:, :, :2, :2]) by a scale_coefficient.\n    So the center, shape and orientation of the local feature stays the same, but the region area changes.\n\n    Args:\n        laf: (torch.Tensor): tensor [BxNx2x3] or [BxNx2x2].\n        scale_coef: (torch.Tensor): broadcastable tensor or float.\n\n\n    Returns:\n        torch.Tensor: tensor  BxNx2x3 .\n\n    Shape:\n        - Input: :math: `(B, N, 2, 3)`\n        - Input: :math: `(B, N,)` or ()\n        - Output: :math: `(B, N, 1, 1)`\n\n    Example:\n        >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3\n        >>> scale = 0.5\n        >>> output = kornia.scale_laf(input, scale)  # BxNx2x3\n    """"""\n    if (type(scale_coef) is not float) and (type(scale_coef) is not torch.Tensor):\n        raise TypeError(\n            ""scale_coef should be float or torch.Tensor ""\n            ""Got {}"".format(type(scale_coef)))\n    raise_error_if_laf_is_not_valid(laf)\n    centerless_laf: torch.Tensor = laf[:, :, :2, :2]\n    return torch.cat([scale_coef * centerless_laf, laf[:, :, :, 2:]], dim=3)\n\n\ndef make_upright(laf: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n    """"""\n    Rectifies the affine matrix, so that it becomes upright\n\n    Args:\n        laf: (torch.Tensor): tensor of LAFs.\n        eps (float): for safe division, (default 1e-9)\n\n    Returns:\n        torch.Tensor: tensor of same shape.\n\n    Shape:\n        - Input: :math:`(B, N, 2, 3)`\n        - Output:  :math:`(B, N, 2, 3)`\n\n    Example:\n        >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3\n        >>> output = kornia.make_upright(input)  #  BxNx2x3\n    """"""\n    raise_error_if_laf_is_not_valid(laf)\n    det = get_laf_scale(laf)\n    scale = det\n    # The function is equivalent to doing 2x2 SVD and reseting rotation\n    # matrix to an identity: U, S, V = svd(LAF); LAF_upright = U * S.\n    b2a2 = torch.sqrt(laf[..., 0:1, 1:2] ** 2 + laf[..., 0:1, 0:1] ** 2) + eps\n    laf1_ell = torch.cat([(b2a2 / det).contiguous(),\n                          torch.zeros_like(det)], dim=3)\n    laf2_ell = torch.cat([((laf[..., 1:2, 1:2] * laf[..., 0:1, 1:2] +\n                            laf[..., 1:2, 0:1] * laf[..., 0:1, 0:1]) / (b2a2 * det)),\n                          (det / b2a2).contiguous()], dim=3)  # type: ignore\n    laf_unit_scale = torch.cat([torch.cat([laf1_ell, laf2_ell], dim=2), laf[..., :, 2:3]], dim=3)\n    return scale_laf(laf_unit_scale, scale)\n\n\ndef ellipse_to_laf(ells: torch.Tensor) -> torch.Tensor:\n    """"""\n    Converts ellipse regions to LAF format. Ellipse (a, b, c)\n    and upright covariance matrix [a11 a12; 0 a22] are connected\n    by inverse matrix square root:\n    A = invsqrt([a b; b c])\n    See also https://github.com/vlfeat/vlfeat/blob/master/toolbox/sift/vl_frame2oell.m\n\n    Args:\n        ells: (torch.Tensor): tensor of ellipses in Oxford format [x y a b c].\n\n    Returns:\n        LAF: (torch.Tensor) tensor of ellipses in LAF format.\n\n    Shape:\n        - Input: :math:`(B, N, 5)`\n        - Output:  :math:`(B, N, 2, 3)`\n\n    Example:\n        >>> input = torch.ones(1, 10, 5)  # BxNx5\n        >>> output = kornia.ellipse_to_laf(input)  #  BxNx2x3\n    """"""\n    n_dims = len(ells.size())\n    if n_dims != 3:\n        raise TypeError(\n            ""ellipse shape should be must be [BxNx5]. ""\n            ""Got {}"".format(ells.size()))\n    B, N, dim = ells.size()\n    if (dim != 5):\n        raise TypeError(\n            ""ellipse shape should be must be [BxNx5]. ""\n            ""Got {}"".format(ells.size()))\n    # Previous implementation was incorrectly using Cholesky decomp as matrix sqrt\n    # ell_shape = torch.cat([torch.cat([ells[..., 2:3], ells[..., 3:4]], dim=2).unsqueeze(2),\n    #                       torch.cat([ells[..., 3:4], ells[..., 4:5]], dim=2).unsqueeze(2)], dim=2).view(-1, 2, 2)\n    # out = torch.matrix_power(torch.cholesky(ell_shape, False), -1).view(B, N, 2, 2)\n\n    # We will calculate 2x2 matrix square root via special case formula\n    # https://en.wikipedia.org/wiki/Square_root_of_a_matrix\n    # ""The Cholesky factorization provides another particular example of square root\n    #  which should not be confused with the unique non-negative square root.""\n    # https://en.wikipedia.org/wiki/Square_root_of_a_2_by_2_matrix\n    # M = (A 0; C D)\n    # R = (sqrt(A) 0; C / (sqrt(A)+sqrt(D)) sqrt(D))\n    a11 = ells[..., 2:3].abs().sqrt()\n    a12 = torch.zeros_like(a11)\n    a22 = ells[..., 4:5].abs().sqrt()\n    a21 = ells[..., 3:4] / (a11 + a22).clamp(1e-9)\n    A = torch.stack([a11, a12, a21, a22], dim=-1).view(B, N, 2, 2).inverse()\n    out = torch.cat([A, ells[..., :2].view(B, N, 2, 1)], dim=3)\n    return out\n\n\ndef laf_to_boundary_points(LAF: torch.Tensor, n_pts: int = 50) -> torch.Tensor:\n    """"""\n    Converts LAFs to boundary points of the regions + center.\n    Used for local features visualization, see visualize_laf function\n\n    Args:\n        LAF: (torch.Tensor).\n        n_pts: number of points to output\n\n    Returns:\n        pts: (torch.Tensor) tensor of boundary points\n\n    Shape:\n        - Input: :math:`(B, N, 2, 3)`\n        - Output:  :math:`(B, N, n_pts, 2)`\n    """"""\n    raise_error_if_laf_is_not_valid(LAF)\n    B, N, _, _ = LAF.size()\n    pts = torch.cat([torch.sin(torch.linspace(0, 2 * math.pi, n_pts - 1)).unsqueeze(-1),\n                     torch.cos(torch.linspace(0, 2 * math.pi, n_pts - 1)).unsqueeze(-1),\n                     torch.ones(n_pts - 1, 1)], dim=1)\n    # Add origin to draw also the orientation\n    pts = torch.cat([torch.tensor([0, 0, 1.]).view(1, 3), pts], dim=0).unsqueeze(0).expand(B * N, n_pts, 3)\n    pts = pts.to(LAF.device).to(LAF.dtype)\n    aux = torch.tensor([0, 0, 1.]).view(1, 1, 3).expand(B * N, 1, 3)\n    HLAF = torch.cat([LAF.view(-1, 2, 3), aux.to(LAF.device).to(LAF.dtype)], dim=1)\n    pts_h = torch.bmm(HLAF, pts.permute(0, 2, 1)).permute(0, 2, 1)\n    return kornia.convert_points_from_homogeneous(pts_h.view(B, N, n_pts, 3))\n\n\ndef get_laf_pts_to_draw(LAF: torch.Tensor,\n                        img_idx: int = 0):\n    """"""Returns numpy array for drawing LAFs (local features).\n\n    Args:\n        LAF: (torch.Tensor).\n        n_pts: number of boundary points to output\n\n    Returns:\n        pts: (torch.Tensor) tensor of boundary points\n\n    Shape:\n        - Input: :math:`(B, N, 2, 3)`\n        - Output:  :math:`(B, N, n_pts, 2)`\n\n    Examples:\n        >>> x, y = kornia.feature.laf.get_laf_pts_to_draw(LAF, img_idx)\n        >>> plt.figure()\n        >>> plt.imshow(kornia.utils.tensor_to_image(img[img_idx]))\n        >>> plt.plot(x, y, \'r\')\n        >>> plt.show()\n    """"""\n    raise_error_if_laf_is_not_valid(LAF)\n    pts = laf_to_boundary_points(LAF[img_idx:img_idx + 1])[0]\n    pts_np = pts.detach().permute(1, 0, 2).cpu().numpy()\n    return (pts_np[..., 0], pts_np[..., 1])\n\n\ndef denormalize_laf(LAF: torch.Tensor, images: torch.Tensor) -> torch.Tensor:\n    """"""De-normalizes LAFs from scale to image scale.\n        >>> B,N,H,W = images.size()\n        >>> MIN_SIZE = min(H,W)\n        [a11 a21 x]\n        [a21 a22 y]\n        becomes\n        [a11*MIN_SIZE a21*MIN_SIZE x*W]\n        [a21*MIN_SIZE a22*MIN_SIZE y*H]\n\n    Args:\n        LAF: (torch.Tensor).\n        images: (torch.Tensor) images, LAFs are detected in\n\n    Returns:\n        LAF: (torch.Tensor).\n\n    Shape:\n        - Input: :math:`(B, N, 2, 3)`\n        - Output:  :math:`(B, N, 2, 3)`\n    """"""\n    raise_error_if_laf_is_not_valid(LAF)\n    n, ch, h, w = images.size()\n    wf = float(w)\n    hf = float(h)\n    min_size = min(hf, wf)\n    coef = torch.ones(1, 1, 2, 3).to(LAF.dtype).to(LAF.device) * min_size\n    coef[0, 0, 0, 2] = wf\n    coef[0, 0, 1, 2] = hf\n    return coef.expand_as(LAF) * LAF\n\n\ndef normalize_laf(LAF: torch.Tensor, images: torch.Tensor) -> torch.Tensor:\n    """"""Normalizes LAFs to [0,1] scale from pixel scale. See below:\n        >>> B,N,H,W = images.size()\n        >>> MIN_SIZE = min(H,W)\n        [a11 a21 x]\n        [a21 a22 y]\n        becomes:\n        [a11/MIN_SIZE a21/MIN_SIZE x/W]\n        [a21/MIN_SIZE a22/MIN_SIZE y/H]\n\n    Args:\n        LAF: (torch.Tensor).\n        images: (torch.Tensor) images, LAFs are detected in\n\n    Returns:\n        LAF: (torch.Tensor).\n\n    Shape:\n        - Input: :math:`(B, N, 2, 3)`\n        - Output:  :math:`(B, N, 2, 3)`\n    """"""\n    raise_error_if_laf_is_not_valid(LAF)\n    n, ch, h, w = images.size()\n    wf: float = float(w)\n    hf: float = float(h)\n    min_size = min(hf, wf)\n    coef = torch.ones(1, 1, 2, 3).to(LAF.dtype).to(LAF.device) / min_size\n    coef[0, 0, 0, 2] = 1.0 / wf\n    coef[0, 0, 1, 2] = 1.0 / hf\n    return coef.expand_as(LAF) * LAF\n\n\ndef generate_patch_grid_from_normalized_LAF(img: torch.Tensor,\n                                            LAF: torch.Tensor,\n                                            PS: int = 32) -> torch.Tensor:\n    """"""Helper function for affine grid generation.\n\n    Args:\n        img: (torch.Tensor) images, LAFs are detected in\n        LAF: (torch.Tensor).\n        PS (int) -- patch size to be extracted\n\n    Returns:\n        grid: (torch.Tensor).\n\n    Shape:\n        - Input: :math:`(B, CH, H, W)`,  :math:`(B, N, 2, 3)`\n        - Output:  :math:`(B, N, PS, PS)`\n    """"""\n    raise_error_if_laf_is_not_valid(LAF)\n    B, N, _, _ = LAF.size()\n    num, ch, h, w = img.size()\n\n    # norm, then renorm is needed for allowing detection on one resolution\n    # and extraction at arbitrary other\n    LAF_renorm = denormalize_laf(LAF, img)\n\n    grid = F.affine_grid(LAF_renorm.view(B * N, 2, 3),  # type: ignore\n                         [B * N, ch, PS, PS], align_corners=False)\n    grid[..., :, 0] = 2.0 * grid[..., :, 0].clone() / float(w) - 1.0\n    grid[..., :, 1] = 2.0 * grid[..., :, 1].clone() / float(h) - 1.0\n    return grid\n\n\ndef extract_patches_simple(img: torch.Tensor,\n                           laf: torch.Tensor,\n                           PS: int = 32,\n                           normalize_lafs_before_extraction: bool = True) -> torch.Tensor:\n    """"""Extract patches defined by LAFs from image tensor.\n    No smoothing applied, huge aliasing (better use extract_patches_from_pyramid)\n\n    Args:\n        img: (torch.Tensor) images, LAFs are detected in\n        laf: (torch.Tensor).\n        PS: (int) patch size, default = 32\n        normalize_lafs_before_extraction (bool):  if True, lafs are normalized to image size, default = True\n\n    Returns:\n        patches: (torch.Tensor) :math:`(B, N, CH, PS,PS)`\n    """"""\n    raise_error_if_laf_is_not_valid(laf)\n    if normalize_lafs_before_extraction:\n        nlaf: torch.Tensor = normalize_laf(laf, img)\n    else:\n        nlaf = laf\n    num, ch, h, w = img.size()\n    B, N, _, _ = laf.size()\n    out = []\n    # for loop temporarily, to be refactored\n    for i in range(B):\n        grid = generate_patch_grid_from_normalized_LAF(img[i:i + 1], nlaf[i:i + 1], PS).to(img.device)\n        out.append(F.grid_sample(img[i:i + 1].expand(grid.size(0), ch, h, w), grid,  # type: ignore\n                                 padding_mode=""border"", align_corners=False))\n    return torch.cat(out, dim=0).view(B, N, ch, PS, PS)\n\n\ndef extract_patches_from_pyramid(img: torch.Tensor,\n                                 laf: torch.Tensor,\n                                 PS: int = 32,\n                                 normalize_lafs_before_extraction: bool = True) -> torch.Tensor:\n    """"""Extract patches defined by LAFs from image tensor.\n    Patches are extracted from appropriate pyramid level\n\n    Args:\n        laf: (torch.Tensor).\n        images: (torch.Tensor) images, LAFs are detected in\n        PS: (int) patch size, default = 32\n        normalize_lafs_before_extraction (bool):  if True, lafs are normalized to image size, default = True\n\n    Returns:\n        patches: (torch.Tensor)  :math:`(B, N, CH, PS,PS)`\n    """"""\n    raise_error_if_laf_is_not_valid(laf)\n    if normalize_lafs_before_extraction:\n        nlaf: torch.Tensor = normalize_laf(laf, img)\n    else:\n        nlaf = laf\n    B, N, _, _ = laf.size()\n    num, ch, h, w = img.size()\n    scale = 2.0 * get_laf_scale(denormalize_laf(nlaf, img)) / float(PS)\n    pyr_idx = (scale.log2() + 0.5).relu().long()\n    cur_img = img\n    cur_pyr_level = int(0)\n    out = torch.zeros(B, N, ch, PS, PS).to(nlaf.dtype).to(nlaf.device)\n    while min(cur_img.size(2), cur_img.size(3)) >= PS:\n        num, ch, h, w = cur_img.size()\n        # for loop temporarily, to be refactored\n        for i in range(B):\n            scale_mask = (pyr_idx[i] == cur_pyr_level).bool().squeeze()\n            if (scale_mask.float().sum()) == 0:\n                continue\n            scale_mask = scale_mask.bool().view(-1)\n            grid = generate_patch_grid_from_normalized_LAF(\n                cur_img[i:i + 1],\n                nlaf[i:i + 1, scale_mask, :, :],\n                PS)\n            patches = F.grid_sample(cur_img[i:i + 1].expand(grid.size(0), ch, h, w), grid,  # type: ignore\n                                    padding_mode=""border"", align_corners=False)\n            out[i].masked_scatter_(scale_mask.view(-1, 1, 1, 1), patches)\n        cur_img = kornia.pyrdown(cur_img)\n        cur_pyr_level += 1\n    return out\n\n\ndef laf_is_inside_image(laf: torch.Tensor, images: torch.Tensor) -> torch.Tensor:\n    """"""Checks if the LAF is touching or partly outside the image boundary. Returns the mask\n    of LAFs, which are fully inside the image, i.e. valid.\n\n    Args:\n        laf (torch.Tensor):  :math:`(B, N, 2, 3)`\n        images (torch.Tensor): images, lafs are detected in :math:`(B, CH, H, W)`\n\n    Returns:\n        mask (torch.Tensor):  :math:`(B, N)`\n    """"""\n    raise_error_if_laf_is_not_valid(laf)\n    n, ch, h, w = images.size()\n    pts: torch.Tensor = laf_to_boundary_points(laf, 12)\n    good_lafs_mask: torch.Tensor = (pts[..., 0] >= 0) * (pts[..., 0] <= w) * (pts[..., 1] >= 0) * (pts[..., 1] <= h)\n    good_lafs_mask = good_lafs_mask.min(dim=2)[0]\n    return good_lafs_mask\n\n\ndef laf_to_three_points(laf: torch.Tensor):\n    """"""Converts local affine frame(LAF) to alternative representation: coordinates of\n    LAF center, LAF-x unit vector, LAF-y unit vector.\n\n    Args:\n        laf (torch.Tensor):  :math:`(B, N, 2, 3)`\n\n    Returns:\n        threepts (torch.Tensor):  :math:`(B, N, 2, 3)`\n    """"""\n    raise_error_if_laf_is_not_valid(laf)\n    three_pts: torch.Tensor = torch.stack([laf[..., 2] + laf[..., 0],\n                                           laf[..., 2] + laf[..., 1],\n                                           laf[..., 2]], dim=-1)\n    return three_pts\n\n\ndef laf_from_three_points(threepts: torch.Tensor):\n    """"""Converts three points to local affine frame.\n    Order is (0,0), (0, 1), (1, 0).\n\n    Args:\n        threepts (torch.Tensor):  :math:`(B, N, 2, 3)`\n\n    Returns:\n        laf (torch.Tensor):  :math:`(B, N, 2, 3)`\n    """"""\n    laf: torch.Tensor = torch.stack([threepts[..., 0] - threepts[..., 2],\n                                     threepts[..., 1] - threepts[..., 2],\n                                     threepts[..., 2]], dim=-1)\n    return laf\n'"
kornia/feature/nms.py,10,"b'from typing import Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef _get_nms_kernel2d(kx: int, ky: int) -> torch.Tensor:\n    """"""Utility function, which returns neigh2channels conv kernel""""""\n    numel: int = ky * kx\n    center: int = numel // 2\n    weight = torch.eye(numel)\n    weight[center, center] = 0\n    return weight.view(numel, 1, ky, kx)\n\n\ndef _get_nms_kernel3d(kd: int, ky: int, kx: int) -> torch.Tensor:\n    """"""Utility function, which returns neigh2channels conv kernel""""""\n    numel: int = kd * ky * kx\n    center: int = numel // 2\n    weight = torch.eye(numel)\n    weight[center, center] = 0\n    return weight.view(numel, 1, kd, ky, kx)\n\n\nclass NonMaximaSuppression2d(nn.Module):\n    r""""""Applies non maxima suppression to filter.\n    """"""\n\n    def __init__(self, kernel_size: Tuple[int, int]):\n        super(NonMaximaSuppression2d, self).__init__()\n        self.kernel_size: Tuple[int, int] = kernel_size\n        self.padding: Tuple[int,\n                            int] = self._compute_zero_padding2d(kernel_size)\n        self.kernel = _get_nms_kernel2d(*kernel_size)\n\n    @staticmethod\n    def _compute_zero_padding2d(\n            kernel_size: Tuple[int, int]) -> Tuple[int, int]:\n        assert isinstance(kernel_size, tuple), type(kernel_size)\n        assert len(kernel_size) == 2, kernel_size\n\n        def pad(x):\n            return (x - 1) // 2  # zero padding function\n\n        ky, kx = kernel_size     # we assume a cubic kernel\n        return (pad(ky), pad(kx))\n\n    def forward(self, x: torch.Tensor, mask_only: bool = False) -> torch.Tensor:  # type: ignore\n        assert len(x.shape) == 4, x.shape\n        B, CH, H, W = x.size()\n        # find local maximum values\n        max_non_center = F.conv2d(x, self.kernel.repeat(CH, 1, 1, 1).to(x.device, x.dtype),\n                                  stride=1,\n                                  padding=self.padding,\n                                  groups=CH).view(B, CH, -1, H, W).max(dim=2)[0]\n        mask = x > max_non_center\n        if mask_only:\n            return mask\n        return x * (mask.to(x.dtype))\n\n\nclass NonMaximaSuppression3d(nn.Module):\n    r""""""Applies non maxima suppression to filter.\n    """"""\n\n    def __init__(self, kernel_size: Tuple[int, int, int]):\n        super(NonMaximaSuppression3d, self).__init__()\n        self.kernel_size: Tuple[int, int, int] = kernel_size\n        self.padding: Tuple[int,\n                            int,\n                            int] = self._compute_zero_padding3d(kernel_size)\n        self.kernel = _get_nms_kernel3d(*kernel_size)\n\n    @staticmethod\n    def _compute_zero_padding3d(\n            kernel_size: Tuple[int, int, int]) -> Tuple[int, int, int]:\n        assert isinstance(kernel_size, tuple), type(kernel_size)\n        assert len(kernel_size) == 3, kernel_size\n\n        def pad(x):\n            return (x - 1) // 2  # zero padding function\n\n        kd, ky, kx = kernel_size     # we assume a cubic kernel\n        return pad(kd), pad(ky), pad(kx)\n\n    def forward(self, x: torch.Tensor, mask_only: bool = False) -> torch.Tensor:  # type: ignore\n        assert len(x.shape) == 5, x.shape\n        # find local maximum values\n        B, CH, D, H, W = x.size()\n        max_non_center = F.conv3d(x, self.kernel.repeat(CH, 1, 1, 1, 1).to(x.device, x.dtype),\n                                  stride=1,\n                                  padding=self.padding,\n                                  groups=CH).view(B, CH, -1, D, H, W).max(dim=2, keepdim=False)[0]\n        mask = x > max_non_center\n        if mask_only:\n            return mask\n        return x * (mask.to(x.dtype))\n\n\n# functiona api\n\n\ndef nms2d(\n        input: torch.Tensor, kernel_size: Tuple[int, int], mask_only: bool = False) -> torch.Tensor:\n    r""""""Applies non maxima suppression to filter.\n\n    See :class:`~kornia.feature.NonMaximaSuppression2d` for details.\n    """"""\n    return NonMaximaSuppression2d(kernel_size)(input, mask_only)\n\n\ndef nms3d(\n        input: torch.Tensor, kernel_size: Tuple[int, int, int], mask_only: bool = False) -> torch.Tensor:\n    r""""""Applies non maxima suppression to filter.\n\n    See :class:`~kornia.feature.NonMaximaSuppression3d` for details.\n    """"""\n    return NonMaximaSuppression3d(kernel_size)(input, mask_only)\n'"
kornia/feature/orientation.py,30,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom kornia.filters import get_gaussian_kernel2d\nfrom kornia.filters import SpatialGradient\nfrom kornia.constants import pi\nfrom kornia.feature import (extract_patches_from_pyramid, make_upright,\n                            normalize_laf, raise_error_if_laf_is_not_valid)\nfrom kornia.geometry import rad2deg, angle_to_rotation_matrix\n\n\nclass PassLAF(nn.Module):\n    """"""Dummy module to use instead of local feature orientation or affine shape estimator""""""\n\n    def forward(self, laf: torch.Tensor, img: torch.Tensor) -> torch.Tensor:  # type: ignore\n        """"""\n        Args:\n            laf: torch.Tensor: 4d tensor\n            img (torch.Tensor): the input image tensor\n\n        Return:\n            torch.Tensor: unchanged laf from the input.""""""\n        return laf\n\n\nclass PatchDominantGradientOrientation(nn.Module):\n    """"""Module, which estimates the dominant gradient orientation of the given patches, in radians.\n    Zero angle points towards right.\n\n    Args:\n            patch_size: int, default = 32\n            num_angular_bins: int, default is 36\n            eps: float, for safe division, and arctan, default is 1e-8""""""\n\n    def __init__(self,\n                 patch_size: int = 32,\n                 num_angular_bins: int = 36, eps: float = 1e-8):\n        super(PatchDominantGradientOrientation, self).__init__()\n        self.patch_size = patch_size\n        self.num_ang_bins = num_angular_bins\n        self.gradient = SpatialGradient(\'sobel\', 1)\n        self.eps = eps\n        self.angular_smooth = nn.Conv1d(1, 1, kernel_size=3, padding=1, bias=False, padding_mode=""circular"")\n        with torch.no_grad():\n            self.angular_smooth.weight[:] = torch.tensor([[[0.33, 0.34, 0.33]]])\n        sigma: float = float(self.patch_size) / math.sqrt(2.0)\n        self.weighting = get_gaussian_kernel2d((self.patch_size, self.patch_size), (sigma, sigma), True)\n        return\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\'\\\n            \'patch_size=\' + str(self.patch_size) + \', \' + \\\n            \'num_ang_bins=\' + str(self.num_ang_bins) + \', \' + \\\n            \'eps=\' + str(self.eps) + \')\'\n\n    def forward(self, patch: torch.Tensor) -> torch.Tensor:  # type: ignore\n        """"""Args:\n            patch: (torch.Tensor) shape [Bx1xHxW]\n        Returns:\n            patch: (torch.Tensor) shape [Bx1] """"""\n        if not torch.is_tensor(patch):\n            raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                            .format(type(patch)))\n        if not len(patch.shape) == 4:\n            raise ValueError(""Invalid input shape, we expect Bx1xHxW. Got: {}""\n                             .format(patch.shape))\n        B, CH, W, H = patch.size()\n        if (W != self.patch_size) or (H != self.patch_size) or (CH != 1):\n            raise TypeError(\n                ""input shape should be must be [Bx1x{}x{}]. ""\n                ""Got {}"".format(self.patch_size, self.patch_size, patch.size()))\n        self.weighting = self.weighting.to(patch.dtype).to(patch.device)\n        self.angular_smooth = self.angular_smooth.to(patch.dtype).to(patch.device)\n        grads: torch.Tensor = self.gradient(patch)\n        # unpack the edges\n        gx: torch.Tensor = grads[:, :, 0]\n        gy: torch.Tensor = grads[:, :, 1]\n\n        mag: torch.Tensor = torch.sqrt(gx * gx + gy * gy + self.eps)\n        ori: torch.Tensor = torch.atan2(gy, gx + self.eps) + 2.0 * pi\n\n        o_big = float(self.num_ang_bins) * (ori + 1.0 * pi) / (2.0 * pi)\n        bo0_big = torch.floor(o_big)\n        wo1_big = o_big - bo0_big\n        bo0_big = bo0_big % self.num_ang_bins\n        bo1_big = (bo0_big + 1) % self.num_ang_bins\n        wo0_big = (1.0 - wo1_big) * mag  # type: ignore\n        wo1_big = wo1_big * mag\n        ang_bins = []\n        for i in range(0, self.num_ang_bins):\n            ang_bins.append(F.adaptive_avg_pool2d((bo0_big == i).to(patch.dtype) * wo0_big +\n                                                  (bo1_big == i).to(patch.dtype) * wo1_big, (1, 1)))\n        ang_bins = torch.cat(ang_bins, 1).view(-1, 1, self.num_ang_bins)   # type: ignore\n        ang_bins = self.angular_smooth(ang_bins)   # type: ignore\n        values, indices = ang_bins.view(-1, self.num_ang_bins).max(1)  # type: ignore\n        angle = -((2. * pi * indices.to(patch.dtype) / float(self.num_ang_bins)) - pi)  # type: ignore\n        return angle\n\n\nclass LAFOrienter(nn.Module):\n    """"""Module, which extracts patches using input images and local affine frames (LAFs),\n    then runs :class:`~kornia.feature.PatchDominantGradientOrientation`\n    on patches and then rotates the LAFs by the estimated angles\n\n    Args:\n            patch_size: int, default = 32\n            num_angular_bins: int, default is 36""""""\n\n    def __init__(self,\n                 patch_size: int = 32,\n                 num_angular_bins: int = 36):\n        super(LAFOrienter, self).__init__()\n        self.patch_size = patch_size\n        self.num_ang_bins = num_angular_bins\n        self.angle_detector = PatchDominantGradientOrientation(self.patch_size, self.num_ang_bins)\n        return\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\'\\\n            \'patch_size=\' + str(self.patch_size) + \', \' + \\\n            \'num_ang_bins=\' + str(self.num_ang_bins) + \')\'\n\n    def forward(self, laf: torch.Tensor, img: torch.Tensor) -> torch.Tensor:  # type: ignore\n        """"""\n        Args:\n            laf: (torch.Tensor), shape [BxNx2x3]\n            img: (torch.Tensor), shape [Bx1xHxW]\n\n        Returns:\n            laf_out: (torch.Tensor), shape [BxNx2x3] """"""\n        raise_error_if_laf_is_not_valid(laf)\n        img_message: str = ""Invalid img shape, we expect BxCxHxW. Got: {}"".format(img.shape)\n        if not torch.is_tensor(img):\n            raise TypeError(""img type is not a torch.Tensor. Got {}""\n                            .format(type(img)))\n        if len(img.shape) != 4:\n            raise ValueError(img_message)\n        if laf.size(0) != img.size(0):\n            raise ValueError(""Batch size of laf and img should be the same. Got {}, {}""\n                             .format(img.size(0), laf.size(0)))\n        B, N = laf.shape[:2]\n        patches: torch.Tensor = extract_patches_from_pyramid(img,\n                                                             laf,\n                                                             self.patch_size).view(-1,\n                                                                                   1,\n                                                                                   self.patch_size,\n                                                                                   self.patch_size)\n        angles_radians: torch.Tensor = self.angle_detector(patches).view(B, N)\n        rotmat: torch.Tensor = angle_to_rotation_matrix(rad2deg(angles_radians)).view(B * N, 2, 2)\n\n        laf_out: torch.Tensor = torch.cat([torch.bmm(make_upright(laf).view(B * N, 2, 3)[:, :2, :2], rotmat),\n                                           laf.view(B * N, 2, 3)[:, :2, 2:]], dim=2).view(B, N, 2, 3)\n        return laf_out\n'"
kornia/feature/responses.py,64,"b'from typing import Tuple, Union, Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom kornia.filters import spatial_gradient, gaussian_blur2d\n\n\ndef harris_response(input: torch.Tensor,\n                    k: Union[torch.Tensor, float] = 0.04,\n                    grads_mode: str = \'sobel\',\n                    sigmas: Optional[torch.Tensor] = None) -> torch.Tensor:\n    r""""""Computes the Harris cornerness function. Function does not do\n    any normalization or nms.The response map is computed according the following formulation:\n\n    .. math::\n        R = max(0, det(M) - k \\cdot trace(M)^2)\n\n    where:\n\n    .. math::\n        M = \\sum_{(x,y) \\in W}\n        \\begin{bmatrix}\n            I^{2}_x & I_x I_y \\\\\n            I_x I_y & I^{2}_y \\\\\n        \\end{bmatrix}\n\n    and :math:`k` is an empirically determined constant\n    :math:`k \xe2\x88\x88 [ 0.04 , 0.06 ]`\n\n    Args:\n        input: torch.Tensor: 4d tensor\n        k (torch.Tensor): the Harris detector free parameter.\n        grads_mode (string): can be \'sobel\' for standalone use or \'diff\' for use on Gaussian pyramid\n        sigmas (optional, torch.Tensor): coefficients to be multiplied by multichannel response. \\n\n                                         Should be shape of (B)\n                                         It is necessary for performing non-maxima-suppression\n                                         across different scale pyramid levels.\\\n                                         See `vlfeat <https://github.com/vlfeat/vlfeat/blob/master/vl/covdet.c#L874>`_\n\n    Return:\n        torch.Tensor: the response map per channel.\n\n    Shape:\n      - Input: :math:`(B, C, H, W)`\n      - Output: :math:`(B, C, H, W)`\n\n    Examples:\n        >>> input = torch.tensor([[[\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n        ]]])  # 1x1x7x7\n        >>> # compute the response map\n        >>> output = harris_response(input, 0.04)\n        tensor([[[[0.0012, 0.0039, 0.0020, 0.0000, 0.0020, 0.0039, 0.0012],\n          [0.0039, 0.0065, 0.0040, 0.0000, 0.0040, 0.0065, 0.0039],\n          [0.0020, 0.0040, 0.0029, 0.0000, 0.0029, 0.0040, 0.0020],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0020, 0.0040, 0.0029, 0.0000, 0.0029, 0.0040, 0.0020],\n          [0.0039, 0.0065, 0.0040, 0.0000, 0.0040, 0.0065, 0.0039],\n          [0.0012, 0.0039, 0.0020, 0.0000, 0.0020, 0.0039, 0.0012]]]])\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(input)))\n    if not len(input.shape) == 4:\n        raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                         .format(input.shape))\n    if sigmas is not None:\n        if not torch.is_tensor(sigmas):\n            raise TypeError(""sigmas type is not a torch.Tensor. Got {}""\n                            .format(type(sigmas)))\n        if (not len(sigmas.shape) == 1) or (sigmas.size(0) != input.size(0)):\n            raise ValueError(""Invalid sigmas shape, we expect B == input.size(0). Got: {}"".format(sigmas.shape))\n    gradients: torch.Tensor = spatial_gradient(input, grads_mode)\n    dx: torch.Tensor = gradients[:, :, 0]\n    dy: torch.Tensor = gradients[:, :, 1]\n\n    # compute the structure tensor M elements\n    def g(x):\n        return gaussian_blur2d(x, (7, 7), (1., 1.))\n\n    dx2: torch.Tensor = g(dx ** 2)\n    dy2: torch.Tensor = g(dy ** 2)\n    dxy: torch.Tensor = g(dx * dy)\n    det_m: torch.Tensor = dx2 * dy2 - dxy * dxy\n    trace_m: torch.Tensor = dx2 + dy2\n    # compute the response map\n    scores: torch.Tensor = det_m - k * (trace_m ** 2)\n    if sigmas is not None:\n        scores = scores * sigmas.pow(4).view(-1, 1, 1, 1)\n    return scores\n\n\ndef gftt_response(input: torch.Tensor,\n                  grads_mode: str = \'sobel\',\n                  sigmas: Optional[torch.Tensor] = None) -> torch.Tensor:\n    r""""""Computes the Shi-Tomasi cornerness function. Function does not do any normalization or nms.\n    The response map is computed according the following formulation:\n\n    .. math::\n        R = min(eig(M))\n\n    where:\n\n    .. math::\n        M = \\sum_{(x,y) \\in W}\n        \\begin{bmatrix}\n            I^{2}_x & I_x I_y \\\\\n            I_x I_y & I^{2}_y \\\\\n        \\end{bmatrix}\n\n    Args:\n        input (torch.Tensor): 4d tensor\n        grads_mode (string): can be \'sobel\' for standalone use or \'diff\' for use on Gaussian pyramid\n        sigmas (optional, torch.Tensor): coefficients to be multiplied by multichannel response. \\n\n                                         Should be shape of (B)\n                                         It is necessary for performing non-maxima-suppression\n                                         across different scale pyramid levels.\\\n                                         See `vlfeat <https://github.com/vlfeat/vlfeat/blob/master/vl/covdet.c#L874>`_\n\n    Return:\n        torch.Tensor: the response map per channel.\n\n    Shape:\n      - Input: :math:`(B, C, H, W)`\n      - Output: :math:`(B, C, H, W)`\n\n    Examples:\n        >>> input = torch.tensor([[[\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n        ]]])  # 1x1x7x7\n        >>> # compute the response map\n        >>> output = gftt_response(input)\n        tensor([[[[0.0155, 0.0334, 0.0194, 0.0000, 0.0194, 0.0334, 0.0155],\n          [0.0334, 0.0575, 0.0339, 0.0000, 0.0339, 0.0575, 0.0334],\n          [0.0194, 0.0339, 0.0497, 0.0000, 0.0497, 0.0339, 0.0194],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0194, 0.0339, 0.0497, 0.0000, 0.0497, 0.0339, 0.0194],\n          [0.0334, 0.0575, 0.0339, 0.0000, 0.0339, 0.0575, 0.0334],\n          [0.0155, 0.0334, 0.0194, 0.0000, 0.0194, 0.0334, 0.0155]]]])\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(input)))\n    if not len(input.shape) == 4:\n        raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                         .format(input.shape))\n    gradients: torch.Tensor = spatial_gradient(input, grads_mode)\n    dx: torch.Tensor = gradients[:, :, 0]\n    dy: torch.Tensor = gradients[:, :, 1]\n\n    # compute the structure tensor M elements\n    def g(x):\n        return gaussian_blur2d(x, (7, 7), (1., 1.))\n\n    dx2: torch.Tensor = g(dx ** 2)\n    dy2: torch.Tensor = g(dy ** 2)\n    dxy: torch.Tensor = g(dx * dy)\n\n    det_m: torch.Tensor = dx2 * dy2 - dxy * dxy\n    trace_m: torch.Tensor = dx2 + dy2\n\n    e1: torch.Tensor = 0.5 * (trace_m + torch.sqrt((trace_m ** 2 - 4 * det_m).abs()))\n    e2: torch.Tensor = 0.5 * (trace_m - torch.sqrt((trace_m ** 2 - 4 * det_m).abs()))\n\n    scores: torch.Tensor = torch.min(e1, e2)\n    if sigmas is not None:\n        scores = scores * sigmas.pow(4).view(-1, 1, 1, 1)\n    return scores\n\n\ndef hessian_response(input: torch.Tensor,\n                     grads_mode: str = \'sobel\',\n                     sigmas: Optional[torch.Tensor] = None) -> torch.Tensor:\n    r""""""Computes the absolute of determinant of the Hessian matrix. Function does not do any normalization or nms.\n    The response map is computed according the following formulation:\n\n    .. math::\n        R = det(H)\n\n    where:\n\n    .. math::\n        M = \\sum_{(x,y) \\in W}\n        \\begin{bmatrix}\n            I_{xx} & I_{xy} \\\\\n            I_{xy} & I_{yy} \\\\\n        \\end{bmatrix}\n\n    Args:\n        input: torch.Tensor: 4d tensor\n        grads_mode (string): can be \'sobel\' for standalone use or \'diff\' for use on Gaussian pyramid\n        sigmas (optional, torch.Tensor): coefficients to be multiplied by multichannel response. \\n\n                                         Should be shape of (B)\n                                         It is necessary for performing non-maxima-suppression\n                                         across different scale pyramid levels.\\\n                                         See `vlfeat <https://github.com/vlfeat/vlfeat/blob/master/vl/covdet.c#L874>`_\n\n    Return:\n         torch.Tensor: the response map per channel.\n\n    Shape:\n       - Input: :math:`(B, C, H, W)`\n       - Output: :math:`(B, C, H, W)`\n\n    Examples:\n         >>> input = torch.tensor([[[\n             [0., 0., 0., 0., 0., 0., 0.],\n             [0., 1., 1., 1., 1., 1., 0.],\n             [0., 1., 1., 1., 1., 1., 0.],\n             [0., 1., 1., 1., 1., 1., 0.],\n             [0., 1., 1., 1., 1., 1., 0.],\n             [0., 1., 1., 1., 1., 1., 0.],\n             [0., 0., 0., 0., 0., 0., 0.],\n         ]]])  # 1x1x7x7\n         >>> # compute the response map\n         >>> output = hessian_response(input)\n         tensor([[[[0.0155, 0.0334, 0.0194, 0.0000, 0.0194, 0.0334, 0.0155],\n           [0.0334, 0.0575, 0.0339, 0.0000, 0.0339, 0.0575, 0.0334],\n           [0.0194, 0.0339, 0.0497, 0.0000, 0.0497, 0.0339, 0.0194],\n           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n           [0.0194, 0.0339, 0.0497, 0.0000, 0.0497, 0.0339, 0.0194],\n           [0.0334, 0.0575, 0.0339, 0.0000, 0.0339, 0.0575, 0.0334],\n           [0.0155, 0.0334, 0.0194, 0.0000, 0.0194, 0.0334, 0.0155]]]])\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(input)))\n    if not len(input.shape) == 4:\n        raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                         .format(input.shape))\n    if sigmas is not None:\n        if not torch.is_tensor(sigmas):\n            raise TypeError(""sigmas type is not a torch.Tensor. Got {}""\n                            .format(type(sigmas)))\n        if (not len(sigmas.shape) == 1) or (sigmas.size(0) != input.size(0)):\n            raise ValueError(""Invalid sigmas shape, we expect B == input.size(0). Got: {}""\n                             .format(sigmas.shape))\n    gradients: torch.Tensor = spatial_gradient(input, grads_mode, 2)\n    dxx: torch.Tensor = gradients[:, :, 0]\n    dxy: torch.Tensor = gradients[:, :, 1]\n    dyy: torch.Tensor = gradients[:, :, 2]\n\n    scores: torch.Tensor = dxx * dyy - dxy ** 2\n    if sigmas is not None:\n        scores = scores * sigmas.pow(4).view(-1, 1, 1, 1)\n    return scores\n\n\nclass CornerHarris(nn.Module):\n    r""""""nn.Module that calculates Harris corners\n    See :func:`~kornia.feature.harris_response` for details.\n    """"""\n\n    def __init__(self, k: Union[float, torch.Tensor],\n                 grads_mode=\'sobel\') -> None:\n        super(CornerHarris, self).__init__()\n        if type(k) is float:\n            self.register_buffer(\'k\', torch.tensor(k))\n        else:\n            self.register_buffer(\'k\', k)  # type: ignore\n        self.grads_mode: str = grads_mode\n        return\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ +\\\n            \'(k=\' + str(self.k) + \', \' +\\\n            \'grads_mode=\' + self.grads_mode + \')\'\n\n    def forward(self, input: torch.Tensor,  # type: ignore\n                sigmas: Optional[torch.Tensor] = None) -> torch.Tensor:\n        return harris_response(input, self.k, self.grads_mode, sigmas)  # type: ignore\n\n\nclass CornerGFTT(nn.Module):\n    r""""""nn.Module that calculates Shi-Tomasi corners\n    See :func:`~kornia.feature.gfft_response` for details.\n    """"""\n\n    def __init__(self, grads_mode=\'sobel\') -> None:\n        super(CornerGFTT, self).__init__()\n        self.grads_mode: str = grads_mode\n        return\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ +\\\n            \'grads_mode=\' + self.grads_mode + \')\'\n\n    def forward(self, input: torch.Tensor,  # type: ignore\n                sigmas: Optional[torch.Tensor] = None) -> torch.Tensor:\n        return gftt_response(input, self.grads_mode, sigmas)\n\n\nclass BlobHessian(nn.Module):\n    r""""""nn.Module that calculates Hessian blobs\n    See :func:`~kornia.feature.hessian_response` for details.\n    """"""\n\n    def __init__(self, grads_mode=\'sobel\') -> None:\n        super(BlobHessian, self).__init__()\n        self.grads_mode: str = grads_mode\n        return\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ +\\\n            \'grads_mode=\' + self.grads_mode + \')\'\n\n    def forward(self, input: torch.Tensor,  # type: ignore\n                sigmas: Optional[torch.Tensor] = None) -> torch.Tensor:\n        return hessian_response(input, self.grads_mode, sigmas)\n'"
kornia/feature/scale_space_detector.py,30,"b'from typing import Tuple, List, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.geometry import angle_to_rotation_matrix\nfrom kornia.feature.responses import BlobHessian\nfrom kornia.geometry import ConvSoftArgmax3d\nfrom kornia.feature.orientation import PassLAF\nfrom kornia.feature.laf import (\n    denormalize_laf,\n    normalize_laf, laf_is_inside_image)\nfrom kornia.geometry.transform import ScalePyramid\n\n\ndef _scale_index_to_scale(max_coords: torch.Tensor, sigmas: torch.Tensor) -> torch.Tensor:\n    """"""Auxilary function for ScaleSpaceDetector. Converts scale level index from ConvSoftArgmax3d\n    to the actual scale, using the sigmas from the ScalePyramid output\n    Args:\n        max_coords: (torch.Tensor): tensor [BxNx3].\n        sigmas: (torch.Tensor): tensor [BxNxD], D >= 1\n\n    Returns:\n        torch.Tensor:  tensor [BxNx3].\n    """"""\n    # depth (scale) in coord_max is represented as (float) index, not the scale yet.\n    # we will interpolate the scale using pytorch.grid_sample function\n    # Because grid_sample is for 4d input only, we will create fake 2nd dimension\n    # ToDo: replace with 3d input, when grid_sample will start to support it\n\n    # Reshape for grid shape\n    B, N, _ = max_coords.shape\n    L: int = sigmas.size(1)\n    scale_coords = max_coords[:, :, 0].contiguous().view(-1, 1, 1, 1)\n\n    # Normalize coordinate\n    scale_coords_index = (2.0 * scale_coords / sigmas.size(1)) - 1.0\n\n    # Dummy dimension\n    dummy_x = torch.zeros_like(scale_coords_index)\n\n    # Create grid\n    scale_grid = torch.cat([scale_coords_index, dummy_x], dim=3)\n\n    # Finally, interpolate the scale value\n    scale_val = F.grid_sample(  # type: ignore\n        sigmas[0].log2().view(1, 1, 1, -1).expand(scale_grid.size(0), 1, 1, L),\n        scale_grid, align_corners=False)\n\n    # Replace the scale_x_y\n    out = torch.cat([torch.pow(2.0, scale_val).view(B, N, 1), max_coords[:, :, 1:]], dim=2)\n    return out\n\n\ndef _create_octave_mask(mask: torch.Tensor, octave_shape: List[int]) -> torch.Tensor:\n    """"""Downsamples a mask based on the given octave shape.""""""\n    mask_shape = octave_shape[-2:]\n    mask_octave = F.interpolate(mask, mask_shape, mode=\'bilinear\', align_corners=False)  # type: ignore\n    return mask_octave.unsqueeze(1)\n\n\nclass ScaleSpaceDetector(nn.Module):\n    """"""Module for differentiable local feature detection, as close as possible to classical\n     local feature detectors like Harris, Hessian-Affine or SIFT (DoG).\n     It has 5 modules inside: scale pyramid generator, response (""cornerness"") function,\n     soft nms function, affine shape estimator and patch orientation estimator.\n     Each of those modules could be replaced with learned custom one, as long, as\n     they respect output shape.\n\n    Args:\n        num_features: (int) Number of features to detect. default = 500. In order to keep everything batchable,\n                      output would always have num_features outputed, even for completely homogeneous images.\n        mr_size: (float), default 6.0. Multiplier for local feature scale compared to the detection scale.\n                    6.0 is matching OpenCV 12.0 convention for SIFT.\n        scale_pyr_module: (nn.Module), which generates scale pyramid.\n                         See :class:`~kornia.geometry.ScalePyramid` for details. Default is ScalePyramid(3, 1.6, 10)\n        resp_module: (nn.Module), which calculates \'cornerness\' of the pixel. Default is BlobHessian().\n        nms_module: (nn.Module), which outputs per-patch coordinates of the responce maxima.\n                    See :class:`~kornia.geometry.ConvSoftArgmax3d` for details.\n        ori_module: (nn.Module) for local feature orientation estimation.  Default is :class:`~kornia.feature.PassLAF`,\n                    which does nothing. See :class:`~kornia.feature.LAFOrienter` for details.\n        aff_module:  (nn.Module) for local feature affine shape estimation. Default is :class:`~kornia.feature.PassLAF`,\n                    which does nothing. See :class:`~kornia.feature.LAFAffineShapeEstimator` for details.\n        minima_are_also_good:  (bool) if True, then both response function minima and maxima are detected\n                                Useful for symmetric responce functions like DoG or Hessian. Default is False\n    """"""\n\n    def __init__(self,\n                 num_features: int = 500,\n                 mr_size: float = 6.0,\n                 scale_pyr_module: nn.Module = ScalePyramid(3, 1.6, 10),\n                 resp_module: nn.Module = BlobHessian(),\n                 nms_module: nn.Module = ConvSoftArgmax3d((3, 3, 3),\n                                                          (1, 1, 1),\n                                                          (1, 1, 1),\n                                                          normalized_coordinates=False,\n                                                          output_value=True),\n                 ori_module: nn.Module = PassLAF(),\n                 aff_module: nn.Module = PassLAF(),\n                 minima_are_also_good: bool = False):\n        super(ScaleSpaceDetector, self).__init__()\n        self.mr_size = mr_size\n        self.num_features = num_features\n        self.scale_pyr = scale_pyr_module\n        self.resp = resp_module\n        self.nms = nms_module\n        self.ori = ori_module\n        self.aff = aff_module\n        self.minima_are_also_good = minima_are_also_good\n        return\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\'\\\n            \'num_features=\' + str(self.num_features) + \', \' + \\\n            \'mr_size=\' + str(self.mr_size) + \', \' + \\\n            \'scale_pyr=\' + self.scale_pyr.__repr__() + \', \' + \\\n            \'resp=\' + self.resp.__repr__() + \', \' + \\\n            \'nms=\' + self.nms.__repr__() + \', \' + \\\n            \'ori=\' + self.ori.__repr__() + \', \' + \\\n            \'aff=\' + self.aff.__repr__() + \')\'\n\n    def detect(self, img: torch.Tensor, num_feats: int,\n               mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        dev: torch.device = img.device\n        dtype: torch.dtype = img.dtype\n        sp, sigmas, pix_dists = self.scale_pyr(img)\n        all_responses = []\n        all_lafs = []\n        for oct_idx, octave in enumerate(sp):\n            sigmas_oct = sigmas[oct_idx]\n            pix_dists_oct = pix_dists[oct_idx]\n\n            B, L, CH, H, W = octave.size()\n            # Run response function\n            oct_resp = self.resp(octave.view(B * L, CH, H, W), sigmas_oct.view(-1))\n\n            # We want nms for scale responses, so reorder to (B, CH, L, H, W)\n            oct_resp = oct_resp.view_as(octave).permute(0, 2, 1, 3, 4)\n\n            if mask is not None:\n                oct_mask: torch.Tensor = _create_octave_mask(mask, oct_resp.shape)\n                oct_resp = oct_mask * oct_resp\n\n            # Differentiable nms\n            coord_max, response_max = self.nms(oct_resp)\n            if self.minima_are_also_good:\n                coord_min, response_min = self.nms(-oct_resp)\n                take_min_mask = (response_min > response_max).to(response_max.dtype)\n                response_max = response_min * take_min_mask + (1 - take_min_mask) * response_max\n                coord_max = coord_min * take_min_mask.unsqueeze(1) + (1 - take_min_mask.unsqueeze(1)) * coord_max\n\n            # Now, lets crop out some small responses\n            responses_flatten = response_max.view(response_max.size(0), -1)  # [B, N]\n            max_coords_flatten = coord_max.view(response_max.size(0), 3, -1).permute(0, 2, 1)  # [B, N, 3]\n\n            if responses_flatten.size(1) > num_feats:\n                resp_flat_best, idxs = torch.topk(responses_flatten, k=num_feats, dim=1)\n                max_coords_best = torch.gather(max_coords_flatten, 1, idxs.unsqueeze(-1).repeat(1, 1, 3))\n            else:\n                resp_flat_best = responses_flatten\n                max_coords_best = max_coords_flatten\n            B, N = resp_flat_best.size()\n\n            # Converts scale level index from ConvSoftArgmax3d to the actual scale, using the sigmas\n            max_coords_best = _scale_index_to_scale(max_coords_best, sigmas_oct)\n\n            # Create local affine frames (LAFs)\n            rotmat = torch.eye(2, dtype=dtype, device=dev).view(1, 1, 2, 2)\n            current_lafs = torch.cat([self.mr_size * max_coords_best[:, :, 0].view(B, N, 1, 1) * rotmat,\n                                      max_coords_best[:, :, 1:3].view(B, N, 2, 1)], dim=3)\n\n            # Zero response lafs, which touch the boundary\n            good_mask = laf_is_inside_image(current_lafs, octave[:, 0])\n            resp_flat_best = resp_flat_best * good_mask.to(dev, dtype)\n\n            # Normalize LAFs\n            current_lafs = normalize_laf(current_lafs, octave[:, 0])  # We don`t need # of scale levels, only shape\n\n            all_responses.append(resp_flat_best)\n            all_lafs.append(current_lafs)\n\n        # Sort and keep best n\n        responses: torch.Tensor = torch.cat(all_responses, dim=1)\n        lafs: torch.Tensor = torch.cat(all_lafs, dim=1)\n        responses, idxs = torch.topk(responses, k=num_feats, dim=1)\n        lafs = torch.gather(lafs, 1, idxs.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, 2, 3))\n        return responses, denormalize_laf(lafs, img)\n\n    def forward(self, img: torch.Tensor,  # type: ignore\n                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:  # type: ignore\n        """"""Three stage local feature detection. First the location and scale of interest points are determined by\n        detect function. Then affine shape and orientation.\n\n        Args:\n            img (torch.Tensor): image to extract features with shape [BxCxHxW]\n            mask (torch.Tensor, optional): a mask with weights where to apply the\n            response function. The shae must same as the input image.\n\n        Returns:\n            lafs (torch.Tensor): shape [BxNx2x3]. Detected local affine frames.\n            responses (torch.Tensor): shape [BxNx1]. Response function values for corresponding lafs""""""\n        responses, lafs = self.detect(img, self.num_features, mask)\n        lafs = self.aff(lafs, img)\n        lafs = self.ori(lafs, img)\n        return lafs, responses\n'"
kornia/feature/siftdesc.py,28,"b'from typing import Tuple\nimport torch\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom kornia.filters import get_gaussian_kernel2d\nfrom kornia.filters import spatial_gradient\nfrom kornia.geometry.conversions import pi\n\n\ndef get_sift_pooling_kernel(ksize: int = 25) -> torch.Tensor:\n    """"""Returns a weighted pooling kernel for SIFT descriptor\n\n    Args:\n        ksize: (int): kernel_size\n\n    Returns:\n        torch.Tensor: kernel\n\n    Shape:\n        Output: :math: `(ksize,ksize)`\n    """"""\n    ks_2: float = float(ksize) / 2.0\n    xc2: torch.Tensor = ks_2 - (torch.arange(ksize).float() + 0.5 - ks_2).abs()  # type: ignore # noqa\n    kernel: torch.Tensor = torch.ger(xc2, xc2) / (ks_2**2)\n    return kernel\n\n\ndef get_sift_bin_ksize_stride_pad(patch_size: int,\n                                  num_spatial_bins: int) -> Tuple:\n    """"""Returns a tuple with SIFT parameters, given the patch size\n    and number of spatial bins.\n\n    Args:\n        patch_size: (int)\n        num_spatial_bins: (int)\n\n    Returns:\n        ksize, stride, pad: ints\n    """"""\n    ksize: int = 2 * int(patch_size / (num_spatial_bins + 1))\n    stride: int = patch_size // num_spatial_bins\n    pad: int = ksize // 4\n    return ksize, stride, pad\n\n\nclass SIFTDescriptor(nn.Module):\n    """"""\n    Module, which computes SIFT descriptors of given patches\n\n    Args:\n        patch_size: (int) Input patch size in pixels (41 is default)\n        num_ang_bins: (int) Number of angular bins. (8 is default)\n        num_spatial_bins: (int) Number of spatial bins (4 is default)\n        clipval: (float) default 0.2\n        rootsift: (bool) if True, RootSIFT (Arandjelovi\xc4\x87 et. al, 2012)\n        is computed\n\n    Returns:\n        Tensor: SIFT descriptor of the patches\n\n    Shape:\n        - Input: (B, 1, num_spatial_bins, num_spatial_bins)\n        - Output: (B, num_ang_bins * num_spatial_bins ** 2)\n\n    Examples::\n        >>> input = torch.rand(23, 1, 32, 32)\n        >>> SIFT = kornia.SIFTDescriptor(32, 8, 4)\n        >>> descs = SIFT(input) # 23x128\n    """"""\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ +\\\n            \'(\' + \'num_ang_bins=\' + str(self.num_ang_bins) +\\\n            \', \' + \'num_spatial_bins=\' + str(self.num_spatial_bins) +\\\n            \', \' + \'patch_size=\' + str(self.patch_size) +\\\n            \', \' + \'rootsift=\' + str(self.rootsift) +\\\n            \', \' + \'clipval=\' + str(self.clipval) + \')\'\n\n    def __init__(self,\n                 patch_size: int = 41,\n                 num_ang_bins: int = 8,\n                 num_spatial_bins: int = 4,\n                 rootsift: bool = True,\n                 clipval: float = 0.2,\n                 ) -> None:\n        super(SIFTDescriptor, self).__init__()\n        self.eps = 1e-10\n        self.num_ang_bins = num_ang_bins\n        self.num_spatial_bins = num_spatial_bins\n        self.clipval = clipval\n        self.rootsift = rootsift\n        self.patch_size = patch_size\n\n        ks: int = self.patch_size\n        sigma: float = float(ks) / math.sqrt(2.0)\n        self.gk = get_gaussian_kernel2d((ks, ks), (sigma, sigma), True)\n\n        (self.bin_ksize,\n         self.bin_stride,\n         self.pad) = get_sift_bin_ksize_stride_pad(patch_size,\n                                                   num_spatial_bins)\n\n        nw = get_sift_pooling_kernel(ksize=self.bin_ksize).float()\n        self.pk = nn.Conv2d(1, 1, kernel_size=(nw.size(0), nw.size(1)),\n                            stride=(self.bin_stride, self.bin_stride),\n                            padding=(self.pad, self.pad),\n                            bias=False)\n        self.pk.weight.data.copy_(nw.reshape(1, 1, nw.size(0), nw.size(1)))  # type: ignore  # noqa\n        return\n\n    def get_pooling_kernel(self) -> torch.Tensor:\n        return self.pk.weight.detach()\n\n    def get_weighting_kernel(self) -> torch.Tensor:\n        return self.gk.detach()\n\n    def forward(self, input):\n        if not torch.is_tensor(input):\n            raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                            .format(type(input)))\n        if not len(input.shape) == 4:\n            raise ValueError(""Invalid input shape, we expect Bx1xHxW. Got: {}""\n                             .format(input.shape))\n        B, CH, W, H = input.size()\n        if (W != self.patch_size) or (H != self.patch_size) or (CH != 1):\n            raise TypeError(\n                ""input shape should be must be [Bx1x{}x{}]. ""\n                ""Got {}"".format(self.patch_size, self.patch_size, input.size()))\n        self.pk = self.pk.to(input.dtype).to(input.device)\n\n        grads: torch.Tensor = spatial_gradient(input, \'diff\')\n        # unpack the edges\n        gx: torch.Tensor = grads[:, :, 0]\n        gy: torch.Tensor = grads[:, :, 1]\n\n        mag: torch.Tensor = torch.sqrt(gx * gx + gy * gy + self.eps)\n        ori: torch.Tensor = torch.atan2(gy, gx + self.eps) + 2.0 * pi\n        mag = mag * self.gk.expand_as(mag).type_as(mag).to(mag.device)\n        o_big: torch.Tensor = float(self.num_ang_bins) * ori / (2.0 * pi)\n\n        bo0_big_: torch.Tensor = torch.floor(o_big)\n        wo1_big_: torch.Tensor = (o_big - bo0_big_)\n        bo0_big: torch.Tensor = bo0_big_ % self.num_ang_bins\n        bo1_big: torch.Tensor = (bo0_big + 1) % self.num_ang_bins\n        wo0_big: torch.Tensor = (1.0 - wo1_big_) * mag  # type: ignore\n        wo1_big: torch.Tensor = wo1_big_ * mag\n\n        ang_bins = []\n        for i in range(0, self.num_ang_bins):\n            out = self.pk((bo0_big == i).to(input.dtype) * wo0_big +  # noqa\n                          (bo1_big == i).to(input.dtype) * wo1_big)\n            ang_bins.append(out)\n        ang_bins = torch.cat(ang_bins, dim=1)\n        ang_bins = ang_bins.view(B, -1)\n        ang_bins = F.normalize(ang_bins, p=2)\n        ang_bins = torch.clamp(ang_bins, 0., float(self.clipval))\n        ang_bins = F.normalize(ang_bins, p=2)\n        if self.rootsift:\n            ang_bins = torch.sqrt(F.normalize(ang_bins, p=1) + self.eps)\n        return ang_bins\n\n\ndef sift_describe(input: torch.Tensor,\n                  patch_size: int = 41,\n                  num_ang_bins: int = 8,\n                  num_spatial_bins: int = 4,\n                  rootsift: bool = True,\n                  clipval: float = 0.2,\n                  ) -> torch.Tensor:\n    r""""""Computes the sift descriptor.\n\n    See :class:`~kornia.feature.SIFTDescriptor` for details.\n    """"""\n\n    return SIFTDescriptor(patch_size,\n                          num_ang_bins,\n                          num_spatial_bins,\n                          rootsift,\n                          clipval)(input)\n'"
kornia/feature/sosnet.py,5,"b'from typing import Dict\n\nimport torch\nimport torch.nn as nn\n\nurls: Dict[str, str] = dict()\nurls[""lib""] = ""https://github.com/yuruntian/SOSNet/tree/master/sosnet-weights/sosnet_32x32_liberty.pth""\nurls[""hp_a""] = ""https://github.com/yuruntian/SOSNet/tree/master/sosnet-weights/sosnet_32x32_hpatches_a.pth""\n\n\nclass SOSNet(nn.Module):\n    """"""\n    128-dimensional SOSNet model definition for 32x32 patches.\n    This is based on the original code from paper\n    ""SOSNet:Second Order Similarity Regularization for Local Descriptor Learning"".\n    Args:\n        pretrained: (bool) Download and set pretrained weights to the model. Default: false.\n    Returns:\n        torch.Tensor: SOSNet descriptor of the patches.\n    Shape:\n        - Input: (B, 1, 32, 32)\n        - Output: (B, 128)\n    Examples:\n        >>> input = torch.rand(8, 1, 32, 32)\n        >>> sosnet = kornia.feature.SOSNet()\n        >>> descs = sosnet(input) # 8x128\n    """"""\n\n    def __init__(self, pretrained: bool = False) -> None:\n        super(SOSNet, self).__init__()\n\n        self.layers = nn.Sequential(\n            nn.InstanceNorm2d(1, affine=False),\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(128, 128, kernel_size=8, bias=False),\n            nn.BatchNorm2d(128, affine=False),\n        )\n        self.desc_norm = nn.Sequential(\n            nn.LocalResponseNorm(256, alpha=256, beta=0.5, k=0)\n        )\n        # load pretrained model\n        if pretrained:\n            pretrained_dict = torch.hub.load_state_dict_from_url(\n                urls[\'lib\'], map_location=lambda storage, loc: storage\n            )\n            self.load_state_dict(pretrained_dict[\'state_dict\'], strict=True)\n\n        return\n\n    def forward(self, input: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:   # type: ignore\n        descr = self.desc_norm(self.layers(input) + eps)\n        descr = descr.view(descr.size(0), -1)\n        return descr\n'"
kornia/filters/__init__.py,0,"b'from .gaussian import GaussianBlur2d, gaussian_blur2d\nfrom .laplacian import Laplacian, laplacian\nfrom .sobel import SpatialGradient, spatial_gradient, SpatialGradient3d, spatial_gradient3d\nfrom .sobel import Sobel, sobel\nfrom .blur import BoxBlur, box_blur\nfrom .median import MedianBlur, median_blur\nfrom .motion import MotionBlur, motion_blur\nfrom .filter import filter2D\nfrom .kernels import (\n    gaussian,\n    laplacian_1d,\n    get_box_kernel2d,\n    get_binary_kernel2d,\n    get_sobel_kernel2d,\n    get_diff_kernel2d,\n    get_spatial_gradient_kernel2d,\n    get_gaussian_kernel1d,\n    get_gaussian_kernel2d,\n    get_laplacian_kernel1d,\n    get_laplacian_kernel2d,\n    get_motion_kernel2d,\n    get_spatial_gradient_kernel3d,\n)\n\n__all__ = [\n    ""get_gaussian_kernel1d"",\n    ""get_gaussian_kernel2d"",\n    ""get_laplacian_kernel1d"",\n    ""get_laplacian_kernel2d"",\n    ""get_spatial_gradient_kernel2d"",\n    ""get_spatial_gradient_kernel3d"",\n    ""get_sobel_kernel2d"",\n    ""get_diff_kernel2d"",\n    ""gaussian_blur2d"",\n    ""get_motion_kernel2d"",\n    ""laplacian"",\n    ""sobel"",\n    ""spatial_gradient"",\n    ""box_blur"",\n    ""median_blur"",\n    ""motion_blur"",\n    ""filter2D"",\n    ""GaussianBlur2d"",\n    ""Laplacian"",\n    ""SpatialGradient"",\n    ""Sobel"",\n    ""BoxBlur"",\n    ""MedianBlur"",\n    ""MotionBlur"",\n    ""SpatialGradient3d"",\n    ""spatial_gradient3d"",\n]\n'"
kornia/filters/blur.py,7,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nimport kornia\nfrom kornia.filters.kernels import get_box_kernel2d\nfrom kornia.filters.kernels import normalize_kernel2d\n\n\nclass BoxBlur(nn.Module):\n    r""""""Blurs an image using the box filter.\n\n    The function smooths an image using the kernel:\n\n    .. math::\n        K = \\frac{1}{\\text{kernel_size}_x * \\text{kernel_size}_y}\n        \\begin{bmatrix}\n            1 & 1 & 1 & \\cdots & 1 & 1 \\\\\n            1 & 1 & 1 & \\cdots & 1 & 1 \\\\\n            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n            1 & 1 & 1 & \\cdots & 1 & 1 \\\\\n        \\end{bmatrix}\n\n    Args:\n        kernel_size (Tuple[int, int]): the blurring kernel size.\n        border_type (str): the padding mode to be applied before convolving.\n          The expected modes are: ``\'constant\'``, ``\'reflect\'``,\n          ``\'replicate\'`` or ``\'circular\'``. Default: ``\'reflect\'``.\n        normalized (bool): if True, L1 norm of the kernel is set to 1.\n\n    Returns:\n        torch.Tensor: the blurred input tensor.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H, W)`\n\n    Example:\n        >>> input = torch.rand(2, 4, 5, 7)\n        >>> blur = kornia.filters.BoxBlur((3, 3))\n        >>> output = blur(input)  # 2x4x5x7\n    """"""\n\n    def __init__(self, kernel_size: Tuple[int, int],\n                 border_type: str = \'reflect\',\n                 normalized: bool = True) -> None:\n        super(BoxBlur, self).__init__()\n        self.kernel_size: Tuple[int, int] = kernel_size\n        self.border_type: str = border_type\n        self.kernel: torch.Tensor = get_box_kernel2d(kernel_size)\n        self.normalized: bool = normalized\n        if self.normalized:\n            self.kernel = normalize_kernel2d(self.kernel)\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ +\\\n            \'(kernel_size=\' + str(self.kernel_size) + \', \' +\\\n            \'normalized=\' + str(self.normalized) + \', \' + \\\n            \'border_type=\' + self.border_type + \')\'\n\n    def forward(self, input: torch.Tensor):  # type: ignore\n        return kornia.filter2D(input, self.kernel, self.border_type)\n\n\n# functiona api\n# TODO: In terms of functional API, there should not be any initialization of an nn.Module.\n#       This logic is reversed.\n\ndef box_blur(input: torch.Tensor,\n             kernel_size: Tuple[int, int],\n             border_type: str = \'reflect\',\n             normalized: bool = True) -> torch.Tensor:\n    r""""""Blurs an image using the box filter.\n\n    See :class:`~kornia.filters.BoxBlur` for details.\n    """"""\n    return BoxBlur(kernel_size, border_type, normalized)(input)\n'"
kornia/filters/filter.py,14,"b'from typing import Tuple, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom kornia.filters.kernels import normalize_kernel2d\n\n\ndef compute_padding(kernel_size: Tuple[int, int]) -> List[int]:\n    """"""Computes padding tuple.""""""\n    # 4 ints:  (padding_left, padding_right,padding_top,padding_bottom)\n    # https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad\n    assert len(kernel_size) == 2, kernel_size\n    computed = [k // 2 for k in kernel_size]\n\n    # for even kernels we need to do asymetric padding :(\n    return [computed[1] - 1 if kernel_size[0] % 2 == 0 else computed[1],\n            computed[1],\n            computed[0] - 1 if kernel_size[1] % 2 == 0 else computed[0],\n            computed[0]]\n\n\ndef filter2D(input: torch.Tensor, kernel: torch.Tensor,\n             border_type: str = \'reflect\',\n             normalized: bool = False) -> torch.Tensor:\n    r""""""Function that convolves a tensor with a kernel.\n\n    The function applies a given kernel to a tensor. The kernel is applied\n    independently at each depth channel of the tensor. Before applying the\n    kernel, the function applies padding according to the specified mode so\n    that the output remains in the same shape.\n\n    Args:\n        input (torch.Tensor): the input tensor with shape of\n          :math:`(B, C, H, W)`.\n        kernel (torch.Tensor): the kernel to be convolved with the input\n          tensor. The kernel shape must be :math:`(1, kH, kW)`.\n        border_type (str): the padding mode to be applied before convolving.\n          The expected modes are: ``\'constant\'``, ``\'reflect\'``,\n          ``\'replicate\'`` or ``\'circular\'``. Default: ``\'reflect\'``.\n        normalized (bool): If True, kernel will be L1 normalized.\n\n    Return:\n        torch.Tensor: the convolved tensor of same size and numbers of channels\n        as the input.\n    """"""\n    if not isinstance(input, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(input)))\n\n    if not isinstance(kernel, torch.Tensor):\n        raise TypeError(""Input kernel type is not a torch.Tensor. Got {}""\n                        .format(type(kernel)))\n\n    if not isinstance(border_type, str):\n        raise TypeError(""Input border_type is not string. Got {}""\n                        .format(type(kernel)))\n\n    if not len(input.shape) == 4:\n        raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                         .format(input.shape))\n\n    if not len(kernel.shape) == 3:\n        raise ValueError(""Invalid kernel shape, we expect 1xHxW. Got: {}""\n                         .format(kernel.shape))\n\n    borders_list: List[str] = [\'constant\', \'reflect\', \'replicate\', \'circular\']\n    if border_type not in borders_list:\n        raise ValueError(""Invalid border_type, we expect the following: {0}.""\n                         ""Got: {1}"".format(borders_list, border_type))\n\n    # prepare kernel\n    b, c, h, w = input.shape\n    tmp_kernel: torch.Tensor = kernel.unsqueeze(0).to(input.device).to(input.dtype)\n    if normalized:\n        tmp_kernel = normalize_kernel2d(tmp_kernel)\n    # pad the input tensor\n    height, width = tmp_kernel.shape[-2:]\n    padding_shape: List[int] = compute_padding((height, width))\n    input_pad: torch.Tensor = F.pad(input, padding_shape, mode=border_type)\n    b, c, hp, wp = input_pad.shape\n    # convolve the tensor with the kernel. Pick the fastest alg\n    kernel_numel: int = height * width\n    if kernel_numel > 81:\n        return F.conv2d(input_pad.reshape(b * c, 1, hp, wp), tmp_kernel, padding=0, stride=1).view(b, c, h, w)\n    return F.conv2d(input_pad, tmp_kernel.expand(c, -1, -1, -1), groups=c, padding=0, stride=1)\n'"
kornia/filters/gaussian.py,6,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nimport kornia\nfrom kornia.filters.kernels import get_gaussian_kernel2d\n\n\nclass GaussianBlur2d(nn.Module):\n    r""""""Creates an operator that blurs a tensor using a Gaussian filter.\n\n    The operator smooths the given tensor with a gaussian kernel by convolving\n    it to each channel. It suports batched operation.\n\n    Arguments:\n        kernel_size (Tuple[int, int]): the size of the kernel.\n        sigma (Tuple[float, float]): the standard deviation of the kernel.\n        border_type (str): the padding mode to be applied before convolving.\n          The expected modes are: ``\'constant\'``, ``\'reflect\'``,\n          ``\'replicate\'`` or ``\'circular\'``. Default: ``\'reflect\'``.\n\n    Returns:\n        Tensor: the blurred tensor.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H, W)`\n\n    Examples::\n\n        >>> input = torch.rand(2, 4, 5, 5)\n        >>> gauss = kornia.filters.GaussianBlur((3, 3), (1.5, 1.5))\n        >>> output = gauss(input)  # 2x4x5x5\n    """"""\n\n    def __init__(self, kernel_size: Tuple[int, int],\n                 sigma: Tuple[float, float],\n                 border_type: str = \'reflect\') -> None:\n        super(GaussianBlur2d, self).__init__()\n        self.kernel_size: Tuple[int, int] = kernel_size\n        self.sigma: Tuple[float, float] = sigma\n        self.kernel: torch.Tensor = torch.unsqueeze(\n            get_gaussian_kernel2d(kernel_size, sigma), dim=0)\n\n        assert border_type in [""constant"", ""reflect"", ""replicate"", ""circular""]\n        self.border_type = border_type\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ +\\\n            \'(kernel_size=\' + str(self.kernel_size) + \', \' +\\\n            \'sigma=\' + str(self.sigma) + \', \' +\\\n            \'border_type=\' + self.border_type + \')\'\n\n    def forward(self, x: torch.Tensor):  # type: ignore\n        return kornia.filter2D(x, self.kernel, self.border_type)\n\n\n######################\n# functional interface\n######################\n# TODO: In terms of functional API, there should not be any initialization of an nn.Module.\n#       This logic is reversed.\n\ndef gaussian_blur2d(\n        input: torch.Tensor,\n        kernel_size: Tuple[int, int],\n        sigma: Tuple[float, float],\n        border_type: str = \'reflect\') -> torch.Tensor:\n    r""""""Function that blurs a tensor using a Gaussian filter.\n\n    See :class:`~kornia.filters.GaussianBlur` for details.\n    """"""\n    return GaussianBlur2d(kernel_size, sigma, border_type)(input)\n'"
kornia/filters/kernels.py,63,"b'from typing import Tuple, List\n\nimport torch\nimport torch.nn as nn\n\nfrom kornia.geometry.transform.affwarp import rotate\n\n\ndef normalize_kernel2d(input: torch.Tensor) -> torch.Tensor:\n    r""""""Normalizes both derivative and smoothing kernel.\n    """"""\n    if len(input.size()) < 2:\n        raise TypeError(""input should be at least 2D tensor. Got {}""\n                        .format(input.size()))\n    norm: torch.Tensor = input.abs().sum(dim=-1).sum(dim=-1)\n    return input / (norm.unsqueeze(-1).unsqueeze(-1))\n\n\ndef gaussian(window_size, sigma):\n    x = torch.arange(window_size).float() - window_size // 2\n    if window_size % 2 == 0:\n        x = x + 0.5\n    gauss = torch.exp((-x.pow(2.0) / float(2 * sigma ** 2)))\n    return gauss / gauss.sum()\n\n\ndef laplacian_1d(window_size) -> torch.Tensor:\n    r""""""One could also use the Laplacian of Gaussian formula\n        to design the filter.\n    """"""\n\n    filter_1d = torch.ones(window_size)\n    filter_1d[window_size // 2] = 1 - window_size\n    laplacian_1d: torch.Tensor = filter_1d\n    return laplacian_1d\n\n\ndef get_box_kernel2d(kernel_size: Tuple[int, int]) -> torch.Tensor:\n    r""""""Utility function that returns a box filter.""""""\n    kx: float = float(kernel_size[0])\n    ky: float = float(kernel_size[1])\n    scale: torch.Tensor = torch.tensor(1.) / torch.tensor([kx * ky])\n    tmp_kernel: torch.Tensor = torch.ones(1, kernel_size[0], kernel_size[1])\n    return scale.to(tmp_kernel.dtype) * tmp_kernel\n\n\ndef get_binary_kernel2d(window_size: Tuple[int, int]) -> torch.Tensor:\n    r""""""Creates a binary kernel to extract the patches. If the window size\n    is HxW will create a (H*W)xHxW kernel.\n    """"""\n    window_range: int = window_size[0] * window_size[1]\n    kernel: torch.Tensor = torch.zeros(window_range, window_range)\n    for i in range(window_range):\n        kernel[i, i] += 1.0\n    return kernel.view(window_range, 1, window_size[0], window_size[1])\n\n\ndef get_sobel_kernel_3x3() -> torch.Tensor:\n    """"""Utility function that returns a sobel kernel of 3x3""""""\n    return torch.tensor([\n        [-1., 0., 1.],\n        [-2., 0., 2.],\n        [-1., 0., 1.],\n    ])\n\n\ndef get_sobel_kernel_5x5_2nd_order() -> torch.Tensor:\n    """"""Utility function that returns a 2nd order sobel kernel of 5x5""""""\n    return torch.tensor([\n        [-1., 0., 2., 0., -1.],\n        [-4., 0., 8., 0., -4.],\n        [-6., 0., 12., 0., -6.],\n        [-4., 0., 8., 0., -4.],\n        [-1., 0., 2., 0., -1.]\n    ])\n\n\ndef _get_sobel_kernel_5x5_2nd_order_xy() -> torch.Tensor:\n    """"""Utility function that returns a 2nd order sobel kernel of 5x5""""""\n    return torch.tensor([\n        [-1., -2., 0., 2., 1.],\n        [-2., -4., 0., 4., 2.],\n        [0., 0., 0., 0., 0.],\n        [2., 4., 0., -4., -2.],\n        [1., 2., 0., -2., -1.]\n    ])\n\n\ndef get_diff_kernel_3x3() -> torch.Tensor:\n    """"""Utility function that returns a sobel kernel of 3x3""""""\n    return torch.tensor([\n        [-0., 0., 0.],\n        [-1., 0., 1.],\n        [-0., 0., 0.],\n    ])\n\n\ndef get_diff_kernel3d(device=torch.device(\'cpu\'), dtype=torch.float) -> torch.Tensor:\n    """"""Utility function that returns a first order derivative kernel of 3x3x3""""""\n    kernel: torch.Tensor = torch.tensor([[[[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [-0.5, 0.0, 0.5],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n                                          ],\n                                         [[[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, -0.5, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.5, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n                                          ],\n                                         [[[0.0, 0.0, 0.0],\n                                           [0.0, -0.5, 0.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [0.0, 0.5, 0.0],\n                                           [0.0, 0.0, 0.0]],\n                                          ],\n                                         ], device=device, dtype=dtype)\n    return kernel.unsqueeze(1)\n\n\ndef get_diff_kernel3d_2nd_order(device=torch.device(\'cpu\'), dtype=torch.float) -> torch.Tensor:\n    """"""Utility function that returns a first order derivative kernel of 3x3x3""""""\n    kernel: torch.Tensor = torch.tensor([[[[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [1.0, -2.0, 1.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n                                          ],\n                                         [[[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, 1.0, 0.0],\n                                           [0.0, -2.0, 0.0],\n                                           [0.0, 1.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n                                          ],\n                                         [[[0.0, 0.0, 0.0],\n                                           [0.0, 1.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [0.0, -2.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [0.0, 1.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n                                          ],\n                                         [[[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[1.0, 0.0, -1.0],\n                                           [0.0, 0.0, 0.0],\n                                           [-1.0, 0.0, 1.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n                                          ],\n                                         [[[0.0, 1.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, -1.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, -1.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 1.0, 0.0]],\n                                          ],\n                                         [[[0.0, 0.0, 0.0],\n                                           [1.0, 0.0, -1.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0],\n                                           [0.0, 0.0, 0.0]],\n\n                                          [[0.0, 0.0, 0.0],\n                                           [-1.0, 0.0, 1.0],\n                                           [0.0, 0.0, 0.0]],\n                                          ],\n                                         ], device=device, dtype=dtype)\n    return kernel.unsqueeze(1)\n\n\ndef get_sobel_kernel2d() -> torch.Tensor:\n    kernel_x: torch.Tensor = get_sobel_kernel_3x3()\n    kernel_y: torch.Tensor = kernel_x.transpose(0, 1)\n    return torch.stack([kernel_x, kernel_y])\n\n\ndef get_diff_kernel2d() -> torch.Tensor:\n    kernel_x: torch.Tensor = get_diff_kernel_3x3()\n    kernel_y: torch.Tensor = kernel_x.transpose(0, 1)\n    return torch.stack([kernel_x, kernel_y])\n\n\ndef get_sobel_kernel2d_2nd_order() -> torch.Tensor:\n    gxx: torch.Tensor = get_sobel_kernel_5x5_2nd_order()\n    gyy: torch.Tensor = gxx.transpose(0, 1)\n    gxy: torch.Tensor = _get_sobel_kernel_5x5_2nd_order_xy()\n    return torch.stack([gxx, gxy, gyy])\n\n\ndef get_diff_kernel2d_2nd_order() -> torch.Tensor:\n    gxx: torch.Tensor = torch.tensor([\n        [0., 0., 0.],\n        [1., -2., 1.],\n        [0., 0., 0.],\n    ])\n    gyy: torch.Tensor = gxx.transpose(0, 1)\n    gxy: torch.Tensor = torch.tensor([\n        [-1., 0., 1.],\n        [0., 0., 0.],\n        [1., 0., -1.],\n    ])\n    return torch.stack([gxx, gxy, gyy])\n\n\ndef get_spatial_gradient_kernel2d(mode: str, order: int) -> torch.Tensor:\n    r""""""Function that returns kernel for 1st or 2nd order image gradients,\n    using one of the following operators: sobel, diff""""""\n    if mode not in [\'sobel\', \'diff\']:\n        raise TypeError(""mode should be either sobel\\\n                         or diff. Got {}"".format(mode))\n    if order not in [1, 2]:\n        raise TypeError(""order should be either 1 or 2\\\n                         Got {}"".format(order))\n    if mode == \'sobel\' and order == 1:\n        kernel: torch.Tensor = get_sobel_kernel2d()\n    elif mode == \'sobel\' and order == 2:\n        kernel = get_sobel_kernel2d_2nd_order()\n    elif mode == \'diff\' and order == 1:\n        kernel = get_diff_kernel2d()\n    elif mode == \'diff\' and order == 2:\n        kernel = get_diff_kernel2d_2nd_order()\n    else:\n        raise NotImplementedError("""")\n    return kernel\n\n\ndef get_spatial_gradient_kernel3d(mode: str, order: int, device=torch.device(\'cpu\'), dtype=torch.float) -> torch.Tensor:\n    r""""""Function that returns kernel for 1st or 2nd order scale pyramid gradients,\n    using one of the following operators: sobel, diff""""""\n    if mode not in [\'sobel\', \'diff\']:\n        raise TypeError(""mode should be either sobel\\\n                         or diff. Got {}"".format(mode))\n    if order not in [1, 2]:\n        raise TypeError(""order should be either 1 or 2\\\n                         Got {}"".format(order))\n    if mode == \'sobel\':\n        raise NotImplementedError(""Sobel kernel for 3d gradient is not implemented yet"")\n    elif mode == \'diff\' and order == 1:\n        kernel = get_diff_kernel3d(device, dtype)\n    elif mode == \'diff\' and order == 2:\n        kernel = get_diff_kernel3d_2nd_order(device, dtype)\n    else:\n        raise NotImplementedError("""")\n    return kernel\n\n\ndef get_gaussian_kernel1d(kernel_size: int,\n                          sigma: float,\n                          force_even: bool = False) -> torch.Tensor:\n    r""""""Function that returns Gaussian filter coefficients.\n\n    Args:\n        kernel_size (int): filter size. It should be odd and positive.\n        sigma (float): gaussian standard deviation.\n        force_even (bool): overrides requirement for odd kernel size.\n\n    Returns:\n        Tensor: 1D tensor with gaussian filter coefficients.\n\n    Shape:\n        - Output: :math:`(\\text{kernel_size})`\n\n    Examples::\n\n        >>> kornia.image.get_gaussian_kernel(3, 2.5)\n        tensor([0.3243, 0.3513, 0.3243])\n\n        >>> kornia.image.get_gaussian_kernel(5, 1.5)\n        tensor([0.1201, 0.2339, 0.2921, 0.2339, 0.1201])\n    """"""\n    if (not isinstance(kernel_size, int) or (\n            (kernel_size % 2 == 0) and not force_even) or (\n            kernel_size <= 0)):\n        raise TypeError(\n            ""kernel_size must be an odd positive integer. ""\n            ""Got {}"".format(kernel_size)\n        )\n    window_1d: torch.Tensor = gaussian(kernel_size, sigma)\n    return window_1d\n\n\ndef get_gaussian_kernel2d(\n        kernel_size: Tuple[int, int],\n        sigma: Tuple[float, float],\n        force_even: bool = False) -> torch.Tensor:\n    r""""""Function that returns Gaussian filter matrix coefficients.\n\n    Args:\n        kernel_size (Tuple[int, int]): filter sizes in the x and y direction.\n         Sizes should be odd and positive.\n        sigma (Tuple[int, int]): gaussian standard deviation in the x and y\n         direction.\n        force_even (bool): overrides requirement for odd kernel size.\n\n    Returns:\n        Tensor: 2D tensor with gaussian filter matrix coefficients.\n\n    Shape:\n        - Output: :math:`(\\text{kernel_size}_x, \\text{kernel_size}_y)`\n\n    Examples::\n\n        >>> kornia.image.get_gaussian_kernel2d((3, 3), (1.5, 1.5))\n        tensor([[0.0947, 0.1183, 0.0947],\n                [0.1183, 0.1478, 0.1183],\n                [0.0947, 0.1183, 0.0947]])\n\n        >>> kornia.image.get_gaussian_kernel2d((3, 5), (1.5, 1.5))\n        tensor([[0.0370, 0.0720, 0.0899, 0.0720, 0.0370],\n                [0.0462, 0.0899, 0.1123, 0.0899, 0.0462],\n                [0.0370, 0.0720, 0.0899, 0.0720, 0.0370]])\n    """"""\n    if not isinstance(kernel_size, tuple) or len(kernel_size) != 2:\n        raise TypeError(\n            ""kernel_size must be a tuple of length two. Got {}"".format(\n                kernel_size\n            )\n        )\n    if not isinstance(sigma, tuple) or len(sigma) != 2:\n        raise TypeError(\n            ""sigma must be a tuple of length two. Got {}"".format(sigma)\n        )\n    ksize_x, ksize_y = kernel_size\n    sigma_x, sigma_y = sigma\n    kernel_x: torch.Tensor = get_gaussian_kernel1d(ksize_x, sigma_x, force_even)\n    kernel_y: torch.Tensor = get_gaussian_kernel1d(ksize_y, sigma_y, force_even)\n    kernel_2d: torch.Tensor = torch.matmul(\n        kernel_x.unsqueeze(-1), kernel_y.unsqueeze(-1).t()\n    )\n    return kernel_2d\n\n\ndef get_laplacian_kernel1d(kernel_size: int) -> torch.Tensor:\n    r""""""Function that returns the coefficients of a 1D Laplacian filter.\n\n    Args:\n        kernel_size (int): filter size. It should be odd and positive.\n\n    Returns:\n        Tensor (float): 1D tensor with laplacian filter coefficients.\n\n    Shape:\n        - Output: math:`(\\text{kernel_size})`\n\n    Examples::\n        >>> kornia.image.get_laplacian_kernel(3)\n        tensor([ 1., -2.,  1.])\n\n        >>> kornia.image.get_laplacian_kernel(5)\n        tensor([ 1.,  1., -4.,  1.,  1.])\n\n    """"""\n    if not isinstance(kernel_size, int) or kernel_size % 2 == 0 or \\\n            kernel_size <= 0:\n        raise TypeError(""ksize must be an odd positive integer. Got {}""\n                        .format(kernel_size))\n    window_1d: torch.Tensor = laplacian_1d(kernel_size)\n    return window_1d\n\n\ndef get_laplacian_kernel2d(kernel_size: int) -> torch.Tensor:\n    r""""""Function that returns Gaussian filter matrix coefficients.\n\n    Args:\n        kernel_size (int): filter size should be odd.\n\n    Returns:\n        Tensor: 2D tensor with laplacian filter matrix coefficients.\n\n    Shape:\n        - Output: :math:`(\\text{kernel_size}_x, \\text{kernel_size}_y)`\n\n    Examples::\n\n        >>> kornia.image.get_laplacian_kernel2d(3)\n        tensor([[ 1.,  1.,  1.],\n                [ 1., -8.,  1.],\n                [ 1.,  1.,  1.]])\n\n        >>> kornia.image.get_laplacian_kernel2d(5)\n        tensor([[  1.,   1.,   1.,   1.,   1.],\n                [  1.,   1.,   1.,   1.,   1.],\n                [  1.,   1., -24.,   1.,   1.],\n                [  1.,   1.,   1.,   1.,   1.],\n                [  1.,   1.,   1.,   1.,   1.]])\n\n    """"""\n    if not isinstance(kernel_size, int) or kernel_size % 2 == 0 or \\\n            kernel_size <= 0:\n        raise TypeError(""ksize must be an odd positive integer. Got {}""\n                        .format(kernel_size))\n\n    kernel = torch.ones((kernel_size, kernel_size))\n    mid = kernel_size // 2\n    kernel[mid, mid] = 1 - kernel_size ** 2\n    kernel_2d: torch.Tensor = kernel\n    return kernel_2d\n\n\ndef get_motion_kernel2d(kernel_size: int, angle: float, direction: float = 0.) -> torch.Tensor:\n    r""""""Function that returns motion blur filter.\n\n    Args:\n        kernel_size (int): motion kernel width and height. It should be odd and positive.\n        angle (float): angle of the motion blur in degrees (anti-clockwise rotation).\n        direction (float): forward/backward direction of the motion blur.\n            Lower values towards -1.0 will point the motion blur towards the back (with angle provided via angle),\n            while higher values towards 1.0 will point the motion blur forward. A value of 0.0 leads to a\n            uniformly (but still angled) motion blur.\n\n    Returns:\n        torch.Tensor: the motion blur kernel.\n\n    Shape:\n        - Output: :math:`(ksize, ksize)`\n\n    Examples::\n        >>> kornia.filters.get_motion_kernel2d(5, 0., 0.)\n        tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n                [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n        >>> kornia.filters.get_motion_kernel2d(3, 215., -0.5)\n            tensor([[0.0000, 0.0412, 0.0732],\n                    [0.1920, 0.3194, 0.0804],\n                    [0.2195, 0.0743, 0.0000]])\n    """"""\n    if not isinstance(kernel_size, int) or kernel_size % 2 == 0 or kernel_size < 3:\n        raise TypeError(""ksize must be an odd integer >= than 3"")\n\n    if not isinstance(angle, float):\n        raise TypeError(""angle must be a float"")\n\n    if not isinstance(direction, float):\n        raise TypeError(""direction must be a float"")\n\n    kernel_tuple: Tuple[int, int] = (kernel_size, kernel_size)\n    # direction from [-1, 1] to [0, 1] range\n    direction = (torch.clamp(torch.tensor(direction), -1., 1.).item() + 1.) / 2.\n    kernel = torch.zeros(kernel_tuple, dtype=torch.float)\n    kernel[kernel_tuple[0] // 2, :] = torch.linspace(direction, 1. - direction, steps=kernel_tuple[0])\n    kernel = kernel.unsqueeze(0).unsqueeze(0)\n    # rotate (counterclockwise) kernel by given angle\n    kernel = rotate(kernel, torch.tensor(angle))\n    kernel = kernel[0][0]\n    kernel /= kernel.sum()\n    return kernel\n'"
kornia/filters/laplacian.py,6,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nimport kornia\nfrom kornia.filters.kernels import get_laplacian_kernel2d\nfrom kornia.filters.kernels import normalize_kernel2d\n\n\nclass Laplacian(nn.Module):\n    r""""""Creates an operator that returns a tensor using a Laplacian filter.\n\n    The operator smooths the given tensor with a laplacian kernel by convolving\n    it to each channel. It supports batched operation.\n\n    Arguments:\n        kernel_size (int): the size of the kernel.\n        border_type (str): the padding mode to be applied before convolving.\n          The expected modes are: ``\'constant\'``, ``\'reflect\'``,\n          ``\'replicate\'`` or ``\'circular\'``. Default: ``\'reflect\'``.\n        normalized (bool): if True, L1 norm of the kernel is set to 1.\n\n    Returns:\n        Tensor: the tensor.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H, W)`\n\n    Examples::\n\n        >>> input = torch.rand(2, 4, 5, 5)\n        >>> laplace = kornia.filters.Laplacian(5)\n        >>> output = laplace(input)  # 2x4x5x5\n    """"""\n\n    def __init__(self,\n                 kernel_size: int, border_type: str = \'reflect\',\n                 normalized: bool = True) -> None:\n        super(Laplacian, self).__init__()\n        self.kernel_size: int = kernel_size\n        self.border_type: str = border_type\n        self.normalized: bool = normalized\n        self.kernel: torch.Tensor = torch.unsqueeze(\n            get_laplacian_kernel2d(kernel_size), dim=0)\n        if self.normalized:\n            self.kernel = normalize_kernel2d(self.kernel)\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ +\\\n            \'(kernel_size=\' + str(self.kernel_size) + \', \' +\\\n            \'normalized=\' + str(self.normalized) + \', \' + \\\n            \'border_type=\' + self.border_type + \')\'\n\n    def forward(self, input: torch.Tensor):  # type: ignore\n        return kornia.filter2D(input, self.kernel, self.border_type)\n\n\n######################\n# functional interface\n######################\n# TODO: In terms of functional API, there should not be any initialization of an nn.Module.\n#       This logic is reversed.\n\ndef laplacian(\n        input: torch.Tensor,\n        kernel_size: int,\n        border_type: str = \'reflect\',\n        normalized: bool = True) -> torch.Tensor:\n    r""""""Function that returns a tensor using a Laplacian filter.\n\n    See :class:`~kornia.filters.Laplacian` for details.\n    """"""\n    return Laplacian(kernel_size, border_type, normalized)(input)\n'"
kornia/filters/median.py,13,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom kornia.filters.kernels import get_binary_kernel2d\n\n\ndef _compute_zero_padding(kernel_size: Tuple[int, int]) -> Tuple[int, int]:\n    r""""""Utility function that computes zero padding tuple.""""""\n    computed: Tuple[int, ...] = tuple([(k - 1) // 2 for k in kernel_size])\n    return computed[0], computed[1]\n\n\nclass MedianBlur(nn.Module):\n    r""""""Blurs an image using the median filter.\n\n    Args:\n        kernel_size (Tuple[int, int]): the blurring kernel size.\n\n    Returns:\n        torch.Tensor: the blurred input tensor.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H, W)`\n\n    Example:\n        >>> input = torch.rand(2, 4, 5, 7)\n        >>> blur = kornia.filters.MedianBlur((3, 3))\n        >>> output = blur(input)  # 2x4x5x7\n    """"""\n\n    def __init__(self, kernel_size: Tuple[int, int]) -> None:\n        super(MedianBlur, self).__init__()\n        self.kernel: torch.Tensor = get_binary_kernel2d(kernel_size)\n        self.padding: Tuple[int, int] = _compute_zero_padding(kernel_size)\n\n    def forward(self, input: torch.Tensor):  # type: ignore\n        if not torch.is_tensor(input):\n            raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                            .format(type(input)))\n        if not len(input.shape) == 4:\n            raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                             .format(input.shape))\n        # prepare kernel\n        b, c, h, w = input.shape\n        kernel: torch.Tensor = self.kernel.to(input.device).to(input.dtype)\n        # map the local window to single vector\n        features: torch.Tensor = F.conv2d(\n            input.reshape(b * c, 1, h, w), kernel, padding=self.padding, stride=1)\n        features = features.view(b, c, -1, h, w)  # BxCx(K_h * K_w)xHxW\n\n        # compute the median along the feature axis\n        median: torch.Tensor = torch.median(features, dim=2)[0]\n        return median\n\n\n# functiona api\n\n\ndef median_blur(input: torch.Tensor,\n                kernel_size: Tuple[int, int]) -> torch.Tensor:\n    r""""""Blurs an image using the median filter.\n\n    See :class:`~kornia.filters.MedianBlur` for details.\n    """"""\n    return MedianBlur(kernel_size)(input)\n'"
kornia/filters/motion.py,7,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom kornia.filters.kernels import get_motion_kernel2d\nfrom kornia.filters.filter import filter2D\n\n\nclass MotionBlur(nn.Module):\n    r""""""Blurs a tensor using the motion filter.\n\n    Args:\n        kernel_size (int): motion kernel width and height. It should be odd and positive.\n        angle (float): angle of the motion blur in degrees (anti-clockwise rotation).\n        direction (float): forward/backward direction of the motion blur.\n            Lower values towards -1.0 will point the motion blur towards the back (with angle provided via angle),\n            while higher values towards 1.0 will point the motion blur forward. A value of 0.0 leads to a\n            uniformly (but still angled) motion blur.\n        border_type (str): the padding mode to be applied before convolving.\n            The expected modes are: ``\'constant\'``, ``\'reflect\'``,\n            ``\'replicate\'`` or ``\'circular\'``. Default: ``\'reflect\'``.\n\n    Returns:\n        torch.Tensor: the blurred input tensor.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H, W)`\n\n    Examples::\n        >>> input = torch.rand(2, 4, 5, 7)\n        >>> motion_blur = kornia.filters.MotionBlur(3, 35., 0.5)\n        >>> output = motion_blur(input)  # 2x4x5x7\n    """"""\n\n    def __init__(\n            self, kernel_size: int, angle: float, direction: float, border_type: str = \'constant\'\n    ) -> None:\n        super(MotionBlur, self).__init__()\n        self.kernel_size = kernel_size\n        self.angle: float = angle\n        self.direction: float = direction\n        self.border_type: str = border_type\n\n    def __repr__(self) -> str:\n        return f\'{self.__class__.__name__} (kernel_size={self.kernel_size}, \' \\\n               f\'angle={self.angle}, direction={self.direction})\'\n\n    def forward(self, x: torch.Tensor):  # type: ignore\n        return motion_blur(x, self.kernel_size, self.angle, self.direction, self.border_type)\n\n\ndef motion_blur(\n    input: torch.Tensor,\n    kernel_size: int,\n    angle: float,\n    direction: float,\n    border_type: str = \'constant\'\n) -> torch.Tensor:\n    r""""""\n    Function that blurs a tensor using the motion filter.\n\n    See :class:`~kornia.filters.MotionBlur` for details.\n    """"""\n    assert border_type in [""constant"", ""reflect"", ""replicate"", ""circular""]\n    kernel: torch.Tensor = torch.unsqueeze(\n        get_motion_kernel2d(kernel_size, angle, direction), dim=0)\n    return filter2D(input, kernel, border_type)\n'"
kornia/filters/sobel.py,33,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.filters.kernels import get_spatial_gradient_kernel2d, get_spatial_gradient_kernel3d\nfrom kornia.filters.kernels import normalize_kernel2d\n\n\nclass SpatialGradient(nn.Module):\n    r""""""Computes the first order image derivative in both x and y using a Sobel\n    operator.\n\n    Return:\n        torch.Tensor: the sobel edges of the input feature map.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, 2, H, W)`\n\n    Examples:\n        >>> input = torch.rand(1, 3, 4, 4)\n        >>> output = kornia.filters.SpatialGradient()(input)  # 1x3x2x4x4\n    """"""\n\n    def __init__(self,\n                 mode: str = \'sobel\',\n                 order: int = 1,\n                 normalized: bool = True) -> None:\n        super(SpatialGradient, self).__init__()\n        self.normalized: bool = normalized\n        self.order: int = order\n        self.mode: str = mode\n        self.kernel = get_spatial_gradient_kernel2d(mode, order)\n        if self.normalized:\n            self.kernel = normalize_kernel2d(self.kernel)\n        return\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ + \'(\'\\\n            \'order=\' + str(self.order) + \', \' + \\\n            \'normalized=\' + str(self.normalized) + \', \' + \\\n            \'mode=\' + self.mode + \')\'\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        if not torch.is_tensor(input):\n            raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                            .format(type(input)))\n        if not len(input.shape) == 4:\n            raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                             .format(input.shape))\n        # prepare kernel\n        b, c, h, w = input.shape\n        tmp_kernel: torch.Tensor = self.kernel.to(input.device).to(input.dtype).detach()\n        kernel: torch.Tensor = tmp_kernel.unsqueeze(1).unsqueeze(1)\n\n        # convolve input tensor with sobel kernel\n        kernel_flip: torch.Tensor = kernel.flip(-3)\n        # Pad with ""replicate for spatial dims, but with zeros for channel\n        spatial_pad = [self.kernel.size(1) // 2,\n                       self.kernel.size(1) // 2,\n                       self.kernel.size(2) // 2,\n                       self.kernel.size(2) // 2]\n        out_channels: int = 3 if self.order == 2 else 2\n        padded_inp: torch.Tensor = F.pad(input.reshape(b * c, 1, h, w), spatial_pad, \'replicate\')[:, :, None]\n        return F.conv3d(padded_inp, kernel_flip, padding=0).view(b, c, out_channels, h, w)\n\n\nclass SpatialGradient3d(nn.Module):\n    r""""""Computes the first and second order volume derivative in x, y and d using a diff\n    operator.\n\n    Return:\n        torch.Tensor: the spatial gradients of the input feature map.\n\n    Shape:\n        - Input: :math:`(B, C, D, H, W)`. D, H, W are spatial dimensions, gradient is calculated w.r.t to them.\n        - Output: :math:`(B, C, 3, D, H, W)` or :math:`(B, C, 6, D, H, W)`\n\n    Examples:\n        >>> input = torch.rand(1, 3, 4, 4)\n        >>> output = kornia.filters.SpatialGradient()(input)  # 1x3x2x4x4\n    """"""\n\n    def __init__(self,\n                 mode: str = \'diff\',\n                 order: int = 1) -> None:\n        super(SpatialGradient3d, self).__init__()\n        self.order: int = order\n        self.mode: str = mode\n        self.kernel = get_spatial_gradient_kernel3d(mode, order)\n        return\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ + \'(\'\\\n            \'order=\' + str(self.order) + \', \' + \\\n            \'mode=\' + self.mode + \')\'\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        if not torch.is_tensor(input):\n            raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                            .format(type(input)))\n        if not len(input.shape) == 5:\n            raise ValueError(""Invalid input shape, we expect BxCxDxHxW. Got: {}""\n                             .format(input.shape))\n        # prepare kernel\n        b, c, d, h, w = input.shape\n        tmp_kernel: torch.Tensor = self.kernel.to(input.device).to(input.dtype).detach()\n        kernel: torch.Tensor = tmp_kernel.repeat(c, 1, 1, 1, 1)\n\n        # convolve input tensor with grad kernel\n        kernel_flip: torch.Tensor = kernel.flip(-3)\n        # Pad with ""replicate for spatial dims, but with zeros for channel\n        spatial_pad = [self.kernel.size(2) // 2,\n                       self.kernel.size(2) // 2,\n                       self.kernel.size(3) // 2,\n                       self.kernel.size(3) // 2,\n                       self.kernel.size(4) // 2,\n                       self.kernel.size(4) // 2]\n        out_ch: int = 6 if self.order == 2 else 3\n        return F.conv3d(F.pad(input, spatial_pad, \'replicate\'), kernel, padding=0, groups=c).view(b, c, out_ch, d, h, w)\n\n\nclass Sobel(nn.Module):\n    r""""""Computes the Sobel operator and returns the magnitude per channel.\n\n    Return:\n        torch.Tensor: the sobel edge gradient maginitudes map.\n\n    Args:\n        normalized (bool): if True, L1 norm of the kernel is set to 1.\n        eps (float): regularization number to avoid NaN during backprop. Default: 1e-6.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H, W)`\n\n    Examples:\n        >>> input = torch.rand(1, 3, 4, 4)\n        >>> output = kornia.filters.Sobel()(input)  # 1x3x4x4\n    """"""\n\n    def __init__(self,\n                 normalized: bool = True, eps: float = 1e-6) -> None:\n        super(Sobel, self).__init__()\n        self.normalized: bool = normalized\n        self.eps: float = eps\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ + \'(\'\\\n            \'normalized=\' + str(self.normalized) + \')\'\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        if not torch.is_tensor(input):\n            raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                            .format(type(input)))\n        if not len(input.shape) == 4:\n            raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                             .format(input.shape))\n        # comput the x/y gradients\n        edges: torch.Tensor = spatial_gradient(input,\n                                               normalized=self.normalized)\n\n        # unpack the edges\n        gx: torch.Tensor = edges[:, :, 0]\n        gy: torch.Tensor = edges[:, :, 1]\n\n        # compute gradient maginitude\n        magnitude: torch.Tensor = torch.sqrt(gx * gx + gy * gy + self.eps)\n        return magnitude\n\n\n# functiona api\n# TODO: In terms of functional API, there should not be any initialization of an nn.Module.\n#       This logic is reversed.\n\ndef spatial_gradient(input: torch.Tensor,\n                     mode: str = \'sobel\',\n                     order: int = 1,\n                     normalized: bool = True) -> torch.Tensor:\n    r""""""Computes the first order image derivative in both x and y using a Sobel\n    operator.\n\n    See :class:`~kornia.filters.SpatialGradient` for details.\n    """"""\n    return SpatialGradient(mode, order, normalized)(input)\n\n\ndef spatial_gradient3d(input: torch.Tensor,\n                       mode: str = \'diff\',\n                       order: int = 1) -> torch.Tensor:\n    r""""""Computes the first or second order image derivative in both x and y and y using a diff\n    operator.\n\n    See :class:`~kornia.filters.SpatialGradient3d` for details.\n    """"""\n    return SpatialGradient3d(mode, order)(input)\n\n\ndef sobel(input: torch.Tensor, normalized: bool = True, eps: float = 1e-6) -> torch.Tensor:\n    r""""""Computes the Sobel operator and returns the magnitude per channel.\n\n    See :class:`~kornia.filters.Sobel` for details.\n    """"""\n    return Sobel(normalized, eps)(input)\n'"
kornia/geometry/__init__.py,0,b'from kornia.geometry.camera import *\nfrom kornia.geometry.conversions import *\nfrom kornia.geometry.linalg import *\nfrom kornia.geometry.transform import *\nfrom kornia.geometry.warp import *\nfrom kornia.geometry.depth import *\nfrom kornia.geometry.dsnt import *\nfrom kornia.geometry.spatial_soft_argmax import *\nfrom kornia.geometry.epipolar import *\n'
kornia/geometry/conversions.py,204,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.constants import pi\n\n__all__ = [\n    # functional api\n    ""rad2deg"",\n    ""deg2rad"",\n    ""convert_points_from_homogeneous"",\n    ""convert_points_to_homogeneous"",\n    ""convert_affinematrix_to_homography"",\n    ""angle_axis_to_rotation_matrix"",\n    ""angle_axis_to_quaternion"",\n    ""rotation_matrix_to_angle_axis"",\n    ""rotation_matrix_to_quaternion"",\n    ""quaternion_to_angle_axis"",\n    ""quaternion_to_rotation_matrix"",\n    ""quaternion_log_to_exp"",\n    ""quaternion_exp_to_log"",\n    ""denormalize_pixel_coordinates"",\n    ""normalize_pixel_coordinates"",\n    ""normalize_quaternion"",\n    ""denormalize_pixel_coordinates3d"",\n    ""normalize_pixel_coordinates3d"",\n]\n\n\ndef rad2deg(tensor: torch.Tensor) -> torch.Tensor:\n    r""""""Function that converts angles from radians to degrees.\n\n    Args:\n        tensor (torch.Tensor): Tensor of arbitrary shape.\n\n    Returns:\n        torch.Tensor: Tensor with same shape as input.\n\n    Example:\n        >>> input = kornia.pi * torch.rand(1, 3, 3)\n        >>> output = kornia.rad2deg(input)\n    """"""\n    if not isinstance(tensor, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(tensor)))\n\n    return 180. * tensor / pi.to(tensor.device).type(tensor.dtype)\n\n\ndef deg2rad(tensor: torch.Tensor) -> torch.Tensor:\n    r""""""Function that converts angles from degrees to radians.\n\n    Args:\n        tensor (torch.Tensor): Tensor of arbitrary shape.\n\n    Returns:\n        torch.Tensor: tensor with same shape as input.\n\n    Examples::\n\n        >>> input = 360. * torch.rand(1, 3, 3)\n        >>> output = kornia.deg2rad(input)\n    """"""\n    if not isinstance(tensor, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(tensor)))\n\n    return tensor * pi.to(tensor.device).type(tensor.dtype) / 180.\n\n\ndef convert_points_from_homogeneous(\n        points: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n    r""""""Function that converts points from homogeneous to Euclidean space.\n\n    Examples::\n\n        >>> input = torch.rand(2, 4, 3)  # BxNx3\n        >>> output = kornia.convert_points_from_homogeneous(input)  # BxNx2\n    """"""\n    if not isinstance(points, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(points)))\n\n    if len(points.shape) < 2:\n        raise ValueError(""Input must be at least a 2D tensor. Got {}"".format(\n            points.shape))\n\n    # we check for points at infinity\n    z_vec: torch.Tensor = points[..., -1:]\n\n    # set the results of division by zeror/near-zero to 1.0\n    # follow the convention of opencv:\n    # https://github.com/opencv/opencv/pull/14411/files\n    mask: torch.Tensor = torch.abs(z_vec) > eps\n    scale: torch.Tensor = torch.ones_like(z_vec).masked_scatter_(\n        mask, torch.tensor(1.0).to(points.device) / z_vec[mask])\n\n    return scale * points[..., :-1]\n\n\ndef convert_points_to_homogeneous(points: torch.Tensor) -> torch.Tensor:\n    r""""""Function that converts points from Euclidean to homogeneous space.\n\n    Examples::\n\n        >>> input = torch.rand(2, 4, 3)  # BxNx3\n        >>> output = kornia.convert_points_to_homogeneous(input)  # BxNx4\n    """"""\n    if not isinstance(points, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(points)))\n    if len(points.shape) < 2:\n        raise ValueError(""Input must be at least a 2D tensor. Got {}"".format(\n            points.shape))\n\n    return torch.nn.functional.pad(points, [0, 1], ""constant"", 1.0)\n\n\ndef convert_affinematrix_to_homography(A: torch.Tensor) -> torch.Tensor:\n    r""""""Function that converts batch of affine matrices from [Bx2x3] to [Bx3x3].\n\n    Examples::\n\n        >>> input = torch.rand(2, 2, 3)  # Bx2x3\n        >>> output = kornia.convert_affinematrix_to_homography(input)  # Bx3x3\n    """"""\n    if not isinstance(A, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(A)))\n    if not (len(A.shape) == 3 and A.shape[-2:] == (2, 3)):\n        raise ValueError(""Input matrix must be a Bx2x3 tensor. Got {}""\n                         .format(A.shape))\n    H: torch.Tensor = torch.nn.functional.pad(A, [0, 0, 0, 1], ""constant"", value=0.)\n    H[..., -1, -1] += 1.0\n    return H\n\n\ndef angle_axis_to_rotation_matrix(angle_axis: torch.Tensor) -> torch.Tensor:\n    r""""""Convert 3d vector of axis-angle rotation to 3x3 rotation matrix\n\n    Args:\n        angle_axis (torch.Tensor): tensor of 3d vector of axis-angle rotations.\n\n    Returns:\n        torch.Tensor: tensor of 3x3 rotation matrices.\n\n    Shape:\n        - Input: :math:`(N, 3)`\n        - Output: :math:`(N, 3, 3)`\n\n    Example:\n        >>> input = torch.rand(1, 3)  # Nx3\n        >>> output = kornia.angle_axis_to_rotation_matrix(input)  # Nx3x3\n    """"""\n    if not isinstance(angle_axis, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(angle_axis)))\n\n    if not angle_axis.shape[-1] == 3:\n        raise ValueError(\n            ""Input size must be a (*, 3) tensor. Got {}"".format(\n                angle_axis.shape))\n\n    def _compute_rotation_matrix(angle_axis, theta2, eps=1e-6):\n        # We want to be careful to only evaluate the square root if the\n        # norm of the angle_axis vector is greater than zero. Otherwise\n        # we get a division by zero.\n        k_one = 1.0\n        theta = torch.sqrt(theta2)\n        wxyz = angle_axis / (theta + eps)\n        wx, wy, wz = torch.chunk(wxyz, 3, dim=1)\n        cos_theta = torch.cos(theta)\n        sin_theta = torch.sin(theta)\n\n        r00 = cos_theta + wx * wx * (k_one - cos_theta)\n        r10 = wz * sin_theta + wx * wy * (k_one - cos_theta)\n        r20 = -wy * sin_theta + wx * wz * (k_one - cos_theta)\n        r01 = wx * wy * (k_one - cos_theta) - wz * sin_theta\n        r11 = cos_theta + wy * wy * (k_one - cos_theta)\n        r21 = wx * sin_theta + wy * wz * (k_one - cos_theta)\n        r02 = wy * sin_theta + wx * wz * (k_one - cos_theta)\n        r12 = -wx * sin_theta + wy * wz * (k_one - cos_theta)\n        r22 = cos_theta + wz * wz * (k_one - cos_theta)\n        rotation_matrix = torch.cat(\n            [r00, r01, r02, r10, r11, r12, r20, r21, r22], dim=1)\n        return rotation_matrix.view(-1, 3, 3)\n\n    def _compute_rotation_matrix_taylor(angle_axis):\n        rx, ry, rz = torch.chunk(angle_axis, 3, dim=1)\n        k_one = torch.ones_like(rx)\n        rotation_matrix = torch.cat(\n            [k_one, -rz, ry, rz, k_one, -rx, -ry, rx, k_one], dim=1)\n        return rotation_matrix.view(-1, 3, 3)\n\n    # stolen from ceres/rotation.h\n\n    _angle_axis = torch.unsqueeze(angle_axis, dim=1)\n    theta2 = torch.matmul(_angle_axis, _angle_axis.transpose(1, 2))\n    theta2 = torch.squeeze(theta2, dim=1)\n\n    # compute rotation matrices\n    rotation_matrix_normal = _compute_rotation_matrix(angle_axis, theta2)\n    rotation_matrix_taylor = _compute_rotation_matrix_taylor(angle_axis)\n\n    # create mask to handle both cases\n    eps = 1e-6\n    mask = (theta2 > eps).view(-1, 1, 1).to(theta2.device)\n    mask_pos = (mask).type_as(theta2)\n    mask_neg = (mask == False).type_as(theta2)  # noqa\n\n    # create output pose matrix\n    batch_size = angle_axis.shape[0]\n    rotation_matrix = torch.eye(3).to(angle_axis.device).type_as(angle_axis)\n    rotation_matrix = rotation_matrix.view(1, 3, 3).repeat(batch_size, 1, 1)\n    # fill output matrix with masked values\n    rotation_matrix[..., :3, :3] = \\\n        mask_pos * rotation_matrix_normal + mask_neg * rotation_matrix_taylor\n    return rotation_matrix  # Nx4x4\n\n\ndef rotation_matrix_to_angle_axis(\n        rotation_matrix: torch.Tensor) -> torch.Tensor:\n    r""""""Convert 3x3 rotation matrix to Rodrigues vector.\n\n    Args:\n        rotation_matrix (torch.Tensor): rotation matrix.\n\n    Returns:\n        torch.Tensor: Rodrigues vector transformation.\n\n    Shape:\n        - Input: :math:`(N, 3, 3)`\n        - Output: :math:`(N, 3)`\n\n    Example:\n        >>> input = torch.rand(2, 3, 3)  # Nx3x3\n        >>> output = kornia.rotation_matrix_to_angle_axis(input)  # Nx3\n    """"""\n    if not isinstance(rotation_matrix, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(rotation_matrix)))\n\n    if not rotation_matrix.shape[-2:] == (3, 3):\n        raise ValueError(\n            ""Input size must be a (*, 3, 3) tensor. Got {}"".format(\n                rotation_matrix.shape))\n    quaternion: torch.Tensor = rotation_matrix_to_quaternion(rotation_matrix)\n    return quaternion_to_angle_axis(quaternion)\n\n\ndef rotation_matrix_to_quaternion(\n        rotation_matrix: torch.Tensor,\n        eps: float = 1e-8) -> torch.Tensor:\n    r""""""Convert 3x3 rotation matrix to 4d quaternion vector.\n    The quaternion vector has components in (x, y, z, w) format.\n\n    Args:\n        rotation_matrix (torch.Tensor): the rotation matrix to convert.\n        eps (float): small value to avoid zero division. Default: 1e-8.\n\n    Return:\n        torch.Tensor: the rotation in quaternion.\n\n    Shape:\n        - Input: :math:`(*, 3, 3)`\n        - Output: :math:`(*, 4)`\n\n    Example:\n        >>> input = torch.rand(4, 3, 3)  # Nx3x3\n        >>> output = kornia.rotation_matrix_to_quaternion(input)  # Nx4\n    """"""\n    if not isinstance(rotation_matrix, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(rotation_matrix)))\n\n    if not rotation_matrix.shape[-2:] == (3, 3):\n        raise ValueError(\n            ""Input size must be a (*, 3, 3) tensor. Got {}"".format(\n                rotation_matrix.shape))\n\n    def safe_zero_division(numerator: torch.Tensor,\n                           denominator: torch.Tensor) -> torch.Tensor:\n        eps: float = torch.finfo(numerator.dtype).tiny  # type: ignore\n        return numerator / torch.clamp(denominator, min=eps)\n\n    rotation_matrix_vec: torch.Tensor = rotation_matrix.view(\n        *rotation_matrix.shape[:-2], 9)\n\n    m00, m01, m02, m10, m11, m12, m20, m21, m22 = torch.chunk(\n        rotation_matrix_vec, chunks=9, dim=-1)\n\n    trace: torch.Tensor = m00 + m11 + m22\n\n    def trace_positive_cond():\n        sq = torch.sqrt(trace + 1.0) * 2.  # sq = 4 * qw.\n        qw = 0.25 * sq\n        qx = safe_zero_division(m21 - m12, sq)\n        qy = safe_zero_division(m02 - m20, sq)\n        qz = safe_zero_division(m10 - m01, sq)\n        return torch.cat([qx, qy, qz, qw], dim=-1)\n\n    def cond_1():\n        sq = torch.sqrt(1.0 + m00 - m11 - m22 + eps) * 2.  # sq = 4 * qx.\n        qw = safe_zero_division(m21 - m12, sq)\n        qx = 0.25 * sq\n        qy = safe_zero_division(m01 + m10, sq)\n        qz = safe_zero_division(m02 + m20, sq)\n        return torch.cat([qx, qy, qz, qw], dim=-1)\n\n    def cond_2():\n        sq = torch.sqrt(1.0 + m11 - m00 - m22 + eps) * 2.  # sq = 4 * qy.\n        qw = safe_zero_division(m02 - m20, sq)\n        qx = safe_zero_division(m01 + m10, sq)\n        qy = 0.25 * sq\n        qz = safe_zero_division(m12 + m21, sq)\n        return torch.cat([qx, qy, qz, qw], dim=-1)\n\n    def cond_3():\n        sq = torch.sqrt(1.0 + m22 - m00 - m11 + eps) * 2.  # sq = 4 * qz.\n        qw = safe_zero_division(m10 - m01, sq)\n        qx = safe_zero_division(m02 + m20, sq)\n        qy = safe_zero_division(m12 + m21, sq)\n        qz = 0.25 * sq\n        return torch.cat([qx, qy, qz, qw], dim=-1)\n\n    where_2 = torch.where(m11 > m22, cond_2(), cond_3())\n    where_1 = torch.where(\n        (m00 > m11) & (m00 > m22), cond_1(), where_2)\n\n    quaternion: torch.Tensor = torch.where(\n        trace > 0., trace_positive_cond(), where_1)\n    return quaternion\n\n\ndef normalize_quaternion(quaternion: torch.Tensor,\n                         eps: float = 1e-12) -> torch.Tensor:\n    r""""""Normalizes a quaternion.\n    The quaternion should be in (x, y, z, w) format.\n\n    Args:\n        quaternion (torch.Tensor): a tensor containing a quaternion to be\n          normalized. The tensor can be of shape :math:`(*, 4)`.\n        eps (Optional[bool]): small value to avoid division by zero.\n          Default: 1e-12.\n\n    Return:\n        torch.Tensor: the normalized quaternion of shape :math:`(*, 4)`.\n\n    Example:\n        >>> quaternion = torch.tensor([1., 0., 1., 0.])\n        >>> kornia.normalize_quaternion(quaternion)\n        tensor([0.7071, 0.0000, 0.7071, 0.0000])\n    """"""\n    if not isinstance(quaternion, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(quaternion)))\n\n    if not quaternion.shape[-1] == 4:\n        raise ValueError(\n            ""Input must be a tensor of shape (*, 4). Got {}"".format(\n                quaternion.shape))\n    return F.normalize(quaternion, p=2, dim=-1, eps=eps)\n\n\n# based on:\n# https://github.com/matthew-brett/transforms3d/blob/8965c48401d9e8e66b6a8c37c65f2fc200a076fa/transforms3d/quaternions.py#L101\n# https://github.com/tensorflow/graphics/blob/master/tensorflow_graphics/geometry/transformation/rotation_matrix_3d.py#L247\n\ndef quaternion_to_rotation_matrix(quaternion: torch.Tensor) -> torch.Tensor:\n    r""""""Converts a quaternion to a rotation matrix.\n    The quaternion should be in (x, y, z, w) format.\n\n    Args:\n        quaternion (torch.Tensor): a tensor containing a quaternion to be\n          converted. The tensor can be of shape :math:`(*, 4)`.\n\n    Return:\n        torch.Tensor: the rotation matrix of shape :math:`(*, 3, 3)`.\n\n    Example:\n        >>> quaternion = torch.tensor([0., 0., 1., 0.])\n        >>> kornia.quaternion_to_rotation_matrix(quaternion)\n        tensor([[[-1.,  0.,  0.],\n                 [ 0., -1.,  0.],\n                 [ 0.,  0.,  1.]]])\n    """"""\n    if not isinstance(quaternion, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(quaternion)))\n\n    if not quaternion.shape[-1] == 4:\n        raise ValueError(\n            ""Input must be a tensor of shape (*, 4). Got {}"".format(\n                quaternion.shape))\n    # normalize the input quaternion\n    quaternion_norm: torch.Tensor = normalize_quaternion(quaternion)\n\n    # unpack the normalized quaternion components\n    x, y, z, w = torch.chunk(quaternion_norm, chunks=4, dim=-1)\n\n    # compute the actual conversion\n    tx: torch.Tensor = 2.0 * x\n    ty: torch.Tensor = 2.0 * y\n    tz: torch.Tensor = 2.0 * z\n    twx: torch.Tensor = tx * w\n    twy: torch.Tensor = ty * w\n    twz: torch.Tensor = tz * w\n    txx: torch.Tensor = tx * x\n    txy: torch.Tensor = ty * x\n    txz: torch.Tensor = tz * x\n    tyy: torch.Tensor = ty * y\n    tyz: torch.Tensor = tz * y\n    tzz: torch.Tensor = tz * z\n    one: torch.Tensor = torch.tensor(1.)\n\n    matrix: torch.Tensor = torch.stack([\n        one - (tyy + tzz), txy - twz, txz + twy,\n        txy + twz, one - (txx + tzz), tyz - twx,\n        txz - twy, tyz + twx, one - (txx + tyy)\n    ], dim=-1).view(-1, 3, 3)\n\n    if len(quaternion.shape) == 1:\n        matrix = torch.squeeze(matrix, dim=0)\n    return matrix\n\n\ndef quaternion_to_angle_axis(quaternion: torch.Tensor) -> torch.Tensor:\n    """"""Convert quaternion vector to angle axis of rotation.\n    The quaternion should be in (x, y, z, w) format.\n\n    Adapted from ceres C++ library: ceres-solver/include/ceres/rotation.h\n\n    Args:\n        quaternion (torch.Tensor): tensor with quaternions.\n\n    Return:\n        torch.Tensor: tensor with angle axis of rotation.\n\n    Shape:\n        - Input: :math:`(*, 4)` where `*` means, any number of dimensions\n        - Output: :math:`(*, 3)`\n\n    Example:\n        >>> quaternion = torch.rand(2, 4)  # Nx4\n        >>> angle_axis = kornia.quaternion_to_angle_axis(quaternion)  # Nx3\n    """"""\n    if not torch.is_tensor(quaternion):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(quaternion)))\n\n    if not quaternion.shape[-1] == 4:\n        raise ValueError(\n            ""Input must be a tensor of shape Nx4 or 4. Got {}"".format(\n                quaternion.shape))\n    # unpack input and compute conversion\n    q1: torch.Tensor = quaternion[..., 1]\n    q2: torch.Tensor = quaternion[..., 2]\n    q3: torch.Tensor = quaternion[..., 3]\n    sin_squared_theta: torch.Tensor = q1 * q1 + q2 * q2 + q3 * q3\n\n    sin_theta: torch.Tensor = torch.sqrt(sin_squared_theta)\n    cos_theta: torch.Tensor = quaternion[..., 0]\n    two_theta: torch.Tensor = 2.0 * torch.where(\n        cos_theta < 0.0, torch.atan2(-sin_theta, -cos_theta),\n        torch.atan2(sin_theta, cos_theta))\n\n    k_pos: torch.Tensor = two_theta / sin_theta\n    k_neg: torch.Tensor = 2.0 * torch.ones_like(sin_theta)\n    k: torch.Tensor = torch.where(sin_squared_theta > 0.0, k_pos, k_neg)\n\n    angle_axis: torch.Tensor = torch.zeros_like(quaternion)[..., :3]\n    angle_axis[..., 0] += q1 * k\n    angle_axis[..., 1] += q2 * k\n    angle_axis[..., 2] += q3 * k\n    return angle_axis\n\n\ndef quaternion_log_to_exp(quaternion: torch.Tensor,\n                          eps: float = 1e-8) -> torch.Tensor:\n    r""""""Applies exponential map to log quaternion.\n    The quaternion should be in (x, y, z, w) format.\n\n    Args:\n        quaternion (torch.Tensor): a tensor containing a quaternion to be\n          converted. The tensor can be of shape :math:`(*, 3)`.\n\n    Return:\n        torch.Tensor: the quaternion exponential map of shape :math:`(*, 4)`.\n\n    Example:\n        >>> quaternion = torch.tensor([0., 0., 0.])\n        >>> kornia.quaternion_log_to_exp(quaternion)\n        tensor([0., 0., 0., 1.])\n    """"""\n    if not isinstance(quaternion, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(quaternion)))\n\n    if not quaternion.shape[-1] == 3:\n        raise ValueError(\n            ""Input must be a tensor of shape (*, 3). Got {}"".format(\n                quaternion.shape))\n    # compute quaternion norm\n    norm_q: torch.Tensor = torch.norm(\n        quaternion, p=2, dim=-1, keepdim=True).clamp(min=eps)\n\n    # compute scalar and vector\n    quaternion_vector: torch.Tensor = quaternion * torch.sin(norm_q) / norm_q\n    quaternion_scalar: torch.Tensor = torch.cos(norm_q)\n\n    # compose quaternion and return\n    quaternion_exp: torch.Tensor = torch.cat(\n        [quaternion_vector, quaternion_scalar], dim=-1)\n    return quaternion_exp\n\n\ndef quaternion_exp_to_log(quaternion: torch.Tensor,\n                          eps: float = 1e-8) -> torch.Tensor:\n    r""""""Applies the log map to a quaternion.\n    The quaternion should be in (x, y, z, w) format.\n\n    Args:\n        quaternion (torch.Tensor): a tensor containing a quaternion to be\n          converted. The tensor can be of shape :math:`(*, 4)`.\n\n    Return:\n        torch.Tensor: the quaternion log map of shape :math:`(*, 3)`.\n\n    Example:\n        >>> quaternion = torch.tensor([0., 0., 0., 1.])\n        >>> kornia.quaternion_exp_to_log(quaternion)\n        tensor([0., 0., 0.])\n    """"""\n    if not isinstance(quaternion, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(quaternion)))\n\n    if not quaternion.shape[-1] == 4:\n        raise ValueError(\n            ""Input must be a tensor of shape (*, 4). Got {}"".format(\n                quaternion.shape))\n    # unpack quaternion vector and scalar\n    quaternion_vector: torch.Tensor = quaternion[..., 0:3]\n    quaternion_scalar: torch.Tensor = quaternion[..., 3:4]\n\n    # compute quaternion norm\n    norm_q: torch.Tensor = torch.norm(\n        quaternion_vector, p=2, dim=-1, keepdim=True).clamp(min=eps)\n\n    # apply log map\n    quaternion_log: torch.Tensor = quaternion_vector * torch.acos(\n        torch.clamp(quaternion_scalar, min=-1.0, max=1.0)) / norm_q\n    return quaternion_log\n\n\n# based on:\n# https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py#L138\n\n\ndef angle_axis_to_quaternion(angle_axis: torch.Tensor) -> torch.Tensor:\n    r""""""Convert an angle axis to a quaternion.\n    The quaternion vector has components in (x, y, z, w) format.\n\n    Adapted from ceres C++ library: ceres-solver/include/ceres/rotation.h\n\n    Args:\n        angle_axis (torch.Tensor): tensor with angle axis.\n\n    Return:\n        torch.Tensor: tensor with quaternion.\n\n    Shape:\n        - Input: :math:`(*, 3)` where `*` means, any number of dimensions\n        - Output: :math:`(*, 4)`\n\n    Example:\n        >>> angle_axis = torch.rand(2, 4)  # Nx4\n        >>> quaternion = kornia.angle_axis_to_quaternion(angle_axis)  # Nx3\n    """"""\n    if not torch.is_tensor(angle_axis):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(angle_axis)))\n\n    if not angle_axis.shape[-1] == 3:\n        raise ValueError(\n            ""Input must be a tensor of shape Nx3 or 3. Got {}"".format(\n                angle_axis.shape))\n    # unpack input and compute conversion\n    a0: torch.Tensor = angle_axis[..., 0:1]\n    a1: torch.Tensor = angle_axis[..., 1:2]\n    a2: torch.Tensor = angle_axis[..., 2:3]\n    theta_squared: torch.Tensor = a0 * a0 + a1 * a1 + a2 * a2\n\n    theta: torch.Tensor = torch.sqrt(theta_squared)\n    half_theta: torch.Tensor = theta * 0.5\n\n    mask: torch.Tensor = theta_squared > 0.0\n    ones: torch.Tensor = torch.ones_like(half_theta)\n\n    k_neg: torch.Tensor = 0.5 * ones\n    k_pos: torch.Tensor = torch.sin(half_theta) / theta\n    k: torch.Tensor = torch.where(mask, k_pos, k_neg)\n    w: torch.Tensor = torch.where(mask, torch.cos(half_theta), ones)\n\n    quaternion: torch.Tensor = torch.zeros_like(angle_axis)\n    quaternion[..., 0:1] += a0 * k\n    quaternion[..., 1:2] += a1 * k\n    quaternion[..., 2:3] += a2 * k\n    return torch.cat([w, quaternion], dim=-1)\n\n\n# based on:\n# https://github.com/ClementPinard/SfmLearner-Pytorch/blob/master/inverse_warp.py#L65-L71\n\ndef normalize_pixel_coordinates(\n        pixel_coordinates: torch.Tensor,\n        height: int,\n        width: int,\n        eps: float = 1e-8) -> torch.Tensor:\n    r""""""Normalize pixel coordinates between -1 and 1.\n\n    Normalized, -1 if on extreme left, 1 if on extreme right (x = w-1).\n\n    Args:\n        pixel_coordinates (torch.Tensor): the grid with pixel coordinates.\n          Shape can be :math:`(*, 2)`.\n        width (int): the maximum width in the x-axis.\n        height (int): the maximum height in the y-axis.\n        eps (float): safe division by zero. (default 1e-8).\n\n    Return:\n        torch.Tensor: the normalized pixel coordinates.\n    """"""\n    if pixel_coordinates.shape[-1] != 2:\n        raise ValueError(""Input pixel_coordinates must be of shape (*, 2). ""\n                         ""Got {}"".format(pixel_coordinates.shape))\n    # compute normalization factor\n    hw: torch.Tensor = torch.stack([\n        torch.tensor(width), torch.tensor(height)\n    ]).to(pixel_coordinates.device).to(pixel_coordinates.dtype)\n\n    factor: torch.Tensor = torch.tensor(2.) / (hw - 1).clamp(eps)\n\n    return factor * pixel_coordinates - 1\n\n\ndef denormalize_pixel_coordinates(\n        pixel_coordinates: torch.Tensor,\n        height: int,\n        width: int,\n        eps: float = 1e-8) -> torch.Tensor:\n    r""""""Denormalize pixel coordinates.\n\n    The input is assumed to be -1 if on extreme left, 1 if on\n    extreme right (x = w-1).\n\n    Args:\n        pixel_coordinates (torch.Tensor): the normalized grid coordinates.\n          Shape can be :math:`(*, 2)`.\n        width (int): the maximum width in the x-axis.\n        height (int): the maximum height in the y-axis.\n        eps (float): safe division by zero. (default 1e-8).\n\n    Return:\n        torch.Tensor: the denormalized pixel coordinates.\n    """"""\n    if pixel_coordinates.shape[-1] != 2:\n        raise ValueError(""Input pixel_coordinates must be of shape (*, 2). ""\n                         ""Got {}"".format(pixel_coordinates.shape))\n    # compute normalization factor\n    hw: torch.Tensor = torch.stack([\n        torch.tensor(width), torch.tensor(height)\n    ]).to(pixel_coordinates.device).to(pixel_coordinates.dtype)\n\n    factor: torch.Tensor = torch.tensor(2.) / (hw - 1).clamp(eps)\n\n    return torch.tensor(1.) / factor * (pixel_coordinates + 1)\n\n\ndef normalize_pixel_coordinates3d(\n        pixel_coordinates: torch.Tensor,\n        depth: int,\n        height: int,\n        width: int,\n        eps: float = 1e-8) -> torch.Tensor:\n    r""""""Normalize pixel coordinates between -1 and 1.\n\n    Normalized, -1 if on extreme left, 1 if on extreme right (x = w-1).\n\n    Args:\n        pixel_coordinates (torch.Tensor): the grid with pixel coordinates.\n          Shape can be :math:`(*, 3)`.\n        depth (int): the maximum depth in the z-axis.\n        height (int): the maximum height in the y-axis.\n        width (int): the maximum width in the x-axis.\n        eps (float): safe division by zero. (default 1e-8).\n\n    Return:\n        torch.Tensor: the normalized pixel coordinates.\n    """"""\n    if pixel_coordinates.shape[-1] != 3:\n        raise ValueError(""Input pixel_coordinates must be of shape (*, 3). ""\n                         ""Got {}"".format(pixel_coordinates.shape))\n    # compute normalization factor\n    dhw: torch.Tensor = torch.stack([\n        torch.tensor(depth), torch.tensor(width), torch.tensor(height)\n    ]).to(pixel_coordinates.device).to(pixel_coordinates.dtype)\n\n    factor: torch.Tensor = torch.tensor(2.) / (dhw - 1).clamp(eps)\n\n    return factor * pixel_coordinates - 1\n\n\ndef denormalize_pixel_coordinates3d(\n        pixel_coordinates: torch.Tensor,\n        depth: int,\n        height: int,\n        width: int,\n        eps: float = 1e-8) -> torch.Tensor:\n    r""""""Denormalize pixel coordinates.\n\n    The input is assumed to be -1 if on extreme left, 1 if on\n    extreme right (x = w-1).\n\n    Args:\n        pixel_coordinates (torch.Tensor): the normalized grid coordinates.\n          Shape can be :math:`(*, 3)`.\n        depth (int): the maximum depth in the x-axis.\n        height (int): the maximum height in the y-axis.\n        width (int): the maximum width in the x-axis.\n        eps (float): safe division by zero. (default 1e-8).\n\n\n    Return:\n        torch.Tensor: the denormalized pixel coordinates.\n    """"""\n    if pixel_coordinates.shape[-1] != 3:\n        raise ValueError(""Input pixel_coordinates must be of shape (*, 3). ""\n                         ""Got {}"".format(pixel_coordinates.shape))\n    # compute normalization factor\n    dhw: torch.Tensor = torch.stack([\n        torch.tensor(depth), torch.tensor(width), torch.tensor(height)\n    ]).to(pixel_coordinates.device).to(pixel_coordinates.dtype)\n\n    factor: torch.Tensor = torch.tensor(2.) / (dhw - 1).clamp(eps)\n\n    return torch.tensor(1.) / factor * (pixel_coordinates + 1)\n'"
kornia/geometry/depth.py,46,"b'""""""Module containing operators to work on RGB-Depth images.""""""\n\nimport torch\nimport torch.nn.functional as F\n\nfrom kornia.geometry import (\n    project_points, unproject_points, transform_points, normalize_pixel_coordinates,\n)\nfrom kornia.utils import create_meshgrid\nfrom kornia.filters import spatial_gradient\n\n\ndef depth_to_3d(depth: torch.Tensor, camera_matrix: torch.Tensor, normalize_points: bool = False) -> torch.Tensor:\n    """"""Compute a 3d point per pixel given its depth value and the camera intrinsics.\n\n    Args:\n        depth (torch.Tensor): image tensor containing a depth value per pixel.\n        camera_matrix (torch.Tensor): tensor containing the camera intrinsics.\n        normalize_points (bool): whether to normalise the pointcloud. This\n            must be set to `True` when the depth is represented as the Euclidean\n            ray length from the camera position. Default is `False`.\n\n    Shape:\n        - Input: :math:`(B, 1, H, W)` and :math:`(B, 3, 3)`\n        - Output: :math:`(B, 3, H, W)`\n\n    Return:\n        torch.Tensor: tensor with a 3d point per pixel of the same resolution as the input.\n\n    """"""\n    if not isinstance(depth, torch.Tensor):\n        raise TypeError(f""Input depht type is not a torch.Tensor. Got {type(depth)}."")\n\n    if not len(depth.shape) == 4 and depth.shape[-3] == 1:\n        raise ValueError(f""Input depth musth have a shape (B, 1, H, W). Got: {depth.shape}"")\n\n    if not isinstance(camera_matrix, torch.Tensor):\n        raise TypeError(f""Input camera_matrix type is not a torch.Tensor. ""\n                        f""Got {type(camera_matrix)}."")\n\n    if not len(camera_matrix.shape) == 3 and camera_matrix.shape[-2:] == (3, 3):\n        raise ValueError(f""Input camera_matrix must have a shape (B, 3, 3). ""\n                         f""Got: {camera_matrix.shape}."")\n\n    # create base coordinates grid\n    batch_size, _, height, width = depth.shape\n    points_2d: torch.Tensor = create_meshgrid(\n        height, width, normalized_coordinates=False)  # 1xHxWx2\n    points_2d = points_2d.to(depth.device).to(depth.dtype)\n\n    # depth should come in Bx1xHxW\n    points_depth: torch.Tensor = depth.permute(0, 2, 3, 1)  # 1xHxWx1\n\n    # project pixels to camera frame\n    camera_matrix_tmp: torch.Tensor = camera_matrix[:, None, None]  # Bx1x1x3x3\n    points_3d: torch.Tensor = unproject_points(\n        points_2d, points_depth, camera_matrix_tmp, normalize=normalize_points)  # BxHxWx3\n\n    return points_3d.permute(0, 3, 1, 2)  # Bx3xHxW\n\n\ndef depth_to_normals(depth: torch.Tensor, camera_matrix: torch.Tensor, normalize_points: bool = False) -> torch.Tensor:\n    """"""Compute the normal surface per pixel.\n\n    Args:\n        depth (torch.Tensor): image tensor containing a depth value per pixel.\n        camera_matrix (torch.Tensor): tensor containing the camera intrinsics.\n        normalize_points (bool): whether to normalise the pointcloud. This\n            must be set to `True` when the depth is represented as the Euclidean\n            ray length from the camera position. Default is `False`.\n\n    Shape:\n        - Input: :math:`(B, 1, H, W)` and :math:`(B, 3, 3)`\n        - Output: :math:`(B, 3, H, W)`\n\n    Return:\n        torch.Tensor: tensor with a normal surface vector per pixel of the same resolution as the input.\n\n    """"""\n    if not isinstance(depth, torch.Tensor):\n        raise TypeError(f""Input depht type is not a torch.Tensor. Got {type(depth)}."")\n\n    if not len(depth.shape) == 4 and depth.shape[-3] == 1:\n        raise ValueError(f""Input depth musth have a shape (B, 1, H, W). Got: {depth.shape}"")\n\n    if not isinstance(camera_matrix, torch.Tensor):\n        raise TypeError(f""Input camera_matrix type is not a torch.Tensor. ""\n                        f""Got {type(camera_matrix)}."")\n\n    if not len(camera_matrix.shape) == 3 and camera_matrix.shape[-2:] == (3, 3):\n        raise ValueError(f""Input camera_matrix must have a shape (B, 3, 3). ""\n                         f""Got: {camera_matrix.shape}."")\n\n    # compute the 3d points from depth\n    xyz: torch.Tensor = depth_to_3d(depth, camera_matrix, normalize_points)  # Bx3xHxW\n\n    # compute the pointcloud spatial gradients\n    gradients: torch.Tensor = spatial_gradient(xyz)  # Bx3x2xHxW\n\n    # compute normals\n    a, b = gradients[:, :, 0], gradients[:, :, 1]  # Bx3xHxW\n\n    normals: torch.Tensor = torch.cross(a, b, dim=1)  # Bx3xHxW\n    return F.normalize(normals, dim=1, p=2)\n\n\ndef warp_frame_depth(\n        image_src: torch.Tensor,\n        depth_dst: torch.Tensor,\n        src_trans_dst: torch.Tensor,\n        camera_matrix: torch.Tensor,\n        normalize_points: bool = False) -> torch.Tensor:\n    """"""Warp a tensor from a source to destination frame by the depth in the destination.\n\n    Compute 3d points from the depth, transform them using given transformation, then project the point cloud to an\n    image plane.\n\n    Args:\n        image_src (torch.Tensor): image tensor in the source frame with shape (BxDxHxW).\n        depth_dst (torch.Tensor): depth tensor in the destination frame with shape (Bx1xHxW).\n        src_trans_dst (torch.Tensor): transformation matrix from destination to source with shape (Bx4x4).\n        camera_matrix (torch.Tensor): tensor containing the camera intrinsics with shape (Bx3x3).\n        normalize_points (bool): whether to normalise the pointcloud. This\n            must be set to `True` when the depth is represented as the Euclidean\n            ray length from the camera position. Default is `False`.\n\n    Return:\n        torch.Tensor: the warped tensor in the source frame with shape (Bx3xHxW).\n\n    """"""\n    if not isinstance(image_src, torch.Tensor):\n        raise TypeError(f""Input image_src type is not a torch.Tensor. Got {type(image_src)}."")\n\n    if not len(image_src.shape) == 4:\n        raise ValueError(f""Input image_src musth have a shape (B, D, H, W). Got: {image_src.shape}"")\n\n    if not isinstance(depth_dst, torch.Tensor):\n        raise TypeError(f""Input depht_dst type is not a torch.Tensor. Got {type(depth_dst)}."")\n\n    if not len(depth_dst.shape) == 4 and depth_dst.shape[-3] == 1:\n        raise ValueError(f""Input depth_dst musth have a shape (B, 1, H, W). Got: {depth_dst.shape}"")\n\n    if not isinstance(src_trans_dst, torch.Tensor):\n        raise TypeError(f""Input src_trans_dst type is not a torch.Tensor. ""\n                        f""Got {type(src_trans_dst)}."")\n\n    if not len(src_trans_dst.shape) == 3 and src_trans_dst.shape[-2:] == (3, 3):\n        raise ValueError(f""Input src_trans_dst must have a shape (B, 3, 3). ""\n                         f""Got: {src_trans_dst.shape}."")\n\n    if not isinstance(camera_matrix, torch.Tensor):\n        raise TypeError(f""Input camera_matrix type is not a torch.Tensor. ""\n                        f""Got {type(camera_matrix)}."")\n\n    if not len(camera_matrix.shape) == 3 and camera_matrix.shape[-2:] == (3, 3):\n        raise ValueError(f""Input camera_matrix must have a shape (B, 3, 3). ""\n                         f""Got: {camera_matrix.shape}."")\n    # unproject source points to camera frame\n    points_3d_dst: torch.Tensor = depth_to_3d(depth_dst, camera_matrix, normalize_points)  # Bx3xHxW\n\n    # transform points from source to destionation\n    points_3d_dst = points_3d_dst.permute(0, 2, 3, 1)  # BxHxWx3\n\n    # apply transformation to the 3d points\n    points_3d_src = transform_points(src_trans_dst[:, None], points_3d_dst)  # BxHxWx3\n\n    # project back to pixels\n    camera_matrix_tmp: torch.Tensor = camera_matrix[:, None, None]  # Bx1x1xHxW\n    points_2d_src: torch.Tensor = project_points(points_3d_src, camera_matrix_tmp)  # BxHxWx2\n\n    # normalize points between [-1 / 1]\n    height, width = depth_dst.shape[-2:]\n    points_2d_src_norm: torch.Tensor = normalize_pixel_coordinates(\n        points_2d_src, height, width)  # BxHxWx2\n\n    return F.grid_sample(image_src, points_2d_src_norm, align_corners=True)  # type: ignore\n'"
kornia/geometry/dsnt.py,35,"b'r""""""Implementation of ""differentiable spatial to numerical"" (soft-argmax)\noperations, as described in the paper ""Numerical Coordinate Regression with\nConvolutional Neural Networks"" by Nibali et al.\n""""""\n\nfrom typing import Tuple\n\nimport torch\nimport torch.nn.functional as F\n\nfrom kornia.testing import check_is_tensor\nfrom kornia.utils.grid import create_meshgrid\n\n\ndef _validate_batched_image_tensor_input(tensor):\n    check_is_tensor(tensor)\n    if not len(tensor.shape) == 4:\n        raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                         .format(tensor.shape))\n\n\ndef spatial_softmax2d(\n        input: torch.Tensor,\n        temperature: torch.Tensor = torch.tensor(1.0),\n) -> torch.Tensor:\n    r""""""Applies the Softmax function over features in each image channel.\n\n    Note that this function behaves differently to `torch.nn.Softmax2d`, which\n    instead applies Softmax over features at each spatial location.\n\n    Returns a 2D probability distribution per image channel.\n\n    Arguments:\n        input (torch.Tensor): the input tensor.\n        temperature (torch.Tensor): factor to apply to input, adjusting the\n          ""smoothness"" of the output distribution. Default is 1.\n\n    Shape:\n        - Input: :math:`(B, N, H, W)`\n        - Output: :math:`(B, N, H, W)`\n    """"""\n    _validate_batched_image_tensor_input(input)\n\n    batch_size, channels, height, width = input.shape\n    x: torch.Tensor = input.view(batch_size, channels, -1)\n\n    x_soft: torch.Tensor = F.softmax(x * temperature, dim=-1)\n\n    return x_soft.view(batch_size, channels, height, width)\n\n\ndef spatial_expectation2d(\n        input: torch.Tensor,\n        normalized_coordinates: bool = True,\n) -> torch.Tensor:\n    r""""""Computes the expectation of coordinate values using spatial probabilities.\n\n    The input heatmap is assumed to represent a valid spatial probability\n    distribution, which can be achieved using\n    :class:`~kornia.geometry.dsnt.spatial_softmax2d`.\n\n    Returns the expected value of the 2D coordinates.\n    The output order of the coordinates is (x, y).\n\n    Arguments:\n        input (torch.Tensor): the input tensor representing dense spatial probabilities.\n        normalized_coordinates (bool): whether to return the\n          coordinates normalized in the range of [-1, 1]. Otherwise,\n          it will return the coordinates in the range of the input shape.\n          Default is True.\n\n    Shape:\n        - Input: :math:`(B, N, H, W)`\n        - Output: :math:`(B, N, 2)`\n\n    Examples:\n        >>> heatmaps = torch.tensor([[[\n            [0., 0., 0.],\n            [0., 0., 0.],\n            [0., 1., 0.]]]])\n        >>> coords = spatial_expectation_2d(heatmaps, False)\n        tensor([[[1.0000, 2.0000]]])\n    """"""\n    _validate_batched_image_tensor_input(input)\n\n    batch_size, channels, height, width = input.shape\n\n    # Create coordinates grid.\n    grid: torch.Tensor = create_meshgrid(height, width, normalized_coordinates, input.device)\n    grid = grid.to(input.dtype)\n\n    pos_x: torch.Tensor = grid[..., 0].reshape(-1)\n    pos_y: torch.Tensor = grid[..., 1].reshape(-1)\n\n    input_flat: torch.Tensor = input.view(batch_size, channels, -1)\n\n    # Compute the expectation of the coordinates.\n    expected_y: torch.Tensor = torch.sum(pos_y * input_flat, -1, keepdim=True)\n    expected_x: torch.Tensor = torch.sum(pos_x * input_flat, -1, keepdim=True)\n\n    output: torch.Tensor = torch.cat([expected_x, expected_y], -1)\n\n    return output.view(batch_size, channels, 2)  # BxNx2\n\n\ndef _safe_zero_division(\n        numerator: torch.Tensor,\n        denominator: torch.Tensor,\n        eps: float = 1e-32,\n) -> torch.Tensor:\n    return numerator / torch.clamp(denominator, min=eps)\n\n\ndef render_gaussian2d(\n        mean: torch.Tensor,\n        std: torch.Tensor,\n        size: Tuple[int, int],\n        normalized_coordinates: bool = True,\n):\n    r""""""Renders the PDF of a 2D Gaussian distribution.\n\n    Arguments:\n        mean (torch.Tensor): the mean location of the Gaussian to render,\n          :math:`(\\mu_x, \\mu_y)`.\n        std (torch.Tensor): the standard deviation of the Gaussian to render,\n          :math:`(\\sigma_x, \\sigma_y)`.\n        size (list): the (height, width) of the output image.\n        normalized_coordinates: whether `mean` and `std` are assumed to use\n          coordinates normalized in the range of [-1, 1]. Otherwise,\n          coordinates are assumed to be in the range of the output shape.\n          Default is True.\n\n    Shape:\n        - `mean`: :math:`(*, 2)`\n        - `std`: :math:`(*, 2)`. Should be able to be broadcast with `mean`.\n        - Output: :math:`(*, H, W)`\n    """"""\n    if not (std.dtype == mean.dtype and std.device == mean.device):\n        raise TypeError(""Expected inputs to have the same dtype and device"")\n    height, width = size\n\n    # Create coordinates grid.\n    grid: torch.Tensor = create_meshgrid(height, width, normalized_coordinates, mean.device)\n    grid = grid.to(mean.dtype)\n    pos_x: torch.Tensor = grid[..., 0].view(height, width)\n    pos_y: torch.Tensor = grid[..., 1].view(height, width)\n\n    # Gaussian PDF = exp(-(x - \\mu)^2 / (2 \\sigma^2))\n    #              = exp(dists * ks),\n    #                where dists = (x - \\mu)^2 and ks = -1 / (2 \\sigma^2)\n\n    # dists <- (x - \\mu)^2\n    dist_x = (pos_x - mean[..., 0, None, None]) ** 2\n    dist_y = (pos_y - mean[..., 1, None, None]) ** 2\n\n    # ks <- -1 / (2 \\sigma^2)\n    k_x = -0.5 * torch.reciprocal(std[..., 0, None, None])\n    k_y = -0.5 * torch.reciprocal(std[..., 1, None, None])\n\n    # Assemble the 2D Gaussian.\n    exps_x = torch.exp(dist_x * k_x)\n    exps_y = torch.exp(dist_y * k_y)\n    gauss = exps_x * exps_y\n\n    # Rescale so that values sum to one.\n    val_sum = gauss.sum(-2, keepdim=True).sum(-1, keepdim=True)\n    gauss = _safe_zero_division(gauss, val_sum)\n\n    return gauss\n'"
kornia/geometry/linalg.py,73,"b'from typing import Optional\n\nimport torch\n\nimport kornia\nfrom kornia.geometry.conversions import convert_points_to_homogeneous\nfrom kornia.geometry.conversions import convert_points_from_homogeneous\nfrom kornia.testing import check_is_tensor\n\n\n__all__ = [\n    ""compose_transformations"",\n    ""relative_transformation"",\n    ""inverse_transformation"",\n    ""transform_points"",\n    ""transform_boxes"",\n    ""perspective_transform_lafs"",\n]\n\n\ndef compose_transformations(\n        trans_01: torch.Tensor, trans_12: torch.Tensor) -> torch.Tensor:\n    r""""""Functions that composes two homogeneous transformations.\n\n    .. math::\n\n        T_0^{2} = \\begin{bmatrix} R_0^1 R_1^{2} & R_0^{1} t_1^{2} + t_0^{1} \\\\\n        \\mathbf{0} & 1\\end{bmatrix}\n\n    Args:\n        trans_01 (torch.Tensor): tensor with the homogenous transformation from\n          a reference frame 1 respect to a frame 0. The tensor has must have a\n          shape of :math:`(B, 4, 4)` or :math:`(4, 4)`.\n        trans_12 (torch.Tensor): tensor with the homogenous transformation from\n          a reference frame 2 respect to a frame 1. The tensor has must have a\n          shape of :math:`(B, 4, 4)` or :math:`(4, 4)`.\n\n    Shape:\n        - Output: :math:`(N, 4, 4)` or :math:`(4, 4)`\n\n    Returns:\n        torch.Tensor: the transformation between the two frames.\n\n    Example::\n        >>> trans_01 = torch.eye(4)  # 4x4\n        >>> trans_12 = torch.eye(4)  # 4x4\n        >>> trans_02 = kornia.compose_transformations(trans_01, trans_12)  # 4x4\n\n    """"""\n    if not torch.is_tensor(trans_01):\n        raise TypeError(""Input trans_01 type is not a torch.Tensor. Got {}""\n                        .format(type(trans_01)))\n    if not torch.is_tensor(trans_12):\n        raise TypeError(""Input trans_12 type is not a torch.Tensor. Got {}""\n                        .format(type(trans_12)))\n    if not trans_01.dim() in (2, 3) and trans_01.shape[-2:] == (4, 4):\n        raise ValueError(""Input trans_01 must be a of the shape Nx4x4 or 4x4.""\n                         "" Got {}"".format(trans_01.shape))\n    if not trans_12.dim() in (2, 3) and trans_12.shape[-2:] == (4, 4):\n        raise ValueError(""Input trans_12 must be a of the shape Nx4x4 or 4x4.""\n                         "" Got {}"".format(trans_12.shape))\n    if not trans_01.dim() == trans_12.dim():\n        raise ValueError(""Input number of dims must match. Got {} and {}""\n                         .format(trans_01.dim(), trans_12.dim()))\n    # unpack input data\n    rmat_01: torch.Tensor = trans_01[..., :3, :3]  # Nx3x3\n    rmat_12: torch.Tensor = trans_12[..., :3, :3]  # Nx3x3\n    tvec_01: torch.Tensor = trans_01[..., :3, -1:]  # Nx3x1\n    tvec_12: torch.Tensor = trans_12[..., :3, -1:]  # Nx3x1\n\n    # compute the actual transforms composition\n    rmat_02: torch.Tensor = torch.matmul(rmat_01, rmat_12)\n    tvec_02: torch.Tensor = torch.matmul(rmat_01, tvec_12) + tvec_01\n\n    # pack output tensor\n    trans_02: torch.Tensor = torch.zeros_like(trans_01)\n    trans_02[..., :3, 0:3] += rmat_02\n    trans_02[..., :3, -1:] += tvec_02\n    trans_02[..., -1, -1:] += 1.0\n    return trans_02\n\n\ndef inverse_transformation(trans_12):\n    r""""""Function that inverts a 4x4 homogeneous transformation\n    :math:`T_1^{2} = \\begin{bmatrix} R_1 & t_1 \\\\ \\mathbf{0} & 1 \\end{bmatrix}`\n\n    The inverse transformation is computed as follows:\n\n    .. math::\n\n        T_2^{1} = (T_1^{2})^{-1} = \\begin{bmatrix} R_1^T & -R_1^T t_1 \\\\\n        \\mathbf{0} & 1\\end{bmatrix}\n\n    Args:\n        trans_12 (torch.Tensor): transformation tensor of shape\n          :math:`(N, 4, 4)` or :math:`(4, 4)`.\n\n    Returns:\n        torch.Tensor: tensor with inverted transformations.\n\n    Shape:\n        - Output: :math:`(N, 4, 4)` or :math:`(4, 4)`\n\n    Example:\n        >>> trans_12 = torch.rand(1, 4, 4)  # Nx4x4\n        >>> trans_21 = kornia.inverse_transformation(trans_12)  # Nx4x4\n    """"""\n    if not torch.is_tensor(trans_12):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(trans_12)))\n    if not trans_12.dim() in (2, 3) and trans_12.shape[-2:] == (4, 4):\n        raise ValueError(""Input size must be a Nx4x4 or 4x4. Got {}""\n                         .format(trans_12.shape))\n    # unpack input tensor\n    rmat_12: torch.Tensor = trans_12[..., :3, 0:3]  # Nx3x3\n    tvec_12: torch.Tensor = trans_12[..., :3, 3:4]  # Nx3x1\n\n    # compute the actual inverse\n    rmat_21: torch.Tensor = torch.transpose(rmat_12, -1, -2)\n    tvec_21: torch.Tensor = torch.matmul(-rmat_21, tvec_12)\n\n    # pack to output tensor\n    trans_21: torch.Tensor = torch.zeros_like(trans_12)\n    trans_21[..., :3, 0:3] += rmat_21\n    trans_21[..., :3, -1:] += tvec_21\n    trans_21[..., -1, -1:] += 1.0\n    return trans_21\n\n\ndef relative_transformation(\n        trans_01: torch.Tensor, trans_02: torch.Tensor) -> torch.Tensor:\n    r""""""Function that computes the relative homogenous transformation from a\n    reference transformation :math:`T_1^{0} = \\begin{bmatrix} R_1 & t_1 \\\\\n    \\mathbf{0} & 1 \\end{bmatrix}` to destination :math:`T_2^{0} =\n    \\begin{bmatrix} R_2 & t_2 \\\\ \\mathbf{0} & 1 \\end{bmatrix}`.\n\n    The relative transformation is computed as follows:\n\n    .. math::\n\n        T_1^{2} = (T_0^{1})^{-1} \\cdot T_0^{2}\n\n    Arguments:\n        trans_01 (torch.Tensor): reference transformation tensor of shape\n         :math:`(N, 4, 4)` or :math:`(4, 4)`.\n        trans_02 (torch.Tensor): destination transformation tensor of shape\n         :math:`(N, 4, 4)` or :math:`(4, 4)`.\n\n    Shape:\n        - Output: :math:`(N, 4, 4)` or :math:`(4, 4)`.\n\n    Returns:\n        torch.Tensor: the relative transformation between the transformations.\n\n    Example::\n        >>> trans_01 = torch.eye(4)  # 4x4\n        >>> trans_02 = torch.eye(4)  # 4x4\n        >>> trans_12 = kornia.relative_transformation(trans_01, trans_02)  # 4x4\n    """"""\n    if not torch.is_tensor(trans_01):\n        raise TypeError(""Input trans_01 type is not a torch.Tensor. Got {}""\n                        .format(type(trans_01)))\n    if not torch.is_tensor(trans_02):\n        raise TypeError(""Input trans_02 type is not a torch.Tensor. Got {}""\n                        .format(type(trans_02)))\n    if not trans_01.dim() in (2, 3) and trans_01.shape[-2:] == (4, 4):\n        raise ValueError(""Input must be a of the shape Nx4x4 or 4x4.""\n                         "" Got {}"".format(trans_01.shape))\n    if not trans_02.dim() in (2, 3) and trans_02.shape[-2:] == (4, 4):\n        raise ValueError(""Input must be a of the shape Nx4x4 or 4x4.""\n                         "" Got {}"".format(trans_02.shape))\n    if not trans_01.dim() == trans_02.dim():\n        raise ValueError(""Input number of dims must match. Got {} and {}""\n                         .format(trans_01.dim(), trans_02.dim()))\n    trans_10: torch.Tensor = inverse_transformation(trans_01)\n    trans_12: torch.Tensor = compose_transformations(trans_10, trans_02)\n    return trans_12\n\n\ndef transform_points(trans_01: torch.Tensor,\n                     points_1: torch.Tensor) -> torch.Tensor:\n    r""""""Function that applies transformations to a set of points.\n\n    Args:\n        trans_01 (torch.Tensor): tensor for transformations of shape\n          :math:`(B, D+1, D+1)`.\n        points_1 (torch.Tensor): tensor of points of shape :math:`(B, N, D)`.\n    Returns:\n        torch.Tensor: tensor of N-dimensional points.\n\n    Shape:\n        - Output: :math:`(B, N, D)`\n\n    Examples:\n\n        >>> points_1 = torch.rand(2, 4, 3)  # BxNx3\n        >>> trans_01 = torch.eye(4).view(1, 4, 4)  # Bx4x4\n        >>> points_0 = kornia.transform_points(trans_01, points_1)  # BxNx3\n    """"""\n    check_is_tensor(trans_01)\n    check_is_tensor(points_1)\n    if not trans_01.device == points_1.device:\n        raise TypeError(""Tensor must be in the same device"")\n    if not trans_01.shape[0] == points_1.shape[0] and trans_01.shape[0] != 1:\n        raise ValueError(""Input batch size must be the same for both tensors or 1"")\n    if not trans_01.shape[-1] == (points_1.shape[-1] + 1):\n        raise ValueError(""Last input dimensions must differe by one unit"")\n    # to homogeneous\n    points_1_h = convert_points_to_homogeneous(points_1)  # BxNxD+1\n    # transform coordinates\n    points_0_h = torch.matmul(\n        trans_01.unsqueeze(1), points_1_h.unsqueeze(-1))\n    points_0_h = torch.squeeze(points_0_h, dim=-1)\n    # to euclidean\n    points_0 = convert_points_from_homogeneous(points_0_h)  # BxNxD\n    return points_0\n\n\ndef transform_boxes(trans_mat: torch.Tensor, boxes: torch.Tensor, mode: str = ""xyxy"") -> torch.Tensor:\n    r"""""" Function that applies a transformation matrix to a box or batch of boxes. Boxes must\n    be a tensor of the shape (N, 4) or a batch of boxes (B, N, 4) and trans_mat must be a (3, 3)\n    transformation matrix or a batch of transformation matrices (B, 3, 3)\n\n    Args:\n        trans_mat (torch.Tensor): The transformation matrix to be applied\n        boxes (torch.Tensor): The boxes to be transformed\n        mode (str): The format in which the boxes are provided. If set to \'xyxy\' the boxes\n                    are assumed to be in the format (xmin, ymin, xmax, ymax). If set to \'xywh\'\n                    the boxes are assumed to be in the format (xmin, ymin, width, height).\n                    Default: \'xyxy\'\n    Returns:\n        torch.Tensor: The set of transformed points in the specified mode\n\n\n    """"""\n\n    if not torch.is_tensor(boxes):\n        raise TypeError(f""Boxes type is not a torch.Tensor. Got {type(boxes)}"")\n\n    if not torch.is_tensor(trans_mat):\n        raise TypeError(f""Tranformation matrix type is not a torch.Tensor. Got {type(trans_mat)}"")\n\n    if not isinstance(mode, str):\n        raise TypeError(f""Mode must be a string. Got {type(mode)}"")\n\n    if mode not in (""xyxy"", ""xywh""):\n        raise ValueError(f""Mode must be one of \'xyxy\', \'xywh\'. Got {mode}"")\n\n    # convert boxes to format xyxy\n    if mode == ""xywh"":\n        boxes[..., -2] = boxes[..., 0] + boxes[..., -2]  # x + w\n        boxes[..., -1] = boxes[..., 1] + boxes[..., -1]  # y + h\n\n    transformed_boxes: torch.Tensor = kornia.transform_points(trans_mat, boxes.view(boxes.shape[0], -1, 2))\n    transformed_boxes = transformed_boxes.view_as(boxes)\n\n    if mode == \'xywh\':\n        transformed_boxes[..., 2] = transformed_boxes[..., 2] - transformed_boxes[..., 0]\n        transformed_boxes[..., 3] = transformed_boxes[..., 3] - transformed_boxes[..., 1]\n\n    return transformed_boxes\n\n\ndef perspective_transform_lafs(trans_01: torch.Tensor,\n                               lafs_1: torch.Tensor) -> torch.Tensor:\n    r""""""Function that applies perspective transformations to a set of local affine frames (LAFs).\n\n    Args:\n        trans_01 (torch.Tensor): tensor for perspective transformations of shape\n          :math:`(B, 3, 3)`.\n        lafs_1 (torch.Tensor): tensor of lafs of shape :math:`(B, N, 2, 3)`.\n    Returns:\n        torch.Tensor: tensor of N-dimensional points.\n\n    Shape:\n        - Output: :math:`(B, N, 2, 3)`\n\n    Examples:\n\n        >>> lafs_1 = torch.rand(2, 4, 2, 3)  # BxNx2x3\n        >>> trans_01 = torch.eye(3).view(1, 3, 3)  # Bx3x3\n        >>> lafs_0 = kornia.perspective_transform_lafs(trans_01, lafs_1)  # BxNx2x3\n    """"""\n    kornia.feature.laf.raise_error_if_laf_is_not_valid(lafs_1)\n    if not torch.is_tensor(trans_01):\n        raise TypeError(""Input type is not a torch.Tensor"")\n    if not trans_01.device == lafs_1.device:\n        raise TypeError(""Tensor must be in the same device"")\n    if not trans_01.shape[0] == lafs_1.shape[0]:\n        raise ValueError(""Input batch size must be the same for both tensors"")\n    if (not (trans_01.shape[-1] == 3)) or (not (trans_01.shape[-2] == 3)):\n        raise ValueError(""Tranformation should be homography"")\n    bs, n, _, _ = lafs_1.size()\n    # First, we convert LAF to points\n    threepts_1 = kornia.feature.laf.laf_to_three_points(lafs_1)\n    points_1 = threepts_1.permute(0, 1, 3, 2).reshape(bs, n * 3, 2)\n\n    # First, transform the points\n    points_0 = transform_points(trans_01, points_1)\n    # Back to LAF format\n    threepts_0 = points_0.view(bs, n, 3, 2).permute(0, 1, 3, 2)\n    return kornia.feature.laf.laf_from_three_points(threepts_0)\n\n\n# TODO:\n# - project_points: from opencv\n\n\n# layer api\n\n# NOTE: is it needed ?\n\'\'\'class TransformPoints(nn.Module):\n    r""""""Creates an object to transform a set of points.\n\n    Args:\n        dst_pose_src (torhc.Tensor): tensor for transformations of\n          shape :math:`(B, D+1, D+1)`.\n\n    Returns:\n        torch.Tensor: tensor of N-dimensional points.\n\n    Shape:\n        - Input: :math:`(B, D, N)`\n        - Output: :math:`(B, N, D)`\n\n    Examples:\n        >>> input = torch.rand(2, 4, 3)  # BxNx3\n        >>> transform = torch.eye(4).view(1, 4, 4)   # Bx4x4\n        >>> transform_op = kornia.TransformPoints(transform)\n        >>> output = transform_op(input)  # BxNx3\n    """"""\n\n    def __init__(self, dst_homo_src: torch.Tensor) -> None:\n        super(TransformPoints, self).__init__()\n        self.dst_homo_src: torch.Tensor = dst_homo_src\n\n    def forward(self, points_src: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return transform_points(self.dst_homo_src, points_src)\n\n\nclass InversePose(nn.Module):\n    r""""""Creates a transformation that inverts a 4x4 pose.\n\n    Args:\n        points (Tensor): tensor with poses.\n\n    Returns:\n        Tensor: tensor with inverted poses.\n\n    Shape:\n        - Input: :math:`(N, 4, 4)`\n        - Output: :math:`(N, 4, 4)`\n\n    Example:\n        >>> pose = torch.rand(1, 4, 4)  # Nx4x4\n        >>> transform = kornia.InversePose()\n        >>> pose_inv = transform(pose)  # Nx4x4\n    """"""\n\n    def __init__(self):\n        super(InversePose, self).__init__()\n\n    def forward(self, input):\n        return inverse_pose(input)\'\'\'\n'"
kornia/geometry/spatial_soft_argmax.py,79,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport kornia\nfrom kornia.geometry import dsnt\nfrom kornia.utils import create_meshgrid, create_meshgrid3d\nfrom kornia.geometry import normalize_pixel_coordinates, normalize_pixel_coordinates3d\nfrom typing import Tuple, Union\n\n\ndef _get_window_grid_kernel2d(h: int, w: int,\n                              device: torch.device = torch.device(\'cpu\')) -> torch.Tensor:\n    r""""""Helper function, which generates a kernel to with window coordinates,\n       residual to window center.\n\n    Args:\n         h (int): kernel height.\n         w (int): kernel width.\n         device (torch.device): device, on which generate.\n\n    Returns:\n        conv_kernel (torch.Tensor) [2x1xhxw]\n    """"""\n    window_grid2d = create_meshgrid(h, w, False, device=device)\n    window_grid2d = normalize_pixel_coordinates(window_grid2d, h, w)\n    conv_kernel = window_grid2d.permute(3, 0, 1, 2)\n    return conv_kernel\n\n\ndef _get_center_kernel2d(h: int, w: int,\n                         device: torch.device = torch.device(\'cpu\')) -> torch.Tensor:\n    r""""""Helper function, which generates a kernel to return center coordinates,\n       when applied with F.conv2d to 2d coordinates grid.\n\n    Args:\n         h (int): kernel height.\n         w (int): kernel width.\n         device (torch.device): device, on which generate.\n\n    Returns:\n        conv_kernel (torch.Tensor) [2x2xhxw]\n    """"""\n    center_kernel = torch.zeros(2, 2, h, w, device=device)\n\n    #  If the size is odd, we have one pixel for center, if even - 2\n    if h % 2 != 0:\n        h_i1 = h // 2\n        h_i2 = (h // 2) + 1\n    else:\n        h_i1 = (h // 2) - 1\n        h_i2 = (h // 2) + 1\n    if w % 2 != 0:\n        w_i1 = w // 2\n        w_i2 = (w // 2) + 1\n    else:\n        w_i1 = (w // 2) - 1\n        w_i2 = (w // 2) + 1\n    center_kernel[(0, 1), (0, 1), h_i1: h_i2, w_i1: w_i2] = 1.0 / float(((h_i2 - h_i1) * (w_i2 - w_i1)))\n    return center_kernel\n\n\ndef _get_center_kernel3d(d: int, h: int, w: int,\n                         device: torch.device = torch.device(\'cpu\')) -> torch.Tensor:\n    r""""""Helper function, which generates a kernel to return center coordinates,\n       when applied with F.conv2d to 3d coordinates grid.\n\n    Args:\n         d (int): kernel depth.\n         h (int): kernel height.\n         w (int): kernel width.\n         device (torch.device): device, on which generate.\n\n    Returns:\n        conv_kernel (torch.Tensor) [3x3xdxhxw]\n    """"""\n    center_kernel = torch.zeros(3, 3, d, h, w, device=device)\n    #  If the size is odd, we have one pixel for center, if even - 2\n    if h % 2 != 0:\n        h_i1 = h // 2\n        h_i2 = (h // 2) + 1\n    else:\n        h_i1 = (h // 2) - 1\n        h_i2 = (h // 2) + 1\n    if w % 2 != 0:\n        w_i1 = w // 2\n        w_i2 = (w // 2) + 1\n    else:\n        w_i1 = (w // 2) - 1\n        w_i2 = (w // 2) + 1\n    if d % 2 != 0:\n        d_i1 = d // 2\n        d_i2 = (d // 2) + 1\n    else:\n        d_i1 = (d // 2) - 1\n        d_i2 = (d // 2) + 1\n    center_num = float((h_i2 - h_i1) * (w_i2 - w_i1) * (d_i2 - d_i1))\n    center_kernel[(0, 1, 2), (0, 1, 2), d_i1: d_i2, h_i1: h_i2, w_i1: w_i2] = 1.0 / center_num\n    return center_kernel\n\n\ndef _get_window_grid_kernel3d(d: int, h: int, w: int,\n                              device: torch.device = torch.device(\'cpu\')) -> torch.Tensor:\n    r""""""Helper function, which generates a kernel to return coordinates,\n       residual to window center.\n\n    Args:\n         d (int): kernel depth.\n         h (int): kernel height.\n         w (int): kernel width.\n         device (torch.device): device, on which generate.\n\n    Returns:\n        conv_kernel (torch.Tensor) [3x1xdxhxw]\n    """"""\n    grid2d = create_meshgrid(h, w, True, device=device)\n    if d > 1:\n        z = torch.linspace(-1, 1, d, device=device).view(d, 1, 1, 1)\n    else:  # only onr channel with index == 0\n        z = torch.zeros(1, 1, 1, 1, device=device)\n    grid3d = torch.cat([z.repeat(1, h, w, 1).contiguous(), grid2d.repeat(d, 1, 1, 1)], dim=3)\n    conv_kernel = grid3d.permute(3, 0, 1, 2).unsqueeze(1)\n    return conv_kernel\n\n\nclass ConvSoftArgmax2d(nn.Module):\n    r""""""Module that calculates soft argmax 2d per window.\n\n    See :func:`~kornia.geometry.conv_soft_argmax2d` for details.\n    """"""\n\n    def __init__(self,\n                 kernel_size: Tuple[int, int] = (3, 3),\n                 stride: Tuple[int, int] = (1, 1),\n                 padding: Tuple[int, int] = (1, 1),\n                 temperature: Union[torch.Tensor, float] = torch.tensor(1.0),\n                 normalized_coordinates: bool = True,\n                 eps: float = 1e-8,\n                 output_value: bool = False) -> None:\n        super(ConvSoftArgmax2d, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.temperature = temperature\n        self.normalized_coordinates = normalized_coordinates\n        self.eps = eps\n        self.output_value = output_value\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ +\\\n            \'(\' + \'kernel_size=\' + str(self.kernel_size) +\\\n            \', \' + \'stride=\' + str(self.stride) +\\\n            \', \' + \'padding=\' + str(self.padding) +\\\n            \', \' + \'temperature=\' + str(self.temperature) +\\\n            \', \' + \'normalized_coordinates=\' + str(self.normalized_coordinates) +\\\n            \', \' + \'eps=\' + str(self.eps) +\\\n            \', \' + \'output_value=\' + str(self.output_value) + \')\'\n\n    def forward(self, x: torch.Tensor):  # type: ignore\n        return conv_soft_argmax2d(x,\n                                  self.kernel_size,\n                                  self.stride,\n                                  self.padding,\n                                  self.temperature,\n                                  self.normalized_coordinates,\n                                  self.eps,\n                                  self.output_value)\n\n\nclass ConvSoftArgmax3d(nn.Module):\n    r""""""Module that calculates soft argmax 3d per window.\n\n    See :func:`~kornia.geometry.conv_soft_argmax3d` for details.\n    """"""\n\n    def __init__(self,\n                 kernel_size: Tuple[int, int, int] = (3, 3, 3),\n                 stride: Tuple[int, int, int] = (1, 1, 1),\n                 padding: Tuple[int, int, int] = (1, 1, 1),\n                 temperature: Union[torch.Tensor, float] = torch.tensor(1.0),\n                 normalized_coordinates: bool = False,\n                 eps: float = 1e-8,\n                 output_value: bool = True,\n                 strict_maxima_bonus: float = 0.0) -> None:\n        super(ConvSoftArgmax3d, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.temperature = temperature\n        self.normalized_coordinates = normalized_coordinates\n        self.eps = eps\n        self.output_value = output_value\n        self.strict_maxima_bonus = strict_maxima_bonus\n        return\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ +\\\n            \'(\' + \'kernel_size=\' + str(self.kernel_size) +\\\n            \', \' + \'stride=\' + str(self.stride) +\\\n            \', \' + \'padding=\' + str(self.padding) +\\\n            \', \' + \'temperature=\' + str(self.temperature) +\\\n            \', \' + \'normalized_coordinates=\' + str(self.normalized_coordinates) +\\\n            \', \' + \'eps=\' + str(self.eps) +\\\n            \', \' + \'strict_maxima_bonus=\' + str(self.strict_maxima_bonus) +\\\n            \', \' + \'output_value=\' + str(self.output_value) + \')\'\n\n    def forward(self, x: torch.Tensor):  # type: ignore\n        return conv_soft_argmax3d(x,\n                                  self.kernel_size,\n                                  self.stride,\n                                  self.padding,\n                                  self.temperature,\n                                  self.normalized_coordinates,\n                                  self.eps,\n                                  self.output_value,\n                                  self.strict_maxima_bonus)\n\n\ndef conv_soft_argmax2d(input: torch.Tensor,\n                       kernel_size: Tuple[int, int] = (3, 3),\n                       stride: Tuple[int, int] = (1, 1),\n                       padding: Tuple[int, int] = (1, 1),\n                       temperature: Union[torch.Tensor, float] = torch.tensor(1.0),\n                       normalized_coordinates: bool = True,\n                       eps: float = 1e-8,\n                       output_value: bool = False) -> Union[torch.Tensor,\n                                                            Tuple[torch.Tensor, torch.Tensor]]:\n    r""""""Function that computes the convolutional spatial Soft-Argmax 2D over the windows\n    of a given input heatmap. Function has two outputs: argmax coordinates and the softmaxpooled heatmap values\n    themselves. On each window, the function computed is\n\n    .. math::\n             ij(X) = \\frac{\\sum{(i,j)} * exp(x / T)  \\in X} {\\sum{exp(x / T)  \\in X}}\n\n    .. math::\n             val(X) = \\frac{\\sum{x * exp(x / T)  \\in X}} {\\sum{exp(x / T)  \\in X}}\n\n    where T is temperature.\n\n    Args:\n        kernel_size (Tuple[int,int]): the size of the window\n        stride  (Tuple[int,int]): the stride of the window.\n        padding (Tuple[int,int]): input zero padding\n        temperature (torch.Tensor): factor to apply to input. Default is 1.\n        normalized_coordinates (bool): whether to return the coordinates normalized in the range of [-1, 1]. Otherwise,\n                                       it will return the coordinates in the range of the input shape. Default is True.\n        eps (float): small value to avoid zero division. Default is 1e-8.\n        output_value (bool): if True, val is outputed, if False, only ij\n\n    Shape:\n        - Input: :math:`(N, C, H_{in}, W_{in})`\n        - Output: :math:`(N, C, 2, H_{out}, W_{out})`, :math:`(N, C, H_{out}, W_{out})`, where\n\n         .. math::\n                  H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] -\n                  (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\n         .. math::\n                  W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] -\n                  (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\n    Examples::\n        >>> input = torch.randn(20, 16, 50, 32)\n        >>> nms_coords, nms_val = conv_soft_argmax2d(input, (3,3), (2,2), (1,1))\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(input)))\n\n    if not len(input.shape) == 4:\n        raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                         .format(input.shape))\n\n    if temperature <= 0:\n        raise ValueError(""Temperature should be positive float or tensor. Got: {}""\n                         .format(temperature))\n\n    b, c, h, w = input.shape\n    kx, ky = kernel_size\n    device: torch.device = input.device\n    dtype: torch.dtype = input.dtype\n    input = input.view(b * c, 1, h, w)\n\n    center_kernel: torch.Tensor = _get_center_kernel2d(kx, ky, device).to(dtype)\n    window_kernel: torch.Tensor = _get_window_grid_kernel2d(kx, ky, device).to(dtype)\n\n    # applies exponential normalization trick\n    # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n    # https://github.com/pytorch/pytorch/blob/bcb0bb7e0e03b386ad837015faba6b4b16e3bfb9/aten/src/ATen/native/SoftMax.cpp#L44\n    x_max = F.adaptive_max_pool2d(input, (1, 1))\n\n    # max is detached to prevent undesired backprop loops in the graph\n    x_exp = ((input - x_max.detach()) / temperature).exp()\n\n    # F.avg_pool2d(.., divisor_override = 1.0) - proper way for sum pool in PyTorch 1.2.\n    # Not available yet in version 1.0, so let\'s do manually\n    pool_coef: float = float(kx * ky)\n\n    # softmax denominator\n    den = pool_coef * F.avg_pool2d(x_exp, kernel_size, stride=stride, padding=padding) + eps\n\n    x_softmaxpool = pool_coef * F.avg_pool2d(x_exp * input,\n                                             kernel_size,\n                                             stride=stride,\n                                             padding=padding) / den\n    x_softmaxpool = x_softmaxpool.view(b, c, x_softmaxpool.size(2), x_softmaxpool.size(3))\n\n    # We need to output also coordinates\n    # Pooled window center coordinates\n    grid_global: torch.Tensor = create_meshgrid(h, w, False, device).to(\n        dtype).permute(0, 3, 1, 2)\n\n    grid_global_pooled = F.conv2d(grid_global,\n                                  center_kernel,\n                                  stride=stride,\n                                  padding=padding)\n\n    # Coordinates of maxima residual to window center\n    # prepare kernel\n    coords_max: torch.Tensor = F.conv2d(x_exp,\n                                        window_kernel,\n                                        stride=stride,\n                                        padding=padding)\n\n    coords_max = coords_max / den.expand_as(coords_max)\n    coords_max = coords_max + grid_global_pooled.expand_as(coords_max)\n    # [:,:, 0, ...] is x\n    # [:,:, 1, ...] is y\n\n    if normalized_coordinates:\n        coords_max = normalize_pixel_coordinates(coords_max.permute(0, 2, 3, 1), h, w)\n        coords_max = coords_max.permute(0, 3, 1, 2)\n\n    # Back B*C -> (b, c)\n    coords_max = coords_max.view(b, c, 2, coords_max.size(2), coords_max.size(3))\n\n    if output_value:\n        return coords_max, x_softmaxpool\n    return coords_max\n\n\ndef conv_soft_argmax3d(input: torch.Tensor,\n                       kernel_size: Tuple[int, int, int] = (3, 3, 3),\n                       stride: Tuple[int, int, int] = (1, 1, 1),\n                       padding: Tuple[int, int, int] = (1, 1, 1),\n                       temperature: Union[torch.Tensor, float] = torch.tensor(1.0),\n                       normalized_coordinates: bool = False,\n                       eps: float = 1e-8,\n                       output_value: bool = True,\n                       strict_maxima_bonus: float = 0.0) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n    r""""""Function that computes the convolutional spatial Soft-Argmax 3D over the windows\n    of a given input heatmap. Function has two outputs: argmax coordinates and the softmaxpooled heatmap values\n    themselves.\n    On each window, the function computed is:\n\n    .. math::\n             ijk(X) = \\frac{\\sum{(i,j,k)} * exp(x / T)  \\in X} {\\sum{exp(x / T)  \\in X}}\n\n    .. math::\n             val(X) = \\frac{\\sum{x * exp(x / T)  \\in X}} {\\sum{exp(x / T)  \\in X}}\n\n    where T is temperature.\n\n    Args:\n        kernel_size (Tuple[int,int,int]):  size of the window\n        stride (Tuple[int,int,int]): stride of the window.\n        padding (Tuple[int,int,int]): input zero padding\n        temperature (torch.Tensor): factor to apply to input. Default is 1.\n        normalized_coordinates (bool): whether to return the coordinates normalized in the range of [-1, 1]. Otherwise,\n                                       it will return the coordinates in the range of the input shape. Default is False.\n        eps (float): small value to avoid zero division. Default is 1e-8.\n        output_value (bool): if True, val is outputed, if False, only ij\n        strict_maxima_bonus (float): pixels, which are strict maxima will score (1 + strict_maxima_bonus) * value.\n                                     This is needed for mimic behavior of strict NMS in classic local features\n    Shape:\n        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`\n        - Output: :math:`(N, C, 3, D_{out}, H_{out}, W_{out})`, :math:`(N, C, D_{out}, H_{out}, W_{out})`, where\n\n         .. math::\n             D_{out} = \\left\\lfloor\\frac{D_{in}  + 2 \\times \\text{padding}[0] -\n             (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\n         .. math::\n             H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[1] -\n             (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\n         .. math::\n             W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[2] -\n             (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n\n    Examples:\n        >>> input = torch.randn(20, 16, 3, 50, 32)\n        >>> nms_coords, nms_val = conv_soft_argmax2d(input, (3, 3, 3), (1, 2, 2), (0, 1, 1))\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(input)))\n\n    if not len(input.shape) == 5:\n        raise ValueError(""Invalid input shape, we expect BxCxDxHxW. Got: {}""\n                         .format(input.shape))\n\n    if temperature <= 0:\n        raise ValueError(""Temperature should be positive float or tensor. Got: {}""\n                         .format(temperature))\n\n    b, c, d, h, w = input.shape\n    kx, ky, kz = kernel_size\n    device: torch.device = input.device\n    dtype: torch.dtype = input.dtype\n    input = input.view(b * c, 1, d, h, w)\n\n    center_kernel: torch.Tensor = _get_center_kernel3d(kx, ky, kz, device).to(dtype)\n    window_kernel: torch.Tensor = _get_window_grid_kernel3d(kx, ky, kz, device).to(dtype)\n\n    # applies exponential normalization trick\n    # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n    # https://github.com/pytorch/pytorch/blob/bcb0bb7e0e03b386ad837015faba6b4b16e3bfb9/aten/src/ATen/native/SoftMax.cpp#L44\n    x_max = F.adaptive_max_pool3d(input, (1, 1, 1))\n\n    # max is detached to prevent undesired backprop loops in the graph\n    x_exp = ((input - x_max.detach()) / temperature).exp()\n\n    pool_coef: float = float(kx * ky * kz)\n\n    # softmax denominator\n    den = pool_coef * F.avg_pool3d(x_exp.view_as(input),\n                                   kernel_size,\n                                   stride=stride,\n                                   padding=padding) + eps\n\n    # We need to output also coordinates\n    # Pooled window center coordinates\n    grid_global: torch.Tensor = create_meshgrid3d(\n        d, h, w, False, device=device).to(dtype).permute(0, 4, 1, 2, 3)\n\n    grid_global_pooled = F.conv3d(grid_global,\n                                  center_kernel,\n                                  stride=stride,\n                                  padding=padding)\n\n    # Coordinates of maxima residual to window center\n    # prepare kernel\n    coords_max: torch.Tensor = F.conv3d(x_exp,\n                                        window_kernel,\n                                        stride=stride,\n                                        padding=padding)\n\n    coords_max = coords_max / den.expand_as(coords_max)\n    coords_max = coords_max + grid_global_pooled.expand_as(coords_max)\n    # [:,:, 0, ...] is depth (scale)\n    # [:,:, 1, ...] is x\n    # [:,:, 2, ...] is y\n\n    if normalized_coordinates:\n        coords_max = normalize_pixel_coordinates3d(coords_max.permute(0, 2, 3, 4, 1), d, h, w)\n        coords_max = coords_max.permute(0, 4, 1, 2, 3)\n\n    # Back B*C -> (b, c)\n    coords_max = coords_max.view(b, c, 3, coords_max.size(2), coords_max.size(3), coords_max.size(4))\n\n    if not output_value:\n        return coords_max\n    x_softmaxpool = pool_coef * F.avg_pool3d(x_exp.view(input.size()) * input,\n                                             kernel_size,\n                                             stride=stride,\n                                             padding=padding) / den\n    if strict_maxima_bonus > 0:\n        in_levels: int = input.size(2)\n        out_levels: int = x_softmaxpool.size(2)\n        skip_levels: int = (in_levels - out_levels) // 2\n        strict_maxima: torch.Tensor = F.avg_pool3d(kornia.feature.nms3d(input, kernel_size), 1, stride, 0)\n        strict_maxima = strict_maxima[:, :, skip_levels:out_levels - skip_levels]\n        x_softmaxpool *= 1.0 + strict_maxima_bonus * strict_maxima\n    x_softmaxpool = x_softmaxpool.view(b,\n                                       c,\n                                       x_softmaxpool.size(2),\n                                       x_softmaxpool.size(3),\n                                       x_softmaxpool.size(4))\n    return coords_max, x_softmaxpool\n\n\ndef spatial_soft_argmax2d(\n        input: torch.Tensor,\n        temperature: torch.Tensor = torch.tensor(1.0),\n        normalized_coordinates: bool = True,\n        eps: float = 1e-8) -> torch.Tensor:\n    r""""""Function that computes the Spatial Soft-Argmax 2D\n    of a given input heatmap.\n\n    Returns the index of the maximum 2d coordinates of the give map.\n    The output order is x-coord and y-coord.\n\n    Arguments:\n        temperature (torch.Tensor): factor to apply to input. Default is 1.\n        normalized_coordinates (bool): whether to return the\n          coordinates normalized in the range of [-1, 1]. Otherwise,\n          it will return the coordinates in the range of the input shape.\n          Default is True.\n        eps (float): small value to avoid zero division. Default is 1e-8.\n\n    Shape:\n        - Input: :math:`(B, N, H, W)`\n        - Output: :math:`(B, N, 2)`\n\n    Examples:\n        >>> input = torch.tensor([[[\n            [0., 0., 0.],\n            [0., 10., 0.],\n            [0., 0., 0.]]]])\n        >>> coords = kornia.spatial_soft_argmax2d(input, False)\n        tensor([[[1.0000, 1.0000]]])\n    """"""\n    input_soft: torch.Tensor = dsnt.spatial_softmax2d(input, temperature)\n    output: torch.Tensor = dsnt.spatial_expectation2d(input_soft, normalized_coordinates)\n    return output\n\n\nclass SpatialSoftArgmax2d(nn.Module):\n    r""""""Module that computes the Spatial Soft-Argmax 2D of a given heatmap.\n\n    See :func:`~kornia.contrib.spatial_soft_argmax2d` for details.\n    """"""\n\n    def __init__(self,\n                 temperature: torch.Tensor = torch.tensor(1.0),\n                 normalized_coordinates: bool = True,\n                 eps: float = 1e-8) -> None:\n        super(SpatialSoftArgmax2d, self).__init__()\n        self.temperature: torch.Tensor = temperature\n        self.normalized_coordinates: bool = normalized_coordinates\n        self.eps: float = eps\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ +\\\n            \'(temperature=\' + str(self.temperature) + \', \' +\\\n            \'normalized_coordinates=\' + str(self.normalized_coordinates) + \', \' + \\\n            \'eps=\' + str(self.eps) + \')\'\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return spatial_soft_argmax2d(input, self.temperature,\n                                     self.normalized_coordinates, self.eps)\n\n\ndef conv_quad_interp3d(input: torch.Tensor, strict_maxima_bonus: float = 1.0, eps: float = 1e-6):\n    r""""""Function that computes the single iteration of quadratic interpolation of of the extremum (max or min) location\n    and value per each 3x3x3 window which contains strict extremum, similar to one done is SIFT\n\n    Args:\n        strict_maxima_bonus (float): pixels, which are strict maxima will score (1 + strict_maxima_bonus) * value.\n                                     This is needed for mimic behavior of strict NMS in classic local features\n        eps (float): parameter to control the hessian matrix ill-condition number.\n    Shape:\n        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`\n        - Output: :math:`(N, C, 3, D_{out}, H_{out}, W_{out})`, :math:`(N, C, D_{out}, H_{out}, W_{out})`, where\n\n         .. math::\n             D_{out} = \\left\\lfloor\\frac{D_{in}  + 2 \\times \\text{padding}[0] -\n             (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\n         .. math::\n             H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[1] -\n             (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\n         .. math::\n             W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[2] -\n             (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n\n    Examples:\n        >>> input = torch.randn(20, 16, 3, 50, 32)\n        >>> nms_coords, nms_val = conv_quad_interp3d(input, 1.0)\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(input)))\n    if not len(input.shape) == 5:\n        raise ValueError(""Invalid input shape, we expect BxCxDxHxW. Got: {}""\n                         .format(input.shape))\n    B, CH, D, H, W = input.shape\n    dev: torch.device = input.device\n    grid_global: torch.Tensor = create_meshgrid3d(D, H, W, False,\n                                                  device=input.device).permute(0, 4, 1, 2, 3)\n    grid_global = grid_global.to(input.dtype)\n\n    # to determine the location we are solving system of linear equations Ax = b, where b is 1st order gradient\n    # and A is Hessian matrix\n    b: torch.Tensor = kornia.filters.spatial_gradient3d(input, order=1, mode=\'diff\')  #\n    b = b.permute(0, 1, 3, 4, 5, 2).reshape(-1, 3, 1)\n    A: torch.Tensor = kornia.filters.spatial_gradient3d(input, order=2, mode=\'diff\')\n    A = A.permute(0, 1, 3, 4, 5, 2).reshape(-1, 6)\n    dxx = A[..., 0]\n    dyy = A[..., 1]\n    dss = A[..., 2]\n    dxy = A[..., 3]\n    dys = A[..., 4]\n    dxs = A[..., 5]\n    # for the Hessian\n    Hes = torch.stack([dxx, dxy, dxs, dxy, dyy, dys, dxs, dys, dss]).view(-1, 3, 3)\n    Hes += torch.eye(3, device=Hes.device)[None] * eps\n\n    nms_mask: torch.Tensor = kornia.feature.nms3d(input, (3, 3, 3), True)\n    x_solved: torch.Tensor = torch.zeros_like(b)\n    x_solved_masked, _ = torch.solve(b[nms_mask.view(-1)], Hes[nms_mask.view(-1)])\n    x_solved.masked_scatter_(nms_mask.view(-1, 1, 1), x_solved_masked)\n    dx: torch.Tensor = -x_solved\n\n    # Ignore ones, which are far from window,\n    dx[(dx.abs().max(dim=1, keepdim=True)[0] > 0.7).view(-1), :, :] = 0\n\n    dy: torch.Tensor = 0.5 * torch.bmm(b.permute(0, 2, 1), dx)\n    y_max = input + dy.view(B, CH, D, H, W)\n    if strict_maxima_bonus > 0:\n        y_max *= (1.0 + strict_maxima_bonus * nms_mask.to(input.dtype))\n\n    dx_res: torch.Tensor = dx.flip(1).reshape(B, CH, D, H, W, 3).permute(0, 1, 5, 2, 3, 4)\n    coords_max: torch.Tensor = grid_global.repeat(B, 1, 1, 1, 1).unsqueeze(1)\n    coords_max = coords_max + dx_res\n    return coords_max, y_max\n\n\nclass ConvQuadInterp3d(nn.Module):\n    r""""""Module that calculates soft argmax 3d per window\n    See :func:`~kornia.geometry.conv_quad_interp3d` for details.\n    """"""\n\n    def __init__(self,\n                 strict_maxima_bonus: float = 1.0, eps: float = 1e-6) -> None:\n        super(ConvQuadInterp3d, self).__init__()\n        self.strict_maxima_bonus = strict_maxima_bonus\n        self.eps = eps\n        return\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ + \'(\' + \'strict_maxima_bonus=\' + str(self.strict_maxima_bonus) + \')\'\n\n    def forward(self, x: torch.Tensor):  # type: ignore\n        return conv_quad_interp3d(x, self.strict_maxima_bonus, self.eps)\n'"
kornia/jit/__init__.py,7,b'import torch\nimport kornia as K\n\n# expose functions to torch.jit\n# TODO: find an automatic way to do this\nrgb_to_grayscale = torch.jit.script(K.color.rgb_to_grayscale)\n\nspatial_soft_argmax2d = torch.jit.script(K.geometry.spatial_soft_argmax2d)\nspatial_softmax2d = torch.jit.script(K.geometry.dsnt.spatial_softmax2d)\nspatial_expectation2d = torch.jit.script(K.geometry.dsnt.spatial_expectation2d)\nrender_gaussian2d = torch.jit.script(K.geometry.dsnt.render_gaussian2d)\nwarp_perspective = torch.jit.script(K.geometry.warp_perspective)\n'
kornia/losses/__init__.py,0,"b'from .ssim import SSIM, ssim\nfrom .dice import DiceLoss, dice_loss\nfrom .tversky import TverskyLoss, tversky_loss\nfrom .focal import FocalLoss, focal_loss\nfrom .depth_smooth import (\n    InverseDepthSmoothnessLoss, inverse_depth_smoothness_loss\n)\nfrom .divergence import kl_div_loss_2d, js_div_loss_2d\nfrom .total_variation import TotalVariation, total_variation\nfrom .psnr import PSNRLoss, psnr_loss\n'"
kornia/losses/depth_smooth.py,24,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Based on\n# https://github.com/tensorflow/models/blob/master/research/struct2depth/model.py#L625-L641\n\n\ndef _gradient_x(img: torch.Tensor) -> torch.Tensor:\n    assert len(img.shape) == 4, img.shape\n    return img[:, :, :, :-1] - img[:, :, :, 1:]\n\n\ndef _gradient_y(img: torch.Tensor) -> torch.Tensor:\n    assert len(img.shape) == 4, img.shape\n    return img[:, :, :-1, :] - img[:, :, 1:, :]\n\n\ndef inverse_depth_smoothness_loss(\n        idepth: torch.Tensor,\n        image: torch.Tensor) -> torch.Tensor:\n    r""""""Computes image-aware inverse depth smoothness loss.\n\n    See :class:`~kornia.losses.InvDepthSmoothnessLoss` for details.\n    """"""\n    if not torch.is_tensor(idepth):\n        raise TypeError(""Input idepth type is not a torch.Tensor. Got {}""\n                        .format(type(idepth)))\n\n    if not torch.is_tensor(image):\n        raise TypeError(""Input image type is not a torch.Tensor. Got {}""\n                        .format(type(image)))\n\n    if not len(idepth.shape) == 4:\n        raise ValueError(""Invalid idepth shape, we expect BxCxHxW. Got: {}""\n                         .format(idepth.shape))\n\n    if not len(image.shape) == 4:\n        raise ValueError(""Invalid image shape, we expect BxCxHxW. Got: {}""\n                         .format(image.shape))\n\n    if not idepth.shape[-2:] == image.shape[-2:]:\n        raise ValueError(""idepth and image shapes must be the same. Got: {} and {}""\n                         .format(idepth.shape, image.shape))\n\n    if not idepth.device == image.device:\n        raise ValueError(\n            ""idepth and image must be in the same device. Got: {} and {}"" .format(\n                idepth.device, image.device))\n\n    if not idepth.dtype == image.dtype:\n        raise ValueError(\n            ""idepth and image must be in the same dtype. Got: {} and {}"" .format(\n                idepth.dtype, image.dtype))\n\n    # compute the gradients\n    idepth_dx: torch.Tensor = _gradient_x(idepth)\n    idepth_dy: torch.Tensor = _gradient_y(idepth)\n    image_dx: torch.Tensor = _gradient_x(image)\n    image_dy: torch.Tensor = _gradient_y(image)\n\n    # compute image weights\n    weights_x: torch.Tensor = torch.exp(\n        -torch.mean(torch.abs(image_dx), dim=1, keepdim=True))\n    weights_y: torch.Tensor = torch.exp(\n        -torch.mean(torch.abs(image_dy), dim=1, keepdim=True))\n\n    # apply image weights to depth\n    smoothness_x: torch.Tensor = torch.abs(idepth_dx * weights_x)\n    smoothness_y: torch.Tensor = torch.abs(idepth_dy * weights_y)\n    return torch.mean(smoothness_x) + torch.mean(smoothness_y)\n\n\nclass InverseDepthSmoothnessLoss(nn.Module):\n    r""""""Criterion that computes image-aware inverse depth smoothness loss.\n\n    .. math::\n\n        \\text{loss} = \\left | \\partial_x d_{ij} \\right | e^{-\\left \\|\n        \\partial_x I_{ij} \\right \\|} + \\left |\n        \\partial_y d_{ij} \\right | e^{-\\left \\| \\partial_y I_{ij} \\right \\|}\n\n\n    Shape:\n        - Inverse Depth: :math:`(N, 1, H, W)`\n        - Image: :math:`(N, 3, H, W)`\n        - Output: scalar\n\n    Examples::\n\n        >>> idepth = torch.rand(1, 1, 4, 5)\n        >>> image = torch.rand(1, 3, 4, 5)\n        >>> smooth = kornia.losses.DepthSmoothnessLoss()\n        >>> loss = smooth(idepth, image)\n    """"""\n\n    def __init__(self) -> None:\n        super(InverseDepthSmoothnessLoss, self).__init__()\n\n    def forward(self, idepth: torch.Tensor, image: torch.Tensor) -> torch.Tensor:  # type:ignore\n        return inverse_depth_smoothness_loss(idepth, image)\n'"
kornia/losses/dice.py,14,"b'from typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.utils import one_hot\n\n\n# based on:\n# https://github.com/kevinzakka/pytorch-goodies/blob/master/losses.py\n\ndef dice_loss(input: torch.Tensor, target: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n    r""""""Function that computes S\xc3\xb8rensen-Dice Coefficient loss.\n\n    See :class:`~kornia.losses.DiceLoss` for details.\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(input)))\n\n    if not len(input.shape) == 4:\n        raise ValueError(""Invalid input shape, we expect BxNxHxW. Got: {}""\n                         .format(input.shape))\n\n    if not input.shape[-2:] == target.shape[-2:]:\n        raise ValueError(""input and target shapes must be the same. Got: {} and {}""\n                         .format(input.shape, input.shape))\n\n    if not input.device == target.device:\n        raise ValueError(\n            ""input and target must be in the same device. Got: {} and {}"" .format(\n                input.device, target.device))\n\n    # compute softmax over the classes axis\n    input_soft: torch.Tensor = F.softmax(input, dim=1)\n\n    # create the labels one hot tensor\n    target_one_hot: torch.Tensor = one_hot(\n        target, num_classes=input.shape[1],\n        device=input.device, dtype=input.dtype)\n\n    # compute the actual dice score\n    dims = (1, 2, 3)\n    intersection = torch.sum(input_soft * target_one_hot, dims)\n    cardinality = torch.sum(input_soft + target_one_hot, dims)\n\n    dice_score = 2. * intersection / (cardinality + eps)\n    return torch.mean(-dice_score + 1.)\n\n\nclass DiceLoss(nn.Module):\n    r""""""Criterion that computes S\xc3\xb8rensen-Dice Coefficient loss.\n\n    According to [1], we compute the S\xc3\xb8rensen-Dice Coefficient as follows:\n\n    .. math::\n\n        \\text{Dice}(x, class) = \\frac{2 |X| \\cap |Y|}{|X| + |Y|}\n\n    where:\n       - :math:`X` expects to be the scores of each class.\n       - :math:`Y` expects to be the one-hot tensor with the class labels.\n\n    the loss, is finally computed as:\n\n    .. math::\n\n        \\text{loss}(x, class) = 1 - \\text{Dice}(x, class)\n\n    [1] https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n\n    Shape:\n        - Input: :math:`(N, C, H, W)` where C = number of classes.\n        - Target: :math:`(N, H, W)` where each value is\n          :math:`0 \xe2\x89\xa4 targets[i] \xe2\x89\xa4 C\xe2\x88\x921`.\n\n    Examples:\n        >>> N = 5  # num_classes\n        >>> loss = kornia.losses.DiceLoss()\n        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)\n        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)\n        >>> output = loss(input, target)\n        >>> output.backward()\n    """"""\n\n    def __init__(self) -> None:\n        super(DiceLoss, self).__init__()\n        self.eps: float = 1e-6\n\n    def forward(  # type: ignore\n            self,\n            input: torch.Tensor,\n            target: torch.Tensor) -> torch.Tensor:\n        return dice_loss(input, target, self.eps)\n'"
kornia/losses/divergence.py,10,"b'r""""""Losses based on the divergence between probability distributions.""""""\n\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef _kl_div_2d(p, q):\n    # D_KL(P || Q)\n    batch, chans, height, width = p.shape\n    unsummed_kl = F.kl_div(\n        q.view(batch * chans, height * width).log(),\n        p.view(batch * chans, height * width),\n        reduction=\'none\',\n    )\n    kl_values = unsummed_kl.sum(-1).view(batch, chans)\n    return kl_values\n\n\ndef _js_div_2d(p, q):\n    # JSD(P || Q)\n    m = 0.5 * (p + q)\n    return 0.5 * _kl_div_2d(p, m) + 0.5 * _kl_div_2d(q, m)\n\n# TODO: add this to the main module\n\n\ndef _reduce_loss(losses, reduction):\n    if reduction == \'none\':\n        return losses\n    else:\n        return torch.mean(losses) if reduction == \'mean\' else torch.sum(losses)\n\n\ndef js_div_loss_2d(\n        input: torch.Tensor,\n        target: torch.Tensor,\n        reduction: str = \'mean\'\n):\n    r""""""Calculates the Jensen-Shannon divergence loss between heatmaps.\n\n    Arguments:\n        input (torch.Tensor): the input tensor.\n        target (torch.Tensor): the target tensor.\n        reduction (string, optional): Specifies the reduction to apply to the\n          output: ``\'none\'`` | ``\'mean\'`` | ``\'sum\'``. ``\'none\'``: no reduction\n          will be applied, ``\'mean\'``: the sum of the output will be divided by\n          the number of elements in the output, ``\'sum\'``: the output will be\n          summed. Default: ``\'mean\'``.\n\n    Shape:\n        - Input: :math:`(B, N, H, W)`\n        - Target: :math:`(B, N, H, W)`, same shape as the input\n    """"""\n    return _reduce_loss(_js_div_2d(target, input), reduction)\n\n\ndef kl_div_loss_2d(\n        input: torch.Tensor,\n        target: torch.Tensor,\n        reduction: str = \'mean\'\n):\n    r""""""Calculates the Kullback-Leibler divergence loss between heatmaps.\n\n    Arguments:\n        input (torch.Tensor): the input tensor.\n        target (torch.Tensor): the target tensor.\n        reduction (string, optional): Specifies the reduction to apply to the\n          output: ``\'none\'`` | ``\'mean\'`` | ``\'sum\'``. ``\'none\'``: no reduction\n          will be applied, ``\'mean\'``: the sum of the output will be divided by\n          the number of elements in the output, ``\'sum\'``: the output will be\n          summed. Default: ``\'mean\'``.\n\n    Shape:\n        - Input: :math:`(B, N, H, W)`\n        - Target: :math:`(B, N, H, W)`, same shape as the input\n    """"""\n    return _reduce_loss(_kl_div_2d(target, input), reduction)\n'"
kornia/losses/focal.py,18,"b'from typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.utils import one_hot\n\n\n# based on:\n# https://github.com/zhezh/focalloss/blob/master/focalloss.py\n\ndef focal_loss(\n        input: torch.Tensor,\n        target: torch.Tensor,\n        alpha: float,\n        gamma: float = 2.0,\n        reduction: str = \'none\',\n        eps: float = 1e-8) -> torch.Tensor:\n    r""""""Function that computes Focal loss.\n\n    See :class:`~kornia.losses.FocalLoss` for details.\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(input)))\n\n    if not len(input.shape) >= 2:\n        raise ValueError(""Invalid input shape, we expect BxCx*. Got: {}""\n                         .format(input.shape))\n\n    if input.size(0) != target.size(0):\n        raise ValueError(\'Expected input batch_size ({}) to match target batch_size ({}).\'\n                         .format(input.size(0), target.size(0)))\n\n    n = input.size(0)\n    out_size = (n,) + input.size()[2:]\n    if target.size()[1:] != input.size()[2:]:\n        raise ValueError(\'Expected target size {}, got {}\'.format(\n            out_size, target.size()))\n\n    if not input.device == target.device:\n        raise ValueError(\n            ""input and target must be in the same device. Got: {} and {}"" .format(\n                input.device, target.device))\n\n    # compute softmax over the classes axis\n    input_soft: torch.Tensor = F.softmax(input, dim=1) + eps\n\n    # create the labels one hot tensor\n    target_one_hot: torch.Tensor = one_hot(\n        target, num_classes=input.shape[1],\n        device=input.device, dtype=input.dtype)\n\n    # compute the actual focal loss\n    weight = torch.pow(-input_soft + 1., gamma)\n\n    focal = -alpha * weight * torch.log(input_soft)\n    loss_tmp = torch.sum(target_one_hot * focal, dim=1)\n\n    if reduction == \'none\':\n        loss = loss_tmp\n    elif reduction == \'mean\':\n        loss = torch.mean(loss_tmp)\n    elif reduction == \'sum\':\n        loss = torch.sum(loss_tmp)\n    else:\n        raise NotImplementedError(""Invalid reduction mode: {}""\n                                  .format(reduction))\n    return loss\n\n\nclass FocalLoss(nn.Module):\n    r""""""Criterion that computes Focal loss.\n\n    According to [1], the Focal loss is computed as follows:\n\n    .. math::\n\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n\n    where:\n       - :math:`p_t` is the model\'s estimated probability for each class.\n\n\n    Arguments:\n        alpha (float): Weighting factor :math:`\\alpha \\in [0, 1]`.\n        gamma (float): Focusing parameter :math:`\\gamma >= 0`.\n        reduction (str, optional): Specifies the reduction to apply to the\n         output: \xe2\x80\x98none\xe2\x80\x99 | \xe2\x80\x98mean\xe2\x80\x99 | \xe2\x80\x98sum\xe2\x80\x99. \xe2\x80\x98none\xe2\x80\x99: no reduction will be applied,\n         \xe2\x80\x98mean\xe2\x80\x99: the sum of the output will be divided by the number of elements\n         in the output, \xe2\x80\x98sum\xe2\x80\x99: the output will be summed. Default: \xe2\x80\x98none\xe2\x80\x99.\n\n    Shape:\n        - Input: :math:`(N, C, *)` where C = number of classes.\n        - Target: :math:`(N, *)` where each value is\n          :math:`0 \xe2\x89\xa4 targets[i] \xe2\x89\xa4 C\xe2\x88\x921`.\n\n    Examples:\n        >>> N = 5  # num_classes\n        >>> kwargs = {""alpha"": 0.5, ""gamma"": 2.0, ""reduction"": \'mean\'}\n        >>> loss = kornia.losses.FocalLoss(**kwargs)\n        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)\n        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)\n        >>> output = loss(input, target)\n        >>> output.backward()\n\n    References:\n        [1] https://arxiv.org/abs/1708.02002\n    """"""\n\n    def __init__(self, alpha: float, gamma: float = 2.0,\n                 reduction: str = \'none\') -> None:\n        super(FocalLoss, self).__init__()\n        self.alpha: float = alpha\n        self.gamma: float = gamma\n        self.reduction: str = reduction\n        self.eps: float = 1e-6\n\n    def forward(  # type: ignore\n            self,\n            input: torch.Tensor,\n            target: torch.Tensor) -> torch.Tensor:\n        return focal_loss(input, target, self.alpha, self.gamma, self.reduction, self.eps)\n'"
kornia/losses/psnr.py,8,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.functional import mse_loss\n\n\nclass PSNRLoss(nn.Module):\n    r""""""Creates a criterion that calculates the PSNR between 2 images. Given an m x n image, the PSNR is:\n\n    .. math::\n\n        \\text{PSNR} = 10 \\log_{10} \\bigg(\\frac{\\text{MAX}_I^2}{MSE(I,T)}\\bigg)\n\n    where\n\n    .. math::\n\n        \\text{MSE}(I,T) = \\frac{1}{mn}\\sum_{i=0}^{m-1}\\sum_{j=0}^{n-1} [I(i,j) - T(i,j)]^2\n\n    and :math:`\\text{MAX}_I` is the maximum possible input value\n    (e.g for floating point images :math:`\\text{MAX}_I=1`).\n\n\n    Arguments:\n        max_val (float): Maximum value of input\n\n    Shape:\n        - input: :math:`(*)`\n        - approximation: :math:`(*)` same shape as input\n        - output: :math:`()` a scalar\n\n    Examples:\n        >>> kornia.losses.psnr_loss(torch.ones(1), 1.2*torch.ones(1), 2)\n        tensor(20.0000) # 10 * log(4/((1.2-1)**2)) / log(10)\n\n    reference:\n        https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Definition\n    """"""\n\n    def __init__(self, max_val: float) -> None:\n        super(PSNRLoss, self).__init__()\n        self.max_val: float = max_val\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return psnr_loss(input, target, self.max_val)\n\n\ndef psnr_loss(input: torch.Tensor, target: torch.Tensor, max_val: float) -> torch.Tensor:\n    r""""""Function that computes PSNR\n\n    See :class:`~kornia.losses.PSNRLoss` for details.\n    """"""\n    if not torch.is_tensor(input) or not torch.is_tensor(target):\n        raise TypeError(f""Expected 2 torch tensors but got {type(input)} and {type(target)}"")\n\n    if input.shape != target.shape:\n        raise TypeError(f""Expected tensors of equal shapes, but got {input.shape} and {target.shape}"")\n    mse_val = mse_loss(input, target, reduction=\'mean\')\n    max_val_tensor: torch.Tensor = torch.tensor(max_val).to(input.device).to(input.dtype)\n    return 10 * torch.log10(max_val_tensor * max_val_tensor / mse_val)\n'"
kornia/losses/ssim.py,21,"b'from typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.filters import get_gaussian_kernel2d, filter2D\n\n\ndef _compute_zero_padding(kernel_size: int) -> int:\n    """"""Computes zero padding.""""""\n    return (kernel_size - 1) // 2\n\n\nclass SSIM(nn.Module):\n    r""""""Creates a criterion that measures the Structural Similarity (SSIM)\n    index between each element in the input `x` and target `y`.\n\n    The index can be described as:\n\n    .. math::\n\n      \\text{SSIM}(x, y) = \\frac{(2\\mu_x\\mu_y+c_1)(2\\sigma_{xy}+c_2)}\n      {(\\mu_x^2+\\mu_y^2+c_1)(\\sigma_x^2+\\sigma_y^2+c_2)}\n\n    where:\n      - :math:`c_1=(k_1 L)^2` and :math:`c_2=(k_2 L)^2` are two variables to\n        stabilize the division with weak denominator.\n      - :math:`L` is the dynamic range of the pixel-values (typically this is\n        :math:`2^{\\#\\text{bits per pixel}}-1`).\n\n    the loss, or the Structural dissimilarity (DSSIM) can be finally described\n    as:\n\n    .. math::\n\n      \\text{loss}(x, y) = \\frac{1 - \\text{SSIM}(x, y)}{2}\n\n    Arguments:\n        window_size (int): the size of the kernel.\n        max_val (float): the dynamic range of the images. Default: 1.\n        reduction (str, optional): Specifies the reduction to apply to the\n         output: \'none\' | \'mean\' | \'sum\'. \'none\': no reduction will be applied,\n         \'mean\': the sum of the output will be divided by the number of elements\n         in the output, \'sum\': the output will be summed. Default: \'none\'.\n\n    Returns:\n        Tensor: the ssim index.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Target :math:`(B, C, H, W)`\n        - Output: scale, if reduction is \'none\', then :math:`(B, C, H, W)`\n\n    Examples::\n\n        >>> input1 = torch.rand(1, 4, 5, 5)\n        >>> input2 = torch.rand(1, 4, 5, 5)\n        >>> ssim = kornia.losses.SSIM(5, reduction=\'none\')\n        >>> loss = ssim(input1, input2)  # 1x4x5x5\n    """"""\n\n    def __init__(\n            self,\n            window_size: int,\n            reduction: str = ""none"",\n            max_val: float = 1.0) -> None:\n        super(SSIM, self).__init__()\n        self.window_size: int = window_size\n        self.max_val: float = max_val\n        self.reduction: str = reduction\n\n        self.window: torch.Tensor = get_gaussian_kernel2d(\n            (window_size, window_size), (1.5, 1.5))\n        self.window = self.window.requires_grad_(False)  # need to disable gradients\n\n        self.padding: int = _compute_zero_padding(window_size)\n\n        self.C1: float = (0.01 * self.max_val) ** 2\n        self.C2: float = (0.03 * self.max_val) ** 2\n\n    def forward(  # type: ignore\n            self,\n            img1: torch.Tensor,\n            img2: torch.Tensor) -> torch.Tensor:\n\n        if not torch.is_tensor(img1):\n            raise TypeError(""Input img1 type is not a torch.Tensor. Got {}""\n                            .format(type(img1)))\n\n        if not torch.is_tensor(img2):\n            raise TypeError(""Input img2 type is not a torch.Tensor. Got {}""\n                            .format(type(img2)))\n\n        if not len(img1.shape) == 4:\n            raise ValueError(""Invalid img1 shape, we expect BxCxHxW. Got: {}""\n                             .format(img1.shape))\n\n        if not len(img2.shape) == 4:\n            raise ValueError(""Invalid img2 shape, we expect BxCxHxW. Got: {}""\n                             .format(img2.shape))\n\n        if not img1.shape == img2.shape:\n            raise ValueError(""img1 and img2 shapes must be the same. Got: {} and {}""\n                             .format(img1.shape, img2.shape))\n\n        if not img1.device == img2.device:\n            raise ValueError(""img1 and img2 must be in the same device. Got: {} and {}""\n                             .format(img1.device, img2.device))\n\n        if not img1.dtype == img2.dtype:\n            raise ValueError(""img1 and img2 must be in the same dtype. Got: {} and {}""\n                             .format(img1.dtype, img2.dtype))\n\n        # prepare kernel\n        b, c, h, w = img1.shape\n        tmp_kernel: torch.Tensor = self.window.to(img1.device).to(img1.dtype)\n        tmp_kernel = torch.unsqueeze(tmp_kernel, dim=0)\n\n        # compute local mean per channel\n        mu1: torch.Tensor = filter2D(img1, tmp_kernel)\n        mu2: torch.Tensor = filter2D(img2, tmp_kernel)\n\n        mu1_sq = mu1.pow(2)\n        mu2_sq = mu2.pow(2)\n        mu1_mu2 = mu1 * mu2\n\n        # compute local sigma per channel\n        sigma1_sq = filter2D(img1 * img1, tmp_kernel) - mu1_sq\n        sigma2_sq = filter2D(img2 * img2, tmp_kernel) - mu2_sq\n        sigma12 = filter2D(img1 * img2, tmp_kernel) - mu1_mu2\n\n        ssim_map = ((2. * mu1_mu2 + self.C1) * (2. * sigma12 + self.C2)) / \\\n            ((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))\n\n        loss = torch.clamp(-ssim_map + 1., min=0, max=1) / 2.\n\n        if self.reduction == ""mean"":\n            loss = torch.mean(loss)\n        elif self.reduction == ""sum"":\n            loss = torch.sum(loss)\n        elif self.reduction == ""none"":\n            pass\n        return loss\n\n\n######################\n# functional interface\n######################\n\n\ndef ssim(\n        img1: torch.Tensor,\n        img2: torch.Tensor,\n        window_size: int,\n        reduction: str = ""none"",\n        max_val: float = 1.0) -> torch.Tensor:\n    r""""""Function that measures the Structural Similarity (SSIM) index between\n    each element in the input `x` and target `y`.\n\n    See :class:`~kornia.losses.SSIM` for details.\n    """"""\n    return SSIM(window_size, reduction, max_val)(img1, img2)\n'"
kornia/losses/total_variation.py,7,"b'import torch\nimport torch.nn as nn\n\n\nclass TotalVariation(nn.Module):\n    r""""""Computes the Total Variation according to\n    [1] https://en.wikipedia.org/wiki/Total_variation\n    Shape:\n        - Input: :math:`(N, C, H, W)` or :math:`(C, H, W)` where C = number of classes.\n        - Output: :math:`(N,)` or :math:`()`\n    Examples:\n        >>> kornia.losses.total_variation(torch.ones(3,4,4)) # tensor(0.)\n        >>> tv = kornia.losses.TotalVariation()\n        >>> output = tv(torch.ones(2,3,4,4)) # tensor([0., 0.])\n        >>> output.backward()\n    """"""\n\n    def __init__(self) -> None:\n        super(TotalVariation, self).__init__()\n\n    def forward(  # type: ignore\n            self, img) -> torch.Tensor:\n        return total_variation(img)\n\n\ndef total_variation(img: torch.Tensor) -> torch.Tensor:\n    r""""""Function that computes Total Variation.\n\n    See :class:`~kornia.losses.TotalVariation` for details.\n    """"""\n    if not torch.is_tensor(img):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(img)}"")\n    img_shape = img.shape\n    if len(img_shape) == 3 or len(img_shape) == 4:\n        pixel_dif1 = img[..., 1:, :] - img[..., :-1, :]\n        pixel_dif2 = img[..., :, 1:] - img[..., :, :-1]\n        reduce_axes = (-3, -2, -1)\n    else:\n        raise ValueError(""Expected input tensor to be of ndim 3 or 4, but got "" + str(len(img_shape)))\n\n    return pixel_dif1.abs().sum(dim=reduce_axes) + pixel_dif2.abs().sum(dim=reduce_axes)\n'"
kornia/losses/tversky.py,16,"b'from typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.utils import one_hot\n\n# based on:\n# https://github.com/kevinzakka/pytorch-goodies/blob/master/losses.py\n\n\ndef tversky_loss(input: torch.Tensor, target: torch.Tensor,\n                 alpha: float, beta: float, eps: float = 1e-8) -> torch.Tensor:\n    r""""""Function that computes Tversky loss.\n\n    See :class:`~kornia.losses.TverskyLoss` for details.\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(input)))\n\n    if not len(input.shape) == 4:\n        raise ValueError(""Invalid input shape, we expect BxNxHxW. Got: {}""\n                         .format(input.shape))\n\n    if not input.shape[-2:] == target.shape[-2:]:\n        raise ValueError(""input and target shapes must be the same. Got: {} and {}""\n                         .format(input.shape, input.shape))\n\n    if not input.device == target.device:\n        raise ValueError(\n            ""input and target must be in the same device. Got: {} and {}"" .format(\n                input.device, target.device))\n\n    # compute softmax over the classes axis\n    input_soft: torch.Tensor = F.softmax(input, dim=1)\n\n    # create the labels one hot tensor\n    target_one_hot: torch.Tensor = one_hot(\n        target, num_classes=input.shape[1],\n        device=input.device, dtype=input.dtype)\n\n    # compute the actual dice score\n    dims = (1, 2, 3)\n    intersection = torch.sum(input_soft * target_one_hot, dims)\n    fps = torch.sum(input_soft * (-target_one_hot + 1.), dims)\n    fns = torch.sum((-input_soft + 1.) * target_one_hot, dims)\n\n    numerator = intersection\n    denominator = intersection + alpha * fps + beta * fns\n    tversky_loss = numerator / (denominator + eps)\n    return torch.mean(-tversky_loss + 1.)\n\n\nclass TverskyLoss(nn.Module):\n    r""""""Criterion that computes Tversky Coeficient loss.\n\n    According to [1], we compute the Tversky Coefficient as follows:\n\n    .. math::\n\n        \\text{S}(P, G, \\alpha; \\beta) =\n          \\frac{|PG|}{|PG| + \\alpha |P \\ G| + \\beta |G \\ P|}\n\n    where:\n       - :math:`P` and :math:`G` are the predicted and ground truth binary\n         labels.\n       - :math:`\\alpha` and :math:`\\beta` control the magnitude of the\n         penalties for FPs and FNs, respectively.\n\n    Notes:\n       - :math:`\\alpha = \\beta = 0.5` => dice coeff\n       - :math:`\\alpha = \\beta = 1` => tanimoto coeff\n       - :math:`\\alpha + \\beta = 1` => F beta coeff\n\n    Shape:\n        - Input: :math:`(N, C, H, W)` where C = number of classes.\n        - Target: :math:`(N, H, W)` where each value is\n          :math:`0 \xe2\x89\xa4 targets[i] \xe2\x89\xa4 C\xe2\x88\x921`.\n\n    Examples:\n        >>> N = 5  # num_classes\n        >>> loss = kornia.losses.TverskyLoss(alpha=0.5, beta=0.5)\n        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)\n        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)\n        >>> output = loss(input, target)\n        >>> output.backward()\n\n    References:\n        [1]: https://arxiv.org/abs/1706.05721\n    """"""\n\n    def __init__(self, alpha: float, beta: float, eps: float = 1e-8) -> None:\n        super(TverskyLoss, self).__init__()\n        self.alpha: float = alpha\n        self.beta: float = beta\n        self.eps: float = eps\n\n    def forward(  # type: ignore\n            self,\n            input: torch.Tensor,\n            target: torch.Tensor) -> torch.Tensor:\n        return tversky_loss(input, target, self.alpha, self.beta, self.eps)\n'"
kornia/testing/__init__.py,8,"b'""""""\nThe testing package contains testing-specific utilities.\n""""""\n\n\nimport torch\nimport numpy as np\n\n__all__ = [\n    \'tensor_to_gradcheck_var\', \'create_eye_batch\',\n]\n\n\ndef create_checkerboard(h, w, nw):\n    """"""Creates a synthetic checkerd board of shape HxW and window size `nw`.\n    """"""\n    return np.kron([[1, 0] * nw, [0, 1] * nw] * nw,\n                   np.ones((h // (2 * nw), w // (2 * nw)))).astype(np.float32)\n\n\ndef create_eye_batch(batch_size, eye_size):\n    """"""Creates a batch of identity matrices of shape Bx3x3\n    """"""\n    return torch.eye(eye_size).view(\n        1, eye_size, eye_size).expand(batch_size, -1, -1)\n\n\ndef create_random_homography(batch_size, eye_size, std_val=1e-3):\n    """"""Creates a batch of random homographies of shape Bx3x3\n    """"""\n    std = torch.FloatTensor(batch_size, eye_size, eye_size)\n    eye = create_eye_batch(batch_size, eye_size)\n    return eye + std.uniform_(-std_val, std_val)\n\n\ndef tensor_to_gradcheck_var(tensor, dtype=torch.float64, requires_grad=True):\n    """"""Converts the input tensor to a valid variable to check the gradient.\n      `gradcheck` needs 64-bit floating point and requires gradient.\n    """"""\n    assert torch.is_tensor(tensor), type(tensor)\n    return tensor.requires_grad_(requires_grad).type(dtype)\n\n\ndef compute_patch_error(x, y, h, w):\n    """"""Compute the absolute error between patches.\n    """"""\n    return torch.abs(x - y)[..., h // 4:-h // 4, w // 4:-w // 4].mean()\n\n\ndef check_is_tensor(obj):\n    """"""Checks whether the supplied object is a tensor.\n    """"""\n    if not isinstance(obj, torch.Tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(type(obj)))\n\n\ndef create_rectified_fundamental_matrix(batch_size):\n    """"""Creates a batch of rectified fundamental matrices of shape Bx3x3\n    """"""\n    F_rect = torch.tensor([[0., 0., 0.],\n                           [0., 0., -1.],\n                           [0., 1., 0.]]).view(1, 3, 3)\n    F_repeat = F_rect.repeat(batch_size, 1, 1)\n    return F_repeat\n\n\ndef create_random_fundamental_matrix(batch_size, std_val=1e-3):\n    """"""Creates a batch of random fundamental matrices of shape Bx3x3\n    """"""\n    F_rect = create_rectified_fundamental_matrix(batch_size)\n    H_left = create_random_homography(batch_size, 3, std_val)\n    H_right = create_random_homography(batch_size, 3, std_val)\n    return H_left.permute(0, 2, 1) @ F_rect @ H_right\n'"
kornia/utils/__init__.py,0,"b'from .one_hot import one_hot\nfrom .grid import create_meshgrid, create_meshgrid3d\nfrom .image import tensor_to_image, image_to_tensor\nfrom .pointcloud_io import save_pointcloud_ply, load_pointcloud_ply\n\nfrom kornia.utils.metrics import *\n\n__all__ = [\n    ""one_hot"",\n    ""create_meshgrid"",\n    ""create_meshgrid3d"",\n    ""tensor_to_image"",\n    ""image_to_tensor"",\n    ""save_pointcloud_ply"",\n    ""load_pointcloud_ply"",\n]\n'"
kornia/utils/grid.py,26,"b'from typing import Optional\n\nimport torch\n\n\ndef create_meshgrid(\n        height: int,\n        width: int,\n        normalized_coordinates: bool = True,\n        device: Optional[torch.device] = torch.device(\'cpu\')) -> torch.Tensor:\n    """"""Generates a coordinate grid for an image.\n\n    When the flag `normalized_coordinates` is set to True, the grid is\n    normalized to be in the range [-1,1] to be consistent with the pytorch\n    function grid_sample.\n    http://pytorch.org/docs/master/nn.html#torch.nn.functional.grid_sample\n\n    Args:\n        height (int): the image height (rows).\n        width (int): the image width (cols).\n        normalized_coordinates (bool): whether to normalize\n          coordinates in the range [-1, 1] in order to be consistent with the\n          PyTorch function grid_sample.\n\n    Return:\n        torch.Tensor: returns a grid tensor with shape :math:`(1, H, W, 2)`.\n    """"""\n    # generate coordinates\n    xs: Optional[torch.Tensor] = None\n    ys: Optional[torch.Tensor] = None\n    if normalized_coordinates:\n        xs = torch.linspace(-1, 1, width, device=device, dtype=torch.float)\n        ys = torch.linspace(-1, 1, height, device=device, dtype=torch.float)\n    else:\n        xs = torch.linspace(0, width - 1, width, device=device, dtype=torch.float)\n        ys = torch.linspace(0, height - 1, height, device=device, dtype=torch.float)\n    # generate grid by stacking coordinates\n    base_grid: torch.Tensor = torch.stack(\n        torch.meshgrid([xs, ys])).transpose(1, 2)  # 2xHxW\n    return torch.unsqueeze(base_grid, dim=0).permute(0, 2, 3, 1)  # 1xHxWx2\n\n\ndef create_meshgrid3d(\n        depth: int,\n        height: int,\n        width: int,\n        normalized_coordinates: bool = True,\n        device: Optional[torch.device] = torch.device(\'cpu\')) -> torch.Tensor:\n    """"""Generates a coordinate grid for an image.\n\n    When the flag `normalized_coordinates` is set to True, the grid is\n    normalized to be in the range [-1,1] to be consistent with the pytorch\n    function grid_sample.\n    http://pytorch.org/docs/master/nn.html#torch.nn.functional.grid_sample\n\n    Args:\n        depth (int): the image depth (channels).\n        height (int): the image height (rows).\n        width (int): the image width (cols).\n        normalized_coordinates (bool): whether to normalize\n          coordinates in the range [-1, 1] in order to be consistent with the\n          PyTorch function grid_sample.\n\n    Return:\n        torch.Tensor: returns a grid tensor with shape :math:`(1, D, H, W, 3)`.\n    """"""\n    xs: Optional[torch.Tensor] = None\n    ys: Optional[torch.Tensor] = None\n    zs: Optional[torch.Tensor] = None\n    if normalized_coordinates:\n        xs = torch.linspace(-1, 1, width, device=device, dtype=torch.float)\n        ys = torch.linspace(-1, 1, height, device=device, dtype=torch.float)\n        zs = torch.linspace(-1, 1, depth, device=device, dtype=torch.float)\n    else:\n        xs = torch.linspace(0, width - 1, width, device=device, dtype=torch.float)\n        ys = torch.linspace(0, height - 1, height, device=device, dtype=torch.float)\n        zs = torch.linspace(0, depth - 1, depth, device=device, dtype=torch.float)\n    # generate grid by stacking coordinates\n    base_grid: torch.Tensor = torch.stack(\n        torch.meshgrid([zs, xs, ys])).transpose(1, 2)  # 3xHxW\n    return base_grid.unsqueeze(0).permute(0, 3, 4, 2, 1)  # 1xHxWx3\n'"
kornia/utils/image.py,12,"b'from typing import Optional\n\nimport numpy as np\nimport torch\n\n\ndef image_to_tensor(image: np.ndarray, keepdim: bool = True) -> torch.Tensor:\n    """"""Converts a numpy image to a PyTorch 4d tensor image.\n    Args:\n        image (numpy.ndarray): image of the form :math:`(H, W, C)`, :math:`(H, W)` or\n            :math:`(B, H, W, C)`.\n        keepdim (bool): If ``False`` unsqueeze the input image to match the shape\n            :math:`(B, H, W, C)`. Default: ``True``\n    Returns:\n        torch.Tensor: tensor of the form :math:`(B, C, H, W)` if keepdim is ``False``,\n            :math:`(C, H, W)` otherwise.\n    """"""\n    if not isinstance(image, (np.ndarray,)):\n        raise TypeError(""Input type must be a numpy.ndarray. Got {}"".format(\n            type(image)))\n\n    if len(image.shape) > 4 or len(image.shape) < 2:\n        raise ValueError(\n            ""Input size must be a two, three or four dimensional array"")\n\n    input_shape = image.shape\n    tensor: torch.Tensor = torch.from_numpy(image)\n\n    if len(input_shape) == 2:\n        # (H, W) -> (1, H, W)\n        tensor = tensor.unsqueeze(0)\n    elif len(input_shape) == 3:\n        # (H, W, C) -> (C, H, W)\n        tensor = tensor.permute(2, 0, 1)\n    elif len(input_shape) == 4:\n        # (B, H, W, C) -> (B, C, H, W)\n        tensor = tensor.permute(0, 3, 1, 2)\n        keepdim = True  # no need to unsqueeze\n    else:\n        raise ValueError(\n            ""Cannot process image with shape {}"".format(input_shape))\n\n    return tensor.unsqueeze(0) if not keepdim else tensor\n\n\ndef _to_bchw(tensor: torch.Tensor, color_channel_num: Optional[int] = None) -> torch.Tensor:\n    """"""Converts a PyTorch tensor image to BCHW format.\n\n    Args:\n        tensor (torch.Tensor): image of the form :math:`(H, W)`, :math:`(C, H, W)`, :math:`(H, W, C)` or\n            :math:`(B, C, H, W)`.\n        color_channel_num (Optional[int]): Color channel of the input tensor.\n            If None, it will not alter the input channel.\n\n    Returns:\n        torch.Tensor: input tensor of the form :math:`(B, H, W, C)`.\n    """"""\n    if not torch.is_tensor(tensor):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(tensor)}"")\n\n    if len(tensor.shape) > 4 or len(tensor.shape) < 2:\n        raise ValueError(f""Input size must be a two, three or four dimensional tensor. Got {tensor.shape}"")\n\n    if len(tensor.shape) == 2:\n        tensor = tensor.unsqueeze(0)\n    if len(tensor.shape) == 3:\n        tensor = tensor.unsqueeze(0)\n    if color_channel_num is not None and color_channel_num != 1:\n        channel_list = [0, 1, 2, 3]\n        channel_list.insert(1, channel_list.pop(color_channel_num))\n        tensor = tensor.permute(*channel_list)\n    return tensor\n\n\ndef tensor_to_image(tensor: torch.Tensor) -> np.array:\n    """"""Converts a PyTorch tensor image to a numpy image. In case the tensor is in the GPU,\n    it will be copied back to CPU.\n\n    Args:\n        tensor (torch.Tensor): image of the form :math:`(H, W)`, :math:`(C, H, W)` or\n            :math:`(B, C, H, W)`.\n\n    Returns:\n        numpy.ndarray: image of the form :math:`(H, W)`, :math:`(H, W, C)` or :math:`(B, H, W, C)`.\n\n    """"""\n    if not torch.is_tensor(tensor):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(tensor)))\n\n    if len(tensor.shape) > 4 or len(tensor.shape) < 2:\n        raise ValueError(\n            ""Input size must be a two, three or four dimensional tensor"")\n\n    input_shape = tensor.shape\n    image: np.array = tensor.cpu().detach().numpy()\n    if len(input_shape) == 2:\n        # (H, W) -> (H, W)\n        image = image\n    elif len(input_shape) == 3:\n        # (C, H, W) -> (H, W, C)\n        if input_shape[0] == 1:\n            # Grayscale for proper plt.imshow needs to be (H,W)\n            image = image.squeeze()\n        else:\n            image = image.transpose(1, 2, 0)\n    elif len(input_shape) == 4:\n        # (B, C, H, W) -> (B, H, W, C)\n        image = image.transpose(0, 2, 3, 1)\n        if input_shape[0] == 1:\n            image = image.squeeze(0)\n        if input_shape[1] == 1:\n            image = image.squeeze(-1)\n    else:\n        raise ValueError(\n            ""Cannot process tensor with shape {}"".format(input_shape))\n\n    return image\n'"
kornia/utils/one_hot.py,15,"b'from typing import Optional\n\nimport torch\n\n\ndef one_hot(labels: torch.Tensor,\n            num_classes: int,\n            device: Optional[torch.device] = None,\n            dtype: Optional[torch.dtype] = None,\n            eps: Optional[float] = 1e-6) -> torch.Tensor:\n    r""""""Converts an integer label x-D tensor to a one-hot (x+1)-D tensor.\n\n    Args:\n        labels (torch.Tensor) : tensor with labels of shape :math:`(N, *)`,\n                                where N is batch size. Each value is an integer\n                                representing correct classification.\n        num_classes (int): number of classes in labels.\n        device (Optional[torch.device]): the desired device of returned tensor.\n         Default: if None, uses the current device for the default tensor type\n         (see torch.set_default_tensor_type()). device will be the CPU for CPU\n         tensor types and the current CUDA device for CUDA tensor types.\n        dtype (Optional[torch.dtype]): the desired data type of returned\n         tensor. Default: if None, infers data type from values.\n\n    Returns:\n        torch.Tensor: the labels in one hot tensor of shape :math:`(N, C, *)`,\n\n    Examples::\n        >>> labels = torch.LongTensor([[[0, 1], [2, 0]]])\n        >>> kornia.losses.one_hot(labels, num_classes=3)\n        tensor([[[[1., 0.],\n                  [0., 1.]],\n                 [[0., 1.],\n                  [0., 0.]],\n                 [[0., 0.],\n                  [1., 0.]]]]\n    """"""\n    if not torch.is_tensor(labels):\n        raise TypeError(""Input labels type is not a torch.Tensor. Got {}""\n                        .format(type(labels)))\n    if not labels.dtype == torch.int64:\n        raise ValueError(\n            ""labels must be of the same dtype torch.int64. Got: {}"" .format(\n                labels.dtype))\n    if num_classes < 1:\n        raise ValueError(""The number of classes must be bigger than one.""\n                         "" Got: {}"".format(num_classes))\n    shape = labels.shape\n    one_hot = torch.zeros(shape[0], num_classes, *shape[1:],\n                          device=device, dtype=dtype)\n    return one_hot.scatter_(1, labels.unsqueeze(1), 1.0) + eps\n'"
kornia/utils/pointcloud_io.py,12,"b'from typing import Optional\n\nimport os\nimport torch\n\n\ndef save_pointcloud_ply(filename: str, pointcloud: torch.Tensor) -> None:\n    r""""""Utility function to save to disk a pointcloud in PLY format.\n\n    Args:\n        filename (str): the path to save the pointcloud.\n        pointcloud (torch.Tensor): tensor containing the pointcloud to save.\n          The tensor must be in the shape of :math:`(*, 3)` where the last\n          component is assumed to be a 3d point coordinate :math:`(X, Y, Z)`.\n    """"""\n    if not isinstance(filename, str) and filename[-3:] == \'.ply\':\n        raise TypeError(""Input filename must be a string in with the .ply  ""\n                        ""extension. Got {}"".format(filename))\n    if not torch.is_tensor(pointcloud):\n        raise TypeError(""Input pointcloud type is not a torch.Tensor. Got {}""\n                        .format(type(pointcloud)))\n    if not len(pointcloud.shape) == 3 and pointcloud.shape[-1] == 3:\n        raise TypeError(""Input pointcloud must be in the following shape ""\n                        ""HxWx3. Got {}."".format(pointcloud.shape))\n    # flatten the input pointcloud in a vector to iterate points\n    xyz_vec: torch.Tensor = pointcloud.reshape(-1, 3)\n\n    with open(filename, \'w\') as f:\n        data_str: str = \'\'\n        num_points: int = xyz_vec.shape[0]\n        for idx in range(num_points):\n            xyz = xyz_vec[idx]\n            if not bool(torch.isfinite(xyz).any()):\n                continue\n            x: float = xyz[0].item()\n            y: float = xyz[1].item()\n            z: float = xyz[2].item()\n            data_str += \'{0} {1} {2}\\n\'.format(x, y, z)\n\n        f.write(""ply\\n"")\n        f.write(""format ascii 1.0\\n"")\n        f.write(""comment arraiy generated\\n"")\n        f.write(""element vertex %d\\n"" % num_points)\n        f.write(""property double x\\n"")\n        f.write(""property double y\\n"")\n        f.write(""property double z\\n"")\n        f.write(""end_header\\n"")\n        f.write(data_str)\n\n\ndef load_pointcloud_ply(\n        filename: str, header_size: Optional[int] = 8) -> torch.Tensor:\n    r""""""Utility function to load from disk a pointcloud in PLY format.\n\n    Args:\n        filename (str): the path to the pointcloud.\n        header_size (Optional[int]): the size of the ply file header that will\n          be skipped during loading. Default is 8 lines.\n\n    Return:\n        torch.Tensor: a tensor containing the loaded point with shape\n          :math:`(*, 3)` where :math:`*` represents the number of points.\n    """"""\n    if not isinstance(filename, str) and filename[-3:] == \'.ply\':\n        raise TypeError(""Input filename must be a string in with the .ply  ""\n                        ""extension. Got {}"".format(filename))\n    if not os.path.isfile(filename):\n        raise ValueError(""Input filename is not an existing file."")\n    if not (isinstance(header_size, int) and header_size > 0):\n        raise TypeError(""Input header_size must be a positive integer. Got {}.""\n                        .format(header_size))\n    # open the file and populate tensor\n    with open(filename, \'r\') as f:\n        points = []\n\n        # skip header\n        lines = f.readlines()[header_size:]\n\n        # iterate over the points\n        for line in lines:\n            x_str, y_str, z_str = line.split()\n            points.append((\n                torch.tensor(float(x_str)),\n                torch.tensor(float(y_str)),\n                torch.tensor(float(z_str)),\n            ))\n\n        # create tensor from list\n        pointcloud: torch.Tensor = torch.tensor(points)\n        return pointcloud\n'"
test/augmentation/test_augmentation.py,169,"b'from typing import Union, Tuple\n\nimport pytest\nimport torch\nimport torch.nn as nn\n\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\nimport kornia\nimport kornia.testing as utils  # test utils\nfrom kornia.constants import pi\nfrom kornia.augmentation import RandomHorizontalFlip, RandomVerticalFlip, ColorJitter, \\\n    RandomErasing, RandomGrayscale, RandomRotation, RandomCrop, RandomResizedCrop, RandomMotionBlur\n\n\nclass TestRandomHorizontalFlip:\n\n    def smoke_test(self, device):\n        f = RandomHorizontalFlip(0.5)\n        repr = ""RandomHorizontalFlip(p=0.5, return_transform=False)""\n        assert str(f) == repr\n\n    def test_random_hflip(self, device):\n\n        f = RandomHorizontalFlip(p=1.0, return_transform=True)\n        f1 = RandomHorizontalFlip(p=0., return_transform=True)\n        f2 = RandomHorizontalFlip(p=1.)\n        f3 = RandomHorizontalFlip(p=0.)\n\n        input = torch.tensor([[0., 0., 0., 0.],\n                              [0., 0., 0., 0.],\n                              [0., 0., 1., 2.]])  # 3 x 4\n\n        input = input.to(device)\n\n        expected = torch.tensor([[0., 0., 0., 0.],\n                                 [0., 0., 0., 0.],\n                                 [2., 1., 0., 0.]])  # 3 x 4\n\n        expected = expected.to(device)\n\n        expected_transform = torch.tensor([[-1., 0., 4.],\n                                           [0., 1., 0.],\n                                           [0., 0., 1.]])  # 3 x 3\n\n        expected_transform = expected_transform.to(device)\n\n        identity = torch.tensor([[1., 0., 0.],\n                                 [0., 1., 0.],\n                                 [0., 0., 1.]])  # 3 x 3\n        identity = identity.to(device)\n\n        assert (f(input)[0] == expected).all()\n        assert (f(input)[1] == expected_transform).all()\n        assert (f1(input)[0] == input).all()\n        assert (f1(input)[1] == identity).all()\n        assert (f2(input) == expected).all()\n        assert (f3(input) == input).all()\n\n    def test_batch_random_hflip(self, device):\n\n        f = RandomHorizontalFlip(p=1.0, return_transform=True)\n        f1 = RandomHorizontalFlip(p=0.0, return_transform=True)\n\n        input = torch.tensor([[[[0., 0., 0.],\n                                [0., 0., 0.],\n                                [0., 1., 1.]]]])  # 1 x 1 x 3 x 3\n        input = input.to(device)\n\n        expected = torch.tensor([[[[0., 0., 0.],\n                                   [0., 0., 0.],\n                                   [1., 1., 0.]]]])  # 1 x 1 x 3 x 3\n        expected = expected.to(device)\n\n        expected_transform = torch.tensor([[[-1., 0., 3.],\n                                            [0., 1., 0.],\n                                            [0., 0., 1.]]])  # 1 x 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        identity = torch.tensor([[[1., 0., 0.],\n                                  [0., 1., 0.],\n                                  [0., 0., 1.]]])  # 1 x 3 x 3\n        identity = identity.to(device)\n\n        input = input.repeat(5, 3, 1, 1)  # 5 x 3 x 3 x 3\n        expected = expected.repeat(5, 3, 1, 1)  # 5 x 3 x 3 x 3\n        expected_transform = expected_transform.repeat(5, 1, 1)  # 5 x 3 x 3\n        identity = identity.repeat(5, 1, 1)  # 5 x 3 x 3\n\n        assert (f(input)[0] == expected).all()\n        assert (f(input)[1] == expected_transform).all()\n        assert (f1(input)[0] == input).all()\n        assert (f1(input)[1] == identity).all()\n\n    def test_same_on_batch(self, device):\n        f = RandomHorizontalFlip(p=0.5, same_on_batch=True)\n        input = torch.eye(3).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 1, 1, 1)\n        res = f(input)\n        assert (res[0] == res[1]).all()\n\n    def test_sequential(self, device):\n\n        f = nn.Sequential(\n            RandomHorizontalFlip(1.0, return_transform=True),\n            RandomHorizontalFlip(1.0, return_transform=True),\n        )\n        f1 = nn.Sequential(\n            RandomHorizontalFlip(1.0, return_transform=True),\n            RandomHorizontalFlip(1.0),\n        )\n\n        input = torch.tensor([[[[0., 0., 0.],\n                                [0., 0., 0.],\n                                [0., 1., 1.]]]])  # 1 x 1 x 3 x 3\n        input = input.to(device)\n\n        expected_transform = torch.tensor([[[-1., 0., 3.],\n                                            [0., 1., 0.],\n                                            [0., 0., 1.]]])  # 1 x 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        expected_transform_1 = expected_transform @ expected_transform\n        expected_transform_1 = expected_transform_1.to(device)\n\n        assert(f(input)[0] == input).all()\n        assert(f(input)[1] == expected_transform_1).all()\n        assert(f1(input)[0] == input).all()\n        assert(f1(input)[1] == expected_transform).all()\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n            return kornia.random_hflip(data)\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]])  # 3 x 3\n\n        # Build jit trace\n        op_trace = torch.jit.trace(op_script, (input, ))\n\n        # Create new inputs\n        input = torch.tensor([[0., 0., 0.],\n                              [5., 5., 0.],\n                              [0., 0., 0.]])  # 3 x 3\n\n        input = input.repeat(2, 1, 1)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[0., 0., 0.],\n                                  [0., 5., 5.],\n                                  [0., 0., 0.]]])  # 3 x 3\n\n        expected = expected.repeat(2, 1, 1)\n\n        actual = op_trace(input)\n\n        assert_allclose(actual, expected)\n\n    def test_gradcheck(self, device):\n        input = torch.rand((3, 3)).to(device)  # 3 x 3\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(RandomHorizontalFlip(p=1.), (input, ), raise_exception=True)\n\n\nclass TestRandomVerticalFlip:\n\n    def smoke_test(self, device):\n        f = RandomVerticalFlip(0.5)\n        repr = ""RandomVerticalFlip(p=0.5, return_transform=False)""\n        assert str(f) == repr\n\n    def test_random_vflip(self, device):\n\n        f = RandomVerticalFlip(p=1.0, return_transform=True)\n        f1 = RandomVerticalFlip(p=0., return_transform=True)\n        f2 = RandomVerticalFlip(p=1.)\n        f3 = RandomVerticalFlip(p=0.)\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]])  # 3 x 3\n        input = input.to(device)\n\n        expected = torch.tensor([[0., 1., 1.],\n                                 [0., 0., 0.],\n                                 [0., 0., 0.]])  # 3 x 3\n        expected = expected.to(device)\n\n        expected_transform = torch.tensor([[1., 0., 0.],\n                                           [0., -1., 3.],\n                                           [0., 0., 1.]])  # 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        identity = torch.tensor([[1., 0., 0.],\n                                 [0., 1., 0.],\n                                 [0., 0., 1.]])  # 3 x 3\n        identity = identity.to(device)\n\n        assert_allclose(f(input)[0], expected)\n        assert_allclose(f(input)[1], expected_transform)\n        assert_allclose(f1(input)[0], input)\n        assert_allclose(f1(input)[1], identity)\n        assert_allclose(f2(input), expected)\n        assert_allclose(f3(input), input)\n\n    def test_batch_random_vflip(self, device):\n\n        f = RandomVerticalFlip(p=1.0, return_transform=True)\n        f1 = RandomVerticalFlip(p=0.0, return_transform=True)\n\n        input = torch.tensor([[[[0., 0., 0.],\n                                [0., 0., 0.],\n                                [0., 1., 1.]]]])  # 1 x 1 x 3 x 3\n        input = input.to(device)\n\n        expected = torch.tensor([[[[0., 1., 1.],\n                                   [0., 0., 0.],\n                                   [0., 0., 0.]]]])  # 1 x 1 x 3 x 3\n        expected = expected.to(device)\n\n        expected_transform = torch.tensor([[[1., 0., 0.],\n                                            [0., -1., 3.],\n                                            [0., 0., 1.]]])  # 1 x 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        identity = torch.tensor([[[1., 0., 0.],\n                                  [0., 1., 0.],\n                                  [0., 0., 1.]]])  # 1 x 3 x 3\n        identity = identity.to(device)\n\n        input = input.repeat(5, 3, 1, 1)  # 5 x 3 x 3 x 3\n        expected = expected.repeat(5, 3, 1, 1)  # 5 x 3 x 3 x 3\n        expected_transform = expected_transform.repeat(5, 1, 1)  # 5 x 3 x 3\n        identity = identity.repeat(5, 1, 1)  # 5 x 3 x 3\n\n        assert_allclose(f(input)[0], expected)\n        assert_allclose(f(input)[1], expected_transform)\n        assert_allclose(f1(input)[0], input)\n        assert_allclose(f1(input)[1], identity)\n\n    def test_same_on_batch(self, device):\n        f = RandomVerticalFlip(p=0.5, same_on_batch=True)\n        input = torch.eye(3).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 1, 1, 1)\n        res = f(input)\n        assert (res[0] == res[1]).all()\n\n    def test_sequential(self, device):\n\n        f = nn.Sequential(\n            RandomVerticalFlip(1.0, return_transform=True),\n            RandomVerticalFlip(1.0, return_transform=True),\n        )\n        f1 = nn.Sequential(\n            RandomVerticalFlip(1.0, return_transform=True),\n            RandomVerticalFlip(1.0),\n        )\n\n        input = torch.tensor([[[[0., 0., 0.],\n                                [0., 0., 0.],\n                                [0., 1., 1.]]]])  # 1 x 1 x 3 x 3\n        input = input.to(device)\n\n        expected_transform = torch.tensor([[[1., 0., 0.],\n                                            [0., -1., 3.],\n                                            [0., 0., 1.]]])  # 1 x 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        expected_transform_1 = expected_transform @ expected_transform\n\n        assert_allclose(f(input)[0], input.squeeze())\n        assert_allclose(f(input)[1], expected_transform_1)\n        assert_allclose(f1(input)[0], input.squeeze())\n        assert_allclose(f1(input)[1], expected_transform)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n            return kornia.random_vflip(data)\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]])  # 3 x 3\n\n        # Build jit trace\n        op_trace = torch.jit.trace(op_script, (input, ))\n\n        # Create new inputs\n        input = torch.tensor([[0., 0., 0.],\n                              [5., 5., 0.],\n                              [0., 0., 0.]])  # 3 x 3\n\n        input = input.repeat(2, 1, 1)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[0., 0., 0.],\n                                  [5., 5., 0.],\n                                  [0., 0., 0.]]])  # 3 x 3\n\n        expected = expected.repeat(2, 1, 1)\n\n        actual = op_trace(input)\n\n        assert_allclose(actual, expected)\n\n    def test_gradcheck(self, device):\n        input = torch.rand((3, 3)).to(device)  # 3 x 3\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(RandomVerticalFlip(p=1.), (input, ), raise_exception=True)\n\n\nclass TestColorJitter:\n\n    def smoke_test(self, device):\n        f = ColorJitter(brightness=0.5, contrast=0.3, saturation=[0.2, 1.2], hue=0.1)\n        repr = ""ColorJitter(brightness=0.5, contrast=0.3, saturation=[0.2, 1.2], hue=0.1, return_transform=False)""\n        assert str(f) == repr\n\n    def test_color_jitter(self, device):\n\n        f = ColorJitter()\n        f1 = ColorJitter(return_transform=True)\n\n        input = torch.rand(3, 5, 5).to(device)  # 3 x 5 x 5\n        expected = input\n\n        expected_transform = torch.eye(3).unsqueeze(0).to(device)  # 3 x 3\n\n        assert_allclose(f(input), expected, atol=1e-4, rtol=1e-5)\n        assert_allclose(f1(input)[0], expected, atol=1e-4, rtol=1e-5)\n        assert_allclose(f1(input)[1], expected_transform)\n\n    def test_color_jitter_batch(self, device):\n        f = ColorJitter()\n        f1 = ColorJitter(return_transform=True)\n\n        input = torch.rand(2, 3, 5, 5).to(device)  # 2 x 3 x 5 x 5\n        expected = input\n\n        expected_transform = torch.eye(3).unsqueeze(0).expand((2, 3, 3)).to(device)  # 2 x 3 x 3\n\n        assert_allclose(f(input), expected, atol=1e-4, rtol=1e-5)\n        assert_allclose(f1(input)[0], expected, atol=1e-4, rtol=1e-5)\n        assert_allclose(f1(input)[1], expected_transform)\n\n    def test_same_on_batch(self, device):\n        f = ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1, same_on_batch=True)\n        input = torch.eye(3).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 1, 1)\n        res = f(input)\n        assert (res[0] == res[1]).all()\n\n    def test_random_brightness(self, device):\n        torch.manual_seed(42)\n        f = ColorJitter(brightness=0.2)\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 3, 1, 1)  # 2 x 3 x 3\n        input = input.to(device)\n\n        expected = torch.tensor([[[[0.2529, 0.3529, 0.4529],\n                                   [0.7529, 0.6529, 0.5529],\n                                   [0.8529, 0.9529, 1.0000]],\n\n                                  [[0.2529, 0.3529, 0.4529],\n                                   [0.7529, 0.6529, 0.5529],\n                                   [0.8529, 0.9529, 1.0000]],\n\n                                  [[0.2529, 0.3529, 0.4529],\n                                   [0.7529, 0.6529, 0.5529],\n                                   [0.8529, 0.9529, 1.0000]]],\n\n\n                                 [[[0.2660, 0.3660, 0.4660],\n                                   [0.7660, 0.6660, 0.5660],\n                                   [0.8660, 0.9660, 1.0000]],\n\n                                  [[0.2660, 0.3660, 0.4660],\n                                   [0.7660, 0.6660, 0.5660],\n                                   [0.8660, 0.9660, 1.0000]],\n\n                                  [[0.2660, 0.3660, 0.4660],\n                                   [0.7660, 0.6660, 0.5660],\n                                   [0.8660, 0.9660, 1.0000]]]])  # 1 x 1 x 3 x 3\n        expected = expected.to(device)\n\n        assert_allclose(f(input), expected)\n\n    def test_random_brightness_tuple(self, device):\n        torch.manual_seed(42)\n        f = ColorJitter(brightness=(0.8, 1.2))\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 3, 1, 1)  # 2 x 3 x 3\n        input = input.to(device)\n\n        expected = torch.tensor([[[[0.2529, 0.3529, 0.4529],\n                                   [0.7529, 0.6529, 0.5529],\n                                   [0.8529, 0.9529, 1.0000]],\n\n                                  [[0.2529, 0.3529, 0.4529],\n                                   [0.7529, 0.6529, 0.5529],\n                                   [0.8529, 0.9529, 1.0000]],\n\n                                  [[0.2529, 0.3529, 0.4529],\n                                   [0.7529, 0.6529, 0.5529],\n                                   [0.8529, 0.9529, 1.0000]]],\n\n\n                                 [[[0.2660, 0.3660, 0.4660],\n                                   [0.7660, 0.6660, 0.5660],\n                                   [0.8660, 0.9660, 1.0000]],\n\n                                  [[0.2660, 0.3660, 0.4660],\n                                   [0.7660, 0.6660, 0.5660],\n                                   [0.8660, 0.9660, 1.0000]],\n\n                                  [[0.2660, 0.3660, 0.4660],\n                                   [0.7660, 0.6660, 0.5660],\n                                   [0.8660, 0.9660, 1.0000]]]])  # 1 x 1 x 3 x 3\n        expected = expected.to(device)\n\n        assert_allclose(f(input), expected)\n\n    def test_random_contrast(self, device):\n        torch.manual_seed(42)\n        f = ColorJitter(contrast=0.2)\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 3, 1, 1)  # 2 x 3 x 3\n        input = input.to(device)\n\n        expected = torch.tensor([[[[0.0953, 0.1906, 0.2859],\n                                   [0.5719, 0.4766, 0.3813],\n                                   [0.6672, 0.7625, 0.9531]],\n\n                                  [[0.0953, 0.1906, 0.2859],\n                                   [0.5719, 0.4766, 0.3813],\n                                   [0.6672, 0.7625, 0.9531]],\n\n                                  [[0.0953, 0.1906, 0.2859],\n                                   [0.5719, 0.4766, 0.3813],\n                                   [0.6672, 0.7625, 0.9531]]],\n\n\n                                 [[[0.1184, 0.2367, 0.3551],\n                                   [0.7102, 0.5919, 0.4735],\n                                   [0.8286, 0.9470, 1.0000]],\n\n                                  [[0.1184, 0.2367, 0.3551],\n                                   [0.7102, 0.5919, 0.4735],\n                                   [0.8286, 0.9470, 1.0000]],\n\n                                  [[0.1184, 0.2367, 0.3551],\n                                   [0.7102, 0.5919, 0.4735],\n                                   [0.8286, 0.9470, 1.0000]]]])\n        expected = expected.to(device)\n\n        assert_allclose(f(input), expected, atol=1e-4, rtol=1e-5)\n\n    def test_random_contrast_list(self, device):\n        torch.manual_seed(42)\n        f = ColorJitter(contrast=[0.8, 1.2])\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 3, 1, 1).to(device)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[[0.0953, 0.1906, 0.2859],\n                                   [0.5719, 0.4766, 0.3813],\n                                   [0.6672, 0.7625, 0.9531]],\n\n                                  [[0.0953, 0.1906, 0.2859],\n                                   [0.5719, 0.4766, 0.3813],\n                                   [0.6672, 0.7625, 0.9531]],\n\n                                  [[0.0953, 0.1906, 0.2859],\n                                   [0.5719, 0.4766, 0.3813],\n                                   [0.6672, 0.7625, 0.9531]]],\n\n\n                                 [[[0.1184, 0.2367, 0.3551],\n                                   [0.7102, 0.5919, 0.4735],\n                                   [0.8286, 0.9470, 1.0000]],\n\n                                  [[0.1184, 0.2367, 0.3551],\n                                   [0.7102, 0.5919, 0.4735],\n                                   [0.8286, 0.9470, 1.0000]],\n\n                                  [[0.1184, 0.2367, 0.3551],\n                                   [0.7102, 0.5919, 0.4735],\n                                   [0.8286, 0.9470, 1.0000]]]])\n        expected = expected.to(device)\n\n        assert_allclose(f(input), expected, atol=1e-4, rtol=1e-5)\n\n    def test_random_saturation(self, device):\n        torch.manual_seed(42)\n        f = ColorJitter(saturation=0.2)\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]],\n\n                               [[1.0, 0.5, 0.6],\n                                [0.6, 0.3, 0.2],\n                                [0.8, 0.1, 0.2]],\n\n                               [[0.6, 0.8, 0.7],\n                                [0.9, 0.3, 0.2],\n                                [0.8, 0.4, .5]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 1, 1, 1).to(device)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[[1.8763e-01, 2.5842e-01, 3.3895e-01],\n                                   [6.2921e-01, 5.0000e-01, 4.0000e-01],\n                                   [7.0974e-01, 8.0000e-01, 1.0000e+00]],\n\n                                  [[1.0000e+00, 5.2921e-01, 6.0974e-01],\n                                   [6.2921e-01, 3.1947e-01, 2.1947e-01],\n                                   [8.0000e-01, 1.6816e-01, 2.7790e-01]],\n\n                                  [[6.3895e-01, 8.0000e-01, 7.0000e-01],\n                                   [9.0000e-01, 3.1947e-01, 2.1947e-01],\n                                   [8.0000e-01, 4.3895e-01, 5.4869e-01]]],\n\n\n                                 [[[1.1921e-07, 1.2953e-01, 2.5302e-01],\n                                   [5.6476e-01, 5.0000e-01, 4.0000e-01],\n                                   [6.8825e-01, 8.0000e-01, 1.0000e+00]],\n\n                                  [[1.0000e+00, 4.6476e-01, 5.8825e-01],\n                                   [5.6476e-01, 2.7651e-01, 1.7651e-01],\n                                   [8.0000e-01, 1.7781e-02, 1.0603e-01]],\n\n                                  [[5.5556e-01, 8.0000e-01, 7.0000e-01],\n                                   [9.0000e-01, 2.7651e-01, 1.7651e-01],\n                                   [8.0000e-01, 3.5302e-01, 4.4127e-01]]]])\n        expected = expected.to(device)\n        assert_allclose(f(input), expected)\n\n    def test_random_saturation_tensor(self, device):\n        torch.manual_seed(42)\n        f = ColorJitter(saturation=torch.tensor(0.2))\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]],\n\n                               [[1.0, 0.5, 0.6],\n                                [0.6, 0.3, 0.2],\n                                [0.8, 0.1, 0.2]],\n\n                               [[0.6, 0.8, 0.7],\n                                [0.9, 0.3, 0.2],\n                                [0.8, 0.4, .5]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 1, 1, 1).to(device)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[[1.8763e-01, 2.5842e-01, 3.3895e-01],\n                                   [6.2921e-01, 5.0000e-01, 4.0000e-01],\n                                   [7.0974e-01, 8.0000e-01, 1.0000e+00]],\n\n                                  [[1.0000e+00, 5.2921e-01, 6.0974e-01],\n                                   [6.2921e-01, 3.1947e-01, 2.1947e-01],\n                                   [8.0000e-01, 1.6816e-01, 2.7790e-01]],\n\n                                  [[6.3895e-01, 8.0000e-01, 7.0000e-01],\n                                   [9.0000e-01, 3.1947e-01, 2.1947e-01],\n                                   [8.0000e-01, 4.3895e-01, 5.4869e-01]]],\n\n\n                                 [[[1.1921e-07, 1.2953e-01, 2.5302e-01],\n                                   [5.6476e-01, 5.0000e-01, 4.0000e-01],\n                                   [6.8825e-01, 8.0000e-01, 1.0000e+00]],\n\n                                  [[1.0000e+00, 4.6476e-01, 5.8825e-01],\n                                   [5.6476e-01, 2.7651e-01, 1.7651e-01],\n                                   [8.0000e-01, 1.7781e-02, 1.0603e-01]],\n\n                                  [[5.5556e-01, 8.0000e-01, 7.0000e-01],\n                                   [9.0000e-01, 2.7651e-01, 1.7651e-01],\n                                   [8.0000e-01, 3.5302e-01, 4.4127e-01]]]])\n        expected = expected.to(device)\n\n        assert_allclose(f(input), expected)\n\n    def test_random_saturation_tuple(self, device):\n        torch.manual_seed(42)\n        f = ColorJitter(saturation=(0.8, 1.2))\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]],\n\n                               [[1.0, 0.5, 0.6],\n                                [0.6, 0.3, 0.2],\n                                [0.8, 0.1, 0.2]],\n\n                               [[0.6, 0.8, 0.7],\n                                [0.9, 0.3, 0.2],\n                                [0.8, 0.4, .5]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 1, 1, 1).to(device)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[[1.8763e-01, 2.5842e-01, 3.3895e-01],\n                                   [6.2921e-01, 5.0000e-01, 4.0000e-01],\n                                   [7.0974e-01, 8.0000e-01, 1.0000e+00]],\n\n                                  [[1.0000e+00, 5.2921e-01, 6.0974e-01],\n                                   [6.2921e-01, 3.1947e-01, 2.1947e-01],\n                                   [8.0000e-01, 1.6816e-01, 2.7790e-01]],\n\n                                  [[6.3895e-01, 8.0000e-01, 7.0000e-01],\n                                   [9.0000e-01, 3.1947e-01, 2.1947e-01],\n                                   [8.0000e-01, 4.3895e-01, 5.4869e-01]]],\n\n\n                                 [[[1.1921e-07, 1.2953e-01, 2.5302e-01],\n                                   [5.6476e-01, 5.0000e-01, 4.0000e-01],\n                                   [6.8825e-01, 8.0000e-01, 1.0000e+00]],\n\n                                  [[1.0000e+00, 4.6476e-01, 5.8825e-01],\n                                   [5.6476e-01, 2.7651e-01, 1.7651e-01],\n                                   [8.0000e-01, 1.7781e-02, 1.0603e-01]],\n\n                                  [[5.5556e-01, 8.0000e-01, 7.0000e-01],\n                                   [9.0000e-01, 2.7651e-01, 1.7651e-01],\n                                   [8.0000e-01, 3.5302e-01, 4.4127e-01]]]])\n        expected = expected.to(device)\n\n        assert_allclose(f(input), expected)\n\n    def test_random_hue(self, device):\n        torch.manual_seed(42)\n        f = ColorJitter(hue=0.1 / pi)\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]],\n\n                               [[1.0, 0.5, 0.6],\n                                [0.6, 0.3, 0.2],\n                                [0.8, 0.1, 0.2]],\n\n                               [[0.6, 0.8, 0.7],\n                                [0.9, 0.3, 0.2],\n                                [0.8, 0.4, .5]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 1, 1, 1).to(device)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[[0.1000, 0.2000, 0.3000],\n                                   [0.6000, 0.5000, 0.4000],\n                                   [0.7000, 0.8000, 1.0000]],\n\n                                  [[1.0000, 0.5251, 0.6167],\n                                   [0.6126, 0.3000, 0.2000],\n                                   [0.8000, 0.1000, 0.2000]],\n\n                                  [[0.5623, 0.8000, 0.7000],\n                                   [0.9000, 0.3084, 0.2084],\n                                   [0.7958, 0.4293, 0.5335]]],\n\n                                 [[[0.1000, 0.2000, 0.3000],\n                                   [0.6116, 0.5000, 0.4000],\n                                   [0.7000, 0.8000, 1.0000]],\n\n                                  [[1.0000, 0.4769, 0.5846],\n                                   [0.6000, 0.3077, 0.2077],\n                                   [0.7961, 0.1000, 0.2000]],\n\n                                  [[0.6347, 0.8000, 0.7000],\n                                   [0.9000, 0.3000, 0.2000],\n                                   [0.8000, 0.3730, 0.4692]]]])\n        expected = expected.to(device)\n\n        assert_allclose(f(input), expected)\n\n    def test_random_hue_list(self, device):\n        torch.manual_seed(42)\n        f = ColorJitter(hue=[-0.1 / pi, 0.1 / pi])\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]],\n\n                               [[1.0, 0.5, 0.6],\n                                [0.6, 0.3, 0.2],\n                                [0.8, 0.1, 0.2]],\n\n                               [[0.6, 0.8, 0.7],\n                                [0.9, 0.3, 0.2],\n                                [0.8, 0.4, .5]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 1, 1, 1).to(device)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[[0.1000, 0.2000, 0.3000],\n                                   [0.6000, 0.5000, 0.4000],\n                                   [0.7000, 0.8000, 1.0000]],\n\n                                  [[1.0000, 0.5251, 0.6167],\n                                   [0.6126, 0.3000, 0.2000],\n                                   [0.8000, 0.1000, 0.2000]],\n\n                                  [[0.5623, 0.8000, 0.7000],\n                                   [0.9000, 0.3084, 0.2084],\n                                   [0.7958, 0.4293, 0.5335]]],\n\n\n                                 [[[0.1000, 0.2000, 0.3000],\n                                   [0.6116, 0.5000, 0.4000],\n                                   [0.7000, 0.8000, 1.0000]],\n\n                                  [[1.0000, 0.4769, 0.5846],\n                                   [0.6000, 0.3077, 0.2077],\n                                   [0.7961, 0.1000, 0.2000]],\n\n                                  [[0.6347, 0.8000, 0.7000],\n                                   [0.9000, 0.3000, 0.2000],\n                                   [0.8000, 0.3730, 0.4692]]]])\n        expected = expected.to(device)\n\n        assert_allclose(f(input), expected)\n\n    def test_random_hue_tensor(self, device):\n        torch.manual_seed(42)\n        f = ColorJitter(hue=torch.tensor([-0.1 / pi, 0.1 / pi]))\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]],\n\n                               [[1.0, 0.5, 0.6],\n                                [0.6, 0.3, 0.2],\n                                [0.8, 0.1, 0.2]],\n\n                               [[0.6, 0.8, 0.7],\n                                [0.9, 0.3, 0.2],\n                                [0.8, 0.4, .5]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 1, 1, 1).to(device)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[[0.1000, 0.2000, 0.3000],\n                                   [0.6000, 0.5000, 0.4000],\n                                   [0.7000, 0.8000, 1.0000]],\n\n                                  [[1.0000, 0.5251, 0.6167],\n                                   [0.6126, 0.3000, 0.2000],\n                                   [0.8000, 0.1000, 0.2000]],\n\n                                  [[0.5623, 0.8000, 0.7000],\n                                   [0.9000, 0.3084, 0.2084],\n                                   [0.7958, 0.4293, 0.5335]]],\n\n\n                                 [[[0.1000, 0.2000, 0.3000],\n                                   [0.6116, 0.5000, 0.4000],\n                                   [0.7000, 0.8000, 1.0000]],\n\n                                  [[1.0000, 0.4769, 0.5846],\n                                   [0.6000, 0.3077, 0.2077],\n                                   [0.7961, 0.1000, 0.2000]],\n\n                                  [[0.6347, 0.8000, 0.7000],\n                                   [0.9000, 0.3000, 0.2000],\n                                   [0.8000, 0.3730, 0.4692]]]])\n        expected = expected.to(device)\n\n        assert_allclose(f(input), expected)\n\n    def test_sequential(self, device):\n\n        f = nn.Sequential(\n            ColorJitter(return_transform=True),\n            ColorJitter(return_transform=True),\n        )\n\n        input = torch.rand(3, 5, 5).to(device)  # 3 x 5 x 5\n\n        expected = input\n\n        expected_transform = torch.eye(3).unsqueeze(0)  # 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        assert_allclose(f(input)[0], expected, atol=1e-4, rtol=1e-5)\n        assert_allclose(f(input)[1], expected_transform)\n\n    def test_color_jitter_batch_sequential(self, device):\n        f = nn.Sequential(\n            ColorJitter(return_transform=True),\n            ColorJitter(return_transform=True),\n        )\n\n        input = torch.rand(2, 3, 5, 5).to(device)  # 2 x 3 x 5 x 5\n        expected = input\n\n        expected_transform = torch.eye(3).unsqueeze(0).expand((2, 3, 3))  # 2 x 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        assert_allclose(f(input)[0], expected, atol=1e-4, rtol=1e-5)\n        assert_allclose(f(input)[0], expected)\n        assert_allclose(f(input)[1], expected_transform)\n\n    def test_gradcheck(self, device):\n        input = torch.rand((3, 5, 5)).to(device)  # 3 x 3\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.color_jitter, (input, ), raise_exception=True)\n\n\nclass TestRectangleRandomErasing:\n    @pytest.mark.parametrize(""erase_scale_range"", [(.001, .001), (1., 1.)])\n    @pytest.mark.parametrize(""aspect_ratio_range"", [(.1, .1), (10., 10.)])\n    @pytest.mark.parametrize(""batch_shape"", [(1, 4, 8, 15), (2, 3, 11, 7)])\n    def test_random_rectangle_erasing_shape(\n            self, batch_shape, erase_scale_range, aspect_ratio_range):\n        input = torch.rand(batch_shape)\n        rand_rec = RandomErasing(1.0, erase_scale_range, aspect_ratio_range)\n        assert rand_rec(input).shape == batch_shape\n\n    @pytest.mark.parametrize(""erase_scale_range"", [(.001, .001), (1., 1.)])\n    @pytest.mark.parametrize(""aspect_ratio_range"", [(.1, .1), (10., 10.)])\n    @pytest.mark.parametrize(""batch_shape"", [(1, 4, 8, 15), (2, 3, 11, 7)])\n    def test_no_rectangle_erasing_shape(\n            self, batch_shape, erase_scale_range, aspect_ratio_range):\n        input = torch.rand(batch_shape)\n        rand_rec = RandomErasing(0., erase_scale_range, aspect_ratio_range)\n        assert rand_rec(input).equal(input)\n\n    @pytest.mark.parametrize(""erase_scale_range"", [(.001, .001), (1., 1.)])\n    @pytest.mark.parametrize(""aspect_ratio_range"", [(.1, .1), (10., 10.)])\n    @pytest.mark.parametrize(""shape"", [(3, 11, 7)])\n    def test_same_on_batch(self, shape, erase_scale_range, aspect_ratio_range):\n        f = RandomErasing(0.5, erase_scale_range, aspect_ratio_range, same_on_batch=True)\n        input = torch.rand(shape).unsqueeze(dim=0).repeat(2, 1, 1, 1)\n        res = f(input)\n        print(f._params)\n        assert (res[0] == res[1]).all()\n\n    def test_gradcheck(self, device):\n        # test parameters\n        batch_shape = (2, 3, 11, 7)\n        erase_scale_range = (.2, .4)\n        aspect_ratio_range = (.3, .5)\n\n        rand_rec = RandomErasing(1.0, erase_scale_range, aspect_ratio_range)\n        rect_params = rand_rec.generate_parameters(batch_shape)\n\n        # evaluate function gradient\n        input = torch.rand(batch_shape).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(\n            rand_rec,\n            (input, rect_params),\n            raise_exception=True,\n        )\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(img):\n            return kornia.augmentation.random_rectangle_erase(img, (.2, .4), (.3, .5))\n\n        batch_size, channels, height, width = 2, 3, 64, 64\n        img = torch.ones(batch_size, channels, height, width)\n        expected = RandomErasing(\n            1.0, (.2, .4), (.3, .5)\n        )(img)\n        actual = op_script(img)\n        assert_allclose(actual, expected)\n\n\nclass TestRandomGrayscale:\n\n    def smoke_test(self, device):\n        f = RandomGrayscale()\n        repr = ""RandomGrayscale(p=0.5, return_transform=False)""\n        assert str(f) == repr\n\n    def test_random_grayscale(self, device):\n\n        f = RandomGrayscale(return_transform=True)\n\n        input = torch.rand(3, 5, 5).to(device)  # 3 x 5 x 5\n\n        expected_transform = torch.eye(3).unsqueeze(0)  # 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        assert_allclose(f(input)[1], expected_transform)\n\n    def test_same_on_batch(self, device):\n        f = RandomGrayscale(p=0.5, same_on_batch=True)\n        input = torch.eye(3).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 1, 1)\n        res = f(input)\n        assert (res[0] == res[1]).all()\n\n    def test_opencv_true(self, device):\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device)\n\n        # Output data generated with OpenCV 4.1.1: cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n        expected = torch.tensor([[[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]],\n\n                                 [[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]],\n\n                                 [[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]]])\n        expected = expected.to(device)\n\n        img_gray = kornia.random_grayscale(data, p=1.)\n        assert_allclose(img_gray, expected)\n\n    def test_opencv_false(self, device):\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device)\n\n        expected = data\n\n        img_gray = kornia.random_grayscale(data, p=0.)\n        assert_allclose(img_gray, expected)\n\n    def test_opencv_true_batch(self, device):\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device)\n        data = data.unsqueeze(0).repeat(4, 1, 1, 1)\n\n        # Output data generated with OpenCV 4.1.1: cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n        expected = torch.tensor([[[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]],\n\n                                 [[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]],\n\n                                 [[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]]])\n        expected = expected.to(device)\n        expected = expected.unsqueeze(0).repeat(4, 1, 1, 1)\n\n        img_gray = kornia.random_grayscale(data, p=1.)\n        assert_allclose(img_gray, expected)\n\n    def test_opencv_false_batch(self, device):\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device)\n        data = data.unsqueeze(0).repeat(4, 1, 1, 1)\n\n        expected = data\n\n        img_gray = kornia.random_grayscale(data, p=0.)\n        assert_allclose(img_gray, expected)\n\n    def test_random_grayscale_sequential_batch(self, device):\n        f = nn.Sequential(\n            RandomGrayscale(p=0., return_transform=True),\n            RandomGrayscale(p=0., return_transform=True),\n        )\n\n        input = torch.rand(2, 3, 5, 5).to(device)  # 2 x 3 x 5 x 5\n        expected = input\n\n        expected_transform = torch.eye(3).unsqueeze(0).expand((2, 3, 3))  # 2 x 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        assert_allclose(f(input)[0], expected)\n        assert_allclose(f(input)[1], expected_transform)\n\n    def test_gradcheck(self, device):\n        input = torch.rand((3, 5, 5)).to(device)  # 3 x 3\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.random_grayscale, (input, 0.), raise_exception=True)\n        assert gradcheck(kornia.random_grayscale, (input, 1.), raise_exception=True)\n\n\nclass TestCenterCrop:\n\n    def test_no_transform(self, device):\n        inp = torch.rand(1, 2, 4, 4).to(device)\n        out = kornia.augmentation.CenterCrop(2)(inp)\n        assert out.shape == (1, 2, 2, 2)\n\n    def test_transform(self, device):\n        inp = torch.rand(1, 2, 5, 4).to(device)\n        out = kornia.augmentation.CenterCrop(2, return_transform=True)(inp)\n        assert len(out) == 2\n        assert out[0].shape == (1, 2, 2, 2)\n        assert out[1].shape == (1, 3, 3)\n\n    def test_no_transform_tuple(self, device):\n        inp = torch.rand(1, 2, 5, 4).to(device)\n        out = kornia.augmentation.CenterCrop((3, 4))(inp)\n        assert out.shape == (1, 2, 3, 4)\n\n    def test_gradcheck(self, device):\n        input = torch.rand(1, 2, 3, 4).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.augmentation.CenterCrop(3), (input,), raise_exception=True)\n\n\nclass TestRandomRotation:\n\n    torch.manual_seed(0)  # for random reproductibility\n\n    def smoke_test(self, device):\n        f = RandomRotation(degrees=45.5)\n        repr = ""RandomHorizontalFlip(degrees=45.5, return_transform=False)""\n        assert str(f) == repr\n\n    def test_random_rotation(self, device):\n        # This is included in doctest\n        torch.manual_seed(0)  # for random reproductibility\n\n        f = RandomRotation(degrees=45.0, return_transform=True)\n        f1 = RandomRotation(degrees=45.0)\n\n        input = torch.tensor([[1., 0., 0., 2.],\n                              [0., 0., 0., 0.],\n                              [0., 1., 2., 0.],\n                              [0., 0., 1., 2.]])  # 4 x 4\n        input = input.to(device)\n\n        expected = torch.tensor([[[0.9824, 0.0088, 0.0000, 1.9649],\n                                  [0.0000, 0.0029, 0.0000, 0.0176],\n                                  [0.0029, 1.0000, 1.9883, 0.0000],\n                                  [0.0000, 0.0088, 1.0117, 1.9649]]])  # 1 x 4 x 4\n        expected = expected.to(device)\n\n        expected_transform = torch.tensor([[[1.0000, -0.0059, 0.0088],\n                                            [0.0059, 1.0000, -0.0088],\n                                            [0.0000, 0.0000, 1.0000]]])  # 1 x 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        expected_2 = torch.tensor([[0.1322, 0.0000, 0.7570, 0.2644],\n                                   [0.3785, 0.0000, 0.4166, 0.0000],\n                                   [0.0000, 0.6309, 1.5910, 1.2371],\n                                   [0.0000, 0.1444, 0.3177, 0.6499]])  # 1 x 4 x 4\n        expected_2 = expected_2.to(device)\n\n        out, mat = f(input)\n        assert_allclose(out, expected, rtol=1e-6, atol=1e-4)\n        assert_allclose(mat, expected_transform, rtol=1e-6, atol=1e-4)\n        assert_allclose(f1(input), expected_2, rtol=1e-6, atol=1e-4)\n\n    def test_batch_random_rotation(self, device):\n\n        torch.manual_seed(0)  # for random reproductibility\n\n        f = RandomRotation(degrees=45.0, return_transform=True)\n\n        input = torch.tensor([[[[1., 0., 0., 2.],\n                                [0., 0., 0., 0.],\n                                [0., 1., 2., 0.],\n                                [0., 0., 1., 2.]]]])  # 1 x 1 x 4 x 4\n        input = input.to(device)\n\n        expected = torch.tensor([[[[0.9824, 0.0088, 0.0000, 1.9649],\n                                   [0.0000, 0.0029, 0.0000, 0.0176],\n                                   [0.0029, 1.0000, 1.9883, 0.0000],\n                                   [0.0000, 0.0088, 1.0117, 1.9649]]],\n                                 [[[0.1322, 0.0000, 0.7570, 0.2644],\n                                   [0.3785, 0.0000, 0.4166, 0.0000],\n                                   [0.0000, 0.6309, 1.5910, 1.2371],\n                                   [0.0000, 0.1444, 0.3177, 0.6499]]]])  # 2 x 1 x 4 x 4\n        expected = expected.to(device)\n\n        expected_transform = torch.tensor([[[1.0000, -0.0059, 0.0088],\n                                            [0.0059, 1.0000, -0.0088],\n                                            [0.0000, 0.0000, 1.0000]],\n\n                                           [[0.9125, 0.4090, -0.4823],\n                                            [-0.4090, 0.9125, 0.7446],\n                                            [0.0000, 0.0000, 1.0000]]])  # 2 x 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        input = input.repeat(2, 1, 1, 1)  # 5 x 3 x 3 x 3\n\n        out, mat = f(input)\n        assert_allclose(out, expected, rtol=1e-6, atol=1e-4)\n        assert_allclose(mat, expected_transform, rtol=1e-6, atol=1e-4)\n\n    def test_same_on_batch(self, device):\n        f = RandomRotation(degrees=40, same_on_batch=True)\n        input = torch.eye(6).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 1, 1)\n        res = f(input)\n        assert (res[0] == res[1]).all()\n\n    def test_sequential(self, device):\n\n        torch.manual_seed(0)  # for random reproductibility\n\n        f = nn.Sequential(\n            RandomRotation(torch.tensor([-45.0, 90]), return_transform=True),\n            RandomRotation(10.4, return_transform=True),\n        )\n        f1 = nn.Sequential(\n            RandomRotation(torch.tensor([-45.0, 90]), return_transform=True),\n            RandomRotation(10.4),\n        )\n\n        input = torch.tensor([[1., 0., 0., 2.],\n                              [0., 0., 0., 0.],\n                              [0., 1., 2., 0.],\n                              [0., 0., 1., 2.]])  # 4 x 4\n        input = input.to(device)\n\n        expected = torch.tensor([[[0.1314, 0.1050, 0.6649, 0.2628],\n                                  [0.3234, 0.0202, 0.4256, 0.1671],\n                                  [0.0525, 0.5976, 1.5199, 1.1306],\n                                  [0.0000, 0.1453, 0.3224, 0.5796]]])  # 1 x 4 x 4\n        expected = expected.to(device)\n\n        expected_transform = torch.tensor([[[0.8864, 0.4629, -0.5240],\n                                            [-0.4629, 0.8864, 0.8647],\n                                            [0.0000, 0.0000, 1.0000]]])  # 1 x 3 x 3\n        expected_transform = expected_transform.to(device)\n\n        expected_transform_2 = torch.tensor([[[0.8381, -0.5455, 1.0610],\n                                              [0.5455, 0.8381, -0.5754],\n                                              [0.0000, 0.0000, 1.0000]]])  # 1 x 3 x 3\n        expected_transform_2 = expected_transform_2.to(device)\n\n        out, mat = f(input)\n        _, mat_2 = f1(input)\n        assert_allclose(out, expected, rtol=1e-6, atol=1e-4)\n        assert_allclose(mat, expected_transform, rtol=1e-6, atol=1e-4)\n        assert_allclose(mat_2, expected_transform_2, rtol=1e-6, atol=1e-4)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n\n        torch.manual_seed(0)  # for random reproductibility\n\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n            return kornia.random_rotation(data, degrees=45.0)\n\n        input = torch.tensor([[1., 0., 0., 2.],\n                              [0., 0., 0., 0.],\n                              [0., 1., 2., 0.],\n                              [0., 0., 1., 2.]])  # 4 x 4\n\n        # Build jit trace\n        op_trace = torch.jit.trace(op_script, (input, ))\n\n        # Create new inputs\n        input = torch.tensor([[0., 0., 0.],\n                              [5., 5., 0.],\n                              [0., 0., 0.]])  # 3 x 3\n\n        expected = torch.tensor([[[0.0000, 0.2584, 0.0000],\n                                  [2.9552, 5.0000, 0.2584],\n                                  [1.6841, 0.4373, 0.0000]]])\n\n        actual = op_trace(input)\n\n        assert_allclose(actual, expected, rtol=1e-6, atol=1e-4)\n\n    def test_gradcheck(self, device):\n\n        torch.manual_seed(0)  # for random reproductibility\n\n        input = torch.rand((3, 3)).to(device)  # 3 x 3\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(RandomRotation(degrees=(15.0, 15.0)), (input, ), raise_exception=True)\n\n\nclass TestRandomCrop:\n    def smoke_test(self, device):\n        f = RandomCrop(size=(2, 3), padding=(0, 1), fill=10, pad_if_needed=False)\n        repr = ""RandomCrop(crop_size=(2, 3), padding=(0, 1), fill=10, pad_if_needed=False,\\\n            return_transform=False)""\n        assert str(f) == repr\n\n    def test_no_padding(self, device):\n        torch.manual_seed(0)\n        inp = torch.tensor([[[\n            [0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]\n        ]]]).to(device)\n        expected = torch.tensor([[[\n            [3., 4., 5.],\n            [6., 7., 8.]\n        ]]]).to(device)\n        rc = RandomCrop(size=(2, 3), padding=None, align_corners=True)\n        out = rc(inp)\n\n        assert_allclose(out, expected)\n\n    def test_no_padding_batch(self, device):\n        torch.manual_seed(0)\n        batch_size = 2\n        inp = torch.tensor([[\n            [0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]\n        ]]).repeat(batch_size, 1, 1, 1).to(device)\n        expected = torch.tensor([[\n            [0., 1., 2.],\n            [3., 4., 5.],\n        ]]).repeat(batch_size, 1, 1, 1).to(device)\n        rc = RandomCrop(size=(2, 3), padding=None, align_corners=True)\n        out = rc(inp)\n\n        assert_allclose(out, expected)\n\n    def test_same_on_batch(self, device):\n        f = RandomCrop(size=(2, 3), padding=1, same_on_batch=True, align_corners=True)\n        input = torch.eye(6).unsqueeze(dim=0).unsqueeze(dim=0).repeat(2, 3, 1, 1)\n        res = f(input)\n        assert (res[0] == res[1]).all()\n\n    def test_padding_batch_1(self, device):\n        torch.manual_seed(0)\n        batch_size = 2\n        inp = torch.tensor([[\n            [0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]\n        ]]).repeat(batch_size, 1, 1, 1).to(device)\n        expected = torch.tensor([[[\n            [0., 0., 0.],\n            [0., 1., 2.]\n        ]], [[\n            [0., 0., 0.],\n            [1., 2., 0.]\n        ]]]).to(device)\n        rc = RandomCrop(size=(2, 3), padding=1, align_corners=True)\n        out = rc(inp)\n\n        assert_allclose(out, expected)\n\n    def test_padding_batch_2(self, device):\n        torch.manual_seed(0)\n        batch_size = 2\n        inp = torch.tensor([[\n            [0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]\n        ]]).repeat(batch_size, 1, 1, 1).to(device)\n        expected = torch.tensor([[[\n            [0., 1., 2.],\n            [3., 4., 5.]\n        ]], [[\n            [1., 2., 10.],\n            [4., 5., 10.]\n        ]]]).to(device)\n        rc = RandomCrop(size=(2, 3), padding=(0, 1), fill=10, align_corners=True)\n        out = rc(inp)\n\n        assert_allclose(out, expected)\n\n    def test_padding_batch_3(self, device):\n        torch.manual_seed(0)\n        batch_size = 2\n        inp = torch.tensor([[\n            [0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]\n        ]]).repeat(batch_size, 1, 1, 1).to(device)\n        expected = torch.tensor([[[\n            [8., 8., 8.],\n            [8., 0., 1.]\n        ]], [[\n            [8., 8., 8.],\n            [1., 2., 8.]\n        ]]]).to(device)\n        rc = RandomCrop(size=(2, 3), padding=(0, 1, 2, 3), fill=8, align_corners=True)\n        out = rc(inp)\n\n        assert_allclose(out, expected)\n\n    def test_pad_if_needed(self, device):\n        torch.manual_seed(0)\n        batch_size = 2\n        inp = torch.tensor([[\n            [0., 1., 2.],\n        ]]).repeat(batch_size, 1, 1, 1).to(device)\n        expected = torch.tensor([[\n            [9., 9., 9.],\n            [0., 1., 2.]\n        ]]).repeat(batch_size, 1, 1, 1).to(device)\n        rc = RandomCrop(size=(2, 3), pad_if_needed=True, fill=9, align_corners=True)\n        out = rc(inp)\n\n        assert_allclose(out, expected)\n\n    def test_gradcheck(self, device):\n        torch.manual_seed(0)  # for random reproductibility\n        inp = torch.rand((3, 3, 3)).to(device)  # 3 x 3\n        inp = utils.tensor_to_gradcheck_var(inp)  # to var\n        assert gradcheck(RandomCrop(size=(3, 3)), (inp, ), raise_exception=True)\n\n\nclass TestRandomResizedCrop:\n    def smoke_test(self, device):\n        f = RandomResizedCrop(size=(2, 3), scale=(1., 1.), ratio=(1.0, 1.0))\n        repr = ""RandomResizedCrop(size=(2, 3), resize_to=(1., 1.), resize_to=(1., 1.)\\\n            , return_transform=False)""\n        assert str(f) == repr\n\n    def test_no_resize(self, device):\n        torch.manual_seed(0)\n        inp = torch.tensor([[\n            [0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]\n        ]]).to(device)\n\n        expected = torch.tensor(\n            [[[[5.3750, 5.8750, 4.5938],\n               [6.3437, 6.7812, 5.2500]]]]).to(device)\n        rrc = RandomResizedCrop(size=(2, 3), scale=(1., 1.), ratio=(1.0, 1.0))\n        # It will crop a size of (2, 2) from the aspect ratio implementation of torch\n        out = rrc(inp)\n        assert_allclose(out, expected)\n\n    def test_same_on_batch(self, device):\n        f = RandomResizedCrop(size=(2, 3), scale=(1., 1.), ratio=(1.0, 1.0), same_on_batch=True)\n        input = torch.tensor([[\n            [0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]\n        ]]).repeat(2, 1, 1, 1).to(device)\n        res = f(input)\n        assert (res[0] == res[1]).all()\n\n    def test_crop_scale_ratio(self, device):\n        # This is included in doctest\n        torch.manual_seed(0)\n        inp = torch.tensor([[\n            [0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]\n        ]]).to(device)\n\n        expected = torch.tensor(\n            [[[[3.7500, 4.7500, 5.7500],\n               [5.2500, 6.2500, 7.2500],\n               [4.5000, 5.2500, 6.0000]]]]).to(device)\n        rrc = RandomResizedCrop(size=(3, 3), scale=(3., 3.), ratio=(2., 2.))\n        # It will crop a size of (2, 2) from the aspect ratio implementation of torch\n        out = rrc(inp)\n        assert_allclose(out, expected)\n\n    def test_crop_scale_ratio_batch(self, device):\n        torch.manual_seed(0)\n        batch_size = 2\n        inp = torch.tensor([[\n            [0., 1., 2.],\n            [3., 4., 5.],\n            [6., 7., 8.]\n        ]]).repeat(batch_size, 1, 1, 1).to(device)\n\n        expected = torch. tensor(\n            [[[[0.0000, 0.7500, 1.5000],\n               [0.7500, 1.7500, 2.7500],\n               [2.2500, 3.2500, 4.2500]]],\n             [[[3.7500, 4.7500, 5.7500],\n               [5.2500, 6.2500, 7.2500],\n               [4.5000, 5.2500, 6.0000]]]]).to(device)\n        rrc = RandomResizedCrop(size=(3, 3), scale=(3., 3.), ratio=(2., 2.))\n        # It will crop a size of (2, 2) from the aspect ratio implementation of torch\n        out = rrc(inp)\n        assert_allclose(out, expected)\n\n    def test_gradcheck(self, device):\n        torch.manual_seed(0)  # for random reproductibility\n        inp = torch.rand((1, 3, 3)).to(device)  # 3 x 3\n        inp = utils.tensor_to_gradcheck_var(inp)  # to var\n        assert gradcheck(RandomResizedCrop(size=(3, 3), scale=(1., 1.), ratio=(1., 1.)), (inp, ), raise_exception=True)\n\n\nclass TestRandomMotionBlur:\n    def test_smoke(self, device):\n        f = RandomMotionBlur(kernel_size=(3, 5), angle=(10, 30), direction=0.5)\n        repr = ""RandomMotionBlur(kernel_size=(3, 5), angle=(10, 30), direction=0.5, ""\\\n            ""border_type=\'constant\', return_transform=False)""\n        assert str(f) == repr\n\n    def test_gradcheck(self, device):\n        torch.manual_seed(0)  # for random reproductibility\n        inp = torch.rand((1, 3, 11, 7)).to(device)\n        inp = utils.tensor_to_gradcheck_var(inp)  # to var\n        # TODO: Gradcheck for param random gen failed. Suspect get_motion_kernel2d issue.\n        params = {\n            \'ksize_factor\': torch.tensor(31),\n            \'angle_factor\': torch.tensor(30.),\n            \'direction_factor\': torch.tensor(-0.5),\n            \'border_type\': torch.tensor([0]),\n        }\n        assert gradcheck(RandomMotionBlur(\n            kernel_size=3, angle=(10, 30), direction=(-0.5, 0.5)), (inp, params), raise_exception=True)\n'"
test/augmentation/test_functional.py,87,"b'import pytest\nimport torch\nimport torch.nn as nn\n\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\nimport kornia\nimport kornia.testing as utils  # test utils\nimport kornia.augmentation.functional as F\nfrom kornia.constants import pi\nfrom kornia.augmentation import ColorJitter\n\n\nclass TestHorizontalFlipFn:\n\n    def test_random_hflip(self):\n        flip_param_0 = {\'batch_prob\': torch.tensor(False)}\n        flip_param_1 = {\'batch_prob\': torch.tensor(True)}\n\n        input = torch.tensor([[0., 0., 0., 0.],\n                              [0., 0., 0., 0.],\n                              [0., 0., 1., 2.]])  # 3 x 4\n\n        expected = torch.tensor([[0., 0., 0., 0.],\n                                 [0., 0., 0., 0.],\n                                 [2., 1., 0., 0.]])  # 3 x 4\n\n        assert (F.apply_hflip(input, params=flip_param_0) == input).all()\n        assert (F.apply_hflip(input, params=flip_param_1) == expected).all()\n\n    def test_batch_random_hflip(self):\n        batch_size = 5\n        flip_param_0 = {\'batch_prob\': torch.tensor([False] * 5)}\n        flip_param_1 = {\'batch_prob\': torch.tensor([True] * 5)}\n\n        input = torch.tensor([[[[0., 0., 0.],\n                                [0., 0., 0.],\n                                [0., 1., 1.]]]])  # 1 x 1 x 3 x 3\n\n        expected = torch.tensor([[[[0., 0., 0.],\n                                   [0., 0., 0.],\n                                   [1., 1., 0.]]]])  # 1 x 1 x 3 x 3\n\n        input = input.repeat(batch_size, 3, 1, 1)  # 5 x 3 x 3 x 3\n        expected = expected.repeat(batch_size, 3, 1, 1)  # 5 x 3 x 3 x 3\n\n        assert (F.apply_hflip(input, params=flip_param_0) == input).all()\n        assert (F.apply_hflip(input, params=flip_param_1) == expected).all()\n\n\nclass TestVerticalFlipFn:\n\n    def test_random_vflip(self, device):\n\n        flip_param_0 = {\'batch_prob\': torch.tensor(False)}\n        flip_param_1 = {\'batch_prob\': torch.tensor(True)}\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]])  # 3 x 3\n        input.to(device)\n\n        expected = torch.tensor([[0., 1., 1.],\n                                 [0., 0., 0.],\n                                 [0., 0., 0.]])  # 3 x 3\n\n        assert (F.apply_vflip(input, params=flip_param_0) == input).all()\n        assert (F.apply_vflip(input, params=flip_param_1) == expected).all()\n\n    def test_batch_random_vflip(self, device):\n        batch_size = 5\n        flip_param_0 = {\'batch_prob\': torch.tensor([False] * 5)}\n        flip_param_1 = {\'batch_prob\': torch.tensor([True] * 5)}\n\n        input = torch.tensor([[[[0., 0., 0.],\n                                [0., 0., 0.],\n                                [0., 1., 1.]]]])  # 1 x 1 x 3 x 3\n        input.to(device)\n\n        expected = torch.tensor([[[[0., 1., 1.],\n                                   [0., 0., 0.],\n                                   [0., 0., 0.]]]])  # 1 x 1 x 3 x 3\n\n        input = input.repeat(batch_size, 3, 1, 1)  # 5 x 3 x 3 x 3\n        expected = expected.repeat(batch_size, 3, 1, 1)  # 5 x 3 x 3 x 3\n\n        assert (F.apply_vflip(input, params=flip_param_0) == input).all()\n        assert (F.apply_vflip(input, params=flip_param_1) == expected).all()\n\n\nclass TestColorJitter:\n\n    def test_color_jitter(self):\n\n        jitter_param = {\n            \'brightness_factor\': torch.tensor(1.),\n            \'contrast_factor\': torch.tensor(1.),\n            \'saturation_factor\': torch.tensor(1.),\n            \'hue_factor\': torch.tensor(0.),\n            \'order\': torch.tensor([2, 3, 0, 1])\n        }\n\n        input = torch.rand(3, 5, 5)  # 3 x 5 x 5\n\n        expected = input\n\n        assert_allclose(F.apply_color_jitter(input, jitter_param), expected, atol=1e-4, rtol=1e-5)\n\n    def test_color_jitter_batch(self):\n        batch_size = 2\n        jitter_param = {\n            \'brightness_factor\': torch.tensor([1.] * batch_size),\n            \'contrast_factor\': torch.tensor([1.] * batch_size),\n            \'saturation_factor\': torch.tensor([1.] * batch_size),\n            \'hue_factor\': torch.tensor([0.] * batch_size),\n            \'order\': torch.tensor([2, 3, 0, 1])\n        }\n\n        input = torch.rand(batch_size, 3, 5, 5)  # 2 x 3 x 5 x 5\n        expected = input\n\n        assert_allclose(F.apply_color_jitter(input, jitter_param), expected, atol=1e-4, rtol=1e-5)\n\n    def test_random_brightness(self):\n        torch.manual_seed(42)\n        jitter_param = {\n            \'brightness_factor\': torch.tensor([1.1529, 1.1660]),\n            \'contrast_factor\': torch.tensor([1., 1.]),\n            \'hue_factor\': torch.tensor([0., 0.]),\n            \'saturation_factor\': torch.tensor([1., 1.]),\n            \'order\': torch.tensor([2, 3, 0, 1])\n        }\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 3, 1, 1)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[[0.2529, 0.3529, 0.4529],\n                                   [0.7529, 0.6529, 0.5529],\n                                   [0.8529, 0.9529, 1.0000]],\n\n                                  [[0.2529, 0.3529, 0.4529],\n                                   [0.7529, 0.6529, 0.5529],\n                                   [0.8529, 0.9529, 1.0000]],\n\n                                  [[0.2529, 0.3529, 0.4529],\n                                   [0.7529, 0.6529, 0.5529],\n                                   [0.8529, 0.9529, 1.0000]]],\n\n\n                                 [[[0.2660, 0.3660, 0.4660],\n                                   [0.7660, 0.6660, 0.5660],\n                                   [0.8660, 0.9660, 1.0000]],\n\n                                  [[0.2660, 0.3660, 0.4660],\n                                   [0.7660, 0.6660, 0.5660],\n                                   [0.8660, 0.9660, 1.0000]],\n\n                                  [[0.2660, 0.3660, 0.4660],\n                                   [0.7660, 0.6660, 0.5660],\n                                   [0.8660, 0.9660, 1.0000]]]])  # 1 x 1 x 3 x 3\n\n        assert_allclose(F.apply_color_jitter(input, jitter_param), expected)\n\n    def test_random_contrast(self):\n        torch.manual_seed(42)\n        jitter_param = {\n            \'brightness_factor\': torch.tensor([1., 1.]),\n            \'contrast_factor\': torch.tensor([0.9531, 1.1837]),\n            \'hue_factor\': torch.tensor([0., 0.]),\n            \'saturation_factor\': torch.tensor([1., 1.]),\n            \'order\': torch.tensor([2, 3, 0, 1])\n        }\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 3, 1, 1)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[[0.0953, 0.1906, 0.2859],\n                                   [0.5719, 0.4766, 0.3813],\n                                   [0.6672, 0.7625, 0.9531]],\n\n                                  [[0.0953, 0.1906, 0.2859],\n                                   [0.5719, 0.4766, 0.3813],\n                                   [0.6672, 0.7625, 0.9531]],\n\n                                  [[0.0953, 0.1906, 0.2859],\n                                   [0.5719, 0.4766, 0.3813],\n                                   [0.6672, 0.7625, 0.9531]]],\n\n\n                                 [[[0.1184, 0.2367, 0.3551],\n                                   [0.7102, 0.5919, 0.4735],\n                                   [0.8286, 0.9470, 1.0000]],\n\n                                  [[0.1184, 0.2367, 0.3551],\n                                   [0.7102, 0.5919, 0.4735],\n                                   [0.8286, 0.9470, 1.0000]],\n\n                                  [[0.1184, 0.2367, 0.3551],\n                                   [0.7102, 0.5919, 0.4735],\n                                   [0.8286, 0.9470, 1.0000]]]])\n\n        assert_allclose(F.apply_color_jitter(input, jitter_param), expected, atol=1e-4, rtol=1e-5)\n\n    def test_random_saturation(self):\n        torch.manual_seed(42)\n        jitter_param = {\n            \'brightness_factor\': torch.tensor([1., 1.]),\n            \'contrast_factor\': torch.tensor([1., 1.]),\n            \'hue_factor\': torch.tensor([0., 0.]),\n            \'saturation_factor\': torch.tensor([0.9026, 1.1175]),\n            \'order\': torch.tensor([2, 3, 0, 1])\n        }\n\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]],\n\n                               [[1.0, 0.5, 0.6],\n                                [0.6, 0.3, 0.2],\n                                [0.8, 0.1, 0.2]],\n\n                               [[0.6, 0.8, 0.7],\n                                [0.9, 0.3, 0.2],\n                                [0.8, 0.4, .5]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 1, 1, 1)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[[1.8763e-01, 2.5842e-01, 3.3895e-01],\n                                   [6.2921e-01, 5.0000e-01, 4.0000e-01],\n                                   [7.0974e-01, 8.0000e-01, 1.0000e+00]],\n\n                                  [[1.0000e+00, 5.2921e-01, 6.0974e-01],\n                                   [6.2921e-01, 3.1947e-01, 2.1947e-01],\n                                   [8.0000e-01, 1.6816e-01, 2.7790e-01]],\n\n                                  [[6.3895e-01, 8.0000e-01, 7.0000e-01],\n                                   [9.0000e-01, 3.1947e-01, 2.1947e-01],\n                                   [8.0000e-01, 4.3895e-01, 5.4869e-01]]],\n\n\n                                 [[[1.1921e-07, 1.2953e-01, 2.5302e-01],\n                                   [5.6476e-01, 5.0000e-01, 4.0000e-01],\n                                   [6.8825e-01, 8.0000e-01, 1.0000e+00]],\n\n                                  [[1.0000e+00, 4.6476e-01, 5.8825e-01],\n                                   [5.6476e-01, 2.7651e-01, 1.7651e-01],\n                                   [8.0000e-01, 1.7781e-02, 1.0603e-01]],\n\n                                  [[5.5556e-01, 8.0000e-01, 7.0000e-01],\n                                   [9.0000e-01, 2.7651e-01, 1.7651e-01],\n                                   [8.0000e-01, 3.5302e-01, 4.4127e-01]]]])\n\n        assert_allclose(F.apply_color_jitter(input, jitter_param), expected, atol=1e-4, rtol=1e-5)\n\n    def test_random_hue(self):\n        torch.manual_seed(42)\n        jitter_param = {\n            \'brightness_factor\': torch.tensor([1., 1.]),\n            \'contrast_factor\': torch.tensor([1., 1.]),\n            \'hue_factor\': torch.tensor([-0.0438 / 2 / pi, 0.0404 / 2 / pi]),\n            \'saturation_factor\': torch.tensor([1., 1.]),\n            \'order\': torch.tensor([2, 3, 0, 1])\n        }\n        input = torch.tensor([[[[0.1, 0.2, 0.3],\n                                [0.6, 0.5, 0.4],\n                                [0.7, 0.8, 1.]],\n\n                               [[1.0, 0.5, 0.6],\n                                [0.6, 0.3, 0.2],\n                                [0.8, 0.1, 0.2]],\n\n                               [[0.6, 0.8, 0.7],\n                                [0.9, 0.3, 0.2],\n                                [0.8, 0.4, .5]]]])  # 1 x 1 x 3 x 3\n        input = input.repeat(2, 1, 1, 1)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[[0.1000, 0.2000, 0.3000],\n                                   [0.6000, 0.5000, 0.4000],\n                                   [0.7000, 0.8000, 1.0000]],\n\n                                  [[1.0000, 0.5251, 0.6167],\n                                   [0.6126, 0.3000, 0.2000],\n                                   [0.8000, 0.1000, 0.2000]],\n\n                                  [[0.5623, 0.8000, 0.7000],\n                                   [0.9000, 0.3084, 0.2084],\n                                   [0.7958, 0.4293, 0.5335]]],\n\n                                 [[[0.1000, 0.2000, 0.3000],\n                                   [0.6116, 0.5000, 0.4000],\n                                   [0.7000, 0.8000, 1.0000]],\n\n                                  [[1.0000, 0.4769, 0.5846],\n                                   [0.6000, 0.3077, 0.2077],\n                                   [0.7961, 0.1000, 0.2000]],\n\n                                  [[0.6347, 0.8000, 0.7000],\n                                   [0.9000, 0.3000, 0.2000],\n                                   [0.8000, 0.3730, 0.4692]]]])\n\n        assert_allclose(F.apply_color_jitter(input, jitter_param), expected, atol=1e-4, rtol=1e-5)\n\n\nclass TestRandomGrayscale:\n\n    def test_opencv_true(self, device):\n        grayscale_params = {\'batch_prob\': torch.tensor([True])}\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device)\n\n        expected = torch.tensor([[[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]],\n\n                                 [[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]],\n\n                                 [[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]]])\n        expected = expected.to(device)\n\n        assert_allclose(F.apply_grayscale(data, grayscale_params), expected)\n\n    def test_opencv_false(self, device):\n        grayscale_params = {\'batch_prob\': torch.tensor([False])}\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device)\n\n        expected = data\n\n        assert_allclose(F.apply_grayscale(data, grayscale_params), expected)\n\n    def test_opencv_true_batch(self, device):\n        batch_size = 4\n        grayscale_params = {\'batch_prob\': torch.tensor([True] * batch_size)}\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device)\n        data = data.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n\n        # Output data generated with OpenCV 4.1.1: cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n        expected = torch.tensor([[[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]],\n\n                                 [[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]],\n\n                                 [[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                  [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                  [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                  [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                  [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]]])\n        expected = expected.to(device)\n        expected = expected.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n\n        assert_allclose(F.apply_grayscale(data, grayscale_params), expected)\n\n    def test_opencv_false_batch(self, device):\n        batch_size = 4\n        grayscale_params = {\'batch_prob\': torch.tensor([False] * batch_size)}\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device)\n        data = data.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n\n        expected = data\n\n        assert_allclose(F.apply_grayscale(data, grayscale_params), expected)\n\n\nclass TestRandomRectangleEarasing:\n\n    def test_rectangle_erasing1(self, device):\n        inputs = torch.ones(1, 1, 10, 10).to(device)\n        rect_params = {\n            ""widths"": torch.tensor([5]),\n            ""heights"": torch.tensor([5]),\n            ""xs"": torch.tensor([5]),\n            ""ys"": torch.tensor([5]),\n            ""values"": torch.tensor([0.])\n        }\n        expected = torch.tensor([[[\n            [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n            [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n            [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n            [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n            [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n            [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n            [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n            [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n            [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n            [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]\n        ]]]).to(device)\n        assert_allclose(F.apply_erase_rectangles(inputs, rect_params), expected)\n\n    def test_rectangle_erasing2(self, device):\n        inputs = torch.ones(3, 3, 3, 3).to(device)\n        rect_params = {\n            ""widths"": torch.tensor([3, 2, 1]),\n            ""heights"": torch.tensor([3, 2, 1]),\n            ""xs"": torch.tensor([0, 1, 2]),\n            ""ys"": torch.tensor([0, 1, 2]),\n            ""values"": torch.tensor([0., 0., 0.])\n        }\n        expected = torch.tensor(\n            [[[[0., 0., 0.],\n               [0., 0., 0.],\n                [0., 0., 0.]],\n\n                [[0., 0., 0.],\n                 [0., 0., 0.],\n                 [0., 0., 0.]],\n\n                [[0., 0., 0.],\n                 [0., 0., 0.],\n                 [0., 0., 0.]]],\n\n                [[[1., 1., 1.],\n                  [1., 0., 0.],\n                    [1., 0., 0.]],\n\n                 [[1., 1., 1.],\n                  [1., 0., 0.],\n                    [1., 0., 0.]],\n\n                 [[1., 1., 1.],\n                  [1., 0., 0.],\n                    [1., 0., 0.]]],\n\n                [[[1., 1., 1.],\n                  [1., 1., 1.],\n                    [1., 1., 0.]],\n\n                 [[1., 1., 1.],\n                  [1., 1., 1.],\n                    [1., 1., 0.]],\n\n                 [[1., 1., 1.],\n                  [1., 1., 1.],\n                    [1., 1., 0.]]]]\n        ).to(device)\n\n        assert_allclose(F.apply_erase_rectangles(inputs, rect_params), expected)\n'"
test/augmentation/test_perspective.py,27,"b'import pytest\n\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\nimport kornia\nimport kornia.augmentation.functional as F\nimport kornia.testing as utils  # test utils\n\n\nclass TestPerspective:\n\n    def test_smoke(self, device):\n        x_data = torch.rand(1, 2, 3, 4).to(device)\n        batch_prob = torch.rand(1) < 0.5\n        start_points = torch.rand(1, 4, 2).to(device)\n        end_points = torch.rand(1, 4, 2).to(device)\n\n        params = dict(\n            batch_prob=batch_prob, start_points=start_points,\n            end_points=end_points, interpolation=torch.tensor(1),\n            align_corners=torch.tensor(False))\n        out_data = F.apply_perspective(x_data, params)\n\n        assert out_data.shape == x_data.shape\n\n    def test_gradcheck(self, device):\n        input = torch.rand(1, 2, 3, 4).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n\n        batch_prob = torch.rand(1) < 0.5\n\n        start_points = torch.rand(1, 4, 2).to(device)\n        start_points = utils.tensor_to_gradcheck_var(start_points)  # to var\n\n        end_points = torch.rand(1, 4, 2).to(device)\n        end_points = utils.tensor_to_gradcheck_var(end_points)  # to var\n\n        params = dict(\n            batch_prob=batch_prob,\n            start_points=start_points,\n            end_points=end_points,\n            interpolation=torch.tensor(1),\n            align_corners=torch.tensor(False)\n        )\n        assert gradcheck(F.apply_perspective, (input, params,), raise_exception=True)\n\n\nclass TestRandomPerspective:\n\n    torch.manual_seed(0)  # for random reproductibility\n\n    def test_smoke_no_transform(self, device):\n        x_data = torch.rand(1, 2, 8, 9).to(device)\n\n        out_perspective = kornia.augmentation.functional.random_perspective(\n            x_data, 0.5, 0.5, return_transform=False)\n\n        assert out_perspective.shape == x_data.shape\n\n    def test_smoke_no_transform_batch(self, device):\n        x_data = torch.rand(2, 2, 8, 9).to(device)\n\n        out_perspective = kornia.augmentation.functional.random_perspective(\n            x_data, 0.5, 0.5, return_transform=False)\n\n        assert out_perspective.shape == x_data.shape\n\n    def test_smoke_transform(self, device):\n        x_data = torch.rand(1, 2, 4, 5).to(device)\n\n        out_perspective = kornia.augmentation.functional.random_perspective(\n            x_data, 0.5, 0.5, return_transform=True)\n\n        assert isinstance(out_perspective, tuple)\n        assert len(out_perspective) == 2\n        assert out_perspective[0].shape == x_data.shape\n        assert out_perspective[1].shape == (1, 3, 3)\n\n    def test_no_transform_module(self, device):\n        x_data = torch.rand(1, 2, 8, 9).to(device)\n        out_perspective = kornia.augmentation.RandomPerspective()(x_data)\n        assert out_perspective.shape == x_data.shape\n\n    def test_transform_module(self, device):\n        x_data = torch.rand(1, 2, 4, 5).to(device)\n\n        out_perspective = kornia.augmentation.RandomPerspective(\n            return_transform=True)(x_data)\n\n        assert isinstance(out_perspective, tuple)\n        assert len(out_perspective) == 2\n        assert out_perspective[0].shape == x_data.shape\n        assert out_perspective[1].shape == (1, 3, 3)\n\n    def test_gradcheck(self, device):\n        input = torch.rand(1, 2, 5, 7).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(F.random_perspective, (input, 0., 1.), raise_exception=True)\n\n\nclass TestRandomAffine:\n\n    torch.manual_seed(0)  # for random reproductibility\n\n    def test_smoke_no_transform(self, device):\n        x_data = torch.rand(1, 2, 8, 9).to(device)\n        out = F.random_affine(x_data, 0.)\n        assert out.shape == x_data.shape\n\n    def test_smoke_no_transform_batch(self, device):\n        x_data = torch.rand(2, 2, 8, 9).to(device)\n        out = F.random_affine(x_data, 0.)\n        assert out.shape == x_data.shape\n\n    def test_batch_multi_params(self, device):\n        x_data = torch.rand(2, 2, 8, 9).to(device)\n        out = F.random_affine(x_data, 0., (0., 0.))\n        assert out.shape == x_data.shape\n\n    def test_smoke_transform(self, device):\n        x_data = torch.rand(1, 2, 4, 5).to(device)\n        out = F.random_affine(x_data, 0., return_transform=True)\n\n        assert isinstance(out, tuple)\n        assert len(out) == 2\n        assert out[0].shape == x_data.shape\n        assert out[1].shape == (1, 3, 3)\n\n    def test_gradcheck(self, device):\n        input = torch.rand(1, 2, 5, 7).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.augmentation.RandomAffine(0.), (input, ), raise_exception=True)\n'"
test/augmentation/test_random_generator.py,26,"b'import torch\n\nfrom kornia.augmentation.random_generator import random_prob_generator, random_color_jitter_generator\n\n\nclass TestRandomProbGen:\n\n    def test_random_prob_gen(self):\n        torch.manual_seed(42)\n        batch_size = 8\n\n        halfs = random_prob_generator(batch_size=batch_size, p=.5)\n        expected_halfs = [False, False, True, False, True, False, True, False]\n        zeros = random_prob_generator(batch_size=batch_size, p=0.)[\'batch_prob\']\n        ones = random_prob_generator(batch_size=batch_size, p=1.)[\'batch_prob\']\n\n        assert list(halfs.keys()) == [\'batch_prob\'], ""Redundant keys found apart from `batch_prob`""\n        assert (halfs[\'batch_prob\'] == torch.tensor(expected_halfs)).long().sum() == batch_size\n        assert (zeros == torch.tensor([False] * batch_size)).long().sum() == batch_size\n        assert (ones == torch.tensor([True] * batch_size)).long().sum() == batch_size\n\n    def test_random_prob_gen_same_on_batch(self):\n        batch_size = 8\n\n        torch.manual_seed(42)\n        falses = random_prob_generator(batch_size=batch_size, p=.5, same_on_batch=True)[\'batch_prob\']\n        assert (falses == torch.tensor([False] * batch_size)).long().sum() == batch_size\n\n        torch.manual_seed(0)\n        trues = random_prob_generator(batch_size=batch_size, p=.5, same_on_batch=True)[\'batch_prob\']\n        assert (trues == torch.tensor([True] * batch_size)).long().sum() == batch_size\n\n\nclass TestColorJitterGen:\n\n    def test_color_jitter_gen(self):\n        torch.manual_seed(42)\n        batch_size = 8\n        jitter_params = random_color_jitter_generator(batch_size, brightness=0.2, contrast=0.3, saturation=0.4, hue=0.1)\n        expected_jitter_params = {\n            \'brightness_factor\': torch.tensor([\n                1.15290772914886474609375, 1.16600155830383300781250, 0.95314550399780273437500,\n                1.18372225761413574218750, 0.956179320812225341796875, 1.04035818576812744140625,\n                0.90262901782989501953125, 1.11745655536651611328125]),\n            \'contrast_factor\': torch.tensor([\n                1.2644628286361694, 0.7799115180969238, 1.260758876800537, 1.056147813796997,\n                1.2216426134109497, 1.0406291484832764, 1.1446564197540283, 0.957642674446106]),\n            \'hue_factor\': torch.tensor([\n                0.07708858698606491, 0.014780893921852112, -0.04668399319052696, 0.02548982948064804,\n                -0.04607366397976875, -0.011727288365364075, -0.040615834295749664, 0.06633710116147995]),\n            \'saturation_factor\': torch.tensor([\n                0.6842519640922546, 0.8155958652496338, 0.8870501518249512, 0.7594910264015198,\n                1.0377532243728638, 0.6049283742904663, 1.3612436056137085, 0.6602127552032471]),\n            \'order\': torch.tensor([3, 2, 0, 1])\n        }\n\n        assert set(list(jitter_params.keys())) == set([\n            \'brightness_factor\', \'contrast_factor\', \'hue_factor\', \'saturation_factor\', \'order\']), \\\n            ""Redundant keys found apart from \\\n                \'brightness_factor\', \'contrast_factor\', \'hue_factor\', \'saturation_factor\', \'order""\n        assert (jitter_params[\'brightness_factor\'] == expected_jitter_params[\'brightness_factor\']) \\\n            .long().sum() == batch_size\n        assert (jitter_params[\'contrast_factor\'] == expected_jitter_params[\'contrast_factor\']) \\\n            .long().sum() == batch_size\n        assert (jitter_params[\'hue_factor\'] == expected_jitter_params[\'hue_factor\']) \\\n            .long().sum() == batch_size\n        assert (jitter_params[\'saturation_factor\'] == expected_jitter_params[\'saturation_factor\']) \\\n            .long().sum() == batch_size\n        assert (jitter_params[\'order\'] == expected_jitter_params[\'order\']).long().sum() == 4\n\n    def test_color_jitter_tuple_gen(self):\n        torch.manual_seed(42)\n        batch_size = 8\n        jitter_params_tuple = random_color_jitter_generator(\n            batch_size, brightness=(0.8, 1.2), contrast=(0.7, 1.3), saturation=(0.6, 1.4), hue=(-0.1, 0.1))\n\n        expected_jitter_params_tuple = {\n            \'brightness_factor\': torch.tensor([\n                1.15290772914886474609375, 1.16600155830383300781250, 0.95314550399780273437500,\n                1.18372225761413574218750, 0.956179320812225341796875, 1.04035818576812744140625,\n                0.90262901782989501953125, 1.11745655536651611328125]),\n            \'contrast_factor\': torch.tensor([\n                1.2644628286361694, 0.7799115180969238, 1.260758876800537, 1.056147813796997,\n                1.2216426134109497, 1.0406291484832764, 1.1446564197540283, 0.957642674446106]),\n            \'hue_factor\': torch.tensor([\n                0.07708858698606491, 0.014780893921852112, -0.04668399319052696, 0.02548982948064804,\n                -0.04607366397976875, -0.011727288365364075, -0.040615834295749664, 0.06633710116147995]),\n            \'saturation_factor\': torch.tensor([\n                0.6842519640922546, 0.8155958652496338, 0.8870501518249512, 0.7594910264015198,\n                1.0377532243728638, 0.6049283742904663, 1.3612436056137085, 0.6602127552032471]),\n            \'order\': torch.tensor([3, 2, 0, 1])\n        }\n        assert set(list(expected_jitter_params_tuple.keys())) == set([\n            \'brightness_factor\', \'contrast_factor\', \'hue_factor\', \'saturation_factor\', \'order\']), \\\n            ""Redundant keys found apart from \\\n                \'brightness_factor\', \'contrast_factor\', \'hue_factor\', \'saturation_factor\', \'order""\n        assert (jitter_params_tuple[\'brightness_factor\'] == expected_jitter_params_tuple[\'brightness_factor\']) \\\n            .long().sum() == batch_size\n        assert (jitter_params_tuple[\'contrast_factor\'] == expected_jitter_params_tuple[\'contrast_factor\']) \\\n            .long().sum() == batch_size\n        assert (jitter_params_tuple[\'hue_factor\'] == expected_jitter_params_tuple[\'hue_factor\']) \\\n            .long().sum() == batch_size\n        assert (jitter_params_tuple[\'saturation_factor\'] == expected_jitter_params_tuple[\'saturation_factor\']) \\\n            .long().sum() == batch_size\n        assert (jitter_params_tuple[\'order\'] == expected_jitter_params_tuple[\'order\']).long().sum() == 4\n\n    def test_random_prob_gen_same_on_batch(self):\n        torch.manual_seed(42)\n        batch_size = 8\n        jitter_params = random_color_jitter_generator(\n            batch_size, brightness=0.2, contrast=0.3, saturation=0.4, hue=0.1, same_on_batch=True)\n\n        expected_res = {\n            \'brightness_factor\': torch.tensor([1.15290772914886474609375] * batch_size),\n            \'contrast_factor\': torch.tensor([1.24900233745574951171875] * batch_size),\n            \'hue_factor\': torch.tensor([-0.0234272480010986328125] * batch_size),\n            \'saturation_factor\': torch.tensor([1.367444515228271484375] * batch_size),\n            \'order\': torch.tensor([2, 3, 0, 1])\n        }\n        assert (jitter_params[\'brightness_factor\'] == expected_res[\'brightness_factor\']).long().sum() == batch_size\n        assert (jitter_params[\'contrast_factor\'] == expected_res[\'contrast_factor\']).long().sum() == batch_size\n        assert (jitter_params[\'saturation_factor\'] == expected_res[\'saturation_factor\']).long().sum() == batch_size\n        assert (jitter_params[\'hue_factor\'] == expected_res[\'hue_factor\']).long().sum() == batch_size\n        assert (jitter_params[\'order\'] == expected_res[\'order\']).long().sum() == 4\n'"
test/augmentation/test_transformation_matrix.py,31,"b""import pytest\nimport torch\nimport torch.nn as nn\n\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\nimport kornia\nimport kornia.testing as utils  # test utils\nimport kornia.augmentation.functional as F\nfrom kornia.constants import pi\nfrom kornia.augmentation import ColorJitter\n\n\nclass TestHorizontalFlipFn:\n\n    def test_random_hflip(self, device):\n        flip_param_0 = {'batch_prob': torch.tensor(False)}\n        flip_param_1 = {'batch_prob': torch.tensor(True)}\n\n        input = torch.tensor([[0., 0., 0., 0.],\n                              [0., 0., 0., 0.],\n                              [0., 0., 1., 2.]])  # 3 x 4\n        input.to(device)\n\n        expected_transform = torch.tensor([[-1., 0., 4.],\n                                           [0., 1., 0.],\n                                           [0., 0., 1.]])  # 3 x 3\n\n        identity = torch.tensor([[1., 0., 0.],\n                                 [0., 1., 0.],\n                                 [0., 0., 1.]])  # 3 x 3\n\n        assert (F.compute_hflip_transformation(input, params=flip_param_0) == identity).all()\n        assert (F.compute_hflip_transformation(input, params=flip_param_1) == expected_transform).all()\n\n    def test_batch_random_hflip(self, device):\n        batch_size = 5\n        flip_param_0 = {'batch_prob': torch.tensor([False] * 5)}\n        flip_param_1 = {'batch_prob': torch.tensor([True] * 5)}\n\n        input = torch.tensor([[[[0., 0., 0.],\n                                [0., 0., 0.],\n                                [0., 1., 1.]]]])  # 1 x 1 x 3 x 3\n        input.to(device)\n\n        expected_transform = torch.tensor([[[-1., 0., 3.],\n                                            [0., 1., 0.],\n                                            [0., 0., 1.]]])  # 1 x 3 x 3\n\n        identity = torch.tensor([[[1., 0., 0.],\n                                  [0., 1., 0.],\n                                  [0., 0., 1.]]])  # 1 x 3 x 3\n\n        input = input.repeat(batch_size, 3, 1, 1)  # 5 x 3 x 3 x 3\n        expected_transform = expected_transform.repeat(batch_size, 1, 1)  # 5 x 3 x 3\n        identity = identity.repeat(batch_size, 1, 1)  # 5 x 3 x 3\n\n        assert (F.compute_hflip_transformation(input, params=flip_param_0) == identity).all()\n        assert (F.compute_hflip_transformation(input, params=flip_param_1) == expected_transform).all()\n\n\nclass TestVerticalFlipFn:\n\n    def test_random_vflip(self, device):\n\n        flip_param_0 = {'batch_prob': torch.tensor(False)}\n        flip_param_1 = {'batch_prob': torch.tensor(True)}\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]])  # 3 x 3\n        input.to(device)\n\n        expected_transform = torch.tensor([[1., 0., 0.],\n                                           [0., -1., 3.],\n                                           [0., 0., 1.]])  # 3 x 3\n\n        identity = torch.tensor([[1., 0., 0.],\n                                 [0., 1., 0.],\n                                 [0., 0., 1.]])  # 3 x 3\n\n        assert (F.compute_vflip_transformation(input, params=flip_param_0) == identity).all()\n        assert (F.compute_vflip_transformation(input, params=flip_param_1) == expected_transform).all()\n\n    def test_batch_random_vflip(self, device):\n        batch_size = 5\n        flip_param_0 = {'batch_prob': torch.tensor([False] * 5)}\n        flip_param_1 = {'batch_prob': torch.tensor([True] * 5)}\n\n        input = torch.tensor([[[[0., 0., 0.],\n                                [0., 0., 0.],\n                                [0., 1., 1.]]]])  # 1 x 1 x 3 x 3\n        input.to(device)\n\n        expected_transform = torch.tensor([[[1., 0., 0.],\n                                            [0., -1., 3.],\n                                            [0., 0., 1.]]])  # 1 x 3 x 3\n\n        identity = torch.tensor([[[1., 0., 0.],\n                                  [0., 1., 0.],\n                                  [0., 0., 1.]]])  # 1 x 3 x 3\n\n        input = input.repeat(batch_size, 3, 1, 1)  # 5 x 3 x 3 x 3\n        expected_transform = expected_transform.repeat(batch_size, 1, 1)  # 5 x 3 x 3\n        identity = identity.repeat(batch_size, 1, 1)  # 5 x 3 x 3\n\n        assert (F.compute_vflip_transformation(input, params=flip_param_0) == identity).all()\n        assert (F.compute_vflip_transformation(input, params=flip_param_1) == expected_transform).all()\n\n\nclass TestIntensityTransformation:\n\n    def test_intensity_transformation(self):\n\n        input = torch.rand(3, 5, 5)  # 3 x 5 x 5\n\n        expected_transform = torch.eye(3).unsqueeze(0)  # 3 x 3\n\n        assert_allclose(F.compute_intensity_transformation(input, {}), expected_transform, atol=1e-4, rtol=1e-5)\n\n    def test_intensity_transformation_batch(self):\n        batch_size = 2\n\n        input = torch.rand(batch_size, 3, 5, 5)  # 2 x 3 x 5 x 5\n\n        expected_transform = torch.eye(3).unsqueeze(0).expand((batch_size, 3, 3))  # 2 x 3 x 3\n\n        assert_allclose(F.compute_intensity_transformation(input, {}), expected_transform, atol=1e-4, rtol=1e-5)\n\n\nclass TestPerspective:\n\n    def test_smoke_transform(self, device):\n        x_data = torch.rand(1, 2, 3, 4).to(device)\n        batch_prob = torch.rand(1) < 0.5\n        start_points = torch.rand(1, 4, 2).to(device)\n        end_points = torch.rand(1, 4, 2).to(device)\n\n        params = dict(batch_prob=batch_prob, start_points=start_points, end_points=end_points)\n        out_data = F.compute_perspective_transformation(x_data, params)\n\n        assert out_data.shape == (1, 3, 3)\n"""
test/color/test_adjust.py,50,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\nfrom kornia.constants import pi\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestAdjustSaturation:\n    def test_saturation_one(self, device):\n        data = torch.tensor([[[.5, .5],\n                              [.5, .5]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n\n        data = data.to(device)\n        expected = data.clone()\n\n        f = kornia.color.AdjustSaturation(1.)\n        assert_allclose(f(data), expected)\n\n    def test_saturation_one_batch(self):\n        data = torch.tensor([[[[.5, .5],\n                               [.5, .5]],\n\n                              [[.5, .5],\n                               [.5, .5]],\n\n                              [[.25, .25],\n                               [.25, .25]]],\n\n                             [[[.5, .5],\n                               [.5, .5]],\n\n                              [[.5, .5],\n                               [.5, .5]],\n\n                              [[.25, .25],\n                               [.25, .25]]]])  # 2x3x2x2\n\n        expected = data\n        f = kornia.color.AdjustSaturation(torch.ones(2))\n        assert_allclose(f(data), expected)\n\n    def test_gradcheck(self):\n        batch_size, channels, height, width = 2, 3, 4, 5\n        img = torch.rand(batch_size, channels, height, width)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.adjust_saturation, (img, 2.),\n                         raise_exception=True)\n\n\nclass TestAdjustHue:\n    def test_hue_one(self, device):\n        data = torch.tensor([[[.5, .5],\n                              [.5, .5]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n\n        data = data.to(device)\n        expected = data.clone()\n\n        f = kornia.color.AdjustHue(0.)\n        assert_allclose(f(data), expected)\n\n    def test_hue_one_batch(self):\n        data = torch.tensor([[[[.5, .5],\n                               [.5, .5]],\n\n                              [[.5, .5],\n                               [.5, .5]],\n\n                              [[.25, .25],\n                               [.25, .25]]],\n\n                             [[[.5, .5],\n                               [.5, .5]],\n\n                              [[.5, .5],\n                               [.5, .5]],\n\n                              [[.25, .25],\n                               [.25, .25]]]])  # 2x3x2x2\n\n        expected = data\n        f = kornia.color.AdjustHue(torch.tensor([0, 0]))\n        assert_allclose(f(data), expected)\n\n    def test_hue_flip_batch(self):\n        data = torch.tensor([[[[.5, .5],\n                               [.5, .5]],\n\n                              [[.5, .5],\n                               [.5, .5]],\n\n                              [[.25, .25],\n                               [.25, .25]]],\n\n                             [[[.5, .5],\n                               [.5, .5]],\n\n                              [[.5, .5],\n                               [.5, .5]],\n\n                              [[.25, .25],\n                               [.25, .25]]]])  # 2x3x2x2\n\n        f = kornia.color.AdjustHue(torch.tensor([-pi, pi]))\n        result = f(data)\n        assert_allclose(result, result.flip(0))\n\n    def test_gradcheck(self):\n        batch_size, channels, height, width = 2, 3, 4, 5\n        img = torch.rand(batch_size, channels, height, width)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.adjust_hue, (img, 2.),\n                         raise_exception=True)\n\n\nclass TestAdjustGamma:\n    def test_gamma_zero(self, device):\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n\n        data = data.to(device)\n        expected = torch.ones_like(data)\n\n        f = kornia.color.AdjustGamma(0.)\n        assert_allclose(f(data), expected)\n\n    def test_gamma_one(self, device):\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n\n        data = data.to(device)\n        expected = data.clone()\n\n        f = kornia.color.AdjustGamma(1.)\n        assert_allclose(f(data), expected)\n\n    def test_gamma_one_gain_two(self, device):\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n\n        expected = torch.tensor([[[1., 1.],\n                                  [1., 1.]],\n\n                                 [[1., 1.],\n                                  [1., 1.]],\n\n                                 [[.5, .5],\n                                  [.5, .5]]])  # 3x2x2\n\n        data = data.to(device)\n        expected = expected.to(device)\n\n        f = kornia.color.AdjustGamma(1., 2.)\n        assert_allclose(f(data), expected)\n\n    def test_gamma_two(self, device):\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n\n        expected = torch.tensor([[[1., 1.],\n                                  [1., 1.]],\n\n                                 [[.25, .25],\n                                  [.25, .25]],\n\n                                 [[.0625, .0625],\n                                  [.0625, .0625]]])  # 3x2x2\n\n        data = data.to(device)\n        expected = expected.to(device)\n\n        f = kornia.color.AdjustGamma(2.)\n        assert_allclose(f(data), expected)\n\n    def test_gamma_two_batch(self):\n        data = torch.tensor([[[[1., 1.],\n                               [1., 1.]],\n\n                              [[.5, .5],\n                               [.5, .5]],\n\n                              [[.25, .25],\n                               [.25, .25]]],\n\n                             [[[1., 1.],\n                               [1., 1.]],\n\n                              [[.5, .5],\n                               [.5, .5]],\n\n                              [[.25, .25],\n                               [.25, .25]]]])  # 2x3x2x2\n\n        expected = torch.tensor([[[[1., 1.],\n                                   [1., 1.]],\n\n                                  [[.25, .25],\n                                   [.25, .25]],\n\n                                  [[.0625, .0625],\n                                   [.0625, .0625]]],\n\n                                 [[[1., 1.],\n                                   [1., 1.]],\n\n                                  [[.25, .25],\n                                   [.25, .25]],\n\n                                  [[.0625, .0625],\n                                   [.0625, .0625]]]])  # 2x3x2x2\n\n        f = kornia.color.AdjustGamma(torch.tensor([2., 2.]), gain=torch.ones(2))\n        assert_allclose(f(data), expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 2, 3, 4, 5\n        img = torch.ones(batch_size, channels, height, width)\n        img = img.to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.adjust_gamma, (img, 1., 2.),\n                         raise_exception=True)\n\n\nclass TestAdjustContrast:\n    def test_factor_zero(self, device):\n        # prepare input data\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n\n        data = data.to(device)\n        expected = torch.zeros_like(data)\n\n        f = kornia.color.AdjustContrast(0.)\n\n        assert_allclose(f(data), expected)\n\n    def test_factor_one(self, device):\n        # prepare input data\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n        data = data.to(device)\n        expected = data.clone()\n\n        f = kornia.color.AdjustContrast(1.)\n\n        assert_allclose(f(data), expected)\n\n    def test_factor_two(self, device):\n        # prepare input data\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n\n        expected = torch.tensor([[[1., 1.],\n                                  [1., 1.]],\n\n                                 [[1., 1.],\n                                  [1., 1.]],\n\n                                 [[.5, .5],\n                                  [.5, .5]]])  # 3x2x2\n\n        data = data.to(device)\n        expected = expected.to(device)\n\n        f = kornia.color.AdjustContrast(2.)\n\n        assert_allclose(f(data), expected)\n\n    def test_factor_tensor(self, device):\n        # prepare input data\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]],\n\n                             [[.5, .5],\n                              [.5, .5]]])  # 4x2x2\n\n        expected = torch.tensor([[[0., 0.],\n                                  [0., 0.]],\n\n                                 [[.5, .5],\n                                  [.5, .5]],\n\n                                 [[.375, .375],\n                                  [.375, .375]],\n\n                                 [[1., 1.],\n                                  [1., 1.]]])  # 4x2x2\n\n        factor = torch.tensor([0, 1, 1.5, 2])\n\n        data = data.to(device)\n        expected = expected.to(device)\n        factor = factor.to(device)\n\n        f = kornia.color.AdjustContrast(factor)\n        assert_allclose(f(data), expected)\n\n    def test_factor_tensor_color(self, device):\n        # prepare input data\n        data = torch.tensor([[[[1., 1.],\n                               [1., 1.]],\n\n                              [[.5, .5],\n                               [.5, .5]],\n\n                              [[.25, .25],\n                               [.25, .25]]],\n\n                             [[[0., 0.],\n                               [0., 0.]],\n\n                              [[.3, .3],\n                               [.3, .3]],\n\n                              [[.6, .6],\n                               [.6, .6]]]])  # 2x3x2x2\n\n        expected = torch.tensor([[[[1., 1.],\n                                   [1., 1.]],\n\n                                  [[.5, .5],\n                                   [.5, .5]],\n\n                                  [[.25, .25],\n                                   [.25, .25]]],\n\n                                 [[[0., 0.],\n                                   [0., 0.]],\n\n                                  [[.6, .6],\n                                   [.6, .6]],\n\n                                  [[1., 1.],\n                                   [1., 1.]]]])  # 2x3x2x2\n\n        factor = torch.tensor([1, 2])\n\n        data = data.to(device)\n        expected = expected.to(device)\n        factor = factor.to(device)\n\n        f = kornia.color.AdjustContrast(factor)\n        assert_allclose(f(data), expected)\n\n    def test_factor_tensor_shape(self, device):\n        # prepare input data\n        data = torch.tensor([[[[1., 1., .5],\n                               [1., 1., .5]],\n\n                              [[.5, .5, .25],\n                               [.5, .5, .25]],\n\n                              [[.25, .25, .25],\n                               [.6, .6, .3]]],\n\n                             [[[0., 0., 1.],\n                               [0., 0., .25]],\n\n                              [[.3, .3, .4],\n                               [.3, .3, .4]],\n\n                              [[.6, .6, 0.],\n                               [.3, .2, .1]]]])  # 2x3x2x3\n\n        expected = torch.tensor([[[[1., 1., .75],\n                                   [1., 1., .75]],\n\n                                  [[.75, .75, .375],\n                                   [.75, .75, .375]],\n\n                                  [[.375, .375, .375],\n                                   [.9, .9, .45]]],\n\n                                 [[[0., 0., 1.],\n                                   [0., 0., .5]],\n\n                                  [[.6, .6, .8],\n                                   [.6, .6, .8]],\n\n                                  [[1., 1., 0.],\n                                   [.6, .4, .2]]]])  # 2x3x2x3\n\n        factor = torch.tensor([1.5, 2.])\n\n        data = data.to(device)\n        expected = expected.to(device)\n        factor = factor.to(device)\n\n        f = kornia.color.AdjustContrast(factor)\n        assert_allclose(f(data), expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 2, 3, 4, 5\n        img = torch.ones(batch_size, channels, height, width)\n        img = img.to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.adjust_contrast, (img, 2.),\n                         raise_exception=True)\n\n\nclass TestAdjustBrightness:\n    def test_factor_zero(self, device):\n        # prepare input data\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n\n        data = data.to(device)\n        expected = data.clone()\n\n        f = kornia.color.AdjustBrightness(0.)\n        assert_allclose(f(data), expected)\n\n    def test_factor_one(self, device):\n        # prepare input data\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n\n        data = data.to(device)\n        expected = torch.ones_like(data)\n\n        f = kornia.color.AdjustBrightness(1.)\n        assert_allclose(f(data), expected)\n\n    def test_factor_minus(self, device):\n        # prepare input data\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.75, .75],\n                              [.75, .75]],\n\n                             [[.25, .25],\n                              [.25, .25]]])  # 3x2x2\n\n        expected = torch.tensor([[[.5, .5],\n                                  [.5, .5]],\n\n                                 [[.25, .25],\n                                  [.25, .25]],\n\n                                 [[0., 0.],\n                                  [0., 0.]]])  # 3x2x2\n\n        data = data.to(device)\n        expected = expected.to(device)\n\n        f = kornia.color.AdjustBrightness(-0.5)\n        assert_allclose(f(data), expected)\n\n    def test_factor_tensor(self, device):\n        # prepare input data\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[.5, .5],\n                              [.5, .5]],\n\n                             [[.25, .25],\n                              [.25, .25]],\n\n                             [[.5, .5],\n                              [.5, .5]]])  # 4x2x2\n\n        factor = torch.tensor([0, 0.5, 0.75, 2])\n\n        data = data.to(device)\n        expected = torch.ones_like(data)\n        factor = factor.to(device)\n\n        f = kornia.color.AdjustBrightness(factor)\n        assert_allclose(f(data), expected)\n\n    def test_factor_tensor_color(self, device):\n        # prepare input data\n        data = torch.tensor([[[[1., 1.],\n                               [1., 1.]],\n\n                              [[.5, .5],\n                               [.5, .5]],\n\n                              [[.25, .25],\n                               [.25, .25]]],\n\n                             [[[0., 0.],\n                               [0., 0.]],\n\n                              [[.3, .3],\n                               [.3, .3]],\n\n                              [[.6, .6],\n                               [.6, .6]]]])  # 2x3x2x2\n\n        expected = torch.tensor([[[[1., 1.],\n                                   [1., 1.]],\n\n                                  [[.75, .75],\n                                   [.75, .75]],\n\n                                  [[.5, .5],\n                                   [.5, .5]]],\n\n                                 [[[.1, .1],\n                                   [.1, .1]],\n\n                                  [[.4, .4],\n                                   [.4, .4]],\n\n                                  [[.7, .7],\n                                   [.7, .7]]]])  # 2x3x2x2\n\n        factor = torch.tensor([0.25, 0.1])\n\n        data = data.to(device)\n        expected = expected.to(device)\n        factor = factor.to(device)\n\n        f = kornia.color.AdjustBrightness(factor)\n        assert_allclose(f(data), expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 2, 3, 4, 5\n        img = torch.ones(batch_size, channels, height, width)\n        img = img.to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.adjust_brightness, (img, 2.),\n                         raise_exception=True)\n'"
test/color/test_core.py,9,"b'import pytest\nimport random\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\ndef random_shape(dim, min_elem=1, max_elem=10):\n    return tuple(random.randint(min_elem, max_elem) for _ in range(dim))\n\n\nclass TestAddWeighted:\n    def get_input(self, size, max_elem=10):\n        shape = random_shape(size, max_elem)\n        src1 = torch.randn(shape)\n        src2 = torch.randn(shape)\n        alpha = random.random()\n        beta = random.random()\n        gamma = random.random()\n        return src1, src2, alpha, beta, gamma\n\n    @pytest.mark.parametrize(""size"", [2, 3, 4, 5])\n    def test_addweighted(self, size, device):\n        src1, src2, alpha, beta, gamma = self.get_input(3)\n        src1 = src1.to(device)\n        src2 = src2.to(device)\n\n        f = kornia.color.AddWeighted(alpha, beta, gamma)\n        assert_allclose(f(src1, src2), src1 * alpha + src2 * beta + gamma)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(src1: torch.Tensor, alpha: float, src2: torch.Tensor,\n                      beta: float, gamma: float) -> torch.Tensor:\n            return kornia.color.add_weighted(src1, alpha, src2, beta, gamma)\n\n        src1, src2, alpha, beta, gamma = self.get_input(3)\n        src1 = src1.to(device)\n        src2 = src2.to(device)\n\n        actual = op_script(src1, alpha, src2, beta, gamma)\n        expected = kornia.color.add_weighted(src1, alpha, src2, beta, gamma)\n        assert_allclose(actual, expected)\n\n    @pytest.mark.parametrize(""size"", [2, 3])\n    def test_gradcheck(self, size, device):\n        shape = random_shape(size, max_elem=5)  # to shave time on gradcheck\n        src1 = torch.randn(shape).to(device)\n        src2 = torch.randn(shape).to(device)\n        alpha = random.random()\n        beta = random.random()\n        gamma = random.random()\n\n        src1 = utils.tensor_to_gradcheck_var(src1)  # to var\n        src2 = utils.tensor_to_gradcheck_var(src2)  # to var\n\n        assert gradcheck(kornia.color.AddWeighted(alpha, beta, gamma), (src1, src2),\n                         raise_exception=True)\n'"
test/color/test_gray.py,18,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestRgbToGrayscale:\n    def test_rgb_to_grayscale(self, device, dtype):\n        channels, height, width = 3, 4, 5\n        img = torch.ones(channels, height, width).to(device, dtype)\n        assert kornia.rgb_to_grayscale(img).shape == (1, height, width)\n\n    def test_rgb_to_grayscale_batch(self, device, dtype):\n        batch_size, channels, height, width = 2, 3, 4, 5\n        img = torch.ones(batch_size, channels, height, width).to(device, dtype)\n        assert kornia.rgb_to_grayscale(img).shape == \\\n            (batch_size, 1, height, width)\n\n    def test_opencv(self, device, dtype):\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device, dtype)\n\n        # Output data generated with OpenCV 4.1.1: cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n        expected = torch.tensor([[0.4684734, 0.8954562, 0.6064363, 0.5236061, 0.6106016],\n                                 [0.1709944, 0.5133104, 0.7915002, 0.5745703, 0.1680204],\n                                 [0.5279005, 0.6092287, 0.3034387, 0.5333768, 0.6064113],\n                                 [0.3503858, 0.5720159, 0.7052018, 0.4558409, 0.3261529],\n                                 [0.6988886, 0.5897652, 0.6532392, 0.7234108, 0.7218805]])\n        expected = expected.to(device, dtype)\n\n        img_gray = kornia.rgb_to_grayscale(data)\n        assert_allclose(img_gray, expected)\n\n    def test_gradcheck(self, device, dtype):\n        batch_size, channels, height, width = 2, 3, 4, 5\n        img = torch.ones(batch_size, channels, height, width).to(device, dtype)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.rgb_to_grayscale, (img,), raise_exception=True)\n\n    def test_jit(self, device, dtype):\n        batch_size, channels, height, width = 2, 3, 64, 64\n        img = torch.ones(batch_size, channels, height, width).to(device, dtype)\n        op = kornia.rgb_to_grayscale\n        op_jit = kornia.jit.rgb_to_grayscale\n        assert_allclose(op(img), op_jit(img))\n\n    def test_jit_trace(self, device, dtype):\n        batch_size, channels, height, width = 2, 3, 64, 64\n        img = torch.ones(batch_size, channels, height, width).to(device, dtype)\n        gray = kornia.color.RgbToGrayscale()\n        gray_traced = torch.jit.trace(kornia.color.RgbToGrayscale(), img)\n        assert_allclose(gray(img), gray_traced(img))\n\n\nclass TestBgrToGrayscale:\n    def test_bgr_to_grayscale(self, device, dtype):\n        channels, height, width = 3, 4, 5\n        img = torch.ones(channels, height, width).to(device, dtype)\n        assert kornia.bgr_to_grayscale(img).shape == (1, height, width)\n\n    def test_bgr_to_grayscale_batch(self, device, dtype):\n        batch_size, channels, height, width = 2, 3, 4, 5\n        img = torch.ones(batch_size, channels, height, width).to(device, dtype)\n        assert kornia.bgr_to_grayscale(img).shape == \\\n            (batch_size, 1, height, width)\n\n    # Output data generated with OpenCV 4.1.1: cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n    def test_opencv(self, device, dtype):\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device, dtype)\n\n        expected = torch.tensor([[0.4485849, 0.8233618, 0.6262833, 0.6218331, 0.6341921],\n                                 [0.3200093, 0.4340172, 0.7107211, 0.5454938, 0.2801398],\n                                 [0.6149265, 0.7018101, 0.3503231, 0.4891168, 0.5292346],\n                                 [0.5096100, 0.4336508, 0.6704276, 0.4525143, 0.2134447],\n                                 [0.7878902, 0.6494595, 0.5211386, 0.6623823, 0.6660464]])\n        expected = expected.to(device, dtype)\n\n        img_gray = kornia.bgr_to_grayscale(data)\n        assert_allclose(img_gray, expected)\n\n    def test_gradcheck(self, device, dtype):\n        batch_size, channels, height, width = 2, 3, 4, 5\n        img = torch.ones(batch_size, channels, height, width).to(device, dtype)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.bgr_to_grayscale, (img,), raise_exception=True)\n\n    def test_module(self, device, dtype):\n        data = torch.tensor([[[[100., 73.],\n                               [200., 22.]],\n\n                              [[50., 10.],\n                               [148, 14, ]],\n\n                              [[225., 255.],\n                               [48., 8.]]]])\n\n        data = data.to(device, dtype)\n\n        assert_allclose(kornia.bgr_to_grayscale(data / 255), kornia.color.BgrToGrayscale()(data / 255))\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device, dtype):\n        batch_size, channels, height, width = 2, 3, 64, 64\n        img = torch.ones(batch_size, channels, height, width).to(device, dtype)\n        gray = kornia.color.BgrToGrayscale()\n        gray_traced = torch.jit.trace(kornia.color.BgrToGrayscale(), img)\n        assert_allclose(gray(img), gray_traced(img))\n'"
test/color/test_histogram.py,31,"b'import pytest\nimport numpy as np\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\nfrom kornia.color import histogram, histogram2d\nimport kornia.testing as utils  # test utils\n\n\nclass TestHistogram:\n    def test_shape(self, device):\n        inp = torch.ones(1, 32, device=device)\n        bins = torch.linspace(0, 255, 128).to(device)\n        bandwidth = torch.Tensor(np.array(0.9)).to(device)\n        pdf = histogram(inp, bins, bandwidth)\n        assert pdf.shape == (1, 128)\n\n    def test_shape_batch(self, device):\n        inp = torch.ones(8, 32, device=device)\n        bins = torch.linspace(0, 255, 128).to(device)\n        bandwidth = torch.Tensor(np.array(0.9)).to(device)\n        pdf = histogram(inp, bins, bandwidth)\n        assert pdf.shape == (8, 128)\n\n    def test_gradcheck(self, device):\n        inp = torch.ones(8, 32, device=device)\n        inp = utils.tensor_to_gradcheck_var(inp)  # to var\n        bins = torch.linspace(0, 255, 128).to(device)\n        bins = utils.tensor_to_gradcheck_var(bins)\n        bandwidth = torch.Tensor(np.array(0.9)).to(device)\n        bandwidth = utils.tensor_to_gradcheck_var(bandwidth)\n        assert gradcheck(histogram, (inp, bins, bandwidth), raise_exception=True)\n\n    def test_uniform_dist(self, device):\n        input1 = torch.linspace(0, 255, 10).unsqueeze(0).to(device)\n\n        pdf = histogram(input1, torch.linspace(0, 255, 10).to(device), torch.Tensor(np.array(2 * 0.4**2)))\n        ans = torch.ones((1, 10)) * 0.1\n        assert((ans.cpu() - pdf.cpu()).sum() < 1e-6)\n\n\nclass TestHistogram2d:\n    def test_shape(self, device):\n        inp1 = torch.ones(1, 32, device=device)\n        inp2 = torch.ones(1, 32, device=device)\n        bins = torch.linspace(0, 255, 128).to(device)\n        bandwidth = torch.Tensor(np.array(0.9)).to(device)\n        pdf = histogram2d(inp1, inp2, bins, bandwidth)\n        assert pdf.shape == (1, 128, 128)\n\n    def test_shape_batch(self, device):\n        inp1 = torch.ones(8, 32, device=device)\n        inp2 = torch.ones(8, 32, device=device)\n        bins = torch.linspace(0, 255, 128).to(device)\n        bandwidth = torch.Tensor(np.array(0.9)).to(device)\n        pdf = histogram2d(inp1, inp2, bins, bandwidth)\n        assert pdf.shape == (8, 128, 128)\n\n    def test_gradcheck(self, device):\n        inp1 = torch.ones(3, 16, device=device)\n        inp2 = torch.ones(3, 16, device=device)\n        inp1 = utils.tensor_to_gradcheck_var(inp1)  # to var\n        inp2 = utils.tensor_to_gradcheck_var(inp2)  # to var\n        bins = torch.linspace(0, 255, 64).to(device)\n        bins = utils.tensor_to_gradcheck_var(bins)\n        bandwidth = torch.Tensor(np.array(0.9)).to(device)\n        bandwidth = utils.tensor_to_gradcheck_var(bandwidth)\n        assert gradcheck(histogram2d, (inp1, inp2, bins, bandwidth), raise_exception=True)\n\n    def test_uniform_dist(self, device):\n        input1 = torch.linspace(0, 255, 10).unsqueeze(0).to(device)\n        input2 = torch.linspace(0, 255, 10).unsqueeze(0).to(device)\n\n        joint_pdf = histogram2d(\n            input1,\n            input2,\n            torch.linspace(0, 255, 10).to(device),\n            torch.Tensor(np.array(2 * 0.4**2)))\n\n        ans = torch.eye(10).unsqueeze(0) * 0.1\n        assert((ans.cpu() - joint_pdf.cpu()).sum() < 1e-6)\n'"
test/color/test_hls.py,24,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\nfrom kornia.constants import pi\n\nimport math\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestRgbToHls:\n\n    def test_rgb_to_hls(self, device):\n\n        data = torch.tensor([[[0.4237059, 0.1935902, 0.8585021, 0.3790484, 0.1389151],\n                              [0.5933651, 0.0474544, 0.2801555, 0.1691061, 0.9221829],\n                              [0.2351739, 0.5852075, 0.5789326, 0.8411915, 0.5960411],\n                              [0.0290176, 0.6459382, 0.8581501, 0.4755400, 0.7735767],\n                              [0.9497226, 0.0919441, 0.5462211, 0.7836787, 0.6403612]],\n\n                             [[0.2280025, 0.1352853, 0.7999730, 0.6658246, 0.4910861],\n                              [0.3499791, 0.1250734, 0.6315800, 0.4785843, 0.8477826],\n                              [0.3646359, 0.2415122, 0.5301932, 0.0782518, 0.8710389],\n                              [0.6957581, 0.6162295, 0.6259052, 0.1753750, 0.6737530],\n                              [0.7678874, 0.9825978, 0.0234877, 0.2485284, 0.8159551]],\n\n                             [[0.7330830, 0.9015747, 0.0229067, 0.4280063, 0.5400181],\n                              [0.0037299, 0.3259412, 0.3467951, 0.9575506, 0.1525899],\n                              [0.9660432, 0.5287710, 0.6654660, 0.3797526, 0.4981400],\n                              [0.7422802, 0.9926301, 0.5334370, 0.7852844, 0.4397180],\n                              [0.2281681, 0.2560037, 0.5134379, 0.5800887, 0.8685090]]])\n        data = data.to(device)\n\n        # OpenCV\n        h_expected = torch.tensor([[4.5945477, 4.268469, 0.9738468, 2.2731707, 3.269344],\n                                   [0.6149417, 3.8969188, 2.292972, 3.7777472, 0.9459598],\n                                   [4.003296, 5.4079432, 4.566101, 5.869351, 1.8194631],\n                                   [3.2098956, 4.271444, 0.29820946, 4.7041655, 0.7340856],\n                                   [0.78329855, 2.2872903, 5.3016634, 5.634379, 3.382815]])\n        h_expected = h_expected.to(device)\n\n        l_expected = torch.tensor([[0.48054275, 0.51843, 0.44070444, 0.5224365, 0.33946657],\n                                   [0.2985475, 0.18669781, 0.45586777, 0.5633283, 0.5373864],\n                                   [0.6006086, 0.41335985, 0.5978296, 0.45972168, 0.6845894],\n                                   [0.38564888, 0.80442977, 0.6957935, 0.48032972, 0.6066474],\n                                   [0.5889454, 0.53727096, 0.2848544, 0.5161035, 0.7544351]])\n        l_expected = l_expected.to(device)\n        s_expected = torch.tensor([[0.5255313, 0.79561585, 0.9480225, 0.30024928, 0.59078425],\n                                   [0.98750657, 0.7458223, 0.3854456, 0.90278864, 0.8317882],\n                                   [0.9149786, 0.4157338, 0.16817844, 0.82978433, 0.5911325],\n                                   [0.9247565, 0.9623155, 0.53370523, 0.63488615, 0.4243758],\n                                   [0.8776869, 0.96239233, 0.91754496, 0.55295944, 0.46453667]])\n        s_expected = s_expected.to(device)\n\n        f = kornia.color.RgbToHls()\n        result = f(data)\n\n        h = result[0, :, :]\n        l = result[1, :, :]\n        s = result[2, :, :]\n\n        assert_allclose(h, h_expected)\n        assert_allclose(l, l_expected)\n        assert_allclose(s, s_expected)\n\n    def test_batch_rgb_to_hls(self, device):\n\n        data = torch.tensor([[[0.4237059, 0.1935902, 0.8585021, 0.3790484, 0.1389151],\n                              [0.5933651, 0.0474544, 0.2801555, 0.1691061, 0.9221829],\n                              [0.2351739, 0.5852075, 0.5789326, 0.8411915, 0.5960411],\n                              [0.0290176, 0.6459382, 0.8581501, 0.4755400, 0.7735767],\n                              [0.9497226, 0.0919441, 0.5462211, 0.7836787, 0.6403612]],\n\n                             [[0.2280025, 0.1352853, 0.7999730, 0.6658246, 0.4910861],\n                              [0.3499791, 0.1250734, 0.6315800, 0.4785843, 0.8477826],\n                              [0.3646359, 0.2415122, 0.5301932, 0.0782518, 0.8710389],\n                              [0.6957581, 0.6162295, 0.6259052, 0.1753750, 0.6737530],\n                              [0.7678874, 0.9825978, 0.0234877, 0.2485284, 0.8159551]],\n\n                             [[0.7330830, 0.9015747, 0.0229067, 0.4280063, 0.5400181],\n                              [0.0037299, 0.3259412, 0.3467951, 0.9575506, 0.1525899],\n                              [0.9660432, 0.5287710, 0.6654660, 0.3797526, 0.4981400],\n                              [0.7422802, 0.9926301, 0.5334370, 0.7852844, 0.4397180],\n                              [0.2281681, 0.2560037, 0.5134379, 0.5800887, 0.8685090]]])\n        data = data.to(device)\n\n        # OpenCV\n        expected = torch.tensor([[[4.5945477, 4.268469, 0.9738468, 2.2731707, 3.269344],\n                                  [0.6149417, 3.8969188, 2.292972, 3.7777472, 0.9459598],\n                                  [4.003296, 5.4079432, 4.566101, 5.869351, 1.8194631],\n                                  [3.2098956, 4.271444, 0.29820946, 4.7041655, 0.7340856],\n                                  [0.78329855, 2.2872903, 5.3016634, 5.634379, 3.382815]],\n\n                                 [[0.48054275, 0.51843, 0.44070444, 0.5224365, 0.33946657],\n                                  [0.2985475, 0.18669781, 0.45586777, 0.5633283, 0.5373864],\n                                  [0.6006086, 0.41335985, 0.5978296, 0.45972168, 0.6845894],\n                                  [0.38564888, 0.80442977, 0.6957935, 0.48032972, 0.6066474],\n                                  [0.5889454, 0.53727096, 0.2848544, 0.5161035, 0.7544351]],\n\n                                 [[0.5255313, 0.79561585, 0.9480225, 0.30024928, 0.59078425],\n                                  [0.98750657, 0.7458223, 0.3854456, 0.90278864, 0.8317882],\n                                  [0.9149786, 0.4157338, 0.16817844, 0.82978433, 0.5911325],\n                                  [0.9247565, 0.9623155, 0.53370523, 0.63488615, 0.4243758],\n                                  [0.8776869, 0.96239233, 0.91754496, 0.55295944, 0.46453667]]])\n        expected = expected.to(device)\n\n        # Kornia\n        f = kornia.color.RgbToHls()\n\n        data = data.repeat(2, 1, 1, 1)  # 2x3x5x5\n        expected = expected.repeat(2, 1, 1, 1)  # 2x3x5x5\n\n        assert_allclose(f(data), expected)\n\n    def test_nan_rgb_to_hls(self, device):\n\n        data = torch.ones(1, 5, 5).to(device)  # 3x5x5\n        data = data.repeat(2, 3, 1, 1)  # 2x3x5x5\n\n        # OpenCV\n        expected = torch.cat([torch.zeros(1, 5, 5), torch.ones(1, 5, 5), torch.zeros(1, 5, 5)], dim=0).to(device)\n        expected = expected.repeat(2, 1, 1, 1)  # 2x3x5x5\n\n        # Kornia\n        f = kornia.color.RgbToHls()\n\n        assert_allclose(f(data), expected)\n\n    def test_gradcheck(self, device):\n\n        data = torch.rand(3, 5, 5).to(device)  # 3x5x5\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n\n        assert gradcheck(kornia.color.RgbToHls(), (data,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> torch.Tensor:\n\n            return kornia.rgb_to_hls(data)\n\n            data = torch.tensor([[[[21., 22.],\n                                   [22., 22.]],\n\n                                  [[13., 14.],\n                                   [14., 14.]],\n\n                                  [[8., 8.],\n                                   [8., 8.]]]])  # 3x2x2\n\n            data = data.to(device)\n            actual = op_script(data)\n            expected = kornia.rgb_to_hls(data)\n            assert_allclose(actual, expected)\n\n\nclass TestHlsToRgb:\n\n    def test_hls_to_rgb(self, device):\n\n        data = torch.tensor([[[0.5513626, 0.8487718, 0.1822479, 0.2851745, 0.2669488],\n                              [0.7596772, 0.4565057, 0.6181599, 0.3852497, 0.7746902],\n                              [0.5742747, 0.1957062, 0.7530835, 0.2104362, 0.9449323],\n                              [0.9918052, 0.2437515, 0.4718738, 0.8502576, 0.1675640],\n                              [0.9210159, 0.0538564, 0.5801026, 0.6110542, 0.3768399]],\n\n                             [[0.4111853, 0.0183454, 0.7832276, 0.2975794, 0.1139528],\n                              [0.6207729, 0.1073406, 0.8335325, 0.5700451, 0.2594557],\n                              [0.7520493, 0.5097187, 0.4719872, 0.9477938, 0.1640292],\n                              [0.8973427, 0.6455371, 0.7567374, 0.3159562, 0.8135307],\n                              [0.0855004, 0.6645504, 0.9923756, 0.6209313, 0.2356791]],\n\n                             [[0.4734681, 0.0422099, 0.7405791, 0.9671807, 0.1793800],\n                              [0.8221875, 0.7219887, 0.3627397, 0.4403201, 0.0024084],\n                              [0.0803350, 0.9432759, 0.0241543, 0.8292291, 0.7745832],\n                              [0.3707901, 0.0851424, 0.5805428, 0.1098685, 0.4238486],\n                              [0.1058410, 0.0816052, 0.5792874, 0.9578886, 0.6281684]]])  # 3x5x5\n        data = data.to(device)\n\n        # OpenCV\n        r_expected = torch.tensor([[0.21650219, 0.01911971, 0.91374826, 0.1760952, 0.10979544],\n                                   [0.6569808, 0.02984191, 0.77314806, 0.3807273, 0.25964087],\n                                   [0.7321301, 0.8110298, 0.4724091, 0.96834683, 0.2910835],\n                                   [0.935407, 0.6478001, 0.615513, 0.3506698, 0.89171433],\n                                   [0.0945498, 0.6919248, 0.98795897, 0.25782573, 0.08763295]])\n        r_expected = r_expected.to(device)\n\n        g_expected = torch.tensor([[0.48587522, 0.017571, 0.9437648, 0.5853925, 0.13439366],\n                                   [0.30897713, 0.18483935, 0.8082967, 0.75936294, 0.25883088],\n                                   [0.7542145, 0.97218925, 0.46058673, 0.9910847, 0.03697497],\n                                   [0.8592784, 0.675717, 0.8979618, 0.28124255, 0.8925654],\n                                   [0.07645091, 0.6548674, 0.99254686, 0.500144, 0.38372523]])\n        g_expected = g_expected.to(device)\n\n        b_expected = torch.tensor([[0.60586834, 0.01897625, 0.62269044, 0.00976634, 0.09351197],\n                                   [0.93256867, 0.14439031, 0.89391685, 0.49867177, 0.2600806],\n                                   [0.7719684, 0.04724807, 0.48338777, 0.904503, 0.12093388],\n                                   [0.8630215, 0.6153573, 0.85029656, 0.34361976, 0.73449594],\n                                   [0.08502806, 0.6371759, 0.99679226, 0.9840369, 0.16492467]])\n        b_expected = b_expected.to(device)\n\n        # Kornia\n        f = kornia.color.HlsToRgb()\n        data[0] = 2 * pi * data[0]\n        result = f(data)\n\n        r = result[0, :, :]\n        g = result[1, :, :]\n        b = result[2, :, :]\n\n        assert_allclose(r, r_expected)\n        assert_allclose(g, g_expected)\n        assert_allclose(b, b_expected)\n\n    def test_batch_hls_to_rgb(self, device):\n\n        data = torch.tensor([[[0.5513626, 0.8487718, 0.1822479, 0.2851745, 0.2669488],\n                              [0.7596772, 0.4565057, 0.6181599, 0.3852497, 0.7746902],\n                              [0.5742747, 0.1957062, 0.7530835, 0.2104362, 0.9449323],\n                              [0.9918052, 0.2437515, 0.4718738, 0.8502576, 0.1675640],\n                              [0.9210159, 0.0538564, 0.5801026, 0.6110542, 0.3768399]],\n\n                             [[0.4111853, 0.0183454, 0.7832276, 0.2975794, 0.1139528],\n                              [0.6207729, 0.1073406, 0.8335325, 0.5700451, 0.2594557],\n                              [0.7520493, 0.5097187, 0.4719872, 0.9477938, 0.1640292],\n                              [0.8973427, 0.6455371, 0.7567374, 0.3159562, 0.8135307],\n                              [0.0855004, 0.6645504, 0.9923756, 0.6209313, 0.2356791]],\n\n                             [[0.4734681, 0.0422099, 0.7405791, 0.9671807, 0.1793800],\n                              [0.8221875, 0.7219887, 0.3627397, 0.4403201, 0.0024084],\n                              [0.0803350, 0.9432759, 0.0241543, 0.8292291, 0.7745832],\n                              [0.3707901, 0.0851424, 0.5805428, 0.1098685, 0.4238486],\n                              [0.1058410, 0.0816052, 0.5792874, 0.9578886, 0.6281684]]])  # 3x5x5\n        data = data.to(device)\n        data[0] = 2 * pi * data[0]\n        data = data.repeat(2, 1, 1, 1)  # 2x3x5x5\n\n        # OpenCV\n        expected = torch.tensor([[[0.21650219, 0.01911971, 0.91374826, 0.1760952, 0.10979544],\n                                  [0.6569808, 0.02984191, 0.77314806, 0.3807273, 0.25964087],\n                                  [0.7321301, 0.8110298, 0.4724091, 0.96834683, 0.2910835],\n                                  [0.935407, 0.6478001, 0.615513, 0.3506698, 0.89171433],\n                                  [0.0945498, 0.6919248, 0.98795897, 0.25782573, 0.08763295]],\n\n                                 [[0.48587522, 0.017571, 0.9437648, 0.5853925, 0.13439366],\n                                  [0.30897713, 0.18483935, 0.8082967, 0.75936294, 0.25883088],\n                                  [0.7542145, 0.97218925, 0.46058673, 0.9910847, 0.03697497],\n                                  [0.8592784, 0.675717, 0.8979618, 0.28124255, 0.8925654],\n                                  [0.07645091, 0.6548674, 0.99254686, 0.500144, 0.38372523]],\n\n                                 [[0.60586834, 0.01897625, 0.62269044, 0.00976634, 0.09351197],\n                                  [0.93256867, 0.14439031, 0.89391685, 0.49867177, 0.2600806],\n                                  [0.7719684, 0.04724807, 0.48338777, 0.904503, 0.12093388],\n                                  [0.8630215, 0.6153573, 0.85029656, 0.34361976, 0.73449594],\n                                  [0.08502806, 0.6371759, 0.99679226, 0.9840369, 0.16492467]]])\n        expected = expected.to(device)\n        expected = expected.repeat(2, 1, 1, 1)  # 2x3x2x2\n\n        # Kornia\n        f = kornia.color.HlsToRgb()\n\n        assert_allclose(f(data), expected)\n\n        data[:, 0] += 2 * pi\n        assert_allclose(f(data), expected)\n\n        data[:, 0] -= 4 * pi\n        assert_allclose(f(data), expected)\n\n    def test_gradcheck(self, device):\n\n        data = torch.rand(3, 5, 5).to(device)  # 3x5x5\n        data[0] = 2 * pi * data[0]\n\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n\n        assert gradcheck(kornia.color.HlsToRgb(), (data,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(data: torch.tensor) -> torch.Tensor:\n            return kornia.hls_to_rgb(data)\n\n            data = torch.tensor([[[[21., 22.],\n                                   [22., 22.]],\n\n                                  [[13., 14.],\n                                   [14., 14.]],\n\n                                  [[8., 8.],\n                                   [8., 8.]]]])  # 3x2x2\n\n            data = data.to(device)\n            actual = op_script(data)\n            expected = kornia.hls_to_rgb(data)\n            assert_allclose(actual, expected)\n'"
test/color/test_hsv.py,24,"b'import pytest\n\nimport kornia\nfrom kornia.constants import pi\nimport kornia.testing as utils  # test utils\n\nimport math\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestRgbToHsv:\n\n    def test_rgb_to_hsv(self, device):\n\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device)\n\n        # OpenCV\n        h_expected = torch.tensor([[1.6519808, 1.3188975, 2.2487938, 3.582216, 2.250954],\n                                   [4.28164, 0.04868213, 0.83454597, 5.533617, 4.319574],\n                                   [3.4185164, 2.7919037, 2.8883224, 1.7474692, 1.3619272],\n                                   [3.6837196, 0.6378961, 5.7213116, 5.2614374, 6.259687],\n                                   [2.929221, 2.5614352, 0.97840965, 1.5729411, 6.0235224]])\n        h_expected = h_expected.to(device)\n\n        s_expected = torch.tensor([[0.46999356, 0.52820253, 0.8132473, 0.65267974, 0.899411],\n                                   [0.9497089, 0.534381, 0.48878422, 0.60298723, 0.9163612],\n                                   [0.6343409, 0.87112963, 0.8101612, 0.9500878, 0.8192622],\n                                   [0.99010557, 0.9023306, 0.42042294, 0.8292772, 0.81847864],\n                                   [0.6755719, 0.8493871, 0.93686795, 0.73741645, 0.40461043]])\n        s_expected = s_expected.to(device)\n\n        v_expected = torch.tensor([[0.5414237, 0.99627006, 0.89471555, 0.81350225, 0.9483274],\n                                   [0.930656, 0.80207086, 0.8933256, 0.9170977, 0.75575125],\n                                   [0.74157417, 0.86247796, 0.41260356, 0.76009345, 0.7279997],\n                                   [0.86927235, 0.8288748, 0.9647092, 0.892202, 0.7614344],\n                                   [0.8932794, 0.8517839, 0.7621747, 0.8983801, 0.99185926]])\n        v_expected = v_expected.to(device)\n\n        f = kornia.color.RgbToHsv()\n        result = f(data)\n\n        h = result[0, :, :]\n        s = result[1, :, :]\n        v = result[2, :, :]\n\n        assert_allclose(h, h_expected)\n        assert_allclose(s, s_expected)\n        assert_allclose(v, v_expected)\n\n    def test_batch_rgb_to_hsv(self, device):\n\n        data = torch.tensor([[[0.3944633, 0.8597369, 0.1670904, 0.2825457, 0.0953912],\n                              [0.1251704, 0.8020709, 0.8933256, 0.9170977, 0.1497008],\n                              [0.2711633, 0.1111478, 0.0783281, 0.2771807, 0.5487481],\n                              [0.0086008, 0.8288748, 0.9647092, 0.8922020, 0.7614344],\n                              [0.2898048, 0.1282895, 0.7621747, 0.5657831, 0.9918593]],\n\n                             [[0.5414237, 0.9962701, 0.8947155, 0.5900949, 0.9483274],\n                              [0.0468036, 0.3933847, 0.8046577, 0.3640994, 0.0632100],\n                              [0.6171775, 0.8624780, 0.4126036, 0.7600935, 0.7279997],\n                              [0.4237089, 0.5365476, 0.5591233, 0.1523191, 0.1382165],\n                              [0.8932794, 0.8517839, 0.7152701, 0.8983801, 0.5905426]],\n\n                             [[0.2869580, 0.4700376, 0.2743714, 0.8135023, 0.2229074],\n                              [0.9306560, 0.3734594, 0.4566821, 0.7599275, 0.7557513],\n                              [0.7415742, 0.6115875, 0.3317572, 0.0379378, 0.1315770],\n                              [0.8692724, 0.0809556, 0.7767404, 0.8742208, 0.1522012],\n                              [0.7708948, 0.4509611, 0.0481175, 0.2358997, 0.6900532]]])\n        data = data.to(device)\n\n        # OpenCV\n        expected = torch.tensor([[[1.6519808, 1.3188975, 2.2487938, 3.582216, 2.250954],\n                                  [4.28164, 0.04868213, 0.83454597, 5.533617, 4.319574],\n                                  [3.4185164, 2.7919037, 2.8883224, 1.7474692, 1.3619272],\n                                  [3.6837196, 0.6378961, 5.7213116, 5.2614374, 6.259687],\n                                  [2.929221, 2.5614352, 0.97840965, 1.5729411, 6.0235224]],\n\n                                 [[0.46999356, 0.52820253, 0.8132473, 0.65267974, 0.899411],\n                                  [0.9497089, 0.534381, 0.48878422, 0.60298723, 0.9163612],\n                                  [0.6343409, 0.87112963, 0.8101612, 0.9500878, 0.8192622],\n                                  [0.99010557, 0.9023306, 0.42042294, 0.8292772, 0.81847864],\n                                  [0.6755719, 0.8493871, 0.93686795, 0.73741645, 0.40461043]],\n\n                                 [[0.5414237, 0.99627006, 0.89471555, 0.81350225, 0.9483274],\n                                  [0.930656, 0.80207086, 0.8933256, 0.9170977, 0.75575125],\n                                  [0.74157417, 0.86247796, 0.41260356, 0.76009345, 0.7279997],\n                                  [0.86927235, 0.8288748, 0.9647092, 0.892202, 0.7614344],\n                                  [0.8932794, 0.8517839, 0.7621747, 0.8983801, 0.99185926]]])\n        expected = expected.to(device)\n\n        # Kornia\n        f = kornia.color.RgbToHsv()\n\n        data = data.repeat(2, 1, 1, 1)  # 2x3x5x5\n        expected = expected.repeat(2, 1, 1, 1)  # 2x3x5x5\n\n        assert_allclose(f(data), expected)\n\n    def test_nan_rgb_to_hsv(self):\n\n        data = torch.zeros(1, 5, 5)  # 3x5x5\n        data = data.repeat(3, 1, 1)  # 2x3x5x5\n\n        # OpenCV\n        expected = torch.zeros(1, 5, 5)  # 3x5x5\n        expected = expected.repeat(3, 1, 1)  # 2x3x5x5\n\n        # Kornia\n        f = kornia.color.RgbToHsv()\n\n        assert_allclose(f(data), expected)\n\n    def test_gradcheck(self, device):\n\n        data = torch.rand(3, 5, 5).to(device)  # 3x2x2\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n\n        assert gradcheck(kornia.color.RgbToHsv(), (data,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> torch.Tensor:\n\n            return kornia.rgb_to_hsv(data)\n            data = torch.tensor([[[[21., 22.],\n                                   [22., 22.]],\n\n                                  [[13., 14.],\n                                   [14., 14.]],\n\n                                  [[8., 8.],\n                                   [8., 8.]]]])  # 3x2x2\n\n            data = data.to(device)\n            actual = op_script(data)\n            expected = kornia.rgb_to_hsv(data)\n            assert_allclose(actual, expected)\n\n\nclass TestHsvToRgb:\n\n    def test_hsv_to_rgb(self, device):\n\n        data = torch.tensor([[[3.5433271, 5.6390061, 1.3766849, 2.5384088, 4.6848912],\n                              [5.7209363, 5.3262630, 6.2059994, 4.1164689, 2.3872600],\n                              [0.6370091, 3.6186798, 5.9170871, 2.8275447, 5.4289737],\n                              [0.2751994, 1.6632686, 1.0049511, 0.7046204, 1.3791083],\n                              [0.7863123, 4.4852505, 4.3064494, 2.5573561, 5.9083076]],\n\n                             [[0.5026655, 0.9453601, 0.5929778, 0.2632897, 0.4590443],\n                              [0.6201433, 0.5610679, 0.9653260, 0.0830478, 0.5000827],\n                              [0.6067343, 0.6422323, 0.6777940, 0.7705711, 0.6050767],\n                              [0.5495264, 0.5573426, 0.4683768, 0.2268902, 0.2116482],\n                              [0.6525245, 0.0022379, 0.4909980, 0.1682271, 0.6327152]],\n\n                             [[0.8471680, 0.9302199, 0.3265766, 0.7944570, 0.7038843],\n                              [0.4833369, 0.2088473, 0.1169234, 0.4966302, 0.6448684],\n                              [0.2713015, 0.5893380, 0.6015301, 0.6801558, 0.2322258],\n                              [0.5704236, 0.6797268, 0.4755683, 0.4811209, 0.5317836],\n                              [0.3236262, 0.0999796, 0.3614958, 0.5117705, 0.8194097]]])  # 3x5x5\n        data = data.to(device)\n\n        # OpenCV\n        r_expected = torch.tensor([[0.4213259, 0.93021995, 0.26564622, 0.58528465, 0.5338429],\n                                   [0.48333693, 0.20884734, 0.11692339, 0.45538613, 0.32238087],\n                                   [0.2713015, 0.2108461, 0.60153013, 0.15604737, 0.23222584],\n                                   [0.5704236, 0.4568531, 0.4755683, 0.48112088, 0.49611038],\n                                   [0.32362622, 0.09981924, 0.20394461, 0.42567685, 0.81940967]])\n        r_expected = r_expected.to(device)\n\n        g_expected = torch.tensor([[0.6838029, 0.0508271, 0.3265766, 0.794457, 0.3807702],\n                                   [0.18359877, 0.0916698, 0.00405421, 0.45823452, 0.6448684],\n                                   [0.20682439, 0.41690278, 0.1938166, 0.68015575, 0.0917114],\n                                   [0.33933756, 0.6797268, 0.4665822, 0.44541004, 0.5317836],\n                                   [0.27101707, 0.09975589, 0.18400209, 0.51177055, 0.30095676]])\n        g_expected = g_expected.to(device)\n\n        b_expected = torch.tensor([[0.84716797, 0.5917818, 0.13292392, 0.6739741, 0.7038843],\n                                   [0.34453064, 0.19874583, 0.01237347, 0.4966302, 0.41256943],\n                                   [0.10669357, 0.589338, 0.3363524, 0.5229789, 0.20633064],\n                                   [0.25696078, 0.30088606, 0.25282317, 0.37195927, 0.41923255],\n                                   [0.11245217, 0.09997964, 0.3614958, 0.46373847, 0.4865534]])\n        b_expected = b_expected.to(device)\n\n        # Kornia\n        f = kornia.color.HsvToRgb()\n        result = f(data)\n\n        r = result[0, :, :]\n        g = result[1, :, :]\n        b = result[2, :, :]\n\n        assert_allclose(r, r_expected)\n        assert_allclose(g, g_expected)\n        assert_allclose(b, b_expected)\n\n    def test_batch_hsv_to_rgb(self, device):\n\n        data = torch.tensor([[[3.5433271, 5.6390061, 1.3766849, 2.5384088, 4.6848912],\n                              [5.7209363, 5.3262630, 6.2059994, 4.1164689, 2.3872600],\n                              [0.6370091, 3.6186798, 5.9170871, 2.8275447, 5.4289737],\n                              [0.2751994, 1.6632686, 1.0049511, 0.7046204, 1.3791083],\n                              [0.7863123, 4.4852505, 4.3064494, 2.5573561, 5.9083076]],\n\n                             [[0.5026655, 0.9453601, 0.5929778, 0.2632897, 0.4590443],\n                              [0.6201433, 0.5610679, 0.9653260, 0.0830478, 0.5000827],\n                              [0.6067343, 0.6422323, 0.6777940, 0.7705711, 0.6050767],\n                              [0.5495264, 0.5573426, 0.4683768, 0.2268902, 0.2116482],\n                              [0.6525245, 0.0022379, 0.4909980, 0.1682271, 0.6327152]],\n\n                             [[0.8471680, 0.9302199, 0.3265766, 0.7944570, 0.7038843],\n                              [0.4833369, 0.2088473, 0.1169234, 0.4966302, 0.6448684],\n                              [0.2713015, 0.5893380, 0.6015301, 0.6801558, 0.2322258],\n                              [0.5704236, 0.6797268, 0.4755683, 0.4811209, 0.5317836],\n                              [0.3236262, 0.0999796, 0.3614958, 0.5117705, 0.8194097]]])  # 3x5x5\n        data = data.to(device)\n        data = data.repeat(2, 1, 1, 1)  # 2x3x5x5\n\n        # OpenCV\n        expected = torch.tensor([[[0.4213259, 0.93021995, 0.26564622, 0.58528465, 0.5338429],\n                                  [0.48333693, 0.20884734, 0.11692339, 0.45538613, 0.32238087],\n                                  [0.2713015, 0.2108461, 0.60153013, 0.15604737, 0.23222584],\n                                  [0.5704236, 0.4568531, 0.4755683, 0.48112088, 0.49611038],\n                                  [0.32362622, 0.09981924, 0.20394461, 0.42567685, 0.81940967]],\n\n                                 [[0.6838029, 0.0508271, 0.3265766, 0.794457, 0.3807702],\n                                  [0.18359877, 0.0916698, 0.00405421, 0.45823452, 0.6448684],\n                                  [0.20682439, 0.41690278, 0.1938166, 0.68015575, 0.0917114],\n                                  [0.33933756, 0.6797268, 0.4665822, 0.44541004, 0.5317836],\n                                  [0.27101707, 0.09975589, 0.18400209, 0.51177055, 0.30095676]],\n\n                                 [[0.84716797, 0.5917818, 0.13292392, 0.6739741, 0.7038843],\n                                  [0.34453064, 0.19874583, 0.01237347, 0.4966302, 0.41256943],\n                                  [0.10669357, 0.589338, 0.3363524, 0.5229789, 0.20633064],\n                                  [0.25696078, 0.30088606, 0.25282317, 0.37195927, 0.41923255],\n                                  [0.11245217, 0.09997964, 0.3614958, 0.46373847, 0.4865534]]])\n        expected = expected.to(device)\n        expected = expected.repeat(2, 1, 1, 1)  # 2x3x5x5\n\n        # Kornia\n        f = kornia.color.HsvToRgb()\n\n        assert_allclose(f(data), expected)\n\n        data[:, 0] += 2 * pi\n        assert_allclose(f(data), expected)\n\n        data[:, 0] -= 4 * pi\n        assert_allclose(f(data), expected)\n\n    def test_gradcheck(self, device):\n\n        data = torch.rand(3, 5, 5).to(device)  # 3x5x5\n        data[0] = 2 * pi * data[0]\n\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n\n        assert gradcheck(kornia.color.HsvToRgb(), (data,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> torch.Tensor:\n            return kornia.hsv_to_rgb(data)\n\n            data = torch.tensor([[[[21., 22.],\n                                   [22., 22.]],\n\n                                  [[13., 14.],\n                                   [14., 14.]],\n\n                                  [[8., 8.],\n                                   [8., 8.]]]])  # 3x2x2\n\n            data = data.to(device)\n            actual = op_script(data)\n            expected = kornia.hsv_to_rgb(data)\n            assert_allclose(actual, expected)\n'"
test/color/test_luv.py,16,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestRgbToLuv:\n\n    @pytest.mark.parametrize(""batch_size"", [None, 1, 2, 5])\n    def test_rgb_to_luv(self, batch_size, device):\n\n        data = torch.tensor([[[[0.13088040, 0.54399723, 0.69396782, 0.63581685, 0.09902618],\n                               [0.59459005, 0.74215373, 0.89662376, 0.25920381, 0.89937686],\n                               [0.29857584, 0.28139791, 0.16441015, 0.55507519, 0.06124221],\n                               [0.40908658, 0.10261389, 0.01691456, 0.76006799, 0.32971736]],\n\n                              [[0.60354551, 0.76201361, 0.79009938, 0.91742945, 0.60044175],\n                               [0.42812678, 0.18552390, 0.04186043, 0.38030245, 0.15420346],\n                               [0.13552373, 0.53955473, 0.79102736, 0.49050815, 0.75271446],\n                               [0.39861023, 0.80680277, 0.82823833, 0.54438462, 0.22063386]],\n\n                              [[0.63231256, 0.18316011, 0.84317145, 0.59529881, 0.15297393],\n                               [0.59235313, 0.36617295, 0.34600773, 0.40304737, 0.61720451],\n                               [0.46040250, 0.42006640, 0.54765106, 0.48982632, 0.13914755],\n                               [0.58402964, 0.89597990, 0.98276161, 0.25019163, 0.69285921]]]])\n\n        # Reference output generated using skimage: rgb2luv(data)\n\n        luv_ref = torch.tensor([[[[58.02612686, 72.48876190, 79.75208282, 86.38912964, 55.25164032],\n                                  [51.66668701, 43.81214523, 48.93865585, 39.03804398, 52.55152512],\n                                  [23.71140671, 52.38661957, 72.54607391, 53.89587402, 67.94892883],\n                                  [45.02897263, 75.98315430, 78.25762177, 61.85069656, 33.77972794]],\n\n                                 [[-41.55438232, -28.05948639, -13.54032803, -35.42317200, -49.27433014],\n                                  [21.34596062, 94.13956451, 137.11340332, -14.69241238, 102.94833374],\n                                  [9.55611229, -30.01761436, -58.94236755, 9.83261871, -62.96137619],\n                                  [-1.55336237, -55.22497559, -56.21067810, 43.76751328, 1.46367633]],\n\n                                 [[-15.29427338, 77.81495667, -13.74480152, 52.17128372, 60.92724228],\n                                     [-27.01125526, -1.72837746, 6.57535267, -7.83582020, -38.45543289],\n                                     [-50.89970779, 17.65329361, 36.54148102, 2.25501800, 78.93702698],\n                                     [-38.39783859, -31.71204376, -46.63606644, 50.16629410, -84.74416351]]]])\n\n        data.to(device)\n        luv_ref.to(device)\n\n        if batch_size is not None:\n            data = data.repeat(batch_size, 1, 1, 1)\n            luv_ref = luv_ref.repeat(batch_size, 1, 1, 1)\n\n        luv = kornia.color.RgbToLuv()\n        out = luv(data)\n        assert_allclose(out, luv_ref)\n\n    def test_grad(self, device):\n\n        data = torch.rand(2, 3, 4, 5).to(device)\n        data = utils.tensor_to_gradcheck_var(data)\n        assert gradcheck(kornia.color.RgbToLuv(), (data,), raise_exception=True)\n\n    @pytest.mark.parametrize(""input_shape"", [(2, 2), (3, 3, 5, 3, 3)])\n    def test_shape(self, input_shape, device):\n        with pytest.raises(ValueError):\n            luv = kornia.color.RgbToLuv()\n            out = luv(torch.ones(*input_shape).to(device))\n\n    def test_inverse(self, device):\n        data = torch.rand(3, 4, 5).to(device)\n        luv = kornia.color.LuvToRgb()\n        rgb = kornia.color.RgbToLuv()\n\n        data_out = luv(rgb(data))\n        assert_allclose(data_out, data, rtol=1e-4, atol=1e-4)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n\n        data = torch.rand((2, 3, 4, 5)).to(device)\n        luv = kornia.color.rgb_to_luv\n        luv_jit = torch.jit.script(kornia.color.rgb_to_luv)\n        assert_allclose(luv_jit(data), luv(data))\n\n\nclass TestLuvToRgb:\n\n    @pytest.mark.parametrize(""batch_size"", [None, 1, 2, 5])\n    def test_rgb_to_luv(self, batch_size, device):\n\n        data = torch.tensor([[[[50.21928787, 23.29810143, 14.98279190, 62.50927353, 72.78904724],\n                               [70.86846924, 68.75330353, 52.81696701, 76.17090607, 88.63134003],\n                               [46.87160873, 72.38699341, 37.71450806, 82.57386780, 74.79967499],\n                               [77.33016968, 47.39180374, 61.76217651, 90.83254242, 86.96239471]],\n\n                              [[65.81327057, -3.69859719, 0.16971001, 14.86583614, -65.54960632],\n                               [-41.03258133, -19.52661896, 64.16155243, -58.53935242, -71.78411102],\n                               [112.05227661, -60.13330460, 43.07910538, -51.01456833, -58.25787354],\n                               [-62.37575531, 50.88882065, -39.27450943, 17.00958824, -24.93779755]],\n\n                              [[-69.53346252, -73.34986877, -11.47461891, 66.73863220, 70.43983459],\n                               [51.92737579, 58.77009583, 45.97863388, 24.44452858, 98.81991577],\n                               [-7.60597992, 78.97976685, -69.31867218, 67.33953857, 14.28889370],\n                               [92.31149292, -85.91405487, -32.83668518, -23.45091820, 69.99038696]]]])\n        # Reference output generated using skimage: luv2rgb(data)\n\n        rgb_ref = torch.tensor([[[[0.78923208, 0.17048222, 0.14947766, 0.65528989, 0.07863078],\n                                  [0.41649094, 0.55222923, 0.72673196, 0.21939684, 0.34298307],\n                                  [0.82763243, 0.24021322, 0.58888060, 0.47255886, 0.16407511],\n                                  [0.30320778, 0.72233224, 0.21593384, 0.98893607, 0.71707106]],\n\n                                 [[0.20532851, 0.13188709, 0.13879408, 0.59964627, 0.80721593],\n                                     [0.75411713, 0.70656943, 0.41770950, 0.82750136, 0.99659365],\n                                     [0.12436169, 0.79804462, 0.10958754, 0.89803618, 0.81000644],\n                                     [0.85726571, 0.17667055, 0.63285238, 0.85567462, 0.91538441]],\n\n                                 [[0.73985511, 0.59308004, 0.21156698, 0.03804367, 0.32732114],\n                                     [0.42489606, 0.33011687, 0.12804756, 0.64905322, 0.25216782],\n                                     [0.41637793, 0.22158240, 0.63437861, 0.46121466, 0.68336427],\n                                     [0.06325728, 0.78878325, 0.74280596, 0.99514300, 0.47176042]]]])\n\n        data.to(device)\n        rgb_ref.to(device)\n\n        if batch_size is not None:\n            data = data.repeat(batch_size, 1, 1, 1)\n            rgb_ref = rgb_ref.repeat(batch_size, 1, 1, 1)\n\n        rgb = kornia.color.LuvToRgb()\n        out = rgb(data)\n        assert_allclose(out, rgb_ref)\n\n    def test_grad(self, device):\n\n        data = kornia.rgb_to_luv(torch.rand(2, 3, 4, 5).to(device))\n        data = utils.tensor_to_gradcheck_var(data)\n        assert gradcheck(kornia.color.LuvToRgb(), (data,), raise_exception=True)\n\n    @pytest.mark.parametrize(""input_shape"", [(2, 2), (3, 3, 5, 3, 3)])\n    def test_shape(self, input_shape, device):\n        with pytest.raises(ValueError):\n            rgb = kornia.color.LuvToRgb()\n            out = rgb(torch.ones(*input_shape).to(device))\n\n    def test_inverse(self, device):\n        data = torch.rand(3, 11, 15).to(device)\n        luv = kornia.color.LuvToRgb()\n        rgb = kornia.color.RgbToLuv()\n\n        data_out = rgb(luv(data))\n        assert_allclose(data_out, data)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n\n        data = torch.rand((2, 3, 4, 5)).to(device)\n        rgb = kornia.color.luv_to_rgb\n        rgb_jit = torch.jit.script(kornia.color.luv_to_rgb)\n        assert_allclose(rgb_jit(data), rgb(data))\n'"
test/color/test_normalize.py,52,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestNormalize:\n    def test_smoke(self):\n        mean = [0.5]\n        std = [0.1]\n        repr = ""Normalize(mean=[0.5], std=[0.1])""\n        assert str(kornia.color.Normalize(mean, std)) == repr\n\n    def test_normalize(self, device):\n\n        # prepare input data\n        data = torch.ones(1, 2, 2).to(device)\n        mean = torch.tensor([0.5]).to(device)\n        std = torch.tensor([2.0]).to(device)\n\n        # expected output\n        expected = torch.tensor([0.25]).repeat(1, 2, 2).view_as(data).to(device)\n\n        f = kornia.color.Normalize(mean, std)\n        assert_allclose(f(data), expected)\n\n    def test_broadcast_normalize(self, device):\n\n        # prepare input data\n        data = torch.ones(2, 3, 1, 1).to(device)\n        data += 2\n\n        mean = torch.tensor([2.0]).to(device)\n        std = torch.tensor([0.5]).to(device)\n\n        # expected output\n        expected = torch.ones_like(data) + 1\n\n        f = kornia.color.Normalize(mean, std)\n        assert_allclose(f(data), expected)\n\n    def test_float_input(self, device):\n\n        data = torch.ones(2, 3, 1, 1).to(device)\n        data += 2\n\n        mean = 2.0\n        std = 0.5\n\n        # expected output\n        expected = torch.ones_like(data) + 1\n\n        f = kornia.color.Normalize(mean, std)\n        assert_allclose(f(data), expected)\n\n    def test_batch_normalize(self, device):\n\n        # prepare input data\n        data = torch.ones(2, 3, 1, 1).to(device)\n        data += 2\n\n        mean = torch.tensor([0.5, 1.0, 2.0]).to(device).repeat(2, 1)\n        std = torch.tensor([2.0, 2.0, 2.0]).to(device).repeat(2, 1)\n\n        # expected output\n        expected = torch.tensor([1.25, 1, 0.5]).to(device).repeat(2, 1, 1).view_as(data)\n\n        f = kornia.color.Normalize(mean, std)\n        assert_allclose(f(data), expected)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(data: torch.Tensor, mean: torch.Tensor, std: torch.Tensor) -> torch.Tensor:\n            return kornia.normalize(data, mean, std)\n\n            data = torch.ones(2, 3, 1, 1).to(device)\n            data += 2\n\n            mean = torch.tensor([0.5, 1.0, 2.0]).repeat(2, 1).to(device)\n            std = torch.tensor([2.0, 2.0, 2.0]).repeat(2, 1).to(device)\n\n            actual = op_script(data, mean, std)\n            expected = kornia.normalize(data, mean, std)\n            assert_allclose(actual, expected)\n\n    def test_gradcheck(self, device):\n\n        # prepare input data\n        data = torch.ones(2, 3, 1, 1).to(device)\n        data += 2\n        mean = torch.tensor([0.5, 1.0, 2.0])\n        std = torch.tensor([2.0, 2.0, 2.0])\n\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n        mean = utils.tensor_to_gradcheck_var(mean)  # to var\n        std = utils.tensor_to_gradcheck_var(std)  # to var\n\n        assert gradcheck(kornia.color.Normalize(mean, std), (data,), raise_exception=True)\n\n    def test_single_value(self, device):\n        # prepare input data\n        mean = torch.tensor(2).to(device)\n        std = torch.tensor(3).to(device)\n        data = torch.ones(2, 3, 256, 313).to(device)\n\n        # expected output\n        expected = (data - mean) / std\n\n        assert_allclose(kornia.normalize(data, mean, std), expected)\n\n\nclass TestDenormalize:\n    def test_smoke(self):\n        mean = [0.5]\n        std = [0.1]\n        repr = ""Denormalize(mean=[0.5], std=[0.1])""\n        assert str(kornia.color.Denormalize(mean, std)) == repr\n\n    def test_denormalize(self):\n\n        # prepare input data\n        data = torch.ones(1, 2, 2)\n        mean = torch.tensor([0.5])\n        std = torch.tensor([2.0])\n\n        # expected output\n        expected = torch.tensor([2.5]).repeat(1, 2, 2).view_as(data)\n\n        f = kornia.color.Denormalize(mean, std)\n        assert_allclose(f(data), expected)\n\n    def test_broadcast_denormalize(self):\n\n        # prepare input data\n        data = torch.ones(2, 3, 1, 1)\n        data += 2\n\n        mean = torch.tensor([2.0])\n        std = torch.tensor([0.5])\n\n        # expected output\n        expected = torch.ones_like(data) + 2.5\n\n        f = kornia.color.Denormalize(mean, std)\n        assert_allclose(f(data), expected)\n\n    def test_float_input(self):\n\n        data = torch.ones(2, 3, 1, 1)\n        data += 2\n\n        mean = 2.0\n        std = 0.5\n\n        # expected output\n        expected = torch.ones_like(data) + 2.5\n\n        f = kornia.color.Denormalize(mean, std)\n        assert_allclose(f(data), expected)\n\n    def test_batch_denormalize(self):\n\n        # prepare input data\n        data = torch.ones(2, 3, 1, 1)\n        data += 2\n\n        mean = torch.tensor([0.5, 1.0, 2.0]).repeat(2, 1)\n        std = torch.tensor([2.0, 2.0, 2.0]).repeat(2, 1)\n\n        # expected output\n        expected = torch.tensor([6.5, 7, 8]).repeat(2, 1, 1).view_as(data)\n\n        f = kornia.color.Denormalize(mean, std)\n        assert_allclose(f(data), expected)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self):\n        @torch.jit.script\n        def op_script(data: torch.Tensor, mean: torch.Tensor, std: torch.Tensor) -> torch.Tensor:\n            return kornia.denormalize(data, mean, std)\n\n            data = torch.ones(2, 3, 1, 1)\n            data += 2\n\n            mean = torch.tensor([0.5, 1.0, 2.0]).repeat(2, 1)\n            std = torch.tensor([2.0, 2.0, 2.0]).repeat(2, 1)\n\n            actual = op_script(data, mean, std)\n            expected = kornia.denormalize(data, mean, std)\n            assert_allclose(actual, expected)\n\n    def test_gradcheck(self):\n\n        # prepare input data\n        data = torch.ones(2, 3, 1, 1)\n        data += 2\n        mean = torch.tensor([0.5, 1.0, 2.0]).double()\n        std = torch.tensor([2.0, 2.0, 2.0]).double()\n\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n\n        assert gradcheck(kornia.color.Denormalize(mean, std), (data,), raise_exception=True)\n\n    def test_single_value(self):\n\n        # prepare input data\n        mean = torch.tensor(2)\n        std = torch.tensor(3)\n        data = torch.ones(2, 3, 256, 313).float()\n\n        # expected output\n        expected = (data * std) + mean\n\n        assert_allclose(kornia.denormalize(data, mean, std), expected)\n'"
test/color/test_rgb.py,29,"b'import kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\nimport pytest\n\n\nclass TestRgbToRgba:\n    def test_smoke(self, device):\n        data = torch.rand(3, 4, 4).to(device)\n        assert kornia.rgb_to_rgba(data, 0.).shape == (4, 4, 4)\n\n    def test_back_and_forth_rgb(self, device):\n        a_val: float = 1.\n        x_rgb = torch.rand(3, 4, 4).to(device)\n        x_rgba = kornia.rgb_to_rgba(x_rgb, a_val)\n        x_rgb_new = kornia.rgba_to_rgb(x_rgba)\n        assert_allclose(x_rgb, x_rgb_new)\n\n    def test_back_and_forth_bgr(self, device):\n        a_val: float = 1.\n        x_bgr = torch.rand(3, 4, 4).to(device)\n        x_rgba = kornia.bgr_to_rgba(x_bgr, a_val)\n        x_bgr_new = kornia.rgba_to_bgr(x_rgba)\n        assert_allclose(x_bgr, x_bgr_new)\n\n    def test_bgr(self, device):\n        a_val: float = 1.\n        x_rgb = torch.rand(3, 4, 4).to(device)\n        x_bgr = kornia.rgb_to_bgr(x_rgb)\n        x_rgba = kornia.rgb_to_rgba(x_rgb, a_val)\n        x_rgba_new = kornia.bgr_to_rgba(x_bgr, a_val)\n        assert_allclose(x_rgba, x_rgba_new)\n\n    def test_single(self, device):\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[2., 2.],\n                              [2., 2.]],\n\n                             [[3., 3.],\n                              [3., 3.]]])  # 3x2x2\n        data = data.to(device)\n\n        aval: float = 0.4\n        expected = torch.tensor([[[1.0, 1.0],\n                                  [1.0, 1.0]],\n\n                                 [[2.0, 2.0],\n                                  [2.0, 2.0]],\n\n                                 [[3.0, 3.0],\n                                  [3.0, 3.0]],\n\n                                 [[0.4, 0.4],\n                                  [0.4, 0.4]]])  # 4x2x2\n        expected = expected.to(device)\n\n        assert_allclose(kornia.rgb_to_rgba(data, aval), expected)\n\n    def test_batch(self, device):\n\n        data = torch.tensor([[[[1., 1.],\n                               [1., 1.]],\n\n                              [[2., 2.],\n                               [2., 2.]],\n\n                              [[3., 3.],\n                               [3., 3.]]],\n\n                             [[[1., 1.],\n                               [1., 1.]],\n\n                              [[2., 2.],\n                               [2., 2.]],\n\n                              [[3., 3.],\n                               [3., 3.]]]])  # 2x3x2x2\n        data = data.to(device)\n\n        aval: float = 45.\n\n        expected = torch.tensor([[[[1.0, 1.0],\n                                   [1.0, 1.0]],\n\n                                  [[2.0, 2.0],\n                                   [2.0, 2.0]],\n\n                                  [[3.0, 3.0],\n                                   [3.0, 3.0]],\n\n                                  [[45., 45.],\n                                   [45., 45.]]],\n\n                                 [[[1.0, 1.0],\n                                   [1.0, 1.0]],\n\n                                  [[2.0, 2.0],\n                                   [2.0, 2.0]],\n\n                                  [[3.0, 3.0],\n                                   [3.0, 3.0]],\n\n                                  [[45., 45.],\n                                   [45., 45.]]]])\n        expected = expected.to(device)\n\n        assert_allclose(kornia.rgb_to_rgba(data, aval), expected)\n\n    def test_gradcheck(self, device):\n        data = torch.rand(1, 3, 2, 2).to(device)\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n        assert gradcheck(kornia.color.RgbToRgba(1.), (data,), raise_exception=True)\n\n\nclass TestBgrToRgb:\n\n    def test_back_and_forth(self, device):\n        data_bgr = torch.rand(1, 3, 3, 2).to(device)\n        data_rgb = kornia.bgr_to_rgb(data_bgr)\n        data_bgr_new = kornia.rgb_to_bgr(data_rgb)\n        assert_allclose(data_bgr, data_bgr_new)\n\n    def test_bgr_to_rgb(self, device):\n\n        data = torch.tensor([[[1., 1.], [1., 1.]],\n                             [[2., 2.], [2., 2.]],\n                             [[3., 3.], [3., 3.]]])  # 3x2x2\n\n        expected = torch.tensor([[[3., 3.], [3., 3.]],\n                                 [[2., 2.], [2., 2.]],\n                                 [[1., 1.], [1., 1.]]])  # 3x2x2\n\n        # move data to the device\n        data = data.to(device)\n        expected = expected.to(device)\n\n        f = kornia.color.BgrToRgb()\n        assert_allclose(f(data), expected)\n\n    def test_batch_bgr_to_rgb(self, device):\n\n        data = torch.tensor([[[[1., 1.], [1., 1.]],\n                              [[2., 2.], [2., 2.]],\n                              [[3., 3.], [3., 3.]]],\n                             [[[1., 1.], [1., 1.]],\n                              [[2., 2.], [2., 2.]],\n                              [[3., 3.], [3., 3.]]]])  # 2x3x2x2\n\n        expected = torch.tensor([[[[3., 3.], [3., 3.]],\n                                  [[2., 2.], [2., 2.]],\n                                  [[1., 1.], [1., 1.]]],\n                                 [[[3., 3.], [3., 3.]],\n                                  [[2., 2.], [2., 2.]],\n                                  [[1., 1.], [1., 1.]]]])  # 2x3x2x2\n\n        # move data to the device\n        data = data.to(device)\n        expected = expected.to(device)\n\n        f = kornia.color.BgrToRgb()\n        out = f(data)\n        assert_allclose(out, expected)\n\n    def test_gradcheck(self, device):\n\n        data = torch.tensor([[[1., 1.], [1., 1.]],\n                             [[2., 2.], [2., 2.]],\n                             [[3., 3.], [3., 3.]]])  # 3x2x2\n\n        data = data.to(device)\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n\n        assert gradcheck(kornia.color.BgrToRgb(), (data,), raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> torch.Tensor:\n            return kornia.bgr_to_rgb(data)\n\n        data = torch.Tensor([[[1., 1.], [1., 1.]],\n                             [[2., 2.], [2., 2.]],\n                             [[3., 3.], [3., 3.]]])  # 3x2x2\n        actual = op_script(data)\n        expected = kornia.bgr_to_rgb(data)\n        assert_allclose(actual, expected)\n\n\nclass TestRgbToBgr:\n\n    def test_back_and_forth(self, device):\n        data_rgb = torch.rand(1, 3, 3, 2).to(device)\n        data_bgr = kornia.rgb_to_bgr(data_rgb)\n        data_rgb_new = kornia.bgr_to_rgb(data_bgr)\n        assert_allclose(data_rgb, data_rgb_new)\n\n    def test_rgb_to_bgr(self, device):\n\n        # prepare input data\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[2., 2.],\n                              [2., 2.]],\n\n                             [[3., 3.],\n                              [3., 3.]]])  # 3x2x2\n\n        expected = torch.tensor([[[3., 3.],\n                                  [3., 3.]],\n\n                                 [[2., 2.],\n                                  [2., 2.]],\n\n                                 [[1., 1.],\n                                  [1., 1.]]])  # 3x2x2\n\n        # move data to the device\n        data = data.to(device)\n        expected = expected.to(device)\n\n        f = kornia.color.RgbToBgr()\n        assert_allclose(f(data), expected)\n\n    def test_gradcheck(self, device):\n\n        # prepare input data\n        data = torch.tensor([[[1., 1.],\n                              [1., 1.]],\n\n                             [[2., 2.],\n                              [2., 2.]],\n\n                             [[3., 3.],\n                              [3., 3.]]])  # 3x2x2\n\n        data = data.to(device)\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n\n        assert gradcheck(kornia.color.RgbToBgr(), (data,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> torch.Tensor:\n            return kornia.rgb_to_bgr(data)\n\n        data = torch.Tensor([[[1., 1.], [1., 1.]],\n                             [[2., 2.], [2., 2.]],\n                             [[3., 3.], [3., 3.]]])  # 3x2x\n        actual = op_script(data)\n        expected = kornia.rgb_to_bgr(data)\n        assert_allclose(actual, expected)\n\n    def test_batch_rgb_to_bgr(self, device):\n\n        # prepare input data\n        data = torch.tensor([[[[1., 1.],\n                               [1., 1.]],\n\n                              [[2., 2.],\n                               [2., 2.]],\n\n                              [[3., 3.],\n                               [3., 3.]]],\n\n                             [[[1., 1.],\n                               [1., 1.]],\n\n                              [[2., 2.],\n                               [2., 2.]],\n\n                              [[3., 3.],\n                               [3., 3.]]]])  # 2x3x2x2\n\n        expected = torch.tensor([[[[3., 3.],\n                                   [3., 3.]],\n\n                                  [[2., 2.],\n                                   [2., 2.]],\n\n                                  [[1., 1.],\n                                   [1., 1.]]],\n\n                                 [[[3., 3.],\n                                   [3., 3.]],\n\n                                  [[2., 2.],\n                                   [2., 2.]],\n\n                                  [[1., 1.],\n                                   [1., 1.]]]])  # 2x3x2x2\n\n        # move data to the device\n        data = data.to(device)\n        expected = expected.to(device)\n\n        f = kornia.color.RgbToBgr()\n        out = f(data)\n        assert_allclose(out, expected)\n'"
test/color/test_xyz.py,16,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestRgbToXyz:\n\n    @pytest.mark.parametrize(""batch_size"", [None, 1, 2, 5])\n    def test_rgb_to_xyz(self, batch_size, device):\n\n        data = torch.tensor([[[0.9637, 0.0586, 0.6470, 0.6212, 0.9622],\n                              [0.8293, 0.4858, 0.8953, 0.2607, 0.3250],\n                              [0.5314, 0.4189, 0.8388, 0.8065, 0.2211],\n                              [0.9682, 0.2928, 0.4118, 0.2533, 0.0455]],\n\n                             [[0.6936, 0.3457, 0.9466, 0.9937, 0.2692],\n                              [0.7485, 0.7320, 0.8323, 0.6889, 0.4831],\n                              [0.1865, 0.7439, 0.1366, 0.8858, 0.2077],\n                              [0.6227, 0.6140, 0.3936, 0.5024, 0.4157]],\n\n                             [[0.6477, 0.9269, 0.7531, 0.7349, 0.9485],\n                                 [0.4264, 0.8539, 0.9830, 0.2269, 0.1138],\n                                 [0.3988, 0.1605, 0.6220, 0.0546, 0.1106],\n                                 [0.2128, 0.5673, 0.0781, 0.1431, 0.3310]]])\n\n        # Reference output generated using OpenCV: cv2.cvtColor(data, cv2.COLOR_RGB2XYZ)\n        xyz_ref = torch.tensor([[[0.7623584, 0.31501925, 0.7412189, 0.7441359, 0.66425407],\n                                 [0.6866283, 0.61618143, 0.84423876, 0.39480132, 0.32732624],\n                                 [0.3578189, 0.4677382, 0.50703406, 0.6592388, 0.18541752],\n                                 [0.6603961, 0.44267434, 0.32468265, 0.30994105, 0.22713262]],\n\n                                [[0.7477299, 0.32658678, 0.86891913, 0.89580274, 0.4656054],\n                                 [0.7424382, 0.6884378, 0.8565741, 0.5644922, 0.4228247],\n                                 [0.2751717, 0.63267857, 0.3209684, 0.8089483, 0.20354219],\n                                 [0.6665957, 0.5423198, 0.37470126, 0.42349333, 0.33085644]],\n\n                                [[0.7167665, 0.92310345, 0.8409531, 0.82877415, 0.95198023],\n                                 [0.51042646, 0.9080406, 1.0505873, 0.30275896, 0.17200153],\n                                 [0.4114541, 0.24927814, 0.62354034, 0.17305644, 0.13412625],\n                                 [0.29514894, 0.6179093, 0.12908883, 0.20075734, 0.3649534]]])\n\n        data.to(device)\n        xyz_ref.to(device)\n\n        if batch_size is not None:\n            data = data.repeat(batch_size, 1, 1, 1)\n            xyz_ref = xyz_ref.repeat(batch_size, 1, 1, 1)\n\n        xyz = kornia.color.RgbToXyz()\n        out = xyz(data)\n        assert_allclose(out, xyz_ref)\n\n    def test_grad(self, device):\n\n        data = torch.rand(2, 3, 4, 5).to(device)\n        data = utils.tensor_to_gradcheck_var(data)\n        assert gradcheck(kornia.color.RgbToXyz(), (data,), raise_exception=True)\n\n    @pytest.mark.parametrize(""input_shape"", [(2, 2), (3, 3, 5, 3, 3)])\n    def test_shape(self, input_shape, device):\n        with pytest.raises(ValueError):\n            xyz = kornia.color.RgbToXyz()\n            out = xyz(torch.ones(*input_shape).to(device))\n\n    def test_inverse(self, device):\n        data = torch.rand(3, 4, 5).to(device)\n        xyz = kornia.color.XyzToRgb()\n        rgb = kornia.color.RgbToXyz()\n\n        data_out = xyz(rgb(data))\n        assert_allclose(data_out, data)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n\n        data = torch.rand((2, 3, 4, 5)).to(device)\n        xyz = kornia.color.rgb_to_xyz\n        xyz_jit = torch.jit.script(kornia.color.rgb_to_xyz)\n        assert_allclose(xyz_jit(data), xyz(data))\n\n\nclass TestXyzToRgb:\n\n    @pytest.mark.parametrize(""batch_size"", [None, 1, 2, 5])\n    def test_xyz_to_rgb(self, batch_size, device):\n\n        data = torch.tensor([[[0.6315228, 0.4196397, 0.33123854, 0.3277169, 0.20501322],\n                              [0.44133404, 0.4823162, 0.528561, 0.51644784, 0.28001237],\n                              [0.31131047, 0.8453884, 0.4486181, 0.6015828, 0.42048606],\n                              [0.5472367, 0.48154795, 0.36668795, 0.39913517, 0.40271503]],\n\n                             [[0.79137707, 0.501063, 0.3700857, 0.57410157, 0.15295872],\n                              [0.570678, 0.76664513, 0.48567873, 0.47680324, 0.2583247],\n                              [0.38080955, 0.9315215, 0.4404478, 0.50659215, 0.5984908],\n                              [0.5388581, 0.76993656, 0.4027568, 0.5952581, 0.68663263]],\n\n                             [[0.86013114, 0.17629854, 0.83010703, 0.27881518, 0.30543375],\n                                 [0.17009716, 0.61201245, 0.33521807, 0.15526368, 0.7401195],\n                                 [0.34011865, 0.6541383, 0.96909684, 0.43090558, 0.70467836],\n                                 [0.6738866, 0.47461915, 0.91508406, 0.44147202, 0.14099535]]])\n        # Reference output generated using OpenCV: cv2.cvtColor(data, cv2.COLOR_XYZ2RGB)\n        rgb_ref = torch.tensor([[[0.4011656, 0.5017336, 0.09065688, 0.04048038, 0.2769511],\n                                 [0.46811658, 0.07937729, 0.79911184, 0.8632158, 0.1413149],\n                                 [0.2538725, 0.98146427, 0.29357457, 0.95588684, 0.09129933],\n                                 [0.6090472, 0.14032376, 0.11294556, 0.15829873, 0.1792411]],\n\n                                [[0.90825266, 0.54057753, 0.40771842, 0.7709542, 0.1009315],\n                                 [0.64988965, 0.99616426, 0.41274834, 0.40036052, 0.24396756],\n                                 [0.42678973, 0.95531154, 0.43172216, 0.3851813, 0.7444883],\n                                 [0.5084846, 0.99737406, 0.4381809, 0.7481805, 0.9036418]],\n\n                                [[0.78309417, 0.10751611, 0.82060075, 0.19588977, 0.3031369],\n                                 [0.08796211, 0.51749885, 0.28474382, 0.09561294, 0.74540937],\n                                 [0.2992335, 0.5486014, 0.95973116, 0.38571155, 0.64634556],\n                                 [0.6330101, 0.3715171, 0.90575427, 0.36752605, 0.03138365]]])\n\n        data.to(device)\n        rgb_ref.to(device)\n\n        if batch_size is not None:\n            data = data.repeat(batch_size, 1, 1, 1)\n            rgb_ref = rgb_ref.repeat(batch_size, 1, 1, 1)\n\n        rgb = kornia.color.XyzToRgb()\n        rgb_out = rgb(data)\n\n        assert_allclose(rgb_out, rgb_ref)\n\n    def test_grad(self, device):\n\n        data = torch.rand(2, 3, 4, 5).to(device)\n        data = utils.tensor_to_gradcheck_var(data)\n        assert gradcheck(kornia.color.XyzToRgb(), (data,), raise_exception=True)\n\n    def test_inverse(self, device):\n        data = torch.rand(3, 4, 5).to(device)\n        xyz = kornia.color.XyzToRgb()\n        rgb = kornia.color.RgbToXyz()\n\n        data_out = rgb(xyz(data))\n        assert_allclose(data_out, data)\n\n    @pytest.mark.parametrize(""input_shape"", [([2, 2],), ([3, 3, 7, 3, 3],)])\n    def test_shape(self, input_shape, device):\n        with pytest.raises(ValueError):\n            rgb = kornia.color.XyzToRgb()\n            out = rgb(torch.ones(*input_shape).to(device))\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n\n        data = torch.rand((2, 3, 4, 5)).to(device)\n        rgb = kornia.color.xyz_to_rgb\n        rgb_jit = torch.jit.script(kornia.color.xyz_to_rgb)\n        assert_allclose(rgb_jit(data), rgb(data))\n'"
test/color/test_ycbcr.py,22,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport numpy as np\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestRgbToYcbcr:\n\n    def test_rgb_to_ycbcr(self):\n        data = torch.tensor([[[0.7925, 0.8704, 0.2771, 0.3279, 0.0193],\n                              [0.2483, 0.6212, 0.9859, 0.5044, 0.5621],\n                              [0.5762, 0.3959, 0.2931, 0.2669, 0.0243],\n                              [0.6989, 0.0529, 0.8344, 0.6523, 0.8980],\n                              [0.5181, 0.9341, 0.2172, 0.0520, 0.7266]],\n                             [[0.8413, 0.0284, 0.3625, 0.8864, 0.5595],\n                              [0.3791, 0.0235, 0.4251, 0.0619, 0.5270],\n                              [0.3516, 0.8005, 0.9571, 0.4113, 0.6119],\n                              [0.0632, 0.8836, 0.0261, 0.1550, 0.4923],\n                              [0.2332, 0.7044, 0.9514, 0.2443, 0.2818]],\n                             [[0.6899, 0.2063, 0.3179, 0.8989, 0.4378],\n                              [0.0384, 0.5230, 0.6416, 0.9749, 0.7863],\n                              [0.8577, 0.3115, 0.2375, 0.5446, 0.9837],\n                              [0.3213, 0.6618, 0.5977, 0.3999, 0.4962],\n                              [0.1385, 0.5831, 0.9756, 0.8714, 0.8017]]])  # 3x5x5\n\n        y_expected = torch.tensor([[0.8094629, 0.30041942, 0.33188093, 0.7208117, 0.38409105],\n                                   [0.30114722, 0.2591345, 0.6174494, 0.29830942, 0.5670594],\n                                   [0.47643244, 0.623786, 0.6765313, 0.38332558, 0.47857922],\n                                   [0.2826805, 0.60994685, 0.33298355, 0.3316514, 0.6140681],\n                                   [0.30756146, 0.75926465, 0.7346588, 0.25828302, 0.4740529]])\n        cr_expected = torch.tensor([[0.48791525, 0.9063825, 0.46094114, 0.21983841, 0.23989305],\n                                    [0.46231723, 0.7581379, 0.7626976, 0.6469568, 0.496467],\n                                    [0.5711212, 0.33752257, 0.22661471, 0.4169921, 0.17608929],\n                                    [0.7967522, 0.10283372, 0.85753804, 0.72865105, 0.70245713],\n                                    [0.65009415, 0.6246666, 0.13107029, 0.35291404, 0.6800583]])\n        cb_expected = torch.tensor([[0.43258172, 0.4469004, 0.492101, 0.60042214, 0.53028214],\n                                    [0.35180065, 0.64882994, 0.51362985, 0.8815981, 0.62366176],\n                                    [0.71505237, 0.32387614, 0.2523859, 0.5909421, 0.78490996],\n                                    [0.52178127, 0.529264, 0.64931786, 0.53848785, 0.43353173],\n                                    [0.4046721, 0.40065458, 0.6358658, 0.8458253, 0.6847739]])\n\n        # Kornia\n        f = kornia.color.RgbToYcbcr()\n        result = f(data)\n\n        y = result[0, :, :]\n        cb = result[1, :, :]\n        cr = result[2, :, :]\n\n        assert_allclose(y, y_expected)\n        assert_allclose(cb, cb_expected)\n        assert_allclose(cr, cr_expected)\n\n    def test_batch_rgb_to_ycbcr(self):\n        data = torch.tensor([[[0.7925, 0.8704, 0.2771, 0.3279, 0.0193],\n                              [0.2483, 0.6212, 0.9859, 0.5044, 0.5621],\n                              [0.5762, 0.3959, 0.2931, 0.2669, 0.0243],\n                              [0.6989, 0.0529, 0.8344, 0.6523, 0.8980],\n                              [0.5181, 0.9341, 0.2172, 0.0520, 0.7266]],\n                             [[0.8413, 0.0284, 0.3625, 0.8864, 0.5595],\n                              [0.3791, 0.0235, 0.4251, 0.0619, 0.5270],\n                              [0.3516, 0.8005, 0.9571, 0.4113, 0.6119],\n                              [0.0632, 0.8836, 0.0261, 0.1550, 0.4923],\n                              [0.2332, 0.7044, 0.9514, 0.2443, 0.2818]],\n                             [[0.6899, 0.2063, 0.3179, 0.8989, 0.4378],\n                              [0.0384, 0.5230, 0.6416, 0.9749, 0.7863],\n                              [0.8577, 0.3115, 0.2375, 0.5446, 0.9837],\n                              [0.3213, 0.6618, 0.5977, 0.3999, 0.4962],\n                              [0.1385, 0.5831, 0.9756, 0.8714, 0.8017]]])  # 3x5x5\n\n        # OpenCV\n        expected = torch.tensor([[[0.8094629, 0.30041942, 0.33188093, 0.7208117, 0.38409105],\n                                  [0.30114722, 0.2591345, 0.6174494, 0.29830942, 0.5670594],\n                                  [0.47643244, 0.623786, 0.6765313, 0.38332558, 0.47857922],\n                                  [0.2826805, 0.60994685, 0.33298355, 0.3316514, 0.6140681],\n                                  [0.30756146, 0.75926465, 0.7346588, 0.25828302, 0.4740529]],\n                                 [[0.43258172, 0.4469004, 0.492101, 0.60042214, 0.53028214],\n                                  [0.35180065, 0.64882994, 0.51362985, 0.8815981, 0.62366176],\n                                  [0.71505237, 0.32387614, 0.2523859, 0.5909421, 0.78490996],\n                                  [0.52178127, 0.529264, 0.64931786, 0.53848785, 0.43353173],\n                                  [0.4046721, 0.40065458, 0.6358658, 0.8458253, 0.6847739]],\n                                 [[0.48791525, 0.9063825, 0.46094114, 0.21983841, 0.23989305],\n                                  [0.46231723, 0.7581379, 0.7626976, 0.6469568, 0.496467],\n                                  [0.5711212, 0.33752257, 0.22661471, 0.4169921, 0.17608929],\n                                  [0.7967522, 0.10283372, 0.85753804, 0.72865105, 0.70245713],\n                                  [0.65009415, 0.6246666, 0.13107029, 0.35291404, 0.6800583]]])\n\n        # Kornia\n        f = kornia.color.RgbToYcbcr()\n        data = data.repeat(2, 1, 1, 1)  # 2x3x5x5\n        expected = expected.repeat(2, 1, 1, 1)  # 2x3x5x5\n        assert_allclose(f(data), expected)\n\n    def test_gradcheck(self):\n        data = torch.rand(3, 5, 5)  # 3x2x2\n\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n\n        assert gradcheck(kornia.color.RgbToYcbcr(), (data,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> torch.Tensor:\n            return kornia.rgb_to_ycbcr(data)\n            data = torch.tensor([[[[21., 22.],\n                                   [22., 22.]],\n\n                                  [[13., 14.],\n                                   [14., 14.]],\n\n                                  [[8., 8.],\n                                   [8., 8.]]]])  # 3x2x2\n\n            actual = op_script(data)\n            expected = kornia.rgb_to_ycbcr(data)\n            assert_allclose(actual, expected)\n\n\nclass TestYcbcrToRgb:\n\n    def test_ycbcr_to_rgb(self):\n        data = torch.tensor([[[0.9892, 0.2620, 0.4184, 0.5286, 0.2793],\n                              [0.0722, 0.8828, 0.8714, 0.0657, 0.7798],\n                              [0.8118, 0.7522, 0.0260, 0.8811, 0.5226],\n                              [0.0644, 0.3648, 0.4448, 0.4202, 0.7316],\n                              [0.9138, 0.1956, 0.4257, 0.6381, 0.1353]],\n                             [[0.7408, 0.8529, 0.5119, 0.0220, 0.0226],\n                              [0.8963, 0.5652, 0.9568, 0.6977, 0.8221],\n                              [0.4645, 0.0478, 0.4952, 0.5492, 0.4861],\n                              [0.9980, 0.9978, 0.0281, 0.5283, 0.8146],\n                              [0.7789, 0.2663, 0.6437, 0.6926, 0.5627]],\n                             [[0.7377, 0.7152, 0.3080, 0.8515, 0.4841],\n                              [0.7192, 0.3297, 0.7337, 0.0230, 0.2464],\n                              [0.6399, 0.8998, 0.3838, 0.3043, 0.3774],\n                              [0.1281, 0.6731, 0.4218, 0.3963, 0.8541],\n                              [0.2245, 0.2413, 0.2351, 0.9522, 0.8158]]])  # 3x5x5\n\n        # OpenCV\n        # expected = cv2.cvtColor(data_cv, cv2.COLOR_YCrCb2RGB)\n        data_cv = data.numpy().transpose(1, 2, 0).copy()\n        data_cv[..., 1:] = data_cv[..., -1:0:-1]\n\n        r_expected = torch.tensor([[1.3226931, 0.5639256, 0.14902398, 1.0217545, 0.2569923],\n                                   [0.37973762, 0.64386904, 1.1992811, -0.603531, 0.4239992],\n                                   [1.0080798, 1.3131194, -0.1370286, 0.60653293, 0.3505922],\n                                   [-0.4573757, 0.6076593, 0.33508536, 0.27470887, 1.2284023],\n                                   [0.52727354, -0.1673561, 0.05404532, 1.2725366, 0.5783674]])\n        g_expected = torch.tensor([[0.736647, -0.01305042, 0.55139434, 0.44206098, 0.4548782],\n                                   [-0.22063601, 0.98196536, 0.54739904, 0.33826917, 0.850068],\n                                   [0.72412336, 0.6222996, 0.110618, 1.0039049, 0.614918],\n                                   [0.15862459, 0.0699634, 0.66296846, 0.4845066, 0.3705502],\n                                   [1.0145653, 0.46070462, 0.5654058, 0.24897486, -0.11174999]])\n        b_expected = torch.tensor([[1.4161384, 0.88769174, 0.4394987, -0.31889397, -0.5671302],\n                                   [0.77483994, 0.99839956, 1.6813064, 0.41622213, 1.3508832],\n                                   [0.7488585, -0.04955059, 0.01748962, 0.9683316, 0.49795526],\n                                   [0.9473541, 1.2473994, -0.3918787, 0.47037587, 1.2893858],\n                                   [1.4082898, -0.21875012, 0.6804801, 0.9795798, 0.24646705]])\n\n        # Kornia\n        f = kornia.color.YcbcrToRgb()\n        result = f(data)\n\n        r = result[0, :, :]\n        g = result[1, :, :]\n        b = result[2, :, :]\n\n        assert_allclose(r, r_expected)\n        assert_allclose(g, g_expected)\n        assert_allclose(b, b_expected)\n\n    def test_batch_ycbcr_to_rgb(self):\n        data = torch.tensor([[[0.9892, 0.2620, 0.4184, 0.5286, 0.2793],\n                              [0.0722, 0.8828, 0.8714, 0.0657, 0.7798],\n                              [0.8118, 0.7522, 0.0260, 0.8811, 0.5226],\n                              [0.0644, 0.3648, 0.4448, 0.4202, 0.7316],\n                              [0.9138, 0.1956, 0.4257, 0.6381, 0.1353]],\n                             [[0.7408, 0.8529, 0.5119, 0.0220, 0.0226],\n                              [0.8963, 0.5652, 0.9568, 0.6977, 0.8221],\n                              [0.4645, 0.0478, 0.4952, 0.5492, 0.4861],\n                              [0.9980, 0.9978, 0.0281, 0.5283, 0.8146],\n                              [0.7789, 0.2663, 0.6437, 0.6926, 0.5627]],\n                             [[0.7377, 0.7152, 0.3080, 0.8515, 0.4841],\n                              [0.7192, 0.3297, 0.7337, 0.0230, 0.2464],\n                              [0.6399, 0.8998, 0.3838, 0.3043, 0.3774],\n                              [0.1281, 0.6731, 0.4218, 0.3963, 0.8541],\n                              [0.2245, 0.2413, 0.2351, 0.9522, 0.8158]]])  # 3x5x5\n\n        # OpenCV\n        # expected = cv2.cvtColor(data_cv, cv2.COLOR_YCrCb2RGB)\n        data_cv = data.numpy().transpose(1, 2, 0).copy()\n        data_cv[..., 1:] = data_cv[..., -1:0:-1]\n\n        expected = torch.tensor([[[1.3226931, 0.5639256, 0.14902398, 1.0217545, 0.2569923],\n                                  [0.37973762, 0.64386904, 1.1992811, -0.603531, 0.4239992],\n                                  [1.0080798, 1.3131194, -0.1370286, 0.60653293, 0.3505922],\n                                  [-0.4573757, 0.6076593, 0.33508536, 0.27470887, 1.2284023],\n                                  [0.52727354, -0.1673561, 0.05404532, 1.2725366, 0.5783674]],\n                                 [[0.736647, -0.01305042, 0.55139434, 0.44206098, 0.4548782],\n                                  [-0.22063601, 0.98196536, 0.54739904, 0.33826917, 0.850068],\n                                  [0.72412336, 0.6222996, 0.110618, 1.0039049, 0.614918],\n                                  [0.15862459, 0.0699634, 0.66296846, 0.4845066, 0.3705502],\n                                  [1.0145653, 0.46070462, 0.5654058, 0.24897486, -0.11174999]],\n                                 [[1.4161384, 0.88769174, 0.4394987, -0.31889397, -0.5671302],\n                                  [0.77483994, 0.99839956, 1.6813064, 0.41622213, 1.3508832],\n                                  [0.7488585, -0.04955059, 0.01748962, 0.9683316, 0.49795526],\n                                  [0.9473541, 1.2473994, -0.3918787, 0.47037587, 1.2893858],\n                                  [1.4082898, -0.21875012, 0.6804801, 0.9795798, 0.24646705]]])\n\n        # Kornia\n        f = kornia.color.YcbcrToRgb()\n        data = data.repeat(2, 1, 1, 1)  # 2x3x5x5\n        expected = expected.repeat(2, 1, 1, 1)  # 2x3x5x5\n        assert_allclose(f(data), expected)\n\n    def test_gradcheck(self):\n        data = torch.rand(3, 5, 5)  # 3x5x5\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n\n        assert gradcheck(kornia.color.YcbcrToRgb(), (data,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> torch.Tensor:\n            return kornia.ycbcr_to_rgb(data)\n\n            data = torch.tensor([[[[21., 22.],\n                                   [22., 22.]],\n\n                                  [[13., 14.],\n                                   [14., 14.]],\n\n                                  [[8., 8.],\n                                   [8., 8.]]]])  # 3x2x2\n\n            actual = op_script(data)\n            expected = kornia.ycbcr_to_rgb(data)\n            assert_allclose(actual, expected)\n'"
test/color/test_yuv.py,10,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestRgbYuvConversion:\n    # Parameterize for CHW and NCHW shapes\n    @pytest.mark.parametrize(\'shape\', ((3, 4, 5), (2, 3, 4, 5)))\n    # RGB to YUV and YUV to RGB should be inverse operations\n    def test_inverse_operations(self, device, shape):\n        input = torch.rand(*shape).to(device)\n        yuv_to_rgb_converter = kornia.color.YuvToRgb()\n        rgb_to_yuv_converter = kornia.color.RgbToYuv()\n\n        assert_allclose(input, yuv_to_rgb_converter(rgb_to_yuv_converter(input)), rtol=0.005, atol=0.005)\n        assert_allclose(input, rgb_to_yuv_converter(yuv_to_rgb_converter(input)), rtol=0.005, atol=0.005)\n\n    def test_gradcheck(self, device):\n\n        # prepare input data\n        data = torch.tensor([[[0.1, 0.2],\n                              [0.1, 0.1]],\n\n                             [[0.2, 0.5],\n                              [0.4, 0.2]],\n\n                             [[0.3, 0.3],\n                              [0.5, 0.5]]]).to(device)  # 3x2x2\n\n        data = utils.tensor_to_gradcheck_var(data)  # to var\n\n        assert gradcheck(kornia.color.YuvToRgb(), (data,),\n                         raise_exception=True)\n        assert gradcheck(kornia.color.RgbToYuv(), (data,),\n                         raise_exception=True)\n\n    def test_rgb_to_yuv_shape(self, device):\n        channels, height, width = 3, 4, 5\n        img = torch.ones(channels, height, width).to(device)\n        assert kornia.rgb_to_yuv(img).shape == (channels, height, width)\n\n    def test_rgb_to_yuv_batch_shape(self, device):\n        batch_size, channels, height, width = 2, 3, 4, 5\n        img = torch.ones(batch_size, channels, height, width).to(device)\n        assert kornia.rgb_to_yuv(img).shape == \\\n            (batch_size, channels, height, width)\n\n    def test_yuv_to_rgb_shape(self, device):\n        channels, height, width = 3, 4, 5\n        img = torch.ones(channels, height, width).to(device)\n        assert kornia.yuv_to_rgb(img).shape == (channels, height, width)\n\n    def test_yuv_to_rgb_batch_shape(self, device):\n        batch_size, channels, height, width = 2, 3, 4, 5\n        img = torch.ones(batch_size, channels, height, width).to(device)\n        assert kornia.yuv_to_rgb(img).shape == \\\n            (batch_size, channels, height, width)\n\n    def test_rgb_to_yuv_type(self):\n        with pytest.raises(TypeError):\n            out = kornia.rgb_to_yuv(1)\n\n    def test_yuv_to_rbg_type(self):\n        with pytest.raises(TypeError):\n            out = kornia.yuv_to_rgb(1)\n\n    @pytest.mark.parametrize(""bad_input_shapes"", [([2, 2],), ([3, 3, 3, 3, 3],), ([2, 2, 2],), ([2, 2, 2, 2],)])\n    def test_rgb_to_yuv_shape_bad(self, bad_input_shapes):\n        with pytest.raises(ValueError):\n            out = kornia.rgb_to_yuv(torch.ones(*bad_input_shapes))\n\n    @pytest.mark.parametrize(""bad_input_shapes"", [([2, 2],), ([3, 3, 3, 3, 3],), ([2, 2, 2],), ([2, 2, 2, 2],)])\n    def test_yuv_to_rbg_shape_bad(self, bad_input_shapes):\n        with pytest.raises(ValueError):\n            out = kornia.yuv_to_rgb(torch.ones(*bad_input_shapes))\n\n    # TODO add cv2 comparision test\n'"
test/color/test_zca.py,27,"b'import math\nimport pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestZCA:\n\n    @pytest.mark.parametrize(""unbiased"", [True, False])\n    def test_zca_unbiased(self, unbiased, device):\n\n        data = torch.tensor([[0, 1],\n                             [1, 0],\n                             [-1, 0],\n                             [0, -1]], dtype=torch.float32).to(device)\n\n        if unbiased:\n            expected = torch.sqrt(1.5 * torch.abs(data)) * torch.sign(data)\n        else:\n            expected = torch.sqrt(2 * torch.abs(data)) * torch.sign(data)\n\n        expected = expected.to(device)\n\n        zca = kornia.color.ZCAWhitening(unbiased=unbiased).fit(data)\n\n        actual = zca(data)\n\n        assert_allclose(actual, expected)\n\n    @pytest.mark.parametrize(""dim"", [0, 1])\n    def test_dim_args(self, dim, device):\n\n        data = torch.tensor([[0, 1],\n                             [1, 0],\n                             [-1, 0],\n                             [0, -1]], dtype=torch.float32).to(device)\n\n        if dim == 1:\n            expected = torch.tensor([[-0.35360718, 0.35360718],\n                                     [0.35351562, -0.35351562],\n                                     [-0.35353088, 0.35353088],\n                                     [0.35353088, -0.35353088]], dtype=torch.float32)\n        elif dim == 0:\n            expected = torch.tensor([[0., 1.2247448],\n                                     [1.2247448, 0.],\n                                     [-1.2247448, 0.],\n                                     [0., -1.2247448]], dtype=torch.float32)\n        expected = expected.to(device)\n\n        zca = kornia.color.ZCAWhitening(dim=dim)\n        actual = zca(data, True)\n\n        assert_allclose(actual, expected)\n\n    @pytest.mark.parametrize(""input_shape"", [(15, 2, 2, 2), (10, 4), (20, 3, 2, 2)])\n    def test_identity(self, input_shape, device):\n        """"""\n\n        Assert that data can be recovered by the inverse transform\n\n        """"""\n\n        data = torch.randn(*input_shape, dtype=torch.float32).to(device)\n\n        zca = kornia.color.ZCAWhitening(compute_inv=True).fit(data)\n\n        data_w = zca(data)\n\n        data_hat = zca.inverse_transform(data_w)\n\n        assert_allclose(data, data_hat)\n\n    def test_grad_zca_individual_transforms(self, device):\n        """"""\n\n        Checks if the gradients of the transforms are correct w.r.t to the input data\n\n        """"""\n\n        data = torch.tensor([[2, 0],\n                             [0, 1],\n                             [-2, 0],\n                             [0, -1]],\n                            dtype=torch.float32).to(device)\n\n        data = utils.tensor_to_gradcheck_var(data)\n\n        def zca_T(x):\n            return kornia.color.zca_mean(x)[0]\n\n        def zca_mu(x):\n            return kornia.color.zca_mean(x)[1]\n\n        def zca_T_inv(x):\n            return kornia.color.zca_mean(x, return_inverse=True)[2]\n\n        assert gradcheck(zca_T, (data,), raise_exception=True)\n        assert gradcheck(zca_mu, (data,), raise_exception=True)\n        assert gradcheck(zca_T_inv, (data,), raise_exception=True)\n\n    def test_grad_zca_with_fit(self, device):\n\n        data = torch.tensor([[2, 0],\n                             [0, 1],\n                             [-2, 0],\n                             [0, -1]],\n                            dtype=torch.float32).to(device)\n\n        data = utils.tensor_to_gradcheck_var(data)\n\n        def zca_fit(x):\n            zca = kornia.color.ZCAWhitening(detach_transforms=False)\n            return zca(x, include_fit=True)\n\n        assert gradcheck(zca_fit, (data,), raise_exception=True)\n\n    def test_grad_detach_zca(self, device):\n\n        data = torch.tensor([[1, 0],\n                             [0, 1],\n                             [-2, 0],\n                             [0, -1]],\n                            dtype=torch.float32).to(device)\n\n        data = utils.tensor_to_gradcheck_var(data)\n        zca = kornia.color.ZCAWhitening()\n\n        zca.fit(data)\n\n        assert gradcheck(zca,\n                         (data,), raise_exception=True)\n\n    def test_not_fitted(self, device):\n\n        with pytest.raises(RuntimeError):\n            data = torch.rand(10, 2).to(device)\n\n            zca = kornia.color.ZCAWhitening()\n            zca(data)\n\n    def test_not_fitted_inv(self, device):\n\n        with pytest.raises(RuntimeError):\n            data = torch.rand(10, 2).to(device)\n\n            zca = kornia.color.ZCAWhitening()\n            zca.inverse_transform(data)\n\n    def test_jit(self, device, dtype):\n\n        data = torch.rand((10, 3, 1, 2)).to(device)\n        zca = kornia.color.ZCAWhitening().fit(data)\n        zca_jit = kornia.color.ZCAWhitening().fit(data)\n        zca_jit = torch.jit.script(zca_jit)\n        assert_allclose(zca_jit(data), zca(data))\n\n    @pytest.mark.parametrize(""unbiased"", [True, False])\n    def test_zca_whiten_func_unbiased(self, unbiased, device):\n\n        data = torch.tensor([[0, 1],\n                             [1, 0],\n                             [-1, 0],\n                             [0, -1]], dtype=torch.float32).to(device)\n\n        if unbiased:\n            expected = torch.sqrt(1.5 * torch.abs(data)) * torch.sign(data)\n        else:\n            expected = torch.sqrt(2 * torch.abs(data)) * torch.sign(data)\n\n        expected = expected.to(device)\n\n        actual = kornia.zca_whiten(data, unbiased=unbiased)\n\n        assert_allclose(actual, expected)\n'"
test/feature/test_affine_shape_estimator.py,18,"b'import pytest\nimport kornia.testing as utils  # test utils\nimport kornia\n\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\nfrom kornia.feature.affine_shape import *\n\n\nclass TestPatchAffineShapeEstimator:\n    def test_shape(self, device):\n        inp = torch.rand(1, 1, 32, 32, device=device)\n        ori = PatchAffineShapeEstimator(32).to(device)\n        ang = ori(inp)\n        assert ang.shape == torch.Size([1, 1, 3])\n\n    def test_shape_batch(self, device):\n        inp = torch.rand(2, 1, 32, 32, device=device)\n        ori = PatchAffineShapeEstimator(32).to(device)\n        ang = ori(inp)\n        assert ang.shape == torch.Size([2, 1, 3])\n\n    def test_print(self, device):\n        sift = PatchAffineShapeEstimator(32)\n        sift.__repr__()\n\n    def test_toy(self, device):\n        aff = PatchAffineShapeEstimator(19).to(device)\n        inp = torch.zeros(1, 1, 19, 19, device=device)\n        inp[:, :, 5:-5, 1:-1] = 1\n        abc = aff(inp)\n        expected = torch.tensor([[[0.4146, 0.0000, 1.0000]]], device=device)\n        assert_allclose(abc, expected, atol=1e-4, rtol=1e-4)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 1, 13, 13\n        ori = PatchAffineShapeEstimator(width).to(device)\n        patches = torch.rand(batch_size, channels, height, width, device=device)\n        patches = utils.tensor_to_gradcheck_var(patches)  # to var\n        assert gradcheck(ori, (patches, ),\n                         raise_exception=True, nondet_tol=1e-4)\n\n\nclass TestLAFAffineShapeEstimator:\n    def test_shape(self, device):\n        inp = torch.rand(1, 1, 32, 32, device=device)\n        laf = torch.rand(1, 1, 2, 3, device=device)\n        ori = LAFAffineShapeEstimator().to(device)\n        out = ori(laf, inp)\n        assert out.shape == laf.shape\n\n    def test_shape_batch(self, device):\n        inp = torch.rand(2, 1, 32, 32, device=device)\n        laf = torch.rand(2, 34, 2, 3, device=device)\n        ori = LAFAffineShapeEstimator().to(device)\n        out = ori(laf, inp)\n        assert out.shape == laf.shape\n\n    def test_print(self, device):\n        sift = LAFAffineShapeEstimator()\n        sift.__repr__()\n\n    def test_toy(self, device):\n        aff = LAFAffineShapeEstimator(32).to(device)\n        inp = torch.zeros(1, 1, 32, 32, device=device)\n        inp[:, :, 15:-15, 9:-9] = 1\n        laf = torch.tensor([[[[20., 0., 16.], [0., 20., 16.]]]], device=device)\n        new_laf = aff(laf, inp)\n        expected = torch.tensor([[[[36.643, 0., 16.], [0., 10.916, 16.]]]], device=device)\n        assert_allclose(new_laf, expected, atol=1e-4, rtol=1e-4)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 1, 40, 40\n        patches = torch.rand(batch_size, channels, height, width, device=device)\n        patches = utils.tensor_to_gradcheck_var(patches)  # to var\n        laf = torch.tensor([[[[5., 0., 26.], [0., 5., 26.]]]], device=device)\n        laf = utils.tensor_to_gradcheck_var(laf)  # to var\n        assert gradcheck(LAFAffineShapeEstimator(11).to(device), (laf, patches),\n                         raise_exception=True, rtol=1e-3, atol=1e-3, nondet_tol=1e-4)\n'"
test/feature/test_hardnet.py,5,"b'import pytest\n\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\nfrom kornia.feature import HardNet\nimport kornia.testing as utils  # test utils\n\n\nclass TestHardNet:\n    def test_shape(self, device):\n        inp = torch.ones(1, 1, 32, 32, device=device)\n        hardnet = HardNet().to(device)\n        hardnet.eval()  # batchnorm with size 1 is not allowed in train mode\n        out = hardnet(inp)\n        assert out.shape == (1, 128)\n\n    def test_shape_batch(self, device):\n        inp = torch.ones(16, 1, 32, 32, device=device)\n        hardnet = HardNet().to(device)\n        out = hardnet(inp)\n        assert out.shape == (16, 128)\n\n    @pytest.mark.skip(""jacobian not well computed"")\n    def test_gradcheck(self, device):\n        patches = torch.rand(2, 1, 32, 32, device=device)\n        patches = utils.tensor_to_gradcheck_var(patches)  # to var\n        hardnet = HardNet().to(patches.device, patches.dtype)\n        assert gradcheck(hardnet, (patches,), eps=1e-4, atol=1e-4,\n                         raise_exception=True, )\n'"
test/feature/test_laf.py,98,"b'import pytest\n\nimport kornia as kornia\nimport kornia.geometry.transform.imgwarp\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\n\nclass TestAngleToRotationMatrix:\n    def test_shape(self, device):\n        inp = torch.ones(1, 3, 4, 4).to(device)\n        rotmat = kornia.geometry.transform.imgwarp.angle_to_rotation_matrix(inp)\n        assert rotmat.shape == (1, 3, 4, 4, 2, 2)\n\n    def test_angles(self, device):\n        ang_deg = torch.tensor([0, 90.], device=device)\n        expected = torch.tensor([[[1.0, 0.], [0., 1.0]],\n                                 [[0, 1.0], [-1.0, 0]]], device=device)\n        rotmat = kornia.geometry.transform.imgwarp.angle_to_rotation_matrix(ang_deg)\n        assert_allclose(rotmat, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width, device=device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.geometry.transform.imgwarp.angle_to_rotation_matrix,\n                         (img,),\n                         raise_exception=True)\n\n\nclass TestGetLAFScale:\n    def test_shape(self, device):\n        inp = torch.ones(1, 3, 2, 3, device=device)\n        rotmat = kornia.feature.get_laf_scale(inp)\n        assert rotmat.shape == (1, 3, 1, 1)\n\n    def test_scale(self, device):\n        inp = torch.tensor([[5., 1, 0], [1, 1, 0]], device=device).float()\n        inp = inp.view(1, 1, 2, 3)\n        expected = torch.tensor([[[[2]]]], device=device).float()\n        rotmat = kornia.feature.get_laf_scale(inp)\n        assert_allclose(rotmat, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 2, 3\n        img = torch.rand(batch_size, channels, height, width, device=device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.feature.get_laf_scale,\n                         (img,),\n                         raise_exception=True)\n\n\nclass TestGetLAFCenter:\n    def test_shape(self, device):\n        inp = torch.ones(1, 3, 2, 3, device=device)\n        xy = kornia.feature.get_laf_center(inp)\n        assert xy.shape == (1, 3, 2)\n\n    def test_center(self, device):\n        inp = torch.tensor([[5., 1, 2], [1, 1, 3]], device=device).float()\n        inp = inp.view(1, 1, 2, 3)\n        expected = torch.tensor([[[2, 3]]], device=device).float()\n        xy = kornia.feature.get_laf_center(inp)\n        assert_allclose(xy, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 2, 3\n        img = torch.rand(batch_size, channels, height, width)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.feature.get_laf_center,\n                         (img,),\n                         raise_exception=True)\n\n\nclass TestGetLAFOri:\n    def test_shape(self, device):\n        inp = torch.ones(1, 3, 2, 3, device=device)\n        ori = kornia.feature.get_laf_orientation(inp)\n        assert ori.shape == (1, 3, 1)\n\n    def test_ori(self, device):\n        inp = torch.tensor([[1, 1, 2], [1, 1, 3]], device=device).float()\n        inp = inp.view(1, 1, 2, 3)\n        expected = torch.tensor([[[45.]]], device=device).float()\n        angle = kornia.feature.get_laf_orientation(inp)\n        assert_allclose(angle, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 2, 3\n        img = torch.rand(batch_size, channels, height, width, device=device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.feature.get_laf_orientation,\n                         (img,),\n                         raise_exception=True)\n\n\nclass TestScaleLAF:\n    def test_shape_float(self, device):\n        inp = torch.ones(7, 3, 2, 3, device=device).float()\n        scale = 23.\n        assert kornia.feature.scale_laf(inp, scale).shape == inp.shape\n\n    def test_shape_tensor(self, device):\n        inp = torch.ones(7, 3, 2, 3, device=device).float()\n        scale = torch.zeros(7, 1, 1, 1, device=device).float()\n        assert kornia.feature.scale_laf(inp, scale).shape == inp.shape\n\n    def test_scale(self, device):\n        inp = torch.tensor([[5., 1, 0.8], [1, 1, -4.]], device=device).float()\n        inp = inp.view(1, 1, 2, 3)\n        scale = torch.tensor([[[[2.]]]], device=device).float()\n        out = kornia.feature.scale_laf(inp, scale)\n        expected = torch.tensor([[[[10., 2, 0.8], [2, 2, -4.]]]], device=device).float()\n        assert_allclose(out, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 2, 3\n        laf = torch.rand(batch_size, channels, height, width, device=device)\n        scale = torch.rand(batch_size, device=device)\n        scale = utils.tensor_to_gradcheck_var(scale)  # to var\n        laf = utils.tensor_to_gradcheck_var(laf)  # to var\n        assert gradcheck(kornia.feature.scale_laf,\n                         (laf, scale),\n                         raise_exception=True, atol=1e-4)\n\n\nclass TestMakeUpright:\n    def test_shape(self, device):\n        inp = torch.ones(5, 3, 2, 3, device=device)\n        rotmat = kornia.feature.make_upright(inp)\n        assert rotmat.shape == (5, 3, 2, 3)\n\n    def test_do_nothing(self, device):\n        inp = torch.tensor([[1, 0, 0], [0, 1, 0]], device=device).float()\n        inp = inp.view(1, 1, 2, 3)\n        expected = torch.tensor([[1, 0, 0], [0, 1, 0]], device=device).float()\n        laf = kornia.feature.make_upright(inp)\n        assert_allclose(laf, expected)\n\n    def test_do_nothing_with_scalea(self, device):\n        inp = torch.tensor([[2, 0, 0], [0, 2, 0]], device=device).float()\n        inp = inp.view(1, 1, 2, 3)\n        expected = torch.tensor([[2, 0, 0], [0, 2, 0]], device=device).float()\n        laf = kornia.feature.make_upright(inp)\n        assert_allclose(laf, expected)\n\n    def test_check_zeros(self, device):\n        inp = torch.rand(4, 5, 2, 3, device=device)\n        laf = kornia.feature.make_upright(inp)\n        must_be_zeros = laf[:, :, 0, 1]\n        assert_allclose(must_be_zeros, torch.zeros_like(must_be_zeros))\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 14, 2, 2, 3\n        img = torch.rand(batch_size, channels, height, width, device=device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.feature.make_upright,\n                         (img,),\n                         raise_exception=True)\n\n\nclass TestELL2LAF:\n    def test_shape(self, device):\n        inp = torch.ones(5, 3, 5, device=device)\n        inp[:, :, 3] = 0\n        rotmat = kornia.feature.ellipse_to_laf(inp)\n        assert rotmat.shape == (5, 3, 2, 3)\n\n    def test_conversion(self, device):\n        inp = torch.tensor([[10, -20, 0.01, 0, 0.01]], device=device).float()\n        inp = inp.view(1, 1, 5)\n        expected = torch.tensor([[10, 0, 10.], [0, 10, -20]], device=device).float()\n        expected = expected.view(1, 1, 2, 3)\n        laf = kornia.feature.ellipse_to_laf(inp)\n        assert_allclose(laf, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height = 1, 2, 5\n        img = torch.rand(batch_size, channels, height, device=device).abs()\n        img[:, :, 2] = img[:, :, 3].abs() + 0.3\n        img[:, :, 4] += 1.\n        # assure it is positive definite\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.feature.ellipse_to_laf,\n                         (img,),\n                         raise_exception=True)\n\n\nclass TestNormalizeLAF:\n    def test_shape(self, device):\n        inp = torch.rand(5, 3, 2, 3)\n        img = torch.rand(5, 3, 10, 10)\n        assert inp.shape == kornia.feature.normalize_laf(inp, img).shape\n\n    def test_conversion(self, device):\n        w, h = 10, 5\n        laf = torch.tensor([[1, 0, 1], [0, 1, 1]]).float()\n        laf = laf.view(1, 1, 2, 3)\n        img = torch.rand(1, 3, h, w)\n        expected = torch.tensor([[0.2, 0, 0.1], [0, 0.2, 0.2]]).float()\n        lafn = kornia.feature.normalize_laf(laf, img)\n        assert_allclose(lafn, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 2, 3\n\n        laf = torch.rand(batch_size, channels, height, width)\n        img = torch.rand(batch_size, 3, 10, 32)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        laf = utils.tensor_to_gradcheck_var(laf)  # to var\n        assert gradcheck(kornia.feature.normalize_laf,\n                         (laf, img,),\n                         raise_exception=True)\n\n\nclass TestLAF2pts:\n    def test_shape(self, device):\n        inp = torch.rand(5, 3, 2, 3, device=device)\n        n_pts = 13\n        assert kornia.feature.laf_to_boundary_points(inp, n_pts).shape == (5, 3, n_pts, 2)\n\n    def test_conversion(self, device):\n        laf = torch.tensor([[1, 0, 1], [0, 1, 1]], device=device).float()\n        laf = laf.view(1, 1, 2, 3)\n        n_pts = 6\n        expected = torch.tensor([[[[1, 1],\n                                   [1, 2],\n                                   [2, 1],\n                                   [1, 0],\n                                   [0, 1],\n                                   [1, 2]]]], device=device).float()\n        pts = kornia.feature.laf_to_boundary_points(laf, n_pts)\n        assert_allclose(pts, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 3, 2, 2, 3\n        laf = torch.rand(batch_size, channels, height, width, device=device)\n        laf = utils.tensor_to_gradcheck_var(laf)  # to var\n        assert gradcheck(kornia.feature.laf_to_boundary_points,\n                         (laf),\n                         raise_exception=True)\n\n\nclass TestDenormalizeLAF:\n    def test_shape(self, device):\n        inp = torch.rand(5, 3, 2, 3, device=device)\n        img = torch.rand(5, 3, 10, 10, device=device)\n        assert inp.shape == kornia.feature.denormalize_laf(inp, img).shape\n\n    def test_conversion(self, device):\n        w, h = 10, 5\n        expected = torch.tensor([[1, 0, 1], [0, 1, 1]], device=device).float()\n        expected = expected.view(1, 1, 2, 3)\n        img = torch.rand(1, 3, h, w, device=device)\n        lafn = torch.tensor([[0.2, 0, 0.1], [0, 0.2, 0.2]], device=device).float()\n        laf = kornia.feature.denormalize_laf(lafn.view(1, 1, 2, 3), img)\n        assert_allclose(laf, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 2, 3\n\n        laf = torch.rand(batch_size, channels, height, width, device=device)\n        img = torch.rand(batch_size, 3, 10, 32, device=device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        laf = utils.tensor_to_gradcheck_var(laf)  # to var\n        assert gradcheck(kornia.feature.denormalize_laf,\n                         (laf, img,),\n                         raise_exception=True)\n\n\nclass TestGenPatchGrid:\n    def test_shape(self, device):\n        laf = torch.rand(5, 3, 2, 3, device=device)\n        img = torch.rand(5, 3, 10, 10, device=device)\n        PS = 3\n        from kornia.feature.laf import generate_patch_grid_from_normalized_LAF\n        grid = generate_patch_grid_from_normalized_LAF(img, laf, PS)\n        assert grid.shape == (15, 3, 3, 2)\n\n    def test_gradcheck(self, device):\n        laf = torch.rand(5, 3, 2, 3, device=device)\n        img = torch.rand(5, 3, 10, 10, device=device)\n        PS = 3\n        from kornia.feature.laf import generate_patch_grid_from_normalized_LAF\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        laf = utils.tensor_to_gradcheck_var(laf)  # to var\n        assert gradcheck(generate_patch_grid_from_normalized_LAF,\n                         (img, laf, PS,),\n                         raise_exception=True)\n\n\nclass TestExtractPatchesSimple:\n    def test_shape(self, device):\n        laf = torch.rand(5, 4, 2, 3, device=device)\n        img = torch.rand(5, 3, 100, 30, device=device)\n        PS = 10\n        patches = kornia.feature.extract_patches_simple(img, laf, PS)\n        assert patches.shape == (5, 4, 3, PS, PS)\n\n    # TODO: check what to do to improve timing\n    # @pytest.mark.skip(""The test takes too long to finish."")\n    def test_gradcheck(self, device):\n        nlaf = torch.tensor([[0.1, 0.001, 0.5], [0, 0.1, 0.5]], device=device).float()\n        nlaf = nlaf.view(1, 1, 2, 3)\n        img = torch.rand(1, 3, 20, 30, device=device)\n        PS = 11\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        nlaf = utils.tensor_to_gradcheck_var(nlaf)  # to var\n        assert gradcheck(kornia.feature.extract_patches_simple,\n                         (img, nlaf, PS, False),\n                         raise_exception=True)\n\n\nclass TestExtractPatchesPyr:\n    def test_shape(self, device):\n        laf = torch.rand(5, 4, 2, 3, device=device)\n        img = torch.rand(5, 3, 100, 30, device=device)\n        PS = 10\n        patches = kornia.feature.extract_patches_from_pyramid(img, laf, PS)\n        assert patches.shape == (5, 4, 3, PS, PS)\n\n    # TODO: check what to do to improve timing\n    # @pytest.mark.skip(""The test takes too long to finish."")\n    def test_gradcheck(self, device):\n        nlaf = torch.tensor([[0.1, 0.001, 0.5], [0, 0.1, 0.5]], device=device).float()\n        nlaf = nlaf.view(1, 1, 2, 3)\n        img = torch.rand(1, 3, 20, 30, device=device)\n        PS = 11\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        nlaf = utils.tensor_to_gradcheck_var(nlaf)  # to var\n        assert gradcheck(kornia.feature.extract_patches_from_pyramid,\n                         (img, nlaf, PS, False),\n                         raise_exception=True)\n\n\nclass TestLAFIsTouchingBoundary:\n    def test_shape(self, device):\n        inp = torch.rand(5, 3, 2, 3, device=device)\n        img = torch.rand(5, 3, 10, 10, device=device)\n        assert (5, 3) == kornia.feature.laf_is_inside_image(inp, img).shape\n\n    def test_touch(self, device):\n        w, h = 10, 5\n        img = torch.rand(1, 3, h, w, device=device)\n        laf = torch.tensor([[[[10, 0, 3], [0, 10, 3]],\n                             [[1, 0, 5], [0, 1, 2]]]], device=device).float()\n        expected = torch.tensor([[False, True]], device=device)\n        assert torch.all(kornia.feature.laf_is_inside_image(laf, img) == expected).item()\n\n\nclass TestGetCreateLAF:\n    def test_shape(self, device):\n        xy = torch.ones(1, 3, 2, device=device)\n        ori = torch.ones(1, 3, 1, device=device)\n        scale = torch.ones(1, 3, 1, 1, device=device)\n        laf = kornia.feature.laf_from_center_scale_ori(xy, scale, ori)\n        assert laf.shape == (1, 3, 2, 3)\n\n    def test_laf(self, device):\n        xy = torch.ones(1, 1, 2, device=device)\n        ori = torch.zeros(1, 1, 1, device=device)\n        scale = 5 * torch.ones(1, 1, 1, 1, device=device)\n        expected = torch.tensor([[[[5, 0, 1], [0, 5, 1]]]], device=device).float()\n        laf = kornia.feature.laf_from_center_scale_ori(xy, scale, ori)\n        assert_allclose(laf, expected)\n\n    def test_cross_consistency(self, device):\n        batch_size, channels = 3, 2\n        xy = torch.rand(batch_size, channels, 2, device=device)\n        ori = torch.rand(batch_size, channels, 1, device=device)\n        scale = torch.abs(torch.rand(batch_size, channels, 1, 1, device=device))\n        laf = kornia.feature.laf_from_center_scale_ori(xy, scale, ori)\n        scale2 = kornia.feature.get_laf_scale(laf)\n        assert_allclose(scale, scale2)\n        xy2 = kornia.feature.get_laf_center(laf)\n        assert_allclose(xy2, xy)\n        ori2 = kornia.feature.get_laf_orientation(laf)\n        assert_allclose(ori2, ori)\n\n    def test_gradcheck(self, device):\n        batch_size, channels = 3, 2\n        xy = utils.tensor_to_gradcheck_var(torch.rand(batch_size, channels, 2, device=device))\n        ori = utils.tensor_to_gradcheck_var(torch.rand(batch_size, channels, 1, device=device))\n        scale = utils.tensor_to_gradcheck_var(torch.abs(torch.rand(batch_size, channels, 1, 1, device=device)))\n        assert gradcheck(kornia.feature.laf_from_center_scale_ori,\n                         (xy, scale, ori,),\n                         raise_exception=True)\n\n\nclass TestGetLAF3pts:\n    def test_shape(self, device):\n        inp = torch.ones(1, 3, 2, 3, device=device)\n        out = kornia.feature.laf_to_three_points(inp)\n        assert out.shape == inp.shape\n\n    def test_batch_shape(self, device):\n        inp = torch.ones(5, 3, 2, 3, device=device)\n        out = kornia.feature.laf_to_three_points(inp)\n        assert out.shape == inp.shape\n\n    def test_conversion(self, device):\n        inp = torch.tensor([[1, 0, 2], [0, 1, 3]], device=device).float().view(1, 1, 2, 3)\n        expected = torch.tensor([[3, 2, 2], [3, 4, 3]], device=device).float().view(1, 1, 2, 3)\n        threepts = kornia.feature.laf_to_three_points(inp)\n        assert_allclose(threepts, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 3, 2, 2, 3\n        inp = torch.rand(batch_size, channels, height, width, device=device)\n        inp = utils.tensor_to_gradcheck_var(inp)  # to var\n        assert gradcheck(kornia.feature.laf_to_three_points,\n                         (inp,),\n                         raise_exception=True)\n\n\nclass TestGetLAFFrom3pts:\n    def test_shape(self, device):\n        inp = torch.ones(1, 3, 2, 3, device=device)\n        out = kornia.feature.laf_from_three_points(inp)\n        assert out.shape == inp.shape\n\n    def test_batch_shape(self, device):\n        inp = torch.ones(5, 3, 2, 3, device=device)\n        out = kornia.feature.laf_from_three_points(inp)\n        assert out.shape == inp.shape\n\n    def test_conversion(self, device):\n        expected = torch.tensor([[1, 0, 2], [0, 1, 3]], device=device).float().view(1, 1, 2, 3)\n        inp = torch.tensor([[3, 2, 2], [3, 4, 3]], device=device).float().view(1, 1, 2, 3)\n        threepts = kornia.feature.laf_from_three_points(inp)\n        assert_allclose(threepts, expected)\n\n    def test_cross_consistency(self, device):\n        batch_size, channels, height, width = 3, 2, 2, 3\n        inp = torch.rand(batch_size, channels, height, width, device=device)\n        inp_2 = kornia.feature.laf_from_three_points(inp)\n        inp_2 = kornia.feature.laf_to_three_points(inp_2)\n        assert_allclose(inp_2, inp)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 3, 2, 2, 3\n        inp = torch.rand(batch_size, channels, height, width, device=device)\n        inp = utils.tensor_to_gradcheck_var(inp)  # to var\n        assert gradcheck(kornia.feature.laf_from_three_points,\n                         (inp,),\n                         raise_exception=True)\n'"
test/feature/test_local_features_orientation.py,26,"b'import pytest\nimport kornia.testing as utils  # test utils\nimport kornia\n\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\nfrom kornia.feature.orientation import *\n\n\nclass TestPassLAF:\n    def test_shape(self, device):\n        inp = torch.rand(1, 1, 32, 32, device=device)\n        laf = torch.rand(1, 1, 2, 3, device=device)\n        ori = PassLAF().to(device)\n        out = ori(laf, inp)\n        assert out.shape == laf.shape\n\n    def test_shape_batch(self, device):\n        inp = torch.rand(2, 1, 32, 32, device=device)\n        laf = torch.rand(2, 34, 2, 3, device=device)\n        ori = PassLAF().to(device)\n        out = ori(laf, inp)\n        assert out.shape == laf.shape\n\n    def test_print(self, device):\n        sift = PassLAF()\n        sift.__repr__()\n\n    def test_pass(self, device):\n        inp = torch.rand(1, 1, 32, 32, device=device)\n        laf = torch.rand(1, 1, 2, 3, device=device)\n        ori = PassLAF().to(device)\n        out = ori(laf, inp)\n        assert_allclose(out, laf)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 1, 21, 21\n        patches = torch.rand(batch_size, channels, height, width, device=device)\n        patches = utils.tensor_to_gradcheck_var(patches)  # to var\n        laf = torch.rand(batch_size, 4, 2, 3)\n        assert gradcheck(PassLAF().to(device), (patches, laf),\n                         raise_exception=True)\n\n\nclass TestPatchDominantGradientOrientation:\n    def test_shape(self, device):\n        inp = torch.rand(1, 1, 32, 32, device=device)\n        ori = PatchDominantGradientOrientation(32).to(device)\n        ang = ori(inp)\n        assert ang.shape == torch.Size([1])\n\n    def test_shape_batch(self, device):\n        inp = torch.rand(10, 1, 32, 32, device=device)\n        ori = PatchDominantGradientOrientation(32).to(device)\n        ang = ori(inp)\n        assert ang.shape == torch.Size([10])\n\n    def test_print(self, device):\n        sift = PatchDominantGradientOrientation(32)\n        sift.__repr__()\n\n    def test_toy(self, device):\n        ori = PatchDominantGradientOrientation(19).to(device)\n        inp = torch.zeros(1, 1, 19, 19, device=device)\n        inp[:, :, :10, :] = 1\n        ang = ori(inp)\n        expected = torch.tensor([90.], device=device)\n        assert_allclose(kornia.rad2deg(ang), expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 1, 13, 13\n        ori = PatchDominantGradientOrientation(width).to(device)\n        patches = torch.rand(batch_size, channels, height, width, device=device)\n        patches = utils.tensor_to_gradcheck_var(patches)  # to var\n        assert gradcheck(ori, (patches, ),\n                         raise_exception=True)\n\n\nclass TestLAFOrienter:\n    def test_shape(self, device):\n        inp = torch.rand(1, 1, 32, 32, device=device)\n        laf = torch.rand(1, 1, 2, 3, device=device)\n        ori = LAFOrienter().to(device)\n        out = ori(laf, inp)\n        assert out.shape == laf.shape\n\n    def test_shape_batch(self, device):\n        inp = torch.rand(2, 1, 32, 32, device=device)\n        laf = torch.rand(2, 34, 2, 3, device=device)\n        ori = LAFOrienter().to(device)\n        out = ori(laf, inp)\n        assert out.shape == laf.shape\n\n    def test_print(self, device):\n        sift = LAFOrienter()\n        sift.__repr__()\n\n    def test_toy(self, device):\n        ori = LAFOrienter(32).to(device)\n        inp = torch.zeros(1, 1, 19, 19, device=device)\n        inp[:, :, :10, :] = 1\n        laf = torch.tensor([[[[0, 5., 8.], [5.0, 0., 8.]]]], device=device)\n        new_laf = ori(laf, inp)\n        expected = torch.tensor([[[[5., 0., 8.], [0., 5., 8.]]]], device=device)\n        assert_allclose(new_laf, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 1, 21, 21\n        patches = torch.rand(batch_size, channels, height, width, device=device).float()\n        patches = utils.tensor_to_gradcheck_var(patches)  # to var\n        laf = torch.ones(batch_size, 2, 2, 3, device=device).float()\n        laf[:, :, 0, 1] = 0\n        laf[:, :, 1, 0] = 0\n        laf = utils.tensor_to_gradcheck_var(laf)  # to var\n        assert gradcheck(LAFOrienter(8).to(device), (laf, patches),\n                         raise_exception=True, rtol=1e-3, atol=1e-3)\n'"
test/feature/test_nms.py,12,"b'import pytest\n\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\n\nclass TestNMS2d:\n    def test_shape(self, device):\n        inp = torch.ones(1, 3, 4, 4, device=device)\n        nms = kornia.feature.NonMaximaSuppression2d((3, 3)).to(device)\n        assert nms(inp).shape == inp.shape\n\n    def test_shape_batch(self, device):\n        inp = torch.ones(4, 3, 4, 4, device=device)\n        nms = kornia.feature.NonMaximaSuppression2d((3, 3)).to(device)\n        assert nms(inp).shape == inp.shape\n\n    def test_nms(self, device):\n        inp = torch.tensor([[[\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 0.1, 1., 0., 1., 1., 0.],\n            [0., 0.7, 1.1, 0., 1., 2., 0.],\n            [0., 0.8, 1., 0., 1., 1., 0.],\n        ]]], device=device).float()\n\n        expected = torch.tensor([[[\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 0, 0, 0., 0, 0., 0.],\n            [0., 0, 1.1, 0., 0., 2., 0.],\n            [0., 0, 0, 0., 0., 0., 0.],\n        ]]], device=device).float()\n        nms = kornia.feature.NonMaximaSuppression2d((3, 3)).to(device)\n        scores = nms(inp)\n        assert_allclose(scores, expected, atol=1e-4, rtol=1e-3)\n\n    def test_gradcheck(self, device):\n        k = 0.04\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width, device=device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.feature.nms2d, (img, (3, 3)),\n                         raise_exception=True, nondet_tol=1e-4)\n\n\nclass TestNMS3d:\n    def test_shape(self, device):\n        inp = torch.ones(1, 1, 3, 4, 4, device=device)\n        nms = kornia.feature.NonMaximaSuppression3d((3, 3, 3)).to(device)\n        assert nms(inp).shape == inp.shape\n\n    def test_shape_batch(self, device):\n        inp = torch.ones(4, 1, 3, 4, 4, device=device)\n        nms = kornia.feature.NonMaximaSuppression3d((3, 3, 3)).to(device)\n        assert nms(inp).shape == inp.shape\n\n    def test_nms(self, device):\n        inp = torch.tensor([[[\n            [[0., 0., 0., 0., 0.],\n             [0., 0., 0., 0., 0.],\n             [0., 0., 1., 0., 0.],\n             [0., 0., 0., 0., 0.],\n             [0., 0., 0., 0., 0.]],\n            [[0., 0., 0., 0., 0.],\n             [0., 0., 1., 0., 0.],\n             [0., 1., 2., 1., 0.],\n             [0., 0., 1., 0., 0.],\n             [0., 0., 0., 0., 0.]],\n            [[0., 0., 0., 0., 0.],\n             [0., 0., 0., 0., 0.],\n             [0., 0., 1., 0., 0.],\n             [0., 0., 0., 0., 0.],\n             [0., 0., 0., 0., 0.]],\n        ]]]).to(device)\n\n        expected = torch.tensor([[[[[0., 0., 0., 0., 0.],\n                                    [0., 0., 0., 0., 0.],\n                                    [0., 0., 0., 0., 0.],\n                                    [0., 0., 0., 0., 0.],\n                                    [0., 0., 0., 0., 0.]],\n                                   [[0., 0., 0., 0., 0.],\n                                    [0., 0., 0., 0., 0.],\n                                    [0., 0., 2., 0., 0.],\n                                    [0., 0., 0., 0., 0.],\n                                    [0., 0., 0., 0., 0.]],\n                                   [[0., 0., 0., 0., 0.],\n                                    [0., 0., 0., 0., 0.],\n                                    [0., 0., 0., 0., 0.],\n                                    [0., 0., 0., 0., 0.],\n                                    [0., 0., 0., 0., 0.]]]]]).to(device)\n        nms = kornia.feature.NonMaximaSuppression3d((3, 3, 3)).to(device)\n        scores = nms(inp)\n        assert_allclose(scores, expected, atol=1e-4, rtol=1e-3)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, depth, height, width = 1, 1, 4, 5, 4\n        img = torch.rand(batch_size, channels, depth, height, width, device=device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.feature.nms3d, (img, (3, 3, 3)),\n                         raise_exception=True, nondet_tol=1e-4)\n'"
test/feature/test_responces_local_features.py,28,"b'import pytest\n\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\n\nclass TestCornerHarris:\n    def test_shape(self, device):\n        inp = torch.ones(1, 3, 4, 4, device=device)\n        harris = kornia.feature.CornerHarris(k=0.04).to(device)\n        assert harris(inp).shape == (1, 3, 4, 4)\n\n    def test_shape_batch(self, device):\n        inp = torch.zeros(2, 6, 4, 4, device=device)\n        harris = kornia.feature.CornerHarris(k=0.04).to(device)\n        assert harris(inp).shape == (2, 6, 4, 4)\n\n    def test_corners(self, device):\n        inp = torch.tensor([[[\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n        ]]], device=device).float()\n\n        expected = torch.tensor([[[\n            [0.0042, 0.0054, 0.0035, 0.0006, 0.0035, 0.0054, 0.0042],\n            [0.0054, 0.0068, 0.0046, 0.0014, 0.0046, 0.0068, 0.0054],\n            [0.0035, 0.0046, 0.0034, 0.0014, 0.0034, 0.0046, 0.0035],\n            [0.0006, 0.0014, 0.0014, 0.0006, 0.0014, 0.0014, 0.0006],\n            [0.0035, 0.0046, 0.0034, 0.0014, 0.0034, 0.0046, 0.0035],\n            [0.0054, 0.0068, 0.0046, 0.0014, 0.0046, 0.0068, 0.0054],\n            [0.0042, 0.0054, 0.0035, 0.0006, 0.0035, 0.0054, 0.0042]]]],\n            device=device).float()\n        harris = kornia.feature.CornerHarris(k=0.04).to(device)\n        scores = harris(inp)\n        assert_allclose(scores, expected, atol=1e-4, rtol=1e-3)\n\n    def test_corners_batch(self, device):\n        inp = torch.tensor([[\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n        ], [\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 1., 1., 1., 1., 0., 0.],\n            [0., 1., 1., 1., 1., 0., 0.],\n            [0., 1., 1., 1., 1., 0., 0.],\n            [0., 1., 1., 1., 1., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n        ]], device=device).repeat(2, 1, 1, 1)\n        expected = torch.tensor([[\n            [0.0415, 0.0541, 0.0346, 0.0058, 0.0346, 0.0541, 0.0415],\n            [0.0541, 0.0678, 0.0457, 0.0145, 0.0457, 0.0678, 0.0541],\n            [0.0346, 0.0457, 0.0335, 0.0139, 0.0335, 0.0457, 0.0346],\n            [0.0058, 0.0145, 0.0139, 0.0064, 0.0139, 0.0145, 0.0058],\n            [0.0346, 0.0457, 0.0335, 0.0139, 0.0335, 0.0457, 0.0346],\n            [0.0541, 0.0678, 0.0457, 0.0145, 0.0457, 0.0678, 0.0541],\n            [0.0415, 0.0541, 0.0346, 0.0058, 0.0346, 0.0541, 0.0415]\n        ], [\n            [0.0415, 0.0547, 0.0447, 0.0440, 0.0490, 0.0182, 0.0053],\n            [0.0547, 0.0688, 0.0557, 0.0549, 0.0610, 0.0229, 0.0066],\n            [0.0447, 0.0557, 0.0444, 0.0437, 0.0489, 0.0168, 0.0035],\n            [0.0440, 0.0549, 0.0437, 0.0431, 0.0481, 0.0166, 0.0034],\n            [0.0490, 0.0610, 0.0489, 0.0481, 0.0541, 0.0205, 0.0060],\n            [0.0182, 0.0229, 0.0168, 0.0166, 0.0205, 0.0081, 0.0025],\n            [0.0053, 0.0066, 0.0035, 0.0034, 0.0060, 0.0025, 0.0008]\n        ]], device=device).repeat(2, 1, 1, 1) / 10.\n        scores = kornia.feature.harris_response(inp, k=0.04)\n        assert_allclose(scores, expected, atol=1e-4, rtol=1e-4)\n\n    def test_gradcheck(self, device):\n        k = 0.04\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width, device=device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.feature.harris_response, (img, k),\n                         raise_exception=True, nondet_tol=1e-4)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input, k):\n            return kornia.feature.harris_response(input, k)\n        k = torch.tensor(0.04)\n        img = torch.rand(2, 3, 4, 5, device=device)\n        actual = op_script(img, k)\n        expected = kornia.feature.harris_response(img, k)\n        assert_allclose(actual, expected)\n\n\nclass TestCornerGFTT:\n    def test_shape(self, device):\n        inp = torch.ones(1, 3, 4, 4, device=device)\n        shi_tomasi = kornia.feature.CornerGFTT().to(device)\n        assert shi_tomasi(inp).shape == (1, 3, 4, 4)\n\n    def test_shape_batch(self, device):\n        inp = torch.zeros(2, 6, 4, 4, device=device)\n        shi_tomasi = kornia.feature.CornerGFTT().to(device)\n        assert shi_tomasi(inp).shape == (2, 6, 4, 4)\n\n    def test_corners(self, device):\n        inp = torch.tensor([[[\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n        ]]], device=device).float()\n\n        expected = torch.tensor([[[\n            [0.0379, 0.0456, 0.0283, 0.0121, 0.0283, 0.0456, 0.0379],\n            [0.0456, 0.0598, 0.0402, 0.0168, 0.0402, 0.0598, 0.0456],\n            [0.0283, 0.0402, 0.0545, 0.0245, 0.0545, 0.0402, 0.0283],\n            [0.0121, 0.0168, 0.0245, 0.0276, 0.0245, 0.0168, 0.0121],\n            [0.0283, 0.0402, 0.0545, 0.0245, 0.0545, 0.0402, 0.0283],\n            [0.0456, 0.0598, 0.0402, 0.0168, 0.0402, 0.0598, 0.0456],\n            [0.0379, 0.0456, 0.0283, 0.0121, 0.0283, 0.0456, 0.0379]]]], device=device).float()\n        shi_tomasi = kornia.feature.CornerGFTT().to(device)\n        scores = shi_tomasi(inp)\n        assert_allclose(scores, expected, atol=1e-4, rtol=1e-3)\n\n    def test_corners_batch(self, device):\n        inp = torch.tensor([[\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 1., 1., 1., 1., 1., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n        ], [\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 1., 1., 1., 1., 0., 0.],\n            [0., 1., 1., 1., 1., 0., 0.],\n            [0., 1., 1., 1., 1., 0., 0.],\n            [0., 1., 1., 1., 1., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n        ]], device=device).repeat(2, 1, 1, 1)\n        expected = torch.tensor([[\n            [0.0379, 0.0456, 0.0283, 0.0121, 0.0283, 0.0456, 0.0379],\n            [0.0456, 0.0598, 0.0402, 0.0168, 0.0402, 0.0598, 0.0456],\n            [0.0283, 0.0402, 0.0545, 0.0245, 0.0545, 0.0402, 0.0283],\n            [0.0121, 0.0168, 0.0245, 0.0276, 0.0245, 0.0168, 0.0121],\n            [0.0283, 0.0402, 0.0545, 0.0245, 0.0545, 0.0402, 0.0283],\n            [0.0456, 0.0598, 0.0402, 0.0168, 0.0402, 0.0598, 0.0456],\n            [0.0379, 0.0456, 0.0283, 0.0121, 0.0283, 0.0456, 0.0379]\n        ], [\n            [0.0379, 0.0462, 0.0349, 0.0345, 0.0443, 0.0248, 0.0112],\n            [0.0462, 0.0608, 0.0488, 0.0483, 0.0581, 0.0274, 0.0119],\n            [0.0349, 0.0488, 0.0669, 0.0664, 0.0460, 0.0191, 0.0084],\n            [0.0345, 0.0483, 0.0664, 0.0660, 0.0455, 0.0189, 0.0083],\n            [0.0443, 0.0581, 0.0460, 0.0455, 0.0555, 0.0262, 0.0114],\n            [0.0248, 0.0274, 0.0191, 0.0189, 0.0262, 0.0172, 0.0084],\n            [0.0112, 0.0119, 0.0084, 0.0083, 0.0114, 0.0084, 0.0046]\n        ]], device=device).repeat(2, 1, 1, 1)\n        shi_tomasi = kornia.feature.CornerGFTT().to(device)\n        scores = shi_tomasi(inp)\n        assert_allclose(scores, expected, atol=1e-4, rtol=1e-4)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width, device=device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.feature.gftt_response, (img),\n                         raise_exception=True, nondet_tol=1e-4)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input):\n            return kornia.feature.gftt_response(input)\n        img = torch.rand(2, 3, 4, 5, device=device)\n        actual = op_script(img)\n        expected = kornia.feature.gftt_response(img)\n        assert_allclose(actual, expected)\n\n\nclass TestBlobHessian:\n    def test_shape(self, device):\n        inp = torch.ones(1, 3, 4, 4, device=device)\n        shi_tomasi = kornia.feature.BlobHessian().to(device)\n        assert shi_tomasi(inp).shape == (1, 3, 4, 4)\n\n    def test_shape_batch(self, device):\n        inp = torch.zeros(2, 6, 4, 4, device=device)\n        shi_tomasi = kornia.feature.BlobHessian().to(device)\n        assert shi_tomasi(inp).shape == (2, 6, 4, 4)\n\n    def test_blobs_batch(self, device):\n        inp = torch.tensor([[\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 1., 1., 1., 0., 0., 0.],\n            [0., 1., 1., 1., 0., 0., 0.],\n            [0., 1., 1., 1., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n        ], [\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 1., 1., 0., 0., 0., 0.],\n            [0., 1., 1., 0., 0., 0., 0.],\n            [0., 0., 0., 1., 1., 0., 0.],\n            [0., 0., 0., 1., 1., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0.],\n        ]], device=device).repeat(2, 1, 1, 1)\n        expected = torch.tensor([[\n            [-0.0564, -0.0759, -0.0342, -0.0759, -0.0564, -0.0057, 0.0000],\n            [-0.0759, -0.0330, 0.0752, -0.0330, -0.0759, -0.0096, 0.0000],\n            [-0.0342, 0.0752, 0.1914, 0.0752, -0.0342, -0.0068, 0.0000],\n            [-0.0759, -0.0330, 0.0752, -0.0330, -0.0759, -0.0096, 0.0000],\n            [-0.0564, -0.0759, -0.0342, -0.0759, -0.0564, -0.0057, 0.0000],\n            [-0.0057, -0.0096, -0.0068, -0.0096, -0.0057, -0.0005, 0.0000],\n            [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n        ], [\n            [-0.0564, -0.0522, -0.0522, -0.0564, -0.0057, 0.0000, 0.0000],\n            [-0.0522, 0.0688, 0.0688, -0.0123, 0.0033, -0.0057, -0.0005],\n            [-0.0522, 0.0688, -0.0755, -0.1111, -0.0123, -0.0564, -0.0057],\n            [-0.0564, -0.0123, -0.1111, -0.0755, 0.0688, -0.0522, -0.0080],\n            [-0.0057, 0.0033, -0.0123, 0.0688, 0.0688, -0.0522, -0.0080],\n            [0.0000, -0.0057, -0.0564, -0.0522, -0.0522, -0.0564, -0.0057],\n            [0.0000, -0.0005, -0.0057, -0.0080, -0.0080, -0.0057, -0.0005]\n        ]], device=device).repeat(2, 1, 1, 1)\n        shi_tomasi = kornia.feature.BlobHessian().to(device)\n        scores = shi_tomasi(inp)\n        assert_allclose(scores, expected, atol=1e-4, rtol=1e-4)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width, device=device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.feature.hessian_response, (img),\n                         raise_exception=True, nondet_tol=1e-4)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input):\n            return kornia.feature.hessian_response(input)\n        img = torch.rand(2, 3, 4, 5, device=device)\n        actual = op_script(img)\n        expected = kornia.feature.hessian_response(img)\n        assert_allclose(actual, expected)\n'"
test/feature/test_scale_space_detector.py,16,"b'import pytest\nimport torch\nimport kornia.testing as utils  # test utils\nimport kornia\n\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\nfrom kornia.feature.scale_space_detector import *\n\n\nclass TestScaleSpaceDetector:\n    def test_shape(self, device):\n        inp = torch.rand(1, 1, 32, 32, device=device)\n        n_feats = 10\n        det = ScaleSpaceDetector(n_feats).to(device)\n        lafs, resps = det(inp)\n        assert lafs.shape == torch.Size([1, n_feats, 2, 3])\n        assert resps.shape == torch.Size([1, n_feats])\n\n    def test_shape_batch(self, device):\n        inp = torch.rand(7, 1, 32, 32, device=device)\n        n_feats = 10\n        det = ScaleSpaceDetector(n_feats).to(device)\n        lafs, resps = det(inp)\n        assert lafs.shape == torch.Size([7, n_feats, 2, 3])\n        assert resps.shape == torch.Size([7, n_feats])\n\n    def test_print(self, device):\n        sift = ScaleSpaceDetector()\n        sift.__repr__()\n\n    def test_toy(self, device):\n        inp = torch.zeros(1, 1, 33, 33, device=device)\n        inp[:, :, 13:-13, 13:-13] = 1.0\n        n_feats = 1\n        det = ScaleSpaceDetector(n_feats,\n                                 resp_module=kornia.feature.BlobHessian(),\n                                 mr_size=3.0).to(device)\n        lafs, resps = det(inp)\n        expected_laf = torch.tensor([[[[6.0543, 0.0000, 16.0], [0.0, 6.0543, 16.0]]]], device=device)\n        expected_resp = torch.tensor([[0.0804]], device=device)\n        assert_allclose(expected_laf, lafs, rtol=0.001, atol=1e-03)\n        assert_allclose(expected_resp, resps, rtol=0.001, atol=1e-03)\n\n    def test_toy_mask(self, device):\n        if ""cuda"" in str(device):\n            pytest.skip(""this cuda test is broken"")\n\n        inp = torch.zeros(1, 1, 15, 10, device=device)\n        inp[:, :, 2:-2, 1:-1] = 1.0\n\n        mask = torch.zeros(1, 1, 15, 10, device=device)\n        mask[:, :, 1:-1, 3:-3] = 1.0\n\n        n_feats = 1\n        det = ScaleSpaceDetector(n_feats,\n                                 resp_module=kornia.feature.BlobHessian(),\n                                 mr_size=3.0).to(device)\n        lafs, resps = det(inp, mask)\n        expected_laf = torch.tensor([[[[6.0478, 0.0000, 4.9978],\n                                       [0.0000, 6.0478, 4.9993]]]], device=device)\n        expected_resp = torch.tensor([[0.0]], device=device)\n        assert_allclose(expected_laf, lafs, rtol=0.001, atol=1e-03)\n        assert_allclose(expected_resp, resps, rtol=0.001, atol=1e-03)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 1, 31, 21\n        patches = torch.rand(batch_size, channels, height, width, device=device)\n        patches = utils.tensor_to_gradcheck_var(patches)  # to var\n        assert gradcheck(ScaleSpaceDetector(2).to(device), patches,\n                         raise_exception=True, nondet_tol=1e-4)\n'"
test/feature/test_siftdesc.py,8,"b'import pytest\nimport kornia.testing as utils  # test utils\n\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\nfrom kornia.feature.siftdesc import *\n\n\n@pytest.mark.parametrize(""ksize"", [5, 13, 25])\ndef test_get_sift_pooling_kernel(ksize):\n    kernel = get_sift_pooling_kernel(ksize)\n    assert kernel.shape == (ksize, ksize)\n\n\n@pytest.mark.parametrize(""ps,n_bins,ksize,stride,pad"",\n                         [(41, 3, 20, 13, 5),\n                          (32, 4, 12, 8, 3)])\ndef test_get_sift_bin_ksize_stride_pad(ps,\n                                       n_bins,\n                                       ksize,\n                                       stride,\n                                       pad):\n    out = get_sift_bin_ksize_stride_pad(ps, n_bins)\n    assert out == (ksize, stride, pad)\n\n\nclass TestSIFTDescriptor:\n    def test_shape(self, device):\n        inp = torch.ones(1, 1, 32, 32, device=device)\n        sift = SIFTDescriptor(32).to(device)\n        out = sift(inp)\n        assert out.shape == (1, 128)\n\n    def test_batch_shape(self, device):\n        inp = torch.ones(2, 1, 15, 15, device=device)\n        sift = SIFTDescriptor(15).to(device)\n        out = sift(inp)\n        assert out.shape == (2, 128)\n\n    def test_batch_shape_non_std(self, device):\n        inp = torch.ones(3, 1, 19, 19, device=device)\n        sift = SIFTDescriptor(19, 5, 3).to(device)\n        out = sift(inp)\n        assert out.shape == (3, (3 ** 2) * 5)\n\n    def test_print(self, device):\n        sift = SIFTDescriptor(41)\n        sift.__repr__()\n\n    def test_toy(self, device):\n        patch = torch.ones(1, 1, 6, 6, device=device).float()\n        patch[0, 0, :, 3:] = 0\n        sift = SIFTDescriptor(6,\n                              num_ang_bins=4,\n                              num_spatial_bins=1,\n                              clipval=0.2,\n                              rootsift=False).to(device)\n        out = sift(patch)\n        expected = torch.tensor([[0, 0, 1., 0]], device=device)\n        assert_allclose(out, expected, atol=1e-3, rtol=1e-3)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 1, 13, 13\n        patches = torch.rand(batch_size, channels, height, width, device=device)\n        patches = utils.tensor_to_gradcheck_var(patches)  # to var\n        assert gradcheck(sift_describe, (patches, 13),\n                         raise_exception=True, nondet_tol=1e-4)\n'"
test/feature/test_sosnet.py,5,"b'import pytest\n\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\nfrom kornia.feature import SOSNet\nimport kornia.testing as utils  # test utils\n\n\nclass TestSOSNet:\n    def test_shape(self, device):\n        inp = torch.ones(1, 1, 32, 32, device=device)\n        sosnet = SOSNet(pretrained=False).to(device)\n        sosnet.eval()  # batchnorm with size 1 is not allowed in train mode\n        out = sosnet(inp)\n        assert out.shape == (1, 128)\n\n    def test_shape_batch(self, device):\n        inp = torch.ones(16, 1, 32, 32, device=device)\n        sosnet = SOSNet(pretrained=False).to(device)\n        out = sosnet(inp)\n        assert out.shape == (16, 128)\n\n    @pytest.mark.skip(""jacobian not well computed"")\n    def test_gradcheck(self, device):\n        patches = torch.rand(2, 1, 32, 32, device=device)\n        patches = utils.tensor_to_gradcheck_var(patches)  # to var\n        sosnet = SOSNet(pretrained=False).to(patches.device, patches.dtype)\n        assert gradcheck(sosnet, (patches,), eps=1e-4, atol=1e-4,\n                         raise_exception=True, )\n'"
test/filters/test_blur.py,14,"b'from typing import Tuple\n\nimport pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\n\nclass TestBoxBlur:\n    def test_shape(self, device):\n        inp = torch.zeros(1, 3, 4, 4).to(device)\n        blur = kornia.filters.BoxBlur((3, 3))\n        assert blur(inp).shape == (1, 3, 4, 4)\n\n    def test_shape_batch(self, device):\n        inp = torch.zeros(2, 6, 4, 4).to(device)\n        blur = kornia.filters.BoxBlur((3, 3))\n        assert blur(inp).shape == (2, 6, 4, 4)\n\n    def test_kernel_3x3(self, device):\n        inp = torch.tensor([[[\n            [1., 1., 1., 1., 1.],\n            [1., 1., 1., 1., 1.],\n            [1., 1., 1., 1., 1.],\n            [2., 2., 2., 2., 2.],\n            [2., 2., 2., 2., 2.]\n        ]]]).to(device)\n\n        kernel_size = (3, 3)\n        actual = kornia.filters.box_blur(inp, kernel_size)\n        assert_allclose(actual[0, 0, 1, 1:4], torch.tensor(1.).to(device))\n\n    def test_kernel_5x5(self, device):\n        inp = torch.tensor([[[\n            [1., 1., 1., 1., 1.],\n            [1., 1., 1., 1., 1.],\n            [1., 1., 1., 1., 1.],\n            [2., 2., 2., 2., 2.],\n            [2., 2., 2., 2., 2.]\n        ]]]).to(device)\n\n        kernel_size = (5, 5)\n        expected = inp.sum((1, 2, 3)) / torch.mul(*kernel_size)\n\n        actual = kornia.filters.box_blur(inp, kernel_size)\n        assert_allclose(actual[:, 0, 2, 2], expected)\n\n    def test_kernel_5x5_batch(self, device):\n        batch_size = 3\n        inp = torch.tensor([[[\n            [1., 1., 1., 1., 1.],\n            [1., 1., 1., 1., 1.],\n            [1., 1., 1., 1., 1.],\n            [2., 2., 2., 2., 2.],\n            [2., 2., 2., 2., 2.]\n        ]]]).repeat(batch_size, 1, 1, 1).to(device)\n\n        kernel_size = (5, 5)\n        expected = inp.sum((1, 2, 3)) / torch.mul(*kernel_size)\n\n        actual = kornia.filters.box_blur(inp, kernel_size)\n        assert_allclose(actual[:, 0, 2, 2], expected)\n\n    def test_noncontiguous(self, device):\n        batch_size = 3\n        inp = torch.rand(3, 5, 5).expand(batch_size, -1, -1, -1).to(device)\n\n        kernel_size = (3, 3)\n        actual = kornia.filters.box_blur(inp, kernel_size)\n        expected = actual\n        assert_allclose(actual, actual)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width).to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.filters.box_blur, (img, (3, 3),),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""undefined value BoxBlur"")\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        op = kornia.filters.box_blur\n        op_script = torch.jit.script(op)\n\n        kernel_size = (3, 3)\n        img = torch.rand(2, 3, 4, 5).to(device)\n        actual = op_script(img, kernel_size)\n        expected = op(img, kernel_size)\n        assert_allclose(actual, expected)\n'"
test/filters/test_filters.py,23,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestFilter2D:\n    def test_smoke(self, device):\n        kernel = torch.rand(1, 3, 3).to(device)\n        input = torch.ones(1, 1, 7, 8).to(device)\n\n        assert kornia.filter2D(input, kernel).shape == input.shape\n\n    def test_mean_filter(self, device):\n        kernel = torch.ones(1, 3, 3).to(device)\n        input = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 5., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n        expected = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 5., 5., 5., 0.],\n            [0., 5., 5., 5., 0.],\n            [0., 5., 5., 5., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n\n        actual = kornia.filter2D(input, kernel)\n        assert_allclose(actual, expected)\n\n    def test_mean_filter_2batch_2ch(self, device):\n        kernel = torch.ones(1, 3, 3).to(device)\n        input = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 5., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).expand(2, 2, -1, -1).to(device)\n        expected = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 5., 5., 5., 0.],\n            [0., 5., 5., 5., 0.],\n            [0., 5., 5., 5., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n\n        actual = kornia.filter2D(input, kernel)\n        assert_allclose(actual, expected)\n\n    def test_normalized_mean_filter(self, device):\n        kernel = torch.ones(1, 3, 3).to(device)\n        input = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 5., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).expand(2, 2, -1, -1).to(device)\n        expected = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 5. / 9., 5. / 9., 5. / 9., 0.],\n            [0., 5. / 9., 5. / 9., 5. / 9., 0.],\n            [0., 5. / 9., 5. / 9., 5. / 9., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n        actual = kornia.filter2D(input, kernel, normalized=True)\n        assert_allclose(actual, expected)\n\n    def test_even_sized_filter(self, device):\n        kernel = torch.ones(1, 2, 2).to(device)\n        input = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 5., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n        expected = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 5., 5., 0., 0.],\n            [0., 5., 5., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n\n        actual = kornia.filter2D(input, kernel)\n        assert_allclose(actual, expected)\n\n    def test_noncontiguous(self, device):\n        batch_size = 3\n        inp = torch.rand(3, 5, 5).expand(batch_size, -1, -1, -1).to(device)\n        kernel = torch.ones(1, 2, 2).to(device)\n\n        actual = kornia.filter2D(inp, kernel)\n        expected = actual\n        assert_allclose(actual, actual)\n\n    def test_gradcheck(self, device):\n        kernel = torch.rand(1, 3, 3).to(device)\n        input = torch.ones(1, 1, 7, 8).to(device)\n\n        # evaluate function gradient\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        kernel = utils.tensor_to_gradcheck_var(kernel)  # to var\n        assert gradcheck(kornia.filter2D, (input, kernel),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""not found compute_padding()"")\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        op = kornia.filter2D\n        op = torch.jit.script(op)\n\n        kernel = torch.rand(1, 3, 3)\n        input = torch.ones(1, 1, 7, 8)\n        expected = op(input, kernel)\n        actual = op_script(input, kernel)\n        assert_allclose(actual, expected)\n'"
test/filters/test_gaussian.py,11,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\n@pytest.mark.parametrize(""window_size"", [5, 11])\n@pytest.mark.parametrize(""sigma"", [1.5, 5.0])\ndef test_get_gaussian_kernel(window_size, sigma):\n    kernel = kornia.get_gaussian_kernel1d(window_size, sigma)\n    assert kernel.shape == (window_size,)\n    assert kernel.sum().item() == pytest.approx(1.0)\n\n\n@pytest.mark.parametrize(""ksize_x"", [5, 11])\n@pytest.mark.parametrize(""ksize_y"", [3, 7])\n@pytest.mark.parametrize(""sigma"", [1.5, 2.1])\ndef test_get_gaussian_kernel2d(ksize_x, ksize_y, sigma):\n    kernel = kornia.get_gaussian_kernel2d((ksize_x, ksize_y), (sigma, sigma))\n    assert kernel.shape == (ksize_x, ksize_y)\n    assert kernel.sum().item() == pytest.approx(1.0)\n\n\nclass TestGaussianBlur:\n    @pytest.mark.parametrize(""batch_shape"", [(1, 4, 8, 15), (2, 3, 11, 7)])\n    def test_gaussian_blur(self, batch_shape, device):\n        kernel_size = (5, 7)\n        sigma = (1.5, 2.1)\n\n        input = torch.rand(batch_shape).to(device)\n        gauss = kornia.filters.GaussianBlur2d(kernel_size, sigma, ""replicate"")\n        assert gauss(input).shape == batch_shape\n\n    def test_noncontiguous(self, device):\n        batch_size = 3\n        inp = torch.rand(3, 5, 5).expand(batch_size, -1, -1, -1).to(device)\n\n        kernel_size = (3, 3)\n        sigma = (1.5, 2.1)\n        gauss = kornia.filters.GaussianBlur2d(kernel_size, sigma, ""replicate"")\n        actual = gauss(inp)\n        expected = actual\n        assert_allclose(actual, actual)\n\n    def test_gradcheck(self, device):\n        # test parameters\n        batch_shape = (2, 3, 11, 7)\n        kernel_size = (5, 3)\n        sigma = (1.5, 2.1)\n\n        # evaluate function gradient\n        input = torch.rand(batch_shape).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(\n            kornia.gaussian_blur2d,\n            (input, kernel_size, sigma, ""replicate""),\n            raise_exception=True,\n        )\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self):\n        @torch.jit.script\n        def op_script(img):\n\n            return kornia.gaussian_blur2d(img, (5, 5), (1.2, 1.2), ""replicate"")\n\n        batch_size, channels, height, width = 2, 3, 64, 64\n        img = torch.ones(batch_size, channels, height, width).to(device)\n        expected = kornia.filters.GaussianBlur2d(\n            (5, 5), (1.2, 1.2), ""replicate""\n        )(img)\n        actual = op_script(img)\n        assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""window_size"", [5])\ndef test_get_laplacian_kernel(window_size):\n    kernel = kornia.get_laplacian_kernel1d(window_size)\n    assert kernel.shape == (window_size,)\n    assert kernel.sum().item() == pytest.approx(0.0)\n\n\n@pytest.mark.parametrize(""window_size"", [7])\ndef test_get_laplacian_kernel2d(window_size):\n    kernel = kornia.get_laplacian_kernel2d(window_size)\n    assert kernel.shape == (window_size, window_size)\n    assert kernel.sum().item() == pytest.approx(0.0)\n    expected = torch.tensor(\n        [\n            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n            [1.0, 1.0, 1.0, -48.0, 1.0, 1.0, 1.0],\n            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n        ]\n    )\n    assert_allclose(expected, kernel)\n\n\nclass TestLaplacian:\n    @pytest.mark.parametrize(""batch_shape"", [(1, 4, 8, 15), (2, 3, 11, 7)])\n    def test_laplacian(self, batch_shape, device):\n        kernel_size = 5\n\n        input = torch.rand(batch_shape).to(device)\n        laplace = kornia.filters.Laplacian(kernel_size)\n        assert laplace(input).shape == batch_shape\n\n    def test_noncontiguous(self, device):\n        batch_size = 3\n        inp = torch.rand(3, 5, 5).expand(batch_size, -1, -1, -1).to(device)\n\n        kernel_size = 3\n        laplace = kornia.filters.Laplacian(kernel_size)\n        actual = laplace(inp)\n        expected = actual\n        assert_allclose(actual, actual)\n\n    def test_gradcheck(self, device):\n        # test parameters\n        batch_shape = (2, 3, 11, 7)\n        kernel_size = 9\n\n        # evaluate function gradient\n        input = torch.rand(batch_shape).to(device)\n        input = utils.tensor_to_gradcheck_var(input)\n        assert gradcheck(\n            kornia.laplacian, (input, kernel_size), raise_exception=True)\n'"
test/filters/test_median.py,13,"b'from typing import Tuple\n\nimport pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\n\nclass TestMedianBlur:\n    def test_shape(self, device):\n        inp = torch.zeros(1, 3, 4, 4).to(device)\n        median = kornia.filters.MedianBlur((3, 3))\n        assert median(inp).shape == (1, 3, 4, 4)\n\n    def test_shape_batch(self, device):\n        inp = torch.zeros(2, 6, 4, 4).to(device)\n        blur = kornia.filters.BoxBlur((3, 3))\n        assert blur(inp).shape == (2, 6, 4, 4)\n\n    def test_kernel_3x3(self, device):\n        inp = torch.tensor([[\n            [0., 0., 0., 0., 0.],\n            [0., 3., 7., 5., 0.],\n            [0., 3., 1., 1., 0.],\n            [0., 6., 9., 2., 0.],\n            [0., 0., 0., 0., 0.]\n        ], [\n            [36., 7.0, 25., 0., 0.],\n            [3.0, 14., 1.0, 0., 0.],\n            [65., 59., 2.0, 0., 0.],\n            [0.0, 0.0, 0.0, 0., 0.],\n            [0.0, 0.0, 0.0, 0., 0.]\n        ]]).repeat(2, 1, 1, 1).to(device)\n\n        kernel_size = (3, 3)\n        actual = kornia.filters.median_blur(inp, kernel_size)\n        assert_allclose(actual[0, 0, 2, 2], torch.tensor(3.).to(device))\n        assert_allclose(actual[0, 1, 1, 1], torch.tensor(14.).to(device))\n\n    def test_noncontiguous(self, device):\n        batch_size = 3\n        inp = torch.rand(3, 5, 5).expand(batch_size, -1, -1, -1).to(device)\n\n        kernel_size = (3, 3)\n        actual = kornia.filters.median_blur(inp, kernel_size)\n        expected = actual\n        assert_allclose(actual, actual)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width).to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.filters.median_blur, (img, (5, 3),),\n                         raise_exception=True)\n\n    @pytest.mark.skip("""")\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input: torch.Tensor,\n                      kernel_size: Tuple[int, int]) -> torch.Tensor:\n            return kornia.filters.median_blur(input, kernel_size)\n        kernel_size = (3, 5)\n        img = torch.rand(2, 3, 4, 5).to(device)\n        actual = op_script(img, kernel_size)\n        expected = kornia.filters.median_blur(img, kernel_size)\n        assert_allclose(actual, expected)\n'"
test/filters/test_motion.py,9,"b'from typing import Tuple\nimport pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\n@pytest.mark.parametrize(""ksize"", [3, 11])\n@pytest.mark.parametrize(""angle"", [0., 360.])\n@pytest.mark.parametrize(""direction"", [-1., 1.])\ndef test_get_motion_kernel2d(ksize, angle, direction):\n    kernel = kornia.get_motion_kernel2d(ksize, angle, direction)\n    assert kernel.shape == (ksize, ksize)\n    assert_allclose(kernel.sum(), 1.)\n\n\nclass TestMotionBlur:\n    @pytest.mark.parametrize(""batch_shape"", [(1, 4, 8, 15), (2, 3, 11, 7)])\n    def test_motion_blur(self, batch_shape, device):\n        ksize = 5\n        angle = 200.\n        direction = 0.3\n\n        input = torch.rand(batch_shape).to(device)\n        motion = kornia.filters.MotionBlur(ksize, angle, direction)\n        assert motion(input).shape == batch_shape\n\n    def test_noncontiguous(self, device):\n        batch_size = 3\n        inp = torch.rand(3, 5, 5).expand(batch_size, -1, -1, -1).to(device)\n\n        kernel_size = 3\n        angle = 200.\n        direction = 0.3\n        actual = kornia.filters.motion_blur(inp, kernel_size, angle, direction)\n        expected = actual\n        assert_allclose(actual, actual)\n\n    def test_gradcheck(self, device):\n        batch_shape = (2, 3, 11, 7)\n        ksize = 9\n        angle = 34.\n        direction = -0.2\n\n        input = torch.rand(batch_shape).to(device)\n        input = utils.tensor_to_gradcheck_var(input)\n        assert gradcheck(\n            kornia.motion_blur,\n            (input, ksize, angle, direction, ""replicate""),\n            raise_exception=True,\n        )\n\n    @pytest.mark.skip("""")\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(\n            input: torch.Tensor,\n            ksize: int,\n            angle: float,\n            direction: float\n        ) -> torch.Tensor:\n            return kornia.filters.motion_blur(input, ksize, angle, direction)\n\n        img = torch.rand(2, 3, 4, 5).to(device)\n        ksize = 5\n        angle = 65.\n        direction = .1\n        actual = op_script(img, ksize, angle, direction)\n        expected = kornia.filters.motion_blur(img, ksize, angle, direction)\n        assert_allclose(actual, expected)\n'"
test/filters/test_sobel.py,32,"b'import pytest\n\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\n\nclass TestSpatialGradient:\n    def test_shape(self, device):\n        inp = torch.zeros(1, 3, 4, 4).to(device)\n        sobel = kornia.filters.SpatialGradient()\n        assert sobel(inp).shape == (1, 3, 2, 4, 4)\n\n    def test_shape_batch(self, device):\n        inp = torch.zeros(2, 6, 4, 4).to(device)\n        sobel = kornia.filters.SpatialGradient()\n        assert sobel(inp).shape == (2, 6, 2, 4, 4)\n\n    def test_edges(self, device):\n        inp = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 0., 1., 0., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 0., 1., 0., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n\n        expected = torch.tensor([[[[\n            [0., 1., 0., -1., 0.],\n            [1., 3., 0., -3., -1.],\n            [2., 4., 0., -4., -2.],\n            [1., 3., 0., -3., -1.],\n            [0., 1., 0., -1., 0.],\n        ], [\n            [0., 1., 2., 1., 0.],\n            [1., 3., 4., 3., 1.],\n            [0., 0., 0., 0., 0],\n            [-1., -3., -4., -3., -1],\n            [0., -1., -2., -1., 0.],\n        ]]]]).to(device)\n\n        edges = kornia.filters.spatial_gradient(inp, normalized=False)\n        assert_allclose(edges, expected)\n\n    def test_edges_norm(self, device):\n        inp = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 0., 1., 0., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 0., 1., 0., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n\n        expected = torch.tensor([[[[\n            [0., 1., 0., -1., 0.],\n            [1., 3., 0., -3., -1.],\n            [2., 4., 0., -4., -2.],\n            [1., 3., 0., -3., -1.],\n            [0., 1., 0., -1., 0.],\n        ], [\n            [0., 1., 2., 1., 0.],\n            [1., 3., 4., 3., 1.],\n            [0., 0., 0., 0., 0],\n            [-1., -3., -4., -3., -1],\n            [0., -1., -2., -1., 0.],\n        ]]]]).to(device) / 8.0\n\n        edges = kornia.filters.spatial_gradient(inp, normalized=True)\n        assert_allclose(edges, expected)\n\n    def test_edges_sep(self, device):\n        inp = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 0., 1., 0., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 0., 1., 0., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n\n        expected = torch.tensor([[[[\n            [0., 0., 0., 0., 0.],\n            [0., 1., 0., -1., 0.],\n            [1., 1., 0., -1., -1.],\n            [0., 1., 0., -1., 0.],\n            [0., 0., 0., 0., 0.]\n        ], [\n            [0., 0., 1., 0., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., -1., -1., -1., 0.],\n            [0., 0., -1., 0., 0.]\n        ]]]]).to(device)\n        edges = kornia.filters.spatial_gradient(inp, \'diff\',\n                                                normalized=False)\n        assert_allclose(edges, expected)\n\n    def test_edges_sep_norm(self, device):\n        inp = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 0., 1., 0., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 0., 1., 0., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n\n        expected = torch.tensor([[[[\n            [0., 0., 0., 0., 0.],\n            [0., 1., 0., -1., 0.],\n            [1., 1., 0., -1., -1.],\n            [0., 1., 0., -1., 0.],\n            [0., 0., 0., 0., 0.]\n        ], [\n            [0., 0., 1., 0., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., -1., -1., -1., 0.],\n            [0., 0., -1., 0., 0.]\n        ]]]]).to(device) / 2.0\n        edges = kornia.filters.spatial_gradient(inp, \'diff\',\n                                                normalized=True)\n        assert_allclose(edges, expected)\n\n    def test_noncontiguous(self, device):\n        batch_size = 3\n        inp = torch.rand(3, 5, 5).expand(batch_size, -1, -1, -1).to(device)\n\n        actual = kornia.filters.spatial_gradient(inp)\n        expected = actual\n        assert_allclose(actual, actual)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width).to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.filters.spatial_gradient, (img,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input):\n            return kornia.filters.spatial_gradient(input)\n        img = torch.rand(2, 3, 4, 5).to(device)\n        actual = op_script(img)\n        expected = kornia.filters.spatial_gradient(img)\n        assert_allclose(actual, expected)\n\n\nclass TestSpatialGradient3d:\n    def test_shape(self, device):\n        inp = torch.zeros(1, 2, 4, 5, 6).to(device)\n        sobel = kornia.filters.SpatialGradient3d()\n        assert sobel(inp).shape == (1, 2, 3, 4, 5, 6)\n\n    def test_shape_batch(self, device):\n        inp = torch.zeros(7, 2, 4, 5, 6).to(device)\n        sobel = kornia.filters.SpatialGradient3d()\n        assert sobel(inp).shape == (7, 2, 3, 4, 5, 6)\n\n    def test_edges(self, device):\n        inp = torch.tensor([[[\n            [[0., 0., 0., 0., 0.],\n             [0., 0., 0., 0., 0.],\n             [0., 0., 1., 0., 0.],\n             [0., 0., 0., 0., 0.],\n             [0., 0., 0., 0., 0.]],\n            [[0., 0., 0., 0., 0.],\n             [0., 0., 1., 0., 0.],\n             [0., 1., 1., 1., 0.],\n             [0., 0., 1., 0., 0.],\n             [0., 0., 0., 0., 0.]],\n            [[0., 0., 0., 0., 0.],\n             [0., 0., 0., 0., 0.],\n             [0., 0., 1., 0., 0.],\n             [0., 0., 0., 0., 0.],\n             [0., 0., 0., 0., 0.]],\n        ]]]).to(device)\n\n        expected = torch.tensor([[[[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.5000, 0.0000, -0.5000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n                                    [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.5000, 0.0000, -0.5000, 0.0000],\n                                     [0.5000, 0.5000, 0.0000, -0.5000, -0.5000],\n                                     [0.0000, 0.5000, 0.0000, -0.5000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n                                    [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.5000, 0.0000, -0.5000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n                                   [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.5000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, -0.5000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n                                    [[0.0000, 0.0000, 0.5000, 0.0000, 0.0000],\n                                     [0.0000, 0.5000, 0.5000, 0.5000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, -0.5000, -0.5000, -0.5000, 0.0000],\n                                     [0.0000, 0.0000, -0.5000, 0.0000, 0.0000]],\n                                    [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.5000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, -0.5000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n                                   [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.5000, 0.0000, 0.0000],\n                                     [0.0000, 0.5000, 0.0000, 0.5000, 0.0000],\n                                     [0.0000, 0.0000, 0.5000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n                                    [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n                                    [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, -0.5000, 0.0000, 0.0000],\n                                     [0.0000, -0.5000, 0.0000, -0.5000, 0.0000],\n                                     [0.0000, 0.0000, -0.5000, 0.0000, 0.0000],\n                                     [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]]])\n\n        expected = expected.to(device)\n        edges = kornia.filters.spatial_gradient3d(inp)\n        assert_allclose(edges, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, depth, height, width = 1, 2, 3, 5, 4\n        img = torch.rand(batch_size, channels, depth, height, width).to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.filters.spatial_gradient3d, (img,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input):\n            return kornia.filters.spatial_gradient(input)\n        img = torch.rand(2, 3, 4, 5).to(device)\n        actual = op_script(img)\n        expected = kornia.filters.spatial_gradient(img)\n        assert_allclose(actual, expected)\n\n\nclass TestSobel:\n    def test_shape(self, device):\n        inp = torch.zeros(1, 3, 4, 4).to(device)\n        sobel = kornia.filters.Sobel()\n        assert sobel(inp).shape == (1, 3, 4, 4)\n\n    def test_shape_batch(self, device):\n        inp = torch.zeros(3, 2, 4, 4).to(device)\n        sobel = kornia.filters.Sobel()\n        assert sobel(inp).shape == (3, 2, 4, 4)\n\n    def test_magnitude(self, device):\n        inp = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 0., 1., 0., 0.],\n            [0., 1., 1., 1., 0.],\n            [0., 0., 1., 0., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n\n        expected = torch.tensor([[[\n            [0., 1.4142, 2.0, 1.4142, 0.],\n            [1.4142, 4.2426, 4.00, 4.2426, 1.4142],\n            [2.0, 4.0000, 0.00, 4.0000, 2.0],\n            [1.4142, 4.2426, 4.00, 4.2426, 1.4142],\n            [0., 1.4142, 2.0, 1.4142, 0.],\n        ]]]).to(device)\n\n        edges = kornia.filters.sobel(inp, normalized=False, eps=0.)\n        assert_allclose(edges, expected)\n\n    def test_noncontiguous(self, device):\n        batch_size = 3\n        inp = torch.rand(3, 5, 5).expand(batch_size, -1, -1, -1).to(device)\n\n        actual = kornia.filters.sobel(inp)\n        expected = actual\n        assert_allclose(actual, actual)\n\n    def test_gradcheck_unnorm(self, device):\n        if ""cuda"" in str(device):\n            pytest.skip(""RuntimeError: Backward is not reentrant, i.e., running backward,"")\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width).to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.filters.sobel, (img, False),\n                         raise_exception=True)\n\n    def test_gradcheck(self, device):\n        if ""cuda"" in str(device):\n            pytest.skip(""RuntimeError: Backward is not reentrant, i.e., running backward,"")\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width).to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.filters.sobel, (img, True),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input):\n            return kornia.filters.sobel(input)\n        img = torch.rand(2, 3, 4, 5).to(device)\n        actual = op_script(img)\n        expected = kornia.filters.sobel(img)\n        assert_allclose(actual, expected)\n'"
test/geometry/__init__.py,0,b''
test/geometry/test_conversions.py,99,"b'from typing import Optional\n\nimport pytest\nimport numpy as np\n\nimport kornia\nfrom kornia.testing import tensor_to_gradcheck_var, create_eye_batch\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\n# based on:\n# https://github.com/ceres-solver/ceres-solver/blob/master/internal/ceres/rotation_test.cc#L271\n\nclass TestAngleAxisToQuaternion:\n\n    def test_smoke(self, device):\n        angle_axis = torch.zeros(3)\n        quaternion = kornia.angle_axis_to_quaternion(angle_axis)\n        assert quaternion.shape == (4,)\n\n    @pytest.mark.parametrize(""batch_size"", (1, 3, 8))\n    def test_smoke_batch(self, device, batch_size):\n        angle_axis = torch.zeros(batch_size, 3).to(device)\n        quaternion = kornia.angle_axis_to_quaternion(angle_axis)\n        assert quaternion.shape == (batch_size, 4)\n\n    def test_zero_angle(self, device):\n        angle_axis = torch.tensor([0., 0., 0.]).to(device)\n        expected = torch.tensor([1., 0., 0., 0.]).to(device)\n        quaternion = kornia.angle_axis_to_quaternion(angle_axis)\n        assert_allclose(quaternion, expected)\n\n    def test_small_angle(self, device, dtype):\n        theta = 1e-2\n        angle_axis = torch.tensor([theta, 0., 0.]).to(device, dtype)\n        expected = torch.tensor([np.cos(theta / 2), np.sin(theta / 2), 0., 0.]).to(device, dtype)\n        quaternion = kornia.angle_axis_to_quaternion(angle_axis)\n        assert_allclose(quaternion, expected)\n\n    def test_x_rotation(self, device, dtype):\n        half_sqrt2 = 0.5 * np.sqrt(2)\n        angle_axis = torch.tensor([kornia.pi / 2, 0., 0.]).to(device, dtype)\n        expected = torch.tensor([half_sqrt2, half_sqrt2, 0., 0.]).to(device, dtype)\n        quaternion = kornia.angle_axis_to_quaternion(angle_axis)\n        assert_allclose(quaternion, expected)\n\n    def test_gradcheck(self, device):\n        eps = 1e-12\n        angle_axis = torch.tensor([0., 0., 0.]).to(device) + eps\n        angle_axis = tensor_to_gradcheck_var(angle_axis)\n        # evaluate function gradient\n        assert gradcheck(kornia.angle_axis_to_quaternion, (angle_axis,),\n                         raise_exception=True)\n\n\nclass TestRotationMatrixToQuaternion:\n\n    @pytest.mark.parametrize(""batch_size"", (1, 3, 8))\n    def test_smoke_batch(self, device, batch_size):\n        matrix = torch.zeros(batch_size, 3, 3).to(device)\n        quaternion = kornia.rotation_matrix_to_quaternion(matrix)\n        assert quaternion.shape == (batch_size, 4)\n\n    def test_identity(self, device):\n        matrix = torch.tensor([\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]).to(device)\n        expected = torch.tensor(\n            [0., 0., 0., 1.],\n        ).to(device)\n        quaternion = kornia.rotation_matrix_to_quaternion(matrix)\n        assert_allclose(quaternion, expected)\n\n    def test_rot_x_45(self, device):\n        matrix = torch.tensor([\n            [1., 0., 0.],\n            [0., 0., -1.],\n            [0., 1., 0.],\n        ]).to(device)\n        pi_half2 = torch.cos(kornia.pi / 4).to(device)\n        expected = torch.tensor(\n            [pi_half2, 0., 0., pi_half2],\n        ).to(device)\n        quaternion = kornia.rotation_matrix_to_quaternion(matrix)\n        assert_allclose(quaternion, expected)\n\n    def test_back_and_forth(self, device):\n        matrix = torch.tensor([\n            [1., 0., 0.],\n            [0., 0., -1.],\n            [0., 1., 0.],\n        ]).to(device)\n        quaternion = kornia.rotation_matrix_to_quaternion(matrix)\n        matrix_hat = kornia.quaternion_to_rotation_matrix(quaternion)\n        assert_allclose(matrix, matrix_hat)\n\n    def test_corner_case(self, device):\n        matrix = torch.tensor([\n            [-0.7799533010, -0.5432914495, 0.3106555045],\n            [0.0492402576, -0.5481169224, -0.8349509239],\n            [0.6238971353, -0.6359263659, 0.4542570710]\n        ]).to(device)\n        quaternion_true = torch.tensor([0.280136495828629, -0.440902262926102,\n                                        0.834015488624573, 0.177614107728004]).to(device)\n        quaternion = kornia.rotation_matrix_to_quaternion(matrix)\n        torch.set_printoptions(precision=10)\n        assert_allclose(quaternion_true, quaternion)\n\n    def test_gradcheck(self, device):\n        matrix = torch.eye(3).to(device)\n        matrix = tensor_to_gradcheck_var(matrix)\n        # evaluate function gradient\n        assert gradcheck(kornia.rotation_matrix_to_quaternion, (matrix,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        op = kornia.quaternion_log_to_exp\n        op_script = torch.jit.script(op)\n\n        quaternion = torch.tensor([0., 0., 1.]).to(device)\n        actual = op_script(quaternion)\n        expected = op(quaternion)\n        assert_allclose(actual, expected)\n\n\nclass TestQuaternionToRotationMatrix:\n\n    @pytest.mark.parametrize(""batch_size"", (1, 3, 8))\n    def test_smoke_batch(self, device, batch_size):\n        quaternion = torch.zeros(batch_size, 4).to(device)\n        matrix = kornia.quaternion_to_rotation_matrix(quaternion)\n        assert matrix.shape == (batch_size, 3, 3)\n\n    def test_unit_quaternion(self, device):\n        quaternion = torch.tensor([0., 0., 0., 1.]).to(device)\n        expected = torch.tensor([\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]).to(device)\n        matrix = kornia.quaternion_to_rotation_matrix(quaternion)\n        assert_allclose(matrix, expected)\n\n    def test_x_rotation(self, device):\n        quaternion = torch.tensor([1., 0., 0., 0.]).to(device)\n        expected = torch.tensor([\n            [1., 0., 0.],\n            [0., -1., 0.],\n            [0., 0., -1.],\n        ]).to(device)\n        matrix = kornia.quaternion_to_rotation_matrix(quaternion)\n        assert_allclose(matrix, expected)\n\n    def test_y_rotation(self, device):\n        quaternion = torch.tensor([0., 1., 0., 0.]).to(device)\n        expected = torch.tensor([\n            [-1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., -1.],\n        ]).to(device)\n        matrix = kornia.quaternion_to_rotation_matrix(quaternion)\n        assert_allclose(matrix, expected)\n\n    def test_z_rotation(self, device):\n        quaternion = torch.tensor([0., 0., 1., 0.]).to(device)\n        expected = torch.tensor([\n            [-1., 0., 0.],\n            [0., -1., 0.],\n            [0., 0., 1.],\n        ]).to(device)\n        matrix = kornia.quaternion_to_rotation_matrix(quaternion)\n        assert_allclose(matrix, expected)\n\n    def test_gradcheck(self, device):\n        quaternion = torch.tensor([0., 0., 0., 1.]).to(device)\n        quaternion = tensor_to_gradcheck_var(quaternion)\n        # evaluate function gradient\n        assert gradcheck(kornia.quaternion_to_rotation_matrix, (quaternion,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input):\n            return kornia.quaternion_to_rotation_matrix(input)\n\n        quaternion = torch.tensor([0., 0., 1., 0.]).to(device)\n        actual = op_script(quaternion)\n        expected = kornia.quaternion_to_rotation_matrix(quaternion)\n        assert_allclose(actual, expected)\n\n\nclass TestQuaternionLogToExp:\n\n    @pytest.mark.parametrize(""batch_size"", (1, 3, 8))\n    def test_smoke_batch(self, device, batch_size):\n        quaternion_log = torch.zeros(batch_size, 3).to(device)\n        quaternion_exp = kornia.quaternion_log_to_exp(quaternion_log)\n        assert quaternion_exp.shape == (batch_size, 4)\n\n    def test_unit_quaternion(self, device):\n        quaternion_log = torch.tensor([0., 0., 0.]).to(device)\n        expected = torch.tensor([0., 0., 0., 1.]).to(device)\n        assert_allclose(kornia.quaternion_log_to_exp(quaternion_log), expected)\n\n    def test_pi_quaternion(self, device):\n        one = torch.tensor(1.).to(device)\n        quaternion_log = torch.tensor([1., 0., 0.]).to(device)\n        expected = torch.tensor([torch.sin(one), 0., 0., torch.cos(one)]).to(device)\n        assert_allclose(kornia.quaternion_log_to_exp(quaternion_log), expected)\n\n    def test_back_and_forth(self, device):\n        quaternion_log = torch.tensor([0., 0., 0.]).to(device)\n        quaternion_exp = kornia.quaternion_log_to_exp(quaternion_log)\n        quaternion_log_hat = kornia.quaternion_exp_to_log(quaternion_exp)\n        assert_allclose(quaternion_log, quaternion_log_hat)\n\n    def test_gradcheck(self, device):\n        quaternion = torch.tensor([0., 0., 1.]).to(device)\n        quaternion = tensor_to_gradcheck_var(quaternion)\n        # evaluate function gradient\n        assert gradcheck(kornia.quaternion_log_to_exp, (quaternion,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        op = kornia.quaternion_log_to_exp\n        op_script = torch.jit.script(op)\n\n        quaternion = torch.tensor([0., 0., 1.]).to(device)\n        actual = op_script(quaternion)\n        expected = op(quaternion)\n        assert_allclose(actual, expected)\n\n\nclass TestQuaternionExpToLog:\n\n    @pytest.mark.parametrize(""batch_size"", (1, 3, 8))\n    def test_smoke_batch(self, device, batch_size):\n        quaternion_exp = torch.zeros(batch_size, 4).to(device)\n        quaternion_log = kornia.quaternion_exp_to_log(quaternion_exp)\n        assert quaternion_log.shape == (batch_size, 3)\n\n    def test_unit_quaternion(self, device):\n        quaternion_exp = torch.tensor([0., 0., 0., 1.]).to(device)\n        expected = torch.tensor([0., 0., 0.]).to(device)\n        assert_allclose(kornia.quaternion_exp_to_log(quaternion_exp), expected)\n\n    def test_pi_quaternion(self, device):\n        quaternion_exp = torch.tensor([1., 0., 0., 0.]).to(device)\n        expected = torch.tensor([kornia.pi / 2, 0., 0.]).to(device)\n        assert_allclose(kornia.quaternion_exp_to_log(quaternion_exp), expected)\n\n    def test_back_and_forth(self, device):\n        quaternion_exp = torch.tensor([1., 0., 0., 0.]).to(device)\n        quaternion_log = kornia.quaternion_exp_to_log(quaternion_exp)\n        quaternion_exp_hat = kornia.quaternion_log_to_exp(quaternion_log)\n        assert_allclose(quaternion_exp, quaternion_exp_hat)\n\n    def test_gradcheck(self, device):\n        quaternion = torch.tensor([1., 0., 0., 0.]).to(device)\n        quaternion = tensor_to_gradcheck_var(quaternion)\n        # evaluate function gradient\n        assert gradcheck(kornia.quaternion_exp_to_log, (quaternion,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        op = kornia.quaternion_exp_to_log\n        op_script = torch.jit.script(op)\n\n        quaternion = torch.tensor([0., 0., 1., 0.]).to(device)\n        actual = op_script(quaternion)\n        expected = op(quaternion)\n        assert_allclose(actual, expected)\n\n\nclass TestQuaternionToAngleAxis:\n\n    def test_smoke(self, device):\n        quaternion = torch.zeros(4).to(device)\n        angle_axis = kornia.quaternion_to_angle_axis(quaternion)\n        assert angle_axis.shape == (3,)\n\n    @pytest.mark.parametrize(""batch_size"", (1, 3, 8))\n    def test_smoke_batch(self, device, batch_size):\n        quaternion = torch.zeros(batch_size, 4).to(device)\n        angle_axis = kornia.quaternion_to_angle_axis(quaternion)\n        assert angle_axis.shape == (batch_size, 3)\n\n    def test_unit_quaternion(self, device):\n        quaternion = torch.tensor([1., 0., 0., 0.]).to(device)\n        expected = torch.tensor([0., 0., 0.]).to(device)\n        angle_axis = kornia.quaternion_to_angle_axis(quaternion)\n        assert_allclose(angle_axis, expected)\n\n    def test_y_rotation(self, device):\n        quaternion = torch.tensor([0., 0., 1., 0.]).to(device)\n        expected = torch.tensor([0., kornia.pi, 0.]).to(device)\n        angle_axis = kornia.quaternion_to_angle_axis(quaternion)\n        assert_allclose(angle_axis, expected)\n\n    def test_z_rotation(self, device, dtype):\n        quaternion = torch.tensor([np.sqrt(3) / 2, 0., 0., 0.5]).to(device, dtype)\n        expected = torch.tensor([0., 0., kornia.pi / 3]).to(device, dtype)\n        angle_axis = kornia.quaternion_to_angle_axis(quaternion)\n        assert_allclose(angle_axis, expected)\n\n    def test_small_angle(self, device, dtype):\n        theta = 1e-2\n        quaternion = torch.tensor([np.cos(theta / 2), np.sin(theta / 2), 0., 0.]).to(device, dtype)\n        expected = torch.tensor([theta, 0., 0.]).to(device, dtype)\n        angle_axis = kornia.quaternion_to_angle_axis(quaternion)\n        assert_allclose(angle_axis, expected)\n\n    def test_gradcheck(self, device):\n        eps = 1e-12\n        quaternion = torch.tensor([1., 0., 0., 0.]).to(device) + eps\n        quaternion = tensor_to_gradcheck_var(quaternion)\n        # evaluate function gradient\n        assert gradcheck(kornia.quaternion_to_angle_axis, (quaternion,),\n                         raise_exception=True)\n\n\ndef test_pi():\n    assert_allclose(kornia.pi, 3.141592)\n\n\n@pytest.mark.parametrize(""batch_shape"", [\n    (2, 3), (1, 2, 3), (2, 3, 3), (5, 5, 3), ])\ndef test_rad2deg(batch_shape, device):\n    # generate input data\n    x_rad = kornia.pi * torch.rand(batch_shape)\n    x_rad = x_rad.to(device)\n\n    # convert radians/degrees\n    x_deg = kornia.rad2deg(x_rad)\n    x_deg_to_rad = kornia.deg2rad(x_deg)\n\n    # compute error\n    assert_allclose(x_rad, x_deg_to_rad)\n\n    # evaluate function gradient\n    assert gradcheck(kornia.rad2deg, (tensor_to_gradcheck_var(x_rad),),\n                     raise_exception=True)\n\n\n@pytest.mark.parametrize(""batch_shape"", [\n    (2, 3), (1, 2, 3), (2, 3, 3), (5, 5, 3), ])\ndef test_deg2rad(batch_shape, device):\n    # generate input data\n    x_deg = 180. * torch.rand(batch_shape)\n    x_deg = x_deg.to(device)\n\n    # convert radians/degrees\n    x_rad = kornia.deg2rad(x_deg)\n    x_rad_to_deg = kornia.rad2deg(x_rad)\n\n    assert_allclose(x_deg, x_rad_to_deg)\n\n    assert gradcheck(kornia.deg2rad, (tensor_to_gradcheck_var(x_deg),),\n                     raise_exception=True)\n\n\nclass TestConvertPointsToHomogeneous:\n    def test_convert_points(self, device):\n        # generate input data\n        points_h = torch.tensor([\n            [1., 2., 1.],\n            [0., 1., 2.],\n            [2., 1., 0.],\n            [-1., -2., -1.],\n            [0., 1., -2.],\n        ]).to(device)\n\n        expected = torch.tensor([\n            [1., 2., 1., 1.],\n            [0., 1., 2., 1.],\n            [2., 1., 0., 1.],\n            [-1., -2., -1., 1.],\n            [0., 1., -2., 1.],\n        ]).to(device)\n\n        # to euclidean\n        points = kornia.convert_points_to_homogeneous(points_h)\n        assert_allclose(points, expected)\n\n    def test_convert_points_batch(self, device):\n        # generate input data\n        points_h = torch.tensor([[\n            [2., 1., 0.],\n        ], [\n            [0., 1., 2.],\n        ], [\n            [0., 1., -2.],\n        ]]).to(device)\n\n        expected = torch.tensor([[\n            [2., 1., 0., 1.],\n        ], [\n            [0., 1., 2., 1.],\n        ], [\n            [0., 1., -2., 1.],\n        ]]).to(device)\n\n        # to euclidean\n        points = kornia.convert_points_to_homogeneous(points_h)\n        assert_allclose(points, expected)\n\n    @pytest.mark.parametrize(""batch_shape"", [\n        (2, 3), (1, 2, 3), (2, 3, 3), (5, 5, 3), ])\n    def test_gradcheck(self, device, batch_shape):\n        points_h = torch.rand(batch_shape).to(device)\n\n        # evaluate function gradient\n        points_h = tensor_to_gradcheck_var(points_h)  # to var\n        assert gradcheck(kornia.convert_points_to_homogeneous, (points_h,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        op = kornia.convert_points_to_homogeneous\n        op_script = torch.jit.script(op)\n\n        points_h = torch.zeros(1, 2, 3).to(device)\n        actual = op_script(points_h)\n        expected = op(points_h)\n\n        assert_allclose(actual, expected)\n\n\nclass TestConvertAtoH:\n    def test_convert_points(self, device):\n        # generate input data\n        A = torch.tensor([\n            [1., 0., 0.],\n            [0., 1., 0.],\n        ]).to(device).view(1, 2, 3)\n\n        expected = torch.tensor([\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]).to(device).view(1, 3, 3)\n\n        # to euclidean\n        H = kornia.geometry.conversions.convert_affinematrix_to_homography(A)\n        assert_allclose(H, expected)\n\n    @pytest.mark.parametrize(""batch_shape"", [\n        (10, 2, 3), (16, 2, 3)])\n    def test_gradcheck(self, device, batch_shape):\n        points_h = torch.rand(batch_shape).to(device)\n\n        # evaluate function gradient\n        points_h = tensor_to_gradcheck_var(points_h)  # to var\n        assert gradcheck(kornia.convert_affinematrix_to_homography, (points_h,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        op = kornia.convert_affinematrix_to_homography\n        op_script = torch.jit.script(op)\n\n        points_h = torch.zeros(1, 2, 3).to(device)\n        actual = op_script(points_h)\n        expected = op(points_h)\n\n        assert_allclose(actual, expected)\n\n\nclass TestConvertPointsFromHomogeneous:\n    def test_convert_points(self, device):\n        # generate input data\n        points_h = torch.tensor([\n            [1., 2., 1.],\n            [0., 1., 2.],\n            [2., 1., 0.],\n            [-1., -2., -1.],\n            [0., 1., -2.],\n        ]).to(device)\n\n        expected = torch.tensor([\n            [1., 2.],\n            [0., 0.5],\n            [2., 1.],\n            [1., 2.],\n            [0., -0.5],\n        ]).to(device)\n\n        # to euclidean\n        points = kornia.convert_points_from_homogeneous(points_h)\n        assert_allclose(points, expected)\n\n    def test_convert_points_batch(self, device):\n        # generate input data\n        points_h = torch.tensor([[\n            [2., 1., 0.],\n        ], [\n            [0., 1., 2.],\n        ], [\n            [0., 1., -2.],\n        ]]).to(device)\n\n        expected = torch.tensor([[\n            [2., 1.],\n        ], [\n            [0., 0.5],\n        ], [\n            [0., -0.5],\n        ]]).to(device)\n\n        # to euclidean\n        points = kornia.convert_points_from_homogeneous(points_h)\n        assert_allclose(points, expected)\n\n    @pytest.mark.parametrize(""batch_shape"", [\n        (2, 3), (1, 2, 3), (2, 3, 3), (5, 5, 3), ])\n    def test_gradcheck(self, device, batch_shape):\n        points_h = torch.rand(batch_shape).to(device)\n\n        # evaluate function gradient\n        points_h = tensor_to_gradcheck_var(points_h)  # to var\n        assert gradcheck(kornia.convert_points_from_homogeneous, (points_h,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        op = kornia.convert_points_from_homogeneous\n        op_script = torch.jit.script(op)\n\n        points_h = torch.zeros(1, 2, 3).to(device)\n        actual = op_script(points_h)\n        expected = op(points_h)\n\n        assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""batch_size"", [1, 2, 5])\ndef test_angle_axis_to_rotation_matrix(batch_size, device):\n    # generate input data\n    angle_axis = torch.rand(batch_size, 3).to(device)\n    eye_batch = create_eye_batch(batch_size, 3).to(device)\n\n    # apply transform\n    rotation_matrix = kornia.angle_axis_to_rotation_matrix(angle_axis)\n\n    rotation_matrix_eye = torch.matmul(\n        rotation_matrix, rotation_matrix.transpose(1, 2))\n    assert_allclose(rotation_matrix_eye, eye_batch)\n\n    # evaluate function gradient\n    angle_axis = tensor_to_gradcheck_var(angle_axis)  # to var\n    assert gradcheck(kornia.angle_axis_to_rotation_matrix, (angle_axis,),\n                     raise_exception=True)\n\n\n\'\'\'@pytest.mark.parametrize(""batch_size"", [1, 2, 5])\ndef test_rotation_matrix_to_angle_axis_gradcheck(batch_size, device_type):\n    # generate input data\n    rmat = torch.rand(batch_size, 3, 3).to(torch.device(device_type))\n\n    # evaluate function gradient\n    rmat = tensor_to_gradcheck_var(rmat)  # to var\n    assert gradcheck(kornia.rotation_matrix_to_angle_axis,\n                     (rmat,), raise_exception=True)\'\'\'\n\n\n\'\'\'def test_rotation_matrix_to_angle_axis(device_type):\n    device = torch.device(device_type)\n    rmat_1 = torch.tensor([[-0.30382753, -0.95095137, -0.05814062],\n                           [-0.71581715, 0.26812278, -0.64476041],\n                           [0.62872461, -0.15427791, -0.76217038]])\n    rvec_1 = torch.tensor([1.50485376, -2.10737739, 0.7214174])\n\n    rmat_2 = torch.tensor([[0.6027768, -0.79275544, -0.09054801],\n                           [-0.67915707, -0.56931658, 0.46327563],\n                           [-0.41881476, -0.21775548, -0.88157628]])\n    rvec_2 = torch.tensor([-2.44916812, 1.18053411, 0.4085298])\n    rmat = torch.stack([rmat_2, rmat_1], dim=0).to(device)\n    rvec = torch.stack([rvec_2, rvec_1], dim=0).to(device)\n\n    assert_allclose(kornia.rotation_matrix_to_angle_axis(rmat), rvec)\'\'\'\n\n\nclass TestNormalizePixelCoordinates:\n    def test_tensor_bhw2(self, device):\n        height, width = 3, 4\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n\n        expected = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=True).to(device)\n\n        grid_norm = kornia.normalize_pixel_coordinates(\n            grid, height, width)\n\n        assert_allclose(grid_norm, expected)\n\n    def test_list(self, device):\n        height, width = 3, 4\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n        grid = grid.contiguous().view(-1, 2)\n\n        expected = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=True).to(device)\n        expected = expected.contiguous().view(-1, 2)\n\n        grid_norm = kornia.normalize_pixel_coordinates(\n            grid, height, width)\n\n        assert_allclose(grid_norm, expected)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        op = kornia.normalize_pixel_coordinates\n        op_script = torch.jit.script(op)\n\n        height, width = 3, 4\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=True).to(device)\n\n        actual = op_script(grid, height, width)\n        expected = op(grid, height, width)\n\n        assert_allclose(actual, expected)\n\n\nclass TestDenormalizePixelCoordinates:\n    def test_tensor_bhw2(self, device):\n        height, width = 3, 4\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=True).to(device)\n\n        expected = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n\n        grid_norm = kornia.denormalize_pixel_coordinates(\n            grid, height, width)\n\n        assert_allclose(grid_norm, expected)\n\n    def test_list(self, device):\n        height, width = 3, 4\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=True).to(device)\n        grid = grid.contiguous().view(-1, 2)\n\n        expected = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n        expected = expected.contiguous().view(-1, 2)\n\n        grid_norm = kornia.denormalize_pixel_coordinates(\n            grid, height, width)\n\n        assert_allclose(grid_norm, expected)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        op = kornia.denormalize_pixel_coordinates\n        op_script = torch.jit.script(op)\n\n        height, width = 3, 4\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=True).to(device)\n\n        actual = op_script(grid, height, width)\n        expected = op(grid, height, width)\n\n        assert_allclose(actual, expected)\n'"
test/geometry/test_depth.py,54,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestDepthTo3d:\n    def test_smoke(self, device):\n        depth = torch.rand(1, 1, 3, 4).to(device)\n        camera_matrix = torch.rand(1, 3, 3).to(device)\n\n        points3d = kornia.depth_to_3d(depth, camera_matrix)\n        assert points3d.shape == (1, 3, 3, 4)\n\n    @pytest.mark.parametrize(""batch_size"", [2, 4, 5])\n    def test_shapes(self, device, batch_size):\n        depth = torch.rand(batch_size, 1, 3, 4).to(device)\n        camera_matrix = torch.rand(batch_size, 3, 3).to(device)\n\n        points3d = kornia.depth_to_3d(depth, camera_matrix)\n        assert points3d.shape == (batch_size, 3, 3, 4)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 4, 5])\n    def test_shapes_broadcast(self, device, batch_size):\n        depth = torch.rand(batch_size, 1, 3, 4).to(device)\n        camera_matrix = torch.rand(1, 3, 3).to(device)\n\n        points3d = kornia.depth_to_3d(depth, camera_matrix)\n        assert points3d.shape == (batch_size, 3, 3, 4)\n\n    def test_unproject_denormalized(self, device):\n        # this is for default normalize_points=False\n        depth = 2 * torch.tensor([[[\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n        ]]]).to(device)\n\n        camera_matrix = torch.tensor([[\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]]).to(device)\n\n        points3d_expected = torch.tensor([[[\n            [0., 2., 4.],\n            [0., 2., 4.],\n            [0., 2., 4.],\n            [0., 2., 4.],\n        ], [\n            [0., 0., 0.],\n            [2., 2., 2.],\n            [4., 4., 4.],\n            [6., 6., 6.],\n        ], [\n            [2., 2., 2.],\n            [2., 2., 2.],\n            [2., 2., 2.],\n            [2., 2., 2.],\n        ]]]).to(device)\n\n        points3d = kornia.depth_to_3d(depth, camera_matrix)  # default is normalize_points=False\n        assert_allclose(points3d, points3d_expected)\n\n    def test_unproject_normalized(self, device):\n        # this is for normalize_points=True\n        depth = 2 * torch.tensor([[[\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n        ]]]).to(device)\n\n        camera_matrix = torch.tensor([[\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]]).to(device)\n\n        points3d_expected = torch.tensor([[[\n            [0.0000, 1.4142, 1.7889],\n            [0.0000, 1.1547, 1.6330],\n            [0.0000, 0.8165, 1.3333],\n            [0.0000, 0.6030, 1.0690],\n        ], [\n            [0.0000, 0.0000, 0.0000],\n            [1.4142, 1.1547, 0.8165],\n            [1.7889, 1.6330, 1.3333],\n            [1.8974, 1.8091, 1.6036],\n        ], [\n            [2.0000, 1.4142, 0.8944],\n            [1.4142, 1.1547, 0.8165],\n            [0.8944, 0.8165, 0.6667],\n            [0.6325, 0.6030, 0.5345],\n        ]]]).to(device)\n\n        points3d = kornia.depth_to_3d(depth, camera_matrix, normalize_points=True)\n        assert_allclose(points3d, points3d_expected)\n\n    def test_unproject_and_project(self, device):\n        depth = 2 * torch.tensor([[[\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n        ]]]).to(device)\n\n        camera_matrix = torch.tensor([[\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]]).to(device)\n\n        points3d = kornia.depth_to_3d(depth, camera_matrix)\n        points2d = kornia.project_points(\n            points3d.permute(0, 2, 3, 1),\n            camera_matrix[:, None, None]\n        )\n        points2d_expected = kornia.create_meshgrid(4, 3, False).to(device)\n        assert_allclose(points2d, points2d_expected)\n\n    def test_gradcheck(self, device):\n        # generate input data\n        depth = torch.rand(1, 1, 3, 4).to(device)\n        depth = utils.tensor_to_gradcheck_var(depth)  # to var\n\n        camera_matrix = torch.rand(1, 3, 3).to(device)\n        camera_matrix = utils.tensor_to_gradcheck_var(camera_matrix)  # to var\n\n        # evaluate function gradient\n        assert gradcheck(kornia.depth_to_3d, (depth, camera_matrix,),\n                         raise_exception=True)\n\n\nclass TestDepthToNormals:\n    def test_smoke(self, device):\n        depth = torch.rand(1, 1, 3, 4).to(device)\n        camera_matrix = torch.rand(1, 3, 3).to(device)\n\n        points3d = kornia.depth_to_normals(depth, camera_matrix)\n        assert points3d.shape == (1, 3, 3, 4)\n\n    @pytest.mark.parametrize(""batch_size"", [2, 4, 5])\n    def test_shapes(self, device, batch_size):\n        depth = torch.rand(batch_size, 1, 3, 4).to(device)\n        camera_matrix = torch.rand(batch_size, 3, 3).to(device)\n\n        points3d = kornia.depth_to_normals(depth, camera_matrix)\n        assert points3d.shape == (batch_size, 3, 3, 4)\n\n    @pytest.mark.parametrize(""batch_size"", [2, 4, 5])\n    def test_shapes_broadcast(self, device, batch_size):\n        depth = torch.rand(batch_size, 1, 3, 4).to(device)\n        camera_matrix = torch.rand(1, 3, 3).to(device)\n\n        points3d = kornia.depth_to_normals(depth, camera_matrix)\n        assert points3d.shape == (batch_size, 3, 3, 4)\n\n    def test_simple(self, device):\n        # this is for default normalize_points=False\n        depth = 2 * torch.tensor([[[\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n        ]]]).to(device)\n\n        camera_matrix = torch.tensor([[\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]]).to(device)\n\n        normals_expected = torch.tensor([[[\n            [0., 0., 0.],\n            [0., 0., 0.],\n            [0., 0., 0.],\n            [0., 0., 0.],\n        ], [\n            [0., 0., 0.],\n            [0., 0., 0.],\n            [0., 0., 0.],\n            [0., 0., 0.],\n        ], [\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n        ]]]).to(device)\n\n        normals = kornia.depth_to_normals(depth, camera_matrix)  # default is normalize_points=False\n        assert_allclose(normals, normals_expected, 1e-3, 1e-3)\n\n    def test_simple_normalized(self, device):\n        # this is for default normalize_points=False\n        depth = 2 * torch.tensor([[[\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n        ]]]).to(device)\n\n        camera_matrix = torch.tensor([[\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]]).to(device)\n\n        normals_expected = torch.tensor([[[\n            [0.3432, 0.4861, 0.7628],\n            [0.2873, 0.4260, 0.6672],\n            [0.2284, 0.3683, 0.5596],\n            [0.1695, 0.2980, 0.4496],\n        ], [\n            [0.3432, 0.2873, 0.2363],\n            [0.4861, 0.4260, 0.3785],\n            [0.8079, 0.7261, 0.6529],\n            [0.8948, 0.8237, 0.7543],\n        ], [\n            [0.8743, 0.8253, 0.6019],\n            [0.8253, 0.7981, 0.6415],\n            [0.5432, 0.5807, 0.5105],\n            [0.4129, 0.4824, 0.4784],\n        ]]]).to(device)\n\n        normals = kornia.depth_to_normals(depth, camera_matrix, normalize_points=True)\n        assert_allclose(normals, normals_expected, 1e-3, 1e-3)\n\n    def test_gradcheck(self, device):\n        # generate input data\n        depth = torch.rand(1, 1, 3, 4).to(device)\n        depth = utils.tensor_to_gradcheck_var(depth)  # to var\n\n        camera_matrix = torch.rand(1, 3, 3).to(device)\n        camera_matrix = utils.tensor_to_gradcheck_var(camera_matrix)  # to var\n\n        # evaluate function gradient\n        assert gradcheck(kornia.depth_to_normals, (depth, camera_matrix,),\n                         raise_exception=True)\n\n\nclass TestWarpFrameDepth:\n    def test_smoke(self, device):\n        image_src = torch.rand(1, 3, 3, 4).to(device)\n        depth_dst = torch.rand(1, 1, 3, 4).to(device)\n        src_trans_dst = torch.rand(1, 4, 4).to(device)\n        camera_matrix = torch.rand(1, 3, 3).to(device)\n\n        image_dst = kornia.warp_frame_depth(image_src, depth_dst, src_trans_dst, camera_matrix)\n        assert image_dst.shape == (1, 3, 3, 4)\n\n    @pytest.mark.parametrize(""batch_size"", [2, 4, 5])\n    @pytest.mark.parametrize(""num_features"", [1, 3, 5])\n    def test_shape(self, device, batch_size, num_features):\n        image_src = torch.rand(batch_size, num_features, 3, 4).to(device)\n        depth_dst = torch.rand(batch_size, 1, 3, 4).to(device)\n        src_trans_dst = torch.rand(batch_size, 4, 4).to(device)\n        camera_matrix = torch.rand(batch_size, 3, 3).to(device)\n\n        image_dst = kornia.warp_frame_depth(image_src, depth_dst, src_trans_dst, camera_matrix)\n        assert image_dst.shape == (batch_size, num_features, 3, 4)\n\n    def test_translation(self, device):\n        # this is for normalize_points=False\n        image_src = torch.tensor([[[\n            [1., 2., 3.],\n            [1., 2., 3.],\n            [1., 2., 3.],\n            [1., 2., 3.],\n        ]]]).to(device)\n\n        depth_dst = torch.tensor([[[\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n        ]]]).to(device)\n\n        src_trans_dst = torch.tensor([[\n            [1., 0., 0., 1.],\n            [0., 1., 0., 0.],\n            [0., 0., 1., 0.],\n            [0., 0., 0., 1.],\n        ]]).to(device)\n\n        h, w = image_src.shape[-2:]\n        camera_matrix = torch.tensor([[\n            [1., 0., w / 2],\n            [0., 1., h / 2],\n            [0., 0., 1.],\n        ]]).to(device)\n\n        image_dst_expected = torch.tensor([[[\n            [2., 3., 0.],\n            [2., 3., 0.],\n            [2., 3., 0.],\n            [2., 3., 0.],\n        ]]]).to(device)\n\n        image_dst = kornia.warp_frame_depth(\n            image_src, depth_dst, src_trans_dst, camera_matrix)  # default is normalize_points=False\n        assert_allclose(image_dst, image_dst_expected, 1e-3, 1e-3)\n\n    def test_translation_normalized(self, device):\n        # this is for normalize_points=True\n        image_src = torch.tensor([[[\n            [1., 2., 3.],\n            [1., 2., 3.],\n            [1., 2., 3.],\n            [1., 2., 3.],\n        ]]]).to(device)\n\n        depth_dst = torch.tensor([[[\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n            [1., 1., 1.],\n        ]]]).to(device)\n\n        src_trans_dst = torch.tensor([[\n            [1., 0., 0., 1.],\n            [0., 1., 0., 0.],\n            [0., 0., 1., 0.],\n            [0., 0., 0., 1.],\n        ]]).to(device)\n\n        h, w = image_src.shape[-2:]\n        camera_matrix = torch.tensor([[\n            [1., 0., w / 2],\n            [0., 1., h / 2],\n            [0., 0., 1.],\n        ]]).to(device)\n\n        image_dst_expected = torch.tensor([[[\n            [0.9223, 0.0000, 0.0000],\n            [2.8153, 1.5000, 0.0000],\n            [2.8028, 2.6459, 0.0000],\n            [2.8153, 1.5000, 0.0000],\n        ]]]).to(device)\n\n        image_dst = kornia.warp_frame_depth(\n            image_src, depth_dst, src_trans_dst, camera_matrix, normalize_points=True)\n        assert_allclose(image_dst, image_dst_expected, 1e-3, 1e-3)\n\n    def test_gradcheck(self, device):\n        image_src = torch.rand(1, 3, 3, 4).to(device)\n        image_src = utils.tensor_to_gradcheck_var(image_src)  # to var\n\n        depth_dst = torch.rand(1, 1, 3, 4).to(device)\n        depth_dst = utils.tensor_to_gradcheck_var(depth_dst)  # to var\n\n        src_trans_dst = torch.rand(1, 4, 4).to(device)\n        src_trans_dst = utils.tensor_to_gradcheck_var(src_trans_dst)  # to var\n\n        camera_matrix = torch.rand(1, 3, 3).to(device)\n        camera_matrix = utils.tensor_to_gradcheck_var(camera_matrix)  # to var\n\n        # evaluate function gradient\n        assert gradcheck(kornia.warp_frame_depth, (image_src, depth_dst, src_trans_dst, camera_matrix,),\n                         raise_exception=True)\n'"
test/geometry/test_dsnt.py,19,"b""import pytest\n\nimport kornia as kornia\n\nimport torch\nfrom torch.testing import assert_allclose\n\n\nclass TestRenderGaussian2d:\n    @pytest.fixture\n    def gaussian(self, device, dtype):\n        return torch.tensor([\n            [0.002969, 0.013306, 0.021938, 0.013306, 0.002969],\n            [0.013306, 0.059634, 0.098320, 0.059634, 0.013306],\n            [0.021938, 0.098320, 0.162103, 0.098320, 0.021938],\n            [0.013306, 0.059634, 0.098320, 0.059634, 0.013306],\n            [0.002969, 0.013306, 0.021938, 0.013306, 0.002969],\n        ], dtype=dtype, device=device)\n\n    def test_pixel_coordinates(self, gaussian, device, dtype):\n        mean = torch.tensor([2.0, 2.0], dtype=dtype, device=device)\n        std = torch.tensor([1.0, 1.0], dtype=dtype, device=device)\n        actual = kornia.geometry.dsnt.render_gaussian2d(mean, std, (5, 5), False)\n        assert_allclose(actual, gaussian, rtol=0, atol=1e-4)\n\n    def test_normalized_coordinates(self, gaussian, device, dtype):\n        mean = torch.tensor([0.0, 0.0], dtype=dtype, device=device)\n        std = torch.tensor([0.25, 0.25], dtype=dtype, device=device)\n        actual = kornia.geometry.dsnt.render_gaussian2d(mean, std, (5, 5), True)\n        assert_allclose(actual, gaussian, rtol=0, atol=1e-4)\n\n    def test_jit(self, device, dtype):\n        mean = torch.tensor([0.0, 0.0], dtype=dtype, device=device)\n        std = torch.tensor([0.25, 0.25], dtype=dtype, device=device)\n        args = (mean, std, (5, 5), True)\n        op = kornia.geometry.dsnt.render_gaussian2d\n        op_jit = kornia.jit.render_gaussian2d\n        assert_allclose(op(*args), op_jit(*args), rtol=0, atol=1e-5)\n\n    def test_jit_trace(self, device, dtype):\n        def op(mean, std):\n            return kornia.geometry.dsnt.render_gaussian2d(mean, std, (5, 5), True)\n        mean = torch.tensor([0.0, 0.0], dtype=dtype, device=device)\n        std = torch.tensor([0.25, 0.25], dtype=dtype, device=device)\n        args = (mean, std)\n        op_jit = torch.jit.trace(op, args)\n        assert_allclose(op(*args), op_jit(*args), rtol=0, atol=1e-5)\n\n\nclass TestSpatialSoftmax2d:\n    @pytest.fixture(params=[\n        torch.ones(1, 1, 5, 7),\n        torch.randn(2, 3, 16, 16),\n    ])\n    def input(self, request, device, dtype):\n        return request.param.to(device, dtype)\n\n    def test_forward(self, input):\n        actual = kornia.geometry.dsnt.spatial_softmax2d(input)\n        assert actual.lt(0).sum().item() == 0, 'expected no negative values'\n        sums = actual.sum(-1).sum(-1)\n        assert_allclose(sums, torch.ones_like(sums))\n\n    def test_jit(self, input):\n        op = kornia.geometry.dsnt.spatial_softmax2d\n        op_jit = kornia.jit.spatial_softmax2d\n        assert_allclose(op(input), op_jit(input), rtol=0, atol=1e-5)\n\n    def test_jit_trace(self, input):\n        op = kornia.geometry.dsnt.spatial_softmax2d\n        op_jit = torch.jit.trace(op, (input,))\n        assert_allclose(op(input), op_jit(input), rtol=0, atol=1e-5)\n\n\nclass TestSpatialExpectation2d:\n    @pytest.fixture(params=[\n        (\n            torch.tensor([[[\n                [0.0, 0.0, 1.0],\n                [0.0, 0.0, 0.0],\n            ]]]),\n            torch.tensor([[[1.0, -1.0]]]),\n            torch.tensor([[[2.0, 0.0]]]),\n        ),\n    ])\n    def example(self, request, device, dtype):\n        input, expected_norm, expected_px = request.param\n        return input.to(device, dtype), expected_norm.to(device, dtype), expected_px.to(device, dtype)\n\n    def test_forward(self, example):\n        input, expected_norm, expected_px = example\n        actual_norm = kornia.geometry.dsnt.spatial_expectation2d(input, True)\n        assert_allclose(actual_norm, expected_norm)\n        actual_px = kornia.geometry.dsnt.spatial_expectation2d(input, False)\n        assert_allclose(actual_px, expected_px)\n\n    def test_jit(self, example):\n        input = example[0]\n        op = kornia.geometry.dsnt.spatial_expectation2d\n        op_jit = kornia.jit.spatial_expectation2d\n        assert_allclose(op(input), op_jit(input), rtol=0, atol=1e-5)\n\n    def test_jit_trace(self, example):\n        input = example[0]\n        op = kornia.geometry.dsnt.spatial_expectation2d\n        op_jit = torch.jit.trace(op, (input,))\n        assert_allclose(op(input), op_jit(input), rtol=0, atol=1e-5)\n"""
test/geometry/test_linalg.py,43,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\ndef identity_matrix(batch_size):\n    r""""""Creates a batched homogeneous identity matrix""""""\n    return torch.eye(4).repeat(batch_size, 1, 1)  # Nx4x4\n\n\ndef euler_angles_to_rotation_matrix(x, y, z):\n    r""""""Create a rotation matrix from x, y, z angles""""""\n    assert x.dim() == 1, x.shape\n    assert x.shape == y.shape == z.shape\n    ones, zeros = torch.ones_like(x), torch.zeros_like(x)\n    # the rotation matrix for the x-axis\n    rx_tmp = [\n        ones, zeros, zeros, zeros,\n        zeros, torch.cos(x), -torch.sin(x), zeros,\n        zeros, torch.sin(x), torch.cos(x), zeros,\n        zeros, zeros, zeros, ones]\n    rx = torch.stack(rx_tmp, dim=-1).view(-1, 4, 4)\n    # the rotation matrix for the y-axis\n    ry_tmp = [\n        torch.cos(y), zeros, torch.sin(y), zeros,\n        zeros, ones, zeros, zeros,\n        -torch.sin(y), zeros, torch.cos(y), zeros,\n        zeros, zeros, zeros, ones]\n    ry = torch.stack(ry_tmp, dim=-1).view(-1, 4, 4)\n    # the rotation matrix for the z-axis\n    rz_tmp = [\n        torch.cos(z), -torch.sin(z), zeros, zeros,\n        torch.sin(z), torch.cos(z), zeros, zeros,\n        zeros, zeros, ones, zeros,\n        zeros, zeros, zeros, ones]\n    rz = torch.stack(rz_tmp, dim=-1).view(-1, 4, 4)\n    return torch.matmul(rz, torch.matmul(ry, rx))  # Bx4x4\n\n\nclass TestTransformPoints:\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 5])\n    @pytest.mark.parametrize(""num_points"", [2, 3, 5])\n    @pytest.mark.parametrize(""num_dims"", [2, 3])\n    def test_transform_points(\n            self, device, batch_size, num_points, num_dims):\n        # generate input data\n        eye_size = num_dims + 1\n        points_src = torch.rand(batch_size, num_points, num_dims)\n        points_src = points_src.to(device)\n\n        dst_homo_src = utils.create_random_homography(batch_size, eye_size)\n        dst_homo_src = dst_homo_src.to(device)\n\n        # transform the points from dst to ref\n        points_dst = kornia.transform_points(dst_homo_src, points_src)\n\n        # transform the points from ref to dst\n        src_homo_dst = torch.inverse(dst_homo_src)\n        points_dst_to_src = kornia.transform_points(src_homo_dst, points_dst)\n\n        # projected should be equal as initial\n        assert_allclose(points_src, points_dst_to_src)\n\n    def test_gradcheck(self, device):\n        # generate input data\n        batch_size, num_points, num_dims = 2, 3, 2\n        eye_size = num_dims + 1\n        points_src = torch.rand(batch_size, num_points, num_dims).to(device)\n        dst_homo_src = utils.create_random_homography(batch_size, eye_size).to(device)\n        # evaluate function gradient\n        points_src = utils.tensor_to_gradcheck_var(points_src)  # to var\n        dst_homo_src = utils.tensor_to_gradcheck_var(dst_homo_src)  # to var\n        assert gradcheck(kornia.transform_points, (dst_homo_src, points_src,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(transform, points):\n            return kornia.transform_points(transform, points)\n\n        points = torch.ones(1, 2, 2).to(device)\n        transform = torch.eye(3)[None].to(device)\n        actual = op_script(transform, points)\n        expected = kornia.transform_points(transform, points)\n\n        assert_allclose(actual, expected)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit_trace(self, device):\n        @torch.jit.script\n        def op_script(transform, points):\n            return kornia.transform_points(transform, points)\n\n        points = torch.ones(1, 2, 2).to(device)\n        transform = torch.eye(3)[None].to(device)\n        op_script_trace = torch.jit.trace(op_script, (transform, points,))\n        actual = op_script_trace(transform, points)\n        expected = kornia.transform_points(transform, points)\n\n        assert_allclose(actual, expected)\n\n\nclass TestTransformBoxes:\n\n    def test_transform_boxes(self, device):\n\n        boxes = torch.tensor([[139.2640, 103.0150, 397.3120, 410.5225]]).to(device)\n\n        expected = torch.tensor([372.7360, 103.0150, 114.6880, 410.5225]).to(device)\n\n        trans_mat = torch.tensor([[[-1., 0., 512.],\n                                   [0., 1., 0.],\n                                   [0., 0., 1.]]]).to(device)\n\n        out = kornia.transform_boxes(trans_mat, boxes)\n        assert_allclose(out, expected)\n\n    def test_transform_multiple_boxes(self, device):\n\n        boxes = torch.tensor([[139.2640, 103.0150, 397.3120, 410.5225],\n                              [1.0240, 80.5547, 512.0000, 512.0000],\n                              [165.2053, 262.1440, 510.6347, 508.9280],\n                              [119.8080, 144.2067, 257.0240, 410.1292]]).to(device)\n\n        boxes = boxes.repeat(2, 1, 1)  # 2 x 4 x 4 two images 4 boxes each\n\n        expected = torch.tensor([[[372.7360, 103.0150, 114.6880, 410.5225],\n                                  [510.9760, 80.5547, 0.0000, 512.0000],\n                                  [346.7947, 262.1440, 1.3653, 508.9280],\n                                  [392.1920, 144.2067, 254.9760, 410.1292]],\n\n                                 [[139.2640, 103.0150, 397.3120, 410.5225],\n                                  [1.0240, 80.5547, 512.0000, 512.0000],\n                                  [165.2053, 262.1440, 510.6347, 508.9280],\n                                  [119.8080, 144.2067, 257.0240, 410.1292]]]).to(device)\n\n        trans_mat = torch.tensor([[[-1., 0., 512.],\n                                   [0., 1., 0.],\n                                   [0., 0., 1.]],\n\n                                  [[1., 0., 0.],\n                                   [0., 1., 0.],\n                                   [0., 0., 1.]]]).to(device)\n\n        out = kornia.transform_boxes(trans_mat, boxes)\n        assert_allclose(out, expected)\n\n    def test_transform_boxes_wh(self, device):\n\n        boxes = torch.tensor([[139.2640, 103.0150, 258.0480, 307.5075],\n                              [1.0240, 80.5547, 510.9760, 431.4453],\n                              [165.2053, 262.1440, 345.4293, 246.7840],\n                              [119.8080, 144.2067, 137.2160, 265.9225]]).to(device)\n\n        expected = torch.tensor([[372.7360, 103.0150, -258.0480, 307.5075],\n                                 [510.9760, 80.5547, -510.9760, 431.4453],\n                                 [346.7947, 262.1440, -345.4293, 246.7840],\n                                 [392.1920, 144.2067, -137.2160, 265.9225]]).to(device)\n\n        trans_mat = torch.tensor([[[-1., 0., 512.],\n                                   [0., 1., 0.],\n                                   [0., 0., 1.]]]).to(device)\n\n        out = kornia.transform_boxes(trans_mat, boxes, mode=\'xywh\')\n        assert_allclose(out, expected)\n\n    def test_gradcheck(self, device):\n\n        boxes = torch.tensor([[139.2640, 103.0150, 258.0480, 307.5075],\n                              [1.0240, 80.5547, 510.9760, 431.4453],\n                              [165.2053, 262.1440, 345.4293, 246.7840],\n                              [119.8080, 144.2067, 137.2160, 265.9225]]).to(device)\n\n        trans_mat = torch.tensor([[[-1., 0., 512.],\n                                   [0., 1., 0.],\n                                   [0., 0., 1.]]]).to(device)\n\n        trans_mat = utils.tensor_to_gradcheck_var(trans_mat)\n        boxes = utils.tensor_to_gradcheck_var(boxes)\n\n        assert gradcheck(kornia.transform_boxes, (trans_mat, boxes), raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(transform, boxes):\n            return kornia.transform_boxes(transform, boxes)\n\n        boxes = torch.tensor([139.2640, 103.0150, 258.0480, 307.5075]).to(device)\n\n        trans_mat = torch.tensor([[[-1., 0., 512.],\n                                   [0., 1., 0.],\n                                   [0., 0., 1.]]]).to(device)\n\n        actual = op_script(trans_mat, boxes)\n        expected = kornia.transform_points(trans_mat, boxes)\n\n        assert_allclose(actual, expected)\n\n\nclass TestComposeTransforms:\n\n    def test_translation_4x4(self, device):\n        offset = 10\n        trans_01 = identity_matrix(batch_size=1)[0].to(device)\n        trans_12 = identity_matrix(batch_size=1)[0].to(device)\n        trans_12[..., :3, -1] += offset  # add offset to translation vector\n\n        trans_02 = kornia.compose_transformations(trans_01, trans_12)\n        assert_allclose(trans_02, trans_12)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 5])\n    def test_translation_Bx4x4(self, device, batch_size):\n        offset = 10\n        trans_01 = identity_matrix(batch_size)\n        trans_12 = identity_matrix(batch_size)\n        trans_12[..., :3, -1] += offset  # add offset to translation vector\n\n        trans_02 = kornia.compose_transformations(trans_01, trans_12)\n        assert_allclose(trans_02, trans_12)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 5])\n    def test_gradcheck(self, device, batch_size):\n        trans_01 = identity_matrix(batch_size).to(device)\n        trans_12 = identity_matrix(batch_size).to(device)\n\n        trans_01 = utils.tensor_to_gradcheck_var(trans_01)  # to var\n        trans_12 = utils.tensor_to_gradcheck_var(trans_12)  # to var\n        assert gradcheck(kornia.compose_transformations, (trans_01, trans_12,),\n                         raise_exception=True)\n\n\nclass TestInverseTransformation:\n\n    def test_translation_4x4(self, device):\n        offset = 10\n        trans_01 = identity_matrix(batch_size=1)[0].to(device)\n        trans_01[..., :3, -1] += offset  # add offset to translation vector\n\n        trans_10 = kornia.inverse_transformation(trans_01)\n        trans_01_hat = kornia.inverse_transformation(trans_10)\n        assert_allclose(trans_01, trans_01_hat)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 5])\n    def test_translation_Bx4x4(self, device, batch_size):\n        offset = 10\n        trans_01 = identity_matrix(batch_size).to(device)\n        trans_01[..., :3, -1] += offset  # add offset to translation vector\n\n        trans_10 = kornia.inverse_transformation(trans_01)\n        trans_01_hat = kornia.inverse_transformation(trans_10)\n        assert_allclose(trans_01, trans_01_hat)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 5])\n    def test_rotation_translation_Bx4x4(self, device, batch_size):\n        offset = 10\n        x, y, z = 0, 0, kornia.pi\n        ones = torch.ones(batch_size).to(device)\n        rmat_01 = euler_angles_to_rotation_matrix(x * ones, y * ones, z * ones)\n\n        trans_01 = identity_matrix(batch_size).to(device)\n        trans_01[..., :3, -1] += offset  # add offset to translation vector\n        trans_01[..., :3, :3] = rmat_01[..., :3, :3]\n\n        trans_10 = kornia.inverse_transformation(trans_01)\n        trans_01_hat = kornia.inverse_transformation(trans_10)\n        assert_allclose(trans_01, trans_01_hat)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 5])\n    def test_gradcheck(self, device, batch_size):\n        trans_01 = identity_matrix(batch_size).to(device)\n        trans_01 = utils.tensor_to_gradcheck_var(trans_01)  # to var\n        assert gradcheck(kornia.inverse_transformation, (trans_01,),\n                         raise_exception=True)\n\n\nclass TestRelativeTransformation:\n\n    def test_translation_4x4(self, device):\n        offset = 10.\n        trans_01 = identity_matrix(batch_size=1)[0].to(device)\n        trans_02 = identity_matrix(batch_size=1)[0].to(device)\n        trans_02[..., :3, -1] += offset  # add offset to translation vector\n\n        trans_12 = kornia.relative_transformation(trans_01, trans_02)\n        trans_02_hat = kornia.compose_transformations(trans_01, trans_12)\n        assert_allclose(trans_02_hat, trans_02)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 5])\n    def test_rotation_translation_Bx4x4(self, device, batch_size):\n        offset = 10.\n        x, y, z = 0., 0., kornia.pi\n        ones = torch.ones(batch_size).to(device)\n        rmat_02 = euler_angles_to_rotation_matrix(x * ones, y * ones, z * ones)\n\n        trans_01 = identity_matrix(batch_size).to(device)\n        trans_02 = identity_matrix(batch_size).to(device)\n        trans_02[..., :3, -1] += offset  # add offset to translation vector\n        trans_02[..., :3, :3] = rmat_02[..., :3, :3]\n\n        trans_12 = kornia.relative_transformation(trans_01, trans_02)\n        trans_02_hat = kornia.compose_transformations(trans_01, trans_12)\n        assert_allclose(trans_02_hat, trans_02)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 5])\n    def test_gradcheck(self, device, batch_size):\n        trans_01 = identity_matrix(batch_size).to(device)\n        trans_02 = identity_matrix(batch_size).to(device)\n\n        trans_01 = utils.tensor_to_gradcheck_var(trans_01)  # to var\n        trans_02 = utils.tensor_to_gradcheck_var(trans_02)  # to var\n        assert gradcheck(kornia.relative_transformation, (trans_01, trans_02,),\n                         raise_exception=True)\n\n\nclass TestTransformLAFs:\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 5])\n    @pytest.mark.parametrize(""num_points"", [2, 3, 5])\n    def test_transform_points(\n            self, batch_size, num_points, device):\n        # generate input data\n        eye_size = 3\n        lafs_src = torch.rand(batch_size, num_points, 2, 3).to(device)\n\n        dst_homo_src = utils.create_random_homography(batch_size, eye_size)\n        dst_homo_src = dst_homo_src.to(device)\n\n        # transform the points from dst to ref\n        lafs_dst = kornia.perspective_transform_lafs(dst_homo_src, lafs_src)\n\n        # transform the points from ref to dst\n        src_homo_dst = torch.inverse(dst_homo_src)\n        lafs_dst_to_src = kornia.perspective_transform_lafs(src_homo_dst, lafs_dst)\n\n        # projected should be equal as initial\n        assert_allclose(lafs_src, lafs_dst_to_src)\n\n    def test_gradcheck(self, device):\n        # generate input data\n        batch_size, num_points, num_dims = 2, 3, 2\n        eye_size = 3\n        points_src = torch.rand(batch_size, num_points, 2, 3).to(device)\n        dst_homo_src = utils.create_random_homography(batch_size, eye_size).to(device)\n        # evaluate function gradient\n        points_src = utils.tensor_to_gradcheck_var(points_src)  # to var\n        dst_homo_src = utils.tensor_to_gradcheck_var(dst_homo_src)  # to var\n        assert gradcheck(kornia.perspective_transform_lafs, (dst_homo_src, points_src,),\n                         raise_exception=True)\n'"
test/geometry/test_perspective.py,42,"b'import pytest\n\nimport kornia\nfrom kornia.testing import tensor_to_gradcheck_var\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestProjectPoints:\n    def test_smoke(self, device):\n        point_3d = torch.zeros(1, 3).to(device)\n        camera_matrix = torch.eye(3).expand(1, -1, -1).to(device)\n        point_2d = kornia.project_points(point_3d, camera_matrix)\n        assert point_2d.shape == (1, 2)\n\n    def test_smoke_batch(self, device):\n        point_3d = torch.zeros(2, 3).to(device)\n        camera_matrix = torch.eye(3).expand(2, -1, -1).to(device)\n        point_2d = kornia.project_points(point_3d, camera_matrix)\n        assert point_2d.shape == (2, 2)\n\n    def test_smoke_batch_multi(self, device):\n        point_3d = torch.zeros(2, 4, 3).to(device)\n        camera_matrix = torch.eye(3).expand(2, 4, -1, -1).to(device)\n        point_2d = kornia.project_points(point_3d, camera_matrix)\n        assert point_2d.shape == (2, 4, 2)\n\n    def test_project_and_unproject(self, device):\n        point_3d = torch.tensor([[10., 2., 30.]]).to(device)\n        depth = point_3d[..., -1:]\n        camera_matrix = torch.tensor([[\n            [2746., 0., 991.],\n            [0., 2748., 619.],\n            [0., 0., 1.],\n        ]]).to(device)\n        point_2d = kornia.project_points(point_3d, camera_matrix)\n        point_3d_hat = kornia.unproject_points(point_2d, depth, camera_matrix)\n        assert_allclose(point_3d, point_3d_hat)\n\n    def test_gradcheck(self, device):\n        # TODO: point [0, 0, 0] crashes\n        points_3d = torch.ones(1, 3).to(device)\n        camera_matrix = torch.eye(3).expand(1, -1, -1).to(device)\n\n        # evaluate function gradient\n        points_3d = tensor_to_gradcheck_var(points_3d)\n        camera_matrix = tensor_to_gradcheck_var(camera_matrix)\n        assert gradcheck(kornia.project_points,\n                         (points_3d, camera_matrix),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(points_3d, camera_matrix):\n            return kornia.project_points(points_3d, camera_matrix)\n\n        points_3d = torch.zeros(1, 3).to(device)\n        camera_matrix = torch.eye(3).expand(1, -1, -1).to(device)\n        actual = op_script(points_3d, camera_matrix)\n        expected = kornia.project_points(points_3d, camera_matrix)\n\n        assert_allclose(actual, expected)\n\n\nclass TestUnprojectPoints:\n    def test_smoke(self, device):\n        points_2d = torch.zeros(1, 2).to(device)\n        depth = torch.ones(1, 1).to(device)\n        camera_matrix = torch.eye(3).expand(1, -1, -1).to(device)\n        point_3d = kornia.unproject_points(points_2d, depth, camera_matrix)\n        assert point_3d.shape == (1, 3)\n\n    def test_smoke_batch(self, device):\n        points_2d = torch.zeros(2, 2).to(device)\n        depth = torch.ones(2, 1).to(device)\n        camera_matrix = torch.eye(3).expand(2, -1, -1).to(device)\n        point_3d = kornia.unproject_points(points_2d, depth, camera_matrix)\n        assert point_3d.shape == (2, 3)\n\n    def test_smoke_multi_batch(self, device):\n        points_2d = torch.zeros(2, 3, 2).to(device)\n        depth = torch.ones(2, 3, 1).to(device)\n        camera_matrix = torch.eye(3).expand(2, 3, -1, -1).to(device)\n        point_3d = kornia.unproject_points(points_2d, depth, camera_matrix)\n        assert point_3d.shape == (2, 3, 3)\n\n    def test_unproject_center(self, device):\n        point_2d = torch.tensor([[0., 0.]]).to(device)\n        depth = torch.tensor([[2.]]).to(device)\n        camera_matrix = torch.tensor([\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]).to(device)\n        expected = torch.tensor([[0., 0., 2.]]).to(device)\n        actual = kornia.unproject_points(point_2d, depth, camera_matrix)\n        assert_allclose(actual, expected)\n\n    def test_unproject_center_normalize(self, device):\n        point_2d = torch.tensor([[0., 0.]]).to(device)\n        depth = torch.tensor([[2.]]).to(device)\n        camera_matrix = torch.tensor([\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]).to(device)\n        expected = torch.tensor([[0., 0., 2.]]).to(device)\n        actual = kornia.unproject_points(point_2d, depth, camera_matrix, True)\n        assert_allclose(actual, expected)\n\n    def test_unproject_and_project(self, device):\n        point_2d = torch.tensor([[0., 0.]]).to(device)\n        depth = torch.tensor([[2.]]).to(device)\n        camera_matrix = torch.tensor([\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]).to(device)\n        point_3d = kornia.unproject_points(point_2d, depth, camera_matrix)\n        point_2d_hat = kornia.project_points(point_3d, camera_matrix)\n        assert_allclose(point_2d, point_2d_hat)\n\n    def test_gradcheck(self, device):\n        points_2d = torch.zeros(1, 2).to(device)\n        depth = torch.ones(1, 1).to(device)\n        camera_matrix = torch.eye(3).expand(1, -1, -1).to(device)\n\n        # evaluate function gradient\n        points_2d = tensor_to_gradcheck_var(points_2d)\n        depth = tensor_to_gradcheck_var(depth)\n        camera_matrix = tensor_to_gradcheck_var(camera_matrix)\n        assert gradcheck(kornia.unproject_points,\n                         (points_2d, depth, camera_matrix),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(points_2d, depth, camera_matrix):\n            return kornia.unproject_points(\n                points_2d, depth, camera_matrix, False)\n\n        points_2d = torch.zeros(1, 2).to(device)\n        depth = torch.ones(1, 1).to(device)\n        camera_matrix = torch.eye(3).expand(1, -1, -1).to(device)\n        actual = op_script(points_2d, depth, camera_matrix)\n        expected = kornia.unproject_points(points_2d, depth, camera_matrix)\n\n        assert_allclose(actual, expected)\n'"
test/geometry/test_pinhole.py,29,"b'import pytest\n\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestPinholeCamera:\n    def _create_intrinsics(self, batch_size, fx, fy, cx, cy):\n        intrinsics = torch.eye(4)\n        intrinsics[..., 0, 0] = fx\n        intrinsics[..., 1, 1] = fy\n        intrinsics[..., 0, 2] = cx\n        intrinsics[..., 1, 2] = cy\n        return intrinsics.expand(batch_size, -1, -1)\n\n    def _create_extrinsics(self, batch_size, tx, ty, tz):\n        extrinsics = torch.eye(4)\n        extrinsics[..., 0, -1] = tx\n        extrinsics[..., 1, -1] = ty\n        extrinsics[..., 2, -1] = tz\n        return extrinsics.expand(batch_size, -1, -1)\n\n    def test_smoke(self, device):\n        intrinsics = torch.eye(4)[None].to(device)\n        extrinsics = torch.eye(4)[None].to(device)\n        height = torch.ones(1).to(device)\n        width = torch.ones(1).to(device)\n        pinhole = kornia.PinholeCamera(intrinsics, extrinsics, height, width)\n        assert isinstance(pinhole, kornia.PinholeCamera)\n\n    def test_pinhole_camera_attributes(self, device):\n        batch_size = 1\n        height, width = 4, 6\n        fx, fy, cx, cy = 1, 2, width / 2, height / 2\n        tx, ty, tz = 1, 2, 3\n\n        intrinsics = self._create_intrinsics(batch_size, fx, fy, cx, cy).to(device)\n        extrinsics = self._create_extrinsics(batch_size, tx, ty, tz).to(device)\n        height = torch.ones(batch_size).to(device) * height\n        width = torch.ones(batch_size).to(device) * width\n\n        pinhole = kornia.PinholeCamera(intrinsics, extrinsics, height, width)\n\n        assert pinhole.batch_size == batch_size\n        assert pinhole.fx.item() == fx\n        assert pinhole.fy.item() == fy\n        assert pinhole.cx.item() == cx\n        assert pinhole.cy.item() == cy\n        assert pinhole.tx.item() == tx\n        assert pinhole.ty.item() == ty\n        assert pinhole.tz.item() == tz\n        assert pinhole.height.item() == height\n        assert pinhole.width.item() == width\n        assert pinhole.rt_matrix.shape == (batch_size, 3, 4)\n        assert pinhole.camera_matrix.shape == (batch_size, 3, 3)\n        assert pinhole.rotation_matrix.shape == (batch_size, 3, 3)\n        assert pinhole.translation_vector.shape == (batch_size, 3, 1)\n\n    def test_pinhole_camera_translation_setters(self, device):\n        batch_size = 1\n        height, width = 4, 6\n        fx, fy, cx, cy = 1, 2, width / 2, height / 2\n        tx, ty, tz = 1, 2, 3\n\n        intrinsics = self._create_intrinsics(batch_size, fx, fy, cx, cy).to(device)\n        extrinsics = self._create_extrinsics(batch_size, tx, ty, tz).to(device)\n        height = torch.ones(batch_size).to(device) * height\n        width = torch.ones(batch_size).to(device) * width\n\n        pinhole = kornia.PinholeCamera(intrinsics, extrinsics, height, width)\n\n        assert pinhole.tx.item() == tx\n        assert pinhole.ty.item() == ty\n        assert pinhole.tz.item() == tz\n\n        # add offset\n        pinhole.tx += 3.\n        pinhole.ty += 2.\n        pinhole.tz += 1.\n\n        assert pinhole.tx.item() == tx + 3.\n        assert pinhole.ty.item() == ty + 2.\n        assert pinhole.tz.item() == tz + 1.\n\n        # set to zero\n        pinhole.tx = 0.\n        pinhole.ty = 0.\n        pinhole.tz = 0.\n\n        assert pinhole.tx.item() == 0.\n        assert pinhole.ty.item() == 0.\n        assert pinhole.tz.item() == 0.\n\n    def test_pinhole_camera_attributes_batch2(self, device):\n        batch_size = 2\n        height, width = 4, 6\n        fx, fy, cx, cy = 1, 2, width / 2, height / 2\n        tx, ty, tz = 1, 2, 3\n\n        intrinsics = self._create_intrinsics(batch_size, fx, fy, cx, cy).to(device)\n        extrinsics = self._create_extrinsics(batch_size, tx, ty, tz).to(device)\n        height = torch.ones(batch_size).to(device) * height\n        width = torch.ones(batch_size).to(device) * width\n\n        pinhole = kornia.PinholeCamera(intrinsics, extrinsics, height, width)\n\n        assert pinhole.batch_size == batch_size\n        assert pinhole.fx.shape[0] == batch_size\n        assert pinhole.fy.shape[0] == batch_size\n        assert pinhole.cx.shape[0] == batch_size\n        assert pinhole.cy.shape[0] == batch_size\n        assert pinhole.tx.shape[0] == batch_size\n        assert pinhole.ty.shape[0] == batch_size\n        assert pinhole.tz.shape[0] == batch_size\n        assert pinhole.height.shape[0] == batch_size\n        assert pinhole.width.shape[0] == batch_size\n        assert pinhole.rt_matrix.shape == (batch_size, 3, 4)\n        assert pinhole.camera_matrix.shape == (batch_size, 3, 3)\n        assert pinhole.rotation_matrix.shape == (batch_size, 3, 3)\n        assert pinhole.translation_vector.shape == (batch_size, 3, 1)\n\n    def test_pinhole_camera_scale(self, device):\n        batch_size = 2\n        height, width = 4, 6\n        fx, fy, cx, cy = 1, 2, width / 2, height / 2\n        tx, ty, tz = 1, 2, 3\n        scale_val = 2.0\n\n        intrinsics = self._create_intrinsics(batch_size, fx, fy, cx, cy).to(device)\n        extrinsics = self._create_extrinsics(batch_size, tx, ty, tz).to(device)\n        height = torch.ones(batch_size).to(device) * height\n        width = torch.ones(batch_size).to(device) * width\n        scale_factor = torch.ones(batch_size).to(device) * scale_val\n\n        pinhole = kornia.PinholeCamera(intrinsics, extrinsics, height, width)\n        pinhole_scale = pinhole.scale(scale_factor)\n\n        assert_allclose(\n            pinhole_scale.intrinsics[..., 0, 0],\n            pinhole.intrinsics[..., 0, 0] * scale_val)  # fx\n        assert_allclose(\n            pinhole_scale.intrinsics[..., 1, 1],\n            pinhole.intrinsics[..., 1, 1] * scale_val)  # fy\n        assert_allclose(\n            pinhole_scale.intrinsics[..., 0, 2],\n            pinhole.intrinsics[..., 0, 2] * scale_val)  # cx\n        assert_allclose(\n            pinhole_scale.intrinsics[..., 1, 2],\n            pinhole.intrinsics[..., 1, 2] * scale_val)  # cy\n        assert_allclose(\n            pinhole_scale.height,\n            pinhole.height * scale_val)\n        assert_allclose(\n            pinhole_scale.width,\n            pinhole.width * scale_val)\n\n    def test_pinhole_camera_scale_inplace(self, device):\n        batch_size = 2\n        height, width = 4, 6\n        fx, fy, cx, cy = 1, 2, width / 2, height / 2\n        tx, ty, tz = 1, 2, 3\n        scale_val = 2.0\n\n        intrinsics = self._create_intrinsics(batch_size, fx, fy, cx, cy).to(device)\n        extrinsics = self._create_extrinsics(batch_size, tx, ty, tz).to(device)\n        height = torch.ones(batch_size).to(device) * height\n        width = torch.ones(batch_size).to(device) * width\n        scale_factor = torch.ones(batch_size).to(device) * scale_val\n\n        pinhole = kornia.PinholeCamera(intrinsics, extrinsics, height, width)\n        pinhole_scale = pinhole.clone()\n        pinhole_scale.scale_(scale_factor)\n\n        assert_allclose(\n            pinhole_scale.intrinsics[..., 0, 0],\n            pinhole.intrinsics[..., 0, 0] * scale_val)  # fx\n        assert_allclose(\n            pinhole_scale.intrinsics[..., 1, 1],\n            pinhole.intrinsics[..., 1, 1] * scale_val)  # fy\n        assert_allclose(\n            pinhole_scale.intrinsics[..., 0, 2],\n            pinhole.intrinsics[..., 0, 2] * scale_val)  # cx\n        assert_allclose(\n            pinhole_scale.intrinsics[..., 1, 2],\n            pinhole.intrinsics[..., 1, 2] * scale_val)  # cy\n        assert_allclose(\n            pinhole_scale.height, pinhole.height * scale_val)\n        assert_allclose(\n            pinhole_scale.width, pinhole.width * scale_val)\n\n\n\'\'\'@pytest.mark.parametrize(""batch_size"", [1, 2, 5, 6])\ndef test_scale_pinhole(batch_size, device_type):\n    # generate input data\n    device = torch.device(device_type)\n    pinholes = torch.rand(batch_size, 12).to(device)\n    scales = torch.rand(batch_size).to(device)\n\n    pinholes_scale = kornia.scale_pinhole(pinholes, scales)\n    assert_allclose(\n        pinholes_scale[..., :6] / scales.unsqueeze(-1), pinholes[..., :6])\n\n    # evaluate function gradient\n    pinholes = utils.tensor_to_gradcheck_var(pinholes)  # to var\n    scales = utils.tensor_to_gradcheck_var(scales)  # to var\n    assert gradcheck(kornia.scale_pinhole, (pinholes, scales,),\n                     raise_exception=True)\n\n\n@pytest.mark.parametrize(""batch_size"", [1, 2, 5, 6])\ndef test_pinhole_matrix(batch_size, device_type):\n    # generate input data\n    image_height, image_width = 32., 32.\n    cx, cy = image_width / 2, image_height / 2\n    fx, fy = 1., 1.\n    rx, ry, rz = 0., 0., 0.\n    tx, ty, tz = 0., 0., 0.\n    offset_x = 10.  # we will apply a 10units offset to `i` camera\n    eps = 1e-6\n\n    pinhole = utils.create_pinhole(\n        fx, fy, cx, cy, image_height, image_width, rx, ry, rx, tx, ty, tz)\n    pinhole = pinhole.repeat(batch_size, 1).to(torch.device(device_type))\n\n    pinhole_matrix = kornia.pinhole_matrix(pinhole)\n\n    ones = torch.ones(batch_size)\n    assert bool((pinhole_matrix[:, 0, 0] == fx * ones).all())\n    assert bool((pinhole_matrix[:, 1, 1] == fy * ones).all())\n    assert bool((pinhole_matrix[:, 0, 2] == cx * ones).all())\n    assert bool((pinhole_matrix[:, 1, 2] == cy * ones).all())\n\n    # functional\n    assert kornia.PinholeMatrix()(pinhole).shape == (batch_size, 4, 4)\n\n    # evaluate function gradient\n    pinhole = utils.tensor_to_gradcheck_var(pinhole)  # to var\n    assert gradcheck(kornia.pinhole_matrix, (pinhole,),\n                     raise_exception=True)\n\n\n@pytest.mark.parametrize(""batch_size"", [1, 2, 5, 6])\ndef test_inverse_pinhole_matrix(batch_size, device_type):\n    # generate input data\n    image_height, image_width = 32., 32.\n    cx, cy = image_width / 2, image_height / 2\n    fx, fy = 1., 1.\n    rx, ry, rz = 0., 0., 0.\n    tx, ty, tz = 0., 0., 0.\n    offset_x = 10.  # we will apply a 10units offset to `i` camera\n    eps = 1e-6\n\n    pinhole = utils.create_pinhole(\n        fx, fy, cx, cy, image_height, image_width, rx, ry, rx, tx, ty, tz)\n    pinhole = pinhole.repeat(batch_size, 1).to(torch.device(device_type))\n\n    pinhole_matrix = kornia.inverse_pinhole_matrix(pinhole)\n\n    ones = torch.ones(batch_size)\n    assert_allclose(pinhole_matrix[:, 0, 0], (1. / fx) * ones)\n    assert_allclose(pinhole_matrix[:, 1, 1], (1. / fy) * ones)\n    assert_allclose(\n        pinhole_matrix[:, 0, 2], (-1. * cx / fx) * ones)\n    assert_allclose(\n        pinhole_matrix[:, 1, 2], (-1. * cy / fx) * ones)\n\n    # functional\n    assert kornia.InversePinholeMatrix()(pinhole).shape == (batch_size, 4, 4)\n\n    # evaluate function gradient\n    pinhole = utils.tensor_to_gradcheck_var(pinhole)  # to var\n    assert gradcheck(kornia.pinhole_matrix, (pinhole,),\n                     raise_exception=True)\n\n\n@pytest.mark.parametrize(""batch_size"", [1, 2, 5, 6])\ndef test_homography_i_H_ref(batch_size, device_type):\n    # generate input data\n    device = torch.device(device_type)\n    image_height, image_width = 32., 32.\n    cx, cy = image_width / 2, image_height / 2\n    fx, fy = 1., 1.\n    rx, ry, rz = 0., 0., 0.\n    tx, ty, tz = 0., 0., 0.\n    offset_x = 10.  # we will apply a 10units offset to `i` camera\n    eps = 1e-6\n\n    pinhole_ref = utils.create_pinhole(\n        fx, fy, cx, cy, image_height, image_width, rx, ry, rx, tx, ty, tz)\n    pinhole_ref = pinhole_ref.repeat(batch_size, 1).to(device)\n\n    pinhole_i = utils.create_pinhole(\n        fx,\n        fy,\n        cx,\n        cy,\n        image_height,\n        image_width,\n        rx,\n        ry,\n        rx,\n        tx + offset_x,\n        ty,\n        tz)\n    pinhole_i = pinhole_i.repeat(batch_size, 1).to(device)\n\n    # compute homography from ref to i\n    i_H_ref = kornia.homography_i_H_ref(pinhole_i, pinhole_ref) + eps\n    i_H_ref_inv = torch.inverse(i_H_ref)\n\n    # compute homography from i to ref\n    ref_H_i = kornia.homography_i_H_ref(pinhole_ref, pinhole_i) + eps\n    assert_allclose(i_H_ref_inv, ref_H_i)\n\n    # evaluate function gradient\n    assert gradcheck(kornia.homography_i_H_ref,\n                     (utils.tensor_to_gradcheck_var(pinhole_ref) + eps,\n                      utils.tensor_to_gradcheck_var(pinhole_i) + eps,),\n                     raise_exception=True)\'\'\'\n'"
test/geometry/test_spatial_softargmax.py,62,"b""import pytest\n\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.nn.functional import mse_loss\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\nfrom kornia.geometry.spatial_soft_argmax import _get_center_kernel2d, _get_center_kernel3d\n\n\nclass TestCenterKernel2d:\n    def test_smoke(self, device):\n        kernel = _get_center_kernel2d(3, 4).to(device)\n        assert kernel.shape == (2, 2, 3, 4)\n\n    def test_odd(self, device):\n        kernel = _get_center_kernel2d(3, 3).to(device)\n        expected = torch.tensor([\n            [[[0., 0., 0.],\n              [0., 1., 0.],\n              [0., 0., 0.]],\n             [[0., 0., 0.],\n              [0., 0., 0.],\n              [0., 0., 0.]]],\n            [[[0., 0., 0.],\n              [0., 0., 0.],\n              [0., 0., 0.]],\n             [[0., 0., 0.],\n              [0., 1., 0.],\n              [0., 0., 0.]]]]).to(device)\n        assert_allclose(kernel, expected)\n\n    def test_even(self, device):\n        kernel = _get_center_kernel2d(2, 2).to(device)\n        expected = torch.ones(2, 2, 2, 2).to(device) * 0.25\n        expected[0, 1] = 0\n        expected[1, 0] = 0\n        assert_allclose(kernel, expected)\n\n\nclass TestCenterKernel3d:\n    def test_smoke(self, device):\n        kernel = _get_center_kernel3d(6, 3, 4).to(device)\n        assert kernel.shape == (3, 3, 6, 3, 4)\n\n    def test_odd(self, device):\n        kernel = _get_center_kernel3d(3, 5, 7).to(device)\n        expected = torch.zeros(3, 3, 3, 5, 7).to(device)\n        expected[0, 0, 1, 2, 3] = 1.\n        expected[1, 1, 1, 2, 3] = 1.\n        expected[2, 2, 1, 2, 3] = 1.\n        assert_allclose(kernel, expected)\n\n    def test_even(self, device):\n        kernel = _get_center_kernel3d(2, 4, 3).to(device)\n        expected = torch.zeros(3, 3, 2, 4, 3).to(device)\n        expected[0, 0, :, 1:3, 1] = 0.25\n        expected[1, 1, :, 1:3, 1] = 0.25\n        expected[2, 2, :, 1:3, 1] = 0.25\n        assert_allclose(kernel, expected)\n\n\nclass TestSpatialSoftArgmax2d:\n    def test_smoke(self, device):\n        input = torch.zeros(1, 1, 2, 3).to(device)\n        m = kornia.SpatialSoftArgmax2d()\n        assert m(input).shape == (1, 1, 2)\n\n    def test_smoke_batch(self, device):\n        input = torch.zeros(2, 1, 2, 3).to(device)\n        m = kornia.SpatialSoftArgmax2d()\n        assert m(input).shape == (2, 1, 2)\n\n    def test_top_left_normalized(self, device):\n        input = torch.zeros(1, 1, 2, 3).to(device)\n        input[..., 0, 0] = 1e16\n\n        coord = kornia.spatial_soft_argmax2d(input, normalized_coordinates=True)\n        assert_allclose(coord[..., 0].item(), -1.0)\n        assert_allclose(coord[..., 1].item(), -1.0)\n\n    def test_top_left(self, device):\n        input = torch.zeros(1, 1, 2, 3).to(device)\n        input[..., 0, 0] = 1e16\n\n        coord = kornia.spatial_soft_argmax2d(input, normalized_coordinates=False)\n        assert_allclose(coord[..., 0].item(), 0.0)\n        assert_allclose(coord[..., 1].item(), 0.0)\n\n    def test_bottom_right_normalized(self, device):\n        input = torch.zeros(1, 1, 2, 3).to(device)\n        input[..., -1, -1] = 1e16\n\n        coord = kornia.spatial_soft_argmax2d(input, normalized_coordinates=True)\n        assert_allclose(coord[..., 0].item(), 1.0)\n        assert_allclose(coord[..., 1].item(), 1.0)\n\n    def test_bottom_right(self, device):\n        input = torch.zeros(1, 1, 2, 3).to(device)\n        input[..., -1, -1] = 1e16\n\n        coord = kornia.spatial_soft_argmax2d(input, normalized_coordinates=False)\n        assert_allclose(coord[..., 0].item(), 2.0)\n        assert_allclose(coord[..., 1].item(), 1.0)\n\n    def test_batch2_n2(self, device):\n        input = torch.zeros(2, 2, 2, 3).to(device)\n        input[0, 0, 0, 0] = 1e16  # top-left\n        input[0, 1, 0, -1] = 1e16  # top-right\n        input[1, 0, -1, 0] = 1e16  # bottom-left\n        input[1, 1, -1, -1] = 1e16  # bottom-right\n\n        coord = kornia.spatial_soft_argmax2d(input)\n        assert_allclose(coord[0, 0, 0].item(), -1.0)  # top-left\n        assert_allclose(coord[0, 0, 1].item(), -1.0)\n        assert_allclose(coord[0, 1, 0].item(), 1.0)  # top-right\n        assert_allclose(coord[0, 1, 1].item(), -1.0)\n        assert_allclose(coord[1, 0, 0].item(), -1.0)  # bottom-left\n        assert_allclose(coord[1, 0, 1].item(), 1.0)\n        assert_allclose(coord[1, 1, 0].item(), 1.0)  # bottom-right\n        assert_allclose(coord[1, 1, 1].item(), 1.0)\n\n    def test_gradcheck(self, device):\n        input = torch.rand(2, 3, 3, 2).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.spatial_soft_argmax2d,\n                         (input), raise_exception=True)\n\n    def test_end_to_end(self, device):\n        input = torch.full((1, 2, 7, 7), 1.0, requires_grad=True).to(device)\n        target = torch.as_tensor([[[0.0, 0.0], [1.0, 1.0]]]).to(device)\n        std = torch.tensor([1.0, 1.0]).to(device)\n\n        hm = kornia.geometry.dsnt.spatial_softmax2d(input)\n        assert_allclose(hm.sum(-1).sum(-1), torch.tensor(1.0).to(device))\n\n        pred = kornia.geometry.dsnt.spatial_expectation2d(hm)\n        assert_allclose(pred, torch.as_tensor([[[0.0, 0.0], [0.0, 0.0]]]).to(device))\n\n        loss1 = mse_loss(pred, target, size_average=None, reduce=None,\n                         reduction='none').mean(-1, keepdim=False)\n        expected_loss1 = torch.as_tensor([[0.0, 1.0]]).to(device)\n        assert_allclose(loss1, expected_loss1)\n\n        target_hm = kornia.geometry.dsnt.render_gaussian2d(\n            target, std, input.shape[-2:]).contiguous()\n\n        loss2 = kornia.losses.js_div_loss_2d(hm, target_hm, reduction='none')\n        expected_loss2 = torch.as_tensor([[0.0087, 0.0818]]).to(device)\n        assert_allclose(loss2, expected_loss2, rtol=0, atol=1e-3)\n\n        loss = (loss1 + loss2).mean()\n        loss.backward()\n\n    def test_jit(self, device, dtype):\n        input = torch.rand((2, 3, 7, 7), dtype=dtype, device=device)\n        op = kornia.spatial_soft_argmax2d\n        op_jit = kornia.jit.spatial_soft_argmax2d\n        assert_allclose(op(input), op_jit(input), rtol=0, atol=1e-5)\n\n    def test_jit_trace(self, device, dtype):\n        input = torch.rand((2, 3, 7, 7), dtype=dtype, device=device)\n        op = kornia.spatial_soft_argmax2d\n        op_jit = torch.jit.trace(op, (input,))\n        assert_allclose(op(input), op_jit(input), rtol=0, atol=1e-5)\n\n\nclass TestConvSoftArgmax2d:\n    def test_smoke(self, device):\n        input = torch.zeros(1, 1, 3, 3).to(device)\n        m = kornia.ConvSoftArgmax2d((3, 3))\n        assert m(input).shape == (1, 1, 2, 3, 3)\n\n    def test_smoke_batch(self, device):\n        input = torch.zeros(2, 5, 3, 3).to(device)\n        m = kornia.ConvSoftArgmax2d()\n        assert m(input).shape == (2, 5, 2, 3, 3)\n\n    def test_smoke_with_val(self, device):\n        input = torch.zeros(1, 1, 3, 3).to(device)\n        m = kornia.ConvSoftArgmax2d((3, 3), output_value=True)\n        coords, val = m(input)\n        assert coords.shape == (1, 1, 2, 3, 3)\n        assert val.shape == (1, 1, 3, 3)\n\n    def test_smoke_batch_with_val(self, device):\n        input = torch.zeros(2, 5, 3, 3).to(device)\n        m = kornia.ConvSoftArgmax2d((3, 3), output_value=True)\n        coords, val = m(input)\n        assert coords.shape == (2, 5, 2, 3, 3)\n        assert val.shape == (2, 5, 3, 3)\n\n    def test_gradcheck(self, device):\n        input = torch.rand(2, 3, 5, 5).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.conv_soft_argmax2d,\n                         (input), raise_exception=True)\n\n    def test_cold_diag(self, device):\n        input = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 1., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 1., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n        softargmax = kornia.ConvSoftArgmax2d((3, 3), (2, 2), (0, 0),\n                                             temperature=0.05,\n                                             normalized_coordinates=False,\n                                             output_value=True)\n        expected_val = torch.tensor([[[[1., 0.],\n                                       [0., 1.]]]]).to(device)\n        expected_coord = torch.tensor([[[[[1., 3.],\n                                          [1., 3.]],\n                                         [[1., 1.],\n                                          [3., 3.]]]]]).to(device)\n        coords, val = softargmax(input)\n        assert_allclose(val, expected_val)\n        assert_allclose(coords, expected_coord)\n\n    def test_hot_diag(self, device):\n        input = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 1., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 1., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n        softargmax = kornia.ConvSoftArgmax2d((3, 3), (2, 2), (0, 0),\n                                             temperature=10.,\n                                             normalized_coordinates=False,\n                                             output_value=True)\n        expected_val = torch.tensor([[[[0.1214, 0.],\n                                       [0., 0.1214]]]]).to(device)\n        expected_coord = torch.tensor([[[[[1., 3.],\n                                          [1., 3.]],\n                                         [[1., 1.],\n                                          [3., 3.]]]]]).to(device)\n        coords, val = softargmax(input)\n        assert_allclose(val, expected_val)\n        assert_allclose(coords, expected_coord)\n\n    def test_cold_diag_norm(self, device):\n        input = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 1., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 1., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n        softargmax = kornia.ConvSoftArgmax2d((3, 3), (2, 2), (0, 0),\n                                             temperature=0.05,\n                                             normalized_coordinates=True,\n                                             output_value=True)\n        expected_val = torch.tensor([[[[1., 0.],\n                                       [0., 1.]]]]).to(device)\n        expected_coord = torch.tensor([[[[[-0.5, 0.5],\n                                          [-0.5, 0.5]],\n                                         [[-0.5, -0.5],\n                                          [0.5, 0.5]]]]]).to(device)\n        coords, val = softargmax(input)\n        assert_allclose(val, expected_val)\n        assert_allclose(coords, expected_coord)\n\n    def test_hot_diag_norm(self, device):\n        input = torch.tensor([[[\n            [0., 0., 0., 0., 0.],\n            [0., 1., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 1., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]).to(device)\n        softargmax = kornia.ConvSoftArgmax2d((3, 3), (2, 2), (0, 0),\n                                             temperature=10.,\n                                             normalized_coordinates=True,\n                                             output_value=True)\n        expected_val = torch.tensor([[[[0.1214, 0.],\n                                       [0., 0.1214]]]]).to(device)\n        expected_coord = torch.tensor([[[[[-0.5, 0.5],\n                                          [-0.5, 0.5]],\n                                         [[-0.5, -0.5],\n                                          [0.5, 0.5]]]]]).to(device)\n        coords, val = softargmax(input)\n        assert_allclose(val, expected_val)\n        assert_allclose(coords, expected_coord)\n\n\nclass TestConvSoftArgmax3d:\n    def test_smoke(self, device):\n        input = torch.zeros(1, 1, 3, 3, 3).to(device)\n        m = kornia.ConvSoftArgmax3d((3, 3, 3), output_value=False)\n        assert m(input).shape == (1, 1, 3, 3, 3, 3)\n\n    def test_smoke_with_val(self, device):\n        input = torch.zeros(1, 1, 3, 3, 3).to(device)\n        m = kornia.ConvSoftArgmax3d((3, 3, 3), output_value=True)\n        coords, val = m(input)\n        assert coords.shape == (1, 1, 3, 3, 3, 3)\n        assert val.shape == (1, 1, 3, 3, 3)\n\n    def test_gradcheck(self, device):\n        input = torch.rand(1, 2, 3, 5, 5).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.conv_soft_argmax3d,\n                         (input), raise_exception=True)\n\n    def test_cold_diag(self, device):\n        input = torch.tensor([[[[\n            [0., 0., 0., 0., 0.],\n            [0., 1., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 1., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]]).to(device)\n        softargmax = kornia.ConvSoftArgmax3d((1, 3, 3), (1, 2, 2), (0, 0, 0),\n                                             temperature=0.05,\n                                             normalized_coordinates=False,\n                                             output_value=True)\n        expected_val = torch.tensor([[[[[1., 0.],\n                                        [0., 1.]]]]]).to(device)\n        expected_coord = torch.tensor([[[\n                                        [[[0., 0.],\n                                          [0., 0.]]],\n                                        [[[1., 3.],\n                                          [1., 3.]]],\n                                        [[[1., 1.],\n                                          [3., 3.]]]]]]).to(device)\n        coords, val = softargmax(input)\n        assert_allclose(val, expected_val)\n        assert_allclose(coords, expected_coord)\n\n    def test_hot_diag(self, device):\n        input = torch.tensor([[[[\n            [0., 0., 0., 0., 0.],\n            [0., 1., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 1., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]]).to(device)\n        softargmax = kornia.ConvSoftArgmax3d((1, 3, 3), (1, 2, 2), (0, 0, 0),\n                                             temperature=10.,\n                                             normalized_coordinates=False,\n                                             output_value=True)\n        expected_val = torch.tensor([[[[[0.1214, 0.],\n                                        [0., 0.1214]]]]]).to(device)\n        expected_coord = torch.tensor([[[\n                                        [[[0., 0.],\n                                          [0., 0.]]],\n                                        [[[1., 3.],\n                                          [1., 3.]]],\n                                        [[[1., 1.],\n                                          [3., 3.]]]]]]).to(device)\n        coords, val = softargmax(input)\n        assert_allclose(val, expected_val)\n        assert_allclose(coords, expected_coord)\n\n    def test_cold_diag_norm(self, device):\n        input = torch.tensor([[[[\n            [0., 0., 0., 0., 0.],\n            [0., 1., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 1., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]]).to(device)\n        softargmax = kornia.ConvSoftArgmax3d((1, 3, 3), (1, 2, 2), (0, 0, 0),\n                                             temperature=0.05,\n                                             normalized_coordinates=True,\n                                             output_value=True)\n        expected_val = torch.tensor([[[[[1., 0.],\n                                        [0., 1.]]]]]).to(device)\n        expected_coord = torch.tensor([[[\n            [[[-1., -1.],\n              [-1., -1.]]],\n            [[[-0.5, 0.5],\n              [-0.5, 0.5]]],\n            [[[-0.5, -0.5],\n              [0.5, 0.5]]]]]]).to(device)\n        coords, val = softargmax(input)\n        assert_allclose(val, expected_val)\n        assert_allclose(coords, expected_coord)\n\n    def test_hot_diag_norm(self, device):\n        input = torch.tensor([[[[\n            [0., 0., 0., 0., 0.],\n            [0., 1., 0., 0., 0.],\n            [0., 0., 0., 0., 0.],\n            [0., 0., 0., 1., 0.],\n            [0., 0., 0., 0., 0.],\n        ]]]]).to(device)\n        softargmax = kornia.ConvSoftArgmax3d((1, 3, 3), (1, 2, 2), (0, 0, 0),\n                                             temperature=10.,\n                                             normalized_coordinates=True,\n                                             output_value=True)\n        expected_val = torch.tensor([[[[[0.1214, 0.],\n                                        [0., 0.1214]]]]]).to(device)\n        expected_coord = torch.tensor([[[\n            [[[-1., -1.],\n              [-1., -1.]]],\n            [[[-0.5, 0.5],\n              [-0.5, 0.5]]],\n            [[[-0.5, -0.5],\n              [0.5, 0.5]]]]]]).to(device)\n        coords, val = softargmax(input)\n        assert_allclose(val, expected_val)\n        assert_allclose(coords, expected_coord)\n\n\nclass TestConvQuadInterp3d:\n    def test_smoke(self, device):\n        input = torch.randn(2, 3, 3, 4, 4).to(device)\n        nms = kornia.geometry.ConvQuadInterp3d(1)\n        coord, val = nms(input)\n        assert coord.shape == (2, 3, 3, 3, 4, 4)\n        assert val.shape == (2, 3, 3, 4, 4)\n\n    def test_gradcheck(self, device):\n        input = torch.rand(1, 2, 3, 5, 5).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.geometry.ConvQuadInterp3d(),\n                         (input), raise_exception=True)\n\n    def test_diag(self, device):\n        input = torch.tensor([[\n            [[0., 0., 0., 0, 0],\n             [0., 0., 0.0, 0, 0.],\n             [0., 0, 0., 0, 0.],\n             [0., 0., 0, 0, 0.],\n             [0., 0., 0., 0, 0.]],\n\n            [[0., 0., 0., 0, 0],\n             [0., 0., 1, 0, 0.],\n             [0., 1, 1.2, 1.1, 0.],\n             [0., 0., 1., 0, 0.],\n             [0., 0., 0., 0, 0.]],\n\n            [[0., 0., 0., 0, 0],\n             [0., 0., 0.0, 0, 0.],\n             [0., 0, 0., 0, 0.],\n             [0., 0., 0, 0, 0.],\n             [0., 0., 0., 0, 0.],\n             ]]]).to(device)\n        input = kornia.gaussian_blur2d(input, (5, 5), (0.5, 0.5)).unsqueeze(0)\n        softargmax = kornia.geometry.ConvQuadInterp3d(1)\n        expected_val = torch.tensor([[[\n            [[0., 0., 0., 0, 0],\n             [0., 0., 0.0, 0, 0.],\n             [0., 0, 0., 0, 0.],\n             [0., 0., 0, 0, 0.],\n             [0., 0., 0., 0, 0.]],\n            [[2.2504e-04, 2.3146e-02, 1.6808e-01, 2.3188e-02, 2.3628e-04],\n             [2.3146e-02, 1.8118e-01, 7.4338e-01, 1.8955e-01, 2.5413e-02],\n             [1.6807e-01, 7.4227e-01, 2.1722e+00, 8.0414e-01, 1.8482e-01],\n             [2.3146e-02, 1.8118e-01, 7.4338e-01, 1.8955e-01, 2.5413e-02],\n             [2.2504e-04, 2.3146e-02, 1.6808e-01, 2.3188e-02, 2.3628e-04]],\n            [[0., 0., 0., 0, 0],\n             [0., 0., 0.0, 0, 0.],\n             [0., 0, 0., 0, 0.],\n             [0., 0., 0, 0, 0.],\n             [0., 0., 0., 0, 0.]]]]]).to(device)\n        expected_coord = torch.tensor([[[[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n                                          [[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n                                           [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n                                           [1.0000, 1.0000, 1.0688, 1.0000, 1.0000],\n                                           [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n                                           [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n\n                                          [[2.0000, 2.0000, 2.0000, 2.0000, 2.0000],\n                                           [2.0000, 2.0000, 2.0000, 2.0000, 2.0000],\n                                           [2.0000, 2.0000, 2.0000, 2.0000, 2.0000],\n                                           [2.0000, 2.0000, 2.0000, 2.0000, 2.0000],\n                                           [2.0000, 2.0000, 2.0000, 2.0000, 2.0000]]],\n\n\n                                         [[[0.0000, 1.0000, 2.0000, 3.0000, 4.0000],\n                                           [0.0000, 1.0000, 2.0000, 3.0000, 4.0000],\n                                             [0.0000, 1.0000, 2.0000, 3.0000, 4.0000],\n                                             [0.0000, 1.0000, 2.0000, 3.0000, 4.0000],\n                                             [0.0000, 1.0000, 2.0000, 3.0000, 4.0000]],\n\n                                          [[0.0000, 1.0000, 2.0000, 3.0000, 4.0000],\n                                             [0.0000, 1.0000, 2.0000, 3.0000, 4.0000],\n                                             [0.0000, 1.0000, 1.8366, 3.0000, 4.0000],\n                                             [0.0000, 1.0000, 2.0000, 3.0000, 4.0000],\n                                             [0.0000, 1.0000, 2.0000, 3.0000, 4.0000]],\n\n                                          [[0.0000, 1.0000, 2.0000, 3.0000, 4.0000],\n                                             [0.0000, 1.0000, 2.0000, 3.0000, 4.0000],\n                                             [0.0000, 1.0000, 2.0000, 3.0000, 4.0000],\n                                             [0.0000, 1.0000, 2.0000, 3.0000, 4.0000],\n                                             [0.0000, 1.0000, 2.0000, 3.0000, 4.0000]]],\n\n\n                                         [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                           [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n                                             [2.0000, 2.0000, 2.0000, 2.0000, 2.0000],\n                                             [3.0000, 3.0000, 3.0000, 3.0000, 3.0000],\n                                             [4.0000, 4.0000, 4.0000, 4.0000, 4.0000]],\n\n                                          [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                             [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n                                             [2.0000, 2.0000, 2.0244, 2.0000, 2.0000],\n                                             [3.0000, 3.0000, 3.0000, 3.0000, 3.0000],\n                                             [4.0000, 4.0000, 4.0000, 4.0000, 4.0000]],\n\n                                          [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n                                             [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n                                             [2.0000, 2.0000, 2.0000, 2.0000, 2.0000],\n                                             [3.0000, 3.0000, 3.0000, 3.0000, 3.0000],\n                                             [4.0000, 4.0000, 4.0000, 4.0000, 4.0000]]]]]]).to(device)\n        coords, val = softargmax(input)\n        assert_allclose(val, expected_val)\n        assert_allclose(coords, expected_coord)\n"""
test/integration/test_focal.py,7,"b'import logging\nimport pytest\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport kornia\n\nlogger = logging.getLogger(__name__)\n\n\nclass TestIntegrationFocalLoss:\n    # optimization\n    thresh = 1e-1\n    lr = 1e-3\n    num_iterations = 1000\n    num_classes = 2\n\n    # focal loss\n    alpha = 2.0\n    gamma = 2.0\n\n    def generate_sample(self, base_target, std_val=0.1):\n        target = base_target.float() / base_target.max()\n        noise = std_val * torch.rand(1, 1, 6, 5).to(base_target.device)\n        return target + noise\n\n    @staticmethod\n    def init_weights(m):\n        if isinstance(m, nn.Conv2d):\n            torch.nn.init.xavier_uniform_(m.weight)\n\n    def test_conv2d_relu(self, device):\n\n        # we generate base sample\n        target = torch.LongTensor(1, 6, 5).fill_(0).to(device)\n        for i in range(1, self.num_classes):\n            target[..., i:-i, i:-i] = i\n\n        m = nn.Sequential(\n            nn.Conv2d(1, self.num_classes, kernel_size=3, padding=1),\n            nn.ReLU(True),\n        ).to(device)\n        m.apply(self.init_weights)\n\n        optimizer = optim.Adam(m.parameters(), lr=self.lr)\n\n        criterion = kornia.losses.FocalLoss(\n            alpha=self.alpha, gamma=self.gamma, reduction=\'mean\')\n        # NOTE: uncomment to compare against vanilla cross entropy\n        # criterion = nn.CrossEntropyLoss()\n\n        for iter_id in range(self.num_iterations):\n            sample = self.generate_sample(target).to(device)\n            output = m(sample)\n            loss = criterion(output, target.to(device))\n            logger.debug(""Loss: {}"".format(loss.item()))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        sample = self.generate_sample(target).to(device)\n        output_argmax = torch.argmax(m(sample), dim=1)\n        logger.debug(""Output argmax: \\n{}"".format(output_argmax))\n\n        # TODO(edgar): replace by IoU or find a more stable solution\n        #              for this test. The issue is that depending on\n        #              the seed to initialize the weights affects the\n        #              final results and slows down the convergence of\n        #              the algorithm.\n        val = F.mse_loss(output_argmax.float(), target.float())\n        if not val.item() < self.thresh:\n            pytest.xfail(""Wrong seed or initial weight values."")\n'"
test/integration/test_soft_argmax2d.py,7,"b'import logging\nimport pytest\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.testing import assert_allclose\n\nimport kornia\n\nlogger = logging.getLogger(__name__)\n\n\nclass TestIntegrationSoftArgmax2d:\n    # optimization\n    lr = 1e-3\n    num_iterations = 500\n\n    # data params\n    height = 240\n    width = 320\n\n    def generate_sample(self, base_target, std_val=1.0):\n        """"""Generates a random sample around the given point.\n        The standard deviation is in pixel.\n        """"""\n        noise = std_val * torch.rand_like(base_target)\n        return base_target + noise\n\n    def test_regression_2d(self, device):\n        # create the parameters to estimate: the heatmap\n        params = nn.Parameter(torch.rand(1, 1, self.height, self.width).to(device))\n\n        # generate base sample\n        target = torch.zeros(1, 1, 2).to(device)\n        target[..., 0] = self.width / 2\n        target[..., 1] = self.height / 2\n\n        # create the optimizer and pass the heatmap\n        optimizer = optim.Adam([params], lr=self.lr)\n\n        # loss criterion\n        criterion = nn.MSELoss()\n\n        # spatial soft-argmax2d module\n        soft_argmax2d = kornia.geometry.SpatialSoftArgmax2d(\n            normalized_coordinates=False)\n\n        # NOTE: check where this comes from\n        temperature = (self.height * self.width) ** (0.5)\n\n        for iter_id in range(self.num_iterations):\n            x = params\n            sample = self.generate_sample(target).to(device)\n            pred = soft_argmax2d(temperature * x)\n            loss = criterion(pred, sample)\n            logger.debug(""Loss: {0:.3f} Pred: {1}"".format(loss.item(), pred))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        assert_allclose(pred[..., 0], target[..., 0], rtol=1e-2, atol=1e-2)\n        assert_allclose(pred[..., 1], target[..., 1], rtol=1e-2, atol=1e-2)\n'"
test/integration/test_warp.py,10,"b'import pytest\n\nimport kornia\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\nclass MyHomography(nn.Module):\n\n    def __init__(self, init_homo: torch.Tensor) -> None:\n        super().__init__()\n        self.homo = nn.Parameter(init_homo.clone().detach())\n\n    def forward(self) -> torch.Tensor:\n        return torch.unsqueeze(self.homo, dim=0)\n\n\nclass TestWarping:\n    # optimization\n    lr = 1e-3\n    num_iterations = 100\n\n    def test_smoke(self, device):\n\n        img_src_t: torch.Tensor = torch.rand(1, 3, 120, 120).to(device)\n        img_dst_t: torch.Tensor = torch.rand(1, 3, 120, 120).to(device)\n\n        init_homo: torch.Tensor = torch.from_numpy(\n            np.array([\n                [0.0415, 1.2731, -1.1731],\n                [-0.9094, 0.5072, 0.4272],\n                [0.0762, 1.3981, 1.0646]\n            ])\n        ).float()\n\n        height, width = img_dst_t.shape[-2:]\n        warper = kornia.HomographyWarper(height, width)\n        dst_homo_src = MyHomography(init_homo=init_homo).to(device)\n\n        learning_rate = self.lr\n        optimizer = optim.Adam(dst_homo_src.parameters(), lr=learning_rate)\n\n        for iter_idx in range(self.num_iterations):\n            # warp the reference image to the destiny with current homography\n            img_src_to_dst = warper(img_src_t, dst_homo_src())\n\n            # compute the photometric loss\n            loss = F.l1_loss(img_src_to_dst, img_dst_t)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            assert not bool(torch.isnan(dst_homo_src.homo.grad).any())\n'"
test/performance/test_imgwarp_speed.py,6,"b'import pytest\nfrom time import time\n\nimport torch\nimport kornia as kornia\n\n\nshapes = [(512, 3, 256, 256), (256, 1, 64, 64)]\nPSs = [224, 32]\n\n\ndef test_performance_speed(device, dtype):\n    if device.type != \'cuda\' or not torch.cuda.is_available():\n        pytest.skip(""Cuda not available in system,"")\n\n    print(""Benchmarking warp_affine"")\n    for input_shape in shapes:\n        for PS in PSs:\n            BS = input_shape[0]\n            inpt = torch.rand(input_shape).to(device)\n            As = torch.eye(3).unsqueeze(0).repeat(BS, 1, 1)[:, :2, :].to(device)\n            As += 0.1 * torch.rand(As.size()).to(device)\n            torch.cuda.synchronize(device)\n            t = time()\n            kornia_wa = kornia.warp_affine(inpt, As, (PS, PS))\n            print(f""inp={input_shape}, PS={PS}, dev={device}, {time() - t}, sec"")\n            torch.cuda.synchronize(device)\n'"
test/utils/test_grid.py,4,"b'import pytest\n\nimport torch\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\nfrom torch.testing import assert_allclose\n\n\ndef test_create_meshgrid():\n    height, width = 4, 6\n    normalized_coordinates = False\n\n    # create the meshgrid and verify shape\n    grid = kornia.utils.create_meshgrid(\n        height, width, normalized_coordinates)\n    assert grid.shape == (1, height, width, 2)\n\n    # check grid corner values\n    assert tuple(grid[0, 0, 0].numpy()) == (0., 0.)\n    assert tuple(\n        grid[0, height - 1, width - 1].numpy()) == (width - 1, height - 1)\n\n\ndef test_normalize_pixel_grid():\n    # generate input data\n    batch_size = 1\n    height, width = 2, 4\n\n    # create points grid\n    grid_norm = kornia.utils.create_meshgrid(\n        height, width, normalized_coordinates=True)\n    grid_norm = torch.unsqueeze(grid_norm, dim=0)\n    grid_pix = kornia.utils.create_meshgrid(\n        height, width, normalized_coordinates=False)\n    grid_pix = torch.unsqueeze(grid_pix, dim=0)\n\n    # grid from pixel space to normalized\n    norm_trans_pix = kornia.normal_transform_pixel(height, width)  # 1x3x3\n    pix_trans_norm = torch.inverse(norm_trans_pix)  # 1x3x3\n    # transform grids\n    grid_pix_to_norm = kornia.transform_points(norm_trans_pix, grid_pix)\n    grid_norm_to_pix = kornia.transform_points(pix_trans_norm, grid_norm)\n    assert_allclose(grid_pix, grid_norm_to_pix)\n    assert_allclose(grid_norm, grid_pix_to_norm)\n\n\ndef test_create_meshgrid3d():\n    depth, height, width = 5, 4, 6\n    normalized_coordinates = False\n\n    # create the meshgrid and verify shape\n    grid = kornia.utils.create_meshgrid3d(\n        depth, height, width, normalized_coordinates)\n    assert grid.shape == (1, depth, height, width, 3)\n\n    # check grid corner values\n    assert tuple(grid[0, 0, 0, 0].numpy()) == (0., 0., 0.)\n    assert tuple(\n        grid[0, depth - 1, height - 1, width - 1].numpy()) == (depth - 1, width - 1, height - 1)\n'"
test/utils/test_image.py,6,"b'import pytest\nimport numpy as np\n\nimport torch\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\n\n@pytest.mark.parametrize(""input_dtype, expected_dtype"",\n                         [(np.uint8, torch.uint8),\n                          (np.float32, torch.float32),\n                          (np.float64, torch.float64), ])\ndef test_image_to_tensor_keep_dtype(input_dtype, expected_dtype):\n    image = np.ones((1, 3, 4, 5), dtype=input_dtype)\n    tensor = kornia.image_to_tensor(image)\n    assert tensor.dtype == expected_dtype\n\n\n@pytest.mark.parametrize(""input_shape, expected"",\n                         [((4, 4), (4, 4)),\n                          ((1, 4, 4), (4, 4)),\n                          ((1, 1, 4, 4), (4, 4)),\n                          ((3, 4, 4), (4, 4, 3)),\n                          ((2, 3, 4, 4), (2, 4, 4, 3)),\n                          ((1, 3, 4, 4), (4, 4, 3)), ])\ndef test_tensor_to_image(device, input_shape, expected):\n    tensor = torch.ones(input_shape).to(device)\n    image = kornia.utils.tensor_to_image(tensor)\n    assert image.shape == expected\n    assert isinstance(image, np.ndarray)\n\n\n@pytest.mark.parametrize(""input_shape, expected"",\n                         [((4, 4), (1, 1, 4, 4)),\n                          ((1, 4, 4), (1, 4, 1, 4)),\n                          ((2, 3, 4), (1, 4, 2, 3)),\n                          ((4, 4, 3), (1, 3, 4, 4)),\n                          ((2, 4, 4, 3), (2, 3, 4, 4)),\n                          ((1, 4, 4, 3), (1, 3, 4, 4)), ])\ndef test_image_to_tensor(input_shape, expected):\n    image = np.ones(input_shape)\n    tensor = kornia.utils.image_to_tensor(image, keepdim=False)\n    assert tensor.shape == expected\n    assert isinstance(tensor, torch.Tensor)\n\n\n@pytest.mark.parametrize(""input_shape, expected"",\n                         [((4, 4), (1, 4, 4)),\n                          ((1, 4, 4), (4, 1, 4)),\n                          ((2, 3, 4), (4, 2, 3)),\n                          ((4, 4, 3), (3, 4, 4)),\n                          ((2, 4, 4, 3), (2, 3, 4, 4)),\n                          ((1, 4, 4, 3), (1, 3, 4, 4)), ])\ndef test_image_to_tensor_keepdim(input_shape, expected):\n    image = np.ones(input_shape)\n    tensor = kornia.utils.image_to_tensor(image, keepdim=True)\n    assert tensor.shape == expected\n    assert isinstance(tensor, torch.Tensor)\n'"
test/utils/test_metrics.py,57,"b'import pytest\n\nimport torch\nimport kornia as kornia\nimport kornia.testing as utils\n\nfrom torch.testing import assert_allclose\n\n\nclass TestMeanIoU:\n    def test_two_classes_perfect(self):\n        batch_size = 1\n        num_classes = 2\n        actual = torch.tensor(\n            [[1, 1, 1, 1, 0, 0, 0, 0]])\n        predicted = torch.tensor(\n            [[1, 1, 1, 1, 0, 0, 0, 0]])\n\n        mean_iou = kornia.utils.metrics.mean_iou(predicted, actual, num_classes)\n        mean_iou_real = torch.tensor(\n            [[1.0, 1.0]], dtype=torch.float32)\n        assert mean_iou.shape == (batch_size, num_classes)\n        assert_allclose(mean_iou, mean_iou_real)\n\n    def test_two_classes_perfect_batch2(self):\n        batch_size = 2\n        num_classes = 2\n        actual = torch.tensor(\n            [[1, 1, 1, 1, 0, 0, 0, 0]]).repeat(batch_size, 1)\n        predicted = torch.tensor(\n            [[1, 1, 1, 1, 0, 0, 0, 0]]).repeat(batch_size, 1)\n\n        mean_iou = kornia.utils.metrics.mean_iou(predicted, actual, num_classes)\n        mean_iou_real = torch.tensor(\n            [[1.0, 1.0]], dtype=torch.float32)\n        assert mean_iou.shape == (batch_size, num_classes)\n        assert_allclose(mean_iou, mean_iou_real)\n\n    def test_two_classes(self):\n        batch_size = 1\n        num_classes = 2\n        actual = torch.tensor(\n            [[1, 1, 1, 1, 0, 0, 0, 0]])\n        predicted = torch.tensor(\n            [[1, 1, 1, 1, 0, 0, 0, 1]])\n\n        mean_iou = kornia.utils.metrics.mean_iou(predicted, actual, num_classes)\n        mean_iou = kornia.utils.metrics.mean_iou(predicted, actual, num_classes)\n        mean_iou_real = torch.tensor(\n            [[0.75, 0.80]], dtype=torch.float32)\n        assert mean_iou.shape == (batch_size, num_classes)\n        assert_allclose(mean_iou, mean_iou_real)\n\n    def test_four_classes_2d_perfect(self):\n        batch_size = 1\n        num_classes = 4\n        actual = torch.tensor(\n            [[[0, 0, 1, 1],\n              [0, 0, 1, 1],\n              [2, 2, 3, 3],\n              [2, 2, 3, 3]]])\n        predicted = torch.tensor(\n            [[[0, 0, 1, 1],\n              [0, 0, 1, 1],\n              [2, 2, 3, 3],\n              [2, 2, 3, 3]]])\n\n        mean_iou = kornia.utils.metrics.mean_iou(predicted, actual, num_classes)\n        mean_iou_real = torch.tensor(\n            [[1.0, 1.0, 1.0, 1.0]], dtype=torch.float32)\n        assert mean_iou.shape == (batch_size, num_classes)\n        assert_allclose(mean_iou, mean_iou_real)\n\n    def test_four_classes_one_missing(self):\n        batch_size = 1\n        num_classes = 4\n        actual = torch.tensor(\n            [[[0, 0, 0, 0],\n              [0, 0, 0, 0],\n              [2, 2, 3, 3],\n              [2, 2, 3, 3]]])\n        predicted = torch.tensor(\n            [[[3, 3, 2, 2],\n              [3, 3, 2, 2],\n              [2, 2, 3, 3],\n              [2, 2, 3, 3]]])\n\n        mean_iou = kornia.utils.metrics.mean_iou(predicted, actual, num_classes)\n        mean_iou_real = torch.tensor(\n            [[0.0, 1.0, 0.5, 0.5]], dtype=torch.float32)\n        assert mean_iou.shape == (batch_size, num_classes)\n        assert_allclose(mean_iou, mean_iou_real)\n\n\nclass TestConfusionMatrix:\n    def test_two_classes(self):\n        num_classes = 2\n        actual = torch.tensor(\n            [[1, 1, 1, 1, 0, 0, 0, 0]])\n        predicted = torch.tensor(\n            [[1, 1, 1, 1, 0, 0, 0, 1]])\n\n        conf_mat = kornia.utils.metrics.confusion_matrix(\n            predicted, actual, num_classes)\n        conf_mat_real = torch.tensor(\n            [[[3, 1],\n              [0, 4]]], dtype=torch.float32)\n        assert_allclose(conf_mat, conf_mat_real)\n\n    def test_two_classes_batch2(self):\n        batch_size = 2\n        num_classes = 2\n        actual = torch.tensor(\n            [[1, 1, 1, 1, 0, 0, 0, 0]]).repeat(batch_size, 1)\n        predicted = torch.tensor(\n            [[1, 1, 1, 1, 0, 0, 0, 1]]).repeat(batch_size, 1)\n\n        conf_mat = kornia.utils.metrics.confusion_matrix(\n            predicted, actual, num_classes)\n        conf_mat_real = torch.tensor(\n            [[[3, 1],\n              [0, 4]]], dtype=torch.float32)\n        assert_allclose(conf_mat, conf_mat_real)\n\n    def test_three_classes(self):\n        num_classes = 3\n        actual = torch.tensor(\n            [[2, 2, 0, 0, 1, 0, 0, 2, 1, 1, 0, 0, 1, 2, 1, 0]])\n        predicted = torch.tensor(\n            [[2, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 1, 0, 0, 2, 2]])\n\n        conf_mat = kornia.utils.metrics.confusion_matrix(\n            predicted, actual, num_classes)\n        conf_mat_real = torch.tensor(\n            [[[4, 1, 2],\n              [3, 0, 2],\n              [1, 2, 1]]], dtype=torch.float32)\n        assert_allclose(conf_mat, conf_mat_real)\n\n    def test_four_classes_one_missing(self):\n        num_classes = 4\n        actual = torch.tensor(\n            [[3, 3, 1, 1, 2, 1, 1, 3, 2, 2, 1, 1, 2, 3, 2, 1]])\n        predicted = torch.tensor(\n            [[3, 2, 1, 1, 1, 1, 1, 2, 1, 3, 3, 2, 1, 1, 3, 3]])\n\n        conf_mat = kornia.utils.metrics.confusion_matrix(\n            predicted, actual, num_classes)\n        conf_mat_real = torch.tensor(\n            [[[0, 0, 0, 0],\n              [0, 4, 1, 2],\n              [0, 3, 0, 2],\n              [0, 1, 2, 1]]], dtype=torch.float32)\n        assert_allclose(conf_mat, conf_mat_real)\n\n    def test_three_classes_normalized(self):\n        num_classes = 3\n        normalized = True\n        actual = torch.tensor(\n            [[2, 2, 0, 0, 1, 0, 0, 2, 1, 1, 0, 0, 1, 2, 1, 0]])\n        predicted = torch.tensor(\n            [[2, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 1, 0, 0, 2, 2]])\n\n        conf_mat = kornia.utils.metrics.confusion_matrix(\n            predicted, actual, num_classes, normalized)\n\n        conf_mat_real = torch.tensor(\n            [[[0.5000, 0.3333, 0.4000],\n              [0.3750, 0.0000, 0.4000],\n              [0.1250, 0.6667, 0.2000]]], dtype=torch.float32)\n        assert_allclose(conf_mat, conf_mat_real)\n\n    def test_four_classes_2d_perfect(self):\n        num_classes = 4\n        actual = torch.tensor(\n            [[[0, 0, 1, 1],\n              [0, 0, 1, 1],\n              [2, 2, 3, 3],\n              [2, 2, 3, 3]]])\n        predicted = torch.tensor(\n            [[[0, 0, 1, 1],\n              [0, 0, 1, 1],\n              [2, 2, 3, 3],\n              [2, 2, 3, 3]]])\n\n        conf_mat = kornia.utils.metrics.confusion_matrix(\n            predicted, actual, num_classes)\n        conf_mat_real = torch.tensor(\n            [[[4, 0, 0, 0],\n              [0, 4, 0, 0],\n              [0, 0, 4, 0],\n              [0, 0, 0, 4]]], dtype=torch.float32)\n        assert_allclose(conf_mat, conf_mat_real)\n\n    def test_four_classes_2d_one_class_nonperfect(self):\n        num_classes = 4\n        actual = torch.tensor(\n            [[[0, 0, 1, 1],\n              [0, 0, 1, 1],\n              [2, 2, 3, 3],\n              [2, 2, 3, 3]]])\n        predicted = torch.tensor(\n            [[[0, 0, 1, 1],\n              [0, 3, 0, 1],\n              [2, 2, 1, 3],\n              [2, 2, 3, 3]]])\n\n        conf_mat = kornia.utils.metrics.confusion_matrix(\n            predicted, actual, num_classes)\n        conf_mat_real = torch.tensor(\n            [[[3, 0, 0, 1],\n              [1, 3, 0, 0],\n              [0, 0, 4, 0],\n              [0, 1, 0, 3]]], dtype=torch.float32)\n        assert_allclose(conf_mat, conf_mat_real)\n\n    def test_four_classes_2d_one_class_missing(self):\n        num_classes = 4\n        actual = torch.tensor(\n            [[[0, 0, 1, 1],\n              [0, 0, 1, 1],\n              [2, 2, 3, 3],\n              [2, 2, 3, 3]]])\n        predicted = torch.tensor(\n            [[[3, 3, 1, 1],\n              [3, 3, 1, 1],\n              [2, 2, 3, 3],\n              [2, 2, 3, 3]]])\n\n        conf_mat = kornia.utils.metrics.confusion_matrix(\n            predicted, actual, num_classes)\n        conf_mat_real = torch.tensor(\n            [[[0, 0, 0, 4],\n              [0, 4, 0, 0],\n              [0, 0, 4, 0],\n              [0, 0, 0, 4]]], dtype=torch.float32)\n        assert_allclose(conf_mat, conf_mat_real)\n\n    def test_four_classes_2d_one_class_no_predicted(self):\n        num_classes = 4\n        actual = torch.tensor(\n            [[[0, 0, 0, 0],\n              [0, 0, 0, 0],\n              [2, 2, 3, 3],\n              [2, 2, 3, 3]]])\n        predicted = torch.tensor(\n            [[[3, 3, 2, 2],\n              [3, 3, 2, 2],\n              [2, 2, 3, 3],\n              [2, 2, 3, 3]]])\n\n        conf_mat = kornia.utils.metrics.confusion_matrix(\n            predicted, actual, num_classes)\n        conf_mat_real = torch.tensor(\n            [[[0, 0, 4, 4],\n              [0, 0, 0, 0],\n              [0, 0, 4, 0],\n              [0, 0, 0, 4]]], dtype=torch.float32)\n        assert_allclose(conf_mat, conf_mat_real)\n'"
test/utils/test_one_hot.py,1,"b'import pytest\n\nimport torch\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\n\nclass TestOneHot:\n    def test_smoke(self):\n        num_classes = 4\n        labels = torch.zeros(2, 2, 1, dtype=torch.int64)\n        labels[0, 0, 0] = 0\n        labels[0, 1, 0] = 1\n        labels[1, 0, 0] = 2\n        labels[1, 1, 0] = 3\n\n        # convert labels to one hot tensor\n        one_hot = kornia.utils.one_hot(labels, num_classes)\n\n        assert pytest.approx(one_hot[0, labels[0, 0, 0], 0, 0].item(), 1.0)\n        assert pytest.approx(one_hot[0, labels[0, 1, 0], 1, 0].item(), 1.0)\n        assert pytest.approx(one_hot[1, labels[1, 0, 0], 0, 0].item(), 1.0)\n        assert pytest.approx(one_hot[1, labels[1, 1, 0], 1, 0].item(), 1.0)\n'"
test/utils/test_pointcloud_io.py,2,"b'import pytest\nimport os\n\nimport kornia\nimport torch\nfrom torch.testing import assert_allclose\n\n\nclass TestSaveLoadPointCloud:\n    def test_save_pointcloud(self):\n        height, width = 10, 8\n        xyz_save = torch.rand(height, width, 3)\n\n        # save to file\n        filename = ""pointcloud.ply""\n        kornia.save_pointcloud_ply(filename, xyz_save)\n\n        # load file\n        xyz_load = kornia.load_pointcloud_ply(filename)\n        assert_allclose(xyz_save.reshape(-1, 3), xyz_load)\n\n        # remove the temporal file\n        if os.path.exists(filename):\n            os.remove(filename)\n'"
kornia/geometry/camera/__init__.py,0,"b'from .pinhole import PinholeCamera, pixel2cam, cam2pixel\nfrom .perspective import unproject_points, project_points\n\n__all__ = [\n    ""PinholeCamera"",\n    ""pixel2cam"",\n    ""cam2pixel"",\n    ""unproject_points"",\n    ""project_points"",\n]\n'"
kornia/geometry/camera/perspective.py,43,"b'from typing import Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom kornia.geometry.linalg import transform_points\nfrom kornia.geometry.conversions import (\n    convert_points_to_homogeneous, convert_points_from_homogeneous\n)\n\n\ndef project_points(\n        point_3d: torch.Tensor,\n        camera_matrix: torch.Tensor) -> torch.Tensor:\n    r""""""Projects a 3d point onto the 2d camera plane.\n\n    Args:\n        point3d (torch.Tensor): tensor containing the 3d points to be projected\n            to the camera plane. The shape of the tensor can be :math:`(*, 3)`.\n        camera_matrix (torch.Tensor): tensor containing the intrinsics camera\n            matrix. The tensor shape must be Bx4x4.\n\n    Returns:\n        torch.Tensor: array of (u, v) cam coordinates with shape :math:`(*, 2)`.\n    """"""\n    if not torch.is_tensor(point_3d):\n        raise TypeError(""Input point_3d type is not a torch.Tensor. Got {}""\n                        .format(type(point_3d)))\n    if not torch.is_tensor(camera_matrix):\n        raise TypeError(""Input camera_matrix type is not a torch.Tensor. Got {}""\n                        .format(type(camera_matrix)))\n    if not (point_3d.device == camera_matrix.device):\n        raise ValueError(""Input tensors must be all in the same device."")\n    if not point_3d.shape[-1] == 3:\n        raise ValueError(""Input points_3d must be in the shape of (*, 3).""\n                         "" Got {}"".format(point_3d.shape))\n    if not camera_matrix.shape[-2:] == (3, 3):\n        raise ValueError(\n            ""Input camera_matrix must be in the shape of (*, 3, 3)."")\n    # projection eq. [u, v, w]\' = K * [x y z 1]\'\n    # u = fx * X / Z + cx\n    # v = fy * Y / Z + cy\n\n    # project back using depth dividing in a safe way\n    xy_coords: torch.Tensor = convert_points_from_homogeneous(point_3d)\n    x_coord: torch.Tensor = xy_coords[..., 0]\n    y_coord: torch.Tensor = xy_coords[..., 1]\n\n    # unpack intrinsics\n    fx: torch.Tensor = camera_matrix[..., 0, 0]\n    fy: torch.Tensor = camera_matrix[..., 1, 1]\n    cx: torch.Tensor = camera_matrix[..., 0, 2]\n    cy: torch.Tensor = camera_matrix[..., 1, 2]\n\n    # apply intrinsics ans return\n    u_coord: torch.Tensor = x_coord * fx + cx\n    v_coord: torch.Tensor = y_coord * fy + cy\n\n    return torch.stack([u_coord, v_coord], dim=-1)\n\n\ndef unproject_points(\n        point_2d: torch.Tensor,\n        depth: torch.Tensor,\n        camera_matrix: torch.Tensor,\n        normalize: bool = False) -> torch.Tensor:\n    r""""""Unprojects a 2d point in 3d.\n\n    Transform coordinates in the pixel frame to the camera frame.\n\n    Args:\n        point2d (torch.Tensor): tensor containing the 2d to be projected to\n            world coordinates. The shape of the tensor can be :math:`(*, 2)`.\n        depth (torch.Tensor): tensor containing the depth value of each 2d\n            points. The tensor shape must be equal to point2d :math:`(*, 1)`.\n        camera_matrix (torch.Tensor): tensor containing the intrinsics camera\n            matrix. The tensor shape must be Bx4x4.\n        normalize (bool, optional): whether to normalize the pointcloud. This\n            must be set to `True` when the depth is represented as the Euclidean\n            ray length from the camera position. Default is `False`.\n\n    Returns:\n        torch.Tensor: tensor of (x, y, z) world coordinates with shape\n        :math:`(*, 3)`.\n    """"""\n    if not torch.is_tensor(point_2d):\n        raise TypeError(""Input point_2d type is not a torch.Tensor. Got {}""\n                        .format(type(point_2d)))\n\n    if not torch.is_tensor(depth):\n        raise TypeError(""Input depth type is not a torch.Tensor. Got {}""\n                        .format(type(depth)))\n\n    if not torch.is_tensor(camera_matrix):\n        raise TypeError(""Input camera_matrix type is not a torch.Tensor. Got {}""\n                        .format(type(camera_matrix)))\n\n    if not (point_2d.device == depth.device == camera_matrix.device):\n        raise ValueError(""Input tensors must be all in the same device."")\n\n    if not point_2d.shape[-1] == 2:\n        raise ValueError(""Input points_2d must be in the shape of (*, 2).""\n                         "" Got {}"".format(point_2d.shape))\n\n    if not depth.shape[-1] == 1:\n        raise ValueError(""Input depth must be in the shape of (*, 1).""\n                         "" Got {}"".format(depth.shape))\n\n    if not camera_matrix.shape[-2:] == (3, 3):\n        raise ValueError(\n            ""Input camera_matrix must be in the shape of (*, 3, 3)."")\n    # projection eq. K_inv * [u v 1]\'\n    # x = (u - cx) * Z / fx\n    # y = (v - cy) * Z / fy\n\n    # unpack coordinates\n    u_coord: torch.Tensor = point_2d[..., 0]\n    v_coord: torch.Tensor = point_2d[..., 1]\n\n    # unpack intrinsics\n    fx: torch.Tensor = camera_matrix[..., 0, 0]\n    fy: torch.Tensor = camera_matrix[..., 1, 1]\n    cx: torch.Tensor = camera_matrix[..., 0, 2]\n    cy: torch.Tensor = camera_matrix[..., 1, 2]\n\n    # projective\n    x_coord: torch.Tensor = (u_coord - cx) / fx\n    y_coord: torch.Tensor = (v_coord - cy) / fy\n\n    xyz: torch.Tensor = torch.stack([x_coord, y_coord], dim=-1)\n    xyz = convert_points_to_homogeneous(xyz)\n\n    if normalize:\n        xyz = F.normalize(xyz, dim=-1, p=2)\n\n    return xyz * depth\n'"
kornia/geometry/camera/pinhole.py,100,"b'from typing import Iterable, Optional, Union\nimport warnings\n\nimport torch\nimport torch.nn as nn\n\nfrom kornia.geometry.linalg import transform_points\nfrom kornia.geometry.linalg import inverse_transformation\n\n\nclass PinholeCamera:\n    r""""""Class that represents a Pinhole Camera model.\n\n    Args:\n        intrinsics (torch.Tensor): tensor with shape :math:`(B, 4, 4)`\n          containing the full 4x4 camera calibration matrix.\n        extrinsics (torch.Tensor): tensor with shape :math:`(B, 4, 4)`\n          containing the full 4x4 rotation-translation matrix.\n        height (torch.Tensor): tensor with shape :math:`(B)` containing the\n          image height.\n        width (torch.Tensor): tensor with shape :math:`(B)` containing the image\n          width.\n\n    .. note::\n        We assume that the class attributes are in batch form in order to take\n        advantage of PyTorch parallelism to boost computing performce.\n    """"""\n\n    def __init__(self, intrinsics: torch.Tensor,\n                 extrinsics: torch.Tensor,\n                 height: torch.Tensor,\n                 width: torch.Tensor) -> None:\n        # verify batch size and shapes\n        self._check_valid([intrinsics, extrinsics, height, width])\n        self._check_valid_params(intrinsics, ""intrinsics"")\n        self._check_valid_params(extrinsics, ""extrinsics"")\n        self._check_valid_shape(height, ""height"")\n        self._check_valid_shape(width, ""width"")\n        # set class attributes\n        self.height: torch.Tensor = height\n        self.width: torch.Tensor = width\n        self._intrinsics: torch.Tensor = intrinsics\n        self._extrinsics: torch.Tensor = extrinsics\n\n    @staticmethod\n    def _check_valid(data_iter: Iterable[torch.Tensor]) -> bool:\n        if not all(data.shape[0] for data in data_iter):\n            raise ValueError(""Arguments shapes must match"")\n        return True\n\n    @staticmethod\n    def _check_valid_params(\n            data: torch.Tensor,\n            data_name: str) -> bool:\n        if len(data.shape) not in (3, 4,) and data.shape[-2:] != (4, 4):  # Shouldn\'t this be an OR logic than AND?\n            raise ValueError(""Argument {0} shape must be in the following shape""\n                             "" Bx4x4 or BxNx4x4. Got {1}"".format(data_name,\n                                                                 data.shape))\n        return True\n\n    @staticmethod\n    def _check_valid_shape(\n            data: torch.Tensor,\n            data_name: str) -> bool:\n        if not len(data.shape) == 1:\n            raise ValueError(""Argument {0} shape must be in the following shape""\n                             "" B. Got {1}"".format(data_name, data.shape))\n        return True\n\n    @property\n    def intrinsics(self) -> torch.Tensor:\n        r""""""The full 4x4 intrinsics matrix.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B, 4, 4)`\n        """"""\n        assert self._check_valid_params(self._intrinsics, ""intrinsics"")\n        return self._intrinsics\n\n    @property\n    def extrinsics(self) -> torch.Tensor:\n        r""""""The full 4x4 extrinsics matrix.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B, 4, 4)`\n        """"""\n        assert self._check_valid_params(self._extrinsics, ""extrinsics"")\n        return self._extrinsics\n\n    @property\n    def batch_size(self) -> int:\n        r""""""Returns the batch size of the storage.\n\n        Returns:\n            int: scalar with the batch size\n        """"""\n        return self.intrinsics.shape[0]\n\n    @property\n    def fx(self) -> torch.Tensor:\n        r""""""Returns the focal lenght in the x-direction.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B)`\n        """"""\n        return self.intrinsics[..., 0, 0]\n\n    @property\n    def fy(self) -> torch.Tensor:\n        r""""""Returns the focal lenght in the y-direction.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B)`\n        """"""\n        return self.intrinsics[..., 1, 1]\n\n    @property\n    def cx(self) -> torch.Tensor:\n        r""""""Returns the x-coordinate of the principal point.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B)`\n        """"""\n        return self.intrinsics[..., 0, 2]\n\n    @property\n    def cy(self) -> torch.Tensor:\n        r""""""Returns the y-coordinate of the principal point.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B)`\n        """"""\n        return self.intrinsics[..., 1, 2]\n\n    @property\n    def tx(self) -> torch.Tensor:\n        r""""""Returns the x-coordinate of the translation vector.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B)`\n        """"""\n        return self.extrinsics[..., 0, -1]\n\n    @tx.setter\n    def tx(self, value) -> \'PinholeCamera\':\n        r""""""Set the x-coordinate of the translation vector with the given\n        value.\n        """"""\n        self.extrinsics[..., 0, -1] = value\n        return self\n\n    @property\n    def ty(self) -> torch.Tensor:\n        r""""""Returns the y-coordinate of the translation vector.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B)`\n        """"""\n        return self.extrinsics[..., 1, -1]\n\n    @ty.setter\n    def ty(self, value) -> \'PinholeCamera\':\n        r""""""Set the y-coordinate of the translation vector with the given\n        value.\n        """"""\n        self.extrinsics[..., 1, -1] = value\n        return self\n\n    @property\n    def tz(self) -> torch.Tensor:\n        r""""""Returns the z-coordinate of the translation vector.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B)`\n        """"""\n        return self.extrinsics[..., 2, -1]\n\n    @tz.setter\n    def tz(self, value) -> \'PinholeCamera\':\n        r""""""Set the y-coordinate of the translation vector with the given\n        value.\n        """"""\n        self.extrinsics[..., 2, -1] = value\n        return self\n\n    @property\n    def rt_matrix(self) -> torch.Tensor:\n        r""""""Returns the 3x4 rotation-translation matrix.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B, 3, 4)`\n        """"""\n        return self.extrinsics[..., :3, :4]\n\n    @property\n    def camera_matrix(self) -> torch.Tensor:\n        r""""""Returns the 3x3 camera matrix containing the intrinsics.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B, 3, 3)`\n        """"""\n        return self.intrinsics[..., :3, :3]\n\n    @property\n    def rotation_matrix(self) -> torch.Tensor:\n        r""""""Returns the 3x3 rotation matrix from the extrinsics.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B, 3, 3)`\n        """"""\n        return self.extrinsics[..., :3, :3]\n\n    @property\n    def translation_vector(self) -> torch.Tensor:\n        r""""""Returns the translation vector from the extrinsics.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B, 3, 1)`\n        """"""\n        return self.extrinsics[..., :3, -1:]\n\n    def clone(self) -> \'PinholeCamera\':\n        r""""""Returns a deep copy of the current object instance.""""""\n        height: torch.Tensor = self.height.clone()\n        width: torch.Tensor = self.width.clone()\n        intrinsics: torch.Tensor = self.intrinsics.clone()\n        extrinsics: torch.Tensor = self.extrinsics.clone()\n        return PinholeCamera(intrinsics, extrinsics, height, width)\n\n    def intrinsics_inverse(self) -> torch.Tensor:\n        r""""""Returns the inverse of the 4x4 instrisics matrix.\n\n        Returns:\n            torch.Tensor: tensor of shape :math:`(B, 4, 4)`\n        """"""\n        return self.intrinsics.inverse()\n\n    def scale(self, scale_factor) -> \'PinholeCamera\':\n        r""""""Scales the pinhole model.\n\n        Args:\n            scale_factor (torch.Tensor): a tensor with the scale factor. It has\n              to be broadcastable with class members. The expected shape is\n              :math:`(B)` or :math:`(1)`.\n\n        Returns:\n            PinholeCamera: the camera model with scaled parameters.\n        """"""\n        # scale the intrinsic parameters\n        intrinsics: torch.Tensor = self.intrinsics.clone()\n        intrinsics[..., 0, 0] *= scale_factor\n        intrinsics[..., 1, 1] *= scale_factor\n        intrinsics[..., 0, 2] *= scale_factor\n        intrinsics[..., 1, 2] *= scale_factor\n        # scale the image height/width\n        height: torch.Tensor = scale_factor * self.height.clone()\n        width: torch.Tensor = scale_factor * self.width.clone()\n        return PinholeCamera(intrinsics, self.extrinsics, height, width)\n\n    def scale_(self, scale_factor) -> \'PinholeCamera\':\n        r""""""Scales the pinhole model in-place.\n\n        Args:\n            scale_factor (torch.Tensor): a tensor with the scale factor. It has\n              to be broadcastable with class members. The expected shape is\n              :math:`(B)` or :math:`(1)`.\n\n        Returns:\n            PinholeCamera: the camera model with scaled parameters.\n        """"""\n        # scale the intrinsic parameters\n        self.intrinsics[..., 0, 0] *= scale_factor\n        self.intrinsics[..., 1, 1] *= scale_factor\n        self.intrinsics[..., 0, 2] *= scale_factor\n        self.intrinsics[..., 1, 2] *= scale_factor\n        # scale the image height/width\n        self.height *= scale_factor\n        self.width *= scale_factor\n        return self\n\n    # NOTE: just for test. Decide if we keep it.\n    @classmethod\n    def from_parameters(\n            self,\n            fx,\n            fy,\n            cx,\n            cy,\n            height,\n            width,\n            tx,\n            ty,\n            tz,\n            batch_size=1):\n        # create the camera matrix\n        intrinsics = torch.zeros(batch_size, 4, 4)\n        intrinsics[..., 0, 0] += fx\n        intrinsics[..., 1, 1] += fy\n        intrinsics[..., 0, 2] += cx\n        intrinsics[..., 1, 2] += cy\n        intrinsics[..., 2, 2] += 1.0\n        intrinsics[..., 3, 3] += 1.0\n        # create the pose matrix\n        extrinsics = torch.eye(4).repeat(batch_size, 1, 1)\n        extrinsics[..., 0, -1] += tx\n        extrinsics[..., 1, -1] += ty\n        extrinsics[..., 2, -1] += tz\n        # create image hegith and width\n        height_tmp = torch.zeros(batch_size)\n        height_tmp[..., 0] += height\n        width_tmp = torch.zeros(batch_size)\n        width_tmp[..., 0] += width\n        return self(intrinsics, extrinsics, height_tmp, width_tmp)\n\n\nclass PinholeCamerasList(PinholeCamera):\n    r""""""Class that represents a list of pinhole cameras.\n\n    The class inherits from :class:`~kornia.PinholeCamera` meaning that\n    it will keep the same class properties but with an extra dimension.\n\n    .. note::\n        The underlying data tensor will be stacked in the first dimension.\n        That\'s it, given a list of two camera instances, the intrinsics tensor\n        will have a shape :math:`(B, N, 4, 4)` where :math:`B` is the batch\n        size and :math:`N` is the numbers of cameras (in this case two).\n\n    Args:\n        pinholes_list (Iterable[PinholeCamera]): a python tuple or list\n          containg a set of `PinholeCamera` instances.\n    """"""\n\n    def __init__(self, pinholes_list: Iterable[PinholeCamera]) -> None:\n        self._initialize_parameters(pinholes_list)\n\n    def _initialize_parameters(\n            self,\n            pinholes: Iterable[PinholeCamera]) -> \'PinholeCamerasList\':\n        r""""""Initialises the class attributes given a cameras list.""""""\n        if not isinstance(pinholes, (list, tuple,)):\n            raise TypeError(""pinhole must of type list or tuple. Got {}""\n                            .format(type(pinholes)))\n        height, width = [], []\n        intrinsics, extrinsics = [], []\n        for pinhole in pinholes:\n            if not isinstance(pinhole, PinholeCamera):\n                raise TypeError(""Argument pinhole must be from type ""\n                                ""PinholeCamera. Got {}"".format(type(pinhole)))\n            height.append(pinhole.height)\n            width.append(pinhole.width)\n            intrinsics.append(pinhole.intrinsics)\n            extrinsics.append(pinhole.extrinsics)\n        # contatenate and set members. We will assume BxNx4x4\n        self.height: torch.Tensor = torch.stack(height, dim=1)\n        self.width: torch.Tensor = torch.stack(width, dim=1)\n        self._intrinsics: torch.Tensor = torch.stack(intrinsics, dim=1)\n        self._extrinsics: torch.Tensor = torch.stack(extrinsics, dim=1)\n        return self\n\n    @property\n    def num_cameras(self) -> int:\n        r""""""Returns the number of pinholes cameras per batch.""""""\n        num_cameras: int = -1\n        if self.intrinsics is not None:\n            num_cameras = int(self.intrinsics.shape[1])\n        return num_cameras\n\n    def get_pinhole(self, idx: int) -> PinholeCamera:\n        r""""""Returns a PinholeCamera object with parameters such as Bx4x4.""""""\n        height: torch.Tensor = self.height[..., idx]\n        width: torch.Tensor = self.width[..., idx]\n        intrinsics: torch.Tensor = self.intrinsics[:, idx]\n        extrinsics: torch.Tensor = self.extrinsics[:, idx]\n        return PinholeCamera(intrinsics, extrinsics, height, width)\n\n\ndef pinhole_matrix(pinholes, eps=1e-6):\n    r""""""Function that returns the pinhole matrix from a pinhole model\n\n    .. note::\n        This method is going to be deprecated in version 0.2 in favour of\n        :attr:`kornia.PinholeCamera.camera_matrix`.\n\n    Args:\n        pinholes (Tensor): tensor of pinhole models.\n\n    Returns:\n        Tensor: tensor of pinhole matrices.\n\n    Shape:\n        - Input: :math:`(N, 12)`\n        - Output: :math:`(N, 4, 4)`\n\n    Example:\n        >>> pinhole = torch.rand(1, 12)    # Nx12\n        >>> pinhole_matrix = kornia.pinhole_matrix(pinhole)  # Nx4x4\n    """"""\n    # warnings.warn(""pinhole_matrix will be deprecated in version 0.2, ""\n    #              ""use PinholeCamera.camera_matrix instead"",\n    #              PendingDeprecationWarning)\n    assert len(pinholes.shape) == 2 and pinholes.shape[1] == 12, pinholes.shape\n    # unpack pinhole values\n    fx, fy, cx, cy = torch.chunk(pinholes[..., :4], 4, dim=1)  # Nx1\n    # create output container\n    k = torch.eye(4, device=pinholes.device, dtype=pinholes.dtype) + eps\n    k = k.view(1, 4, 4).repeat(pinholes.shape[0], 1, 1)  # Nx4x4\n    # fill output with pinhole values\n    k[..., 0, 0:1] = fx\n    k[..., 0, 2:3] = cx\n    k[..., 1, 1:2] = fy\n    k[..., 1, 2:3] = cy\n    return k\n\n\ndef inverse_pinhole_matrix(pinhole, eps=1e-6):\n    r""""""Returns the inverted pinhole matrix from a pinhole model\n\n    .. note::\n        This method is going to be deprecated in version 0.2 in favour of\n        :attr:`kornia.PinholeCamera.intrinsics_inverse()`.\n\n    Args:\n        pinholes (Tensor): tensor with pinhole models.\n\n    Returns:\n        Tensor: tensor of inverted pinhole matrices.\n\n    Shape:\n        - Input: :math:`(N, 12)`\n        - Output: :math:`(N, 4, 4)`\n\n    Example:\n        >>> pinhole = torch.rand(1, 12)    # Nx12\n        >>> pinhole_matrix_inv = kornia.inverse_pinhole_matrix(pinhole)  # Nx4x4\n    """"""\n    # warnings.warn(""inverse_pinhole_matrix will be deprecated in version 0.2, ""\n    #              ""use PinholeCamera.intrinsics_inverse() instead"",\n    #              PendingDeprecationWarning)\n    assert len(pinhole.shape) == 2 and pinhole.shape[1] == 12, pinhole.shape\n    # unpack pinhole values\n    fx, fy, cx, cy = torch.chunk(pinhole[..., :4], 4, dim=1)  # Nx1\n    # create output container\n    k = torch.eye(4, device=pinhole.device, dtype=pinhole.dtype)\n    k = k.view(1, 4, 4).repeat(pinhole.shape[0], 1, 1)  # Nx4x4\n    # fill output with inverse values\n    k[..., 0, 0:1] = 1. / (fx + eps)\n    k[..., 1, 1:2] = 1. / (fy + eps)\n    k[..., 0, 2:3] = -1. * cx / (fx + eps)\n    k[..., 1, 2:3] = -1. * cy / (fy + eps)\n    return k\n\n\ndef scale_pinhole(pinholes, scale):\n    r""""""Scales the pinhole matrix for each pinhole model.\n\n    .. note::\n        This method is going to be deprecated in version 0.2 in favour of\n        :attr:`kornia.PinholeCamera.scale()`.\n\n    Args:\n        pinholes (Tensor): tensor with the pinhole model.\n        scale (Tensor): tensor of scales.\n\n    Returns:\n        Tensor: tensor of scaled pinholes.\n\n    Shape:\n        - Input: :math:`(N, 12)` and :math:`(N, 1)`\n        - Output: :math:`(N, 12)`\n\n    Example:\n        >>> pinhole_i = torch.rand(1, 12)  # Nx12\n        >>> scales = 2.0 * torch.ones(1)   # N\n        >>> pinhole_i_scaled = kornia.scale_pinhole(pinhole_i)  # Nx12\n    """"""\n    # warnings.warn(""scale_pinhole will be deprecated in version 0.2, ""\n    #              ""use PinholeCamera.scale() instead"",\n    #              PendingDeprecationWarning)\n    assert len(pinholes.shape) == 2 and pinholes.shape[1] == 12, pinholes.shape\n    assert len(scale.shape) == 1, scale.shape\n    pinholes_scaled = pinholes.clone()\n    pinholes_scaled[..., :6] = pinholes[..., :6] * scale.unsqueeze(-1)\n    return pinholes_scaled\n\n\ndef get_optical_pose_base(pinholes):\n    """"""Get extrinsic transformation matrices for pinholes\n\n    Args:\n        pinholes (Tensor): tensor of form [fx fy cx cy h w rx ry rz tx ty tz]\n                           of size (N, 12).\n\n    Returns:\n        Tensor: tensor of extrinsic transformation matrices of size (N, 4, 4).\n\n    """"""\n    assert len(pinholes.shape) == 2 and pinholes.shape[1] == 12, pinholes.shape\n    optical_pose_parent = pinholes[..., 6:]\n    return rtvec_to_pose(optical_pose_parent)\n\n\ndef homography_i_H_ref(pinhole_i, pinhole_ref):\n    r""""""Homography from reference to ith pinhole\n\n    .. note::\n        The pinhole model is represented in a single vector as follows:\n\n        .. math::\n            pinhole = (f_x, f_y, c_x, c_y, height, width,\n            r_x, r_y, r_z, t_x, t_y, t_z)\n\n        where:\n            :math:`(r_x, r_y, r_z)` is the rotation vector in angle-axis\n            convention.\n\n            :math:`(t_x, t_y, t_z)` is the translation vector.\n\n    .. math::\n\n        H_{ref}^{i} = K_{i} * T_{ref}^{i} * K_{ref}^{-1}\n\n    Args:\n        pinhole_i (Tensor): tensor with pinhole model for ith frame.\n        pinhole_ref (Tensor): tensor with pinhole model for reference frame.\n\n    Returns:\n        Tensor: tensors that convert depth points (u, v, d) from\n        pinhole_ref to pinhole_i.\n\n    Shape:\n        - Input: :math:`(N, 12)` and :math:`(N, 12)`\n        - Output: :math:`(N, 4, 4)`\n\n    Example:\n        >>> pinhole_i = torch.rand(1, 12)    # Nx12\n        >>> pinhole_ref = torch.rand(1, 12)  # Nx12\n        >>> i_H_ref = kornia.homography_i_H_ref(pinhole_i, pinhole_ref)  # Nx4x4\n    """"""\n    assert len(\n        pinhole_i.shape) == 2 and pinhole_i.shape[1] == 12, pinhole.shape\n    assert pinhole_i.shape == pinhole_ref.shape, pinhole_ref.shape\n    i_pose_base = get_optical_pose_base(pinhole_i)\n    ref_pose_base = get_optical_pose_base(pinhole_ref)\n    i_pose_ref = torch.matmul(i_pose_base,\n                              inverse_transformation(ref_pose_base))\n    return torch.matmul(\n        pinhole_matrix(pinhole_i),\n        torch.matmul(i_pose_ref, inverse_pinhole_matrix(pinhole_ref)))\n\n# based on:\n# https://github.com/ClementPinard/SfmLearner-Pytorch/blob/master/inverse_warp.py#L26\n\n\ndef pixel2cam(depth: torch.Tensor, intrinsics_inv: torch.Tensor,\n              pixel_coords: torch.Tensor) -> torch.Tensor:\n    r""""""Transform coordinates in the pixel frame to the camera frame.\n\n    Args:\n        depth (torch.Tensor): the source depth maps. Shape must be Bx1xHxW.\n        intrinsics_inv (torch.Tensor): the inverse intrinsics camera matrix.\n          Shape must be Bx4x4.\n        pixel_coords (torch.Tensor): the grid with the homogeneous camera\n          coordinates. Shape must be BxHxWx3.\n\n    Returns:\n        torch.Tensor: array of (u, v, 1) cam coordinates with shape BxHxWx3.\n    """"""\n    if not len(depth.shape) == 4 and depth.shape[1] == 1:\n        raise ValueError(""Input depth has to be in the shape of ""\n                         ""Bx1xHxW. Got {}"".format(depth.shape))\n    if not len(intrinsics_inv.shape) == 3:\n        raise ValueError(""Input intrinsics_inv has to be in the shape of ""\n                         ""Bx4x4. Got {}"".format(intrinsics_inv.shape))\n    if not len(pixel_coords.shape) == 4 and pixel_coords.shape[3] == 3:\n        raise ValueError(""Input pixel_coords has to be in the shape of ""\n                         ""BxHxWx3. Got {}"".format(intrinsics_inv.shape))\n    cam_coords: torch.Tensor = transform_points(\n        intrinsics_inv[:, None], pixel_coords)\n    return cam_coords * depth.permute(0, 2, 3, 1)\n\n\n# based on\n# https://github.com/ClementPinard/SfmLearner-Pytorch/blob/master/inverse_warp.py#L43\n\ndef cam2pixel(\n        cam_coords_src: torch.Tensor,\n        dst_proj_src: torch.Tensor,\n        eps: Optional[float] = 1e-6) -> torch.Tensor:\n    r""""""Transform coordinates in the camera frame to the pixel frame.\n\n    Args:\n        cam_coords (torch.Tensor): pixel coordinates defined in the first\n          camera coordinates system. Shape must be BxHxWx3.\n        dst_proj_src (torch.Tensor): the projection matrix between the\n          reference and the non reference camera frame. Shape must be Bx4x4.\n\n    Returns:\n        torch.Tensor: array of [-1, 1] coordinates of shape BxHxWx2.\n    """"""\n    if not len(cam_coords_src.shape) == 4 and cam_coords_src.shape[3] == 3:\n        raise ValueError(""Input cam_coords_src has to be in the shape of ""\n                         ""BxHxWx3. Got {}"".format(cam_coords_src.shape))\n    if not len(dst_proj_src.shape) == 3 and dst_proj_src.shape[-2:] == (4, 4):\n        raise ValueError(""Input dst_proj_src has to be in the shape of ""\n                         ""Bx4x4. Got {}"".format(dst_proj_src.shape))\n    b, h, w, _ = cam_coords_src.shape\n    # apply projection matrix to points\n    point_coords: torch.Tensor = transform_points(\n        dst_proj_src[:, None], cam_coords_src)\n    x_coord: torch.Tensor = point_coords[..., 0]\n    y_coord: torch.Tensor = point_coords[..., 1]\n    z_coord: torch.Tensor = point_coords[..., 2]\n\n    # compute pixel coordinates\n    u_coord: torch.Tensor = x_coord / (z_coord + eps)\n    v_coord: torch.Tensor = y_coord / (z_coord + eps)\n\n    # stack and return the coordinates, that\'s the actual flow\n    pixel_coords_dst: torch.Tensor = torch.stack([u_coord, v_coord], dim=-1)\n    return pixel_coords_dst  # (B*N)xHxWx2\n\n\n# layer api\n\n\n\'\'\'class PinholeMatrix(nn.Module):\n    r""""""Creates an object that returns the pinhole matrix from a pinhole model\n\n    Args:\n        pinholes (Tensor): tensor of pinhole models.\n\n    Returns:\n        Tensor: tensor of pinhole matrices.\n\n    Shape:\n        - Input: :math:`(N, 12)`\n        - Output: :math:`(N, 4, 4)`\n\n    Example:\n        >>> pinhole = torch.rand(1, 12)          # Nx12\n        >>> transform = kornia.PinholeMatrix()\n        >>> pinhole_matrix = transform(pinhole)  # Nx4x4\n    """"""\n\n    def __init__(self):\n        super(PinholeMatrix, self).__init__()\n\n    def forward(self, input):\n        return pinhole_matrix(input)\n\n\nclass InversePinholeMatrix(nn.Module):\n    r""""""Returns and object that inverts a pinhole matrix from a pinhole model\n\n    Args:\n        pinholes (Tensor): tensor with pinhole models.\n\n    Returns:\n        Tensor: tensor of inverted pinhole matrices.\n\n    Shape:\n        - Input: :math:`(N, 12)`\n        - Output: :math:`(N, 4, 4)`\n\n    Example:\n        >>> pinhole = torch.rand(1, 12)              # Nx12\n        >>> transform = kornia.InversePinholeMatrix()\n        >>> pinhole_matrix_inv = transform(pinhole)  # Nx4x4\n    """"""\n\n    def __init__(self):\n        super(InversePinholeMatrix, self).__init__()\n\n    def forward(self, input):\n        return inverse_pinhole_matrix(input)\'\'\'\n'"
kornia/geometry/epipolar/__init__.py,0,b'from .numeric import *\nfrom .fundamental import *\nfrom .projection import *\nfrom .essential import *\nfrom .triangulation import *\nfrom .metrics import *\nfrom .scene import *\n'
kornia/geometry/epipolar/essential.py,44,"b'""""""Module containing functionalities for the Essential matrix.""""""\nfrom typing import Tuple\n\nimport torch\n\nfrom kornia.geometry.epipolar import numeric\nfrom kornia.geometry.epipolar import projection\nfrom kornia.geometry.epipolar import triangulation\n\n\ndef essential_from_fundamental(F_mat: torch.Tensor, K1: torch.Tensor, K2: torch.Tensor) -> torch.Tensor:\n    r""""""Get Essential matrix from Fundamental and Camera matrices.\n\n    Uses the method from Hartley/Zisserman 9.6 pag 257 (formula 9.12).\n\n    Args:\n        F_mat (torch.Tensor): The fundamental matrix with shape of :math:`(*, 3, 3)`.\n        K1 (torch.Tensor): The camera matrix from first camera with shape :math:`(*, 3, 3)`.\n        K2 (torch.Tensor): The camera matrix from second camera with shape :math:`(*, 3, 3)`.\n\n    Returns:\n        torch.Tensor: The essential matrix with shape :math:`(*, 3, 3)`.\n\n    """"""\n    assert len(F_mat.shape) >= 2 and F_mat.shape[-2:] == (3, 3), F_mat.shape\n    assert len(K1.shape) >= 2 and K1.shape[-2:] == (3, 3), K1.shape\n    assert len(K2.shape) >= 2 and K2.shape[-2:] == (3, 3), K2.shape\n    assert len(F_mat.shape[:-2]) == len(K1.shape[:-2]) == len(K2.shape[:-2])\n\n    return K2.transpose(-2, -1) @ F_mat @ K1\n\n\ndef decompose_essential_matrix(E_mat: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    r""""""Decompose an essential matrix to possible rotations and translation.\n\n    This function decomposes the essential matrix E using svd decomposition [96]\n    and give the possible solutions: :math:`R1, R2, t`.\n\n    Args:\n       E_mat (torch.Tensor): The essential matrix in the form of :math:`(*, 3, 3)`.\n\n    Returns:\n       Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing the first and\n       second possible rotation matrices and the translation vector. The shape of the tensors\n       with be same input :math:`[(*, 3, 3), (*, 3, 3), (*, 3, 1)]`.\n\n    """"""\n    assert len(E_mat.shape) >= 2 and E_mat.shape[-2:], E_mat.shape\n\n    # decompose matrix by its singular values\n    U, S, V = torch.svd(E_mat)\n    Vt = V.transpose(-2, -1)\n\n    mask = torch.ones_like(E_mat)\n    mask[..., -1:] *= -1.   # fill last column with negative values\n\n    maskt = mask.transpose(-2, -1)\n\n    # avoid singularities\n    U = torch.where((torch.det(U) < 0.)[..., None, None], U * mask, U)\n    Vt = torch.where((torch.det(Vt) < 0.)[..., None, None], Vt * maskt, Vt)\n\n    W = numeric.cross_product_matrix(torch.tensor([[0., 0., 1.]]).type_as(E_mat))\n    W[..., 2, 2] += 1.\n\n    # reconstruct rotations and retrieve translation vector\n    U_W_Vt = U @ W @ Vt\n    U_Wt_Vt = U @ W.transpose(-2, -1) @ Vt\n\n    # return values\n    R1 = U_W_Vt\n    R2 = U_Wt_Vt\n    T = U[..., -1:]\n    return (R1, R2, T)\n\n\ndef essential_from_Rt(R1: torch.Tensor, t1: torch.Tensor, R2: torch.Tensor, t2: torch.Tensor) -> torch.Tensor:\n    r""""""Get the Essential matrix from Camera motion (Rs and ts).\n\n    Reference: Hartley/Zisserman 9.6 pag 257 (formula 9.12)\n\n    Args:\n        R1 (torch.Tensor): The first camera rotation matrix with shape :math:`(*, 3, 3)`.\n        t1 (torch.Tensor): The first camera translation vector with shape :math:`(*, 3, 1)`.\n        R2 (torch.Tensor): The second camera rotation matrix with shape :math:`(*, 3, 3)`.\n        t2 (torch.Tensor): The second camera translation vector with shape :math:`(*, 3, 1)`.\n\n    Returns:\n        torch.Tensor: The Essential matrix with the shape :math:`(*, 3, 3)`.\n\n    """"""\n    assert len(R1.shape) >= 2 and R1.shape[-2:] == (3, 3), R1.shape\n    assert len(t1.shape) >= 2 and t1.shape[-2:] == (3, 1), t1.shape\n    assert len(R2.shape) >= 2 and R2.shape[-2:] == (3, 3), R2.shape\n    assert len(t2.shape) >= 2 and t2.shape[-2:] == (3, 1), t2.shape\n\n    # first compute the camera relative motion\n    R, t = relative_camera_motion(R1, t1, R2, t2)\n\n    # get the cross product from relative translation vector\n    Tx = numeric.cross_product_matrix(t[..., 0])\n\n    return (Tx @ R)\n\n\ndef motion_from_essential(E_mat: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    r""""""Get Motion (R\'s and t\'s ) from Essential matrix.\n\n    Computes and return four possible poses exist for the decomposition of the Essential\n    matrix. The posible solutions are :math:`[R1,t], [R1,\xe2\x88\x92t], [R2,t], [R2,\xe2\x88\x92t]`.\n\n    Args:\n        E_mat (torch.Tensor): The essential matrix in the form of :math:`(*, 3, 3)`.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: The rotation and translation containing the four\n        possible combination for the retrieved motion. The tuple is as following :math:`[(*, 4, 3, 3), (*, 4, 3, 1)]`.\n\n    """"""\n    assert len(E_mat.shape) >= 2 and E_mat.shape[-2:], E_mat.shape\n\n    # decompose the essential matrix by its possible poses\n    R1, R2, t = decompose_essential_matrix(E_mat)\n\n    # compbine and returns the four possible solutions\n    Rs = torch.stack([R1, R1, R2, R2], dim=-3)\n    Ts = torch.stack([t, -t, t, -t], dim=-3)\n\n    return (Rs, Ts)\n\n\ndef motion_from_essential_choose_solution(\n    E_mat: torch.Tensor,\n    K1: torch.Tensor,\n    K2: torch.Tensor,\n    x1: torch.Tensor,\n    x2: torch.Tensor\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    r""""""Recovers the relative camera rotation and the translation from an estimated essential matrix.\n\n    The method check the corresponding points in two images and also returns the triangulated\n    3d points. Internally uses :py:meth:`~kornia.geometry.epipolar.decompose_essential_matrix` and then chooses\n    the best solution based on the combination that gives more 3d points in front of the camera plane from\n    :py:meth:`~kornia.geometry.epipolar.triangulate_points`.\n\n    Args:\n        E_mat (torch.Tensor): The essential matrix in the form of :math:`(*, 3, 3)`.\n        K1 (torch.Tensor): The camera matrix from first camera with shape :math:`(*, 3, 3)`.\n        K2 (torch.Tensor): The camera matrix from second camera with shape :math:`(*, 3, 3)`.\n        x1 (torch.Tensor): The set of points seen from the first camera frame in the camera plane\n          coordinates with shape :math:`(*, N, 2)`.\n        x2 (torch.Tensor): The set of points seen from the first camera frame in the camera plane\n          coordinates with shape :math:`(*, N, 2)`.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The rotation and translation plus the\n        3d triangulated points. The tuple is as following :math:`[(*, 3, 3), (*, 3, 1), (*, N, 3)]`.\n\n    """"""\n    assert len(E_mat.shape) >= 2 and E_mat.shape[-2:], E_mat.shape\n    assert len(K1.shape) >= 2 and K1.shape[-2:] == (3, 3), K1.shape\n    assert len(K2.shape) >= 2 and K2.shape[-2:] == (3, 3), K2.shape\n    assert len(x1.shape) >= 2 and x1.shape[-1] == 2, x1.shape\n    assert len(x2.shape) >= 2 and x2.shape[-1] == 2, x2.shape\n    assert len(E_mat.shape[:-2]) == len(K1.shape[:-2]) == len(K2.shape[:-2])\n\n    # compute four possible pose solutions\n    Rs, ts = motion_from_essential(E_mat)\n\n    # set reference view pose and compute projection matrix\n    R1 = numeric.eye_like(3, E_mat)  # Bx3x3\n    t1 = numeric.vec_like(3, E_mat)  # Bx3x1\n\n    # compute the projection matrices for first camera\n    R1 = R1[:, None].expand(-1, 4, -1, -1)\n    t1 = t1[:, None].expand(-1, 4, -1, -1)\n    K1 = K1[:, None].expand(-1, 4, -1, -1)\n    P1 = projection.projection_from_KRt(K1, R1, t1)  # 1x4x4x4\n\n    # compute the projection matrices for second camera\n    R2 = Rs\n    t2 = ts\n    K2 = K2[:, None].expand(-1, 4, -1, -1)\n    P2 = projection.projection_from_KRt(K2, R2, t2)  # Bx4x4x4\n\n    # triangulate the points\n    x1 = x1[:, None].expand(-1, 4, -1, -1)\n    x2 = x2[:, None].expand(-1, 4, -1, -1)\n    X = triangulation.triangulate_points(P1, P2, x1, x2)  # Bx4xNx3\n\n    # project points and compute their depth values\n    d1 = projection.depth(R1, t1, X)\n    d2 = projection.depth(R2, t2, X)\n\n    # verify the point values that have a postive depth value\n    mask = ((d1 > 0.) & (d2 > 0.))\n    mask_indices = torch.max(mask.sum(-1), dim=-1, keepdim=True)[1]\n\n    # get pose and points 3d and return\n    R_out = Rs[:, mask_indices][:, 0, 0]\n    t_out = ts[:, mask_indices][:, 0, 0]\n    points3d_out = X[:, mask_indices][:, 0, 0]\n\n    return R_out, t_out, points3d_out\n\n\ndef relative_camera_motion(\n    R1: torch.Tensor, t1: torch.Tensor, R2: torch.Tensor, t2: torch.Tensor\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    r""""""Computes the relative camera motion between two cameras.\n\n    Given the motion parameters of two cameras, computes the motion parameters of the second\n    one assuming the first one to be at the origin. If :math:`T1` and :math:`T2` are the camera motions,\n    the computed relative motion is :math:`T = T_{2}T^{\xe2\x88\x921}_{1}`.\n\n    Args:\n        R1 (torch.Tensor): The first camera rotation matrix with shape :math:`(*, 3, 3)`.\n        t1 (torch.Tensor): The first camera translation vector with shape :math:`(*, 3, 1)`.\n        R2 (torch.Tensor): The second camera rotation matrix with shape :math:`(*, 3, 3)`.\n        t2 (torch.Tensor): The second camera translation vector with shape :math:`(*, 3, 1)`.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: A tuple with the relatice rotation matrix and\n        translation vector with the shape of :math:`[(*, 3, 3), (*, 3, 1)]`.\n\n    """"""\n    assert len(R1.shape) >= 2 and R1.shape[-2:] == (3, 3), R1.shape\n    assert len(t1.shape) >= 2 and t1.shape[-2:] == (3, 1), t1.shape\n    assert len(R2.shape) >= 2 and R2.shape[-2:] == (3, 3), R2.shape\n    assert len(t2.shape) >= 2 and t2.shape[-2:] == (3, 1), t2.shape\n\n    # compute first the relative rotation\n    R = R2 @ R1.transpose(-2, -1)\n\n    # compute the relative translation vector\n    t = t2 - R @ t1\n\n    return (R, t)\n'"
kornia/geometry/epipolar/fundamental.py,46,"b'""""""Module containing the functionalities for computing the Fundamental Matrix.""""""\n\nfrom typing import Tuple\n\nimport torch\nimport kornia\n\n\ndef normalize_points(points: torch.Tensor, eps: float = 1e-8) -> Tuple[torch.Tensor, torch.Tensor]:\n    r""""""Normalizes points (isotropic).\n\n    Computes the transformation matrix such that the two principal moments of the set of points\n    are equal to unity, forming an approximately symmetric circular cloud of points of radius 1\n    about the origin. Reference: Hartley/Zisserman 4.4.4 pag.107\n\n    This operation is an essential step before applying the DLT algorithm in order to consider\n    the result as optimal.\n\n    Args:\n       points (torch.Tensor): Tensor containing the points to be normalized with shape :math:`(B, N, 2)`.\n       eps (float): epsilon value to avoid numerical unstabilities. Default: 1e-8.\n\n    Returns:\n       Tuple[torch.Tensor, torch.Tensor]: tuple containing the normalized points in the\n       shape :math:`(B, N, 2)` and the transformation matrix in the shape :math:`(B, 3, 3)`.\n\n    """"""\n    assert len(points.shape) == 3, points.shape\n    assert points.shape[-1] == 2, points.shape\n\n    x_mean = torch.mean(points, dim=1, keepdim=True)  # Bx1x2\n\n    scale = (points - x_mean).norm(dim=-1).mean(dim=-1)   # B\n    scale = torch.sqrt(torch.tensor(2.)) / (scale + eps)  # B\n\n    ones, zeros = torch.ones_like(scale), torch.zeros_like(scale)\n\n    transform = torch.stack([\n        scale, zeros, -scale * x_mean[..., 0, 0],\n        zeros, scale, -scale * x_mean[..., 0, 1],\n        zeros, zeros, ones], dim=-1)  # Bx9\n\n    transform = transform.view(-1, 3, 3)  # Bx3x3\n    points_norm = kornia.transform_points(transform, points)  # BxNx2\n\n    return (points_norm, transform)\n\n\ndef normalize_transformation(M: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n    r""""""Normalizes a given transformation matrix.\n\n    The function trakes the transformation matrix and normalize so that the value in\n    the last row and column is one.\n\n    Args:\n        M (torch.Tensor): The transformation to be normalized of any shape with a minimum size of 2x2.\n        eps (float): small value to avoid unstabilities during the backpropagation.\n\n    Returns:\n        torch.Tensor: the normalized transformation matrix with same shape as the input.\n\n    """"""\n    assert len(M.shape) >= 2, M.shape\n    norm_val: torch.Tensor = M[..., -1:, -1:]\n    return torch.where(norm_val.abs() > eps, M / (norm_val + eps), M)\n\n\ndef find_fundamental(points1: torch.Tensor, points2: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:\n    r""""""Computes the fundamental matrix using the DLT formulation.\n\n    The linear system is solved by using the Weighted Least Squares Solution for the 8 Points algorithm.\n\n    Args:\n        points1 (torch.Tensor): A set of points in the first image with a tensor shape :math:`(B, N, 2)`.\n        points2 (torch.Tensor): A set of points in the second image with a tensor shape :math:`(B, N, 2)`.\n        weights (torch.Tensor): Tensor containing the weights per point correspondence with a shape of :math:`(B, N)`.\n\n    Returns:\n        torch.Tensor: the computed fundamental matrix with shape :math:`(B, 3, 3)`.\n\n    """"""\n    assert points1.shape == points2.shape, (points1.shape, points2.shape)\n    assert len(weights.shape) == 2 and weights.shape[1] == points1.shape[1], weights.shape\n\n    points1_norm, transform1 = normalize_points(points1)\n    points2_norm, transform2 = normalize_points(points2)\n\n    x1, y1 = torch.chunk(points1_norm, dim=-1, chunks=2)  # Bx1xN\n    x2, y2 = torch.chunk(points2_norm, dim=-1, chunks=2)  # Bx1xN\n\n    ones = torch.ones_like(x1)\n\n    # build equations system and solve DLT\n    # https://www.cc.gatech.edu/~afb/classes/CS4495-Fall2013/slides/CS4495-09-TwoViews-2.pdf\n    # [x * x\', x * y\', x, y * x\', y * y\', y, x\', y\', 1]\n\n    X = torch.cat([\n        x2 * x1, x2 * y1, x2, y2 * x1, y2 * y1, y2, x1, y1, ones\n    ], dim=-1)  # BxNx9\n\n    # apply the weights to the linear system\n    w_diag = torch.diag_embed(weights)\n    X = X.transpose(-2, -1) @ w_diag @ X\n\n    # compute eigevectors and retrieve the one with the smallest eigenvalue\n    _, _, V = torch.svd(X)\n    F_mat = V[..., -1].view(-1, 3, 3)\n\n    # reconstruct and force the matrix to have rank2\n    U, S, V = torch.svd(F_mat)\n    rank_mask = torch.tensor([1., 1., 0]).to(F_mat.device)\n\n    F_projected = U @ (torch.diag_embed(S * rank_mask) @ V.transpose(-2, -1))\n    F_est = transform2.transpose(-2, -1) @ (F_projected @ transform1)\n\n    return normalize_transformation(F_est)\n\n\ndef compute_correspond_epilines(points: torch.Tensor, F_mat: torch.Tensor) -> torch.Tensor:\n    r""""""Computes the corresponding epipolar line for a given set of points.\n\n    Args:\n        points (torch.Tensor): tensor containing the set of points to project in\n          the shape of :math:`(B, N, 2)`.\n        F_mat (torch.Tensor): the fundamental to use for projection the points in\n          the shape of :math:`(B, 3, 3)`.\n\n    Returns:\n        torch.Tensor: a tensor with shape :math:`(B, N, 3)` containing a vector of the epipolar\n        lines corresponding to the points to the other image. Each line is described as\n        :math:`ax + by + c = 0` and encoding the vectors as :math:`(a, b, c)`.\n\n    """"""\n    assert len(points.shape) == 3 and points.shape[2] == 2, points.shape\n    assert len(F_mat.shape) == 3 and F_mat.shape[-2:] == (3, 3), F_mat.shape\n\n    points_h: torch.Tensor = kornia.convert_points_to_homogeneous(points)\n\n    # project points and retrieve lines components\n    a, b, c = torch.chunk(F_mat @ points_h.permute(0, 2, 1), dim=1, chunks=3)\n\n    # compute normal and compose equation line\n    nu: torch.Tensor = a * a + b * b\n    nu = torch.where(nu > 0., 1. / torch.sqrt(nu), torch.ones_like(nu))\n\n    line = torch.cat([a * nu, b * nu, c * nu], dim=1)  # Bx3xN\n    return line.permute(0, 2, 1)  # BxNx3\n\n\ndef fundamental_from_essential(E_mat: torch.Tensor, K1: torch.Tensor, K2: torch.Tensor) -> torch.Tensor:\n    r""""""Get the Fundamental matrix from Essential and camera matrices.\n\n    Uses the method from Hartley/Zisserman 9.6 pag 257 (formula 9.12).\n\n    Args:\n        E_mat (torch.Tensor): The essential matrix with shape of :math:`(*, 3, 3)`.\n        K1 (torch.Tensor): The camera matrix from first camera with shape :math:`(*, 3, 3)`.\n        K2 (torch.Tensor): The camera matrix from second camera with shape :math:`(*, 3, 3)`.\n\n    Returns:\n         torch.Tensor: The fundamental matrix with shape :math:`(*, 3, 3)`.\n\n    """"""\n    assert len(E_mat.shape) >= 2 and E_mat.shape[-2:] == (3, 3), E_mat.shape\n    assert len(K1.shape) >= 2 and K1.shape[-2:] == (3, 3), K1.shape\n    assert len(K2.shape) >= 2 and K2.shape[-2:] == (3, 3), K2.shape\n    assert len(E_mat.shape[:-2]) == len(K1.shape[:-2]) == len(K2.shape[:-2])\n\n    return K2.inverse().transpose(-2, -1) @ E_mat @ K1.inverse()\n\n\n# adapted from:\n# https://github.com/opencv/opencv_contrib/blob/master/modules/sfm/src/fundamental.cpp#L109\n# https://github.com/openMVG/openMVG/blob/160643be515007580086650f2ae7f1a42d32e9fb/src/openMVG/multiview/projection.cpp#L134\n\ndef fundamental_from_projections(P1: torch.Tensor, P2: torch.Tensor) -> torch.Tensor:\n    r""""""Get the Fundamental matrix from Projection matrices.\n\n    Args:\n        P1 (torch.Tensor): The projection matrix from first camera with shape :math:`(*, 3, 4)`.\n        P2 (torch.Tensor): The projection matrix from second camera with shape :math:`(*, 3, 4)`.\n\n    Returns:\n         torch.Tensor: The fundamental matrix with shape :math:`(*, 3, 3)`.\n\n    """"""\n    assert len(P1.shape) >= 2 and P1.shape[-2:] == (3, 4), P1.shape\n    assert len(P2.shape) >= 2 and P2.shape[-2:] == (3, 4), P2.shape\n    assert P1.shape[:-2] == P2.shape[:-2]  # this function does not support broadcasting\n\n    def vstack(x, y):\n        return torch.cat([x, y], dim=-2)\n\n    X1 = P1[..., 1:, :]\n    X2 = vstack(P1[..., 2:3, :], P1[..., 0:1, :])\n    X3 = P1[..., :2, :]\n\n    Y1 = P2[..., 1:, :]\n    Y2 = vstack(P2[..., 2:3, :], P2[..., 0:1, :])\n    Y3 = P2[..., :2, :]\n\n    X1Y1, X2Y1, X3Y1 = vstack(X1, Y1), vstack(X2, Y1), vstack(X3, Y1)\n    X1Y2, X2Y2, X3Y2 = vstack(X1, Y2), vstack(X2, Y2), vstack(X3, Y2)\n    X1Y3, X2Y3, X3Y3 = vstack(X1, Y3), vstack(X2, Y3), vstack(X3, Y3)\n\n    F_vec = torch.cat([\n        X1Y1.det(), X2Y1.det(), X3Y1.det(),\n        X1Y2.det(), X2Y2.det(), X3Y2.det(),\n        X1Y3.det(), X2Y3.det(), X3Y3.det(),\n    ], dim=-1)\n\n    return F_vec.view(*P1.shape[:-2], 3, 3)\n'"
kornia/geometry/epipolar/metrics.py,38,"b'""""""Module including useful metrics for Structure from Motion.""""""\n\nimport torch\nimport kornia\n\n\ndef sampson_epipolar_distance(pts1: torch.Tensor,\n                              pts2: torch.Tensor,\n                              Fm: torch.Tensor,\n                              squared: bool = True,\n                              eps: float = 1e-8) -> torch.Tensor:\n    r""""""Returns Sampson distance for correspondences given the fundamental matrix.\n\n    Args:\n        pts1 (torch.Tensor): correspondences from the left images with shape\n          (B, N, 2 or 3). If they are not homogenuous, converted automatically.\n        pts2 (torch.Tensor): correspondences from the right images with shape\n          (B, N, 2 or 3). If they are not homogenuous, converted automatically.\n        Fm (torch.Tensor): Fundamental matrices with shape :math:`(B, 3, 3)`. Called Fm to\n          avoid ambiguity with torch.nn.functional.\n        squared (bool): if True (default), the squared distance is returned.\n        eps (float): Small constant for safe sqrt. Default 1e-9.\n\n    Returns:\n        torch.Tensor: the computed Sampson distance with shape :math:`(B, N)`.\n\n    """"""\n    if not isinstance(Fm, torch.Tensor):\n        raise TypeError(""Fm type is not a torch.Tensor. Got {}"".format(\n            type(Fm)))\n\n    if (len(Fm.shape) != 3) or not Fm.shape[-2:] == (3, 3):\n        raise ValueError(\n            ""Fm must be a (*, 3, 3) tensor. Got {}"".format(\n                Fm.shape))\n\n    if pts1.size(-1) == 2:\n        pts1 = kornia.convert_points_to_homogeneous(pts1)\n\n    if pts2.size(-1) == 2:\n        pts2 = kornia.convert_points_to_homogeneous(pts2)\n\n    # From Hartley and Zisserman, Sampson error (11.9)\n    # sam =  (x\'^T F x) ** 2 / (  (((Fx)_1**2) + (Fx)_2**2)) +  (((F^Tx\')_1**2) + (F^Tx\')_2**2)) )\n\n    # line1_in_2: torch.Tensor = (F @ pts1.permute(0,2,1)).permute(0,2,1)\n    # line2_in_1: torch.Tensor = (F.permute(0,2,1) @ pts2.permute(0,2,1)).permute(0,2,1)\n    # Instead we can just transpose F once and switch the order of multiplication\n    F_t: torch.Tensor = Fm.permute(0, 2, 1)\n    line1_in_2: torch.Tensor = pts1 @ F_t\n    line2_in_1: torch.Tensor = pts2 @ Fm\n\n    # numerator = (x\'^T F x) ** 2\n    numerator: torch.Tensor = (pts2 * line1_in_2).sum(2).pow(2)\n\n    # denominator = (((Fx)_1**2) + (Fx)_2**2)) +  (((F^Tx\')_1**2) + (F^Tx\')_2**2))\n    denominator: torch.Tensor = line1_in_2[..., :2].norm(2, dim=2).pow(2) + line2_in_1[..., :2].norm(2, dim=2).pow(2)\n    out: torch.Tensor = numerator / denominator\n    if squared:\n        return out\n    return (out + eps).sqrt()\n\n\ndef symmetrical_epipolar_distance(pts1: torch.Tensor,\n                                  pts2: torch.Tensor,\n                                  Fm: torch.Tensor,\n                                  squared: bool = True,\n                                  eps: float = 1e-8) -> torch.Tensor:\n    r""""""Returns symmetrical epipolar distance for correspondences given the fundamental matrix.\n\n    Args:\n       pts1 (torch.Tensor): correspondences from the left images with shape\n         (B, N, 2 or 3). If they are not homogenuous, converted automatically.\n       pts2 (torch.Tensor): correspondences from the right images with shape\n         (B, N, 2 or 3). If they are not homogenuous, converted automatically.\n       Fm (torch.Tensor): Fundamental matrices with shape :math:`(B, 3, 3)`. Called Fm to\n         avoid ambiguity with torch.nn.functional.\n       squared (bool): if True (default), the squared distance is returned.\n       eps (float): Small constant for safe sqrt. Default 1e-9.\n\n    Returns:\n        torch.Tensor: the computed Symmetrical distance with shape :math:`(B, N)`.\n\n    """"""\n    if not isinstance(Fm, torch.Tensor):\n        raise TypeError(""Fm type is not a torch.Tensor. Got {}"".format(\n            type(Fm)))\n\n    if (len(Fm.shape) != 3) or not Fm.shape[-2:] == (3, 3):\n        raise ValueError(\n            ""Fm must be a (*, 3, 3) tensor. Got {}"".format(\n                Fm.shape))\n\n    if pts1.size(-1) == 2:\n        pts1 = kornia.convert_points_to_homogeneous(pts1)\n\n    if pts2.size(-1) == 2:\n        pts2 = kornia.convert_points_to_homogeneous(pts2)\n\n    # From Hartley and Zisserman, symmetric epipolar distance (11.10)\n    # sed = (x\'^T F x) ** 2 /  (((Fx)_1**2) + (Fx)_2**2)) +  1/ (((F^Tx\')_1**2) + (F^Tx\')_2**2))\n\n    # line1_in_2: torch.Tensor = (F @ pts1.permute(0,2,1)).permute(0,2,1)\n    # line2_in_1: torch.Tensor = (F.permute(0,2,1) @ pts2.permute(0,2,1)).permute(0,2,1)\n\n    # Instead we can just transpose F once and switch the order of multiplication\n    F_t: torch.Tensor = Fm.permute(0, 2, 1)\n    line1_in_2: torch.Tensor = pts1 @ F_t\n    line2_in_1: torch.Tensor = pts2 @ Fm\n\n    # numerator = (x\'^T F x) ** 2\n    numerator: torch.Tensor = (pts2 * line1_in_2).sum(2).pow(2)\n\n    # denominator_inv =  1/ (((Fx)_1**2) + (Fx)_2**2)) +  1/ (((F^Tx\')_1**2) + (F^Tx\')_2**2))\n    denominator_inv: torch.Tensor = (1. / (line1_in_2[..., :2].norm(2, dim=2).pow(2)) +\n                                     1. / (line2_in_1[..., :2].norm(2, dim=2).pow(2)))\n    out: torch.Tensor = numerator * denominator_inv\n    if squared:\n        return out\n    return (out + eps).sqrt()\n'"
kornia/geometry/epipolar/numeric.py,12,"b'""""""Module containing numerical functionalities for SfM""""""\n\nimport torch\n\n\n# TODO: this should go to `kornia.geometry.linalg`\n\ndef cross_product_matrix(x: torch.Tensor) -> torch.Tensor:\n    r""""""Returns the cross_product_matrix symmetric matrix of a vector.\n\n    Args:\n        x (torch.Tensor): The input vector to construct the matrix in the shape :math:`(B, 3)`.\n\n    Returns:\n        torch.Tensor: The constructed cross_product_matrix symmetric matrix with shape :math:`(B, 3, 3)`.\n\n    """"""\n    assert len(x.shape) == 2 and x.shape[1] == 3, x.shape\n    # get vector compononens\n    x0 = x[..., 0]\n    x1 = x[..., 1]\n    x2 = x[..., 2]\n\n    # construct the matrix, reshape to 3x3 and return\n    zeros = torch.zeros_like(x0)\n    cross_product_matrix_flat = torch.stack([\n        zeros, -x2, x1,\n        x2, zeros, -x0,\n        -x1, x0, zeros], dim=-1)\n    return cross_product_matrix_flat.view(-1, 3, 3)\n\n\ndef eye_like(n: int, input: torch.Tensor) -> torch.Tensor:\n    r""""""Returns a 2-D tensor with ones on the diagonal and zeros elsewhere with same size as the input.\n\n    Args:\n        n (int): the number of rows :math:`(N)`.\n        input (torch.Tensor): image tensor that will determine the batch size of the output matrix.\n          The expected shape is :math:`(B, *)`.\n\n    Returns:\n        torch.Tensor: The identity matrix with same size as input :math:`(*, N, N)`.\n\n    """"""\n    assert n > 0, (type(n), n)\n    assert len(input.shape) >= 1, input.shape\n\n    identity = torch.eye(n, device=input.device, dtype=input.dtype)\n    return identity[None].repeat(input.shape[0], 1, 1)\n\n\ndef vec_like(n, tensor):\n    r""""""Returns a 2-D tensor with a vector containing zeros with same size as the input.\n\n    Args:\n        n (int): the number of rows :math:`(N)`.\n        input (torch.Tensor): image tensor that will determine the batch size of the output matrix.\n          The expected shape is :math:`(B, *)`.\n\n    Returns:\n        torch.Tensor: The vector with same size as input :math:`(*, N, 1)`.\n\n    """"""\n    assert n > 0, (type(n), n)\n    assert len(tensor.shape) >= 1, tensor.shape\n\n    vec = torch.zeros(n, 1, device=tensor.device, dtype=tensor.dtype)\n    return vec[None].repeat(tensor.shape[0], 1, 1)\n'"
kornia/geometry/epipolar/projection.py,35,"b'""""""Module for image projections.""""""\nfrom typing import Union\n\nimport torch\n\nfrom kornia.geometry.epipolar import numeric\n\n\ndef intrinsics_like(focal: float, input: torch.Tensor) -> torch.Tensor:\n    r""""""Returns a 3x3 instrinsics matrix, with same size as the input.\n\n    The center of projection will be based in the input image size.\n\n    Args:\n        focal (float): the focal length for tha camera matrix.\n        input (torch.Tensor): image tensor that will determine the batch size and image height\n          and width. It is assumed to be a tensor in the shape of :math:`(B, C, H, W)`.\n\n    Returns:\n        torch.Tensor: The camera matrix with the shape of :math:`(B, 3, 3)`.\n\n    """"""\n    assert len(input.shape) == 4, input.shape\n    assert focal > 0, focal\n\n    B, _, H, W = input.shape\n\n    intrinsics = numeric.eye_like(3, input)\n    intrinsics[..., 0, 0] *= focal\n    intrinsics[..., 1, 1] *= focal\n    intrinsics[..., 0, 2] += 1. * W / 2\n    intrinsics[..., 1, 2] += 1. * H / 2\n    return intrinsics\n\n\ndef random_intrinsics(low: Union[float, torch.Tensor],\n                      high: Union[float, torch.Tensor]) -> torch.Tensor:\n    r""""""Generates a random camera matrix based on a given uniform distribution.\n\n    Args:\n        low (Union[float, torch.Tensor]): lower range (inclusive).\n        high (Union[float, torch.Tensor]): upper range (exclusive).\n\n    Returns:\n        torch.Tensor: The random camera matrix with the shape of :math:`(1, 3, 3)`.\n\n    """"""\n    sampler = torch.distributions.Uniform(low, high)\n    fx, fy, cx, cy = [sampler.sample((1,)) for _ in range(4)]\n    zeros, ones = torch.zeros_like(fx), torch.ones_like(fx)\n    camera_matrix: torch.Tensor = torch.cat([\n        fx, zeros, cx,\n        zeros, fy, cy,\n        zeros, zeros, ones,\n    ])\n    return camera_matrix.view(1, 3, 3)\n\n\ndef scale_intrinsics(\n        camera_matrix: torch.Tensor, scale_factor: Union[float, torch.Tensor]) -> torch.Tensor:\n    r""""""Scale a camera matrix containing the intrinsics.\n\n    Applies the scaling factor to the focal length and center of projection.\n\n    Args:\n        camera_matrix (torch.Tensor): the camera calibration matrix containing the intrinsic\n          parameters. The expected shape for the tensor is :math:`(B, 3, 3)`.\n        scale_factor (Union[float, torch.Tensor]): the scaling factor to be applied.\n\n    Returns:\n        torch.Tensor: The scaled camera matrix with shame shape as input :math:`(B, 3, 3)`.\n\n    """"""\n    K_scale = camera_matrix.clone()\n    K_scale[..., 0, 0] *= scale_factor\n    K_scale[..., 1, 1] *= scale_factor\n    K_scale[..., 0, 2] *= scale_factor\n    K_scale[..., 1, 2] *= scale_factor\n    return K_scale\n\n\ndef projection_from_KRt(K: torch.Tensor, R: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n    r""""""Get the projection matrix P from K, R and t.\n\n    This function estimate the projection matrix by solving the following equation: :math:`P = K * [R|t]`.\n\n    Args:\n       K (torch.Tensor): the camera matrix with the instrinsics with shape :math:`(B, 3, 3)`.\n       R (torch.Tensor): The rotation matrix with shape :math:`(B, 3, 3)`.\n       t (torch.Tensor): The translation vector with shape :math:`(B, 3, 1)`.\n\n    Returns:\n       torch.Tensor: The projection matrix P with shape :math:`(B, 4, 4)`.\n\n    """"""\n    assert K.shape[-2:] == (3, 3), K.shape\n    assert R.shape[-2:] == (3, 3), R.shape\n    assert t.shape[-2:] == (3, 1), t.shape\n    assert len(K.shape) == len(R.shape) == len(t.shape)\n\n    Rt: torch.Tensor = torch.cat([R, t], dim=-1)  # 3x4\n    Rt_h = torch.nn.functional.pad(Rt, [0, 0, 0, 1], ""constant"", 0.)  # 4x4\n    Rt_h[..., -1, -1] += 1.\n\n    K_h: torch.Tensor = torch.nn.functional.pad(K, [0, 1, 0, 1], ""constant"", 0.)  # 4x4\n    K_h[..., -1, -1] += 1.\n\n    return K @ Rt\n\n\ndef depth(R: torch.Tensor, t: torch.Tensor, X: torch.Tensor) -> torch.Tensor:\n    r""""""Returns the depth of a point transformed by a rigid transform.\n\n    Args:\n       R (torch.Tensor): The rotation matrix with shape :math:`(*, 3, 3)`.\n       t (torch.Tensor): The translation vector with shape :math:`(*, 3, 1)`.\n       X (torch.Tensor): The 3d points with shape :math:`(*, 3)`.\n\n    Returns:\n       torch.Tensor: The depth value per point with shape :math:`(*, 1)`.\n\n    """"""\n    X_tmp = R @ X.transpose(-2, -1)\n    X_out = X_tmp[..., 2, :] + t[..., 2, :]\n    return X_out\n\n\n# adapted from:\n# https://github.com/opencv/opencv_contrib/blob/master/modules/sfm/src/fundamental.cpp#L61\n# https://github.com/mapillary/OpenSfM/blob/master/opensfm/multiview.py#L14\n\n\ndef _nullspace(A):\n    \'\'\'Compute the null space of A.\n    Return the smallest singular value and the corresponding vector.\n    \'\'\'\n    u, s, vh = torch.svd(A)\n    return s[..., -1], vh[..., -1]\n\n\ndef projections_from_fundamental(F_mat: torch.Tensor) -> torch.Tensor:\n    r""""""Get the projection matrices from the Fundamenal Matrix.\n\n    Args:\n       F_mat (torch.Tensor): the fundamenal matrix with the shape :math:`(*, 3, 3)`.\n\n    Returns:\n        torch.Tensor: The projection matrices with shape :math:`(*, 4, 4, 2)`.\n\n    """"""\n    assert len(F_mat.shape) >= 2, F_mat.shape\n    assert F_mat.shape[-2:] == (3, 3), F_mat.shape\n\n    R1 = numeric.eye_like(3, F_mat)  # Bx3x3\n    t1 = numeric.vec_like(3, F_mat)  # Bx3\n\n    Ft_mat = F_mat.transpose(-2, -1)\n\n    _, e2 = _nullspace(Ft_mat)\n\n    R2 = numeric.cross_product_matrix(e2) @ F_mat  # Bx3x3\n    t2 = e2[..., :, None]  # Bx3x1\n\n    P1 = torch.cat([R1, t1], dim=-1)  # Bx3x4\n    P2 = torch.cat([R2, t2], dim=-1)  # Bx3x4\n\n    return torch.stack([P1, P2], dim=-1)\n'"
kornia/geometry/epipolar/scene.py,11,"b'""""""Module to generate synthetic 3d scenes.""""""\nfrom typing import Dict\n\nimport torch\nimport kornia\n\nfrom kornia.geometry import epipolar\n\n\ndef generate_scene(num_views: int, num_points: int) -> Dict[str, torch.Tensor]:\n    # Generate the 3d points\n    points3d = torch.rand(1, num_points, 3)  # NxMx3\n\n    # Create random camera matrix\n    K = epipolar.random_intrinsics(0., 100.)  # 1x3x3\n\n    # Create random rotation per view\n    ang = torch.rand(num_views, 1) * kornia.pi * 2.\n\n    rvec = torch.rand(num_views, 3)\n    rvec = ang * rvec / torch.norm(rvec, dim=1, keepdim=True)  # Nx3\n    rot_mat = kornia.angle_axis_to_rotation_matrix(rvec)  # Nx3x3\n    # matches with cv2.Rodrigues -> yay !\n\n    # Create random translation per view\n    tx = torch.empty(num_views).uniform_(-0.5, 0.5)\n    ty = torch.empty(num_views).uniform_(-0.5, 0.5)\n    tz = torch.empty(num_views).uniform_(-1.0, 2.0)\n    tvec = torch.stack([tx, ty, tz], dim=1)[..., None]\n\n    # Make sure the shape is in front of the camera\n    points3d_trans = (rot_mat @ points3d.transpose(-2, -1)) + tvec\n    min_dist = torch.min(points3d_trans[:, 2], dim=1)[0]\n    tvec[:, 2, 0] = torch.where(min_dist < 0, tz - min_dist + 1., tz)\n\n    # compute projection matrices\n    P = epipolar.projection_from_KRt(K, rot_mat, tvec)\n\n    # project points3d and backproject to image plane\n    points2d = kornia.transform_points(P, points3d.expand(num_views, -1, -1))\n\n    return dict(K=K, R=rot_mat, t=tvec, P=P, points3d=points3d, points2d=points2d)\n'"
kornia/geometry/epipolar/triangulation.py,10,"b'""""""Module with the functionalites for triangulation.""""""\n\nimport torch\nimport kornia\n\n\n# https://github.com/opencv/opencv_contrib/blob/master/modules/sfm/src/triangulation.cpp#L68\n\ndef triangulate_points(P1: torch.Tensor, P2: torch.Tensor, points1: torch.Tensor,\n                       points2: torch.Tensor) -> torch.Tensor:\n    r""""""Reconstructs a bunch of points by triangulation.\n\n    Triangulates the 3d position of 2d correspondences between several images.\n    Reference: Internally it uses DLT method from Hartley/Zisserman 12.2 pag.312\n\n    The input points are assumend to be in homogeneous coordinate system and being inliers\n    correspondences. The method does not perform any robust estimation.\n\n    Args:\n        P1 (torch.Tensor): The projection matrix for the first camera with shape :math:`(*, 3, 4)`.\n        P2 (torch.Tensor): The projection matrix for the second camera with shape :math:`(*, 3, 4)`.\n        points1 (torch.Tensor): The set of points seen from the first camera frame in the camera plane\n          coordinates with shape :math:`(*, N, 2)`.\n        points2 (torch.Tensor): The set of points seen from the second camera frame in the camera plane\n          coordinates with shape :math:`(*, N, 2)`.\n\n    Returns:\n        torch.Tensor: The reconstructed 3d points in the world frame with shape :math:`(*, N, 3)`.\n\n    """"""\n    assert len(P1.shape) >= 2 and P1.shape[-2:] == (3, 4), P1.shape\n    assert len(P2.shape) >= 2 and P2.shape[-2:] == (3, 4), P2.shape\n    assert len(P1.shape[:-2]) == len(P2.shape[:-2]), (P1.shape, P2.shape)\n    assert len(points1.shape) >= 2 and points1.shape[-1] == 2, points1.shape\n    assert len(points2.shape) >= 2 and points2.shape[-1] == 2, points2.shape\n    assert len(points1.shape[:-2]) == len(points2.shape[:-2]), (points1.shape, points2.shape)\n    assert len(P1.shape[:-2]) == len(points1.shape[:-2]), (P1.shape, points1.shape)\n\n    # allocate and construct the equations matrix with shape (*, 4, 4)\n    points_shape = max(points1.shape, points2.shape)  # this allows broadcasting\n    X = torch.zeros(points_shape[:-1] + (4, 4)).type_as(points1)\n\n    for i in range(4):\n        X[..., 0, i] = points1[..., 0] * P1[..., 2:3, i] - P1[..., 0:1, i]\n        X[..., 1, i] = points1[..., 1] * P1[..., 2:3, i] - P1[..., 1:2, i]\n        X[..., 2, i] = points2[..., 0] * P2[..., 2:3, i] - P2[..., 0:1, i]\n        X[..., 3, i] = points2[..., 1] * P2[..., 2:3, i] - P2[..., 1:2, i]\n\n    # 1. Solve the system Ax=0 with smallest eigenvalue\n    # 2. Return homogeneous coordinates\n\n    U, S, V = torch.svd(X)\n\n    points3d_h = V[..., -1]\n    points3d: torch.Tensor = kornia.convert_points_from_homogeneous(points3d_h)\n    return points3d\n'"
kornia/geometry/transform/__init__.py,0,b'from kornia.geometry.transform.crop import *\nfrom kornia.geometry.transform.imgwarp import *\nfrom kornia.geometry.transform.pyramid import *\nfrom kornia.geometry.transform.affwarp import *\nfrom kornia.geometry.transform.flips import *\n'
kornia/geometry/transform/affwarp.py,98,"b'from typing import Union, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom kornia.geometry.transform.imgwarp import (\n    warp_affine, get_rotation_matrix2d,\n)\n\n__all__ = [\n    ""affine"",\n    ""scale"",\n    ""rotate"",\n    ""translate"",\n    ""shear"",\n    ""resize"",\n    ""Scale"",\n    ""Rotate"",\n    ""Translate"",\n    ""Shear"",\n    ""Resize"",\n]\n\n# utilities to compute affine matrices\n\n\ndef _compute_tensor_center(tensor: torch.Tensor) -> torch.Tensor:\n    """"""Computes the center of tensor plane.""""""\n    height, width = tensor.shape[-2:]\n    center_x: float = float(width - 1) / 2\n    center_y: float = float(height - 1) / 2\n    center: torch.Tensor = torch.tensor(\n        [center_x, center_y],\n        device=tensor.device, dtype=tensor.dtype)\n    return center\n\n\ndef _compute_rotation_matrix(angle: torch.Tensor,\n                             center: torch.Tensor) -> torch.Tensor:\n    """"""Computes a pure affine rotation matrix.""""""\n    scale: torch.Tensor = torch.ones_like(angle)\n    matrix: torch.Tensor = get_rotation_matrix2d(center, angle, scale)\n    return matrix\n\n\ndef _compute_translation_matrix(translation: torch.Tensor) -> torch.Tensor:\n    """"""Computes affine matrix for translation.""""""\n    matrix: torch.Tensor = torch.eye(\n        3, device=translation.device, dtype=translation.dtype)\n    matrix = matrix.repeat(translation.shape[0], 1, 1)\n\n    dx, dy = torch.chunk(translation, chunks=2, dim=-1)\n    matrix[..., 0, 2:3] += dx\n    matrix[..., 1, 2:3] += dy\n    return matrix\n\n\ndef _compute_scaling_matrix(scale: torch.Tensor,\n                            center: torch.Tensor) -> torch.Tensor:\n    """"""Computes affine matrix for scaling.""""""\n    angle: torch.Tensor = torch.zeros_like(scale)\n    matrix: torch.Tensor = get_rotation_matrix2d(center, angle, scale)\n    return matrix\n\n\ndef _compute_shear_matrix(shear: torch.Tensor) -> torch.Tensor:\n    """"""Computes affine matrix for shearing.""""""\n    matrix: torch.Tensor = torch.eye(3, device=shear.device, dtype=shear.dtype)\n    matrix = matrix.repeat(shear.shape[0], 1, 1)\n\n    shx, shy = torch.chunk(shear, chunks=2, dim=-1)\n    matrix[..., 0, 1:2] += shx\n    matrix[..., 1, 0:1] += shy\n    return matrix\n\n\n# based on:\n# https://github.com/anibali/tvl/blob/master/src/tvl/transforms.py#L166\n\ndef affine(tensor: torch.Tensor, matrix: torch.Tensor, mode: str = \'bilinear\',\n           align_corners: bool = False) -> torch.Tensor:\n    r""""""Apply an affine transformation to the image.\n\n    Args:\n        tensor (torch.Tensor): The image tensor to be warped.\n        matrix (torch.Tensor): The 2x3 affine transformation matrix.\n        mode (str): \'bilinear\' | \'nearest\'\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n\n    Returns:\n        torch.Tensor: The warped image.\n    """"""\n    # warping needs data in the shape of BCHW\n    is_unbatched: bool = tensor.ndimension() == 3\n    if is_unbatched:\n        tensor = torch.unsqueeze(tensor, dim=0)\n\n    # we enforce broadcasting since by default grid_sample it does not\n    # give support for that\n    matrix = matrix.expand(tensor.shape[0], -1, -1)\n\n    # warp the input tensor\n    height: int = tensor.shape[-2]\n    width: int = tensor.shape[-1]\n    warped: torch.Tensor = warp_affine(tensor, matrix, (height, width), mode,\n                                       align_corners=align_corners)\n\n    # return in the original shape\n    if is_unbatched:\n        warped = torch.squeeze(warped, dim=0)\n\n    return warped\n\n\n# based on:\n# https://github.com/anibali/tvl/blob/master/src/tvl/transforms.py#L185\n\ndef rotate(tensor: torch.Tensor, angle: torch.Tensor,\n           center: Union[None, torch.Tensor] = None, mode: str = \'bilinear\',\n           align_corners: bool = False) -> torch.Tensor:\n    r""""""Rotate the image anti-clockwise about the centre.\n\n    See :class:`~kornia.Rotate` for details.\n    """"""\n    if not torch.is_tensor(tensor):\n        raise TypeError(""Input tensor type is not a torch.Tensor. Got {}""\n                        .format(type(tensor)))\n    if not torch.is_tensor(angle):\n        raise TypeError(""Input angle type is not a torch.Tensor. Got {}""\n                        .format(type(angle)))\n    if center is not None and not torch.is_tensor(angle):\n        raise TypeError(""Input center type is not a torch.Tensor. Got {}""\n                        .format(type(center)))\n    if len(tensor.shape) not in (3, 4,):\n        raise ValueError(""Invalid tensor shape, we expect CxHxW or BxCxHxW. ""\n                         ""Got: {}"".format(tensor.shape))\n\n    # compute the rotation center\n    if center is None:\n        center = _compute_tensor_center(tensor)\n\n    # compute the rotation matrix\n    # TODO: add broadcasting to get_rotation_matrix2d for center\n    angle = angle.expand(tensor.shape[0])\n    center = center.expand(tensor.shape[0], -1)\n    rotation_matrix: torch.Tensor = _compute_rotation_matrix(angle, center)\n\n    # warp using the affine transform\n    return affine(tensor, rotation_matrix[..., :2, :3], mode, align_corners)\n\n\ndef translate(tensor: torch.Tensor, translation: torch.Tensor,\n              align_corners: bool = False) -> torch.Tensor:\n    r""""""Translate the tensor in pixel units.\n\n    See :class:`~kornia.Translate` for details.\n    """"""\n    if not torch.is_tensor(tensor):\n        raise TypeError(""Input tensor type is not a torch.Tensor. Got {}""\n                        .format(type(tensor)))\n    if not torch.is_tensor(translation):\n        raise TypeError(""Input translation type is not a torch.Tensor. Got {}""\n                        .format(type(translation)))\n    if len(tensor.shape) not in (3, 4,):\n        raise ValueError(""Invalid tensor shape, we expect CxHxW or BxCxHxW. ""\n                         ""Got: {}"".format(tensor.shape))\n\n    # compute the translation matrix\n    translation_matrix: torch.Tensor = _compute_translation_matrix(translation)\n\n    # warp using the affine transform\n    return affine(tensor, translation_matrix[..., :2, :3], align_corners=align_corners)\n\n\ndef scale(tensor: torch.Tensor, scale_factor: torch.Tensor,\n          center: Union[None, torch.Tensor] = None,\n          align_corners: bool = False) -> torch.Tensor:\n    r""""""Scales the input image.\n\n    See :class:`~kornia.Scale` for details.\n    """"""\n    if not torch.is_tensor(tensor):\n        raise TypeError(""Input tensor type is not a torch.Tensor. Got {}""\n                        .format(type(tensor)))\n    if not torch.is_tensor(scale_factor):\n        raise TypeError(""Input scale_factor type is not a torch.Tensor. Got {}""\n                        .format(type(scale_factor)))\n\n    # compute the tensor center\n    if center is None:\n        center = _compute_tensor_center(tensor)\n\n    # compute the rotation matrix\n    # TODO: add broadcasting to get_rotation_matrix2d for center\n    center = center.expand(tensor.shape[0], -1)\n    scale_factor = scale_factor.expand(tensor.shape[0])\n    scaling_matrix: torch.Tensor = _compute_scaling_matrix(scale_factor, center)\n\n    # warp using the affine transform\n    return affine(tensor, scaling_matrix[..., :2, :3], align_corners=align_corners)\n\n\ndef shear(tensor: torch.Tensor, shear: torch.Tensor, align_corners: bool = False) -> torch.Tensor:\n    r""""""Shear the tensor.\n\n    See :class:`~kornia.Shear` for details.\n    """"""\n    if not torch.is_tensor(tensor):\n        raise TypeError(""Input tensor type is not a torch.Tensor. Got {}""\n                        .format(type(tensor)))\n    if not torch.is_tensor(shear):\n        raise TypeError(""Input shear type is not a torch.Tensor. Got {}""\n                        .format(type(shear)))\n    if len(tensor.shape) not in (3, 4,):\n        raise ValueError(""Invalid tensor shape, we expect CxHxW or BxCxHxW. ""\n                         ""Got: {}"".format(tensor.shape))\n\n    # compute the translation matrix\n    shear_matrix: torch.Tensor = _compute_shear_matrix(shear)\n\n    # warp using the affine transform\n    return affine(tensor, shear_matrix[..., :2, :3], align_corners=align_corners)\n\n\ndef resize(input: torch.Tensor, size: Union[int, Tuple[int, int]],\n           interpolation: str = \'bilinear\', align_corners: bool = False) -> torch.Tensor:\n    r""""""Resize the input torch.Tensor to the given size.\n\n    See :class:`~kornia.Resize` for details.\n    """"""\n    if not torch.is_tensor(input):\n        raise TypeError(""Input tensor type is not a torch.Tensor. Got {}""\n                        .format(type(input)))\n\n    new_size: Tuple[int, int]\n\n    if isinstance(size, int):\n        w, h = input.shape[-2:]\n        if (w <= h and w == size) or (h <= w and h == size):\n            return input\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n        else:\n            oh = size\n            ow = int(size * w / h)\n        new_size = (ow, oh)\n    else:\n        new_size = size\n    return torch.nn.functional.interpolate(input, size=new_size, mode=interpolation, align_corners=align_corners)\n\n\nclass Resize(nn.Module):\n    r""""""Resize the input torch.Tensor to the given size.\n\n    Args:\n        size (int, tuple(int, int)): Desired output size. If size is a sequence like (h, w),\n        output size will be matched to this. If size is an int, smaller edge of the image will\n        be matched to this number. i.e, if height > width, then image will be rescaled\n        to (size * height / width, size)\n        interpolation (str):  algorithm used for upsampling: \'nearest\' | \'linear\' | \'bilinear\' |\n        \'bicubic\' | \'trilinear\' | \'area\'. Default: \'bilinear\'.\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n    Returns:\n        torch.Tensor: The resized tensor.\n    """"""\n\n    def __init__(self, size: Union[int, Tuple[int, int]], interpolation: str = \'bilinear\',\n                 align_corners: bool = False) -> None:\n        super(Resize, self).__init__()\n        self.size: Union[int, Tuple[int, int]] = size\n        self.interpolation: str = interpolation\n        self.align_corners: bool = align_corners\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return resize(input, self.size, self.interpolation, align_corners=self.align_corners)\n\n\nclass Rotate(nn.Module):\n    r""""""Rotate the tensor anti-clockwise about the centre.\n\n    Args:\n        angle (torch.Tensor): The angle through which to rotate. The tensor\n          must have a shape of (B), where B is batch size.\n        center (torch.Tensor): The center through which to rotate. The tensor\n          must have a shape of (B, 2), where B is batch size and last\n          dimension contains cx and cy.\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n    Returns:\n        torch.Tensor: The rotated tensor.\n    """"""\n\n    def __init__(self, angle: torch.Tensor,\n                 center: Union[None, torch.Tensor] = None,\n                 align_corners: bool = False) -> None:\n        super(Rotate, self).__init__()\n        self.angle: torch.Tensor = angle\n        self.center: Union[None, torch.Tensor] = center\n        self.align_corners: bool = align_corners\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rotate(input, self.angle, self.center, align_corners=self.align_corners)\n\n\nclass Translate(nn.Module):\n    r""""""Translate the tensor in pixel units.\n\n    Args:\n        translation (torch.Tensor): tensor containing the amount of pixels to\n          translate in the x and y direction. The tensor must have a shape of\n          (B, 2), where B is batch size, last dimension contains dx dy.\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n    Returns:\n        torch.Tensor: The translated tensor.\n    """"""\n\n    def __init__(self, translation: torch.Tensor, align_corners: bool = False) -> None:\n        super(Translate, self).__init__()\n        self.translation: torch.Tensor = translation\n        self.align_corners: bool = align_corners\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return translate(input, self.translation, self.align_corners)\n\n\nclass Scale(nn.Module):\n    r""""""Scale the tensor by a factor.\n\n    Args:\n        scale_factor (torch.Tensor): The scale factor apply. The tensor\n          must have a shape of (B), where B is batch size.\n        center (torch.Tensor): The center through which to scale. The tensor\n          must have a shape of (B, 2), where B is batch size and last\n          dimension contains cx and cy.\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n    Returns:\n        torch.Tensor: The scaled tensor.\n    """"""\n\n    def __init__(self, scale_factor: torch.Tensor,\n                 center: Union[None, torch.Tensor] = None,\n                 align_corners: bool = False) -> None:\n        super(Scale, self).__init__()\n        self.scale_factor: torch.Tensor = scale_factor\n        self.center: Union[None, torch.Tensor] = center\n        self.align_corners: bool = align_corners\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return scale(input, self.scale_factor, self.center, self.align_corners)\n\n\nclass Shear(nn.Module):\n    r""""""Shear the tensor.\n\n    Args:\n        tensor (torch.Tensor): The image tensor to be skewed.\n        shear (torch.Tensor): tensor containing the angle to shear\n          in the x and y direction. The tensor must have a shape of\n          (B, 2), where B is batch size, last dimension contains shx shy.\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n    Returns:\n        torch.Tensor: The skewed tensor.\n    """"""\n\n    def __init__(self, shear: torch.Tensor,\n                 align_corners: bool = False) -> None:\n        super(Shear, self).__init__()\n        self.shear: torch.Tensor = shear\n        self.align_corners: bool = align_corners\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return shear(input, self.shear, self.align_corners)\n'"
kornia/geometry/transform/crop.py,39,"b'from typing import Tuple, Union\n\nimport torch\n\nfrom kornia.geometry.transform.imgwarp import (\n    warp_perspective, get_perspective_transform, warp_affine\n)\n\n__all__ = [\n    ""crop_and_resize"",\n    ""crop_by_boxes"",\n    ""center_crop"",\n]\n\n\ndef crop_and_resize(tensor: torch.Tensor, boxes: torch.Tensor, size: Tuple[int, int],\n                    interpolation: str = \'bilinear\', align_corners: bool = False) -> torch.Tensor:\n    r""""""Extracts crops from the input tensor and resizes them.\n\n    Args:\n        tensor (torch.Tensor): the reference tensor of shape BxCxHxW.\n        boxes (torch.Tensor): a tensor containing the coordinates of the\n          bounding boxes to be extracted. The tensor must have the shape\n          of Bx4x2, where each box is defined in the following (clockwise)\n          order: top-left, top-right, bottom-right and bottom-left. The\n          coordinates must be in the x, y order.\n        size (Tuple[int, int]): a tuple with the height and width that will be\n          used to resize the extracted patches.\n        align_corners (bool): mode for grid_generation. Default: False. See\n          https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for details\n    Returns:\n        torch.Tensor: tensor containing the patches with shape BxN1xN2\n    Example:\n        >>> input = torch.tensor([[\n                [1., 2., 3., 4.],\n                [5., 6., 7., 8.],\n                [9., 10., 11., 12.],\n                [13., 14., 15., 16.],\n            ]])\n        >>> boxes = torch.tensor([[\n                [1., 1.],\n                [2., 1.],\n                [2., 2.],\n                [1., 2.],\n            ]])  # 1x4x2\n        >>> kornia.crop_and_resize(input, boxes, (2, 2))\n        tensor([[[ 6.0000,  7.0000],\n                 [ 10.0000, 11.0000]]])\n    """"""\n    if not torch.is_tensor(tensor):\n        raise TypeError(""Input tensor type is not a torch.Tensor. Got {}""\n                        .format(type(tensor)))\n    if not torch.is_tensor(boxes):\n        raise TypeError(""Input boxes type is not a torch.Tensor. Got {}""\n                        .format(type(boxes)))\n    if not len(tensor.shape) in (3, 4,):\n        raise ValueError(""Input tensor must be in the shape of CxHxW or ""\n                         ""BxCxHxW. Got {}"".format(tensor.shape))\n    if not isinstance(size, (tuple, list,)) and len(size) == 2:\n        raise ValueError(""Input size must be a tuple/list of length 2. Got {}""\n                         .format(size))\n    # unpack input data\n    dst_h: torch.Tensor = torch.tensor(size[0])\n    dst_w: torch.Tensor = torch.tensor(size[1])\n\n    # [x, y] origin\n    # top-left, top-right, bottom-right, bottom-left\n    points_src: torch.Tensor = boxes\n\n    # [x, y] destination\n    # top-left, top-right, bottom-right, bottom-left\n    points_dst: torch.Tensor = torch.tensor([[\n        [0, 0],\n        [dst_w - 1, 0],\n        [dst_w - 1, dst_h - 1],\n        [0, dst_h - 1],\n    ]], device=tensor.device).expand(points_src.shape[0], -1, -1)\n\n    return crop_by_boxes(tensor, points_src, points_dst, interpolation, align_corners)\n\n\ndef center_crop(tensor: torch.Tensor, size: Tuple[int, int],\n                interpolation: str = \'bilinear\',\n                align_corners: bool = True) -> torch.Tensor:\n    r""""""Crops the given tensor at the center.\n\n    Args:\n        tensor (torch.Tensor): the input tensor with shape (C, H, W) or\n          (B, C, H, W).\n        size (Tuple[int, int]): a tuple with the expected height and width\n          of the output patch.\n        align_corners (bool): mode for grid_generation. Default: False. See\n          https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for details\n    Returns:\n        torch.Tensor: the output tensor with patches.\n\n    Examples:\n        >>> input = torch.tensor([[\n                [1., 2., 3., 4.],\n                [5., 6., 7., 8.],\n                [9., 10., 11., 12.],\n                [13., 14., 15., 16.],\n             ]])\n        >>> kornia.center_crop(input, (2, 4))\n        tensor([[[ 5.0000,  6.0000,  7.0000,  8.0000],\n                 [ 9.0000, 10.0000, 11.0000, 12.0000]]])\n    """"""\n    if not torch.is_tensor(tensor):\n        raise TypeError(""Input tensor type is not a torch.Tensor. Got {}""\n                        .format(type(tensor)))\n\n    if not len(tensor.shape) in (3, 4,):\n        raise ValueError(""Input tensor must be in the shape of CxHxW or ""\n                         ""BxCxHxW. Got {}"".format(tensor.shape))\n\n    if not isinstance(size, (tuple, list,)) and len(size) == 2:\n        raise ValueError(""Input size must be a tuple/list of length 2. Got {}""\n                         .format(size))\n\n    # unpack input sizes\n    dst_h, dst_w = size\n    src_h, src_w = tensor.shape[-2:]\n\n    # compute start/end offsets\n    dst_h_half = dst_h / 2\n    dst_w_half = dst_w / 2\n    src_h_half = src_h / 2\n    src_w_half = src_w / 2\n\n    start_x = src_w_half - dst_w_half\n    start_y = src_h_half - dst_h_half\n\n    end_x = start_x + dst_w - 1\n    end_y = start_y + dst_h - 1\n    # [y, x] origin\n    # top-left, top-right, bottom-right, bottom-left\n    points_src: torch.Tensor = torch.tensor([[\n        [start_x, start_y],\n        [end_x, start_y],\n        [end_x, end_y],\n        [start_x, end_y],\n    ]], device=tensor.device)\n\n    # [y, x] destination\n    # top-left, top-right, bottom-right, bottom-left\n    points_dst: torch.Tensor = torch.tensor([[\n        [0, 0],\n        [dst_w - 1, 0],\n        [dst_w - 1, dst_h - 1],\n        [0, dst_h - 1],\n    ]], device=tensor.device).expand(points_src.shape[0], -1, -1)\n    return crop_by_boxes(tensor,\n                         points_src.to(tensor.dtype),\n                         points_dst.to(tensor.dtype),\n                         interpolation,\n                         align_corners)\n\n\ndef crop_by_boxes(tensor, src_box, dst_box,\n                  interpolation: str = \'bilinear\',\n                  align_corners: bool = False) -> torch.Tensor:\n    """"""A wrapper performs crop transform with bounding boxes.\n\n    Note:\n        If the src_box is smaller than dst_box, the following error will be thrown.\n        RuntimeError: solve_cpu: For batch 0: U(2,2) is zero, singular U.\n    """"""\n    if tensor.ndimension() not in [3, 4]:\n        raise TypeError(""Only tensor with shape (C, H, W) and (B, C, H, W) supported. Got %s"" % str(tensor.shape))\n    # warping needs data in the shape of BCHW\n    is_unbatched: bool = tensor.ndimension() == 3\n    if is_unbatched:\n        tensor = torch.unsqueeze(tensor, dim=0)\n\n    # compute transformation between points and warp\n    # Note: Tensor.dtype must be float. ""solve_cpu"" not implemented for \'Long\'\n    dst_trans_src: torch.Tensor = get_perspective_transform(src_box.to(tensor.dtype), dst_box.to(tensor.dtype))\n    # simulate broadcasting\n    dst_trans_src = dst_trans_src.expand(tensor.shape[0], -1, -1).type_as(tensor)\n\n    bbox = _infer_bounding_box(dst_box)\n    patches: torch.Tensor = warp_affine(\n        tensor, dst_trans_src[:, :2, :], (int(bbox[0].int().data.item()), int(bbox[1].int().data.item())),\n        flags=interpolation, align_corners=align_corners)\n\n    # return in the original shape\n    if is_unbatched:\n        patches = torch.squeeze(patches, dim=0)\n\n    return patches\n\n\ndef _infer_bounding_box(boxes: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    r""""""Auto-infer the output sizes.\n\n    Args:\n        boxes (torch.Tensor): a tensor containing the coordinates of the\n          bounding boxes to be extracted. The tensor must have the shape\n          of Bx4x2, where each box is defined in the following (clockwise)\n          order: top-left, top-right, bottom-right, bottom-left. The\n          coordinates must be in the x, y order.\n\n    Returns:\n        (torch.Tensor, torch.Tensor): tensor containing the patches with shape BxN1xN2\n\n    Example:\n        >>> boxes = torch.tensor([[\n                [1., 1.],\n                [2., 1.],\n                [2., 2.],\n                [1., 2.],\n            ]])  # 1x4x2\n        >>> _infer_bounding_box(boxes)\n        (2, 2)\n    """"""\n    assert torch.allclose((boxes[:, 1, 0] - boxes[:, 0, 0] + 1), (boxes[:, 2, 0] - boxes[:, 3, 0] + 1)), \\\n        ""Boxes must have be square, while get widths %s and %s"" % \\\n        (str(boxes[:, 1, 0] - boxes[:, 0, 0] + 1), str(boxes[:, 2, 0] - boxes[:, 3, 0] + 1))\n    assert torch.allclose((boxes[:, 2, 1] - boxes[:, 0, 1] + 1), (boxes[:, 3, 1] - boxes[:, 1, 1] + 1)), \\\n        ""Boxes must have be square, while get heights %s and %s"" % \\\n        (str(boxes[:, 2, 1] - boxes[:, 0, 1] + 1), str(boxes[:, 3, 1] - boxes[:, 1, 1] + 1))\n    assert len((boxes[:, 1, 0] - boxes[:, 0, 0] + 1).unique()) == 1, \\\n        ""Boxes can only have one widths, got %s"" % str((boxes[:, 1, 0] - boxes[:, 0, 0] + 1).unique())\n    assert len((boxes[:, 2, 1] - boxes[:, 0, 1] + 1).unique()) == 1, \\\n        ""Boxes can only have one heights, got %s"" % str((boxes[:, 2, 1] - boxes[:, 0, 1] + 1).unique())\n\n    width: torch.Tensor = (boxes[:, 1, 0] - boxes[:, 0, 0] + 1)[0]\n    height: torch.Tensor = (boxes[:, 2, 1] - boxes[:, 0, 1] + 1)[0]\n    return (height, width)\n'"
kornia/geometry/transform/flips.py,24,"b'import torch\nimport torch.nn as nn\n\n\nclass Vflip(nn.Module):\n    r""""""Vertically flip a tensor image or a batch of tensor images. Input must\n    be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.\n\n    Args:\n        input (torch.Tensor): input tensor\n\n    Returns:\n        torch.Tensor: The vertically flipped image tensor\n\n    Examples:\n        >>> input = torch.tensor([[[\n            [0., 0., 0.],\n            [0., 0., 0.],\n            [0., 1., 1.]]]])\n        >>> kornia.vflip(input)\n        tensor([[[0, 1, 1],\n                 [0, 0, 0],\n                 [0, 0, 0]]])\n    """"""\n\n    def __init__(self) -> None:\n\n        super(Vflip, self).__init__()\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return vflip(input)\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n\nclass Hflip(nn.Module):\n    r""""""Horizontally flip a tensor image or a batch of tensor images. Input must\n    be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.\n\n    Args:\n        input (torch.Tensor): input tensor\n\n    Returns:\n        torch.Tensor: The horizontally flipped image tensor\n\n    Examples:\n        >>> input = torch.tensor([[[\n            [0., 0., 0.],\n            [0., 0., 0.],\n            [0., 1., 1.]]]])\n        >>> kornia.hflip(input)\n        tensor([[[0, 0, 0],\n                 [0, 0, 0],\n                 [1, 1, 0]]])\n    """"""\n\n    def __init__(self) -> None:\n\n        super(Hflip, self).__init__()\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return hflip(input)\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n\nclass Rot180(nn.Module):\n    r""""""Rotate a tensor image or a batch of tensor images\n        180 degrees. Input must be a tensor of shape (C, H, W)\n        or a batch of tensors :math:`(*, C, H, W)`.\n\n        Args:\n            input (torch.Tensor): input tensor\n\n        Examples:\n            >>> input = torch.tensor([[[\n                [0., 0., 0.],\n                [0., 0., 0.],\n                [0., 1., 1.]]]])\n            >>> kornia.rot180(input)\n            tensor([[[1, 1, 0],\n                    [0, 0, 0],\n                    [0, 0, 0]]])\n        """"""\n\n    def __init__(self) -> None:\n\n        super(Rot180, self).__init__()\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        return rot180(input)\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n\ndef rot180(input: torch.Tensor) -> torch.Tensor:\n    r""""""Rotate a tensor image or a batch of tensor images\n    180 degrees. Input must be a tensor of shape (C, H, W)\n    or a batch of tensors :math:`(*, C, H, W)`.\n\n    Args:\n        input (torch.Tensor): input tensor\n\n    Returns:\n        torch.Tensor: The rotated image tensor\n\n    """"""\n\n    return torch.flip(input, [-2, -1])\n\n\ndef hflip(input: torch.Tensor) -> torch.Tensor:\n    r""""""Horizontally flip a tensor image or a batch of tensor images. Input must\n    be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.\n\n    Args:\n        input (torch.Tensor): input tensor\n\n    Returns:\n        torch.Tensor: The horizontally flipped image tensor\n\n    """"""\n\n    return torch.flip(input, [-1])\n\n\ndef vflip(input: torch.Tensor) -> torch.Tensor:\n    r""""""Vertically flip a tensor image or a batch of tensor images. Input must\n    be a tensor of shape (C, H, W) or a batch of tensors :math:`(*, C, H, W)`.\n\n    Args:\n        input (torch.Tensor): input tensor\n\n    Returns:\n        torch.Tensor: The vertically flipped image tensor\n\n    """"""\n\n    return torch.flip(input, [-2])\n'"
kornia/geometry/transform/imgwarp.py,98,"b'from typing import Tuple, Optional\n\nimport torch\nimport torch.nn.functional as F\n\nfrom kornia.geometry.conversions import deg2rad\nfrom kornia.geometry.warp import (\n    normalize_homography, homography_warp\n)\n\n# TODO: move to utils or conversions\nfrom kornia.geometry.conversions import (\n    deg2rad, normalize_pixel_coordinates, convert_affinematrix_to_homography\n)\nfrom kornia.testing import check_is_tensor\n\n__all__ = [\n    ""warp_perspective"",\n    ""warp_affine"",\n    ""get_perspective_transform"",\n    ""get_rotation_matrix2d"",\n    ""remap"",\n    ""invert_affine_transform"",\n    ""angle_to_rotation_matrix"",\n    ""get_affine_matrix2d""\n]\n\n\ndef transform_warp_impl(src: torch.Tensor, dst_pix_trans_src_pix: torch.Tensor,\n                        dsize_src: Tuple[int, int], dsize_dst: Tuple[int, int],\n                        grid_mode: str, padding_mode: str,\n                        align_corners: bool) -> torch.Tensor:\n    """"""Compute the transform in normalized cooridnates and perform the warping.\n    """"""\n    dst_norm_trans_src_norm: torch.Tensor = normalize_homography(\n        dst_pix_trans_src_pix, dsize_src, dsize_dst)\n\n    src_norm_trans_dst_norm = torch.inverse(dst_norm_trans_src_norm)\n    return homography_warp(src, src_norm_trans_dst_norm, dsize_dst, grid_mode, padding_mode,\n                           align_corners, True)\n\n\ndef warp_perspective(src: torch.Tensor, M: torch.Tensor, dsize: Tuple[int, int],\n                     flags: str = \'bilinear\', border_mode: str = \'zeros\',\n                     align_corners: bool = False) -> torch.Tensor:\n    r""""""Applies a perspective transformation to an image.\n\n    The function warp_perspective transforms the source image using\n    the specified matrix:\n\n    .. math::\n        \\text{dst} (x, y) = \\text{src} \\left(\n        \\frac{M_{11} x + M_{12} y + M_{13}}{M_{31} x + M_{32} y + M_{33}} ,\n        \\frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}}\n        \\right )\n\n    Args:\n        src (torch.Tensor): input image.\n        M (Tensor): transformation matrix.\n        dsize (tuple): size of the output image (height, width).\n        flags (str): interpolation mode to calculate output values\n          \'bilinear\' | \'nearest\'. Default: \'bilinear\'.\n        border_mode (str): padding mode for outside grid values\n          \'zeros\' | \'border\' | \'reflection\'. Default: \'zeros\'.\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n\n    Returns:\n        Tensor: the warped input image.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)` and :math:`(B, 3, 3)`\n        - Output: :math:`(B, C, H, W)`\n\n    .. note::\n       See a working example `here <https://kornia.readthedocs.io/en/latest/\n       tutorials/warp_perspective.html>`_.\n    """"""\n    check_is_tensor(src)\n    check_is_tensor(M)\n\n    if not len(src.shape) == 4:\n        raise ValueError(""Input src must be a BxCxHxW tensor. Got {}""\n                         .format(src.shape))\n\n    if not (len(M.shape) == 3 or M.shape[-2:] == (3, 3)):\n        raise ValueError(""Input M must be a Bx3x3 tensor. Got {}""\n                         .format(M.shape))\n\n    # launches the warper\n    h, w = src.shape[-2:]\n    return transform_warp_impl(src, M, (h, w), dsize, flags, border_mode, align_corners)\n\n\ndef warp_affine(src: torch.Tensor, M: torch.Tensor,\n                dsize: Tuple[int, int], flags: str = \'bilinear\',\n                padding_mode: str = \'zeros\',\n                align_corners: bool = False) -> torch.Tensor:\n    r""""""Applies an affine transformation to a tensor.\n\n    The function warp_affine transforms the source tensor using\n    the specified matrix:\n\n    .. math::\n        \\text{dst}(x, y) = \\text{src} \\left( M_{11} x + M_{12} y + M_{13} ,\n        M_{21} x + M_{22} y + M_{23} \\right )\n\n    Args:\n        src (torch.Tensor): input tensor of shape :math:`(B, C, H, W)`.\n        M (torch.Tensor): affine transformation of shape :math:`(B, 2, 3)`.\n        dsize (Tuple[int, int]): size of the output image (height, width).\n        mode (str): interpolation mode to calculate output values\n          \'bilinear\' | \'nearest\'. Default: \'bilinear\'.\n        padding_mode (str): padding mode for outside grid values\n          \'zeros\' | \'border\' | \'reflection\'. Default: \'zeros\'.\n        align_corners (bool): mode for grid_generation. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for details\n\n    Returns:\n        torch.Tensor: the warped tensor.\n\n    Shape:\n        - Output: :math:`(B, C, H, W)`\n\n    .. note::\n       See a working example `here <https://kornia.readthedocs.io/en/latest/\n       tutorials/warp_affine.html>`__.\n    """"""\n    if not torch.is_tensor(src):\n        raise TypeError(""Input src type is not a torch.Tensor. Got {}""\n                        .format(type(src)))\n\n    if not torch.is_tensor(M):\n        raise TypeError(""Input M type is not a torch.Tensor. Got {}""\n                        .format(type(M)))\n\n    if not len(src.shape) == 4:\n        raise ValueError(""Input src must be a BxCxHxW tensor. Got {}""\n                         .format(src.shape))\n\n    if not (len(M.shape) == 3 or M.shape[-2:] == (2, 3)):\n        raise ValueError(""Input M must be a Bx2x3 tensor. Got {}""\n                         .format(M.shape))\n    B, C, H, W = src.size()\n    dsize_src = (H, W)\n    out_size = dsize\n    # we generate a 3x3 transformation matrix from 2x3 affine\n    M_3x3: torch.Tensor = convert_affinematrix_to_homography(M)\n    dst_norm_trans_src_norm: torch.Tensor = normalize_homography(\n        M_3x3, dsize_src, out_size)\n    src_norm_trans_dst_norm = torch.inverse(dst_norm_trans_src_norm)\n    grid = F.affine_grid(src_norm_trans_dst_norm[:, :2, :],  # type: ignore\n                         [B, C, out_size[0], out_size[1]],\n                         align_corners=align_corners)\n    return F.grid_sample(src, grid,  # type: ignore\n                         align_corners=align_corners,\n                         mode=flags,\n                         padding_mode=padding_mode)\n\n\ndef get_perspective_transform(src, dst):\n    r""""""Calculates a perspective transform from four pairs of the corresponding\n    points.\n\n    The function calculates the matrix of a perspective transform so that:\n\n    .. math ::\n\n        \\begin{bmatrix}\n        t_{i}x_{i}^{\'} \\\\\n        t_{i}y_{i}^{\'} \\\\\n        t_{i} \\\\\n        \\end{bmatrix}\n        =\n        \\textbf{map_matrix} \\cdot\n        \\begin{bmatrix}\n        x_{i} \\\\\n        y_{i} \\\\\n        1 \\\\\n        \\end{bmatrix}\n\n    where\n\n    .. math ::\n        dst(i) = (x_{i}^{\'},y_{i}^{\'}), src(i) = (x_{i}, y_{i}), i = 0,1,2,3\n\n    Args:\n        src (Tensor): coordinates of quadrangle vertices in the source image.\n        dst (Tensor): coordinates of the corresponding quadrangle vertices in\n            the destination image.\n\n    Returns:\n        Tensor: the perspective transformation.\n\n    Shape:\n        - Input: :math:`(B, 4, 2)` and :math:`(B, 4, 2)`\n        - Output: :math:`(B, 3, 3)`\n    """"""\n    if not torch.is_tensor(src):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(src)))\n    if not torch.is_tensor(dst):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                        .format(type(dst)))\n    if not src.shape[-2:] == (4, 2):\n        raise ValueError(""Inputs must be a Bx4x2 tensor. Got {}""\n                         .format(src.shape))\n    if not src.shape == dst.shape:\n        raise ValueError(""Inputs must have the same shape. Got {}""\n                         .format(dst.shape))\n    if not (src.shape[0] == dst.shape[0]):\n        raise ValueError(""Inputs must have same batch size dimension. Expect {} but got {}""\n                         .format(src.shape, dst.shape))\n\n    def ax(p, q):\n        ones = torch.ones_like(p)[..., 0:1]\n        zeros = torch.zeros_like(p)[..., 0:1]\n        return torch.cat(\n            [p[:, 0:1], p[:, 1:2], ones, zeros, zeros, zeros,\n             -p[:, 0:1] * q[:, 0:1], -p[:, 1:2] * q[:, 0:1]\n             ], dim=1)\n\n    def ay(p, q):\n        ones = torch.ones_like(p)[..., 0:1]\n        zeros = torch.zeros_like(p)[..., 0:1]\n        return torch.cat(\n            [zeros, zeros, zeros, p[:, 0:1], p[:, 1:2], ones,\n             -p[:, 0:1] * q[:, 1:2], -p[:, 1:2] * q[:, 1:2]], dim=1)\n    # we build matrix A by using only 4 point correspondence. The linear\n    # system is solved with the least square method, so here\n    # we could even pass more correspondence\n    p = []\n    p.append(ax(src[:, 0], dst[:, 0]))\n    p.append(ay(src[:, 0], dst[:, 0]))\n\n    p.append(ax(src[:, 1], dst[:, 1]))\n    p.append(ay(src[:, 1], dst[:, 1]))\n\n    p.append(ax(src[:, 2], dst[:, 2]))\n    p.append(ay(src[:, 2], dst[:, 2]))\n\n    p.append(ax(src[:, 3], dst[:, 3]))\n    p.append(ay(src[:, 3], dst[:, 3]))\n\n    # A is Bx8x8\n    A = torch.stack(p, dim=1)\n\n    # b is a Bx8x1\n    b = torch.stack([\n        dst[:, 0:1, 0], dst[:, 0:1, 1],\n        dst[:, 1:2, 0], dst[:, 1:2, 1],\n        dst[:, 2:3, 0], dst[:, 2:3, 1],\n        dst[:, 3:4, 0], dst[:, 3:4, 1],\n    ], dim=1)\n\n    # solve the system Ax = b\n    X, LU = torch.solve(b, A)\n\n    # create variable to return\n    batch_size = src.shape[0]\n    M = torch.ones(batch_size, 9, device=src.device, dtype=src.dtype)\n    M[..., :8] = torch.squeeze(X, dim=-1)\n    return M.view(-1, 3, 3)  # Bx3x3\n\n\ndef angle_to_rotation_matrix(angle: torch.Tensor) -> torch.Tensor:\n    r""""""Create a rotation matrix out of angles in degrees.\n    Args:\n        angle: (torch.Tensor): tensor of angles in degrees, any shape.\n\n    Returns:\n        torch.Tensor: tensor of *x2x2 rotation matrices.\n\n    Shape:\n        - Input: :math:`(*)`\n        - Output: :math:`(*, 2, 2)`\n\n    Example:\n        >>> input = torch.rand(1, 3)  # Nx3\n        >>> output = kornia.angle_to_rotation_matrix(input)  # Nx3x2x2\n    """"""\n    ang_rad = deg2rad(angle)\n    cos_a: torch.Tensor = torch.cos(ang_rad)\n    sin_a: torch.Tensor = torch.sin(ang_rad)\n    return torch.stack([cos_a, sin_a, -sin_a, cos_a], dim=-1).view(*angle.shape, 2, 2)\n\n\ndef get_rotation_matrix2d(\n        center: torch.Tensor,\n        angle: torch.Tensor,\n        scale: torch.Tensor) -> torch.Tensor:\n    r""""""Calculates an affine matrix of 2D rotation.\n\n    The function calculates the following matrix:\n\n    .. math::\n        \\begin{bmatrix}\n            \\alpha & \\beta & (1 - \\alpha) \\cdot \\text{x}\n            - \\beta \\cdot \\text{y} \\\\\n            -\\beta & \\alpha & \\beta \\cdot \\text{x}\n            + (1 - \\alpha) \\cdot \\text{y}\n        \\end{bmatrix}\n\n    where\n\n    .. math::\n        \\alpha = \\text{scale} \\cdot cos(\\text{angle}) \\\\\n        \\beta = \\text{scale} \\cdot sin(\\text{angle})\n\n    The transformation maps the rotation center to itself\n    If this is not the target, adjust the shift.\n\n    Args:\n        center (Tensor): center of the rotation in the source image.\n        angle (Tensor): rotation angle in degrees. Positive values mean\n            counter-clockwise rotation (the coordinate origin is assumed to\n            be the top-left corner).\n        scale (Tensor): isotropic scale factor.\n\n    Returns:\n        Tensor: the affine matrix of 2D rotation.\n\n    Shape:\n        - Input: :math:`(B, 2)`, :math:`(B)` and :math:`(B)`\n        - Output: :math:`(B, 2, 3)`\n\n    Example:\n        >>> center = torch.zeros(1, 2)\n        >>> scale = torch.ones(1)\n        >>> angle = 45. * torch.ones(1)\n        >>> M = kornia.get_rotation_matrix2d(center, angle, scale)\n        tensor([[[ 0.7071,  0.7071,  0.0000],\n                 [-0.7071,  0.7071,  0.0000]]])\n    """"""\n    if not torch.is_tensor(center):\n        raise TypeError(""Input center type is not a torch.Tensor. Got {}""\n                        .format(type(center)))\n    if not torch.is_tensor(angle):\n        raise TypeError(""Input angle type is not a torch.Tensor. Got {}""\n                        .format(type(angle)))\n    if not torch.is_tensor(scale):\n        raise TypeError(""Input scale type is not a torch.Tensor. Got {}""\n                        .format(type(scale)))\n    if not (len(center.shape) == 2 and center.shape[1] == 2):\n        raise ValueError(""Input center must be a Bx2 tensor. Got {}""\n                         .format(center.shape))\n    if not len(angle.shape) == 1:\n        raise ValueError(""Input angle must be a B tensor. Got {}""\n                         .format(angle.shape))\n    if not len(scale.shape) == 1:\n        raise ValueError(""Input scale must be a B tensor. Got {}""\n                         .format(scale.shape))\n    if not (center.shape[0] == angle.shape[0] == scale.shape[0]):\n        raise ValueError(""Inputs must have same batch size dimension. Got center {}, angle {} and scale {}""\n                         .format(center.shape, angle.shape, scale.shape))\n    # convert angle and apply scale\n    scaled_rotation: torch.Tensor = angle_to_rotation_matrix(angle) * scale.view(-1, 1, 1)\n    alpha: torch.Tensor = scaled_rotation[:, 0, 0]\n    beta: torch.Tensor = scaled_rotation[:, 0, 1]\n\n    # unpack the center to x, y coordinates\n    x: torch.Tensor = center[..., 0]\n    y: torch.Tensor = center[..., 1]\n\n    # create output tensor\n    batch_size: int = center.shape[0]\n    one = torch.tensor(1.).to(center.device)\n    M: torch.Tensor = torch.zeros(\n        batch_size, 2, 3, device=center.device, dtype=center.dtype)\n    M[..., 0:2, 0:2] = scaled_rotation\n    M[..., 0, 2] = (one - alpha) * x - beta * y\n    M[..., 1, 2] = beta * x + (one - alpha) * y\n    return M\n\n\ndef remap(tensor: torch.Tensor, map_x: torch.Tensor,\n          map_y: torch.Tensor,\n          align_corners: bool = False) -> torch.Tensor:\n    r""""""Applies a generic geometrical transformation to a tensor.\n\n    The function remap transforms the source tensor using the specified map:\n\n    .. math::\n        \\text{dst}(x, y) = \\text{src}(map_x(x, y), map_y(x, y))\n\n    Args:\n        tensor (torch.Tensor): the tensor to remap with shape (B, D, H, W).\n          Where D is the number of channels.\n        map_x (torch.Tensor): the flow in the x-direction in pixel coordinates.\n          The tensor must be in the shape of (B, H, W).\n        map_y (torch.Tensor): the flow in the y-direction in pixel coordinates.\n          The tensor must be in the shape of (B, H, W).\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n\n    Returns:\n        torch.Tensor: the warped tensor.\n\n    Example:\n        >>> grid = kornia.utils.create_meshgrid(2, 2, False)  # 1x2x2x2\n        >>> grid += 1  # apply offset in both directions\n        >>> input = torch.ones(1, 1, 2, 2)\n        >>> kornia.remap(input, grid[..., 0], grid[..., 1])   # 1x1x2x2\n        tensor([[[[1., 0.],\n                  [0., 0.]]]])\n\n    """"""\n    if not torch.is_tensor(tensor):\n        raise TypeError(""Input tensor type is not a torch.Tensor. Got {}""\n                        .format(type(tensor)))\n    if not torch.is_tensor(map_x):\n        raise TypeError(""Input map_x type is not a torch.Tensor. Got {}""\n                        .format(type(map_x)))\n    if not torch.is_tensor(map_y):\n        raise TypeError(""Input map_y type is not a torch.Tensor. Got {}""\n                        .format(type(map_y)))\n    if not tensor.shape[-2:] == map_x.shape[-2:] == map_y.shape[-2:]:\n        raise ValueError(""Inputs last two dimensions must match."")\n\n    batch_size, _, height, width = tensor.shape\n\n    # grid_sample need the grid between -1/1\n    map_xy: torch.Tensor = torch.stack([map_x, map_y], dim=-1)\n    map_xy_norm: torch.Tensor = normalize_pixel_coordinates(\n        map_xy, height, width)\n\n    # simulate broadcasting since grid_sample does not support it\n    map_xy_norm = map_xy_norm.expand(batch_size, -1, -1, -1)\n\n    # warp ans return\n    tensor_warped: torch.Tensor = F.grid_sample(tensor, map_xy_norm, align_corners=align_corners)  # type: ignore\n    return tensor_warped\n\n\ndef invert_affine_transform(matrix: torch.Tensor) -> torch.Tensor:\n    r""""""Inverts an affine transformation.\n\n    The function computes an inverse affine transformation represented by\n    2\xc3\x973 matrix:\n\n    .. math::\n        \\begin{bmatrix}\n            a_{11} & a_{12} & b_{1} \\\\\n            a_{21} & a_{22} & b_{2} \\\\\n        \\end{bmatrix}\n\n    The result is also a 2\xc3\x973 matrix of the same type as M.\n\n    Args:\n        matrix (torch.Tensor): original affine transform. The tensor must be\n          in the shape of (B, 2, 3).\n\n    Return:\n        torch.Tensor: the reverse affine transform.\n    """"""\n    if not torch.is_tensor(matrix):\n        raise TypeError(""Input matrix type is not a torch.Tensor. Got {}""\n                        .format(type(matrix)))\n    if not (len(matrix.shape) == 3 and matrix.shape[-2:] == (2, 3)):\n        raise ValueError(""Input matrix must be a Bx2x3 tensor. Got {}""\n                         .format(matrix.shape))\n    matrix_tmp: torch.Tensor = convert_affinematrix_to_homography(matrix)\n    matrix_inv: torch.Tensor = torch.inverse(matrix_tmp)\n    return matrix_inv[..., :2, :3]\n\n\ndef get_affine_matrix2d(translations: torch.Tensor, center: torch.Tensor, scale: torch.Tensor, angle: torch.Tensor,\n                        sx: Optional[torch.Tensor] = None, sy: Optional[torch.Tensor] = None) -> torch.Tensor:\n    r""""""Composes affine matrix Bx3x3 from the components\n    Returns:\n        torch.Tensor: params to be passed to the affine transformation.\n    """"""\n    transform: torch.Tensor = get_rotation_matrix2d(center, -angle, scale)\n    transform[..., 2] += translations  # tx/ty\n    # pad transform to get Bx3x3\n    transform_h = convert_affinematrix_to_homography(transform)\n\n    if sx is not None:\n        x, y = torch.split(center, 1, dim=-1)\n        x = x.view(-1)\n        y = y.view(-1)\n        sx_tan = torch.tan(sx)  # type: ignore\n        sy_tan = torch.tan(sy)  # type: ignore\n        zeros = torch.zeros_like(sx)  # type: ignore\n        ones = torch.ones_like(sx)  # type: ignore\n        shear_mat = torch.stack([ones, -sx_tan, sx_tan * x,  # type: ignore   # noqa: E241\n                                 -sy_tan, ones + sx_tan * sy_tan, sy_tan * (-sx_tan * x + y)],  # noqa: E241\n                                dim=-1).view(-1, 2, 3)\n        shear_mat = convert_affinematrix_to_homography(shear_mat)\n        transform_h = transform_h @ shear_mat\n    return transform_h\n'"
kornia/geometry/transform/pyramid.py,40,"b'from typing import Tuple, List\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.filters import gaussian_blur2d, filter2D\n\n__all__ = [\n    ""PyrDown"",\n    ""PyrUp"",\n    ""ScalePyramid"",\n    ""pyrdown"",\n    ""pyrup"",\n    ""build_pyramid"",\n]\n\n\ndef _get_pyramid_gaussian_kernel() -> torch.Tensor:\n    """"""Utility function that return a pre-computed gaussian kernel.""""""\n    return torch.tensor([[\n        [1., 4., 6., 4., 1.],\n        [4., 16., 24., 16., 4.],\n        [6., 24., 36., 24., 6.],\n        [4., 16., 24., 16., 4.],\n        [1., 4., 6., 4., 1.]\n    ]]) / 256.\n\n\nclass PyrDown(nn.Module):\n    r""""""Blurs a tensor and downsamples it.\n\n    Args:\n        border_type (str): the padding mode to be applied before convolving.\n          The expected modes are: ``\'constant\'``, ``\'reflect\'``,\n          ``\'replicate\'`` or ``\'circular\'``. Default: ``\'reflect\'``.\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n\n    Return:\n        torch.Tensor: the downsampled tensor.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H / 2, W / 2)`\n\n    Examples:\n        >>> input = torch.rand(1, 2, 4, 4)\n        >>> output = kornia.transform.PyrDown()(input)  # 1x2x2x2\n    """"""\n\n    def __init__(self, border_type: str = \'reflect\', align_corners: bool = False) -> None:\n        super(PyrDown, self).__init__()\n        self.border_type: str = border_type\n        self.kernel: torch.Tensor = _get_pyramid_gaussian_kernel()\n        self.align_corners: bool = align_corners\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        if not torch.is_tensor(input):\n            raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                            .format(type(input)))\n        if not len(input.shape) == 4:\n            raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                             .format(input.shape))\n        # blur image\n        x_blur: torch.Tensor = filter2D(input, self.kernel, self.border_type)\n\n        # downsample.\n        out: torch.Tensor = F.interpolate(x_blur, scale_factor=0.5, mode=\'bilinear\',\n                                          align_corners=self.align_corners)\n        return out\n\n\nclass PyrUp(nn.Module):\n    r""""""Upsamples a tensor and then blurs it.\n\n    Args:\n        borde_type (str): the padding mode to be applied before convolving.\n          The expected modes are: ``\'constant\'``, ``\'reflect\'``,\n          ``\'replicate\'`` or ``\'circular\'``. Default: ``\'reflect\'``.\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n\n    Return:\n        torch.Tensor: the upsampled tensor.\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output: :math:`(B, C, H * 2, W * 2)`\n\n    Examples:\n        >>> input = torch.rand(1, 2, 4, 4)\n        >>> output = kornia.transform.PyrUp()(input)  # 1x2x8x8\n    """"""\n\n    def __init__(self, border_type: str = \'reflect\', align_corners: bool = False):\n        super(PyrUp, self).__init__()\n        self.border_type: str = border_type\n        self.kernel: torch.Tensor = _get_pyramid_gaussian_kernel()\n        self.align_corners: bool = align_corners\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:  # type: ignore\n        if not torch.is_tensor(input):\n            raise TypeError(""Input type is not a torch.Tensor. Got {}""\n                            .format(type(input)))\n        if not len(input.shape) == 4:\n            raise ValueError(""Invalid input shape, we expect BxCxHxW. Got: {}""\n                             .format(input.shape))\n        # upsample tensor\n        b, c, height, width = input.shape\n        x_up: torch.Tensor = F.interpolate(input, size=(height * 2, width * 2),\n                                           mode=\'bilinear\', align_corners=self.align_corners)\n\n        # blurs upsampled tensor\n        x_blur: torch.Tensor = filter2D(x_up, self.kernel, self.border_type)\n        return x_blur\n\n\nclass ScalePyramid(nn.Module):\n    r""""""Creates an scale pyramid of image, usually used for local feature\n    detection. Images are consequently smoothed with Gaussian blur and\n    downscaled.\n    Arguments:\n        n_levels (int): number of the levels in octave.\n        init_sigma (float): initial blur level.\n        min_size (int): the minimum size of the octave in pixels. Default is 5\n        double_image (bool): add 2x upscaled image as 1st level of pyramid. OpenCV SIFT does this. Default is False\n    Returns:\n        Tuple(List(Tensors), List(Tensors), List(Tensors)):\n        1st output: images\n        2nd output: sigmas (coefficients for scale conversion)\n        3rd output: pixelDists (coefficients for coordinate conversion)\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output 1st: :math:`[(B, NL, C, H, W), (B, NL, C, H/2, W/2), ...]`\n        - Output 2nd: :math:`[(B, NL), (B, NL), (B, NL), ...]`\n        - Output 3rd: :math:`[(B, NL), (B, NL), (B, NL), ...]`\n\n    Examples::\n        >>> input = torch.rand(2, 4, 100, 100)\n        >>> sp, sigmas, pds = kornia.ScalePyramid(3, 15)(input)\n    """"""\n\n    def __init__(self,\n                 n_levels: int = 3,\n                 init_sigma: float = 1.6,\n                 min_size: int = 5,\n                 double_image: bool = False):\n        super(ScalePyramid, self).__init__()\n        self.n_levels = n_levels\n        self.init_sigma = init_sigma\n        self.min_size = min_size\n        self.border = min_size // 2 - 1\n        self.sigma_step = 2 ** (1. / float(self.n_levels))\n        self.double_image = double_image\n        return\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__ +\\\n            \'(n_levels=\' + str(self.n_levels) + \', \' +\\\n            \'init_sigma=\' + str(self.init_sigma) + \', \' + \\\n            \'min_size=\' + str(self.min_size) + \', \' + \\\n            \'border=\' + str(self.border) + \', \' + \\\n            \'sigma_step=\' + str(self.sigma_step) + \\\n            \'double_image=\' + str(self.double_image) + \')\'\n\n    def get_kernel_size(self, sigma: float):\n        ksize = int(2.0 * 4.0 * sigma + 1.0)\n        if ksize % 2 == 0:\n            ksize += 1\n        return ksize\n\n    def forward(self, x: torch.Tensor) -> Tuple[  # type: ignore\n            List, List, List]:\n        bs, ch, h, w = x.size()\n        pixel_distance = 1.0\n        cur_sigma = 0.5\n        if self.double_image:\n            x = F.interpolate(x, scale_factor=2.0, mode=\'bilinear\', align_corners=False)\n            pixel_distance = 0.5\n            cur_sigma *= 2.0\n        if self.init_sigma > cur_sigma:\n            sigma = math.sqrt(self.init_sigma**2 - cur_sigma**2)\n            cur_sigma = self.init_sigma\n            ksize = self.get_kernel_size(sigma)\n            cur_level = gaussian_blur2d(x, (ksize, ksize), (sigma, sigma))\n        else:\n            cur_level = x\n        sigmas = [cur_sigma * torch.ones(bs, self.n_levels).to(x.device).to(x.dtype)]\n        pixel_dists = [pixel_distance * torch.ones(\n                       bs,\n                       self.n_levels).to(\n                       x.device).to(x.dtype)]\n        pyr = [[cur_level.unsqueeze(1)]]\n        oct_idx = 0\n        while True:\n            cur_level = pyr[-1][0].squeeze(1)\n            for level_idx in range(1, self.n_levels):\n                sigma = cur_sigma * math.sqrt(self.sigma_step**2 - 1.0)\n                cur_level = gaussian_blur2d(\n                    cur_level, (ksize, ksize), (sigma, sigma))\n                cur_sigma *= self.sigma_step\n                pyr[-1].append(cur_level.unsqueeze(1))\n                sigmas[-1][:, level_idx] = cur_sigma\n                pixel_dists[-1][:, level_idx] = pixel_distance\n            nextOctaveFirstLevel = F.interpolate(pyr[-1][-1].squeeze(1), scale_factor=0.5,\n                                                 mode=\'bilinear\',\n                                                 align_corners=False)\n            pixel_distance *= 2.0\n            cur_sigma = self.init_sigma\n            if (min(nextOctaveFirstLevel.size(2),\n                    nextOctaveFirstLevel.size(3)) <= self.min_size):\n                break\n            pyr.append([nextOctaveFirstLevel.unsqueeze(1)])\n            sigmas.append(cur_sigma * torch.ones(\n                          bs,\n                          self.n_levels).to(\n                          x.device))\n            pixel_dists.append(\n                pixel_distance * torch.ones(\n                    bs,\n                    self.n_levels).to(\n                    x.device))\n            oct_idx += 1\n        for i in range(len(pyr)):\n            pyr[i] = torch.cat(pyr[i], dim=1)  # type: ignore\n        return pyr, sigmas, pixel_dists\n\n\n# functional api\n\n\ndef pyrdown(\n        input: torch.Tensor,\n        border_type: str = \'reflect\', align_corners: bool = False) -> torch.Tensor:\n    r""""""Blurs a tensor and downsamples it.\n\n    See :class:`~kornia.transform.PyrDown` for details.\n    """"""\n    return PyrDown(border_type, align_corners)(input)\n\n\ndef pyrup(input: torch.Tensor, border_type: str = \'reflect\', align_corners: bool = False) -> torch.Tensor:\n    r""""""Upsamples a tensor and then blurs it.\n\n    See :class:`~kornia.transform.PyrUp` for details.\n    """"""\n    return PyrUp(border_type, align_corners)(input)\n\n\ndef build_pyramid(\n        input: torch.Tensor,\n        max_level: int,\n        border_type: str = \'reflect\', align_corners: bool = False) -> List[torch.Tensor]:\n    r""""""Constructs the Gaussian pyramid for an image.\n\n    The function constructs a vector of images and builds the Gaussian pyramid\n    by recursively applying pyrDown to the previously built pyramid layers.\n\n    Args:\n        input (torch.Tensor): the tensor to be used to constructuct the pyramid.\n        max_level (int): 0-based index of the last (the smallest) pyramid layer.\n          It must be non-negative.\n        border_type (str): the padding mode to be applied before convolving.\n          The expected modes are: ``\'constant\'``, ``\'reflect\'``,\n          ``\'replicate\'`` or ``\'circular\'``. Default: ``\'reflect\'``.\n        align_corners(bool): interpolation flag. Default: False. See\n\n    Shape:\n        - Input: :math:`(B, C, H, W)`\n        - Output :math:`[(B, NL, C, H, W), (B, NL, C, H/2, W/2), ...]`\n    """"""\n    if not isinstance(input, torch.Tensor):\n        raise TypeError(f""Input type is not a torch.Tensor. Got {type(input)}"")\n\n    if not len(input.shape) == 4:\n        raise ValueError(f""Invalid input shape, we expect BxCxHxW. Got: {input.shape}"")\n\n    if not isinstance(max_level, int) or max_level < 0:\n        raise ValueError(f""Invalid max_level, it must be a positive integer. Got: {max_level}"")\n\n    # create empty list and append the original image\n    pyramid: List[torch.Tensor] = []\n    pyramid.append(input)\n\n    # iterate and downsample\n\n    for _ in range(max_level - 1):\n        img_curr: torch.Tensor = pyramid[-1]\n        img_down: torch.Tensor = pyrdown(img_curr, border_type, align_corners)\n        pyramid.append(img_down)\n\n    return pyramid\n'"
kornia/geometry/warp/__init__.py,0,b'from kornia.geometry.warp.homography_warper import *\nfrom kornia.geometry.warp.depth_warper import *\n'
kornia/geometry/warp/depth_warper.py,33,"b'from typing import Union\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.geometry.linalg import transform_points\nfrom kornia.geometry.linalg import relative_transformation\nfrom kornia.geometry.conversions import convert_points_to_homogeneous\nfrom kornia.geometry.conversions import normalize_pixel_coordinates\nfrom kornia.geometry.camera import PinholeCamera\nfrom kornia.geometry.camera import cam2pixel, pixel2cam\nfrom kornia.utils import create_meshgrid\n\n\n__all__ = [\n    ""depth_warp"",\n    ""DepthWarper"",\n]\n\n\nclass DepthWarper(nn.Module):\n    r""""""Warps a patch by depth.\n\n    .. math::\n        P_{src}^{\\{dst\\}} = K_{dst} * T_{src}^{\\{dst\\}}\n\n        I_{src} = \\\\omega(I_{dst}, P_{src}^{\\{dst\\}}, D_{src})\n\n    Args:\n        pinholes_dst (PinholeCamera): the pinhole models for the destination\n          frame.\n        height (int): the height of the image to warp.\n        width (int): the width of the image to warp.\n        mode (str): interpolation mode to calculate output values\n          \'bilinear\' | \'nearest\'. Default: \'bilinear\'.\n        padding_mode (str): padding mode for outside grid values\n           \'zeros\' | \'border\' | \'reflection\'. Default: \'zeros\'.\n        align_corners(bool): interpolation flag. Default: True. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n    """"""\n\n    def __init__(self,\n                 pinhole_dst: PinholeCamera,\n                 height: int, width: int,\n                 mode: str = \'bilinear\',\n                 padding_mode: str = \'zeros\',\n                 align_corners: bool = True):\n        super(DepthWarper, self).__init__()\n        # constructor members\n        self.width: int = width\n        self.height: int = height\n        self.mode: str = mode\n        self.padding_mode: str = padding_mode\n        self.eps = 1e-6\n        self.align_corners: bool = align_corners\n\n        # state members\n        self._pinhole_dst: PinholeCamera = pinhole_dst\n        self._pinhole_src: Union[None, PinholeCamera] = None\n        self._dst_proj_src: Union[None, torch.Tensor] = None\n\n        self.grid: torch.Tensor = self._create_meshgrid(height, width)\n\n    @staticmethod\n    def _create_meshgrid(height: int, width: int) -> torch.Tensor:\n        grid: torch.Tensor = create_meshgrid(\n            height, width, normalized_coordinates=False)  # 1xHxWx2\n        return convert_points_to_homogeneous(grid)  # append ones to last dim\n\n    def compute_projection_matrix(\n            self, pinhole_src: PinholeCamera) -> \'DepthWarper\':\n        r""""""Computes the projection matrix from the source to destinaion frame.\n        """"""\n        if not isinstance(self._pinhole_dst, PinholeCamera):\n            raise TypeError(""Member self._pinhole_dst expected to be of class ""\n                            ""PinholeCamera. Got {}""\n                            .format(type(self._pinhole_dst)))\n        if not isinstance(pinhole_src, PinholeCamera):\n            raise TypeError(""Argument pinhole_src expected to be of class ""\n                            ""PinholeCamera. Got {}"".format(type(pinhole_src)))\n        # compute the relative pose between the non reference and the reference\n        # camera frames.\n        dst_trans_src: torch.Tensor = relative_transformation(\n            pinhole_src.extrinsics, self._pinhole_dst.extrinsics)\n\n        # compute the projection matrix between the non reference cameras and\n        # the reference.\n        dst_proj_src: torch.Tensor = torch.matmul(\n            self._pinhole_dst.intrinsics, dst_trans_src)\n\n        # update class members\n        self._pinhole_src = pinhole_src\n        self._dst_proj_src = dst_proj_src\n        return self\n\n    def _compute_projection(self, x, y, invd):\n        point = torch.FloatTensor([[[x], [y], [1.0], [invd]]])\n        flow = torch.matmul(\n            self._dst_proj_src, point.to(self._dst_proj_src.device))\n        z = 1. / flow[:, 2]\n        x = (flow[:, 0] * z)\n        y = (flow[:, 1] * z)\n        return torch.cat([x, y], 1)\n\n    def compute_subpixel_step(self) -> torch.Tensor:\n        """"""This computes the required inverse depth step to achieve sub pixel\n        accurate sampling of the depth cost volume, per camera.\n\n        Szeliski, Richard, and Daniel Scharstein.\n        ""Symmetric sub-pixel stereo matching."" European Conference on Computer\n        Vision. Springer Berlin Heidelberg, 2002.\n        """"""\n        delta_d = 0.01\n        xy_m1 = self._compute_projection(self.width / 2, self.height / 2,\n                                         1.0 - delta_d)\n        xy_p1 = self._compute_projection(self.width / 2, self.height / 2,\n                                         1.0 + delta_d)\n        dx = torch.norm((xy_p1 - xy_m1), 2, dim=-1) / 2.0\n        dxdd = dx / (delta_d)  # pixel*(1/meter)\n        # half pixel sampling, we\'re interested in the min for all cameras\n        return torch.min(0.5 / dxdd)\n\n    def warp_grid(self, depth_src: torch.Tensor) -> torch.Tensor:\n        """"""Computes a grid for warping a given the depth from the reference\n        pinhole camera.\n\n        The function `compute_projection_matrix` has to be called beforehand in\n        order to have precomputed the relative projection matrices encoding the\n        relative pose and the intrinsics between the reference and a non\n        reference camera.\n        """"""\n        # TODO: add type and value checkings\n        if self._dst_proj_src is None or self._pinhole_src is None:\n            raise ValueError(""Please, call compute_projection_matrix."")\n\n        if len(depth_src.shape) != 4:\n            raise ValueError(""Input depth_src has to be in the shape of ""\n                             ""Bx1xHxW. Got {}"".format(depth_src.shape))\n\n        # unpack depth attributes\n        batch_size, _, height, width = depth_src.shape\n        device: torch.device = depth_src.device\n        dtype: torch.dtype = depth_src.dtype\n\n        # expand the base coordinate grid according to the input batch size\n        pixel_coords: torch.Tensor = self.grid.to(device).to(dtype).expand(\n            batch_size, -1, -1, -1)  # BxHxWx3\n\n        # reproject the pixel coordinates to the camera frame\n        cam_coords_src: torch.Tensor = pixel2cam(\n            depth_src,\n            self._pinhole_src.intrinsics_inverse().to(dtype),\n            pixel_coords)  # BxHxWx3\n\n        # reproject the camera coordinates to the pixel\n        pixel_coords_src: torch.Tensor = cam2pixel(\n            cam_coords_src, self._dst_proj_src.to(dtype))  # (B*N)xHxWx2\n\n        # normalize between -1 and 1 the coordinates\n        pixel_coords_src_norm: torch.Tensor = normalize_pixel_coordinates(\n            pixel_coords_src, self.height, self.width)\n        return pixel_coords_src_norm\n\n    def forward(  # type: ignore\n            self,\n            depth_src: torch.Tensor,\n            patch_dst: torch.Tensor) -> torch.Tensor:\n        """"""Warps a tensor from destination frame to reference given the depth\n        in the reference frame.\n\n        Args:\n            depth_src (torch.Tensor): the depth in the reference frame. The\n              tensor must have a shape :math:`(B, 1, H, W)`.\n            patch_dst (torch.Tensor): the patch in the destination frame. The\n              tensor must have a shape :math:`(B, C, H, W)`.\n\n        Return:\n            torch.Tensor: the warped patch from destination frame to reference.\n\n        Shape:\n            - Output: :math:`(N, C, H, W)` where C = number of channels.\n\n        Example:\n            >>> # pinholes camera models\n            >>> pinhole_dst = kornia.PinholeCamera(...)\n            >>> pinhole_src = kornia.PinholeCamera(...)\n            >>> # create the depth warper, compute the projection matrix\n            >>> warper = kornia.DepthWarper(pinhole_dst, height, width)\n            >>> warper.compute_projection_matrix(pinhole_src)\n            >>> # warp the destionation frame to reference by depth\n            >>> depth_src = torch.ones(1, 1, 32, 32)  # Nx1xHxW\n            >>> image_dst = torch.rand(1, 3, 32, 32)  # NxCxHxW\n            >>> image_src = warper(depth_src, image_dst)  # NxCxHxW\n        """"""\n        return F.grid_sample(patch_dst, self.warp_grid(depth_src),  # type: ignore\n                             mode=self.mode, padding_mode=self.padding_mode,\n                             align_corners=self.align_corners)\n\n\n# functional api\n\ndef depth_warp(pinhole_dst: PinholeCamera,\n               pinhole_src: PinholeCamera,\n               depth_src: torch.Tensor,\n               patch_dst: torch.Tensor,\n               height: int, width: int,\n               align_corners: bool = True):\n    r""""""Function that warps a tensor from destination frame to reference\n    given the depth in the reference frame.\n\n    See :class:`~kornia.geometry.warp.DepthWarper` for details.\n\n    Example:\n        >>> # pinholes camera models\n        >>> pinhole_dst = kornia.PinholeCamera(...)\n        >>> pinhole_src = kornia.PinholeCamera(...)\n        >>> # warp the destionation frame to reference by depth\n        >>> depth_src = torch.ones(1, 1, 32, 32)  # Nx1xHxW\n        >>> image_dst = torch.rand(1, 3, 32, 32)  # NxCxHxW\n        >>> image_src = kornia.depth_warp(pinhole_dst, pinhole_src,\n        >>>     depth_src, image_dst, height, width)  # NxCxHxW\n    """"""\n    warper = DepthWarper(pinhole_dst, height, width, align_corners=align_corners)\n    warper.compute_projection_matrix(pinhole_src)\n    return warper(depth_src, patch_dst)\n'"
kornia/geometry/warp/homography_warper.py,36,"b'from typing import Tuple, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom kornia.utils import create_meshgrid\nfrom kornia.geometry.linalg import transform_points\nfrom kornia.testing import check_is_tensor\n\n\n__all__ = [\n    ""HomographyWarper"",\n    ""homography_warp"",\n    ""warp_grid"",\n    ""normalize_homography"",\n    ""normal_transform_pixel"",\n]\n\n\ndef warp_grid(grid: torch.Tensor, src_homo_dst: torch.Tensor) -> torch.Tensor:\n    r""""""Compute the grid to warp the coordinates grid by the homography/ies.\n\n    Args:\n        grid: Unwrapped grid of the shape :math:`(1, N, W, 2)`.\n        src_homo_dst (torch.Tensor): Homography or homographies (stacked) to\n          transform all points in the grid. Shape of the homography\n          has to be :math:`(1, 3, 3)` or :math:`(N, 1, 3, 3)`.\n\n\n    Returns:\n        torch.Tensor: the transformed grid of shape :math:`(N, H, W, 2)`.\n    """"""\n    batch_size: int = src_homo_dst.size(0)\n    _, height, width, _ = grid.size()\n    # expand grid to match the input batch size\n    grid = grid.expand(batch_size, -1, -1, -1)  # NxHxWx2\n    if len(src_homo_dst.shape) == 3:  # local homography case\n        src_homo_dst = src_homo_dst.view(batch_size, 1, 3, 3)  # Nx1x3x3\n    # perform the actual grid transformation,\n    # the grid is copied to input device and casted to the same type\n    flow: torch.Tensor = transform_points(src_homo_dst, grid.to(src_homo_dst))  # NxHxWx2\n    return flow.view(batch_size, height, width, 2)  # NxHxWx2\n\n\n# functional api\ndef homography_warp(patch_src: torch.Tensor,\n                    src_homo_dst: torch.Tensor,\n                    dsize: Tuple[int, int],\n                    mode: str = \'bilinear\',\n                    padding_mode: str = \'zeros\',\n                    align_corners: bool = False,\n                    normalized_coordinates: bool = True) -> torch.Tensor:\n    r""""""Function that warps image patchs or tensors by homographies.\n\n    See :class:`~kornia.geometry.warp.HomographyWarper` for details.\n\n    Args:\n        patch_src (torch.Tensor): The image or tensor to warp. Should be from\n                                  source of shape :math:`(N, C, H, W)`.\n        src_homo_dst (torch.Tensor): The homography or stack of homographies\n                                     from destination to source of shape\n                                     :math:`(N, 3, 3)`.\n        dsize (Tuple[int, int]): The height and width of the image to warp.\n        mode (str): interpolation mode to calculate output values\n          \'bilinear\' | \'nearest\'. Default: \'bilinear\'.\n        padding_mode (str): padding mode for outside grid values\n          \'zeros\' | \'border\' | \'reflection\'. Default: \'zeros\'.\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n        normalized_coordinates (bool): Whether the homography assumes [-1, 1] normalized\n                                       coordinates or not.\n\n    Return:\n        torch.Tensor: Patch sampled at locations from source to destination.\n\n    Example:\n        >>> input = torch.rand(1, 3, 32, 32)\n        >>> homography = torch.eye(3).view(1, 3, 3)\n        >>> output = kornia.homography_warp(input, homography, (32, 32))\n    """"""\n    if not src_homo_dst.device == patch_src.device:\n        raise TypeError(""Patch and homography must be on the same device. \\\n                         Got patch.device: {} src_H_dst.device: {}."".format(\n                        patch_src.device, src_homo_dst.device))\n\n    height, width = dsize\n    grid = create_meshgrid(height, width, normalized_coordinates=normalized_coordinates)\n    warped_grid = warp_grid(grid, src_homo_dst)\n\n    return F.grid_sample(patch_src, warped_grid, mode=mode, padding_mode=padding_mode,\n                         align_corners=align_corners)\n\n\n# layer api\nclass HomographyWarper(nn.Module):\n    r""""""Warp tensors by homographies.\n\n    .. math::\n\n        X_{dst} = H_{src}^{\\{dst\\}} * X_{src}\n\n    Args:\n        height (int): The height of the destination tensor.\n        width (int): The width of the destination tensor.\n        mode (str): interpolation mode to calculate output values\n          \'bilinear\' | \'nearest\'. Default: \'bilinear\'.\n        padding_mode (str): padding mode for outside grid values\n          \'zeros\' | \'border\' | \'reflection\'. Default: \'zeros\'.\n        normalized_coordinates (bool): wether to use a grid with\n          normalized coordinates.\n        align_corners(bool): interpolation flag. Default: False. See\n        https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.interpolate for detail\n    """"""\n    _warped_grid: Optional[torch.Tensor]\n\n    def __init__(\n            self,\n            height: int,\n            width: int,\n            mode: str = \'bilinear\',\n            padding_mode: str = \'zeros\',\n            normalized_coordinates: bool = True,\n            align_corners: bool = False) -> None:\n        super(HomographyWarper, self).__init__()\n        self.width: int = width\n        self.height: int = height\n        self.mode: str = mode\n        self.padding_mode: str = padding_mode\n        self.normalized_coordinates: bool = normalized_coordinates\n        self.align_corners: bool = align_corners\n        # create base grid to compute the flow\n        self.grid: torch.Tensor = create_meshgrid(\n            height, width, normalized_coordinates=normalized_coordinates)\n\n        # initialice the warped destination grid\n        self._warped_grid = None\n\n    def precompute_warp_grid(self, src_homo_dst: torch.Tensor) -> None:\n        r""""""Compute and store internaly the transformations of the points.\n\n        Useful when the same homography/homographies are reused.\n\n        Args:\n            src_homo_dst (torch.Tensor): Homography or homographies (stacked) to\n              transform all points in the grid. Shape of the homography\n              has to be :math:`(1, 3, 3)` or :math:`(N, 1, 3, 3)`.\n              The homography assumes normalized coordinates [-1, 1] if\n              normalized_coordinates is True.\n         """"""\n        self._warped_grid = warp_grid(self.grid, src_homo_dst)\n\n    def forward(\n            self,\n            patch_src: torch.Tensor,\n            src_homo_dst: Optional[torch.Tensor] = None) -> torch.Tensor:\n        r""""""Warp a tensor from source into reference frame.\n\n        Args:\n            patch_src (torch.Tensor): The tensor to warp.\n            src_homo_dst (torch.Tensor, optional): The homography or stack of\n              homographies from destination to source. The homography assumes\n              normalized coordinates [-1, 1] if normalized_coordinates is True.\n              Default: None.\n\n        Return:\n            torch.Tensor: Patch sampled at locations from source to destination.\n\n        Shape:\n            - Input: :math:`(N, C, H, W)` and :math:`(N, 3, 3)`\n            - Output: :math:`(N, C, H, W)`\n\n        Example:\n            >>> input = torch.rand(1, 3, 32, 32)\n            >>> homography = torch.eye(3).view(1, 3, 3)\n            >>> warper = kornia.HomographyWarper(32, 32)\n            >>> # without precomputing the warp\n            >>> output = warper(input, homography)  # NxCxHxW\n            >>> # precomputing the warp\n            >>> warper.precompute_warp_grid(homography)\n            >>> output = warper(input)  # NxCxHxW\n        """"""\n        _warped_grid = self._warped_grid\n        if src_homo_dst is not None:\n            warped_patch = homography_warp(\n                patch_src, src_homo_dst, (self.height, self.width), mode=self.mode,\n                padding_mode=self.padding_mode, align_corners=self.align_corners,\n                normalized_coordinates=self.normalized_coordinates)\n        elif _warped_grid is not None:\n            if not _warped_grid.device == patch_src.device:\n                raise TypeError(""Patch and warped grid must be on the same device. \\\n                                 Got patch.device: {} warped_grid.device: {}. Wheter \\\n                                 recall precompute_warp_grid() with the correct device \\\n                                 for the homograhy or change the patch device."".format(\n                                patch_src.device, _warped_grid.device))\n            warped_patch = F.grid_sample(\n                patch_src, _warped_grid, mode=self.mode, padding_mode=self.padding_mode,\n                align_corners=self.align_corners)\n        else:\n            raise RuntimeError(""Unknown warping. If homographies are not provided \\\n                                they must be presetted using the method: \\\n                                precompute_warp_grid()."")\n\n        return warped_patch\n\n\ndef normal_transform_pixel(height: int, width: int) -> torch.Tensor:\n    r""""""Compute the normalization matrix from image size in pixels to [-1, 1].\n\n    Args:\n        height (int): image height.\n        width (int): image width.\n\n    Returns:\n        Tensor: normalized transform.\n\n    Shape:\n        Output: :math:`(1, 3, 3)`\n    """"""\n    tr_mat = torch.tensor([[1.0, 0.0, -1.0],\n                           [0.0, 1.0, -1.0],\n                           [0.0, 0.0, 1.0]])  # 3x3\n\n    tr_mat[0, 0] = tr_mat[0, 0] * 2.0 / (width - 1.0)\n    tr_mat[1, 1] = tr_mat[1, 1] * 2.0 / (height - 1.0)\n\n    tr_mat = tr_mat.unsqueeze(0)  # 1x3x3\n    return tr_mat\n\n\ndef normalize_homography(dst_pix_trans_src_pix: torch.Tensor,\n                         dsize_src: Tuple[int, int], dsize_dst: Tuple[int, int]) -> torch.Tensor:\n    r""""""Normalize a given homography in pixels to [-1, 1].\n\n    Args:\n        dst_pix_trans_src_pix (torch.Tensor): homography/ies from source to destiantion to be\n          normalized. :math:`(B, 3, 3)`\n        dsize_src (tuple): size of the source image (height, width).\n        dsize_src (tuple): size of the destination image (height, width).\n\n    Returns:\n        Tensor: the normalized homography.\n\n    Shape:\n        Output: :math:`(B, 3, 3)`\n    """"""\n    check_is_tensor(dst_pix_trans_src_pix)\n\n    if not (len(dst_pix_trans_src_pix.shape) == 3 or dst_pix_trans_src_pix.shape[-2:] == (3, 3)):\n        raise ValueError(""Input dst_pix_trans_src_pix must be a Bx3x3 tensor. Got {}""\n                         .format(dst_pix_trans_src_pix.shape))\n\n    # source and destination sizes\n    src_h, src_w = dsize_src\n    dst_h, dst_w = dsize_dst\n    # compute the transformation pixel/norm for src/dst\n    src_norm_trans_src_pix: torch.Tensor = normal_transform_pixel(\n        src_h, src_w).to(dst_pix_trans_src_pix)\n    src_pix_trans_src_norm = torch.inverse(src_norm_trans_src_pix)\n    dst_norm_trans_dst_pix: torch.Tensor = normal_transform_pixel(\n        dst_h, dst_w).to(dst_pix_trans_src_pix)\n    # compute chain transformations\n    dst_norm_trans_src_norm: torch.Tensor = (\n        dst_norm_trans_dst_pix @ (dst_pix_trans_src_pix @ src_pix_trans_src_norm)\n    )\n    return dst_norm_trans_src_norm\n'"
kornia/utils/metrics/__init__.py,0,"b'from .confusion_matrix import confusion_matrix\nfrom .mean_iou import mean_iou\n\n__all__ = [\n    ""confusion_matrix"",\n    ""mean_iou"",\n]\n'"
kornia/utils/metrics/confusion_matrix.py,21,"b'from typing import Optional\n\nimport torch\n\n# Inspired by:\n# https://github.com/pytorch/tnt/blob/master/torchnet/meter/confusionmeter.py#L68-L73  # noqa\n\n\ndef confusion_matrix(\n        input: torch.Tensor,\n        target: torch.Tensor,\n        num_classes: int,\n        normalized: Optional[bool] = False) -> torch.Tensor:\n    r""""""Compute confusion matrix to evaluate the accuracy of a classification.\n\n    Args:\n        input (torch.Tensor) : tensor with estimated targets returned by a\n          classifier. The shape can be :math:`(B, *)` and must contain integer\n          values between 0 and K-1.\n        target (torch.Tensor) : tensor with ground truth (correct) target\n          values. The shape can be :math:`(B, *)` and must contain integer\n          values between 0 and K-1, whete targets are assumed to be provided as\n          one-hot vectors.\n        num_classes (int): total possible number of classes in target.\n        normalized: (Optional[bool]): wether to return the confusion matrix\n          normalized. Default: False.\n\n    Returns:\n        torch.Tensor: a tensor containing the confusion matrix with shape\n        :math:`(B, K, K)` where K is the number of classes.\n    """"""\n    if not torch.is_tensor(input) and input.dtype is not torch.int64:\n        raise TypeError(""Input input type is not a torch.Tensor with ""\n                        ""torch.int64 dtype. Got {}"".format(type(input)))\n    if not torch.is_tensor(target) and target.dtype is not torch.int64:\n        raise TypeError(""Input target type is not a torch.Tensor with ""\n                        ""torch.int64 dtype. Got {}"".format(type(target)))\n    if not input.shape == target.shape:\n        raise ValueError(""Inputs input and target must have the same shape. ""\n                         ""Got: {} and {}"".format(input.shape, target.shape))\n    if not input.device == target.device:\n        raise ValueError(""Inputs must be in the same device. ""\n                         ""Got: {} - {}"".format(input.device, target.device))\n    if not isinstance(num_classes, int) or num_classes < 2:\n        raise ValueError(""The number of classes must be an intenger bigger ""\n                         ""than two. Got: {}"".format(num_classes))\n    batch_size: int = input.shape[0]\n\n    # hack for bitcounting 2 arrays together\n    # NOTE: torch.bincount does not implement batched version\n    pre_bincount: torch.Tensor = input + target * num_classes\n    pre_bincount_vec: torch.Tensor = pre_bincount.view(batch_size, -1)\n\n    confusion_list = []\n    for iter_id in range(batch_size):\n        pb: torch.Tensor = pre_bincount_vec[iter_id]\n        bin_count: torch.Tensor = torch.bincount(pb, minlength=num_classes**2)\n        confusion_list.append(bin_count)\n\n    confusion_vec: torch.Tensor = torch.stack(confusion_list)\n    confusion_mat: torch.Tensor = confusion_vec.view(\n        batch_size, num_classes, num_classes).to(torch.float32)  # BxKxK\n\n    if normalized:\n        norm_val: torch.Tensor = torch.sum(confusion_mat, dim=1, keepdim=True)\n        confusion_mat = confusion_mat / (norm_val + 1e-6)\n\n    return confusion_mat\n'"
kornia/utils/metrics/mean_iou.py,16,"b'from typing import Optional\n\nimport torch\nfrom kornia.utils.metrics.confusion_matrix import confusion_matrix\n\n\ndef mean_iou(\n        input: torch.Tensor,\n        target: torch.Tensor,\n        num_classes: int,\n        eps: Optional[float] = 1e-6) -> torch.Tensor:\n    r""""""Calculate mean Intersection-Over-Union (mIOU).\n\n    The function internally computes the confusion matrix.\n\n    Args:\n        input (torch.Tensor) : tensor with estimated targets returned by a\n          classifier. The shape can be :math:`(B, *)` and must contain integer\n          values between 0 and K-1.\n        target (torch.Tensor) : tensor with ground truth (correct) target\n          values. The shape can be :math:`(B, *)` and must contain integer\n          values between 0 and K-1, whete targets are assumed to be provided as\n          one-hot vectors.\n        num_classes (int): total possible number of classes in target.\n\n    Returns:\n        torch.Tensor: a tensor representing the mean intersection-over union\n        with shape :math:`(B, K)` where K is the number of classes.\n    """"""\n    if not torch.is_tensor(input) and input.dtype is not torch.int64:\n        raise TypeError(""Input input type is not a torch.Tensor with ""\n                        ""torch.int64 dtype. Got {}"".format(type(input)))\n    if not torch.is_tensor(target) and target.dtype is not torch.int64:\n        raise TypeError(""Input target type is not a torch.Tensor with ""\n                        ""torch.int64 dtype. Got {}"".format(type(target)))\n    if not input.shape == target.shape:\n        raise ValueError(""Inputs input and target must have the same shape. ""\n                         ""Got: {} and {}"".format(input.shape, target.shape))\n    if not input.device == target.device:\n        raise ValueError(""Inputs must be in the same device. ""\n                         ""Got: {} - {}"".format(input.device, target.device))\n    if not isinstance(num_classes, int) or num_classes < 2:\n        raise ValueError(""The number of classes must be an intenger bigger ""\n                         ""than two. Got: {}"".format(num_classes))\n    # we first compute the confusion matrix\n    conf_mat: torch.Tensor = confusion_matrix(input, target, num_classes)\n\n    # compute the actual intersection over union\n    sum_over_row = torch.sum(conf_mat, dim=1)\n    sum_over_col = torch.sum(conf_mat, dim=2)\n    conf_mat_diag = torch.diagonal(conf_mat, dim1=-2, dim2=-1)\n    denominator = sum_over_row + sum_over_col - conf_mat_diag\n\n    # NOTE: we add epsilon so that samples that are neither in the\n    # prediction or ground truth are taken into account.\n    ious = (conf_mat_diag + eps) / (denominator + eps)\n    return ious\n'"
test/geometry/epipolar/test_common.py,4,"b'from typing import Dict\n\nimport torch\nimport kornia.geometry.epipolar as epi\n\n\ndef generate_two_view_random_scene(\n    device: torch.device = torch.device(""cpu""),\n    dtype: torch.dtype = torch.float32\n) -> Dict[str, torch.Tensor]:\n\n    num_views: int = 2\n    num_points: int = 30\n\n    scene: Dict[str, torch.Tensor] = epi.generate_scene(num_views, num_points)\n\n    # internal parameters (same K)\n    K1 = scene[\'K\'].to(device, dtype)\n    K2 = K1.clone()\n\n    # rotation\n    R1 = scene[\'R\'][0:1].to(device, dtype)\n    R2 = scene[\'R\'][1:2].to(device, dtype)\n\n    # translation\n    t1 = scene[\'t\'][0:1].to(device, dtype)\n    t2 = scene[\'t\'][1:2].to(device, dtype)\n\n    # projection matrix, P = K(R|t)\n    P1 = scene[\'P\'][0:1].to(device, dtype)\n    P2 = scene[\'P\'][1:2].to(device, dtype)\n\n    # fundamental matrix\n    F_mat = epi.fundamental_from_projections(\n        P1[..., :3, :], P2[..., :3, :])\n\n    F_mat = epi.normalize_transformation(F_mat)\n\n    # points 3d\n    X = scene[\'points3d\'].to(device, dtype)\n\n    # projected points\n    x1 = scene[\'points2d\'][0:1].to(device, dtype)\n    x2 = scene[\'points2d\'][1:2].to(device, dtype)\n\n    return dict(K1=K1, K2=K2, R1=R1, R2=R2, t1=t1, t2=t2, P1=P1, P2=P2, F=F_mat, X=X, x1=x1, x2=x2)\n'"
test/geometry/epipolar/test_epipolar_metrics.py,14,"b'import pytest\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\nimport kornia.geometry.epipolar as epi\nimport kornia.testing as utils\n\n\nclass TestSymmetricalEpipolarDistance:\n\n    def test_smoke(self, device, dtype):\n        pts1 = torch.rand(1, 4, 3, device=device, dtype=dtype)\n        pts2 = torch.rand(1, 4, 3, device=device, dtype=dtype)\n        Fm = utils.create_random_fundamental_matrix(1).type_as(pts1)\n        assert epi.symmetrical_epipolar_distance(pts1, pts2, Fm).shape == (1, 4)\n\n    def test_batch(self, device, dtype):\n        batch_size = 5\n        pts1 = torch.rand(batch_size, 4, 3, device=device, dtype=dtype)\n        pts2 = torch.rand(batch_size, 4, 3, device=device, dtype=dtype)\n        Fm = utils.create_random_fundamental_matrix(1).type_as(pts1)\n        assert epi.symmetrical_epipolar_distance(pts1, pts2, Fm).shape == (5, 4)\n\n    def test_gradcheck(self, device):\n        # generate input data\n        batch_size, num_points, num_dims = 2, 3, 2\n        points1 = torch.rand(batch_size, num_points, num_dims, device=device, dtype=torch.float64, requires_grad=True)\n        points2 = torch.rand(batch_size, num_points, num_dims, device=device, dtype=torch.float64)\n        Fm = utils.create_random_fundamental_matrix(batch_size).type_as(points2)\n        assert gradcheck(epi.symmetrical_epipolar_distance, (points1, points2, Fm),\n                         raise_exception=True)\n\n\nclass TestSampsonEpipolarDistance:\n\n    def test_smoke(self, device, dtype):\n        pts1 = torch.rand(1, 4, 3, device=device, dtype=dtype)\n        pts2 = torch.rand(1, 4, 3, device=device, dtype=dtype)\n        Fm = utils.create_random_fundamental_matrix(1).type_as(pts1)\n        assert epi.sampson_epipolar_distance(pts1, pts2, Fm).shape == (1, 4)\n\n    def test_batch(self, device, dtype):\n        batch_size = 5\n        pts1 = torch.rand(batch_size, 4, 3, device=device, dtype=dtype)\n        pts2 = torch.rand(batch_size, 4, 3, device=device, dtype=dtype)\n        Fm = utils.create_random_fundamental_matrix(1).type_as(pts1)\n        assert epi.sampson_epipolar_distance(pts1, pts2, Fm).shape == (5, 4)\n\n    def test_gradcheck(self, device):\n        # generate input data\n        batch_size, num_points, num_dims = 2, 3, 2\n        points1 = torch.rand(batch_size, num_points, num_dims, device=device, dtype=torch.float64, requires_grad=True)\n        points2 = torch.rand(batch_size, num_points, num_dims, device=device, dtype=torch.float64)\n        Fm = utils.create_random_fundamental_matrix(batch_size).type_as(points2)\n        assert gradcheck(epi.sampson_epipolar_distance, (points1, points2, Fm),\n                         raise_exception=True)\n'"
test/geometry/epipolar/test_essential.py,68,"b'import pytest\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\nimport kornia.geometry.epipolar as epi\n\nimport test_common as utils\n\n\nclass TestEssentialFromFundamental:\n    def test_smoke(self, device, dtype):\n        F_mat = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        K1 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        K2 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        E_mat = epi.essential_from_fundamental(F_mat, K1, K2)\n        assert E_mat.shape == (1, 3, 3)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 4, 7])\n    def test_shape(self, batch_size, device, dtype):\n        B: int = batch_size\n        F_mat = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        K1 = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        K2 = torch.rand(1, 3, 3, device=device, dtype=dtype)  # check broadcasting\n        E_mat = epi.essential_from_fundamental(F_mat, K1, K2)\n        assert E_mat.shape == (B, 3, 3)\n\n    def test_from_to_fundamental(self, device, dtype):\n        F_mat = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        K1 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        K2 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        E_mat = epi.essential_from_fundamental(F_mat, K1, K2)\n        F_hat = epi.fundamental_from_essential(E_mat, K1, K2)\n        assert_allclose(F_mat, F_hat, atol=1e-4, rtol=1e-4)\n\n    def test_shape_large(self, device, dtype):\n        F_mat = torch.rand(1, 2, 3, 3, device=device, dtype=dtype)\n        K1 = torch.rand(1, 2, 3, 3, device=device, dtype=dtype)\n        K2 = torch.rand(1, 1, 3, 3, device=device, dtype=dtype)  # check broadcasting\n        E_mat = epi.essential_from_fundamental(F_mat, K1, K2)\n        assert E_mat.shape == (1, 2, 3, 3)\n\n    def test_from_fundamental(self, device, dtype):\n\n        scene = utils.generate_two_view_random_scene(device, dtype)\n\n        F_mat = scene[\'F\']\n\n        K1 = scene[\'K1\']\n        K2 = scene[\'K2\']\n\n        E_mat = epi.essential_from_fundamental(F_mat, K1, K2)\n        F_hat = epi.fundamental_from_essential(E_mat, K1, K2)\n\n        F_mat_norm = epi.normalize_transformation(F_mat)\n        F_hat_norm = epi.normalize_transformation(F_hat)\n        assert_allclose(F_mat_norm, F_hat_norm)\n\n    def test_gradcheck(self, device):\n        F_mat = torch.rand(1, 3, 3, device=device, dtype=torch.float64, requires_grad=True)\n        K1 = torch.rand(1, 3, 3, device=device, dtype=torch.float64)\n        K2 = torch.rand(1, 3, 3, device=device, dtype=torch.float64)\n        assert gradcheck(epi.essential_from_fundamental,\n                         (F_mat, K1, K2,), raise_exception=True)\n\n\nclass TestRelativeCameraMotion:\n    def test_smoke(self, device, dtype):\n        R1 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        t1 = torch.rand(1, 3, 1, device=device, dtype=dtype)\n        R2 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        t2 = torch.rand(1, 3, 1, device=device, dtype=dtype)\n        R, t = epi.relative_camera_motion(R1, t1, R2, t2)\n        assert R.shape == (1, 3, 3)\n        assert t.shape == (1, 3, 1)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 3, 5, 8, ])\n    def test_shape(self, batch_size, device, dtype):\n        B: int = batch_size\n        R1 = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        t1 = torch.rand(B, 3, 1, device=device, dtype=dtype)\n        R2 = torch.rand(1, 3, 3, device=device, dtype=dtype)  # check broadcasting\n        t2 = torch.rand(B, 3, 1, device=device, dtype=dtype)\n        R, t = epi.relative_camera_motion(R1, t1, R2, t2)\n        assert R.shape == (B, 3, 3)\n        assert t.shape == (B, 3, 1)\n\n    def test_translation(self, device, dtype):\n        R1 = torch.tensor([[\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]], device=device, dtype=dtype)\n\n        t1 = torch.tensor([[[10.], [0.], [0.]]]).type_as(R1)\n\n        R2 = epi.eye_like(3, R1)\n        t2 = epi.vec_like(3, t1)\n\n        R_expected = R1.clone()\n        t_expected = -t1\n\n        R, t = epi.relative_camera_motion(R1, t1, R2, t2)\n        assert_allclose(R_expected, R)\n        assert_allclose(t_expected, t)\n\n    def test_rotate_z(self, device, dtype):\n        R1 = torch.tensor([[\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]], device=device, dtype=dtype)\n\n        R2 = torch.tensor([[\n            [0., 0., 0.],\n            [0., 0., 0.],\n            [0., 0., 1.],\n        ]], device=device, dtype=dtype)\n\n        t1 = epi.vec_like(3, R1)\n        t2 = epi.vec_like(3, R2)\n\n        R_expected = R2.clone()\n        t_expected = t1\n\n        R, t = epi.relative_camera_motion(R1, t1, R2, t2)\n        assert_allclose(R_expected, R)\n        assert_allclose(t_expected, t)\n\n    def test_gradcheck(self, device):\n        R1 = torch.rand(1, 3, 3, device=device, dtype=torch.float64, requires_grad=True)\n        R2 = torch.rand(1, 3, 3, device=device, dtype=torch.float64)\n        t1 = torch.rand(1, 3, 1, device=device, dtype=torch.float64)\n        t2 = torch.rand(1, 3, 1, device=device, dtype=torch.float64)\n        assert gradcheck(epi.relative_camera_motion,\n                         (R1, t1, R2, t2,), raise_exception=True)\n\n\nclass TestEssentalFromRt:\n    def test_smoke(self, device, dtype):\n        R1 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        t1 = torch.rand(1, 3, 1, device=device, dtype=dtype)\n        R2 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        t2 = torch.rand(1, 3, 1, device=device, dtype=dtype)\n        E_mat = epi.essential_from_Rt(R1, t1, R2, t2)\n        assert E_mat.shape == (1, 3, 3)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 3, 5, 8, ])\n    def test_shape(self, batch_size, device, dtype):\n        B: int = batch_size\n        R1 = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        t1 = torch.rand(B, 3, 1, device=device, dtype=dtype)\n        R2 = torch.rand(1, 3, 3, device=device, dtype=dtype)  # check broadcasting\n        t2 = torch.rand(B, 3, 1, device=device, dtype=dtype)\n        E_mat = epi.essential_from_Rt(R1, t1, R2, t2)\n        assert E_mat.shape == (B, 3, 3)\n\n    def test_from_fundamental_Rt(self, device, dtype):\n\n        scene = utils.generate_two_view_random_scene(device, dtype)\n\n        E_from_Rt = epi.essential_from_Rt(\n            scene[\'R1\'], scene[\'t1\'], scene[\'R2\'], scene[\'t2\'])\n\n        E_from_F = epi.essential_from_fundamental(\n            scene[\'F\'], scene[\'K1\'], scene[\'K2\'])\n\n        E_from_Rt_norm = epi.normalize_transformation(E_from_Rt)\n        E_from_F_norm = epi.normalize_transformation(E_from_F)\n        assert_allclose(E_from_Rt_norm, E_from_F_norm, rtol=1e-3, atol=1e-3)\n\n    def test_gradcheck(self, device):\n        R1 = torch.rand(1, 3, 3, device=device, dtype=torch.float64, requires_grad=True)\n        R2 = torch.rand(1, 3, 3, device=device, dtype=torch.float64)\n        t1 = torch.rand(1, 3, 1, device=device, dtype=torch.float64)\n        t2 = torch.rand(1, 3, 1, device=device, dtype=torch.float64)\n        assert gradcheck(epi.essential_from_Rt,\n                         (R1, t1, R2, t2,), raise_exception=True)\n\n\nclass TestDecomposeEssentialMatrix:\n    def test_smoke(self, device, dtype):\n        E_mat = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        R1, R2, t = epi.decompose_essential_matrix(E_mat)\n        assert R1.shape == (1, 3, 3)\n        assert R2.shape == (1, 3, 3)\n        assert t.shape == (1, 3, 1)\n\n    @pytest.mark.parametrize(""batch_shape"", [\n        (1, 3, 3), (2, 3, 3), (2, 1, 3, 3), (3, 2, 1, 3, 3),\n    ])\n    def test_shape(self, batch_shape, device, dtype):\n        E_mat = torch.rand(batch_shape, device=device, dtype=dtype)\n        R1, R2, t = epi.decompose_essential_matrix(E_mat)\n        assert R1.shape == batch_shape\n        assert R2.shape == batch_shape\n        assert t.shape == batch_shape[:-1] + (1,)\n\n    def test_gradcheck(self, device):\n        E_mat = torch.rand(1, 3, 3, device=device, dtype=torch.float64, requires_grad=True)\n\n        def eval_rot1(input):\n            return epi.decompose_essential_matrix(input)[0]\n\n        def eval_rot2(input):\n            return epi.decompose_essential_matrix(input)[1]\n\n        def eval_vec(input):\n            return epi.decompose_essential_matrix(input)[2]\n\n        assert gradcheck(eval_rot1, (E_mat,), raise_exception=True)\n        assert gradcheck(eval_rot2, (E_mat,), raise_exception=True)\n        assert gradcheck(eval_vec, (E_mat,), raise_exception=True)\n\n\nclass TestMotionFromEssential:\n    def test_smoke(self, device, dtype):\n        E_mat = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        Rs, Ts = epi.motion_from_essential(E_mat)\n        assert Rs.shape == (1, 4, 3, 3)\n        assert Ts.shape == (1, 4, 3, 1)\n\n    @pytest.mark.parametrize(""batch_shape"", [\n        (1, 3, 3), (2, 3, 3), (2, 1, 3, 3), (3, 2, 1, 3, 3),\n    ])\n    def test_shape(self, batch_shape, device, dtype):\n        E_mat = torch.rand(batch_shape, device=device, dtype=dtype)\n        Rs, Ts = epi.motion_from_essential(E_mat)\n        assert Rs.shape == batch_shape[:-2] + (4, 3, 3)\n        assert Ts.shape == batch_shape[:-2] + (4, 3, 1)\n\n    def test_two_view(self, device, dtype):\n        scene = utils.generate_two_view_random_scene(device, dtype)\n\n        R1, t1 = scene[\'R1\'], scene[\'t1\']\n        R2, t2 = scene[\'R2\'], scene[\'t2\']\n\n        E_mat = epi.essential_from_Rt(R1, t1, R2, t2)\n\n        R, t = epi.relative_camera_motion(R1, t1, R2, t2)\n        t = torch.nn.functional.normalize(t, dim=1)\n\n        Rs, ts = epi.motion_from_essential(E_mat)\n\n        rot_error = (Rs - R).abs().sum((-2, -1))\n        vec_error = (ts - t).abs().sum((-1))\n\n        rtol: float = 1e-4\n        assert (rot_error < rtol).any() & (vec_error < rtol).any()\n\n    def test_gradcheck(self, device):\n        E_mat = torch.rand(1, 3, 3, device=device, dtype=torch.float64, requires_grad=True)\n\n        def eval_rot(input):\n            return epi.motion_from_essential(input)[0]\n\n        def eval_vec(input):\n            return epi.motion_from_essential(input)[1]\n\n        assert gradcheck(eval_rot, (E_mat,), raise_exception=True)\n        assert gradcheck(eval_vec, (E_mat,), raise_exception=True)\n\n\nclass TestMotionFromEssentialChooseSolution:\n    def test_smoke(self, device, dtype):\n        E_mat = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        K1 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        K2 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        x1 = torch.rand(1, 1, 2, device=device, dtype=dtype)\n        x2 = torch.rand(1, 1, 2, device=device, dtype=dtype)\n        R, t, X = epi.motion_from_essential_choose_solution(E_mat, K1, K2, x1, x2)\n        assert R.shape == (1, 3, 3)\n        assert t.shape == (1, 3, 1)\n        assert X.shape == (1, 1, 3)\n\n    @pytest.mark.parametrize(""batch_size, num_points"", [\n        (1, 3), (2, 3), (2, 8), (3, 2),\n    ])\n    def test_shape(self, batch_size, num_points, device, dtype):\n        B, N = batch_size, num_points\n        E_mat = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        K1 = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        K2 = torch.rand(1, 3, 3, device=device, dtype=dtype)  # check for broadcasting\n        x1 = torch.rand(B, N, 2, device=device, dtype=dtype)\n        x2 = torch.rand(B, 1, 2, device=device, dtype=dtype)  # check for broadcasting\n        R, t, X = epi.motion_from_essential_choose_solution(E_mat, K1, K2, x1, x2)\n        assert R.shape == (B, 3, 3)\n        assert t.shape == (B, 3, 1)\n        assert X.shape == (B, N, 3)\n\n    def test_two_view(self, device, dtype):\n\n        scene = utils.generate_two_view_random_scene(device, dtype)\n\n        E_mat = epi.essential_from_Rt(\n            scene[\'R1\'], scene[\'t1\'], scene[\'R2\'], scene[\'t2\'])\n\n        R, t = epi.relative_camera_motion(\n            scene[\'R1\'], scene[\'t1\'], scene[\'R2\'], scene[\'t2\'])\n        t = torch.nn.functional.normalize(t, dim=1)\n\n        R_hat, t_hat, X_hat = epi.motion_from_essential_choose_solution(\n            E_mat, scene[\'K1\'], scene[\'K2\'], scene[\'x1\'], scene[\'x2\'])\n\n        assert_allclose(t, t_hat)\n        assert_allclose(R, R_hat, rtol=1e-4, atol=1e-4)\n\n    def test_gradcheck(self, device):\n        E_mat = torch.rand(1, 3, 3, device=device, dtype=torch.float64, requires_grad=True)\n        K1 = torch.rand(1, 3, 3, device=device, dtype=torch.float64)\n        K2 = torch.rand(1, 3, 3, device=device, dtype=torch.float64)\n        x1 = torch.rand(1, 2, 2, device=device, dtype=torch.float64)\n        x2 = torch.rand(1, 2, 2, device=device, dtype=torch.float64)\n\n        assert gradcheck(epi.motion_from_essential_choose_solution,\n                         (E_mat, K1, K2, x1, x2,), raise_exception=True)\n'"
test/geometry/epipolar/test_fundamental.py,65,"b'import pytest\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\nimport kornia.geometry.epipolar as epi\n\nimport test_common as utils\n\n\nclass TestNormalizePoints:\n    def test_smoke(self, device, dtype):\n        points = torch.rand(1, 1, 2, device=device, dtype=dtype)\n        output = epi.normalize_points(points)\n        assert len(output) == 2\n        assert output[0].shape == (1, 1, 2)\n        assert output[1].shape == (1, 3, 3)\n\n    @pytest.mark.parametrize(\n        ""batch_size, num_points"", [(1, 2), (2, 3), (3, 2)],\n    )\n    def test_shape(self, batch_size, num_points, device, dtype):\n        B, N = batch_size, num_points\n        points = torch.rand(B, N, 2, device=device, dtype=dtype)\n        output = epi.normalize_points(points)\n        assert output[0].shape == (B, N, 2)\n        assert output[1].shape == (B, 3, 3)\n\n    def test_mean_std(self, device, dtype):\n        points = torch.tensor([[\n            [0., 0.], [0., 2.], [1., 1.], [1., 3.]\n        ]], device=device, dtype=dtype)\n\n        points_norm, trans = epi.normalize_points(points)\n        points_std, points_mean = torch.std_mean(points_norm, dim=1)\n\n        assert_allclose(points_mean, torch.zeros_like(points_mean))\n        assert (points_std < 2.).all()\n\n    def test_gradcheck(self, device):\n        points = torch.rand(2, 3, 2,\n                            device=device, requires_grad=True, dtype=torch.float64)\n        assert gradcheck(epi.normalize_points, (points,), raise_exception=True)\n\n\nclass TestNormalizeTransformation:\n    def test_smoke(self, device, dtype):\n        trans = torch.rand(2, 2, device=device, dtype=dtype)\n        trans_norm = epi.normalize_transformation(trans)\n        assert trans_norm.shape == (2, 2)\n\n    @pytest.mark.parametrize(\n        ""batch_size, rows, cols"", [(1, 2, 2), (2, 3, 3), (3, 4, 4), (2, 1, 2)],\n    )\n    def test_shape(self, batch_size, rows, cols, device, dtype):\n        B, N, M = batch_size, rows, cols\n        trans = torch.rand(B, N, M, device=device, dtype=dtype)\n        trans_norm = epi.normalize_transformation(trans)\n        assert trans_norm.shape == (B, N, M)\n\n    def test_check_last_val(self, device, dtype):\n        trans = torch.tensor([[\n            [0.0, 0.0, 1.0],\n            [0.0, 2.0, 0.0],\n            [0.5, 0.0, 0.5]\n        ]], device=device, dtype=dtype)\n\n        trans_expected = torch.tensor([[\n            [0.0, 0.0, 2.0],\n            [0.0, 4.0, 0.0],\n            [1.0, 0.0, 1.0]\n        ]], device=device, dtype=dtype)\n\n        trans_norm = epi.normalize_transformation(trans)\n        assert_allclose(trans_norm, trans_expected)\n\n    def test_check_corner_case(self, device, dtype):\n        trans = torch.tensor([[\n            [0.0, 0.0, 1.0],\n            [0.0, 2.0, 0.0],\n            [0.5, 0.0, 0.0]\n        ]], device=device, dtype=dtype)\n\n        trans_expected = trans.clone()\n\n        trans_norm = epi.normalize_transformation(trans)\n        assert_allclose(trans_norm, trans_expected)\n\n    def test_gradcheck(self, device):\n        trans = torch.rand(2, 3, 3,\n                           device=device, requires_grad=True, dtype=torch.float64)\n        assert gradcheck(epi.normalize_transformation, (trans,), raise_exception=True)\n\n\nclass TestFindFundamental:\n    def test_smoke(self, device, dtype):\n        points1 = torch.rand(1, 1, 2, device=device, dtype=dtype)\n        points2 = torch.rand(1, 1, 2, device=device, dtype=dtype)\n        weights = torch.ones(1, 1, device=device, dtype=dtype)\n        F_mat = epi.find_fundamental(points1, points2, weights)\n        assert F_mat.shape == (1, 3, 3)\n\n    @pytest.mark.parametrize(\n        ""batch_size, num_points"", [(1, 2), (2, 3), (3, 2)],\n    )\n    def test_shape(self, batch_size, num_points, device, dtype):\n        B, N = batch_size, num_points\n        points1 = torch.rand(B, N, 2, device=device, dtype=dtype)\n        points2 = torch.rand(B, N, 2, device=device, dtype=dtype)\n        weights = torch.ones(B, N, device=device, dtype=dtype)\n        F_mat = epi.find_fundamental(points1, points2, weights)\n        assert F_mat.shape == (B, 3, 3)\n\n    def test_opencv(self, device, dtype):\n        points1 = torch.tensor([[\n            [0.8569, 0.5982],\n            [0.0059, 0.9649],\n            [0.1968, 0.8846],\n            [0.6084, 0.3467],\n            [0.9633, 0.5274],\n            [0.8941, 0.8939],\n            [0.0863, 0.5133],\n            [0.2645, 0.8882],\n            [0.2411, 0.3045],\n            [0.8199, 0.4107]]], device=device, dtype=dtype)\n\n        points2 = torch.tensor([[\n            [0.0928, 0.3013],\n            [0.0989, 0.9649],\n            [0.0341, 0.4827],\n            [0.8294, 0.4469],\n            [0.2230, 0.2998],\n            [0.1722, 0.8182],\n            [0.5264, 0.8869],\n            [0.8908, 0.1233],\n            [0.2338, 0.7663],\n            [0.4466, 0.5696]]], device=device, dtype=dtype)\n\n        weights = torch.ones(1, 10, device=device, dtype=dtype)\n\n        # generated with OpenCV using above points\n        # import cv2\n        # Fm_expected, _ = cv2.findFundamentalMat(\n        #   points1.detach().numpy().reshape(-1, 1, 2),\n        #   points2.detach().numpy().reshape(-1, 1, 2), cv2.FM_8POINT)\n\n        Fm_expected = torch.tensor([[\n            [-0.47408533, 0.22033807, -0.00346677],\n            [0.54935973, 1.31080955, -1.25028275],\n            [-0.36690215, -1.08143769, 1.]]], device=device, dtype=dtype)\n\n        F_mat = epi.find_fundamental(points1, points2, weights)\n        assert_allclose(F_mat, Fm_expected, rtol=1e-4, atol=1e-4)\n\n    def test_synthetic_sampson(self, device, dtype):\n\n        scene: Dict[str, torch.Tensor] = utils.generate_two_view_random_scene(device, dtype)\n\n        x1 = scene[\'x1\']\n        x2 = scene[\'x2\']\n\n        weights = torch.ones_like(x1)[..., 0]\n        F_est = epi.find_fundamental(x1, x2, weights)\n\n        error = epi.sampson_epipolar_distance(x1, x2, F_est)\n        assert_allclose(error, torch.tensor(0., device=device, dtype=dtype), atol=1e-4, rtol=1e-4)\n\n    def test_gradcheck(self, device):\n        points1 = torch.rand(1, 10, 2, device=device, dtype=torch.float64, requires_grad=True)\n        points2 = torch.rand(1, 10, 2, device=device, dtype=torch.float64)\n        weights = torch.ones(1, 10, device=device, dtype=torch.float64)\n        assert gradcheck(epi.find_fundamental,\n                         (points1, points2, weights,), raise_exception=True)\n\n\nclass TestComputeCorrespondEpilimes:\n    def test_smoke(self, device, dtype):\n        point = torch.rand(1, 1, 2, device=device, dtype=dtype)\n        F_mat = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        lines = epi.compute_correspond_epilines(point, F_mat)\n        assert lines.shape == (1, 1, 3)\n\n    @pytest.mark.parametrize(\n        ""batch_size, num_points"", [(1, 2), (2, 3), (3, 2)],\n    )\n    def test_shape(self, batch_size, num_points, device, dtype):\n        B, N = batch_size, num_points\n        point = torch.rand(B, N, 2, device=device, dtype=dtype)\n        F_mat = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        lines = epi.compute_correspond_epilines(point, F_mat)\n        assert lines.shape == (B, N, 3)\n\n    def test_opencv(self, device, dtype):\n        point = torch.rand(1, 2, 2, device=device, dtype=dtype)\n        F_mat = torch.rand(1, 3, 3, device=device, dtype=dtype)\n\n        point = torch.tensor([[\n            [0.9794, 0.7994],\n            [0.8163, 0.8500],\n        ]], device=device, dtype=dtype)\n\n        F_mat = torch.tensor([[\n            [0.1185, 0.4438, 0.9869],\n            [0.5670, 0.9447, 0.4100],\n            [0.1546, 0.2554, 0.4485],\n        ]], device=device, dtype=dtype)\n\n        # generated with OpenCV using above points\n        # import cv2\n        # lines_expected = cv2.computeCorrespondEpilines(\n        #    point.detach().numpy().reshape(-1, 1, 2), 0,\n        #    F_mat.detach().numpy()[0]).transpose(1, 0, 2)\n\n        lines_expected = torch.tensor([[\n            [0.64643687, 0.7629675, 0.35658622],\n            [0.65710586, 0.7537983, 0.35616538],\n        ]], device=device, dtype=dtype)\n\n        lines_est = epi.compute_correspond_epilines(point, F_mat)\n        assert_allclose(lines_est, lines_expected, rtol=1e-4, atol=1e-4)\n\n    def test_gradcheck(self, device):\n        point = torch.rand(1, 4, 2, device=device, dtype=torch.float64, requires_grad=True)\n        F_mat = torch.rand(1, 3, 3, device=device, dtype=torch.float64)\n        assert gradcheck(epi.compute_correspond_epilines,\n                         (point, F_mat,), raise_exception=True)\n\n\nclass TestFundamentlFromEssential:\n    def test_smoke(self, device, dtype):\n        E_mat = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        K1 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        K2 = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        F_mat = epi.fundamental_from_essential(E_mat, K1, K2)\n        assert F_mat.shape == (1, 3, 3)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 4, 7])\n    def test_shape(self, batch_size, device, dtype):\n        B: int = batch_size\n        E_mat = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        K1 = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        K2 = torch.rand(1, 3, 3, device=device, dtype=dtype)  # check broadcasting\n        F_mat = epi.fundamental_from_essential(E_mat, K1, K2)\n        assert F_mat.shape == (B, 3, 3)\n\n    def test_shape_large(self, device, dtype):\n        E_mat = torch.rand(1, 2, 3, 3, device=device, dtype=dtype)\n        K1 = torch.rand(1, 2, 3, 3, device=device, dtype=dtype)\n        K2 = torch.rand(1, 1, 3, 3, device=device, dtype=dtype)  # check broadcasting\n        F_mat = epi.fundamental_from_essential(E_mat, K1, K2)\n        assert F_mat.shape == (1, 2, 3, 3)\n\n    def test_from_to_essential(self, device, dtype):\n        scene = utils.generate_two_view_random_scene(device, dtype)\n\n        F_mat = scene[\'F\']\n        E_mat = epi.essential_from_fundamental(F_mat, scene[\'K1\'], scene[\'K2\'])\n        F_hat = epi.fundamental_from_essential(E_mat, scene[\'K1\'], scene[\'K2\'])\n\n        F_mat_norm = epi.normalize_transformation(F_mat)\n        F_hat_norm = epi.normalize_transformation(F_hat)\n        assert_allclose(F_mat_norm, F_hat_norm)\n\n    def test_gradcheck(self, device):\n        E_mat = torch.rand(1, 3, 3, device=device, dtype=torch.float64, requires_grad=True)\n        K1 = torch.rand(1, 3, 3, device=device, dtype=torch.float64)\n        K2 = torch.rand(1, 3, 3, device=device, dtype=torch.float64)\n        assert gradcheck(epi.fundamental_from_essential,\n                         (E_mat, K1, K2,), raise_exception=True)\n\n\nclass TestFundamentalFromProjections:\n    def test_smoke(self, device, dtype):\n        P1 = torch.rand(1, 3, 4, device=device, dtype=dtype)\n        P2 = torch.rand(1, 3, 4, device=device, dtype=dtype)\n        F_mat = epi.fundamental_from_projections(P1, P2)\n        assert F_mat.shape == (1, 3, 3)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 4, 7])\n    def test_shape(self, batch_size, device, dtype):\n        B: int = batch_size\n        P1 = torch.rand(B, 3, 4, device=device, dtype=dtype)\n        P2 = torch.rand(B, 3, 4, device=device, dtype=dtype)\n        F_mat = epi.fundamental_from_projections(P1, P2)\n        assert F_mat.shape == (B, 3, 3)\n\n    def test_shape_large(self, device, dtype):\n        P1 = torch.rand(1, 2, 3, 4, device=device, dtype=dtype)\n        P2 = torch.rand(1, 2, 3, 4, device=device, dtype=dtype)\n        F_mat = epi.fundamental_from_projections(P1, P2)\n        assert F_mat.shape == (1, 2, 3, 3)\n\n    def test_from_to_projections(self, device, dtype):\n        P1 = torch.tensor([[\n            [1., 0., 0., 0.],\n            [0., 1., 0., 0.],\n            [1., 0., 1., 0.],\n        ]], device=device, dtype=dtype)\n\n        P2 = torch.tensor([[\n            [1., 1., 1., 3.],\n            [0., 2., 0., 3.],\n            [0., 1., 1., 0.],\n        ]], device=device, dtype=dtype)\n\n        F_mat = epi.fundamental_from_projections(P1, P2)\n        P_mat = epi.projections_from_fundamental(F_mat)\n        F_hat = epi.fundamental_from_projections(P_mat[..., 0], P_mat[..., 1])\n\n        F_mat_norm = epi.normalize_transformation(F_mat)\n        F_hat_norm = epi.normalize_transformation(F_hat)\n        assert_allclose(F_mat_norm, F_hat_norm)\n\n    def test_gradcheck(self, device):\n        P1 = torch.rand(1, 3, 4, device=device, dtype=torch.float64, requires_grad=True)\n        P2 = torch.rand(1, 3, 4, device=device, dtype=torch.float64)\n        assert gradcheck(epi.fundamental_from_projections,\n                         (P1, P2,), raise_exception=True)\n'"
test/geometry/epipolar/test_numeric.py,10,"b'import pytest\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\nimport kornia.geometry.epipolar as epi\n\n\nclass TestSkewSymmetric:\n    def test_smoke(self, device, dtype):\n        vec = torch.rand(1, 3, device=device, dtype=dtype)\n        cross_product_matrix = epi.cross_product_matrix(vec)\n        assert cross_product_matrix.shape == (1, 3, 3)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 4, 7])\n    def test_shape(self, batch_size, device, dtype):\n        B = batch_size\n        vec = torch.rand(B, 3, device=device, dtype=dtype)\n        cross_product_matrix = epi.cross_product_matrix(vec)\n        assert cross_product_matrix.shape == (B, 3, 3)\n\n    def test_mean_std(self, device, dtype):\n        vec = torch.tensor([[1., 2., 3.]], device=device, dtype=dtype)\n        cross_product_matrix = epi.cross_product_matrix(vec)\n        assert_allclose(cross_product_matrix[..., 0, 1], -cross_product_matrix[..., 1, 0])\n        assert_allclose(cross_product_matrix[..., 0, 2], -cross_product_matrix[..., 2, 0])\n        assert_allclose(cross_product_matrix[..., 1, 2], -cross_product_matrix[..., 2, 1])\n\n    def test_gradcheck(self, device):\n        vec = torch.ones(2, 3, device=device, requires_grad=True, dtype=torch.float64)\n        assert gradcheck(epi.cross_product_matrix, (vec,), raise_exception=True)\n\n\nclass TestEyeLike:\n    def test_smoke(self, device, dtype):\n        image = torch.rand(1, 3, 4, 4, device=device, dtype=dtype)\n        identity = epi.eye_like(3, image)\n        assert identity.shape == (1, 3, 3)\n        assert identity.device == image.device\n        assert identity.dtype == image.dtype\n\n    @pytest.mark.parametrize(\n        ""batch_size, eye_size"", [(1, 2), (2, 3), (3, 3), (2, 4)],\n    )\n    def test_shape(self, batch_size, eye_size, device, dtype):\n        B, N = batch_size, eye_size\n        image = torch.rand(B, 3, 4, 4, device=device, dtype=dtype)\n        identity = epi.eye_like(N, image)\n        assert identity.shape == (B, N, N)\n        assert identity.device == image.device\n        assert identity.dtype == image.dtype\n\n\nclass TestVecLike:\n    def test_smoke(self, device, dtype):\n        image = torch.rand(1, 3, 4, 4, device=device, dtype=dtype)\n        vec = epi.vec_like(3, image)\n        assert vec.shape == (1, 3, 1)\n        assert vec.device == image.device\n        assert vec.dtype == image.dtype\n\n    @pytest.mark.parametrize(\n        ""batch_size, eye_size"", [(1, 2), (2, 3), (3, 3), (2, 4)],\n    )\n    def test_shape(self, batch_size, eye_size, device, dtype):\n        B, N = batch_size, eye_size\n        image = torch.rand(B, 3, 4, 4, device=device, dtype=dtype)\n        vec = epi.vec_like(N, image)\n        assert vec.shape == (B, N, 1)\n        assert vec.device == image.device\n        assert vec.dtype == image.dtype\n'"
test/geometry/epipolar/test_projection.py,31,"b'import pytest\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\nimport kornia.geometry.epipolar as epi\n\n\nclass TestIntrinsicsLike:\n    def test_smoke(self, device, dtype):\n        image = torch.rand(1, 3, 4, 4, device=device, dtype=dtype)\n        focal = torch.rand(1, device=device, dtype=dtype)\n        camera_matrix = epi.intrinsics_like(focal, image)\n        assert camera_matrix.shape == (1, 3, 3)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 4, 9])\n    def test_shape(self, batch_size, device, dtype):\n        B: int = batch_size\n        focal: float = 100.0\n        image = torch.rand(B, 3, 4, 4, device=device, dtype=dtype)\n        camera_matrix = epi.intrinsics_like(focal, image)\n        assert camera_matrix.shape == (B, 3, 3)\n        assert camera_matrix.device == image.device\n        assert camera_matrix.dtype == image.dtype\n\n\nclass TestScaleIntrinsics:\n    def test_smoke_float(self, device, dtype):\n        scale_factor: float = 1.0\n        camera_matrix = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        camera_matrix_scale = epi.scale_intrinsics(camera_matrix, scale_factor)\n        assert camera_matrix_scale.shape == (1, 3, 3)\n\n    def test_smoke_tensor(self, device, dtype):\n        scale_factor = torch.tensor(1.0)\n        camera_matrix = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        camera_matrix_scale = epi.scale_intrinsics(camera_matrix, scale_factor)\n        assert camera_matrix_scale.shape == (1, 3, 3)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 4, 9])\n    def test_shape(self, batch_size, device, dtype):\n        B: int = batch_size\n        scale_factor = torch.rand(B, device=device, dtype=dtype)\n        camera_matrix = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        camera_matrix_scale = epi.scale_intrinsics(camera_matrix, scale_factor)\n        assert camera_matrix_scale.shape == (B, 3, 3)\n\n    def test_scale_double(self, device, dtype):\n        scale_factor = torch.tensor(0.5)\n        camera_matrix = torch.tensor([[\n            [100., 0., 50.],\n            [0., 100., 50.],\n            [0., 0., 1.],\n        ]], device=device, dtype=dtype)\n\n        camera_matrix_expected = torch.tensor([[\n            [50., 0., 25.],\n            [0., 50., 25.],\n            [0., 0., 1.],\n        ]], device=device, dtype=dtype)\n\n        camera_matrix_scale = epi.scale_intrinsics(camera_matrix, scale_factor)\n        assert_allclose(camera_matrix_scale, camera_matrix_expected)\n\n    def test_gradcheck(self, device):\n        scale_factor = torch.ones(1, device=device, dtype=torch.float64, requires_grad=True)\n        camera_matrix = torch.ones(1, 3, 3, device=device, dtype=torch.float64)\n        assert gradcheck(epi.scale_intrinsics,\n                         (camera_matrix, scale_factor,), raise_exception=True)\n\n\nclass TestProjectionFromKRt:\n    def test_smoke(self, device, dtype):\n        K = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        R = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        t = torch.rand(1, 3, 1, device=device, dtype=dtype)\n        P = epi.projection_from_KRt(K, R, t)\n        assert P.shape == (1, 3, 4)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 4])\n    def test_shape(self, batch_size, device, dtype):\n        B: int = batch_size\n        K = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        R = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        t = torch.rand(B, 3, 1, device=device, dtype=dtype)\n        P = epi.projection_from_KRt(K, R, t)\n        assert P.shape == (B, 3, 4)\n\n    def test_simple(self, device, dtype):\n        K = torch.tensor([[\n            [10., 0., 30.],\n            [0., 20., 40.],\n            [0., 0., 1.],\n        ]], device=device, dtype=dtype)\n\n        R = torch.tensor([[\n            [1., 0., 0.],\n            [0., 1., 0.],\n            [0., 0., 1.],\n        ]], device=device, dtype=dtype)\n\n        t = torch.tensor([\n            [[1.], [2.], [3.]],\n        ], device=device, dtype=dtype)\n\n        P_expected = torch.tensor([[\n            [10., 0., 30., 100.],\n            [0., 20., 40., 160.],\n            [0., 0., 1., 3.],\n        ]], device=device, dtype=dtype)\n\n        P_estimated = epi.projection_from_KRt(K, R, t)\n        assert_allclose(P_estimated, P_expected)\n\n    def test_gradcheck(self, device):\n        K = torch.rand(1, 3, 3, device=device, dtype=torch.float64, requires_grad=True)\n        R = torch.rand(1, 3, 3, device=device, dtype=torch.float64)\n        t = torch.rand(1, 3, 1, device=device, dtype=torch.float64)\n        assert gradcheck(epi.projection_from_KRt,\n                         (K, R, t,), raise_exception=True)\n\n\nclass TestProjectionsFromFundamental:\n    def test_smoke(self, device, dtype):\n        F_mat = torch.rand(1, 3, 3, device=device, dtype=dtype)\n        P = epi.projections_from_fundamental(F_mat)\n        assert P.shape == (1, 3, 4, 2)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 4])\n    def test_shape(self, batch_size, device, dtype):\n        B: int = batch_size\n        F_mat = torch.rand(B, 3, 3, device=device, dtype=dtype)\n        P = epi.projections_from_fundamental(F_mat)\n        assert P.shape == (B, 3, 4, 2)\n\n    def test_gradcheck(self, device):\n        F_mat = torch.rand(1, 3, 3, device=device, dtype=torch.float64, requires_grad=True)\n        assert gradcheck(epi.projections_from_fundamental,\n                         (F_mat,), raise_exception=True)\n'"
test/geometry/epipolar/test_triangulation.py,15,"b'import pytest\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\nimport kornia\nimport kornia.geometry.epipolar as epi\n\n\nclass TestTriangulation:\n    def test_smoke(self, device, dtype):\n        P1 = torch.rand(1, 3, 4, device=device, dtype=dtype)\n        P2 = torch.rand(1, 3, 4, device=device, dtype=dtype)\n        points1 = torch.rand(1, 1, 2, device=device, dtype=dtype)\n        points2 = torch.rand(1, 1, 2, device=device, dtype=dtype)\n        points3d = epi.triangulate_points(P1, P2, points1, points2)\n        assert points3d.shape == (1, 1, 3)\n\n    @pytest.mark.parametrize(""batch_size, num_points"", [(1, 3), (2, 4), (3, 5)])\n    def test_shape(self, batch_size, num_points, device, dtype):\n        B, N = batch_size, num_points\n        P1 = torch.rand(B, 3, 4, device=device, dtype=dtype)\n        P2 = torch.rand(1, 3, 4, device=device, dtype=dtype)\n        points1 = torch.rand(1, N, 2, device=device, dtype=dtype)\n        points2 = torch.rand(B, N, 2, device=device, dtype=dtype)\n        points3d = epi.triangulate_points(P1, P2, points1, points2)\n        assert points3d.shape == (B, N, 3)\n\n    def test_two_view(self, device, dtype):\n        num_views: int = 2\n        num_points: int = 10\n        scene: Dict[str, torch.Tensor] = epi.generate_scene(num_views, num_points)\n\n        P1 = scene[\'P\'][0:1]\n        P2 = scene[\'P\'][1:2]\n        x1 = scene[\'points2d\'][0:1]\n        x2 = scene[\'points2d\'][1:2]\n\n        X = epi.triangulate_points(P1, P2, x1, x2)\n        x_reprojected = kornia.transform_points(scene[\'P\'], X.expand(num_views, -1, -1))\n\n        assert_allclose(scene[\'points3d\'], X, rtol=1e-4, atol=1e-4)\n        assert_allclose(scene[\'points2d\'], x_reprojected)\n\n    def test_gradcheck(self, device):\n        points1 = torch.rand(1, 8, 2, device=device, dtype=torch.float64, requires_grad=True)\n        points2 = torch.rand(1, 8, 2, device=device, dtype=torch.float64)\n        P1 = epi.eye_like(3, points1)\n        P1 = torch.nn.functional.pad(P1, [0, 1])\n        P2 = epi.eye_like(3, points2)\n        P2 = torch.nn.functional.pad(P2, [0, 1])\n        assert gradcheck(epi.triangulate_points,\n                         (P1, P2, points1, points2,), raise_exception=True)\n'"
test/geometry/transform/test_affine.py,72,"b'import pytest\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestResize:\n    def test_smoke(self, device):\n        inp = torch.rand(1, 3, 3, 4).to(device)\n        out = kornia.resize(inp, (3, 4))\n        assert_allclose(inp, out)\n\n    def test_upsize(self, device):\n        inp = torch.rand(1, 3, 3, 4).to(device)\n        out = kornia.resize(inp, (6, 8))\n        assert out.shape == (1, 3, 6, 8)\n\n    def test_downsize(self, device):\n        inp = torch.rand(1, 3, 5, 2).to(device)\n        out = kornia.resize(inp, (3, 1))\n        assert out.shape == (1, 3, 3, 1)\n\n    def test_one_param(self, device):\n        inp = torch.rand(1, 3, 5, 2).to(device)\n        out = kornia.resize(inp, 10)\n        assert out.shape == (1, 3, 25, 10)\n\n    def test_gradcheck(self, device):\n        # test parameters\n        new_size = 4\n        input = torch.rand(1, 2, 3, 4).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.Resize(new_size), (input, ), raise_exception=True)\n\n\nclass TestRotate:\n    def test_angle90(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 2.],\n            [3., 4.],\n            [5., 6.],\n            [7., 8.],\n        ]]).to(device)\n        expected = torch.tensor([[\n            [0., 0.],\n            [4., 6.],\n            [3., 5.],\n            [0., 0.],\n        ]]).to(device)\n        # prepare transformation\n        angle = torch.tensor([90.]).to(device)\n        transform = kornia.Rotate(angle, align_corners=True)\n        assert_allclose(transform(inp), expected)\n\n    def test_angle90_batch2(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 2.],\n            [3., 4.],\n            [5., 6.],\n            [7., 8.],\n        ]]).repeat(2, 1, 1, 1).to(device)\n        expected = torch.tensor([[[\n            [0., 0.],\n            [4., 6.],\n            [3., 5.],\n            [0., 0.],\n        ]], [[\n            [0., 0.],\n            [5., 3.],\n            [6., 4.],\n            [0., 0.],\n        ]]]).to(device)\n        # prepare transformation\n        angle = torch.tensor([90., -90.]).to(device)\n        transform = kornia.Rotate(angle, align_corners=True)\n        assert_allclose(transform(inp), expected)\n\n    def test_angle90_batch2_broadcast(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 2.],\n            [3., 4.],\n            [5., 6.],\n            [7., 8.],\n        ]]).repeat(2, 1, 1, 1).to(device)\n        expected = torch.tensor([[[\n            [0., 0.],\n            [4., 6.],\n            [3., 5.],\n            [0., 0.],\n        ]], [[\n            [0., 0.],\n            [4., 6.],\n            [3., 5.],\n            [0., 0.],\n        ]]]).to(device)\n        # prepare transformation\n        angle = torch.tensor([90.]).to(device)\n        transform = kornia.Rotate(angle, align_corners=True)\n        assert_allclose(transform(inp), expected)\n\n    def test_gradcheck(self, device):\n        # test parameters\n        angle = torch.tensor([90.]).to(device)\n        angle = utils.tensor_to_gradcheck_var(\n            angle, requires_grad=False)  # to var\n\n        # evaluate function gradient\n        input = torch.rand(1, 2, 3, 4).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.rotate, (input, angle,), raise_exception=True)\n\n    @pytest.mark.skip(\'Need deep look into it since crashes everywhere.\')\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        angle = torch.tensor([90.]).to(device)\n        batch_size, channels, height, width = 2, 3, 64, 64\n        img = torch.ones(batch_size, channels, height, width).to(device)\n        rot = kornia.Rotate(angle)\n        rot_traced = torch.jit.trace(kornia.Rotate(angle), img)\n        assert_allclose(rot(img), rot_traced(img))\n\n\nclass TestTranslate:\n    def test_dxdy(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 2.],\n            [3., 4.],\n            [5., 6.],\n            [7., 8.],\n        ]]).to(device)\n        expected = torch.tensor([[\n            [0., 1.],\n            [0., 3.],\n            [0., 5.],\n            [0., 7.],\n        ]]).to(device)\n        # prepare transformation\n        translation = torch.tensor([[1., 0.]]).to(device)\n        transform = kornia.Translate(translation, align_corners=True)\n        assert_allclose(transform(inp), expected)\n\n    def test_dxdy_batch(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 2.],\n            [3., 4.],\n            [5., 6.],\n            [7., 8.],\n        ]]).repeat(2, 1, 1, 1).to(device)\n        expected = torch.tensor([[[\n            [0., 1.],\n            [0., 3.],\n            [0., 5.],\n            [0., 7.],\n        ]], [[\n            [0., 0.],\n            [0., 1.],\n            [0., 3.],\n            [0., 5.],\n        ]]]).to(device)\n        # prepare transformation\n        translation = torch.tensor([[1., 0.], [1., 1.]]).to(device)\n        transform = kornia.Translate(translation, align_corners=True)\n        assert_allclose(transform(inp), expected)\n\n    def test_dxdy_batch_broadcast(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 2.],\n            [3., 4.],\n            [5., 6.],\n            [7., 8.],\n        ]]).repeat(2, 1, 1, 1).to(device)\n        expected = torch.tensor([[[\n            [0., 1.],\n            [0., 3.],\n            [0., 5.],\n            [0., 7.],\n        ]], [[\n            [0., 1.],\n            [0., 3.],\n            [0., 5.],\n            [0., 7.],\n        ]]]).to(device)\n        # prepare transformation\n        translation = torch.tensor([[1., 0.]]).to(device)\n        transform = kornia.Translate(translation, align_corners=True)\n        assert_allclose(transform(inp), expected)\n\n    def test_gradcheck(self, device):\n        # test parameters\n        translation = torch.tensor([[1., 0.]]).to(device)\n        translation = utils.tensor_to_gradcheck_var(\n            translation, requires_grad=False)  # to var\n\n        # evaluate function gradient\n        input = torch.rand(1, 2, 3, 4).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.translate, (input, translation,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(\'Need deep look into it since crashes everywhere.\')\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        translation = torch.tensor([[1., 0.]]).to(device)\n        batch_size, channels, height, width = 2, 3, 64, 64\n        img = torch.ones(batch_size, channels, height, width).to(device)\n        trans = kornia.Translate(translation)\n        trans_traced = torch.jit.trace(kornia.Translate(translation), img)\n        assert_allclose(trans(img), trans_traced(img))\n\n\nclass TestScale:\n    def test_scale_factor_2(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [0., 0., 0., 0.],\n            [0., 1., 1., 0.],\n            [0., 1., 1., 0.],\n            [0., 0., 0., 0.]\n        ]]).to(device)\n        # prepare transformation\n        scale_factor = torch.tensor([2.]).to(device)\n        transform = kornia.Scale(scale_factor)\n        assert_allclose(transform(inp).sum().item(), 12.25)\n\n    def test_scale_factor_05(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.]\n        ]]).to(device)\n        expected = torch.tensor([[\n            [0., 0., 0., 0.],\n            [0., 1., 1., 0.],\n            [0., 1., 1., 0.],\n            [0., 0., 0., 0.]\n        ]]).to(device)\n        # prepare transformation\n        scale_factor = torch.tensor([0.5]).to(device)\n        transform = kornia.Scale(scale_factor)\n        assert_allclose(transform(inp), expected)\n\n    def test_scale_factor_05_batch2(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.]\n        ]]).repeat(2, 1, 1, 1).to(device)\n        expected = torch.tensor([[\n            [0., 0., 0., 0.],\n            [0., 1., 1., 0.],\n            [0., 1., 1., 0.],\n            [0., 0., 0., 0.]\n        ]]).to(device)\n        # prepare transformation\n        scale_factor = torch.tensor([0.5, 0.5]).to(device)\n        transform = kornia.Scale(scale_factor)\n        assert_allclose(transform(inp), expected)\n\n    def test_scale_factor_05_batch2_broadcast(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.]\n        ]]).repeat(2, 1, 1, 1).to(device)\n        expected = torch.tensor([[\n            [0., 0., 0., 0.],\n            [0., 1., 1., 0.],\n            [0., 1., 1., 0.],\n            [0., 0., 0., 0.]\n        ]]).to(device)\n        # prepare transformation\n        scale_factor = torch.tensor([0.5]).to(device)\n        transform = kornia.Scale(scale_factor)\n        assert_allclose(transform(inp), expected)\n\n    def test_gradcheck(self, device):\n        # test parameters\n        scale_factor = torch.tensor([0.5]).to(device)\n        scale_factor = utils.tensor_to_gradcheck_var(\n            scale_factor, requires_grad=False)  # to var\n\n        # evaluate function gradient\n        input = torch.rand(1, 2, 3, 4).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.scale, (input, scale_factor,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(\'Need deep look into it since crashes everywhere.\')\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        scale_factor = torch.tensor([0.5]).to(device)\n        batch_size, channels, height, width = 2, 3, 64, 64\n        img = torch.ones(batch_size, channels, height, width).to(device)\n        trans = kornia.Scale(scale_factor)\n        trans_traced = torch.jit.trace(kornia.Scale(scale_factor), img)\n        assert_allclose(trans(img), trans_traced(img))\n\n\nclass TestShear:\n    def test_shear_x(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.]\n        ]]).to(device)\n        expected = torch.tensor(\n            [[[0.75, 1., 1., 1.],\n              [0.25, 1., 1., 1.],\n              [0., 0.75, 1., 1.],\n              [0., 0.25, 1., 1.]]]).to(device)\n\n        # prepare transformation\n        shear = torch.tensor([[0.5, 0.0]]).to(device)\n        transform = kornia.Shear(shear)\n        assert_allclose(transform(inp), expected)\n\n    def test_shear_y(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.]\n        ]]).to(device)\n        expected = torch.tensor(\n            [[[0.75, 0.25, 0., 0.],\n              [1., 1., 0.75, 0.25],\n              [1., 1., 1., 1.],\n              [1., 1., 1., 1.]]]).to(device)\n\n        # prepare transformation\n        shear = torch.tensor([[0.0, 0.5]]).to(device)\n        transform = kornia.Shear(shear)\n        assert_allclose(transform(inp), expected)\n\n    def test_shear_batch2(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.]\n        ]]).repeat(2, 1, 1, 1).to(device)\n\n        expected = torch.tensor(\n            [[[[0.75, 1., 1., 1.],\n               [0.25, 1., 1., 1.],\n               [0., 0.75, 1., 1.],\n               [0., 0.25, 1., 1.]]],\n             [[[0.75, 0.25, 0., 0.],\n               [1., 1., 0.75, 0.25],\n               [1., 1., 1., 1.],\n               [1., 1., 1., 1.]]]]).to(device)\n\n        # prepare transformation\n        shear = torch.tensor([[0.5, 0.0], [0.0, 0.5]]).to(device)\n        transform = kornia.Shear(shear)\n        assert_allclose(transform(inp), expected)\n\n    def test_shear_batch2_broadcast(self, device):\n        # prepare input data\n        inp = torch.tensor([[\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.]\n        ]]).repeat(2, 1, 1, 1).to(device)\n\n        expected = torch.tensor([[[\n            [0.75, 1., 1., 1.],\n            [0.25, 1., 1., 1.],\n            [0., 0.75, 1., 1.],\n            [0., 0.25, 1., 1.]]]]).to(device)\n\n        # prepare transformation\n        shear = torch.tensor([[0.5, 0.0]]).to(device)\n        transform = kornia.Shear(shear)\n        assert_allclose(transform(inp), expected)\n\n    def test_gradcheck(self, device):\n        # test parameters\n        shear = torch.tensor([[0.5, 0.0]]).to(device)\n        shear = utils.tensor_to_gradcheck_var(\n            shear, requires_grad=False)  # to var\n\n        # evaluate function gradient\n        input = torch.rand(1, 2, 3, 4).to(device)\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n        assert gradcheck(kornia.shear, (input, shear,), raise_exception=True)\n\n    @pytest.mark.skip(\'Need deep look into it since crashes everywhere.\')\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        shear = torch.tensor([[0.5, 0.0]]).to(device)\n        batch_size, channels, height, width = 2, 3, 64, 64\n        img = torch.ones(batch_size, channels, height, width).to(device)\n        trans = kornia.Shear(shear)\n        trans_traced = torch.jit.trace(kornia.Shear(shear), img)\n        assert_allclose(trans(img), trans_traced(img))\n\n\nclass TestAffine2d:\n\n    def test_compose_affine_matrix_3x3(self, device):\n        """""" To get parameters:\n        import torchvision as tv\n        from PIL import Image\n        from torch import Tensor as T\n        import math\n        import random\n        img_size = (96,96)\n        seed = 42\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n        np.random.seed(seed)  # Numpy module.\n        random.seed(seed)  # Python random module.\n        torch.manual_seed(seed)\n        tfm = tv.transforms.RandomAffine(degrees=(-25.0,25.0),\n                                        scale=(0.6, 1.4) ,\n                                        translate=(0, 0.1),\n                                        shear=(-25., 25., -20., 20.))\n        angle, translations, scale, shear = tfm.get_params(tfm.degrees, tfm.translate,\n                                                        tfm.scale, tfm.shear, img_size)\n        print (angle, translations, scale, shear)\n        output_size = img_size\n        center = (img.size[0] * 0.5 + 0.5, img.size[1] * 0.5 + 0.5)\n\n        matrix = tv.transforms.functional._get_inverse_affine_matrix(center, angle, translations, scale, shear)\n        matrix = np.array(matrix).reshape(2,3)\n        print (matrix)\n        """"""\n        from torch import Tensor as T\n        import math\n        batch_size, ch, height, width = 1, 1, 96, 96\n        angle, translations = 6.971339922894188, (0.0, -4.0)\n        scale, shear = 0.7785685905190581, [11.8235607082617, 7.06797949691645]\n        matrix_expected = T([[1.27536969, 4.26828945e-01, -3.23493180e+01],\n                             [2.18297196e-03, 1.29424165e+00, -9.19962753e+00]])\n        center = T([float(width), float(height)]).view(1, 2) / 2. + 0.5\n        center = center.expand(batch_size, -1)\n        matrix_kornia = kornia.get_affine_matrix2d(\n            T(translations).view(-1, 2),\n            center,\n            T([scale]).view(-1),\n            T([angle]).view(-1),\n            T([math.radians(shear[0])]).view(-1, 1),\n            T([math.radians(shear[1])]).view(-1, 1))\n        matrix_kornia = matrix_kornia.inverse()[0, :2].detach().cpu()\n        assert_allclose(matrix_kornia, matrix_expected)\n'"
test/geometry/transform/test_crop.py,52,"b'from typing import Tuple\n\nimport pytest\n\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\n\nclass TestBoundingBoxInferring:\n    def test_bounding_boxes_dim_inferring(self, device):\n        boxes = torch.tensor([[\n            [1., 1.],\n            [3., 1.],\n            [3., 2.],\n            [1., 2.],\n        ]]).to(device)\n        expected_height = 2\n        expected_width = 3\n        h, w = kornia.geometry.transform.crop._infer_bounding_box(boxes)\n        assert (h, w) == (expected_height, expected_width)\n\n    def test_bounding_boxes_dim_inferring_batch(self, device):\n        boxes = torch.tensor([[\n            [1., 1.],\n            [3., 1.],\n            [3., 2.],\n            [1., 2.],\n        ], [\n            [2., 2.],\n            [4., 2.],\n            [4., 3.],\n            [2., 3.],\n        ]]).to(device)\n        expected_height = 2\n        expected_width = 3\n        h, w = kornia.geometry.transform.crop._infer_bounding_box(boxes)\n        assert (h, w) == (expected_height, expected_width)\n\n    def test_gradcheck(self, device):\n        boxes = torch.tensor([[\n            [1., 1.],\n            [3., 1.],\n            [3., 2.],\n            [1., 2.],\n        ]]).to(device)\n        boxes = utils.tensor_to_gradcheck_var(boxes)\n        assert gradcheck(kornia.kornia.geometry.transform.crop._infer_bounding_box,\n                         (boxes,), raise_exception=True)\n\n\nclass TestCropAndResize:\n    def test_crop(self, device):\n        inp = torch.tensor([[\n            [1., 2., 3., 4.],\n            [5., 6., 7., 8.],\n            [9., 10., 11., 12.],\n            [13., 14., 15., 16.],\n        ]]).to(device)\n\n        height, width = 2, 3\n        expected = torch.tensor(\n            [[[6.7222, 7.1667, 7.6111],\n              [9.3889, 9.8333, 10.2778]]]).to(device)\n\n        boxes = torch.tensor([[\n            [1., 1.],\n            [2., 1.],\n            [2., 2.],\n            [1., 2.],\n        ]]).to(device)  # 1x4x2\n\n        patches = kornia.crop_and_resize(inp, boxes, (height, width))\n        assert_allclose(patches, expected)\n\n    def test_crop_batch(self, device):\n        inp = torch.tensor([[[\n            [1., 2., 3., 4.],\n            [5., 6., 7., 8.],\n            [9., 10., 11., 12.],\n            [13., 14., 15., 16.],\n        ]], [[\n            [1., 5., 9., 13.],\n            [2., 6., 10., 14.],\n            [3., 7., 11., 15.],\n            [4., 8., 12., 16.],\n        ]]]).to(device)\n\n        height, width = 2, 2\n        expected = torch.tensor([[[\n            [6., 7.],\n            [10., 11.],\n        ]], [[\n            [7., 15.],\n            [8., 16.],\n        ]]]).to(device)\n\n        boxes = torch.tensor([[\n            [1., 1.],\n            [2., 1.],\n            [2., 2.],\n            [1., 2.],\n        ], [\n            [1., 2.],\n            [3., 2.],\n            [3., 3.],\n            [1., 3.],\n        ]]).to(device)  # 2x4x2\n\n        patches = kornia.crop_and_resize(inp, boxes, (height, width), align_corners=True)\n        assert_allclose(patches, expected)\n\n    def test_crop_batch_broadcast(self, device):\n        inp = torch.tensor([[[\n            [1., 2., 3., 4.],\n            [5., 6., 7., 8.],\n            [9., 10., 11., 12.],\n            [13., 14., 15., 16.],\n        ]], [[\n            [1., 5., 9., 13.],\n            [2., 6., 10., 14.],\n            [3., 7., 11., 15.],\n            [4., 8., 12., 16.],\n        ]]]).to(device)\n\n        height, width = 2, 2\n        expected = torch.tensor([[[\n            [6., 7.],\n            [10., 11.],\n        ]], [[\n            [6., 10.],\n            [7., 11.],\n        ]]]).to(device)\n\n        boxes = torch.tensor([[\n            [1., 1.],\n            [2., 1.],\n            [2., 2.],\n            [1., 2.],\n        ]]).to(device)  # 1x4x2\n\n        patches = kornia.crop_and_resize(inp, boxes, (height, width), align_corners=True)\n        assert_allclose(patches, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width).to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n\n        boxes = torch.tensor([[\n            [1., 1.],\n            [2., 1.],\n            [2., 2.],\n            [1., 2.],\n        ]]).to(device)  # 1x4x2\n        boxes = utils.tensor_to_gradcheck_var(\n            boxes, requires_grad=False)  # to var\n\n        crop_height, crop_width = 4, 2\n        assert gradcheck(kornia.crop_and_resize,\n                         (img, boxes, (crop_height, crop_width),),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input: torch.Tensor,\n                      boxes: torch.Tensor,\n                      size: Tuple[int, int]) -> torch.Tensor:\n            return kornia.crop_and_resize(input, boxes, size)\n        img = torch.tensor([[\n            [1., 2., 3., 4.],\n            [5., 6., 7., 8.],\n            [9., 10., 11., 12.],\n            [13., 14., 15., 16.],\n        ]]).to(device)\n        boxes = torch.tensor([[\n            [1., 1.],\n            [2., 1.],\n            [2., 2.],\n            [1., 2.],\n        ]]).to(device)  # 1x4x2\n\n        crop_height, crop_width = 4, 2\n        actual = op_script(img, boxes, (crop_height, crop_width))\n        expected = kornia.crop_and_resize(img, boxes, (crop_height, crop_width))\n        assert_allclose(actual, expected)\n\n\nclass TestCenterCrop:\n    def test_center_crop_h2_w4(self, device):\n        inp = torch.tensor([[\n            [1., 2., 3., 4.],\n            [5., 6., 7., 8.],\n            [9., 10., 11., 12.],\n            [13., 14., 15., 16.],\n        ]]).to(device)\n\n        height, width = 2, 4\n        expected = torch.tensor([[\n            [5., 6., 7., 8.],\n            [9., 10., 11., 12.],\n        ]]).to(device)\n\n        out_crop = kornia.center_crop(inp, (height, width))\n        assert_allclose(out_crop, expected)\n\n    def test_center_crop_h4_w2(self, device):\n        inp = torch.tensor([[\n            [1., 2., 3., 4.],\n            [5., 6., 7., 8.],\n            [9., 10., 11., 12.],\n            [13., 14., 15., 16.],\n        ]]).to(device)\n\n        height, width = 4, 2\n        expected = torch.tensor([[\n            [2., 3.],\n            [6., 7.],\n            [10., 11.],\n            [14., 15.],\n        ]]).to(device)\n\n        out_crop = kornia.center_crop(inp, (height, width))\n        assert_allclose(out_crop, expected)\n\n    def test_center_crop_h4_w2_batch(self, device):\n        inp = torch.tensor([[\n            [1., 2., 3., 4.],\n            [5., 6., 7., 8.],\n            [9., 10., 11., 12.],\n            [13., 14., 15., 16.],\n        ], [\n            [1., 5., 9., 13.],\n            [2., 6., 10., 14.],\n            [3., 7., 11., 15.],\n            [4., 8., 12., 16.],\n        ]]).to(device)\n\n        height, width = 4, 2\n        expected = torch.tensor([[\n            [2., 3.],\n            [6., 7.],\n            [10., 11.],\n            [14., 15.],\n        ], [\n            [5., 9.],\n            [6., 10.],\n            [7., 11.],\n            [8., 12.],\n        ]]).to(device)\n\n        out_crop = kornia.center_crop(inp, (height, width))\n        assert_allclose(out_crop, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width).to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n\n        crop_height, crop_width = 4, 2\n        assert gradcheck(kornia.center_crop, (img, (crop_height, crop_width),),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input: torch.Tensor,\n                      size: Tuple[int, int]) -> torch.Tensor:\n            return kornia.center_crop(input, size)\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.ones(batch_size, channels, height, width).to(device)\n\n        crop_height, crop_width = 4, 2\n        actual = op_script(img, (crop_height, crop_width))\n        expected = kornia.center_crop(img, (crop_height, crop_width))\n        assert_allclose(actual, expected)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit_trace(self, device):\n        @torch.jit.script\n        def op_script(input: torch.Tensor,\n                      size: Tuple[int, int]) -> torch.Tensor:\n            return kornia.center_crop(input, size)\n        # 1. Trace op\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.ones(batch_size, channels, height, width).to(device)\n\n        crop_height, crop_width = 4, 2\n        op_trace = torch.jit.trace(\n            op_script,\n            (img, (torch.tensor(crop_height), torch.tensor(crop_width))))\n\n        # 2. Generate new input\n        batch_size, channels, height, width = 2, 1, 6, 3\n        img = torch.ones(batch_size, channels, height, width).to(device)\n\n        # 3. Evaluate\n        crop_height, crop_width = 2, 3\n        actual = op_trace(\n            img, (torch.tensor(crop_height), torch.tensor(crop_width)))\n        expected = kornia.center_crop(img, (crop_height, crop_width))\n        assert_allclose(actual, expected)\n\n\nclass TestCropByBoxes:\n    def test_crop_by_boxes_no_resizing(self, device):\n        inp = torch.tensor([[\n            [1., 2., 3., 4.],\n            [5., 6., 7., 8.],\n            [9., 10., 11., 12.],\n            [13., 14., 15., 16.],\n        ]]).to(device)\n\n        src = torch.tensor([[\n            [1., 1.],\n            [2., 1.],\n            [2., 2.],\n            [1., 2.],\n        ]]).to(device)  # 1x4x2\n\n        dst = torch.tensor([[\n            [0., 0.],\n            [1., 0.],\n            [1., 1.],\n            [0., 1.],\n        ]]).to(device)  # 1x4x2\n\n        expected = torch.tensor([[\n            [6., 7.],\n            [10., 11.],\n        ]]).to(device)\n\n        patches = kornia.geometry.transform.crop_by_boxes(inp, src, dst, align_corners=True)\n        assert_allclose(patches, expected)\n\n    def test_crop_by_boxes_resizing(self, device):\n        inp = torch.tensor([[\n            [1., 2., 3., 4.],\n            [5., 6., 7., 8.],\n            [9., 10., 11., 12.],\n            [13., 14., 15., 16.],\n        ]]).to(device)\n\n        src = torch.tensor([[\n            [1., 1.],\n            [2., 1.],\n            [2., 2.],\n            [1., 2.],\n        ]]).to(device)  # 1x4x2\n\n        dst = torch.tensor([[\n            [0., 0.],\n            [2., 0.],\n            [2., 1.],\n            [0., 1.],\n        ]]).to(device)  # 1x4x2\n\n        expected = torch.tensor([[\n            [6., 6.5, 7.],\n            [10., 10.5, 11.],\n        ]]).to(device)\n\n        patches = kornia.geometry.transform.crop_by_boxes(inp, src, dst, align_corners=True)\n        assert_allclose(patches, expected)\n\n    def test_gradcheck(self, device):\n        inp = torch.randn((1, 3, 3)).to(device)\n        src = torch.tensor([[\n            [1., 0.],\n            [2., 0.],\n            [2., 1.],\n            [1., 1.]]]).to(device)\n        dst = torch.tensor([[\n            [0., 0.],\n            [1., 0.],\n            [1., 1.],\n            [0., 1.]]]).to(device)\n\n        inp = utils.tensor_to_gradcheck_var(inp, requires_grad=True)  # to var\n\n        assert gradcheck(kornia.geometry.transform.crop_by_boxes,\n                         (inp, src, dst,),\n                         raise_exception=True)\n'"
test/geometry/transform/test_flip.py,35,"b'import kornia\nimport torch\nimport pytest\n\nimport kornia.testing as utils  # test utils\n\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\n\nclass TestVflip:\n    def smoke_test(self, device):\n        f = kornia.Vflip()\n        repr = ""Vflip()""\n        assert str(f) == repr\n\n    def test_vflip(self, device):\n\n        f = kornia.Vflip()\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 3 x 3\n\n        expected = torch.tensor([[0., 1., 1.],\n                                 [0., 0., 0.],\n                                 [0., 0., 0.]]).to(device)  # 3 x 3\n\n        assert (f(input) == expected).all()\n\n    def test_batch_vflip(self, device):\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 3 x 3\n\n        input = input.repeat(2, 1, 1).to(device)  # 2 x 3 x 3\n\n        f = kornia.Vflip()\n        expected = torch.tensor([[[0., 1., 1.],\n                                  [0., 0., 0.],\n                                  [0., 0., 0.]]]).to(device)  # 1 x 3 x 3\n\n        expected = expected.repeat(2, 1, 1)  # 2 x 3 x 3\n\n        assert (f(input) == expected).all()\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> torch.Tensor:\n\n            return kornia.vflip(data)\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 3 x 3\n\n        # Build jit trace\n        op_trace = torch.jit.trace(op_script, (input, ))\n\n        # Create new inputs\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [5., 5., 0.]]).to(device)  # 3 x 3\n\n        input = input.repeat(2, 1, 1)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[5., 5., 0.],\n                                  [0., 0., 0.],\n                                  [0., 0., 0.]]]).to(device)  # 3 x 3\n\n        expected = expected.repeat(2, 1, 1)\n\n        actual = op_trace(input)\n\n        assert_allclose(actual, expected)\n\n    def test_gradcheck(self, device):\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 3 x 3\n\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n\n        assert gradcheck(kornia.Vflip(), (input,), raise_exception=True)\n\n\nclass TestHflip:\n\n    def smoke_test(self, device):\n        f = kornia.Hflip()\n        repr = ""Hflip()""\n        assert str(f) == repr\n\n    def test_hflip(self, device):\n\n        f = kornia.Hflip()\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 3 x 3\n\n        expected = torch.tensor([[0., 0., 0.],\n                                 [0., 0., 0.],\n                                 [1., 1., 0.]]).to(device)  # 3 x 3\n\n        assert (f(input) == expected).all()\n\n    def test_batch_hflip(self, device):\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 1 x 3 x 3\n\n        input = input.repeat(2, 1, 1)  # 2 x 3 x 3\n\n        f = kornia.Hflip()\n        expected = torch.tensor([[[0., 0., 0.],\n                                  [0., 0., 0.],\n                                  [1., 1., 0.]]]).to(device)  # 3 x 3\n\n        expected = expected.repeat(2, 1, 1)  # 2 x 3 x 3\n\n        assert (f(input) == expected).all()\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> torch.Tensor:\n\n            return kornia.hflip(data)\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 3 x 3\n\n        # Build jit trace\n        op_trace = torch.jit.trace(op_script, (input, ))\n\n        # Create new inputs\n        input = torch.tensor([[0., 0., 0.],\n                              [5., 5., 0.],\n                              [0., 0., 0.]]).to(device)  # 3 x 3\n\n        input = input.repeat(2, 1, 1)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[0., 0., 0.],\n                                  [0., 5., 5.],\n                                  [0., 0., 0.]]]).to(device)  # 3 x 3\n\n        expected = expected.repeat(2, 1, 1)\n\n        actual = op_trace(input)\n\n        assert_allclose(actual, expected)\n\n    def test_gradcheck(self, device):\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 3 x 3\n\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n\n        assert gradcheck(kornia.Hflip(), (input,), raise_exception=True)\n\n\nclass TestRot180:\n\n    def smoke_test(self, device):\n        f = kornia.Rot180()\n        repr = ""Rot180()""\n        assert str(f) == repr\n\n    def test_rot180(self, device):\n\n        f = kornia.Rot180()\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 3 x 3\n\n        expected = torch.tensor([[1., 1., 0.],\n                                 [0., 0., 0.],\n                                 [0., 0., 0.]]).to(device)  # 3 x 3\n\n        assert (f(input) == expected).all()\n\n    def test_batch_rot180(self, device):\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 3 x 3\n\n        input = input.repeat(2, 1, 1)  # 2 x 3 x 3\n\n        f = kornia.Rot180()\n        expected = torch.tensor([[1., 1., 0.],\n                                 [0., 0., 0.],\n                                 [0., 0., 0.]]).to(device)  # 1 x 3 x 3\n\n        expected = expected.repeat(2, 1, 1)  # 2 x 3 x 3\n\n        assert (f(input) == expected).all()\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(data: torch.Tensor) -> torch.Tensor:\n\n            return kornia.rot180(data)\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 3 x 3\n\n        # Build jit trace\n        op_trace = torch.jit.trace(op_script, (input, ))\n\n        # Create new inputs\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [5., 5., 0.]]).to(device)  # 3 x 3\n\n        input = input.repeat(2, 1, 1)  # 2 x 3 x 3\n\n        expected = torch.tensor([[[0., 5., 5.],\n                                  [0., 0., 0.],\n                                  [0., 0., 0.]]]).to(device)  # 3 x 3\n\n        expected = expected.repeat(2, 1, 1)\n\n        actual = op_trace(input)\n\n        assert_allclose(actual, expected)\n\n    def test_gradcheck(self, device):\n\n        input = torch.tensor([[0., 0., 0.],\n                              [0., 0., 0.],\n                              [0., 1., 1.]]).to(device)  # 3 x 3\n\n        input = utils.tensor_to_gradcheck_var(input)  # to var\n\n        assert gradcheck(kornia.Rot180(), (input,), raise_exception=True)\n'"
test/geometry/transform/test_imgwarp.py,64,"b'import pytest\n\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\n@pytest.mark.parametrize(""batch_shape"", [(1, 1, 7, 32), (2, 3, 16, 31)])\ndef test_warp_perspective_rotation(batch_shape, device):\n    # generate input data\n    batch_size, channels, height, width = batch_shape\n    alpha = 0.5 * kornia.pi * torch.ones(batch_size).to(device)  # 90 deg rotation\n\n    # create data patch\n    patch = torch.rand(batch_shape).to(device)\n\n    # create transformation (rotation)\n    M = torch.eye(3, device=device).repeat(batch_size, 1, 1)  # Bx3x3\n    M[:, 0, 0] = torch.cos(alpha)\n    M[:, 0, 1] = -torch.sin(alpha)\n    M[:, 1, 0] = torch.sin(alpha)\n    M[:, 1, 1] = torch.cos(alpha)\n\n    # apply transformation and inverse\n    _, _, h, w = patch.shape\n    patch_warped = kornia.warp_perspective(patch, M, dsize=(height, width), align_corners=True)\n    patch_warped_inv = kornia.warp_perspective(\n        patch_warped, torch.inverse(M), dsize=(height, width), align_corners=True)\n\n    # generate mask to compute error\n    mask = torch.ones_like(patch)\n    mask_warped_inv = kornia.warp_perspective(\n        kornia.warp_perspective(patch, M, dsize=(height, width), align_corners=True),\n        torch.inverse(M),\n        dsize=(height, width), align_corners=True)\n\n    assert_allclose(mask_warped_inv * patch,\n                    mask_warped_inv * patch_warped_inv)\n\n    # evaluate function gradient\n    patch = utils.tensor_to_gradcheck_var(patch)  # to var\n    M = utils.tensor_to_gradcheck_var(M, requires_grad=False)  # to var\n    assert gradcheck(\n        kornia.warp_perspective, (patch, M, (\n            height,\n            width,\n        )),\n        raise_exception=True)\n\n\n@pytest.mark.parametrize(""batch_size"", [1, 2, 5])\ndef test_get_perspective_transform(batch_size, device):\n    # generate input data\n    h_max, w_max = 64, 32  # height, width\n    h = torch.ceil(h_max * torch.rand(batch_size)).to(device)\n    w = torch.ceil(w_max * torch.rand(batch_size)).to(device)\n\n    norm = torch.rand(batch_size, 4, 2).to(device)\n    points_src = torch.zeros_like(norm)\n    points_src[:, 1, 0] = h\n    points_src[:, 2, 1] = w\n    points_src[:, 3, 0] = h\n    points_src[:, 3, 1] = w\n    points_dst = points_src + norm\n\n    # compute transform from source to target\n    dst_homo_src = kornia.get_perspective_transform(points_src, points_dst)\n\n    assert_allclose(\n        kornia.transform_points(dst_homo_src, points_src), points_dst)\n\n    # compute gradient check\n    points_src = utils.tensor_to_gradcheck_var(points_src)  # to var\n    points_dst = utils.tensor_to_gradcheck_var(points_dst)  # to var\n    assert gradcheck(\n        kornia.get_perspective_transform, (\n            points_src,\n            points_dst,\n        ),\n        raise_exception=True)\n\n\n@pytest.mark.parametrize(""batch_size"", [1, 2, 5])\ndef test_rotation_matrix2d(batch_size, device):\n    # generate input data\n    center_base = torch.zeros(batch_size, 2).to(device)\n    angle_base = torch.ones(batch_size).to(device)\n    scale_base = torch.ones(batch_size).to(device)\n\n    # 90 deg rotation\n    center = center_base\n    angle = 90. * angle_base\n    scale = scale_base\n    M = kornia.get_rotation_matrix2d(center, angle, scale)\n\n    for i in range(batch_size):\n        assert_allclose(M[i, 0, 0].item(), 0.0)\n        assert_allclose(M[i, 0, 1].item(), 1.0)\n        assert_allclose(M[i, 1, 0].item(), -1.0)\n        assert_allclose(M[i, 1, 1].item(), 0.0)\n\n    # 90 deg rotation + 2x scale\n    center = center_base\n    angle = 90. * angle_base\n    scale = 2. * scale_base\n    M = kornia.get_rotation_matrix2d(center, angle, scale)\n\n    for i in range(batch_size):\n        assert_allclose(M[i, 0, 0].item(), 0.0)\n        assert_allclose(M[i, 0, 1].item(), 2.0)\n        assert_allclose(M[i, 1, 0].item(), -2.0)\n        assert_allclose(M[i, 1, 1].item(), 0.0)\n\n    # 45 deg rotation\n    center = center_base\n    angle = 45. * angle_base\n    scale = scale_base\n    M = kornia.get_rotation_matrix2d(center, angle, scale)\n\n    for i in range(batch_size):\n        assert_allclose(M[i, 0, 0].item(), 0.7071)\n        assert_allclose(M[i, 0, 1].item(), 0.7071)\n        assert_allclose(M[i, 1, 0].item(), -0.7071)\n        assert_allclose(M[i, 1, 1].item(), 0.7071)\n\n    # evaluate function gradient\n    center = utils.tensor_to_gradcheck_var(center)  # to var\n    angle = utils.tensor_to_gradcheck_var(angle)  # to var\n    scale = utils.tensor_to_gradcheck_var(scale)  # to var\n    assert gradcheck(\n        kornia.get_rotation_matrix2d, (center, angle, scale),\n        raise_exception=True)\n\n\nclass TestWarpPerspective:\n    @pytest.mark.parametrize(""batch_size"", [1, 5])\n    @pytest.mark.parametrize(""channels"", [1, 5])\n    def test_crop(self, device, batch_size, channels):\n        # generate input data\n        src_h, src_w = 3, 3\n        dst_h, dst_w = 3, 3\n\n        # [x, y] origin\n        # top-left, top-right, bottom-right, bottom-left\n        points_src = torch.FloatTensor([[\n            [0, 0],\n            [0, src_w - 1],\n            [src_h - 1, src_w - 1],\n            [src_h - 1, 0],\n        ]]).to(device)\n\n        # [x, y] destination\n        # top-left, top-right, bottom-right, bottom-left\n        points_dst = torch.FloatTensor([[\n            [0, 0],\n            [0, dst_w - 1],\n            [dst_h - 1, dst_w - 1],\n            [dst_h - 1, 0],\n        ]]).to(device)\n\n        # compute transformation between points\n        dst_trans_src = kornia.get_perspective_transform(points_src,\n                                                         points_dst).expand(\n            batch_size, -1, -1)\n\n        # warp tensor\n        patch = torch.FloatTensor([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]]).expand(batch_size, channels, -1, -1).to(device)\n\n        expected = torch.tensor(\n            [[[[0.2500, 0.9167, 1.5833],\n               [2.1667, 5.1667, 6.5000],\n               [4.8333, 10.5000, 11.8333]]]]).to(device)\n\n        # warp and assert\n        patch_warped = kornia.warp_perspective(patch, dst_trans_src,\n                                               (dst_h, dst_w))\n        assert_allclose(patch_warped, expected)\n\n        # check jit\n        patch_warped_jit = kornia.jit.warp_perspective(patch, dst_trans_src,\n                                                       (dst_h, dst_w))\n        assert_allclose(patch_warped, patch_warped_jit)\n\n    def test_crop_center_resize(self, device):\n        # generate input data\n        dst_h, dst_w = 4, 4\n\n        # [x, y] origin\n        # top-left, top-right, bottom-right, bottom-left\n        points_src = torch.FloatTensor([[\n            [1, 1],\n            [1, 2],\n            [2, 2],\n            [2, 1],\n        ]]).to(device)\n\n        # [x, y] destination\n        # top-left, top-right, bottom-right, bottom-left\n        points_dst = torch.FloatTensor([[\n            [0, 0],\n            [0, dst_w - 1],\n            [dst_h - 1, dst_w - 1],\n            [dst_h - 1, 0],\n        ]]).to(device)\n\n        # compute transformation between points\n        dst_trans_src = kornia.get_perspective_transform(points_src, points_dst)\n\n        # warp tensor\n        patch = torch.FloatTensor([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]]).to(device)\n\n        expected = torch.tensor(\n            [[[[5.1667, 5.6111, 6.0556, 6.5000],\n               [6.9444, 7.3889, 7.8333, 8.2778],\n               [8.7222, 9.1667, 9.6111, 10.0556],\n               [10.5000, 10.9444, 11.3889, 11.8333]]]]).to(device)\n\n        # warp and assert\n        patch_warped = kornia.warp_perspective(patch, dst_trans_src,\n                                               (dst_h, dst_w))\n        assert_allclose(patch_warped, expected)\n\n        # check jit\n        patch_warped_jit = kornia.jit.warp_perspective(patch, dst_trans_src,\n                                                       (dst_h, dst_w))\n        assert_allclose(patch_warped, patch_warped_jit)\n\n\nclass TestWarpAffine:\n    def test_smoke(self, device):\n        batch_size, channels, height, width = 1, 2, 3, 4\n        aff_ab = torch.eye(2, 3)[None].to(device)  # 1x2x3\n        img_b = torch.rand(batch_size, channels, height, width).to(device)\n        img_a = kornia.warp_affine(img_b, aff_ab, (height, width))\n        assert img_b.shape == img_a.shape\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 5])\n    def test_translation(self, device, batch_size):\n        offset = 1.\n        channels, height, width = 1, 3, 4\n        aff_ab = torch.eye(2, 3).repeat(batch_size, 1, 1).to(device)  # Bx2x3\n        aff_ab[..., -1] += offset\n        img_b = torch.arange(float(height * width)).view(\n            1, channels, height, width).repeat(batch_size, 1, 1, 1).to(device)\n        img_a = kornia.warp_affine(img_b, aff_ab, (height, width), align_corners=True)\n        assert_allclose(img_b[..., :2, :3], img_a[..., 1:, 1:])\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 3, 4\n        aff_ab = torch.eye(2, 3)[None].to(device)  # 1x2x3\n        img_b = torch.rand(batch_size, channels, height, width).to(device)\n        aff_ab = utils.tensor_to_gradcheck_var(\n            aff_ab, requires_grad=False)  # to var\n        img_b = utils.tensor_to_gradcheck_var(img_b)  # to var\n        assert gradcheck(\n            kornia.warp_affine, (\n                img_b,\n                aff_ab,\n                (height, width),\n            ),\n            raise_exception=True)\n\n\nclass TestRemap:\n    def test_smoke(self, device):\n        height, width = 3, 4\n        input = torch.ones(1, 1, height, width).to(device)\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n        input_warped = kornia.remap(input, grid[..., 0], grid[..., 1], align_corners=True)\n        assert_allclose(input, input_warped)\n\n    def test_shift(self, device):\n        height, width = 3, 4\n        inp = torch.tensor([[[\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n        ]]]).to(device)\n        expected = torch.tensor([[[\n            [1., 1., 1., 0.],\n            [1., 1., 1., 0.],\n            [0., 0., 0., 0.],\n        ]]]).to(device)\n\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n        grid += 1.  # apply shift in both x/y direction\n\n        input_warped = kornia.remap(inp, grid[..., 0], grid[..., 1], align_corners=True)\n        assert_allclose(input_warped, expected)\n\n    def test_shift_batch(self, device):\n        height, width = 3, 4\n        inp = torch.tensor([[[\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n        ]]]).repeat(2, 1, 1, 1).to(device)\n\n        expected = torch.tensor([[[\n            [1., 1., 1., 0.],\n            [1., 1., 1., 0.],\n            [1., 1., 1., 0.],\n        ]], [[\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [0., 0., 0., 0.],\n        ]]]).to(device)\n\n        # generate a batch of grids\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n        grid = grid.repeat(2, 1, 1, 1)\n        grid[0, ..., 0] += 1.  # apply shift in the x direction\n        grid[1, ..., 1] += 1.  # apply shift in the y direction\n\n        input_warped = kornia.remap(inp, grid[..., 0], grid[..., 1], align_corners=True)\n        assert_allclose(input_warped, expected)\n\n    def test_shift_batch_broadcast(self, device):\n        height, width = 3, 4\n        inp = torch.tensor([[[\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n            [1., 1., 1., 1.],\n        ]]]).repeat(2, 1, 1, 1).to(device)\n        expected = torch.tensor([[[\n            [1., 1., 1., 0.],\n            [1., 1., 1., 0.],\n            [0., 0., 0., 0.],\n        ]]]).to(device)\n\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n        grid += 1.  # apply shift in both x/y direction\n\n        input_warped = kornia.remap(inp, grid[..., 0], grid[..., 1], align_corners=True)\n        assert_allclose(input_warped, expected)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 3, 4\n        img = torch.rand(batch_size, channels, height, width).to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n        grid = utils.tensor_to_gradcheck_var(\n            grid, requires_grad=False)  # to var\n\n        assert gradcheck(kornia.remap, (img, grid[..., 0], grid[..., 1],),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input, map1, map2):\n            return kornia.remap(input, map1, map2)\n        batch_size, channels, height, width = 1, 1, 3, 4\n        img = torch.ones(batch_size, channels, height, width).to(device)\n\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n        grid += 1.  # apply some shift\n\n        input = (img, grid[..., 0], grid[..., 1],)\n        actual = op_script(*input)\n        expected = kornia.remap(*input)\n        assert_allclose(actual, expected)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit_trace(self, device):\n        @torch.jit.script\n        def op_script(input, map1, map2):\n            return kornia.remap(input, map1, map2)\n        # 1. Trace op\n        batch_size, channels, height, width = 1, 1, 3, 4\n        img = torch.ones(batch_size, channels, height, width).to(device)\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n        grid += 1.  # apply some shift\n        input_tuple = (img, grid[..., 0], grid[..., 1])\n        op_traced = torch.jit.trace(op_script, input_tuple)\n\n        # 2. Generate different input\n        batch_size, channels, height, width = 2, 2, 2, 5\n        img = torch.ones(batch_size, channels, height, width).to(device)\n        grid = kornia.utils.create_meshgrid(\n            height, width, normalized_coordinates=False).to(device)\n        grid += 2.  # apply some shift\n\n        # 3. Apply to different input\n        input_tuple = (img, grid[..., 0], grid[..., 1])\n        actual = op_script(*input_tuple)\n        expected = kornia.remap(*input_tuple)\n        assert_allclose(actual, expected)\n\n\nclass TestInvertAffineTransform:\n    def test_smoke(self, device):\n        matrix = torch.eye(2, 3).to(device)[None]\n        matrix_inv = kornia.invert_affine_transform(matrix)\n        assert_allclose(matrix, matrix_inv)\n\n    def test_rot90(self, device):\n        angle = torch.tensor([90.]).to(device)\n        scale = torch.tensor([1.]).to(device)\n        center = torch.tensor([[0., 0.]]).to(device)\n        expected = torch.tensor([[\n            [0., -1., 0.],\n            [1., 0., 0.],\n        ]]).to(device)\n        matrix = kornia.get_rotation_matrix2d(center, angle, scale)\n        matrix_inv = kornia.invert_affine_transform(matrix)\n        assert_allclose(matrix_inv, expected)\n\n    def test_rot90_batch(self, device):\n        angle = torch.tensor([90.]).to(device)\n        scale = torch.tensor([1.]).to(device)\n        center = torch.tensor([[0., 0.]]).to(device)\n        expected = torch.tensor([[\n            [0., -1., 0.],\n            [1., 0., 0.],\n        ]]).to(device)\n        matrix = kornia.get_rotation_matrix2d(\n            center, angle, scale).repeat(2, 1, 1)\n        matrix_inv = kornia.invert_affine_transform(matrix)\n        assert_allclose(matrix_inv, expected)\n\n    def test_gradcheck(self, device):\n        matrix = torch.eye(2, 3).to(device)[None]\n        matrix = utils.tensor_to_gradcheck_var(matrix)  # to var\n        assert gradcheck(kornia.invert_affine_transform, (matrix,),\n                         raise_exception=True)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input):\n            return kornia.invert_affine_transform(input)\n        matrix = torch.eye(2, 3).to(device)\n        op_traced = torch.jit.trace(op_script, matrix)\n        actual = op_traced(matrix)\n        expected = kornia.invert_affine_transform(matrix)\n        assert_allclose(actual, expected)\n\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit_trace(self, device):\n        @torch.jit.script\n        def op_script(input):\n            return kornia.invert_affine_transform(input)\n        matrix = torch.eye(2, 3).to(device)\n        matrix_2 = torch.eye(2, 3).repeat(2, 1, 1)\n        op_traced = torch.jit.trace(op_script, matrix)\n        actual = op_traced(matrix_2)\n        expected = kornia.invert_affine_transform(matrix_2)\n        assert_allclose(actual, expected)\n'"
test/geometry/transform/test_pyramid.py,28,"b'import pytest\n\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.testing import assert_allclose\nfrom torch.autograd import gradcheck\n\n\nclass TestPyrUp:\n    def test_shape(self, device):\n        inp = torch.zeros(1, 2, 4, 4).to(device)\n        pyr = kornia.geometry.PyrUp()\n        assert pyr(inp).shape == (1, 2, 8, 8)\n\n    def test_shape_batch(self, device):\n        inp = torch.zeros(2, 2, 4, 4).to(device)\n        pyr = kornia.geometry.PyrUp()\n        assert pyr(inp).shape == (2, 2, 8, 8)\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 5, 4\n        # TODO: cuda test is not working\n        img = torch.rand(batch_size, channels, height, width)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.geometry.pyrup, (img,), raise_exception=True)\n\n    @pytest.mark.skip("""")\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input):\n            return kornia.geometry.pyrup(input)\n        img = torch.rand(2, 3, 4, 5).to(device)\n        actual = op_script(img)\n        expected = kornia.geometry.pyrup(img)\n        assert_allclose(actual, expected)\n\n\nclass TestPyrDown:\n    def test_shape(self, device):\n        inp = torch.zeros(1, 2, 4, 4).to(device)\n        pyr = kornia.geometry.PyrDown()\n        assert pyr(inp).shape == (1, 2, 2, 2)\n\n    def test_shape_batch(self, device):\n        inp = torch.zeros(2, 2, 4, 4).to(device)\n        pyr = kornia.geometry.PyrDown()\n        assert pyr(inp).shape == (2, 2, 2, 2)\n\n    def test_symmetry_preserving(self, device):\n        inp = torch.zeros(1, 1, 6, 6).to(device)\n        inp[:, :, 2:4, 2:4] = 1.0\n        pyr_out = kornia.geometry.PyrDown()(inp).squeeze()\n        assert torch.allclose(pyr_out, pyr_out.flip(0))\n        assert torch.allclose(pyr_out, pyr_out.flip(1))\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 5, 4\n        img = torch.rand(batch_size, channels, height, width).to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.geometry.pyrdown, (img,), raise_exception=True)\n\n    @pytest.mark.skip("""")\n    @pytest.mark.skip(reason=""turn off all jit for a while"")\n    def test_jit(self, device):\n        @torch.jit.script\n        def op_script(input):\n            return kornia.geometry.pyrdown(input)\n        img = torch.rand(2, 3, 4, 5).to(device)\n        actual = op_script(img)\n        expected = kornia.geometry.pyrdown(img)\n        assert_allclose(actual, expected)\n\n\nclass TestScalePyramid:\n    def test_shape_tuple(self, device):\n        inp = torch.zeros(3, 2, 12, 12).to(device)\n        SP = kornia.geometry.ScalePyramid(n_levels=1, min_size=10)\n        out = SP(inp)\n        assert len(out) == 3\n        assert len(out[0]) == 1\n        assert len(out[1]) == 1\n        assert len(out[2]) == 1\n\n    def test_shape_batch(self, device):\n        inp = torch.zeros(3, 2, 21, 21).to(device)\n        SP = kornia.geometry.ScalePyramid(n_levels=1)\n        sp, sigmas, pd = SP(inp)\n        assert sp[0].shape == (3, 1, 2, 21, 21)\n\n    def test_shape_batch_double(self, device):\n        inp = torch.zeros(3, 2, 12, 12).to(device)\n        SP = kornia.geometry.ScalePyramid(n_levels=1, double_image=True)\n        sp, sigmas, pd = SP(inp)\n        assert sp[0].shape == (3, 1, 2, 24, 24)\n\n    def test_n_levels_shape(self, device):\n        inp = torch.zeros(1, 1, 32, 32).to(device)\n        SP = kornia.geometry.ScalePyramid(n_levels=3)\n        sp, sigmas, pd = SP(inp)\n        assert sp[0].shape == (1, 3, 1, 32, 32)\n\n    def test_blur_order(self, device):\n        inp = torch.rand(1, 1, 21, 21).to(device)\n        SP = kornia.geometry.ScalePyramid(n_levels=3)\n        sp, sigmas, pd = SP(inp)\n        for i, pyr_level in enumerate(sp):\n            for ii, img in enumerate(pyr_level):\n                img = img.squeeze().view(3, -1)\n                max_per_blur_level_val, _ = img.max(dim=1)\n                assert torch.argmax(max_per_blur_level_val).item() == 0\n        return\n\n    def test_symmetry_preserving(self, device):\n        inp = torch.zeros(1, 1, 22, 22).to(device)\n        inp[0, 0, 8:14, 8:14] = 1.0\n        SP = kornia.geometry.ScalePyramid(n_levels=3)\n        sp, sigmas, pd = SP(inp)\n        for i, pyr_level in enumerate(sp):\n            for ii, img in enumerate(pyr_level):\n                img = img.squeeze()\n                assert torch.allclose(img, img.flip(1))\n                assert torch.allclose(img, img.flip(2))\n        return\n\n    def test_gradcheck(self, device):\n        batch_size, channels, height, width = 1, 2, 7, 9\n        # TODO: cuda test is not working\n        img = torch.rand(batch_size, channels, height, width)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        from kornia.geometry import ScalePyramid as SP\n\n        def sp_tuple(img):\n            sp, sigmas, pd = SP()(img)\n            return tuple(sp)\n        assert gradcheck(sp_tuple, (img,), raise_exception=True)\n\n\nclass TestBuildPyramid:\n    def test_smoke(self, device):\n        input = torch.ones(1, 2, 4, 5).to(device)\n        pyramid = kornia.build_pyramid(input, max_level=1)\n        assert len(pyramid) == 1\n        assert pyramid[0].shape == (1, 2, 4, 5)\n\n    @pytest.mark.parametrize(""batch_size"", (1, 2, 3))\n    @pytest.mark.parametrize(""channels"", (1, 3))\n    @pytest.mark.parametrize(""max_level"", (2, 3, 4))\n    def test_num_levels(self, device, batch_size, channels, max_level):\n        height, width = 16, 20\n        input = torch.rand(batch_size, channels, height, width).to(device)\n        pyramid = kornia.build_pyramid(input, max_level)\n        assert len(pyramid) == max_level\n        for i in range(1, max_level):\n            img = pyramid[i]\n            denom = 2 ** i\n            expected_shape = (batch_size, channels, height // denom, width // denom)\n            assert img.shape == expected_shape\n\n    def test_gradcheck(self, device):\n        max_level = 1\n        batch_size, channels, height, width = 1, 2, 7, 9\n        img = torch.rand(batch_size, channels, height, width).to(device)\n        img = utils.tensor_to_gradcheck_var(img)  # to var\n        assert gradcheck(kornia.build_pyramid, (img, max_level,), raise_exception=True)\n'"
test/geometry/warp/test_depth_warper.py,9,"b'import pytest\n\nimport kornia\nimport kornia.testing as utils  # test utils\nfrom kornia.geometry.conversions import normalize_pixel_coordinates\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestDepthWarper:\n    eps = 1e-6\n\n    def _create_pinhole_pair(self, batch_size):\n        # prepare data\n        fx, fy = 1., 1.\n        height, width = 3, 5\n        cx, cy = width / 2, height / 2\n        tx, ty, tz = 0, 0, 0\n\n        # create pinhole cameras\n        pinhole_src = kornia.PinholeCamera.from_parameters(\n            fx, fy, cx, cy, height, width, tx, ty, tz, batch_size)\n        pinhole_dst = kornia.PinholeCamera.from_parameters(\n            fx, fy, cx, cy, height, width, tx, ty, tz, batch_size)\n        return pinhole_src, pinhole_dst\n\n    @pytest.mark.parametrize(""batch_size"", (1, 2,))\n    def test_compute_projection_matrix(self, batch_size):\n        height, width = 3, 5  # output shape\n        pinhole_src, pinhole_dst = self._create_pinhole_pair(batch_size)\n        pinhole_dst.tx += 1.  # apply offset to tx\n\n        # create warper\n        warper = kornia.DepthWarper(pinhole_dst, height, width)\n        assert warper._dst_proj_src is None\n\n        # initialize projection matrices\n        warper.compute_projection_matrix(pinhole_src)\n        assert warper._dst_proj_src is not None\n\n        # retreive computed projection matrix and compare to expected\n        dst_proj_src = warper._dst_proj_src\n        dst_proj_src_expected = torch.eye(\n            4)[None].repeat(batch_size, 1, 1)  # Bx4x4\n        dst_proj_src_expected[..., 0, -2] += pinhole_src.cx\n        dst_proj_src_expected[..., 1, -2] += pinhole_src.cy\n        dst_proj_src_expected[..., 0, -1] += 1.  # offset to x-axis\n        assert_allclose(dst_proj_src, dst_proj_src_expected)\n\n    @pytest.mark.parametrize(""batch_size"", (1, 2,))\n    def test_warp_grid_offset_x1_depth1(self, device, batch_size):\n        height, width = 3, 5  # output shape\n        pinhole_src, pinhole_dst = self._create_pinhole_pair(batch_size)\n        pinhole_dst.tx += 1.  # apply offset to tx\n\n        # initialize depth to one\n        depth_src = torch.ones(batch_size, 1, height, width)\n\n        # create warper, initialize projection matrices and warp grid\n        warper = kornia.DepthWarper(pinhole_dst, height, width)\n        warper.compute_projection_matrix(pinhole_src)\n\n        grid_warped = warper.warp_grid(depth_src)\n        assert grid_warped.shape == (batch_size, height, width, 2)\n\n        # normalize base meshgrid\n        grid = warper.grid[..., :2]\n        grid_norm = normalize_pixel_coordinates(grid, height, width)\n\n        # check offset in x-axis\n        assert_allclose(\n            grid_warped[..., -2, 0], grid_norm[..., -1, 0])\n        # check that y-axis remain the same\n        assert_allclose(\n            grid_warped[..., -1, 1], grid_norm[..., -1, 1])\n\n    @pytest.mark.parametrize(""batch_size"", (1, 2,))\n    def test_warp_grid_offset_x1y1_depth1(self, device, batch_size):\n        height, width = 3, 5  # output shape\n        pinhole_src, pinhole_dst = self._create_pinhole_pair(batch_size)\n        pinhole_dst.tx += 1.  # apply offset to tx\n        pinhole_dst.ty += 1.  # apply offset to ty\n\n        # initialize depth to one\n        depth_src = torch.ones(batch_size, 1, height, width)\n\n        # create warper, initialize projection matrices and warp grid\n        warper = kornia.DepthWarper(pinhole_dst, height, width)\n        warper.compute_projection_matrix(pinhole_src)\n\n        grid_warped = warper.warp_grid(depth_src)\n        assert grid_warped.shape == (batch_size, height, width, 2)\n\n        # normalize base meshgrid\n        grid = warper.grid[..., :2]\n        grid_norm = normalize_pixel_coordinates(grid, height, width)\n\n        # check offset in x-axis\n        assert_allclose(\n            grid_warped[..., -2, 0], grid_norm[..., -1, 0])\n        # check that y-axis remain the same\n        assert_allclose(\n            grid_warped[..., -2, :, 1], grid_norm[..., -1, :, 1])\n\n    @pytest.mark.parametrize(""batch_size"", (1, 2,))\n    def test_warp_tensor_offset_x1y1(self, device, batch_size):\n        channels, height, width = 3, 3, 5  # output shape\n        pinhole_src, pinhole_dst = self._create_pinhole_pair(batch_size)\n        pinhole_dst.tx += 1.  # apply offset to tx\n        pinhole_dst.ty += 1.  # apply offset to ty\n\n        # initialize depth to one\n        depth_src = torch.ones(batch_size, 1, height, width)\n\n        # create warper, initialize projection matrices and warp grid\n        warper = kornia.DepthWarper(pinhole_dst, height, width)\n        warper.compute_projection_matrix(pinhole_src)\n\n        # create patch to warp\n        patch_dst = torch.arange(float(height * width)).view(\n            1, 1, height, width).expand(batch_size, channels, -1, -1)\n\n        # warpd source patch by depth\n        patch_src = warper(depth_src, patch_dst)\n\n        # compare patches\n        assert_allclose(\n            patch_dst[..., 1:, 1:], patch_src[..., :2, :4])\n\n    @pytest.mark.parametrize(""batch_size"", (1, 2,))\n    def test_compute_projection(self, device, batch_size):\n        height, width = 3, 5  # output shape\n        pinhole_src, pinhole_dst = self._create_pinhole_pair(batch_size)\n\n        # create warper, initialize projection matrices and warp grid\n        warper = kornia.DepthWarper(pinhole_dst, height, width)\n        warper.compute_projection_matrix(pinhole_src)\n\n        # test compute_projection\n        xy_projected = warper._compute_projection(0.0, 0.0, 1.0)\n        assert xy_projected.shape == (batch_size, 2)\n\n    @pytest.mark.parametrize(""batch_size"", (1, 2,))\n    def test_compute_subpixel_step(self, device, batch_size):\n        height, width = 3, 5  # output shape\n        pinhole_src, pinhole_dst = self._create_pinhole_pair(batch_size)\n\n        # create warper, initialize projection matrices and warp grid\n        warper = kornia.DepthWarper(pinhole_dst, height, width)\n        warper.compute_projection_matrix(pinhole_src)\n\n        # test compute_subpixel_step\n        subpixel_step = warper.compute_subpixel_step()\n        assert pytest.approx(subpixel_step.item(), 0.3536)\n\n    @pytest.mark.parametrize(""batch_size"", (1, 2))\n    def test_gradcheck(self, device, batch_size):\n        # prepare data\n        channels, height, width = 3, 3, 5  # output shape\n        pinhole_src, pinhole_dst = self._create_pinhole_pair(batch_size)\n\n        # initialize depth to one\n        depth_src = torch.ones(batch_size, 1, height, width)\n        depth_src = utils.tensor_to_gradcheck_var(depth_src)  # to var\n\n        # create patch to warp\n        img_dst = torch.ones(batch_size, channels, height, width)\n        img_dst = utils.tensor_to_gradcheck_var(img_dst)  # to var\n\n        # evaluate function gradient\n        assert gradcheck(kornia.depth_warp,\n                         (pinhole_dst,\n                          pinhole_src,\n                          depth_src,\n                          img_dst,\n                          height,\n                          width,\n                          ),\n                         raise_exception=True)\n\n    # TODO(edgar): we should include a test showing some kind of occlusion\n    # def test_warp_with_occlusion(self):\n    #    pass\n'"
test/geometry/warp/test_homography_warper.py,20,"b'import pytest\n\nimport kornia as kornia\nimport kornia.testing as utils  # test utils\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\n\nclass TestHomographyWarper:\n\n    num_tests = 10\n    threshold = 0.1\n\n    def test_identity(self, device):\n        # create input data\n        height, width = 2, 5\n        patch_src = torch.rand(1, 1, height, width).to(device)\n        dst_homo_src = utils.create_eye_batch(batch_size=1, eye_size=3).to(device)\n\n        # instantiate warper\n        warper = kornia.HomographyWarper(height, width, align_corners=True)\n\n        # warp from source to destination\n        patch_dst = warper(patch_src, dst_homo_src)\n        assert_allclose(patch_src, patch_dst)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 3])\n    def test_normalize_homography_identity(self, batch_size, device):\n        # create input data\n        height, width = 2, 5\n        dst_homo_src = utils.create_eye_batch(batch_size=batch_size, eye_size=3).to(device)\n\n        res = torch.tensor([[[0.5, 0.0, -1.0],\n                             [0.0, 2.0, -1.0],\n                             [0.0, 0.0, 1.0]]])\n        assert (kornia.normal_transform_pixel(height, width) == res).all()\n\n        norm_homo = kornia.normalize_homography(dst_homo_src, (height, width), (height, width))\n        assert (norm_homo == dst_homo_src).all()\n\n        norm_homo = kornia.normalize_homography(dst_homo_src,\n                                                (height, width),\n                                                (height, width))\n        assert (norm_homo == dst_homo_src).all()\n\n        # change output scale\n        norm_homo = kornia.normalize_homography(dst_homo_src,\n                                                (height, width),\n                                                (height * 2, width // 2))\n        res = torch.tensor([[[4.0, 0.0, 3.0],\n                             [0.0, 1 / 3, -2 / 3],\n                             [0.0, 0.0, 1.0]]]).to(device)\n        assert_allclose(norm_homo, res)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 3])\n    def test_normalize_homography_general(self, batch_size, device):\n        # create input data\n        height, width = 2, 5\n        dst_homo_src = torch.eye(3).to(device)\n        dst_homo_src[..., 0, 0] = 0.5\n        dst_homo_src[..., 1, 1] = 2.0\n        dst_homo_src[..., 0, 2] = 1.0\n        dst_homo_src[..., 1, 2] = 2.0\n        dst_homo_src = dst_homo_src.expand(batch_size, -1, -1)\n\n        norm_homo = kornia.normalize_homography(dst_homo_src, (height, width), (height, width))\n        res = torch.tensor([[[0.5, 0.0, 0.0],\n                             [0.0, 2.0, 5.0],\n                             [0.0, 0.0, 1.0]]]).to(device)\n        assert (norm_homo == res).all()\n\n    @pytest.mark.parametrize(""offset"", [1, 3, 7])\n    @pytest.mark.parametrize(""shape"", [\n        (4, 5), (2, 6), (4, 3), (5, 7), ])\n    def test_warp_grid_translation(self, shape, offset, device):\n        # create input data\n        height, width = shape\n        dst_homo_src = utils.create_eye_batch(batch_size=1, eye_size=3).to(device)\n        dst_homo_src[..., 0, 2] = offset  # apply offset in x\n        grid = kornia.create_meshgrid(height, width, normalized_coordinates=False)\n        flow = kornia.warp_grid(grid, dst_homo_src)\n\n        # the grid the src plus the offset should be equal to the flow\n        # on the x-axis, y-axis remains the same.\n        assert_allclose(\n            grid[..., 0].to(device) + offset, flow[..., 0])\n        assert_allclose(\n            grid[..., 1].to(device), flow[..., 1])\n\n    @pytest.mark.parametrize(""batch_shape"", [\n        (1, 1, 4, 5), (2, 2, 4, 6), (3, 1, 5, 7), ])\n    def test_identity_resize(self, device, batch_shape):\n        # create input data\n        batch_size, channels, height, width = batch_shape\n        patch_src = torch.rand(batch_size, channels, height, width).to(device)\n        dst_homo_src = utils.create_eye_batch(batch_size, eye_size=3).to(device)\n\n        # instantiate warper warp from source to destination\n        warper = kornia.HomographyWarper(height // 2, width // 2, align_corners=True)\n        patch_dst = warper(patch_src, dst_homo_src)\n\n        # check the corners\n        assert_allclose(\n            patch_src[..., 0, 0], patch_dst[..., 0, 0])\n        assert_allclose(\n            patch_src[..., 0, -1], patch_dst[..., 0, -1])\n        assert_allclose(\n            patch_src[..., -1, 0], patch_dst[..., -1, 0])\n        assert_allclose(\n            patch_src[..., -1, -1], patch_dst[..., -1, -1])\n\n    @pytest.mark.parametrize(""shape"", [\n        (4, 5), (2, 6), (4, 3), (5, 7), ])\n    def test_translation(self, device, shape):\n        # create input data\n        offset = 2.  # in pixel\n        height, width = shape\n        patch_src = torch.rand(1, 1, height, width).to(device)\n        dst_homo_src = utils.create_eye_batch(batch_size=1, eye_size=3).to(device)\n        dst_homo_src[..., 0, 2] = offset / (width - 1)  # apply offset in x\n\n        # instantiate warper and from source to destination\n        warper = kornia.HomographyWarper(height, width, align_corners=True)\n        patch_dst = warper(patch_src, dst_homo_src)\n        assert_allclose(patch_src[..., 1:], patch_dst[..., :-1])\n\n    @pytest.mark.parametrize(""batch_shape"", [\n        (1, 1, 3, 5), (2, 2, 4, 3), (3, 1, 2, 3), ])\n    def test_rotation(self, device, batch_shape):\n        # create input data\n        batch_size, channels, height, width = batch_shape\n        patch_src = torch.rand(batch_size, channels, height, width).to(device)\n        # rotation of 90deg\n        dst_homo_src = torch.eye(3).to(device)\n        dst_homo_src[..., 0, 0] = 0.0\n        dst_homo_src[..., 0, 1] = 1.0\n        dst_homo_src[..., 1, 0] = -1.0\n        dst_homo_src[..., 1, 1] = 0.0\n        dst_homo_src = dst_homo_src.expand(batch_size, -1, -1)\n\n        # instantiate warper and warp from source to destination\n        warper = kornia.HomographyWarper(height, width, align_corners=True)\n        patch_dst = warper(patch_src, dst_homo_src)\n\n        # check the corners\n        assert_allclose(\n            patch_src[..., 0, 0], patch_dst[..., 0, -1])\n        assert_allclose(\n            patch_src[..., 0, -1], patch_dst[..., -1, -1])\n        assert_allclose(\n            patch_src[..., -1, 0], patch_dst[..., 0, 0])\n        assert_allclose(\n            patch_src[..., -1, -1], patch_dst[..., -1, 0])\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 3])\n    def test_homography_warper(self, device, batch_size):\n        # generate input data\n        height, width = 128, 64\n        eye_size = 3  # identity 3x3\n\n        # create checkerboard\n        board = utils.create_checkerboard(height, width, 4)\n        patch_src = torch.from_numpy(board).view(\n            1, 1, height, width).expand(batch_size, 1, height, width).to(device)\n\n        # create base homography\n        dst_homo_src = utils.create_eye_batch(batch_size, eye_size).to(device)\n\n        # instantiate warper\n        warper = kornia.HomographyWarper(height, width, align_corners=True)\n\n        for i in range(self.num_tests):\n            # generate homography noise\n            homo_delta = torch.rand_like(dst_homo_src) * 0.3\n\n            dst_homo_src_i = dst_homo_src + homo_delta\n\n            # transform the points from dst to ref\n            patch_dst = warper(patch_src, dst_homo_src_i)\n            patch_dst_to_src = warper(patch_dst, torch.inverse(dst_homo_src_i))\n\n            # same transform precomputing the grid\n            warper.precompute_warp_grid(torch.inverse(dst_homo_src_i))\n            patch_dst_to_src_precomputed = warper(patch_dst)\n            assert (patch_dst_to_src_precomputed == patch_dst_to_src).all()\n\n            # projected should be equal as initial\n            error = utils.compute_patch_error(\n                patch_src, patch_dst_to_src, height, width)\n\n            assert error.item() < self.threshold\n\n            # check functional api\n            patch_dst_to_src_functional = kornia.homography_warp(\n                patch_dst, torch.inverse(dst_homo_src_i), (height, width), align_corners=True)\n\n            assert_allclose(\n                patch_dst_to_src, patch_dst_to_src_functional)\n\n    @pytest.mark.parametrize(""batch_shape"", [\n        (1, 1, 7, 5), (2, 3, 8, 5), (1, 1, 7, 16), ])\n    def test_gradcheck(self, device, batch_shape):\n        # generate input data\n        eye_size = 3  # identity 3x3\n\n        # create checkerboard\n        patch_src = torch.rand(batch_shape).to(device)\n        patch_src = utils.tensor_to_gradcheck_var(patch_src)  # to var\n\n        # create base homography\n        batch_size, _, height, width = patch_src.shape\n        dst_homo_src = utils.create_eye_batch(batch_size, eye_size).to(device)\n        dst_homo_src = utils.tensor_to_gradcheck_var(\n            dst_homo_src, requires_grad=False)  # to var\n\n        # instantiate warper\n        warper = kornia.HomographyWarper(height, width, align_corners=True)\n\n        # evaluate function gradient\n        assert gradcheck(warper, (patch_src, dst_homo_src,),\n                         raise_exception=True)\n\n    @pytest.mark.parametrize(""batch_size"", [1, 2, 3])\n    @pytest.mark.parametrize(""align_corners"", [True, False])\n    @pytest.mark.parametrize(""normalized_coordinates"", [True, False])\n    def test_jit_warp_homography(self, device, batch_size, align_corners, normalized_coordinates):\n        # generate input data\n        height, width = 128, 64\n        eye_size = 3  # identity 3x3\n\n        # create checkerboard\n        board = utils.create_checkerboard(height, width, 4)\n        patch_src = torch.from_numpy(board).view(1, 1, height, width).expand(batch_size, 1, height,\n                                                                             width).to(device)\n\n        # create base homography\n        dst_homo_src = utils.create_eye_batch(batch_size, eye_size).to(device)\n\n        for i in range(self.num_tests):\n            # generate homography noise\n            homo_delta = torch.rand_like(dst_homo_src) * 0.3\n\n            dst_homo_src_i = dst_homo_src + homo_delta\n\n            # transform the points with and without jit\n            patch_dst = kornia.homography_warp(\n                patch_src, dst_homo_src_i, (height, width), align_corners=align_corners,\n                normalized_coordinates=normalized_coordinates)\n            patch_dst_jit = torch.jit.script(kornia.homography_warp)(\n                patch_src, dst_homo_src_i, (height, width), align_corners=align_corners,\n                normalized_coordinates=normalized_coordinates)\n\n            assert_allclose(patch_dst, patch_dst_jit)\n'"
