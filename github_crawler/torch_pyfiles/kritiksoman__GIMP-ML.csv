file_path,api_count,code
gimp-plugins/colorize.py,6,"b'import os \nbaseLoc = os.path.dirname(os.path.realpath(__file__))+\'/\'\n\n\nfrom gimpfu import *\nimport sys\n\nsys.path.extend([baseLoc+\'gimpenv/lib/python2.7\',baseLoc+\'gimpenv/lib/python2.7/site-packages\',baseLoc+\'gimpenv/lib/python2.7/site-packages/setuptools\',baseLoc+\'neural-colorization\'])\n\n\nimport torch\nfrom model import generator\nfrom torch.autograd import Variable\nfrom scipy.ndimage import zoom\nfrom PIL import Image\nfrom argparse import Namespace\nimport numpy as np\n# from skimage.color import rgb2yuv,yuv2rgb\nimport cv2\n\ndef getcolor(input_image):\n    p = np.repeat(input_image, 3, axis=2)\n\n    if torch.cuda.is_available():\n        g_available=1\n    else:\n        g_available=-1\n\n    args=Namespace(model=baseLoc+\'neural-colorization/model.pth\',gpu=g_available)\n\n    G = generator()\n\n    if torch.cuda.is_available():\n        G=G.cuda()\n        G.load_state_dict(torch.load(args.model))\n    else:\n        G.load_state_dict(torch.load(args.model,map_location=torch.device(\'cpu\')))\n\n    p = p.astype(np.float32)\n    p = p / 255\n    img_yuv = cv2.cvtColor(p, cv2.COLOR_RGB2YUV)\n    # img_yuv = rgb2yuv(p)\n    H,W,_ = img_yuv.shape\n    infimg = np.expand_dims(np.expand_dims(img_yuv[...,0], axis=0), axis=0)\n    img_variable = Variable(torch.Tensor(infimg-0.5))\n    if args.gpu>=0:\n        img_variable=img_variable.cuda(args.gpu)\n    res = G(img_variable)\n    uv=res.cpu().detach().numpy()\n    uv[:,0,:,:] *= 0.436\n    uv[:,1,:,:] *= 0.615\n    (_,_,H1,W1) = uv.shape\n    uv = zoom(uv,(1,1,float(H)/H1,float(W)/W1))\n    yuv = np.concatenate([infimg,uv],axis=1)[0]\n    # rgb=yuv2rgb(yuv.transpose(1,2,0))\n    # out=(rgb.clip(min=0,max=1)*255)[:,:,[0,1,2]]\n    rgb = cv2.cvtColor(yuv.transpose(1, 2, 0)*255, cv2.COLOR_YUV2RGB)\n    rgb = rgb.clip(min=0,max=255)\n    out = rgb.astype(np.uint8)\n\n    return out\n\ndef channelData(layer):#convert gimp image to numpy\n    region=layer.get_pixel_rgn(0, 0, layer.width,layer.height)\n    pixChars=region[:,:] # Take whole layer\n    bpp=region.bpp\n    # return np.frombuffer(pixChars,dtype=np.uint8).reshape(len(pixChars)/bpp,bpp)\n    return np.frombuffer(pixChars,dtype=np.uint8).reshape(layer.height,layer.width,bpp)\n\ndef createResultLayer(image,name,result):\n    rlBytes=np.uint8(result).tobytes();\n    rl=gimp.Layer(image,name,image.width,image.height,image.active_layer.type,100,NORMAL_MODE)\n    region=rl.get_pixel_rgn(0, 0, rl.width,rl.height,True)\n    region[:,:]=rlBytes\n    image.add_layer(rl,0)\n    gimp.displays_flush()\n\ndef genNewImg(name,layer_np):\n    h,w,d=layer_np.shape\n    img=pdb.gimp_image_new(w, h, RGB)\n    display=pdb.gimp_display_new(img)\n\n    rlBytes=np.uint8(layer_np).tobytes();\n    rl=gimp.Layer(img,name,img.width,img.height,RGB,100,NORMAL_MODE)\n    region=rl.get_pixel_rgn(0, 0, rl.width,rl.height,True)\n    region[:,:]=rlBytes\n\n    pdb.gimp_image_insert_layer(img, rl, None, 0)\n    gimp.displays_flush()\n\ndef colorize(img, layer) :\n    gimp.progress_init(""Coloring "" + layer.name + ""..."")\n\n    imgmat = channelData(layer)\n    cpy=getcolor(imgmat)\n\n    genNewImg(layer.name+\'_colored\',cpy)\n\n    \n\nregister(\n    ""colorize"",\n    ""colorize"",\n    ""Generate monocular disparity map based on deep learning."",\n    ""Kritik Soman"",\n    ""Your"",\n    ""2020"",\n    ""colorize..."",\n    ""*"",      # Alternately use RGB, RGB*, GRAY*, INDEXED etc.\n    [   (PF_IMAGE, ""image"", ""Input image"", None),\n        (PF_DRAWABLE, ""drawable"", ""Input drawable"", None),\n    ],\n    [],\n    colorize, menu=""<Image>/Layer/GIML-ML"")\n\nmain()\n'"
gimp-plugins/deblur.py,0,"b'import os \nbaseLoc = os.path.dirname(os.path.realpath(__file__))+\'/\'\n\n\nfrom gimpfu import *\nimport sys\nsys.path.extend([baseLoc+\'gimpenv/lib/python2.7\',baseLoc+\'gimpenv/lib/python2.7/site-packages\',baseLoc+\'gimpenv/lib/python2.7/site-packages/setuptools\',baseLoc+\'DeblurGANv2\'])\n\nimport cv2\nfrom predictorClass import Predictor\nimport numpy as np\n\ndef channelData(layer):#convert gimp image to numpy\n    region=layer.get_pixel_rgn(0, 0, layer.width,layer.height)\n    pixChars=region[:,:] # Take whole layer\n    bpp=region.bpp\n    # return np.frombuffer(pixChars,dtype=np.uint8).reshape(len(pixChars)/bpp,bpp)\n    return np.frombuffer(pixChars,dtype=np.uint8).reshape(layer.height,layer.width,bpp)\n\ndef createResultLayer(image,name,result):\n    rlBytes=np.uint8(result).tobytes();\n    rl=gimp.Layer(image,name,image.width,image.height,image.active_layer.type,100,NORMAL_MODE)\n    region=rl.get_pixel_rgn(0, 0, rl.width,rl.height,True)\n    region[:,:]=rlBytes\n    image.add_layer(rl,0)\n    gimp.displays_flush()\n\ndef getdeblur(img):\n    predictor = Predictor(weights_path=baseLoc+\'DeblurGANv2/\'+\'best_fpn.h5\')\n    pred = predictor(img, None)\n    return pred\n\ndef deblur(img, layer):\n    gimp.progress_init(""Running for "" + layer.name + ""..."")\n    imgmat = channelData(layer)\n    pred = getdeblur(imgmat)\n    createResultLayer(img,\'deblur_\'+layer.name,pred)\n\n\nregister(\n    ""deblur"",\n    ""deblur"",\n    ""Running deblurring."",\n    ""Kritik Soman"",\n    ""Your"",\n    ""2020"",\n    ""deblur..."",\n    ""*"",      # Alternately use RGB, RGB*, GRAY*, INDEXED etc.\n    [   (PF_IMAGE, ""image"", ""Input image"", None),\n        (PF_DRAWABLE, ""drawable"", ""Input drawable"", None),\n    ],\n    [],\n    deblur, menu=""<Image>/Layer/GIML-ML"")\n\nmain()\n'"
gimp-plugins/deeplabv3.py,6,"b'import os \nbaseLoc = os.path.dirname(os.path.realpath(__file__))+\'/\'\n\n\nfrom gimpfu import *\nimport sys\nsys.path.extend([baseLoc+\'gimpenv/lib/python2.7\',baseLoc+\'gimpenv/lib/python2.7/site-packages\',baseLoc+\'gimpenv/lib/python2.7/site-packages/setuptools\'])\n\n\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms, datasets\nimport numpy as np\n\ndef getSeg(input_image):\n    model = torch.load(baseLoc+\'deeplabv3/deeplabv3+model.pt\')\n    model.eval()\n    preprocess = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    input_image = Image.fromarray(input_image)\n    input_tensor = preprocess(input_image)\n    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n    if torch.cuda.is_available():\n        input_batch = input_batch.to(\'cuda\')\n        model.to(\'cuda\')\n\n    with torch.no_grad():\n        output = model(input_batch)[\'out\'][0]\n    output_predictions = output.argmax(0)\n\n\n    # create a color pallette, selecting a color for each class\n    palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n    colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n    colors = (colors % 255).numpy().astype(""uint8"")\n\n    # plot the semantic segmentation predictions of 21 classes in each color\n    r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n\n    tmp = np.array(r)\n    tmp2 = 10*np.repeat(tmp[:, :, np.newaxis], 3, axis=2)\n\n\n    return  tmp2\n\n\ndef channelData(layer):#convert gimp image to numpy\n    region=layer.get_pixel_rgn(0, 0, layer.width,layer.height)\n    pixChars=region[:,:] # Take whole layer\n    bpp=region.bpp\n    return np.frombuffer(pixChars,dtype=np.uint8).reshape(layer.height,layer.width,bpp)\n\ndef createResultLayer(image,name,result):\n    rlBytes=np.uint8(result).tobytes();\n    rl=gimp.Layer(image,name,image.width,image.height,image.active_layer.type,100,NORMAL_MODE)\n    region=rl.get_pixel_rgn(0, 0, rl.width,rl.height,True)\n    region[:,:]=rlBytes\n    image.add_layer(rl,0)\n    gimp.displays_flush()\n\ndef deeplabv3(img, layer) :\n    if torch.cuda.is_available():\n        gimp.progress_init(""(Using GPU) Generating semantic segmentation map for "" + layer.name + ""..."")\n    else:\n        gimp.progress_init(""(Using CPU) Generating semantic segmentation map for "" + layer.name + ""..."")\n\n    imgmat = channelData(layer) \n    cpy=getSeg(imgmat)    \n    createResultLayer(img,\'new_output\',cpy)\n\n\nregister(\n    ""deeplabv3"",\n    ""deeplabv3"",\n    ""Generate semantic segmentation map based on deep learning."",\n    ""Kritik Soman"",\n    ""GIMP-ML"",\n    ""2020"",\n    ""deeplabv3..."",\n    ""*"",      # Alternately use RGB, RGB*, GRAY*, INDEXED etc.\n    [   (PF_IMAGE, ""image"", ""Input image"", None),\n        (PF_DRAWABLE, ""drawable"", ""Input drawable"", None)\n    ],\n    [],\n    deeplabv3, menu=""<Image>/Layer/GIML-ML"")\n\nmain()\n'"
gimp-plugins/facegen.py,3,"b'import os \nbaseLoc = os.path.dirname(os.path.realpath(__file__))+\'/\'\n\n\nfrom gimpfu import *\nimport sys\n\nsys.path.extend([baseLoc+\'gimpenv/lib/python2.7\',baseLoc+\'gimpenv/lib/python2.7/site-packages\',baseLoc+\'gimpenv/lib/python2.7/site-packages/setuptools\',baseLoc+\'CelebAMask-HQ/MaskGAN_demo\'])\n\n\nimport torch\nfrom argparse import Namespace\nfrom models.models import create_model\nfrom data.base_dataset import get_params, get_transform, normalize\nimport os\nimport numpy as np\nfrom PIL import Image\n\ncolors = np.array([[0, 0, 0], [204, 0, 0], [76, 153, 0], \\\n[204, 204, 0], [51, 51, 255], [204, 0, 204], [0, 255, 255], \\\n[51, 255, 255], [102, 51, 0], [255, 0, 0], [102, 204, 0], \\\n[255, 255, 0], [0, 0, 153], [0, 0, 204], [255, 51, 153], \\\n[0, 204, 204], [0, 51, 0], [255, 153, 51], [0, 204, 0]])\ncolors = colors.astype(np.uint8)\n\ndef getlabelmat(mask,idx):\n    x=np.zeros((mask.shape[0],mask.shape[1],3))\n    x[mask==idx,0]=colors[idx][0] \n    x[mask==idx,1]=colors[idx][1] \n    x[mask==idx,2]=colors[idx][2]\n    return x \n\ndef colorMask(mask):\n    x=np.zeros((mask.shape[0],mask.shape[1],3))\n    for idx in range(19):\n        x=x+getlabelmat(mask,idx)\n    # mask=np.dstack((mask1,mask2,mask3))\n    return np.uint8(x)\n\ndef labelMask(mask):\n    x=np.zeros((mask.shape[0],mask.shape[1],3))\n    for idx in range(19):\n        tmp=np.logical_and(mask[:,:,0]==colors[idx][0],mask[:,:,1]==colors[idx][1])\n        tmp2=np.logical_and(tmp,mask[:,:,2]==colors[idx][2])\n        x[tmp2]=idx\n    return x     \n\ndef getOptions():\n    mydict={\'aspect_ratio\': 1.0,\n    \'batchSize\': 1,\n    \'checkpoints_dir\': baseLoc+\'CelebAMask-HQ/MaskGAN_demo/checkpoints\',\n    \'cluster_path\': \'features_clustered_010.npy\',\n    \'data_type\': 32,\n    \'dataroot\': \'../Data_preprocessing/\',\n    \'display_winsize\': 512,\n    \'engine\': None,\n    \'export_onnx\': None,\n    \'fineSize\': 512,\n    \'gpu_ids\': [0],\n    \'how_many\': 1000,\n    \'input_nc\': 3,\n    \'isTrain\': False,\n    \'label_nc\': 19,\n    \'loadSize\': 512,\n    \'max_dataset_size\': \'inf\',\n    \'model\': \'pix2pixHD\',\n    \'nThreads\': 2,\n    \'n_blocks_global\': 4,\n    \'n_blocks_local\': 3,\n    \'n_downsample_global\': 4,\n    \'n_local_enhancers\': 1,\n    \'name\': \'label2face_512p\',\n    \'netG\': \'global\',\n    \'ngf\': 64,\n    \'niter_fix_global\': 0,\n    \'no_flip\': False,\n    \'norm\': \'instance\',\n    \'ntest\': \'inf\',\n    \'onnx\': None,\n    \'output_nc\': 3,\n    \'phase\': \'test\',\n    \'resize_or_crop\': \'scale_width\',\n    \'results_dir\': \'./results/\',\n    \'serial_batches\': False,\n    \'tf_log\': False,\n    \'use_dropout\': False,\n    \'use_encoded_image\': False,\n    \'verbose\': False,\n    \'which_epoch\': \'latest\'}\n    args = Namespace(**mydict)\n    return args\n\ndef channelData(layer):#convert gimp image to numpy\n    region=layer.get_pixel_rgn(0, 0, layer.width,layer.height)\n    pixChars=region[:,:] # Take whole layer\n    bpp=region.bpp\n    # return np.frombuffer(pixChars,dtype=np.uint8).reshape(len(pixChars)/bpp,bpp)\n    return np.frombuffer(pixChars,dtype=np.uint8).reshape(layer.height,layer.width,bpp)\n\ndef createResultLayer(image,name,result):\n    rlBytes=np.uint8(result).tobytes();\n    rl=gimp.Layer(image,name,image.width,image.height,image.active_layer.type,100,NORMAL_MODE)\n    region=rl.get_pixel_rgn(0, 0, rl.width,rl.height,True)\n    region[:,:]=rlBytes\n    image.add_layer(rl,0)\n    gimp.displays_flush()\n\ndef getnewface(img,mask,mask_m):\n    h,w,d = img.shape\n    img = Image.fromarray(img)\n    lmask = labelMask(mask)\n    lmask_m = labelMask(mask_m)\n\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(0)\n    opt = getOptions()\n\n    model = create_model(opt)   \n\n    params = get_params(opt, (512,512))\n    transform_mask = get_transform(opt, params, method=Image.NEAREST, normalize=False, normalize_mask=True)\n    transform_image = get_transform(opt, params)\n    mask = transform_mask(Image.fromarray(np.uint8(lmask))) \n    mask_m = transform_mask(Image.fromarray(np.uint8(lmask_m)))\n    img = transform_image(img)\n \n    generated = model.inference(torch.FloatTensor([mask_m.numpy()]), torch.FloatTensor([mask.numpy()]), torch.FloatTensor([img.numpy()]))   \n\n    result = generated.permute(0, 2, 3, 1)\n    if torch.cuda.is_available():\n        result = result.cpu().numpy()\n    else:\n        result = result.detach().numpy()\n\n    result = (result + 1) * 127.5\n    result = np.asarray(result[0,:,:,:], dtype=np.uint8)\n    result = Image.fromarray(result)\n    result = result.resize([w,h])\n    \n    result = np.array(result)\n    return result\n\n\ndef facegen(imggimp, curlayer,layeri,layerm,layermm) :\n    if torch.cuda.is_available():\n        gimp.progress_init(""(Using GPU) Running face gen for "" + layeri.name + ""..."")\n    else:\n        gimp.progress_init(""(Using CPU) Running face gen for "" + layeri.name + ""..."")\n\n    img = channelData(layeri)\n    mask = channelData(layerm)\n    mask_m = channelData(layermm)\n\n    cpy=getnewface(img,mask,mask_m)\n    createResultLayer(imggimp,\'new_output\',cpy)\n\n    \n\nregister(\n    ""facegen"",\n    ""facegen"",\n    ""Running face gen."",\n    ""Kritik Soman"",\n    ""Your"",\n    ""2020"",\n    ""facegen..."",\n    ""*"",      # Alternately use RGB, RGB*, GRAY*, INDEXED etc.\n    [   (PF_IMAGE, ""image"", ""Input image"", None),\n        (PF_DRAWABLE, ""drawable"", ""Input drawable"", None),\n        (PF_LAYER, ""drawinglayer"", ""Original Image:"", None),\n        (PF_LAYER, ""drawinglayer"", ""Original Mask:"", None),\n        (PF_LAYER, ""drawinglayer"", ""Modified Mask:"", None),\n    ],\n    [],\n    facegen, menu=""<Image>/Layer/GIML-ML"")\n\nmain()\n'"
gimp-plugins/faceparse.py,13,"b'import os \nbaseLoc = os.path.dirname(os.path.realpath(__file__))+\'/\'\n\n\nfrom gimpfu import *\nimport sys\n\nsys.path.extend([baseLoc+\'gimpenv/lib/python2.7\',baseLoc+\'gimpenv/lib/python2.7/site-packages\',baseLoc+\'gimpenv/lib/python2.7/site-packages/setuptools\',baseLoc+\'face-parsing.PyTorch\'])\n\n\nfrom model import BiSeNet\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms, datasets\nimport numpy as np\n\ncolors = np.array([[0,0,0],\n[204,0,0],\n[0,255,255],\n[51,255,255],\n[51,51,255],\n[204,0,204],\n[204,204,0],\n[102,51,0],\n[255,0,0],\n[0,204,204],\n[76,153,0],\n[102,204,0],\n[255,255,0],\n[0,0,153],\n[255,153,51],\n[0,51,0],\n[0,204,0],\n[0,0,204],\n[255,51,153]])\ncolors = colors.astype(np.uint8)\n\ndef getlabelmat(mask,idx):\n    x=np.zeros((mask.shape[0],mask.shape[1],3))\n    x[mask==idx,0]=colors[idx][0] \n    x[mask==idx,1]=colors[idx][1] \n    x[mask==idx,2]=colors[idx][2]\n    return x \n\ndef colorMask(mask):\n    x=np.zeros((mask.shape[0],mask.shape[1],3))\n    for idx in range(19):\n        x=x+getlabelmat(mask,idx)\n    return np.uint8(x)\n\ndef getface(input_image):\n    save_pth = baseLoc+\'face-parsing.PyTorch/79999_iter.pth\'\n    input_image = Image.fromarray(input_image)\n\n    n_classes = 19\n    net = BiSeNet(n_classes=n_classes)\n    if torch.cuda.is_available():\n        net.cuda()\n        net.load_state_dict(torch.load(save_pth))\n    else:\n        net.load_state_dict(torch.load(save_pth, map_location=lambda storage, loc: storage))\n\n\n    net.eval()\n\n    \n    to_tensor = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n\n\n    with torch.no_grad():\n        img = input_image.resize((512, 512), Image.BILINEAR)\n        img = to_tensor(img)\n        img = torch.unsqueeze(img, 0)\n        if torch.cuda.is_available():\n            img = img.cuda()\n        out = net(img)[0]\n        if torch.cuda.is_available():\n            parsing = out.squeeze(0).cpu().numpy().argmax(0)\n        else:\n            parsing = out.squeeze(0).numpy().argmax(0)\n    \n    parsing = Image.fromarray(np.uint8(parsing))\n    parsing = parsing.resize(input_image.size) \n    parsing = np.array(parsing)\n\n    return parsing\n\ndef getSeg(input_image):\n    model = torch.load(baseLoc+\'deeplabv3+model.pt\')\n    model.eval()\n    preprocess = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    input_image = Image.fromarray(input_image)\n\n    input_tensor = preprocess(input_image)\n    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n\n    # move the input and model to GPU for speed if available\n    if torch.cuda.is_available():\n        input_batch = input_batch.to(\'cuda\')\n        model.to(\'cuda\')\n\n    with torch.no_grad():\n        output = model(input_batch)[\'out\'][0]\n    output_predictions = output.argmax(0)\n\n\n    # create a color pallette, selecting a color for each class\n    palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n    colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n    colors = (colors % 255).numpy().astype(""uint8"")\n\n    r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n\n    tmp = np.array(r)\n    tmp2 = 10*np.repeat(tmp[:, :, np.newaxis], 3, axis=2)\n\n    return  tmp2\n\ndef channelData(layer):#convert gimp image to numpy\n    region=layer.get_pixel_rgn(0, 0, layer.width,layer.height)\n    pixChars=region[:,:] # Take whole layer\n    bpp=region.bpp\n    return np.frombuffer(pixChars,dtype=np.uint8).reshape(layer.height,layer.width,bpp)\n\ndef createResultLayer(image,name,result):\n    rlBytes=np.uint8(result).tobytes();\n    rl=gimp.Layer(image,name,image.width,image.height,image.active_layer.type,100,NORMAL_MODE)\n    region=rl.get_pixel_rgn(0, 0, rl.width,rl.height,True)\n    region[:,:]=rlBytes\n    image.add_layer(rl,0)\n    gimp.displays_flush()\n\ndef faceparse(img, layer) :\n    if torch.cuda.is_available():\n        gimp.progress_init(""(Using GPU) Running face parse for "" + layer.name + ""..."")\n    else:\n        gimp.progress_init(""(Using CPU) Running face parse for "" + layer.name + ""..."")\n\n    imgmat = channelData(layer)\n    cpy=getface(imgmat)\n    cpy = colorMask(cpy)\n    createResultLayer(img,\'new_output\',cpy)\n\n\n    \n\nregister(\n    ""faceparse"",\n    ""faceparse"",\n    ""Running face parse."",\n    ""Kritik Soman"",\n    ""Your"",\n    ""2020"",\n    ""faceparse..."",\n    ""*"",      # Alternately use RGB, RGB*, GRAY*, INDEXED etc.\n    [   (PF_IMAGE, ""image"", ""Input image"", None),\n        (PF_DRAWABLE, ""drawable"", ""Input drawable"", None),\n    ],\n    [],\n    faceparse, menu=""<Image>/Layer/GIML-ML"")\n\nmain()\n'"
gimp-plugins/invert.py,0,"b'\nfrom gimpfu import *\n\ndef invert(img, layer) :\n    gimp.progress_init(""Inverting "" + layer.name + ""..."")\n    pdb.gimp_undo_push_group_start(img)\n    pdb.gimp_invert(layer)\n    pdb.gimp_undo_push_group_end(img)\n\nregister(\n    ""Invert"",\n    ""Invert"",\n    ""Invert"",\n    ""Kritik Soman"",\n    ""Your Name"",\n    ""2020"",\n    ""Invert..."",\n    ""*"",      # Alternately use RGB, RGB*, GRAY*, INDEXED etc.\n    [   (PF_IMAGE, ""image"", ""Input image"", None),\n        (PF_DRAWABLE, ""drawable"", ""Input drawable"", None),\n    ],\n    [],\n    invert, menu=""<Image>/Filters/Enhance"")\n\nmain()\n'"
gimp-plugins/monodepth.py,7,"b'import os \nbaseLoc = os.path.dirname(os.path.realpath(__file__))+\'/\'\n\n\nfrom gimpfu import *\nimport sys\nsys.path.extend([baseLoc+\'gimpenv/lib/python2.7\',baseLoc+\'gimpenv/lib/python2.7/site-packages\',baseLoc+\'gimpenv/lib/python2.7/site-packages/setuptools\',baseLoc+\'monodepth2\'])\n\n\nimport PIL.Image as pil\nimport networks\nimport torch\nfrom torchvision import transforms\nimport os\nimport numpy as np\nimport cv2\n# import matplotlib as mpl\n# import matplotlib.cm as cm\n\ndef getMonoDepth(input_image):\n    if torch.cuda.is_available():\n        device = torch.device(""cuda"")\n    else:\n        device = torch.device(""cpu"")\n    loc=baseLoc+\'monodepth2/\'\n\n    model_path = os.path.join(loc+""models"", \'mono+stereo_640x192\')\n    encoder_path = os.path.join(model_path, ""encoder.pth"")\n    depth_decoder_path = os.path.join(model_path, ""depth.pth"")\n\n    # LOADING PRETRAINED MODEL\n    encoder = networks.ResnetEncoder(18, False)\n    loaded_dict_enc = torch.load(encoder_path, map_location=device)\n\n    # extract the height and width of image that this model was trained with\n    feed_height = loaded_dict_enc[\'height\']\n    feed_width = loaded_dict_enc[\'width\']\n    filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n    encoder.load_state_dict(filtered_dict_enc)\n    encoder.to(device)\n    encoder.eval()\n\n    depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n\n    loaded_dict = torch.load(depth_decoder_path, map_location=device)\n    depth_decoder.load_state_dict(loaded_dict)\n\n    depth_decoder.to(device)\n    depth_decoder.eval()\n\n    with torch.no_grad():\n        input_image = pil.fromarray(input_image)\n        # input_image = pil.open(image_path).convert(\'RGB\')\n        original_width, original_height = input_image.size\n        input_image = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n        input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n\n        # PREDICTION\n        input_image = input_image.to(device)\n        features = encoder(input_image)\n        outputs = depth_decoder(features)\n\n        disp = outputs[(""disp"", 0)]\n        disp_resized = torch.nn.functional.interpolate(\n            disp, (original_height, original_width), mode=""bilinear"", align_corners=False)\n\n        # Saving colormapped depth image\n        disp_resized_np = disp_resized.squeeze().cpu().numpy()\n        vmax = np.percentile(disp_resized_np, 95)\n        vmin = disp_resized_np.min()\n        disp_resized_np = vmin + (disp_resized_np - vmin) * (vmax - vmin) / (disp_resized_np.max() - vmin)\n        disp_resized_np = (255 * (disp_resized_np - vmin) / (vmax - vmin)).astype(np.uint8)\n        colormapped_im = cv2.applyColorMap(disp_resized_np, cv2.COLORMAP_HOT)\n        colormapped_im = cv2.cvtColor(colormapped_im, cv2.COLOR_BGR2RGB)\n        # normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=vmax)\n        # mapper = cm.ScalarMappable(norm=normalizer, cmap=\'magma\')\n        # colormapped_im = (mapper.to_rgba(disp_resized_np)[:, :, :3] * 255).astype(np.uint8)\n    return colormapped_im\n\ndef channelData(layer):#convert gimp image to numpy\n    region=layer.get_pixel_rgn(0, 0, layer.width,layer.height)\n    pixChars=region[:,:] # Take whole layer\n    bpp=region.bpp\n    # return np.frombuffer(pixChars,dtype=np.uint8).reshape(len(pixChars)/bpp,bpp)\n    return np.frombuffer(pixChars,dtype=np.uint8).reshape(layer.height,layer.width,bpp)\n\ndef createResultLayer(image,name,result):\n    rlBytes=np.uint8(result).tobytes();\n    rl=gimp.Layer(image,name,image.width,image.height,image.active_layer.type,100,NORMAL_MODE)\n    region=rl.get_pixel_rgn(0, 0, rl.width,rl.height,True)\n    region[:,:]=rlBytes\n    image.add_layer(rl,0)\n    gimp.displays_flush()\n\ndef MonoDepth(img, layer) :\n    gimp.progress_init(""Generating disparity map for "" + layer.name + ""..."")\n\n    imgmat = channelData(layer)\n    cpy=getMonoDepth(imgmat)\n\n    createResultLayer(img,\'new_output\',cpy)\n\n\n    \n\nregister(\n    ""MonoDepth"",\n    ""MonoDepth"",\n    ""Generate monocular disparity map based on deep learning."",\n    ""Kritik Soman"",\n    ""Your"",\n    ""2020"",\n    ""MonoDepth..."",\n    ""*"",      # Alternately use RGB, RGB*, GRAY*, INDEXED etc.\n    [   (PF_IMAGE, ""image"", ""Input image"", None),\n        (PF_DRAWABLE, ""drawable"", ""Input drawable"", None),\n    ],\n    [],\n    MonoDepth, menu=""<Image>/Layer/GIML-ML"")\n\nmain()\n'"
gimp-plugins/super_resolution.py,6,"b'import os \nbaseLoc = os.path.dirname(os.path.realpath(__file__))+\'/\'\n\nfrom gimpfu import *\nimport sys\nsys.path.extend([baseLoc+\'gimpenv/lib/python2.7\',baseLoc+\'gimpenv/lib/python2.7/site-packages\',baseLoc+\'gimpenv/lib/python2.7/site-packages/setuptools\',baseLoc+\'pytorch-SRResNet\'])\n\n\nfrom argparse import Namespace\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nfrom PIL import Image\n\n\ndef getlabelmat(mask,idx):\n    x=np.zeros((mask.shape[0],mask.shape[1],3))\n    x[mask==idx,0]=colors[idx][0] \n    x[mask==idx,1]=colors[idx][1] \n    x[mask==idx,2]=colors[idx][2]\n    return x \n\ndef colorMask(mask):\n    x=np.zeros((mask.shape[0],mask.shape[1],3))\n    for idx in range(19):\n        x=x+getlabelmat(mask,idx)\n    return np.uint8(x)\n\n\ndef getnewimg(input_image,s):\n    opt=Namespace(cuda=torch.cuda.is_available(),\n        model=baseLoc+\'pytorch-SRResNet/model/model_srresnet.pth\',\n        dataset=\'Set5\',scale=s,gpus=0)\n\n    im_l=Image.fromarray(input_image)\n    cuda = opt.cuda\n\n    if cuda:\n        model = torch.load(opt.model)[""model""]\n    else:\n        model = torch.load(opt.model,map_location=torch.device(\'cpu\'))[""model""]\n\n    im_l=np.array(im_l)\n    im_l = im_l.astype(float)\n    im_input = im_l.astype(np.float32).transpose(2,0,1)\n    im_input = im_input.reshape(1,im_input.shape[0],im_input.shape[1],im_input.shape[2])\n    im_input = Variable(torch.from_numpy(im_input/255.).float())\n\n    if cuda:\n        model = model.cuda()\n        im_input = im_input.cuda()\n    else:\n        model = model.cpu()\n\n    HR_4x = model(im_input)\n\n    HR_4x = HR_4x.cpu()\n\n    im_h = HR_4x.data[0].numpy().astype(np.float32)\n\n    im_h = im_h*255.\n    im_h = np.clip(im_h, 0., 255.)\n    im_h = im_h.transpose(1,2,0).astype(np.uint8)\n\n    return im_h\n\n\ndef channelData(layer):#convert gimp image to numpy\n    region=layer.get_pixel_rgn(0, 0, layer.width,layer.height)\n    pixChars=region[:,:] # Take whole layer\n    bpp=region.bpp\n    return np.frombuffer(pixChars,dtype=np.uint8).reshape(layer.height,layer.width,bpp)\n\ndef createResultLayer(image,name,result):\n    rlBytes=np.uint8(result).tobytes();\n    rl=gimp.Layer(image,name,image.width,image.height,image.active_layer.type,100,NORMAL_MODE)\n    region=rl.get_pixel_rgn(0, 0, rl.width,rl.height,True)\n    region[:,:]=rlBytes\n    image.add_layer(rl,0)\n    gimp.displays_flush()\n\ndef genNewImg(name,layer_np):\n    h,w,d=layer_np.shape\n    img=pdb.gimp_image_new(w, h, RGB)\n    display=pdb.gimp_display_new(img)\n    \n    rlBytes=np.uint8(layer_np).tobytes();\n    rl=gimp.Layer(img,name,img.width,img.height,RGB,100,NORMAL_MODE)\n    region=rl.get_pixel_rgn(0, 0, rl.width,rl.height,True)\n    region[:,:]=rlBytes\n    \n    pdb.gimp_image_insert_layer(img, rl, None, 0)\n    \n    gimp.displays_flush()\n    \n\ndef super_resolution(img, layer,scale) :\n    if torch.cuda.is_available():\n        gimp.progress_init(""(Using GPU) Running for "" + layer.name + ""..."")\n    else:\n        gimp.progress_init(""(Using CPU) Running for "" + layer.name + ""..."")\n\n    imgmat = channelData(layer)\n    cpy = getnewimg(imgmat,scale)\n    genNewImg(layer.name+\'_upscaled\',cpy)\n    \n\nregister(\n    ""super-resolution"",\n    ""super-resolution"",\n    ""Running super-resolution."",\n    ""Kritik Soman"",\n    ""Your"",\n    ""2020"",\n    ""super-resolution..."",\n    ""*"",      # Alternately use RGB, RGB*, GRAY*, INDEXED etc.\n    [   (PF_IMAGE, ""image"", ""Input image"", None),\n        (PF_DRAWABLE, ""drawable"", ""Input drawable"", None),\n        (PF_SLIDER, ""Scale"",  ""Scale"", 4, (1.1, 4, 0.5))\n    ],\n    [],\n    super_resolution, menu=""<Image>/Layer/GIML-ML"")\n\nmain()\n'"
gimp-plugins/DeblurGANv2/adversarial_trainer.py,1,"b""import torch\nimport copy\n\n\nclass GANFactory:\n    factories = {}\n\n    def __init__(self):\n        pass\n\n    def add_factory(gan_id, model_factory):\n        GANFactory.factories.put[gan_id] = model_factory\n\n    add_factory = staticmethod(add_factory)\n\n    # A Template Method:\n\n    def create_model(gan_id, net_d=None, criterion=None):\n        if gan_id not in GANFactory.factories:\n            GANFactory.factories[gan_id] = \\\n                eval(gan_id + '.Factory()')\n        return GANFactory.factories[gan_id].create(net_d, criterion)\n\n    create_model = staticmethod(create_model)\n\n\nclass GANTrainer(object):\n    def __init__(self, net_d, criterion):\n        self.net_d = net_d\n        self.criterion = criterion\n\n    def loss_d(self, pred, gt):\n        pass\n\n    def loss_g(self, pred, gt):\n        pass\n\n    def get_params(self):\n        pass\n\n\nclass NoGAN(GANTrainer):\n    def __init__(self, net_d, criterion):\n        GANTrainer.__init__(self, net_d, criterion)\n\n    def loss_d(self, pred, gt):\n        return [0]\n\n    def loss_g(self, pred, gt):\n        return 0\n\n    def get_params(self):\n        return [torch.nn.Parameter(torch.Tensor(1))]\n\n    class Factory:\n        @staticmethod\n        def create(net_d, criterion): return NoGAN(net_d, criterion)\n\n\nclass SingleGAN(GANTrainer):\n    def __init__(self, net_d, criterion):\n        GANTrainer.__init__(self, net_d, criterion)\n        self.net_d = self.net_d.cuda()\n\n    def loss_d(self, pred, gt):\n        return self.criterion(self.net_d, pred, gt)\n\n    def loss_g(self, pred, gt):\n        return self.criterion.get_g_loss(self.net_d, pred, gt)\n\n    def get_params(self):\n        return self.net_d.parameters()\n\n    class Factory:\n        @staticmethod\n        def create(net_d, criterion): return SingleGAN(net_d, criterion)\n\n\nclass DoubleGAN(GANTrainer):\n    def __init__(self, net_d, criterion):\n        GANTrainer.__init__(self, net_d, criterion)\n        self.patch_d = net_d['patch'].cuda()\n        self.full_d = net_d['full'].cuda()\n        self.full_criterion = copy.deepcopy(criterion)\n\n    def loss_d(self, pred, gt):\n        return (self.criterion(self.patch_d, pred, gt) + self.full_criterion(self.full_d, pred, gt)) / 2\n\n    def loss_g(self, pred, gt):\n        return (self.criterion.get_g_loss(self.patch_d, pred, gt) + self.full_criterion.get_g_loss(self.full_d, pred,\n                                                                                                  gt)) / 2\n\n    def get_params(self):\n        return list(self.patch_d.parameters()) + list(self.full_d.parameters())\n\n    class Factory:\n        @staticmethod\n        def create(net_d, criterion): return DoubleGAN(net_d, criterion)\n\n"""
gimp-plugins/DeblurGANv2/aug.py,0,"b""from typing import List\n\nimport albumentations as albu\n\n\ndef get_transforms(size, scope = 'geometric', crop='random'):\n    augs = {'strong': albu.Compose([albu.HorizontalFlip(),\n                                    albu.ShiftScaleRotate(shift_limit=0.0, scale_limit=0.2, rotate_limit=20, p=.4),\n                                    albu.ElasticTransform(),\n                                    albu.OpticalDistortion(),\n                                    albu.OneOf([\n                                        albu.CLAHE(clip_limit=2),\n                                        albu.IAASharpen(),\n                                        albu.IAAEmboss(),\n                                        albu.RandomBrightnessContrast(),\n                                        albu.RandomGamma()\n                                    ], p=0.5),\n                                    albu.OneOf([\n                                        albu.RGBShift(),\n                                        albu.HueSaturationValue(),\n                                    ], p=0.5),\n                                    ]),\n            'weak': albu.Compose([albu.HorizontalFlip(),\n                                  ]),\n            'geometric': albu.OneOf([albu.HorizontalFlip(always_apply=True),\n                                     albu.ShiftScaleRotate(always_apply=True),\n                                     albu.Transpose(always_apply=True),\n                                     albu.OpticalDistortion(always_apply=True),\n                                     albu.ElasticTransform(always_apply=True),\n                                     ])\n            }\n\n    aug_fn = augs[scope]\n    crop_fn = {'random': albu.RandomCrop(size, size, always_apply=True),\n               'center': albu.CenterCrop(size, size, always_apply=True)}[crop]\n    pad = albu.PadIfNeeded(size, size)\n\n    pipeline = albu.Compose([aug_fn, crop_fn, pad], additional_targets={'target': 'image'})\n\n    def process(a, b):\n        r = pipeline(image=a, target=b)\n        return r['image'], r['target']\n\n    return process\n\n\ndef get_normalize():\n    normalize = albu.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    normalize = albu.Compose([normalize], additional_targets={'target': 'image'})\n\n    def process(a, b):\n        r = normalize(image=a, target=b)\n        return r['image'], r['target']\n\n    return process\n\n\ndef _resolve_aug_fn(name):\n    d = {\n        'cutout': albu.Cutout,\n        'rgb_shift': albu.RGBShift,\n        'hsv_shift': albu.HueSaturationValue,\n        'motion_blur': albu.MotionBlur,\n        'median_blur': albu.MedianBlur,\n        'snow': albu.RandomSnow,\n        'shadow': albu.RandomShadow,\n        'fog': albu.RandomFog,\n        'brightness_contrast': albu.RandomBrightnessContrast,\n        'gamma': albu.RandomGamma,\n        'sun_flare': albu.RandomSunFlare,\n        'sharpen': albu.IAASharpen,\n        'jpeg': albu.JpegCompression,\n        'gray': albu.ToGray,\n        # ToDo: pixelize\n        # ToDo: partial gray\n    }\n    return d[name]\n\n\ndef get_corrupt_function(config):\n    augs = []\n    for aug_params in config:\n        name = aug_params.pop('name')\n        cls = _resolve_aug_fn(name)\n        prob = aug_params.pop('prob') if 'prob' in aug_params else .5\n        augs.append(cls(p=prob, **aug_params))\n\n    augs = albu.OneOf(augs)\n\n    def process(x):\n        return augs(image=x)['image']\n\n    return process\n"""
gimp-plugins/DeblurGANv2/dataset.py,1,"b""import os\nfrom copy import deepcopy\nfrom functools import partial\nfrom glob import glob\nfrom hashlib import sha1\nfrom typing import Callable, Iterable, Optional, Tuple\n\nimport cv2\nimport numpy as np\nfrom glog import logger\nfrom joblib import Parallel, cpu_count, delayed\nfrom skimage.io import imread\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\nimport aug\n\n\ndef subsample(data: Iterable, bounds: Tuple[float, float], hash_fn: Callable, n_buckets=100, salt='', verbose=True):\n    data = list(data)\n    buckets = split_into_buckets(data, n_buckets=n_buckets, salt=salt, hash_fn=hash_fn)\n\n    lower_bound, upper_bound = [x * n_buckets for x in bounds]\n    msg = f'Subsampling buckets from {lower_bound} to {upper_bound}, total buckets number is {n_buckets}'\n    if salt:\n        msg += f'; salt is {salt}'\n    if verbose:\n        logger.info(msg)\n    return np.array([sample for bucket, sample in zip(buckets, data) if lower_bound <= bucket < upper_bound])\n\n\ndef hash_from_paths(x: Tuple[str, str], salt: str = '') -> str:\n    path_a, path_b = x\n    names = ''.join(map(os.path.basename, (path_a, path_b)))\n    return sha1(f'{names}_{salt}'.encode()).hexdigest()\n\n\ndef split_into_buckets(data: Iterable, n_buckets: int, hash_fn: Callable, salt=''):\n    hashes = map(partial(hash_fn, salt=salt), data)\n    return np.array([int(x, 16) % n_buckets for x in hashes])\n\n\ndef _read_img(x: str):\n    img = cv2.imread(x)\n    if img is None:\n        logger.warning(f'Can not read image {x} with OpenCV, switching to scikit-image')\n        img = imread(x)\n    return img\n\n\nclass PairedDataset(Dataset):\n    def __init__(self,\n                 files_a: Tuple[str],\n                 files_b: Tuple[str],\n                 transform_fn: Callable,\n                 normalize_fn: Callable,\n                 corrupt_fn: Optional[Callable] = None,\n                 preload: bool = True,\n                 preload_size: Optional[int] = 0,\n                 verbose=True):\n\n        assert len(files_a) == len(files_b)\n\n        self.preload = preload\n        self.data_a = files_a\n        self.data_b = files_b\n        self.verbose = verbose\n        self.corrupt_fn = corrupt_fn\n        self.transform_fn = transform_fn\n        self.normalize_fn = normalize_fn\n        logger.info(f'Dataset has been created with {len(self.data_a)} samples')\n\n        if preload:\n            preload_fn = partial(self._bulk_preload, preload_size=preload_size)\n            if files_a == files_b:\n                self.data_a = self.data_b = preload_fn(self.data_a)\n            else:\n                self.data_a, self.data_b = map(preload_fn, (self.data_a, self.data_b))\n            self.preload = True\n\n    def _bulk_preload(self, data: Iterable[str], preload_size: int):\n        jobs = [delayed(self._preload)(x, preload_size=preload_size) for x in data]\n        jobs = tqdm(jobs, desc='preloading images', disable=not self.verbose)\n        return Parallel(n_jobs=cpu_count(), backend='threading')(jobs)\n\n    @staticmethod\n    def _preload(x: str, preload_size: int):\n        img = _read_img(x)\n        if preload_size:\n            h, w, *_ = img.shape\n            h_scale = preload_size / h\n            w_scale = preload_size / w\n            scale = max(h_scale, w_scale)\n            img = cv2.resize(img, fx=scale, fy=scale, dsize=None)\n            assert min(img.shape[:2]) >= preload_size, f'weird img shape: {img.shape}'\n        return img\n\n    def _preprocess(self, img, res):\n        def transpose(x):\n            return np.transpose(x, (2, 0, 1))\n\n        return map(transpose, self.normalize_fn(img, res))\n\n    def __len__(self):\n        return len(self.data_a)\n\n    def __getitem__(self, idx):\n        a, b = self.data_a[idx], self.data_b[idx]\n        if not self.preload:\n            a, b = map(_read_img, (a, b))\n        a, b = self.transform_fn(a, b)\n        if self.corrupt_fn is not None:\n            a = self.corrupt_fn(a)\n        a, b = self._preprocess(a, b)\n        return {'a': a, 'b': b}\n\n    @staticmethod\n    def from_config(config):\n        config = deepcopy(config)\n        files_a, files_b = map(lambda x: sorted(glob(config[x], recursive=True)), ('files_a', 'files_b'))\n        transform_fn = aug.get_transforms(size=config['size'], scope=config['scope'], crop=config['crop'])\n        normalize_fn = aug.get_normalize()\n        corrupt_fn = aug.get_corrupt_function(config['corrupt'])\n\n        hash_fn = hash_from_paths\n        # ToDo: add more hash functions\n        verbose = config.get('verbose', True)\n        data = subsample(data=zip(files_a, files_b),\n                         bounds=config.get('bounds', (0, 1)),\n                         hash_fn=hash_fn,\n                         verbose=verbose)\n\n        files_a, files_b = map(list, zip(*data))\n\n        return PairedDataset(files_a=files_a,\n                             files_b=files_b,\n                             preload=config['preload'],\n                             preload_size=config['preload_size'],\n                             corrupt_fn=corrupt_fn,\n                             normalize_fn=normalize_fn,\n                             transform_fn=transform_fn,\n                             verbose=verbose)\n"""
gimp-plugins/DeblurGANv2/metric_counter.py,0,"b""import logging\nfrom collections import defaultdict\n\nimport numpy as np\nfrom tensorboardX import SummaryWriter\n\nWINDOW_SIZE = 100\n\n\nclass MetricCounter:\n    def __init__(self, exp_name):\n        self.writer = SummaryWriter(exp_name)\n        logging.basicConfig(filename='{}.log'.format(exp_name), level=logging.DEBUG)\n        self.metrics = defaultdict(list)\n        self.images = defaultdict(list)\n        self.best_metric = 0\n\n    def add_image(self, x: np.ndarray, tag: str):\n        self.images[tag].append(x)\n\n    def clear(self):\n        self.metrics = defaultdict(list)\n        self.images = defaultdict(list)\n\n    def add_losses(self, l_G, l_content, l_D=0):\n        for name, value in zip(('G_loss', 'G_loss_content', 'G_loss_adv', 'D_loss'),\n                               (l_G, l_content, l_G - l_content, l_D)):\n            self.metrics[name].append(value)\n\n    def add_metrics(self, psnr, ssim):\n        for name, value in zip(('PSNR', 'SSIM'),\n                               (psnr, ssim)):\n            self.metrics[name].append(value)\n\n    def loss_message(self):\n        metrics = ((k, np.mean(self.metrics[k][-WINDOW_SIZE:])) for k in ('G_loss', 'PSNR', 'SSIM'))\n        return '; '.join(map(lambda x: f'{x[0]}={x[1]:.4f}', metrics))\n\n    def write_to_tensorboard(self, epoch_num, validation=False):\n        scalar_prefix = 'Validation' if validation else 'Train'\n        for tag in ('G_loss', 'D_loss', 'G_loss_adv', 'G_loss_content', 'SSIM', 'PSNR'):\n            self.writer.add_scalar(f'{scalar_prefix}_{tag}', np.mean(self.metrics[tag]), global_step=epoch_num)\n        for tag in self.images:\n            imgs = self.images[tag]\n            if imgs:\n                imgs = np.array(imgs)\n                self.writer.add_images(tag, imgs[:, :, :, ::-1].astype('float32') / 255, dataformats='NHWC',\n                                       global_step=epoch_num)\n                self.images[tag] = []\n\n    def update_best_model(self):\n        cur_metric = np.mean(self.metrics['PSNR'])\n        if self.best_metric < cur_metric:\n            self.best_metric = cur_metric\n            return True\n        return False\n"""
gimp-plugins/DeblurGANv2/predict.py,5,"b""import os\nfrom glob import glob\n# from typing import Optional\n\nimport cv2\nimport numpy as np\nimport torch\nimport yaml\nfrom fire import Fire\nfrom tqdm import tqdm\n\nfrom aug import get_normalize\nfrom models.networks import get_generator\n\n\nclass Predictor:\n    def __init__(self, weights_path, model_name=''):\n        with open('config/config.yaml') as cfg:\n            config = yaml.load(cfg)\n        model = get_generator(model_name or config['model'])\n        model.load_state_dict(torch.load(weights_path, map_location=lambda storage, loc: storage)['model'])\n        if torch.cuda.is_available():\n            self.model = model.cuda()\n        else:\n            self.model = model\n        self.model.train(True)\n        # GAN inference should be in train mode to use actual stats in norm layers,\n        # it's not a bug\n        self.normalize_fn = get_normalize()\n\n    @staticmethod\n    def _array_to_batch(x):\n        x = np.transpose(x, (2, 0, 1))\n        x = np.expand_dims(x, 0)\n        return torch.from_numpy(x)\n\n    def _preprocess(self, x, mask):\n        x, _ = self.normalize_fn(x, x)\n        if mask is None:\n            mask = np.ones_like(x, dtype=np.float32)\n        else:\n            mask = np.round(mask.astype('float32') / 255)\n\n        h, w, _ = x.shape\n        block_size = 32\n        min_height = (h // block_size + 1) * block_size\n        min_width = (w // block_size + 1) * block_size\n\n        pad_params = {'mode': 'constant',\n                      'constant_values': 0,\n                      'pad_width': ((0, min_height - h), (0, min_width - w), (0, 0))\n                      }\n        x = np.pad(x, **pad_params)\n        mask = np.pad(mask, **pad_params)\n\n        return map(self._array_to_batch, (x, mask)), h, w\n\n    @staticmethod\n    def _postprocess(x):\n        x, = x\n        x = x.detach().cpu().float().numpy()\n        x = (np.transpose(x, (1, 2, 0)) + 1) / 2.0 * 255.0\n        return x.astype('uint8')\n\n    def __call__(self, img, mask, ignore_mask=True):\n        (img, mask), h, w = self._preprocess(img, mask)\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                inputs = [img.cuda()]\n            else:\n                inputs = [img]\n            if not ignore_mask:\n                inputs += [mask]\n            pred = self.model(*inputs)\n        return self._postprocess(pred)[:h, :w, :]\n\ndef sorted_glob(pattern):\n    return sorted(glob(pattern))\n\ndef main(img_pattern,\n         mask_pattern = None,\n         weights_path='best_fpn.h5',\n         out_dir='submit/',\n         side_by_side = False):\n\n\n    imgs = sorted_glob(img_pattern)\n    masks = sorted_glob(mask_pattern) if mask_pattern is not None else [None for _ in imgs]\n    pairs = zip(imgs, masks)\n    names = sorted([os.path.basename(x) for x in glob(img_pattern)])\n    predictor = Predictor(weights_path=weights_path)\n\n    # os.makedirs(out_dir)\n    for name, pair in tqdm(zip(names, pairs), total=len(names)):\n        f_img, f_mask = pair\n        img, mask = map(cv2.imread, (f_img, f_mask))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        pred = predictor(img, mask)\n        if side_by_side:\n            pred = np.hstack((img, pred))\n        pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n        cv2.imwrite(os.path.join(out_dir, name),\n                    pred)\n\n\nif __name__ == '__main__':\n    Fire(main)\n"""
gimp-plugins/DeblurGANv2/predictorClass.py,5,"b""from models.networks import get_generator_new\n# from aug import get_normalize\nimport torch\nimport numpy as np\n\nconfig = {'project': 'deblur_gan', 'warmup_num': 3, 'optimizer': {'lr': 0.0001, 'name': 'adam'},\n          'val': {'preload': False, 'bounds': [0.9, 1], 'crop': 'center', 'files_b': '/datasets/my_dataset/**/*.jpg',\n                  'files_a': '/datasets/my_dataset/**/*.jpg', 'scope': 'geometric',\n                  'corrupt': [{'num_holes': 3, 'max_w_size': 25, 'max_h_size': 25, 'name': 'cutout', 'prob': 0.5},\n                              {'quality_lower': 70, 'name': 'jpeg', 'quality_upper': 90}, {'name': 'motion_blur'},\n                              {'name': 'median_blur'}, {'name': 'gamma'}, {'name': 'rgb_shift'}, {'name': 'hsv_shift'},\n                              {'name': 'sharpen'}], 'preload_size': 0, 'size': 256}, 'val_batches_per_epoch': 100,\n          'num_epochs': 200, 'batch_size': 1, 'experiment_desc': 'fpn', 'train_batches_per_epoch': 1000,\n          'train': {'preload': False, 'bounds': [0, 0.9], 'crop': 'random', 'files_b': '/datasets/my_dataset/**/*.jpg',\n                    'files_a': '/datasets/my_dataset/**/*.jpg', 'preload_size': 0,\n                    'corrupt': [{'num_holes': 3, 'max_w_size': 25, 'max_h_size': 25, 'name': 'cutout', 'prob': 0.5},\n                                {'quality_lower': 70, 'name': 'jpeg', 'quality_upper': 90}, {'name': 'motion_blur'},\n                                {'name': 'median_blur'}, {'name': 'gamma'}, {'name': 'rgb_shift'},\n                                {'name': 'hsv_shift'}, {'name': 'sharpen'}], 'scope': 'geometric', 'size': 256},\n          'scheduler': {'min_lr': 1e-07, 'name': 'linear', 'start_epoch': 50}, 'image_size': [256, 256],\n          'phase': 'train',\n          'model': {'d_name': 'double_gan', 'disc_loss': 'wgan-gp', 'blocks': 9, 'content_loss': 'perceptual',\n                    'adv_lambda': 0.001, 'dropout': True, 'g_name': 'fpn_inception', 'd_layers': 3,\n                    'learn_residual': True, 'norm_layer': 'instance'}}\n\n\nclass Predictor:\n    def __init__(self, weights_path, model_name=''):\n        # model = get_generator(model_name or config['model'])\n        model = get_generator_new(weights_path[0:-11])\n        model.load_state_dict(torch.load(weights_path, map_location=lambda storage, loc: storage)['model'])\n        if torch.cuda.is_available():\n            self.model = model.cuda()\n        else:\n            self.model = model\n        self.model.train(True)\n        # GAN inference should be in train mode to use actual stats in norm layers,\n        # it's not a bug\n        # self.normalize_fn = get_normalize()\n\n    @staticmethod\n    def _array_to_batch(x):\n        x = np.transpose(x, (2, 0, 1))\n        x = np.expand_dims(x, 0)\n        return torch.from_numpy(x)\n\n    def _preprocess(self, x, mask):\n        # x, _ = self.normalize_fn(x, x)\n        x = ((x.astype(np.float32) / 255) - 0.5) / 0.5\n        if mask is None:\n            mask = np.ones_like(x, dtype=np.float32)\n        else:\n            mask = np.round(mask.astype('float32') / 255)\n\n        h, w, _ = x.shape\n        block_size = 32\n        min_height = (h // block_size + 1) * block_size\n        min_width = (w // block_size + 1) * block_size\n\n        pad_params = {'mode': 'constant',\n                      'constant_values': 0,\n                      'pad_width': ((0, min_height - h), (0, min_width - w), (0, 0))\n                      }\n        x = np.pad(x, **pad_params)\n        mask = np.pad(mask, **pad_params)\n\n        return map(self._array_to_batch, (x, mask)), h, w\n\n    @staticmethod\n    def _postprocess(x):\n        x, = x\n        x = x.detach().cpu().float().numpy()\n        x = (np.transpose(x, (1, 2, 0)) + 1) / 2.0 * 255.0\n        return x.astype('uint8')\n\n    def __call__(self, img, mask, ignore_mask=True):\n        (img, mask), h, w = self._preprocess(img, mask)\n        with torch.no_grad():\n            if torch.cuda.is_available():\n                inputs = [img.cuda()]\n            else:\n                inputs = [img]\n            if not ignore_mask:\n                inputs += [mask]\n            pred = self.model(*inputs)\n        return self._postprocess(pred)[:h, :w, :]\n"""
gimp-plugins/DeblurGANv2/schedulers.py,1,"b'import math\n\nfrom torch.optim import lr_scheduler\n\n\nclass WarmRestart(lr_scheduler.CosineAnnealingLR):\n    """"""This class implements Stochastic Gradient Descent with Warm Restarts(SGDR): https://arxiv.org/abs/1608.03983.\n\n    Set the learning rate of each parameter group using a cosine annealing schedule, When last_epoch=-1, sets initial lr as lr.\n    This can\'t support scheduler.step(epoch). please keep epoch=None.\n    """"""\n\n    def __init__(self, optimizer, T_max=30, T_mult=1, eta_min=0, last_epoch=-1):\n        """"""implements SGDR\n\n        Parameters:\n        ----------\n        T_max : int\n            Maximum number of epochs.\n        T_mult : int\n            Multiplicative factor of T_max.\n        eta_min : int\n            Minimum learning rate. Default: 0.\n        last_epoch : int\n            The index of last epoch. Default: -1.\n        """"""\n        self.T_mult = T_mult\n        super().__init__(optimizer, T_max, eta_min, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch == self.T_max:\n            self.last_epoch = 0\n            self.T_max *= self.T_mult\n        return [self.eta_min + (base_lr - self.eta_min) * (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2 for\n                base_lr in self.base_lrs]\n\n\nclass LinearDecay(lr_scheduler._LRScheduler):\n    """"""This class implements LinearDecay\n\n    """"""\n\n    def __init__(self, optimizer, num_epochs, start_epoch=0, min_lr=0, last_epoch=-1):\n        """"""implements LinearDecay\n\n        Parameters:\n        ----------\n\n        """"""\n        self.num_epochs = num_epochs\n        self.start_epoch = start_epoch\n        self.min_lr = min_lr\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch < self.start_epoch:\n            return self.base_lrs\n        return [base_lr - ((base_lr - self.min_lr) / self.num_epochs) * (self.last_epoch - self.start_epoch) for\n                base_lr in self.base_lrs]\n'"
gimp-plugins/DeblurGANv2/test_aug.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom aug import get_transforms\n\n\nclass AugTest(unittest.TestCase):\n    @staticmethod\n    def make_images():\n        img = (np.random.rand(100, 100, 3) * 255).astype('uint8')\n        return img.copy(), img.copy()\n\n    def test_aug(self):\n        for scope in ('strong', 'weak'):\n            for crop in ('random', 'center'):\n                aug_pipeline = get_transforms(80, scope=scope, crop=crop)\n                a, b = self.make_images()\n                a, b = aug_pipeline(a, b)\n                np.testing.assert_allclose(a, b)\n"""
gimp-plugins/DeblurGANv2/test_dataset.py,1,"b""import os\nimport unittest\nfrom shutil import rmtree\nfrom tempfile import mkdtemp\n\nimport cv2\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\nfrom dataset import PairedDataset\n\n\ndef make_img():\n    return (np.random.rand(100, 100, 3) * 255).astype('uint8')\n\n\nclass AugTest(unittest.TestCase):\n    tmp_dir = mkdtemp()\n    raw = os.path.join(tmp_dir, 'raw')\n    gt = os.path.join(tmp_dir, 'gt')\n\n    def setUp(self):\n        for d in (self.raw, self.gt):\n            os.makedirs(d)\n\n        for i in range(5):\n            for d in (self.raw, self.gt):\n                img = make_img()\n                cv2.imwrite(os.path.join(d, f'{i}.png'), img)\n\n    def tearDown(self):\n        rmtree(self.tmp_dir)\n\n    def dataset_gen(self, equal=True):\n        base_config = {'files_a': os.path.join(self.raw, '*.png'),\n                       'files_b': os.path.join(self.raw if equal else self.gt, '*.png'),\n                       'size': 32,\n                       }\n        for b in ([0, 1], [0, 0.9]):\n            for scope in ('strong', 'weak'):\n                for crop in ('random', 'center'):\n                    for preload in (0, 1):\n                        for preload_size in (0, 64):\n                            config = base_config.copy()\n                            config['bounds'] = b\n                            config['scope'] = scope\n                            config['crop'] = crop\n                            config['preload'] = preload\n                            config['preload_size'] = preload_size\n                            config['verbose'] = False\n                            dataset = PairedDataset.from_config(config)\n                            yield dataset\n\n    def test_equal_datasets(self):\n        for dataset in self.dataset_gen(equal=True):\n            dataloader = DataLoader(dataset=dataset,\n                                    batch_size=2,\n                                    shuffle=True,\n                                    drop_last=True)\n            dataloader = iter(dataloader)\n            batch = next(dataloader)\n            a, b = map(lambda x: x.numpy(), map(batch.get, ('a', 'b')))\n\n            np.testing.assert_allclose(a, b)\n\n    def test_datasets(self):\n        for dataset in self.dataset_gen(equal=False):\n            dataloader = DataLoader(dataset=dataset,\n                                    batch_size=2,\n                                    shuffle=True,\n                                    drop_last=True)\n            dataloader = iter(dataloader)\n            batch = next(dataloader)\n            a, b = map(lambda x: x.numpy(), map(batch.get, ('a', 'b')))\n\n            assert not np.all(a == b), 'images should not be the same'\n"""
gimp-plugins/DeblurGANv2/test_metrics.py,4,"b'from __future__ import print_function\nimport argparse\nimport numpy as np\nimport torch\nimport cv2\nimport yaml\nimport os\nfrom torchvision import models, transforms\nfrom torch.autograd import Variable\nimport shutil\nimport glob\nimport tqdm\nfrom util.metrics import PSNR\nfrom albumentations import Compose, CenterCrop, PadIfNeeded\nfrom PIL import Image\nfrom ssim.ssimlib import SSIM\nfrom models.networks import get_generator\n\n\ndef get_args():\n\tparser = argparse.ArgumentParser(\'Test an image\')\n\tparser.add_argument(\'--img_folder\', required=True, help=\'GoPRO Folder\')\n\tparser.add_argument(\'--weights_path\', required=True, help=\'Weights path\')\n\n\treturn parser.parse_args()\n\n\ndef prepare_dirs(path):\n\tif os.path.exists(path):\n\t\tshutil.rmtree(path)\n\tos.makedirs(path)\n\n\ndef get_gt_image(path):\n\tdir, filename = os.path.split(path)\n\tbase, seq = os.path.split(dir)\n\tbase, _ = os.path.split(base)\n\timg = cv2.cvtColor(cv2.imread(os.path.join(base, \'sharp\', seq, filename)), cv2.COLOR_BGR2RGB)\n\treturn img\n\n\ndef test_image(model, image_path):\n\timg_transforms = transforms.Compose([\n\t\ttransforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n\t])\n\tsize_transform = Compose([\n\t\tPadIfNeeded(736, 1280)\n\t])\n\tcrop = CenterCrop(720, 1280)\n\timg = cv2.imread(image_path)\n\timg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\timg_s = size_transform(image=img)[\'image\']\n\timg_tensor = torch.from_numpy(np.transpose(img_s / 255, (2, 0, 1)).astype(\'float32\'))\n\timg_tensor = img_transforms(img_tensor)\n\twith torch.no_grad():\n\t\timg_tensor = Variable(img_tensor.unsqueeze(0).cuda())\n\t\tresult_image = model(img_tensor)\n\tresult_image = result_image[0].cpu().float().numpy()\n\tresult_image = (np.transpose(result_image, (1, 2, 0)) + 1) / 2.0 * 255.0\n\tresult_image = crop(image=result_image)[\'image\']\n\tresult_image = result_image.astype(\'uint8\')\n\tgt_image = get_gt_image(image_path)\n\t_, filename = os.path.split(image_path)\n\tpsnr = PSNR(result_image, gt_image)\n\tpilFake = Image.fromarray(result_image)\n\tpilReal = Image.fromarray(gt_image)\n\tssim = SSIM(pilFake).cw_ssim_value(pilReal)\n\treturn psnr, ssim\n\n\ndef test(model, files):\n\tpsnr = 0\n\tssim = 0\n\tfor file in tqdm.tqdm(files):\n\t\tcur_psnr, cur_ssim = test_image(model, file)\n\t\tpsnr += cur_psnr\n\t\tssim += cur_ssim\n\tprint(""PSNR = {}"".format(psnr / len(files)))\n\tprint(""SSIM = {}"".format(ssim / len(files)))\n\n\nif __name__ == \'__main__\':\n\targs = get_args()\n\twith open(\'config/config.yaml\') as cfg:\n\t\tconfig = yaml.load(cfg)\n\tmodel = get_generator(config[\'model\'])\n\tmodel.load_state_dict(torch.load(args.weights_path)[\'model\'])\n\tmodel = model.cuda()\n\tfilenames = sorted(glob.glob(args.img_folder + \'/test\' + \'/blur/**/*.png\', recursive=True))\n\ttest(model, filenames)\n'"
gimp-plugins/DeblurGANv2/testing.py,0,"b""import cv2\nfrom predictorClass import Predictor\n\npredictor = Predictor(weights_path='best_fpn.h5')\nimg = cv2.imread('img/img.jpg')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\npred = predictor(img, None)\npred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\ncv2.imwrite('submit/img.jpg',pred)\n"""
gimp-plugins/DeblurGANv2/train.py,4,"b'import logging\nfrom functools import partial\n\nimport cv2\nimport torch\nimport torch.optim as optim\nimport tqdm\nimport yaml\nfrom joblib import cpu_count\nfrom torch.utils.data import DataLoader\n\nfrom adversarial_trainer import GANFactory\nfrom dataset import PairedDataset\nfrom metric_counter import MetricCounter\nfrom models.losses import get_loss\nfrom models.models import get_model\nfrom models.networks import get_nets\nfrom schedulers import LinearDecay, WarmRestart\n\ncv2.setNumThreads(0)\n\n\nclass Trainer:\n    def __init__(self, config, train: DataLoader, val: DataLoader):\n        self.config = config\n        self.train_dataset = train\n        self.val_dataset = val\n        self.adv_lambda = config[\'model\'][\'adv_lambda\']\n        self.metric_counter = MetricCounter(config[\'experiment_desc\'])\n        self.warmup_epochs = config[\'warmup_num\']\n\n    def train(self):\n        self._init_params()\n        for epoch in range(0, config[\'num_epochs\']):\n            if (epoch == self.warmup_epochs) and not (self.warmup_epochs == 0):\n                self.netG.module.unfreeze()\n                self.optimizer_G = self._get_optim(self.netG.parameters())\n                self.scheduler_G = self._get_scheduler(self.optimizer_G)\n            self._run_epoch(epoch)\n            self._validate(epoch)\n            self.scheduler_G.step()\n            self.scheduler_D.step()\n\n            if self.metric_counter.update_best_model():\n                torch.save({\n                    \'model\': self.netG.state_dict()\n                }, \'best_{}.h5\'.format(self.config[\'experiment_desc\']))\n            torch.save({\n                \'model\': self.netG.state_dict()\n            }, \'last_{}.h5\'.format(self.config[\'experiment_desc\']))\n            print(self.metric_counter.loss_message())\n            logging.debug(""Experiment Name: %s, Epoch: %d, Loss: %s"" % (\n                self.config[\'experiment_desc\'], epoch, self.metric_counter.loss_message()))\n\n    def _run_epoch(self, epoch):\n        self.metric_counter.clear()\n        for param_group in self.optimizer_G.param_groups:\n            lr = param_group[\'lr\']\n\n        epoch_size = config.get(\'train_batches_per_epoch\') or len(self.train_dataset)\n        tq = tqdm.tqdm(self.train_dataset, total=epoch_size)\n        tq.set_description(\'Epoch {}, lr {}\'.format(epoch, lr))\n        i = 0\n        for data in tq:\n            inputs, targets = self.model.get_input(data)\n            outputs = self.netG(inputs)\n            loss_D = self._update_d(outputs, targets)\n            self.optimizer_G.zero_grad()\n            loss_content = self.criterionG(outputs, targets)\n            loss_adv = self.adv_trainer.loss_g(outputs, targets)\n            loss_G = loss_content + self.adv_lambda * loss_adv\n            loss_G.backward()\n            self.optimizer_G.step()\n            self.metric_counter.add_losses(loss_G.item(), loss_content.item(), loss_D)\n            curr_psnr, curr_ssim, img_for_vis = self.model.get_images_and_metrics(inputs, outputs, targets)\n            self.metric_counter.add_metrics(curr_psnr, curr_ssim)\n            tq.set_postfix(loss=self.metric_counter.loss_message())\n            if not i:\n                self.metric_counter.add_image(img_for_vis, tag=\'train\')\n            i += 1\n            if i > epoch_size:\n                break\n        tq.close()\n        self.metric_counter.write_to_tensorboard(epoch)\n\n    def _validate(self, epoch):\n        self.metric_counter.clear()\n        epoch_size = config.get(\'val_batches_per_epoch\') or len(self.val_dataset)\n        tq = tqdm.tqdm(self.val_dataset, total=epoch_size)\n        tq.set_description(\'Validation\')\n        i = 0\n        for data in tq:\n            inputs, targets = self.model.get_input(data)\n            outputs = self.netG(inputs)\n            loss_content = self.criterionG(outputs, targets)\n            loss_adv = self.adv_trainer.loss_g(outputs, targets)\n            loss_G = loss_content + self.adv_lambda * loss_adv\n            self.metric_counter.add_losses(loss_G.item(), loss_content.item())\n            curr_psnr, curr_ssim, img_for_vis = self.model.get_images_and_metrics(inputs, outputs, targets)\n            self.metric_counter.add_metrics(curr_psnr, curr_ssim)\n            if not i:\n                self.metric_counter.add_image(img_for_vis, tag=\'val\')\n            i += 1\n            if i > epoch_size:\n                break\n        tq.close()\n        self.metric_counter.write_to_tensorboard(epoch, validation=True)\n\n    def _update_d(self, outputs, targets):\n        if self.config[\'model\'][\'d_name\'] == \'no_gan\':\n            return 0\n        self.optimizer_D.zero_grad()\n        loss_D = self.adv_lambda * self.adv_trainer.loss_d(outputs, targets)\n        loss_D.backward(retain_graph=True)\n        self.optimizer_D.step()\n        return loss_D.item()\n\n    def _get_optim(self, params):\n        if self.config[\'optimizer\'][\'name\'] == \'adam\':\n            optimizer = optim.Adam(params, lr=self.config[\'optimizer\'][\'lr\'])\n        elif self.config[\'optimizer\'][\'name\'] == \'sgd\':\n            optimizer = optim.SGD(params, lr=self.config[\'optimizer\'][\'lr\'])\n        elif self.config[\'optimizer\'][\'name\'] == \'adadelta\':\n            optimizer = optim.Adadelta(params, lr=self.config[\'optimizer\'][\'lr\'])\n        else:\n            raise ValueError(""Optimizer [%s] not recognized."" % self.config[\'optimizer\'][\'name\'])\n        return optimizer\n\n    def _get_scheduler(self, optimizer):\n        if self.config[\'scheduler\'][\'name\'] == \'plateau\':\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                             mode=\'min\',\n                                                             patience=self.config[\'scheduler\'][\'patience\'],\n                                                             factor=self.config[\'scheduler\'][\'factor\'],\n                                                             min_lr=self.config[\'scheduler\'][\'min_lr\'])\n        elif self.config[\'optimizer\'][\'name\'] == \'sgdr\':\n            scheduler = WarmRestart(optimizer)\n        elif self.config[\'scheduler\'][\'name\'] == \'linear\':\n            scheduler = LinearDecay(optimizer,\n                                    min_lr=self.config[\'scheduler\'][\'min_lr\'],\n                                    num_epochs=self.config[\'num_epochs\'],\n                                    start_epoch=self.config[\'scheduler\'][\'start_epoch\'])\n        else:\n            raise ValueError(""Scheduler [%s] not recognized."" % self.config[\'scheduler\'][\'name\'])\n        return scheduler\n\n    @staticmethod\n    def _get_adversarial_trainer(d_name, net_d, criterion_d):\n        if d_name == \'no_gan\':\n            return GANFactory.create_model(\'NoGAN\')\n        elif d_name == \'patch_gan\' or d_name == \'multi_scale\':\n            return GANFactory.create_model(\'SingleGAN\', net_d, criterion_d)\n        elif d_name == \'double_gan\':\n            return GANFactory.create_model(\'DoubleGAN\', net_d, criterion_d)\n        else:\n            raise ValueError(""Discriminator Network [%s] not recognized."" % d_name)\n\n    def _init_params(self):\n        self.criterionG, criterionD = get_loss(self.config[\'model\'])\n        self.netG, netD = get_nets(self.config[\'model\'])\n        self.netG.cuda()\n        self.adv_trainer = self._get_adversarial_trainer(self.config[\'model\'][\'d_name\'], netD, criterionD)\n        self.model = get_model(self.config[\'model\'])\n        self.optimizer_G = self._get_optim(filter(lambda p: p.requires_grad, self.netG.parameters()))\n        self.optimizer_D = self._get_optim(self.adv_trainer.get_params())\n        self.scheduler_G = self._get_scheduler(self.optimizer_G)\n        self.scheduler_D = self._get_scheduler(self.optimizer_D)\n\n\nif __name__ == \'__main__\':\n    with open(\'config/config.yaml\', \'r\') as f:\n        config = yaml.load(f)\n\n    batch_size = config.pop(\'batch_size\')\n    get_dataloader = partial(DataLoader, batch_size=batch_size, num_workers=cpu_count(), shuffle=True, drop_last=True)\n\n    datasets = map(config.pop, (\'train\', \'val\'))\n    datasets = map(PairedDataset.from_config, datasets)\n    train, val = map(get_dataloader, datasets)\n    trainer = Trainer(config, train=train, val=val)\n    trainer.train()\n'"
gimp-plugins/face-parsing.PyTorch/evaluate.py,7,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nfrom logger import setup_logger\nfrom model import BiSeNet\nfrom face_dataset import FaceMask\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nimport math\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport cv2\n\ndef vis_parsing_maps(im, parsing_anno, stride, save_im=False, save_path=\'vis_results/parsing_map_on_im.jpg\'):\n    # Colors for all 20 parts\n    part_colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0],\n                   [255, 0, 85], [255, 0, 170],\n                   [0, 255, 0], [85, 255, 0], [170, 255, 0],\n                   [0, 255, 85], [0, 255, 170],\n                   [0, 0, 255], [85, 0, 255], [170, 0, 255],\n                   [0, 85, 255], [0, 170, 255],\n                   [255, 255, 0], [255, 255, 85], [255, 255, 170],\n                   [255, 0, 255], [255, 85, 255], [255, 170, 255],\n                   [0, 255, 255], [85, 255, 255], [170, 255, 255]]\n\n    im = np.array(im)\n    vis_im = im.copy().astype(np.uint8)\n    vis_parsing_anno = parsing_anno.copy().astype(np.uint8)\n    vis_parsing_anno = cv2.resize(vis_parsing_anno, None, fx=stride, fy=stride, interpolation=cv2.INTER_NEAREST)\n    vis_parsing_anno_color = np.zeros((vis_parsing_anno.shape[0], vis_parsing_anno.shape[1], 3)) + 255\n\n    num_of_class = np.max(vis_parsing_anno)\n\n    for pi in range(1, num_of_class + 1):\n        index = np.where(vis_parsing_anno == pi)\n        vis_parsing_anno_color[index[0], index[1], :] = part_colors[pi]\n\n    vis_parsing_anno_color = vis_parsing_anno_color.astype(np.uint8)\n    # print(vis_parsing_anno_color.shape, vis_im.shape)\n    vis_im = cv2.addWeighted(cv2.cvtColor(vis_im, cv2.COLOR_RGB2BGR), 0.4, vis_parsing_anno_color, 0.6, 0)\n\n    # Save result or not\n    if save_im:\n        cv2.imwrite(save_path, vis_im, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n\n    # return vis_im\n\ndef evaluate(respth=\'./res/test_res\', dspth=\'./data\', cp=\'model_final_diss.pth\'):\n\n    if not os.path.exists(respth):\n        os.makedirs(respth)\n\n    n_classes = 19\n    net = BiSeNet(n_classes=n_classes)\n    net.cuda()\n    save_pth = osp.join(\'res/cp\', cp)\n    net.load_state_dict(torch.load(save_pth))\n    net.eval()\n\n    to_tensor = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n    with torch.no_grad():\n        for image_path in os.listdir(dspth):\n            img = Image.open(osp.join(dspth, image_path))\n            image = img.resize((512, 512), Image.BILINEAR)\n            img = to_tensor(image)\n            img = torch.unsqueeze(img, 0)\n            img = img.cuda()\n            out = net(img)[0]\n            parsing = out.squeeze(0).cpu().numpy().argmax(0)\n\n            vis_parsing_maps(image, parsing, stride=1, save_im=True, save_path=osp.join(respth, image_path))\n\n\n\n\n\n\n\nif __name__ == ""__main__"":\n    setup_logger(\'./res\')\n    evaluate()\n'"
gimp-plugins/face-parsing.PyTorch/face_dataset.py,1,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\n\nimport os.path as osp\nimport os\nfrom PIL import Image\nimport numpy as np\nimport json\nimport cv2\n\nfrom transform import *\n\n\n\nclass FaceMask(Dataset):\n    def __init__(self, rootpth, cropsize=(640, 480), mode=\'train\', *args, **kwargs):\n        super(FaceMask, self).__init__(*args, **kwargs)\n        assert mode in (\'train\', \'val\', \'test\')\n        self.mode = mode\n        self.ignore_lb = 255\n        self.rootpth = rootpth\n\n        self.imgs = os.listdir(os.path.join(self.rootpth, \'CelebA-HQ-img\'))\n\n        #  pre-processing\n        self.to_tensor = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n            ])\n        self.trans_train = Compose([\n            ColorJitter(\n                brightness=0.5,\n                contrast=0.5,\n                saturation=0.5),\n            HorizontalFlip(),\n            RandomScale((0.75, 1.0, 1.25, 1.5, 1.75, 2.0)),\n            RandomCrop(cropsize)\n            ])\n\n    def __getitem__(self, idx):\n        impth = self.imgs[idx]\n        img = Image.open(osp.join(self.rootpth, \'CelebA-HQ-img\', impth))\n        img = img.resize((512, 512), Image.BILINEAR)\n        label = Image.open(osp.join(self.rootpth, \'mask\', impth[:-3]+\'png\')).convert(\'P\')\n        # print(np.unique(np.array(label)))\n        if self.mode == \'train\':\n            im_lb = dict(im=img, lb=label)\n            im_lb = self.trans_train(im_lb)\n            img, label = im_lb[\'im\'], im_lb[\'lb\']\n        img = self.to_tensor(img)\n        label = np.array(label).astype(np.int64)[np.newaxis, :]\n        return img, label\n\n    def __len__(self):\n        return len(self.imgs)\n\n\nif __name__ == ""__main__"":\n    face_data = \'/home/zll/data/CelebAMask-HQ/CelebA-HQ-img\'\n    face_sep_mask = \'/home/zll/data/CelebAMask-HQ/CelebAMask-HQ-mask-anno\'\n    mask_path = \'/home/zll/data/CelebAMask-HQ/mask\'\n    counter = 0\n    total = 0\n    for i in range(15):\n        # files = os.listdir(osp.join(face_sep_mask, str(i)))\n\n        atts = [\'skin\', \'l_brow\', \'r_brow\', \'l_eye\', \'r_eye\', \'eye_g\', \'l_ear\', \'r_ear\', \'ear_r\',\n                \'nose\', \'mouth\', \'u_lip\', \'l_lip\', \'neck\', \'neck_l\', \'cloth\', \'hair\', \'hat\']\n\n        for j in range(i*2000, (i+1)*2000):\n\n            mask = np.zeros((512, 512))\n\n            for l, att in enumerate(atts, 1):\n                total += 1\n                file_name = \'\'.join([str(j).rjust(5, \'0\'), \'_\', att, \'.png\'])\n                path = osp.join(face_sep_mask, str(i), file_name)\n\n                if os.path.exists(path):\n                    counter += 1\n                    sep_mask = np.array(Image.open(path).convert(\'P\'))\n                    # print(np.unique(sep_mask))\n\n                    mask[sep_mask == 225] = l\n            cv2.imwrite(\'{}/{}.png\'.format(mask_path, j), mask)\n            print(j)\n\n    print(counter, total)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
gimp-plugins/face-parsing.PyTorch/logger.py,1,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport os.path as osp\nimport time\nimport sys\nimport logging\n\nimport torch.distributed as dist\n\n\ndef setup_logger(logpth):\n    logfile = 'BiSeNet-{}.log'.format(time.strftime('%Y-%m-%d-%H-%M-%S'))\n    logfile = osp.join(logpth, logfile)\n    FORMAT = '%(levelname)s %(filename)s(%(lineno)d): %(message)s'\n    log_level = logging.INFO\n    if dist.is_initialized() and not dist.get_rank()==0:\n        log_level = logging.ERROR\n    logging.basicConfig(level=log_level, format=FORMAT, filename=logfile)\n    logging.root.addHandler(logging.StreamHandler())\n\n\n"""
gimp-plugins/face-parsing.PyTorch/loss.py,10,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\n\nclass OhemCELoss(nn.Module):\n    def __init__(self, thresh, n_min, ignore_lb=255, *args, **kwargs):\n        super(OhemCELoss, self).__init__()\n        self.thresh = -torch.log(torch.tensor(thresh, dtype=torch.float)).cuda()\n        self.n_min = n_min\n        self.ignore_lb = ignore_lb\n        self.criteria = nn.CrossEntropyLoss(ignore_index=ignore_lb, reduction='none')\n\n    def forward(self, logits, labels):\n        N, C, H, W = logits.size()\n        loss = self.criteria(logits, labels).view(-1)\n        loss, _ = torch.sort(loss, descending=True)\n        if loss[self.n_min] > self.thresh:\n            loss = loss[loss>self.thresh]\n        else:\n            loss = loss[:self.n_min]\n        return torch.mean(loss)\n\n\nclass SoftmaxFocalLoss(nn.Module):\n    def __init__(self, gamma, ignore_lb=255, *args, **kwargs):\n        super(SoftmaxFocalLoss, self).__init__()\n        self.gamma = gamma\n        self.nll = nn.NLLLoss(ignore_index=ignore_lb)\n\n    def forward(self, logits, labels):\n        scores = F.softmax(logits, dim=1)\n        factor = torch.pow(1.-scores, self.gamma)\n        log_score = F.log_softmax(logits, dim=1)\n        log_score = factor * log_score\n        loss = self.nll(log_score, labels)\n        return loss\n\n\nif __name__ == '__main__':\n    torch.manual_seed(15)\n    criteria1 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n    criteria2 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n    net1 = nn.Sequential(\n        nn.Conv2d(3, 19, kernel_size=3, stride=2, padding=1),\n    )\n    net1.cuda()\n    net1.train()\n    net2 = nn.Sequential(\n        nn.Conv2d(3, 19, kernel_size=3, stride=2, padding=1),\n    )\n    net2.cuda()\n    net2.train()\n\n    with torch.no_grad():\n        inten = torch.randn(16, 3, 20, 20).cuda()\n        lbs = torch.randint(0, 19, [16, 20, 20]).cuda()\n        lbs[1, :, :] = 255\n\n    logits1 = net1(inten)\n    logits1 = F.interpolate(logits1, inten.size()[2:], mode='bilinear')\n    logits2 = net2(inten)\n    logits2 = F.interpolate(logits2, inten.size()[2:], mode='bilinear')\n\n    loss1 = criteria1(logits1, lbs)\n    loss2 = criteria2(logits2, lbs)\n    loss = loss1 + loss2\n    print(loss.detach().cpu())\n    loss.backward()\n"""
gimp-plugins/face-parsing.PyTorch/makeup.py,0,"b""import cv2\nimport os\nimport numpy as np\nfrom skimage.filters import gaussian\n\n\ndef sharpen(img):\n    img = img * 1.0\n    gauss_out = gaussian(img, sigma=5, multichannel=True)\n\n    alpha = 1.5\n    img_out = (img - gauss_out) * alpha + img\n\n    img_out = img_out / 255.0\n\n    mask_1 = img_out < 0\n    mask_2 = img_out > 1\n\n    img_out = img_out * (1 - mask_1)\n    img_out = img_out * (1 - mask_2) + mask_2\n    img_out = np.clip(img_out, 0, 1)\n    img_out = img_out * 255\n    return np.array(img_out, dtype=np.uint8)\n\n\ndef hair(image, parsing, part=17, color=[230, 50, 20]):\n    b, g, r = color      #[10, 50, 250]       # [10, 250, 10]\n    tar_color = np.zeros_like(image)\n    tar_color[:, :, 0] = b\n    tar_color[:, :, 1] = g\n    tar_color[:, :, 2] = r\n\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    tar_hsv = cv2.cvtColor(tar_color, cv2.COLOR_BGR2HSV)\n\n    if part == 12 or part == 13:\n        image_hsv[:, :, 0:2] = tar_hsv[:, :, 0:2]\n    else:\n        image_hsv[:, :, 0:1] = tar_hsv[:, :, 0:1]\n\n    changed = cv2.cvtColor(image_hsv, cv2.COLOR_HSV2BGR)\n\n    if part == 17:\n        changed = sharpen(changed)\n\n    changed[parsing != part] = image[parsing != part]\n    # changed = cv2.resize(changed, (512, 512))\n    return changed\n\n#\n# def lip(image, parsing, part=17, color=[230, 50, 20]):\n#     b, g, r = color      #[10, 50, 250]       # [10, 250, 10]\n#     tar_color = np.zeros_like(image)\n#     tar_color[:, :, 0] = b\n#     tar_color[:, :, 1] = g\n#     tar_color[:, :, 2] = r\n#\n#     image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n#     il, ia, ib = cv2.split(image_lab)\n#\n#     tar_lab = cv2.cvtColor(tar_color, cv2.COLOR_BGR2Lab)\n#     tl, ta, tb = cv2.split(tar_lab)\n#\n#     image_lab[:, :, 0] = np.clip(il - np.mean(il) + tl, 0, 100)\n#     image_lab[:, :, 1] = np.clip(ia - np.mean(ia) + ta, -127, 128)\n#     image_lab[:, :, 2] = np.clip(ib - np.mean(ib) + tb, -127, 128)\n#\n#\n#     changed = cv2.cvtColor(image_lab, cv2.COLOR_Lab2BGR)\n#\n#     if part == 17:\n#         changed = sharpen(changed)\n#\n#     changed[parsing != part] = image[parsing != part]\n#     # changed = cv2.resize(changed, (512, 512))\n#     return changed\n\n\nif __name__ == '__main__':\n    # 1  face\n    # 10 nose\n    # 11 teeth\n    # 12 upper lip\n    # 13 lower lip\n    # 17 hair\n    num = 116\n    table = {\n        'hair': 17,\n        'upper_lip': 12,\n        'lower_lip': 13\n    }\n    image_path = '/home/zll/data/CelebAMask-HQ/test-img/{}.jpg'.format(num)\n    parsing_path = 'res/test_res/{}.png'.format(num)\n\n    image = cv2.imread(image_path)\n    ori = image.copy()\n    parsing = np.array(cv2.imread(parsing_path, 0))\n    parsing = cv2.resize(parsing, image.shape[0:2], interpolation=cv2.INTER_NEAREST)\n\n    parts = [table['hair'], table['upper_lip'], table['lower_lip']]\n    # colors = [[20, 20, 200], [100, 100, 230], [100, 100, 230]]\n    colors = [[100, 200, 100]]\n    for part, color in zip(parts, colors):\n        image = hair(image, parsing, part, color)\n    cv2.imwrite('res/makeup/116_ori.png', cv2.resize(ori, (512, 512)))\n    cv2.imwrite('res/makeup/116_2.png', cv2.resize(image, (512, 512)))\n\n    cv2.imshow('image', cv2.resize(ori, (512, 512)))\n    cv2.imshow('color', cv2.resize(image, (512, 512)))\n\n    # cv2.imshow('image', ori)\n    # cv2.imshow('color', image)\n\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
gimp-plugins/face-parsing.PyTorch/model.py,6,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n\nfrom resnet import Resnet18\n# from modules.bn import InPlaceABNSync as BatchNorm2d\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n        super(ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_chan,\n                out_chan,\n                kernel_size = ks,\n                stride = stride,\n                padding = padding,\n                bias = False)\n        self.bn = nn.BatchNorm2d(out_chan)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = F.relu(self.bn(x))\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\nclass BiSeNetOutput(nn.Module):\n    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n        super(BiSeNetOutput, self).__init__()\n        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.conv_out(x)\n        return x\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass AttentionRefinementModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(AttentionRefinementModule, self).__init__()\n        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n        self.bn_atten = nn.BatchNorm2d(out_chan)\n        self.sigmoid_atten = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv(x)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv_atten(atten)\n        atten = self.bn_atten(atten)\n        atten = self.sigmoid_atten(atten)\n        out = torch.mul(feat, atten)\n        return out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n\nclass ContextPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(ContextPath, self).__init__()\n        self.resnet = Resnet18()\n        self.arm16 = AttentionRefinementModule(256, 128)\n        self.arm32 = AttentionRefinementModule(512, 128)\n        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n        self.conv_avg = ConvBNReLU(512, 128, ks=1, stride=1, padding=0)\n\n        self.init_weight()\n\n    def forward(self, x):\n        H0, W0 = x.size()[2:]\n        feat8, feat16, feat32 = self.resnet(x)\n        H8, W8 = feat8.size()[2:]\n        H16, W16 = feat16.size()[2:]\n        H32, W32 = feat32.size()[2:]\n\n        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n        avg = self.conv_avg(avg)\n        avg_up = F.interpolate(avg, (H32, W32), mode=\'nearest\')\n\n        feat32_arm = self.arm32(feat32)\n        feat32_sum = feat32_arm + avg_up\n        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode=\'nearest\')\n        feat32_up = self.conv_head32(feat32_up)\n\n        feat16_arm = self.arm16(feat16)\n        feat16_sum = feat16_arm + feat32_up\n        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode=\'nearest\')\n        feat16_up = self.conv_head16(feat16_up)\n\n        return feat8, feat16_up, feat32_up  # x8, x8, x16\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\n### This is not used, since I replace this with the resnet feature with the same size\nclass SpatialPath(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(SpatialPath, self).__init__()\n        self.conv1 = ConvBNReLU(3, 64, ks=7, stride=2, padding=3)\n        self.conv2 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv3 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n        self.conv_out = ConvBNReLU(64, 128, ks=1, stride=1, padding=0)\n        self.init_weight()\n\n    def forward(self, x):\n        feat = self.conv1(x)\n        feat = self.conv2(feat)\n        feat = self.conv3(feat)\n        feat = self.conv_out(feat)\n        return feat\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass FeatureFusionModule(nn.Module):\n    def __init__(self, in_chan, out_chan, *args, **kwargs):\n        super(FeatureFusionModule, self).__init__()\n        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n        self.conv1 = nn.Conv2d(out_chan,\n                out_chan//4,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.conv2 = nn.Conv2d(out_chan//4,\n                out_chan,\n                kernel_size = 1,\n                stride = 1,\n                padding = 0,\n                bias = False)\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n        self.init_weight()\n\n    def forward(self, fsp, fcp):\n        fcat = torch.cat([fsp, fcp], dim=1)\n        feat = self.convblk(fcat)\n        atten = F.avg_pool2d(feat, feat.size()[2:])\n        atten = self.conv1(atten)\n        atten = self.relu(atten)\n        atten = self.conv2(atten)\n        atten = self.sigmoid(atten)\n        feat_atten = torch.mul(feat, atten)\n        feat_out = feat_atten + feat\n        return feat_out\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module, nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nclass BiSeNet(nn.Module):\n    def __init__(self, n_classes, *args, **kwargs):\n        super(BiSeNet, self).__init__()\n        self.cp = ContextPath()\n        ## here self.sp is deleted\n        self.ffm = FeatureFusionModule(256, 256)\n        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n        self.conv_out16 = BiSeNetOutput(128, 64, n_classes)\n        self.conv_out32 = BiSeNetOutput(128, 64, n_classes)\n        self.init_weight()\n\n    def forward(self, x):\n        H, W = x.size()[2:]\n        feat_res8, feat_cp8, feat_cp16 = self.cp(x)  # here return res3b1 feature\n        feat_sp = feat_res8  # use res3b1 feature to replace spatial path feature\n        feat_fuse = self.ffm(feat_sp, feat_cp8)\n\n        feat_out = self.conv_out(feat_fuse)\n        feat_out16 = self.conv_out16(feat_cp8)\n        feat_out32 = self.conv_out32(feat_cp16)\n\n        feat_out = F.interpolate(feat_out, (H, W), mode=\'bilinear\', align_corners=True)\n        feat_out16 = F.interpolate(feat_out16, (H, W), mode=\'bilinear\', align_corners=True)\n        feat_out32 = F.interpolate(feat_out32, (H, W), mode=\'bilinear\', align_corners=True)\n        return feat_out, feat_out16, feat_out32\n\n    def init_weight(self):\n        for ly in self.children():\n            if isinstance(ly, nn.Conv2d):\n                nn.init.kaiming_normal_(ly.weight, a=1)\n                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\n    def get_params(self):\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n        for name, child in self.named_children():\n            child_wd_params, child_nowd_params = child.get_params()\n            if isinstance(child, FeatureFusionModule) or isinstance(child, BiSeNetOutput):\n                lr_mul_wd_params += child_wd_params\n                lr_mul_nowd_params += child_nowd_params\n            else:\n                wd_params += child_wd_params\n                nowd_params += child_nowd_params\n        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n\n\nif __name__ == ""__main__"":\n    net = BiSeNet(19)\n    net.cuda()\n    net.eval()\n    in_ten = torch.randn(16, 3, 640, 480).cuda()\n    out, out16, out32 = net(in_ten)\n    print(out.shape)\n\n    net.get_params()\n'"
gimp-plugins/face-parsing.PyTorch/optimizer.py,1,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nimport torch\nimport logging\n\nlogger = logging.getLogger()\n\nclass Optimizer(object):\n    def __init__(self,\n                model,\n                lr0,\n                momentum,\n                wd,\n                warmup_steps,\n                warmup_start_lr,\n                max_iter,\n                power,\n                *args, **kwargs):\n        self.warmup_steps = warmup_steps\n        self.warmup_start_lr = warmup_start_lr\n        self.lr0 = lr0\n        self.lr = self.lr0\n        self.max_iter = float(max_iter)\n        self.power = power\n        self.it = 0\n        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = model.get_params()\n        param_list = [\n                {'params': wd_params},\n                {'params': nowd_params, 'weight_decay': 0},\n                {'params': lr_mul_wd_params, 'lr_mul': True},\n                {'params': lr_mul_nowd_params, 'weight_decay': 0, 'lr_mul': True}]\n        self.optim = torch.optim.SGD(\n                param_list,\n                lr = lr0,\n                momentum = momentum,\n                weight_decay = wd)\n        self.warmup_factor = (self.lr0/self.warmup_start_lr)**(1./self.warmup_steps)\n\n\n    def get_lr(self):\n        if self.it <= self.warmup_steps:\n            lr = self.warmup_start_lr*(self.warmup_factor**self.it)\n        else:\n            factor = (1-(self.it-self.warmup_steps)/(self.max_iter-self.warmup_steps))**self.power\n            lr = self.lr0 * factor\n        return lr\n\n\n    def step(self):\n        self.lr = self.get_lr()\n        for pg in self.optim.param_groups:\n            if pg.get('lr_mul', False):\n                pg['lr'] = self.lr * 10\n            else:\n                pg['lr'] = self.lr\n        if self.optim.defaults.get('lr_mul', False):\n            self.optim.defaults['lr'] = self.lr * 10\n        else:\n            self.optim.defaults['lr'] = self.lr\n        self.it += 1\n        self.optim.step()\n        if self.it == self.warmup_steps+2:\n            logger.info('==> warmup done, start to implement poly lr strategy')\n\n    def zero_grad(self):\n        self.optim.zero_grad()\n\n"""
gimp-plugins/face-parsing.PyTorch/prepropess_data.py,0,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport os.path as osp\nimport os\nimport cv2\nfrom transform import *\nfrom PIL import Image\n\nface_data = '/home/zll/data/CelebAMask-HQ/CelebA-HQ-img'\nface_sep_mask = '/home/zll/data/CelebAMask-HQ/CelebAMask-HQ-mask-anno'\nmask_path = '/home/zll/data/CelebAMask-HQ/mask'\ncounter = 0\ntotal = 0\nfor i in range(15):\n\n    atts = ['skin', 'l_brow', 'r_brow', 'l_eye', 'r_eye', 'eye_g', 'l_ear', 'r_ear', 'ear_r',\n            'nose', 'mouth', 'u_lip', 'l_lip', 'neck', 'neck_l', 'cloth', 'hair', 'hat']\n\n    for j in range(i * 2000, (i + 1) * 2000):\n\n        mask = np.zeros((512, 512))\n\n        for l, att in enumerate(atts, 1):\n            total += 1\n            file_name = ''.join([str(j).rjust(5, '0'), '_', att, '.png'])\n            path = osp.join(face_sep_mask, str(i), file_name)\n\n            if os.path.exists(path):\n                counter += 1\n                sep_mask = np.array(Image.open(path).convert('P'))\n                # print(np.unique(sep_mask))\n\n                mask[sep_mask == 225] = l\n        cv2.imwrite('{}/{}.png'.format(mask_path, j), mask)\n        print(j)\n\nprint(counter, total)"""
gimp-plugins/face-parsing.PyTorch/resnet.py,5,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as modelzoo\n\n# from modules.bn import InPlaceABNSync as BatchNorm2d\n\nresnet18_url = \'https://download.pytorch.org/models/resnet18-5c106cde.pth\'\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_chan, out_chan, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_chan, out_chan, stride)\n        self.bn1 = nn.BatchNorm2d(out_chan)\n        self.conv2 = conv3x3(out_chan, out_chan)\n        self.bn2 = nn.BatchNorm2d(out_chan)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = None\n        if in_chan != out_chan or stride != 1:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_chan, out_chan,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_chan),\n                )\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = F.relu(self.bn1(residual))\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n\n        shortcut = x\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = shortcut + residual\n        out = self.relu(out)\n        return out\n\n\ndef create_layer_basic(in_chan, out_chan, bnum, stride=1):\n    layers = [BasicBlock(in_chan, out_chan, stride=stride)]\n    for i in range(bnum-1):\n        layers.append(BasicBlock(out_chan, out_chan, stride=1))\n    return nn.Sequential(*layers)\n\n\nclass Resnet18(nn.Module):\n    def __init__(self):\n        super(Resnet18, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)\n        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)\n        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)\n        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)\n        self.init_weight()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        feat8 = self.layer2(x) # 1/8\n        feat16 = self.layer3(feat8) # 1/16\n        feat32 = self.layer4(feat16) # 1/32\n        return feat8, feat16, feat32\n\n    def init_weight(self):\n        state_dict = modelzoo.load_url(resnet18_url)\n        self_state_dict = self.state_dict()\n        for k, v in state_dict.items():\n            if \'fc\' in k: continue\n            self_state_dict.update({k: v})\n        self.load_state_dict(self_state_dict)\n\n    def get_params(self):\n        wd_params, nowd_params = [], []\n        for name, module in self.named_modules():\n            if isinstance(module, (nn.Linear, nn.Conv2d)):\n                wd_params.append(module.weight)\n                if not module.bias is None:\n                    nowd_params.append(module.bias)\n            elif isinstance(module,  nn.BatchNorm2d):\n                nowd_params += list(module.parameters())\n        return wd_params, nowd_params\n\n\nif __name__ == ""__main__"":\n    net = Resnet18()\n    x = torch.randn(16, 3, 224, 224)\n    out = net(x)\n    print(out[0].size())\n    print(out[1].size())\n    print(out[2].size())\n    net.get_params()\n'"
gimp-plugins/face-parsing.PyTorch/test.py,7,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nfrom logger import setup_logger\nfrom model import BiSeNet\n\nimport torch\n\nimport os\nimport os.path as osp\nimport numpy as np\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport cv2\n\ndef vis_parsing_maps(im, parsing_anno, stride, save_im=False, save_path=\'vis_results/parsing_map_on_im.jpg\'):\n    # Colors for all 20 parts\n    part_colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0],\n                   [255, 0, 85], [255, 0, 170],\n                   [0, 255, 0], [85, 255, 0], [170, 255, 0],\n                   [0, 255, 85], [0, 255, 170],\n                   [0, 0, 255], [85, 0, 255], [170, 0, 255],\n                   [0, 85, 255], [0, 170, 255],\n                   [255, 255, 0], [255, 255, 85], [255, 255, 170],\n                   [255, 0, 255], [255, 85, 255], [255, 170, 255],\n                   [0, 255, 255], [85, 255, 255], [170, 255, 255]]\n\n    im = np.array(im)\n    vis_im = im.copy().astype(np.uint8)\n    vis_parsing_anno = parsing_anno.copy().astype(np.uint8)\n    vis_parsing_anno = cv2.resize(vis_parsing_anno, None, fx=stride, fy=stride, interpolation=cv2.INTER_NEAREST)\n    vis_parsing_anno_color = np.zeros((vis_parsing_anno.shape[0], vis_parsing_anno.shape[1], 3)) + 255\n\n    num_of_class = np.max(vis_parsing_anno)\n\n    for pi in range(1, num_of_class + 1):\n        index = np.where(vis_parsing_anno == pi)\n        vis_parsing_anno_color[index[0], index[1], :] = part_colors[pi]\n\n    vis_parsing_anno_color = vis_parsing_anno_color.astype(np.uint8)\n    # print(vis_parsing_anno_color.shape, vis_im.shape)\n    vis_im = cv2.addWeighted(cv2.cvtColor(vis_im, cv2.COLOR_RGB2BGR), 0.4, vis_parsing_anno_color, 0.6, 0)\n\n    # Save result or not\n    if save_im:\n        cv2.imwrite(save_path[:-4] +\'.png\', vis_parsing_anno)\n        cv2.imwrite(save_path, vis_im, [int(cv2.IMWRITE_JPEG_QUALITY), 100])\n\n    # return vis_im\n\ndef evaluate(respth=\'./res/test_res\', dspth=\'./data\', cp=\'model_final_diss.pth\'):\n\n    if not os.path.exists(respth):\n        os.makedirs(respth)\n\n    n_classes = 19\n    net = BiSeNet(n_classes=n_classes)\n    save_pth = osp.join(\'res/cp\', cp)\n    \n    if torch.cuda.is_available():\n        net.cuda()\n        net.load_state_dict(torch.load(save_pth))\n    else:\n        net.load_state_dict(torch.load(save_pth, map_location=lambda storage, loc: storage))\n\n\n    net.eval()\n\n    to_tensor = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n    with torch.no_grad():\n        for image_path in os.listdir(dspth):\n            img = Image.open(osp.join(dspth, image_path))\n            image = img.resize((512, 512), Image.BILINEAR)\n            img = to_tensor(image)\n            img = torch.unsqueeze(img, 0)\n            if torch.cuda.is_available():\n                img = img.cuda()\n            out = net(img)[0]\n            if torch.cuda.is_available():\n                parsing = out.squeeze(0).cpu().numpy().argmax(0)\n            else:\n                parsing = out.squeeze(0).numpy().argmax(0)\n            # print(parsing)\n            print(np.unique(parsing))\n\n            vis_parsing_maps(image, parsing, stride=1, save_im=True, save_path=osp.join(respth, image_path))\n\n\n\n\n\n\n\nif __name__ == ""__main__"":\n    evaluate(dspth=\'makeup/116_ori.png\', cp=\'79999_iter.pth\')\n\n\n'"
gimp-plugins/face-parsing.PyTorch/train.py,10,"b'#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\nfrom logger import setup_logger\nfrom model import BiSeNet\nfrom face_dataset import FaceMask\nfrom loss import OhemCELoss\nfrom evaluate import evaluate\nfrom optimizer import Optimizer\nimport cv2\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.distributed as dist\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport datetime\nimport argparse\n\n\nrespth = \'./res\'\nif not osp.exists(respth):\n    os.makedirs(respth)\nlogger = logging.getLogger()\n\n\ndef parse_args():\n    parse = argparse.ArgumentParser()\n    parse.add_argument(\n            \'--local_rank\',\n            dest = \'local_rank\',\n            type = int,\n            default = -1,\n            )\n    return parse.parse_args()\n\n\ndef train():\n    args = parse_args()\n    torch.cuda.set_device(args.local_rank)\n    dist.init_process_group(\n                backend = \'nccl\',\n                init_method = \'tcp://127.0.0.1:33241\',\n                world_size = torch.cuda.device_count(),\n                rank=args.local_rank\n                )\n    setup_logger(respth)\n\n    # dataset\n    n_classes = 19\n    n_img_per_gpu = 16\n    n_workers = 8\n    cropsize = [448, 448]\n    data_root = \'/home/zll/data/CelebAMask-HQ/\'\n\n    ds = FaceMask(data_root, cropsize=cropsize, mode=\'train\')\n    sampler = torch.utils.data.distributed.DistributedSampler(ds)\n    dl = DataLoader(ds,\n                    batch_size = n_img_per_gpu,\n                    shuffle = False,\n                    sampler = sampler,\n                    num_workers = n_workers,\n                    pin_memory = True,\n                    drop_last = True)\n\n    # model\n    ignore_idx = -100\n    net = BiSeNet(n_classes=n_classes)\n    net.cuda()\n    net.train()\n    net = nn.parallel.DistributedDataParallel(net,\n            device_ids = [args.local_rank, ],\n            output_device = args.local_rank\n            )\n    score_thres = 0.7\n    n_min = n_img_per_gpu * cropsize[0] * cropsize[1]//16\n    LossP = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    Loss2 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n    Loss3 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n\n    ## optimizer\n    momentum = 0.9\n    weight_decay = 5e-4\n    lr_start = 1e-2\n    max_iter = 80000\n    power = 0.9\n    warmup_steps = 1000\n    warmup_start_lr = 1e-5\n    optim = Optimizer(\n            model = net.module,\n            lr0 = lr_start,\n            momentum = momentum,\n            wd = weight_decay,\n            warmup_steps = warmup_steps,\n            warmup_start_lr = warmup_start_lr,\n            max_iter = max_iter,\n            power = power)\n\n    ## train loop\n    msg_iter = 50\n    loss_avg = []\n    st = glob_st = time.time()\n    diter = iter(dl)\n    epoch = 0\n    for it in range(max_iter):\n        try:\n            im, lb = next(diter)\n            if not im.size()[0] == n_img_per_gpu:\n                raise StopIteration\n        except StopIteration:\n            epoch += 1\n            sampler.set_epoch(epoch)\n            diter = iter(dl)\n            im, lb = next(diter)\n        im = im.cuda()\n        lb = lb.cuda()\n        H, W = im.size()[2:]\n        lb = torch.squeeze(lb, 1)\n\n        optim.zero_grad()\n        out, out16, out32 = net(im)\n        lossp = LossP(out, lb)\n        loss2 = Loss2(out16, lb)\n        loss3 = Loss3(out32, lb)\n        loss = lossp + loss2 + loss3\n        loss.backward()\n        optim.step()\n\n        loss_avg.append(loss.item())\n\n        #  print training log message\n        if (it+1) % msg_iter == 0:\n            loss_avg = sum(loss_avg) / len(loss_avg)\n            lr = optim.lr\n            ed = time.time()\n            t_intv, glob_t_intv = ed - st, ed - glob_st\n            eta = int((max_iter - it) * (glob_t_intv / it))\n            eta = str(datetime.timedelta(seconds=eta))\n            msg = \', \'.join([\n                    \'it: {it}/{max_it}\',\n                    \'lr: {lr:4f}\',\n                    \'loss: {loss:.4f}\',\n                    \'eta: {eta}\',\n                    \'time: {time:.4f}\',\n                ]).format(\n                    it = it+1,\n                    max_it = max_iter,\n                    lr = lr,\n                    loss = loss_avg,\n                    time = t_intv,\n                    eta = eta\n                )\n            logger.info(msg)\n            loss_avg = []\n            st = ed\n        if dist.get_rank() == 0:\n            if (it+1) % 5000 == 0:\n                state = net.module.state_dict() if hasattr(net, \'module\') else net.state_dict()\n                if dist.get_rank() == 0:\n                    torch.save(state, \'./res/cp/{}_iter.pth\'.format(it))\n                evaluate(dspth=\'/home/zll/data/CelebAMask-HQ/test-img\', cp=\'{}_iter.pth\'.format(it))\n\n    #  dump the final model\n    save_pth = osp.join(respth, \'model_final_diss.pth\')\n    # net.cpu()\n    state = net.module.state_dict() if hasattr(net, \'module\') else net.state_dict()\n    if dist.get_rank() == 0:\n        torch.save(state, save_pth)\n    logger.info(\'training done, model saved to: {}\'.format(save_pth))\n\n\nif __name__ == ""__main__"":\n    train()\n'"
gimp-plugins/face-parsing.PyTorch/transform.py,0,"b""#!/usr/bin/python\n# -*- encoding: utf-8 -*-\n\n\nfrom PIL import Image\nimport PIL.ImageEnhance as ImageEnhance\nimport random\nimport numpy as np\n\nclass RandomCrop(object):\n    def __init__(self, size, *args, **kwargs):\n        self.size = size\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        assert im.size == lb.size\n        W, H = self.size\n        w, h = im.size\n\n        if (W, H) == (w, h): return dict(im=im, lb=lb)\n        if w < W or h < H:\n            scale = float(W) / w if w < h else float(H) / h\n            w, h = int(scale * w + 1), int(scale * h + 1)\n            im = im.resize((w, h), Image.BILINEAR)\n            lb = lb.resize((w, h), Image.NEAREST)\n        sw, sh = random.random() * (w - W), random.random() * (h - H)\n        crop = int(sw), int(sh), int(sw) + W, int(sh) + H\n        return dict(\n                im = im.crop(crop),\n                lb = lb.crop(crop)\n                    )\n\n\nclass HorizontalFlip(object):\n    def __init__(self, p=0.5, *args, **kwargs):\n        self.p = p\n\n    def __call__(self, im_lb):\n        if random.random() > self.p:\n            return im_lb\n        else:\n            im = im_lb['im']\n            lb = im_lb['lb']\n\n            # atts = [1 'skin', 2 'l_brow', 3 'r_brow', 4 'l_eye', 5 'r_eye', 6 'eye_g', 7 'l_ear', 8 'r_ear', 9 'ear_r',\n            #         10 'nose', 11 'mouth', 12 'u_lip', 13 'l_lip', 14 'neck', 15 'neck_l', 16 'cloth', 17 'hair', 18 'hat']\n\n            flip_lb = np.array(lb)\n            flip_lb[lb == 2] = 3\n            flip_lb[lb == 3] = 2\n            flip_lb[lb == 4] = 5\n            flip_lb[lb == 5] = 4\n            flip_lb[lb == 7] = 8\n            flip_lb[lb == 8] = 7\n            flip_lb = Image.fromarray(flip_lb)\n            return dict(im = im.transpose(Image.FLIP_LEFT_RIGHT),\n                        lb = flip_lb.transpose(Image.FLIP_LEFT_RIGHT),\n                    )\n\n\nclass RandomScale(object):\n    def __init__(self, scales=(1, ), *args, **kwargs):\n        self.scales = scales\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        W, H = im.size\n        scale = random.choice(self.scales)\n        w, h = int(W * scale), int(H * scale)\n        return dict(im = im.resize((w, h), Image.BILINEAR),\n                    lb = lb.resize((w, h), Image.NEAREST),\n                )\n\n\nclass ColorJitter(object):\n    def __init__(self, brightness=None, contrast=None, saturation=None, *args, **kwargs):\n        if not brightness is None and brightness>0:\n            self.brightness = [max(1-brightness, 0), 1+brightness]\n        if not contrast is None and contrast>0:\n            self.contrast = [max(1-contrast, 0), 1+contrast]\n        if not saturation is None and saturation>0:\n            self.saturation = [max(1-saturation, 0), 1+saturation]\n\n    def __call__(self, im_lb):\n        im = im_lb['im']\n        lb = im_lb['lb']\n        r_brightness = random.uniform(self.brightness[0], self.brightness[1])\n        r_contrast = random.uniform(self.contrast[0], self.contrast[1])\n        r_saturation = random.uniform(self.saturation[0], self.saturation[1])\n        im = ImageEnhance.Brightness(im).enhance(r_brightness)\n        im = ImageEnhance.Contrast(im).enhance(r_contrast)\n        im = ImageEnhance.Color(im).enhance(r_saturation)\n        return dict(im = im,\n                    lb = lb,\n                )\n\n\nclass MultiScale(object):\n    def __init__(self, scales):\n        self.scales = scales\n\n    def __call__(self, img):\n        W, H = img.size\n        sizes = [(int(W*ratio), int(H*ratio)) for ratio in self.scales]\n        imgs = []\n        [imgs.append(img.resize(size, Image.BILINEAR)) for size in sizes]\n        return imgs\n\n\nclass Compose(object):\n    def __init__(self, do_list):\n        self.do_list = do_list\n\n    def __call__(self, im_lb):\n        for comp in self.do_list:\n            im_lb = comp(im_lb)\n        return im_lb\n\n\n\n\nif __name__ == '__main__':\n    flip = HorizontalFlip(p = 1)\n    crop = RandomCrop((321, 321))\n    rscales = RandomScale((0.75, 1.0, 1.5, 1.75, 2.0))\n    img = Image.open('data/img.jpg')\n    lb = Image.open('data/label.png')\n"""
gimp-plugins/monodepth2/evaluate_depth.py,5,"b'from __future__ import absolute_import, division, print_function\n\nimport os\nimport cv2\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom layers import disp_to_depth\nfrom utils import readlines\nfrom options import MonodepthOptions\nimport datasets\nimport networks\n\ncv2.setNumThreads(0)  # This speeds up evaluation 5x on our unix systems (OpenCV 3.3.1)\n\n\nsplits_dir = os.path.join(os.path.dirname(__file__), ""splits"")\n\n# Models which were trained with stereo supervision were trained with a nominal\n# baseline of 0.1 units. The KITTI rig has a baseline of 54cm. Therefore,\n# to convert our stereo predictions to real-world scale we multiply our depths by 5.4.\nSTEREO_SCALE_FACTOR = 5.4\n\n\ndef compute_errors(gt, pred):\n    """"""Computation of error metrics between predicted and ground truth depths\n    """"""\n    thresh = np.maximum((gt / pred), (pred / gt))\n    a1 = (thresh < 1.25     ).mean()\n    a2 = (thresh < 1.25 ** 2).mean()\n    a3 = (thresh < 1.25 ** 3).mean()\n\n    rmse = (gt - pred) ** 2\n    rmse = np.sqrt(rmse.mean())\n\n    rmse_log = (np.log(gt) - np.log(pred)) ** 2\n    rmse_log = np.sqrt(rmse_log.mean())\n\n    abs_rel = np.mean(np.abs(gt - pred) / gt)\n\n    sq_rel = np.mean(((gt - pred) ** 2) / gt)\n\n    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n\n\ndef batch_post_process_disparity(l_disp, r_disp):\n    """"""Apply the disparity post-processing method as introduced in Monodepthv1\n    """"""\n    _, h, w = l_disp.shape\n    m_disp = 0.5 * (l_disp + r_disp)\n    l, _ = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n    l_mask = (1.0 - np.clip(20 * (l - 0.05), 0, 1))[None, ...]\n    r_mask = l_mask[:, :, ::-1]\n    return r_mask * l_disp + l_mask * r_disp + (1.0 - l_mask - r_mask) * m_disp\n\n\ndef evaluate(opt):\n    """"""Evaluates a pretrained model using a specified test set\n    """"""\n    MIN_DEPTH = 1e-3\n    MAX_DEPTH = 80\n\n    assert sum((opt.eval_mono, opt.eval_stereo)) == 1, \\\n        ""Please choose mono or stereo evaluation by setting either --eval_mono or --eval_stereo""\n\n    if opt.ext_disp_to_eval is None:\n\n        opt.load_weights_folder = os.path.expanduser(opt.load_weights_folder)\n\n        assert os.path.isdir(opt.load_weights_folder), \\\n            ""Cannot find a folder at {}"".format(opt.load_weights_folder)\n\n        print(""-> Loading weights from {}"".format(opt.load_weights_folder))\n\n        filenames = readlines(os.path.join(splits_dir, opt.eval_split, ""test_files.txt""))\n        encoder_path = os.path.join(opt.load_weights_folder, ""encoder.pth"")\n        decoder_path = os.path.join(opt.load_weights_folder, ""depth.pth"")\n\n        encoder_dict = torch.load(encoder_path)\n\n        dataset = datasets.KITTIRAWDataset(opt.data_path, filenames,\n                                           encoder_dict[\'height\'], encoder_dict[\'width\'],\n                                           [0], 4, is_train=False)\n        dataloader = DataLoader(dataset, 16, shuffle=False, num_workers=opt.num_workers,\n                                pin_memory=True, drop_last=False)\n\n        encoder = networks.ResnetEncoder(opt.num_layers, False)\n        depth_decoder = networks.DepthDecoder(encoder.num_ch_enc)\n\n        model_dict = encoder.state_dict()\n        encoder.load_state_dict({k: v for k, v in encoder_dict.items() if k in model_dict})\n        depth_decoder.load_state_dict(torch.load(decoder_path))\n\n        encoder.cuda()\n        encoder.eval()\n        depth_decoder.cuda()\n        depth_decoder.eval()\n\n        pred_disps = []\n\n        print(""-> Computing predictions with size {}x{}"".format(\n            encoder_dict[\'width\'], encoder_dict[\'height\']))\n\n        with torch.no_grad():\n            for data in dataloader:\n                input_color = data[(""color"", 0, 0)].cuda()\n\n                if opt.post_process:\n                    # Post-processed results require each image to have two forward passes\n                    input_color = torch.cat((input_color, torch.flip(input_color, [3])), 0)\n\n                output = depth_decoder(encoder(input_color))\n\n                pred_disp, _ = disp_to_depth(output[(""disp"", 0)], opt.min_depth, opt.max_depth)\n                pred_disp = pred_disp.cpu()[:, 0].numpy()\n\n                if opt.post_process:\n                    N = pred_disp.shape[0] // 2\n                    pred_disp = batch_post_process_disparity(pred_disp[:N], pred_disp[N:, :, ::-1])\n\n                pred_disps.append(pred_disp)\n\n        pred_disps = np.concatenate(pred_disps)\n\n    else:\n        # Load predictions from file\n        print(""-> Loading predictions from {}"".format(opt.ext_disp_to_eval))\n        pred_disps = np.load(opt.ext_disp_to_eval)\n\n        if opt.eval_eigen_to_benchmark:\n            eigen_to_benchmark_ids = np.load(\n                os.path.join(splits_dir, ""benchmark"", ""eigen_to_benchmark_ids.npy""))\n\n            pred_disps = pred_disps[eigen_to_benchmark_ids]\n\n    if opt.save_pred_disps:\n        output_path = os.path.join(\n            opt.load_weights_folder, ""disps_{}_split.npy"".format(opt.eval_split))\n        print(""-> Saving predicted disparities to "", output_path)\n        np.save(output_path, pred_disps)\n\n    if opt.no_eval:\n        print(""-> Evaluation disabled. Done."")\n        quit()\n\n    elif opt.eval_split == \'benchmark\':\n        save_dir = os.path.join(opt.load_weights_folder, ""benchmark_predictions"")\n        print(""-> Saving out benchmark predictions to {}"".format(save_dir))\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n\n        for idx in range(len(pred_disps)):\n            disp_resized = cv2.resize(pred_disps[idx], (1216, 352))\n            depth = STEREO_SCALE_FACTOR / disp_resized\n            depth = np.clip(depth, 0, 80)\n            depth = np.uint16(depth * 256)\n            save_path = os.path.join(save_dir, ""{:010d}.png"".format(idx))\n            cv2.imwrite(save_path, depth)\n\n        print(""-> No ground truth is available for the KITTI benchmark, so not evaluating. Done."")\n        quit()\n\n    gt_path = os.path.join(splits_dir, opt.eval_split, ""gt_depths.npz"")\n    gt_depths = np.load(gt_path, fix_imports=True, encoding=\'latin1\')[""data""]\n\n    print(""-> Evaluating"")\n\n    if opt.eval_stereo:\n        print(""   Stereo evaluation - ""\n              ""disabling median scaling, scaling by {}"".format(STEREO_SCALE_FACTOR))\n        opt.disable_median_scaling = True\n        opt.pred_depth_scale_factor = STEREO_SCALE_FACTOR\n    else:\n        print(""   Mono evaluation - using median scaling"")\n\n    errors = []\n    ratios = []\n\n    for i in range(pred_disps.shape[0]):\n\n        gt_depth = gt_depths[i]\n        gt_height, gt_width = gt_depth.shape[:2]\n\n        pred_disp = pred_disps[i]\n        pred_disp = cv2.resize(pred_disp, (gt_width, gt_height))\n        pred_depth = 1 / pred_disp\n\n        if opt.eval_split == ""eigen"":\n            mask = np.logical_and(gt_depth > MIN_DEPTH, gt_depth < MAX_DEPTH)\n\n            crop = np.array([0.40810811 * gt_height, 0.99189189 * gt_height,\n                             0.03594771 * gt_width,  0.96405229 * gt_width]).astype(np.int32)\n            crop_mask = np.zeros(mask.shape)\n            crop_mask[crop[0]:crop[1], crop[2]:crop[3]] = 1\n            mask = np.logical_and(mask, crop_mask)\n\n        else:\n            mask = gt_depth > 0\n\n        pred_depth = pred_depth[mask]\n        gt_depth = gt_depth[mask]\n\n        pred_depth *= opt.pred_depth_scale_factor\n        if not opt.disable_median_scaling:\n            ratio = np.median(gt_depth) / np.median(pred_depth)\n            ratios.append(ratio)\n            pred_depth *= ratio\n\n        pred_depth[pred_depth < MIN_DEPTH] = MIN_DEPTH\n        pred_depth[pred_depth > MAX_DEPTH] = MAX_DEPTH\n\n        errors.append(compute_errors(gt_depth, pred_depth))\n\n    if not opt.disable_median_scaling:\n        ratios = np.array(ratios)\n        med = np.median(ratios)\n        print("" Scaling ratios | med: {:0.3f} | std: {:0.3f}"".format(med, np.std(ratios / med)))\n\n    mean_errors = np.array(errors).mean(0)\n\n    print(""\\n  "" + (""{:>8} | "" * 7).format(""abs_rel"", ""sq_rel"", ""rmse"", ""rmse_log"", ""a1"", ""a2"", ""a3""))\n    print((""&{: 8.3f}  "" * 7).format(*mean_errors.tolist()) + ""\\\\\\\\"")\n    print(""\\n-> Done!"")\n\n\nif __name__ == ""__main__"":\n    options = MonodepthOptions()\n    evaluate(options.parse())\n'"
gimp-plugins/monodepth2/evaluate_pose.py,5,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom layers import transformation_from_parameters\nfrom utils import readlines\nfrom options import MonodepthOptions\nfrom datasets import KITTIOdomDataset\nimport networks\n\n\n# from https://github.com/tinghuiz/SfMLearner\ndef dump_xyz(source_to_target_transformations):\n    xyzs = []\n    cam_to_world = np.eye(4)\n    xyzs.append(cam_to_world[:3, 3])\n    for source_to_target_transformation in source_to_target_transformations:\n        cam_to_world = np.dot(cam_to_world, source_to_target_transformation)\n        xyzs.append(cam_to_world[:3, 3])\n    return xyzs\n\n\n# from https://github.com/tinghuiz/SfMLearner\ndef compute_ate(gtruth_xyz, pred_xyz_o):\n\n    # Make sure that the first matched frames align (no need for rotational alignment as\n    # all the predicted/ground-truth snippets have been converted to use the same coordinate\n    # system with the first frame of the snippet being the origin).\n    offset = gtruth_xyz[0] - pred_xyz_o[0]\n    pred_xyz = pred_xyz_o + offset[None, :]\n\n    # Optimize the scaling factor\n    scale = np.sum(gtruth_xyz * pred_xyz) / np.sum(pred_xyz ** 2)\n    alignment_error = pred_xyz * scale - gtruth_xyz\n    rmse = np.sqrt(np.sum(alignment_error ** 2)) / gtruth_xyz.shape[0]\n    return rmse\n\n\ndef evaluate(opt):\n    """"""Evaluate odometry on the KITTI dataset\n    """"""\n    assert os.path.isdir(opt.load_weights_folder), \\\n        ""Cannot find a folder at {}"".format(opt.load_weights_folder)\n\n    assert opt.eval_split == ""odom_9"" or opt.eval_split == ""odom_10"", \\\n        ""eval_split should be either odom_9 or odom_10""\n\n    sequence_id = int(opt.eval_split.split(""_"")[1])\n\n    filenames = readlines(\n        os.path.join(os.path.dirname(__file__), ""splits"", ""odom"",\n                     ""test_files_{:02d}.txt"".format(sequence_id)))\n\n    dataset = KITTIOdomDataset(opt.data_path, filenames, opt.height, opt.width,\n                               [0, 1], 4, is_train=False)\n    dataloader = DataLoader(dataset, opt.batch_size, shuffle=False,\n                            num_workers=opt.num_workers, pin_memory=True, drop_last=False)\n\n    pose_encoder_path = os.path.join(opt.load_weights_folder, ""pose_encoder.pth"")\n    pose_decoder_path = os.path.join(opt.load_weights_folder, ""pose.pth"")\n\n    pose_encoder = networks.ResnetEncoder(opt.num_layers, False, 2)\n    pose_encoder.load_state_dict(torch.load(pose_encoder_path))\n\n    pose_decoder = networks.PoseDecoder(pose_encoder.num_ch_enc, 1, 2)\n    pose_decoder.load_state_dict(torch.load(pose_decoder_path))\n\n    pose_encoder.cuda()\n    pose_encoder.eval()\n    pose_decoder.cuda()\n    pose_decoder.eval()\n\n    pred_poses = []\n\n    print(""-> Computing pose predictions"")\n\n    opt.frame_ids = [0, 1]  # pose network only takes two frames as input\n\n    with torch.no_grad():\n        for inputs in dataloader:\n            for key, ipt in inputs.items():\n                inputs[key] = ipt.cuda()\n\n            all_color_aug = torch.cat([inputs[(""color_aug"", i, 0)] for i in opt.frame_ids], 1)\n\n            features = [pose_encoder(all_color_aug)]\n            axisangle, translation = pose_decoder(features)\n\n            pred_poses.append(\n                transformation_from_parameters(axisangle[:, 0], translation[:, 0]).cpu().numpy())\n\n    pred_poses = np.concatenate(pred_poses)\n\n    gt_poses_path = os.path.join(opt.data_path, ""poses"", ""{:02d}.txt"".format(sequence_id))\n    gt_global_poses = np.loadtxt(gt_poses_path).reshape(-1, 3, 4)\n    gt_global_poses = np.concatenate(\n        (gt_global_poses, np.zeros((gt_global_poses.shape[0], 1, 4))), 1)\n    gt_global_poses[:, 3, 3] = 1\n    gt_xyzs = gt_global_poses[:, :3, 3]\n\n    gt_local_poses = []\n    for i in range(1, len(gt_global_poses)):\n        gt_local_poses.append(\n            np.linalg.inv(np.dot(np.linalg.inv(gt_global_poses[i - 1]), gt_global_poses[i])))\n\n    ates = []\n    num_frames = gt_xyzs.shape[0]\n    track_length = 5\n    for i in range(0, num_frames - 1):\n        local_xyzs = np.array(dump_xyz(pred_poses[i:i + track_length - 1]))\n        gt_local_xyzs = np.array(dump_xyz(gt_local_poses[i:i + track_length - 1]))\n\n        ates.append(compute_ate(gt_local_xyzs, local_xyzs))\n\n    print(""\\n   Trajectory error: {:0.3f}, std: {:0.3f}\\n"".format(np.mean(ates), np.std(ates)))\n\n    save_path = os.path.join(opt.load_weights_folder, ""poses.npy"")\n    np.save(save_path, pred_poses)\n    print(""-> Predictions saved to"", save_path)\n\n\nif __name__ == ""__main__"":\n    options = MonodepthOptions()\n    evaluate(options.parse())\n'"
gimp-plugins/monodepth2/export_gt_depth.py,0,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport argparse\nimport numpy as np\nimport PIL.Image as pil\n\nfrom utils import readlines\nfrom kitti_utils import generate_depth_map\n\n\ndef export_gt_depths_kitti():\n\n    parser = argparse.ArgumentParser(description=\'export_gt_depth\')\n\n    parser.add_argument(\'--data_path\',\n                        type=str,\n                        help=\'path to the root of the KITTI data\',\n                        required=True)\n    parser.add_argument(\'--split\',\n                        type=str,\n                        help=\'which split to export gt from\',\n                        required=True,\n                        choices=[""eigen"", ""eigen_benchmark""])\n    opt = parser.parse_args()\n\n    split_folder = os.path.join(os.path.dirname(__file__), ""splits"", opt.split)\n    lines = readlines(os.path.join(split_folder, ""test_files.txt""))\n\n    print(""Exporting ground truth depths for {}"".format(opt.split))\n\n    gt_depths = []\n    for line in lines:\n\n        folder, frame_id, _ = line.split()\n        frame_id = int(frame_id)\n\n        if opt.split == ""eigen"":\n            calib_dir = os.path.join(opt.data_path, folder.split(""/"")[0])\n            velo_filename = os.path.join(opt.data_path, folder,\n                                         ""velodyne_points/data"", ""{:010d}.bin"".format(frame_id))\n            gt_depth = generate_depth_map(calib_dir, velo_filename, 2, True)\n        elif opt.split == ""eigen_benchmark"":\n            gt_depth_path = os.path.join(opt.data_path, folder, ""proj_depth"",\n                                         ""groundtruth"", ""image_02"", ""{:010d}.png"".format(frame_id))\n            gt_depth = np.array(pil.open(gt_depth_path)).astype(np.float32) / 256\n\n        gt_depths.append(gt_depth.astype(np.float32))\n\n    output_path = os.path.join(split_folder, ""gt_depths.npz"")\n\n    print(""Saving to {}"".format(opt.split))\n\n    np.savez_compressed(output_path, data=np.array(gt_depths))\n\n\nif __name__ == ""__main__"":\n    export_gt_depths_kitti()\n'"
gimp-plugins/monodepth2/layers.py,39,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef disp_to_depth(disp, min_depth, max_depth):\n    """"""Convert network\'s sigmoid output into depth prediction\n    The formula for this conversion is given in the \'additional considerations\'\n    section of the paper.\n    """"""\n    min_disp = 1 / max_depth\n    max_disp = 1 / min_depth\n    scaled_disp = min_disp + (max_disp - min_disp) * disp\n    depth = 1 / scaled_disp\n    return scaled_disp, depth\n\n\ndef transformation_from_parameters(axisangle, translation, invert=False):\n    """"""Convert the network\'s (axisangle, translation) output into a 4x4 matrix\n    """"""\n    R = rot_from_axisangle(axisangle)\n    t = translation.clone()\n\n    if invert:\n        R = R.transpose(1, 2)\n        t *= -1\n\n    T = get_translation_matrix(t)\n\n    if invert:\n        M = torch.matmul(R, T)\n    else:\n        M = torch.matmul(T, R)\n\n    return M\n\n\ndef get_translation_matrix(translation_vector):\n    """"""Convert a translation vector into a 4x4 transformation matrix\n    """"""\n    T = torch.zeros(translation_vector.shape[0], 4, 4).to(device=translation_vector.device)\n\n    t = translation_vector.contiguous().view(-1, 3, 1)\n\n    T[:, 0, 0] = 1\n    T[:, 1, 1] = 1\n    T[:, 2, 2] = 1\n    T[:, 3, 3] = 1\n    T[:, :3, 3, None] = t\n\n    return T\n\n\ndef rot_from_axisangle(vec):\n    """"""Convert an axisangle rotation into a 4x4 transformation matrix\n    (adapted from https://github.com/Wallacoloo/printipi)\n    Input \'vec\' has to be Bx1x3\n    """"""\n    angle = torch.norm(vec, 2, 2, True)\n    axis = vec / (angle + 1e-7)\n\n    ca = torch.cos(angle)\n    sa = torch.sin(angle)\n    C = 1 - ca\n\n    x = axis[..., 0].unsqueeze(1)\n    y = axis[..., 1].unsqueeze(1)\n    z = axis[..., 2].unsqueeze(1)\n\n    xs = x * sa\n    ys = y * sa\n    zs = z * sa\n    xC = x * C\n    yC = y * C\n    zC = z * C\n    xyC = x * yC\n    yzC = y * zC\n    zxC = z * xC\n\n    rot = torch.zeros((vec.shape[0], 4, 4)).to(device=vec.device)\n\n    rot[:, 0, 0] = torch.squeeze(x * xC + ca)\n    rot[:, 0, 1] = torch.squeeze(xyC - zs)\n    rot[:, 0, 2] = torch.squeeze(zxC + ys)\n    rot[:, 1, 0] = torch.squeeze(xyC + zs)\n    rot[:, 1, 1] = torch.squeeze(y * yC + ca)\n    rot[:, 1, 2] = torch.squeeze(yzC - xs)\n    rot[:, 2, 0] = torch.squeeze(zxC - ys)\n    rot[:, 2, 1] = torch.squeeze(yzC + xs)\n    rot[:, 2, 2] = torch.squeeze(z * zC + ca)\n    rot[:, 3, 3] = 1\n\n    return rot\n\n\nclass ConvBlock(nn.Module):\n    """"""Layer to perform a convolution followed by ELU\n    """"""\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n\n        self.conv = Conv3x3(in_channels, out_channels)\n        self.nonlin = nn.ELU(inplace=True)\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = self.nonlin(out)\n        return out\n\n\nclass Conv3x3(nn.Module):\n    """"""Layer to pad and convolve input\n    """"""\n    def __init__(self, in_channels, out_channels, use_refl=True):\n        super(Conv3x3, self).__init__()\n\n        if use_refl:\n            self.pad = nn.ReflectionPad2d(1)\n        else:\n            self.pad = nn.ZeroPad2d(1)\n        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n\n    def forward(self, x):\n        out = self.pad(x)\n        out = self.conv(out)\n        return out\n\n\nclass BackprojectDepth(nn.Module):\n    """"""Layer to transform a depth image into a point cloud\n    """"""\n    def __init__(self, batch_size, height, width):\n        super(BackprojectDepth, self).__init__()\n\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n\n        meshgrid = np.meshgrid(range(self.width), range(self.height), indexing=\'xy\')\n        self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\n        self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),\n                                      requires_grad=False)\n\n        self.ones = nn.Parameter(torch.ones(self.batch_size, 1, self.height * self.width),\n                                 requires_grad=False)\n\n        self.pix_coords = torch.unsqueeze(torch.stack(\n            [self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0)\n        self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\n        self.pix_coords = nn.Parameter(torch.cat([self.pix_coords, self.ones], 1),\n                                       requires_grad=False)\n\n    def forward(self, depth, inv_K):\n        cam_points = torch.matmul(inv_K[:, :3, :3], self.pix_coords)\n        cam_points = depth.view(self.batch_size, 1, -1) * cam_points\n        cam_points = torch.cat([cam_points, self.ones], 1)\n\n        return cam_points\n\n\nclass Project3D(nn.Module):\n    """"""Layer which projects 3D points into a camera with intrinsics K and at position T\n    """"""\n    def __init__(self, batch_size, height, width, eps=1e-7):\n        super(Project3D, self).__init__()\n\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.eps = eps\n\n    def forward(self, points, K, T):\n        P = torch.matmul(K, T)[:, :3, :]\n\n        cam_points = torch.matmul(P, points)\n\n        pix_coords = cam_points[:, :2, :] / (cam_points[:, 2, :].unsqueeze(1) + self.eps)\n        pix_coords = pix_coords.view(self.batch_size, 2, self.height, self.width)\n        pix_coords = pix_coords.permute(0, 2, 3, 1)\n        pix_coords[..., 0] /= self.width - 1\n        pix_coords[..., 1] /= self.height - 1\n        pix_coords = (pix_coords - 0.5) * 2\n        return pix_coords\n\n\ndef upsample(x):\n    """"""Upsample input tensor by a factor of 2\n    """"""\n    return F.interpolate(x, scale_factor=2, mode=""nearest"")\n\n\ndef get_smooth_loss(disp, img):\n    """"""Computes the smoothness loss for a disparity image\n    The color image is used for edge-aware smoothness\n    """"""\n    grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n    grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n\n    grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n    grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n\n    grad_disp_x *= torch.exp(-grad_img_x)\n    grad_disp_y *= torch.exp(-grad_img_y)\n\n    return grad_disp_x.mean() + grad_disp_y.mean()\n\n\nclass SSIM(nn.Module):\n    """"""Layer to compute the SSIM loss between a pair of images\n    """"""\n    def __init__(self):\n        super(SSIM, self).__init__()\n        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n\n        self.refl = nn.ReflectionPad2d(1)\n\n        self.C1 = 0.01 ** 2\n        self.C2 = 0.03 ** 2\n\n    def forward(self, x, y):\n        x = self.refl(x)\n        y = self.refl(y)\n\n        mu_x = self.mu_x_pool(x)\n        mu_y = self.mu_y_pool(y)\n\n        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n\n        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n\n        return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)\n\n\ndef compute_depth_errors(gt, pred):\n    """"""Computation of error metrics between predicted and ground truth depths\n    """"""\n    thresh = torch.max((gt / pred), (pred / gt))\n    a1 = (thresh < 1.25     ).float().mean()\n    a2 = (thresh < 1.25 ** 2).float().mean()\n    a3 = (thresh < 1.25 ** 3).float().mean()\n\n    rmse = (gt - pred) ** 2\n    rmse = torch.sqrt(rmse.mean())\n\n    rmse_log = (torch.log(gt) - torch.log(pred)) ** 2\n    rmse_log = torch.sqrt(rmse_log.mean())\n\n    abs_rel = torch.mean(torch.abs(gt - pred) / gt)\n\n    sq_rel = torch.mean((gt - pred) ** 2 / gt)\n\n    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n'"
gimp-plugins/monodepth2/options.py,0,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport argparse\n\nfile_dir = os.path.dirname(__file__)  # the directory that options.py resides in\n\n\nclass MonodepthOptions:\n    def __init__(self):\n        self.parser = argparse.ArgumentParser(description=""Monodepthv2 options"")\n\n        # PATHS\n        self.parser.add_argument(""--data_path"",\n                                 type=str,\n                                 help=""path to the training data"",\n                                 default=os.path.join(file_dir, ""kitti_data""))\n        self.parser.add_argument(""--log_dir"",\n                                 type=str,\n                                 help=""log directory"",\n                                 default=os.path.join(os.path.expanduser(""~""), ""tmp""))\n\n        # TRAINING options\n        self.parser.add_argument(""--model_name"",\n                                 type=str,\n                                 help=""the name of the folder to save the model in"",\n                                 default=""mdp"")\n        self.parser.add_argument(""--split"",\n                                 type=str,\n                                 help=""which training split to use"",\n                                 choices=[""eigen_zhou"", ""eigen_full"", ""odom"", ""benchmark""],\n                                 default=""eigen_zhou"")\n        self.parser.add_argument(""--num_layers"",\n                                 type=int,\n                                 help=""number of resnet layers"",\n                                 default=18,\n                                 choices=[18, 34, 50, 101, 152])\n        self.parser.add_argument(""--dataset"",\n                                 type=str,\n                                 help=""dataset to train on"",\n                                 default=""kitti"",\n                                 choices=[""kitti"", ""kitti_odom"", ""kitti_depth"", ""kitti_test""])\n        self.parser.add_argument(""--png"",\n                                 help=""if set, trains from raw KITTI png files (instead of jpgs)"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--height"",\n                                 type=int,\n                                 help=""input image height"",\n                                 default=192)\n        self.parser.add_argument(""--width"",\n                                 type=int,\n                                 help=""input image width"",\n                                 default=640)\n        self.parser.add_argument(""--disparity_smoothness"",\n                                 type=float,\n                                 help=""disparity smoothness weight"",\n                                 default=1e-3)\n        self.parser.add_argument(""--scales"",\n                                 nargs=""+"",\n                                 type=int,\n                                 help=""scales used in the loss"",\n                                 default=[0, 1, 2, 3])\n        self.parser.add_argument(""--min_depth"",\n                                 type=float,\n                                 help=""minimum depth"",\n                                 default=0.1)\n        self.parser.add_argument(""--max_depth"",\n                                 type=float,\n                                 help=""maximum depth"",\n                                 default=100.0)\n        self.parser.add_argument(""--use_stereo"",\n                                 help=""if set, uses stereo pair for training"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--frame_ids"",\n                                 nargs=""+"",\n                                 type=int,\n                                 help=""frames to load"",\n                                 default=[0, -1, 1])\n\n        # OPTIMIZATION options\n        self.parser.add_argument(""--batch_size"",\n                                 type=int,\n                                 help=""batch size"",\n                                 default=12)\n        self.parser.add_argument(""--learning_rate"",\n                                 type=float,\n                                 help=""learning rate"",\n                                 default=1e-4)\n        self.parser.add_argument(""--num_epochs"",\n                                 type=int,\n                                 help=""number of epochs"",\n                                 default=20)\n        self.parser.add_argument(""--scheduler_step_size"",\n                                 type=int,\n                                 help=""step size of the scheduler"",\n                                 default=15)\n\n        # ABLATION options\n        self.parser.add_argument(""--v1_multiscale"",\n                                 help=""if set, uses monodepth v1 multiscale"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--avg_reprojection"",\n                                 help=""if set, uses average reprojection loss"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--disable_automasking"",\n                                 help=""if set, doesn\'t do auto-masking"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--predictive_mask"",\n                                 help=""if set, uses a predictive masking scheme as in Zhou et al"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--no_ssim"",\n                                 help=""if set, disables ssim in the loss"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--weights_init"",\n                                 type=str,\n                                 help=""pretrained or scratch"",\n                                 default=""pretrained"",\n                                 choices=[""pretrained"", ""scratch""])\n        self.parser.add_argument(""--pose_model_input"",\n                                 type=str,\n                                 help=""how many images the pose network gets"",\n                                 default=""pairs"",\n                                 choices=[""pairs"", ""all""])\n        self.parser.add_argument(""--pose_model_type"",\n                                 type=str,\n                                 help=""normal or shared"",\n                                 default=""separate_resnet"",\n                                 choices=[""posecnn"", ""separate_resnet"", ""shared""])\n\n        # SYSTEM options\n        self.parser.add_argument(""--no_cuda"",\n                                 help=""if set disables CUDA"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--num_workers"",\n                                 type=int,\n                                 help=""number of dataloader workers"",\n                                 default=12)\n\n        # LOADING options\n        self.parser.add_argument(""--load_weights_folder"",\n                                 type=str,\n                                 help=""name of model to load"")\n        self.parser.add_argument(""--models_to_load"",\n                                 nargs=""+"",\n                                 type=str,\n                                 help=""models to load"",\n                                 default=[""encoder"", ""depth"", ""pose_encoder"", ""pose""])\n\n        # LOGGING options\n        self.parser.add_argument(""--log_frequency"",\n                                 type=int,\n                                 help=""number of batches between each tensorboard log"",\n                                 default=250)\n        self.parser.add_argument(""--save_frequency"",\n                                 type=int,\n                                 help=""number of epochs between each save"",\n                                 default=1)\n\n        # EVALUATION options\n        self.parser.add_argument(""--eval_stereo"",\n                                 help=""if set evaluates in stereo mode"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--eval_mono"",\n                                 help=""if set evaluates in mono mode"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--disable_median_scaling"",\n                                 help=""if set disables median scaling in evaluation"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--pred_depth_scale_factor"",\n                                 help=""if set multiplies predictions by this number"",\n                                 type=float,\n                                 default=1)\n        self.parser.add_argument(""--ext_disp_to_eval"",\n                                 type=str,\n                                 help=""optional path to a .npy disparities file to evaluate"")\n        self.parser.add_argument(""--eval_split"",\n                                 type=str,\n                                 default=""eigen"",\n                                 choices=[\n                                    ""eigen"", ""eigen_benchmark"", ""benchmark"", ""odom_9"", ""odom_10""],\n                                 help=""which split to run eval on"")\n        self.parser.add_argument(""--save_pred_disps"",\n                                 help=""if set saves predicted disparities"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--no_eval"",\n                                 help=""if set disables evaluation"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--eval_eigen_to_benchmark"",\n                                 help=""if set assume we are loading eigen results from npy but ""\n                                      ""we want to evaluate using the new benchmark."",\n                                 action=""store_true"")\n        self.parser.add_argument(""--eval_out_dir"",\n                                 help=""if set will output the disparities to this folder"",\n                                 type=str)\n        self.parser.add_argument(""--post_process"",\n                                 help=""if set will perform the flipping post processing ""\n                                      ""from the original monodepth paper"",\n                                 action=""store_true"")\n\n    def parse(self):\n        self.options = self.parser.parse_args()\n        return self.options\n'"
gimp-plugins/monodepth2/test_simple.py,7,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport sys\nimport glob\nimport argparse\nimport numpy as np\nimport PIL.Image as pil\n# import cv2\nimport matplotlib as mpl\nimport matplotlib.cm as cm\n\nimport torch\nfrom torchvision import transforms, datasets\n\nimport networks\nfrom layers import disp_to_depth\nfrom utils import download_model_if_doesnt_exist\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Simple testing funtion for Monodepthv2 models.\')\n\n    parser.add_argument(\'--image_path\', type=str,\n                        help=\'path to a test image or folder of images\', required=True)\n    parser.add_argument(\'--model_name\', type=str,\n                        help=\'name of a pretrained model to use\',\n                        choices=[\n                            ""mono_640x192"",\n                            ""stereo_640x192"",\n                            ""mono+stereo_640x192"",\n                            ""mono_no_pt_640x192"",\n                            ""stereo_no_pt_640x192"",\n                            ""mono+stereo_no_pt_640x192"",\n                            ""mono_1024x320"",\n                            ""stereo_1024x320"",\n                            ""mono+stereo_1024x320""])\n    parser.add_argument(\'--ext\', type=str,\n                        help=\'image extension to search for in folder\', default=""jpg"")\n    parser.add_argument(""--no_cuda"",\n                        help=\'if set, disables CUDA\',\n                        action=\'store_true\')\n\n    return parser.parse_args()\n\n\ndef test_simple(args):\n    """"""Function to predict for a single image or folder of images\n    """"""\n    assert args.model_name is not None, \\\n        ""You must specify the --model_name parameter; see README.md for an example""\n\n    if torch.cuda.is_available() and not args.no_cuda:\n        device = torch.device(""cuda"")\n    else:\n        device = torch.device(""cpu"")\n\n    download_model_if_doesnt_exist(args.model_name)\n    model_path = os.path.join(""models"", args.model_name)\n    print(""-> Loading model from "", model_path)\n    encoder_path = os.path.join(model_path, ""encoder.pth"")\n    depth_decoder_path = os.path.join(model_path, ""depth.pth"")\n\n    # LOADING PRETRAINED MODEL\n    print(""   Loading pretrained encoder"")\n    encoder = networks.ResnetEncoder(18, False)\n    loaded_dict_enc = torch.load(encoder_path, map_location=device)\n\n    # extract the height and width of image that this model was trained with\n    feed_height = loaded_dict_enc[\'height\']\n    feed_width = loaded_dict_enc[\'width\']\n    filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n    encoder.load_state_dict(filtered_dict_enc)\n    encoder.to(device)\n    encoder.eval()\n\n    print(""   Loading pretrained decoder"")\n    depth_decoder = networks.DepthDecoder(\n        num_ch_enc=encoder.num_ch_enc, scales=range(4))\n\n    loaded_dict = torch.load(depth_decoder_path, map_location=device)\n    depth_decoder.load_state_dict(loaded_dict)\n\n    depth_decoder.to(device)\n    depth_decoder.eval()\n\n    # FINDING INPUT IMAGES\n    if os.path.isfile(args.image_path):\n        # Only testing on a single image\n        paths = [args.image_path]\n        output_directory = os.path.dirname(args.image_path)\n    elif os.path.isdir(args.image_path):\n        # Searching folder for images\n        paths = glob.glob(os.path.join(args.image_path, \'*.{}\'.format(args.ext)))\n        output_directory = args.image_path\n    else:\n        raise Exception(""Can not find args.image_path: {}"".format(args.image_path))\n\n    print(""-> Predicting on {:d} test images"".format(len(paths)))\n\n    # PREDICTING ON EACH IMAGE IN TURN\n    with torch.no_grad():\n        for idx, image_path in enumerate(paths):\n\n            if image_path.endswith(""_disp.jpg""):\n                # don\'t try to predict disparity for a disparity image!\n                continue\n\n            # Load image and preprocess\n            # input_image = cv2.imread(image_path)\n            # input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n            input_image = pil.open(image_path).convert(\'RGB\')\n            original_width, original_height = input_image.size\n            # input_image = cv2.resize(input_image, (feed_width, feed_height))\n            input_image = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n            input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n\n            # PREDICTION\n            input_image = input_image.to(device)\n            features = encoder(input_image)\n            outputs = depth_decoder(features)\n\n            disp = outputs[(""disp"", 0)]\n            disp_resized = torch.nn.functional.interpolate(\n                disp, (original_height, original_width), mode=""bilinear"", align_corners=False)\n\n            # Saving numpy file\n            output_name = os.path.splitext(os.path.basename(image_path))[0]\n            name_dest_npy = os.path.join(output_directory, ""{}_disp.npy"".format(output_name))\n            scaled_disp, _ = disp_to_depth(disp, 0.1, 100)\n            np.save(name_dest_npy, scaled_disp.cpu().numpy())\n\n            # Saving colormapped depth image\n            disp_resized_np = disp_resized.squeeze().cpu().numpy()\n            vmax = np.percentile(disp_resized_np, 95)\n            normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=vmax)\n            mapper = cm.ScalarMappable(norm=normalizer, cmap=\'magma\')\n            colormapped_im = (mapper.to_rgba(disp_resized_np)[:, :, :3] * 255).astype(np.uint8)\n            im = pil.fromarray(colormapped_im)\n\n            name_dest_im = os.path.join(output_directory, ""{}_disp.jpeg"".format(output_name))\n            im.save(name_dest_im)\n            # cv2.imwrite(\'/Users/kritiksoman/Downloads/gimp-plugins/out5.jpg\',cv2.cvtColor(colormapped_im, cv2.COLOR_RGB2BGR))\n\n            print(""   Processed {:d} of {:d} images - saved prediction to {}"".format(\n                idx + 1, len(paths), name_dest_im))\n\n    print(\'-> Done!\')\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    test_simple(args)\n'"
gimp-plugins/monodepth2/train.py,0,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom trainer import Trainer\nfrom options import MonodepthOptions\n\noptions = MonodepthOptions()\nopts = options.parse()\n\n\nif __name__ == ""__main__"":\n    trainer = Trainer(opts)\n    trainer.train()\n'"
gimp-plugins/monodepth2/trainer.py,25,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport time\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tensorboardX import SummaryWriter\n\nimport json\n\nfrom utils import *\nfrom kitti_utils import *\nfrom layers import *\n\nimport datasets\nimport networks\nfrom IPython import embed\n\n\nclass Trainer:\n    def __init__(self, options):\n        self.opt = options\n        self.log_path = os.path.join(self.opt.log_dir, self.opt.model_name)\n\n        # checking height and width are multiples of 32\n        assert self.opt.height % 32 == 0, ""\'height\' must be a multiple of 32""\n        assert self.opt.width % 32 == 0, ""\'width\' must be a multiple of 32""\n\n        self.models = {}\n        self.parameters_to_train = []\n\n        self.device = torch.device(""cpu"" if self.opt.no_cuda else ""cuda"")\n\n        self.num_scales = len(self.opt.scales)\n        self.num_input_frames = len(self.opt.frame_ids)\n        self.num_pose_frames = 2 if self.opt.pose_model_input == ""pairs"" else self.num_input_frames\n\n        assert self.opt.frame_ids[0] == 0, ""frame_ids must start with 0""\n\n        self.use_pose_net = not (self.opt.use_stereo and self.opt.frame_ids == [0])\n\n        if self.opt.use_stereo:\n            self.opt.frame_ids.append(""s"")\n\n        self.models[""encoder""] = networks.ResnetEncoder(\n            self.opt.num_layers, self.opt.weights_init == ""pretrained"")\n        self.models[""encoder""].to(self.device)\n        self.parameters_to_train += list(self.models[""encoder""].parameters())\n\n        self.models[""depth""] = networks.DepthDecoder(\n            self.models[""encoder""].num_ch_enc, self.opt.scales)\n        self.models[""depth""].to(self.device)\n        self.parameters_to_train += list(self.models[""depth""].parameters())\n\n        if self.use_pose_net:\n            if self.opt.pose_model_type == ""separate_resnet"":\n                self.models[""pose_encoder""] = networks.ResnetEncoder(\n                    self.opt.num_layers,\n                    self.opt.weights_init == ""pretrained"",\n                    num_input_images=self.num_pose_frames)\n\n                self.models[""pose_encoder""].to(self.device)\n                self.parameters_to_train += list(self.models[""pose_encoder""].parameters())\n\n                self.models[""pose""] = networks.PoseDecoder(\n                    self.models[""pose_encoder""].num_ch_enc,\n                    num_input_features=1,\n                    num_frames_to_predict_for=2)\n\n            elif self.opt.pose_model_type == ""shared"":\n                self.models[""pose""] = networks.PoseDecoder(\n                    self.models[""encoder""].num_ch_enc, self.num_pose_frames)\n\n            elif self.opt.pose_model_type == ""posecnn"":\n                self.models[""pose""] = networks.PoseCNN(\n                    self.num_input_frames if self.opt.pose_model_input == ""all"" else 2)\n\n            self.models[""pose""].to(self.device)\n            self.parameters_to_train += list(self.models[""pose""].parameters())\n\n        if self.opt.predictive_mask:\n            assert self.opt.disable_automasking, \\\n                ""When using predictive_mask, please disable automasking with --disable_automasking""\n\n            # Our implementation of the predictive masking baseline has the the same architecture\n            # as our depth decoder. We predict a separate mask for each source frame.\n            self.models[""predictive_mask""] = networks.DepthDecoder(\n                self.models[""encoder""].num_ch_enc, self.opt.scales,\n                num_output_channels=(len(self.opt.frame_ids) - 1))\n            self.models[""predictive_mask""].to(self.device)\n            self.parameters_to_train += list(self.models[""predictive_mask""].parameters())\n\n        self.model_optimizer = optim.Adam(self.parameters_to_train, self.opt.learning_rate)\n        self.model_lr_scheduler = optim.lr_scheduler.StepLR(\n            self.model_optimizer, self.opt.scheduler_step_size, 0.1)\n\n        if self.opt.load_weights_folder is not None:\n            self.load_model()\n\n        print(""Training model named:\\n  "", self.opt.model_name)\n        print(""Models and tensorboard events files are saved to:\\n  "", self.opt.log_dir)\n        print(""Training is using:\\n  "", self.device)\n\n        # data\n        datasets_dict = {""kitti"": datasets.KITTIRAWDataset,\n                         ""kitti_odom"": datasets.KITTIOdomDataset}\n        self.dataset = datasets_dict[self.opt.dataset]\n\n        fpath = os.path.join(os.path.dirname(__file__), ""splits"", self.opt.split, ""{}_files.txt"")\n\n        train_filenames = readlines(fpath.format(""train""))\n        val_filenames = readlines(fpath.format(""val""))\n        img_ext = \'.png\' if self.opt.png else \'.jpg\'\n\n        num_train_samples = len(train_filenames)\n        self.num_total_steps = num_train_samples // self.opt.batch_size * self.opt.num_epochs\n\n        train_dataset = self.dataset(\n            self.opt.data_path, train_filenames, self.opt.height, self.opt.width,\n            self.opt.frame_ids, 4, is_train=True, img_ext=img_ext)\n        self.train_loader = DataLoader(\n            train_dataset, self.opt.batch_size, True,\n            num_workers=self.opt.num_workers, pin_memory=True, drop_last=True)\n        val_dataset = self.dataset(\n            self.opt.data_path, val_filenames, self.opt.height, self.opt.width,\n            self.opt.frame_ids, 4, is_train=False, img_ext=img_ext)\n        self.val_loader = DataLoader(\n            val_dataset, self.opt.batch_size, True,\n            num_workers=self.opt.num_workers, pin_memory=True, drop_last=True)\n        self.val_iter = iter(self.val_loader)\n\n        self.writers = {}\n        for mode in [""train"", ""val""]:\n            self.writers[mode] = SummaryWriter(os.path.join(self.log_path, mode))\n\n        if not self.opt.no_ssim:\n            self.ssim = SSIM()\n            self.ssim.to(self.device)\n\n        self.backproject_depth = {}\n        self.project_3d = {}\n        for scale in self.opt.scales:\n            h = self.opt.height // (2 ** scale)\n            w = self.opt.width // (2 ** scale)\n\n            self.backproject_depth[scale] = BackprojectDepth(self.opt.batch_size, h, w)\n            self.backproject_depth[scale].to(self.device)\n\n            self.project_3d[scale] = Project3D(self.opt.batch_size, h, w)\n            self.project_3d[scale].to(self.device)\n\n        self.depth_metric_names = [\n            ""de/abs_rel"", ""de/sq_rel"", ""de/rms"", ""de/log_rms"", ""da/a1"", ""da/a2"", ""da/a3""]\n\n        print(""Using split:\\n  "", self.opt.split)\n        print(""There are {:d} training items and {:d} validation items\\n"".format(\n            len(train_dataset), len(val_dataset)))\n\n        self.save_opts()\n\n    def set_train(self):\n        """"""Convert all models to training mode\n        """"""\n        for m in self.models.values():\n            m.train()\n\n    def set_eval(self):\n        """"""Convert all models to testing/evaluation mode\n        """"""\n        for m in self.models.values():\n            m.eval()\n\n    def train(self):\n        """"""Run the entire training pipeline\n        """"""\n        self.epoch = 0\n        self.step = 0\n        self.start_time = time.time()\n        for self.epoch in range(self.opt.num_epochs):\n            self.run_epoch()\n            if (self.epoch + 1) % self.opt.save_frequency == 0:\n                self.save_model()\n\n    def run_epoch(self):\n        """"""Run a single epoch of training and validation\n        """"""\n        self.model_lr_scheduler.step()\n\n        print(""Training"")\n        self.set_train()\n\n        for batch_idx, inputs in enumerate(self.train_loader):\n\n            before_op_time = time.time()\n\n            outputs, losses = self.process_batch(inputs)\n\n            self.model_optimizer.zero_grad()\n            losses[""loss""].backward()\n            self.model_optimizer.step()\n\n            duration = time.time() - before_op_time\n\n            # log less frequently after the first 2000 steps to save time & disk space\n            early_phase = batch_idx % self.opt.log_frequency == 0 and self.step < 2000\n            late_phase = self.step % 2000 == 0\n\n            if early_phase or late_phase:\n                self.log_time(batch_idx, duration, losses[""loss""].cpu().data)\n\n                if ""depth_gt"" in inputs:\n                    self.compute_depth_losses(inputs, outputs, losses)\n\n                self.log(""train"", inputs, outputs, losses)\n                self.val()\n\n            self.step += 1\n\n    def process_batch(self, inputs):\n        """"""Pass a minibatch through the network and generate images and losses\n        """"""\n        for key, ipt in inputs.items():\n            inputs[key] = ipt.to(self.device)\n\n        if self.opt.pose_model_type == ""shared"":\n            # If we are using a shared encoder for both depth and pose (as advocated\n            # in monodepthv1), then all images are fed separately through the depth encoder.\n            all_color_aug = torch.cat([inputs[(""color_aug"", i, 0)] for i in self.opt.frame_ids])\n            all_features = self.models[""encoder""](all_color_aug)\n            all_features = [torch.split(f, self.opt.batch_size) for f in all_features]\n\n            features = {}\n            for i, k in enumerate(self.opt.frame_ids):\n                features[k] = [f[i] for f in all_features]\n\n            outputs = self.models[""depth""](features[0])\n        else:\n            # Otherwise, we only feed the image with frame_id 0 through the depth encoder\n            features = self.models[""encoder""](inputs[""color_aug"", 0, 0])\n            outputs = self.models[""depth""](features)\n\n        if self.opt.predictive_mask:\n            outputs[""predictive_mask""] = self.models[""predictive_mask""](features)\n\n        if self.use_pose_net:\n            outputs.update(self.predict_poses(inputs, features))\n\n        self.generate_images_pred(inputs, outputs)\n        losses = self.compute_losses(inputs, outputs)\n\n        return outputs, losses\n\n    def predict_poses(self, inputs, features):\n        """"""Predict poses between input frames for monocular sequences.\n        """"""\n        outputs = {}\n        if self.num_pose_frames == 2:\n            # In this setting, we compute the pose to each source frame via a\n            # separate forward pass through the pose network.\n\n            # select what features the pose network takes as input\n            if self.opt.pose_model_type == ""shared"":\n                pose_feats = {f_i: features[f_i] for f_i in self.opt.frame_ids}\n            else:\n                pose_feats = {f_i: inputs[""color_aug"", f_i, 0] for f_i in self.opt.frame_ids}\n\n            for f_i in self.opt.frame_ids[1:]:\n                if f_i != ""s"":\n                    # To maintain ordering we always pass frames in temporal order\n                    if f_i < 0:\n                        pose_inputs = [pose_feats[f_i], pose_feats[0]]\n                    else:\n                        pose_inputs = [pose_feats[0], pose_feats[f_i]]\n\n                    if self.opt.pose_model_type == ""separate_resnet"":\n                        pose_inputs = [self.models[""pose_encoder""](torch.cat(pose_inputs, 1))]\n                    elif self.opt.pose_model_type == ""posecnn"":\n                        pose_inputs = torch.cat(pose_inputs, 1)\n\n                    axisangle, translation = self.models[""pose""](pose_inputs)\n                    outputs[(""axisangle"", 0, f_i)] = axisangle\n                    outputs[(""translation"", 0, f_i)] = translation\n\n                    # Invert the matrix if the frame id is negative\n                    outputs[(""cam_T_cam"", 0, f_i)] = transformation_from_parameters(\n                        axisangle[:, 0], translation[:, 0], invert=(f_i < 0))\n\n        else:\n            # Here we input all frames to the pose net (and predict all poses) together\n            if self.opt.pose_model_type in [""separate_resnet"", ""posecnn""]:\n                pose_inputs = torch.cat(\n                    [inputs[(""color_aug"", i, 0)] for i in self.opt.frame_ids if i != ""s""], 1)\n\n                if self.opt.pose_model_type == ""separate_resnet"":\n                    pose_inputs = [self.models[""pose_encoder""](pose_inputs)]\n\n            elif self.opt.pose_model_type == ""shared"":\n                pose_inputs = [features[i] for i in self.opt.frame_ids if i != ""s""]\n\n            axisangle, translation = self.models[""pose""](pose_inputs)\n\n            for i, f_i in enumerate(self.opt.frame_ids[1:]):\n                if f_i != ""s"":\n                    outputs[(""axisangle"", 0, f_i)] = axisangle\n                    outputs[(""translation"", 0, f_i)] = translation\n                    outputs[(""cam_T_cam"", 0, f_i)] = transformation_from_parameters(\n                        axisangle[:, i], translation[:, i])\n\n        return outputs\n\n    def val(self):\n        """"""Validate the model on a single minibatch\n        """"""\n        self.set_eval()\n        try:\n            inputs = self.val_iter.next()\n        except StopIteration:\n            self.val_iter = iter(self.val_loader)\n            inputs = self.val_iter.next()\n\n        with torch.no_grad():\n            outputs, losses = self.process_batch(inputs)\n\n            if ""depth_gt"" in inputs:\n                self.compute_depth_losses(inputs, outputs, losses)\n\n            self.log(""val"", inputs, outputs, losses)\n            del inputs, outputs, losses\n\n        self.set_train()\n\n    def generate_images_pred(self, inputs, outputs):\n        """"""Generate the warped (reprojected) color images for a minibatch.\n        Generated images are saved into the `outputs` dictionary.\n        """"""\n        for scale in self.opt.scales:\n            disp = outputs[(""disp"", scale)]\n            if self.opt.v1_multiscale:\n                source_scale = scale\n            else:\n                disp = F.interpolate(\n                    disp, [self.opt.height, self.opt.width], mode=""bilinear"", align_corners=False)\n                source_scale = 0\n\n            _, depth = disp_to_depth(disp, self.opt.min_depth, self.opt.max_depth)\n\n            outputs[(""depth"", 0, scale)] = depth\n\n            for i, frame_id in enumerate(self.opt.frame_ids[1:]):\n\n                if frame_id == ""s"":\n                    T = inputs[""stereo_T""]\n                else:\n                    T = outputs[(""cam_T_cam"", 0, frame_id)]\n\n                # from the authors of https://arxiv.org/abs/1712.00175\n                if self.opt.pose_model_type == ""posecnn"":\n\n                    axisangle = outputs[(""axisangle"", 0, frame_id)]\n                    translation = outputs[(""translation"", 0, frame_id)]\n\n                    inv_depth = 1 / depth\n                    mean_inv_depth = inv_depth.mean(3, True).mean(2, True)\n\n                    T = transformation_from_parameters(\n                        axisangle[:, 0], translation[:, 0] * mean_inv_depth[:, 0], frame_id < 0)\n\n                cam_points = self.backproject_depth[source_scale](\n                    depth, inputs[(""inv_K"", source_scale)])\n                pix_coords = self.project_3d[source_scale](\n                    cam_points, inputs[(""K"", source_scale)], T)\n\n                outputs[(""sample"", frame_id, scale)] = pix_coords\n\n                outputs[(""color"", frame_id, scale)] = F.grid_sample(\n                    inputs[(""color"", frame_id, source_scale)],\n                    outputs[(""sample"", frame_id, scale)],\n                    padding_mode=""border"")\n\n                if not self.opt.disable_automasking:\n                    outputs[(""color_identity"", frame_id, scale)] = \\\n                        inputs[(""color"", frame_id, source_scale)]\n\n    def compute_reprojection_loss(self, pred, target):\n        """"""Computes reprojection loss between a batch of predicted and target images\n        """"""\n        abs_diff = torch.abs(target - pred)\n        l1_loss = abs_diff.mean(1, True)\n\n        if self.opt.no_ssim:\n            reprojection_loss = l1_loss\n        else:\n            ssim_loss = self.ssim(pred, target).mean(1, True)\n            reprojection_loss = 0.85 * ssim_loss + 0.15 * l1_loss\n\n        return reprojection_loss\n\n    def compute_losses(self, inputs, outputs):\n        """"""Compute the reprojection and smoothness losses for a minibatch\n        """"""\n        losses = {}\n        total_loss = 0\n\n        for scale in self.opt.scales:\n            loss = 0\n            reprojection_losses = []\n\n            if self.opt.v1_multiscale:\n                source_scale = scale\n            else:\n                source_scale = 0\n\n            disp = outputs[(""disp"", scale)]\n            color = inputs[(""color"", 0, scale)]\n            target = inputs[(""color"", 0, source_scale)]\n\n            for frame_id in self.opt.frame_ids[1:]:\n                pred = outputs[(""color"", frame_id, scale)]\n                reprojection_losses.append(self.compute_reprojection_loss(pred, target))\n\n            reprojection_losses = torch.cat(reprojection_losses, 1)\n\n            if not self.opt.disable_automasking:\n                identity_reprojection_losses = []\n                for frame_id in self.opt.frame_ids[1:]:\n                    pred = inputs[(""color"", frame_id, source_scale)]\n                    identity_reprojection_losses.append(\n                        self.compute_reprojection_loss(pred, target))\n\n                identity_reprojection_losses = torch.cat(identity_reprojection_losses, 1)\n\n                if self.opt.avg_reprojection:\n                    identity_reprojection_loss = identity_reprojection_losses.mean(1, keepdim=True)\n                else:\n                    # save both images, and do min all at once below\n                    identity_reprojection_loss = identity_reprojection_losses\n\n            elif self.opt.predictive_mask:\n                # use the predicted mask\n                mask = outputs[""predictive_mask""][""disp"", scale]\n                if not self.opt.v1_multiscale:\n                    mask = F.interpolate(\n                        mask, [self.opt.height, self.opt.width],\n                        mode=""bilinear"", align_corners=False)\n\n                reprojection_losses *= mask\n\n                # add a loss pushing mask to 1 (using nn.BCELoss for stability)\n                weighting_loss = 0.2 * nn.BCELoss()(mask, torch.ones(mask.shape).cuda())\n                loss += weighting_loss.mean()\n\n            if self.opt.avg_reprojection:\n                reprojection_loss = reprojection_losses.mean(1, keepdim=True)\n            else:\n                reprojection_loss = reprojection_losses\n\n            if not self.opt.disable_automasking:\n                # add random numbers to break ties\n                identity_reprojection_loss += torch.randn(\n                    identity_reprojection_loss.shape).cuda() * 0.00001\n\n                combined = torch.cat((identity_reprojection_loss, reprojection_loss), dim=1)\n            else:\n                combined = reprojection_loss\n\n            if combined.shape[1] == 1:\n                to_optimise = combined\n            else:\n                to_optimise, idxs = torch.min(combined, dim=1)\n\n            if not self.opt.disable_automasking:\n                outputs[""identity_selection/{}"".format(scale)] = (\n                    idxs > identity_reprojection_loss.shape[1] - 1).float()\n\n            loss += to_optimise.mean()\n\n            mean_disp = disp.mean(2, True).mean(3, True)\n            norm_disp = disp / (mean_disp + 1e-7)\n            smooth_loss = get_smooth_loss(norm_disp, color)\n\n            loss += self.opt.disparity_smoothness * smooth_loss / (2 ** scale)\n            total_loss += loss\n            losses[""loss/{}"".format(scale)] = loss\n\n        total_loss /= self.num_scales\n        losses[""loss""] = total_loss\n        return losses\n\n    def compute_depth_losses(self, inputs, outputs, losses):\n        """"""Compute depth metrics, to allow monitoring during training\n\n        This isn\'t particularly accurate as it averages over the entire batch,\n        so is only used to give an indication of validation performance\n        """"""\n        depth_pred = outputs[(""depth"", 0, 0)]\n        depth_pred = torch.clamp(F.interpolate(\n            depth_pred, [375, 1242], mode=""bilinear"", align_corners=False), 1e-3, 80)\n        depth_pred = depth_pred.detach()\n\n        depth_gt = inputs[""depth_gt""]\n        mask = depth_gt > 0\n\n        # garg/eigen crop\n        crop_mask = torch.zeros_like(mask)\n        crop_mask[:, :, 153:371, 44:1197] = 1\n        mask = mask * crop_mask\n\n        depth_gt = depth_gt[mask]\n        depth_pred = depth_pred[mask]\n        depth_pred *= torch.median(depth_gt) / torch.median(depth_pred)\n\n        depth_pred = torch.clamp(depth_pred, min=1e-3, max=80)\n\n        depth_errors = compute_depth_errors(depth_gt, depth_pred)\n\n        for i, metric in enumerate(self.depth_metric_names):\n            losses[metric] = np.array(depth_errors[i].cpu())\n\n    def log_time(self, batch_idx, duration, loss):\n        """"""Print a logging statement to the terminal\n        """"""\n        samples_per_sec = self.opt.batch_size / duration\n        time_sofar = time.time() - self.start_time\n        training_time_left = (\n            self.num_total_steps / self.step - 1.0) * time_sofar if self.step > 0 else 0\n        print_string = ""epoch {:>3} | batch {:>6} | examples/s: {:5.1f}"" + \\\n            "" | loss: {:.5f} | time elapsed: {} | time left: {}""\n        print(print_string.format(self.epoch, batch_idx, samples_per_sec, loss,\n                                  sec_to_hm_str(time_sofar), sec_to_hm_str(training_time_left)))\n\n    def log(self, mode, inputs, outputs, losses):\n        """"""Write an event to the tensorboard events file\n        """"""\n        writer = self.writers[mode]\n        for l, v in losses.items():\n            writer.add_scalar(""{}"".format(l), v, self.step)\n\n        for j in range(min(4, self.opt.batch_size)):  # write a maxmimum of four images\n            for s in self.opt.scales:\n                for frame_id in self.opt.frame_ids:\n                    writer.add_image(\n                        ""color_{}_{}/{}"".format(frame_id, s, j),\n                        inputs[(""color"", frame_id, s)][j].data, self.step)\n                    if s == 0 and frame_id != 0:\n                        writer.add_image(\n                            ""color_pred_{}_{}/{}"".format(frame_id, s, j),\n                            outputs[(""color"", frame_id, s)][j].data, self.step)\n\n                writer.add_image(\n                    ""disp_{}/{}"".format(s, j),\n                    normalize_image(outputs[(""disp"", s)][j]), self.step)\n\n                if self.opt.predictive_mask:\n                    for f_idx, frame_id in enumerate(self.opt.frame_ids[1:]):\n                        writer.add_image(\n                            ""predictive_mask_{}_{}/{}"".format(frame_id, s, j),\n                            outputs[""predictive_mask""][(""disp"", s)][j, f_idx][None, ...],\n                            self.step)\n\n                elif not self.opt.disable_automasking:\n                    writer.add_image(\n                        ""automask_{}/{}"".format(s, j),\n                        outputs[""identity_selection/{}"".format(s)][j][None, ...], self.step)\n\n    def save_opts(self):\n        """"""Save options to disk so we know what we ran this experiment with\n        """"""\n        models_dir = os.path.join(self.log_path, ""models"")\n        if not os.path.exists(models_dir):\n            os.makedirs(models_dir)\n        to_save = self.opt.__dict__.copy()\n\n        with open(os.path.join(models_dir, \'opt.json\'), \'w\') as f:\n            json.dump(to_save, f, indent=2)\n\n    def save_model(self):\n        """"""Save model weights to disk\n        """"""\n        save_folder = os.path.join(self.log_path, ""models"", ""weights_{}"".format(self.epoch))\n        if not os.path.exists(save_folder):\n            os.makedirs(save_folder)\n\n        for model_name, model in self.models.items():\n            save_path = os.path.join(save_folder, ""{}.pth"".format(model_name))\n            to_save = model.state_dict()\n            if model_name == \'encoder\':\n                # save the sizes - these are needed at prediction time\n                to_save[\'height\'] = self.opt.height\n                to_save[\'width\'] = self.opt.width\n                to_save[\'use_stereo\'] = self.opt.use_stereo\n            torch.save(to_save, save_path)\n\n        save_path = os.path.join(save_folder, ""{}.pth"".format(""adam""))\n        torch.save(self.model_optimizer.state_dict(), save_path)\n\n    def load_model(self):\n        """"""Load model(s) from disk\n        """"""\n        self.opt.load_weights_folder = os.path.expanduser(self.opt.load_weights_folder)\n\n        assert os.path.isdir(self.opt.load_weights_folder), \\\n            ""Cannot find folder {}"".format(self.opt.load_weights_folder)\n        print(""loading model from folder {}"".format(self.opt.load_weights_folder))\n\n        for n in self.opt.models_to_load:\n            print(""Loading {} weights..."".format(n))\n            path = os.path.join(self.opt.load_weights_folder, ""{}.pth"".format(n))\n            model_dict = self.models[n].state_dict()\n            pretrained_dict = torch.load(path)\n            pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            self.models[n].load_state_dict(model_dict)\n\n        # loading adam state\n        optimizer_load_path = os.path.join(self.opt.load_weights_folder, ""adam.pth"")\n        if os.path.isfile(optimizer_load_path):\n            print(""Loading Adam weights"")\n            optimizer_dict = torch.load(optimizer_load_path)\n            self.model_optimizer.load_state_dict(optimizer_dict)\n        else:\n            print(""Cannot find Adam weights so Adam is randomly initialized"")\n'"
gimp-plugins/monodepth2/utils.py,0,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\nimport os\nimport hashlib\nimport zipfile\nfrom six.moves import urllib\n\n\ndef readlines(filename):\n    """"""Read all the lines in a text file and return as a list\n    """"""\n    with open(filename, \'r\') as f:\n        lines = f.read().splitlines()\n    return lines\n\n\ndef normalize_image(x):\n    """"""Rescale image pixels to span range [0, 1]\n    """"""\n    ma = float(x.max().cpu().data)\n    mi = float(x.min().cpu().data)\n    d = ma - mi if ma != mi else 1e5\n    return (x - mi) / d\n\n\ndef sec_to_hm(t):\n    """"""Convert time in seconds to time in hours, minutes and seconds\n    e.g. 10239 -> (2, 50, 39)\n    """"""\n    t = int(t)\n    s = t % 60\n    t //= 60\n    m = t % 60\n    t //= 60\n    return t, m, s\n\n\ndef sec_to_hm_str(t):\n    """"""Convert time in seconds to a nice string\n    e.g. 10239 -> \'02h50m39s\'\n    """"""\n    h, m, s = sec_to_hm(t)\n    return ""{:02d}h{:02d}m{:02d}s"".format(h, m, s)\n\n\ndef download_model_if_doesnt_exist(model_name):\n    """"""If pretrained kitti model doesn\'t exist, download and unzip it\n    """"""\n    # values are tuples of (<google cloud URL>, <md5 checksum>)\n    download_paths = {\n        ""mono_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip"",\n             ""a964b8356e08a02d009609d9e3928f7c""),\n        ""stereo_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip"",\n             ""3dfb76bcff0786e4ec07ac00f658dd07""),\n        ""mono+stereo_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip"",\n             ""c024d69012485ed05d7eaa9617a96b81""),\n        ""mono_no_pt_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip"",\n             ""9c2f071e35027c895a4728358ffc913a""),\n        ""stereo_no_pt_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip"",\n             ""41ec2de112905f85541ac33a854742d1""),\n        ""mono+stereo_no_pt_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip"",\n             ""46c3b824f541d143a45c37df65fbab0a""),\n        ""mono_1024x320"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip"",\n             ""0ab0766efdfeea89a0d9ea8ba90e1e63""),\n        ""stereo_1024x320"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip"",\n             ""afc2f2126d70cf3fdf26b550898b501a""),\n        ""mono+stereo_1024x320"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip"",\n             ""cdc5fc9b23513c07d5b19235d9ef08f7""),\n        }\n\n    if not os.path.exists(""models""):\n        os.makedirs(""models"")\n\n    model_path = os.path.join(""models"", model_name)\n\n    def check_file_matches_md5(checksum, fpath):\n        if not os.path.exists(fpath):\n            return False\n        with open(fpath, \'rb\') as f:\n            current_md5checksum = hashlib.md5(f.read()).hexdigest()\n        return current_md5checksum == checksum\n\n    # see if we have the model already downloaded...\n    if not os.path.exists(os.path.join(model_path, ""encoder.pth"")):\n\n        model_url, required_md5checksum = download_paths[model_name]\n\n        if not check_file_matches_md5(required_md5checksum, model_path + "".zip""):\n            print(""-> Downloading pretrained model to {}"".format(model_path + "".zip""))\n            urllib.request.urlretrieve(model_url, model_path + "".zip"")\n\n        if not check_file_matches_md5(required_md5checksum, model_path + "".zip""):\n            print(""   Failed to download a file which matches the checksum - quitting"")\n            quit()\n\n        print(""   Unzipping model..."")\n        with zipfile.ZipFile(model_path + "".zip"", \'r\') as f:\n            f.extractall(model_path)\n\n        print(""   Model unzipped to {}"".format(model_path))\n'"
gimp-plugins/neural-colorization/build_dataset_directory.py,0,"b'import os\nimport shutil\nimport argparse\nimage_extensions = {\'.jpg\', \'.jpeg\', \'.JPG\', \'.JPEG\'}\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""Put all places 365 images in single folder."")\n    parser.add_argument(""-i"",\n                        ""--input_dir"",\n                        required=True,\n                        type=str,\n                        help=""input folder: the folder containing unzipped places 365 files"")\n    parser.add_argument(""-o"",\n                        ""--output_dir"",\n                        required=True,\n                        type=str,\n                        help=""output folder: the folder to put all images"")\n    args = parser.parse_args()\n    return args\n\ndef genlist(image_dir):\n    image_list = []\n    for filename in os.listdir(image_dir):\n        path = os.path.join(image_dir,filename)\n        if os.path.isdir(path):\n            image_list = image_list + genlist(path)\n        else:\n            ext = os.path.splitext(filename)[1]\n            if ext in image_extensions:\n                image_list.append(os.path.join(image_dir, filename))\n    return image_list\n\n\nargs = parse_args()\nif not os.path.exists(args.output_dir):\n    os.makedirs(args.output_dir)\nflist = genlist(args.input_dir)\nfor i,p in enumerate(flist):\n    if os.path.getsize(p) != 0:\n        os.rename(p,os.path.join(args.output_dir,str(i)+\'.jpg\'))\nshutil.rmtree(args.input_dir)\nprint(\'done\')'"
gimp-plugins/neural-colorization/colorize.py,5,"b'import torch\nfrom model import generator\nfrom torch.autograd import Variable\nfrom scipy.ndimage import zoom\nimport cv2\nimport os\nfrom PIL import Image\nimport argparse\nimport numpy as np\nfrom skimage.color import rgb2yuv,yuv2rgb\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""Colorize images"")\n    parser.add_argument(""-i"",\n                        ""--input"",\n                        type=str,\n                        required=True,\n                        help=""input image/input dir"")\n    parser.add_argument(""-o"",\n                        ""--output"",\n                        type=str,\n                        required=True,\n                        help=""output image/output dir"")\n    parser.add_argument(""-m"",\n                        ""--model"",\n                        type=str,\n                        required=True,\n                        help=""location for model (Generator)"")\n    parser.add_argument(""--gpu"",\n                        type=int,\n                        default=-1,\n                        help=""which GPU to use? [-1 for cpu]"")\n    args = parser.parse_args()\n    return args\n\nargs = parse_args()\n\nG = generator()\n\nif torch.cuda.is_available():\n# args.gpu>=0:\n    G=G.cuda(args.gpu)\n    G.load_state_dict(torch.load(args.model))\nelse:\n    G.load_state_dict(torch.load(args.model,map_location=torch.device(\'cpu\')))\n\ndef inference(G,in_path,out_path):\n    p=Image.open(in_path).convert(\'RGB\')\n    img_yuv = rgb2yuv(p)\n    H,W,_ = img_yuv.shape\n    infimg = np.expand_dims(np.expand_dims(img_yuv[...,0], axis=0), axis=0)\n    img_variable = Variable(torch.Tensor(infimg-0.5))\n    if args.gpu>=0:\n        img_variable=img_variable.cuda(args.gpu)\n    res = G(img_variable)\n    uv=res.cpu().detach().numpy()\n    uv[:,0,:,:] *= 0.436\n    uv[:,1,:,:] *= 0.615\n    (_,_,H1,W1) = uv.shape\n    uv = zoom(uv,(1,1,H/H1,W/W1))\n    yuv = np.concatenate([infimg,uv],axis=1)[0]\n    rgb=yuv2rgb(yuv.transpose(1,2,0))\n    cv2.imwrite(out_path,(rgb.clip(min=0,max=1)*256)[:,:,[2,1,0]])\n\n\nif not os.path.isdir(args.input):\n    inference(G,args.input,args.output)\nelse:\n    if not os.path.exists(args.output):\n        os.makedirs(args.output)\n    for f in os.listdir(args.input):\n        inference(G,os.path.join(args.input,f),os.path.join(args.output,f))\n\n'"
gimp-plugins/neural-colorization/model.py,2,"b'import torch\nimport torch.nn as nn\nfrom functools import reduce\nfrom torch.autograd import Variable\n\n\nclass shave_block(nn.Module):\n    def __init__(self, s):\n        super(shave_block, self).__init__()\n        self.s=s\n    def forward(self,x):\n        return x[:,:,self.s:-self.s,self.s:-self.s]\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\ndef generator():\n    G = nn.Sequential( # Sequential,\n        nn.ReflectionPad2d((40, 40, 40, 40)),\n        nn.Conv2d(1,32,(9, 9),(1, 1),(4, 4)),\n        nn.BatchNorm2d(32),\n        nn.ReLU(),\n        nn.Conv2d(32,64,(3, 3),(2, 2),(1, 1)),\n        nn.BatchNorm2d(64),\n        nn.ReLU(),\n        nn.Conv2d(64,128,(3, 3),(2, 2),(1, 1)),\n        nn.BatchNorm2d(128),\n        nn.ReLU(),\n        nn.Sequential( # Sequential,\n            LambdaMap(lambda x: x, # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(128,128,(3, 3)),\n                    nn.BatchNorm2d(128),\n                    nn.ReLU(),\n                    nn.Conv2d(128,128,(3, 3)),\n                    nn.BatchNorm2d(128),\n                    ),\n                shave_block(2),\n                ),\n            LambdaReduce(lambda x,y: x+y), # CAddTable,\n            ),\n        nn.Sequential( # Sequential,\n            LambdaMap(lambda x: x, # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(128,128,(3, 3)),\n                    nn.BatchNorm2d(128),\n                    nn.ReLU(),\n                    nn.Conv2d(128,128,(3, 3)),\n                    nn.BatchNorm2d(128),\n                    ),\n                shave_block(2),\n                ),\n            LambdaReduce(lambda x,y: x+y), # CAddTable,\n            ),\n        nn.Sequential( # Sequential,\n            LambdaMap(lambda x: x, # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(128,128,(3, 3)),\n                    nn.BatchNorm2d(128),\n                    nn.ReLU(),\n                    nn.Conv2d(128,128,(3, 3)),\n                    nn.BatchNorm2d(128),\n                    ),\n                shave_block(2),\n                ),\n            LambdaReduce(lambda x,y: x+y), # CAddTable,\n            ),\n        nn.Sequential( # Sequential,\n            LambdaMap(lambda x: x, # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(128,128,(3, 3)),\n                    nn.BatchNorm2d(128),\n                    nn.ReLU(),\n                    nn.Conv2d(128,128,(3, 3)),\n                    nn.BatchNorm2d(128),\n                    ),\n                shave_block(2),\n                ),\n            LambdaReduce(lambda x,y: x+y), # CAddTable,\n            ),\n        nn.Sequential( # Sequential,\n            LambdaMap(lambda x: x, # ConcatTable,\n                nn.Sequential( # Sequential,\n                    nn.Conv2d(128,128,(3, 3)),\n                    nn.BatchNorm2d(128),\n                    nn.ReLU(),\n                    nn.Conv2d(128,128,(3, 3)),\n                    nn.BatchNorm2d(128),\n                    ),\n                shave_block(2),\n                ),\n            LambdaReduce(lambda x,y: x+y), # CAddTable,\n            ),\n        nn.ConvTranspose2d(128,64,(3, 3),(2, 2),(1, 1),(1, 1)),\n        nn.BatchNorm2d(64),\n        nn.ReLU(),\n        nn.ConvTranspose2d(64,32,(3, 3),(2, 2),(1, 1),(1, 1)),\n        nn.BatchNorm2d(32),\n        nn.ReLU(),\n        nn.Conv2d(32,2,(9, 9),(1, 1),(4, 4)),\n        nn.Tanh(),\n    )\n    return G'"
gimp-plugins/neural-colorization/resize_all_imgs.py,0,"b'from multiprocessing import Pool\nfrom PIL import Image\nimport os\nimport argparse\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""Resize all colorful imgs to 256*256 for training"")\n    parser.add_argument(""-d"",\n                        ""--dir"",\n                        required=True,\n                        type=str,\n                        help=""The directory includes all jpg images"")\n    parser.add_argument(""-n"",\n                        ""--nprocesses"",\n                        default=10,\n                        type=int,\n                        help=""Using how many processes"")\n    args = parser.parse_args()\n    return args\n\ndef doit(x):\n    a=Image.open(x)\n    if a.getbands()!=(\'R\',\'G\',\'B\'):\n        os.remove(x)\n        return\n    a.resize((256,256),Image.BICUBIC).save(x)\n    return\n\nargs=parse_args()\npool = Pool(processes=args.nprocesses)\njpgs = []\nflist = os.listdir(args.dir)\nfull_flist = [os.path.join(args.dir,x) for x in flist]\npool.map(doit, full_flist)\nprint(\'done\')'"
gimp-plugins/neural-colorization/train.py,19,"b'import torch\nimport torch.nn as nn\nimport argparse\nfrom torch.autograd import Variable\nimport torchvision.models as models\nimport os\nfrom torch.utils import data\nfrom model import generator\nimport numpy as np\nfrom PIL import Image\nfrom skimage.color import rgb2yuv,yuv2rgb\nimport cv2\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""Train a GAN based model"")\n    parser.add_argument(""-d"",\n                        ""--training_dir"",\n                        type=str,\n                        required=True,\n                        help=""Training directory (folder contains all 256*256 images)"")\n    parser.add_argument(""-t"",\n                        ""--test_image"",\n                        type=str,\n                        default=None,\n                        help=""Test image location"")\n    parser.add_argument(""-c"",\n                        ""--checkpoint_location"",\n                        type=str,\n                        required=True,\n                        help=""Place to save checkpoints"")\n    parser.add_argument(""-e"",\n                        ""--epoch"",\n                        type=int,\n                        default=120,\n                        help=""Epoches to run training"")\n    parser.add_argument(""--gpu"",\n                        type=int,\n                        default=0,\n                        help=""which GPU to use?"")\n    parser.add_argument(""-b"",\n                        ""--batch_size"",\n                        type=int,\n                        default=20,\n                        help=""batch size"")\n    parser.add_argument(""-w"",\n                        ""--num_workers"",\n                        type=int,\n                        default=6,\n                        help=""Number of workers to fetch data"")\n    parser.add_argument(""-p"",\n                        ""--pixel_loss_weights"",\n                        type=float,\n                        default=1000.0,\n                        help=""Pixel-wise loss weights"")\n    parser.add_argument(""--g_every"",\n                        type=int,\n                        default=1,\n                        help=""Training generator every k iteration"")\n    parser.add_argument(""--g_lr"",\n                        type=float,\n                        default=1e-4,\n                        help=""learning rate for generator"")\n    parser.add_argument(""--d_lr"",\n                        type=float,\n                        default=1e-4,\n                        help=""learning rate for discriminator"")\n    parser.add_argument(""-i"",\n                        ""--checkpoint_every"",\n                        type=int,\n                        default=100,\n                        help=""Save checkpoint every k iteration (checkpoints for same epoch will overwrite)"")\n    parser.add_argument(""--d_init"",\n                        type=str,\n                        default=None,\n                        help=""Init weights for discriminator"")\n    parser.add_argument(""--g_init"",\n                        type=str,\n                        default=None,\n                        help=""Init weights for generator"")\n    args = parser.parse_args()\n    return args\n\n# define data generator\nclass img_data(data.Dataset):\n    def __init__(self, path):\n        files = os.listdir(path)\n        self.files = [os.path.join(path,x) for x in files]\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        img = Image.open(self.files[index])\n        yuv = rgb2yuv(img)\n        y = yuv[...,0]-0.5\n        u_t = yuv[...,1] / 0.43601035\n        v_t = yuv[...,2] / 0.61497538\n        return torch.Tensor(np.expand_dims(y,axis=0)),torch.Tensor(np.stack([u_t,v_t],axis=0))\n\n\nargs = parse_args()\nif not os.path.exists(os.path.join(args.checkpoint_location,\'weights\')):\n    os.makedirs(os.path.join(args.checkpoint_location,\'weights\'))\n\n# Define G, same as torch version\nG = generator().cuda(args.gpu)\n\n# define D\nD = models.resnet18(pretrained=False,num_classes=2)\nD.fc = nn.Sequential(nn.Linear(512, 1), nn.Sigmoid())\nD = D.cuda(args.gpu)\n\ntrainset = img_data(args.training_dir)\nparams = {\'batch_size\': args.batch_size,\n          \'shuffle\': True,\n          \'num_workers\': args.num_workers}\ntraining_generator = data.DataLoader(trainset, **params)\nif args.test_image is not None:\n    test_img = Image.open(args.test_image).convert(\'RGB\').resize((256,256))\n    test_yuv = rgb2yuv(test_img)\n    test_inf = test_yuv[...,0].reshape(1,1,256,256)\n    test_var = Variable(torch.Tensor(test_inf-0.5)).cuda(args.gpu)\nif args.d_init is not None:\n    D.load_state_dict(torch.load(args.d_init))\nif args.g_init is not None:\n    G.load_state_dict(torch.load(args.g_init))\n\n# save test image for beginning\nif args.test_image is not None:\n    test_res = G(test_var)\n    uv=test_res.cpu().detach().numpy()\n    uv[:,0,:,:] *= 0.436\n    uv[:,1,:,:] *= 0.615\n    test_yuv = np.concatenate([test_inf,uv],axis=1).reshape(3,256,256)\n    test_rgb = yuv2rgb(test_yuv.transpose(1,2,0))\n    cv2.imwrite(os.path.join(args.checkpoint_location,\'test_init.jpg\'),(test_rgb.clip(min=0,max=1)*256)[:,:,[2,1,0]])\n\ni=0\nadversarial_loss = torch.nn.BCELoss()\noptimizer_G = torch.optim.Adam(G.parameters(), lr=args.g_lr, betas=(0.5, 0.999))\noptimizer_D = torch.optim.Adam(D.parameters(), lr=args.d_lr, betas=(0.5, 0.999))\nfor epoch in range(args.epoch):\n    for y, uv in training_generator:\n        # Adversarial ground truths\n        valid = Variable(torch.Tensor(y.size(0), 1).fill_(1.0), requires_grad=False).cuda(args.gpu)\n        fake = Variable(torch.Tensor(y.size(0), 1).fill_(0.0), requires_grad=False).cuda(args.gpu)\n\n        yvar = Variable(y).cuda(args.gpu)\n        uvvar = Variable(uv).cuda(args.gpu)\n        real_imgs = torch.cat([yvar,uvvar],dim=1)\n\n        optimizer_G.zero_grad()\n        uvgen = G(yvar)\n        # Generate a batch of images\n        gen_imgs = torch.cat([yvar.detach(),uvgen],dim=1)\n\n        # Loss measures generator\'s ability to fool the discriminator\n        g_loss_gan = adversarial_loss(D(gen_imgs), valid)\n        g_loss = g_loss_gan + args.pixel_loss_weights * torch.mean((uvvar-uvgen)**2)\n        if i%args.g_every==0:\n            g_loss.backward()\n            optimizer_G.step()\n\n        optimizer_D.zero_grad()\n\n        # Measure discriminator\'s ability to classify real from generated samples\n        real_loss = adversarial_loss(D(real_imgs), valid)\n        fake_loss = adversarial_loss(D(gen_imgs.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n        d_loss.backward()\n        optimizer_D.step()\n        i+=1\n        if i%args.checkpoint_every==0:\n            print (""Epoch: %d: [D loss: %f] [G total loss: %f] [G GAN Loss: %f]"" % (epoch, d_loss.item(), g_loss.item(), g_loss_gan.item()))\n\n            torch.save(D.state_dict(), os.path.join(args.checkpoint_location,\'weights\',\'D\'+str(epoch)+\'.pth\'))\n            torch.save(G.state_dict(), os.path.join(args.checkpoint_location,\'weights\',\'G\'+str(epoch)+\'.pth\'))\n            if args.test_image is not None:\n                test_res = G(test_var)\n                uv=test_res.cpu().detach().numpy()\n                uv[:,0,:,:] *= 0.436\n                uv[:,1,:,:] *= 0.615\n                test_yuv = np.concatenate([test_inf,uv],axis=1).reshape(3,256,256)\n                test_rgb = yuv2rgb(test_yuv.transpose(1,2,0))\n                cv2.imwrite(os.path.join(args.checkpoint_location,\'test_epoch_\'+str(epoch)+\'.jpg\'),(test_rgb.clip(min=0,max=1)*256)[:,:,[2,1,0]])\ntorch.save(D.state_dict(), os.path.join(args.checkpoint_location,\'D_final.pth\'))\ntorch.save(G.state_dict(), os.path.join(args.checkpoint_location,\'G_final.pth\'))\n'"
gimp-plugins/pytorch-SRResNet/dataset.py,2,"b'import torch.utils.data as data\r\nimport torch\r\nimport h5py\r\n\r\nclass DatasetFromHdf5(data.Dataset):\r\n    def __init__(self, file_path):\r\n        super(DatasetFromHdf5, self).__init__()\r\n        hf = h5py.File(file_path)\r\n        self.data = hf.get(""data"")\r\n        self.target = hf.get(""label"")\r\n\r\n    def __getitem__(self, index):\r\n        return torch.from_numpy(self.data[index,:,:,:]).float(), torch.from_numpy(self.target[index,:,:,:]).float()\r\n\r\n    def __len__(self):\r\n        return self.data.shape[0]'"
gimp-plugins/pytorch-SRResNet/demo.py,4,"b'import argparse, os\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\nimport time, math\r\nimport scipy.io as sio\r\nimport matplotlib.pyplot as plt\r\n\r\nparser = argparse.ArgumentParser(description=""PyTorch SRResNet Demo"")\r\nparser.add_argument(""--cuda"", action=""store_true"", help=""use cuda?"")\r\nparser.add_argument(""--model"", default=""model/model_srresnet.pth"", type=str, help=""model path"")\r\nparser.add_argument(""--image"", default=""butterfly_GT"", type=str, help=""image name"")\r\nparser.add_argument(""--dataset"", default=""Set5"", type=str, help=""dataset name"")\r\nparser.add_argument(""--scale"", default=4, type=int, help=""scale factor, Default: 4"")\r\nparser.add_argument(""--gpus"", default=""0"", type=str, help=""gpu ids (default: 0)"")\r\n\r\ndef PSNR(pred, gt, shave_border=0):\r\n    height, width = pred.shape[:2]\r\n    pred = pred[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    gt = gt[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    imdff = pred - gt\r\n    rmse = math.sqrt(np.mean(imdff ** 2))\r\n    if rmse == 0:\r\n        return 100\r\n    return 20 * math.log10(255.0 / rmse)\r\n\r\nopt = parser.parse_args()\r\ncuda = opt.cuda\r\n\r\nif cuda:\r\n    print(""=> use gpu id: \'{}\'"".format(opt.gpus))\r\n    os.environ[""CUDA_VISIBLE_DEVICES""] = opt.gpus\r\n    if not torch.cuda.is_available():\r\n            raise Exception(""No GPU found or Wrong gpu id, please run without --cuda"")\r\n\r\nmodel = torch.load(opt.model)[""model""]\r\n\r\nim_gt = sio.loadmat(""testsets/"" + opt.dataset + ""/"" + opt.image + "".mat"")[\'im_gt\']\r\nim_b = sio.loadmat(""testsets/"" + opt.dataset + ""/"" + opt.image + "".mat"")[\'im_b\']\r\nim_l = sio.loadmat(""testsets/"" + opt.dataset + ""/"" + opt.image + "".mat"")[\'im_l\']\r\n           \r\nim_gt = im_gt.astype(float).astype(np.uint8)\r\nim_b = im_b.astype(float).astype(np.uint8)\r\nim_l = im_l.astype(float).astype(np.uint8)      \r\n\r\nim_input = im_l.astype(np.float32).transpose(2,0,1)\r\nim_input = im_input.reshape(1,im_input.shape[0],im_input.shape[1],im_input.shape[2])\r\nim_input = Variable(torch.from_numpy(im_input/255.).float())\r\n\r\nif cuda:\r\n    model = model.cuda()\r\n    im_input = im_input.cuda()\r\nelse:\r\n    model = model.cpu()\r\n    \r\nstart_time = time.time()\r\nout = model(im_input)\r\nelapsed_time = time.time() - start_time\r\n\r\nout = out.cpu()\r\n\r\nim_h = out.data[0].numpy().astype(np.float32)\r\n\r\nim_h = im_h*255.\r\nim_h[im_h<0] = 0\r\nim_h[im_h>255.] = 255.            \r\nim_h = im_h.transpose(1,2,0)\r\n\r\nprint(""Dataset="",opt.dataset)\r\nprint(""Scale="",opt.scale)\r\nprint(""It takes {}s for processing"".format(elapsed_time))\r\n\r\nfig = plt.figure()\r\nax = plt.subplot(""131"")\r\nax.imshow(im_gt)\r\nax.set_title(""GT"")\r\n\r\nax = plt.subplot(""132"")\r\nax.imshow(im_b)\r\nax.set_title(""Input(Bicubic)"")\r\n\r\nax = plt.subplot(""133"")\r\nax.imshow(im_h.astype(np.uint8))\r\nax.set_title(""Output(SRResNet)"")\r\nplt.show()\r\n'"
gimp-plugins/pytorch-SRResNet/eval.py,4,"b'import matlab.engine\r\nimport argparse, os\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\nimport time, math, glob\r\nimport scipy.io as sio\r\nimport cv2\r\n\r\nparser = argparse.ArgumentParser(description=""PyTorch SRResNet Eval"")\r\nparser.add_argument(""--cuda"", action=""store_true"", help=""use cuda?"")\r\nparser.add_argument(""--model"", default=""model/model_srresnet.pth"", type=str, help=""model path"")\r\nparser.add_argument(""--dataset"", default=""Set5"", type=str, help=""dataset name, Default: Set5"")\r\nparser.add_argument(""--scale"", default=4, type=int, help=""scale factor, Default: 4"")\r\nparser.add_argument(""--gpus"", default=""0"", type=str, help=""gpu ids (default: 0)"")\r\n\r\ndef PSNR(pred, gt, shave_border=0):\r\n    height, width = pred.shape[:2]\r\n    pred = pred[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    gt = gt[shave_border:height - shave_border, shave_border:width - shave_border]\r\n    imdff = pred - gt\r\n    rmse = math.sqrt(np.mean(imdff ** 2))\r\n    if rmse == 0:\r\n        return 100\r\n    return 20 * math.log10(255.0 / rmse)\r\n\r\nopt = parser.parse_args()\r\ncuda = opt.cuda\r\neng = matlab.engine.start_matlab()\r\n\r\nif cuda:\r\n    print(""=> use gpu id: \'{}\'"".format(opt.gpus))\r\n    os.environ[""CUDA_VISIBLE_DEVICES""] = opt.gpus\r\n    if not torch.cuda.is_available():\r\n            raise Exception(""No GPU found or Wrong gpu id, please run without --cuda"")\r\n\r\nmodel = torch.load(opt.model)[""model""]\r\n\r\nimage_list = glob.glob(""./testsets/"" + opt.dataset + ""/*.*"") \r\n\r\navg_psnr_predicted = 0.0\r\navg_psnr_bicubic = 0.0\r\navg_elapsed_time = 0.0\r\n\r\nfor image_name in image_list:\r\n    print(""Processing "", image_name)\r\n    im_gt_y = sio.loadmat(image_name)[\'im_gt_y\']\r\n    im_b_y = sio.loadmat(image_name)[\'im_b_y\']\r\n    im_l = sio.loadmat(image_name)[\'im_l\']\r\n\r\n    im_gt_y = im_gt_y.astype(float)\r\n    im_b_y = im_b_y.astype(float)\r\n    im_l = im_l.astype(float)\r\n\r\n    psnr_bicubic = PSNR(im_gt_y, im_b_y,shave_border=opt.scale)\r\n    avg_psnr_bicubic += psnr_bicubic\r\n\r\n    im_input = im_l.astype(np.float32).transpose(2,0,1)\r\n    im_input = im_input.reshape(1,im_input.shape[0],im_input.shape[1],im_input.shape[2])\r\n    im_input = Variable(torch.from_numpy(im_input/255.).float())\r\n\r\n    if cuda:\r\n        model = model.cuda()\r\n        im_input = im_input.cuda()\r\n    else:\r\n        model = model.cpu()\r\n\r\n    start_time = time.time()\r\n    HR_4x = model(im_input)\r\n    elapsed_time = time.time() - start_time\r\n    avg_elapsed_time += elapsed_time\r\n\r\n    HR_4x = HR_4x.cpu()\r\n\r\n    im_h = HR_4x.data[0].numpy().astype(np.float32)\r\n\r\n    im_h = im_h*255.\r\n    im_h = np.clip(im_h, 0., 255.)\r\n    im_h = im_h.transpose(1,2,0).astype(np.float32)\r\n\r\n    im_h_matlab = matlab.double((im_h / 255.).tolist())\r\n    im_h_ycbcr = eng.rgb2ycbcr(im_h_matlab)\r\n    im_h_ycbcr = np.array(im_h_ycbcr._data).reshape(im_h_ycbcr.size, order=\'F\').astype(np.float32) * 255.\r\n    im_h_y = im_h_ycbcr[:,:,0]\r\n\r\n    psnr_predicted = PSNR(im_gt_y, im_h_y,shave_border=opt.scale)\r\n    avg_psnr_predicted += psnr_predicted\r\n\r\nprint(""Scale="", opt.scale)\r\nprint(""Dataset="", opt.dataset)\r\nprint(""PSNR_predicted="", avg_psnr_predicted/len(image_list))\r\nprint(""PSNR_bicubic="", avg_psnr_bicubic/len(image_list))\r\nprint(""It takes average {}s for processing"".format(avg_elapsed_time/len(image_list)))\r\n'"
gimp-plugins/pytorch-SRResNet/main_srresnet.py,13,"b'import argparse, os\r\nimport torch\r\nimport math, random\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.autograd import Variable\r\nfrom torch.utils.data import DataLoader\r\nfrom srresnet import _NetG\r\nfrom dataset import DatasetFromHdf5\r\nfrom torchvision import models\r\nimport torch.utils.model_zoo as model_zoo\r\n\r\n# Training settings\r\nparser = argparse.ArgumentParser(description=""PyTorch SRResNet"")\r\nparser.add_argument(""--batchSize"", type=int, default=16, help=""training batch size"")\r\nparser.add_argument(""--nEpochs"", type=int, default=500, help=""number of epochs to train for"")\r\nparser.add_argument(""--lr"", type=float, default=1e-4, help=""Learning Rate. Default=1e-4"")\r\nparser.add_argument(""--step"", type=int, default=200, help=""Sets the learning rate to the initial LR decayed by momentum every n epochs, Default: n=500"")\r\nparser.add_argument(""--cuda"", action=""store_true"", help=""Use cuda?"")\r\nparser.add_argument(""--resume"", default="""", type=str, help=""Path to checkpoint (default: none)"")\r\nparser.add_argument(""--start-epoch"", default=1, type=int, help=""Manual epoch number (useful on restarts)"")\r\nparser.add_argument(""--threads"", type=int, default=0, help=""Number of threads for data loader to use, Default: 1"")\r\nparser.add_argument(""--pretrained"", default="""", type=str, help=""path to pretrained model (default: none)"")\r\nparser.add_argument(""--vgg_loss"", action=""store_true"", help=""Use content loss?"")\r\nparser.add_argument(""--gpus"", default=""0"", type=str, help=""gpu ids (default: 0)"")\r\n\r\ndef main():\r\n\r\n    global opt, model, netContent\r\n    opt = parser.parse_args()\r\n    print(opt)\r\n\r\n    cuda = opt.cuda\r\n    if cuda:\r\n        print(""=> use gpu id: \'{}\'"".format(opt.gpus))\r\n        os.environ[""CUDA_VISIBLE_DEVICES""] = opt.gpus\r\n        if not torch.cuda.is_available():\r\n                raise Exception(""No GPU found or Wrong gpu id, please run without --cuda"")\r\n\r\n    opt.seed = random.randint(1, 10000)\r\n    print(""Random Seed: "", opt.seed)\r\n    torch.manual_seed(opt.seed)\r\n    if cuda:\r\n        torch.cuda.manual_seed(opt.seed)\r\n\r\n    cudnn.benchmark = True\r\n\r\n    print(""===> Loading datasets"")\r\n    train_set = DatasetFromHdf5(""/path/to/your/hdf5/data/like/rgb_srresnet_x4.h5"")\r\n    training_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, \\\r\n        batch_size=opt.batchSize, shuffle=True)\r\n\r\n    if opt.vgg_loss:\r\n        print(\'===> Loading VGG model\')\r\n        netVGG = models.vgg19()\r\n        netVGG.load_state_dict(model_zoo.load_url(\'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\'))\r\n        class _content_model(nn.Module):\r\n            def __init__(self):\r\n                super(_content_model, self).__init__()\r\n                self.feature = nn.Sequential(*list(netVGG.features.children())[:-1])\r\n                \r\n            def forward(self, x):\r\n                out = self.feature(x)\r\n                return out\r\n\r\n        netContent = _content_model()\r\n\r\n    print(""===> Building model"")\r\n    model = _NetG()\r\n    criterion = nn.MSELoss(size_average=False)\r\n\r\n    print(""===> Setting GPU"")\r\n    if cuda:\r\n        model = model.cuda()\r\n        criterion = criterion.cuda()\r\n        if opt.vgg_loss:\r\n            netContent = netContent.cuda() \r\n\r\n    # optionally resume from a checkpoint\r\n    if opt.resume:\r\n        if os.path.isfile(opt.resume):\r\n            print(""=> loading checkpoint \'{}\'"".format(opt.resume))\r\n            checkpoint = torch.load(opt.resume)\r\n            opt.start_epoch = checkpoint[""epoch""] + 1\r\n            model.load_state_dict(checkpoint[""model""].state_dict())\r\n        else:\r\n            print(""=> no checkpoint found at \'{}\'"".format(opt.resume))\r\n\r\n    # optionally copy weights from a checkpoint\r\n    if opt.pretrained:\r\n        if os.path.isfile(opt.pretrained):\r\n            print(""=> loading model \'{}\'"".format(opt.pretrained))\r\n            weights = torch.load(opt.pretrained)\r\n            model.load_state_dict(weights[\'model\'].state_dict())\r\n        else:\r\n            print(""=> no model found at \'{}\'"".format(opt.pretrained))\r\n\r\n    print(""===> Setting Optimizer"")\r\n    optimizer = optim.Adam(model.parameters(), lr=opt.lr)\r\n\r\n    print(""===> Training"")\r\n    for epoch in range(opt.start_epoch, opt.nEpochs + 1):\r\n        train(training_data_loader, optimizer, model, criterion, epoch)\r\n        save_checkpoint(model, epoch)\r\n\r\ndef adjust_learning_rate(optimizer, epoch):\r\n    """"""Sets the learning rate to the initial LR decayed by 10""""""\r\n    lr = opt.lr * (0.1 ** (epoch // opt.step))\r\n    return lr \r\n\r\ndef train(training_data_loader, optimizer, model, criterion, epoch):\r\n\r\n    lr = adjust_learning_rate(optimizer, epoch-1)\r\n    \r\n    for param_group in optimizer.param_groups:\r\n        param_group[""lr""] = lr\r\n\r\n    print(""Epoch={}, lr={}"".format(epoch, optimizer.param_groups[0][""lr""]))\r\n    model.train()\r\n\r\n    for iteration, batch in enumerate(training_data_loader, 1):\r\n\r\n        input, target = Variable(batch[0]), Variable(batch[1], requires_grad=False)\r\n\r\n        if opt.cuda:\r\n            input = input.cuda()\r\n            target = target.cuda()\r\n\r\n        output = model(input)\r\n        loss = criterion(output, target)\r\n\r\n        if opt.vgg_loss:\r\n            content_input = netContent(output)\r\n            content_target = netContent(target)\r\n            content_target = content_target.detach()\r\n            content_loss = criterion(content_input, content_target)\r\n\r\n        optimizer.zero_grad()\r\n\r\n        if opt.vgg_loss:\r\n            netContent.zero_grad()\r\n            content_loss.backward(retain_graph=True)\r\n\r\n        loss.backward()\r\n\r\n        optimizer.step()\r\n\r\n        if iteration%100 == 0:\r\n            if opt.vgg_loss:\r\n                print(""===> Epoch[{}]({}/{}): Loss: {:.5} Content_loss {:.5}"".format(epoch, iteration, len(training_data_loader), loss.data[0], content_loss.data[0]))\r\n            else:\r\n                print(""===> Epoch[{}]({}/{}): Loss: {:.5}"".format(epoch, iteration, len(training_data_loader), loss.data[0]))\r\n\r\ndef save_checkpoint(model, epoch):\r\n    model_out_path = ""checkpoint/"" + ""model_epoch_{}.pth"".format(epoch)\r\n    state = {""epoch"": epoch ,""model"": model}\r\n    if not os.path.exists(""checkpoint/""):\r\n        os.makedirs(""checkpoint/"")\r\n\r\n    torch.save(state, model_out_path)\r\n\r\n    print(""Checkpoint saved to {}"".format(model_out_path))\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
gimp-plugins/pytorch-SRResNet/srresnet.py,3,"b'import torch\r\nimport torch.nn as nn\r\nimport math\r\n\r\nclass _Residual_Block(nn.Module):\r\n    def __init__(self):\r\n        super(_Residual_Block, self).__init__()\r\n\r\n        self.conv1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.in1 = nn.InstanceNorm2d(64, affine=True)\r\n        self.relu = nn.LeakyReLU(0.2, inplace=True)\r\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.in2 = nn.InstanceNorm2d(64, affine=True)\r\n\r\n    def forward(self, x):\r\n        identity_data = x\r\n        output = self.relu(self.in1(self.conv1(x)))\r\n        output = self.in2(self.conv2(output))\r\n        output = torch.add(output,identity_data)\r\n        return output \r\n\r\nclass _NetG(nn.Module):\r\n    def __init__(self):\r\n        super(_NetG, self).__init__()\r\n\r\n        self.conv_input = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=9, stride=1, padding=4, bias=False)\r\n        self.relu = nn.LeakyReLU(0.2, inplace=True)\r\n        \r\n        self.residual = self.make_layer(_Residual_Block, 16)\r\n\r\n        self.conv_mid = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\r\n        self.bn_mid = nn.InstanceNorm2d(64, affine=True)\r\n\r\n        self.upscale4x = nn.Sequential(\r\n            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.PixelShuffle(2),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.PixelShuffle(2),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n        )\r\n\r\n        self.conv_output = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=9, stride=1, padding=4, bias=False)\r\n        \r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n\r\n    def make_layer(self, block, num_of_layer):\r\n        layers = []\r\n        for _ in range(num_of_layer):\r\n            layers.append(block())\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        out = self.relu(self.conv_input(x))\r\n        residual = out\r\n        out = self.residual(out)\r\n        out = self.bn_mid(self.conv_mid(out))\r\n        out = torch.add(out,residual)\r\n        out = self.upscale4x(out)\r\n        out = self.conv_output(out)\r\n        return out\r\n\r\nclass _NetD(nn.Module):\r\n    def __init__(self):\r\n        super(_NetD, self).__init__()\r\n\r\n        self.features = nn.Sequential(\r\n        \r\n            # input is (3) x 96 x 96\r\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (64) x 96 x 96\r\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),            \r\n            nn.BatchNorm2d(64),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (64) x 96 x 96\r\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),            \r\n            nn.BatchNorm2d(128),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n            \r\n            # state size. (64) x 48 x 48\r\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\r\n            nn.BatchNorm2d(128),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (128) x 48 x 48\r\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\r\n            nn.BatchNorm2d(256),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (256) x 24 x 24\r\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\r\n            nn.BatchNorm2d(256),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (256) x 12 x 12\r\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False),            \r\n            nn.BatchNorm2d(512),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n\r\n            # state size. (512) x 12 x 12\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),            \r\n            nn.BatchNorm2d(512),\r\n            nn.LeakyReLU(0.2, inplace=True),\r\n        )\r\n\r\n        self.LeakyReLU = nn.LeakyReLU(0.2, inplace=True)\r\n        self.fc1 = nn.Linear(512 * 6 * 6, 1024)\r\n        self.fc2 = nn.Linear(1024, 1)\r\n        self.sigmoid = nn.Sigmoid()\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                m.weight.data.normal_(0.0, 0.02)\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                m.weight.data.normal_(1.0, 0.02)\r\n                m.bias.data.fill_(0)\r\n\r\n    def forward(self, input):\r\n\r\n        out = self.features(input)\r\n\r\n        # state size. (512) x 6 x 6\r\n        out = out.view(out.size(0), -1)\r\n\r\n        # state size. (512 x 6 x 6)\r\n        out = self.fc1(out)\r\n\r\n        # state size. (1024)\r\n        out = self.LeakyReLU(out)\r\n\r\n        out = self.fc2(out)\r\n        out = self.sigmoid(out)\r\n        return out.view(-1, 1).squeeze(1)'"
gimp-plugins/DeblurGANv2/models/__init__.py,0,b''
gimp-plugins/DeblurGANv2/models/fpn_densenet.py,2,"b'import torch\nimport torch.nn as nn\n\nfrom torchvision.models import resnet50, densenet121, densenet201\n\n\nclass FPNSegHead(nn.Module):\n    def __init__(self, num_in, num_mid, num_out):\n        super().__init__()\n\n        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.block0(x), inplace=True)\n        x = nn.functional.relu(self.block1(x), inplace=True)\n        return x\n\n\nclass FPNDense(nn.Module):\n\n    def __init__(self, output_ch=3, num_filters=128, num_filters_fpn=256, pretrained=True):\n        super().__init__()\n\n        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n\n        self.fpn = FPN(num_filters=num_filters_fpn, pretrained=pretrained)\n\n        # The segmentation heads on top of the FPN\n\n        self.head1 = FPNSegHead(num_filters_fpn, num_filters, num_filters)\n        self.head2 = FPNSegHead(num_filters_fpn, num_filters, num_filters)\n        self.head3 = FPNSegHead(num_filters_fpn, num_filters, num_filters)\n        self.head4 = FPNSegHead(num_filters_fpn, num_filters, num_filters)\n\n        self.smooth = nn.Sequential(\n            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_filters),\n            nn.ReLU(),\n        )\n\n        self.smooth2 = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_filters // 2),\n            nn.ReLU(),\n        )\n\n        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        map0, map1, map2, map3, map4 = self.fpn(x)\n\n        map4 = nn.functional.upsample(self.head4(map4), scale_factor=8, mode=""nearest"")\n        map3 = nn.functional.upsample(self.head3(map3), scale_factor=4, mode=""nearest"")\n        map2 = nn.functional.upsample(self.head2(map2), scale_factor=2, mode=""nearest"")\n        map1 = nn.functional.upsample(self.head1(map1), scale_factor=1, mode=""nearest"")\n\n        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n        smoothed = self.smooth2(smoothed + map0)\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n\n        final = self.final(smoothed)\n\n        nn.Tanh(final)\n\n\nclass FPN(nn.Module):\n\n    def __init__(self, num_filters=256, pretrained=True):\n        """"""Creates an `FPN` instance for feature extraction.\n        Args:\n          num_filters: the number of filters in each output pyramid level\n          pretrained: use ImageNet pre-trained backbone feature extractor\n        """"""\n\n        super().__init__()\n\n        self.features = densenet121(pretrained=pretrained).features\n\n        self.enc0 = nn.Sequential(self.features.conv0,\n                                  self.features.norm0,\n                                  self.features.relu0)\n        self.pool0 = self.features.pool0\n        self.enc1 = self.features.denseblock1  # 256\n        self.enc2 = self.features.denseblock2  # 512\n        self.enc3 = self.features.denseblock3  # 1024\n        self.enc4 = self.features.denseblock4  # 2048\n        self.norm = self.features.norm5  # 2048\n\n        self.tr1 = self.features.transition1  # 256\n        self.tr2 = self.features.transition2  # 512\n        self.tr3 = self.features.transition3  # 1024\n\n        self.lateral4 = nn.Conv2d(1024, num_filters, kernel_size=1, bias=False)\n        self.lateral3 = nn.Conv2d(1024, num_filters, kernel_size=1, bias=False)\n        self.lateral2 = nn.Conv2d(512, num_filters, kernel_size=1, bias=False)\n        self.lateral1 = nn.Conv2d(256, num_filters, kernel_size=1, bias=False)\n        self.lateral0 = nn.Conv2d(64, num_filters // 2, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        # Bottom-up pathway, from ResNet\n        enc0 = self.enc0(x)\n\n        pooled = self.pool0(enc0)\n\n        enc1 = self.enc1(pooled)  # 256\n        tr1 = self.tr1(enc1)\n\n        enc2 = self.enc2(tr1)  # 512\n        tr2 = self.tr2(enc2)\n\n        enc3 = self.enc3(tr2)  # 1024\n        tr3 = self.tr3(enc3)\n\n        enc4 = self.enc4(tr3)  # 2048\n        enc4 = self.norm(enc4)\n\n        # Lateral connections\n\n        lateral4 = self.lateral4(enc4)\n        lateral3 = self.lateral3(enc3)\n        lateral2 = self.lateral2(enc2)\n        lateral1 = self.lateral1(enc1)\n        lateral0 = self.lateral0(enc0)\n\n        # Top-down pathway\n\n        map4 = lateral4\n        map3 = lateral3 + nn.functional.upsample(map4, scale_factor=2, mode=""nearest"")\n        map2 = lateral2 + nn.functional.upsample(map3, scale_factor=2, mode=""nearest"")\n        map1 = lateral1 + nn.functional.upsample(map2, scale_factor=2, mode=""nearest"")\n\n        return lateral0, map1, map2, map3, map4\n'"
gimp-plugins/DeblurGANv2/models/fpn_inception.py,6,"b'import torch\nimport torch.nn as nn\n# from pretrainedmodels import inceptionresnetv2\n# from torchsummary import summary\nimport torch.nn.functional as F\n\nclass FPNHead(nn.Module):\n    def __init__(self, num_in, num_mid, num_out):\n        super(FPNHead,self).__init__()\n\n        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.block0(x), inplace=True)\n        x = nn.functional.relu(self.block1(x), inplace=True)\n        return x\n\nclass ConvBlock(nn.Module):\n    def __init__(self, num_in, num_out, norm_layer):\n        super().__init__()\n\n        self.block = nn.Sequential(nn.Conv2d(num_in, num_out, kernel_size=3, padding=1),\n                                 norm_layer(num_out),\n                                 nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n\nclass FPNInception(nn.Module):\n\n    def __init__(self, norm_layer, output_ch=3, num_filters=128, num_filters_fpn=256):\n        super(FPNInception,self).__init__()\n\n        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n        self.fpn = FPN(num_filters=num_filters_fpn, norm_layer=norm_layer)\n\n        # The segmentation heads on top of the FPN\n\n        self.head1 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head2 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head3 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head4 = FPNHead(num_filters_fpn, num_filters, num_filters)\n\n        self.smooth = nn.Sequential(\n            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n            norm_layer(num_filters),\n            nn.ReLU(),\n        )\n\n        self.smooth2 = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n            norm_layer(num_filters // 2),\n            nn.ReLU(),\n        )\n\n        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n\n    def unfreeze(self):\n        self.fpn.unfreeze()\n\n    def forward(self, x):\n        map0, map1, map2, map3, map4 = self.fpn(x)\n\n        map4 = nn.functional.upsample(self.head4(map4), scale_factor=8, mode=""nearest"")\n        map3 = nn.functional.upsample(self.head3(map3), scale_factor=4, mode=""nearest"")\n        map2 = nn.functional.upsample(self.head2(map2), scale_factor=2, mode=""nearest"")\n        map1 = nn.functional.upsample(self.head1(map1), scale_factor=1, mode=""nearest"")\n\n        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n        smoothed = self.smooth2(smoothed + map0)\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n\n        final = self.final(smoothed)\n        res = torch.tanh(final) + x\n\n        return torch.clamp(res, min = -1,max = 1)\n\n\nclass FPN(nn.Module):\n\n    def __init__(self, norm_layer, num_filters=256):\n        """"""Creates an `FPN` instance for feature extraction.\n        Args:\n          num_filters: the number of filters in each output pyramid level\n          pretrained: use ImageNet pre-trained backbone feature extractor\n        """"""\n\n        super(FPN,self).__init__()\n        self.inception = inceptionresnetv2(num_classes=1000, pretrained=\'imagenet\')\n        # self.inception = torch.load(\'inceptionresnetv2-520b38e4.pth\')\n        self.enc0 = self.inception.conv2d_1a\n        self.enc1 = nn.Sequential(\n            self.inception.conv2d_2a,\n            self.inception.conv2d_2b,\n            self.inception.maxpool_3a,\n        ) # 64\n        self.enc2 = nn.Sequential(\n            self.inception.conv2d_3b,\n            self.inception.conv2d_4a,\n            self.inception.maxpool_5a,\n        )  # 192\n        self.enc3 = nn.Sequential(\n            self.inception.mixed_5b,\n            self.inception.repeat,\n            self.inception.mixed_6a,\n        )   # 1088\n        self.enc4 = nn.Sequential(\n            self.inception.repeat_1,\n            self.inception.mixed_7a,\n        ) #2080\n        self.td1 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n        self.td2 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n        self.td3 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n        self.pad = nn.ReflectionPad2d(1)\n        self.lateral4 = nn.Conv2d(2080, num_filters, kernel_size=1, bias=False)\n        self.lateral3 = nn.Conv2d(1088, num_filters, kernel_size=1, bias=False)\n        self.lateral2 = nn.Conv2d(192, num_filters, kernel_size=1, bias=False)\n        self.lateral1 = nn.Conv2d(64, num_filters, kernel_size=1, bias=False)\n        self.lateral0 = nn.Conv2d(32, num_filters // 2, kernel_size=1, bias=False)\n\n        for param in self.inception.parameters():\n            param.requires_grad = False\n\n    def unfreeze(self):\n        for param in self.inception.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n\n        # Bottom-up pathway, from ResNet\n        enc0 = self.enc0(x)\n\n        enc1 = self.enc1(enc0) # 256\n\n        enc2 = self.enc2(enc1) # 512\n\n        enc3 = self.enc3(enc2) # 1024\n\n        enc4 = self.enc4(enc3) # 2048\n\n        # Lateral connections\n\n        lateral4 = self.pad(self.lateral4(enc4))\n        lateral3 = self.pad(self.lateral3(enc3))\n        lateral2 = self.lateral2(enc2)\n        lateral1 = self.pad(self.lateral1(enc1))\n        lateral0 = self.lateral0(enc0)\n\n        # Top-down pathway\n        pad = (1, 2, 1, 2)  # pad last dim by 1 on each side\n        pad1 = (0, 1, 0, 1)\n        map4 = lateral4\n        map3 = self.td1(lateral3 + nn.functional.upsample(map4, scale_factor=2, mode=""nearest""))\n        map2 = self.td2(F.pad(lateral2, pad, ""reflect"") + nn.functional.upsample(map3, scale_factor=2, mode=""nearest""))\n        map1 = self.td3(lateral1 + nn.functional.upsample(map2, scale_factor=2, mode=""nearest""))\n        return F.pad(lateral0, pad1, ""reflect""), map1, map2, map3, map4\n'"
gimp-plugins/DeblurGANv2/models/fpn_inception_simple.py,5,"b'import torch\nimport torch.nn as nn\nfrom pretrainedmodels import inceptionresnetv2\nfrom torchsummary import summary\nimport torch.nn.functional as F\n\nclass FPNHead(nn.Module):\n    def __init__(self, num_in, num_mid, num_out):\n        super().__init__()\n\n        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.block0(x), inplace=True)\n        x = nn.functional.relu(self.block1(x), inplace=True)\n        return x\n\nclass ConvBlock(nn.Module):\n    def __init__(self, num_in, num_out, norm_layer):\n        super().__init__()\n\n        self.block = nn.Sequential(nn.Conv2d(num_in, num_out, kernel_size=3, padding=1),\n                                 norm_layer(num_out),\n                                 nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n\nclass FPNInceptionSimple(nn.Module):\n\n    def __init__(self, norm_layer, output_ch=3, num_filters=128, num_filters_fpn=256):\n        super().__init__()\n\n        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n        self.fpn = FPN(num_filters=num_filters_fpn, norm_layer=norm_layer)\n\n        # The segmentation heads on top of the FPN\n\n        self.head1 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head2 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head3 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head4 = FPNHead(num_filters_fpn, num_filters, num_filters)\n\n        self.smooth = nn.Sequential(\n            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n            norm_layer(num_filters),\n            nn.ReLU(),\n        )\n\n        self.smooth2 = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n            norm_layer(num_filters // 2),\n            nn.ReLU(),\n        )\n\n        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n\n    def unfreeze(self):\n        self.fpn.unfreeze()\n\n    def forward(self, x):\n\n        map0, map1, map2, map3, map4 = self.fpn(x)\n\n        map4 = nn.functional.upsample(self.head4(map4), scale_factor=8, mode=""nearest"")\n        map3 = nn.functional.upsample(self.head3(map3), scale_factor=4, mode=""nearest"")\n        map2 = nn.functional.upsample(self.head2(map2), scale_factor=2, mode=""nearest"")\n        map1 = nn.functional.upsample(self.head1(map1), scale_factor=1, mode=""nearest"")\n\n        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n        smoothed = self.smooth2(smoothed + map0)\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n\n        final = self.final(smoothed)\n        res = torch.tanh(final) + x\n\n        return torch.clamp(res, min = -1,max = 1)\n\n\nclass FPN(nn.Module):\n\n    def __init__(self, norm_layer, num_filters=256):\n        """"""Creates an `FPN` instance for feature extraction.\n        Args:\n          num_filters: the number of filters in each output pyramid level\n          pretrained: use ImageNet pre-trained backbone feature extractor\n        """"""\n\n        super().__init__()\n        self.inception = inceptionresnetv2(num_classes=1000, pretrained=\'imagenet\')\n\n        self.enc0 = self.inception.conv2d_1a\n        self.enc1 = nn.Sequential(\n            self.inception.conv2d_2a,\n            self.inception.conv2d_2b,\n            self.inception.maxpool_3a,\n        ) # 64\n        self.enc2 = nn.Sequential(\n            self.inception.conv2d_3b,\n            self.inception.conv2d_4a,\n            self.inception.maxpool_5a,\n        )  # 192\n        self.enc3 = nn.Sequential(\n            self.inception.mixed_5b,\n            self.inception.repeat,\n            self.inception.mixed_6a,\n        )   # 1088\n        self.enc4 = nn.Sequential(\n            self.inception.repeat_1,\n            self.inception.mixed_7a,\n        ) #2080\n\n        self.pad = nn.ReflectionPad2d(1)\n        self.lateral4 = nn.Conv2d(2080, num_filters, kernel_size=1, bias=False)\n        self.lateral3 = nn.Conv2d(1088, num_filters, kernel_size=1, bias=False)\n        self.lateral2 = nn.Conv2d(192, num_filters, kernel_size=1, bias=False)\n        self.lateral1 = nn.Conv2d(64, num_filters, kernel_size=1, bias=False)\n        self.lateral0 = nn.Conv2d(32, num_filters // 2, kernel_size=1, bias=False)\n\n        for param in self.inception.parameters():\n            param.requires_grad = False\n\n    def unfreeze(self):\n        for param in self.inception.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n\n        # Bottom-up pathway, from ResNet\n        enc0 = self.enc0(x)\n\n        enc1 = self.enc1(enc0) # 256\n\n        enc2 = self.enc2(enc1) # 512\n\n        enc3 = self.enc3(enc2) # 1024\n\n        enc4 = self.enc4(enc3) # 2048\n\n        # Lateral connections\n\n        lateral4 = self.pad(self.lateral4(enc4))\n        lateral3 = self.pad(self.lateral3(enc3))\n        lateral2 = self.lateral2(enc2)\n        lateral1 = self.pad(self.lateral1(enc1))\n        lateral0 = self.lateral0(enc0)\n\n        # Top-down pathway\n        pad = (1, 2, 1, 2)  # pad last dim by 1 on each side\n        pad1 = (0, 1, 0, 1)\n        map4 = lateral4\n        map3 = lateral3 + nn.functional.upsample(map4, scale_factor=2, mode=""nearest"")\n        map2 = F.pad(lateral2, pad, ""reflect"") + nn.functional.upsample(map3, scale_factor=2, mode=""nearest"")\n        map1 = lateral1 + nn.functional.upsample(map2, scale_factor=2, mode=""nearest"")\n        return F.pad(lateral0, pad1, ""reflect""), map1, map2, map3, map4\n'"
gimp-plugins/DeblurGANv2/models/fpn_mobilenet.py,5,"b'import torch\nimport torch.nn as nn\nfrom mobilenet_v2 import MobileNetV2\n\nclass FPNHead(nn.Module):\n    def __init__(self, num_in, num_mid, num_out):\n        super().__init__()\n\n        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.block0(x), inplace=True)\n        x = nn.functional.relu(self.block1(x), inplace=True)\n        return x\n\n\nclass FPNMobileNet(nn.Module):\n\n    def __init__(self, norm_layer, output_ch=3, num_filters=64, num_filters_fpn=128, pretrained=True):\n        super().__init__()\n\n        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n\n        self.fpn = FPN(num_filters=num_filters_fpn, norm_layer = norm_layer, pretrained=pretrained)\n\n        # The segmentation heads on top of the FPN\n\n        self.head1 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head2 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head3 = FPNHead(num_filters_fpn, num_filters, num_filters)\n        self.head4 = FPNHead(num_filters_fpn, num_filters, num_filters)\n\n        self.smooth = nn.Sequential(\n            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n            norm_layer(num_filters),\n            nn.ReLU(),\n        )\n\n        self.smooth2 = nn.Sequential(\n            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n            norm_layer(num_filters // 2),\n            nn.ReLU(),\n        )\n\n        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n\n    def unfreeze(self):\n        self.fpn.unfreeze()\n\n    def forward(self, x):\n\n        map0, map1, map2, map3, map4 = self.fpn(x)\n\n        map4 = nn.functional.upsample(self.head4(map4), scale_factor=8, mode=""nearest"")\n        map3 = nn.functional.upsample(self.head3(map3), scale_factor=4, mode=""nearest"")\n        map2 = nn.functional.upsample(self.head2(map2), scale_factor=2, mode=""nearest"")\n        map1 = nn.functional.upsample(self.head1(map1), scale_factor=1, mode=""nearest"")\n\n        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n        smoothed = self.smooth2(smoothed + map0)\n        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=""nearest"")\n\n        final = self.final(smoothed)\n        res = torch.tanh(final) + x\n\n        return torch.clamp(res, min=-1, max=1)\n\n\nclass FPN(nn.Module):\n\n    def __init__(self, norm_layer, num_filters=128, pretrained=True):\n        """"""Creates an `FPN` instance for feature extraction.\n        Args:\n          num_filters: the number of filters in each output pyramid level\n          pretrained: use ImageNet pre-trained backbone feature extractor\n        """"""\n\n        super().__init__()\n        net = MobileNetV2(n_class=1000)\n\n        if pretrained:\n            #Load weights into the project directory\n            state_dict = torch.load(\'mobilenetv2.pth.tar\') # add map_location=\'cpu\' if no gpu\n            net.load_state_dict(state_dict)\n        self.features = net.features\n\n        self.enc0 = nn.Sequential(*self.features[0:2])\n        self.enc1 = nn.Sequential(*self.features[2:4])\n        self.enc2 = nn.Sequential(*self.features[4:7])\n        self.enc3 = nn.Sequential(*self.features[7:11])\n        self.enc4 = nn.Sequential(*self.features[11:16])\n\n        self.td1 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n        self.td2 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n        self.td3 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n                                 norm_layer(num_filters),\n                                 nn.ReLU(inplace=True))\n\n        self.lateral4 = nn.Conv2d(160, num_filters, kernel_size=1, bias=False)\n        self.lateral3 = nn.Conv2d(64, num_filters, kernel_size=1, bias=False)\n        self.lateral2 = nn.Conv2d(32, num_filters, kernel_size=1, bias=False)\n        self.lateral1 = nn.Conv2d(24, num_filters, kernel_size=1, bias=False)\n        self.lateral0 = nn.Conv2d(16, num_filters // 2, kernel_size=1, bias=False)\n\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def unfreeze(self):\n        for param in self.features.parameters():\n            param.requires_grad = True\n\n\n    def forward(self, x):\n\n        # Bottom-up pathway, from ResNet\n        enc0 = self.enc0(x)\n\n        enc1 = self.enc1(enc0) # 256\n\n        enc2 = self.enc2(enc1) # 512\n\n        enc3 = self.enc3(enc2) # 1024\n\n        enc4 = self.enc4(enc3) # 2048\n\n        # Lateral connections\n\n        lateral4 = self.lateral4(enc4)\n        lateral3 = self.lateral3(enc3)\n        lateral2 = self.lateral2(enc2)\n        lateral1 = self.lateral1(enc1)\n        lateral0 = self.lateral0(enc0)\n\n        # Top-down pathway\n        map4 = lateral4\n        map3 = self.td1(lateral3 + nn.functional.upsample(map4, scale_factor=2, mode=""nearest""))\n        map2 = self.td2(lateral2 + nn.functional.upsample(map3, scale_factor=2, mode=""nearest""))\n        map1 = self.td3(lateral1 + nn.functional.upsample(map2, scale_factor=2, mode=""nearest""))\n        return lateral0, map1, map2, map3, map4\n\n'"
gimp-plugins/DeblurGANv2/models/losses.py,16,"b'import torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom util.image_pool import ImagePool\n\n\n###############################################################################\n# Functions\n###############################################################################\n\nclass ContentLoss():\n    def initialize(self, loss):\n        self.criterion = loss\n\n    def get_loss(self, fakeIm, realIm):\n        return self.criterion(fakeIm, realIm)\n\n    def __call__(self, fakeIm, realIm):\n        return self.get_loss(fakeIm, realIm)\n\n\nclass PerceptualLoss():\n\n    def contentFunc(self):\n        conv_3_3_layer = 14\n        cnn = models.vgg19(pretrained=True).features\n        cnn = cnn.cuda()\n        model = nn.Sequential()\n        model = model.cuda()\n        model = model.eval()\n        for i, layer in enumerate(list(cnn)):\n            model.add_module(str(i), layer)\n            if i == conv_3_3_layer:\n                break\n        return model\n\n    def initialize(self, loss):\n        with torch.no_grad():\n            self.criterion = loss\n            self.contentFunc = self.contentFunc()\n            self.transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    def get_loss(self, fakeIm, realIm):\n        fakeIm = (fakeIm + 1) / 2.0\n        realIm = (realIm + 1) / 2.0\n        fakeIm[0, :, :, :] = self.transform(fakeIm[0, :, :, :])\n        realIm[0, :, :, :] = self.transform(realIm[0, :, :, :])\n        f_fake = self.contentFunc.forward(fakeIm)\n        f_real = self.contentFunc.forward(realIm)\n        f_real_no_grad = f_real.detach()\n        loss = self.criterion(f_fake, f_real_no_grad)\n        return 0.006 * torch.mean(loss) + 0.5 * nn.MSELoss()(fakeIm, realIm)\n\n    def __call__(self, fakeIm, realIm):\n        return self.get_loss(fakeIm, realIm)\n\n\nclass GANLoss(nn.Module):\n    def __init__(self, use_l1=True, target_real_label=1.0, target_fake_label=0.0,\n                 tensor=torch.FloatTensor):\n        super(GANLoss, self).__init__()\n        self.real_label = target_real_label\n        self.fake_label = target_fake_label\n        self.real_label_var = None\n        self.fake_label_var = None\n        self.Tensor = tensor\n        if use_l1:\n            self.loss = nn.L1Loss()\n        else:\n            self.loss = nn.BCEWithLogitsLoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        if target_is_real:\n            create_label = ((self.real_label_var is None) or\n                            (self.real_label_var.numel() != input.numel()))\n            if create_label:\n                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n                self.real_label_var = Variable(real_tensor, requires_grad=False)\n            target_tensor = self.real_label_var\n        else:\n            create_label = ((self.fake_label_var is None) or\n                            (self.fake_label_var.numel() != input.numel()))\n            if create_label:\n                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n            target_tensor = self.fake_label_var\n        return target_tensor.cuda()\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n\n\nclass DiscLoss(nn.Module):\n    def name(self):\n        return \'DiscLoss\'\n\n    def __init__(self):\n        super(DiscLoss, self).__init__()\n\n        self.criterionGAN = GANLoss(use_l1=False)\n        self.fake_AB_pool = ImagePool(50)\n\n    def get_g_loss(self, net, fakeB, realB):\n        # First, G(A) should fake the discriminator\n        pred_fake = net.forward(fakeB)\n        return self.criterionGAN(pred_fake, 1)\n\n    def get_loss(self, net, fakeB, realB):\n        # Fake\n        # stop backprop to the generator by detaching fake_B\n        # Generated Image Disc Output should be close to zero\n        self.pred_fake = net.forward(fakeB.detach())\n        self.loss_D_fake = self.criterionGAN(self.pred_fake, 0)\n\n        # Real\n        self.pred_real = net.forward(realB)\n        self.loss_D_real = self.criterionGAN(self.pred_real, 1)\n\n        # Combined loss\n        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n        return self.loss_D\n\n    def __call__(self, net, fakeB, realB):\n        return self.get_loss(net, fakeB, realB)\n\n\nclass RelativisticDiscLoss(nn.Module):\n    def name(self):\n        return \'RelativisticDiscLoss\'\n\n    def __init__(self):\n        super(RelativisticDiscLoss, self).__init__()\n\n        self.criterionGAN = GANLoss(use_l1=False)\n        self.fake_pool = ImagePool(50)  # create image buffer to store previously generated images\n        self.real_pool = ImagePool(50)\n\n    def get_g_loss(self, net, fakeB, realB):\n        # First, G(A) should fake the discriminator\n        self.pred_fake = net.forward(fakeB)\n\n        # Real\n        self.pred_real = net.forward(realB)\n        errG = (self.criterionGAN(self.pred_real - torch.mean(self.fake_pool.query()), 0) +\n                self.criterionGAN(self.pred_fake - torch.mean(self.real_pool.query()), 1)) / 2\n        return errG\n\n    def get_loss(self, net, fakeB, realB):\n        # Fake\n        # stop backprop to the generator by detaching fake_B\n        # Generated Image Disc Output should be close to zero\n        self.fake_B = fakeB.detach()\n        self.real_B = realB\n        self.pred_fake = net.forward(fakeB.detach())\n        self.fake_pool.add(self.pred_fake)\n\n        # Real\n        self.pred_real = net.forward(realB)\n        self.real_pool.add(self.pred_real)\n\n        # Combined loss\n        self.loss_D = (self.criterionGAN(self.pred_real - torch.mean(self.fake_pool.query()), 1) +\n                       self.criterionGAN(self.pred_fake - torch.mean(self.real_pool.query()), 0)) / 2\n        return self.loss_D\n\n    def __call__(self, net, fakeB, realB):\n        return self.get_loss(net, fakeB, realB)\n\n\nclass RelativisticDiscLossLS(nn.Module):\n    def name(self):\n        return \'RelativisticDiscLossLS\'\n\n    def __init__(self):\n        super(RelativisticDiscLossLS, self).__init__()\n\n        self.criterionGAN = GANLoss(use_l1=True)\n        self.fake_pool = ImagePool(50)  # create image buffer to store previously generated images\n        self.real_pool = ImagePool(50)\n\n    def get_g_loss(self, net, fakeB, realB):\n        # First, G(A) should fake the discriminator\n        self.pred_fake = net.forward(fakeB)\n\n        # Real\n        self.pred_real = net.forward(realB)\n        errG = (torch.mean((self.pred_real - torch.mean(self.fake_pool.query()) + 1) ** 2) +\n                torch.mean((self.pred_fake - torch.mean(self.real_pool.query()) - 1) ** 2)) / 2\n        return errG\n\n    def get_loss(self, net, fakeB, realB):\n        # Fake\n        # stop backprop to the generator by detaching fake_B\n        # Generated Image Disc Output should be close to zero\n        self.fake_B = fakeB.detach()\n        self.real_B = realB\n        self.pred_fake = net.forward(fakeB.detach())\n        self.fake_pool.add(self.pred_fake)\n\n        # Real\n        self.pred_real = net.forward(realB)\n        self.real_pool.add(self.pred_real)\n\n        # Combined loss\n        self.loss_D = (torch.mean((self.pred_real - torch.mean(self.fake_pool.query()) - 1) ** 2) +\n                       torch.mean((self.pred_fake - torch.mean(self.real_pool.query()) + 1) ** 2)) / 2\n        return self.loss_D\n\n    def __call__(self, net, fakeB, realB):\n        return self.get_loss(net, fakeB, realB)\n\n\nclass DiscLossLS(DiscLoss):\n    def name(self):\n        return \'DiscLossLS\'\n\n    def __init__(self):\n        super(DiscLossLS, self).__init__()\n        self.criterionGAN = GANLoss(use_l1=True)\n\n    def get_g_loss(self, net, fakeB, realB):\n        return DiscLoss.get_g_loss(self, net, fakeB)\n\n    def get_loss(self, net, fakeB, realB):\n        return DiscLoss.get_loss(self, net, fakeB, realB)\n\n\nclass DiscLossWGANGP(DiscLossLS):\n    def name(self):\n        return \'DiscLossWGAN-GP\'\n\n    def __init__(self):\n        super(DiscLossWGANGP, self).__init__()\n        self.LAMBDA = 10\n\n    def get_g_loss(self, net, fakeB, realB):\n        # First, G(A) should fake the discriminator\n        self.D_fake = net.forward(fakeB)\n        return -self.D_fake.mean()\n\n    def calc_gradient_penalty(self, netD, real_data, fake_data):\n        alpha = torch.rand(1, 1)\n        alpha = alpha.expand(real_data.size())\n        alpha = alpha.cuda()\n\n        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n\n        interpolates = interpolates.cuda()\n        interpolates = Variable(interpolates, requires_grad=True)\n\n        disc_interpolates = netD.forward(interpolates)\n\n        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                                  grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.LAMBDA\n        return gradient_penalty\n\n    def get_loss(self, net, fakeB, realB):\n        self.D_fake = net.forward(fakeB.detach())\n        self.D_fake = self.D_fake.mean()\n\n        # Real\n        self.D_real = net.forward(realB)\n        self.D_real = self.D_real.mean()\n        # Combined loss\n        self.loss_D = self.D_fake - self.D_real\n        gradient_penalty = self.calc_gradient_penalty(net, realB.data, fakeB.data)\n        return self.loss_D + gradient_penalty\n\n\ndef get_loss(model):\n    if model[\'content_loss\'] == \'perceptual\':\n        content_loss = PerceptualLoss()\n        content_loss.initialize(nn.MSELoss())\n    elif model[\'content_loss\'] == \'l1\':\n        content_loss = ContentLoss()\n        content_loss.initialize(nn.L1Loss())\n    else:\n        raise ValueError(""ContentLoss [%s] not recognized."" % model[\'content_loss\'])\n\n    if model[\'disc_loss\'] == \'wgan-gp\':\n        disc_loss = DiscLossWGANGP()\n    elif model[\'disc_loss\'] == \'lsgan\':\n        disc_loss = DiscLossLS()\n    elif model[\'disc_loss\'] == \'gan\':\n        disc_loss = DiscLoss()\n    elif model[\'disc_loss\'] == \'ragan\':\n        disc_loss = RelativisticDiscLoss()\n    elif model[\'disc_loss\'] == \'ragan-ls\':\n        disc_loss = RelativisticDiscLossLS()\n    else:\n        raise ValueError(""GAN Loss [%s] not recognized."" % model[\'disc_loss\'])\n    return content_loss, disc_loss\n'"
gimp-plugins/DeblurGANv2/models/mobilenet_v2.py,1,"b'import torch.nn as nn\nimport math\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n'"
gimp-plugins/DeblurGANv2/models/models.py,1,"b""import numpy as np\nimport torch.nn as nn\nfrom skimage.measure import compare_ssim as SSIM\n\nfrom util.metrics import PSNR\n\n\nclass DeblurModel(nn.Module):\n    def __init__(self):\n        super(DeblurModel, self).__init__()\n\n    def get_input(self, data):\n        img = data['a']\n        inputs = img\n        targets = data['b']\n        inputs, targets = inputs.cuda(), targets.cuda()\n        return inputs, targets\n\n    def tensor2im(self, image_tensor, imtype=np.uint8):\n        image_numpy = image_tensor[0].cpu().float().numpy()\n        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n        return image_numpy.astype(imtype)\n\n    def get_images_and_metrics(self, inp, output, target):\n        inp = self.tensor2im(inp)\n        fake = self.tensor2im(output.data)\n        real = self.tensor2im(target.data)\n        psnr = PSNR(fake, real)\n        ssim = SSIM(fake, real, multichannel=True)\n        vis_img = np.hstack((inp, fake, real))\n        return psnr, ssim, vis_img\n\n\ndef get_model(model_config):\n    return DeblurModel()\n"""
gimp-plugins/DeblurGANv2/models/networks.py,7,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.autograd import Variable\nimport numpy as np\nfrom fpn_mobilenet import FPNMobileNet\nfrom fpn_inception import FPNInception\n# from fpn_inception_simple import FPNInceptionSimple\nfrom unet_seresnext import UNetSEResNext\nfrom fpn_densenet import FPNDense\n###############################################################################\n# Functions\n###############################################################################\n\n\ndef get_norm_layer(norm_type=\'instance\'):\n    if norm_type == \'batch\':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == \'instance\':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n    else:\n        raise NotImplementedError(\'normalization layer [%s] is not found\' % norm_type)\n    return norm_layer\n\n##############################################################################\n# Classes\n##############################################################################\n\n\n# Defines the generator that consists of Resnet blocks between a few\n# downsampling/upsampling operations.\n# Code and idea originally from Justin Johnson\'s architecture.\n# https://github.com/jcjohnson/fast-neural-style/\nclass ResnetGenerator(nn.Module):\n    def __init__(self, input_nc=3, output_nc=3, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, use_parallel=True, learn_residual=True, padding_type=\'reflect\'):\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        self.use_parallel = use_parallel\n        self.learn_residual = learn_residual\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n                           bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        output = self.model(input)\n        if self.learn_residual:\n            output = input + output\n            output = torch.clamp(output,min = -1,max = 1)\n        return output\n\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        conv_block = []\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim),\n                       nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\nclass DicsriminatorTail(nn.Module):\n    def __init__(self, nf_mult, n_layers, ndf=64, norm_layer=nn.BatchNorm2d, use_parallel=True):\n        super(DicsriminatorTail, self).__init__()\n        self.use_parallel = use_parallel\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = int(np.ceil((kw-1)/2))\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence = [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\nclass MultiScaleDiscriminator(nn.Module):\n    def __init__(self, input_nc=3, ndf=64, norm_layer=nn.BatchNorm2d, use_parallel=True):\n        super(MultiScaleDiscriminator, self).__init__()\n        self.use_parallel = use_parallel\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = int(np.ceil((kw-1)/2))\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        for n in range(1, 3):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        self.scale_one = nn.Sequential(*sequence)\n        self.first_tail = DicsriminatorTail(nf_mult=nf_mult, n_layers=3)\n        nf_mult_prev = 4\n        nf_mult = 8\n\n        self.scale_two = nn.Sequential(\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True))\n        nf_mult_prev = nf_mult\n        self.second_tail = DicsriminatorTail(nf_mult=nf_mult, n_layers=4)\n        self.scale_three = nn.Sequential(\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True))\n        self.third_tail = DicsriminatorTail(nf_mult=nf_mult, n_layers=5)\n\n    def forward(self, input):\n        x = self.scale_one(input)\n        x_1 = self.first_tail(x)\n        x = self.scale_two(x)\n        x_2 = self.second_tail(x)\n        x = self.scale_three(x)\n        x = self.third_tail(x)\n        return [x_1, x_2, x]\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc=3, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, use_parallel=True):\n        super(NLayerDiscriminator, self).__init__()\n        self.use_parallel = use_parallel\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = int(np.ceil((kw-1)/2))\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\ndef get_fullD(model_config):\n    model_d = NLayerDiscriminator(n_layers=5,\n                                  norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']),\n                                  use_sigmoid=False)\n    return model_d\n\n\ndef get_generator(model_config):\n    generator_name = model_config[\'g_name\']\n    if generator_name == \'resnet\':\n        model_g = ResnetGenerator(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']),\n                                  use_dropout=model_config[\'dropout\'],\n                                  n_blocks=model_config[\'blocks\'],\n                                  learn_residual=model_config[\'learn_residual\'])\n    elif generator_name == \'fpn_mobilenet\':\n        model_g = FPNMobileNet(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']))\n    elif generator_name == \'fpn_inception\':\n        # model_g = FPNInception(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']))\n        # torch.save(model_g, \'mymodel.pth\')\n        model_g = torch.load(\'mymodel.pth\')\n    elif generator_name == \'fpn_inception_simple\':\n        model_g = FPNInceptionSimple(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']))\n    elif generator_name == \'fpn_dense\':\n        model_g = FPNDense()\n    elif generator_name == \'unet_seresnext\':\n        model_g = UNetSEResNext(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']),\n                                pretrained=model_config[\'pretrained\'])\n    else:\n        raise ValueError(""Generator Network [%s] not recognized."" % generator_name)\n\n    return nn.DataParallel(model_g)\n\ndef get_generator_new(weights_path):\n\n    model_g = torch.load(weights_path+\'mymodel.pth\')\n\n    return nn.DataParallel(model_g)\n\ndef get_discriminator(model_config):\n    discriminator_name = model_config[\'d_name\']\n    if discriminator_name == \'no_gan\':\n        model_d = None\n    elif discriminator_name == \'patch_gan\':\n        model_d = NLayerDiscriminator(n_layers=model_config[\'d_layers\'],\n                                      norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']),\n                                      use_sigmoid=False)\n        model_d = nn.DataParallel(model_d)\n    elif discriminator_name == \'double_gan\':\n        patch_gan = NLayerDiscriminator(n_layers=model_config[\'d_layers\'],\n                                        norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']),\n                                        use_sigmoid=False)\n        patch_gan = nn.DataParallel(patch_gan)\n        full_gan = get_fullD(model_config)\n        full_gan = nn.DataParallel(full_gan)\n        model_d = {\'patch\': patch_gan,\n                   \'full\': full_gan}\n    elif discriminator_name == \'multi_scale\':\n        model_d = MultiScaleDiscriminator(norm_layer=get_norm_layer(norm_type=model_config[\'norm_layer\']))\n        model_d = nn.DataParallel(model_d)\n    else:\n        raise ValueError(""Discriminator Network [%s] not recognized."" % discriminator_name)\n\n    return model_d\n\n\ndef get_nets(model_config):\n    return get_generator(model_config), get_discriminator(model_config)\n'"
gimp-plugins/DeblurGANv2/models/senet.py,2,"b'from __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = [\'SENet\', \'senet154\', \'se_resnet50\', \'se_resnet101\', \'se_resnet152\',\n           \'se_resnext50_32x4d\', \'se_resnext101_32x4d\']\n\npretrained_settings = {\n    \'senet154\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet50\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet101\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet152\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext50_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Base class for bottlenecks that implements `forward()` method.\n    """"""\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    """"""\n    Bottleneck for SENet154.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1)\n        self.bn1 = nn.InstanceNorm2d(planes * 2, affine=False)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups)\n        self.bn2 = nn.InstanceNorm2d(planes * 4, affine=False)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1)\n        self.bn3 = nn.InstanceNorm2d(planes * 4, affine=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    """"""\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1,\n                               stride=stride)\n        self.bn1 = nn.InstanceNorm2d(planes, affine=False)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups)\n        self.bn2 = nn.InstanceNorm2d(planes, affine=False)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1)\n        self.bn3 = nn.InstanceNorm2d(planes * 4, affine=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    """"""\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1,\n                               stride=1)\n        self.bn1 = nn.InstanceNorm2d(width, affine=False)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups)\n        self.bn2 = nn.InstanceNorm2d(width, affine=False)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1)\n        self.bn3 = nn.InstanceNorm2d(planes * 4, affine=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        """"""\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        """"""\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, 64, 3, stride=2, padding=1)),\n                (\'bn1\', nn.InstanceNorm2d(64, affine=False)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n                (\'conv2\', nn.Conv2d(64, 64, 3, stride=1, padding=1)),\n                (\'bn2\', nn.InstanceNorm2d(64, affine=False)),\n                (\'relu2\', nn.ReLU(inplace=True)),\n                (\'conv3\', nn.Conv2d(64, inplanes, 3, stride=1, padding=1)),\n                (\'bn3\', nn.InstanceNorm2d(inplanes, affine=False)),\n                (\'relu3\', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3)),\n                (\'bn1\', nn.InstanceNorm2d(inplanes, affine=False)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append((\'pool\', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding),\n                nn.InstanceNorm2d(planes * block.expansion, affine=False),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings[\'num_classes\'], \\\n        \'num_classes should be {}, but is {}\'.format(\n            settings[\'num_classes\'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n\n\ndef senet154(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'senet154\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet50\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet101\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet152\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext101_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model'"
gimp-plugins/DeblurGANv2/models/unet_seresnext.py,10,"b'import torch\nfrom torch import nn\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nfrom torch.nn import Sequential\nfrom collections import OrderedDict\nimport torchvision\nfrom torch.nn import functional as F\nfrom senet import se_resnext50_32x4d\n\n\ndef conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_, out):\n        super(ConvRelu, self).__init__()\n        self.conv = conv3x3(in_, out)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.activation(x)\n        return x\n\nclass UNetSEResNext(nn.Module):\n\n    def __init__(self, num_classes=3, num_filters=32,\n             pretrained=True, is_deconv=True):\n        super().__init__()\n        self.num_classes = num_classes\n        pretrain = \'imagenet\' if pretrained is True else None\n        self.encoder = se_resnext50_32x4d(num_classes=1000, pretrained=pretrain)\n        bottom_channel_nr = 2048\n\n        self.conv1 = self.encoder.layer0\n        #self.se_e1 = SCSEBlock(64)\n        self.conv2 = self.encoder.layer1\n        #self.se_e2 = SCSEBlock(64 * 4)\n        self.conv3 = self.encoder.layer2\n        #self.se_e3 = SCSEBlock(128 * 4)\n        self.conv4 = self.encoder.layer3\n        #self.se_e4 = SCSEBlock(256 * 4)\n        self.conv5 = self.encoder.layer4\n        #self.se_e5 = SCSEBlock(512 * 4)\n\n        self.center = DecoderCenter(bottom_channel_nr, num_filters * 8 *2, num_filters * 8, False)\n\n        self.dec5 = DecoderBlockV(bottom_channel_nr + num_filters * 8, num_filters * 8 * 2, num_filters * 2, is_deconv)\n        #self.se_d5 = SCSEBlock(num_filters * 2)\n        self.dec4 = DecoderBlockV(bottom_channel_nr // 2 + num_filters * 2, num_filters * 8, num_filters * 2, is_deconv)\n        #self.se_d4 = SCSEBlock(num_filters * 2)\n        self.dec3 = DecoderBlockV(bottom_channel_nr // 4 + num_filters * 2, num_filters * 4, num_filters * 2, is_deconv)\n        #self.se_d3 = SCSEBlock(num_filters * 2)\n        self.dec2 = DecoderBlockV(bottom_channel_nr // 8 + num_filters * 2, num_filters * 2, num_filters * 2, is_deconv)\n        #self.se_d2 = SCSEBlock(num_filters * 2)\n        self.dec1 = DecoderBlockV(num_filters * 2, num_filters, num_filters * 2, is_deconv)\n        #self.se_d1 = SCSEBlock(num_filters * 2)\n        self.dec0 = ConvRelu(num_filters * 10, num_filters * 2)\n        self.final = nn.Conv2d(num_filters * 2, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        conv1 = self.conv1(x)\n        #conv1 = self.se_e1(conv1)\n        conv2 = self.conv2(conv1)\n        #conv2 = self.se_e2(conv2)\n        conv3 = self.conv3(conv2)\n        #conv3 = self.se_e3(conv3)\n        conv4 = self.conv4(conv3)\n        #conv4 = self.se_e4(conv4)\n        conv5 = self.conv5(conv4)\n        #conv5 = self.se_e5(conv5)\n\n        center = self.center(conv5)\n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n        #dec5 = self.se_d5(dec5)\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        #dec4 = self.se_d4(dec4)\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        #dec3 = self.se_d3(dec3)\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        #dec2 = self.se_d2(dec2)\n        dec1 = self.dec1(dec2)\n        #dec1 = self.se_d1(dec1)\n\n        f = torch.cat((\n            dec1,\n            F.upsample(dec2, scale_factor=2, mode=\'bilinear\', align_corners=False),\n            F.upsample(dec3, scale_factor=4, mode=\'bilinear\', align_corners=False),\n            F.upsample(dec4, scale_factor=8, mode=\'bilinear\', align_corners=False),\n            F.upsample(dec5, scale_factor=16, mode=\'bilinear\', align_corners=False),\n        ), 1)\n\n        dec0 = self.dec0(f)\n\n        return self.final(dec0)\n\nclass DecoderBlockV(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n        super(DecoderBlockV, self).__init__()\n        self.in_channels = in_channels\n\n        if is_deconv:\n            self.block = nn.Sequential(\n                ConvRelu(in_channels, middle_channels),\n                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n                                   padding=1),\n                nn.InstanceNorm2d(out_channels, affine=False),\n                nn.ReLU(inplace=True)\n\n            )\n        else:\n            self.block = nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=\'bilinear\'),\n                ConvRelu(in_channels, middle_channels),\n                ConvRelu(middle_channels, out_channels),\n            )\n\n    def forward(self, x):\n        return self.block(x)\n\n\n\nclass DecoderCenter(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n        super(DecoderCenter, self).__init__()\n        self.in_channels = in_channels\n\n\n        if is_deconv:\n            """"""\n                Paramaters for Deconvolution were chosen to avoid artifacts, following\n                link https://distill.pub/2016/deconv-checkerboard/\n            """"""\n\n            self.block = nn.Sequential(\n                ConvRelu(in_channels, middle_channels),\n                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n                                   padding=1),\n                nn.InstanceNorm2d(out_channels, affine=False),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            self.block = nn.Sequential(\n                ConvRelu(in_channels, middle_channels),\n                ConvRelu(middle_channels, out_channels)\n\n            )\n\n    def forward(self, x):\n        return self.block(x)\n'"
gimp-plugins/DeblurGANv2/util/__init__.py,0,b''
gimp-plugins/DeblurGANv2/util/image_pool.py,3,"b'import random\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom collections import deque\n\n\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        self.sample_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = deque()\n\n    def add(self, images):\n        if self.pool_size == 0:\n            return images\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n            else:\n                self.images.popleft()\n                self.images.append(image)\n\n    def query(self):\n        if len(self.images) > self.sample_size:\n            return_images = list(random.sample(self.images, self.sample_size))\n        else:\n            return_images = list(self.images)\n        return torch.cat(return_images, 0)\n'"
gimp-plugins/DeblurGANv2/util/metrics.py,3,"b'import math\nfrom math import exp\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n    return gauss / gauss.sum()\n\n\ndef create_window(window_size, channel):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n    return window\n\n\ndef SSIM(img1, img2):\n    (_, channel, _, _) = img1.size()\n    window_size = 11\n    window = create_window(window_size, channel)\n\n    if img1.is_cuda:\n        window = window.cuda(img1.get_device())\n    window = window.type_as(img1)\n\n    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n\n    C1 = 0.01 ** 2\n    C2 = 0.03 ** 2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n    return ssim_map.mean()\n\n\ndef PSNR(img1, img2):\n    mse = np.mean((img1 / 255. - img2 / 255.) ** 2)\n    if mse == 0:\n        return 100\n    PIXEL_MAX = 1\n    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n'"
gimp-plugins/face-parsing.PyTorch/modules/__init__.py,0,"b'from .bn import ABN, InPlaceABN, InPlaceABNSync\nfrom .functions import ACT_RELU, ACT_LEAKY_RELU, ACT_ELU, ACT_NONE\nfrom .misc import GlobalAvgPool2d, SingleGPU\nfrom .residual import IdentityResidualBlock\nfrom .dense import DenseModule\n'"
gimp-plugins/face-parsing.PyTorch/modules/bn.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\n\ntry:\n    from queue import Queue\nexcept ImportError:\n    from Queue import Queue\n\nfrom .functions import *\n\n\nclass ABN(nn.Module):\n    """"""Activated Batch Normalization\n\n    This gathers a `BatchNorm2d` and an activation function in a single module\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(ABN, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        self.activation = activation\n        self.slope = slope\n        if self.affine:\n            self.weight = nn.Parameter(torch.ones(num_features))\n            self.bias = nn.Parameter(torch.zeros(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.constant_(self.running_mean, 0)\n        nn.init.constant_(self.running_var, 1)\n        if self.affine:\n            nn.init.constant_(self.weight, 1)\n            nn.init.constant_(self.bias, 0)\n\n    def forward(self, x):\n        x = functional.batch_norm(x, self.running_mean, self.running_var, self.weight, self.bias,\n                                  self.training, self.momentum, self.eps)\n\n        if self.activation == ACT_RELU:\n            return functional.relu(x, inplace=True)\n        elif self.activation == ACT_LEAKY_RELU:\n            return functional.leaky_relu(x, negative_slope=self.slope, inplace=True)\n        elif self.activation == ACT_ELU:\n            return functional.elu(x, inplace=True)\n        else:\n            return x\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \', slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass InPlaceABN(ABN):\n    """"""InPlace Activated Batch Normalization""""""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an InPlace Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(InPlaceABN, self).__init__(num_features, eps, momentum, affine, activation, slope)\n\n    def forward(self, x):\n        return inplace_abn(x, self.weight, self.bias, self.running_mean, self.running_var,\n                           self.training, self.momentum, self.eps, self.activation, self.slope)\n\n\nclass InPlaceABNSync(ABN):\n    """"""InPlace Activated Batch Normalization with cross-GPU synchronization\n    This assumes that it will be replicated across GPUs using the same mechanism as in `nn.DistributedDataParallel`.\n    """"""\n\n    def forward(self, x):\n        return inplace_abn_sync(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                   self.training, self.momentum, self.eps, self.activation, self.slope)\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \', slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\n'"
gimp-plugins/face-parsing.PyTorch/modules/deeplab.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\n\nfrom models._util import try_index\nfrom .bn import ABN\n\n\nclass DeeplabV3(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 hidden_channels=256,\n                 dilations=(12, 24, 36),\n                 norm_act=ABN,\n                 pooling_size=None):\n        super(DeeplabV3, self).__init__()\n        self.pooling_size = pooling_size\n\n        self.map_convs = nn.ModuleList([\n            nn.Conv2d(in_channels, hidden_channels, 1, bias=False),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[0], padding=dilations[0]),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[1], padding=dilations[1]),\n            nn.Conv2d(in_channels, hidden_channels, 3, bias=False, dilation=dilations[2], padding=dilations[2])\n        ])\n        self.map_bn = norm_act(hidden_channels * 4)\n\n        self.global_pooling_conv = nn.Conv2d(in_channels, hidden_channels, 1, bias=False)\n        self.global_pooling_bn = norm_act(hidden_channels)\n\n        self.red_conv = nn.Conv2d(hidden_channels * 4, out_channels, 1, bias=False)\n        self.pool_red_conv = nn.Conv2d(hidden_channels, out_channels, 1, bias=False)\n        self.red_bn = norm_act(out_channels)\n\n        self.reset_parameters(self.map_bn.activation, self.map_bn.slope)\n\n    def reset_parameters(self, activation, slope):\n        gain = nn.init.calculate_gain(activation, slope)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight.data, gain)\n                if hasattr(m, ""bias"") and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, ABN):\n                if hasattr(m, ""weight"") and m.weight is not None:\n                    nn.init.constant_(m.weight, 1)\n                if hasattr(m, ""bias"") and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        # Map convolutions\n        out = torch.cat([m(x) for m in self.map_convs], dim=1)\n        out = self.map_bn(out)\n        out = self.red_conv(out)\n\n        # Global pooling\n        pool = self._global_pooling(x)\n        pool = self.global_pooling_conv(pool)\n        pool = self.global_pooling_bn(pool)\n        pool = self.pool_red_conv(pool)\n        if self.training or self.pooling_size is None:\n            pool = pool.repeat(1, 1, x.size(2), x.size(3))\n\n        out += pool\n        out = self.red_bn(out)\n        return out\n\n    def _global_pooling(self, x):\n        if self.training or self.pooling_size is None:\n            pool = x.view(x.size(0), x.size(1), -1).mean(dim=-1)\n            pool = pool.view(x.size(0), x.size(1), 1, 1)\n        else:\n            pooling_size = (min(try_index(self.pooling_size, 0), x.shape[2]),\n                            min(try_index(self.pooling_size, 1), x.shape[3]))\n            padding = (\n                (pooling_size[1] - 1) // 2,\n                (pooling_size[1] - 1) // 2 if pooling_size[1] % 2 == 1 else (pooling_size[1] - 1) // 2 + 1,\n                (pooling_size[0] - 1) // 2,\n                (pooling_size[0] - 1) // 2 if pooling_size[0] % 2 == 1 else (pooling_size[0] - 1) // 2 + 1\n            )\n\n            pool = functional.avg_pool2d(x, pooling_size, stride=1)\n            pool = functional.pad(pool, pad=padding, mode=""replicate"")\n        return pool\n'"
gimp-plugins/face-parsing.PyTorch/modules/dense.py,3,"b'from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\nfrom .bn import ABN\n\n\nclass DenseModule(nn.Module):\n    def __init__(self, in_channels, growth, layers, bottleneck_factor=4, norm_act=ABN, dilation=1):\n        super(DenseModule, self).__init__()\n        self.in_channels = in_channels\n        self.growth = growth\n        self.layers = layers\n\n        self.convs1 = nn.ModuleList()\n        self.convs3 = nn.ModuleList()\n        for i in range(self.layers):\n            self.convs1.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(in_channels)),\n                (""conv"", nn.Conv2d(in_channels, self.growth * bottleneck_factor, 1, bias=False))\n            ])))\n            self.convs3.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(self.growth * bottleneck_factor)),\n                (""conv"", nn.Conv2d(self.growth * bottleneck_factor, self.growth, 3, padding=dilation, bias=False,\n                                   dilation=dilation))\n            ])))\n            in_channels += self.growth\n\n    @property\n    def out_channels(self):\n        return self.in_channels + self.growth * self.layers\n\n    def forward(self, x):\n        inputs = [x]\n        for i in range(self.layers):\n            x = torch.cat(inputs, dim=1)\n            x = self.convs1[i](x)\n            x = self.convs3[i](x)\n            inputs += [x]\n\n        return torch.cat(inputs, dim=1)\n'"
gimp-plugins/face-parsing.PyTorch/modules/functions.py,6,"b'from os import path\nimport torch \nimport torch.distributed as dist\nimport torch.autograd as autograd\nimport torch.cuda.comm as comm\nfrom torch.autograd.function import once_differentiable\nfrom torch.utils.cpp_extension import load\n\n_src_path = path.join(path.dirname(path.abspath(__file__)), ""src"")\n_backend = load(name=""inplace_abn"",\n                extra_cflags=[""-O3""],\n                sources=[path.join(_src_path, f) for f in [\n                    ""inplace_abn.cpp"",\n                    ""inplace_abn_cpu.cpp"",\n                    ""inplace_abn_cuda.cu"",\n                    ""inplace_abn_cuda_half.cu""\n                ]],\n                extra_cuda_cflags=[""--expt-extended-lambda""])\n\n# Activation names\nACT_RELU = ""relu""\nACT_LEAKY_RELU = ""leaky_relu""\nACT_ELU = ""elu""\nACT_NONE = ""none""\n\n\ndef _check(fn, *args, **kwargs):\n    success = fn(*args, **kwargs)\n    if not success:\n        raise RuntimeError(""CUDA Error encountered in {}"".format(fn))\n\n\ndef _broadcast_shape(x):\n    out_size = []\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            out_size.append(1)\n        else:\n            out_size.append(s)\n    return out_size\n\n\ndef _reduce(x):\n    if len(x.size()) == 2:\n        return x.sum(dim=0)\n    else:\n        n, c = x.size()[0:2]\n        return x.contiguous().view((n, c, -1)).sum(2).sum(0)\n\n\ndef _count_samples(x):\n    count = 1\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            count *= s\n    return count\n\n\ndef _act_forward(ctx, x):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_forward(x, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_forward(x)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\ndef _act_backward(ctx, x, dx):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _backend.leaky_relu_backward(x, dx, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _backend.elu_backward(x, dx)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\nclass InPlaceABN(autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        count = _count_samples(x)\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * count / (count - 1))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n        else:\n            # TODO: implement simplified CUDA backward for inference mode\n            edz = dz.new_zeros(dz.size(1))\n            eydz = dz.new_zeros(dz.size(1))\n\n        dx = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = eydz * weight.sign() if ctx.affine else None\n        dbias = edz if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\nclass InPlaceABNSync(autograd.Function):\n    @classmethod\n    def forward(cls, ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01, equal_batches=True):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n        ctx.affine = weight is not None and bias is not None\n\n        # Prepare inputs\n        ctx.world_size = dist.get_world_size() if dist.is_initialized() else 1\n\n        #count = _count_samples(x)\n        batch_size = x.new_tensor([x.shape[0]],dtype=torch.long)\n\n        x = x.contiguous()\n        weight = weight.contiguous() if ctx.affine else x.new_empty(0)\n        bias = bias.contiguous() if ctx.affine else x.new_empty(0)\n\n        if ctx.training:\n            mean, var = _backend.mean_var(x)\n            if ctx.world_size>1:\n                # get global batch size\n                if equal_batches:\n                    batch_size *= ctx.world_size\n                else:\n                    dist.all_reduce(batch_size, dist.ReduceOp.SUM)\n\n                ctx.factor = x.shape[0]/float(batch_size.item())\n\n                mean_all = mean.clone() * ctx.factor\n                dist.all_reduce(mean_all, dist.ReduceOp.SUM)\n\n                var_all = (var + (mean - mean_all) ** 2) * ctx.factor\n                dist.all_reduce(var_all, dist.ReduceOp.SUM)\n\n                mean = mean_all\n                var = var_all\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            count = batch_size.item() * x.view(x.shape[0],x.shape[1],-1).shape[-1]\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * (float(count) / (count - 1)))\n\n            # Mark in-place modified tensors\n            ctx.mark_dirty(x, running_mean, running_var)\n        else:\n            mean, var = running_mean.contiguous(), running_var.contiguous()\n            ctx.mark_dirty(x)\n\n        # BN forward + activation\n        _backend.forward(x, mean, var, weight, bias, ctx.affine, ctx.eps)\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, var, weight, bias)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, var, weight, bias = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.training:\n            edz, eydz = _backend.edz_eydz(z, dz, weight, bias, ctx.affine, ctx.eps)\n            edz_local = edz.clone()\n            eydz_local = eydz.clone()\n\n            if ctx.world_size>1:\n                edz *= ctx.factor\n                dist.all_reduce(edz, dist.ReduceOp.SUM)\n\n                eydz *= ctx.factor\n                dist.all_reduce(eydz, dist.ReduceOp.SUM)\n        else:\n            edz_local = edz = dz.new_zeros(dz.size(1))\n            eydz_local = eydz = dz.new_zeros(dz.size(1))\n\n        dx = _backend.backward(z, dz, var, weight, bias, edz, eydz, ctx.affine, ctx.eps)\n        dweight = eydz_local * weight.sign() if ctx.affine else None\n        dbias = edz_local if ctx.affine else None\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\ninplace_abn = InPlaceABN.apply\ninplace_abn_sync = InPlaceABNSync.apply\n\n__all__ = [""inplace_abn"", ""inplace_abn_sync"", ""ACT_RELU"", ""ACT_LEAKY_RELU"", ""ACT_ELU"", ""ACT_NONE""]\n'"
gimp-plugins/face-parsing.PyTorch/modules/misc.py,2,"b'import torch.nn as nn\nimport torch\nimport torch.distributed as dist\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        """"""Global average pooling over the input\'s spatial dimensions""""""\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        in_size = inputs.size()\n        return inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n\nclass SingleGPU(nn.Module):\n    def __init__(self, module):\n        super(SingleGPU, self).__init__()\n        self.module=module\n\n    def forward(self, input):\n        return self.module(input.cuda(non_blocking=True))\n\n'"
gimp-plugins/face-parsing.PyTorch/modules/residual.py,1,"b'from collections import OrderedDict\n\nimport torch.nn as nn\n\nfrom .bn import ABN\n\n\nclass IdentityResidualBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 channels,\n                 stride=1,\n                 dilation=1,\n                 groups=1,\n                 norm_act=ABN,\n                 dropout=None):\n        """"""Configurable identity-mapping residual block\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels.\n        channels : list of int\n            Number of channels in the internal feature maps. Can either have two or three elements: if three construct\n            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then\n            `3 x 3` then `1 x 1` convolutions.\n        stride : int\n            Stride of the first `3 x 3` convolution\n        dilation : int\n            Dilation to apply to the `3 x 3` convolutions.\n        groups : int\n            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with\n            bottleneck blocks.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        dropout: callable\n            Function to create Dropout Module.\n        """"""\n        super(IdentityResidualBlock, self).__init__()\n\n        # Check parameters for inconsistencies\n        if len(channels) != 2 and len(channels) != 3:\n            raise ValueError(""channels must contain either two or three values"")\n        if len(channels) == 2 and groups != 1:\n            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")\n\n        is_bottleneck = len(channels) == 3\n        need_proj_conv = stride != 1 or in_channels != channels[-1]\n\n        self.bn1 = norm_act(in_channels)\n        if not is_bottleneck:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,\n                                    dilation=dilation)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    dilation=dilation))\n            ]\n            if dropout is not None:\n                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]\n        else:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    groups=groups, dilation=dilation)),\n                (""bn3"", norm_act(channels[1])),\n                (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))\n            ]\n            if dropout is not None:\n                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]\n        self.convs = nn.Sequential(OrderedDict(layers))\n\n        if need_proj_conv:\n            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n\n    def forward(self, x):\n        if hasattr(self, ""proj_conv""):\n            bn1 = self.bn1(x)\n            shortcut = self.proj_conv(bn1)\n        else:\n            shortcut = x.clone()\n            bn1 = self.bn1(x)\n\n        out = self.convs(bn1)\n        out.add_(shortcut)\n\n        return out\n'"
gimp-plugins/monodepth2/networks/__init__.py,0,b'from .resnet_encoder import ResnetEncoder\nfrom .depth_decoder import DepthDecoder\nfrom .pose_decoder import PoseDecoder\nfrom .pose_cnn import PoseCNN\n'
gimp-plugins/monodepth2/networks/depth_decoder.py,2,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom collections import OrderedDict\nfrom layers import *\n\n\nclass DepthDecoder(nn.Module):\n    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):\n        super(DepthDecoder, self).__init__()\n\n        self.num_output_channels = num_output_channels\n        self.use_skips = use_skips\n        self.upsample_mode = \'nearest\'\n        self.scales = scales\n\n        self.num_ch_enc = num_ch_enc\n        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n\n        # decoder\n        self.convs = OrderedDict()\n        for i in range(4, -1, -1):\n            # upconv_0\n            num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]\n            num_ch_out = self.num_ch_dec[i]\n            self.convs[(""upconv"", i, 0)] = ConvBlock(num_ch_in, num_ch_out)\n\n            # upconv_1\n            num_ch_in = self.num_ch_dec[i]\n            if self.use_skips and i > 0:\n                num_ch_in += self.num_ch_enc[i - 1]\n            num_ch_out = self.num_ch_dec[i]\n            self.convs[(""upconv"", i, 1)] = ConvBlock(num_ch_in, num_ch_out)\n\n        for s in self.scales:\n            self.convs[(""dispconv"", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)\n\n        self.decoder = nn.ModuleList(list(self.convs.values()))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_features):\n        self.outputs = {}\n\n        # decoder\n        x = input_features[-1]\n        for i in range(4, -1, -1):\n            x = self.convs[(""upconv"", i, 0)](x)\n            x = [upsample(x)]\n            if self.use_skips and i > 0:\n                x += [input_features[i - 1]]\n            x = torch.cat(x, 1)\n            x = self.convs[(""upconv"", i, 1)](x)\n            if i in self.scales:\n                self.outputs[(""disp"", i)] = self.sigmoid(self.convs[(""dispconv"", i)](x))\n\n        return self.outputs\n'"
gimp-plugins/monodepth2/networks/pose_cnn.py,1,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport torch\nimport torch.nn as nn\n\n\nclass PoseCNN(nn.Module):\n    def __init__(self, num_input_frames):\n        super(PoseCNN, self).__init__()\n\n        self.num_input_frames = num_input_frames\n\n        self.convs = {}\n        self.convs[0] = nn.Conv2d(3 * num_input_frames, 16, 7, 2, 3)\n        self.convs[1] = nn.Conv2d(16, 32, 5, 2, 2)\n        self.convs[2] = nn.Conv2d(32, 64, 3, 2, 1)\n        self.convs[3] = nn.Conv2d(64, 128, 3, 2, 1)\n        self.convs[4] = nn.Conv2d(128, 256, 3, 2, 1)\n        self.convs[5] = nn.Conv2d(256, 256, 3, 2, 1)\n        self.convs[6] = nn.Conv2d(256, 256, 3, 2, 1)\n\n        self.pose_conv = nn.Conv2d(256, 6 * (num_input_frames - 1), 1)\n\n        self.num_convs = len(self.convs)\n\n        self.relu = nn.ReLU(True)\n\n        self.net = nn.ModuleList(list(self.convs.values()))\n\n    def forward(self, out):\n\n        for i in range(self.num_convs):\n            out = self.convs[i](out)\n            out = self.relu(out)\n\n        out = self.pose_conv(out)\n        out = out.mean(3).mean(2)\n\n        out = 0.01 * out.view(-1, self.num_input_frames - 1, 1, 6)\n\n        axisangle = out[..., :3]\n        translation = out[..., 3:]\n\n        return axisangle, translation\n'"
gimp-plugins/monodepth2/networks/pose_decoder.py,2,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport torch\nimport torch.nn as nn\nfrom collections import OrderedDict\n\n\nclass PoseDecoder(nn.Module):\n    def __init__(self, num_ch_enc, num_input_features, num_frames_to_predict_for=None, stride=1):\n        super(PoseDecoder, self).__init__()\n\n        self.num_ch_enc = num_ch_enc\n        self.num_input_features = num_input_features\n\n        if num_frames_to_predict_for is None:\n            num_frames_to_predict_for = num_input_features - 1\n        self.num_frames_to_predict_for = num_frames_to_predict_for\n\n        self.convs = OrderedDict()\n        self.convs[(""squeeze"")] = nn.Conv2d(self.num_ch_enc[-1], 256, 1)\n        self.convs[(""pose"", 0)] = nn.Conv2d(num_input_features * 256, 256, 3, stride, 1)\n        self.convs[(""pose"", 1)] = nn.Conv2d(256, 256, 3, stride, 1)\n        self.convs[(""pose"", 2)] = nn.Conv2d(256, 6 * num_frames_to_predict_for, 1)\n\n        self.relu = nn.ReLU()\n\n        self.net = nn.ModuleList(list(self.convs.values()))\n\n    def forward(self, input_features):\n        last_features = [f[-1] for f in input_features]\n\n        cat_features = [self.relu(self.convs[""squeeze""](f)) for f in last_features]\n        cat_features = torch.cat(cat_features, 1)\n\n        out = cat_features\n        for i in range(3):\n            out = self.convs[(""pose"", i)](out)\n            if i != 2:\n                out = self.relu(out)\n\n        out = out.mean(3).mean(2)\n\n        out = 0.01 * out.view(-1, self.num_frames_to_predict_for, 1, 6)\n\n        axisangle = out[..., :3]\n        translation = out[..., 3:]\n\n        return axisangle, translation\n'"
gimp-plugins/monodepth2/networks/resnet_encoder.py,3,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.utils.model_zoo as model_zoo\n\n\nclass ResNetMultiImageInput(models.ResNet):\n    """"""Constructs a resnet model with varying number of input images.\n    Adapted from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n    """"""\n    def __init__(self, block, layers, num_classes=1000, num_input_images=1):\n        super(ResNetMultiImageInput, self).__init__(block, layers)\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(\n            num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n\ndef resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n    """"""Constructs a ResNet model.\n    Args:\n        num_layers (int): Number of resnet layers. Must be 18 or 50\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        num_input_images (int): Number of frames stacked as input\n    """"""\n    assert num_layers in [18, 50], ""Can only run with 18 or 50 layer resnet""\n    blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n    block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n    model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n\n    if pretrained:\n        loaded = model_zoo.load_url(models.resnet.model_urls[\'resnet{}\'.format(num_layers)])\n        loaded[\'conv1.weight\'] = torch.cat(\n            [loaded[\'conv1.weight\']] * num_input_images, 1) / num_input_images\n        model.load_state_dict(loaded)\n    return model\n\n\nclass ResnetEncoder(nn.Module):\n    """"""Pytorch module for a resnet encoder\n    """"""\n    def __init__(self, num_layers, pretrained, num_input_images=1):\n        super(ResnetEncoder, self).__init__()\n\n        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n\n        resnets = {18: models.resnet18,\n                   34: models.resnet34,\n                   50: models.resnet50,\n                   101: models.resnet101,\n                   152: models.resnet152}\n\n        if num_layers not in resnets:\n            raise ValueError(""{} is not a valid number of resnet layers"".format(num_layers))\n\n        if num_input_images > 1:\n            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n        else:\n            self.encoder = resnets[num_layers](pretrained)\n\n        if num_layers > 34:\n            self.num_ch_enc[1:] *= 4\n\n    def forward(self, input_image):\n        self.features = []\n        x = (input_image - 0.45) / 0.225\n        x = self.encoder.conv1(x)\n        x = self.encoder.bn1(x)\n        self.features.append(self.encoder.relu(x))\n        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n        self.features.append(self.encoder.layer2(self.features[-1]))\n        self.features.append(self.encoder.layer3(self.features[-1]))\n        self.features.append(self.encoder.layer4(self.features[-1]))\n\n        return self.features\n'"
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/data/__init__.py,0,b''
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/data/aligned_dataset.py,0,"b""## Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport os.path\nfrom data.base_dataset import BaseDataset, get_params, get_transform, normalize\nfrom data.image_folder import make_dataset, make_dataset_test\nfrom PIL import Image\nimport torch\nimport numpy as np\n\nclass AlignedDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot    \n\n        ### input A (label maps)\n        if opt.isTrain or opt.use_encoded_image:\n            dir_A = '_A' if self.opt.label_nc == 0 else '_label'\n            self.dir_A = os.path.join(opt.dataroot, opt.phase + dir_A)\n            self.A_paths = sorted(make_dataset(self.dir_A))\n            self.AR_paths = make_dataset(self.dir_A)\n\n        ### input A inter 1 (label maps)\n        if opt.isTrain or opt.use_encoded_image:\n            dir_A_inter_1 = '_label_inter_1'\n            self.dir_A_inter_1 = os.path.join(opt.dataroot, opt.phase + dir_A_inter_1)\n            self.A_paths_inter_1 = sorted(make_dataset(self.dir_A_inter_1))\n\n        ### input A inter 2 (label maps)\n        if opt.isTrain or opt.use_encoded_image:\n            dir_A_inter_2 = '_label_inter_2'\n            self.dir_A_inter_2 = os.path.join(opt.dataroot, opt.phase + dir_A_inter_2)\n            self.A_paths_inter_2 = sorted(make_dataset(self.dir_A_inter_2))\n\n        ### input A test (label maps)\n        if not (opt.isTrain or opt.use_encoded_image):\n            dir_A = '_A' if self.opt.label_nc == 0 else '_label'\n            self.dir_A = os.path.join(opt.dataroot, opt.phase + dir_A)\n            self.A_paths = sorted(make_dataset_test(self.dir_A))\n            dir_AR = '_AR' if self.opt.label_nc == 0 else '_labelref'\n            self.dir_AR = os.path.join(opt.dataroot, opt.phase + dir_AR)\n            self.AR_paths = sorted(make_dataset_test(self.dir_AR))\n\n        ### input B (real images)\n        dir_B = '_B' if self.opt.label_nc == 0 else '_img'\n        self.dir_B = os.path.join(opt.dataroot, opt.phase + dir_B)  \n        self.B_paths = sorted(make_dataset(self.dir_B))\n        self.BR_paths = sorted(make_dataset(self.dir_B))\n\n        self.dataset_size = len(self.A_paths) \n \n    def __getitem__(self, index):        \n        ### input A (label maps)\n        A_path = self.A_paths[index]\n        AR_path = self.AR_paths[index]\n        A = Image.open(A_path)\n        AR = Image.open(AR_path)\n\n        if self.opt.isTrain:\n            A_path_inter_1 = self.A_paths_inter_1[index]\n            A_path_inter_2 = self.A_paths_inter_2[index]\n            A_inter_1 = Image.open(A_path_inter_1)\n            A_inter_2 = Image.open(A_path_inter_2)\n  \n        params = get_params(self.opt, A.size)\n        if self.opt.label_nc == 0:\n            transform_A = get_transform(self.opt, params)\n            A_tensor = transform_A(A.convert('RGB'))\n            if self.opt.isTrain:\n                A_inter_1_tensor = transform_A(A_inter_1.convert('RGB'))\n                A_inter_2_tensor = transform_A(A_inter_2.convert('RGB'))\n            AR_tensor = transform_A(AR.convert('RGB'))\n        else:\n            transform_A = get_transform(self.opt, params, method=Image.NEAREST, normalize=False)\n            A_tensor = transform_A(A) * 255.0\n            if self.opt.isTrain:\n                A_inter_1_tensor = transform_A(A_inter_1) * 255.0\n                A_inter_2_tensor = transform_A(A_inter_2) * 255.0\n            AR_tensor = transform_A(AR) * 255.0\n        B_tensor = inst_tensor = feat_tensor = 0\n        ### input B (real images)\n        B_path = self.B_paths[index]   \n        BR_path = self.BR_paths[index]   \n        B = Image.open(B_path).convert('RGB')\n        BR = Image.open(BR_path).convert('RGB')\n        transform_B = get_transform(self.opt, params)      \n        B_tensor = transform_B(B)\n        BR_tensor = transform_B(BR)\n\n        if self.opt.isTrain:\n            input_dict = {'inter_label_1': A_inter_1_tensor, 'label': A_tensor, 'inter_label_2': A_inter_2_tensor, 'label_ref': AR_tensor, 'image': B_tensor, 'image_ref': BR_tensor, 'path': A_path, 'path_ref': AR_path}\n        else:\n            input_dict = {'label': A_tensor, 'label_ref': AR_tensor, 'image': B_tensor, 'image_ref': BR_tensor, 'path': A_path, 'path_ref': AR_path}\n\n        return input_dict\n\n    def __len__(self):\n        return len(self.A_paths) // self.opt.batchSize * self.opt.batchSize\n\n    def name(self):\n        return 'AlignedDataset'\n"""
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/data/base_data_loader.py,0,"b'\nclass BaseDataLoader():\n    def __init__(self):\n        pass\n    \n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n\n        \n        \n'"
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/data/base_dataset.py,1,"b""### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport random\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return 'BaseDataset'\n\n    def initialize(self, opt):\n        pass\n\ndef get_params(opt, size):\n    w, h = size\n    new_h = h\n    new_w = w\n    if opt.resize_or_crop == 'resize_and_crop':\n        new_h = new_w = opt.loadSize            \n    elif opt.resize_or_crop == 'scale_width_and_crop':\n        new_w = opt.loadSize\n        new_h = opt.loadSize * h // w\n\n    x = random.randint(0, np.maximum(0, new_w - opt.fineSize))\n    y = random.randint(0, np.maximum(0, new_h - opt.fineSize))\n    \n    #flip = random.random() > 0.5\n    flip = 0\n    return {'crop_pos': (x, y), 'flip': flip}\n\ndef get_transform(opt, params, method=Image.BICUBIC, normalize=True, normalize_mask=False):\n    transform_list = []\n    if 'resize' in opt.resize_or_crop:\n        osize = [opt.loadSize, opt.loadSize]\n        transform_list.append(transforms.Scale(osize, method))   \n    elif 'scale_width' in opt.resize_or_crop:\n        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.loadSize, method)))\n        \n    if 'crop' in opt.resize_or_crop:\n        transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.fineSize)))\n\n    if opt.resize_or_crop == 'none':\n        base = float(2 ** opt.n_downsample_global)\n        if opt.netG == 'local':\n            base *= (2 ** opt.n_local_enhancers)\n        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base, method)))\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n\n    transform_list += [transforms.ToTensor()]\n\n    if normalize:\n        transform_list += [transforms.Normalize((0.5, 0.5, 0.5),\n                                                (0.5, 0.5, 0.5))]\n    if normalize_mask:\n        transform_list += [transforms.Normalize((0, 0, 0),\n                                                (1 / 255., 1 / 255., 1 / 255.))]\n\n    return transforms.Compose(transform_list)\n\ndef normalize():    \n    return transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n\ndef __make_power_2(img, base, method=Image.BICUBIC):\n    ow, oh = img.size        \n    h = int(round(oh / base) * base)\n    w = int(round(ow / base) * base)\n    if (h == oh) and (w == ow):\n        return img\n    return img.resize((w, h), method)\n\ndef __scale_width(img, target_width, method=Image.BICUBIC):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img    \n    w = target_width\n    h = int(target_width * oh / ow)    \n    return img.resize((w, h), method)\n\ndef __crop(img, pos, size):\n    ow, oh = img.size\n    x1, y1 = pos\n    tw = th = size\n    if (ow > tw or oh > th):        \n        return img.crop((x1, y1, x1 + tw, y1 + th))\n    return img\n\ndef __flip(img, flip):\n    if flip:\n        return img.transpose(Image.FLIP_LEFT_RIGHT)\n    return img\n"""
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/data/custom_dataset_data_loader.py,2,"b'import torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\n\n\ndef CreateDataset(opt):\n    dataset = None\n    from data.aligned_dataset import AlignedDataset\n    dataset = AlignedDataset()\n\n    print(""dataset [%s] was created"" % (dataset.name()))\n    dataset.initialize(opt)\n    return dataset\n\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.dataset = CreateDataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batchSize,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.nThreads))\n\n    def load_data(self):\n        return self.dataloader\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n'"
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/data/data_loader.py,0,b'\ndef CreateDataLoader(opt):\n    from data.custom_dataset_data_loader import CustomDatasetDataLoader\n    data_loader = CustomDatasetDataLoader()\n    print(data_loader.name())\n    data_loader.initialize(opt)\n    return data_loader\n'
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/data/image_folder.py,1,"b'###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\nimport torch.utils.data as data\nfrom PIL import Image\nimport os\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\', \'.tiff\'\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    f = dir.split(\'/\')[-1].split(\'_\')[-1]\n    print (dir, f)\n    for i in range(len([name for name in os.listdir(dir) if os.path.isfile(os.path.join(dir, name))])):\n        if f == \'label\' or f == \'1\' or f == \'2\':\n            img = str(i) + \'.png\'\n        else:\n            img = str(i) + \'.jpg\'\n        path = os.path.join(dir, img)\n        #print(path)\n        images.append(path)\n    return images\n\ndef make_dataset_test(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    f = dir.split(\'/\')[-1].split(\'_\')[-1]\n    for i in range(len([name for name in os.listdir(dir) if os.path.isfile(os.path.join(dir, name))])):\n        if f == \'label\' or f == \'labelref\':\n            img = str(i) + \'.png\'\n        else:\n            img = str(i) + \'.jpg\'\n        path = os.path.join(dir, img)\n        #print(path)\n        images.append(path)\n    return images\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/models/__init__.py,0,b''
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/models/base_model.py,7,"b""### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport os\nimport torch\nimport sys\n\nclass BaseModel(torch.nn.Module):\n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # used in test time, no backprop\n    def test(self):\n        pass\n\n    def get_image_paths(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def get_current_errors(self):\n        return {}\n\n    def save(self, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, network_label, epoch_label, gpu_ids):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(network.cpu().state_dict(), save_path)\n        if len(gpu_ids) and torch.cuda.is_available():\n            network.cuda()\n\n    # helper loading function that can be used by subclasses\n    def load_network(self, network, network_label, epoch_label, save_dir=''):        \n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        print (save_filename)\n        if not save_dir:\n            save_dir = self.save_dir\n        save_path = os.path.join(save_dir, save_filename)        \n        if not os.path.isfile(save_path):\n            print('%s not exists yet!' % save_path)\n            if network_label == 'G':\n                raise('Generator must exist!')\n        else:\n            #network.load_state_dict(torch.load(save_path))\n            try:\n                network.load_state_dict(torch.load(save_path))\n            except:   \n                pretrained_dict = torch.load(save_path)                \n                model_dict = network.state_dict()\n                try:\n                    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}                    \n                    network.load_state_dict(pretrained_dict)\n                    if self.opt.verbose:\n                        print('Pretrained network %s has excessive layers; Only loading layers that are used' % network_label)\n                except:\n                    print('Pretrained network %s has fewer layers; The following are not initialized:' % network_label)\n                    for k, v in pretrained_dict.items():                      \n                        if v.size() == model_dict[k].size():\n                            model_dict[k] = v\n\n                    if sys.version_info >= (3,0):\n                        not_initialized = set()\n                    else:\n                        from sets import Set\n                        not_initialized = Set()                    \n\n                    for k, v in model_dict.items():\n                        if k not in pretrained_dict or v.size() != pretrained_dict[k].size():\n                            not_initialized.add(k.split('.')[0])\n                    \n                    print(sorted(not_initialized))\n                    network.load_state_dict(model_dict)                  \n\n    def update_learning_rate():\n        pass\n"""
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/models/models.py,1,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport torch\n\ndef create_model(opt):\n    if opt.model == \'pix2pixHD\':\n        from .pix2pixHD_model import Pix2PixHDModel, InferenceModel\n        if opt.isTrain:\n            model = Pix2PixHDModel()\n        else:\n            model = InferenceModel()\n\n    model.initialize(opt)\n    if opt.verbose:\n        print(""model [%s] was created"" % (model.name()))\n\n    if opt.isTrain and len(opt.gpu_ids):\n        model = torch.nn.DataParallel(model, device_ids=opt.gpu_ids)\n\n    return model\n'"
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/models/networks.py,24,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport torch\nimport torch.nn as nn\nimport functools\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn.functional as F\n\n###############################################################################\n# Functions\n###############################################################################\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv2d\') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find(\'BatchNorm2d\') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\ndef get_norm_layer(norm_type=\'instance\'):\n    if norm_type == \'batch\':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == \'instance\':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n    else:\n        raise NotImplementedError(\'normalization layer [%s] is not found\' % norm_type)\n    return norm_layer\n\ndef define_G(input_nc, output_nc, ngf, netG, n_downsample_global=3, n_blocks_global=9, n_local_enhancers=1, \n             n_blocks_local=3, norm=\'instance\', gpu_ids=[]):    \n    norm_layer = get_norm_layer(norm_type=norm)     \n    if netG == \'global\':    \n        netG = GlobalGenerator(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, norm_layer)       \n    elif netG == \'local\':        \n        netG = LocalEnhancer(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, \n                                  n_local_enhancers, n_blocks_local, norm_layer)\n    else:\n        raise(\'generator not implemented!\')\n    print(netG)\n    # if len(gpu_ids) > 0:\n        # assert(torch.cuda.is_available())   \n        # netG.cuda(gpu_ids[0])\n    netG.apply(weights_init)\n    return netG\n\ndef define_D(input_nc, ndf, n_layers_D, norm=\'instance\', use_sigmoid=False, num_D=1, getIntermFeat=False, gpu_ids=[]):        \n    norm_layer = get_norm_layer(norm_type=norm)   \n    netD = MultiscaleDiscriminator(input_nc, ndf, n_layers_D, norm_layer, use_sigmoid, num_D, getIntermFeat)   \n    print(netD)\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        netD.cuda(gpu_ids[0])\n    netD.apply(weights_init)\n    return netD\n\ndef define_VAE(input_nc, gpu_ids=[]):    \n    netVAE = VAE(19, 32, 32, 1024) \n    print(netVAE)\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())   \n        netVAE.cuda(gpu_ids[0])\n    return netVAE\n\ndef define_B(input_nc, output_nc, ngf, n_downsample_global=3, n_blocks_global=3, norm=\'instance\', gpu_ids=[]):    \n    norm_layer = get_norm_layer(norm_type=norm)     \n    netB = BlendGenerator(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, norm_layer)       \n    print(netB)\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())   \n        netB.cuda(gpu_ids[0])\n    netB.apply(weights_init)\n    return netB\n\ndef print_network(net):\n    if isinstance(net, list):\n        net = net[0]\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(net)\n    print(\'Total number of parameters: %d\' % num_params)\n\n##############################################################################\n# Losses\n##############################################################################\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n                 tensor=torch.FloatTensor):\n        super(GANLoss, self).__init__()\n        self.real_label = target_real_label\n        self.fake_label = target_fake_label\n        self.real_label_var = None\n        self.fake_label_var = None\n        self.Tensor = tensor\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        target_tensor = None\n        if target_is_real:\n            create_label = ((self.real_label_var is None) or\n                            (self.real_label_var.numel() != input.numel()))\n            if create_label:\n                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n                self.real_label_var = Variable(real_tensor, requires_grad=False)\n            target_tensor = self.real_label_var\n        else:\n            create_label = ((self.fake_label_var is None) or\n                            (self.fake_label_var.numel() != input.numel()))\n            if create_label:\n                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n            target_tensor = self.fake_label_var\n        return target_tensor\n\n    def __call__(self, input, target_is_real):\n        if isinstance(input[0], list):\n            loss = 0\n            for input_i in input:\n                pred = input_i[-1]\n                target_tensor = self.get_target_tensor(pred, target_is_real)\n                loss += self.loss(pred, target_tensor)\n            return loss\n        else:            \n            target_tensor = self.get_target_tensor(input[-1], target_is_real)\n            return self.loss(input[-1], target_tensor)\n\nclass VGGLoss(nn.Module):\n    def __init__(self, gpu_ids):\n        super(VGGLoss, self).__init__()        \n        self.vgg = Vgg19().cuda()\n        self.criterion = nn.L1Loss()\n        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]        \n\n    def forward(self, x, y):              \n        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n        loss = 0\n        for i in range(len(x_vgg)):\n            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())        \n        return loss\n\n##############################################################################\n# Generator\n##############################################################################\nclass GlobalGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9, norm_layer=nn.BatchNorm2d, \n                 padding_type=\'reflect\'):\n        assert(n_blocks >= 0)\n        super(GlobalGenerator, self).__init__()        \n        activation = nn.ReLU(True)        \n\n        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), norm_layer(ngf), activation]\n        ### downsample\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n                      norm_layer(ngf * mult * 2), activation]\n\n        ### resnet blocks\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, norm_type=\'adain\', padding_type=padding_type)]\n        ### upsample  \n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n                       norm_layer(int(ngf * mult / 2)), activation]\n        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]        \n        self.model = nn.Sequential(*model)\n  \n        # style encoder\n        self.enc_style = StyleEncoder(5, 3, 16, self.get_num_adain_params(self.model), norm=\'none\', activ=\'relu\', pad_type=\'reflect\')\n        # label encoder\n        self.enc_label = LabelEncoder(5, 19, 16, 64, norm=\'none\', activ=\'relu\', pad_type=\'reflect\')\n\n    def assign_adain_params(self, adain_params, model):\n        # assign the adain_params to the AdaIN layers in model\n        for m in model.modules():\n            if m.__class__.__name__ == ""AdaptiveInstanceNorm2d"":\n                mean = adain_params[:, :m.num_features]\n                std = adain_params[:, m.num_features:2*m.num_features]\n                m.bias = mean.contiguous().view(-1)\n                m.weight = std.contiguous().view(-1)\n                if adain_params.size(1) > 2*m.num_features:\n                    adain_params = adain_params[:, 2*m.num_features:]\n\n    def get_num_adain_params(self, model):\n        # return the number of AdaIN parameters needed by the model\n        num_adain_params = 0\n        for m in model.modules():\n            if m.__class__.__name__ == ""AdaptiveInstanceNorm2d"":\n                num_adain_params += 2*m.num_features\n        return num_adain_params\n\n    def forward(self, input, input_ref, image_ref):\n        fea1, fea2 = self.enc_label(input_ref)\n        adain_params = self.enc_style((image_ref, fea1, fea2))\n        self.assign_adain_params(adain_params, self.model)\n        return self.model(input)         \n\nclass BlendGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=3, norm_layer=nn.BatchNorm2d, \n                 padding_type=\'reflect\'):\n        assert(n_blocks >= 0)\n        super(BlendGenerator, self).__init__()        \n        activation = nn.ReLU(True)        \n\n        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), norm_layer(ngf), activation]\n        ### downsample\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n                      norm_layer(ngf * mult * 2), activation]\n\n        ### resnet blocks\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, norm_type=\'in\', padding_type=padding_type)]\n        \n        ### upsample         \n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n                       norm_layer(int(ngf * mult / 2)), activation]\n        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Sigmoid()]        \n        self.model = nn.Sequential(*model)\n    \n    def forward(self, input1, input2):\n        m = self.model(torch.cat([input1, input2], 1))\n        return input1 * m + input2 * (1-m), m            \n\n# Define the Multiscale Discriminator.\nclass MultiscaleDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n                 use_sigmoid=False, num_D=3, getIntermFeat=False):\n        super(MultiscaleDiscriminator, self).__init__()\n        self.num_D = num_D\n        self.n_layers = n_layers\n        self.getIntermFeat = getIntermFeat\n     \n        for i in range(num_D):\n            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\n            if getIntermFeat:                                \n                for j in range(n_layers+2):\n                    setattr(self, \'scale\'+str(i)+\'_layer\'+str(j), getattr(netD, \'model\'+str(j)))                                   \n            else:\n                setattr(self, \'layer\'+str(i), netD.model)\n\n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n\n    def singleD_forward(self, model, input):\n        if self.getIntermFeat:\n            result = [input]\n            for i in range(len(model)):\n                result.append(model[i](result[-1]))\n            return result[1:]\n        else:\n            return [model(input)]\n\n    def forward(self, input):        \n        num_D = self.num_D\n        result = []\n        input_downsampled = input\n        for i in range(num_D):\n            if self.getIntermFeat:\n                model = [getattr(self, \'scale\'+str(num_D-1-i)+\'_layer\'+str(j)) for j in range(self.n_layers+2)]\n            else:\n                model = getattr(self, \'layer\'+str(num_D-1-i))\n            result.append(self.singleD_forward(model, input_downsampled))\n            if i != (num_D-1):\n                input_downsampled = self.downsample(input_downsampled)\n        return result\n        \n# Define the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\n        super(NLayerDiscriminator, self).__init__()\n        self.getIntermFeat = getIntermFeat\n        self.n_layers = n_layers\n\n        kw = 4\n        padw = int(np.ceil((kw-1.0)/2))\n        sequence = [[nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]]\n\n        nf = ndf\n        for n in range(1, n_layers):\n            nf_prev = nf\n            nf = min(nf * 2, 512)\n            sequence += [[\n                nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw),\n                norm_layer(nf), nn.LeakyReLU(0.2, True)\n            ]]\n\n        nf_prev = nf\n        nf = min(nf * 2, 512)\n        sequence += [[\n            nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\n            norm_layer(nf),\n            nn.LeakyReLU(0.2, True)\n        ]]\n\n        sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\n\n        if use_sigmoid:\n            sequence += [[nn.Sigmoid()]]\n\n        if getIntermFeat:\n            for n in range(len(sequence)):\n                setattr(self, \'model\'+str(n), nn.Sequential(*sequence[n]))\n        else:\n            sequence_stream = []\n            for n in range(len(sequence)):\n                sequence_stream += sequence[n]\n            self.model = nn.Sequential(*sequence_stream)\n\n    def forward(self, input):\n        if self.getIntermFeat:\n            res = [input]\n            for n in range(self.n_layers+2):\n                model = getattr(self, \'model\'+str(n))\n                res.append(model(res[-1]))\n            return res[1:]\n        else:\n            return self.model(input)        \n\nfrom torchvision import models\nclass Vgg19(torch.nn.Module):\n    def __init__(self, requires_grad=False):\n        super(Vgg19, self).__init__()\n        vgg_pretrained_features = models.vgg19(pretrained=True).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for x in range(2):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(2, 7):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(7, 12):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(12, 21):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(21, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h_relu1 = self.slice1(X)\n        h_relu2 = self.slice2(h_relu1)        \n        h_relu3 = self.slice3(h_relu2)        \n        h_relu4 = self.slice4(h_relu3)        \n        h_relu5 = self.slice5(h_relu4)                \n        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n        return out\n\n# Define the MaskVAE\nclass VAE(nn.Module):\n    def __init__(self, nc, ngf, ndf, latent_variable_size):\n        super(VAE, self).__init__()\n        #self.cuda = True\n        self.nc = nc\n        self.ngf = ngf\n        self.ndf = ndf\n        self.latent_variable_size = latent_variable_size\n\n        # encoder\n        self.e1 = nn.Conv2d(nc, ndf, 4, 2, 1)\n        self.bn1 = nn.BatchNorm2d(ndf)\n\n        self.e2 = nn.Conv2d(ndf, ndf*2, 4, 2, 1)\n        self.bn2 = nn.BatchNorm2d(ndf*2)\n\n        self.e3 = nn.Conv2d(ndf*2, ndf*4, 4, 2, 1)\n        self.bn3 = nn.BatchNorm2d(ndf*4)\n\n        self.e4 = nn.Conv2d(ndf*4, ndf*8, 4, 2, 1)\n        self.bn4 = nn.BatchNorm2d(ndf*8)\n\n        self.e5 = nn.Conv2d(ndf*8, ndf*16, 4, 2, 1)\n        self.bn5 = nn.BatchNorm2d(ndf*16)\n\n        self.e6 = nn.Conv2d(ndf*16, ndf*32, 4, 2, 1)\n        self.bn6 = nn.BatchNorm2d(ndf*32)\n\n        self.e7 = nn.Conv2d(ndf*32, ndf*64, 4, 2, 1)\n        self.bn7 = nn.BatchNorm2d(ndf*64)\n\n        self.fc1 = nn.Linear(ndf*64*4*4, latent_variable_size)\n        self.fc2 = nn.Linear(ndf*64*4*4, latent_variable_size)\n\n        # decoder\n        self.d1 = nn.Linear(latent_variable_size, ngf*64*4*4)\n\n        self.up1 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.pd1 = nn.ReplicationPad2d(1)\n        self.d2 = nn.Conv2d(ngf*64, ngf*32, 3, 1)\n        self.bn8 = nn.BatchNorm2d(ngf*32, 1.e-3)\n\n        self.up2 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.pd2 = nn.ReplicationPad2d(1)\n        self.d3 = nn.Conv2d(ngf*32, ngf*16, 3, 1)\n        self.bn9 = nn.BatchNorm2d(ngf*16, 1.e-3)\n\n        self.up3 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.pd3 = nn.ReplicationPad2d(1)\n        self.d4 = nn.Conv2d(ngf*16, ngf*8, 3, 1)\n        self.bn10 = nn.BatchNorm2d(ngf*8, 1.e-3)\n\n        self.up4 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.pd4 = nn.ReplicationPad2d(1)\n        self.d5 = nn.Conv2d(ngf*8, ngf*4, 3, 1)\n        self.bn11 = nn.BatchNorm2d(ngf*4, 1.e-3)\n\n        self.up5 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.pd5 = nn.ReplicationPad2d(1)\n        self.d6 = nn.Conv2d(ngf*4, ngf*2, 3, 1)\n        self.bn12 = nn.BatchNorm2d(ngf*2, 1.e-3)\n\n        self.up6 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.pd6 = nn.ReplicationPad2d(1)\n        self.d7 = nn.Conv2d(ngf*2, ngf, 3, 1)\n        self.bn13 = nn.BatchNorm2d(ngf, 1.e-3)\n\n        self.up7 = nn.UpsamplingNearest2d(scale_factor=2)\n        self.pd7 = nn.ReplicationPad2d(1)\n        self.d8 = nn.Conv2d(ngf, nc, 3, 1)\n\n        self.leakyrelu = nn.LeakyReLU(0.2)\n        self.relu = nn.ReLU()\n        #self.sigmoid = nn.Sigmoid()\n        self.maxpool = nn.MaxPool2d((2, 2), (2, 2))\n\n    def encode(self, x):\n        h1 = self.leakyrelu(self.bn1(self.e1(x)))\n        h2 = self.leakyrelu(self.bn2(self.e2(h1)))\n        h3 = self.leakyrelu(self.bn3(self.e3(h2)))\n        h4 = self.leakyrelu(self.bn4(self.e4(h3)))\n        h5 = self.leakyrelu(self.bn5(self.e5(h4)))\n        h6 = self.leakyrelu(self.bn6(self.e6(h5))) \n        h7 = self.leakyrelu(self.bn7(self.e7(h6)))\n        h7 = h7.view(-1, self.ndf*64*4*4)\n        return self.fc1(h7), self.fc2(h7)\n\n    def reparametrize(self, mu, logvar):\n        std = logvar.mul(0.5).exp_()\n        #if self.cuda:\n        eps = torch.cuda.FloatTensor(std.size()).normal_()\n        #else:\n        #    eps = torch.FloatTensor(std.size()).normal_()\n        eps = Variable(eps)\n        return eps.mul(std).add_(mu)\n\n    def decode(self, z):\n        h1 = self.relu(self.d1(z))\n        h1 = h1.view(-1, self.ngf*64, 4, 4)\n        h2 = self.leakyrelu(self.bn8(self.d2(self.pd1(self.up1(h1)))))\n        h3 = self.leakyrelu(self.bn9(self.d3(self.pd2(self.up2(h2)))))\n        h4 = self.leakyrelu(self.bn10(self.d4(self.pd3(self.up3(h3)))))\n        h5 = self.leakyrelu(self.bn11(self.d5(self.pd4(self.up4(h4)))))\n        h6 = self.leakyrelu(self.bn12(self.d6(self.pd5(self.up5(h5)))))\n        h7 = self.leakyrelu(self.bn13(self.d7(self.pd6(self.up6(h6)))))\n        return self.d8(self.pd7(self.up7(h7)))\n\n    def get_latent_var(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparametrize(mu, logvar)\n        return z, mu, logvar.mul(0.5).exp_()\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparametrize(mu, logvar)\n        res = self.decode(z)\n        \n        return res, x, mu, logvar\n\n# style encode part\nclass StyleEncoder(nn.Module):\n    def __init__(self, n_downsample, input_dim, dim, style_dim, norm, activ, pad_type):\n        super(StyleEncoder, self).__init__()\n        self.model = []\n        self.model_middle = []\n        self.model_last = []\n        self.model += [ConvBlock(input_dim, dim, 7, 1, 3, norm=norm, activation=activ, pad_type=pad_type)]\n        for i in range(2):\n            self.model += [ConvBlock(dim, 2 * dim, 4, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n            dim *= 2\n        for i in range(n_downsample - 2):\n            self.model_middle += [ConvBlock(dim, dim, 4, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n        self.model_last += [nn.AdaptiveAvgPool2d(1)] # global average pooling\n        self.model_last += [nn.Conv2d(dim, style_dim, 1, 1, 0)]\n        \n        self.model = nn.Sequential(*self.model)\n        self.model_middle = nn.Sequential(*self.model_middle)\n        self.model_last = nn.Sequential(*self.model_last)\n        \n        self.output_dim = dim\n        \n        self.sft1 = SFTLayer()\n        self.sft2 = SFTLayer()\n\n    def forward(self, x):\n        fea = self.model(x[0])\n        fea = self.sft1((fea, x[1]))\n        fea = self.model_middle(fea)\n        fea = self.sft2((fea, x[2])) \n        return self.model_last(fea)\n\n# label encode part\nclass LabelEncoder(nn.Module):\n    def __init__(self, n_downsample, input_dim, dim, style_dim, norm, activ, pad_type):\n        super(LabelEncoder, self).__init__()\n        self.model = []\n        self.model_last = [nn.ReLU()]\n        self.model += [ConvBlock(input_dim, dim, 7, 1, 3, norm=norm, activation=activ, pad_type=pad_type)]\n        self.model += [ConvBlock(dim, 2 * dim, 4, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n        dim *= 2\n        self.model += [ConvBlock(dim, 2 * dim, 4, 2, 1, norm=norm, activation=\'none\', pad_type=pad_type)]\n        dim *= 2 \n        for i in range(n_downsample - 3):\n            self.model_last += [ConvBlock(dim, dim, 4, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n        self.model_last += [ConvBlock(dim, dim, 4, 2, 1, norm=norm, activation=\'none\', pad_type=pad_type)]\n        self.model = nn.Sequential(*self.model)\n        self.model_last = nn.Sequential(*self.model_last)\n        self.output_dim = dim\n\n    def forward(self, x):\n        fea = self.model(x)\n        return fea, self.model_last(fea)\n\n# Define the basic block\nclass ConvBlock(nn.Module):\n    def __init__(self, input_dim ,output_dim, kernel_size, stride,\n                 padding=0, norm=\'none\', activation=\'relu\', pad_type=\'zero\'):\n        super(ConvBlock, self).__init__()\n        self.use_bias = True\n        # initialize padding\n        if pad_type == \'reflect\':\n            self.pad = nn.ReflectionPad2d(padding)\n        elif pad_type == \'replicate\':\n            self.pad = nn.ReplicationPad2d(padding)\n        elif pad_type == \'zero\':\n            self.pad = nn.ZeroPad2d(padding)\n        else:\n            assert 0, ""Unsupported padding type: {}"".format(pad_type)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == \'bn\':\n            self.norm = nn.BatchNorm2d(norm_dim)\n        elif norm == \'in\':\n            #self.norm = nn.InstanceNorm2d(norm_dim, track_running_stats=True)\n            self.norm = nn.InstanceNorm2d(norm_dim)\n        elif norm == \'ln\':\n            self.norm = LayerNorm(norm_dim)\n        elif norm == \'adain\':\n            self.norm = AdaptiveInstanceNorm2d(norm_dim)\n        elif norm == \'none\' or norm == \'sn\':\n            self.norm = None\n        else:\n            assert 0, ""Unsupported normalization: {}"".format(norm)\n\n        # initialize activation\n        if activation == \'relu\':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == \'lrelu\':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == \'prelu\':\n            self.activation = nn.PReLU()\n        elif activation == \'selu\':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == \'tanh\':\n            self.activation = nn.Tanh()\n        elif activation == \'none\':\n            self.activation = None\n        else:\n            assert 0, ""Unsupported activation: {}"".format(activation)\n\n        # initialize convolution\n        if norm == \'sn\':\n            self.conv = SpectralNorm(nn.Conv2d(input_dim, output_dim, kernel_size, stride, bias=self.use_bias))\n        else:\n            self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, bias=self.use_bias)\n\n    def forward(self, x):\n        x = self.conv(self.pad(x))\n        if self.norm:\n            x = self.norm(x)\n        if self.activation:\n            x = self.activation(x)\n        return x\n\nclass LinearBlock(nn.Module):\n    def __init__(self, input_dim, output_dim, norm=\'none\', activation=\'relu\'):\n        super(LinearBlock, self).__init__()\n        use_bias = True\n        # initialize fully connected layer\n        if norm == \'sn\':\n            self.fc = SpectralNorm(nn.Linear(input_dim, output_dim, bias=use_bias))\n        else:\n            self.fc = nn.Linear(input_dim, output_dim, bias=use_bias)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == \'bn\':\n            self.norm = nn.BatchNorm1d(norm_dim)\n        elif norm == \'in\':\n            self.norm = nn.InstanceNorm1d(norm_dim)\n        elif norm == \'ln\':\n            self.norm = LayerNorm(norm_dim)\n        elif norm == \'none\' or norm == \'sn\':\n            self.norm = None\n        else:\n            assert 0, ""Unsupported normalization: {}"".format(norm)\n\n        # initialize activation\n        if activation == \'relu\':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == \'lrelu\':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == \'prelu\':\n            self.activation = nn.PReLU()\n        elif activation == \'selu\':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == \'tanh\':\n            self.activation = nn.Tanh()\n        elif activation == \'none\':\n            self.activation = None\n        else:\n            assert 0, ""Unsupported activation: {}"".format(activation)\n\n    def forward(self, x):\n        out = self.fc(x)\n        if self.norm:\n            out = self.norm(out)\n        if self.activation:\n            out = self.activation(out)\n        return out\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, norm_type, padding_type, use_dropout=False):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, norm_type, padding_type, use_dropout)\n\n    def build_conv_block(self, dim, norm_type, padding_type, use_dropout):\n        conv_block = []\n        conv_block += [ConvBlock(dim ,dim, 3, 1, 1, norm=norm_type, activation=\'relu\', pad_type=padding_type)]\n        conv_block += [ConvBlock(dim ,dim, 3, 1, 1, norm=norm_type, activation=\'none\', pad_type=padding_type)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\nclass SFTLayer(nn.Module):\n    def __init__(self):\n        super(SFTLayer, self).__init__()\n        self.SFT_scale_conv1 = nn.Conv2d(64, 64, 1)\n        self.SFT_scale_conv2 = nn.Conv2d(64, 64, 1) \n        self.SFT_shift_conv1 = nn.Conv2d(64, 64, 1)\n        self.SFT_shift_conv2 = nn.Conv2d(64, 64, 1)\n\n    def forward(self, x):\n        scale = self.SFT_scale_conv2(F.leaky_relu(self.SFT_scale_conv1(x[1]), 0.1, inplace=True))\n        shift = self.SFT_shift_conv2(F.leaky_relu(self.SFT_shift_conv1(x[1]), 0.1, inplace=True))\n        return x[0] * scale + shift\n\nclass ConvBlock_SFT(nn.Module):\n    def __init__(self, dim, norm_type, padding_type, use_dropout=False):\n        super(ResnetBlock_SFT, self).__init__()        \n        self.sft1 = SFTLayer()\n        self.conv1 = ConvBlock(dim ,dim, 4, 2, 1, norm=norm_type, activation=\'none\', pad_type=padding_type)\n   \n    def forward(self, x):\n        fea = self.sft1((x[0], x[1]))        \n        fea = F.relu(self.conv1(fea), inplace=True)\n        return (x[0] + fea, x[1])\n\nclass ConvBlock_SFT_last(nn.Module):\n    def __init__(self, dim, norm_type, padding_type, use_dropout=False):\n        super(ResnetBlock_SFT_last, self).__init__()\n        self.sft1 = SFTLayer()\n        self.conv1 = ConvBlock(dim ,dim, 4, 2, 1, norm=norm_type, activation=\'none\', pad_type=padding_type)\n\n    def forward(self, x):\n        fea = self.sft1((x[0], x[1]))        \n        fea = F.relu(self.conv1(fea), inplace=True)\n        return x[0] + fea\n\n# Definition of normalization layer\nclass AdaptiveInstanceNorm2d(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(AdaptiveInstanceNorm2d, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        # weight and bias are dynamically assigned\n        self.weight = None\n        self.bias = None\n        # just dummy buffers, not used\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n\n    def forward(self, x):\n        assert self.weight is not None and self.bias is not None, ""Please assign weight and bias before calling AdaIN!""\n        b, c = x.size(0), x.size(1)\n        running_mean = self.running_mean.repeat(b)\n        running_var = self.running_var.repeat(b)\n\n        # Apply instance norm\n        x_reshaped = x.contiguous().view(1, b * c, *x.size()[2:])\n\n        out = F.batch_norm(\n            x_reshaped, running_mean, running_var, self.weight, self.bias,\n            True, self.momentum, self.eps)\n\n        return out.view(b, c, *x.size()[2:])\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' + str(self.num_features) + \')\'\n\nclass LayerNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, affine=True):\n        super(LayerNorm, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n\n        if self.affine:\n            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n            self.beta = nn.Parameter(torch.zeros(num_features))\n\n    def forward(self, x):\n        shape = [-1] + [1] * (x.dim() - 1)\n        # print(x.size())\n        if x.size(0) == 1:\n            # These two lines run much faster in pytorch 0.4 than the two lines listed below.\n            mean = x.view(-1).mean().view(*shape)\n            std = x.view(-1).std().view(*shape)\n        else:\n            mean = x.view(x.size(0), -1).mean(1).view(*shape)\n            std = x.view(x.size(0), -1).std(1).view(*shape)\n\n        x = (x - mean) / (std + self.eps)\n\n        if self.affine:\n            shape = [1, -1] + [1] * (x.dim() - 2)\n            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n        return x\n\ndef l2normalize(v, eps=1e-12):\n    return v / (v.norm() + eps)\n\nclass SpectralNorm(nn.Module):\n    """"""\n    Based on the paper ""Spectral Normalization for Generative Adversarial Networks"" by Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida\n    and the Pytorch implementation https://github.com/christiancosgrove/pytorch-spectral-normalization-gan\n    """"""\n    def __init__(self, module, name=\'weight\', power_iterations=1):\n        super(SpectralNorm, self).__init__()\n        self.module = module\n        self.name = name\n        self.power_iterations = power_iterations\n        if not self._made_params():\n            self._make_params()\n\n    def _update_u_v(self):\n        u = getattr(self.module, self.name + ""_u"")\n        v = getattr(self.module, self.name + ""_v"")\n        w = getattr(self.module, self.name + ""_bar"")\n\n        height = w.data.shape[0]\n        for _ in range(self.power_iterations):\n            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n\n        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n        sigma = u.dot(w.view(height, -1).mv(v))\n        setattr(self.module, self.name, w / sigma.expand_as(w))\n\n    def _made_params(self):\n        try:\n            u = getattr(self.module, self.name + ""_u"")\n            v = getattr(self.module, self.name + ""_v"")\n            w = getattr(self.module, self.name + ""_bar"")\n            return True\n        except AttributeError:\n            return False\n\n\n    def _make_params(self):\n        w = getattr(self.module, self.name)\n\n        height = w.data.shape[0]\n        width = w.view(height, -1).data.shape[1]\n\n        u = nn.Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n        v = nn.Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n        u.data = l2normalize(u.data)\n        v.data = l2normalize(v.data)\n        w_bar = nn.Parameter(w.data)\n\n        del self.module._parameters[self.name]\n\n        self.module.register_parameter(self.name + ""_u"", u)\n        self.module.register_parameter(self.name + ""_v"", v)\n        self.module.register_parameter(self.name + ""_bar"", w_bar)\n\n    def forward(self, *args):\n        self._update_u_v()\n        return self.module.forward(*args)\n'"
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/models/pix2pixHD_model.py,34,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport numpy as np\nimport torch\nimport os\nfrom torch.autograd import Variable\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\n\ndef generate_discrete_label(inputs, label_nc):\n    pred_batch = []\n    size = inputs.size()\n    for input in inputs:\n        input = input.view(1, label_nc, size[2], size[3])\n        pred = np.squeeze(input.data.max(1)[1].cpu().numpy(), axis=0)\n        pred_batch.append(pred)\n\n    pred_batch = np.array(pred_batch)\n    pred_batch = torch.from_numpy(pred_batch)\n    label_map = []\n    for p in pred_batch:\n        p = p.view(1, 512, 512)\n        label_map.append(p)\n    label_map = torch.stack(label_map, 0)\n    size = label_map.size()\n    oneHot_size = (size[0], label_nc, size[2], size[3])\n    if torch.cuda.is_available():\n        input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n        input_label = input_label.scatter_(1, label_map.data.long().cuda(), 1.0)\n    else:\n        input_label = torch.FloatTensor(torch.Size(oneHot_size)).zero_()\n        input_label = input_label.scatter_(1, label_map.data.long(), 1.0)\n                      \n    return input_label\n\nclass Pix2PixHDModel(BaseModel):\n    def name(self):\n        return \'Pix2PixHDModel\'\n    \n    def init_loss_filter(self, use_gan_feat_loss, use_vgg_loss):\n        flags = (True, use_gan_feat_loss, use_vgg_loss, True, use_gan_feat_loss, use_vgg_loss, True, True, True, True)\n        def loss_filter(g_gan, g_gan_feat, g_vgg, gb_gan, gb_gan_feat, gb_vgg, d_real, d_fake, d_blend):\n            return [l for (l,f) in zip((g_gan,g_gan_feat,g_vgg,gb_gan,gb_gan_feat,gb_vgg,d_real,d_fake,d_blend),flags) if f]\n        return loss_filter\n    \n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        if opt.resize_or_crop != \'none\' or not opt.isTrain: # when training at full res this causes OOM\n            torch.backends.cudnn.benchmark = True\n        self.isTrain = opt.isTrain\n        input_nc = opt.label_nc if opt.label_nc != 0 else opt.input_nc\n\n        ##### define networks        \n        # Generator network\n        netG_input_nc = input_nc        \n        # Main Generator\n        self.netG = networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n                                      opt.n_blocks_local, opt.norm, gpu_ids=self.gpu_ids)        \n\n        # Discriminator network\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            netD_input_nc = input_nc + opt.output_nc\n            netB_input_nc = opt.output_nc * 2\n            self.netD = networks.define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt.norm, use_sigmoid, \n                                          opt.num_D, not opt.no_ganFeat_loss, gpu_ids=self.gpu_ids)\n            self.netB = networks.define_B(netB_input_nc, opt.output_nc, 32, 3, 3, opt.norm, gpu_ids=self.gpu_ids)        \n            \n        if self.opt.verbose:\n                print(\'---------- Networks initialized -------------\')\n\n        # load networks\n        if not self.isTrain or opt.continue_train or opt.load_pretrain:\n            pretrained_path = \'\' if not self.isTrain else opt.load_pretrain\n            print (pretrained_path)\n            self.load_network(self.netG, \'G\', opt.which_epoch, pretrained_path)            \n            if self.isTrain:\n                self.load_network(self.netB, \'B\', opt.which_epoch, pretrained_path)  \n                self.load_network(self.netD, \'D\', opt.which_epoch, pretrained_path)  \n        \n        # set loss functions and optimizers\n        if self.isTrain:\n            if opt.pool_size > 0 and (len(self.gpu_ids)) > 1:\n                raise NotImplementedError(""Fake Pool Not Implemented for MultiGPU"")\n            self.fake_pool = ImagePool(opt.pool_size)\n            self.old_lr = opt.lr\n\n            # define loss functions\n            self.loss_filter = self.init_loss_filter(not opt.no_ganFeat_loss, not opt.no_vgg_loss)\n            \n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)   \n            self.criterionFeat = torch.nn.L1Loss()\n            if not opt.no_vgg_loss:             \n                self.criterionVGG = networks.VGGLoss(self.gpu_ids)\n                \n            # Names so we can breakout loss\n            self.loss_names = self.loss_filter(\'G_GAN\',\'G_GAN_Feat\',\'G_VGG\',\'GB_GAN\',\'GB_GAN_Feat\',\'GB_VGG\',\'D_real\',\'D_fake\',\'D_blend\')\n            # initialize optimizers\n            # optimizer G\n            if opt.niter_fix_global > 0:                \n                import sys\n                if sys.version_info >= (3,0):\n                    finetune_list = set()\n                else:\n                    from sets import Set\n                    finetune_list = Set()\n\n                params_dict = dict(self.netG.named_parameters())\n                params = []\n                for key, value in params_dict.items():       \n                    if key.startswith(\'model\' + str(opt.n_local_enhancers)):                    \n                        params += [value]\n                        finetune_list.add(key.split(\'.\')[0])  \n                print(\'------------- Only training the local enhancer network (for %d epochs) ------------\' % opt.niter_fix_global)\n                print(\'The layers that are finetuned are \', sorted(finetune_list))                         \n            else:\n                params = list(self.netG.parameters())\n            \n            self.optimizer_G = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))                            \n\n            # optimizer D                        \n            params = list(self.netD.parameters())    \n            self.optimizer_D = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))\n   \n            # optimizer G + B                        \n            params = list(self.netG.parameters()) + list(self.netB.parameters())     \n            self.optimizer_GB = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))\n    \n    def encode_input(self, inter_label_map_1, label_map, inter_label_map_2, real_image, label_map_ref, real_image_ref, infer=False):\n        \n        if self.opt.label_nc == 0:\n            if torch.cuda.is_available():\n                input_label = label_map.data.cuda()\n                inter_label_1 = inter_label_map_1.data.cuda()\n                inter_label_2 = inter_label_map_2.data.cuda() \n                input_label_ref = label_map_ref.data.cuda()\n            else:\n                input_label = label_map.data\n                inter_label_1 = inter_label_map_1.data\n                inter_label_2 = inter_label_map_2.data \n                input_label_ref = label_map_ref.data\n                \n        else:\n            # create one-hot vector for label map \n            size = label_map.size()\n            oneHot_size = (size[0], self.opt.label_nc, size[2], size[3])\n            if torch.cuda.is_available():\n                input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n                input_label = input_label.scatter_(1, label_map.data.long().cuda(), 1.0)\n                inter_label_1 = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n                inter_label_1 = inter_label_1.scatter_(1, inter_label_map_1.data.long().cuda(), 1.0)\n                inter_label_2 = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n                inter_label_2 = inter_label_2.scatter_(1, inter_label_map_2.data.long().cuda(), 1.0)\n                input_label_ref = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n                input_label_ref = input_label_ref.scatter_(1, label_map_ref.data.long().cuda(), 1.0) \n            else:\n                input_label = torch.FloatTensor(torch.Size(oneHot_size)).zero_()\n                input_label = input_label.scatter_(1, label_map.data.long(), 1.0)\n                inter_label_1 = torch.FloatTensor(torch.Size(oneHot_size)).zero_()\n                inter_label_1 = inter_label_1.scatter_(1, inter_label_map_1.data.long(), 1.0)\n                inter_label_2 = torch.FloatTensor(torch.Size(oneHot_size)).zero_()\n                inter_label_2 = inter_label_2.scatter_(1, inter_label_map_2.data.long(), 1.0)\n                input_label_ref = torch.FloatTensor(torch.Size(oneHot_size)).zero_()\n                input_label_ref = input_label_ref.scatter_(1, label_map_ref.data.long(), 1.0) \n                    \n            if self.opt.data_type == 16:\n                input_label = input_label.half()\n                inter_label_1 = inter_label_1.half()\n                inter_label_2 = inter_label_2.half() \n                input_label_ref = input_label_ref.half()\n \n        input_label = Variable(input_label, volatile=infer)\n        inter_label_1 = Variable(inter_label_1, volatile=infer)\n        inter_label_2 = Variable(inter_label_2, volatile=infer)\n        input_label_ref = Variable(input_label_ref, volatile=infer)   \n        if torch.cuda.is_available():     \n            real_image = Variable(real_image.data.cuda()) \n            real_image_ref = Variable(real_image_ref.data.cuda())\n        else:\n            real_image = Variable(real_image.data) \n            real_image_ref = Variable(real_image_ref.data)\n\n        return inter_label_1, input_label, inter_label_2, real_image, input_label_ref, real_image_ref\n\n    def encode_input_test(self, label_map, label_map_ref, real_image_ref, infer=False):\n        \n        if self.opt.label_nc == 0:\n            if torch.cuda.is_available(): \n                input_label = label_map.data.cuda()\n                input_label_ref = label_map_ref.data.cuda() \n            else:\n                input_label = label_map.data\n                input_label_ref = label_map_ref.data\n\n        else:\n            # create one-hot vector for label map \n            size = label_map.size()\n            oneHot_size = (size[0], self.opt.label_nc, size[2], size[3])\n            if torch.cuda.is_available(): \n                input_label = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n                input_label = input_label.scatter_(1, label_map.data.long().cuda(), 1.0)\n                input_label_ref = torch.cuda.FloatTensor(torch.Size(oneHot_size)).zero_()\n                input_label_ref = input_label_ref.scatter_(1, label_map_ref.data.long().cuda(), 1.0)\n                real_image_ref = Variable(real_image_ref.data.cuda()) \n         \n            else:\n                input_label = torch.FloatTensor(torch.Size(oneHot_size)).zero_()\n                input_label = input_label.scatter_(1, label_map.data.long(), 1.0)\n                input_label_ref = torch.FloatTensor(torch.Size(oneHot_size)).zero_()\n                input_label_ref = input_label_ref.scatter_(1, label_map_ref.data.long(), 1.0) \n                real_image_ref = Variable(real_image_ref.data) \n        \n                \n            if self.opt.data_type == 16:\n                input_label = input_label.half()\n                input_label_ref = input_label_ref.half()\n \n        input_label = Variable(input_label, volatile=infer)\n        input_label_ref = Variable(input_label_ref, volatile=infer) \n        \n        return input_label, input_label_ref, real_image_ref\n\n    def discriminate(self, input_label, test_image, use_pool=False):\n        input_concat = torch.cat((input_label, test_image.detach()), dim=1)\n        if use_pool:            \n            fake_query = self.fake_pool.query(input_concat)\n            return self.netD.forward(fake_query)\n        else:\n            return self.netD.forward(input_concat)   \n\n    def forward(self, inter_label_1, label, inter_label_2, image, label_ref, image_ref, infer=False):\n\n        # Encode Inputs\n        inter_label_1, input_label, inter_label_2, real_image, input_label_ref, real_image_ref = self.encode_input(inter_label_1, label, inter_label_2, image, label_ref, image_ref)\n        \n        fake_inter_1 = self.netG.forward(inter_label_1, input_label, real_image)\n        fake_image = self.netG.forward(input_label, input_label, real_image)\n        fake_inter_2 = self.netG.forward(inter_label_2, input_label, real_image)\n\n        blend_image, alpha = self.netB.forward(fake_inter_1, fake_inter_2)\n\n        # Fake Detection and Loss\n        pred_fake_pool = self.discriminate(input_label, fake_image, use_pool=True)\n        loss_D_fake = self.criterionGAN(pred_fake_pool, False)        \n        pred_blend_pool = self.discriminate(input_label, blend_image, use_pool=True)\n        loss_D_blend = self.criterionGAN(pred_blend_pool, False)\n\n        # Real Detection and Loss        \n        pred_real = self.discriminate(input_label, real_image)\n        loss_D_real = self.criterionGAN(pred_real, True)\n\n        # GAN loss (Fake Passability Loss)        \n        pred_fake = self.netD.forward(torch.cat((input_label, fake_image), dim=1))        \n        loss_G_GAN = self.criterionGAN(pred_fake, True)               \n        pred_blend = self.netD.forward(torch.cat((input_label, blend_image), dim=1))        \n        loss_GB_GAN = self.criterionGAN(pred_blend, True)               \n\n        # GAN feature matching loss\n        loss_G_GAN_Feat = 0\n        loss_GB_GAN_Feat = 0 \n        if not self.opt.no_ganFeat_loss:\n            feat_weights = 4.0 / (self.opt.n_layers_D + 1)\n            D_weights = 1.0 / self.opt.num_D\n            for i in range(self.opt.num_D):\n                for j in range(len(pred_fake[i])-1):\n                    loss_G_GAN_Feat += D_weights * feat_weights * \\\n                        self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * self.opt.lambda_feat\n                    loss_GB_GAN_Feat += D_weights * feat_weights * \\\n                        self.criterionFeat(pred_blend[i][j], pred_real[i][j].detach()) * self.opt.lambda_feat\n    \n        # VGG feature matching loss\n        loss_G_VGG = 0\n        loss_GB_VGG = 0\n        if not self.opt.no_vgg_loss:\n            loss_G_VGG += self.criterionVGG(fake_image, real_image) * self.opt.lambda_feat\n            loss_GB_VGG += self.criterionVGG(blend_image, real_image) * self.opt.lambda_feat\n\n        # Only return the fake_B image if necessary to save BW\n        return [ self.loss_filter( loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_GB_GAN, loss_GB_GAN_Feat, loss_GB_VGG, loss_D_real, loss_D_fake, loss_D_blend ), None if not infer else fake_inter_1, fake_image, fake_inter_2, blend_image, alpha, real_image, inter_label_1, input_label, inter_label_2 ]\n\n    def inference(self, label, label_ref, image_ref):\n\n        # Encode Inputs        \n        image_ref = Variable(image_ref)\n        input_label, input_label_ref, real_image_ref = self.encode_input_test(Variable(label), Variable(label_ref), image_ref, infer=True)        \n           \n        if torch.__version__.startswith(\'0.4\'):\n            with torch.no_grad():\n                fake_image = self.netG.forward(input_label, input_label_ref, real_image_ref)\n        else:\n            fake_image = self.netG.forward(input_label, input_label_ref, real_image_ref)\n        return fake_image\n\n    def save(self, which_epoch):\n        self.save_network(self.netG, \'G\', which_epoch, self.gpu_ids)\n        self.save_network(self.netD, \'D\', which_epoch, self.gpu_ids)\n        self.save_network(self.netB, \'B\', which_epoch, self.gpu_ids)\n      \n    def update_fixed_params(self):\n        # after fixing the global generator for a number of iterations, also start finetuning it\n        params = list(self.netG.parameters())\n        if self.gen_features:\n            params += list(self.netE.parameters())           \n        self.optimizer_G = torch.optim.Adam(params, lr=self.opt.lr, betas=(self.opt.beta1, 0.999))\n        if self.opt.verbose:\n            print(\'------------ Now also finetuning global generator -----------\')\n\n    def update_learning_rate(self):\n        lrd = self.opt.lr / self.opt.niter_decay\n        lr = self.old_lr - lrd        \n        for param_group in self.optimizer_D.param_groups:\n            param_group[\'lr\'] = lr\n        for param_group in self.optimizer_G.param_groups:\n            param_group[\'lr\'] = lr\n        if self.opt.verbose:\n            print(\'update learning rate: %f -> %f\' % (self.old_lr, lr))\n        self.old_lr = lr\n\nclass InferenceModel(Pix2PixHDModel):\n    def forward(self, inp):\n        label = inp\n        return self.inference(label)\n\n        \n'"
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/options/__init__.py,0,b''
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/options/base_options.py,1,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport argparse\nimport os\nfrom util import util\nimport torch\n\nclass BaseOptions():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):    \n        # experiment specifics\n        self.parser.add_argument(\'--name\', type=str, default=\'label2face_512p\', help=\'name of the experiment. It decides where to store samples and models\')        \n        self.parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        self.parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        self.parser.add_argument(\'--model\', type=str, default=\'pix2pixHD\', help=\'which model to use\')\n        self.parser.add_argument(\'--norm\', type=str, default=\'instance\', help=\'instance normalization or batch normalization\')        \n        self.parser.add_argument(\'--use_dropout\', action=\'store_true\', help=\'use dropout for the generator\')\n        self.parser.add_argument(\'--data_type\', default=32, type=int, choices=[8, 16, 32], help=""Supported data type i.e. 8, 16, 32 bit"")\n        self.parser.add_argument(\'--verbose\', action=\'store_true\', default=False, help=\'toggles verbose\')\n\n        # input/output sizes       \n        self.parser.add_argument(\'--batchSize\', type=int, default=1, help=\'input batch size\')\n        self.parser.add_argument(\'--loadSize\', type=int, default=512, help=\'scale images to this size\')\n        self.parser.add_argument(\'--fineSize\', type=int, default=512, help=\'then crop to this size\')\n        self.parser.add_argument(\'--label_nc\', type=int, default=19, help=\'# of input label channels\')\n        self.parser.add_argument(\'--input_nc\', type=int, default=3, help=\'# of input image channels\')\n        self.parser.add_argument(\'--output_nc\', type=int, default=3, help=\'# of output image channels\')\n\n        # for setting inputs\n        self.parser.add_argument(\'--dataroot\', type=str, default=\'../Data_preprocessing/\') \n        self.parser.add_argument(\'--resize_or_crop\', type=str, default=\'scale_width\', help=\'scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]\')\n        self.parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')        \n        self.parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data argumentation\') \n        self.parser.add_argument(\'--nThreads\', default=2, type=int, help=\'# threads for loading data\')                \n        self.parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n\n        # for displays\n        self.parser.add_argument(\'--display_winsize\', type=int, default=512,  help=\'display window size\')\n        self.parser.add_argument(\'--tf_log\', action=\'store_true\', help=\'if specified, use tensorboard logging. Requires tensorflow installed\')\n\n        # for generator\n        self.parser.add_argument(\'--netG\', type=str, default=\'global\', help=\'selects model to use for netG\')\n        self.parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        self.parser.add_argument(\'--n_downsample_global\', type=int, default=4, help=\'number of downsampling layers in netG\') \n        self.parser.add_argument(\'--n_blocks_global\', type=int, default=4, help=\'number of residual blocks in the global generator network\')\n        self.parser.add_argument(\'--n_blocks_local\', type=int, default=3, help=\'number of residual blocks in the local enhancer network\')\n        self.parser.add_argument(\'--n_local_enhancers\', type=int, default=1, help=\'number of local enhancers to use\')        \n        self.parser.add_argument(\'--niter_fix_global\', type=int, default=0, help=\'number of epochs that we only train the outmost local enhancer\')        \n\n        self.initialized = True\n\n    def parse(self, save=True):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n        self.opt.isTrain = self.isTrain   # train or test\n\n        str_ids = self.opt.gpu_ids.split(\',\')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(id)\n        \n        # set gpu ids\n        # if len(self.opt.gpu_ids) > 0:\n        #     torch.cuda.set_device(self.opt.gpu_ids[0])\n\n        args = vars(self.opt)\n\n        print(\'------------ Options -------------\')\n        for k, v in sorted(args.items()):\n            print(\'%s: %s\' % (str(k), str(v)))\n        print(\'-------------- End ----------------\')\n\n        # save to the disk        \n        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n        util.mkdirs(expr_dir)\n        if save and not self.opt.continue_train:\n            file_name = os.path.join(expr_dir, \'opt.txt\')\n            with open(file_name, \'wt\') as opt_file:\n                opt_file.write(\'------------ Options -------------\\n\')\n                for k, v in sorted(args.items()):\n                    opt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n                opt_file.write(\'-------------- End ----------------\\n\')\n        return self.opt\n'"
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/options/test_options.py,0,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nfrom .base_options import BaseOptions\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        self.parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        self.parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        self.parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        self.parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        self.parser.add_argument(\'--how_many\', type=int, default=1000, help=\'how many test images to run\')       \n        self.parser.add_argument(\'--cluster_path\', type=str, default=\'features_clustered_010.npy\', help=\'the path for clustered results of encoded features\')\n        self.parser.add_argument(\'--use_encoded_image\', action=\'store_true\', help=\'if specified, encode the real image to get the feature map\')\n        self.parser.add_argument(""--export_onnx"", type=str, help=""export ONNX model to a given file"")\n        self.parser.add_argument(""--engine"", type=str, help=""run serialized TRT engine"")\n        self.parser.add_argument(""--onnx"", type=str, help=""run ONNX model via TRT"")        \n        self.isTrain = False\n'"
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/options/train_options.py,0,"b""### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nfrom .base_options import BaseOptions\n\nclass TrainOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        # for displays\n        self.parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results on screen')\n        self.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n        self.parser.add_argument('--save_latest_freq', type=int, default=1000, help='frequency of saving the latest results')\n        self.parser.add_argument('--save_epoch_freq', type=int, default=10, help='frequency of saving checkpoints at the end of epochs')        \n        self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        self.parser.add_argument('--debug', action='store_true', help='only do one epoch and displays at each iteration')\n\n        # for training\n        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        self.parser.add_argument('--load_pretrain', type=str, default='./checkpoints/label2face_512p', help='load the pretrained model from the specified location') \n        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n        self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        self.parser.add_argument('--lr', type=float, default=0.00005, help='initial learning rate for adam')\n\n        # for discriminators        \n        self.parser.add_argument('--num_D', type=int, default=2, help='number of discriminators to use')\n        self.parser.add_argument('--n_layers_D', type=int, default=3, help='only used if which_model_netD==n_layers')\n        self.parser.add_argument('--ndf', type=int, default=64, help='# of discrim filters in first conv layer')    \n        self.parser.add_argument('--lambda_feat', type=float, default=10.0, help='weight for feature matching loss')                \n        self.parser.add_argument('--no_ganFeat_loss', action='store_true', help='if specified, do *not* use discriminator feature matching loss')\n        self.parser.add_argument('--no_vgg_loss', action='store_true', help='if specified, do *not* use VGG feature matching loss')        \n        self.parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n        self.parser.add_argument('--pool_size', type=int, default=0, help='the size of image buffer that stores previously generated images')\n\n        self.isTrain = True\n"""
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/util/__init__.py,0,b''
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/util/image_pool.py,3,"b'import random\nimport torch\nfrom torch.autograd import Variable\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size-1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = Variable(torch.cat(return_images, 0))\n        return return_images\n'"
gimp-plugins/CelebAMask-HQ/MaskGAN_demo/util/util.py,2,"b'from __future__ import print_function\n\nprint (\'?\')\nimport torch\nimport numpy as np\nfrom PIL import Image\n# import numpy as np\nimport os\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8, normalize=True):\n    if isinstance(image_tensor, list):\n        image_numpy = []\n        for i in range(len(image_tensor)):\n            image_numpy.append(tensor2im(image_tensor[i], imtype, normalize))\n        return image_numpy\n    image_numpy = image_tensor.cpu().float().numpy()\n    #if normalize:\n    #    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    #else:\n    #    image_numpy = np.transpose(image_numpy, (1, 2, 0)) * 255.0\n    image_numpy = (image_numpy + 1) / 2.0\n    image_numpy = np.clip(image_numpy, 0, 1)\n    if image_numpy.shape[2] == 1 or image_numpy.shape[2] > 3:        \n        image_numpy = image_numpy[:,:,0]\n\n    return image_numpy\n\n# Converts a one-hot tensor into a colorful label map\ndef tensor2label(label_tensor, n_label, imtype=np.uint8):\n    if n_label == 0:\n        return tensor2im(label_tensor, imtype)\n    label_tensor = label_tensor.cpu().float()    \n    if label_tensor.size()[0] > 1:\n        label_tensor = label_tensor.max(0, keepdim=True)[1]\n    label_tensor = Colorize(n_label)(label_tensor)\n    #label_numpy = np.transpose(label_tensor.numpy(), (1, 2, 0))\n    label_numpy = label_tensor.numpy()\n    label_numpy = label_numpy / 255.0\n\n    return label_numpy\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n###############################################################################\n# Code from\n# https://github.com/ycszen/pytorch-seg/blob/master/transform.py\n# Modified so it complies with the Citscape label map colors\n###############################################################################\ndef uint82bin(n, count=8):\n    """"""returns the binary of integer n, count refers to amount of bits""""""\n    return \'\'.join([str((n >> y) & 1) for y in range(count-1, -1, -1)])\n\ndef labelcolormap(N):\n    if N == 35: # cityscape\n        cmap = np.array([(  0,  0,  0), (  0,  0,  0), (  0,  0,  0), (  0,  0,  0), (  0,  0,  0), (111, 74,  0), ( 81,  0, 81),\n                     (128, 64,128), (244, 35,232), (250,170,160), (230,150,140), ( 70, 70, 70), (102,102,156), (190,153,153),\n                     (180,165,180), (150,100,100), (150,120, 90), (153,153,153), (153,153,153), (250,170, 30), (220,220,  0),\n                     (107,142, 35), (152,251,152), ( 70,130,180), (220, 20, 60), (255,  0,  0), (  0,  0,142), (  0,  0, 70),\n                     (  0, 60,100), (  0,  0, 90), (  0,  0,110), (  0, 80,100), (  0,  0,230), (119, 11, 32), (  0,  0,142)], \n                     dtype=np.uint8)\n    else:\n        cmap = np.zeros((N, 3), dtype=np.uint8)\n        for i in range(N):\n            r, g, b = 0, 0, 0\n            id = i\n            for j in range(7):\n                str_id = uint82bin(id)\n                r = r ^ (np.uint8(str_id[-1]) << (7-j))\n                g = g ^ (np.uint8(str_id[-2]) << (7-j))\n                b = b ^ (np.uint8(str_id[-3]) << (7-j))\n                id = id >> 3\n            cmap[i, 0] = r\n            cmap[i, 1] = g\n            cmap[i, 2] = b\n    return cmap\n\nclass Colorize(object):\n    def __init__(self, n=35):\n        self.cmap = labelcolormap(n)\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.size()\n        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n\n        for label in range(0, len(self.cmap)):\n            mask = (label == gray_image[0]).cpu()\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        return color_image\n'"
