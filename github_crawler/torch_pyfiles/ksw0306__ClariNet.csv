file_path,api_count,code
data.py,10,"b'import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport os\n\n\nuse_cuda = torch.cuda.is_available()\n\nmax_time_steps = 16000\nupsample_conditional_features = True\nhop_length = 256\n\n\nclass LJspeechDataset(Dataset):\n    def __init__(self, data_root, train=True, test_size=0.05):\n        self.data_root = data_root\n        self.lengths = []\n        self.train = train\n        self.test_size = test_size\n\n        self.paths = [self.collect_files(0), self.collect_files(1)]\n\n    def __len__(self):\n        return len(self.paths[0])\n\n    def __getitem__(self, idx):\n        wav = np.load(self.paths[0][idx])\n        mel = np.load(self.paths[1][idx])\n        return wav, mel\n\n    def interest_indices(self, paths):\n        test_num_samples = int(self.test_size * len(paths))\n        train_indices, test_indices = range(0, len(paths) - test_num_samples), \\\n                                      range(len(paths) - test_num_samples, len(paths))\n        return train_indices if self.train else test_indices\n\n    def collect_files(self, col):\n        meta = os.path.join(self.data_root, ""train.txt"")\n        with open(meta, ""rb"") as f:\n            lines = f.readlines()\n        l = lines[0].decode(""utf-8"").split(""|"")\n        assert len(l) == 4\n        self.lengths = list(\n            map(lambda l: int(l.decode(""utf-8"").split(""|"")[2]), lines))\n\n        paths = list(map(lambda l: l.decode(""utf-8"").split(""|"")[col], lines))\n        paths = list(map(lambda f: os.path.join(self.data_root, f), paths))\n\n        # Filter by train/test\n        indices = self.interest_indices(paths)\n        paths = list(np.array(paths)[indices])\n        self.lengths = list(np.array(self.lengths)[indices])\n        self.lengths = list(map(int, self.lengths))\n        return paths\n\n\ndef _pad(seq, max_len, constant_values=0):\n    return np.pad(seq, (0, max_len - len(seq)),\n                  mode=\'constant\', constant_values=constant_values)\n\n\ndef _pad_2d(x, max_len, b_pad=0):\n    x = np.pad(x, [(b_pad, max_len - len(x) - b_pad), (0, 0)],\n               mode=""constant"", constant_values=0)\n    return x\n\n\ndef collate_fn(batch):\n    """"""\n    Create batch\n\n    Args : batch(tuple) : List of tuples / (x, c)  x : list of (T,) c : list of (T, D)\n\n    Returns : Tuple of batch / Network inputs x (B, C, T), Network targets (B, T, 1)\n    """"""\n\n    local_conditioning = len(batch[0]) >= 2\n\n    if local_conditioning:\n        new_batch = []\n        for idx in range(len(batch)):\n            x, c = batch[idx]\n            if upsample_conditional_features:\n                assert len(x) % len(c) == 0 and len(x) // len(c) == hop_length\n\n                max_steps = max_time_steps - max_time_steps % hop_length   # To ensure Divisibility\n\n                if len(x) > max_steps:\n                    max_time_frames = max_steps // hop_length\n                    s = np.random.randint(0, len(c) - max_time_frames)\n                    ts = s * hop_length\n                    x = x[ts:ts + hop_length * max_time_frames]\n                    c = c[s:s + max_time_frames]\n                    assert len(x) % len(c) == 0 and len(x) // len(c) == hop_length\n            else:\n                pass\n            new_batch.append((x, c))\n        batch = new_batch\n    else:\n        pass\n\n    input_lengths = [len(x[0]) for x in batch]\n    max_input_len = max(input_lengths)\n\n    # x_batch : [B, T, 1]\n    x_batch = np.array([_pad_2d(x[0].reshape(-1, 1), max_input_len) for x in batch], dtype=np.float32)\n    assert len(x_batch.shape) == 3\n\n    y_batch = np.array([_pad(x[0], max_input_len) for x in batch], dtype=np.float32)\n    assert len(y_batch.shape) == 2\n\n    if local_conditioning:\n        max_len = max([len(x[1]) for x in batch])\n        c_batch = np.array([_pad_2d(x[1], max_len) for x in batch], dtype=np.float32)\n        assert len(c_batch.shape) == 3\n        # (B x C x T\')\n        c_batch = torch.tensor(c_batch).transpose(1, 2).contiguous()\n        del max_len\n    else:\n        c_batch = None\n\n    # Convert to channel first i.e., (B, C, T) / C = 1\n    x_batch = torch.tensor(x_batch).transpose(1, 2).contiguous()\n\n    # Add extra axis i.e., (B, T, 1)\n    y_batch = torch.tensor(y_batch).unsqueeze(-1).contiguous()\n\n    input_lengths = torch.tensor(input_lengths)\n    return x_batch, y_batch, c_batch, input_lengths\n\n\ndef collate_fn_synthesize(batch):\n    """"""\n    Create batch\n\n    Args : batch(tuple) : List of tuples / (x, c)  x : list of (T,) c : list of (T, D)\n\n    Returns : Tuple of batch / Network inputs x (B, C, T), Network targets (B, T, 1)\n    """"""\n\n    local_conditioning = len(batch[0]) >= 2\n\n    if local_conditioning:\n        new_batch = []\n        for idx in range(len(batch)):\n            x, c = batch[idx]\n            if upsample_conditional_features:\n                assert len(x) % len(c) == 0 and len(x) // len(c) == hop_length\n            new_batch.append((x, c))\n        batch = new_batch\n    else:\n        pass\n\n    input_lengths = [len(x[0]) for x in batch]\n    max_input_len = max(input_lengths)\n\n    # x_batch : [B, T, 1]\n    x_batch = np.array([_pad_2d(x[0].reshape(-1, 1), max_input_len) for x in batch], dtype=np.float32)\n    assert len(x_batch.shape) == 3\n\n    y_batch = np.array([_pad(x[0], max_input_len) for x in batch], dtype=np.float32)\n    assert len(y_batch.shape) == 2\n\n    if local_conditioning:\n        max_len = max([len(x[1]) for x in batch])\n        c_batch = np.array([_pad_2d(x[1], max_len) for x in batch], dtype=np.float32)\n        assert len(c_batch.shape) == 3\n        # (B x C x T\')\n        c_batch = torch.tensor(c_batch).transpose(1, 2).contiguous()\n    else:\n        c_batch = None\n\n    # Convert to channel first i.e., (B, C, T) / C = 1\n    x_batch = torch.tensor(x_batch).transpose(1, 2).contiguous()\n\n    # Add extra axis i.e., (B, T, 1)\n    y_batch = torch.tensor(y_batch).unsqueeze(-1).contiguous()\n\n    input_lengths = torch.tensor(input_lengths)\n\n    return x_batch, y_batch, c_batch, input_lengths\n\n'"
loss.py,9,"b'import math\nimport torch\nfrom torch.distributions.normal import Normal\n\n\ndef gaussian_loss(y_hat, y, log_std_min=-9.0):\n    assert y_hat.dim() == 3\n    assert y_hat.size(1) == 2\n\n    # (B x T x C)\n    y_hat = y_hat.transpose(1, 2)\n\n    mean = y_hat[:, :, :1]\n    log_std = torch.clamp(y_hat[:, :, 1:], min=log_std_min)\n\n    log_probs = -0.5 * (- math.log(2.0 * math.pi) - 2. * log_std - torch.pow(y - mean, 2) * torch.exp((-2.0 * log_std)))\n    return log_probs.squeeze()\n\n\ndef sample_from_gaussian(y_hat):\n    assert y_hat.size(1) == 2\n\n    y_hat = y_hat.transpose(1, 2)\n    mean = y_hat[:, :, :1]\n    log_std = y_hat[:, :, 1:]\n    dist = Normal(mean, torch.exp(log_std))\n    sample = dist.sample()\n    sample = torch.clamp(torch.clamp(sample, min=-1.), max=1.)\n    del dist\n    return sample\n\n\ndef KL_gaussians(mu_q, logs_q, mu_p, logs_p, log_std_min=-6.0, regularization=True):\n    # KL (q || p)\n    # q ~ N(mu_q, logs_q.exp_()), p ~ N(mu_p, logs_p.exp_())\n    logs_q_org = logs_q\n    logs_p_org = logs_p\n    logs_q = torch.clamp(logs_q, min=log_std_min)\n    logs_p = torch.clamp(logs_p, min=log_std_min)\n    KL_loss = (logs_p - logs_q) + 0.5 * ((torch.exp(2. * logs_q) + torch.pow(mu_p - mu_q, 2)) * torch.exp(-2. * logs_p) - 1.)\n    if regularization:\n        reg_loss = torch.pow(logs_q_org - logs_p_org, 2)\n    else:\n        reg_loss = None\n    return KL_loss, reg_loss\n'"
modules.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom loss import gaussian_loss, KL_gaussians\nimport numpy as np\nimport math\n\n\nclass Conv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, causal=False, mode=\'SAME\'):\n        super(Conv, self).__init__()\n\n        self.causal = causal\n        self.mode = mode\n        if self.causal and self.mode == \'SAME\':\n            self.padding = dilation * (kernel_size - 1)\n        elif self.mode == \'SAME\':\n            self.padding = dilation * (kernel_size - 1) // 2\n        else:\n            self.padding = 0\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation, padding=self.padding)\n        self.conv = nn.utils.weight_norm(self.conv)\n        nn.init.kaiming_normal_(self.conv.weight)\n\n    def forward(self, tensor):\n        out = self.conv(tensor)\n        if self.causal and self.padding is not 0:\n            out = out[:, :, :-self.padding]\n        return out\n\n    def remove_weight_norm(self):\n        nn.utils.remove_weight_norm(self.conv)\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, skip_channels, kernel_size, dilation,\n                 cin_channels=None, local_conditioning=True, causal=False, mode=\'SAME\'):\n        super(ResBlock, self).__init__()\n        self.causal = causal\n        self.local_conditioning = local_conditioning\n        self.cin_channels = cin_channels\n        self.mode = mode\n\n        self.filter_conv = Conv(in_channels, out_channels, kernel_size, dilation, causal, mode)\n        self.gate_conv = Conv(in_channels, out_channels, kernel_size, dilation, causal, mode)\n        self.res_conv = nn.Conv1d(out_channels, in_channels, kernel_size=1)\n        self.skip_conv = nn.Conv1d(out_channels, skip_channels, kernel_size=1)\n        self.res_conv = nn.utils.weight_norm(self.res_conv)\n        self.skip_conv = nn.utils.weight_norm(self.skip_conv)\n        nn.init.kaiming_normal_(self.res_conv.weight)\n        nn.init.kaiming_normal_(self.skip_conv.weight)\n\n        if self.local_conditioning:\n            self.filter_conv_c = nn.Conv1d(cin_channels, out_channels, kernel_size=1)\n            self.gate_conv_c = nn.Conv1d(cin_channels, out_channels, kernel_size=1)\n            self.filter_conv_c = nn.utils.weight_norm(self.filter_conv_c)\n            self.gate_conv_c = nn.utils.weight_norm(self.gate_conv_c)\n            nn.init.kaiming_normal_(self.filter_conv_c.weight)\n            nn.init.kaiming_normal_(self.gate_conv_c.weight)\n\n    def forward(self, tensor, c=None):\n        h_filter = self.filter_conv(tensor)\n        h_gate = self.gate_conv(tensor)\n\n        if self.local_conditioning:\n            h_filter += self.filter_conv_c(c)\n            h_gate += self.gate_conv_c(c)\n\n        out = F.tanh(h_filter) * F.sigmoid(h_gate)\n\n        res = self.res_conv(out)\n        skip = self.skip_conv(out)\n        if self.mode == \'SAME\':\n            return (tensor + res) * math.sqrt(0.5), skip\n        else:\n            return (tensor[:, :, 1:] + res) * math.sqrt(0.5), skip\n\n    def remove_weight_norm(self):\n        self.filter_conv.remove_weight_norm()\n        self.gate_conv.remove_weight_norm()\n        nn.utils.remove_weight_norm(self.res_conv)\n        nn.utils.remove_weight_norm(self.skip_conv)\n        nn.utils.remove_weight_norm(self.filter_conv_c)\n        nn.utils.remove_weight_norm(self.gate_conv_c)\n\n\nclass GaussianLoss(nn.Module):\n    def __init__(self):\n        super(GaussianLoss, self).__init__()\n\n    def forward(self, input, target, size_average=True):\n        losses = gaussian_loss(input, target)\n        if size_average:\n            return losses.mean()\n        else:\n            return losses.mean(1).sum(0)\n\n\nclass KL_Loss(nn.Module):\n    def __init__(self):\n        super(KL_Loss, self).__init__()\n\n    def forward(self, mu_q, logs_q, mu_p, logs_p, regularization=True, size_average=True):\n        KL_loss, reg_loss = KL_gaussians(mu_q, logs_q, mu_p, logs_p, regularization=regularization)\n        loss_tot = KL_loss + reg_loss * 4.\n\n        if size_average:\n            return loss_tot.mean(), KL_loss.mean(), reg_loss.mean()\n        else:\n            return loss_tot.sum(), KL_loss.sum(), reg_loss.sum()\n\n\nclass ExponentialMovingAverage(object):\n    def __init__(self, decay):\n        self.decay = decay\n        self.shadow = {}\n\n    def register(self, name, val):\n        self.shadow[name] = val.clone()\n\n    def update(self, name, x):\n        assert name in self.shadow\n        new_average = self.decay * x + (1.0 - self.decay) * self.shadow[name]\n        self.shadow[name] = new_average.clone()\n\n\ndef stft(y, scale=\'linear\'):\n    D = torch.stft(y, n_fft=1024, hop_length=256, win_length=1024)#, window=torch.hann_window(1024).cuda())\n    D = torch.sqrt(D.pow(2).sum(-1) + 1e-10)\n    # D = torch.sqrt(torch.clamp(D.pow(2).sum(-1), min=1e-10))\n    if scale == \'linear\':\n        return D\n    elif scale == \'log\':\n        S = 2 * torch.log(torch.clamp(D, 1e-10, float(""inf"")))\n        return S\n    else:\n        pass\n\n'"
preprocessing.py,0,"b'from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nimport librosa\nfrom multiprocessing import cpu_count\nimport argparse\n\n\ndef build_from_path(in_dir, out_dir, num_workers=1):\n    executor = ProcessPoolExecutor(max_workers=num_workers)\n    futures = []\n    index = 1\n    with open(os.path.join(in_dir, \'metadata.csv\'), encoding=\'utf-8\') as f:\n        for line in f:\n            parts = line.strip().split(\'|\')\n            wav_path = os.path.join(in_dir, \'wavs\', \'%s.wav\' % parts[0])\n            text = parts[2]\n            futures.append(executor.submit(\n                partial(_process_utterance, out_dir, index, wav_path, text)))\n            index += 1\n    return [future.result() for future in futures]\n\n\ndef _process_utterance(out_dir, index, wav_path, text):\n    # Load the audio to a numpy array:\n    wav, sr = librosa.load(wav_path, sr=22050)\n\n    wav = wav / np.abs(wav).max() * 0.999\n    out = wav\n    constant_values = 0.0\n    out_dtype = np.float32\n    n_fft = 1024\n    hop_length = 256\n    reference = 20.0\n    min_db = -100\n\n    # Compute a mel-scale spectrogram from the trimmed wav:\n    # (N, D)\n    mel_spectrogram = librosa.feature.melspectrogram(wav, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=80, fmin=125, fmax=7600).T\n\n    # mel_spectrogram = np.round(mel_spectrogram, decimals=2)\n    mel_spectrogram = 20 * np.log10(np.maximum(1e-4, mel_spectrogram)) - reference\n    mel_spectrogram = np.clip((mel_spectrogram - min_db) / (-min_db), 0, 1)\n\n    pad = (out.shape[0] // hop_length + 1) * hop_length - out.shape[0]\n    pad_l = pad // 2\n    pad_r = pad // 2 + pad % 2\n\n    # zero pad for quantized signal\n    out = np.pad(out, (pad_l, pad_r), mode=""constant"", constant_values=constant_values)\n    N = mel_spectrogram.shape[0]\n    assert len(out) >= N * hop_length\n\n    # time resolution adjustment\n    # ensure length of raw audio is multiple of hop_size so that we can use\n    # transposed convolution to upsample\n    out = out[:N * hop_length]\n    assert len(out) % hop_length == 0\n\n    timesteps = len(out)\n\n    # Write the spectrograms to disk:\n    audio_filename = \'ljspeech-audio-%05d.npy\' % index\n    mel_filename = \'ljspeech-mel-%05d.npy\' % index\n    np.save(os.path.join(out_dir, audio_filename),\n            out.astype(out_dtype), allow_pickle=False)\n    np.save(os.path.join(out_dir, mel_filename),\n            mel_spectrogram.astype(np.float32), allow_pickle=False)\n\n    # Return a tuple describing this training example:\n    return audio_filename, mel_filename, timesteps, text\n\n\ndef preprocess(in_dir, out_dir, num_workers):\n    os.makedirs(out_dir, exist_ok=True)\n    metadata = build_from_path(in_dir, out_dir, num_workers)\n    write_metadata(metadata, out_dir)\n\n\ndef write_metadata(metadata, out_dir):\n    with open(os.path.join(out_dir, \'train.txt\'), \'w\', encoding=\'utf-8\') as f:\n        for m in metadata:\n            f.write(\'|\'.join([str(x) for x in m]) + \'\\n\')\n    frames = sum([m[2] for m in metadata])\n    sr = 22050\n    hours = frames / sr / 3600\n    print(\'Wrote %d utterances, %d time steps (%.2f hours)\' % (len(metadata), frames, hours))\n    print(\'Max input length:  %d\' % max(len(m[3]) for m in metadata))\n    print(\'Max output length: %d\' % max(m[2] for m in metadata))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Preprocessing\',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--in_dir\', \'-i\', type=str, default=\'./\', help=\'In Directory\')\n    parser.add_argument(\'--out_dir\', \'-o\', type=str, default=\'./\', help=\'Out Directory\')\n    args = parser.parse_args()\n\n    num_workers = cpu_count()\n    preprocess(args.in_dir, args.out_dir, num_workers)\n'"
synthesize.py,7,"b'import time\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom data import LJspeechDataset, collate_fn, collate_fn_synthesize\nfrom wavenet import Wavenet\nimport librosa\nimport os\nimport argparse\n\n\nparser = argparse.ArgumentParser(description=\'Train WaveNet of LJSpeech\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--data_path\', type=str, default=\'./DATASETS/ljspeech/\', help=\'Dataset Path\')\nparser.add_argument(\'--sample_path\', type=str, default=\'./samples\', help=\'Sample Path\')\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./params\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./params\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--loss\', type=str, default=\'./loss\', help=\'Folder to save loss\')\nparser.add_argument(\'--log\', type=str, default=\'./log\', help=\'Log folder.\')\nparser.add_argument(\'--model_name\', type=str, default=\'wavenet_gaussian_01\', help=\'Model Name\')\nparser.add_argument(\'--load_step\', type=int, default=0, help=\'Load Step\')\n\n# Optimization options\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=1000, help=\'Number of epochs to train.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=1, help=\'Batch size.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=1e-3, help=\'The Learning Rate.\')\nparser.add_argument(\'--ema_decay\', type=float, default=0.9999, help=\'Exponential Moving Average Decay\')\n\nparser.add_argument(\'--num_blocks\', type=int, default=2, help=\'Number of blocks\')\nparser.add_argument(\'--num_layers\', type=int, default=10, help=\'Number of layers\')\nparser.add_argument(\'--residual_channels\', type=int, default=128, help=\'Residual Channels\')\nparser.add_argument(\'--gate_channels\', type=int, default=256, help=\'Gate Channels\')\nparser.add_argument(\'--skip_channels\', type=int, default=128, help=\'Skip Channels\')\nparser.add_argument(\'--kernel_size\', type=int, default=2, help=\'Kernel Size\')\nparser.add_argument(\'--cin_channels\', type=int, default=80, help=\'Cin Channels\')\n\nparser.add_argument(\'--num_samples\', type=int, default=5, help=\'Number of Samples\')\n\nparser.add_argument(\'--num_workers\', type=int, default=1, help=\'Number of workers\')\nparser.add_argument(\'--index\', type=int, default=0, help=\'Index\')\n\n\nargs = parser.parse_args()\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n\nif not os.path.isdir(args.sample_path):\n    os.makedirs(args.sample_path)\nif not os.path.isdir(os.path.join(args.sample_path, args.model_name)):\n    os.makedirs(os.path.join(args.sample_path, args.model_name))\n\n# LOAD DATASETS\ntrain_dataset = LJspeechDataset(args.data_path, True, 0.1)\ntest_dataset = LJspeechDataset(args.data_path, False, 0.1)\n\ntrain_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn,\n                          num_workers=args.num_workers, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=collate_fn_synthesize,\n                         num_workers=args.num_workers, pin_memory=True)\n\n\ndef build_model():\n    model = Wavenet(out_channels=2,\n                    num_blocks=args.num_blocks,\n                    num_layers=args.num_layers,\n                    residual_channels=args.residual_channels,\n                    gate_channels=args.gate_channels,\n                    skip_channels=args.skip_channels,\n                    kernel_size=args.kernel_size,\n                    cin_channels=args.cin_channels,\n                    upsample_scales=[16, 16])\n    return model\n\n\ndef load_checkpoint(path, model):\n    print(""Load checkpoint from: {}"".format(path))\n    checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n    model.load_state_dict(checkpoint[""state_dict""])\n    return model\n\n\nstep = args.load_step\npath = os.path.join(args.load, args.model_name, ""checkpoint_step{:09d}_ema.pth"".format(step))\nmodel = build_model()\nmodel = load_checkpoint(path, model)\nmodel.to(device)\n\nmodel.eval()\n\nfor i, (x, y, c, _) in enumerate(test_loader):\n    if i < args.num_samples:\n        x, c = x.to(device), c.to(device)\n\n        wav_truth_name = \'{}/{}/generate_{}_{}_truth.wav\'.format(args.sample_path, args.model_name, step, i)\n        librosa.output.write_wav(wav_truth_name, y.squeeze().numpy(), sr=22050)\n        torch.cuda.synchronize()\n        start_time = time.time()\n\n        with torch.no_grad():\n            y_gen = model.generate(x.size()[-1], c).squeeze()\n        torch.cuda.synchronize()\n        print(\'{} seconds\'.format(time.time()-start_time))\n        wav = y_gen.numpy()\n        wav_name = \'{}/{}/generate_{}_{}.wav\'.format(args.sample_path, args.model_name, step, i)\n        librosa.output.write_wav(wav_name, wav, sr=22050)\n        del y_gen\n\n'"
synthesize_student.py,10,"b'import time\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom data import LJspeechDataset, collate_fn_synthesize\nfrom wavenet import Wavenet\nfrom wavenet_iaf import Wavenet_Student\nfrom torch.distributions.normal import Normal\nimport librosa\nimport os\nimport argparse\n\n\nparser = argparse.ArgumentParser(description=\'Train WaveNet of LJSpeech\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--data_path\', type=str, default=\'./DATASETS/ljspeech/\', help=\'Dataset Path\')\nparser.add_argument(\'--sample_path\', type=str, default=\'./samples\', help=\'Sample Path\')\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./params\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./params\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--loss\', type=str, default=\'./loss\', help=\'Folder to save loss\')\nparser.add_argument(\'--log\', type=str, default=\'./log\', help=\'Log folder.\')\n\nparser.add_argument(\'--teacher_name\', type=str, default=\'wavenet_gaussian_01\', help=\'Teacher Name\')\nparser.add_argument(\'--model_name\', type=str, default=\'clarinet_01\', help=\'Model Name\')\nparser.add_argument(\'--teacher_load_step\', type=int, default=0, help=\'Teacher Load Step\')\nparser.add_argument(\'--load_step\', type=int, default=0, help=\'Student Load Step\')\n\nparser.add_argument(\'--num_blocks_t\', type=int, default=2, help=\'Number of blocks (Teacher)\')\nparser.add_argument(\'--num_layers_t\', type=int, default=10, help=\'Number of layers (Teacher)\')\nparser.add_argument(\'--num_layers_s\', type=int, default=10, help=\'Number of layers (Student)\')\nparser.add_argument(\'--residual_channels\', type=int, default=128, help=\'Residual Channels\')\nparser.add_argument(\'--gate_channels\', type=int, default=256, help=\'Gate Channels\')\nparser.add_argument(\'--skip_channels\', type=int, default=128, help=\'Skip Channels\')\nparser.add_argument(\'--kernel_size\', type=int, default=2, help=\'Kernel Size\')\nparser.add_argument(\'--cin_channels\', type=int, default=80, help=\'Cin Channels\')\n\nparser.add_argument(\'--temp\', type=float, default=1.0, help=\'Temperature\')\nparser.add_argument(\'--num_samples\', type=int, default=10, help=\'Number of samples\')\n\nparser.add_argument(\'--num_workers\', type=int, default=1, help=\'Number of workers\')\n\n\nargs = parser.parse_args()\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n\nif not os.path.isdir(args.sample_path):\n    os.makedirs(args.sample_path)\nif not os.path.isdir(os.path.join(args.sample_path, args.teacher_name)):\n    os.makedirs(os.path.join(args.sample_path, args.teacher_name))\nif not os.path.isdir(os.path.join(args.sample_path, args.teacher_name, args.model_name)):\n    os.makedirs(os.path.join(args.sample_path, args.teacher_name, args.model_name))\n\n# LOAD DATASETS\ntest_dataset = LJspeechDataset(args.data_path, False, 0.1)\n\ntest_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn_synthesize,\n                         num_workers=args.num_workers, pin_memory=True)\n\n\ndef build_model():\n    model_t = Wavenet(out_channels=2,\n                      num_blocks=args.num_blocks_t,\n                      num_layers=args.num_layers_t,\n                      residual_channels=args.residual_channels,\n                      gate_channels=args.gate_channels,\n                      skip_channels=args.skip_channels,\n                      kernel_size=args.kernel_size,\n                      cin_channels=args.cin_channels,\n                      upsample_scales=[16, 16])\n    return model_t\n\n\ndef build_student():\n    model_s = Wavenet_Student(num_blocks_student=[1, 1, 1, 1, 1, 1],\n                              num_layers=args.num_layers_s)\n    return model_s\n\n\ndef load_checkpoint(path, model):\n    print(""Load checkpoint from: {}"".format(path))\n    checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n    try:\n        model.load_state_dict(checkpoint[""state_dict""])\n    except RuntimeError:\n        print(""INFO: this model is trained with DataParallel. Creating new state_dict without module..."")\n        state_dict = checkpoint[""state_dict""]\n        from collections import OrderedDict\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            name = k[7:]  # remove `module.`\n            new_state_dict[name] = v\n        model.load_state_dict(new_state_dict)\n    return model\n\n\nstep_t = args.teacher_load_step\nstep_s = args.load_step\npath_t = os.path.join(args.load, args.teacher_name, ""checkpoint_step{:09d}_ema.pth"".format(step_t))\npath_s = os.path.join(args.load, args.teacher_name, args.model_name, ""checkpoint_step{:09d}_ema.pth"".format(step_s))\nmodel_t = build_model()\nmodel_t = load_checkpoint(path_t, model_t)\nmodel_s = build_student()\nmodel_s = load_checkpoint(path_s, model_s)\nmodel_t.to(device)\nmodel_s.to(device)\n\nmodel_t.eval()\nmodel_s.eval()\nprint(\'remove_weight_norm\')\nmodel_s.remove_weight_norm()\n\nfor i, (x, y, c, l) in enumerate(test_loader):\n    if i < args.num_samples:\n        x, y, c = x.to(device), y.to(device), c.to(device)\n        print(x.size())\n        q_0 = Normal(x.new_zeros(x.size()), x.new_ones(x.size()))\n        z = q_0.sample() * args.temp\n\n        wav_truth_name = \'{}/{}/{}/generate_{}_{}_truth.wav\'.format(args.sample_path,\n                                                                    args.teacher_name,\n                                                                    args.model_name,\n                                                                    args.load_step,\n                                                                    i)\n        librosa.output.write_wav(wav_truth_name, y.squeeze().to(torch.device(""cpu"")).numpy(), sr=22050)\n        torch.cuda.synchronize()\n        start_time = time.time()\n\n        with torch.no_grad():\n            c_up = model_t.upsample(c)\n            y_gen = model_s.generate(z, c_up).squeeze()\n        torch.cuda.synchronize()\n        print(\'{} seconds\'.format(time.time() - start_time))\n        wav = y_gen.to(torch.device(""cpu"")).data.numpy()\n        wav_name = \'{}/{}/{}/generate_{}_{}_{}.wav\'.format(args.sample_path,\n                                                           args.teacher_name,\n                                                           args.model_name,\n                                                           args.load_step,\n                                                           i,\n                                                           args.temp)\n        librosa.output.write_wav(wav_name, wav, sr=22050)\n        print(\'{} Saved!\'.format(wav_name))\n\n'"
train.py,11,"b'import torch\nfrom torch import optim\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom data import LJspeechDataset, collate_fn\nfrom modules import ExponentialMovingAverage, GaussianLoss\nfrom wavenet import Wavenet\nfrom torch.distributions.normal import Normal\nimport numpy as np\nimport librosa\nimport os\nimport argparse\nimport json\nimport time\nimport gc\n\ntorch.backends.cudnn.benchmark = True\nnp.set_printoptions(precision=4)\n\nparser = argparse.ArgumentParser(description=\'Train WaveNet of LJSpeech\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--data_path\', type=str, default=\'./DATASETS/ljspeech/\', help=\'Dataset Path\')\nparser.add_argument(\'--sample_path\', type=str, default=\'./samples\', help=\'Sample Path\')\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./params\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./params\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--loss\', type=str, default=\'./loss\', help=\'Folder to save loss\')\nparser.add_argument(\'--log\', type=str, default=\'./log\', help=\'Log folder.\')\n\nparser.add_argument(\'--model_name\', type=str, default=\'wavenet_gaussian_01\', help=\'Model Name\')\nparser.add_argument(\'--load_step\', type=int, default=0, help=\'Model Load Step\')\n\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=1000, help=\'Number of epochs to train.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=8, help=\'Batch size.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.001, help=\'The Learning Rate.\')\nparser.add_argument(\'--ema_decay\', type=float, default=0.9999, help=\'Exponential Moving Average Decay\')\n\nparser.add_argument(\'--num_blocks\', type=int, default=2, help=\'Number of blocks\')\nparser.add_argument(\'--num_layers\', type=int, default=10, help=\'Number of layers\')\nparser.add_argument(\'--residual_channels\', type=int, default=128, help=\'Residual Channels\')\nparser.add_argument(\'--gate_channels\', type=int, default=256, help=\'Gate Channels\')\nparser.add_argument(\'--skip_channels\', type=int, default=128, help=\'Skip Channels\')\nparser.add_argument(\'--kernel_size\', type=int, default=2, help=\'Kernel Size\')\nparser.add_argument(\'--cin_channels\', type=int, default=80, help=\'Cin Channels\')\nparser.add_argument(\'--num_workers\', type=int, default=2, help=\'Number of workers\')\n\nargs = parser.parse_args()\n\n# Init logger\nif not os.path.isdir(args.log):\n    os.makedirs(args.log)\n\n# Checkpoint dir\nif not os.path.isdir(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.loss):\n    os.makedirs(args.loss)\nif not os.path.isdir(args.sample_path):\n    os.makedirs(args.sample_path)\nif not os.path.isdir(os.path.join(args.save, args.model_name)):\n    os.makedirs(os.path.join(args.save, args.model_name))\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n# LOAD DATASETS\ntrain_dataset = LJspeechDataset(args.data_path, True, 0.1)\ntest_dataset = LJspeechDataset(args.data_path, False, 0.1)\ntrain_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn,\n                          num_workers=args.num_workers, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=collate_fn,\n                         num_workers=args.num_workers, pin_memory=True)\n\n\ndef build_model():\n    model = Wavenet(out_channels=2,\n                    num_blocks=args.num_blocks,\n                    num_layers=args.num_layers,\n                    residual_channels=args.residual_channels,\n                    gate_channels=args.gate_channels,\n                    skip_channels=args.skip_channels,\n                    kernel_size=args.kernel_size,\n                    cin_channels=args.cin_channels,\n                    upsample_scales=[16, 16])\n    return model\n\n\ndef clone_as_averaged_model(model, ema):\n    assert ema is not None\n    averaged_model = build_model()\n    averaged_model.to(device)\n    averaged_model.load_state_dict(model.state_dict())\n\n\tfor name, param in averaged_model.named_parameters():\n        if name in ema.shadow:\n            param.data = ema.shadow[name].clone().data\n    return averaged_model\n\n\ndef train(epoch, model, optimizer, ema):\n    global global_step\n    epoch_loss = 0.\n    running_loss = 0.\n    model.train()\n    start_time = time.time()\n    display_step = 100\n    for batch_idx, (x, y, c, _) in enumerate(train_loader):\n        global_step += 1\n        if global_step == 200000:\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] *= 0.5\n                state[\'learning_rate\'] = param_group[\'lr\']\n        if global_step == 400000:\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] *= 0.5\n                state[\'learning_rate\'] = param_group[\'lr\']\n        if global_step == 600000:\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] *= 0.5\n                state[\'learning_rate\'] = param_group[\'lr\']\n        x, y, c = x.to(device), y.to(device), c.to(device)\n\n        optimizer.zero_grad()\n        y_hat = model(x, c)\n        loss = criterion(y_hat[:, :, :-1], y[:, 1:, :], size_average=True)\n        loss.backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), 10.)\n        optimizer.step()\n        if ema is not None:\n            for name, param in model.named_parameters():\n                if name in ema.shadow:\n                    ema.update(name, param.data)\n\n        running_loss += loss.item() / display_step\n        epoch_loss += loss.item()\n        if (batch_idx + 1) % 100 == 0:\n            end_time = time.time()\n            print(\'Global Step : {}, [{}, {}] loss : {:.4f}\'.format(global_step, epoch, batch_idx + 1, running_loss))\n            print(\'100 Step Time : {}\'.format(end_time - start_time))\n            start_time = time.time()\n            running_loss = 0.\n        del y_hat, x, y, c, loss\n    gc.collect()\n    print(\'{} Epoch Training Loss : {:.4f}\'.format(epoch, epoch_loss / (len(train_loader))))\n    del running_loss\n    return epoch_loss / len(train_loader)\n\n\ndef evaluate(model, ema=None):\n    if ema is not None:\n        model_ema = clone_as_averaged_model(model, ema)\n    model_ema.eval()\n    running_loss = 0.\n    epoch_loss = 0.\n    display_step = 100\n    for batch_idx, (x, y, c, _) in enumerate(test_loader):\n        x, y, c = x.to(device), y.to(device), c.to(device)\n\n        y_hat = model_ema(x, c)\n\n        loss = criterion(y_hat[:, :, :-1], y[:, 1:, :], size_average=True)\n\n        running_loss += loss.item() / display_step\n        epoch_loss += loss.item()\n\n        if (batch_idx + 1) % display_step == 0:\n            print(\'{} Loss : {:.4f}\'.format(batch_idx + 1, running_loss))\n            running_loss = 0.\n        del y_hat, x, y, c, loss\n    del model_ema\n    epoch_loss /= len(test_loader)\n    print(\'Evaluation Loss : {:.4f}\'.format(epoch_loss))\n    return epoch_loss\n\n\ndef save_checkpoint(model, optimizer, global_step, global_epoch, ema=None):\n    checkpoint_path = os.path.join(args.save, args.model_name, ""checkpoint_step{:09d}.pth"".format(global_step))\n    optimizer_state = optimizer.state_dict()\n    torch.save({""state_dict"": model.state_dict(),\n                ""optimizer"": optimizer_state,\n                ""global_step"": global_step,\n                ""global_epoch"": global_epoch}, checkpoint_path)\n    if ema is not None:\n        averaged_model = clone_as_averaged_model(model, ema)\n        checkpoint_path = os.path.join(args.save, args.model_name, ""checkpoint_step{:09d}_ema.pth"".format(global_step))\n        torch.save({""state_dict"": averaged_model.state_dict(),\n                    ""optimizer"": optimizer_state,\n                    ""global_step"": global_step,\n                    ""global_epoch"": global_epoch}, checkpoint_path)\n\n\ndef load_checkpoint(step, model, optimizer, ema=None):\n    global global_step\n    global global_epoch\n\n    checkpoint_path = os.path.join(args.save, args.model_name, ""checkpoint_step{:09d}.pth"".format(step))\n    print(""Load checkpoint from: {}"".format(checkpoint_path))\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint[""state_dict""])\n    optimizer.load_state_dict(checkpoint[""optimizer""])\n    global_step = checkpoint[""global_step""]\n    global_epoch = checkpoint[""global_epoch""]\n\n    if ema is not None:\n        checkpoint_path = os.path.join(args.save, args.model_name, ""checkpoint_step{:09d}_ema.pth"".format(step))\n        checkpoint = torch.load(checkpoint_path)\n        averaged_model = build_model()\n        averaged_model.to(device)\n        averaged_model.load_state_dict(checkpoint[""state_dict""])\n        for name, param in averaged_model.named_parameters():\n            if param.requires_grad:\n                ema.register(name, param.data)\n    return model, optimizer, ema\n\n\nmodel = build_model()\nmodel.to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\ncriterion = GaussianLoss()\n\nema = ExponentialMovingAverage(args.ema_decay)\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        ema.register(name, param.data)\n\nglobal_step, global_epoch = 0, 0\nload_step = args.load_step\n\nlog = open(os.path.join(args.log, \'{}.txt\'.format(args.model_name)), \'w\')\nstate = {k: v for k, v in args._get_kwargs()}\n\nif load_step == 0:\n    list_train_loss, list_loss = [], []\n    log.write(json.dumps(state) + \'\\n\')\n    test_loss = 100.0\nelse:\n    model, optimizer, ema = load_checkpoint(load_step, model, optimizer, ema)\n    list_train_loss = np.load(\'{}/{}_train.npy\'.format(args.loss, args.model_name)).tolist()\n    list_loss = np.load(\'{}/{}.npy\'.format(args.loss, args.model_name)).tolist()\n    list_train_loss = list_train_loss[:global_epoch]\n    list_loss = list_loss[:global_epoch]\n    test_loss = np.min(list_loss)\n\nfor epoch in range(global_epoch + 1, args.epochs + 1):\n    training_epoch_loss = train(epoch, model, optimizer, ema)\n    with torch.no_grad():\n        test_epoch_loss = evaluate(model, ema)\n    \n    state[\'training_loss\'] = training_epoch_loss\n    state[\'eval_loss\'] = test_epoch_loss\n    state[\'epoch\'] = epoch\n    list_train_loss.append(training_epoch_loss)\n    list_loss.append(test_epoch_loss)\n\n    if test_loss > test_epoch_loss:\n        test_loss = test_epoch_loss\n        save_checkpoint(model, optimizer, global_step, epoch, ema)\n        print(\'Epoch {} Model Saved! Loss : {:.4f}\'.format(epoch, test_loss))\n    np.save(\'{}/{}_train.npy\'.format(args.loss, args.model_name), list_train_loss)\n    np.save(\'{}/{}.npy\'.format(args.loss, args.model_name), list_loss)\n\n    log.write(\'%s\\n\' % json.dumps(state))\n    log.flush()\n    print(state)\n    gc.collect()\n\nlog.close()\n'"
train_student.py,19,"b'import torch\nfrom torch import optim\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.distributions.normal import Normal\nfrom data import LJspeechDataset, collate_fn, collate_fn_synthesize\nfrom modules import ExponentialMovingAverage, KL_Loss, stft\nfrom wavenet import Wavenet\nfrom wavenet_iaf import Wavenet_Student\nimport numpy as np\nimport librosa\nimport os\nimport argparse\nimport json\nimport time\n\ntorch.backends.cudnn.benchmark = True\nnp.set_printoptions(precision=4)\n\nparser = argparse.ArgumentParser(description=\'Train WaveNet of LJSpeech\',\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--data_path\', type=str, default=\'./DATASETS/ljspeech/\', help=\'Dataset Path\')\nparser.add_argument(\'--sample_path\', type=str, default=\'./samples\', help=\'Sample Path\')\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./params\', help=\'Folder to save checkpoints.\')\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./params\', help=\'Checkpoint path to resume / test.\')\nparser.add_argument(\'--loss\', type=str, default=\'./loss\', help=\'Folder to save loss\')\nparser.add_argument(\'--log\', type=str, default=\'./log\', help=\'Log folder.\')\n\nparser.add_argument(\'--teacher_name\', type=str, default=\'wavenet_gaussian_01\', help=\'Model Name\')\nparser.add_argument(\'--model_name\', type=str, default=\'clarinet_01\', help=\'Model Name\')\nparser.add_argument(\'--teacher_load_step\', type=int, default=0, help=\'Teacher Load Step\')\nparser.add_argument(\'--load_step\', type=int, default=0, help=\'Student Load Step\')\n\nparser.add_argument(\'--KL_type\', type=str, default=\'qp\', help=\'KL_pq vs KL_qp\')\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=1000, help=\'Number of epochs to train.\')\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=8, help=\'Batch size.\')\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=1e-3, help=\'The Learning Rate.\')\nparser.add_argument(\'--ema_decay\', type=float, default=0.9999, help=\'Exponential Moving Average Decay\')\nparser.add_argument(\'--num_blocks_t\', type=int, default=2, help=\'Number of blocks (Teacher)\')\nparser.add_argument(\'--num_layers_t\', type=int, default=10, help=\'Number of layers (Teacher)\')\nparser.add_argument(\'--num_layers_s\', type=int, default=10, help=\'Number of layers (Student)\')\nparser.add_argument(\'--residual_channels\', type=int, default=128, help=\'Residual Channels\')\nparser.add_argument(\'--gate_channels\', type=int, default=256, help=\'Gate Channels\')\nparser.add_argument(\'--skip_channels\', type=int, default=128, help=\'Skip Channels\')\nparser.add_argument(\'--kernel_size\', type=int, default=2, help=\'Kernel Size\')\nparser.add_argument(\'--cin_channels\', type=int, default=80, help=\'Cin Channels\')\nparser.add_argument(\'--num_workers\', type=int, default=3, help=\'Number of workers\')\nparser.add_argument(\'--num_gpu\', type=int, default=1, help=\'Number of GPUs to use. >1 uses DataParallel\')\n\nargs = parser.parse_args()\n\n# Init logger\nif not os.path.isdir(args.log):\n    os.makedirs(args.log)\n\n# Checkpoint dir\nif not os.path.isdir(args.save):\n    os.makedirs(args.save)\nif not os.path.isdir(args.loss):\n    os.makedirs(args.loss)\nif not os.path.isdir(os.path.join(args.save, args.teacher_name)):\n    os.makedirs(os.path.join(args.save, args.teacher_name))\nif not os.path.isdir(os.path.join(args.save, args.teacher_name, args.model_name)):\n    os.makedirs(os.path.join(args.save, args.teacher_name, args.model_name))\nif not os.path.isdir(os.path.join(args.sample_path, args.teacher_name)):\n    os.makedirs(os.path.join(args.sample_path, args.teacher_name))\nif not os.path.isdir(os.path.join(args.sample_path, args.teacher_name, args.model_name)):\n    os.makedirs(os.path.join(args.sample_path, args.teacher_name, args.model_name))\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n# LOAD DATASETS\ntrain_dataset = LJspeechDataset(args.data_path, True, 0.1)\ntest_dataset = LJspeechDataset(args.data_path, False, 0.1)\n\ntrain_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn,\n                          num_workers=args.num_workers, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=collate_fn,\n                         num_workers=args.num_workers)\nsynth_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn_synthesize,\n                          num_workers=args.num_workers, pin_memory=True)\n\n\ndef build_model():\n    model_t = Wavenet(out_channels=2,\n                      num_blocks=args.num_blocks_t,\n                      num_layers=args.num_layers_t,\n                      residual_channels=args.residual_channels,\n                      gate_channels=args.gate_channels,\n                      skip_channels=args.skip_channels,\n                      kernel_size=args.kernel_size,\n                      cin_channels=args.cin_channels,\n                      upsample_scales=[16, 16])\n    return model_t\n\n\ndef build_student():\n    model_s = Wavenet_Student(num_blocks_student=[1, 1, 1, 1, 1, 1],\n                              num_layers=args.num_layers_s)\n    return model_s\n\n\ndef clone_as_averaged_model(model_s, ema):\n    assert ema is not None\n    averaged_model = build_student()\n    averaged_model.to(device)\n    if args.num_gpu > 1:\n        averaged_model = torch.nn.DataParallel(averaged_model)\n    averaged_model.load_state_dict(model_s.state_dict())\n\n\tfor name, param in averaged_model.named_parameters():\n        if name in ema.shadow:\n            param.data = ema.shadow[name].clone().data\n    return averaged_model\n\n\ndef train(epoch, model_t, model_s, optimizer, ema):\n    global global_step\n    epoch_loss = 0.0\n    running_loss = [0.0, 0.0, 0.0, 0.0]\n    model_t.eval()\n    model_s.train()\n    start_time = time.time()\n    display_step = 100\n    for batch_idx, (x, y, c, _) in enumerate(train_loader):\n        global_step += 1\n        if global_step == 200000:\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] *= 0.5\n                state[\'learning_rate\'] = param_group[\'lr\']\n        if global_step == 400000:\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] *= 0.5\n                state[\'learning_rate\'] = param_group[\'lr\']\n        if global_step == 600000:\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] *= 0.5\n                state[\'learning_rate\'] = param_group[\'lr\']\n\n        x, y, c = x.to(device), y.to(device), c.to(device)\n\n        q_0 = Normal(x.new_zeros(x.size()), x.new_ones(x.size()))\n        z = q_0.sample()\n\n        optimizer.zero_grad()\n        c_up = model_t.upsample(c)\n        x_student, mu_s, logs_s = model_s(z, c_up)  # q_T ~ N(mu_tot, logs_tot.exp_())\n\n        mu_logs_t = model_t(x_student, c)\n\n        if args.KL_type == \'pq\':\n            loss_t, loss_KL, loss_reg = criterion_t(mu_logs_t[:, 0:1, :-1], mu_logs_t[:, 1:, :-1], mu_s, logs_s)\n        elif args.KL_type == \'qp\':\n            loss_t, loss_KL, loss_reg = criterion_t(mu_s, logs_s, mu_logs_t[:, 0:1, :-1], mu_logs_t[:, 1:, :-1])\n\n        stft_student = stft(x_student[:, 0, 1:], scale=\'linear\')\n        stft_truth = stft(x[:, 0, 1:], scale=\'linear\')\n        loss_frame = criterion_frame(stft_student, stft_truth)\n        loss_tot = loss_t + loss_frame\n        loss_tot.backward()\n\n        nn.utils.clip_grad_norm_(model_s.parameters(), 10.)\n        optimizer.step()\n        if ema is not None:\n            for name, param in model_s.named_parameters():\n                if name in ema.shadow:\n                    ema.update(name, param.data)\n\n        running_loss[0] += loss_tot.item() / display_step\n        running_loss[1] += loss_KL.item() / display_step\n        running_loss[2] += loss_reg.item() / display_step\n        running_loss[3] += loss_frame.item() / display_step\n        epoch_loss += loss_tot.item()\n        if (batch_idx + 1) % display_step == 0:\n            end_time = time.time()\n            print(\'Global Step : {}, [{}, {}] [Total Loss, KL Loss, Reg Loss, Frame Loss] : {}\'\n                  .format(global_step, epoch, batch_idx + 1, np.array(running_loss)))\n            print(\'{} Step Time : {}\'.format(display_step, end_time - start_time))\n            start_time = time.time()\n            running_loss = [0.0, 0.0, 0.0, 0.0]\n        del loss_tot, loss_frame, loss_KL, loss_reg, loss_t, x, y, c, c_up, stft_student, stft_truth, q_0, z\n        del x_student, mu_s, logs_s, mu_logs_t\n    print(\'{} Epoch Training Loss : {:.4f}\'.format(epoch, epoch_loss / (len(train_loader))))\n    return epoch_loss / len(train_loader)\n\n\ndef evaluate(model_t, model_s, ema=None):\n    if ema is not None:\n        model_s_ema = clone_as_averaged_model(model_s, ema)\n    model_t.eval()\n    model_s_ema.eval()\n    running_loss = [0., 0., 0., 0.]\n    epoch_loss = 0.\n\n    display_step = 100\n    for batch_idx, (x, y, c, _) in enumerate(test_loader):\n        x, y, c = x.to(device), y.to(device), c.to(device)\n\n        q_0 = Normal(x.new_zeros(x.size()), x.new_ones(x.size()))\n        z = q_0.sample()\n        c_up = model_t.upsample(c)\n\n        x_student, mu_s, logs_s = model_s_ema(z, c_up)\n\n        mu_logs_t = model_t(x_student, c)\n\n        if args.KL_type == \'pq\':\n            loss_t, loss_KL, loss_reg = criterion_t(mu_logs_t[:, 0:1, :-1], mu_logs_t[:, 1:, :-1], mu_s, logs_s)\n        elif args.KL_type == \'qp\':\n            loss_t, loss_KL, loss_reg = criterion_t(mu_s, logs_s, mu_logs_t[:, 0:1, :-1], mu_logs_t[:, 1:, :-1])\n\n        stft_student = stft(x_student[:, 0, 1:], scale=\'linear\')\n        stft_truth = stft(x[:, 0, 1:], scale=\'linear\')\n\n        loss_frame = criterion_frame(stft_student, stft_truth.detach())\n\n        loss_tot = loss_t + loss_frame\n\n        running_loss[0] += loss_tot.item() / display_step\n        running_loss[1] += loss_KL.item() / display_step\n        running_loss[2] += loss_reg.item() / display_step\n        running_loss[3] += loss_frame.item() / display_step\n        epoch_loss += loss_tot.item()\n\n        if (batch_idx + 1) % display_step == 0:\n            print(\'{} [Total, KL, Reg, Frame Loss] : {}\'.format(batch_idx + 1, np.array(running_loss)))\n            running_loss = [0., 0., 0., 0.]\n        del loss_tot, loss_frame, loss_KL, loss_reg, loss_t, x, y, c, c_up, stft_student, stft_truth, q_0, z\n        del x_student, mu_s, logs_s, mu_logs_t\n    epoch_loss /= len(test_loader)\n    print(\'Evaluation Loss : {:.4f}\'.format(epoch_loss))\n    del model_s_ema\n    return epoch_loss\n\n\ndef synthesize(model_t, model_s, ema=None):\n    global global_step\n    if ema is not None:\n        model_s_ema = clone_as_averaged_model(model_s, ema)\n    model_s_ema.eval()\n    for batch_idx, (x, y, c, _) in enumerate(synth_loader):\n        if batch_idx == 0:\n            x, c = x.to(device), c.to(device)\n\n            q_0 = Normal(x.new_zeros(x.size()), x.new_ones(x.size()))\n            z = q_0.sample()\n            wav_truth_name = \'{}/{}/{}/generate_{}_{}_truth.wav\'.format(args.sample_path, args.teacher_name,\n                                                                        args.model_name, global_step, batch_idx)\n            librosa.output.write_wav(wav_truth_name, y.squeeze().numpy(), sr=22050)\n            print(\'{} Saved!\'.format(wav_truth_name))\n\n            torch.cuda.synchronize()\n            start_time = time.time()\n            c_up = model_t.upsample(c)\n\n            with torch.no_grad():\n                if args.num_gpu == 1:\n                    y_gen = model_s_ema.generate(z, c_up).squeeze()\n                else:\n                    y_gen = model_s_ema.module.generate(z, c_up).squeeze()\n            torch.cuda.synchronize()\n            print(\'{} seconds\'.format(time.time() - start_time))\n            wav = y_gen.to(torch.device(""cpu"")).data.numpy()\n            wav_name = \'{}/{}/{}/generate_{}_{}.wav\'.format(args.sample_path, args.teacher_name,\n                                                            args.model_name, global_step, batch_idx)\n            librosa.output.write_wav(wav_name, wav, sr=22050)\n            print(\'{} Saved!\'.format(wav_name))\n            del y_gen, wav, x,  y, c, c_up, z, q_0\n    del model_s_ema\n\n\ndef save_checkpoint(model, optimizer, global_step, global_epoch, ema=None):\n    checkpoint_path = os.path.join(args.save, args.teacher_name, args.model_name, ""checkpoint_step{:09d}.pth"".format(global_step))\n    optimizer_state = optimizer.state_dict()\n    torch.save({""state_dict"": model.state_dict(),\n                ""optimizer"": optimizer_state,\n                ""global_step"": global_step,\n                ""global_epoch"": global_epoch}, checkpoint_path)\n    if ema is not None:\n        averaged_model = clone_as_averaged_model(model, ema)\n        checkpoint_path = os.path.join(args.save, args.teacher_name, args.model_name, ""checkpoint_step{:09d}_ema.pth"".format(global_step))\n        torch.save({""state_dict"": averaged_model.state_dict(),\n                    ""optimizer"": optimizer_state,\n                    ""global_step"": global_step,\n                    ""global_epoch"": global_epoch}, checkpoint_path)\n\n\ndef load_checkpoint(step, model_s, optimizer, ema=None):\n    global global_step\n    global global_epoch\n\n    checkpoint_path = os.path.join(args.save, args.teacher_name, args.model_name, ""checkpoint_step{:09d}.pth"".format(step))\n    print(""Load checkpoint from: {}"".format(checkpoint_path))\n    checkpoint = torch.load(checkpoint_path)\n    model_s.load_state_dict(checkpoint[""state_dict""])\n    optimizer.load_state_dict(checkpoint[""optimizer""])\n    global_step = checkpoint[""global_step""]\n    global_epoch = checkpoint[""global_epoch""]\n\n    if ema is not None:\n        checkpoint_path = os.path.join(args.save, args.teacher_name, args.model_name, ""checkpoint_step{:09d}_ema.pth"".format(step))\n        checkpoint = torch.load(checkpoint_path)\n        averaged_model = build_student()\n        averaged_model.to(device)\n        try:\n            averaged_model.load_state_dict(checkpoint[""state_dict""])\n        except RuntimeError:\n            print(""INFO: this model is trained with DataParallel. Creating new state_dict without module..."")\n            state_dict = checkpoint[""state_dict""]\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in state_dict.items():\n                name = k[7:]  # remove `module.`\n                new_state_dict[name] = v\n            averaged_model.load_state_dict(new_state_dict)\n        for name, param in averaged_model.named_parameters():\n            if param.requires_grad:\n                ema.register(name, param.data)\n    return model_s, optimizer, ema\n\n\ndef load_teacher_checkpoint(path, model_t):\n    print(""Load checkpoint from: {}"".format(path))\n    checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n    model_t.load_state_dict(checkpoint[""state_dict""])\n    return model_t\n\n\nteacher_step = args.teacher_load_step\npath = os.path.join(args.load, args.teacher_name, ""checkpoint_step{:09d}_ema.pth"".format(teacher_step))\nmodel_t = build_model()\nmodel_t = load_teacher_checkpoint(path, model_t)\nmodel_s = build_student()\n\nmodel_t.to(device)\nmodel_s.to(device)\nif args.num_gpu > 1:\n    #model_t = torch.nn.DataParallel(model_t)\n    model_s = torch.nn.DataParallel(model_s)\n\noptimizer = optim.Adam(model_s.parameters(), lr=args.learning_rate)\ncriterion_t = KL_Loss()\ncriterion_frame = nn.MSELoss()\n\nema = ExponentialMovingAverage(args.ema_decay)\nfor name, param in model_s.named_parameters():\n    if param.requires_grad:\n        ema.register(name, param.data)\nfor name, param in model_t.named_parameters():\n    if param.requires_grad:\n        param.requires_grad = False\n\nglobal_step, global_epoch = 0, 0\nload_step = args.load_step\n\nlog = open(os.path.join(args.log, \'{}.txt\'.format(args.model_name)), \'w\')\nstate = {k: v for k, v in args._get_kwargs()}\n\nif load_step == 0:\n    list_train_loss, list_loss = [], []\n    log.write(json.dumps(state) + \'\\n\')\n    test_loss = 100.0\nelse:\n    model_s, optimizer, ema = load_checkpoint(load_step, model_s, optimizer, ema)\n    list_train_loss = np.load(\'{}/{}_train.npy\'.format(args.loss, args.model_name)).tolist()\n    list_loss = np.load(\'{}/{}.npy\'.format(args.loss, args.model_name)).tolist()\n    list_train_loss = list_train_loss[:global_epoch]\n    list_loss = list_loss[:global_epoch]\n    test_loss = np.min(list_loss)\n\nfor epoch in range(global_epoch + 1, args.epochs + 1):\n    training_epoch_loss = train(epoch, model_t, model_s, optimizer, ema)\n    with torch.no_grad():\n        test_epoch_loss = evaluate(model_t, model_s, ema)\n\n    state[\'training_loss\'] = training_epoch_loss\n    state[\'eval_loss\'] = test_epoch_loss\n    state[\'epoch\'] = epoch\n    list_train_loss.append(training_epoch_loss)\n    list_loss.append(test_epoch_loss)\n\n    if test_loss > test_epoch_loss:\n        test_loss = test_epoch_loss\n        save_checkpoint(model_s, optimizer, global_step, epoch, ema)\n        print(\'Epoch {} Model Saved! Loss : {:.4f}\'.format(epoch, test_loss))\n        synthesize(model_t, model_s, ema)\n    np.save(\'{}/{}_train.npy\'.format(args.loss, args.model_name), list_train_loss)\n    np.save(\'{}/{}.npy\'.format(args.loss, args.model_name), list_loss)\n\n    log.write(\'%s\\n\' % json.dumps(state))\n    log.flush()\n    print(state)\n\nlog.close()\n'"
wavenet.py,5,"b'import torch\nfrom torch import nn\nfrom modules import Conv, ResBlock\nfrom loss import sample_from_gaussian\nimport time\n\n\nclass Wavenet(nn.Module):\n    def __init__(self, out_channels=1, num_blocks=3, num_layers=10,\n                 residual_channels=512, gate_channels=512, skip_channels=512,\n                 kernel_size=2, cin_channels=128,\n                 upsample_scales=None, causal=True):\n        super(Wavenet, self). __init__()\n\n        self.causal = causal\n        self.num_blocks = num_blocks\n        self.num_layers = num_layers\n        self.out_channels = out_channels\n        self.gate_channels = gate_channels\n        self.residual_channels = residual_channels\n        self.skip_channels = skip_channels\n        self.cin_channels = cin_channels\n        self.kernel_size = kernel_size\n\n        self.front_channels = 32\n        self.front_conv = nn.Sequential(\n            Conv(1, self.residual_channels, self.front_channels, causal=self.causal),\n            nn.ReLU()\n        )\n\n        self.res_blocks = nn.ModuleList()\n        for b in range(self.num_blocks):\n            for n in range(self.num_layers):\n                self.res_blocks.append(ResBlock(self.residual_channels, self.gate_channels, self.skip_channels,\n                                                self.kernel_size, dilation=self.kernel_size**n,\n                                                cin_channels=self.cin_channels, local_conditioning=True,\n                                                causal=self.causal, mode=\'SAME\'))\n\n        self.final_conv = nn.Sequential(\n            nn.ReLU(),\n            Conv(self.skip_channels, self.skip_channels, 1, causal=self.causal),\n            nn.ReLU(),\n            Conv(self.skip_channels, self.out_channels, 1, causal=self.causal)\n        )\n\n        self.upsample_conv = nn.ModuleList()\n        for s in upsample_scales:\n            convt = nn.ConvTranspose2d(1, 1, (3, 2 * s), padding=(1, s // 2), stride=(1, s))\n            convt = nn.utils.weight_norm(convt)\n            nn.init.kaiming_normal_(convt.weight)\n            self.upsample_conv.append(convt)\n            self.upsample_conv.append(nn.LeakyReLU(0.4))\n\n    def forward(self, x, c):\n        c = self.upsample(c)\n        out = self.wavenet(x, c)\n        return out\n\n    def generate(self, num_samples, c=None):\n        rf_size = self.receptive_field_size()\n\n        x_rf = torch.zeros(1, 1, rf_size).to(torch.device(\'cuda\'))\n        x = torch.zeros(1, 1, num_samples + 1).to(torch.device(\'cuda\'))\n\n        c_upsampled = self.upsample(c)\n        local_cond = c_upsampled\n\n        timer = time.perf_counter()\n        torch.cuda.synchronize()\n        for i in range(num_samples):\n            if i % 1000 == 0:\n                torch.cuda.synchronize()\n                timer_end = time.perf_counter()\n                print(""generating {}-th sample: {:.4f} samples per second.."".format(i, 1000/(timer_end - timer)))\n                timer = time.perf_counter()\n            if i >= rf_size:\n                start_idx = i - rf_size + 1\n            else:\n                start_idx = 0\n\n            if local_cond is not None:\n                cond_c = local_cond[:, :, start_idx:i + 1]\n            else:\n                cond_c = None\n            \n            i_rf = min(i, rf_size)\n\n            x_in = x_rf[:, :, -(i_rf+1):]\n\n            out = self.wavenet(x_in, cond_c)\n\n            x[:, :, i + 1] = sample_from_gaussian(out[:, :, -1:])\n            x_rf = self.roll_dim2_and_zerofill_last(x_rf, -1)\n            x_rf[:, :, -1] = x[:, :, i + 1]\n\n        return x[:, :, 1:].cpu()\n\n    def roll_dim2_and_zerofill_last(self, x, n):\n        x_left, x_right = x[:, :, :-n], x[:, :, -n:]\n        x_left.zero_()\n        return torch.cat((x_right, x_left), dim=2)\n\n    def upsample(self, c):\n        if self.upsample_conv is not None:\n            # B x 1 x C x T\'\n            c = c.unsqueeze(1)\n            for f in self.upsample_conv:\n                c = f(c)\n            # B x C x T\n            c = c.squeeze(1)\n        return c\n\n    def wavenet(self, tensor, c=None):\n        h = self.front_conv(tensor)\n        skip = 0\n        for i, f in enumerate(self.res_blocks):\n            h, s = f(h, c)\n            skip += s\n        out = self.final_conv(skip)\n        return out\n\n    def receptive_field_size(self):\n        num_dir = 1 if self.causal else 2\n        dilations = [2 ** (i % self.num_layers) for i in range(self.num_layers * self.num_blocks)]\n        return num_dir * (self.kernel_size - 1) * sum(dilations) + self.front_channels\n'"
wavenet_iaf.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom modules import Conv, ResBlock\n\n\nclass Wavenet_Student(nn.Module):\n    def __init__(self, num_blocks_student=[1, 1, 1, 1, 1, 1], num_layers=10,\n                 front_channels=32, residual_channels=64, gate_channels=128, skip_channels=64,\n                 kernel_size=3, cin_channels=80, causal=True):\n        super(Wavenet_Student, self).__init__()\n        self.num_blocks = num_blocks_student\n        self.num_flow = len(self.num_blocks)\n        self.num_layers = num_layers\n\n        self.iafs = nn.ModuleList()\n        for i in range(self.num_flow):\n            self.iafs.append(Wavenet_Flow(out_channels=2,\n                                          num_blocks=self.num_blocks[i], num_layers=self.num_layers,\n                                          front_channels=front_channels, residual_channels=residual_channels,\n                                          gate_channels=gate_channels, skip_channels=skip_channels,\n                                          kernel_size=kernel_size, cin_channels=cin_channels, causal=causal))\n\n    def forward(self, z, c):\n        return self.iaf(z, c)\n\n    def iaf(self, z, c_up):\n        mu_tot, logs_tot = 0., 0.\n        for i, iaf in enumerate(self.iafs):\n            mu_logs = iaf(z, c_up)\n            mu = mu_logs[:, 0:1, :-1]\n            logs = mu_logs[:, 1:, :-1]\n            mu_tot = mu_tot * torch.exp(logs) + mu\n            logs_tot = logs_tot + logs\n            z = z[:, :, 1:] * torch.exp(logs) + mu\n            z = F.pad(z, pad=(1, 0), mode='constant', value=0)\n        return z, mu_tot, logs_tot\n\n    def receptive_field(self):\n        receptive_field = 1\n        for iaf in self.iafs:\n            receptive_field += iaf.receptive_field_size() - 1\n        return receptive_field\n\n    def generate(self, z, c_up):\n        x, _, _ = self.iaf(z, c_up)\n        return x\n\n    def remove_weight_norm(self):\n        for iaf in self.iafs:\n            iaf.remove_weight_norm()\n\n\nclass Wavenet_Flow(nn.Module):\n    def __init__(self, out_channels=1, num_blocks=1, num_layers=10,\n                 front_channels=32, residual_channels=64, gate_channels=32, skip_channels=None,\n                 kernel_size=3, cin_channels=80, causal=True):\n        super(Wavenet_Flow, self). __init__()\n\n        self.causal = causal\n        self.num_blocks = num_blocks\n        self.num_layers = num_layers\n        self.front_channels = front_channels\n        self.out_channels = out_channels\n        self.gate_channels = gate_channels\n        self.residual_channels = residual_channels\n        self.skip_channels = skip_channels\n        self.cin_channels = cin_channels\n        self.kernel_size = kernel_size\n\n        self.front_conv = nn.Sequential(\n            Conv(1, self.residual_channels, self.front_channels, causal=self.causal),\n            nn.ReLU()\n        )\n        self.res_blocks = nn.ModuleList()\n        self.res_blocks_fast = nn.ModuleList()\n        for b in range(self.num_blocks):\n            for n in range(self.num_layers):\n                self.res_blocks.append(ResBlock(self.residual_channels, self.gate_channels, self.skip_channels,\n                                                self.kernel_size, dilation=2**n,\n                                                cin_channels=self.cin_channels, local_conditioning=True,\n                                                causal=self.causal, mode='SAME'))\n        self.final_conv = nn.Sequential(\n            nn.ReLU(),\n            Conv(self.skip_channels, self.skip_channels, 1, causal=self.causal),\n            nn.ReLU(),\n            Conv(self.skip_channels, self.out_channels, 1, causal=self.causal)\n        )\n\n    def forward(self, x, c):\n        return self.wavenet(x, c)\n\n    def wavenet(self, tensor, c=None):\n        h = self.front_conv(tensor)\n        skip = 0\n        for i, f in enumerate(self.res_blocks):\n            h, s = f(h, c)\n            skip += s\n        out = self.final_conv(skip)\n        return out\n\n    def receptive_field_size(self):\n        num_dir = 1 if self.causal else 2\n        dilations = [2 ** (i % self.num_layers) for i in range(self.num_layers * self.num_blocks)]\n        return num_dir * (self.kernel_size - 1) * sum(dilations) + 1 + (self.front_channels - 1)\n\n    def remove_weight_norm(self):\n        for f in self.res_blocks:\n            f.remove_weight_norm()\n"""
