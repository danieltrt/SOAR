file_path,api_count,code
data.py,5,"b'import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport os\n\nmax_time_steps = 16000\nupsample_conditional_features = True\nhop_length = 256\n\n\nclass LJspeechDataset(Dataset):\n    def __init__(self, data_root, train=True, test_size=0.05):\n        self.data_root = data_root\n        self.lengths = []\n        self.train = train\n        self.test_size = test_size\n\n        self.paths = [self.collect_files(0), self.collect_files(1)]\n\n    def __len__(self):\n        return len(self.paths[0])\n\n    def __getitem__(self, idx):\n        wav = np.load(self.paths[0][idx])\n        mel = np.load(self.paths[1][idx])\n        return wav, mel\n\n    def interest_indices(self, paths):\n        test_num_samples = int(self.test_size * len(paths))\n        train_indices, test_indices = range(0, len(paths) - test_num_samples), \\\n                                      range(len(paths) - test_num_samples, len(paths))\n        return train_indices if self.train else test_indices\n\n    def collect_files(self, col):\n        meta = os.path.join(self.data_root, ""train.txt"")\n        with open(meta, ""rb"") as f:\n            lines = f.readlines()\n        l = lines[0].decode(""utf-8"").split(""|"")\n        assert len(l) == 4\n        self.lengths = list(\n            map(lambda l: int(l.decode(""utf-8"").split(""|"")[2]), lines))\n\n        paths = list(map(lambda l: l.decode(""utf-8"").split(""|"")[col], lines))\n        paths = list(map(lambda f: os.path.join(self.data_root, f), paths))\n\n        # Filter by train/test\n        indices = self.interest_indices(paths)\n        paths = list(np.array(paths)[indices])\n        self.lengths = list(np.array(self.lengths)[indices])\n        self.lengths = list(map(int, self.lengths))\n        return paths\n\n\ndef _pad(seq, max_len, constant_values=0):\n    return np.pad(seq, (0, max_len - len(seq)),\n                  mode=\'constant\', constant_values=constant_values)\n\n\ndef _pad_2d(x, max_len, b_pad=0):\n    x = np.pad(x, [(b_pad, max_len - len(x) - b_pad), (0, 0)],\n               mode=""constant"", constant_values=0)\n    return x\n\n\ndef collate_fn(batch):\n    """"""\n    Create batch\n\n    Args : batch(tuple) : List of tuples / (x, c)  x : list of (T,) c : list of (T, D)\n\n    Returns : Tuple of batch / Network inputs x (B, C, T), Network targets (B, T, 1)\n    """"""\n\n    local_conditioning = len(batch[0]) >= 2\n\n    if local_conditioning:\n        new_batch = []\n        for idx in range(len(batch)):\n            x, c = batch[idx]\n            if upsample_conditional_features:\n                assert len(x) % len(c) == 0 and len(x) // len(c) == hop_length\n\n                max_steps = max_time_steps - max_time_steps % hop_length  # To ensure Divisibility\n\n                if len(x) > max_steps:\n                    max_time_frames = max_steps // hop_length\n                    s = np.random.randint(0, len(c) - max_time_frames)\n                    ts = s * hop_length\n                    x = x[ts:ts + hop_length * max_time_frames]\n                    c = c[s:s + max_time_frames]\n                    assert len(x) % len(c) == 0 and len(x) // len(c) == hop_length\n            else:\n                pass\n            new_batch.append((x, c))\n        batch = new_batch\n    else:\n        pass\n\n    input_lengths = [len(x[0]) for x in batch]\n    max_input_len = max(input_lengths)\n\n    # x_batch : [B, T, 1]\n    x_batch = np.array([_pad_2d(x[0].reshape(-1, 1), max_input_len) for x in batch], dtype=np.float32)\n    assert len(x_batch.shape) == 3\n    if local_conditioning:\n        max_len = max([len(x[1]) for x in batch])\n        c_batch = np.array([_pad_2d(x[1], max_len) for x in batch], dtype=np.float32)\n        assert len(c_batch.shape) == 3\n        # (B x C x T\')\n        c_batch = torch.tensor(c_batch).transpose(1, 2).contiguous()\n        del max_len\n    else:\n        c_batch = None\n\n    # Convert to channel first i.e., (B, C, T) / C = 1\n    x_batch = torch.tensor(x_batch).transpose(1, 2).contiguous()\n    return x_batch, c_batch\n\n\ndef collate_fn_synthesize(batch):\n    """"""\n    Create batch\n\n    Args : batch(tuple) : List of tuples / (x, c)  x : list of (T,) c : list of (T, D)\n\n    Returns : Tuple of batch / Network inputs x (B, C, T), Network targets (B, T, 1)\n    """"""\n\n    local_conditioning = len(batch[0]) >= 2\n\n    if local_conditioning:\n        new_batch = []\n        for idx in range(len(batch)):\n            x, c = batch[idx]\n            if upsample_conditional_features:\n                assert len(x) % len(c) == 0 and len(x) // len(c) == hop_length\n            new_batch.append((x, c))\n        batch = new_batch\n    else:\n        pass\n\n    input_lengths = [len(x[0]) for x in batch]\n    max_input_len = max(input_lengths)\n\n    x_batch = np.array([_pad_2d(x[0].reshape(-1, 1), max_input_len) for x in batch], dtype=np.float32)\n    assert len(x_batch.shape) == 3\n\n    if local_conditioning:\n        max_len = max([len(x[1]) for x in batch])\n        c_batch = np.array([_pad_2d(x[1], max_len) for x in batch], dtype=np.float32)\n        assert len(c_batch.shape) == 3\n        # (B x C x T\')\n        c_batch = torch.tensor(c_batch).transpose(1, 2).contiguous()\n    else:\n        c_batch = None\n\n    # Convert to channel first i.e., (B, C, T) / C = 1\n    x_batch = torch.tensor(x_batch).transpose(1, 2).contiguous()\n    return x_batch, c_batch\n'"
model.py,14,"b'import torch\nfrom torch import nn\nfrom math import log, pi\nfrom modules import Wavenet\nimport math\n\nlogabs = lambda x: torch.log(torch.abs(x))\n\n\nclass ActNorm(nn.Module):\n    def __init__(self, in_channel, logdet=True, pretrained=False):\n        super().__init__()\n\n        self.loc = nn.Parameter(torch.zeros(1, in_channel, 1))\n        self.scale = nn.Parameter(torch.ones(1, in_channel, 1))\n\n        self.initialized = pretrained\n        self.logdet = logdet\n\n    def initialize(self, x):\n        with torch.no_grad():\n            flatten = x.permute(1, 0, 2).contiguous().view(x.shape[1], -1)\n            mean = (\n                flatten.mean(1)\n                .unsqueeze(1)\n                .unsqueeze(2)\n                .permute(1, 0, 2)\n            )\n            std = (\n                flatten.std(1)\n                .unsqueeze(1)\n                .unsqueeze(2)\n                .permute(1, 0, 2)\n            )\n\n            self.loc.data.copy_(-mean)\n            self.scale.data.copy_(1 / (std + 1e-6))\n\n    def forward(self, x):\n        B, _, T = x.size()\n\n        if not self.initialized:\n            self.initialize(x)\n            self.initialized = True\n\n        log_abs = logabs(self.scale)\n\n        logdet = torch.sum(log_abs) * B * T\n\n        if self.logdet:\n            return self.scale * (x + self.loc), logdet\n\n        else:\n            return self.scale * (x + self.loc)\n\n    def reverse(self, output):\n        return output / self.scale - self.loc\n\n\nclass AffineCoupling(nn.Module):\n    def __init__(self, in_channel, cin_channel, filter_size=256, num_layer=6, affine=True):\n        super().__init__()\n\n        self.affine = affine\n        self.net = Wavenet(in_channels=in_channel//2, out_channels=in_channel if self.affine else in_channel//2,\n                           num_blocks=1, num_layers=num_layer, residual_channels=filter_size,\n                           gate_channels=filter_size, skip_channels=filter_size,\n                           kernel_size=3, cin_channels=cin_channel//2, causal=False)\n\n    def forward(self, x, c=None):\n        in_a, in_b = x.chunk(2, 1)\n        c_a, c_b = c.chunk(2, 1)\n\n        if self.affine:\n            log_s, t = self.net(in_a, c_a).chunk(2, 1)\n\n            out_b = (in_b - t) * torch.exp(-log_s)\n            logdet = torch.sum(-log_s)\n        else:\n            net_out = self.net(in_a, c_a)\n            out_b = in_b + net_out\n            logdet = None\n        return torch.cat([in_a, out_b], 1), logdet\n\n    def reverse(self, output, c=None):\n        out_a, out_b = output.chunk(2, 1)\n        c_a, c_b = c.chunk(2, 1)\n\n        if self.affine:\n            log_s, t = self.net(out_a, c_a).chunk(2, 1)\n            in_b = out_b * torch.exp(log_s) + t\n        else:\n            net_out = self.net(out_a, c_a)\n            in_b = out_b - net_out\n\n        return torch.cat([out_a, in_b], 1)\n\n\ndef change_order(x, c=None):\n    x_a, x_b = x.chunk(2, 1)\n    c_a, c_b = c.chunk(2, 1)\n    return torch.cat([x_b, x_a], 1), torch.cat([c_b, c_a], 1)\n\n\nclass Flow(nn.Module):\n    def __init__(self, in_channel, cin_channel, filter_size, num_layer, affine=True, pretrained=False):\n        super().__init__()\n\n        self.actnorm = ActNorm(in_channel, pretrained=pretrained)\n        self.coupling = AffineCoupling(in_channel, cin_channel, filter_size=filter_size,\n                                       num_layer=num_layer, affine=affine)\n\n    def forward(self, x, c=None):\n        out, logdet = self.actnorm(x)\n        out, det = self.coupling(out, c)\n        out, c = change_order(out, c)\n\n        if det is not None:\n            logdet = logdet + det\n\n        return out, c, logdet\n\n    def reverse(self, output, c=None):\n        output, c = change_order(output, c)\n        x = self.coupling.reverse(output, c)\n        x = self.actnorm.reverse(x)\n        return x, c\n\n\ndef gaussian_log_p(x, mean, log_sd):\n    return -0.5 * log(2 * pi) - log_sd - 0.5 * (x - mean) ** 2 / torch.exp(2 * log_sd)\n\n\ndef gaussian_sample(eps, mean, log_sd):\n    return mean + torch.exp(log_sd) * eps\n\n\nclass Block(nn.Module):\n    def __init__(self, in_channel, cin_channel, n_flow, n_layer, affine=True, pretrained=False, split=False):\n        super().__init__()\n\n        self.split = split\n        squeeze_dim = in_channel * 2\n        squeeze_dim_c = cin_channel * 2\n\n        self.flows = nn.ModuleList()\n        for i in range(n_flow):\n            self.flows.append(Flow(squeeze_dim, squeeze_dim_c, filter_size=256, num_layer=n_layer, affine=affine,\n                                   pretrained=pretrained))\n        if self.split:\n            self.prior = Wavenet(in_channels=squeeze_dim // 2, out_channels=squeeze_dim,\n                                 num_blocks=1, num_layers=2, residual_channels=256,\n                                 gate_channels=256, skip_channels=256,\n                                 kernel_size=3, cin_channels=squeeze_dim_c, causal=False)\n\n    def forward(self, x, c):\n        b_size, n_channel, T = x.size()\n        squeezed_x = x.view(b_size, n_channel, T // 2, 2).permute(0, 1, 3, 2)\n        out = squeezed_x.contiguous().view(b_size, n_channel * 2, T // 2)\n        squeezed_c = c.view(b_size, -1, T // 2, 2).permute(0, 1, 3, 2)\n        c = squeezed_c.contiguous().view(b_size, -1, T // 2)\n        logdet, log_p = 0, 0\n\n        for flow in self.flows:\n            out, c, det = flow(out, c)\n            logdet = logdet + det\n        if self.split:\n            out, z = out.chunk(2, 1)\n            # WaveNet prior\n            mean, log_sd = self.prior(out, c).chunk(2, 1)\n            log_p = gaussian_log_p(z, mean, log_sd).sum()\n        return out, c, logdet, log_p\n\n    def reverse(self, output, c, eps=None):\n        if self.split:\n            mean, log_sd = self.prior(output, c).chunk(2, 1)\n            z_new = gaussian_sample(eps, mean, log_sd)\n\n            x = torch.cat([output, z_new], 1)\n        else:\n            x = output\n\n        for flow in self.flows[::-1]:\n            x, c = flow.reverse(x, c)\n\n        b_size, n_channel, T = x.size()\n\n        unsqueezed_x = x.view(b_size, n_channel // 2, 2, T).permute(0, 1, 3, 2)\n        unsqueezed_x = unsqueezed_x.contiguous().view(b_size, n_channel // 2, T * 2)\n        unsqueezed_c = c.view(b_size, -1, 2, T).permute(0, 1, 3, 2)\n        unsqueezed_c = unsqueezed_c.contiguous().view(b_size, -1, T * 2)\n\n        return unsqueezed_x, unsqueezed_c\n\n\nclass Flowavenet(nn.Module):\n    def __init__(self, in_channel, cin_channel, n_block, n_flow, n_layer, affine=True, pretrained=False,\n                 block_per_split=8):\n        super().__init__()\n        self.block_per_split = block_per_split\n\n        self.blocks = nn.ModuleList()\n        self.n_block = n_block\n        for i in range(self.n_block):\n            split = False if (i + 1) % self.block_per_split or i == self.n_block - 1 else True\n            self.blocks.append(Block(in_channel, cin_channel, n_flow, n_layer, affine=affine, \n                                     pretrained=pretrained, split=split))\n            cin_channel *= 2\n            if not split:\n                in_channel *= 2\n\n        self.upsample_conv = nn.ModuleList()\n        for s in [16, 16]:\n            convt = nn.ConvTranspose2d(1, 1, (3, 2 * s), padding=(1, s // 2), stride=(1, s))\n            convt = nn.utils.weight_norm(convt)\n            nn.init.kaiming_normal_(convt.weight)\n            self.upsample_conv.append(convt)\n            self.upsample_conv.append(nn.LeakyReLU(0.4))\n\n    def forward(self, x, c):\n        B, _, T = x.size()\n        logdet, log_p_sum = 0, 0\n        out = x\n        c = self.upsample(c)\n        for block in self.blocks:\n            out, c, logdet_new, logp_new = block(out, c)\n            logdet = logdet + logdet_new\n            log_p_sum = log_p_sum + logp_new\n        log_p_sum += 0.5 * (- log(2.0 * pi) - out.pow(2)).sum()\n        logdet = logdet / (B * T)\n        log_p = log_p_sum / (B * T)\n        return log_p, logdet\n\n    def reverse(self, z, c):\n        _, _, T = z.size()\n        _, _, t_c = c.size()\n        if T != t_c:\n            c = self.upsample(c)\n        z_list = []\n        x = z\n        for i in range(self.n_block):\n            b_size, _, T = x.size()\n            squeezed_x = x.view(b_size, -1, T // 2, 2).permute(0, 1, 3, 2)\n            x = squeezed_x.contiguous().view(b_size, -1, T // 2)\n            squeezed_c = c.view(b_size, -1, T // 2, 2).permute(0, 1, 3, 2)\n            c = squeezed_c.contiguous().view(b_size, -1, T // 2)\n            if not ((i + 1) % self.block_per_split or i == self.n_block - 1):\n                x, z = x.chunk(2, 1)\n                z_list.append(z)\n\n        for i, block in enumerate(self.blocks[::-1]):\n            index = self.n_block - i\n            if not (index % self.block_per_split or index == self.n_block):\n                x, c = block.reverse(x, c, z_list[index // self.block_per_split - 1])\n            else:\n                x, c = block.reverse(x, c)\n        return x\n\n    def upsample(self, c):\n        c = c.unsqueeze(1)\n        for f in self.upsample_conv:\n            c = f(c)\n        c = c.squeeze(1)\n        return c\n'"
modules.py,4,"b'import torch\nimport torch.nn as nn\nimport math\n\n\nclass Conv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, causal=True):\n        super(Conv, self).__init__()\n\n        self.causal = causal\n        if self.causal:\n            self.padding = dilation * (kernel_size - 1)\n        else:\n            self.padding = dilation * (kernel_size - 1) // 2\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation, padding=self.padding)\n        self.conv = nn.utils.weight_norm(self.conv)\n        nn.init.kaiming_normal_(self.conv.weight)\n\n    def forward(self, tensor):\n        out = self.conv(tensor)\n        if self.causal and self.padding is not 0:\n            out = out[:, :, :-self.padding]\n        return out\n\n\nclass ZeroConv1d(nn.Module):\n    def __init__(self, in_channel, out_channel):\n        super().__init__()\n\n        self.conv = nn.Conv1d(in_channel, out_channel, 1, padding=0)\n        self.conv.weight.data.zero_()\n        self.conv.bias.data.zero_()\n        self.scale = nn.Parameter(torch.zeros(1, out_channel, 1))\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = out * torch.exp(self.scale * 3)\n        return out\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, skip_channels, kernel_size, dilation,\n                 cin_channels=None, local_conditioning=True, causal=False):\n        super(ResBlock, self).__init__()\n        self.causal = causal\n        self.local_conditioning = local_conditioning\n        self.cin_channels = cin_channels\n        self.skip = True if skip_channels is not None else False\n\n        self.filter_conv = Conv(in_channels, out_channels, kernel_size, dilation, causal)\n        self.gate_conv = Conv(in_channels, out_channels, kernel_size, dilation, causal)\n        self.res_conv = nn.Conv1d(out_channels, in_channels, kernel_size=1)\n        self.res_conv = nn.utils.weight_norm(self.res_conv)\n        nn.init.kaiming_normal_(self.res_conv.weight)\n        if self.skip:\n            self.skip_conv = nn.Conv1d(out_channels, skip_channels, kernel_size=1)\n            self.skip_conv = nn.utils.weight_norm(self.skip_conv)\n            nn.init.kaiming_normal_(self.skip_conv.weight)\n\n        if self.local_conditioning:\n            self.filter_conv_c = nn.Conv1d(cin_channels, out_channels, kernel_size=1)\n            self.gate_conv_c = nn.Conv1d(cin_channels, out_channels, kernel_size=1)\n            self.filter_conv_c = nn.utils.weight_norm(self.filter_conv_c)\n            self.gate_conv_c = nn.utils.weight_norm(self.gate_conv_c)\n            nn.init.kaiming_normal_(self.filter_conv_c.weight)\n            nn.init.kaiming_normal_(self.gate_conv_c.weight)\n\n    def forward(self, tensor, c=None):\n        h_filter = self.filter_conv(tensor)\n        h_gate = self.gate_conv(tensor)\n\n        if self.local_conditioning:\n            h_filter += self.filter_conv_c(c)\n            h_gate += self.gate_conv_c(c)\n\n        out = torch.tanh(h_filter) * torch.sigmoid(h_gate)\n\n        res = self.res_conv(out)\n        skip = self.skip_conv(out) if self.skip else None\n        return (tensor + res) * math.sqrt(0.5), skip\n\n\nclass Wavenet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=2, num_blocks=1, num_layers=6,\n                 residual_channels=256, gate_channels=256, skip_channels=256,\n                 kernel_size=3, cin_channels=80, causal=True):\n        super(Wavenet, self).__init__()\n\n        self.skip = True if skip_channels is not None else False\n        self.front_conv = nn.Sequential(\n            Conv(in_channels, residual_channels, 3, causal=causal),\n            nn.ReLU()\n        )\n\n        self.res_blocks = nn.ModuleList()\n        for b in range(num_blocks):\n            for n in range(num_layers):\n                self.res_blocks.append(ResBlock(residual_channels, gate_channels, skip_channels,\n                                                kernel_size, dilation=2**n,\n                                                cin_channels=cin_channels, local_conditioning=True,\n                                                causal=causal))\n\n        last_channels = skip_channels if self.skip else residual_channels\n        self.final_conv = nn.Sequential(\n            nn.ReLU(),\n            Conv(last_channels, last_channels, 1, causal=causal),\n            nn.ReLU(),\n            ZeroConv1d(last_channels, out_channels)\n        )\n\n    def forward(self, x, c=None):\n        h = self.front_conv(x)\n        skip = 0\n        for i, f in enumerate(self.res_blocks):\n            if self.skip:\n                h, s = f(h, c)\n                skip += s\n            else:\n                h, _ = f(h, c)\n        if self.skip:\n            out = self.final_conv(skip)\n        else:\n            out = self.final_conv(h)\n        return out\n'"
preprocessing.py,0,"b'from concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport numpy as np\nimport os\nimport librosa\nfrom multiprocessing import cpu_count\nimport argparse\n\n\ndef build_from_path(in_dir, out_dir, num_workers=1):\n    executor = ProcessPoolExecutor(max_workers=num_workers)\n    futures = []\n    index = 1\n    with open(os.path.join(in_dir, \'metadata.csv\'), encoding=\'utf-8\') as f:\n        for line in f:\n            parts = line.strip().split(\'|\')\n            wav_path = os.path.join(in_dir, \'wavs\', \'%s.wav\' % parts[0])\n            text = parts[2]\n            futures.append(executor.submit(\n                partial(_process_utterance, out_dir, index, wav_path, text)))\n            index += 1\n    return [future.result() for future in futures]\n\n\ndef _process_utterance(out_dir, index, wav_path, text):\n    # Load the audio to a numpy array:\n    wav, sr = librosa.load(wav_path, sr=22050)\n\n    wav = wav / np.abs(wav).max() * 0.999\n    out = wav\n    constant_values = 0.0\n    out_dtype = np.float32\n    n_fft = 1024\n    hop_length = 256\n    reference = 20.0\n    min_db = -100\n\n    # Compute a mel-scale spectrogram from the trimmed wav:\n    # (N, D)\n    mel_spectrogram = librosa.feature.melspectrogram(wav, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=80,\n                                                     fmin=125, fmax=7600).T\n\n    # mel_spectrogram = np.round(mel_spectrogram, decimals=2)\n    mel_spectrogram = 20 * np.log10(np.maximum(1e-4, mel_spectrogram)) - reference\n    mel_spectrogram = np.clip((mel_spectrogram - min_db) / (-min_db), 0, 1)\n\n    pad = (out.shape[0] // hop_length + 1) * hop_length - out.shape[0]\n    pad_l = pad // 2\n    pad_r = pad // 2 + pad % 2\n\n    # zero pad for quantized signal\n    out = np.pad(out, (pad_l, pad_r), mode=""constant"", constant_values=constant_values)\n    N = mel_spectrogram.shape[0]\n    assert len(out) >= N * hop_length\n\n    # time resolution adjustment\n    # ensure length of raw audio is multiple of hop_size so that we can use\n    # transposed convolution to upsample\n    out = out[:N * hop_length]\n    assert len(out) % hop_length == 0\n\n    timesteps = len(out)\n\n    # Write the spectrograms to disk:\n    audio_filename = \'ljspeech-audio-%05d.npy\' % index\n    mel_filename = \'ljspeech-mel-%05d.npy\' % index\n    np.save(os.path.join(out_dir, audio_filename),\n            out.astype(out_dtype), allow_pickle=False)\n    np.save(os.path.join(out_dir, mel_filename),\n            mel_spectrogram.astype(np.float32), allow_pickle=False)\n\n    # Return a tuple describing this training example:\n    return audio_filename, mel_filename, timesteps, text\n\n\ndef preprocess(in_dir, out_dir, num_workers):\n    os.makedirs(out_dir, exist_ok=True)\n    metadata = build_from_path(in_dir, out_dir, num_workers)\n    write_metadata(metadata, out_dir)\n\n\ndef write_metadata(metadata, out_dir):\n    with open(os.path.join(out_dir, \'train.txt\'), \'w\', encoding=\'utf-8\') as f:\n        for m in metadata:\n            f.write(\'|\'.join([str(x) for x in m]) + \'\\n\')\n    frames = sum([m[2] for m in metadata])\n    sr = 22050\n    hours = frames / sr / 3600\n    print(\'Wrote %d utterances, %d time steps (%.2f hours)\' % (len(metadata), frames, hours))\n    print(\'Max input length:  %d\' % max(len(m[3]) for m in metadata))\n    print(\'Max output length: %d\' % max(m[2] for m in metadata))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Preprocessing\',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--in_dir\', \'-i\', type=str, default=\'./\', help=\'In Directory\')\n    parser.add_argument(\'--out_dir\', \'-o\', type=str, default=\'./\', help=\'Out Directory\')\n    args = parser.parse_args()\n\n    num_workers = cpu_count()\n    preprocess(args.in_dir, args.out_dir, num_workers)\n'"
synthesize.py,12,"b'import torch\r\nfrom torch.utils.data import DataLoader\r\nfrom data import LJspeechDataset, collate_fn_synthesize\r\nfrom model import Flowavenet\r\nfrom torch.distributions.normal import Normal\r\nimport numpy as np\r\nimport librosa\r\nimport os\r\nimport argparse\r\nimport time\r\n\r\ntorch.backends.cudnn.benchmark = False\r\nnp.set_printoptions(precision=4)\r\nparser = argparse.ArgumentParser(description=\'Train FloWaveNet of LJSpeech\',\r\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\nparser.add_argument(\'--data_path\', type=str, default=\'./DATASETS/ljspeech/\', help=\'Dataset Path\')\r\nparser.add_argument(\'--sample_path\', type=str, default=\'./samples\', help=\'Sample Path\')\r\nparser.add_argument(\'--model_name\', type=str, default=\'flowavenet\', help=\'Model Name\')\r\nparser.add_argument(\'--num_samples\', type=int, default=10, help=\'# of audio samples\')\r\nparser.add_argument(\'--load_step\', type=int, default=0, help=\'Load Step\')\r\nparser.add_argument(\'--temp\', type=float, default=0.8, help=\'Temperature\')\r\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./params\', help=\'Checkpoint path to resume / test.\')\r\nparser.add_argument(\'--n_layer\', type=int, default=2, help=\'Number of layers\')\r\nparser.add_argument(\'--n_flow\', type=int, default=6, help=\'Number of layers\')\r\nparser.add_argument(\'--n_block\', type=int, default=8, help=\'Number of layers\')\r\nparser.add_argument(\'--cin_channels\', type=int, default=80, help=\'Cin Channels\')\r\nparser.add_argument(\'--block_per_split\', type=int, default=4, help=\'Block per split\')\r\nparser.add_argument(\'--num_workers\', type=int, default=0, help=\'Number of workers\')\r\nparser.add_argument(\'--log\', type=str, default=\'./log\', help=\'Log folder.\')\r\nargs = parser.parse_args()\r\n\r\nif not os.path.isdir(args.sample_path):\r\n    os.makedirs(args.sample_path)\r\nif not os.path.isdir(os.path.join(args.sample_path, args.model_name)):\r\n    os.makedirs(os.path.join(args.sample_path, args.model_name))\r\n\r\nuse_cuda = torch.cuda.is_available()\r\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\r\n\r\n# LOAD DATASETS\r\ntest_dataset = LJspeechDataset(args.data_path, False, 0.1)\r\nsynth_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn_synthesize,\r\n                          num_workers=args.num_workers, pin_memory=True)\r\n\r\n\r\ndef build_model():\r\n    model = Flowavenet(in_channel=1,\r\n                       cin_channel=args.cin_channels,\r\n                       n_block=args.n_block,\r\n                       n_flow=args.n_flow,\r\n                       n_layer=args.n_layer,\r\n                       affine=True,\r\n                       pretrained=True,\r\n                       block_per_split=args.block_per_split)\r\n    return model\r\n\r\n\r\ndef synthesize(model):\r\n    global global_step\r\n    for batch_idx, (x, c) in enumerate(synth_loader):\r\n        if batch_idx < args.num_samples:\r\n            x, c = x.to(device), c.to(device)\r\n\r\n            q_0 = Normal(x.new_zeros(x.size()), x.new_ones(x.size()))\r\n            z = q_0.sample() * args.temp\r\n            torch.cuda.synchronize()\r\n            start_time = time.time()\r\n\r\n            with torch.no_grad():\r\n                y_gen = model.reverse(z, c).squeeze()\r\n            torch.cuda.synchronize()\r\n            print(\'{} seconds\'.format(time.time() - start_time))\r\n            wav = y_gen.to(torch.device(""cpu"")).data.numpy()\r\n            wav_name = \'{}/{}/generate_{}_{}_{}.wav\'.format(args.sample_path, args.model_name,\r\n                                                            global_step, batch_idx, args.temp)\r\n            librosa.output.write_wav(wav_name, wav, sr=22050)\r\n            print(\'{} Saved!\'.format(wav_name))\r\n\r\n\r\ndef load_checkpoint(step, model):\r\n    checkpoint_path = os.path.join(args.load, args.model_name, ""checkpoint_step{:09d}.pth"".format(step))\r\n    print(""Load checkpoint from: {}"".format(checkpoint_path))\r\n    checkpoint = torch.load(checkpoint_path)\r\n    # generalized load procedure for both single-gpu and DataParallel models\r\n    # https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/3\r\n    try:\r\n        model.load_state_dict(checkpoint[""state_dict""])\r\n    except RuntimeError:\r\n        print(""INFO: this model is trained with DataParallel. Creating new state_dict without module..."")\r\n        state_dict = checkpoint[""state_dict""]\r\n        from collections import OrderedDict\r\n        new_state_dict = OrderedDict()\r\n        for k, v in state_dict.items():\r\n            name = k[7:]  # remove `module.`\r\n            new_state_dict[name] = v\r\n        model.load_state_dict(new_state_dict)\r\n    return model\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    step = args.load_step\r\n    global_step = step\r\n    model = build_model()\r\n    model = load_checkpoint(step, model)\r\n    model = model.to(device)\r\n    model.eval()\r\n\r\n    with torch.no_grad():\r\n        synthesize(model)\r\n'"
train.py,18,"b'import torch\r\nfrom torch import optim\r\nimport torch.nn as nn\r\nfrom torch.utils.data import DataLoader\r\nfrom data import LJspeechDataset, collate_fn, collate_fn_synthesize\r\nfrom model import Flowavenet\r\nfrom torch.distributions.normal import Normal\r\nimport numpy as np\r\nimport librosa\r\nimport os\r\nimport argparse\r\nimport time\r\nimport json\r\nimport gc\r\n\r\ntorch.backends.cudnn.benchmark = True\r\nnp.set_printoptions(precision=4)\r\ntorch.manual_seed(1111)\r\n\r\nparser = argparse.ArgumentParser(description=\'Train FloWaveNet of LJSpeech\',\r\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\nparser.add_argument(\'--data_path\', type=str, default=\'./DATASETS/ljspeech/\', help=\'Dataset Path\')\r\nparser.add_argument(\'--sample_path\', type=str, default=\'./samples\', help=\'Sample Path\')\r\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./params\', help=\'Folder to save checkpoints.\')\r\nparser.add_argument(\'--load\', \'-l\', type=str, default=\'./params\', help=\'Checkpoint path\')\r\nparser.add_argument(\'--log\', type=str, default=\'./log\', help=\'Log folder.\')\r\nparser.add_argument(\'--model_name\', type=str, default=\'flowavenet\', help=\'Model Name\')\r\nparser.add_argument(\'--load_step\', type=int, default=0, help=\'Load Step\')\r\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=5000, help=\'Number of epochs to train.\')\r\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=2, help=\'Batch size.\')\r\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.001, help=\'The Learning Rate.\')\r\nparser.add_argument(\'--loss\', type=str, default=\'./loss\', help=\'Folder to save loss\')\r\nparser.add_argument(\'--n_layer\', type=int, default=2, help=\'Number of layers\')\r\nparser.add_argument(\'--n_flow\', type=int, default=6, help=\'Number of layers\')\r\nparser.add_argument(\'--n_block\', type=int, default=8, help=\'Number of layers\')\r\nparser.add_argument(\'--cin_channels\', type=int, default=80, help=\'Cin Channels\')\r\nparser.add_argument(\'--block_per_split\', type=int, default=4, help=\'Block per split\')\r\nparser.add_argument(\'--num_workers\', type=int, default=2, help=\'Number of workers\')\r\nparser.add_argument(\'--num_gpu\', type=int, default=1, help=\'Number of GPUs to use. >1 uses DataParallel\')\r\nargs = parser.parse_args()\r\n\r\n# Init logger\r\nif not os.path.isdir(args.log):\r\n    os.makedirs(args.log)\r\n\r\n# Checkpoint dir\r\nif not os.path.isdir(args.save):\r\n    os.makedirs(args.save)\r\nif not os.path.isdir(args.loss):\r\n    os.makedirs(args.loss)\r\nif not os.path.isdir(args.sample_path):\r\n    os.makedirs(args.sample_path)\r\nif not os.path.isdir(os.path.join(args.sample_path, args.model_name)):\r\n    os.makedirs(os.path.join(args.sample_path, args.model_name))\r\nif not os.path.isdir(os.path.join(args.save, args.model_name)):\r\n    os.makedirs(os.path.join(args.save, args.model_name))\r\n\r\nuse_cuda = torch.cuda.is_available()\r\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\r\n\r\n# LOAD DATASETS\r\ntrain_dataset = LJspeechDataset(args.data_path, True, 0.1)\r\ntest_dataset = LJspeechDataset(args.data_path, False, 0.1)\r\ntrain_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn,\r\n                          num_workers=args.num_workers, pin_memory=True)\r\ntest_loader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=collate_fn,\r\n                         num_workers=args.num_workers, pin_memory=True)\r\nsynth_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn_synthesize,\r\n                          num_workers=args.num_workers, pin_memory=True)\r\n\r\n\r\ndef build_model():\r\n    pretrained = True if args.load_step > 0 else False\r\n    model = Flowavenet(in_channel=1,\r\n                       cin_channel=args.cin_channels,\r\n                       n_block=args.n_block,\r\n                       n_flow=args.n_flow,\r\n                       n_layer=args.n_layer,\r\n                       affine=True,\r\n                       pretrained=pretrained,\r\n                       block_per_split=args.block_per_split)\r\n    return model\r\n\r\n\r\ndef train(epoch, model, optimizer, scheduler):\r\n    global global_step\r\n    epoch_loss = 0.0\r\n    running_loss = [0., 0., 0.]\r\n    model.train()\r\n    display_step = 100\r\n    for batch_idx, (x, c) in enumerate(train_loader):\r\n        scheduler.step()\r\n        global_step += 1\r\n\r\n        x, c = x.to(device), c.to(device)\r\n\r\n        optimizer.zero_grad()\r\n        log_p, logdet = model(x, c)\r\n        log_p, logdet = torch.mean(log_p), torch.mean(logdet)\r\n\r\n        loss = -(log_p + logdet)\r\n        loss.backward()\r\n\r\n        nn.utils.clip_grad_norm_(model.parameters(), 1.)\r\n        optimizer.step()\r\n\r\n        running_loss[0] += loss.item() / display_step\r\n        running_loss[1] += log_p.item() / display_step\r\n        running_loss[2] += logdet.item() / display_step\r\n\r\n        epoch_loss += loss.item()\r\n        if (batch_idx + 1) % display_step == 0:\r\n            print(\'Global Step : {}, [{}, {}] [Log pdf, Log p(z), Log Det] : {}\'\r\n                  .format(global_step, epoch, batch_idx + 1, np.array(running_loss)))\r\n            running_loss = [0., 0., 0.]\r\n        del x, c, log_p, logdet, loss\r\n    del running_loss\r\n    gc.collect()\r\n    print(\'{} Epoch Training Loss : {:.4f}\'.format(epoch, epoch_loss / (len(train_loader))))\r\n    return epoch_loss / len(train_loader)\r\n\r\n\r\ndef evaluate(model):\r\n    model.eval()\r\n    running_loss = [0., 0., 0.]\r\n    epoch_loss = 0.\r\n    display_step = 100\r\n    for batch_idx, (x, c) in enumerate(test_loader):\r\n        x, c = x.to(device), c.to(device)\r\n        log_p, logdet = model(x, c)\r\n        log_p, logdet = torch.mean(log_p), torch.mean(logdet)\r\n        loss = -(log_p + logdet)\r\n\r\n        running_loss[0] += loss.item() / display_step\r\n        running_loss[1] += log_p.item() / display_step\r\n        running_loss[2] += logdet.item() / display_step\r\n        epoch_loss += loss.item()\r\n\r\n        if (batch_idx + 1) % 100 == 0:\r\n            print(\'Global Step : {}, [{}, {}] [Log pdf, Log p(z), Log Det] : {}\'\r\n                  .format(global_step, epoch, batch_idx + 1, np.array(running_loss)))\r\n            running_loss = [0., 0., 0.]\r\n        del x, c, log_p, logdet, loss\r\n    del running_loss\r\n    epoch_loss /= len(test_loader)\r\n    print(\'Evaluation Loss : {:.4f}\'.format(epoch_loss))\r\n    return epoch_loss\r\n\r\n\r\ndef synthesize(model):\r\n    global global_step\r\n    model.eval()\r\n    for batch_idx, (x, c) in enumerate(synth_loader):\r\n        if batch_idx == 0:\r\n            x, c = x.to(device), c.to(device)\r\n\r\n            q_0 = Normal(x.new_zeros(x.size()), x.new_ones(x.size()))\r\n            z = q_0.sample()\r\n\r\n            start_time = time.time()\r\n            with torch.no_grad():\r\n                if args.num_gpu == 1:\r\n                    y_gen = model.reverse(z, c).squeeze()\r\n                else:\r\n                    y_gen = model.module.reverse(z, c).squeeze()\r\n            wav = y_gen.to(torch.device(""cpu"")).data.numpy()\r\n            wav_name = \'{}/{}/generate_{}_{}.wav\'.format(args.sample_path, args.model_name, global_step, batch_idx)\r\n            print(\'{} seconds\'.format(time.time() - start_time))\r\n            librosa.output.write_wav(wav_name, wav, sr=22050)\r\n            print(\'{} Saved!\'.format(wav_name))\r\n            del x, c, z, q_0, y_gen, wav\r\n\r\n\r\ndef save_checkpoint(model, optimizer, scheduler, global_step, global_epoch):\r\n    checkpoint_path = os.path.join(args.save, args.model_name, ""checkpoint_step{:09d}.pth"".format(global_step))\r\n    optimizer_state = optimizer.state_dict()\r\n    scheduler_state = scheduler.state_dict()\r\n    torch.save({""state_dict"": model.state_dict(),\r\n                ""optimizer"": optimizer_state,\r\n                ""scheduler"": scheduler_state,\r\n                ""global_step"": global_step,\r\n                ""global_epoch"": global_epoch}, checkpoint_path)\r\n\r\n\r\ndef load_checkpoint(step, model, optimizer, scheduler):\r\n    global global_step\r\n    global global_epoch\r\n\r\n    checkpoint_path = os.path.join(args.save, args.model_name, ""checkpoint_step{:09d}.pth"".format(step))\r\n    print(""Load checkpoint from: {}"".format(checkpoint_path))\r\n    checkpoint = torch.load(checkpoint_path)\r\n\r\n    # generalized load procedure for both single-gpu and DataParallel models\r\n    # https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/3\r\n    try:\r\n        model.load_state_dict(checkpoint[""state_dict""])\r\n    except RuntimeError:\r\n        print(""INFO: this model is trained with DataParallel. Creating new state_dict without module..."")\r\n        state_dict = checkpoint[""state_dict""]\r\n        from collections import OrderedDict\r\n        new_state_dict = OrderedDict()\r\n        for k, v in state_dict.items():\r\n            name = k[7:]  # remove `module.`\r\n            new_state_dict[name] = v\r\n        model.load_state_dict(new_state_dict)\r\n\r\n    optimizer.load_state_dict(checkpoint[""optimizer""])\r\n    scheduler.load_state_dict(checkpoint[""scheduler""])\r\n    global_step = checkpoint[""global_step""]\r\n    global_epoch = checkpoint[""global_epoch""]\r\n\r\n    return model, optimizer, scheduler\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    model = build_model()\r\n    model.to(device)\r\n\r\n    pretrained = True if args.load_step > 0 else False\r\n    if pretrained is False:\r\n        # do ActNorm initialization first (if model.pretrained is True, this does nothing so no worries)\r\n        x_seed, c_seed = next(iter(train_loader))\r\n        x_seed, c_seed = x_seed.to(device), c_seed.to(device)\r\n        with torch.no_grad():\r\n            _, _ = model(x_seed, c_seed)\r\n        del x_seed, c_seed, _\r\n    # then convert the model to DataParallel later (since ActNorm init from the DataParallel is wacky)\r\n\r\n    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\r\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200000, gamma=0.5)\r\n    criterion_frame = nn.MSELoss()\r\n\r\n    global_step = 0\r\n    global_epoch = 0\r\n    load_step = args.load_step\r\n\r\n    log = open(os.path.join(args.log, \'{}.txt\'.format(args.model_name)), \'w\')\r\n    state = {k: v for k, v in args._get_kwargs()}\r\n\r\n    if load_step == 0:\r\n        list_train_loss, list_loss = [], []\r\n        log.write(json.dumps(state) + \'\\n\')\r\n        test_loss = 100.0\r\n    else:\r\n        model, optimizer, scheduler = load_checkpoint(load_step, model, optimizer, scheduler)\r\n        list_train_loss = np.load(\'{}/{}_train.npy\'.format(args.loss, args.model_name)).tolist()\r\n        list_loss = np.load(\'{}/{}.npy\'.format(args.loss, args.model_name)).tolist()\r\n        list_train_loss = list_train_loss[:global_epoch]\r\n        list_loss = list_loss[:global_epoch]\r\n        test_loss = np.min(list_loss)\r\n\r\n    if args.num_gpu > 1:\r\n        print(""num_gpu > 1 detected. converting the model to DataParallel..."")\r\n        model = torch.nn.DataParallel(model)\r\n\r\n    for epoch in range(global_epoch + 1, args.epochs + 1):\r\n        training_epoch_loss = train(epoch, model, optimizer, scheduler)\r\n        with torch.no_grad():\r\n            test_epoch_loss = evaluate(model)\r\n\r\n        state[\'training_loss\'] = training_epoch_loss\r\n        state[\'eval_loss\'] = test_epoch_loss\r\n        state[\'epoch\'] = epoch\r\n        list_train_loss.append(training_epoch_loss)\r\n        list_loss.append(test_epoch_loss)\r\n\r\n        if test_loss > test_epoch_loss:\r\n            test_loss = test_epoch_loss\r\n            save_checkpoint(model, optimizer, scheduler, global_step, epoch)\r\n            print(\'Epoch {} Model Saved! Loss : {:.4f}\'.format(epoch, test_loss))\r\n            with torch.no_grad():\r\n                synthesize(model)\r\n        np.save(\'{}/{}_train.npy\'.format(args.loss, args.model_name), list_train_loss)\r\n        np.save(\'{}/{}.npy\'.format(args.loss, args.model_name), list_loss)\r\n\r\n        log.write(\'%s\\n\' % json.dumps(state))\r\n        log.flush()\r\n        print(state)\r\n        gc.collect()\r\n\r\n    log.close()\r\n'"
train_apex.py,24,"b'import torch\r\nfrom torch import optim\r\nimport torch.nn as nn\r\nfrom torch.utils.data import DataLoader\r\nfrom data import LJspeechDataset, collate_fn, collate_fn_synthesize\r\nfrom model import Flowavenet\r\nfrom torch.distributions.normal import Normal\r\nimport numpy as np\r\nimport librosa\r\nimport argparse\r\nimport time\r\nimport json\r\nimport gc\r\nimport os\r\nfrom tqdm import tqdm\r\nfrom apex import amp\r\nfrom apex.parallel import DistributedDataParallel\r\n\r\n# Distributed Training implemented with Apex utilities https://github.com/NVIDIA/apex,\r\n# which handle some issues with specific nodes in the FloWaveNet architecture.\r\n\r\n# List of changes made in train.py:\r\n# 1. Determine local_rank and world_size for torch.distributed.init_process_group\r\n# 2. Set a current device with torch.cuda.set_device\r\n# 3. Wrap dataset with torch.utils.data.distributed.DistributedSampler\r\n# 4. Apply amp.scale_loss at each backward pass\r\n# 5. Clip gradient with amp.master_params\r\n# 6. Divide step_size by world_size (not sure if this is necessary)\r\n# 7. Initialize model and optimizer with amp.initialize\r\n# 8. Wrap model with apex.parallel.DistributedDataParallel\r\n# 9. Handle evaluation and messages on the first node using args.local_rank\r\n\r\n# For example, to run on 4 GPUs, use the following command:\r\n# python -m torch.distributed.launch --nproc_per_node=4 train_apex.py --num_workers 2 --epochs 1000\r\n\r\ntorch.backends.cudnn.benchmark = True\r\nnp.set_printoptions(precision=4)\r\ntorch.manual_seed(1111)\r\n\r\nparser = argparse.ArgumentParser(description=\'Train FloWaveNet of LJSpeech on multiple GPUs with Apex\',\r\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\nparser.add_argument(\'--local_rank\', default=0, type=int)\r\nparser.add_argument(\'--data_path\', type=str, default=\'./DATASETS/ljspeech/\', help=\'Dataset Path\')\r\nparser.add_argument(\'--sample_path\', type=str, default=\'./samples\', help=\'Sample Path\')\r\nparser.add_argument(\'--save\', \'-s\', type=str, default=\'./params\', help=\'Folder to save checkpoints.\')\r\nparser.add_argument(\'--load_step\', \'-l\', type=int, default=0, help=\'Load Step\')\r\nparser.add_argument(\'--log\', type=str, default=\'./log\', help=\'Log folder.\')\r\nparser.add_argument(\'--model_name\', type=str, default=\'flowavenet\', help=\'Model Name\')\r\nparser.add_argument(\'--epochs\', \'-e\', type=int, default=5000, help=\'Number of epochs to train.\')\r\nparser.add_argument(\'--batch_size\', \'-b\', type=int, default=2, help=\'Batch size.\')\r\nparser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.001, help=\'The Learning Rate.\')\r\nparser.add_argument(\'--loss\', type=str, default=\'./loss\', help=\'Folder to save loss\')\r\nparser.add_argument(\'--n_layer\', type=int, default=2, help=\'Number of layers\')\r\nparser.add_argument(\'--n_flow\', type=int, default=6, help=\'Number of layers\')\r\nparser.add_argument(\'--n_block\', type=int, default=8, help=\'Number of layers\')\r\nparser.add_argument(\'--cin_channels\', type=int, default=80, help=\'Cin Channels\')\r\nparser.add_argument(\'--block_per_split\', type=int, default=4, help=\'Block per split\')\r\nparser.add_argument(\'--num_workers\', type=int, default=2, help=\'Number of workers\')\r\nargs = parser.parse_args()\r\n\r\ncurrent_env = os.environ.copy()\r\nworld_size = int(current_env[\'WORLD_SIZE\'])\r\n\r\ntorch.distributed.init_process_group(backend=\'nccl\', world_size=world_size, rank=args.local_rank)\r\ntorch.cuda.set_device(args.local_rank)\r\n\r\nif args.local_rank == 0:\r\n    # Init logger\r\n    if not os.path.isdir(args.log):\r\n        os.makedirs(args.log)\r\n    # Checkpoint dir\r\n    if not os.path.isdir(args.save):\r\n        os.makedirs(args.save)\r\n    if not os.path.isdir(args.loss):\r\n        os.makedirs(args.loss)\r\n    if not os.path.isdir(args.sample_path):\r\n        os.makedirs(args.sample_path)\r\n    if not os.path.isdir(os.path.join(args.sample_path, args.model_name)):\r\n        os.makedirs(os.path.join(args.sample_path, args.model_name))\r\n    if not os.path.isdir(os.path.join(args.save, args.model_name)):\r\n        os.makedirs(os.path.join(args.save, args.model_name))\r\n\r\nuse_cuda = torch.cuda.is_available()\r\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\r\n\r\n# LOAD DATASETS\r\ntrain_dataset = LJspeechDataset(args.data_path, True, 0.1)\r\ntest_dataset = LJspeechDataset(args.data_path, False, 0.1)\r\n\r\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\r\n\r\ntrain_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=train_sampler, drop_last=True, collate_fn=collate_fn,\r\n                          num_workers=args.num_workers, pin_memory=True)\r\ntest_loader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=collate_fn,\r\n                         num_workers=args.num_workers, pin_memory=True)\r\nsynth_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn_synthesize,\r\n                          num_workers=args.num_workers, pin_memory=True)\r\n\r\n\r\ndef build_model():\r\n    pretrained = True if args.load_step > 0 else False\r\n    model = Flowavenet(in_channel=1,\r\n                       cin_channel=args.cin_channels,\r\n                       n_block=args.n_block,\r\n                       n_flow=args.n_flow,\r\n                       n_layer=args.n_layer,\r\n                       affine=True,\r\n                       pretrained=pretrained,\r\n                       block_per_split=args.block_per_split)\r\n    return model\r\n\r\n\r\ndef train(epoch, model, optimizer, scheduler):\r\n    global global_step\r\n\r\n    epoch_loss = 0.0\r\n    running_num = 0\r\n    running_loss = np.zeros(3)\r\n\r\n    train_sampler.set_epoch(epoch)\r\n    model.train()\r\n\r\n    bar = tqdm(train_loader) if args.local_rank == 0 else train_loader\r\n\r\n    for batch_idx, (x, c) in enumerate(bar):\r\n\r\n        scheduler.step()\r\n        global_step += 1\r\n\r\n        x, c = x.to(device, non_blocking=True), c.to(device, non_blocking=True)\r\n\r\n        optimizer.zero_grad()\r\n\r\n        log_p, logdet = model(x, c)\r\n        log_p, logdet = torch.mean(log_p), torch.mean(logdet)\r\n\r\n        loss = -(log_p + logdet)\r\n\r\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\r\n            scaled_loss.backward()\r\n\r\n        nn.utils.clip_grad_norm_(amp.master_params(optimizer), 1.)\r\n         \r\n        optimizer.step()\r\n\r\n        running_num += 1\r\n        running_loss[0] += loss.item()\r\n        running_loss[1] += log_p.item()\r\n        running_loss[2] += logdet.item()\r\n\r\n        epoch_loss += loss.item()\r\n\r\n        if args.local_rank == 0:\r\n            bar.set_description(\'{}/{}, [Log pdf, Log p(z), Log Det] : {}\'\r\n                                .format(epoch, global_step, running_loss / running_num))\r\n            if (batch_idx + 1) % 100 == 0:\r\n                running_num = 0\r\n                running_loss = np.zeros(3)\r\n\r\n        del x, c, log_p, logdet, loss\r\n    del running_loss\r\n    gc.collect()\r\n    print(\'{}/{}/{} Training Loss : {:.4f}\'.format(epoch, global_step, args.local_rank, epoch_loss / (len(train_loader))))\r\n    return epoch_loss / len(train_loader)\r\n\r\n\r\ndef evaluate(model):\r\n    model.eval()\r\n    running_loss = [0., 0., 0.]\r\n    epoch_loss = 0.\r\n    display_step = 100\r\n    for batch_idx, (x, c) in enumerate(test_loader):\r\n        x, c = x.to(device), c.to(device)\r\n        log_p, logdet = model(x, c)\r\n        log_p, logdet = torch.mean(log_p), torch.mean(logdet)\r\n        loss = -(log_p + logdet)\r\n\r\n        running_loss[0] += loss.item() / display_step\r\n        running_loss[1] += log_p.item() / display_step\r\n        running_loss[2] += logdet.item() / display_step\r\n        epoch_loss += loss.item()\r\n\r\n        if (batch_idx + 1) % 100 == 0:\r\n            print(\'Global Step : {}, [{}, {}] [Log pdf, Log p(z), Log Det] : {}\'\r\n                  .format(global_step, epoch, batch_idx + 1, np.array(running_loss)))\r\n            running_loss = [0., 0., 0.]\r\n        del x, c, log_p, logdet, loss\r\n    del running_loss\r\n    epoch_loss /= len(test_loader)\r\n    print(\'Evaluation Loss : {:.4f}\'.format(epoch_loss))\r\n    return epoch_loss\r\n\r\n\r\ndef synthesize(model):\r\n    global global_step\r\n    model.eval()\r\n    for batch_idx, (x, c) in enumerate(synth_loader):\r\n        if batch_idx == 0:\r\n            x, c = x.to(device), c.to(device)\r\n\r\n            q_0 = Normal(x.new_zeros(x.size()), x.new_ones(x.size()))\r\n            z = q_0.sample()\r\n\r\n            start_time = time.time()\r\n            with torch.no_grad():\r\n                y_gen = model.module.reverse(z, c).squeeze()\r\n            wav = y_gen.to(torch.device(""cpu"")).data.numpy()\r\n            wav_name = \'{}/{}/generate_{}_{}.wav\'.format(args.sample_path, args.model_name, global_step, batch_idx)\r\n            print(\'{} seconds\'.format(time.time() - start_time))\r\n            librosa.output.write_wav(wav_name, wav, sr=22050)\r\n            print(\'{} Saved!\'.format(wav_name))\r\n            del x, c, z, q_0, y_gen, wav\r\n\r\n\r\ndef save_checkpoint(model, optimizer, scheduler, global_step, global_epoch):\r\n    checkpoint_path = os.path.join(args.save, args.model_name, ""checkpoint_step{:09d}.pth"".format(global_step))\r\n    optimizer_state = optimizer.state_dict()\r\n    scheduler_state = scheduler.state_dict()\r\n    torch.save({""state_dict"": model.state_dict(),\r\n                ""optimizer"": optimizer_state,\r\n                ""scheduler"": scheduler_state,\r\n                ""global_step"": global_step,\r\n                ""global_epoch"": global_epoch}, checkpoint_path)\r\n\r\n\r\ndef load_checkpoint(step, model, optimizer, scheduler):\r\n    global global_step\r\n    global global_epoch\r\n\r\n    checkpoint_path = os.path.join(args.save, args.model_name, ""checkpoint_step{:09d}.pth"".format(step))\r\n    print(""Rank {} load checkpoint from: {}"".format(args.local_rank, checkpoint_path))\r\n    checkpoint = torch.load(checkpoint_path)\r\n\r\n    # generalized load procedure for both single-gpu and DataParallel models\r\n    # https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/3\r\n    try:\r\n        model.load_state_dict(checkpoint[""state_dict""])\r\n    except RuntimeError:\r\n        print(""INFO: this model is trained with DataParallel. Creating new state_dict without module..."")\r\n        state_dict = checkpoint[""state_dict""]\r\n        from collections import OrderedDict\r\n        new_state_dict = OrderedDict()\r\n        for k, v in state_dict.items():\r\n            name = k[7:]  # remove `module.`\r\n            new_state_dict[name] = v\r\n        model.load_state_dict(new_state_dict)\r\n\r\n    optimizer.load_state_dict(checkpoint[""optimizer""])\r\n    scheduler.load_state_dict(checkpoint[""scheduler""])\r\n    global_step = checkpoint[""global_step""]\r\n    global_epoch = checkpoint[""global_epoch""]\r\n\r\n    return model, optimizer, scheduler\r\n\r\n\r\nif __name__ == ""__main__"":\r\n\r\n    model = build_model()\r\n    model.to(device)\r\n\r\n    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\r\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200000 // world_size, gamma=0.5)\r\n\r\n    pretrained = True if args.load_step > 0 else False\r\n    if pretrained is False:\r\n        # do ActNorm initialization first (if model.pretrained is True, this does nothing so no worries)\r\n        x_seed, c_seed = next(iter(train_loader))\r\n        x_seed, c_seed = x_seed.to(device), c_seed.to(device)\r\n        with torch.no_grad():\r\n            _, _ = model(x_seed, c_seed)\r\n        del x_seed, c_seed, _\r\n    # then convert the model to DataParallel later (since ActNorm init from the DataParallel is wacky)\r\n\r\n    model, optimizer = amp.initialize(model, optimizer, opt_level=""O0"")\r\n    model = DistributedDataParallel(model)\r\n\r\n    global_step = 0\r\n    global_epoch = 0\r\n\r\n    if args.load_step == 0:\r\n        list_train_loss, list_loss = [], []\r\n        test_loss = 100.0\r\n    else:\r\n        model, optimizer, scheduler = load_checkpoint(args.load_step, model, optimizer, scheduler)\r\n        list_train_loss = np.load(\'{}/{}_train.npy\'.format(args.loss, args.model_name)).tolist()\r\n        list_loss = np.load(\'{}/{}.npy\'.format(args.loss, args.model_name)).tolist()\r\n        list_train_loss = list_train_loss[:global_epoch]\r\n        list_loss = list_loss[:global_epoch]\r\n        test_loss = np.min(list_loss)\r\n\r\n    for epoch in range(global_epoch + 1, args.epochs + 1):\r\n\r\n        training_epoch_loss = train(epoch, model, optimizer, scheduler)\r\n\r\n        if args.local_rank > 0:\r\n            gc.collect()\r\n            continue\r\n\r\n        with torch.no_grad():\r\n            test_epoch_loss = evaluate(model)\r\n\r\n        if test_loss > test_epoch_loss:\r\n            test_loss = test_epoch_loss\r\n            save_checkpoint(model, optimizer, scheduler, global_step, epoch)\r\n            print(\'Epoch {} Model Saved! Loss : {:.4f}\'.format(epoch, test_loss))\r\n            with torch.no_grad():\r\n                synthesize(model)\r\n\r\n        list_train_loss.append(training_epoch_loss)\r\n        list_loss.append(test_epoch_loss)\r\n\r\n        np.save(\'{}/{}_train.npy\'.format(args.loss, args.model_name), list_train_loss)\r\n        np.save(\'{}/{}.npy\'.format(args.loss, args.model_name), list_loss)\r\n\r\n        state = {k: v for k, v in args._get_kwargs()}\r\n        state[\'training_loss\'] = training_epoch_loss\r\n        state[\'eval_loss\'] = test_epoch_loss\r\n        state[\'epoch\'] = epoch\r\n\r\n        with open(os.path.join(args.log, \'%s.txt\' % args.model_name), \'a\') as log:\r\n            log.write(\'%s\\n\' % json.dumps(state))\r\n\r\n        gc.collect()\r\n'"
