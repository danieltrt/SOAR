file_path,api_count,code
data.py,7,"b'# -----------------------------------------------------------\n# Stacked Cross Attention Network implementation based on \n# https://arxiv.org/abs/1803.08024.\n# ""Stacked Cross Attention for Image-Text Matching""\n# Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He\n#\n# Writen by Kuang-Huei Lee, 2018\n# ---------------------------------------------------------------\n""""""Data provider""""""\n\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport os\nimport nltk\nfrom PIL import Image\nimport numpy as np\nimport json as jsonmod\n\n\nclass PrecompDataset(data.Dataset):\n    """"""\n    Load precomputed captions and image features\n    Possible options: f30k_precomp, coco_precomp\n    """"""\n\n    def __init__(self, data_path, data_split, vocab):\n        self.vocab = vocab\n        loc = data_path + \'/\'\n\n        # Captions\n        self.captions = []\n        with open(loc+\'%s_caps.txt\' % data_split, \'rb\') as f:\n            for line in f:\n                self.captions.append(line.strip())\n\n        # Image features\n        self.images = np.load(loc+\'%s_ims.npy\' % data_split)\n        self.length = len(self.captions)\n        # rkiros data has redundancy in images, we divide by 5, 10crop doesn\'t\n        if self.images.shape[0] != self.length:\n            self.im_div = 5\n        else:\n            self.im_div = 1\n        # the development set for coco is large and so validation would be slow\n        if data_split == \'dev\':\n            self.length = 5000\n\n    def __getitem__(self, index):\n        # handle the image redundancy\n        img_id = index/self.im_div\n        image = torch.Tensor(self.images[img_id])\n        caption = self.captions[index]\n        vocab = self.vocab\n\n        # Convert caption (string) to word ids.\n        tokens = nltk.tokenize.word_tokenize(\n            str(caption).lower().decode(\'utf-8\'))\n        caption = []\n        caption.append(vocab(\'<start>\'))\n        caption.extend([vocab(token) for token in tokens])\n        caption.append(vocab(\'<end>\'))\n        target = torch.Tensor(caption)\n        return image, target, index, img_id\n\n    def __len__(self):\n        return self.length\n\n\ndef collate_fn(data):\n    """"""Build mini-batch tensors from a list of (image, caption) tuples.\n    Args:\n        data: list of (image, caption) tuple.\n            - image: torch tensor of shape (3, 256, 256).\n            - caption: torch tensor of shape (?); variable length.\n\n    Returns:\n        images: torch tensor of shape (batch_size, 3, 256, 256).\n        targets: torch tensor of shape (batch_size, padded_length).\n        lengths: list; valid length for each padded caption.\n    """"""\n    # Sort a data list by caption length\n    data.sort(key=lambda x: len(x[1]), reverse=True)\n    images, captions, ids, img_ids = zip(*data)\n\n    # Merge images (convert tuple of 3D tensor to 4D tensor)\n    images = torch.stack(images, 0)\n\n    # Merget captions (convert tuple of 1D tensor to 2D tensor)\n    lengths = [len(cap) for cap in captions]\n    targets = torch.zeros(len(captions), max(lengths)).long()\n    for i, cap in enumerate(captions):\n        end = lengths[i]\n        targets[i, :end] = cap[:end]\n\n    return images, targets, lengths, ids\n\n\ndef get_precomp_loader(data_path, data_split, vocab, opt, batch_size=100,\n                       shuffle=True, num_workers=2):\n    """"""Returns torch.utils.data.DataLoader for custom coco dataset.""""""\n    dset = PrecompDataset(data_path, data_split, vocab)\n\n    data_loader = torch.utils.data.DataLoader(dataset=dset,\n                                              batch_size=batch_size,\n                                              shuffle=shuffle,\n                                              pin_memory=True,\n                                              collate_fn=collate_fn)\n    return data_loader\n\n\ndef get_loaders(data_name, vocab, batch_size, workers, opt):\n    dpath = os.path.join(opt.data_path, data_name)\n    train_loader = get_precomp_loader(dpath, \'train\', vocab, opt,\n                                      batch_size, True, workers)\n    val_loader = get_precomp_loader(dpath, \'dev\', vocab, opt,\n                                    batch_size, False, workers)\n    return train_loader, val_loader\n\n\ndef get_test_loader(split_name, data_name, vocab, batch_size,\n                    workers, opt):\n    dpath = os.path.join(opt.data_path, data_name)\n    test_loader = get_precomp_loader(dpath, split_name, vocab, opt,\n                                     batch_size, False, workers)\n    return test_loader\n'"
evaluation.py,7,"b'# -----------------------------------------------------------\n# Stacked Cross Attention Network implementation based on \n# https://arxiv.org/abs/1803.08024.\n# ""Stacked Cross Attention for Image-Text Matching""\n# Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He\n#\n# Writen by Kuang-Huei Lee, 2018\n# ---------------------------------------------------------------\n""""""Evaluation""""""\n\nfrom __future__ import print_function\nimport os\n\nimport sys\nfrom data import get_test_loader\nimport time\nimport numpy as np\nfrom vocab import Vocabulary, deserialize_vocab  # NOQA\nimport torch\nfrom model import SCAN, xattn_score_t2i, xattn_score_i2t\nfrom collections import OrderedDict\nimport time\nfrom torch.autograd import Variable\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=0):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / (.0001 + self.count)\n\n    def __str__(self):\n        """"""String representation for logging\n        """"""\n        # for values that should be recorded exactly e.g. iteration number\n        if self.count == 0:\n            return str(self.val)\n        # for stats\n        return \'%.4f (%.4f)\' % (self.val, self.avg)\n\n\nclass LogCollector(object):\n    """"""A collection of logging objects that can change from train to val""""""\n\n    def __init__(self):\n        # to keep the order of logged variables deterministic\n        self.meters = OrderedDict()\n\n    def update(self, k, v, n=0):\n        # create a new meter if previously not recorded\n        if k not in self.meters:\n            self.meters[k] = AverageMeter()\n        self.meters[k].update(v, n)\n\n    def __str__(self):\n        """"""Concatenate the meters in one log line\n        """"""\n        s = \'\'\n        for i, (k, v) in enumerate(self.meters.iteritems()):\n            if i > 0:\n                s += \'  \'\n            s += k + \' \' + str(v)\n        return s\n\n    def tb_log(self, tb_logger, prefix=\'\', step=None):\n        """"""Log using tensorboard\n        """"""\n        for k, v in self.meters.iteritems():\n            tb_logger.log_value(prefix + k, v.val, step=step)\n\n\ndef encode_data(model, data_loader, log_step=10, logging=print):\n    """"""Encode all images and captions loadable by `data_loader`\n    """"""\n    batch_time = AverageMeter()\n    val_logger = LogCollector()\n\n    # switch to evaluate mode\n    model.val_start()\n\n    end = time.time()\n\n    # np array to keep all the embeddings\n    img_embs = None\n    cap_embs = None\n    cap_lens = None\n\n    max_n_word = 0\n    for i, (images, captions, lengths, ids) in enumerate(data_loader):\n        max_n_word = max(max_n_word, max(lengths))\n\n    for i, (images, captions, lengths, ids) in enumerate(data_loader):\n        # make sure val logger is used\n        model.logger = val_logger\n\n        # compute the embeddings\n        img_emb, cap_emb, cap_len = model.forward_emb(images, captions, lengths, volatile=True)\n        #print(img_emb)\n        if img_embs is None:\n            if img_emb.dim() == 3:\n                img_embs = np.zeros((len(data_loader.dataset), img_emb.size(1), img_emb.size(2)))\n            else:\n                img_embs = np.zeros((len(data_loader.dataset), img_emb.size(1)))\n            cap_embs = np.zeros((len(data_loader.dataset), max_n_word, cap_emb.size(2)))\n            cap_lens = [0] * len(data_loader.dataset)\n        # cache embeddings\n        img_embs[ids] = img_emb.data.cpu().numpy().copy()\n        cap_embs[ids,:max(lengths),:] = cap_emb.data.cpu().numpy().copy()\n        for j, nid in enumerate(ids):\n            cap_lens[nid] = cap_len[j]\n\n        # measure accuracy and record loss\n        model.forward_loss(img_emb, cap_emb, cap_len)\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % log_step == 0:\n            logging(\'Test: [{0}/{1}]\\t\'\n                    \'{e_log}\\t\'\n                    \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                    .format(\n                        i, len(data_loader), batch_time=batch_time,\n                        e_log=str(model.logger)))\n        del images, captions\n    return img_embs, cap_embs, cap_lens\n\n\ndef evalrank(model_path, data_path=None, split=\'dev\', fold5=False):\n    """"""\n    Evaluate a trained model on either dev or test. If `fold5=True`, 5 fold\n    cross-validation is done (only for MSCOCO). Otherwise, the full data is\n    used for evaluation.\n    """"""\n    # load model and options\n    checkpoint = torch.load(model_path)\n    opt = checkpoint[\'opt\']\n    print(opt)\n    if data_path is not None:\n        opt.data_path = data_path\n\n    # load vocabulary used by the model\n    vocab = deserialize_vocab(os.path.join(opt.vocab_path, \'%s_vocab.json\' % opt.data_name))\n    opt.vocab_size = len(vocab)\n\n    # construct model\n    model = SCAN(opt)\n\n    # load model state\n    model.load_state_dict(checkpoint[\'model\'])\n\n    print(\'Loading dataset\')\n    data_loader = get_test_loader(split, opt.data_name, vocab,\n                                  opt.batch_size, opt.workers, opt)\n\n    print(\'Computing results...\')\n    img_embs, cap_embs, cap_lens = encode_data(model, data_loader)\n    print(\'Images: %d, Captions: %d\' %\n          (img_embs.shape[0] / 5, cap_embs.shape[0]))\n\n\n    if not fold5:\n        # no cross-validation, full evaluation\n        img_embs = np.array([img_embs[i] for i in range(0, len(img_embs), 5)])\n        start = time.time()\n        if opt.cross_attn == \'t2i\':\n            sims = shard_xattn_t2i(img_embs, cap_embs, cap_lens, opt, shard_size=128)\n        elif opt.cross_attn == \'i2t\':\n            sims = shard_xattn_i2t(img_embs, cap_embs, cap_lens, opt, shard_size=128)\n        else:\n            raise NotImplementedError\n        end = time.time()\n        print(""calculate similarity time:"", end-start)\n\n        r, rt = i2t(img_embs, cap_embs, cap_lens, sims, return_ranks=True)\n        ri, rti = t2i(img_embs, cap_embs, cap_lens, sims, return_ranks=True)\n        ar = (r[0] + r[1] + r[2]) / 3\n        ari = (ri[0] + ri[1] + ri[2]) / 3\n        rsum = r[0] + r[1] + r[2] + ri[0] + ri[1] + ri[2]\n        print(""rsum: %.1f"" % rsum)\n        print(""Average i2t Recall: %.1f"" % ar)\n        print(""Image to text: %.1f %.1f %.1f %.1f %.1f"" % r)\n        print(""Average t2i Recall: %.1f"" % ari)\n        print(""Text to image: %.1f %.1f %.1f %.1f %.1f"" % ri)\n    else:\n        # 5fold cross-validation, only for MSCOCO\n        results = []\n        for i in range(5):\n            img_embs_shard = img_embs[i * 5000:(i + 1) * 5000:5]\n            cap_embs_shard = cap_embs[i * 5000:(i + 1) * 5000]\n            cap_lens_shard = cap_lens[i * 5000:(i + 1) * 5000]\n            start = time.time()\n            if opt.cross_attn == \'t2i\':\n                sims = shard_xattn_t2i(img_embs_shard, cap_embs_shard, cap_lens_shard, opt, shard_size=128)\n            elif opt.cross_attn == \'i2t\':\n                sims = shard_xattn_i2t(img_embs_shard, cap_embs_shard, cap_lens_shard, opt, shard_size=128)\n            else:\n                raise NotImplementedError\n            end = time.time()\n            print(""calculate similarity time:"", end-start)\n\n            r, rt0 = i2t(img_embs_shard, cap_embs_shard, cap_lens_shard, sims, return_ranks=True)\n            print(""Image to text: %.1f, %.1f, %.1f, %.1f, %.1f"" % r)\n            ri, rti0 = t2i(img_embs_shard, cap_embs_shard, cap_lens_shard, sims, return_ranks=True)\n            print(""Text to image: %.1f, %.1f, %.1f, %.1f, %.1f"" % ri)\n\n            if i == 0:\n                rt, rti = rt0, rti0\n            ar = (r[0] + r[1] + r[2]) / 3\n            ari = (ri[0] + ri[1] + ri[2]) / 3\n            rsum = r[0] + r[1] + r[2] + ri[0] + ri[1] + ri[2]\n            print(""rsum: %.1f ar: %.1f ari: %.1f"" % (rsum, ar, ari))\n            results += [list(r) + list(ri) + [ar, ari, rsum]]\n\n        print(""-----------------------------------"")\n        print(""Mean metrics: "")\n        mean_metrics = tuple(np.array(results).mean(axis=0).flatten())\n        print(""rsum: %.1f"" % (mean_metrics[10] * 6))\n        print(""Average i2t Recall: %.1f"" % mean_metrics[11])\n        print(""Image to text: %.1f %.1f %.1f %.1f %.1f"" %\n              mean_metrics[:5])\n        print(""Average t2i Recall: %.1f"" % mean_metrics[12])\n        print(""Text to image: %.1f %.1f %.1f %.1f %.1f"" %\n              mean_metrics[5:10])\n\n    torch.save({\'rt\': rt, \'rti\': rti}, \'ranks.pth.tar\')\n\n\ndef softmax(X, axis):\n    """"""\n    Compute the softmax of each element along an axis of X.\n    """"""\n    y = np.atleast_2d(X)\n    # subtract the max for numerical stability\n    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n    # exponentiate y\n    y = np.exp(y)\n    # take the sum along the specified axis\n    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n    # finally: divide elementwise\n    p = y / ax_sum\n    return p\n\n\ndef shard_xattn_t2i(images, captions, caplens, opt, shard_size=128):\n    """"""\n    Computer pairwise t2i image-caption distance with locality sharding\n    """"""\n    n_im_shard = (len(images)-1)/shard_size + 1\n    n_cap_shard = (len(captions)-1)/shard_size + 1\n    \n    d = np.zeros((len(images), len(captions)))\n    for i in range(n_im_shard):\n        im_start, im_end = shard_size*i, min(shard_size*(i+1), len(images))\n        for j in range(n_cap_shard):\n            sys.stdout.write(\'\\r>> shard_xattn_t2i batch (%d,%d)\' % (i,j))\n            cap_start, cap_end = shard_size*j, min(shard_size*(j+1), len(captions))\n            im = Variable(torch.from_numpy(images[im_start:im_end]), volatile=True).cuda()\n            s = Variable(torch.from_numpy(captions[cap_start:cap_end]), volatile=True).cuda()\n            l = caplens[cap_start:cap_end]\n            sim = xattn_score_t2i(im, s, l, opt)\n            d[im_start:im_end, cap_start:cap_end] = sim.data.cpu().numpy()\n    sys.stdout.write(\'\\n\')\n    return d\n\n\ndef shard_xattn_i2t(images, captions, caplens, opt, shard_size=128):\n    """"""\n    Computer pairwise i2t image-caption distance with locality sharding\n    """"""\n    n_im_shard = (len(images)-1)/shard_size + 1\n    n_cap_shard = (len(captions)-1)/shard_size + 1\n    \n    d = np.zeros((len(images), len(captions)))\n    for i in range(n_im_shard):\n        im_start, im_end = shard_size*i, min(shard_size*(i+1), len(images))\n        for j in range(n_cap_shard):\n            sys.stdout.write(\'\\r>> shard_xattn_i2t batch (%d,%d)\' % (i,j))\n            cap_start, cap_end = shard_size*j, min(shard_size*(j+1), len(captions))\n            im = Variable(torch.from_numpy(images[im_start:im_end]), volatile=True).cuda()\n            s = Variable(torch.from_numpy(captions[cap_start:cap_end]), volatile=True).cuda()\n            l = caplens[cap_start:cap_end]\n            sim = xattn_score_i2t(im, s, l, opt)\n            d[im_start:im_end, cap_start:cap_end] = sim.data.cpu().numpy()\n    sys.stdout.write(\'\\n\')\n    return d\n\n\ndef i2t(images, captions, caplens, sims, npts=None, return_ranks=False):\n    """"""\n    Images->Text (Image Annotation)\n    Images: (N, n_region, d) matrix of images\n    Captions: (5N, max_n_word, d) matrix of captions\n    CapLens: (5N) array of caption lengths\n    sims: (N, 5N) matrix of similarity im-cap\n    """"""\n    npts = images.shape[0]\n    ranks = np.zeros(npts)\n    top1 = np.zeros(npts)\n    for index in range(npts):\n        inds = np.argsort(sims[index])[::-1]\n        # Score\n        rank = 1e20\n        for i in range(5 * index, 5 * index + 5, 1):\n            tmp = np.where(inds == i)[0][0]\n            if tmp < rank:\n                rank = tmp\n        ranks[index] = rank\n        top1[index] = inds[0]\n\n    # Compute metrics\n    r1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n    r5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n    r10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n    medr = np.floor(np.median(ranks)) + 1\n    meanr = ranks.mean() + 1\n    if return_ranks:\n        return (r1, r5, r10, medr, meanr), (ranks, top1)\n    else:\n        return (r1, r5, r10, medr, meanr)\n\n\ndef t2i(images, captions, caplens, sims, npts=None, return_ranks=False):\n    """"""\n    Text->Images (Image Search)\n    Images: (N, n_region, d) matrix of images\n    Captions: (5N, max_n_word, d) matrix of captions\n    CapLens: (5N) array of caption lengths\n    sims: (N, 5N) matrix of similarity im-cap\n    """"""\n    npts = images.shape[0]\n    ranks = np.zeros(5 * npts)\n    top1 = np.zeros(5 * npts)\n\n    # --> (5N(caption), N(image))\n    sims = sims.T\n\n    for index in range(npts):\n        for i in range(5):\n            inds = np.argsort(sims[5 * index + i])[::-1]\n            ranks[5 * index + i] = np.where(inds == index)[0][0]\n            top1[5 * index + i] = inds[0]\n\n    # Compute metrics\n    r1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n    r5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n    r10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n    medr = np.floor(np.median(ranks)) + 1\n    meanr = ranks.mean() + 1\n    if return_ranks:\n        return (r1, r5, r10, medr, meanr), (ranks, top1)\n    else:\n        return (r1, r5, r10, medr, meanr)\n'"
model.py,30,"b'# -----------------------------------------------------------\n# Stacked Cross Attention Network implementation based on \n# https://arxiv.org/abs/1803.08024.\n# ""Stacked Cross Attention for Image-Text Matching""\n# Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He\n#\n# Writen by Kuang-Huei Lee, 2018\n# ---------------------------------------------------------------\n""""""SCAN model""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init\nimport torchvision.models as models\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.nn.utils.weight_norm import weight_norm\nimport torch.backends.cudnn as cudnn\nfrom torch.nn.utils.clip_grad import clip_grad_norm\nimport numpy as np\nfrom collections import OrderedDict\n\n\ndef l1norm(X, dim, eps=1e-8):\n    """"""L1-normalize columns of X\n    """"""\n    norm = torch.abs(X).sum(dim=dim, keepdim=True) + eps\n    X = torch.div(X, norm)\n    return X\n\n\ndef l2norm(X, dim, eps=1e-8):\n    """"""L2-normalize columns of X\n    """"""\n    norm = torch.pow(X, 2).sum(dim=dim, keepdim=True).sqrt() + eps\n    X = torch.div(X, norm)\n    return X\n\n\ndef EncoderImage(data_name, img_dim, embed_size, precomp_enc_type=\'basic\', \n                 no_imgnorm=False):\n    """"""A wrapper to image encoders. Chooses between an different encoders\n    that uses precomputed image features.\n    """"""\n    if precomp_enc_type == \'basic\':\n        img_enc = EncoderImagePrecomp(\n            img_dim, embed_size, no_imgnorm)\n    elif precomp_enc_type == \'weight_norm\':\n        img_enc = EncoderImageWeightNormPrecomp(\n            img_dim, embed_size, no_imgnorm)\n    else:\n        raise ValueError(""Unknown precomp_enc_type: {}"".format(precomp_enc_type))\n\n    return img_enc\n\n\nclass EncoderImagePrecomp(nn.Module):\n\n    def __init__(self, img_dim, embed_size, no_imgnorm=False):\n        super(EncoderImagePrecomp, self).__init__()\n        self.embed_size = embed_size\n        self.no_imgnorm = no_imgnorm\n        self.fc = nn.Linear(img_dim, embed_size)\n\n        self.init_weights()\n\n    def init_weights(self):\n        """"""Xavier initialization for the fully connected layer\n        """"""\n        r = np.sqrt(6.) / np.sqrt(self.fc.in_features +\n                                  self.fc.out_features)\n        self.fc.weight.data.uniform_(-r, r)\n        self.fc.bias.data.fill_(0)\n\n    def forward(self, images):\n        """"""Extract image feature vectors.""""""\n        # assuming that the precomputed features are already l2-normalized\n\n        features = self.fc(images)\n\n        # normalize in the joint embedding space\n        if not self.no_imgnorm:\n            features = l2norm(features, dim=-1)\n\n        return features\n\n    def load_state_dict(self, state_dict):\n        """"""Copies parameters. overwritting the default one to\n        accept state_dict from Full model\n        """"""\n        own_state = self.state_dict()\n        new_state = OrderedDict()\n        for name, param in state_dict.items():\n            if name in own_state:\n                new_state[name] = param\n\n        super(EncoderImagePrecomp, self).load_state_dict(new_state)\n\n\nclass EncoderImageWeightNormPrecomp(nn.Module):\n\n    def __init__(self, img_dim, embed_size, no_imgnorm=False):\n        super(EncoderImageWeightNormPrecomp, self).__init__()\n        self.embed_size = embed_size\n        self.no_imgnorm = no_imgnorm\n        self.fc = weight_norm(nn.Linear(img_dim, embed_size), dim=None)\n\n    def forward(self, images):\n        """"""Extract image feature vectors.""""""\n        # assuming that the precomputed features are already l2-normalized\n\n        features = self.fc(images)\n\n        # normalize in the joint embedding space\n        if not self.no_imgnorm:\n            features = l2norm(features, dim=-1)\n\n        return features\n\n    def load_state_dict(self, state_dict):\n        """"""Copies parameters. overwritting the default one to\n        accept state_dict from Full model\n        """"""\n        own_state = self.state_dict()\n        new_state = OrderedDict()\n        for name, param in state_dict.items():\n            if name in own_state:\n                new_state[name] = param\n\n        super(EncoderImageWeightNormPrecomp, self).load_state_dict(new_state)\n\n\n# RNN Based Language Model\nclass EncoderText(nn.Module):\n\n    def __init__(self, vocab_size, word_dim, embed_size, num_layers,\n                 use_bi_gru=False, no_txtnorm=False):\n        super(EncoderText, self).__init__()\n        self.embed_size = embed_size\n        self.no_txtnorm = no_txtnorm\n\n        # word embedding\n        self.embed = nn.Embedding(vocab_size, word_dim)\n\n        # caption embedding\n        self.use_bi_gru = use_bi_gru\n        self.rnn = nn.GRU(word_dim, embed_size, num_layers, batch_first=True, bidirectional=use_bi_gru)\n\n        self.init_weights()\n\n    def init_weights(self):\n        self.embed.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, x, lengths):\n        """"""Handles variable size captions\n        """"""\n        # Embed word ids to vectors\n        x = self.embed(x)\n        packed = pack_padded_sequence(x, lengths, batch_first=True)\n\n        # Forward propagate RNN\n        out, _ = self.rnn(packed)\n\n        # Reshape *final* output to (batch_size, hidden_size)\n        padded = pad_packed_sequence(out, batch_first=True)\n        cap_emb, cap_len = padded\n\n        if self.use_bi_gru:\n            cap_emb = (cap_emb[:,:,:cap_emb.size(2)/2] + cap_emb[:,:,cap_emb.size(2)/2:])/2\n\n        # normalization in the joint embedding space\n        if not self.no_txtnorm:\n            cap_emb = l2norm(cap_emb, dim=-1)\n\n        return cap_emb, cap_len\n\n\ndef func_attention(query, context, opt, smooth, eps=1e-8):\n    """"""\n    query: (n_context, queryL, d)\n    context: (n_context, sourceL, d)\n    """"""\n    batch_size_q, queryL = query.size(0), query.size(1)\n    batch_size, sourceL = context.size(0), context.size(1)\n\n\n    # Get attention\n    # --> (batch, d, queryL)\n    queryT = torch.transpose(query, 1, 2)\n\n    # (batch, sourceL, d)(batch, d, queryL)\n    # --> (batch, sourceL, queryL)\n    attn = torch.bmm(context, queryT)\n    if opt.raw_feature_norm == ""softmax"":\n        # --> (batch*sourceL, queryL)\n        attn = attn.view(batch_size*sourceL, queryL)\n        attn = nn.Softmax()(attn)\n        # --> (batch, sourceL, queryL)\n        attn = attn.view(batch_size, sourceL, queryL)\n    elif opt.raw_feature_norm == ""l2norm"":\n        attn = l2norm(attn, 2)\n    elif opt.raw_feature_norm == ""clipped_l2norm"":\n        attn = nn.LeakyReLU(0.1)(attn)\n        attn = l2norm(attn, 2)\n    elif opt.raw_feature_norm == ""l1norm"":\n        attn = l1norm_d(attn, 2)\n    elif opt.raw_feature_norm == ""clipped_l1norm"":\n        attn = nn.LeakyReLU(0.1)(attn)\n        attn = l1norm_d(attn, 2)\n    elif opt.raw_feature_norm == ""clipped"":\n        attn = nn.LeakyReLU(0.1)(attn)\n    elif opt.raw_feature_norm == ""no_norm"":\n        pass\n    else:\n        raise ValueError(""unknown first norm type:"", opt.raw_feature_norm)\n    # --> (batch, queryL, sourceL)\n    attn = torch.transpose(attn, 1, 2).contiguous()\n    # --> (batch*queryL, sourceL)\n    attn = attn.view(batch_size*queryL, sourceL)\n    attn = nn.Softmax()(attn*smooth)\n    # --> (batch, queryL, sourceL)\n    attn = attn.view(batch_size, queryL, sourceL)\n    # --> (batch, sourceL, queryL)\n    attnT = torch.transpose(attn, 1, 2).contiguous()\n\n    # --> (batch, d, sourceL)\n    contextT = torch.transpose(context, 1, 2)\n    # (batch x d x sourceL)(batch x sourceL x queryL)\n    # --> (batch, d, queryL)\n    weightedContext = torch.bmm(contextT, attnT)\n    # --> (batch, queryL, d)\n    weightedContext = torch.transpose(weightedContext, 1, 2)\n\n    return weightedContext, attnT\n\n\ndef cosine_similarity(x1, x2, dim=1, eps=1e-8):\n    """"""Returns cosine similarity between x1 and x2, computed along dim.""""""\n    w12 = torch.sum(x1 * x2, dim)\n    w1 = torch.norm(x1, 2, dim)\n    w2 = torch.norm(x2, 2, dim)\n    return (w12 / (w1 * w2).clamp(min=eps)).squeeze()\n\n\ndef xattn_score_t2i(images, captions, cap_lens, opt):\n    """"""\n    Images: (n_image, n_regions, d) matrix of images\n    Captions: (n_caption, max_n_word, d) matrix of captions\n    CapLens: (n_caption) array of caption lengths\n    """"""\n    similarities = []\n    n_image = images.size(0)\n    n_caption = captions.size(0)\n    for i in range(n_caption):\n        # Get the i-th text description\n        n_word = cap_lens[i]\n        cap_i = captions[i, :n_word, :].unsqueeze(0).contiguous()\n        # --> (n_image, n_word, d)\n        cap_i_expand = cap_i.repeat(n_image, 1, 1)\n        """"""\n            word(query): (n_image, n_word, d)\n            image(context): (n_image, n_regions, d)\n            weiContext: (n_image, n_word, d)\n            attn: (n_image, n_region, n_word)\n        """"""\n        weiContext, attn = func_attention(cap_i_expand, images, opt, smooth=opt.lambda_softmax)\n        cap_i_expand = cap_i_expand.contiguous()\n        weiContext = weiContext.contiguous()\n        # (n_image, n_word)\n        row_sim = cosine_similarity(cap_i_expand, weiContext, dim=2)\n        if opt.agg_func == \'LogSumExp\':\n            row_sim.mul_(opt.lambda_lse).exp_()\n            row_sim = row_sim.sum(dim=1, keepdim=True)\n            row_sim = torch.log(row_sim)/opt.lambda_lse\n        elif opt.agg_func == \'Max\':\n            row_sim = row_sim.max(dim=1, keepdim=True)[0]\n        elif opt.agg_func == \'Sum\':\n            row_sim = row_sim.sum(dim=1, keepdim=True)\n        elif opt.agg_func == \'Mean\':\n            row_sim = row_sim.mean(dim=1, keepdim=True)\n        else:\n            raise ValueError(""unknown aggfunc: {}"".format(opt.agg_func))\n        similarities.append(row_sim)\n\n    # (n_image, n_caption)\n    similarities = torch.cat(similarities, 1)\n    \n    return similarities\n\n\ndef xattn_score_i2t(images, captions, cap_lens, opt):\n    """"""\n    Images: (batch_size, n_regions, d) matrix of images\n    Captions: (batch_size, max_n_words, d) matrix of captions\n    CapLens: (batch_size) array of caption lengths\n    """"""\n    similarities = []\n    n_image = images.size(0)\n    n_caption = captions.size(0)\n    n_region = images.size(1)\n    for i in range(n_caption):\n        # Get the i-th text description\n        n_word = cap_lens[i]\n        cap_i = captions[i, :n_word, :].unsqueeze(0).contiguous()\n        # (n_image, n_word, d)\n        cap_i_expand = cap_i.repeat(n_image, 1, 1)\n        """"""\n            word(query): (n_image, n_word, d)\n            image(context): (n_image, n_region, d)\n            weiContext: (n_image, n_region, d)\n            attn: (n_image, n_word, n_region)\n        """"""\n        weiContext, attn = func_attention(images, cap_i_expand, opt, smooth=opt.lambda_softmax)\n        # (n_image, n_region)\n        row_sim = cosine_similarity(images, weiContext, dim=2)\n        if opt.agg_func == \'LogSumExp\':\n            row_sim.mul_(opt.lambda_lse).exp_()\n            row_sim = row_sim.sum(dim=1, keepdim=True)\n            row_sim = torch.log(row_sim)/opt.lambda_lse\n        elif opt.agg_func == \'Max\':\n            row_sim = row_sim.max(dim=1, keepdim=True)[0]\n        elif opt.agg_func == \'Sum\':\n            row_sim = row_sim.sum(dim=1, keepdim=True)\n        elif opt.agg_func == \'Mean\':\n            row_sim = row_sim.mean(dim=1, keepdim=True)\n        else:\n            raise ValueError(""unknown aggfunc: {}"".format(opt.agg_func))\n        similarities.append(row_sim)\n\n    # (n_image, n_caption)\n    similarities = torch.cat(similarities, 1)\n    return similarities\n\n\nclass ContrastiveLoss(nn.Module):\n    """"""\n    Compute contrastive loss\n    """"""\n    def __init__(self, opt, margin=0, max_violation=False):\n        super(ContrastiveLoss, self).__init__()\n        self.opt = opt\n        self.margin = margin\n        self.max_violation = max_violation\n\n    def forward(self, im, s, s_l):\n        # compute image-sentence score matrix\n        if self.opt.cross_attn == \'t2i\':\n            scores = xattn_score_t2i(im, s, s_l, self.opt)\n        elif self.opt.cross_attn == \'i2t\':\n            scores = xattn_score_i2t(im, s, s_l, self.opt)\n        else:\n            raise ValueError(""unknown first norm type:"", opt.raw_feature_norm)\n        diagonal = scores.diag().view(im.size(0), 1)\n        d1 = diagonal.expand_as(scores)\n        d2 = diagonal.t().expand_as(scores)\n\n        # compare every diagonal score to scores in its column\n        # caption retrieval\n        cost_s = (self.margin + scores - d1).clamp(min=0)\n        # compare every diagonal score to scores in its row\n        # image retrieval\n        cost_im = (self.margin + scores - d2).clamp(min=0)\n\n        # clear diagonals\n        mask = torch.eye(scores.size(0)) > .5\n        I = Variable(mask)\n        if torch.cuda.is_available():\n            I = I.cuda()\n        cost_s = cost_s.masked_fill_(I, 0)\n        cost_im = cost_im.masked_fill_(I, 0)\n\n        # keep the maximum violating negative for each query\n        if self.max_violation:\n            cost_s = cost_s.max(1)[0]\n            cost_im = cost_im.max(0)[0]\n        return cost_s.sum() + cost_im.sum()\n\n\nclass SCAN(object):\n    """"""\n    Stacked Cross Attention Network (SCAN) model\n    """"""\n    def __init__(self, opt):\n        # Build Models\n        self.grad_clip = opt.grad_clip\n        self.img_enc = EncoderImage(opt.data_name, opt.img_dim, opt.embed_size,\n                                    precomp_enc_type=opt.precomp_enc_type,\n                                    no_imgnorm=opt.no_imgnorm)\n        self.txt_enc = EncoderText(opt.vocab_size, opt.word_dim,\n                                   opt.embed_size, opt.num_layers, \n                                   use_bi_gru=opt.bi_gru,  \n                                   no_txtnorm=opt.no_txtnorm)\n        if torch.cuda.is_available():\n            self.img_enc.cuda()\n            self.txt_enc.cuda()\n            cudnn.benchmark = True\n\n        # Loss and Optimizer\n        self.criterion = ContrastiveLoss(opt=opt,\n                                         margin=opt.margin,\n                                         max_violation=opt.max_violation)\n        params = list(self.txt_enc.parameters())\n        params += list(self.img_enc.fc.parameters())\n\n        self.params = params\n\n        self.optimizer = torch.optim.Adam(params, lr=opt.learning_rate)\n\n        self.Eiters = 0\n\n    def state_dict(self):\n        state_dict = [self.img_enc.state_dict(), self.txt_enc.state_dict()]\n        return state_dict\n\n    def load_state_dict(self, state_dict):\n        self.img_enc.load_state_dict(state_dict[0])\n        self.txt_enc.load_state_dict(state_dict[1])\n\n    def train_start(self):\n        """"""switch to train mode\n        """"""\n        self.img_enc.train()\n        self.txt_enc.train()\n\n    def val_start(self):\n        """"""switch to evaluate mode\n        """"""\n        self.img_enc.eval()\n        self.txt_enc.eval()\n\n    def forward_emb(self, images, captions, lengths, volatile=False):\n        """"""Compute the image and caption embeddings\n        """"""\n        # Set mini-batch dataset\n        images = Variable(images, volatile=volatile)\n        captions = Variable(captions, volatile=volatile)\n        if torch.cuda.is_available():\n            images = images.cuda()\n            captions = captions.cuda()\n\n        # Forward\n        img_emb = self.img_enc(images)\n\n        # cap_emb (tensor), cap_lens (list)\n        cap_emb, cap_lens = self.txt_enc(captions, lengths)\n        return img_emb, cap_emb, cap_lens\n\n    def forward_loss(self, img_emb, cap_emb, cap_len, **kwargs):\n        """"""Compute the loss given pairs of image and caption embeddings\n        """"""\n        loss = self.criterion(img_emb, cap_emb, cap_len)\n        self.logger.update(\'Le\', loss.data[0], img_emb.size(0))\n        return loss\n\n    def train_emb(self, images, captions, lengths, ids=None, *args):\n        """"""One training step given images and captions.\n        """"""\n        self.Eiters += 1\n        self.logger.update(\'Eit\', self.Eiters)\n        self.logger.update(\'lr\', self.optimizer.param_groups[0][\'lr\'])\n\n        # compute the embeddings\n        img_emb, cap_emb, cap_lens = self.forward_emb(images, captions, lengths)\n\n        # measure accuracy and record loss\n        self.optimizer.zero_grad()\n        loss = self.forward_loss(img_emb, cap_emb, cap_lens)\n\n        # compute gradient and do SGD step\n        loss.backward()\n        if self.grad_clip > 0:\n            clip_grad_norm(self.params, self.grad_clip)\n        self.optimizer.step()\n'"
train.py,3,"b'# -----------------------------------------------------------\n# Stacked Cross Attention Network implementation based on \n# https://arxiv.org/abs/1803.08024.\n# ""Stacked Cross Attention for Image-Text Matching""\n# Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He\n#\n# Writen by Kuang-Huei Lee, 2018\n# ---------------------------------------------------------------\n""""""Training script""""""\n\nimport os\nimport time\nimport shutil\n\nimport torch\nimport numpy\n\nimport data\nfrom vocab import Vocabulary, deserialize_vocab\nfrom model import SCAN\nfrom evaluation import i2t, t2i, AverageMeter, LogCollector, encode_data, shard_xattn_t2i, shard_xattn_i2t\nfrom torch.autograd import Variable\n\nimport logging\nimport tensorboard_logger as tb_logger\n\nimport argparse\n\n\ndef main():\n    # Hyper Parameters\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_path\', default=\'./data/\',\n                        help=\'path to datasets\')\n    parser.add_argument(\'--data_name\', default=\'precomp\',\n                        help=\'{coco,f30k}_precomp\')\n    parser.add_argument(\'--vocab_path\', default=\'./vocab/\',\n                        help=\'Path to saved vocabulary json files.\')\n    parser.add_argument(\'--margin\', default=0.2, type=float,\n                        help=\'Rank loss margin.\')\n    parser.add_argument(\'--num_epochs\', default=30, type=int,\n                        help=\'Number of training epochs.\')\n    parser.add_argument(\'--batch_size\', default=128, type=int,\n                        help=\'Size of a training mini-batch.\')\n    parser.add_argument(\'--word_dim\', default=300, type=int,\n                        help=\'Dimensionality of the word embedding.\')\n    parser.add_argument(\'--embed_size\', default=1024, type=int,\n                        help=\'Dimensionality of the joint embedding.\')\n    parser.add_argument(\'--grad_clip\', default=2., type=float,\n                        help=\'Gradient clipping threshold.\')\n    parser.add_argument(\'--num_layers\', default=1, type=int,\n                        help=\'Number of GRU layers.\')\n    parser.add_argument(\'--learning_rate\', default=.0002, type=float,\n                        help=\'Initial learning rate.\')\n    parser.add_argument(\'--lr_update\', default=15, type=int,\n                        help=\'Number of epochs to update the learning rate.\')\n    parser.add_argument(\'--workers\', default=10, type=int,\n                        help=\'Number of data loader workers.\')\n    parser.add_argument(\'--log_step\', default=10, type=int,\n                        help=\'Number of steps to print and record the log.\')\n    parser.add_argument(\'--val_step\', default=500, type=int,\n                        help=\'Number of steps to run validation.\')\n    parser.add_argument(\'--logger_name\', default=\'./runs/runX/log\',\n                        help=\'Path to save Tensorboard log.\')\n    parser.add_argument(\'--model_name\', default=\'./runs/runX/checkpoint\',\n                        help=\'Path to save the model.\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                        help=\'path to latest checkpoint (default: none)\')\n    parser.add_argument(\'--max_violation\', action=\'store_true\',\n                        help=\'Use max instead of sum in the rank loss.\')\n    parser.add_argument(\'--img_dim\', default=2048, type=int,\n                        help=\'Dimensionality of the image embedding.\')\n    parser.add_argument(\'--no_imgnorm\', action=\'store_true\',\n                        help=\'Do not normalize the image embeddings.\')\n    parser.add_argument(\'--no_txtnorm\', action=\'store_true\',\n                        help=\'Do not normalize the text embeddings.\')\n    parser.add_argument(\'--raw_feature_norm\', default=""clipped_l2norm"",\n                        help=\'clipped_l2norm|l2norm|clipped_l1norm|l1norm|no_norm|softmax\')\n    parser.add_argument(\'--agg_func\', default=""LogSumExp"",\n                        help=\'LogSumExp|Mean|Max|Sum\')\n    parser.add_argument(\'--cross_attn\', default=""t2i"",\n                        help=\'t2i|i2t\')\n    parser.add_argument(\'--precomp_enc_type\', default=""basic"",\n                        help=\'basic|weight_norm\')\n    parser.add_argument(\'--bi_gru\', action=\'store_true\',\n                        help=\'Use bidirectional GRU.\')\n    parser.add_argument(\'--lambda_lse\', default=6., type=float,\n                        help=\'LogSumExp temp.\')\n    parser.add_argument(\'--lambda_softmax\', default=9., type=float,\n                        help=\'Attention softmax temperature.\')\n    opt = parser.parse_args()\n    print(opt)\n\n    logging.basicConfig(format=\'%(asctime)s %(message)s\', level=logging.INFO)\n    tb_logger.configure(opt.logger_name, flush_secs=5)\n\n    # Load Vocabulary Wrapper\n    vocab = deserialize_vocab(os.path.join(opt.vocab_path, \'%s_vocab.json\' % opt.data_name))\n    opt.vocab_size = len(vocab)\n\n    # Load data loaders\n    train_loader, val_loader = data.get_loaders(\n        opt.data_name, vocab, opt.batch_size, opt.workers, opt)\n\n    # Construct the model\n    model = SCAN(opt)\n\n    best_rsum = 0\n    start_epoch = 0\n    # optionally resume from a checkpoint\n    if opt.resume:\n        if os.path.isfile(opt.resume):\n            print(""=> loading checkpoint \'{}\'"".format(opt.resume))\n            checkpoint = torch.load(opt.resume)\n            start_epoch = checkpoint[\'epoch\'] + 1\n            best_rsum = checkpoint[\'best_rsum\']\n            model.load_state_dict(checkpoint[\'model\'])\n            # Eiters is used to show logs as the continuation of another\n            # training\n            model.Eiters = checkpoint[\'Eiters\']\n            print(""=> loaded checkpoint \'{}\' (epoch {}, best_rsum {})""\n                  .format(opt.resume, start_epoch, best_rsum))\n            validate(opt, val_loader, model)\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(opt.resume))\n\n    # Train the Model\n    for epoch in range(start_epoch, opt.num_epochs):\n        print(opt.logger_name)\n        print(opt.model_name)\n\n        adjust_learning_rate(opt, model.optimizer, epoch)\n\n        # train for one epoch\n        train(opt, train_loader, model, epoch, val_loader)\n\n        # evaluate on validation set\n        rsum = validate(opt, val_loader, model)\n\n        # remember best R@ sum and save checkpoint\n        is_best = rsum > best_rsum\n        best_rsum = max(rsum, best_rsum)\n        if not os.path.exists(opt.model_name):\n            os.mkdir(opt.model_name)\n        save_checkpoint({\n            \'epoch\': epoch,\n            \'model\': model.state_dict(),\n            \'best_rsum\': best_rsum,\n            \'opt\': opt,\n            \'Eiters\': model.Eiters,\n        }, is_best, filename=\'checkpoint_{}.pth.tar\'.format(epoch), prefix=opt.model_name + \'/\')\n\n\ndef train(opt, train_loader, model, epoch, val_loader):\n    # average meters to record the training statistics\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    train_logger = LogCollector()\n\n    end = time.time()\n    for i, train_data in enumerate(train_loader):\n        # switch to train mode\n        model.train_start()\n\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        # make sure train logger is used\n        model.logger = train_logger\n\n        # Update the model\n        model.train_emb(*train_data)\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # Print log info\n        if model.Eiters % opt.log_step == 0:\n            logging.info(\n                \'Epoch: [{0}][{1}/{2}]\\t\'\n                \'{e_log}\\t\'\n                \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                .format(\n                    epoch, i, len(train_loader), batch_time=batch_time,\n                    data_time=data_time, e_log=str(model.logger)))\n\n        # Record logs in tensorboard\n        tb_logger.log_value(\'epoch\', epoch, step=model.Eiters)\n        tb_logger.log_value(\'step\', i, step=model.Eiters)\n        tb_logger.log_value(\'batch_time\', batch_time.val, step=model.Eiters)\n        tb_logger.log_value(\'data_time\', data_time.val, step=model.Eiters)\n        model.logger.tb_log(tb_logger, step=model.Eiters)\n\n        # validate at every val_step\n        if model.Eiters % opt.val_step == 0:\n            validate(opt, val_loader, model)\n\n\ndef validate(opt, val_loader, model):\n    # compute the encoding for all the validation images and captions\n    img_embs, cap_embs, cap_lens = encode_data(\n        model, val_loader, opt.log_step, logging.info)\n\n    img_embs = numpy.array([img_embs[i] for i in range(0, len(img_embs), 5)])\n\n    start = time.time()\n    if opt.cross_attn == \'t2i\':\n        sims = shard_xattn_t2i(img_embs, cap_embs, cap_lens, opt, shard_size=128)\n    elif opt.cross_attn == \'i2t\':\n        sims = shard_xattn_i2t(img_embs, cap_embs, cap_lens, opt, shard_size=128)\n    else:\n        raise NotImplementedError\n    end = time.time()\n    print(""calculate similarity time:"", end-start)\n\n    # caption retrieval\n    (r1, r5, r10, medr, meanr) = i2t(img_embs, cap_embs, cap_lens, sims)\n    logging.info(""Image to text: %.1f, %.1f, %.1f, %.1f, %.1f"" %\n                 (r1, r5, r10, medr, meanr))\n    # image retrieval\n    (r1i, r5i, r10i, medri, meanr) = t2i(\n        img_embs, cap_embs, cap_lens, sims)\n    logging.info(""Text to image: %.1f, %.1f, %.1f, %.1f, %.1f"" %\n                 (r1i, r5i, r10i, medri, meanr))\n    # sum of recalls to be used for early stopping\n    currscore = r1 + r5 + r10 + r1i + r5i + r10i\n\n    # record metrics in tensorboard\n    tb_logger.log_value(\'r1\', r1, step=model.Eiters)\n    tb_logger.log_value(\'r5\', r5, step=model.Eiters)\n    tb_logger.log_value(\'r10\', r10, step=model.Eiters)\n    tb_logger.log_value(\'medr\', medr, step=model.Eiters)\n    tb_logger.log_value(\'meanr\', meanr, step=model.Eiters)\n    tb_logger.log_value(\'r1i\', r1i, step=model.Eiters)\n    tb_logger.log_value(\'r5i\', r5i, step=model.Eiters)\n    tb_logger.log_value(\'r10i\', r10i, step=model.Eiters)\n    tb_logger.log_value(\'medri\', medri, step=model.Eiters)\n    tb_logger.log_value(\'meanr\', meanr, step=model.Eiters)\n    tb_logger.log_value(\'rsum\', currscore, step=model.Eiters)\n\n    return currscore\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\', prefix=\'\'):\n    tries = 15\n    error = None\n\n    # deal with unstable I/O. Usually not necessary.\n    while tries:\n        try:\n            torch.save(state, prefix + filename)\n            if is_best:\n                shutil.copyfile(prefix + filename, prefix + \'model_best.pth.tar\')\n        except IOError as e:\n            error = e\n            tries -= 1\n        else:\n            break\n        print(\'model save {} failed, remaining {} trials\'.format(filename, tries))\n        if not tries:\n            raise error\n\n\ndef adjust_learning_rate(opt, optimizer, epoch):\n    """"""Sets the learning rate to the initial LR\n       decayed by 10 every 30 epochs""""""\n    lr = opt.learning_rate * (0.1 ** (epoch // opt.lr_update))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\nif __name__ == \'__main__\':\n    main()\n'"
vocab.py,0,"b'# -----------------------------------------------------------\n# Stacked Cross Attention Network implementation based on \n# https://arxiv.org/abs/1803.08024.\n# ""Stacked Cross Attention for Image-Text Matching""\n# Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He\n#\n# Writen by Kuang-Huei Lee, 2018\n# ---------------------------------------------------------------\n""""""Vocabulary wrapper""""""\n\nimport nltk\nfrom collections import Counter\nimport argparse\nimport os\nimport json\n\nannotations = {\n    \'coco_precomp\': [\'train_caps.txt\', \'dev_caps.txt\'],\n    \'f30k_precomp\': [\'train_caps.txt\', \'dev_caps.txt\'],\n}\n\n\nclass Vocabulary(object):\n    """"""Simple vocabulary wrapper.""""""\n\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n\n    def __call__(self, word):\n        if word not in self.word2idx:\n            return self.word2idx[\'<unk>\']\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.word2idx)\n\n\ndef serialize_vocab(vocab, dest):\n    d = {}\n    d[\'word2idx\'] = vocab.word2idx\n    d[\'idx2word\'] = vocab.idx2word\n    d[\'idx\'] = vocab.idx\n    with open(dest, ""w"") as f:\n        json.dump(d, f)\n\n\ndef deserialize_vocab(src):\n    with open(src) as f:\n        d = json.load(f)\n    vocab = Vocabulary()\n    vocab.word2idx = d[\'word2idx\']\n    vocab.idx2word = d[\'idx2word\']\n    vocab.idx = d[\'idx\']\n    return vocab\n\n\ndef from_txt(txt):\n    captions = []\n    with open(txt, \'rb\') as f:\n        for line in f:\n            captions.append(line.strip())\n    return captions\n\n\ndef build_vocab(data_path, data_name, caption_file, threshold):\n    """"""Build a simple vocabulary wrapper.""""""\n    counter = Counter()\n    for path in caption_file[data_name]:\n        full_path = os.path.join(os.path.join(data_path, data_name), path)\n        captions = from_txt(full_path)\n        for i, caption in enumerate(captions):\n            tokens = nltk.tokenize.word_tokenize(\n                caption.lower().decode(\'utf-8\'))\n            counter.update(tokens)\n\n            if i % 1000 == 0:\n                print(""[%d/%d] tokenized the captions."" % (i, len(captions)))\n\n    # Discard if the occurrence of the word is less than min_word_cnt.\n    words = [word for word, cnt in counter.items() if cnt >= threshold]\n\n    # Create a vocab wrapper and add some special tokens.\n    vocab = Vocabulary()\n    vocab.add_word(\'<pad>\')\n    vocab.add_word(\'<start>\')\n    vocab.add_word(\'<end>\')\n    vocab.add_word(\'<unk>\')\n\n    # Add words to the vocabulary.\n    for i, word in enumerate(words):\n        vocab.add_word(word)\n    return vocab\n\n\ndef main(data_path, data_name):\n    vocab = build_vocab(data_path, data_name, caption_file=annotations, threshold=4)\n    serialize_vocab(vocab, \'./vocab/%s_vocab.json\' % data_name)\n    print(""Saved vocabulary file to "", \'./vocab/%s_vocab.json\' % data_name)\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_path\', default=\'data\')\n    parser.add_argument(\'--data_name\', default=\'f30k_precomp\',\n                        help=\'{coco,f30k}_precomp\')\n    opt = parser.parse_args()\n    main(opt.data_path, opt.data_name)\n'"
util/convert_data.py,0,"b'# -----------------------------------------------------------\n# Stacked Cross Attention Network implementation based on \n# https://arxiv.org/abs/1803.08024.\n# ""Stacked Cross Attention for Image-Text Matching""\n# Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He\n#\n# Writen by Kuang-Huei Lee, 2018\n# ---------------------------------------------------------------\n""""""Convert image features from bottom up attention to numpy array""""""\nimport os\nimport base64\nimport csv\nimport sys\nimport zlib\nimport json\nimport argparse\n\nimport numpy as np\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--imgid_list\', default=\'data/coco_precomp/train_ids.txt\',\n                    help=\'Path to list of image id\')\nparser.add_argument(\'--input_file\', default=\'/media/data/kualee/coco_bottom_up_feature/trainval_36/trainval_resnet101_faster_rcnn_genome_36.tsv\',\n                    help=\'tsv of all image data (output of bottom-up-attention/tools/generate_tsv.py), \\\n                    where each columns are: [image_id, image_w, image_h, num_boxes, boxes, features].\')\nparser.add_argument(\'--output_dir\', default=\'data/coco_precomp/\',\n                    help=\'Output directory.\')\nparser.add_argument(\'--split\', default=\'train\',\n                    help=\'train|dev|test\')\nopt = parser.parse_args()\nprint(opt)\n\n\nmeta = []\nfeature = {}\nfor line in open(opt.imgid_list):\n    sid = int(line.strip())\n    meta.append(sid)\n    feature[sid] = None\n\ncsv.field_size_limit(sys.maxsize)\nFIELDNAMES = [\'image_id\', \'image_w\', \'image_h\', \'num_boxes\', \'boxes\', \'features\']\n\nif __name__ == \'__main__\':\n    with open(opt.input_file, ""r+b"") as tsv_in_file:\n        reader = csv.DictReader(tsv_in_file, delimiter=\'\\t\', fieldnames = FIELDNAMES)\n        for item in reader:\n            item[\'image_id\'] = int(item[\'image_id\'])\n            item[\'image_h\'] = int(item[\'image_h\'])\n            item[\'image_w\'] = int(item[\'image_w\'])\n            item[\'num_boxes\'] = int(item[\'num_boxes\'])\n            for field in [\'boxes\', \'features\']:\n                data = item[field]\n                buf = base64.decodestring(data)\n                temp = np.frombuffer(buf, dtype=np.float32)\n                item[field] = temp.reshape((item[\'num_boxes\'],-1))\n            if item[\'image_id\'] in feature:\n                feature[item[\'image_id\']] = item[\'features\']\n    data_out = np.stack([feature[sid] for sid in meta], axis=0)\n    print(""Final numpy array shape:"", data_out.shape)\n    np.save(os.path.join(opt.output_dir, \'{}_ims.npy\'.format(opt.split)), data_out)\n'"
