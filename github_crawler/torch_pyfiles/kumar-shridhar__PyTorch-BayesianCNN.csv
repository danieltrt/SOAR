file_path,api_count,code
__init__.py,0,b''
config_bayesian.py,0,"b""############### Configuration file for Bayesian ###############\nlayer_type = 'lrt'  # 'bbb' or 'lrt'\nactivation_type = 'softplus'  # 'softplus' or 'relu'\npriors={\n    'prior_mu': 0,\n    'prior_sigma': 0.1,\n    'posterior_mu_initial': (0, 0.1),  # (mean, std) normal_\n    'posterior_rho_initial': (-5, 0.1),  # (mean, std) normal_\n}\n\nn_epochs = 200\nlr_start = 0.001\nnum_workers = 4\nvalid_size = 0.2\nbatch_size = 256\ntrain_ens = 1\nvalid_ens = 1\nbeta_type = 0.1  # 'Blundell', 'Standard', etc. Use float for const value\n"""
config_frequentist.py,0,b'############### Configuration file for Frequentist ###############\nn_epochs = 200\nlr = 0.001\nnum_workers = 4\nvalid_size = 0.2\nbatch_size = 256\n'
main_bayesian.py,6,"b'from __future__ import print_function\n\nimport os\nimport argparse\n\nimport torch\nimport numpy as np\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.nn import functional as F\n\nimport data\nimport utils\nimport metrics\nimport config_bayesian as cfg\nfrom models.BayesianModels.Bayesian3Conv3FC import BBB3Conv3FC\nfrom models.BayesianModels.BayesianAlexNet import BBBAlexNet\nfrom models.BayesianModels.BayesianLeNet import BBBLeNet\n\n# CUDA settings\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\ndef getModel(net_type, inputs, outputs, priors, layer_type, activation_type):\n    if (net_type == \'lenet\'):\n        return BBBLeNet(outputs, inputs, priors, layer_type, activation_type)\n    elif (net_type == \'alexnet\'):\n        return BBBAlexNet(outputs, inputs, priors, layer_type, activation_type)\n    elif (net_type == \'3conv3fc\'):\n        return BBB3Conv3FC(outputs, inputs, priors, layer_type, activation_type)\n    else:\n        raise ValueError(\'Network should be either [LeNet / AlexNet / 3Conv3FC\')\n\n\ndef train_model(net, optimizer, criterion, trainloader, num_ens=1, beta_type=0.1, epoch=None, num_epochs=None):\n    net.train()\n    training_loss = 0.0\n    accs = []\n    kl_list = []\n    for i, (inputs, labels) in enumerate(trainloader, 1):\n\n        optimizer.zero_grad()\n\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n\n        kl = 0.0\n        for j in range(num_ens):\n            net_out, _kl = net(inputs)\n            kl += _kl\n            outputs[:, :, j] = F.log_softmax(net_out, dim=1)\n        \n        kl = kl / num_ens\n        kl_list.append(kl.item())\n        log_outputs = utils.logmeanexp(outputs, dim=2)\n\n        beta = metrics.get_beta(i-1, len(trainloader), beta_type, epoch, num_epochs)\n        loss = criterion(log_outputs, labels, kl, beta)\n        loss.backward()\n        optimizer.step()\n\n        accs.append(metrics.acc(log_outputs.data, labels))\n        training_loss += loss.cpu().data.numpy()\n    return training_loss/len(trainloader), np.mean(accs), np.mean(kl_list)\n\n\ndef validate_model(net, criterion, validloader, num_ens=1, beta_type=0.1, epoch=None, num_epochs=None):\n    """"""Calculate ensemble accuracy and NLL Loss""""""\n    net.train()\n    valid_loss = 0.0\n    accs = []\n\n    for i, (inputs, labels) in enumerate(validloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n        kl = 0.0\n        for j in range(num_ens):\n            net_out, _kl = net(inputs)\n            kl += _kl\n            outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n\n        log_outputs = utils.logmeanexp(outputs, dim=2)\n\n        beta = metrics.get_beta(i-1, len(validloader), beta_type, epoch, num_epochs)\n        valid_loss += criterion(log_outputs, labels, kl, beta).item()\n        accs.append(metrics.acc(log_outputs, labels))\n\n    return valid_loss/len(validloader), np.mean(accs)\n\n\ndef run(dataset, net_type):\n\n    # Hyper Parameter settings\n    layer_type = cfg.layer_type\n    activation_type = cfg.activation_type\n    priors = cfg.priors\n\n    train_ens = cfg.train_ens\n    valid_ens = cfg.valid_ens\n    n_epochs = cfg.n_epochs\n    lr_start = cfg.lr_start\n    num_workers = cfg.num_workers\n    valid_size = cfg.valid_size\n    batch_size = cfg.batch_size\n    beta_type = cfg.beta_type\n\n    trainset, testset, inputs, outputs = data.getDataset(dataset)\n    train_loader, valid_loader, test_loader = data.getDataloader(\n        trainset, testset, valid_size, batch_size, num_workers)\n    net = getModel(net_type, inputs, outputs, priors, layer_type, activation_type).to(device)\n\n    ckpt_dir = f\'checkpoints/{dataset}/bayesian\'\n    ckpt_name = f\'checkpoints/{dataset}/bayesian/model_{net_type}_{layer_type}_{activation_type}.pt\'\n\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir, exist_ok=True)\n\n    criterion = metrics.ELBO(len(trainset)).to(device)\n    optimizer = Adam(net.parameters(), lr=lr_start)\n    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n    valid_loss_max = np.Inf\n    for epoch in range(n_epochs):  # loop over the dataset multiple times\n\n        train_loss, train_acc, train_kl = train_model(net, optimizer, criterion, train_loader, num_ens=train_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n        valid_loss, valid_acc = validate_model(net, criterion, valid_loader, num_ens=valid_ens, beta_type=beta_type, epoch=epoch, num_epochs=n_epochs)\n        lr_sched.step(valid_loss)\n\n        print(\'Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}\'.format(\n            epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n\n        # save model if validation accuracy has increased\n        if valid_loss <= valid_loss_max:\n            print(\'Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\'.format(\n                valid_loss_max, valid_loss))\n            torch.save(net.state_dict(), ckpt_name)\n            valid_loss_max = valid_loss\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description = ""PyTorch Bayesian Model Training"")\n    parser.add_argument(\'--net_type\', default=\'lenet\', type=str, help=\'model\')\n    parser.add_argument(\'--dataset\', default=\'MNIST\', type=str, help=\'dataset = [MNIST/CIFAR10/CIFAR100]\')\n    args = parser.parse_args()\n\n    run(args.dataset, args.net_type)\n'"
main_frequentist.py,4,"b'from __future__ import print_function\n\nimport os\nimport argparse\n\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.optim import Adam, lr_scheduler\n\nimport data\nimport utils\nimport metrics\nimport config_frequentist as cfg\nfrom models.NonBayesianModels.AlexNet import AlexNet\nfrom models.NonBayesianModels.LeNet import LeNet\nfrom models.NonBayesianModels.ThreeConvThreeFC import ThreeConvThreeFC\n\n# CUDA settings\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\ndef getModel(net_type, inputs, outputs):\n    if (net_type == \'lenet\'):\n        return LeNet(outputs, inputs)\n    elif (net_type == \'alexnet\'):\n        return AlexNet(outputs, inputs)\n    elif (net_type == \'3conv3fc\'):\n        return ThreeConvThreeFC(outputs, inputs)\n    else:\n        raise ValueError(\'Network should be either [LeNet / AlexNet / 3Conv3FC\')\n\n\ndef train_model(net, optimizer, criterion, train_loader):\n    train_loss = 0.0\n    net.train()\n    accs = []\n    for data, target in train_loader:\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = net(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*data.size(0)\n        accs.append(metrics.acc(output.detach(), target))\n    return train_loss, np.mean(accs)\n\n\ndef validate_model(net, criterion, valid_loader):\n    valid_loss = 0.0\n    net.eval()\n    accs = []\n    for data, target in valid_loader:\n        data, target = data.to(device), target.to(device)\n        output = net(data)\n        loss = criterion(output, target)\n        valid_loss += loss.item()*data.size(0)\n        accs.append(metrics.acc(output.detach(), target))\n    return valid_loss, np.mean(accs)\n\n\ndef run(dataset, net_type):\n\n    # Hyper Parameter settings\n    n_epochs = cfg.n_epochs\n    lr = cfg.lr\n    num_workers = cfg.num_workers\n    valid_size = cfg.valid_size\n    batch_size = cfg.batch_size\n\n    trainset, testset, inputs, outputs = data.getDataset(dataset)\n    train_loader, valid_loader, test_loader = data.getDataloader(\n        trainset, testset, valid_size, batch_size, num_workers)\n    net = getModel(net_type, inputs, outputs).to(device)\n\n    ckpt_dir = f\'checkpoints/{dataset}/frequentist\'\n    ckpt_name = f\'checkpoints/{dataset}/frequentist/model_{net_type}.pt\'\n\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir, exist_ok=True)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(net.parameters(), lr=lr)\n    lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n    valid_loss_min = np.Inf\n    for epoch in range(1, n_epochs+1):\n\n        train_loss, train_acc = train_model(net, optimizer, criterion, train_loader)\n        valid_loss, valid_acc = validate_model(net, criterion, valid_loader)\n        lr_sched.step(valid_loss)\n\n        train_loss = train_loss/len(train_loader.dataset)\n        valid_loss = valid_loss/len(valid_loader.dataset)\n            \n        print(\'Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f}\'.format(\n            epoch, train_loss, train_acc, valid_loss, valid_acc))\n        \n        # save model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print(\'Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\'.format(\n                valid_loss_min, valid_loss))\n            torch.save(net.state_dict(), ckpt_name)\n            valid_loss_min = valid_loss\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description = ""PyTorch Frequentist Model Training"")\n    parser.add_argument(\'--net_type\', default=\'lenet\', type=str, help=\'model\')\n    parser.add_argument(\'--dataset\', default=\'MNIST\', type=str, help=\'dataset = [MNIST/CIFAR10/CIFAR100]\')\n    args = parser.parse_args()\n\n    run(args.dataset, args.net_type)\n'"
metrics.py,2,"b'import numpy as np\nimport torch.nn.functional as F\nfrom torch import nn\nimport torch\n\n\nclass ELBO(nn.Module):\n    def __init__(self, train_size):\n        super(ELBO, self).__init__()\n        self.train_size = train_size\n\n    def forward(self, input, target, kl, beta):\n        assert not target.requires_grad\n        return F.nll_loss(input, target, reduction=\'mean\') * self.train_size + beta * kl\n\n\n# def lr_linear(epoch_num, decay_start, total_epochs, start_value):\n#     if epoch_num < decay_start:\n#         return start_value\n#     return start_value*float(total_epochs-epoch_num)/float(total_epochs-decay_start)\n\n\ndef acc(outputs, targets):\n    return np.mean(outputs.cpu().numpy().argmax(axis=1) == targets.data.cpu().numpy())\n\n\ndef calculate_kl(mu_p, sig_p, mu_q, sig_q):\n    kl = 0.5 * (2 * torch.log(sig_p / sig_q) - 1 + (sig_q / sig_p).pow(2) + ((mu_p - mu_q) / sig_p).pow(2)).sum()\n    return kl\n\n\ndef get_beta(batch_idx, m, beta_type, epoch, num_epochs):\n    if type(beta_type) is float:\n        return beta_type\n\n    if beta_type == ""Blundell"":\n        beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1)\n    elif beta_type == ""Soenderby"":\n        if epoch is None or num_epochs is None:\n            raise ValueError(\'Soenderby method requires both epoch and num_epochs to be passed.\')\n        beta = min(epoch / (num_epochs // 4), 1)\n    elif beta_type == ""Standard"":\n        beta = 1 / m\n    else:\n        beta = 0\n    return beta\n'"
uncertainty_estimation.py,11,"b'import argparse\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom PIL import Image\nimport torchvision\nfrom torch.nn import functional as F\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\nimport data\nfrom main_bayesian import getModel\nimport config_bayesian as cfg\n\n\n# CUDA settings\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nmnist_set = None\nnotmnist_set = None\n\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    ])\n\n\ndef init_dataset(notmnist_dir):\n    global mnist_set\n    global notmnist_set\n    mnist_set, _, _, _ = data.getDataset(\'MNIST\')\n    notmnist_set = torchvision.datasets.ImageFolder(root=notmnist_dir)\n\n\ndef get_uncertainty_per_image(model, input_image, T=15, normalized=False):\n    input_image = input_image.unsqueeze(0)\n    input_images = input_image.repeat(T, 1, 1, 1)\n\n    net_out, _ = model(input_images)\n    pred = torch.mean(net_out, dim=0).cpu().detach().numpy()\n    if normalized:\n        prediction = F.softplus(net_out)\n        p_hat = prediction / torch.sum(prediction, dim=1).unsqueeze(1)\n    else:\n        p_hat = F.softmax(net_out, dim=1)\n    p_hat = p_hat.detach().cpu().numpy()\n    p_bar = np.mean(p_hat, axis=0)\n\n    temp = p_hat - np.expand_dims(p_bar, 0)\n    epistemic = np.dot(temp.T, temp) / T\n    epistemic = np.diag(epistemic)\n\n    aleatoric = np.diag(p_bar) - (np.dot(p_hat.T, p_hat) / T)\n    aleatoric = np.diag(aleatoric)\n\n    return pred, epistemic, aleatoric\n\n\ndef get_uncertainty_per_batch(model, batch, T=15, normalized=False):\n    batch_predictions = []\n    net_outs = []\n    batches = batch.unsqueeze(0).repeat(T, 1, 1, 1, 1)\n\n    preds = []\n    epistemics = []\n    aleatorics = []\n    \n    for i in range(T):  # for T batches\n        net_out, _ = model(batches[i].cuda())\n        net_outs.append(net_out)\n        if normalized:\n            prediction = F.softplus(net_out)\n            prediction = prediction / torch.sum(prediction, dim=1).unsqueeze(1)\n        else:\n            prediction = F.softmax(net_out, dim=1)\n        batch_predictions.append(prediction)\n    \n    for sample in range(batch.shape[0]):\n        # for each sample in a batch\n        pred = torch.cat([a_batch[sample].unsqueeze(0) for a_batch in net_outs], dim=0)\n        pred = torch.mean(pred, dim=0)\n        preds.append(pred)\n\n        p_hat = torch.cat([a_batch[sample].unsqueeze(0) for a_batch in batch_predictions], dim=0).detach().cpu().numpy()\n        p_bar = np.mean(p_hat, axis=0)\n\n        temp = p_hat - np.expand_dims(p_bar, 0)\n        epistemic = np.dot(temp.T, temp) / T\n        epistemic = np.diag(epistemic)\n        epistemics.append(epistemic)\n\n        aleatoric = np.diag(p_bar) - (np.dot(p_hat.T, p_hat) / T)\n        aleatoric = np.diag(aleatoric)\n        aleatorics.append(aleatoric)\n\n    epistemic = np.vstack(epistemics)  # (batch_size, categories)\n    aleatoric = np.vstack(aleatorics)  # (batch_size, categories)\n    preds = torch.cat([i.unsqueeze(0) for i in preds]).cpu().detach().numpy()  # (batch_size, categories)\n\n    return preds, epistemic, aleatoric\n\n\ndef get_sample(dataset, sample_type=\'mnist\'):\n    idx = np.random.randint(len(dataset.targets))\n    if sample_type==\'mnist\':\n        sample = dataset.data[idx]\n        truth = dataset.targets[idx]\n    else:\n        path, truth = dataset.samples[idx]\n        sample = torch.from_numpy(np.array(Image.open(path)))\n\n    sample = sample.unsqueeze(0)\n    sample = transform(sample)\n    return sample.to(device), truth\n\n\ndef run(net_type, weight_path, notmnist_dir):\n    init_dataset(notmnist_dir)\n\n    layer_type = cfg.layer_type\n    activation_type = cfg.activation_type\n\n    net = getModel(net_type, 1, 10, priors=None, layer_type=layer_type, activation_type=activation_type)\n    net.load_state_dict(torch.load(weight_path))\n    net.train()\n    net.to(device)\n\n    fig = plt.figure()\n    fig.suptitle(\'Uncertainty Estimation\', fontsize=\'x-large\')\n    mnist_img = fig.add_subplot(321)\n    notmnist_img = fig.add_subplot(322)\n    epi_stats_norm = fig.add_subplot(323)\n    ale_stats_norm = fig.add_subplot(324)\n    epi_stats_soft = fig.add_subplot(325)\n    ale_stats_soft = fig.add_subplot(326)\n\n    sample_mnist, truth_mnist = get_sample(mnist_set)\n    pred_mnist, epi_mnist_norm, ale_mnist_norm = get_uncertainty_per_image(net, sample_mnist, T=25, normalized=True)\n    pred_mnist, epi_mnist_soft, ale_mnist_soft = get_uncertainty_per_image(net, sample_mnist, T=25, normalized=False)\n    mnist_img.imshow(sample_mnist.squeeze().cpu(), cmap=\'gray\')\n    mnist_img.axis(\'off\')\n    mnist_img.set_title(\'MNIST Truth: {} Prediction: {}\'.format(int(truth_mnist), int(np.argmax(pred_mnist))))\n\n    sample_notmnist, truth_notmnist = get_sample(notmnist_set, sample_type=\'notmnist\')\n    pred_notmnist, epi_notmnist_norm, ale_notmnist_norm = get_uncertainty_per_image(net, sample_notmnist, T=25, normalized=True)\n    pred_notmnist, epi_notmnist_soft, ale_notmnist_soft = get_uncertainty_per_image(net, sample_notmnist, T=25, normalized=False)\n    notmnist_img.imshow(sample_notmnist.squeeze().cpu(), cmap=\'gray\')\n    notmnist_img.axis(\'off\')\n    notmnist_img.set_title(\'notMNIST Truth: {}({}) Prediction: {}({})\'.format(\n        int(truth_notmnist), chr(65 + truth_notmnist), int(np.argmax(pred_notmnist)), chr(65 + np.argmax(pred_notmnist))))\n\n    x = list(range(10))\n    data = pd.DataFrame({\n        \'epistemic_norm\': np.hstack([epi_mnist_norm, epi_notmnist_norm]),\n        \'aleatoric_norm\': np.hstack([ale_mnist_norm, ale_notmnist_norm]),\n        \'epistemic_soft\': np.hstack([epi_mnist_soft, epi_notmnist_soft]),\n        \'aleatoric_soft\': np.hstack([ale_mnist_soft, ale_notmnist_soft]),\n        \'category\': np.hstack([x, x]),\n        \'dataset\': np.hstack([[\'MNIST\']*10, [\'notMNIST\']*10])\n    })\n    print(data)\n    sns.barplot(x=\'category\', y=\'epistemic_norm\', hue=\'dataset\', data=data, ax=epi_stats_norm)\n    sns.barplot(x=\'category\', y=\'aleatoric_norm\', hue=\'dataset\', data=data, ax=ale_stats_norm)\n    epi_stats_norm.set_title(\'Epistemic Uncertainty (Normalized)\')\n    ale_stats_norm.set_title(\'Aleatoric Uncertainty (Normalized)\')\n\n    sns.barplot(x=\'category\', y=\'epistemic_soft\', hue=\'dataset\', data=data, ax=epi_stats_soft)\n    sns.barplot(x=\'category\', y=\'aleatoric_soft\', hue=\'dataset\', data=data, ax=ale_stats_soft)\n    epi_stats_soft.set_title(\'Epistemic Uncertainty (Softmax)\')\n    ale_stats_soft.set_title(\'Aleatoric Uncertainty (Softmax)\')\n\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description = ""PyTorch Uncertainty Estimation b/w MNIST and notMNIST"")\n    parser.add_argument(\'--net_type\', default=\'lenet\', type=str, help=\'model\')\n    parser.add_argument(\'--weights_path\', default=\'checkpoints/MNIST/bayesian/model_lenet.pt\', type=str, help=\'weights for model\')\n    parser.add_argument(\'--notmnist_dir\', default=\'data/notMNIST_small/\', type=str, help=\'weights for model\')\n    args = parser.parse_args()\n\n    run(args.net_type, args.weights_path, args.notmnist_dir)\n'"
utils.py,3,"b'import os\nimport torch\nimport numpy as np\nfrom torch.nn import functional as F\n\nimport config_bayesian as cfg\n\n\n# cifar10 classes\ncifar10_classes = [\'airplane\', \'automobile\', \'bird\', \'cat\', \'deer\',\n                   \'dog\', \'frog\', \'horse\', \'ship\', \'truck\']\n\n\ndef logmeanexp(x, dim=None, keepdim=False):\n    """"""Stable computation of log(mean(exp(x))""""""\n\n    \n    if dim is None:\n        x, dim = x.view(-1), 0\n    x_max, _ = torch.max(x, dim, keepdim=True)\n    x = x_max + torch.log(torch.mean(torch.exp(x - x_max), dim, keepdim=True))\n    return x if keepdim else x.squeeze(dim)\n\n# check if dimension is correct\n\n# def dimension_check(x, dim=None, keepdim=False):\n#     if dim is None:\n#         x, dim = x.view(-1), 0\n\n#     return x if keepdim else x.squeeze(dim)\n\n\ndef adjust_learning_rate(optimizer, lr):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef save_array_to_file(numpy_array, filename):\n    file = open(filename, \'a\')\n    shape = "" "".join(map(str, numpy_array.shape))\n    np.savetxt(file, numpy_array.flatten(), newline="" "", fmt=""%.3f"")\n    file.write(""\\n"")\n    file.close()\n'"
Mixtures/config_mixtures.py,0,b'############### Configuration file for Training of SplitMNIST and Mixtures ###############\nn_epochs = 5\nlr_start = 0.001\nnum_workers = 4\nvalid_size = 0.2\nbatch_size = 256\ntrain_ens = 15\nvalid_ens = 10\n'
Mixtures/gmm.py,48,"b'# Courtesy of https://github.com/ldeecke/gmm-torch\n\nimport torch\nimport numpy as np\n\nfrom math import pi\n\n\nclass GaussianMixture(torch.nn.Module):\n    """"""\n    Fits a mixture of k=1,..,K Gaussians to the input data. Input tensors are expected to be flat with dimensions (n: number of samples, d: number of features).\n    The model then extends them to (n, k: number of components, d).\n    The model parametrization (mu, sigma) is stored as (1, k, d), and probabilities are shaped (n, k, 1) if they relate to an individual sample, or (1, k, 1) if they assign membership probabilities to one of the mixture components.\n    """"""\n    def __init__(self, n_components, n_features, mu_init=None, var_init=None, eps=1.e-6):\n        """"""\n        Initializes the model and brings all tensors into their required shape. The class expects data to be fed as a flat tensor in (n, d). The class owns:\n            x:              torch.Tensor (n, k, d)\n            mu:             torch.Tensor (1, k, d)\n            var:            torch.Tensor (1, k, d)\n            pi:             torch.Tensor (1, k, 1)\n            eps:            float\n            n_components:   int\n            n_features:     int\n            score:          float\n        args:\n            n_components:   int\n            n_features:     int\n            mu_init:        torch.Tensor (1, k, d)\n            var_init:       torch.Tensor (1, k, d)\n            eps:            float\n        """"""\n\n        super(GaussianMixture, self).__init__()\n\n        self.eps = eps\n        self.n_components = n_components\n        self.n_features = n_features\n        self.log_likelihood = -np.inf\n\n        self.mu_init = mu_init\n        self.var_init = var_init\n\n        self._init_params()\n\n\n    def _init_params(self):\n        if self.mu_init is not None:\n            assert self.mu_init.size() == (1, self.n_components, self.n_features), ""Input mu_init does not have required tensor dimensions (1, %i, %i)"" % (self.n_components, self.n_features)\n            # (1, k, d)\n            self.mu = torch.nn.Parameter(self.mu_init, requires_grad=False)\n        else:\n            self.mu = torch.nn.Parameter(torch.randn(1, self.n_components, self.n_features), requires_grad=False)\n\n        if self.var_init is not None:\n            assert self.var_init.size() == (1, self.n_components, self.n_features), ""Input var_init does not have required tensor dimensions (1, %i, %i)"" % (self.n_components, self.n_features)\n            # (1, k, d)\n            self.var = torch.nn.Parameter(self.var_init, requires_grad=False)\n        else:\n            self.var = torch.nn.Parameter(torch.ones(1, self.n_components, self.n_features), requires_grad=False)\n\n        # (1, k, 1)\n        self.pi = torch.nn.Parameter(torch.Tensor(1, self.n_components, 1), requires_grad=False).fill_(1./self.n_components)\n\n        self.params_fitted = False\n\n\n    def bic(self, x):\n        """"""\n        Bayesian information criterion for samples x.\n        args:\n            x:      torch.Tensor (n, d) or (n, k, d)\n        returns:\n            bic:    float\n        """"""\n        n = x.shape[0]\n\n        if len(x.size()) == 2:\n            # (n, d) --> (n, k, d)\n            x = x.unsqueeze(1).expand(n, self.n_components, x.size(1))\n\n        bic = -2. * self.__score(self.pi, self.__p_k(x, self.mu, self.var), sum_data=True) * n + self.n_components * np.log(n)\n\n        return bic\n\n\n    def fit(self, x, warm_start=False, delta=1e-8, n_iter=1000):\n        """"""\n        Public method that fits data to the model.\n        args:\n            n_iter:     int\n            delta:      float\n        """"""\n\n        if not warm_start and self.params_fitted:\n            self._init_params()\n\n        if len(x.size()) == 2:\n            # (n, d) --> (n, k, d)\n            x = x.unsqueeze(1).expand(x.size(0), self.n_components, x.size(1))\n\n        i = 0\n        j = np.inf\n\n        while (i <= n_iter) and (j >= delta):\n\n            log_likelihood_old = self.log_likelihood\n            mu_old = self.mu\n            var_old = self.var\n\n            self.__em(x)\n            self.log_likelihood = self.__score(self.pi, self.__p_k(x, self.mu, self.var))\n\n            if (self.log_likelihood.abs() == float(""Inf"")) or (self.log_likelihood == float(""nan"")):\n                # when the log-likelihood assumes inane values, reinitialize model\n                self.__init__(self.n_components, self.n_features)\n\n            i += 1\n            j = self.log_likelihood - log_likelihood_old\n            if j <= delta:\n                # when the score decreases, revert to old parameters\n                self.__update_mu(mu_old)\n                self.__update_var(var_old)\n\n        self.params_fitted = True\n\n\n    def predict(self, x, probs=False):\n        """"""\n        Assigns input data to one of the mixture components by evaluating the likelihood under each. If probs=True returns normalized probabilities of class membership instead.\n        args:\n            x:          torch.Tensor (n, d) or (n, k, d)\n            probs:      bool\n        returns:\n            y:          torch.LongTensor (n)\n        """"""\n\n        if len(x.size()) == 2:\n            # (n, d) --> (n, k, d)\n            x = x.unsqueeze(1).expand(x.size(0), self.n_components, x.size(1))\n\n        p_k = self.__p_k(x, self.mu, self.var)\n        if probs:\n            return p_k / (p_k.sum(1, keepdim=True) + self.eps)\n        else:\n            _, predictions = torch.max(p_k, 1)\n            return torch.squeeze(predictions).type(torch.LongTensor)\n\n    def predict_proba(self, x):\n        """"""\n        Returns normalized probabilities of class membership.\n        args:\n            x:          torch.Tensor (n, d) or (n, k, d)\n        returns:\n            y:          torch.LongTensor (n)\n        """"""\n        return self.predict(x, probs=True)\n\n\n    def score_samples(self, x):\n        """"""\n        Computes log-likelihood of data (x) under the current model.\n        args:\n            x:          torch.Tensor (n, d) or (n, k, d)\n        returns:\n            score:      torch.LongTensor (n)\n        """"""\n        if len(x.size()) == 2:\n            # (n, d) --> (n, k, d)\n            x = x.unsqueeze(1).expand(x.size(0), self.n_components, x.size(1))\n\n        score = self.__score(self.pi, self.__p_k(x, self.mu, self.var), sum_data=False)\n        return score\n\n\n    def __p_k(self, x, mu, var):\n        """"""\n        Returns a tensor with dimensions (n, k, 1) indicating the likelihood of data belonging to the k-th Gaussian.\n        args:\n            x:      torch.Tensor (n, k, d)\n            mu:     torch.Tensor (1, k, d)\n            var:    torch.Tensor (1, k, d)\n        returns:\n            p_k:    torch.Tensor (n, k, 1)\n        """"""\n\n        # (1, k, d) --> (n, k, d)\n        mu = mu.expand(x.size(0), self.n_components, self.n_features)\n        var = var.expand(x.size(0), self.n_components, self.n_features)\n\n        # (n, k, d) --> (n, k, 1)\n        exponent = torch.exp(-.5 * torch.sum((x - mu) * (x - mu) / var, 2, keepdim=True))\n        # (n, k, d) --> (n, k, 1)\n        prefactor = torch.rsqrt(((2. * pi) ** self.n_features) * torch.prod(var, dim=2, keepdim=True) + self.eps)\n\n        return prefactor * exponent\n\n\n    def __e_step(self, pi, p_k):\n        """"""\n        Computes weights that indicate the probabilistic belief that a data point was generated by one of the k mixture components. This is the so-called expectation step of the EM-algorithm.\n        args:\n            pi:         torch.Tensor (1, k, 1)\n            p_k:        torch.Tensor (n, k, 1)\n        returns:\n            weights:    torch.Tensor (n, k, 1)\n        """"""\n\n        weights = pi * p_k\n        return torch.div(weights, torch.sum(weights, 1, keepdim=True) + self.eps)\n\n\n    def __m_step(self, x, weights):\n        """"""\n        Updates the model\'s parameters. This is the maximization step of the EM-algorithm.\n        args:\n            x:          torch.Tensor (n, k, d)\n            weights:    torch.Tensor (n, k, 1)\n        returns:\n            pi_new:     torch.Tensor (1, k, 1)\n            mu_new:     torch.Tensor (1, k, d)\n            var_new:    torch.Tensor (1, k, d)\n        """"""\n\n        # (n, k, 1) --> (1, k, 1)\n        n_k = torch.sum(weights, 0, keepdim=True)\n        pi_new = torch.div(n_k, torch.sum(n_k, 1, keepdim=True) + self.eps)\n        # (n, k, d) --> (1, k, d)\n        mu_new = torch.div(torch.sum(weights * x, 0, keepdim=True), n_k + self.eps)\n        # (n, k, d) --> (1, k, d)\n        var_new = torch.div(torch.sum(weights * (x - mu_new) * (x - mu_new), 0, keepdim=True), n_k + self.eps)\n\n        return pi_new, mu_new, var_new\n\n\n    def __em(self, x):\n        """"""\n        Performs one iteration of the expectation-maximization algorithm by calling the respective subroutines.\n        args:\n            x:          torch.Tensor (n, k, d)\n        """"""\n\n        weights = self.__e_step(self.pi, self.__p_k(x, self.mu, self.var))\n        pi_new, mu_new, var_new = self.__m_step(x, weights)\n\n        self.__update_pi(pi_new)\n        self.__update_mu(mu_new)\n        self.__update_var(var_new)\n\n\n    def __score(self, pi, p_k, sum_data=True):\n        """"""\n        Computes the log-likelihood of the data under the model.\n        args:\n            pi:         torch.Tensor (1, k, 1)\n            p_k:        torch.Tensor (n, k, 1)\n        """"""\n\n        weights = pi * p_k\n        if sum_data:\n            return torch.sum(torch.log(torch.sum(weights, 1) + self.eps))\n        else:\n            return torch.log(torch.sum(weights, 1) + self.eps)\n\n\n    def __update_mu(self, mu):\n        """"""\n        Updates mean to the provided value.\n        args:\n            mu:         torch.FloatTensor\n        """"""\n\n        assert mu.size() in [(self.n_components, self.n_features), (1, self.n_components, self.n_features)], ""Input mu does not have required tensor dimensions (%i, %i) or (1, %i, %i)"" % (self.n_components, self.n_features, self.n_components, self.n_features)\n\n        if mu.size() == (self.n_components, self.n_features):\n            self.mu = mu.unsqueeze(0)\n        elif mu.size() == (1, self.n_components, self.n_features):\n            self.mu.data = mu\n\n\n    def __update_var(self, var):\n        """"""\n        Updates variance to the provided value.\n        args:\n            var:        torch.FloatTensor\n        """"""\n\n        assert var.size() in [(self.n_components, self.n_features), (1, self.n_components, self.n_features)], ""Input var does not have required tensor dimensions (%i, %i) or (1, %i, %i)"" % (self.n_components, self.n_features, self.n_components, self.n_features)\n\n        if var.size() == (self.n_components, self.n_features):\n            self.var = var.unsqueeze(0)\n        elif var.size() == (1, self.n_components, self.n_features):\n            self.var.data = var\n\n\n    def __update_pi(self, pi):\n        """"""\n        Updates pi to the provided value.\n        args:\n            pi:         torch.FloatTensor\n        """"""\n\n        assert pi.size() in [(1, self.n_components, 1)], ""Input pi does not have required tensor dimensions (%i, %i, %i)"" % (1, self.n_components, 1)\n\n        self.pi.data = pi'"
Mixtures/main.py,5,"b'import sys\nsys.path.append(\'..\')\n\nimport os\nimport torch\nimport numpy as np\nfrom collections import OrderedDict\n\nimport gmm\nimport utils\nimport utils_mixture\nimport config_bayesian as cfg\n\n\ndef feedforward_and_save_mean_var(net, dataloader, task_no, num_ens=1):\n    cfg.mean_var_dir = ""Mixtures/mean_vars/task-{}/"".format(task_no)\n    if not os.path.exists(cfg.mean_var_dir):\n        os.makedirs(cfg.mean_var_dir, exist_ok=True)\n        cfg.record_mean_var = True\n        cfg.record_layers = None  # All layers\n        cfg.record_now = True\n        cfg.curr_batch_no = 0  # Not required\n        cfg.curr_epoch_no = 0  # Not required\n\n    net.train()  # To get distribution of mean and var\n    accs = []\n\n    for i, (inputs, labels) in enumerate(dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n        kl = 0.0\n        for j in range(num_ens):\n            net_out, _kl = net(inputs)\n            kl += _kl\n            outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n\n        log_outputs = utils.logmeanexp(outputs, dim=2)\n        accs.append(metrics.acc(log_outputs, labels))\n    return np.mean(accs)\n\n\ndef _get_ordered_layer_name(mean_var_path):\n    # Order files according to creation time\n    files = os.listdir(mean_var_path)\n    files = [os.path.join(mean_var_path, f) for f in files]\n    files.sort(key=os.path.getctime)\n    layer_names = [f.split(\'/\')[-1].split(\'.\')[0] for f in files]\n    return layer_names\n\n\ndef _get_layer_wise_mean_var_per_task(mean_var_path):\n    # Order files according to creation time\n    # To get the correct model architecture\n    files = os.listdir(mean_var_path)\n    files = [os.path.join(mean_var_path, f) for f in files]\n    files.sort(key=os.path.getctime)\n    layer_names = [f.split(\'/\')[-1].split(\'.\')[0] for f in files]\n\n    mean_var = OrderedDict()\n    for i in range(len(files)):\n        data = {}\n        mean, var = utils.load_mean_std_from_file(files[i])\n        mean, var = np.vstack(mean), np.vstack(var)  # shape is (len(trainset), output shape)\n        data[\'mean\'] = mean\n        data[\'var\'] = var\n        data[\'mean.mu\'] = np.mean(mean, axis=0)\n        data[\'var.mu\'] = np.mean(var, axis=0)\n        data[\'mean.var\'] = np.var(mean, axis=0)\n        data[\'var.var\'] = np.var(var, axis=0)\n        mean_var[layer_names[i]] = data\n    return mean_var\n\n\ndef get_mean_vars_for_all_tasks(mean_var_dir):\n    all_tasks = {}\n    for task in os.listdir(mean_var_dir):\n        path_to_task = os.path.join(mean_var_dir, task)\n        mean_var_per_task = _get_layer_wise_mean_var_per_task(path_to_task)\n        all_tasks[task] = mean_var_per_task\n    return all_tasks\n\n\ndef fit_to_gmm(num_tasks, layer_name, data_type, all_tasks):\n    data = np.vstack([all_tasks[f\'task-{i}\'][layer_name][data_type] for i in range(1, num_tasks+1)])\n    data = torch.tensor(data).float()\n    #data_mu = torch.cat([torch.tensor(all_tasks[f\'task-{i}\'][layer_name][data_type+\'.mu\']).unsqueeze(0) for i in range(1, num_tasks+1)], dim=0).float().unsqueeze(0)\n    #data_var = torch.cat([torch.tensor(all_tasks[f\'task-{i}\'][layer_name][data_type+\'.var\']).unsqueeze(0) for i in range(1, num_tasks+1)], dim=0).float().unsqueeze(0)\n    model = gmm.GaussianMixture(n_components=num_tasks, n_features=np.prod(data.shape[1:]))#, mu_init=data_mu, var_init=data_var)\n    data = data[torch.randperm(data.size()[0])]  # Shuffling of data\n    model.fit(data)\n    return model.predict(data)\n\n\ndef main():\n    num_tasks = 2\n    weights_dir = ""checkpoints/MNIST/bayesian/splitted/2-tasks/""\n\n    loader_task1, loader_task2 = utils_mixture.get_splitmnist_dataloaders(num_tasks)\n    train_loader_task1 = loader_task1[0]\n    train_loader_task2 = loader_task2[0]\n\n    net_task1, net_task2 = utils_mixture.get_splitmnist_models(num_tasks, True, weights_dir)\n    net_task1.cuda()\n    net_task2.cuda()\n\n    print(""Task-1 Accuracy:"", feedforward_and_save_mean_var(net_task1, train_loader_task1, task_no=1))\n    print(""Task-2 Accuracy:"", feedforward_and_save_mean_var(net_task2, train_loader_task2, task_no=2))\n\n    mean_vars_all_tasks = get_mean_vars_for_all_tasks(""Mixtures/mean_vars/"")\n    \n\n\nif __name__ == \'__main__\':\n    all_tasks = get_mean_vars_for_all_tasks(""Mixtures/mean_vars/"")\n    y = fit_to_gmm(2, \'fc3\', \'mean\', all_tasks)\n    print(""Cluster0"", (1-y).sum())\n    print(""Cluster1"", y.sum())\n'"
Mixtures/mixture_experiment.py,6,"b'import sys\nsys.path.append(\'..\')\n\nimport os\nimport datetime\nimport torch\nimport contextlib\n\nfrom utils_mixture import *\nfrom layers.BBBLinear import BBBLinear\n\n\n@contextlib.contextmanager\ndef print_to_logfile(file):\n    # capture all outputs to a log file while still printing it\n    class Logger:\n        def __init__(self, file):\n            self.terminal = sys.stdout\n            self.log = file\n\n        def write(self, message):\n            self.terminal.write(message)\n            self.log.write(message)\n\n        def __getattr__(self, attr):\n            return getattr(self.terminal, attr)\n\n    logger = Logger(file)\n\n    _stdout = sys.stdout\n    sys.stdout = logger\n    try:\n        yield logger.log\n    finally:\n        sys.stdout = _stdout\n\n\ndef initiate_experiment(experiment):\n\n    def decorator(*args, **kwargs):\n        log_file_dir = ""experiments/mixtures/""\n        log_file = log_file_dir + experiment.__name__ + "".txt""\n        if not os.path.exists(log_file):\n            os.makedirs(log_file_dir, exist_ok=True)\n        with print_to_logfile(open(log_file, \'a\')):\n            print(""Performing experiment:"", experiment.__name__)\n            print(""Date-Time:"", datetime.datetime.now())\n            print(""\\n"", end="""")\n            print(""Args:"", args)\n            print(""Kwargs:"", kwargs)\n            print(""\\n"", end="""")\n            experiment(*args, **kwargs)\n            print(""\\n\\n"", end="""")\n    return decorator\n\n\n@initiate_experiment\ndef experiment_regular_prediction_bayesian(weights_dir=None, num_ens=10):\n    num_tasks = 2\n    weights_dir = ""checkpoints/MNIST/bayesian/splitted/2-tasks/"" if weights_dir is None else weights_dir\n\n    loaders1, loaders2 = get_splitmnist_dataloaders(num_tasks)\n    net1, net2 = get_splitmnist_models(num_tasks, bayesian=True, pretrained=True, weights_dir=weights_dir)\n    net1.cuda()\n    net2.cuda()\n\n    print(""Model-1, Task-1-Dataset=> Accuracy:"", predict_regular(net1, loaders1[1], bayesian=True, num_ens=num_ens))\n    print(""Model-2, Task-2-Dataset=> Accuracy:"", predict_regular(net2, loaders2[1], bayesian=True, num_ens=num_ens))\n\n\n@initiate_experiment\ndef experiment_regular_prediction_frequentist(weights_dir=None):\n    num_tasks = 2\n    weights_dir = ""checkpoints/MNIST/frequentist/splitted/2-tasks/"" if weights_dir is None else weights_dir\n\n    loaders1, loaders2 = get_splitmnist_dataloaders(num_tasks)\n    net1, net2 = get_splitmnist_models(num_tasks, bayesian=False, pretrained=True, weights_dir=weights_dir)\n    net1.cuda()\n    net2.cuda()\n\n    print(""Model-1, Task-1-Dataset=> Accuracy:"", predict_regular(net1, loaders1[1], bayesian=False))\n    print(""Model-2, Task-2-Dataset=> Accuracy:"", predict_regular(net2, loaders2[1], bayesian=False))\n\n\n@initiate_experiment\ndef experiment_simultaneous_without_mixture_model_with_uncertainty(uncertainty_type=""epistemic_softmax"", T=25, weights_dir=None):\n    num_tasks = 2\n    weights_dir = ""checkpoints/MNIST/bayesian/splitted/2-tasks/"" if weights_dir is None else weights_dir\n\n    loaders1, loaders2 = get_splitmnist_dataloaders(num_tasks)\n    net1, net2 = get_splitmnist_models(num_tasks, True, True, weights_dir)\n    net1.cuda()\n    net2.cuda()\n\n    print(""Both Models, Task-1-Dataset=> Accuracy: {:.3}\\tModel-1-Preferred: {:.3}\\tModel-2-Preferred: {:.3}\\t"" \\\n          ""Task-1-Dataset-Uncertainty: {:.3}\\tTask-2-Dataset-Uncertainty: {:.3}"".format(\n        *predict_using_uncertainty_separate_models(net1, net2, loaders1[1], uncertainty_type=uncertainty_type, T=T)))\n    print(""Both Models, Task-2-Dataset=> Accuracy: {:.3}\\tModel-1-Preferred: {:.3}\\tModel-2-Preferred: {:.3}\\t"" \\\n          ""Task-1-Dataset-Uncertainty: {:.3}\\tTask-2-Dataset-Uncertainty: {:.3}"".format(\n        *predict_using_uncertainty_separate_models(net1, net2, loaders2[1], uncertainty_type=uncertainty_type, T=T)))\n\n\n@initiate_experiment\ndef experiment_simultaneous_without_mixture_model_with_confidence(weights_dir=None):\n    num_tasks = 2\n    weights_dir = ""checkpoints/MNIST/frequentist/splitted/2-tasks/"" if weights_dir is None else weights_dir\n\n    loaders1, loaders2 = get_splitmnist_dataloaders(num_tasks)\n    net1, net2 = get_splitmnist_models(num_tasks, False, True, weights_dir)\n    net1.cuda()\n    net2.cuda()\n\n    print(""Both Models, Task-1-Dataset=> Accuracy: {:.3}\\tModel-1-Preferred: {:.3}\\tModel-2-Preferred: {:.3}"".format(\n        *predict_using_confidence_separate_models(net1, net2, loaders1[1])))\n    print(""Both Models, Task-2-Dataset=> Accuracy: {:.3}\\tModel-1-Preferred: {:.3}\\tModel-2-Preferred: {:.3}"".format(\n        *predict_using_confidence_separate_models(net1, net2, loaders2[1])))\n\n\n@initiate_experiment\ndef wip_experiment_average_weights_mixture_model():\n    num_tasks = 2\n    weights_dir = ""checkpoints/MNIST/bayesian/splitted/2-tasks/""\n\n    loaders1, loaders2 = get_splitmnist_dataloaders(num_tasks)\n    net1, net2 = get_splitmnist_models(num_tasks, True, weights_dir)\n    net1.cuda()\n    net2.cuda()\n    net_mix = get_mixture_model(num_tasks, weights_dir, include_last_layer=True)\n    net_mix.cuda()\n\n    print(""Model-1, Loader-1:"", calculate_accuracy(net1, loaders1[1]))\n    print(""Model-2, Loader-2:"", calculate_accuracy(net2, loaders2[1]))\n    print(""Model-1, Loader-2:"", calculate_accuracy(net1, loaders2[1]))\n    print(""Model-2, Loader-1:"", calculate_accuracy(net2, loaders1[1]))\n    print(""Model-Mix, Loader-1:"", calculate_accuracy(net_mix, loaders1[1]))\n    print(""Model-Mix, Loader-2:"", calculate_accuracy(net_mix, loaders2[1]))\n\n\n@initiate_experiment\ndef wip_experiment_simultaneous_average_weights_mixture_model_with_uncertainty():\n    num_tasks = 2\n    weights_dir = ""checkpoints/MNIST/bayesian/splitted/2-tasks/""\n\n    loaders1, loaders2 = get_splitmnist_dataloaders(num_tasks)\n    net1, net2 = get_splitmnist_models(num_tasks, True, weights_dir)\n    net1.cuda()\n    net2.cuda()\n    net_mix = get_mixture_model(num_tasks, weights_dir, include_last_layer=False)\n    net_mix.cuda()\n\n    # Creating 2 sets of last layer\n    fc3_1 = BBBLinear(84, 5, name=\'fc3_1\') # hardcoded for lenet\n    weights_1 = torch.load(weights_dir + ""model_lenet_2.1.pt"")\n    fc3_1.W = torch.nn.Parameter(weights_1[\'fc3.W\'])\n    fc3_1.log_alpha = torch.nn.Parameter(weights_1[\'fc3.log_alpha\'])\n\n    fc3_2 = BBBLinear(84, 5, name=\'fc3_2\') # hardcoded for lenet\n    weights_2 = torch.load(weights_dir + ""model_lenet_2.2.pt"")\n    fc3_2.W = torch.nn.Parameter(weights_2[\'fc3.W\'])\n    fc3_2.log_alpha = torch.nn.Parameter(weights_2[\'fc3.log_alpha\'])\n\n    fc3_1, fc3_2 = fc3_1.cuda(), fc3_2.cuda()\n\n    print(""Model-1, Loader-1:"", calculate_accuracy(net1, loaders1[1]))\n    print(""Model-2, Loader-2:"", calculate_accuracy(net2, loaders2[1]))\n    print(""Model-Mix, Loader-1:"", predict_using_epistemic_uncertainty_with_mixture_model(net_mix, fc3_1, fc3_2, loaders1[1]))\n    print(""Model-Mix, Loader-2:"", predict_using_epistemic_uncertainty_with_mixture_model(net_mix, fc3_1, fc3_2, loaders2[1]))\n\n\nif __name__ == \'__main__\':\n    experiment_simultaneous_without_mixture_model_with_confidence()\n'"
Mixtures/temp_gmm.py,4,"b""import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch\n\nimport gmm\n\ndef create_synthetic_data(num_gaussians, num_features, num_samples, means, vars):\n    assert len(means[0]) == len(vars[0]) == num_features\n    samples = []\n    for g in range(num_gaussians):\n        loc = torch.tensor(means[g]).float()\n        covariance_matrix = torch.eye(num_features).float() * torch.tensor(vars[g]).float()\n        dist = torch.distributions.multivariate_normal.MultivariateNormal(\n            loc = loc, covariance_matrix=covariance_matrix)\n        \n        for i in range(num_samples//num_gaussians):\n            sample = dist.sample()\n            samples.append(sample.unsqueeze(0))\n        \n    samples = torch.cat(samples, axis=0)\n    return samples\n\n\ndef plot_data(data, y=None):\n    if y is not None:\n        for sample, target in zip(data, y):\n            if target==0:\n                plt.scatter(*sample, color='blue')\n            elif target==1:\n                plt.scatter(*sample, color='red')\n            elif target==2:\n                plt.scatter(*sample, color='green')\n    else:\n        for sample in data:\n            plt.scatter(*sample, color='black')\n    plt.show(block=False)\n    plt.pause(2)\n    plt.close()\n\n\nmeans = [[1, 4], [5, 5], [2, -1]]  # list of task's means(which is mean of each feature)\nvars = [[0.1, 0.1], [0.05, 0.4], [0.5, 0.2]]  # list of task's vars(which is var of each feature)\ndata = create_synthetic_data(3, 2, 600, means, vars)  # shape: (total_samples, num_features)\nplot_data(data)\nmodel = gmm.GaussianMixture(3, 2)  # (num_gaussians, num_features)\nmodel.fit(data)\ny = model.predict(data)\nplot_data(data, y)\n\n"""
Mixtures/train_splitted.py,3,"b'import sys\nsys.path.append(\'..\')\n\nimport os\nimport argparse\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom torch.optim import Adam\n\nimport utils\nimport metrics\nimport config_mixtures as cfg\nimport utils_mixture as mix_utils\nfrom main_bayesian import train_model as train_bayesian, validate_model as validate_bayesian\nfrom main_frequentist import train_model as train_frequentist, validate_model as validate_frequentist\n\n# CUDA settings\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\ndef train_splitted(num_tasks, bayesian=True, net_type=\'lenet\'):\n    assert 10 % num_tasks == 0\n\n    # Hyper Parameter settings\n    train_ens = cfg.train_ens\n    valid_ens = cfg.valid_ens\n    n_epochs = cfg.n_epochs\n    lr_start = cfg.lr_start\n\n    if bayesian:\n        ckpt_dir = f""checkpoints/MNIST/bayesian/splitted/{num_tasks}-tasks/""\n    else:\n        ckpt_dir = f""checkpoints/MNIST/frequentist/splitted/{num_tasks}-tasks/""\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir, exist_ok=True)\n\n    loaders, datasets = mix_utils.get_splitmnist_dataloaders(num_tasks, return_datasets=True)\n    models = mix_utils.get_splitmnist_models(num_tasks, bayesian=bayesian, pretrained=False, net_type=net_type)\n\n    for task in range(1, num_tasks + 1):\n        print(f""Training task-{task}.."")\n        trainset, testset, _, _ = datasets[task-1]\n        train_loader, valid_loader, _ = loaders[task-1]\n        net = models[task-1]\n        net = net.to(device)\n        ckpt_name = ckpt_dir + f""model_{net_type}_{num_tasks}.{task}.pt""\n\n        criterion = (metrics.ELBO(len(trainset)) if bayesian else nn.CrossEntropyLoss()).to(device)\n        optimizer = Adam(net.parameters(), lr=lr_start)\n        valid_loss_max = np.Inf\n        for epoch in range(n_epochs):  # loop over the dataset multiple times\n            utils.adjust_learning_rate(optimizer, metrics.lr_linear(epoch, 0, n_epochs, lr_start))\n\n            if bayesian:\n                train_loss, train_acc, train_kl = train_bayesian(net, optimizer, criterion, train_loader, num_ens=train_ens)\n                valid_loss, valid_acc = validate_bayesian(net, criterion, valid_loader, num_ens=valid_ens)\n                print(\'Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f} \\ttrain_kl_div: {:.4f}\'.format(\n                    epoch, train_loss, train_acc, valid_loss, valid_acc, train_kl))\n            else:\n                train_loss, train_acc = train_frequentist(net, optimizer, criterion, train_loader)\n                valid_loss, valid_acc = validate_frequentist(net, criterion, valid_loader)\n                print(\'Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tValidation Loss: {:.4f} \\tValidation Accuracy: {:.4f}\'.format(\n                    epoch, train_loss, train_acc, valid_loss, valid_acc))\n\n            # save model if validation accuracy has increased\n            if valid_loss <= valid_loss_max:\n                print(\'Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\'.format(\n                    valid_loss_max, valid_loss))\n                torch.save(net.state_dict(), ckpt_name)\n                valid_loss_max = valid_loss\n\n        print(f""Done training task-{task}"")\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description = ""PyTorch Bayesian Split Model Training"")\n    parser.add_argument(\'--num_tasks\', default=2, type=int, help=\'number of tasks\')\n    parser.add_argument(\'--net_type\', default=\'lenet\', type=str, help=\'model\')\n    parser.add_argument(\'--bayesian\', default=1, type=int, help=\'is_bayesian_model(0/1)\')\n    args = parser.parse_args()\n\n    train_splitted(args.num_tasks, bool(args.bayesian), args.net_type)\n'"
Mixtures/utils_mixture.py,21,"b'import sys\nsys.path.append(\'..\')\n\nimport os\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nimport data\nimport utils\nimport metrics\nfrom main_bayesian import getModel as getBayesianModel\nfrom main_frequentist import getModel as getFrequentistModel\nimport config_mixtures as cfg\nimport uncertainty_estimation as ue\n\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\nclass Pass(nn.Module):\n    def __init__(self):\n        super(Pass, self).__init__()\n\n    def forward(self, x):\n        return x\n\n\ndef _get_splitmnist_datasets(num_tasks):\n    datasets = []\n    for i in range(1, num_tasks + 1):\n        name = \'SplitMNIST-{}.{}\'.format(num_tasks, i)\n        datasets.append(data.getDataset(name))\n    return datasets\n\n\ndef get_splitmnist_dataloaders(num_tasks, return_datasets=False):\n    loaders = []\n    datasets = _get_splitmnist_datasets(num_tasks)\n    for i in range(1, num_tasks + 1):\n        trainset, testset, _, _ = datasets[i-1]\n        curr_loaders = data.getDataloader(\n            trainset, testset, cfg.valid_size, cfg.batch_size, cfg.num_workers)\n        loaders.append(curr_loaders)  # (train_loader, valid_loader, test_loader)\n    if return_datasets:\n        return loaders, datasets\n    return loaders\n\n\ndef get_splitmnist_models(num_tasks, bayesian=True, pretrained=False, weights_dir=None, net_type=\'lenet\'):\n    inputs = 1\n    outputs = 10 // num_tasks\n    models = []\n    if pretrained:\n        assert weights_dir\n    for i in range(1, num_tasks + 1):\n        if bayesian:\n            model = getBayesianModel(net_type, inputs, outputs)\n        else:\n            model = getFrequentistModel(net_type, inputs, outputs)\n        models.append(model)\n        if pretrained:\n            weight_path = weights_dir + f""model_{net_type}_{num_tasks}.{i}.pt""\n            models[-1].load_state_dict(torch.load(weight_path))\n    return models\n\n\ndef get_mixture_model(num_tasks, weights_dir, net_type=\'lenet\', include_last_layer=True):\n    """"""\n    Current implementation is based on average value of weights\n    """"""\n    net = getBayesianModel(net_type, 1, 5)\n    if not include_last_layer:\n        net.fc3 = Pass()\n\n    task_weights = []\n    for i in range(1, num_tasks + 1):\n        weight_path = weights_dir + f""model_{net_type}_{num_tasks}.{i}.pt""\n        task_weights.append(torch.load(weight_path))\n\n    mixture_weights = net.state_dict().copy()\n    layer_list = list(mixture_weights.keys())\n\n    for key in mixture_weights:\n        if key in layer_list:\n            concat_weights = torch.cat([w[key].unsqueeze(0) for w in task_weights] , dim=0)\n            average_weight = torch.mean(concat_weights, dim=0)\n            mixture_weights[key] = average_weight\n\n    net.load_state_dict(mixture_weights)\n    return net\n\n\ndef predict_regular(net, validloader, bayesian=True, num_ens=10):\n    """"""\n    For both Bayesian and Frequentist models\n    """"""\n    net.eval()\n    accs = []\n\n    for i, (inputs, labels) in enumerate(validloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        if bayesian:\n            outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n            for j in range(num_ens):\n                net_out, _ = net(inputs)\n                outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n\n            log_outputs = utils.logmeanexp(outputs, dim=2)\n            accs.append(metrics.acc(log_outputs, labels))\n        else:\n            output = net(inputs)\n            accs.append(metrics.acc(output.detach(), labels))\n\n    return np.mean(accs)\n\n\ndef predict_using_uncertainty_separate_models(net1, net2, valid_loader, uncertainty_type=\'epistemic_softmax\', T=25):\n    """"""\n    For Bayesian models\n    """"""\n    accs = []\n    total_u1 = 0.0\n    total_u2 = 0.0\n    set1_selected = 0\n    set2_selected = 0\n\n    epi_or_ale, soft_or_norm = uncertainty_type.split(\'_\')\n    soft_or_norm = True if soft_or_norm==\'normalized\' else False\n\n    for i, (inputs, labels) in enumerate(valid_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        pred1, epi1, ale1 = ue.get_uncertainty_per_batch(net1, inputs, T=T, normalized=soft_or_norm)\n        pred2, epi2, ale2 = ue.get_uncertainty_per_batch(net2, inputs, T=T, normalized=soft_or_norm)\n\n        if epi_or_ale==\'epistemic\':\n            u1 = np.sum(epi1, axis=1)\n            u2 = np.sum(epi2, axis=1)\n        elif epi_or_ale==\'aleatoric\':\n            u1 = np.sum(ale1, axis=1)\n            u2 = np.sum(ale2, axis=1)\n        elif epi_or_ale==\'both\':\n            u1 = np.sum(epi1, axis=1) + np.sum(ale1, axis=1)\n            u2 = np.sum(epi2, axis=1) + np.sum(ale2, axis=1)\n        else:\n            raise ValueError(""Not correct uncertainty type"")\n\n        total_u1 += np.sum(u1).item()\n        total_u2 += np.sum(u2).item()\n\n        set1_preferred = u2 > u1  # idx where set1 has less uncertainty\n        set1_preferred = np.expand_dims(set1_preferred, 1)\n        preds = np.where(set1_preferred, pred1, pred2)\n\n        set1_selected += np.sum(set1_preferred)\n        set2_selected += np.sum(~set1_preferred)\n\n        accs.append(metrics.acc(torch.tensor(preds), labels))\n\n    return np.mean(accs), set1_selected/(set1_selected + set2_selected), \\\n        set2_selected/(set1_selected + set2_selected), total_u1, total_u2\n\n\ndef predict_using_confidence_separate_models(net1, net2, valid_loader):\n    """"""\n    For Frequentist models\n    """"""\n    accs = []\n    set1_selected = 0\n    set2_selected = 0\n\n    for i, (inputs, labels) in enumerate(valid_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        pred1 = F.softmax(net1(inputs), dim=1)\n        pred2 = F.softmax(net2(inputs), dim=1)\n\n        set1_preferred = pred1.max(dim=1)[0] > pred2.max(dim=1)[0]  # idx where set1 has more confidence\n        preds = torch.where(set1_preferred.unsqueeze(1), pred1, pred2)\n\n        set1_selected += torch.sum(set1_preferred).float().item()\n        set2_selected += torch.sum(~set1_preferred).float().item()\n\n        accs.append(metrics.acc(preds.detach(), labels))\n\n    return np.mean(accs), set1_selected/(set1_selected + set2_selected), \\\n        set2_selected/(set1_selected + set2_selected)\n\n\ndef wip_predict_using_epistemic_uncertainty_with_mixture_model(model, fc3_1, fc3_2, valid_loader, T=10):\n    accs = []\n    total_epistemic_1 = 0.0\n    total_epistemic_2 = 0.0\n    set_1_selected = 0\n    set_2_selected = 0\n\n    for i, (inputs, labels) in enumerate(valid_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = []\n        for i in range(inputs.shape[0]):  # loop over batch\n            input_image = inputs[i].unsqueeze(0)\n\n            p_hat_1 = []\n            p_hat_2 = []\n            preds_1 = []\n            preds_2 = []\n            for t in range(T):\n                net_out_mix, _ = model(input_image)\n\n                # set_1\n                net_out_1 = fc3_1(net_out_mix)\n                preds_1.append(net_out_1)\n                prediction = F.softplus(net_out_1)\n                prediction = prediction / torch.sum(prediction, dim=1)\n                p_hat_1.append(prediction.cpu().detach())\n\n                # set_2\n                net_out_2 = fc3_2(net_out_mix)\n                preds_2.append(net_out_2)\n                prediction = F.softplus(net_out_2)\n                prediction = prediction / torch.sum(prediction, dim=1)\n                p_hat_2.append(prediction.cpu().detach())\n\n            # set_1\n            p_hat = torch.cat(p_hat_1, dim=0).numpy()\n            p_bar = np.mean(p_hat, axis=0)\n\n            preds = torch.cat(preds_1, dim=0)\n            pred_set_1 = torch.sum(preds, dim=0).unsqueeze(0)\n\n            temp = p_hat - np.expand_dims(p_bar, 0)\n            epistemic = np.dot(temp.T, temp) / T\n            epistemic_set_1 = np.sum(np.diag(epistemic)).item()\n            total_epistemic_1 += epistemic_set_1\n\n            # set_2\n            p_hat = torch.cat(p_hat_2, dim=0).numpy()\n            p_bar = np.mean(p_hat, axis=0)\n\n            preds = torch.cat(preds_2, dim=0)\n            pred_set_2 = torch.sum(preds, dim=0).unsqueeze(0)\n\n            temp = p_hat - np.expand_dims(p_bar, 0)\n            epistemic = np.dot(temp.T, temp) / T\n            epistemic_set_2 = np.sum(np.diag(epistemic)).item()\n            total_epistemic_2 += epistemic_set_2\n\n            if epistemic_set_1 > epistemic_set_2:\n                set_2_selected += 1\n                outputs.append(pred_set_2)\n            else:\n                set_1_selected += 1\n                outputs.append(pred_set_1)\n\n        outputs = torch.cat(outputs, dim=0)\n        accs.append(metrics.acc(outputs.detach(), labels))\n\n    return np.mean(accs), set_1_selected/(set_1_selected + set_2_selected), \\\n        set_2_selected/(set_1_selected + set_2_selected), total_epistemic_1, total_epistemic_2\n'"
data/__init__.py,0,b'from .data import getDataset\nfrom .data import getDataloader'
data/data.py,6,"b""import numpy as np\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        label = self.labels[idx]\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample, label\n\n\ndef extract_classes(dataset, classes):\n    idx = torch.zeros_like(dataset.targets, dtype=torch.bool)\n    for target in classes:\n        idx = idx | (dataset.targets==target)\n\n    data, targets = dataset.data[idx], dataset.targets[idx]\n    return data, targets\n\n\ndef getDataset(dataset):\n    transform_split_mnist = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((32, 32)),\n        transforms.ToTensor(),\n        ])\n\n    transform_mnist = transforms.Compose([\n        transforms.Resize((32, 32)),\n        transforms.ToTensor(),\n        ])\n\n    transform_cifar = transforms.Compose([\n        transforms.Resize((32, 32)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ])\n\n    if(dataset == 'CIFAR10'):\n        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n        num_classes = 10\n        inputs=3\n\n    elif(dataset == 'CIFAR100'):\n        trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_cifar)\n        testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_cifar)\n        num_classes = 100\n        inputs = 3\n        \n    elif(dataset == 'MNIST'):\n        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n        num_classes = 10\n        inputs = 1\n\n    elif(dataset == 'SplitMNIST-2.1'):\n        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n\n        train_data, train_targets = extract_classes(trainset, [0, 1, 2, 3, 4])\n        test_data, test_targets = extract_classes(testset, [0, 1, 2, 3, 4])\n\n        trainset = CustomDataset(train_data, train_targets, transform=transform_split_mnist)\n        testset = CustomDataset(test_data, test_targets, transform=transform_split_mnist)\n        num_classes = 5\n        inputs = 1\n\n    elif(dataset == 'SplitMNIST-2.2'):\n        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n\n        train_data, train_targets = extract_classes(trainset, [5, 6, 7, 8, 9])\n        test_data, test_targets = extract_classes(testset, [5, 6, 7, 8, 9])\n        train_targets -= 5 # Mapping target 5-9 to 0-4\n        test_targets -= 5 # Hence, add 5 after prediction\n\n        trainset = CustomDataset(train_data, train_targets, transform=transform_split_mnist)\n        testset = CustomDataset(test_data, test_targets, transform=transform_split_mnist)\n        num_classes = 5\n        inputs = 1\n\n    elif(dataset == 'SplitMNIST-5.1'):\n        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n\n        train_data, train_targets = extract_classes(trainset, [0, 1])\n        test_data, test_targets = extract_classes(testset, [0, 1])\n\n        trainset = CustomDataset(train_data, train_targets, transform=transform_split_mnist)\n        testset = CustomDataset(test_data, test_targets, transform=transform_split_mnist)\n        num_classes = 2\n        inputs = 1\n\n    elif(dataset == 'SplitMNIST-5.2'):\n        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n\n        train_data, train_targets = extract_classes(trainset, [2, 3])\n        test_data, test_targets = extract_classes(testset, [2, 3])\n        train_targets -= 2 # Mapping target 2-3 to 0-1\n        test_targets -= 2 # Hence, add 2 after prediction\n\n        trainset = CustomDataset(train_data, train_targets, transform=transform_split_mnist)\n        testset = CustomDataset(test_data, test_targets, transform=transform_split_mnist)\n        num_classes = 2\n        inputs = 1\n\n    elif(dataset == 'SplitMNIST-5.3'):\n        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n\n        train_data, train_targets = extract_classes(trainset, [4, 5])\n        test_data, test_targets = extract_classes(testset, [4, 5])\n        train_targets -= 4 # Mapping target 4-5 to 0-1\n        test_targets -= 4 # Hence, add 4 after prediction\n\n        trainset = CustomDataset(train_data, train_targets, transform=transform_split_mnist)\n        testset = CustomDataset(test_data, test_targets, transform=transform_split_mnist)\n        num_classes = 2\n        inputs = 1\n\n    elif(dataset == 'SplitMNIST-5.4'):\n        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n\n        train_data, train_targets = extract_classes(trainset, [6, 7])\n        test_data, test_targets = extract_classes(testset, [6, 7])\n        train_targets -= 6 # Mapping target 6-7 to 0-1\n        test_targets -= 6 # Hence, add 6 after prediction\n\n        trainset = CustomDataset(train_data, train_targets, transform=transform_split_mnist)\n        testset = CustomDataset(test_data, test_targets, transform=transform_split_mnist)\n        num_classes = 2\n        inputs = 1\n\n    elif(dataset == 'SplitMNIST-5.5'):\n        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n\n        train_data, train_targets = extract_classes(trainset, [8, 9])\n        test_data, test_targets = extract_classes(testset, [8, 9])\n        train_targets -= 8 # Mapping target 8-9 to 0-1\n        test_targets -= 8 # Hence, add 8 after prediction\n\n        trainset = CustomDataset(train_data, train_targets, transform=transform_split_mnist)\n        testset = CustomDataset(test_data, test_targets, transform=transform_split_mnist)\n        num_classes = 2\n        inputs = 1\n\n    return trainset, testset, inputs, num_classes\n\n\ndef getDataloader(trainset, testset, valid_size, batch_size, num_workers):\n    num_train = len(trainset)\n    indices = list(range(num_train))\n    np.random.shuffle(indices)\n    split = int(np.floor(valid_size * num_train))\n    train_idx, valid_idx = indices[split:], indices[:split]\n\n    train_sampler = SubsetRandomSampler(train_idx)\n    valid_sampler = SubsetRandomSampler(valid_idx)\n\n    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n        sampler=train_sampler, num_workers=num_workers)\n    valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n        sampler=valid_sampler, num_workers=num_workers)\n    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n        num_workers=num_workers)\n\n    return train_loader, valid_loader, test_loader\n"""
layers/__init__.py,0,"b'from .BBB.BBBLinear import BBBLinear as BBB_Linear\nfrom .BBB.BBBConv import BBBConv2d as BBB_Conv2d\n\nfrom .BBB_LRT.BBBLinear import BBBLinear as BBB_LRT_Linear\nfrom .BBB_LRT.BBBConv import BBBConv2d as BBB_LRT_Conv2d\n\nfrom .misc import FlattenLayer, ModuleWrapper\n'"
layers/misc.py,0,"b'from torch import nn\n\n\nclass ModuleWrapper(nn.Module):\n    """"""Wrapper for nn.Module with support for arbitrary flags and a universal forward pass""""""\n\n    def __init__(self):\n        super(ModuleWrapper, self).__init__()\n\n    def set_flag(self, flag_name, value):\n        setattr(self, flag_name, value)\n        for m in self.children():\n            if hasattr(m, \'set_flag\'):\n                m.set_flag(flag_name, value)\n\n    def forward(self, x):\n        for module in self.children():\n            x = module(x)\n\n        kl = 0.0\n        for module in self.modules():\n            if hasattr(module, \'kl_loss\'):\n                kl = kl + module.kl_loss()\n\n        return x, kl\n\n\nclass FlattenLayer(ModuleWrapper):\n\n    def __init__(self, num_features):\n        super(FlattenLayer, self).__init__()\n        self.num_features = num_features\n\n    def forward(self, x):\n        return x.view(-1, self.num_features)\n'"
tests/test_models.py,9,"b'import os\nimport sys\nimport math\nimport pytest\nimport unittest\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport utils\nfrom models.BayesianModels.Bayesian3Conv3FC import BBB3Conv3FC\nfrom models.BayesianModels.BayesianAlexNet import BBBAlexNet\nfrom models.BayesianModels.BayesianLeNet import BBBLeNet\nfrom models.NonBayesianModels.AlexNet import AlexNet\nfrom models.NonBayesianModels.LeNet import LeNet\nfrom models.NonBayesianModels.ThreeConvThreeFC import ThreeConvThreeFC\nfrom layers.BBBConv import BBBConv2d\nfrom layers.BBBLinear import  BBBLinear\nfrom layers.misc import FlattenLayer\n\ncuda_available = torch.cuda.is_available()\nbayesian_models = [BBBLeNet, BBBAlexNet, BBB3Conv3FC]\nnon_bayesian_models = [LeNet, AlexNet, ThreeConvThreeFC]\n\nclass TestModelForwardpass:\n\n    @pytest.mark.parametrize(""model"", bayesian_models)\n    def test_cpu_bayesian(self, model):\n        batch_size = np.random.randint(1, 256)\n        batch = torch.randn((batch_size, 3, 32, 32))\n        net = model(10, 3)\n        out = net(batch)\n        assert out[0].shape[0]==batch_size\n    \n    @pytest.mark.parametrize(""model"", non_bayesian_models)\n    def test_cpu_frequentist(self, model):\n        batch_size = np.random.randint(1, 256)\n        batch = torch.randn((batch_size, 3, 32, 32))\n        net = model(10, 3)\n        out = net(batch)\n        assert out.shape[0]==batch_size\n\n    @pytest.mark.skipif(not cuda_available, reason=""CUDA not available"")\n    @pytest.mark.parametrize(""model"", bayesian_models)\n    def test_gpu_bayesian(self, model):\n        batch_size = np.random.randint(1, 256)\n        batch = torch.randn((batch_size, 3, 32, 32))\n        net = model(10, 3)\n        if cuda_available:\n            net = net.cuda()\n            batch = batch.cuda()\n        out = net(batch)\n        assert out[0].shape[0]==batch_size\n\n    @pytest.mark.skipif(not cuda_available, reason=""CUDA not available"")\n    @pytest.mark.parametrize(""model"", non_bayesian_models)\n    def test_gpu_frequentist(self, model):\n        batch_size = np.random.randint(1, 256)\n        batch = torch.randn((batch_size, 3, 32, 32))\n        net = model(10, 3)\n        if cuda_available:\n            net = net.cuda()\n            batch = batch.cuda()\n        out = net(batch)\n        assert out.shape[0]==batch_size\n\n\nclass TestBayesianLayers:\n\n    def test_flatten(self):\n        batch_size = np.random.randint(1, 256)\n        batch = torch.randn((batch_size, 64, 4, 4))\n\n        layer = FlattenLayer(4 * 4 * 64)\n        batch = layer(batch)\n\n        assert batch.shape[0]==batch_size\n        assert batch.shape[1]==(4 * 4 *64)\n\n    def test_conv(self):\n        batch_size = np.random.randint(1, 256)\n        batch = torch.randn((batch_size, 16, 24, 24))\n\n        layer = BBBConv2d(16, 6, 4, alpha_shape=(1,1), padding=0, bias=False)\n        batch = layer(batch)\n\n        assert batch.shape[0]==batch_size\n        assert batch.shape[1]==6\n\n    def test_linear(self):\n        batch_size = np.random.randint(1, 256)\n        batch = torch.randn((batch_size, 128))\n\n        layer = BBBLinear(128, 64, alpha_shape=(1,1), bias=False)\n        batch = layer(batch)\n\n        assert batch.shape[0]==batch_size\n        assert batch.shape[1]==64\n'"
layers/BBB/BBBConv.py,12,"b'import sys\nsys.path.append("".."")\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\nfrom metrics import calculate_kl as KL_DIV\nfrom ..misc import ModuleWrapper\n\n\nclass BBBConv2d(ModuleWrapper):\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=0, dilation=1, bias=True, priors=None):\n\n        super(BBBConv2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = (kernel_size, kernel_size)\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = 1\n        self.use_bias = bias\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n        if priors is None:\n            priors = {\n                \'prior_mu\': 0,\n                \'prior_sigma\': 0.1,\n                \'posterior_mu_initial\': (0, 0.1),\n                \'posterior_rho_initial\': (-3, 0.1),\n            }\n        self.prior_mu = priors[\'prior_mu\']\n        self.prior_sigma = priors[\'prior_sigma\']\n        self.posterior_mu_initial = priors[\'posterior_mu_initial\']\n        self.posterior_rho_initial = priors[\'posterior_rho_initial\']\n\n        self.W_mu = Parameter(torch.empty((out_channels, in_channels, *self.kernel_size), device=self.device))\n        self.W_rho = Parameter(torch.empty((out_channels, in_channels, *self.kernel_size), device=self.device))\n\n        if self.use_bias:\n            self.bias_mu = Parameter(torch.empty((out_channels), device=self.device))\n            self.bias_rho = Parameter(torch.empty((out_channels), device=self.device))\n        else:\n            self.register_parameter(\'bias_mu\', None)\n            self.register_parameter(\'bias_rho\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.W_mu.data.normal_(*self.posterior_mu_initial)\n        self.W_rho.data.normal_(*self.posterior_rho_initial)\n\n        if self.use_bias:\n            self.bias_mu.data.normal_(*self.posterior_mu_initial)\n            self.bias_rho.data.normal_(*self.posterior_rho_initial)\n\n    def forward(self, input, sample=True):\n        if self.training or sample:\n            W_eps = torch.empty(self.W_mu.size()).normal_(0, 1).to(self.device)\n            self.W_sigma = torch.log1p(torch.exp(self.W_rho))\n            weight = self.W_mu + W_eps * self.W_sigma\n\n            if self.use_bias:\n                bias_eps = torch.empty(self.bias_mu.size()).normal_(0, 1).to(self.device)\n                self.bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n                bias = self.bias_mu + bias_eps * self.bias_sigma\n            else:\n                bias = None\n        else:\n            weight = self.W_mu\n            bias = self.bias_mu if self.use_bias else None\n\n        return F.conv2d(input, weight, bias, self.stride, self.padding, self.dilation, self.groups)\n\n    def kl_loss(self):\n        kl = KL_DIV(self.prior_mu, self.prior_sigma, self.W_mu, self.W_sigma)\n        if self.use_bias:\n            kl += KL_DIV(self.prior_mu, self.prior_sigma, self.bias_mu, self.bias_sigma)\n        return kl\n'"
layers/BBB/BBBLinear.py,12,"b'import sys\nsys.path.append("".."")\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\nfrom metrics import calculate_kl as KL_DIV\nfrom ..misc import ModuleWrapper\n\n\nclass BBBLinear(ModuleWrapper):\n    def __init__(self, in_features, out_features, bias=True, priors=None):\n        super(BBBLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.use_bias = bias\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n        if priors is None:\n            priors = {\n                \'prior_mu\': 0,\n                \'prior_sigma\': 0.1,\n                \'posterior_mu_initial\': (0, 0.1),\n                \'posterior_rho_initial\': (-3, 0.1),\n            }\n        self.prior_mu = priors[\'prior_mu\']\n        self.prior_sigma = priors[\'prior_sigma\']\n        self.posterior_mu_initial = priors[\'posterior_mu_initial\']\n        self.posterior_rho_initial = priors[\'posterior_rho_initial\']\n\n        self.W_mu = Parameter(torch.empty((out_features, in_features), device=self.device))\n        self.W_rho = Parameter(torch.empty((out_features, in_features), device=self.device))\n\n        if self.use_bias:\n            self.bias_mu = Parameter(torch.empty((out_features), device=self.device))\n            self.bias_rho = Parameter(torch.empty((out_features), device=self.device))\n        else:\n            self.register_parameter(\'bias_mu\', None)\n            self.register_parameter(\'bias_rho\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.W_mu.data.normal_(*self.posterior_mu_initial)\n        self.W_rho.data.normal_(*self.posterior_rho_initial)\n\n        if self.use_bias:\n            self.bias_mu.data.normal_(*self.posterior_mu_initial)\n            self.bias_rho.data.normal_(*self.posterior_rho_initial)\n\n    def forward(self, input, sample=True):\n        if self.training or sample:\n            W_eps = torch.empty(self.W_mu.size()).normal_(0, 1).to(self.device)\n            self.W_sigma = torch.log1p(torch.exp(self.W_rho))\n            weight = self.W_mu + W_eps * self.W_sigma\n\n            if self.use_bias:\n                bias_eps = torch.empty(self.bias_mu.size()).normal_(0, 1).to(self.device)\n                self.bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n                bias = self.bias_mu + bias_eps * self.bias_sigma\n            else:\n                bias = None\n        else:\n            weight = self.W_mu\n            bias = self.bias_mu if self.use_bias else None\n\n        return F.linear(input, weight, bias)\n\n    def kl_loss(self):\n        kl = KL_DIV(self.prior_mu, self.prior_sigma, self.W_mu, self.W_sigma)\n        if self.use_bias:\n            kl += KL_DIV(self.prior_mu, self.prior_sigma, self.bias_mu, self.bias_sigma)\n        return kl\n'"
layers/BBB/__init__.py,0,b''
layers/BBB_LRT/BBBConv.py,12,"b'import sys\nsys.path.append("".."")\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\nimport utils\nfrom metrics import calculate_kl as KL_DIV\nimport config_bayesian as cfg\nfrom ..misc import ModuleWrapper\n\n\nclass BBBConv2d(ModuleWrapper):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, bias=True, priors=None):\n        super(BBBConv2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = (kernel_size, kernel_size)\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = 1\n        self.use_bias = bias\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n        if priors is None:\n            priors = {\n                \'prior_mu\': 0,\n                \'prior_sigma\': 0.1,\n                \'posterior_mu_initial\': (0, 0.1),\n                \'posterior_rho_initial\': (-3, 0.1),\n            }\n        self.prior_mu = priors[\'prior_mu\']\n        self.prior_sigma = priors[\'prior_sigma\']\n        self.posterior_mu_initial = priors[\'posterior_mu_initial\']\n        self.posterior_rho_initial = priors[\'posterior_rho_initial\']\n\n        self.W_mu = Parameter(torch.Tensor(out_channels, in_channels, *self.kernel_size))\n        self.W_rho = Parameter(torch.Tensor(out_channels, in_channels, *self.kernel_size))\n        if self.use_bias:\n            self.bias_mu = Parameter(torch.Tensor(out_channels))\n            self.bias_rho = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter(\'bias_mu\', None)\n            self.register_parameter(\'bias_rho\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.W_mu.data.normal_(*self.posterior_mu_initial)\n        self.W_rho.data.normal_(*self.posterior_rho_initial)\n\n        if self.use_bias:\n            self.bias_mu.data.normal_(*self.posterior_mu_initial)\n            self.bias_rho.data.normal_(*self.posterior_rho_initial)\n\n    def forward(self, x):\n\n        self.W_sigma = torch.log1p(torch.exp(self.W_rho))\n        if self.use_bias:\n            self.bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n            bias_var = self.bias_sigma ** 2\n        else:\n            self.bias_sigma = bias_var = None\n\n        act_mu = F.conv2d(\n            x, self.W_mu, self.bias_mu, self.stride, self.padding, self.dilation, self.groups)\n        act_var = 1e-16 + F.conv2d(\n            x ** 2, self.W_sigma ** 2, bias_var, self.stride, self.padding, self.dilation, self.groups)\n        act_std = torch.sqrt(act_var)\n\n        if self.training or sample:\n            eps = torch.empty(act_mu.size()).normal_(0, 1).to(self.device)\n            return act_mu + act_std * eps\n        else:\n            return act_mu\n\n    def kl_loss(self):\n        kl = KL_DIV(self.prior_mu, self.prior_sigma, self.W_mu, self.W_sigma)\n        if self.use_bias:\n            kl += KL_DIV(self.prior_mu, self.prior_sigma, self.bias_mu, self.bias_sigma)\n        return kl\n'"
layers/BBB_LRT/BBBLinear.py,12,"b'import sys\nsys.path.append("".."")\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\nimport utils\nfrom metrics import calculate_kl as KL_DIV\nimport config_bayesian as cfg\nfrom ..misc import ModuleWrapper\n\n\nclass BBBLinear(ModuleWrapper):\n\n    def __init__(self, in_features, out_features, bias=True, priors=None):\n        super(BBBLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.use_bias = bias\n        self.device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n        if priors is None:\n                priors = {\n                \'prior_mu\': 0,\n                \'prior_sigma\': 0.1,\n                \'posterior_mu_initial\': (0, 0.1),\n                \'posterior_rho_initial\': (-3, 0.1),\n            }\n        self.prior_mu = priors[\'prior_mu\']\n        self.prior_sigma = priors[\'prior_sigma\']\n        self.posterior_mu_initial = priors[\'posterior_mu_initial\']\n        self.posterior_rho_initial = priors[\'posterior_rho_initial\']\n\n        self.W_mu = Parameter(torch.Tensor(out_features, in_features))\n        self.W_rho = Parameter(torch.Tensor(out_features, in_features))\n        if self.use_bias:\n            self.bias_mu = Parameter(torch.Tensor(out_features))\n            self.bias_rho = Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter(\'bias_mu\', None)\n            self.register_parameter(\'bias_rho\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.W_mu.data.normal_(*self.posterior_mu_initial)\n        self.W_rho.data.normal_(*self.posterior_rho_initial)\n\n        if self.use_bias:\n            self.bias_mu.data.normal_(*self.posterior_mu_initial)\n            self.bias_rho.data.normal_(*self.posterior_rho_initial)\n\n    def forward(self, x, sample=True):\n\n        self.W_sigma = torch.log1p(torch.exp(self.W_rho))\n        if self.use_bias:\n            self.bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n            bias_var = self.bias_sigma ** 2\n        else:\n            self.bias_sigma = bias_var = None\n\n        act_mu = F.linear(x, self.W_mu, self.bias_mu)\n        act_var = 1e-16 + F.linear(x ** 2, self.W_sigma ** 2, bias_var)\n        act_std = torch.sqrt(act_var)\n\n        if self.training or sample:\n            eps = torch.empty(act_mu.size()).normal_(0, 1).to(self.device)\n            return act_mu + act_std * eps\n        else:\n            return act_mu\n\n    def kl_loss(self):\n        kl = KL_DIV(self.prior_mu, self.prior_sigma, self.W_mu, self.W_sigma)\n        if self.use_bias:\n            kl += KL_DIV(self.prior_mu, self.prior_sigma, self.bias_mu, self.bias_sigma)\n        return kl\n'"
layers/BBB_LRT/__init__.py,0,b''
models/BayesianModels/Bayesian3Conv3FC.py,1,"b'import math\nimport torch.nn as nn\nfrom layers import BBB_Linear, BBB_Conv2d\nfrom layers import BBB_LRT_Linear, BBB_LRT_Conv2d\nfrom layers import FlattenLayer, ModuleWrapper\n\nclass BBB3Conv3FC(ModuleWrapper):\n    """"""\n\n    Simple Neural Network having 3 Convolution\n    and 3 FC layers with Bayesian layers.\n    """"""\n    def __init__(self, outputs, inputs, priors, layer_type=\'lrt\', activation_type=\'softplus\'):\n        super(BBB3Conv3FC, self).__init__()\n\n        self.num_classes = outputs\n        self.layer_type = layer_type\n        self.priors = priors\n\n        if layer_type==\'lrt\':\n            BBBLinear = BBB_LRT_Linear\n            BBBConv2d = BBB_LRT_Conv2d\n        elif layer_type==\'bbb\':\n            BBBLinear = BBB_Linear\n            BBBConv2d = BBB_Conv2d\n        else:\n            raise ValueError(""Undefined layer_type"")\n        \n        if activation_type==\'softplus\':\n            self.act = nn.Softplus\n        elif activation_type==\'relu\':\n            self.act = nn.ReLU\n        else:\n            raise ValueError(""Only softplus or relu supported"")\n\n        self.conv1 = BBBConv2d(inputs, 32, 5, padding=2, bias=True, priors=self.priors)\n        self.act1 = self.act()\n        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.conv2 = BBBConv2d(32, 64, 5, padding=2, bias=True, priors=self.priors)\n        self.act2 = self.act()\n        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.conv3 = BBBConv2d(64, 128, 5, padding=1, bias=True, priors=self.priors)\n        self.act3 = self.act()\n        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.flatten = FlattenLayer(2 * 2 * 128)\n        self.fc1 = BBBLinear(2 * 2 * 128, 1000, bias=True, priors=self.priors)\n        self.act4 = self.act()\n\n        self.fc2 = BBBLinear(1000, 1000, bias=True, priors=self.priors)\n        self.act5 = self.act()\n\n        self.fc3 = BBBLinear(1000, outputs, bias=True, priors=self.priors)\n'"
models/BayesianModels/BayesianAlexNet.py,1,"b'import torch.nn as nn\nimport math\nfrom layers import BBB_Linear, BBB_Conv2d\nfrom layers import BBB_LRT_Linear, BBB_LRT_Conv2d\nfrom layers import FlattenLayer, ModuleWrapper\n\n\nclass BBBAlexNet(ModuleWrapper):\n    \'\'\'The architecture of AlexNet with Bayesian Layers\'\'\'\n\n    def __init__(self, outputs, inputs, priors, layer_type=\'lrt\', activation_type=\'softplus\'):\n        super(BBBAlexNet, self).__init__()\n\n        self.num_classes = outputs\n        self.layer_type = layer_type\n        self.priors = priors\n\n        if layer_type==\'lrt\':\n            BBBLinear = BBB_LRT_Linear\n            BBBConv2d = BBB_LRT_Conv2d\n        elif layer_type==\'bbb\':\n            BBBLinear = BBB_Linear\n            BBBConv2d = BBB_Conv2d\n        else:\n            raise ValueError(""Undefined layer_type"")\n        \n        if activation_type==\'softplus\':\n            self.act = nn.Softplus\n        elif activation_type==\'relu\':\n            self.act = nn.ReLU\n        else:\n            raise ValueError(""Only softplus or relu supported"")\n\n        self.conv1 = BBBConv2d(inputs, 64, 11, stride=4, padding=5, bias=True, priors=self.priors)\n        self.act1 = self.act()\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = BBBConv2d(64, 192, 5, padding=2, bias=True, priors=self.priors)\n        self.act2 = self.act()\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv3 = BBBConv2d(192, 384, 3, padding=1, bias=True, priors=self.priors)\n        self.act3 = self.act()\n\n        self.conv4 = BBBConv2d(384, 256, 3, padding=1, bias=True, priors=self.priors)\n        self.act4 = self.act()\n\n        self.conv5 = BBBConv2d(256, 128, 3, padding=1, bias=True, priors=self.priors)\n        self.act5 = self.act()\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.flatten = FlattenLayer(1 * 1 * 128)\n        self.classifier = BBBLinear(1 * 1 * 128, outputs, bias=True, priors=self.priors)\n'"
models/BayesianModels/BayesianLeNet.py,1,"b'import math\nimport torch.nn as nn\nfrom layers import BBB_Linear, BBB_Conv2d\nfrom layers import BBB_LRT_Linear, BBB_LRT_Conv2d\nfrom layers import FlattenLayer, ModuleWrapper\n\n\nclass BBBLeNet(ModuleWrapper):\n    \'\'\'The architecture of LeNet with Bayesian Layers\'\'\'\n\n    def __init__(self, outputs, inputs, priors, layer_type=\'lrt\', activation_type=\'softplus\'):\n        super(BBBLeNet, self).__init__()\n\n        self.num_classes = outputs\n        self.layer_type = layer_type\n        self.priors = priors\n\n        if layer_type==\'lrt\':\n            BBBLinear = BBB_LRT_Linear\n            BBBConv2d = BBB_LRT_Conv2d\n        elif layer_type==\'bbb\':\n            BBBLinear = BBB_Linear\n            BBBConv2d = BBB_Conv2d\n        else:\n            raise ValueError(""Undefined layer_type"")\n        \n        if activation_type==\'softplus\':\n            self.act = nn.Softplus\n        elif activation_type==\'relu\':\n            self.act = nn.ReLU\n        else:\n            raise ValueError(""Only softplus or relu supported"")\n\n        self.conv1 = BBBConv2d(inputs, 6, 5, padding=0, bias=True, priors=self.priors)\n        self.act1 = self.act()\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv2 = BBBConv2d(6, 16, 5, padding=0, bias=True, priors=self.priors)\n        self.act2 = self.act()\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.flatten = FlattenLayer(5 * 5 * 16)\n        self.fc1 = BBBLinear(5 * 5 * 16, 120, bias=True, priors=self.priors)\n        self.act3 = self.act()\n\n        self.fc2 = BBBLinear(120, 84, bias=True, priors=self.priors)\n        self.act4 = self.act()\n\n        self.fc3 = BBBLinear(84, outputs, bias=True, priors=self.priors)\n'"
models/NonBayesianModels/AlexNet.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\ndef conv_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        #nn.init.xavier_uniform(m.weight, gain=np.sqrt(2))\n        nn.init.normal_(m.weight, mean=0, std=1)\n        nn.init.constant(m.bias, 0)\n\nclass AlexNet(nn.Module):\n\n    def __init__(self, num_classes, inputs=3):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(inputs, 64, kernel_size=11, stride=4, padding=5),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.classifier = nn.Linear(256, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n"""
models/NonBayesianModels/LeNet.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\ndef conv_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        #nn.init.xavier_uniform(m.weight, gain=np.sqrt(2))\n        nn.init.normal_(m.weight, mean=0, std=1)\n        nn.init.constant(m.bias, 0)\n\nclass LeNet(nn.Module):\n    def __init__(self, num_classes, inputs=3):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(inputs, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1   = nn.Linear(16*5*5, 120)\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, num_classes)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = self.fc3(out)\n\n        return(out)\n"""
models/NonBayesianModels/ThreeConvThreeFC.py,1,"b'import torch.nn as nn\nfrom layers.misc import FlattenLayer\n\n\ndef conv_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        #nn.init.xavier_uniform(m.weight, gain=np.sqrt(2))\n        nn.init.normal_(m.weight, mean=0, std=1)\n        nn.init.constant(m.bias, 0)\n\nclass ThreeConvThreeFC(nn.Module):\n    """"""\n    To train on CIFAR-10:\n    https://arxiv.org/pdf/1207.0580.pdf\n    """"""\n    def __init__(self, outputs, inputs):\n        super(ThreeConvThreeFC, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(inputs, 32, 5, stride=1, padding=2),\n            nn.Softplus(),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(32, 64, 5, stride=1, padding=2),\n            nn.Softplus(),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 128, 5, stride=1, padding=1),\n            nn.Softplus(),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.classifier = nn.Sequential(\n            FlattenLayer(2 * 2 * 128),\n            nn.Linear(2 * 2 * 128, 1000),\n            nn.Softplus(),\n            nn.Linear(1000, 1000),\n            nn.Softplus(),\n            nn.Linear(1000, outputs)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n'"
