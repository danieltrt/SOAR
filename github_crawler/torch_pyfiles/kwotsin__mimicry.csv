file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\n__version__ = \'0.1.13\'\nurl = \'https://github.com/kwotsin/mimicry\'\n\ninstall_requires = [\n    \'numpy\',\n    \'scipy\',\n    \'requests\',\n    \'torch\',\n    \'tensorflow\',\n    \'torchvision\',\n    \'six\',\n    \'matplotlib\',\n    \'Pillow\',\n    \'scikit-image\',\n    \'pytest\',\n    \'scikit-learn\',\n    \'future\',\n    \'pytest-cov\',\n    \'pandas\',\n    \'psutil\',\n    \'yapf\',\n    \'lmdb\',\n]\n\nsetup_requires = [\'pytest-runner\']\ntests_require = [\'pytest\', \'pytest-cov\', \'mock\']\n\nlong_description = """"""\nMimicry is a lightweight PyTorch library aimed towards the reproducibility of GAN research.\n\nComparing GANs is often difficult - mild differences in implementations and evaluation methodologies can result in huge performance differences.\nMimicry aims to resolve this by providing:\n    (a) Standardized implementations of popular GANs that closely reproduce reported scores;\n    (b) Baseline scores of GANs trained and evaluated under the same conditions;\n    (c) A framework for researchers to focus on implementation of GANs without rewriting most of GAN training boilerplate code, with support for multiple GAN evaluation metrics.\n\nWe provide a model zoo and set of baselines to benchmark different GANs of the same model size trained under the same conditions, using multiple metrics. To ensure reproducibility, we verify scores of our implemented models against reported scores in literature.\n""""""\n\nsetup(\n    name=\'torch_mimicry\',\n    version=__version__,\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    description=\'Mimicry: Towards the Reproducibility of GAN Research\',\n    author=\'Kwot Sin Lee\',\n    author_email=\'ksl36@cam.ac.uk\',\n    url=url,\n    download_url=\'{}/archive/{}.tar.gz\'.format(url, __version__),\n    keywords=[\n        \'pytorch\',\n        \'generative-adversarial-networks\',\n        \'gans\',\n        \'GAN\',\n    ],\n    python_requires=\'>=3.6\',\n    install_requires=install_requires,\n    setup_requires=setup_requires,\n    tests_require=tests_require,\n    packages=find_packages(),\n)\n'"
examples/sngan_example.py,3,"b'import torch\nimport torch.optim as optim\nimport torch_mimicry as mmc\nfrom torch_mimicry.nets import sngan\n\n\nif __name__ == ""__main__"":\n    # Data handling objects\n    device = torch.device(\'cuda:0\' if torch.cuda.is_available() else ""cpu"")\n    dataset = mmc.datasets.load_dataset(root=\'./datasets\', name=\'cifar10\')\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=64, shuffle=True, num_workers=4)\n\n    # Define models and optimizers\n    netG = sngan.SNGANGenerator32().to(device)\n    netD = sngan.SNGANDiscriminator32().to(device)\n    optD = optim.Adam(netD.parameters(), 2e-4, betas=(0.0, 0.9))\n    optG = optim.Adam(netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n    # Start training\n    trainer = mmc.training.Trainer(\n        netD=netD,\n        netG=netG,\n        optD=optD,\n        optG=optG,\n        n_dis=5,\n        num_steps=100000,\n        lr_decay=\'linear\',\n        dataloader=dataloader,\n        log_dir=\'./log/example\',\n        device=device)\n    trainer.train()\n\n    # Evaluate fid\n    mmc.metrics.evaluate(\n        metric=\'fid\',\n        log_dir=\'./log/example\',\n        netG=netG,\n        dataset_name=\'cifar10\',\n        num_real_samples=50000,\n        num_fake_samples=50000,\n        evaluate_step=100000,\n        device=device)\n\n    # Evaluate kid\n    mmc.metrics.evaluate(\n        metric=\'kid\',\n        log_dir=\'./log/example\',\n        netG=netG,\n        dataset_name=\'cifar10\',\n        num_subsets=50,\n        subset_size=1000,\n        evaluate_step=100000,        \n        device=device)\n\n    # Evaluate inception score\n    mmc.metrics.evaluate(\n        metric=\'inception_score\',\n        log_dir=\'./log/example\',\n        netG=netG,\n        num_samples=50000,\n        evaluate_step=100000,        \n        device=device)'"
examples/ssgan_tutorial.py,12,"b'""""""\nTutorial of using SSGAN.\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\n\nimport torch_mimicry as mmc\nfrom torch_mimicry.nets import gan\nfrom torch_mimicry.modules import SNLinear\nfrom torch_mimicry.modules import GBlock, DBlock, DBlockOptimized\n\n#######################\n#        Models\n#######################\nclass SSGANGenerator(gan.BaseGenerator):\n    def __init__(self,\n                 nz=128,\n                 ngf=256,\n                 bottom_width=4,\n                 loss_type=\'hinge\',\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         **kwargs)\n        self.ss_loss_scale = 0.2\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block4 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf)\n        self.c5 = nn.Conv2d(ngf, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x):\n        """"""\n        Feedforward function.\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n    def train_step(self,\n                   real_batch,\n                   netD,\n                   optG,\n                   log_data,\n                   device=None,\n                   global_step=None,\n                   **kwargs):\n        """"""\n        Train step function.\n        """"""\n        self.zero_grad()\n\n        # Get only batch size from real batch\n        batch_size = real_batch[0].shape[0]\n\n        # Produce fake images and logits\n        fake_images = self.generate_images(num_images=batch_size,\n                                           device=device)\n        output, _ = netD(fake_images)\n\n        # Compute GAN loss, upright images only.\n        errG = self.compute_gan_loss(output)\n\n        # Compute SS loss, rotates the images. -- only for fake images!\n        errG_SS = netD.compute_ss_loss(images=fake_images,\n                                       scale=self.ss_loss_scale)\n\n        # Backprop and update gradients\n        errG_total = errG + errG_SS\n        errG_total.backward()\n        optG.step()\n\n        # Log statistics\n        log_data.add_metric(\'errG\', errG, group=\'loss\')\n        log_data.add_metric(\'errG_SS\', errG_SS, group=\'loss_SS\')\n\n        return log_data\n\n\nclass SSGANDiscriminator(gan.BaseDiscriminator):\n    def __init__(self, ndf=128, loss_type=\'hinge\', **kwargs):\n        super().__init__(ndf=ndf, loss_type=loss_type, **kwargs)\n        self.num_classes = 4\n        self.ss_loss_scale = 1.0\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf)\n        self.block2 = DBlock(self.ndf, self.ndf, downsample=True)\n        self.block3 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.block4 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.l5 = SNLinear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        nn.init.xavier_uniform_(self.l5.weight.data, 1.0)\n\n        # Rotation class prediction layer\n        self.l_y = SNLinear(self.ndf, self.num_classes)\n        nn.init.xavier_uniform_(self.l_y.weight.data, 1.0)\n\n        \n\n    def forward(self, x):\n        """"""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits,\n        and rotation classes.\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l5(h)\n\n        # Produce the class output logits\n        output_classes = self.l_y(h)\n\n        return output, output_classes\n\n\n    def _rot_tensor(self, image, deg):\n        """"""\n        Rotation for pytorch tensors using rotation matrix. Takes in a tensor of (C, H, W shape).\n        """"""\n        if deg == 90:\n            return image.transpose(1, 2).flip(1)\n\n        elif deg == 180:\n            return image.flip(1).flip(2)\n\n        elif deg == 270:\n            return image.transpose(1, 2).flip(2)\n\n        elif deg == 0:\n            return image\n\n        else:\n            raise NotImplementedError(\n                ""Function only supports 90,180,270,0 degree rotation."")\n\n    def _rotate_batch(self, images):\n        """"""\n        Rotate a quarter batch of images in each of 4 directions.\n        """"""\n        N, C, H, W = images.shape\n        choices = [(i, i * 4 // N) for i in range(N)]\n\n        # Collect rotated images and labels\n        ret = []\n        ret_labels = []\n        degrees = [0, 90, 180, 270]\n        for i in range(N):\n            idx, rot_label = choices[i]\n\n            # Rotate images\n            image = self._rot_tensor(images[idx],\n                                     deg=degrees[rot_label])  # (C, H, W) shape\n            image = torch.unsqueeze(image, 0)  # (1, C, H, W) shape\n\n            # Get labels accordingly\n            label = torch.from_numpy(np.array(rot_label))  # Zero dimension\n            label = torch.unsqueeze(label, 0)\n\n            ret.append(image)\n            ret_labels.append(label)\n\n        # Concatenate images and labels to (N, C, H, W) and (N, ) shape respectively.\n        ret = torch.cat(ret, dim=0)\n        ret_labels = torch.cat(ret_labels, dim=0).to(ret.device)\n\n        return ret, ret_labels\n\n    def compute_ss_loss(self, images, scale):\n        """"""\n        Function to compute SS loss.\n        """"""\n        # Rotate images and produce labels here.\n        images_rot, class_labels = self._rotate_batch(images=images)\n\n        # Compute SS loss\n        _, output_classes = self.forward(images_rot)\n\n        err_SS = F.cross_entropy(input=output_classes, target=class_labels)\n\n        # Scale SS loss\n        err_SS = scale * err_SS\n\n        return err_SS\n\n    def train_step(self,\n                   real_batch,\n                   netG,\n                   optD,\n                   log_data,\n                   device=None,                   \n                   global_step=None,\n                   **kwargs):\n        """"""\n        Train step function for discirminator.\n        """"""\n        self.zero_grad()\n\n        # Produce real images\n        real_images, _ = real_batch\n        batch_size = real_images.shape[0]  # Match batch sizes for last iter\n\n        # Produce fake images\n        fake_images = netG.generate_images(num_images=batch_size,\n                                           device=device).detach()\n\n        # Compute real and fake logits for gan loss\n        output_real, _ = self.forward(real_images)\n        output_fake, _ = self.forward(fake_images)\n\n        # Compute GAN loss, upright images only.\n        errD = self.compute_gan_loss(output_real=output_real,\n                                     output_fake=output_fake)\n\n        # Compute SS loss, rotates the images. -- only for real images!\n        errD_SS = self.compute_ss_loss(images=real_images,\n                                       scale=self.ss_loss_scale)\n\n        # Backprop and update gradients\n        errD_total = errD + errD_SS\n        errD_total.backward()\n        optD.step()\n\n        # Compute probabilities\n        D_x, D_Gz = self.compute_probs(output_real=output_real,\n                                       output_fake=output_fake)\n\n        # Log statistics for D once out of loop\n        log_data.add_metric(\'errD\', errD, group=\'loss\')\n        log_data.add_metric(\'errD_SS\', errD_SS, group=\'loss_SS\')\n        log_data.add_metric(\'D(x)\', D_x, group=\'prob\')\n        log_data.add_metric(\'D(G(z))\', D_Gz, group=\'prob\')\n\n        return log_data\n\n\n#########################\n#        Training\n#########################\n# Directories\ndataset_dir = \'./datasets\'\nlog_dir = \'./log/ssgan\'\n\n# Data handling objects\ndevice = torch.device(\'cuda:0\' if torch.cuda.is_available() else ""cpu"")\ndataset = mmc.datasets.load_dataset(root=\'./datasets\', name=\'cifar10\')\ndataloader = torch.utils.data.DataLoader(\n    dataset, batch_size=64, shuffle=True, num_workers=4)\n\n# Define models and optimizers\nnetG = SSGANGenerator().to(device)\nnetD = SSGANDiscriminator().to(device)\noptD = optim.Adam(netD.parameters(), 2e-4, betas=(0.0, 0.9))\noptG = optim.Adam(netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n# Start training\ntrainer = mmc.training.Trainer(\n    netD=netD,\n    netG=netG,\n    optD=optD,\n    optG=optG,\n    n_dis=2,\n    num_steps=100000,\n    dataloader=dataloader,\n    log_dir=log_dir,\n    device=device)\ntrainer.train()\n\n##########################\n#       Evaluation\n##########################\n# Evaluate fid\nmmc.metrics.evaluate(\n    metric=\'fid\',\n    log_dir=log_dir,\n    netG=netG,\n    dataset_name=\'cifar10\',\n    num_real_samples=10000,\n    num_fake_samples=10000,\n    evaluate_step=100000,\n    device=device)'"
torch_mimicry/__init__.py,0,"b'from torch_mimicry import nets, training, metrics, datasets, modules\n\n__version__ = ""0.1.13""\n'"
docs/source/conf.py,0,"b""import datetime\nimport sphinx_rtd_theme\nimport doctest\nimport torch_mimicry\n\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'sphinx.ext.githubpages',\n]\n\nsource_suffix = '.rst'\nmaster_doc = 'index'\n\nauthor = 'Kwot Sin Lee'\nproject = 'torch_mimicry'\ncopyright = '{}, {}'.format(datetime.datetime.now().year, author)\n\nversion = 'master'\nrelease = torch_mimicry.__version__\n\nhtml_theme = 'sphinx_rtd_theme'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\ndoctest_default_flags = doctest.NORMALIZE_WHITESPACE\nintersphinx_mapping = {'python': ('https://docs.python.org/', None)}\n\nhtml_theme_options = {\n    'collapse_navigation': False,\n    'display_version': True,\n    'logo_only': True,\n}\n\nhtml_logo = '_static/img/mimicry_logo.png'\nhtml_static_path = ['_static']\nhtml_context = {'css_files': ['_static/css/custom.css']}\n\nadd_module_names = False\n\n\ndef setup(app):\n    def skip(app, what, name, obj, skip, options):\n        members = [\n            '__init__',\n            '__repr__',\n            '__weakref__',\n            '__dict__',\n            '__module__',\n        ]\n        return True if name in members else skip\n\n    app.connect('autodoc-skip-member', skip)\n"""
tests/datasets/test_data_utils.py,1,"b'import os\nimport shutil\n\nimport numpy as np\nimport pytest\nimport torch\nfrom skimage.io import imsave\n\nfrom torch_mimicry.datasets import data_utils\n\n\nclass TestDataUtils:\n    def setup(self):\n        self.dataset_dir = \'./datasets\'\n        self.test_dir = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), ""test_dir"")\n\n        if not os.path.exists(self.test_dir):\n            os.makedirs(self.test_dir)\n\n    def create_test_image(self, H, W, C):\n        img = np.random.randn(H, W, C)\n        return img\n\n    def create_imagenet_images(self):\n        # Root diectory\n        imagenet_dir = os.path.join(self.test_dir, \'imagenet\')\n        if not os.path.exists(imagenet_dir):\n            os.makedirs(imagenet_dir)\n\n        # Meta file\n        meta_file = os.path.join(imagenet_dir, \'meta.bin\')\n        shutil.copy(\'./tests/datasets/imagenet/test.bin\', meta_file)\n        _, classes = torch.load(meta_file)\n        classes = list(set(classes))\n\n        # Create images\n        num_images = 1\n        H, W, C = 32, 32, 3\n\n        for i, class_id in enumerate(classes):\n            class_dir = os.path.join(imagenet_dir, \'train\', class_id)\n            if not os.path.exists(class_dir):\n                os.makedirs(class_dir)\n\n            for j in range(num_images):\n                img_name = os.path.join(class_dir,\n                                        \'{}_{}.JPEG\'.format(class_id, j))\n\n                img = self.create_test_image(H, W, C)\n                imsave(img_name, img)\n\n    def test_lsun_bedroom_load_error(self):\n        with pytest.raises(ValueError):\n            data_utils.load_lsun_bedroom_dataset(\n                root=os.path.join(self.test_dir, \'does_not_exist\'))\n\n    def test_load_imagenet_dataset(self):\n        self.create_imagenet_images()\n\n        sizes = [32, 128]\n        for size in sizes:\n            dataset = data_utils.load_imagenet_dataset(size=size,\n                                                       root=self.test_dir,\n                                                       download=False)\n            img = dataset[0][0]\n\n            assert img.shape == (3, size, size)\n\n    def test_load_fake_dataset(self):\n        H, W, C = 32, 32, 3\n        dataset = data_utils.load_fake_dataset(root=self.dataset_dir,\n                                               image_size=(C, H, W))\n\n        img = dataset[0][0]\n\n        assert img.shape == (C, H, W)\n\n    def test_load_wrong_dataset(self):\n        with pytest.raises(ValueError):\n            data_utils.load_dataset(\'random\', \'wrong_dataset\')\n\n    @pytest.mark.skipif(not os.path.exists(\'./datasets/celeba/celeba\'),\n                        reason=\'Requires download.\')\n    def test_load_celeba_dataset(self):\n        sizes = [64, 128]\n\n        for size in sizes:\n            dataset = data_utils.load_celeba_dataset(size=size,\n                                                     root=self.dataset_dir,\n                                                     download=False)\n\n            img = dataset[0][0]\n\n            assert img.shape == (3, size, size)\n\n    @pytest.mark.skipif(not os.path.exists(\'./datasets/stl10\'),\n                        reason=\'Requires download.\')\n    def test_load_stl10_dataset(self):\n        if not os.path.exists(\'./datasets/stl10\'):\n            return\n\n        dataset = data_utils.load_stl10_dataset(size=48,\n                                                root=self.dataset_dir,\n                                                download=True)\n\n        img = dataset[0][0]\n\n        assert img.shape == (3, 48, 48)\n\n    @pytest.mark.skipif(not os.path.exists(\'./datasets/cifar10\'),\n                        reason=\'Requires download.\')\n    def test_load_cifar10_dataset(self):\n        dataset = data_utils.load_cifar10_dataset(root=self.dataset_dir)\n\n        img = dataset[0][0]\n\n        assert img.shape == (3, 32, 32)\n\n    @pytest.mark.skipif(not os.path.exists(\'./datasets/cifar100\'),\n                        reason=\'Requires download.\')\n    def test_load_cifar100_dataset(self):\n        dataset = data_utils.load_cifar100_dataset(root=self.dataset_dir)\n\n        img = dataset[0][0]\n\n        assert img.shape == (3, 32, 32)\n\n    @pytest.mark.skipif(\n        not os.path.exists(\'./datasets/lsun/bedroom_train_lmdb\'),\n        reason=\'Requires download.\')\n    def test_load_lsun_bedroom_dataset(self):\n        dataset = data_utils.load_lsun_bedroom_dataset(root=self.dataset_dir,\n                                                       size=128)\n\n        img = dataset[0][0]\n\n        assert img.shape == (3, 128, 128)\n\n    def teardown(self):\n        shutil.rmtree(self.test_dir)\n\n\nif __name__ == ""__main__"":\n    test = TestDataUtils()\n    test.setup()\n    test.test_load_lsun_bedroom_dataset()\n    test.test_load_celeba_dataset()\n    test.test_load_imagenet_dataset()\n    test.test_load_stl10_dataset()\n    test.test_load_cifar10_dataset()\n    test.test_load_cifar100_dataset()\n    test.test_load_wrong_dataset()\n    test.test_lsun_bedroom_load_error()\n    test.teardown()\n'"
tests/datasets/test_image_loader.py,1,"b'import os\nimport shutil\n\nimport numpy as np\nimport pytest\nimport torch\nfrom skimage.io import imsave\n\nfrom torch_mimicry.datasets import image_loader\n\n\nclass TestImageLoader:\n    def setup(self):\n        self.dataset_dir = \'./datasets\'\n        self.test_dir = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), ""test_dir"")\n\n        if not os.path.exists(os.path.join(self.test_dir)):\n            os.makedirs(self.test_dir)\n\n    def create_test_image(self, H, W, C):\n        img = np.random.randn(H, W, C)\n        return img\n\n    def create_imagenet_images(self):\n        imagenet_dir = os.path.join(self.test_dir, \'imagenet\')\n        if not os.path.exists(imagenet_dir):\n            os.makedirs(imagenet_dir)\n\n        # Meta file\n        meta_file = os.path.join(imagenet_dir, \'meta.bin\')\n        shutil.copy(\'./tests/datasets/imagenet/test.bin\', meta_file)\n        _, classes = torch.load(meta_file)\n        classes = list(set(classes))\n\n        # Create images\n        num_images = 1\n        H, W, C = 32, 32, 3\n\n        for i, class_id in enumerate(classes):\n            class_dir = os.path.join(imagenet_dir, \'train\', class_id)\n            if not os.path.exists(class_dir):\n                os.makedirs(class_dir)\n\n            for j in range(num_images):\n                img_name = os.path.join(class_dir,\n                                        \'{}_{}.JPEG\'.format(class_id, j))\n\n                img = self.create_test_image(H, W, C)\n                imsave(img_name, img)\n\n    def test_lsun_bedroom_load_error(self):\n        with pytest.raises(ValueError):\n            image_loader.get_lsun_bedroom_images(num_samples=1,\n                                                 root=os.path.join(\n                                                     self.test_dir,\n                                                     \'does_not_exist\'))\n\n    def test_imagenet_small_sample_error(self):\n        with pytest.raises(ValueError):\n            image_loader.get_imagenet_images(1)\n\n    def test_get_imagenet_images(self):\n        self.create_imagenet_images()\n\n        num_images = 1000\n        sizes = [32, 128]\n\n        for size in sizes:\n            images = image_loader.get_imagenet_images(num_images,\n                                                      size=size,\n                                                      root=self.test_dir)\n\n            assert images.shape == (num_images, size, size, 3)\n            assert np.mean(images[0]) != np.mean(images[1])\n\n    def test_get_fake_data_images(self):\n        num_images = 10\n        H, W, C = 32, 32, 3\n        images = image_loader.get_fake_data_images(num_images,\n                                                   root=self.test_dir)\n\n        assert images.shape == (num_images, H, W, C)\n        assert np.mean(images[0]) != np.mean(images[1])\n\n    @pytest.mark.skipif(not os.path.exists(\'./datasets/stl10\'),\n                        reason=\'Requires download.\')\n    def test_get_stl10_images(self):\n        num_images = 10\n        H, W, C = 48, 48, 3\n        images = image_loader.get_stl10_images(num_images,\n                                               size=H,\n                                               root=self.dataset_dir)\n\n        assert images.shape == (num_images, H, W, C)\n        assert np.mean(images[0]) != np.mean(images[1])\n\n    @pytest.mark.skipif(not os.path.exists(\'./datasets/cifar10\'),\n                        reason=\'Requires download.\')\n    def test_get_cifar10_images(self):\n        num_images = 10\n        H, W, C = 32, 32, 3\n        images = image_loader.get_cifar10_images(num_images,\n                                                 root=self.dataset_dir)\n\n        assert images.shape == (num_images, H, W, C)\n        assert np.mean(images[0]) != np.mean(images[1])\n\n    @pytest.mark.skipif(not os.path.exists(\'./datasets/cifar100\'),\n                        reason=\'Requires download.\')\n    def test_get_cifar100_images(self):\n        num_images = 10\n        H, W, C = 32, 32, 3\n        images = image_loader.get_cifar100_images(num_images,\n                                                  root=self.dataset_dir)\n\n        assert images.shape == (num_images, H, W, C)\n        assert np.mean(images[0]) != np.mean(images[1])\n\n    @pytest.mark.skipif(\n        not os.path.exists(\'./datasets/lsun/bedroom_train_lmdb\'),\n        reason=\'Requires download.\')\n    def test_get_lsun_bedroom_images(self):\n        num_images = 10\n        H, W, C = 128, 128, 3\n        images = image_loader.get_lsun_bedroom_images(num_images,\n                                                      size=H,\n                                                      root=self.dataset_dir)\n\n        assert images.shape == (num_images, H, W, C)\n        assert np.mean(images[0]) != np.mean(images[1])\n\n    @pytest.mark.skipif(not os.path.exists(\'./datasets/celeba/celeba\'),\n                        reason=\'Requires download.\')\n    def test_get_celeba_images(self):\n        num_images = 10\n        sizes = [64, 128]\n\n        for size in sizes:\n            images = image_loader.get_celeba_images(num_images,\n                                                    size=size,\n                                                    root=self.dataset_dir,\n                                                    download=False)\n\n            assert images.shape == (num_images, size, size, 3)\n            assert np.mean(images[0]) != np.mean(images[1])\n\n    def teardown(self):\n        shutil.rmtree(self.test_dir)\n\n\nif __name__ == ""__main__"":\n    test = TestImageLoader()\n    test.setup()\n    test.test_get_lsun_bedroom_images()\n    test.test_get_celeba_images()\n    test.test_get_imagenet_images()\n    test.test_get_stl10_images()\n    test.test_get_cifar10_images()\n    test.test_get_cifar100_images()\n    test.test_get_fake_data_images()\n    test.test_imagenet_small_sample_error()\n    test.test_lsun_bedroom_load_error()\n    test.teardown()\n'"
tests/metrics/test_compute_fid.py,2,"b'import os\nimport shutil\n\nimport tensorflow as tf\nimport torch\n\nfrom torch_mimicry.metrics import compute_fid\nfrom torch_mimicry.metrics.inception_model import inception_utils\nfrom torch_mimicry.nets.gan import gan\n\n\nclass ExampleGen(gan.BaseGenerator):\n    def __init__(self,\n                 bottom_width=4,\n                 nz=4,\n                 ngf=256,\n                 loss_type=\'gan\',\n                 *args,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         *args,\n                         **kwargs)\n\n    def forward(self, x):\n        output = torch.ones(x.shape[0], 3, 32, 32)\n\n        return output\n\n\nclass TestComputeFID:\n    def setup(self):\n        self.netG = ExampleGen()\n        self.num_real_samples = 10\n        self.num_fake_samples = 10\n        self.batch_size = 10\n        self.device = torch.device(""cpu"")\n\n        # Create inception graph once.\n        self.inception_path = \'./metrics/inception_model\'\n        if not os.path.exists(self.inception_path):\n            os.makedirs(self.inception_path)\n\n        inception_utils.create_inception_graph(self.inception_path)\n\n        # Directory\n        self.log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                    ""test_log"")\n        if not os.path.exists(self.log_dir):\n            os.makedirs(self.log_dir)\n\n    def test_compute_gen_dist_stats(self):\n        if self.device.index is not None:\n            # Avoid unbounded memory usage\n            gpu_options = tf.GPUOptions(allow_growth=True,\n                                        per_process_gpu_memory_fraction=0.15,\n                                        visible_device_list=str(\n                                            self.device.index))\n            config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n\n        else:\n            config = tf.compat.v1.ConfigProto(device_count={\'GPU\': 0})\n\n        with tf.compat.v1.Session(config=config) as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n\n            m_fake, s_fake = compute_fid.compute_gen_dist_stats(\n                netG=self.netG,\n                num_samples=self.num_fake_samples,\n                sess=sess,\n                device=self.device,\n                seed=0,\n                batch_size=self.batch_size,\n                print_every=1)\n\n            assert m_fake.shape == (2048, )\n            assert s_fake.shape == (2048, 2048)\n\n    def test_compute_real_dist_stats(self):\n        if self.device.index is not None:\n            # Avoid unbounded memory usage\n            gpu_options = tf.GPUOptions(allow_growth=True,\n                                        per_process_gpu_memory_fraction=0.15,\n                                        visible_device_list=str(\n                                            self.device.index))\n            config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n\n        else:\n            config = tf.compat.v1.ConfigProto(device_count={\'GPU\': 0})\n\n        with tf.compat.v1.Session(config=config) as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n\n            m_real, s_real = compute_fid.compute_real_dist_stats(\n                num_samples=self.num_real_samples,\n                sess=sess,\n                dataset_name=\'fake_data\',\n                batch_size=self.batch_size,\n                stats_file=None,\n                log_dir=self.log_dir,\n                seed=0,\n                verbose=True)\n\n            assert m_real.shape == (2048, )\n            assert s_real.shape == (2048, 2048)\n\n    def test_fid_score(self):\n        score = compute_fid.fid_score(num_real_samples=self.num_real_samples,\n                                      num_fake_samples=self.num_fake_samples,\n                                      netG=self.netG,\n                                      device=self.device,\n                                      seed=99,\n                                      batch_size=self.batch_size,\n                                      dataset_name=\'fake_data\',\n                                      log_dir=self.log_dir)\n\n        assert type(score) == float\n\n    def teardown(self):\n        if os.path.exists(self.log_dir):\n            shutil.rmtree(self.log_dir)\n        del self.netG\n\n\nif __name__ == ""__main__"":\n    test = TestComputeFID()\n    test.setup()\n    test.test_compute_gen_dist_stats()\n    test.test_compute_real_dist_stats()\n    test.test_fid_score()\n    test.teardown()\n'"
tests/metrics/test_compute_is.py,2,"b'import torch\n\nfrom torch_mimicry.metrics import compute_is\nfrom torch_mimicry.nets.gan import gan\n\n\nclass ExampleGen(gan.BaseGenerator):\n    def __init__(self,\n                 bottom_width=4,\n                 nz=4,\n                 ngf=256,\n                 loss_type=\'gan\',\n                 *args,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         *args,\n                         **kwargs)\n\n    def forward(self, x):\n        output = torch.ones(x.shape[0], 3, 32, 32)\n\n        return output\n\n\nclass TestComputeIS:\n    def setup(self):\n        self.device = torch.device(\'cpu\')\n        self.netG = ExampleGen()\n\n    def test_compute_inception_score(self):\n        mean, std = compute_is.inception_score(netG=self.netG,\n                                               device=self.device,\n                                               num_samples=10,\n                                               batch_size=10)\n\n        assert type(mean) == float\n        assert type(std) == float\n\n    def teardown(self):\n        del self.netG\n\n\nif __name__ == ""__main__"":\n    test = TestComputeIS()\n    test.setup()\n    test.test_compute_inception_score()\n'"
tests/metrics/test_compute_kid.py,2,"b'import os\nimport shutil\n\nimport tensorflow as tf\nimport torch\n\nfrom torch_mimicry.metrics import compute_kid\nfrom torch_mimicry.metrics.inception_model import inception_utils\nfrom torch_mimicry.nets.gan import gan\n\n\nclass ExampleGen(gan.BaseGenerator):\n    def __init__(self,\n                 bottom_width=4,\n                 nz=4,\n                 ngf=256,\n                 loss_type=\'gan\',\n                 *args,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         *args,\n                         **kwargs)\n\n    def forward(self, x):\n        output = torch.ones(x.shape[0], 3, 32, 32)\n\n        return output\n\n\nclass TestComputeKID:\n    def setup(self):\n        self.netG = ExampleGen()\n        self.num_subsets = 10\n        self.subset_size = 5\n        self.num_samples = self.subset_size * self.num_subsets\n        self.device = torch.device(""cpu"")\n\n        # Create inception graph once.\n        self.inception_path = \'./metrics/inception_model\'\n\n        if not os.path.exists(self.inception_path):\n            os.makedirs(self.inception_path)\n        inception_utils.create_inception_graph(self.inception_path)\n\n        # Directory\n        self.log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                    ""test_log"")\n        if not os.path.exists(self.log_dir):\n            os.makedirs(self.log_dir)\n\n    def test_compute_gen_dist_feat(self):\n        if self.device.index is not None:\n            # Avoid unbounded memory usage\n            gpu_options = tf.GPUOptions(allow_growth=True,\n                                        per_process_gpu_memory_fraction=0.15,\n                                        visible_device_list=str(\n                                            self.device.index))\n            config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n\n        else:\n            config = tf.compat.v1.ConfigProto(device_count={\'GPU\': 0})\n\n        with tf.compat.v1.Session(config=config) as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n\n            fake_feat = compute_kid.compute_gen_dist_feat(\n                netG=self.netG,\n                num_samples=self.num_samples,\n                sess=sess,\n                seed=0,\n                device=self.device,\n                batch_size=10,\n                print_every=1)\n\n            assert fake_feat.shape == (self.num_samples, 2048)\n\n    def test_compute_real_dist_feat(self):\n        if self.device.index is not None:\n            # Avoid unbounded memory usage\n            gpu_options = tf.GPUOptions(allow_growth=True,\n                                        per_process_gpu_memory_fraction=0.15,\n                                        visible_device_list=str(\n                                            self.device.index))\n            config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n\n        else:\n            config = tf.compat.v1.ConfigProto(device_count={\'GPU\': 0})\n\n        with tf.compat.v1.Session(config=config) as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n\n            real_feat = compute_kid.compute_real_dist_feat(\n                num_samples=self.num_samples,\n                sess=sess,\n                dataset_name=\'fake_data\',\n                batch_size=10,\n                log_dir=self.log_dir)\n\n            print(real_feat.shape)\n\n            assert real_feat.shape == (self.num_samples, 2048)\n\n    def test_kid_score(self):\n        score, var = compute_kid.kid_score(num_subsets=self.num_subsets,\n                                           subset_size=self.subset_size,\n                                           netG=self.netG,\n                                           device=self.device,\n                                           dataset_name=\'fake_data\',\n                                           batch_size=10,\n                                           log_dir=self.log_dir,\n                                           seed=0)\n\n        assert type(score) == float\n        assert type(var) == float\n\n    def teardown(self):\n        if os.path.exists(self.log_dir):\n            shutil.rmtree(self.log_dir)\n        del self.netG\n\n\nif __name__ == ""__main__"":\n    test = TestComputeKID()\n    test.setup()\n    test.test_compute_gen_dist_feat()\n    test.test_compute_real_dist_feat()\n    test.test_kid_score()\n    test.teardown()\n'"
tests/metrics/test_compute_metrics.py,2,"b'import os\nimport shutil\n\nimport pytest\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.metrics import compute_metrics\nfrom torch_mimicry.nets.gan import gan\n\n\nclass ExampleGen(gan.BaseGenerator):\n    def __init__(self,\n                 bottom_width=4,\n                 nz=4,\n                 ngf=16,\n                 loss_type=\'gan\',\n                 *args,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         *args,\n                         **kwargs)\n        self.linear = nn.Linear(self.nz, 3072)\n\n    def forward(self, x):\n        output = self.linear(x)\n        output = output.view(x.shape[0], 3, 32, 32)\n\n        return output\n\n\nclass TestMetrics:\n    def setup(self):\n        # Default values\n        self.device = torch.device(\'cpu\')\n        self.start_seed = 0\n        self.evaluate_step = 100000\n        self.dataset_name = \'fake_data\'\n\n        # Test directory\n        self.log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                    ""test_log"")\n        if not os.path.exists(self.log_dir):\n            os.makedirs(self.log_dir)\n\n        # Create example checkpoint\n        self.netG = ExampleGen()\n        self.netG.save_checkpoint(directory=os.path.join(\n            self.log_dir, \'checkpoints\', \'netG\'),\n                                  global_step=self.evaluate_step,\n                                  optimizer=None)\n\n    @pytest.mark.skipif(True, reason=\'Docker OOM.\'\n                        )  # TODO: Restore when CI plan is good.\n    def test_evaluate_fid(self):\n        kwargs = {\n            \'metric\': \'fid\',\n            \'log_dir\': self.log_dir,\n            \'netG\': self.netG,\n            \'dataset_name\': self.dataset_name,\n            \'num_real_samples\': 10,\n            \'num_fake_samples\': 10,\n            \'evaluate_step\': self.evaluate_step,\n            \'start_seed\': self.start_seed,\n            \'device\': self.device\n        }\n\n        scores = compute_metrics.evaluate(**kwargs)[self.evaluate_step]\n        assert type(scores) == list\n        assert all(map(lambda x: type(x) == float, scores))\n\n    @pytest.mark.skipif(True, reason=\'Docker OOM.\'\n                        )  # TODO: Restore when CI plan is good.\n    def test_evaluate_kid(self):\n        kwargs = {\n            \'metric\': \'kid\',\n            \'log_dir\': self.log_dir,\n            \'evaluate_step\': self.evaluate_step,\n            \'num_subsets\': 10,\n            \'subset_size\': 10,\n            \'netG\': self.netG,\n            \'device\': self.device,\n            \'start_seed\': self.start_seed,\n            \'dataset_name\': self.dataset_name\n        }\n\n        scores = compute_metrics.evaluate(**kwargs)[self.evaluate_step]\n        assert type(scores) == list\n        assert all(map(lambda x: type(x) == float, scores))\n\n    @pytest.mark.skipif(True, reason=\'Docker OOM.\'\n                        )  # TODO: Restore when CI plan is good.\n    def test_evaluate_is(self):\n        kwargs = {\n            \'metric\': \'inception_score\',\n            \'log_dir\': self.log_dir,\n            \'netG\': self.netG,\n            \'num_samples\': 10,\n            \'num_runs\': 3,\n            \'evaluate_step\': self.evaluate_step,\n            \'device\': self.device,\n            \'start_seed\': self.start_seed,\n        }\n\n        scores = compute_metrics.evaluate(**kwargs)[self.evaluate_step]\n        assert type(scores) == list\n        assert all(map(lambda x: type(x) == float, scores))\n\n    def test_arguments(self):\n        for metric in [\'fid\', \'kid\', \'inception_score\']:\n            with pytest.raises(ValueError):\n                compute_metrics.evaluate(metric=metric,\n                                         log_dir=self.log_dir,\n                                         netG=self.netG,\n                                         dataset_name=self.dataset_name,\n                                         evaluate_step=self.evaluate_step,\n                                         device=self.device)\n\n    def test_wrong_metric(self):\n        with pytest.raises(ValueError):\n            compute_metrics.evaluate(metric=\'wrong_metric\',\n                                     log_dir=self.log_dir,\n                                     netG=self.netG,\n                                     dataset_name=self.dataset_name,\n                                     evaluate_step=self.evaluate_step,\n                                     device=self.device)\n\n    def teardown(self):\n        del self.netG\n        shutil.rmtree(self.log_dir)\n\n\nif __name__ == ""__main__"":\n    test = TestMetrics()\n    test.setup()\n    test.test_arguments()\n    test.test_wrong_metric()\n    test.test_evaluate_fid()\n    test.test_evaluate_kid()\n    test.test_evaluate_is()\n    test.teardown()\n'"
tests/modules/test_layers.py,4,"b'import torch\nimport torch.nn as nn\n\nfrom torch_mimicry.modules import layers\n\n\nclass TestLayers:\n    def setup(self):\n        self.N, self.C, self.H, self.W = (4, 3, 32, 32)\n        self.n_out = 8\n\n    def test_ConditionalBatchNorm2d(self):\n        num_classes = 10\n        X = torch.ones(self.N, self.C, self.H, self.W)\n        y = torch.randint(low=0, high=num_classes, size=(self.N, ))\n\n        # Setup cond. BN --> Note: because we usually do\n        # BN-ReLU-Conv, we take num feat as input channels\n        conv = nn.Conv2d(self.C, self.n_out, 1, 1, 0)\n        bn = layers.ConditionalBatchNorm2d(num_features=self.C,\n                                           num_classes=num_classes)\n\n        output = bn(X, y)\n        output = conv(output)\n\n        assert output.shape == (self.N, 8, self.H, self.W)\n\n    def test_SelfAttention(self):\n        ngf = 16\n        X = torch.ones(self.N, ngf, 16, 16)\n        sa_layer = layers.SelfAttention(ngf)\n\n        assert sa_layer(X).shape == (self.N, ngf, 16, 16)\n\n\nif __name__ == ""__main__"":\n    test = TestLayers()\n    test.setup()\n    test.test_ConditionalBatchNorm2d()\n    test.test_SelfAttention()'"
tests/modules/test_losses.py,10,"b'import torch\n\nfrom torch_mimicry.modules import losses\n\n\nclass TestLosses:\n    def setup(self):\n        self.output_real = torch.ones(4, 1)\n        self.output_fake = torch.ones(4, 1)\n        self.device = torch.device(\'cpu\')\n\n    def test_minimax_loss(self):\n        loss_gen = losses.minimax_loss_gen(output_fake=self.output_fake)\n\n        loss_dis = losses.minimax_loss_dis(output_fake=self.output_fake,\n                                           output_real=self.output_real)\n\n        assert loss_gen.dtype == torch.float32\n        assert loss_dis.dtype == torch.float32\n        assert loss_gen.item() - 0.3133 < 1e-2\n        assert loss_dis.item() - 1.6265 < 1e-2\n\n    def test_ns_loss(self):\n        loss_gen = losses.ns_loss_gen(self.output_fake)\n\n        assert loss_gen.dtype == torch.float32\n        assert loss_gen.item() - 0.3133 < 1e-2\n\n    def test_wasserstein_loss(self):\n        loss_gen = losses.wasserstein_loss_gen(self.output_fake)\n        loss_dis = losses.wasserstein_loss_dis(output_real=self.output_real,\n                                               output_fake=self.output_fake)\n\n        assert loss_gen.dtype == torch.float32\n        assert loss_dis.dtype == torch.float32\n        assert loss_gen.item() + 1.0 < 1e-2\n        assert loss_dis.item() < 1e-2\n\n    def test_hinge_loss(self):\n        loss_gen = losses.hinge_loss_gen(output_fake=self.output_fake)\n        loss_dis = losses.hinge_loss_dis(output_fake=self.output_fake,\n                                         output_real=self.output_real)\n\n        assert loss_gen.dtype == torch.float32\n        assert loss_dis.dtype == torch.float32\n        assert loss_gen.item() + 1.0 < 1e-2\n        assert loss_dis.item() - 2.0 < 1e-2\n\n    def teardown(self):\n        del self.output_real\n        del self.output_fake\n        del self.device\n\n\nif __name__ == ""__main__"":\n    test = TestLosses()\n    test.setup()\n    test.test_minimax_loss()\n    test.test_ns_loss()\n    test.test_wasserstein_loss()\n    test.test_hinge_loss()\n    test.teardown()\n'"
tests/modules/test_resblocks.py,2,"b'from itertools import product\n\nimport torch\n\nfrom torch_mimicry.modules import resblocks\n\n\nclass TestResBlocks:\n    def setup(self):\n        self.images = torch.ones(4, 3, 16, 16)\n\n    def test_GBlock(self):\n        # Arguments\n        num_classes_list = [0, 10]\n        spectral_norm_list = [True, False]\n        in_channels = 3\n        out_channels = 8\n        args_comb = product(num_classes_list, spectral_norm_list)\n\n        for args in args_comb:\n            num_classes = args[0]\n            spectral_norm = args[1]\n\n            if num_classes > 0:\n                y = torch.ones((4, ), dtype=torch.int64)\n            else:\n                y = None\n\n            gen_block_up = resblocks.GBlock(in_channels=in_channels,\n                                            out_channels=out_channels,\n                                            upsample=True,\n                                            num_classes=num_classes,\n                                            spectral_norm=spectral_norm)\n\n            gen_block = resblocks.GBlock(in_channels=in_channels,\n                                         out_channels=out_channels,\n                                         upsample=False,\n                                         num_classes=num_classes,\n                                         spectral_norm=spectral_norm)\n\n            assert gen_block_up(self.images, y).shape == (4, 8, 32, 32)\n            assert gen_block(self.images, y).shape == (4, 8, 16, 16)\n\n    def test_DBlocks(self):\n        in_channels = 3\n        out_channels = 8\n\n        for spectral_norm in [True, False]:\n            dis_block_down = resblocks.DBlock(in_channels=in_channels,\n                                              out_channels=out_channels,\n                                              downsample=True,\n                                              spectral_norm=spectral_norm)\n\n            dis_block = resblocks.DBlock(in_channels=in_channels,\n                                         out_channels=out_channels,\n                                         downsample=False,\n                                         spectral_norm=spectral_norm)\n\n            dis_block_opt = resblocks.DBlockOptimized(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                spectral_norm=spectral_norm)\n\n            assert dis_block(self.images).shape == (4, out_channels, 16, 16)\n            assert dis_block_down(self.images).shape == (4, out_channels, 8, 8)\n            assert dis_block_opt(self.images).shape == (4, out_channels, 8, 8)\n\n    def teardown(self):\n        del self.images\n\n\nif __name__ == ""__main__"":\n    test = TestResBlocks()\n    test.setup()\n    test.test_GBlock()\n    test.test_DBlocks()\n    test.teardown()\n'"
tests/modules/test_spectral_norm.py,8,"b'import torch\nimport torch.nn as nn\n\nfrom torch_mimicry.modules import spectral_norm\n\n\nclass TestSpectralNorm:\n    def setup(self):\n        torch.manual_seed(0)\n        self.N, self.C, self.H, self.W = (32, 16, 32, 32)\n        self.n_in = self.C\n        self.n_out = 32\n\n    def test_SNConv2d(self):\n        conv = spectral_norm.SNConv2d(self.n_in, self.n_out, 1, 1, 0)\n        conv_def = nn.utils.spectral_norm(\n            nn.Conv2d(self.n_in, self.n_out, 1, 1, 0))\n\n        # Init with ones to test implementation without randomness.\n        nn.init.ones_(conv.weight.data)\n        nn.init.ones_(conv_def.weight.data)\n\n        # Get outputs\n        X = torch.ones(self.N, self.C, self.H, self.W)\n        output = conv(X)\n        output_def = conv_def(X)\n\n        # Test valid shape\n        assert output.shape == output_def.shape == (32, 32, 32, 32)\n\n        # Test per element it is very close to default implementation\n        # to preserve correctness even when user toggles b/w implementations\n        assert abs(torch.mean(output_def) - torch.mean(output)) < 1\n\n    def test_SNLinear(self):\n        linear = spectral_norm.SNLinear(self.n_in, self.n_out)\n        linear_def = nn.utils.spectral_norm(nn.Linear(self.n_in, self.n_out))\n\n        nn.init.ones_(linear.weight.data)\n        nn.init.ones_(linear_def.weight.data)\n\n        X = torch.ones(self.N, self.n_in)\n        output = linear(X)\n        output_def = linear_def(X)\n\n        assert output.shape == output_def.shape == (32, 32)\n        assert abs(torch.mean(output_def) - torch.mean(output)) < 1\n\n    def test_SNEmbedding(self):\n        embedding = spectral_norm.SNEmbedding(self.N, self.n_out)\n        embedding_def = nn.utils.spectral_norm(nn.Embedding(\n            self.N, self.n_out))\n\n        nn.init.ones_(embedding.weight.data)\n        nn.init.ones_(embedding_def.weight.data)\n\n        X = torch.ones(self.N, dtype=torch.int64)\n        output = embedding(X)\n        output_def = embedding_def(X)\n\n        assert output.shape == output_def.shape == (32, 32)\n        assert abs(torch.mean(output_def) - torch.mean(output)) < 1\n\n\nif __name__ == ""__main__"":\n    test = TestSpectralNorm()\n    test.setup()\n    test.test_SNConv2d()\n    test.test_SNLinear()\n    test.test_SNEmbedding()\n'"
tests/training/test_logger.py,2,"b'import os\nimport shutil\n\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.gan import gan, cgan\nfrom torch_mimicry.training import logger, metric_log\n\n\nclass ExampleGen(gan.BaseGenerator):\n    def __init__(self,\n                 bottom_width=4,\n                 nz=4,\n                 ngf=16,\n                 loss_type=\'gan\',\n                 *args,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         *args,\n                         **kwargs)\n        self.linear = nn.Linear(self.nz, 3072)\n\n    def forward(self, x):\n        output = self.linear(x)\n        output = output.view(x.shape[0], 3, 32, 32)\n\n        return output\n\n\nclass ExampleConditionalGen(cgan.BaseConditionalGenerator):\n    def __init__(self,\n                 bottom_width=4,\n                 nz=4,\n                 ngf=16,\n                 loss_type=\'gan\',\n                 num_classes=10,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         num_classes=num_classes,\n                         **kwargs)\n        self.linear = nn.Linear(self.nz, 3072)\n\n    def forward(self, x, y=None):\n        output = self.linear(x)\n        output = output.view(x.shape[0], 3, 32, 32)\n\n        return output\n\n\nclass TestLogger:\n    def setup(self):\n        self.log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                    ""test_log"")\n\n        self.logger = logger.Logger(log_dir=self.log_dir,\n                                    num_steps=100,\n                                    dataset_size=50000,\n                                    flush_secs=30,\n                                    device=torch.device(\'cpu\'))\n\n        self.scalars = [\n            \'errG\',\n            \'errD\',\n            \'D(x)\',\n            \'D(G(z))\',\n            \'img\',\n            \'lr_D\',\n            \'lr_G\',\n        ]\n\n    def test_print_log(self):\n        log_data = metric_log.MetricLog()\n        global_step = 10\n\n        # Populate log data with some value\n        for scalar in self.scalars:\n            if scalar == \'img\':\n                continue\n\n            log_data.add_metric(scalar, 1.0)\n\n        printed = self.logger.print_log(global_step=global_step,\n                                        log_data=log_data,\n                                        time_taken=10)\n\n        assert printed == (\n            \'INFO: [Epoch 1/1][Global Step: 10/100] \' +\n            \'\\n| D(G(z)): 1.0\\n| D(x): 1.0\\n| errD: 1.0\\n| errG: 1.0\' +\n            \'\\n| lr_D: 1.0\\n| lr_G: 1.0\\n| (10.0000 sec/idx)\')\n\n    def test_vis_images(self):\n        netG = ExampleGen()\n        netG_conditional = ExampleConditionalGen()\n\n        global_step = 10\n        num_images = 64\n\n        # Test unconditional\n        self.logger.vis_images(netG, global_step, num_images)\n        img_dir = os.path.join(self.log_dir, \'images\')\n        filenames = os.listdir(img_dir)\n        assert \'fake_samples_step_10.png\' in filenames\n        assert \'fixed_fake_samples_step_10.png\' in filenames\n\n        # Remove images\n        for file in filenames:\n            os.remove(os.path.join(img_dir, file))\n\n        # Test conditional\n        self.logger.vis_images(netG_conditional, global_step, num_images)\n        assert \'fake_samples_step_10.png\' in filenames\n        assert \'fixed_fake_samples_step_10.png\' in filenames\n\n    def teardown(self):\n        shutil.rmtree(self.log_dir)\n\n\nif __name__ == ""__main__"":\n    test = TestLogger()\n    test.setup()\n    test.test_print_log()\n    test.test_vis_images()\n    test.teardown()\n'"
tests/training/test_metric_log.py,0,"b'from torch_mimicry.training import metric_log\n\n\nclass TestMetricLog:\n    def setup(self):\n        self.log_data = metric_log.MetricLog()\n\n    def test_add_metric(self):\n        # Singular metric\n        self.log_data.add_metric(\'singular\', 1.0124214)\n        assert self.log_data[\'singular\'] == 1.0124\n\n        # Multiple metrics under same group\n        self.log_data.add_metric(\'errD\', 1.00001, group=\'loss\')\n        self.log_data.add_metric(\'errG\', 1.0011, group=\'loss\')\n\n        assert self.log_data.get_group_name(\n            \'errD\') == self.log_data.get_group_name(\'errG\')\n\n    def teardown(self):\n        del self.log_data\n\n\nif __name__ == ""__main__"":\n    test = TestMetricLog()\n    test.setup()\n    test.test_add_metric()\n    test.teardown()\n'"
tests/training/test_scheduler.py,2,"b'import torch.nn as nn\nimport torch.optim as optim\n\nfrom torch_mimicry.training import scheduler, metric_log\n\n\nclass TestLRScheduler:\n    def setup(self):\n        self.netD = nn.Linear(10, 10)\n        self.netG = nn.Linear(10, 10)\n\n        self.num_steps = 10\n        self.lr_D = 2e-4\n        self.lr_G = 2e-4\n\n    def get_lr(self, optimizer):\n        return optimizer.param_groups[0][\'lr\']\n\n    def test_linear_decay(self):\n        optD = optim.Adam(self.netD.parameters(), self.lr_D, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), self.lr_G, betas=(0.0, 0.9))\n\n        lr_scheduler = scheduler.LRScheduler(lr_decay=\'linear\',\n                                             optD=optD,\n                                             optG=optG,\n                                             num_steps=self.num_steps)\n\n        log_data = metric_log.MetricLog()\n        for step in range(1, self.num_steps + 1):\n            lr_scheduler.step(log_data, step)\n\n            curr_lr = ((1 - step / self.num_steps) * self.lr_D)\n\n            assert (curr_lr - self.get_lr(optD) < 1e-5)\n            assert (curr_lr - self.get_lr(optG) < 1e-5)\n\n    def test_no_decay(self):\n        optD = optim.Adam(self.netD.parameters(), self.lr_D, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), self.lr_G, betas=(0.0, 0.9))\n\n        lr_scheduler = scheduler.LRScheduler(lr_decay=\'None\',\n                                             optD=optD,\n                                             optG=optG,\n                                             num_steps=self.num_steps)\n\n        log_data = metric_log.MetricLog()\n        for step in range(1, self.num_steps + 1):\n            lr_scheduler.step(log_data, step)\n\n            assert (self.lr_D == self.get_lr(optD))\n            assert (self.lr_G == self.get_lr(optG))\n\n\nif __name__ == ""__main__"":\n    test = TestLRScheduler()\n    test.setup()\n    test.test_linear_decay()\n    test.test_no_decay()\n'"
tests/training/test_trainer.py,8,"b'import os\nimport shutil\n\nimport numpy as np\nimport pytest\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\n\nfrom torch_mimicry.nets.gan import gan\nfrom torch_mimicry.training.trainer import Trainer\nfrom torch_mimicry.utils import common\n\n\nclass ExampleDataset(Dataset):\n    def __init__(self):\n        super().__init__()\n\n        self.data = np.random.randn(10, 32, 32, 3).astype(np.float32)\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, idx):\n        img = self.data[idx] * np.random.randn()\n        target = -1\n\n        return img, target\n\n\nclass ExampleGen(gan.BaseGenerator):\n    def __init__(self,\n                 bottom_width=4,\n                 nz=4,\n                 ngf=16,\n                 loss_type=\'gan\',\n                 *args,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         *args,\n                         **kwargs)\n        self.linear = nn.Linear(self.nz, 3072)\n\n    def forward(self, x):\n        output = self.linear(x)\n        output = output.view(x.shape[0], 3, 32, 32)\n\n        return output\n\n\nclass ExampleDis(gan.BaseDiscriminator):\n    def __init__(self, ndf=16, loss_type=\'gan\', *args, **kwargs):\n        super().__init__(ndf=ndf, loss_type=loss_type, *args, **kwargs)\n        self.linear = nn.Linear(3072, 1)\n\n    def forward(self, x):\n        output = x.view(x.shape[0], -1)\n        output = self.linear(output)\n\n        return output\n\n\nclass TestTrainer:\n    def setup(self):\n        os.environ[""CUDA_VISIBLE_DEVICES""] = """"\n\n        self.dataset = ExampleDataset()\n        self.dataloader = torch.utils.data.DataLoader(dataset=self.dataset,\n                                                      batch_size=50,\n                                                      shuffle=True,\n                                                      num_workers=8)\n\n        # Define models\n        self.netG = ExampleGen()\n        self.netD = ExampleDis()\n\n        # Build optimizers\n        self.optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        self.optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Build directories\n        self.log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                    ""test_log"")\n\n        # Build test object\n        self.trainer = Trainer(netD=self.netD,\n                               netG=self.netG,\n                               optD=self.optD,\n                               optG=self.optG,\n                               log_dir=self.log_dir,\n                               dataloader=self.dataloader,\n                               num_steps=2,\n                               device=\'cpu\',\n                               save_steps=1,\n                               log_steps=1,\n                               vis_steps=1,\n                               lr_decay=\'linear\',\n                               save_when_end=True)\n\n    def test_attributes(self):\n        # Check parameters\n        default_params = {\n            \'n_dis\': 1,\n            \'num_steps\': 2,\n            \'batch_size\': 50,\n            \'lr_decay\': \'linear\',\n            \'optD\': self.optD.__repr__(),\n            \'optG\': self.optG.__repr__(),\n            \'print_steps\': 1,\n            \'vis_steps\': 1,\n            \'flush_secs\': 30,\n            \'log_steps\': 1,\n            \'save_steps\': 1,\n            \'save_when_end\': True,\n        }\n        for name, param in default_params.items():\n            print(name, self.trainer.params[name], param)\n            assert self.trainer.params[name] == param\n\n        # Test optional parameters not covered\n        netG_ckpt_file = os.path.join(self.log_dir, \'netG.pth\')\n        netD_ckpt_file = os.path.join(self.log_dir, \'netD.pth\')\n        device = None\n\n        extra_trainer = Trainer(netD=self.netD,\n                                netG=self.netG,\n                                optD=self.optD,\n                                optG=self.optG,\n                                netG_ckpt_file=netG_ckpt_file,\n                                netD_ckpt_file=netD_ckpt_file,\n                                log_dir=os.path.join(self.log_dir, \'extra\'),\n                                dataloader=self.dataloader,\n                                num_steps=2,\n                                device=device,\n                                save_steps=float(\'inf\'),\n                                log_steps=float(\'inf\'),\n                                vis_steps=float(\'inf\'),\n                                lr_decay=\'linear\',\n                                save_when_end=False)\n\n        assert extra_trainer.netG_ckpt_file == netG_ckpt_file\n        assert extra_trainer.netG_ckpt_file == netG_ckpt_file\n        assert extra_trainer.device == torch.device(\n            \'cuda:0\' if torch.cuda.is_available() else ""cpu"")\n\n    def test_get_latest_checkpoint(self):\n        ckpt_files = [\n            \'netG_1000_steps.pth\', \'netG_10000_steps.pth\', \'netG_*.pth\',\n            \'asdasd.pth\'\n        ]\n\n        test_dir = os.path.join(self.log_dir, \'test_checkpoint\')\n\n        if not os.path.exists(test_dir):\n            os.makedirs(test_dir)\n\n        for file in ckpt_files:\n            with open(os.path.join(test_dir, file), \'w\') as f:\n                pass\n\n        chosen_file = self.trainer._get_latest_checkpoint(test_dir)\n\n        assert os.path.basename(chosen_file) == ""netG_10000_steps.pth""\n\n        # Case where no checkpoint files:\n        empty_dir = os.path.join(self.log_dir, \'empty\')\n        if not os.path.exists(empty_dir):\n            os.makedirs(empty_dir)\n\n        assert self.trainer._get_latest_checkpoint(empty_dir) is None\n\n    def test_fetch_data(self):\n        iter_dataloader = iter(self.dataloader)\n\n        batches = []\n        for i in range(2):\n            iter_dataloader, real_batch = self.trainer._fetch_data(\n                iter_dataloader=iter_dataloader)\n            batches.append(real_batch)\n\n        # Test different images and labels at each fetch.\n        for i in range(1, len(batches)):\n            images_1, labels_1 = batches[i - 1]\n            images_2, labels_2 = batches[i]\n\n            assert torch.sum((images_1 - images_2)**2) > 1\n\n    def test_train(self):\n        def get_params_sum(net):\n            total = 0\n            for param in net.parameters():\n                total = total + torch.sum(param)\n\n            return total\n\n        def get_lr(optimizer):\n            return optimizer.param_groups[0][\'lr\']\n\n        # Test parameters updated\n        netD_params_before = get_params_sum(self.trainer.netD)\n        netG_params_before = get_params_sum(self.trainer.netG)\n\n        self.trainer.train()\n\n        netD_params_after = get_params_sum(self.trainer.netD)\n        netG_params_after = get_params_sum(self.trainer.netG)\n\n        assert abs(netD_params_before.item() - netD_params_after.item()) > 0\n        assert abs(netG_params_before.item() - netG_params_after.item()) > 0\n\n        # Test LR\n        assert get_lr(self.trainer.optD) == get_lr(self.trainer.scheduler.optD)\n        assert get_lr(self.trainer.optG) == get_lr(self.trainer.scheduler.optG)\n\n        # Test saving functions\n        netG_files = os.listdir(self.trainer.netG_ckpt_dir)\n        netD_files = os.listdir(self.trainer.netD_ckpt_dir)\n        assert set(netD_files) == {\'netD_2_steps.pth\', \'netD_1_steps.pth\'}\n        assert set(netG_files) == {\'netG_2_steps.pth\', \'netG_1_steps.pth\'}\n\n        # Test tensorboard writing summaries\n        tb_dir = os.path.join(self.log_dir, \'data\')\n        for root, dirs, files in os.walk(tb_dir):\n            for file in files:\n                assert file.startswith(\'events\')\n\n        # Test visualisations\n        img_dir = os.path.join(self.log_dir, \'images\')\n        img_files = os.listdir(img_dir)\n        check = set([\n            \'fake_samples_step_1.png\', \'fixed_fake_samples_step_2.png\',\n            \'fake_samples_step_2.png\', \'fixed_fake_samples_step_1.png\'\n        ])\n        assert set(img_files) == check\n\n    def test_log_params(self):\n        new_params = {\n            \'n_dis\': 1,\n            \'lr_decay\': \'None\',\n            \'print_steps\': 1,\n            \'vis_steps\': 500,\n            \'flush_secs\': 30,\n            \'log_steps\': 50,\n            \'save_steps\': 5000,\n            \'save_when_end\': True,\n        }\n\n        with pytest.raises(ValueError):\n            self.trainer._log_params(new_params)\n\n        params_file = os.path.join(self.log_dir, \'params.json\')\n        assert os.path.exists(params_file)\n\n        params_check = common.load_from_json(params_file)\n        assert self.trainer.params == params_check\n\n    def test_restore_models_and_step(self):\n        # Test mismatched steps\n        self.netG.save_checkpoint(directory=self.trainer.netG_ckpt_dir,\n                                  global_step=99,\n                                  optimizer=self.optG)\n        self.netD.save_checkpoint(directory=self.trainer.netD_ckpt_dir,\n                                  global_step=999,\n                                  optimizer=self.optD)\n\n        # Cache\n        tmp_netG_ckpt_file = self.trainer.netG_ckpt_file\n        tmp_netD_ckpt_file = self.trainer.netD_ckpt_file\n        self.trainer.netG_ckpt_file = os.path.join(self.trainer.netG_ckpt_dir,\n                                                   \'netG_99_steps.pth\')\n        self.trainer.netD_ckpt_file = os.path.join(self.trainer.netD_ckpt_dir,\n                                                   \'netD_999_steps.pth\')\n\n        with pytest.raises(ValueError):\n            self.trainer._restore_models_and_step()\n\n        # Remove and restore\n        os.remove(self.trainer.netG_ckpt_file)\n        os.remove(self.trainer.netD_ckpt_file)\n        self.trainer.netG_ckpt_file = tmp_netG_ckpt_file\n        self.trainer.netD_ckpt_file = tmp_netD_ckpt_file\n\n    def teardown(self):\n        shutil.rmtree(self.log_dir)\n\n        del self.dataset,\n        del self.dataloader,\n        del self.netG,\n        del self.netD,\n        del self.optG,\n        del self.optD,\n        del self.trainer\n\n\nif __name__ == ""__main__"":\n    test = TestTrainer()\n    test.setup()\n    test.test_attributes()\n    test.test_get_latest_checkpoint()\n    test.test_restore_models_and_step()\n    test.test_fetch_data()\n    test.test_train()\n    test.test_log_params()\n    test.teardown()\n'"
tests/utils/test_common.py,1,"b'import os\nimport shutil\n\nimport numpy as np\nimport torch\nfrom PIL import Image\n\nfrom torch_mimicry.utils import common\n\n\nclass TestCommon:\n    def setup(self):\n        # Build directories\n        self.log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                    ""test_log"")\n        if not os.path.exists(self.log_dir):\n            os.makedirs(self.log_dir)\n\n    def test_json_write_and_load(self):\n        dict_to_write = dict(a=1, b=2, c=3)\n        output_file = os.path.join(self.log_dir, \'output.json\')\n        common.write_to_json(dict_to_write, output_file)\n        check = common.load_from_json(output_file)\n\n        assert dict_to_write == check\n\n    def test_load_and_save_image(self):\n        image, label = common.load_images(n=1)\n\n        image = torch.squeeze(image, dim=0)\n        output_file = os.path.join(self.log_dir, \'test_img.png\')\n        common.save_tensor_image(image, output_file=output_file)\n\n        check = np.array(Image.open(output_file))\n\n        assert check.shape == (32, 32, 3)\n        assert label.shape == (1, )\n\n    def teardown(self):\n        shutil.rmtree(self.log_dir)\n\n\nif __name__ == ""__main__"":\n    test = TestCommon()\n    test.setup()\n    test.test_json_write_and_load()\n    test.test_load_and_save_image()\n    test.teardown()\n'"
torch_mimicry/datasets/__init__.py,0,b'from . import imagenet\nfrom .data_utils import *\nfrom .image_loader import *\n'
torch_mimicry/datasets/data_utils.py,2,"b'""""""\nScript for loading datasets.\n""""""\nimport os\n\nimport torchvision\nfrom torchvision import transforms\n\nfrom torch_mimicry.datasets.imagenet import imagenet\n\n\ndef load_dataset(root, name, **kwargs):\n    """"""\n    Loads different datasets specifically for GAN training. \n    By default, all images are normalized to values in the range [-1, 1].\n\n    Args:\n        root (str): Path to where datasets are stored.\n        name (str): Name of dataset to load.\n\n    Returns:\n        Dataset: Torch Dataset object for a specific dataset.\n    """"""\n    if name == ""cifar10"":\n        return load_cifar10_dataset(root, **kwargs)\n\n    elif name == ""cifar100"":\n        return load_cifar100_dataset(root, **kwargs)\n\n    elif name == ""imagenet_32"":\n        return load_imagenet_dataset(root, size=32, **kwargs)\n\n    elif name == ""imagenet_128"":\n        return load_imagenet_dataset(root, size=128, **kwargs)\n\n    elif name == ""stl10_48"":\n        return load_stl10_dataset(root, size=48, **kwargs)\n\n    elif name == ""celeba_64"":\n        return load_celeba_dataset(root, size=64, **kwargs)\n\n    elif name == ""celeba_128"":\n        return load_celeba_dataset(root, size=128, **kwargs)\n\n    elif name == ""lsun_bedroom_128"":\n        return load_lsun_bedroom_dataset(root, size=128, **kwargs)\n\n    elif name == ""fake_data"":\n        return load_fake_dataset(root, **kwargs)\n\n    else:\n        raise ValueError(""Invalid dataset name {} selected."".format(name))\n\n\ndef load_fake_dataset(root,\n                      transform_data=True,\n                      convert_tensor=True,\n                      **kwargs):\n    """"""\n    Loads fake dataset for testing.\n\n    Args:\n        root (str): Path to where datasets are stored.\n        transform_data (bool): If True, preprocesses data.\n        convert_tensor (bool): If True, converts image to tensor and preprocess \n            to range [-1, 1].\n\n    Returns:\n        Dataset: Torch Dataset object.\n    """"""\n    dataset_dir = os.path.join(root, \'fake_data\')\n    if not os.path.exists(dataset_dir):\n        os.makedirs(dataset_dir)\n\n    if transform_data:\n        transforms_list = []\n        if convert_tensor:\n            transforms_list += [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, ), (0.5, ))\n            ]\n\n        transform = transforms.Compose(transforms_list)\n\n    else:\n        transform = None\n\n    dataset = torchvision.datasets.FakeData(transform=transform, **kwargs)\n\n    return dataset\n\n\ndef load_lsun_bedroom_dataset(root,\n                              size=128,\n                              transform_data=True,\n                              convert_tensor=True,\n                              **kwargs):\n    """"""\n    Loads LSUN-Bedroom dataset.\n\n    Args:\n        root (str): Path to where datasets are stored.\n        size (int): Size to resize images to.\n        transform_data (bool): If True, preprocesses data.\n        convert_tensor (bool): If True, converts image to tensor and preprocess \n            to range [-1, 1].\n\n    Returns:\n        Dataset: Torch Dataset object.   \n    """"""\n    dataset_dir = os.path.join(root, \'lsun\')\n    if not os.path.exists(dataset_dir):\n        raise ValueError(\n            ""Missing directory {}. Download the dataset to this directory."".\n            format(dataset_dir))\n\n    if transform_data:\n        transforms_list = [transforms.CenterCrop(256), transforms.Resize(size)]\n        if convert_tensor:\n            transforms_list += [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, ), (0.5, ))\n            ]\n\n        transform = transforms.Compose(transforms_list)\n\n    else:\n        transform = None\n\n    dataset = torchvision.datasets.LSUN(root=dataset_dir,\n                                        classes=[\'bedroom_train\'],\n                                        transform=transform,\n                                        **kwargs)\n\n    return dataset\n\n\ndef load_celeba_dataset(root,\n                        transform_data=True,\n                        convert_tensor=True,\n                        download=True,\n                        split=\'all\',\n                        size=64,\n                        **kwargs):\n    """"""\n    Loads the CelebA dataset.\n\n    Args:\n        root (str): Path to where datasets are stored.\n        size (int): Size to resize images to.\n        transform_data (bool): If True, preprocesses data.\n        split (str): The split of data to use.\n        download (bool): If True, downloads the dataset.\n        convert_tensor (bool): If True, converts image to tensor and preprocess \n            to range [-1, 1].\n\n    Returns:\n        Dataset: Torch Dataset object.   \n    """"""\n    dataset_dir = os.path.join(root, \'celeba\')\n    if not os.path.exists(dataset_dir):\n        os.makedirs(dataset_dir)\n\n    if transform_data:\n        # Build default transforms for scaling outputs to -1 to 1.\n        transforms_list = [\n            transforms.CenterCrop(\n                178),  # Because each image is size (178, 218) spatially.\n            transforms.Resize(size)\n        ]\n        if convert_tensor:\n            transforms_list += [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, ), (0.5, ))\n            ]\n\n        transform = transforms.Compose(transforms_list)\n\n    else:\n        transform = None\n\n    if download:\n        print(""INFO: download is True. Downloading CelebA images..."")\n\n    dataset = torchvision.datasets.CelebA(root=dataset_dir,\n                                          transform=transform,\n                                          download=download,\n                                          split=split,\n                                          **kwargs)\n\n    return dataset\n\n\ndef load_stl10_dataset(root,\n                       size=48,\n                       split=\'unlabeled\',\n                       download=True,\n                       transform_data=True,\n                       convert_tensor=True,\n                       **kwargs):\n    """"""\n    Loads the STL10 dataset.\n\n    Args:\n        root (str): Path to where datasets are stored.\n        size (int): Size to resize images to.\n        transform_data (bool): If True, preprocesses data.\n        split (str): The split of data to use.\n        download (bool): If True, downloads the dataset.\n        convert_tensor (bool): If True, converts image to tensor and preprocess \n            to range [-1, 1].\n\n    Returns:\n        Dataset: Torch Dataset object.   \n    """"""\n    dataset_dir = os.path.join(root, \'stl10\')\n    if not os.path.exists(dataset_dir):\n        os.makedirs(dataset_dir)\n\n    if transform_data:\n        transforms_list = [transforms.Resize(size)]\n        if convert_tensor:\n            transforms_list += [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, ), (0.5, ))\n            ]\n\n        transform = transforms.Compose(transforms_list)\n\n    else:\n        transform = None\n\n    dataset = torchvision.datasets.STL10(root=dataset_dir,\n                                         split=split,\n                                         transform=transform,\n                                         download=download,\n                                         **kwargs)\n\n    return dataset\n\n\ndef load_imagenet_dataset(root,\n                          size=32,\n                          split=\'train\',\n                          download=True,\n                          transform_data=True,\n                          convert_tensor=True,\n                          **kwargs):\n    """"""\n    Loads the ImageNet dataset.\n\n    Args:\n        root (str): Path to where datasets are stored.\n        size (int): Size to resize images to.\n        transform_data (bool): If True, preprocesses data.\n        split (str): The split of data to use.\n        download (bool): If True, downloads the dataset.\n        convert_tensor (bool): If True, converts image to tensor and preprocess \n            to range [-1, 1].\n\n    Returns:\n        Dataset: Torch Dataset object.   \n    """"""\n    dataset_dir = os.path.join(root, \'imagenet\')\n    if not os.path.exists(dataset_dir):\n        os.makedirs(dataset_dir)\n\n    if transform_data:\n        transforms_list = [transforms.CenterCrop(224), transforms.Resize(size)]\n        if convert_tensor:\n            transforms_list += [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, ), (0.5, ))\n            ]\n\n        transform = transforms.Compose(transforms_list)\n\n    else:\n        transform = None\n\n    dataset = imagenet.ImageNet(root=dataset_dir,\n                                split=split,\n                                transform=transform,\n                                download=download,\n                                **kwargs)\n\n    return dataset\n\n\ndef load_cifar100_dataset(root,\n                          split=\'train\',\n                          download=True,\n                          transform_data=True,\n                          convert_tensor=True,\n                          **kwargs):\n    """"""\n    Loads the CIFAR-100 dataset.\n\n    Args:\n        root (str): Path to where datasets are stored.\n        transform_data (bool): If True, preprocesses data.\n        split (str): The split of data to use.\n        download (bool): If True, downloads the dataset.\n        convert_tensor (bool): If True, converts image to tensor and preprocess \n            to range [-1, 1].\n\n    Returns:\n        Dataset: Torch Dataset object.   \n    """"""\n    dataset_dir = os.path.join(root, \'cifar100\')\n    if not os.path.exists(dataset_dir):\n        os.makedirs(dataset_dir)\n\n    if transform_data:\n        transforms_list = []\n        if convert_tensor:\n            transforms_list += [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, ), (0.5, ))\n            ]\n\n        transform = transforms.Compose(transforms_list)\n    else:\n        transform = None\n\n    # Build datasets\n    if split == ""all"":\n        train_dataset = torchvision.datasets.CIFAR100(root=dataset_dir,\n                                                      train=True,\n                                                      transform=transform,\n                                                      download=download,\n                                                      **kwargs)\n\n        test_dataset = torchvision.datasets.CIFAR100(root=dataset_dir,\n                                                     train=False,\n                                                     transform=transform,\n                                                     download=download,\n                                                     **kwargs)\n\n        # Merge the datasets\n        dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset])\n\n    elif split == ""train"":\n        dataset = torchvision.datasets.CIFAR100(root=dataset_dir,\n                                                train=True,\n                                                transform=transform,\n                                                download=download,\n                                                **kwargs)\n\n    elif split == ""test"":\n        dataset = torchvision.datasets.CIFAR100(root=dataset_dir,\n                                                train=False,\n                                                transform=transform,\n                                                download=download,\n                                                **kwargs)\n\n    else:\n        raise ValueError(""split argument must one of [\'train\', \'val\', \'all\']"")\n\n    return dataset\n\n\ndef load_cifar10_dataset(root,\n                         split=\'train\',\n                         download=True,\n                         transform_data=True,\n                         **kwargs):\n    """"""\n    Loads the CIFAR-10 dataset.\n    \n    Args:\n        root (str): Path to where datasets are stored.\n        transform_data (bool): If True, preprocesses data.\n        split (str): The split of data to use.\n        download (bool): If True, downloads the dataset.\n        convert_tensor (bool): If True, converts image to tensor and preprocess \n            to range [-1, 1].\n\n    Returns:\n        Dataset: Torch Dataset object.   \n    """"""\n    dataset_dir = os.path.join(root, \'cifar10\')\n    if not os.path.exists(dataset_dir):\n        os.makedirs(dataset_dir)\n\n    if transform_data:\n        transform = transforms.Compose(\n            [transforms.ToTensor(),\n             transforms.Normalize((0.5, ), (0.5, ))])\n    else:\n        transform = None\n\n    # Build datasets\n    if split == ""all"":\n        train_dataset = torchvision.datasets.CIFAR10(root=dataset_dir,\n                                                     train=True,\n                                                     transform=transform,\n                                                     download=download,\n                                                     **kwargs)\n\n        test_dataset = torchvision.datasets.CIFAR10(root=dataset_dir,\n                                                    train=False,\n                                                    transform=transform,\n                                                    download=download,\n                                                    **kwargs)\n\n        # Merge the datasets\n        dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset])\n\n    elif split == ""train"":\n        dataset = torchvision.datasets.CIFAR10(root=dataset_dir,\n                                               train=True,\n                                               transform=transform,\n                                               download=download,\n                                               **kwargs)\n\n    elif split == ""test"":\n        dataset = torchvision.datasets.CIFAR10(root=dataset_dir,\n                                               train=False,\n                                               transform=transform,\n                                               download=download,\n                                               **kwargs)\n\n    else:\n        raise ValueError(""split argument must one of [\'train\', \'val\', \'all\']"")\n\n    return dataset\n'"
torch_mimicry/datasets/image_loader.py,0,"b'""""""\nLoads randomly sampled images from datasets for computing metrics.\n""""""\nimport os\n\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nfrom torch_mimicry.datasets import data_utils\n\n\ndef get_random_images(dataset, num_samples):\n    """"""\n    Randomly sample without replacement num_samples images.\n\n    Args:\n        dataset (Dataset): Torch Dataset object for indexing elements.\n        num_samples (int): The number of images to randomly sample.\n\n    Returns:\n        Tensor: Batch of num_samples images in np array form.\n    """"""\n    choices = np.random.choice(range(len(dataset)),\n                               size=num_samples,\n                               replace=False)\n\n    images = []\n    for choice in choices:\n        img = np.array(dataset[choice][0])\n        img = np.expand_dims(img, axis=0)\n        images.append(img)\n    images = np.concatenate(images, axis=0)\n\n    return images\n\n\ndef get_imagenet_images(num_samples, root=\'./datasets\', size=32):\n    """"""\n    Directly reads the imagenet folder for obtaining random images sampled in equal proportion\n    for each class.\n\n    Args:\n        num_samples (int): The number of images to randomly sample.\n        root (str): The root directory where all datasets are stored.\n        size (int): Size of image to resize to.\n\n    Returns:\n        Tensor: Batch of num_samples images in np array form.\n    """"""\n    if num_samples < 1000:\n        raise ValueError(\n            ""num_samples {} must be at least 1000 to ensure images are sampled from each class.""\n            .format(num_samples))\n\n    data_dir = os.path.join(root, \'imagenet\', \'train\')\n    class_dirs = os.listdir(data_dir)\n    images = []\n\n    # Randomly choose equal proportion from each class.\n    for class_dir in class_dirs:\n        filenames = [\n            os.path.join(data_dir, class_dir, name)\n            for name in os.listdir(os.path.join(data_dir, class_dir))\n        ]\n\n        choices = np.random.choice(list(range(len(filenames))),\n                                   size=(num_samples // 1000 or num_samples),\n                                   replace=False)\n        for i in range(len(choices)):\n            img = Image.open(filenames[int(choices[i])])\n            img = transforms.CenterCrop(224)(img)\n            img = transforms.Resize(size)(img)\n            img = np.asarray(img)\n\n            # Convert grayscale to rgb\n            if len(img.shape) == 2:\n                tmp = np.expand_dims(img, axis=2)\n                img = np.concatenate([tmp, tmp, tmp], axis=2)\n\n            # account for rgba images\n            elif img.shape[2] == 4:\n                img = img[:, :, :3]\n\n            img = np.expand_dims(img, axis=0)\n            images.append(img)\n\n    images = np.concatenate(images, axis=0)\n    return images\n\n\ndef get_fake_data_images(num_samples, root=\'./datasets\', size=32, **kwargs):\n    """"""\n    Loads fake images, especially for testing.\n\n    Args:\n        num_samples (int): The number of images to randomly sample.\n        root (str): The root directory where all datasets are stored.\n        size (int): Size of image to resize to.\n\n    Returns:\n        Tensor: Batch of num_samples images in np array form.\n    """"""\n    dataset = data_utils.load_fake_dataset(\n        root=root,\n        image_size=(3, size, size),\n        transform_data=True,\n        convert_tensor=False,  # Prevents normalization.\n        **kwargs)\n\n    images = get_random_images(dataset, num_samples)\n\n    return images\n\n\ndef get_lsun_bedroom_images(num_samples,\n                            root=\'./datasets\',\n                            size=128,\n                            **kwargs):\n    """"""\n    Loads randomly sampled LSUN-Bedroom training images.\n\n    Args:\n        num_samples (int): The number of images to randomly sample.\n        root (str): The root directory where all datasets are stored.\n        size (int): Size of image to resize to.\n\n    Returns:\n        Tensor: Batch of num_samples images in np array form.\n    """"""\n    dataset = data_utils.load_lsun_bedroom_dataset(\n        root=root,\n        size=size,\n        transform_data=True,\n        convert_tensor=False,  # Prevents normalization.\n        **kwargs)\n\n    images = get_random_images(dataset, num_samples)\n\n    return images\n\n\ndef get_celeba_images(num_samples, root=\'./datasets\', size=128, **kwargs):\n    """"""\n    Loads randomly sampled CelebA images.\n\n    Args:\n        num_samples (int): The number of images to randomly sample.\n        root (str): The root directory where all datasets are stored.\n        size (int): Size of image to resize to.\n\n    Returns:\n        Tensor: Batch of num_samples images in np array form.\n    """"""\n    dataset = data_utils.load_celeba_dataset(\n        root=root,\n        size=size,\n        transform_data=True,\n        convert_tensor=False,  # Prevents normalization.\n        **kwargs)\n\n    images = get_random_images(dataset, num_samples)\n\n    return images\n\n\ndef get_stl10_images(num_samples, root=\'./datasets\', size=48, **kwargs):\n    """"""\n    Loads randomly sampled STL-10 images.\n\n    Args:\n        num_samples (int): The number of images to randomly sample.\n        root (str): The root directory where all datasets are stored.\n        size (int): Size of image to resize to.\n\n    Returns:\n        Tensor: Batch of num_samples images in np array form.\n    """"""\n    dataset = data_utils.load_stl10_dataset(\n        root=root,\n        size=size,\n        transform_data=True,\n        convert_tensor=False,  # Prevents normalization.\n        **kwargs)\n\n    images = get_random_images(dataset, num_samples)\n\n    return images\n\n\ndef get_cifar10_images(num_samples, root=""./datasets"", **kwargs):\n    """"""\n    Loads randomly sampled CIFAR-10 training images.\n\n    Args:\n        num_samples (int): The number of images to randomly sample.\n        root (str): The root directory where all datasets are stored.\n\n    Returns:\n        Tensor: Batch of num_samples images in np array form.\n    """"""\n    dataset = data_utils.load_cifar10_dataset(root=root,\n                                              transform_data=False,\n                                              **kwargs)\n\n    images = get_random_images(dataset, num_samples)\n\n    return images\n\n\ndef get_cifar100_images(num_samples, root=""./datasets"", **kwargs):\n    """"""\n    Loads randomly sampled CIFAR-100 training images.\n\n    Args:\n        num_samples (int): The number of images to randomly sample.\n        root (str): The root directory where all datasets are stored.\n\n    Returns:\n        Tensor: Batch of num_samples images in np array form.\n    """"""\n    dataset = data_utils.load_cifar100_dataset(root=root,\n                                               split=\'train\',\n                                               download=True,\n                                               transform_data=False,\n                                               **kwargs)\n\n    images = get_random_images(dataset, num_samples)\n\n    return images\n\n\ndef get_dataset_images(dataset_name, num_samples=50000, **kwargs):\n    """"""\n    Randomly sample num_samples images based on input dataset name.\n\n    Args:\n        dataset_name (str): Dataset name to load images from.\n        num_samples (int): The number of images to randomly sample.\n\n    Returns:\n        Tensor: Batch of num_samples images from the specific dataset in np array form.\n    """"""\n    if dataset_name == ""imagenet_32"":\n        images = get_imagenet_images(num_samples, size=32, **kwargs)\n\n    elif dataset_name == ""imagenet_128"":\n        images = get_imagenet_images(num_samples, size=128, **kwargs)\n\n    elif dataset_name == ""celeba_64"":\n        images = get_celeba_images(num_samples, size=64, **kwargs)\n\n    elif dataset_name == ""celeba_128"":\n        images = get_celeba_images(num_samples, size=128, **kwargs)\n\n    elif dataset_name == ""stl10_48"":\n        images = get_stl10_images(num_samples, **kwargs)\n\n    elif dataset_name == ""cifar10"":\n        images = get_cifar10_images(num_samples, **kwargs)\n\n    elif dataset_name == ""cifar10_test"":\n        images = get_cifar10_images(num_samples, split=\'test\', **kwargs)\n\n    elif dataset_name == ""cifar100"":\n        images = get_cifar100_images(num_samples, **kwargs)\n\n    elif dataset_name == ""lsun_bedroom_128"":\n        images = get_lsun_bedroom_images(num_samples, size=128, **kwargs)\n\n    elif dataset_name == ""fake_data"":\n        images = get_fake_data_images(num_samples, size=32, **kwargs)\n\n    else:\n        raise ValueError(""Invalid dataset name {}."".format(dataset_name))\n\n    # Check shape and permute if needed\n    if images.shape[1] == 3:\n        images = images.transpose((0, 2, 3, 1))\n\n    # Ensure the values lie within the correct range, otherwise there might be some\n    # preprocessing error from the library causing ill-valued scores.\n    if np.min(images) < 0 or np.max(images) > 255:\n        raise ValueError(\n            \'Image pixel values must lie between 0 to 255 inclusive.\')\n\n    return images\n'"
torch_mimicry/metrics/__init__.py,0,"b'from . import fid, kid, inception_score, inception_model\nfrom .compute_fid import *\nfrom .compute_is import *\nfrom .compute_kid import *\nfrom .compute_metrics import *\n'"
torch_mimicry/metrics/compute_fid.py,5,"b'""""""\nPyTorch interface for computing FID.\n""""""\nimport os\nimport random\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nimport torch\n\nfrom torch_mimicry.datasets.image_loader import get_dataset_images\nfrom torch_mimicry.metrics.fid import fid_utils\nfrom torch_mimicry.metrics.inception_model import inception_utils\n\n\ndef compute_real_dist_stats(num_samples,\n                            sess,\n                            dataset_name,\n                            batch_size,\n                            stats_file=None,\n                            seed=0,\n                            verbose=True,\n                            log_dir=\'./log\'):\n    """"""\n    Reads the image data and compute the FID mean and cov statistics\n    for real images.\n\n    Args:\n        num_samples (int): Number of real images to compute statistics.\n        sess (Session): TensorFlow session to use.\n        dataset_name (str): The name of the dataset to load.\n        batch_size (int): The batch size to feedforward for inference.\n        stats_file (str): The statistics file to load from if there is already one.\n        verbose (bool): If True, prints progress of computation.\n        log_dir (str): Directory where feature statistics can be stored.\n\n    Returns:\n        ndarray: Mean features stored as np array.\n        ndarray: Covariance of features stored as np array.\n    """"""\n    # Create custom stats file name\n    if stats_file is None:\n        stats_dir = os.path.join(log_dir, \'metrics\', \'fid\', \'statistics\')\n        if not os.path.exists(stats_dir):\n            os.makedirs(stats_dir)\n\n        stats_file = os.path.join(\n            stats_dir,\n            ""fid_stats_{}_{}k_run_{}.npz"".format(dataset_name,\n                                                 num_samples // 1000, seed))\n\n    if stats_file and os.path.exists(stats_file):\n        print(""INFO: Loading existing statistics for real images..."")\n        f = np.load(stats_file)\n        m_real, s_real = f[\'mu\'][:], f[\'sigma\'][:]\n        f.close()\n\n    else:\n        # Obtain the numpy format data\n        print(""INFO: Obtaining images..."")\n        images = get_dataset_images(dataset_name, num_samples=num_samples)\n\n        # Compute the mean and cov\n        print(""INFO: Computing statistics for real images..."")\n        m_real, s_real = fid_utils.calculate_activation_statistics(\n            images=images, sess=sess, batch_size=batch_size, verbose=verbose)\n\n        if not os.path.exists(stats_file):\n            print(""INFO: Saving statistics for real images..."")\n            np.savez(stats_file, mu=m_real, sigma=s_real)\n\n    return m_real, s_real\n\n\ndef _normalize_images(images):\n    """"""\n    Given a tensor of images, uses the torchvision\n    normalization method to convert floating point data to integers. See reference\n    at: https://pytorch.org/docs/stable/_modules/torchvision/utils.html#save_image\n\n    The function uses the normalization from make_grid and save_image functions.\n\n    Args:\n        images (Tensor): Batch of images of shape (N, 3, H, W).\n\n    Returns:\n        ndarray: Batch of normalized images of shape (N, H, W, 3).\n    """"""\n    # Shift the image from [-1, 1] range to [0, 1] range.\n    min_val = float(images.min())\n    max_val = float(images.max())\n    images.clamp_(min=min_val, max=max_val)\n    images.add_(-min_val).div_(max_val - min_val + 1e-5)\n\n    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n    images = images.mul_(255).add_(0.5).clamp_(0, 255).permute(0, 2, 3, 1).to(\n        \'cpu\', torch.uint8).numpy()\n\n    return images\n\n\ndef compute_gen_dist_stats(netG,\n                           num_samples,\n                           sess,\n                           device,\n                           seed,\n                           batch_size,\n                           print_every=20,\n                           verbose=True):\n    """"""\n    Directly produces the images and convert them into numpy format without\n    saving the images on disk.\n\n    Args:\n        netG (Module): Torch Module object representing the generator model.\n        num_samples (int): The number of fake images for computing statistics.\n        sess (Session): TensorFlow session to use.\n        device (str): Device identifier to use for computation.\n        seed (int): The random seed to use.\n        batch_size (int): The number of samples per batch for inference.\n        print_every (int): Interval for printing log.\n        verbose (bool): If True, prints progress.\n\n    Returns:\n        ndarray: Mean features stored as np array.\n        ndarray: Covariance of features stored as np array.\n    """"""\n    with torch.no_grad():\n        # Set model to evaluation mode\n        netG.eval()\n\n        # Inference variables\n        batch_size = min(num_samples, batch_size)\n\n        # Collect all samples()\n        images = []\n        start_time = time.time()\n        for idx in range(num_samples // batch_size):\n            # Collect fake image\n            fake_images = netG.generate_images(num_images=batch_size,\n                                               device=device).detach().cpu()\n            images.append(fake_images)\n\n            # Print some statistics\n            if (idx + 1) % print_every == 0:\n                end_time = time.time()\n                print(\n                    ""INFO: Generated image {}/{} [Random Seed {}] ({:.4f} sec/idx)""\n                    .format(\n                        (idx + 1) * batch_size, num_samples, seed,\n                        (end_time - start_time) / (print_every * batch_size)))\n                start_time = end_time\n\n        # Produce images in the required (N, H, W, 3) format for FID computation\n        images = torch.cat(images, 0)  # Gives (N, 3, H, W)\n        images = _normalize_images(images)  # Gives (N, H, W, 3)\n\n    # Compute the FID\n    print(""INFO: Computing statistics for fake images..."")\n    m_fake, s_fake = fid_utils.calculate_activation_statistics(\n        images=images, sess=sess, batch_size=batch_size, verbose=verbose)\n\n    return m_fake, s_fake\n\n\ndef fid_score(num_real_samples,\n              num_fake_samples,\n              netG,\n              device,\n              seed,\n              dataset_name,\n              batch_size=50,\n              verbose=True,\n              stats_file=None,\n              log_dir=\'./log\'):\n    """"""\n    Computes FID stats using functions that store images in memory for speed and fidelity.\n    Fidelity since by storing images in memory, we don\'t subject the scores to different read/write\n    implementations of imaging libraries.\n\n    Args:\n        num_real_samples (int): The number of real images to use for FID.\n        num_fake_samples (int): The number of fake images to use for FID.\n        netG (Module): Torch Module object representing the generator model.\n        device (str): Device identifier to use for computation.\n        seed (int): The random seed to use.\n        dataset_name (str): The name of the dataset to load.\n        batch_size (int): The batch size to feedforward for inference.\n        verbose (bool): If True, prints progress.\n        stats_file (str): The statistics file to load from if there is already one.\n        log_dir (str): Directory where feature statistics can be stored.\n\n    Returns:\n        float: Scalar FID score.\n    """"""\n    start_time = time.time()\n\n    # Make sure the random seeds are fixed\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Setup directories\n    inception_path = os.path.join(log_dir, \'metrics\', \'inception_model\')\n\n    # Setup the inception graph\n    inception_utils.create_inception_graph(inception_path)\n\n    # Start producing statistics for real and fake images\n    if device and device.index is not None:\n        # Avoid unbounded memory usage\n        gpu_options = tf.GPUOptions(allow_growth=True,\n                                    per_process_gpu_memory_fraction=0.15,\n                                    visible_device_list=str(device.index))\n        config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n\n    else:\n        config = tf.compat.v1.ConfigProto(device_count={\'GPU\': 0})\n\n    with tf.compat.v1.Session(config=config) as sess:\n        sess.run(tf.compat.v1.global_variables_initializer())\n\n        m_real, s_real = compute_real_dist_stats(num_samples=num_real_samples,\n                                                 sess=sess,\n                                                 dataset_name=dataset_name,\n                                                 batch_size=batch_size,\n                                                 verbose=verbose,\n                                                 stats_file=stats_file,\n                                                 log_dir=log_dir,\n                                                 seed=seed)\n\n        m_fake, s_fake = compute_gen_dist_stats(netG=netG,\n                                                num_samples=num_fake_samples,\n                                                sess=sess,\n                                                device=device,\n                                                seed=seed,\n                                                batch_size=batch_size,\n                                                verbose=verbose)\n\n        FID_score = fid_utils.calculate_frechet_distance(mu1=m_real,\n                                                         sigma1=s_real,\n                                                         mu2=m_fake,\n                                                         sigma2=s_fake)\n\n        print(""INFO: FID Score: {} [Time Taken: {:.4f} secs]"".format(\n            FID_score,\n            time.time() - start_time))\n\n        return float(FID_score)\n'"
torch_mimicry/metrics/compute_is.py,5,"b'""""""\nPyTorch interface for computing Inception Score.\n""""""\nimport os\nimport random\nimport time\n\nimport numpy as np\nimport torch\n\nfrom torch_mimicry.metrics.inception_model import inception_utils\nfrom torch_mimicry.metrics.inception_score import inception_score_utils as tf_inception_score\n\n\ndef _normalize_images(images):\n    """"""\n    Given a tensor of images, uses the torchvision\n    normalization method to convert floating point data to integers. See reference\n    at: https://pytorch.org/docs/stable/_modules/torchvision/utils.html#save_image\n\n    The function uses the normalization from make_grid and save_image functions.\n\n    Args:\n        images (Tensor): Batch of images of shape (N, 3, H, W).\n\n    Returns:\n        ndarray: Batch of normalized images of shape (N, H, W, 3).\n    """"""\n    # Shift the image from [-1, 1] range to [0, 1] range.\n    min_val = float(images.min())\n    max_val = float(images.max())\n    images.clamp_(min=min_val, max=max_val)\n    images.add_(-min_val).div_(max_val - min_val + 1e-5)\n\n    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n    images = images.mul_(255).add_(0.5).clamp_(0, 255).permute(0, 2, 3, 1).to(\n        \'cpu\', torch.uint8).numpy()\n\n    return images\n\n\ndef inception_score(netG,\n                    device,\n                    num_samples,\n                    batch_size=50,\n                    splits=10,\n                    log_dir=\'./log\',\n                    seed=0,\n                    print_every=20):\n    """"""\n    Computes the inception score of generated images.\n\n    Args:\n        netG (Module): The generator model to use for generating images.\n        device (Device): Torch device object to send model and data to.\n        num_samples (int): The number of samples to generate.\n        batch_size (int): Batch size per feedforward step for inception model.\n        splits (int): The number of splits to use for computing IS.\n        log_dir (str): Path to store metric computation objects.\n        seed (int): Random seed for generation.\n    Returns:\n        Mean and standard deviation of the inception score computed from using\n        num_samples generated images.\n    """"""\n    # Make sure the random seeds are fixed\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Build inception\n    inception_path = os.path.join(log_dir, \'metrics/inception_model\')\n    inception_utils.create_inception_graph(inception_path)\n\n    # Inference variables\n    batch_size = min(batch_size, num_samples)\n    num_batches = num_samples // batch_size\n\n    # Get images\n    images = []\n    with torch.no_grad():\n        start_time = time.time()\n        for idx in range(num_batches):\n            # noise = torch.randn((batch_size, netG.nz), device=device)\n            # fake_images = netG(noise)\n\n            fake_images = netG.generate_images(num_images=batch_size,\n                                               device=device).detach().cpu()\n\n            fake_images = _normalize_images(fake_images)\n            images.append(fake_images)\n\n            if (idx + 1) % min(print_every, num_batches) == 0:\n                end_time = time.time()\n                print(\n                    ""INFO: Generated image {}/{} [Random Seed {}] ({:.4f} sec/idx)""\n                    .format(\n                        (idx + 1) * batch_size, num_samples, seed,\n                        (end_time - start_time) / (print_every * batch_size)))\n                start_time = end_time\n\n    images = np.concatenate(images, axis=0)\n\n    return tf_inception_score.get_inception_score(images,\n                                                  splits=splits,\n                                                  device=device)\n'"
torch_mimicry/metrics/compute_kid.py,5,"b'""""""\nPyTorch interface for computing KID.\n""""""\nimport os\nimport random\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nimport torch\n\nfrom torch_mimicry.datasets.image_loader import get_dataset_images\nfrom torch_mimicry.metrics.inception_model import inception_utils\nfrom torch_mimicry.metrics.kid import kid_utils\n\n\ndef compute_real_dist_feat(num_samples,\n                           sess,\n                           dataset_name,\n                           batch_size,\n                           seed=0,\n                           verbose=True,\n                           feat_file=None,\n                           log_dir=\'./log\'):\n    """"""\n    Reads the image data and compute the real image features.\n\n    Args:\n        num_samples (int): Number of real images to compute features.\n        sess (Session): TensorFlow session to use.\n        dataset_name (str): The name of the dataset to load.\n        batch_size (int): The batch size to feedforward for inference.\n        feat_file (str): The features file to load from if there is already one.\n        verbose (bool): If True, prints progress of computation.\n        log_dir (str): Directory where features can be stored.\n\n    Returns:\n        ndarray: Inception features of real images.\n    """"""\n    # Create custom feat file name\n    if feat_file is None:\n        feat_dir = os.path.join(log_dir, \'metrics\', \'kid\', \'features\')\n        if not os.path.exists(feat_dir):\n            os.makedirs(feat_dir)\n\n        feat_file = os.path.join(\n            feat_dir,\n            ""kid_feat_{}_{}k_run_{}.npz"".format(dataset_name,\n                                                num_samples // 1000, seed))\n\n    if feat_file and os.path.exists(feat_file):\n        print(""INFO: Loading existing features for real images..."")\n        f = np.load(feat_file)\n        real_feat = f[\'feat\'][:]\n        f.close()\n\n    else:\n        # Obtain the numpy format data\n        print(""INFO: Obtaining images..."")\n        images = get_dataset_images(dataset_name, num_samples=num_samples)\n\n        # Compute the mean and cov\n        print(""INFO: Computing features for real images..."")\n        real_feat = inception_utils.get_activations(images=images,\n                                                    sess=sess,\n                                                    batch_size=batch_size,\n                                                    verbose=verbose)\n\n        print(""INFO: Saving features for real images..."")\n        np.savez(feat_file, feat=real_feat)\n\n    return real_feat\n\n\ndef _normalize_images(images):\n    """"""\n    Given a tensor of images, uses the torchvision\n    normalization method to convert floating point data to integers. See reference\n    at: https://pytorch.org/docs/stable/_modules/torchvision/utils.html#save_image\n\n    The function uses the normalization from make_grid and save_image functions.\n\n    Args:\n        images (Tensor): Batch of images of shape (N, 3, H, W).\n\n    Returns:\n        ndarray: Batch of normalized images of shape (N, H, W, 3).\n    """"""\n    # Shift the image from [-1, 1] range to [0, 1] range.\n    min_val = float(images.min())\n    max_val = float(images.max())\n    images.clamp_(min=min_val, max=max_val)\n    images.add_(-min_val).div_(max_val - min_val + 1e-5)\n\n    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n    images = images.mul_(255).add_(0.5).clamp_(0, 255).permute(0, 2, 3, 1).to(\n        \'cpu\', torch.uint8).numpy()\n\n    return images\n\n\ndef compute_gen_dist_feat(netG,\n                          num_samples,\n                          sess,\n                          device,\n                          seed,\n                          batch_size,\n                          print_every=20,\n                          verbose=True):\n    """"""\n    Directly produces the images and convert them into numpy format without\n    saving the images on disk.\n\n    Args:\n        netG (Module): Torch Module object representing the generator model.\n        num_samples (int): The number of fake images for computing features.\n        sess (Session): TensorFlow session to use.\n        device (str): Device identifier to use for computation.\n        seed (int): The random seed to use.\n        batch_size (int): The number of samples per batch for inference.\n        print_every (int): Interval for printing log.\n        verbose (bool): If True, prints progress.\n\n    Returns:\n        ndarray: Inception features of generated images.\n    """"""\n    batch_size = min(num_samples, batch_size)\n\n    with torch.no_grad():\n        # Set model to evaluation mode\n        netG.eval()\n\n        # Collect num_samples of fake images\n        images = []\n\n        # Collect all samples\n        start_time = time.time()\n        for idx in range(num_samples // batch_size):\n            fake_images = netG.generate_images(num_images=batch_size,\n                                               device=device).detach().cpu()\n\n            # Collect fake image\n            images.append(fake_images)\n\n            # Print some statistics\n            if (idx + 1) % print_every == 0:\n                end_time = time.time()\n                print(\n                    ""INFO: Generated image {}/{} [Random Seed {}] ({:.4f} sec/idx)""\n                    .format(\n                        (idx + 1) * batch_size, num_samples, seed,\n                        (end_time - start_time) / (print_every * batch_size)))\n                start_time = end_time\n\n        # Produce images in the required (N, H, W, 3) format for kid computation\n        images = torch.cat(images, 0)  # Gives (N, 3, H, W)\n        images = _normalize_images(images)  # Gives (N, H, W, 3)\n\n    # Compute the kid\n    print(""INFO: Computing features for fake images..."")\n    fake_feat = inception_utils.get_activations(images=images,\n                                                sess=sess,\n                                                batch_size=batch_size,\n                                                verbose=verbose)\n\n    return fake_feat\n\n\ndef kid_score(num_subsets,\n              subset_size,\n              netG,\n              device,\n              seed,\n              dataset_name,\n              batch_size=50,\n              verbose=True,\n              feat_file=None,\n              log_dir=\'./log\'):\n    """"""\n    Computes KID score.\n\n    Args:\n        num_subsets (int): Number of subsets to compute average MMD.\n        subset_size (int): Size of subset for computing MMD.\n        netG (Module): Torch Module object representing the generator model.\n        device (str): Device identifier to use for computation.\n        seed (int): The random seed to use.\n        dataset_name (str): The name of the dataset to load.\n        batch_size (int): The batch size to feedforward for inference.\n        feat_file (str): The path to specific inception features for real images.\n        log_dir (str): Directory where features can be stored.\n        verbose (bool): If True, prints progress.\n\n    Returns:\n        tuple: Scalar mean and std of KID scores computed.\n    """"""\n    start_time = time.time()\n\n    # Make sure the random seeds are fixed\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Directories\n    inception_path = os.path.join(log_dir, \'metrics\', \'inception_model\')\n\n    # Setup the inception graph\n    inception_utils.create_inception_graph(inception_path)\n\n    # Decide sample size\n    num_samples = int(num_subsets * subset_size)\n\n    # Start producing features for real and fake images\n    if device.index is not None:\n        # Avoid unbounded memory usage\n        gpu_options = tf.GPUOptions(allow_growth=True,\n                                    per_process_gpu_memory_fraction=0.15,\n                                    visible_device_list=str(device.index))\n        config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n\n    else:\n        config = tf.compat.v1.ConfigProto(device_count={\'GPU\': 0})\n\n    with tf.compat.v1.Session(config=config) as sess:\n        sess.run(tf.compat.v1.global_variables_initializer())\n\n        real_feat = compute_real_dist_feat(num_samples=num_samples,\n                                           sess=sess,\n                                           dataset_name=dataset_name,\n                                           batch_size=batch_size,\n                                           verbose=verbose,\n                                           feat_file=feat_file,\n                                           log_dir=log_dir,\n                                           seed=seed)\n\n        fake_feat = compute_gen_dist_feat(netG=netG,\n                                          num_samples=num_samples,\n                                          sess=sess,\n                                          device=device,\n                                          seed=seed,\n                                          batch_size=batch_size,\n                                          verbose=verbose)\n\n        # Compute the KID score\n        scores = kid_utils.polynomial_mmd_averages(real_feat,\n                                                   fake_feat,\n                                                   n_subsets=num_subsets,\n                                                   subset_size=subset_size)\n\n        mmd_score, mmd_std = float(np.mean(scores)), float(np.std(scores))\n\n        print(""INFO: KID: {:.4f} \xc2\xb1 {:.4f} [Time Taken: {:.4f} secs]"".format(\n            mmd_score, mmd_std,\n            time.time() - start_time))\n\n        return mmd_score, mmd_std\n'"
torch_mimicry/metrics/compute_metrics.py,1,"b'""""""\nComputes different GAN metrics for a generator.\n""""""\nimport os\n\nimport numpy as np\nimport torch\n\nfrom torch_mimicry.metrics import compute_fid, compute_is, compute_kid\nfrom torch_mimicry.utils import common\n\n\ndef evaluate(metric,\n             netG,\n             log_dir,\n             evaluate_range=None,\n             evaluate_step=None,\n             num_runs=3,\n             start_seed=0,\n             overwrite=False,\n             write_to_json=True,\n             device=None,\n             **kwargs):\n    """"""\n    Evaluates a generator over several runs.\n\n    Args:\n        metric (str): The name of the metric for evaluation.\n        netG (Module): Torch generator model to evaluate.\n        log_dir (str): The path to the log directory.\n        evaluate_range (tuple): The 3 valued tuple for defining a for loop.\n        evaluate_step (int): The specific checkpoint to load. Used in place of evaluate_range.\n        device (str): Device identifier to use for computation.\n        num_runs (int): The number of runs to compute FID for each checkpoint.\n        start_seed (int): Starting random seed to use.\n        write_to_json (bool): If True, writes to an output json file in log_dir.\n        overwrite (bool): If True, then overwrites previous metric score.\n\n    Returns:\n        None\n    """"""\n    if metric == \'kid\':\n        if \'num_subsets\' not in kwargs or \'subset_size\' not in kwargs:\n            raise ValueError(\n                ""num_subsets and subset_size must be provided for KID computation.""\n            )\n\n    elif metric == \'fid\':\n        if \'num_real_samples\' not in kwargs or \'num_fake_samples\' not in kwargs:\n            raise ValueError(\n                ""num_real_samples and num_fake_samples must be provided for FID computation.""\n            )\n\n    elif metric == \'inception_score\':\n        if \'num_samples\' not in kwargs:\n            raise ValueError(\n                ""num_samples must be provided for IS computation."")\n\n    else:\n        choices = [\'fid\', \'kid\', \'inception_score\']\n        raise ValueError(""Invalid metric {} selected. Choose from {}."".format(\n            metric, choices))\n\n    if evaluate_range and evaluate_step or not (evaluate_step\n                                                or evaluate_range):\n        raise ValueError(\n            ""Only one of evaluate_step or evaluate_range can be defined."")\n\n    if evaluate_range:\n        if (type(evaluate_range) != tuple\n                or not all(map(lambda x: type(x) == int, evaluate_range))):\n            raise ValueError(\n                ""evaluate_range must be a tuple of ints (start, end, step)."")\n\n    ckpt_dir = os.path.join(log_dir, \'checkpoints\', \'netG\')\n\n    if not os.path.exists(ckpt_dir):\n        raise ValueError(\n            ""Checkpoint directory {} cannot be found in log_dir."".format(\n                ckpt_dir))\n\n    if device is None:\n        device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n    # Decide naming convention\n    names_dict = {\n        \'fid\': \'FID\',\n        \'inception_score\': \'Inception Score\',\n        \'kid\': \'KID\',\n    }\n\n    # Set output file and restore if available.\n    if metric == \'fid\':\n        output_file = os.path.join(\n            log_dir,\n            \'fid_{}k_{}k.json\'.format(kwargs[\'num_real_samples\'] // 1000,\n                                      kwargs[\'num_fake_samples\'] // 1000))\n\n    elif metric == \'inception_score\':\n        output_file = os.path.join(\n            log_dir,\n            \'inception_score_{}k.json\'.format(kwargs[\'num_samples\'] // 1000))\n\n    elif metric == \'kid\':\n        output_file = os.path.join(\n            log_dir, \'kid_{}k_{}_subsets.json\'.format(\n                kwargs[\'num_subsets\'] * kwargs[\'subset_size\'] // 1000,\n                kwargs[\'num_subsets\']))\n\n    if os.path.exists(output_file):\n        scores_dict = common.load_from_json(output_file)\n        scores_dict = dict([(int(k), v) for k, v in scores_dict.items()])\n\n    else:\n        scores_dict = {}\n\n    # Evaluate across a range\n    start, end, interval = evaluate_range or (evaluate_step, evaluate_step,\n                                              evaluate_step)\n    for step in range(start, end + 1, interval):\n        # Skip computed scores\n        if step in scores_dict and write_to_json and not overwrite:\n            print(""INFO: {} at step {} has been computed. Skipping..."".format(\n                names_dict[metric], step))\n            continue\n\n        # Load and restore the model checkpoint\n        ckpt_file = os.path.join(ckpt_dir, \'netG_{}_steps.pth\'.format(step))\n        if not os.path.exists(ckpt_file):\n            print(""INFO: Checkpoint at step {} does not exist. Skipping..."".\n                  format(step))\n            continue\n        netG.restore_checkpoint(ckpt_file=ckpt_file, optimizer=None)\n\n        # Compute score for each seed\n        scores = []\n        for seed in range(start_seed, start_seed + num_runs):\n            print(""INFO: Computing {} in memory..."".format(names_dict[metric]))\n\n            # Obtain only the raw score without var\n            if metric == ""fid"":\n                score = compute_fid.fid_score(netG=netG,\n                                              seed=seed,\n                                              device=device,\n                                              log_dir=log_dir,\n                                              **kwargs)\n\n            elif metric == ""inception_score"":\n                score, _ = compute_is.inception_score(netG=netG,\n                                                      seed=seed,\n                                                      device=device,\n                                                      log_dir=log_dir,\n                                                      **kwargs)\n\n            elif metric == ""kid"":\n                score, _ = compute_kid.kid_score(netG=netG,\n                                                 device=device,\n                                                 seed=seed,\n                                                 log_dir=log_dir,\n                                                 **kwargs)\n\n            scores.append(score)\n            print(""INFO: {} (step {}) [seed {}]: {}"".format(\n                names_dict[metric], step, seed, score))\n\n        scores_dict[step] = scores\n\n    # Print the scores in order\n    for step in range(start, end + 1, interval):\n        if step in scores_dict:\n            scores = scores_dict[step]\n            mean = np.mean(scores)\n            std = np.std(scores)\n\n            print(""INFO: {} (step {}): {} (\xc2\xb1 {}) "".format(\n                names_dict[metric], step, mean, std))\n\n    # Save to output file\n    if write_to_json:\n        common.write_to_json(scores_dict, output_file)\n\n    print(""INFO: {} Evaluation completed!"".format(names_dict[metric]))\n\n    return scores_dict\n'"
torch_mimicry/modules/__init__.py,0,b'from .layers import *\nfrom .losses import *\nfrom .resblocks import *\nfrom .spectral_norm import *'
torch_mimicry/modules/layers.py,5,"b'""""""\nScript for building specific layers needed by GAN architecture.\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch_mimicry.modules import spectral_norm\n\n\nclass SelfAttention(nn.Module):\n    """"""\n    Self-attention layer based on version used in BigGAN code:\n    https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py\n    """"""\n    def __init__(self, num_feat, spectral_norm=True):\n        super().__init__()\n        self.num_feat = num_feat\n\n        if spectral_norm:\n            self.f = SNConv2d(self.num_feat,\n                              self.num_feat >> 3,\n                              1,\n                              1,\n                              padding=0,\n                              bias=False)\n            self.g = SNConv2d(self.num_feat,\n                              self.num_feat >> 3,\n                              1,\n                              1,\n                              padding=0,\n                              bias=False)\n            self.h = SNConv2d(self.num_feat,\n                              self.num_feat >> 1,\n                              1,\n                              1,\n                              padding=0,\n                              bias=False)\n            self.o = SNConv2d(self.num_feat >> 1,\n                              self.num_feat,\n                              1,\n                              1,\n                              padding=0,\n                              bias=False)\n\n        else:\n            self.f = nn.Conv2d(self.num_feat,\n                               self.num_feat >> 3,\n                               1,\n                               1,\n                               padding=0,\n                               bias=False)\n            self.g = nn.Conv2d(self.num_feat,\n                               self.num_feat >> 3,\n                               1,\n                               1,\n                               padding=0,\n                               bias=False)\n            self.h = nn.Conv2d(self.num_feat,\n                               self.num_feat >> 1,\n                               1,\n                               1,\n                               padding=0,\n                               bias=False)\n            self.o = nn.Conv2d(self.num_feat >> 1,\n                               self.num_feat,\n                               1,\n                               1,\n                               padding=0,\n                               bias=False)\n\n        self.gamma = nn.Parameter(torch.tensor(0.), requires_grad=True)\n\n    def forward(self, x):\n        """"""\n        Feedforward function. Implementation differs from actual SAGAN paper,\n        see note from BigGAN:\n        https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py#L142\n        """"""\n        # 1x1 convs to project input feature map\n        f = self.f(x)\n        g = F.max_pool2d(self.g(x), [2, 2])\n        h = F.max_pool2d(self.h(x), [2, 2])\n\n        # Reshape layers\n        f = f.view(-1, self.num_feat >> 3, x.shape[2] * x.shape[3])\n        g = g.view(-1, self.num_feat >> 3, x.shape[2] * x.shape[3] >> 2)\n        h = h.view(-1, self.num_feat >> 1, x.shape[2] * x.shape[3] >> 2)\n\n        # Compute attention map probabiltiies\n        beta = F.softmax(torch.bmm(f.transpose(1, 2), g), -1)\n\n        # Weigh output features by attention map\n        o = self.o(\n            torch.bmm(h, beta.transpose(1, 2)).view(-1, self.num_feat >> 1,\n                                                    x.shape[2], x.shape[3]))\n\n        return self.gamma * o + x\n\n\ndef SNConv2d(*args, **kwargs):\n    r""""""\n    Wrapper for applying spectral norm on conv2d layer.\n    """"""\n    if kwargs.get(\'default\', True):\n        return nn.utils.spectral_norm(nn.Conv2d(*args, **kwargs))\n\n    else:\n        return spectral_norm.SNConv2d(*args, **kwargs)\n\n\ndef SNLinear(*args, **kwargs):\n    r""""""\n    Wrapper for applying spectral norm on linear layer.\n    """"""\n    if kwargs.get(\'default\', True):\n        return nn.utils.spectral_norm(nn.Linear(*args, **kwargs))\n\n    else:\n        return spectral_norm.SNLinear(*args, **kwargs)\n\n\ndef SNEmbedding(*args, **kwargs):\n    r""""""\n    Wrapper for applying spectral norm on embedding layer.\n    """"""\n    if kwargs.get(\'default\', True):\n        return nn.utils.spectral_norm(nn.Embedding(*args, **kwargs))\n\n    else:\n        return spectral_norm.SNEmbedding(*args, **kwargs)\n\n\nclass ConditionalBatchNorm2d(nn.Module):\n    r""""""\n    Conditional Batch Norm as implemented in\n    https://github.com/pytorch/pytorch/issues/8985\n\n    Attributes:\n        num_features (int): Size of feature map for batch norm.\n        num_classes (int): Determines size of embedding layer to condition BN.\n    """"""\n    def __init__(self, num_features, num_classes):\n        super().__init__()\n        self.num_features = num_features\n        self.bn = nn.BatchNorm2d(num_features, affine=False)\n        self.embed = nn.Embedding(num_classes, num_features * 2)\n        self.embed.weight.data[:, :num_features].normal_(\n            1, 0.02)  # Initialise scale at N(1, 0.02)\n        self.embed.weight.data[:,\n                               num_features:].zero_()  # Initialise bias at 0\n\n    def forward(self, x, y):\n        r""""""\n        Feedforwards for conditional batch norm.\n\n        Args:\n            x (Tensor): Input feature map.\n            y (Tensor): Input class labels for embedding.\n\n        Returns:\n            Tensor: Output feature map.\n        """"""\n        out = self.bn(x)\n        gamma, beta = self.embed(y).chunk(\n            2, 1)  # divide into 2 chunks, split from dim 1.\n        out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(\n            -1, self.num_features, 1, 1)\n\n        return out\n'"
torch_mimicry/modules/losses.py,7,"b'""""""\nLoss functions definitions.\n""""""\nimport torch\nimport torch.nn.functional as F\n\n\ndef _bce_loss_with_logits(output, labels, **kwargs):\n    r""""""\n    Wrapper for BCE loss with logits.\n    """"""\n    return F.binary_cross_entropy_with_logits(output, labels, **kwargs)\n\n\ndef minimax_loss_gen(output_fake, real_label_val=1.0, **kwargs):\n    r""""""\n    Standard minimax loss for GANs through the BCE Loss with logits fn.\n\n    Args:\n        output (Tensor): Discriminator output logits.\n        labels (Tensor): Labels for computing cross entropy.\n\n    Returns:\n        Tensor: A scalar tensor loss output.      \n    """"""\n    # Produce real labels so G is rewarded if D is fooled\n    real_labels = torch.full((output_fake.shape[0], 1),\n                             real_label_val,\n                             device=output_fake.device)\n\n    loss = _bce_loss_with_logits(output_fake, real_labels, **kwargs)\n\n    return loss\n\n\ndef minimax_loss_dis(output_fake,\n                     output_real,\n                     real_label_val=1.0,\n                     fake_label_val=0.0,\n                     **kwargs):\n    r""""""\n    Standard minimax loss for GANs through the BCE Loss with logits fn.\n\n    Args:\n        output_fake (Tensor): Discriminator output logits for fake images.    \n        output_real (Tensor): Discriminator output logits for real images.\n        real_label_val (int): Label for real images.\n        fake_label_val (int): Label for fake images.\n        device (torch.device): Torch device object for sending created data.\n\n    Returns:\n        Tensor: A scalar tensor loss output.      \n    """"""\n    # Produce real and fake labels.\n    fake_labels = torch.full((output_fake.shape[0], 1),\n                             fake_label_val,\n                             device=output_fake.device)\n    real_labels = torch.full((output_real.shape[0], 1),\n                             real_label_val,\n                             device=output_real.device)\n\n    # FF, compute loss and backprop D\n    errD_fake = _bce_loss_with_logits(output=output_fake,\n                                      labels=fake_labels,\n                                      **kwargs)\n\n    errD_real = _bce_loss_with_logits(output=output_real,\n                                      labels=real_labels,\n                                      **kwargs)\n\n    # Compute cumulative error\n    loss = errD_real + errD_fake\n\n    return loss\n\n\ndef ns_loss_gen(output_fake):\n    r""""""\n    Non-saturating loss for generator.\n\n    Args:\n        output_fake (Tensor): Discriminator output logits for fake images.\n\n    Returns:\n        Tensor: A scalar tensor loss output.    \n    """"""\n    output_fake = torch.sigmoid(output_fake)\n\n    return -torch.mean(torch.log(output_fake + 1e-8))\n\n\ndef wasserstein_loss_dis(output_real, output_fake):\n    r""""""\n    Computes the wasserstein loss for the discriminator.\n\n    Args:\n        output_real (Tensor): Discriminator output logits for real images.\n        output_fake (Tensor): Discriminator output logits for fake images.\n\n    Returns:\n        Tensor: A scalar tensor loss output.        \n    """"""\n    loss = -1.0 * output_real.mean() + output_fake.mean()\n\n    return loss\n\n\ndef wasserstein_loss_gen(output_fake):\n    r""""""\n    Computes the wasserstein loss for generator.\n\n    Args:\n        output_fake (Tensor): Discriminator output logits for fake images.\n\n    Returns:\n        Tensor: A scalar tensor loss output.\n    """"""\n    loss = -output_fake.mean()\n\n    return loss\n\n\ndef hinge_loss_dis(output_fake, output_real):\n    r""""""\n    Hinge loss for discriminator.\n\n    Args:\n        output_fake (Tensor): Discriminator output logits for fake images.\n        output_real (Tensor): Discriminator output logits for real images.\n\n    Returns:\n        Tensor: A scalar tensor loss output.        \n    """"""\n    loss = F.relu(1.0 - output_real).mean() + \\\n           F.relu(1.0 + output_fake).mean()\n\n    return loss\n\n\ndef hinge_loss_gen(output_fake):\n    r""""""\n    Hinge loss for generator.\n\n    Args:\n        output_fake (Tensor): Discriminator output logits for fake images.\n\n    Returns:\n        Tensor: A scalar tensor loss output.      \n    """"""\n    loss = -output_fake.mean()\n\n    return loss\n'"
torch_mimicry/modules/resblocks.py,2,"b'""""""\nImplementation of residual blocks for discriminator and generator.\nWe follow the official SNGAN Chainer implementation as closely as possible:\nhttps://github.com/pfnet-research/sngan_projection\n""""""\nimport math\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch_mimicry.modules import SNConv2d, ConditionalBatchNorm2d\n\n\nclass GBlock(nn.Module):\n    r""""""\n    Residual block for generator.\n\n    Uses bilinear (rather than nearest) interpolation, and align_corners\n    set to False. This is as per how torchvision does upsampling, as seen in:\n    https://github.com/pytorch/vision/blob/master/torchvision/models/segmentation/_utils.py\n\n    Attributes:\n        in_channels (int): The channel size of input feature map.\n        out_channels (int): The channel size of output feature map.\n        hidden_channels (int): The channel size of intermediate feature maps.\n        upsample (bool): If True, upsamples the input feature map.\n        num_classes (int): If more than 0, uses conditional batch norm instead.\n        spectral_norm (bool): If True, uses spectral norm for convolutional layers.\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 hidden_channels=None,\n                 upsample=False,\n                 num_classes=0,\n                 spectral_norm=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels if hidden_channels is not None else out_channels\n        self.learnable_sc = in_channels != out_channels or upsample\n        self.upsample = upsample\n\n        self.num_classes = num_classes\n        self.spectral_norm = spectral_norm\n\n        # Build the layers\n        # Note: Can\'t use something like self.conv = SNConv2d to save code length\n        # this results in somehow spectral norm working worse consistently.\n        if self.spectral_norm:\n            self.c1 = SNConv2d(self.in_channels,\n                               self.hidden_channels,\n                               3,\n                               1,\n                               padding=1)\n            self.c2 = SNConv2d(self.hidden_channels,\n                               self.out_channels,\n                               3,\n                               1,\n                               padding=1)\n        else:\n            self.c1 = nn.Conv2d(self.in_channels,\n                                self.hidden_channels,\n                                3,\n                                1,\n                                padding=1)\n            self.c2 = nn.Conv2d(self.hidden_channels,\n                                self.out_channels,\n                                3,\n                                1,\n                                padding=1)\n\n        if self.num_classes == 0:\n            self.b1 = nn.BatchNorm2d(self.in_channels)\n            self.b2 = nn.BatchNorm2d(self.hidden_channels)\n        else:\n            self.b1 = ConditionalBatchNorm2d(self.in_channels,\n                                             self.num_classes)\n            self.b2 = ConditionalBatchNorm2d(self.hidden_channels,\n                                             self.num_classes)\n\n        self.activation = nn.ReLU(True)\n\n        nn.init.xavier_uniform_(self.c1.weight.data, math.sqrt(2.0))\n        nn.init.xavier_uniform_(self.c2.weight.data, math.sqrt(2.0))\n\n        # Shortcut layer\n        if self.learnable_sc:\n            if self.spectral_norm:\n                self.c_sc = SNConv2d(in_channels,\n                                     out_channels,\n                                     1,\n                                     1,\n                                     padding=0)\n            else:\n                self.c_sc = nn.Conv2d(in_channels,\n                                      out_channels,\n                                      1,\n                                      1,\n                                      padding=0)\n\n            nn.init.xavier_uniform_(self.c_sc.weight.data, 1.0)\n\n    def _upsample_conv(self, x, conv):\n        r""""""\n        Helper function for performing convolution after upsampling.\n        """"""\n        return conv(\n            F.interpolate(x,\n                          scale_factor=2,\n                          mode=\'bilinear\',\n                          align_corners=False))\n\n    def _residual(self, x):\n        r""""""\n        Helper function for feedforwarding through main layers.\n        """"""\n        h = x\n        h = self.b1(h)\n        h = self.activation(h)\n        h = self._upsample_conv(h, self.c1) if self.upsample else self.c1(h)\n        h = self.b2(h)\n        h = self.activation(h)\n        h = self.c2(h)\n\n        return h\n\n    def _residual_conditional(self, x, y):\n        r""""""\n        Helper function for feedforwarding through main layers, including conditional BN.\n        """"""\n        h = x\n        h = self.b1(h, y)\n        h = self.activation(h)\n        h = self._upsample_conv(h, self.c1) if self.upsample else self.c1(h)\n        h = self.b2(h, y)\n        h = self.activation(h)\n        h = self.c2(h)\n\n        return h\n\n    def _shortcut(self, x):\n        r""""""\n        Helper function for feedforwarding through shortcut layers.\n        """"""\n        if self.learnable_sc:\n            x = self._upsample_conv(\n                x, self.c_sc) if self.upsample else self.c_sc(x)\n            return x\n        else:\n            return x\n\n    def forward(self, x, y=None):\n        r""""""\n        Residual block feedforward function.\n        """"""\n        if y is None:\n            return self._residual(x) + self._shortcut(x)\n\n        else:\n            return self._residual_conditional(x, y) + self._shortcut(x)\n\n\nclass DBlock(nn.Module):\n    """"""\n    Residual block for discriminator.\n\n    Attributes:\n        in_channels (int): The channel size of input feature map.\n        out_channels (int): The channel size of output feature map.\n        hidden_channels (int): The channel size of intermediate feature maps.\n        downsample (bool): If True, downsamples the input feature map.\n        spectral_norm (bool): If True, uses spectral norm for convolutional layers.        \n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 hidden_channels=None,\n                 downsample=False,\n                 spectral_norm=True):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels if hidden_channels is not None else in_channels\n        self.downsample = downsample\n        self.learnable_sc = (in_channels != out_channels) or downsample\n        self.spectral_norm = spectral_norm\n\n        # Build the layers\n        if self.spectral_norm:\n            self.c1 = SNConv2d(self.in_channels, self.hidden_channels, 3, 1, 1)\n            self.c2 = SNConv2d(self.hidden_channels, self.out_channels, 3, 1,\n                               1)\n        else:\n            self.c1 = nn.Conv2d(self.in_channels, self.hidden_channels, 3, 1,\n                                1)\n            self.c2 = nn.Conv2d(self.hidden_channels, self.out_channels, 3, 1,\n                                1)\n\n        self.activation = nn.ReLU(True)\n\n        nn.init.xavier_uniform_(self.c1.weight.data, math.sqrt(2.0))\n        nn.init.xavier_uniform_(self.c2.weight.data, math.sqrt(2.0))\n\n        # Shortcut layer\n        if self.learnable_sc:\n            if self.spectral_norm:\n                self.c_sc = SNConv2d(in_channels, out_channels, 1, 1, 0)\n            else:\n                self.c_sc = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n\n            nn.init.xavier_uniform_(self.c_sc.weight.data, 1.0)\n\n    def _residual(self, x):\n        """"""\n        Helper function for feedforwarding through main layers.\n        """"""\n        h = x\n        h = self.activation(h)\n        h = self.c1(h)\n        h = self.activation(h)\n        h = self.c2(h)\n        if self.downsample:\n            h = F.avg_pool2d(h, 2)\n\n        return h\n\n    def _shortcut(self, x):\n        """"""\n        Helper function for feedforwarding through shortcut layers.\n        """"""\n        if self.learnable_sc:\n            x = self.c_sc(x)\n            return F.avg_pool2d(x, 2) if self.downsample else x\n\n        else:\n            return x\n\n    def forward(self, x):\n        """"""\n        Residual block feedforward function.\n        """"""\n        return self._residual(x) + self._shortcut(x)\n\n\nclass DBlockOptimized(nn.Module):\n    """"""\n    Optimized residual block for discriminator. This is used as the first residual block,\n    where there is a definite downsampling involved. Follows the official SNGAN reference implementation\n    in chainer.\n\n    Attributes:\n        in_channels (int): The channel size of input feature map.\n        out_channels (int): The channel size of output feature map.\n        spectral_norm (bool): If True, uses spectral norm for convolutional layers.        \n    """"""\n    def __init__(self, in_channels, out_channels, spectral_norm=True):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.spectral_norm = spectral_norm\n\n        # Build the layers\n        if self.spectral_norm:\n            self.c1 = SNConv2d(self.in_channels, self.out_channels, 3, 1, 1)\n            self.c2 = SNConv2d(self.out_channels, self.out_channels, 3, 1, 1)\n            self.c_sc = SNConv2d(self.in_channels, self.out_channels, 1, 1, 0)\n        else:\n            self.c1 = nn.Conv2d(self.in_channels, self.out_channels, 3, 1, 1)\n            self.c2 = nn.Conv2d(self.out_channels, self.out_channels, 3, 1, 1)\n            self.c_sc = nn.Conv2d(self.in_channels, self.out_channels, 1, 1, 0)\n\n        self.activation = nn.ReLU(True)\n\n        nn.init.xavier_uniform_(self.c1.weight.data, math.sqrt(2.0))\n        nn.init.xavier_uniform_(self.c2.weight.data, math.sqrt(2.0))\n        nn.init.xavier_uniform_(self.c_sc.weight.data, 1.0)\n\n    def _residual(self, x):\n        """"""\n        Helper function for feedforwarding through main layers.\n        """"""\n        h = x\n        h = self.c1(h)\n        h = self.activation(h)\n        h = self.c2(h)\n        h = F.avg_pool2d(h, 2)\n\n        return h\n\n    def _shortcut(self, x):\n        """"""\n        Helper function for feedforwarding through shortcut layers.\n        """"""\n        return self.c_sc(F.avg_pool2d(x, 2))\n\n    def forward(self, x):\n        """"""\n        Residual block feedforward function.\n        """"""\n        return self._residual(x) + self._shortcut(x)\n'"
torch_mimicry/modules/spectral_norm.py,9,"b'""""""\nImplementation of spectral normalization for GANs.\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SpectralNorm(object):\n    r""""""\n    Spectral Normalization for GANs (Miyato 2018).\n\n    Inheritable class for performing spectral normalization of weights,\n    as approximated using power iteration.\n\n    Details: See Algorithm 1 of Appendix A (Miyato 2018).\n\n    Attributes:\n        n_dim (int): Number of dimensions.\n        num_iters (int): Number of iterations for power iter.\n        eps (float): Epsilon for zero division tolerance when normalizing.\n    """"""\n    def __init__(self, n_dim, num_iters=1, eps=1e-12):\n        self.num_iters = num_iters\n        self.eps = eps\n\n        # Register a singular vector for each sigma\n        self.register_buffer(\'sn_u\', torch.randn(1, n_dim))\n        self.register_buffer(\'sn_sigma\', torch.ones(1))\n\n    @property\n    def u(self):\n        return getattr(self, \'sn_u\')\n\n    @property\n    def sigma(self):\n        return getattr(self, \'sn_sigma\')\n\n    def _power_iteration(self, W, u, num_iters, eps=1e-12):\n        with torch.no_grad():\n            for _ in range(num_iters):\n                v = F.normalize(torch.matmul(u, W), eps=eps)\n                u = F.normalize(torch.matmul(v, W.t()), eps=eps)\n\n        # Note: must have gradients, otherwise weights do not get updated!\n        sigma = torch.mm(u, torch.mm(W, v.t()))\n\n        return sigma, u, v\n\n    def sn_weights(self):\n        r""""""\n        Spectrally normalize current weights of the layer.\n        """"""\n        W = self.weight.view(self.weight.shape[0], -1)\n\n        # Power iteration\n        sigma, u, v = self._power_iteration(W=W,\n                                            u=self.u,\n                                            num_iters=self.num_iters,\n                                            eps=self.eps)\n\n        # Update only during training\n        if self.training:\n            with torch.no_grad():\n                self.sigma[:] = sigma\n                self.u[:] = u\n\n        return self.weight / sigma\n\n\nclass SNConv2d(nn.Conv2d, SpectralNorm):\n    r""""""\n    Spectrally normalized layer for Conv2d.\n\n    Attributes:\n        in_channels (int): Input channel dimension.\n        out_channels (int): Output channel dimensions.\n    """"""\n    def __init__(self, in_channels, out_channels, *args, **kwargs):\n        nn.Conv2d.__init__(self, in_channels, out_channels, *args, **kwargs)\n\n        SpectralNorm.__init__(self,\n                              n_dim=out_channels,\n                              num_iters=kwargs.get(\'num_iters\', 1))\n\n    def forward(self, x):\n        return F.conv2d(input=x,\n                        weight=self.sn_weights(),\n                        bias=self.bias,\n                        stride=self.stride,\n                        padding=self.padding,\n                        dilation=self.dilation,\n                        groups=self.groups)\n\n\nclass SNLinear(nn.Linear, SpectralNorm):\n    r""""""\n    Spectrally normalized layer for Linear.\n\n    Attributes:\n        in_features (int): Input feature dimensions.\n        out_features (int): Output feature dimensions.\n    """"""\n    def __init__(self, in_features, out_features, *args, **kwargs):\n        nn.Linear.__init__(self, in_features, out_features, *args, **kwargs)\n\n        SpectralNorm.__init__(self,\n                              n_dim=out_features,\n                              num_iters=kwargs.get(\'num_iters\', 1))\n\n    def forward(self, x):\n        return F.linear(input=x, weight=self.sn_weights(), bias=self.bias)\n\n\nclass SNEmbedding(nn.Embedding, SpectralNorm):\n    r""""""\n    Spectrally normalized layer for Embedding.\n\n    Attributes:\n        num_embeddings (int): Number of embeddings.\n        embedding_dim (int): Dimensions of each embedding vector\n    """"""\n    def __init__(self, num_embeddings, embedding_dim, *args, **kwargs):\n        nn.Embedding.__init__(self, num_embeddings, embedding_dim, *args,\n                              **kwargs)\n\n        SpectralNorm.__init__(self, n_dim=num_embeddings)\n\n    def forward(self, x):\n        return F.embedding(input=x, weight=self.sn_weights())\n'"
torch_mimicry/nets/__init__.py,0,"b'from . import (basemodel, gan, dcgan, wgan_gp, sngan, cgan_pd, ssgan,\n               infomax_gan, sagan)\n'"
torch_mimicry/training/__init__.py,0,b'from .logger import *\nfrom .metric_log import *\nfrom .scheduler import *\nfrom .trainer import *'
torch_mimicry/training/logger.py,7,"b'""""""\nImplementation of the Logger object for performing training logging and visualisation.\n""""""\nimport os\n\nimport numpy as np\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import utils as vutils\n\n\nclass Logger:\n    """"""\n    Writes summaries and visualises training progress.\n    \n    Attributes:\n        log_dir (str): The path to store logging information.\n        num_steps (int): Total number of training iterations.\n        dataset_size (int): The number of examples in the dataset.\n        device (Device): Torch device object to send data to.\n        flush_secs (int): Number of seconds before flushing summaries to disk.\n        writers (dict): A dictionary of tensorboard writers with keys as metric names.\n        num_epochs (int): The number of epochs, for extra information.\n    """"""\n    def __init__(self,\n                 log_dir,\n                 num_steps,\n                 dataset_size,\n                 device,\n                 flush_secs=120,\n                 **kwargs):\n        self.log_dir = log_dir\n        self.num_steps = num_steps\n        self.dataset_size = dataset_size\n        self.flush_secs = flush_secs\n        self.num_epochs = self._get_epoch(num_steps)\n        self.device = device\n        self.writers = {}\n\n        # Create log directory if haven\'t already\n        if not os.path.exists(self.log_dir):\n            os.makedirs(self.log_dir)\n\n    def _get_epoch(self, steps):\n        """"""\n        Helper function for getting epoch.\n        """"""\n        return max(int(steps / self.dataset_size), 1)\n\n    def _build_writer(self, metric):\n        writer = SummaryWriter(log_dir=os.path.join(self.log_dir, \'data\',\n                                                    metric),\n                               flush_secs=self.flush_secs)\n\n        return writer\n\n    def write_summaries(self, log_data, global_step):\n        """"""\n        Tasks appropriate writers to write the summaries in tensorboard. Creates additional\n        writers for summary writing if there are new scalars to log in log_data.\n\n        Args:\n            log_data (MetricLog): Dict-like object to collect log data for TB writing.\n            global_step (int): Global step variable for syncing logs.\n\n        Returns:\n            None\n        """"""\n        for metric, data in log_data.items():\n            if metric not in self.writers:\n                self.writers[metric] = self._build_writer(metric)\n\n            # Write with a group name if it exists\n            name = log_data.get_group_name(metric) or metric\n            self.writers[metric].add_scalar(name,\n                                            log_data[metric],\n                                            global_step=global_step)\n\n    def close_writers(self):\n        """"""\n        Closes all writers.\n        """"""\n        for metric in self.writers:\n            self.writers[metric].close()\n\n    def print_log(self, global_step, log_data, time_taken):\n        """"""\n        Formats the string to print to stdout based on training information.\n\n        Args:\n            log_data (MetricLog): Dict-like object to collect log data for TB writing.\n            global_step (int): Global step variable for syncing logs.\n            time_taken (float): Time taken for one training iteration.\n\n        Returns:\n            str: String to be printed to stdout.\n        """"""\n        # Basic information\n        log_to_show = [\n            ""INFO: [Epoch {:d}/{:d}][Global Step: {:d}/{:d}]"".format(\n                self._get_epoch(global_step), self.num_epochs, global_step,\n                self.num_steps)\n        ]\n\n        # Display GAN information as fed from user.\n        GAN_info = [""""]\n        metrics = sorted(log_data.keys())\n\n        for metric in metrics:\n            GAN_info.append(\'{}: {}\'.format(metric, log_data[metric]))\n\n        # Add train step time information\n        GAN_info.append(""({:.4f} sec/idx)"".format(time_taken))\n\n        # Accumulate to log\n        log_to_show.append(""\\n| "".join(GAN_info))\n\n        # Finally print the output\n        ret = "" "".join(log_to_show)\n        print(ret)\n\n        return ret\n\n    def _get_fixed_noise(self, nz, num_images, output_dir=None):\n        """"""\n        Produce the fixed gaussian noise vectors used across all models\n        for consistency.\n        """"""\n        if output_dir is None:\n            output_dir = os.path.join(self.log_dir, \'viz\')\n\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        output_file = os.path.join(output_dir,\n                                   \'fixed_noise_nz_{}.pth\'.format(nz))\n\n        if os.path.exists(output_file):\n            noise = torch.load(output_file)\n\n        else:\n            noise = torch.randn((num_images, nz))\n            torch.save(noise, output_file)\n\n        return noise.to(self.device)\n\n    def _get_fixed_labels(self, num_images, num_classes):\n        """"""\n        Produces fixed class labels for generating fixed images.\n        """"""\n        labels = np.array([i % num_classes for i in range(num_images)])\n        labels = torch.from_numpy(labels).to(self.device)\n\n        return labels\n\n    def vis_images(self, netG, global_step, num_images=64):\n        """"""\n        Produce visualisations of the G(z), one fixed and one random.\n\n        Args:\n            netG (Module): Generator model object for producing images.\n            global_step (int): Global step variable for syncing logs.\n            num_images (int): The number of images to visualise.\n\n        Returns:\n            None\n        """"""\n        img_dir = os.path.join(self.log_dir, \'images\')\n        if not os.path.exists(img_dir):\n            os.makedirs(img_dir)\n\n        with torch.no_grad():\n            # Generate random images\n            noise = torch.randn((num_images, netG.nz), device=self.device)\n            fake_images = netG(noise).detach().cpu()\n\n            # Generate fixed random images\n            fixed_noise = self._get_fixed_noise(nz=netG.nz,\n                                                num_images=num_images)\n\n            if hasattr(netG, \'num_classes\') and netG.num_classes > 0:\n                fixed_labels = self._get_fixed_labels(num_images,\n                                                      netG.num_classes)\n                fixed_fake_images = netG(fixed_noise,\n                                         fixed_labels).detach().cpu()\n            else:\n                fixed_fake_images = netG(fixed_noise).detach().cpu()\n\n            # Map name to results\n            images_dict = {\n                \'fixed_fake\': fixed_fake_images,\n                \'fake\': fake_images\n            }\n\n            # Visualise all results\n            for name, images in images_dict.items():\n                images_viz = vutils.make_grid(images,\n                                              padding=2,\n                                              normalize=True)\n\n                vutils.save_image(images_viz,\n                                  \'{}/{}_samples_step_{}.png\'.format(\n                                      img_dir, name, global_step),\n                                  normalize=True)\n\n                if \'img\' not in self.writers:\n                    self.writers[\'img\'] = self._build_writer(\'img\')\n\n                self.writers[\'img\'].add_image(\'{}_vis\'.format(name),\n                                              images_viz,\n                                              global_step=global_step)\n'"
torch_mimicry/training/metric_log.py,0,"b'""""""\nMetricLog object for intelligently logging data to display them more intuitively.\n""""""\n\n\nclass MetricLog:\n    """"""\n    A dictionary-like object that logs data, and includes an extra dict to map the metrics\n    to its group name, if any, and the corresponding precision to print out.\n\n    Attributes:\n        metrics_dict (dict): A dictionary mapping to another dict containing\n            the corresponding value, precision, and the group this metric belongs to.\n    """"""\n    def __init__(self, **kwargs):\n        self.metrics_dict = {}\n\n    def add_metric(self, name, value, group=None, precision=4):\n        """"""\n        Logs metric to internal dict, but with an additional option\n        of grouping certain metrics together.\n\n        Args:\n            name (str): Name of metric to log.\n            value (Tensor/Float): Value of the metric to log.\n            group (str): Name of the group to classify different metrics together.\n            precision (int): The number of floating point precision to represent the value.\n\n        Returns:\n            None\n        """"""\n        # Grab tensor values only\n        try:\n            value = value.item()\n        except AttributeError:\n            value = value\n\n        self.metrics_dict[name] = dict(value=value,\n                                       group=group,\n                                       precision=precision)\n\n    def __getitem__(self, key):\n        return round(self.metrics_dict[key][\'value\'],\n                     self.metrics_dict[key][\'precision\'])\n\n    def get_group_name(self, name):\n        """"""\n        Obtains the group name of a particular metric. For example, errD and errG\n        which represents the discriminator/generator losses could fall under a\n        group name called ""loss"".\n\n        Args:\n            name (str): The name of the metric to retrieve group name.\n\n        Returns:\n            str: A string representing the group name of the metric.\n        """"""\n        return self.metrics_dict[name][\'group\']\n\n    def keys(self):\n        """"""\n        Dict like functionality for retrieving keys.\n        """"""\n        return self.metrics_dict.keys()\n\n    def items(self):\n        """"""\n        Dict like functionality for retrieving items.\n        """"""\n        return self.metrics_dict.items()\n'"
torch_mimicry/training/scheduler.py,0,"b'""""""\nImplementation of a specific learning rate scheduler for GANs.\n""""""\n\n\nclass LRScheduler:\n    """"""\n    Learning rate scheduler for training GANs. Supports GAN specific LR scheduling\n    policies, such as the linear decay policy using in SN-GAN paper as based on the\n    original chainer implementation. However, one could safely ignore this class\n    and instead use the official PyTorch scheduler wrappers around a optimizer\n    for other scheduling policies.\n\n    Attributes:\n        lr_decay (str): The learning rate decay policy to use.\n        optD (Optimizer): Torch optimizer object for discriminator.\n        optG (Optimizer): Torch optimizer object for generator.\n        num_steps (int): The number of training iterations.\n        lr_D (float): The initial learning rate of optD.\n        lr_G (float): The initial learning rate of optG.\n    """"""\n    def __init__(self, lr_decay, optD, optG, num_steps, **kwargs):\n        if lr_decay not in [None, \'None\', \'linear\']:\n            raise NotImplementedError(\n                ""lr_decay {} is not currently supported."")\n\n        self.lr_decay = lr_decay\n        self.optD = optD\n        self.optG = optG\n        self.num_steps = num_steps\n\n        # Cache the initial learning rate for uses later\n        self.lr_D = optD.param_groups[0][\'lr\']\n        self.lr_G = optG.param_groups[0][\'lr\']\n\n    def linear_decay(self, optimizer, global_step, lr_value_range,\n                     lr_step_range):\n        """"""\n        Performs linear decay of the optimizer learning rate based on the number of global\n        steps taken. Follows SNGAN\'s chainer implementation of linear decay, as seen in the\n        chainer references:\n        https://docs.chainer.org/en/stable/reference/generated/chainer.training.extensions.LinearShift.html\n        https://github.com/chainer/chainer/blob/v6.2.0/chainer/training/extensions/linear_shift.py#L66\n\n        Note: assumes that the optimizer has only one parameter group to update!\n\n        Args:\n            optimizer (Optimizer): Torch optimizer object to update learning rate.\n            global_step (int): The current global step of the training.\n            lr_value_range (tuple): A tuple of floats (x,y) to decrease from x to y.\n            lr_step_range (tuple): A tuple of ints (i, j) to start decreasing \n                when global_step > i, and until j.\n\n        Returns:\n            float: Float representing the new updated learning rate.\n        """"""\n        # Compute the new learning rate\n        v1, v2 = lr_value_range\n        s1, s2 = lr_step_range\n\n        if global_step <= s1:\n            updated_lr = v1\n\n        elif global_step >= s2:\n            updated_lr = v2\n\n        else:\n            scale_factor = (global_step - s1) / (s2 - s1)\n            updated_lr = v1 + scale_factor * (v2 - v1)\n\n        # Update the learning rate\n        optimizer.param_groups[0][\'lr\'] = updated_lr\n\n        return updated_lr\n\n    def step(self, log_data, global_step):\n        """"""\n        Takes a step for updating learning rate and updates the input log_data\n        with the current status.\n\n        Args:\n            log_data (MetricLog): Object for logging the updated learning rate metric.\n            global_step (int): The current global step of the training.\n\n        Returns:\n            MetricLog: MetricLog object containing the updated learning rate at the current global step.\n        """"""\n        if self.lr_decay == ""linear"":\n            lr_D = self.linear_decay(optimizer=self.optD,\n                                     global_step=global_step,\n                                     lr_value_range=(self.lr_D, 0.0),\n                                     lr_step_range=(0, self.num_steps))\n\n            lr_G = self.linear_decay(optimizer=self.optG,\n                                     global_step=global_step,\n                                     lr_value_range=(self.lr_G, 0.0),\n                                     lr_step_range=(0, self.num_steps))\n\n        elif self.lr_decay in [None, ""None""]:\n            lr_D = self.lr_D\n            lr_G = self.lr_G\n\n        else:\n            raise ValueError(""Invalid lr_decay method {} selected."".format(\n                self.lr_decay))\n\n        # Update metrics log\n        log_data.add_metric(\'lr_D\', lr_D, group=\'lr\', precision=6)\n        log_data.add_metric(\'lr_G\', lr_G, group=\'lr\', precision=6)\n\n        return log_data\n'"
torch_mimicry/training/trainer.py,2,"b'""""""\nImplementation of Trainer object for training GANs.\n""""""\nimport os\nimport re\nimport time\n\nimport torch\n\nfrom torch_mimicry.training import scheduler, logger, metric_log\nfrom torch_mimicry.utils import common\n\n\nclass Trainer:\n    """"""\n    Trainer object for constructing the GAN training pipeline.\n\n    Attributes:\n        netD (Module): Torch discriminator model.\n        netG (Module): Torch generator model.\n        optD (Optimizer): Torch optimizer object for discriminator.\n        optG (Optimizer): Torch optimizer object for generator.\n        dataloader (DataLoader): Torch object for loading data from a dataset object.\n        num_steps (int): The number of training iterations.\n        n_dis (int): Number of discriminator update steps per generator training step.\n        lr_decay (str): The learning rate decay policy to use.\n        log_dir (str): The path to storing logging information and checkpoints.\n        device (Device): Torch device object to send model/data to.\n        logger (Logger): Logger object for visualising training information.\n        scheduler (LRScheduler): GAN training specific learning rate scheduler object.\n        params (dict): Dictionary of training hyperparameters.\n        netD_ckpt_file (str): Custom checkpoint file to restore discriminator from.\n        netG_ckpt_file (str): Custom checkpoint file to restore generator from.\n        print_steps (int): Number of training steps before printing training info to stdout.\n        vis_steps (int): Number of training steps before visualising images with TensorBoard.\n        flush_secs (int): Number of seconds before flushing summaries to disk.\n        log_steps (int): Number of training steps before writing summaries to TensorBoard.\n        save_steps (int): Number of training steps bfeore checkpointing.\n        save_when_end (bool): If True, saves final checkpoint when training concludes.\n    """"""\n    def __init__(self,\n                 netD,\n                 netG,\n                 optD,\n                 optG,\n                 dataloader,\n                 num_steps,\n                 log_dir=\'./log\',\n                 n_dis=1,\n                 netG_ckpt_file=None,\n                 netD_ckpt_file=None,\n                 lr_decay=None,\n                 device=None,\n                 **kwargs):\n        self.netD = netD\n        self.netG = netG\n        self.optD = optD\n        self.optG = optG\n        self.n_dis = n_dis\n        self.lr_decay = lr_decay\n        self.dataloader = dataloader\n        self.num_steps = num_steps\n        self.device = device\n\n        self.log_dir = log_dir\n        if not os.path.exists(self.log_dir):\n            os.makedirs(self.log_dir)\n\n        # Obtain custom or latest checkpoint files\n        if netG_ckpt_file:\n            self.netG_ckpt_dir = os.path.dirname(netG_ckpt_file)\n            self.netG_ckpt_file = netG_ckpt_file\n        else:\n            self.netG_ckpt_dir = os.path.join(self.log_dir, \'checkpoints\',\n                                              \'netG\')\n            self.netG_ckpt_file = self._get_latest_checkpoint(\n                self.netG_ckpt_dir)  # can be None\n\n        if netD_ckpt_file:\n            self.netD_ckpt_dir = os.path.dirname(netD_ckpt_file)\n            self.netD_ckpt_file = netD_ckpt_file\n        else:\n            self.netD_ckpt_dir = os.path.join(self.log_dir, \'checkpoints\',\n                                              \'netD\')\n            self.netD_ckpt_file = self._get_latest_checkpoint(\n                self.netD_ckpt_dir)  # can be None\n\n        # Default parameters, unless provided by kwargs\n        default_params = {\n            \'print_steps\': kwargs.get(\'print_steps\', 1),\n            \'vis_steps\': kwargs.get(\'vis_steps\', 500),\n            \'flush_secs\': kwargs.get(\'flush_secs\', 30),\n            \'log_steps\': kwargs.get(\'log_steps\', 50),\n            \'save_steps\': kwargs.get(\'save_steps\', 5000),\n            \'save_when_end\': kwargs.get(\'save_when_end\', True),\n        }\n        for param in default_params:\n            self.__dict__[param] = default_params[param]\n\n        # Hyperparameters for logging experiments\n        self.params = {\n            \'log_dir\': self.log_dir,\n            \'num_steps\': self.num_steps,\n            \'batch_size\': self.dataloader.batch_size,\n            \'n_dis\': self.n_dis,\n            \'lr_decay\': self.lr_decay,\n            \'optD\': optD.__repr__(),\n            \'optG\': optG.__repr__(),\n        }\n        self.params.update(default_params)\n\n        # Log training hyperparmaeters\n        self._log_params(self.params)\n\n        # Device for hosting model and data\n        if not self.device:\n            self.device = torch.device(\n                \'cuda:0\' if torch.cuda.is_available() else ""cpu"")\n\n        # Ensure model and data are in the same device\n        for net in [self.netD, self.netG]:\n            if net.device != self.device:\n                net.to(self.device)\n\n        # Training helper objects\n        self.logger = logger.Logger(log_dir=self.log_dir,\n                                    num_steps=self.num_steps,\n                                    dataset_size=len(self.dataloader),\n                                    flush_secs=self.flush_secs,\n                                    device=self.device)\n\n        self.scheduler = scheduler.LRScheduler(lr_decay=self.lr_decay,\n                                               optD=self.optD,\n                                               optG=self.optG,\n                                               num_steps=self.num_steps)\n\n    def _log_params(self, params):\n        """"""\n        Takes the argument options to save into a json file.\n        """"""\n        params_file = os.path.join(self.log_dir, \'params.json\')\n\n        # Check for discrepancy with previous training config.\n        if os.path.exists(params_file):\n            check = common.load_from_json(params_file)\n\n            if params != check:\n                diffs = []\n                for k in params:\n                    if k in check and params[k] != check[k]:\n                        diffs.append(\'{}: Expected {} but got {}.\'.format(\n                            k, check[k], params[k]))\n\n                diff_string = \'\\n\'.join(diffs)\n                raise ValueError(\n                    ""Current hyperparameter configuration is different from previously:\\n{}""\n                    .format(diff_string))\n\n        common.write_to_json(params, params_file)\n\n    def _get_latest_checkpoint(self, ckpt_dir):\n        """"""\n        Given a checkpoint dir, finds the checkpoint with the latest training step.\n        """"""\n        def _get_step_number(k):\n            """"""\n            Helper function to get step number from checkpoint files.\n            """"""\n            search = re.search(r\'(\\d+)_steps\', k)\n\n            if search:\n                return int(search.groups()[0])\n            else:\n                return -float(\'inf\')\n\n        if not os.path.exists(ckpt_dir):\n            return None\n\n        files = os.listdir(ckpt_dir)\n        if len(files) == 0:\n            return None\n\n        ckpt_file = max(files, key=lambda x: _get_step_number(x))\n\n        return os.path.join(ckpt_dir, ckpt_file)\n\n    def _fetch_data(self, iter_dataloader):\n        """"""\n        Fetches the next set of data and refresh the iterator when it is exhausted.\n        Follows python EAFP, so no iterator.hasNext() is used.\n        """"""\n        try:\n            real_batch = next(iter_dataloader)\n        except StopIteration:\n            iter_dataloader = iter(self.dataloader)\n            real_batch = next(iter_dataloader)\n\n        real_batch = (real_batch[0].to(self.device),\n                      real_batch[1].to(self.device))\n\n        return iter_dataloader, real_batch\n\n    def _restore_models_and_step(self):\n        """"""\n        Restores model and optimizer checkpoints and ensures global step is in sync.\n        """"""\n        global_step_D = global_step_G = 0\n\n        if self.netD_ckpt_file and os.path.exists(self.netD_ckpt_file):\n            print(""INFO: Restoring checkpoint for D..."")\n            global_step_D = self.netD.restore_checkpoint(\n                ckpt_file=self.netD_ckpt_file, optimizer=self.optD)\n\n        if self.netG_ckpt_file and os.path.exists(self.netG_ckpt_file):\n            print(""INFO: Restoring checkpoint for G..."")\n            global_step_G = self.netG.restore_checkpoint(\n                ckpt_file=self.netG_ckpt_file, optimizer=self.optG)\n\n        if global_step_G != global_step_D:\n            raise ValueError(\'G and D Networks are out of sync.\')\n        else:\n            global_step = global_step_G  # Restores global step\n\n        return global_step\n\n    def train(self):\n        """"""\n        Runs the training pipeline with all given parameters in Trainer.\n        """"""\n        # Restore models\n        global_step = self._restore_models_and_step()\n        print(""INFO: Starting training from global step {}..."".format(\n            global_step))\n\n        try:\n            start_time = time.time()\n\n            # Iterate through data\n            iter_dataloader = iter(self.dataloader)\n            while global_step < self.num_steps:\n                log_data = metric_log.MetricLog()  # log data for tensorboard\n\n                # -------------------------\n                #   One Training Step\n                # -------------------------\n                # Update n_dis times for D\n                for i in range(self.n_dis):\n                    iter_dataloader, real_batch = self._fetch_data(\n                        iter_dataloader=iter_dataloader)\n\n                    # -----------------------\n                    #   Update G Network\n                    # -----------------------\n                    # Update G, but only once.\n                    if i == 0:\n                        log_data = self.netG.train_step(\n                            real_batch=real_batch,\n                            netD=self.netD,\n                            optG=self.optG,\n                            global_step=global_step,\n                            log_data=log_data,\n                            device=self.device)\n\n                    # ------------------------\n                    #   Update D Network\n                    # -----------------------\n                    log_data = self.netD.train_step(real_batch=real_batch,\n                                                    netG=self.netG,\n                                                    optD=self.optD,\n                                                    log_data=log_data,\n                                                    global_step=global_step,\n                                                    device=self.device)\n\n                # --------------------------------\n                #   Update Training Variables\n                # -------------------------------\n                global_step += 1\n\n                log_data = self.scheduler.step(log_data=log_data,\n                                               global_step=global_step)\n\n                # -------------------------\n                #   Logging and Metrics\n                # -------------------------\n                if global_step % self.log_steps == 0:\n                    self.logger.write_summaries(log_data=log_data,\n                                                global_step=global_step)\n\n                if global_step % self.print_steps == 0:\n                    curr_time = time.time()\n                    self.logger.print_log(global_step=global_step,\n                                          log_data=log_data,\n                                          time_taken=(curr_time - start_time) /\n                                          self.print_steps)\n                    start_time = curr_time\n\n                if global_step % self.vis_steps == 0:\n                    self.logger.vis_images(netG=self.netG,\n                                           global_step=global_step)\n\n                if global_step % self.save_steps == 0:\n                    print(""INFO: Saving checkpoints..."")\n                    self.netG.save_checkpoint(directory=self.netG_ckpt_dir,\n                                              global_step=global_step,\n                                              optimizer=self.optG)\n\n                    self.netD.save_checkpoint(directory=self.netD_ckpt_dir,\n                                              global_step=global_step,\n                                              optimizer=self.optD)\n\n            # Save models at the very end of training\n            if self.save_when_end:\n                print(""INFO: Saving final checkpoints..."")\n                self.netG.save_checkpoint(directory=self.netG_ckpt_dir,\n                                          global_step=global_step,\n                                          optimizer=self.optG)\n\n                self.netD.save_checkpoint(directory=self.netD_ckpt_dir,\n                                          global_step=global_step,\n                                          optimizer=self.optD)\n\n        except KeyboardInterrupt:\n            print(""INFO: Saving checkpoints from keyboard interrupt..."")\n            self.netG.save_checkpoint(directory=self.netG_ckpt_dir,\n                                      global_step=global_step,\n                                      optimizer=self.optG)\n\n            self.netD.save_checkpoint(directory=self.netD_ckpt_dir,\n                                      global_step=global_step,\n                                      optimizer=self.optD)\n\n        finally:\n            self.logger.close_writers()\n\n        print(""INFO: Training Ended."")\n'"
torch_mimicry/utils/__init__.py,0,b'from .common import *\r\n'
torch_mimicry/utils/common.py,2,"b'""""""\nScript for common utility functions.\n""""""\nimport json\nimport os\n\nimport numpy as np\nimport torch\nfrom skimage import io\n\n\ndef write_to_json(dict_to_write, output_file):\n    """"""\n    Outputs a given dictionary as a JSON file with indents.\n\n    Args:\n        dict_to_write (dict): Input dictionary to output.\n        output_file (str): File path to write the dictionary.\n\n    Returns:\n        None\n    """"""\n    with open(output_file, \'w\') as file:\n        json.dump(dict_to_write, file, indent=4)\n\n\ndef load_from_json(json_file):\n    """"""\n    Loads a JSON file as a dictionary and return it.\n\n    Args:\n        json_file (str): Input JSON file to read.\n\n    Returns:\n        dict: Dictionary loaded from the JSON file.\n    """"""\n    with open(json_file, \'r\') as file:\n        return json.load(file)\n\n\ndef save_tensor_image(x, output_file):\n    """"""\n    Saves an input image tensor as some numpy array, useful for tests.\n\n    Args:\n        x (Tensor): A 3D tensor image of shape (3, H, W).\n        output_file (str): The output image file to save the tensor.\n\n    Returns:\n        None\n    """"""\n    folder = os.path.dirname(output_file)\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n\n    x = x.permute(1, 2, 0).numpy()\n    io.imsave(output_file, x)\n\n\ndef load_images(n=1, size=32):\n    """"""\n    Load n image tensors with some fake labels.\n\n    Args:\n        n (int): Number of random images to load.\n        size (int): Spatial size of random image.\n\n    Returns:\n        Tensor: Random images of shape (n, 3, size, size) and 0-valued labels.\n    """"""\n    images = torch.randn(n, 3, size, size)\n    labels = torch.from_numpy(np.array([0 * n]))\n\n    return images, labels'"
tests/metrics/fid/test_fid.py,0,"b'import numpy as np\nimport tensorflow as tf\n\nfrom torch_mimicry.metrics.fid import fid_utils\nfrom torch_mimicry.metrics.inception_model import inception_utils\n\n\nclass TestFID:\n    def setup(self):\n        self.images = np.ones((4, 32, 32, 3))\n        self.sess = tf.compat.v1.Session()\n\n    def test_calculate_activation_statistics(self):\n        inception_path = \'./metrics/inception_model\'\n        inception_utils.create_inception_graph(inception_path)\n\n        mu, sigma = fid_utils.calculate_activation_statistics(\n            images=self.images, sess=self.sess)\n\n        assert mu.shape == (2048, )\n        assert sigma.shape == (2048, 2048)\n\n    def test_calculate_frechet_distance(self):\n        mu1, sigma1 = np.ones((16, )), np.ones((16, 16))\n        mu2, sigma2 = mu1 * 2, sigma1 * 2\n\n        score = fid_utils.calculate_frechet_distance(mu1=mu1,\n                                                     mu2=mu2,\n                                                     sigma1=sigma1,\n                                                     sigma2=sigma2)\n\n        assert type(score) == np.float64\n\n    def teardown(self):\n        del self.images\n        self.sess.close()\n\n\nif __name__ == ""__main__"":\n    test = TestFID()\n    test.setup()\n    test.test_calculate_activation_statistics()\n    test.test_calculate_frechet_distance()\n    test.teardown()\n'"
tests/metrics/inception_model/test_inception_utils.py,0,"b'import numpy as np\nimport tensorflow as tf\n\nfrom torch_mimicry.metrics.inception_model import inception_utils\n\n\nclass TestInceptionUtils:\n    def test_get_activations(self):\n        inception_path = \'./metrics/inception_model\'\n        inception_utils.create_inception_graph(inception_path)\n\n        images = np.ones((4, 32, 32, 3))\n        with tf.compat.v1.Session() as sess:\n            feat = inception_utils.get_activations(images=images, sess=sess)\n\n            assert feat.shape == (4, 2048)\n\n\nif __name__ == ""__main__"":\n    test = TestInceptionUtils()\n    test.test_get_activations()\n'"
tests/metrics/inception_score/test_inception_score.py,0,"b'import numpy as np\n\nfrom torch_mimicry.metrics.inception_model import inception_utils\nfrom torch_mimicry.metrics.inception_score import inception_score_utils\n\n\nclass TestInceptionScore:\n    def test_get_predictions(self):\n        inception_utils.create_inception_graph(\'./metrics/inception_model\')\n\n        images = np.ones((4, 32, 32, 3))\n        preds = inception_score_utils.get_predictions(images)\n\n        assert preds.shape == (4, 1008)\n\n    def test_get_inception_score(self):\n        images = np.ones((4, 32, 32, 3))\n        mean, std = inception_score_utils.get_inception_score(images)\n\n        assert type(mean) == float\n        assert type(std) == float\n\n\nif __name__ == ""__main__"":\n    test = TestInceptionScore()\n    test.test_get_predictions()\n    test.test_get_inception_score()\n'"
tests/metrics/kid/test_kid.py,0,"b'import numpy as np\nfrom sklearn.metrics.pairwise import polynomial_kernel\n\nfrom torch_mimicry.metrics.kid import kid_utils\n\n\nclass TestKID:\n    def setup(self):\n        self.codes_g = np.ones((4, 16))\n        self.codes_r = np.ones((4, 16))\n\n    def test_polynomial_mmd(self):\n        score = kid_utils.polynomial_mmd(codes_g=self.codes_g,\n                                         codes_r=self.codes_r)\n\n        assert type(score) == np.float64\n        assert score < 1e-5\n\n    def test_polynomial_mmd_averages(self):\n\n        scores = kid_utils.polynomial_mmd_averages(codes_g=self.codes_g,\n                                                   codes_r=self.codes_r,\n                                                   n_subsets=4,\n                                                   subset_size=1)\n\n        assert len(scores) == 4\n        assert type(scores[0]) == np.float64\n\n    def test_compute_mmd2(self):\n        X = self.codes_g\n        Y = self.codes_r\n        K_XX = polynomial_kernel(X)\n        K_YY = polynomial_kernel(Y)\n        K_XY = polynomial_kernel(X, Y)\n\n        mmd_est_args = [\'u-statistic\', \'unbiased\']\n\n        for mmd_est in mmd_est_args:\n            for unit_diagonal in [True, False]:\n                mmd2_score = kid_utils._compute_mmd2(\n                    K_XX=K_XX,\n                    K_YY=K_YY,\n                    K_XY=K_XY,\n                    mmd_est=mmd_est,\n                    unit_diagonal=unit_diagonal)\n\n                assert type(mmd2_score) == np.float64\n\n    def teardown(self):\n        del self.codes_g\n        del self.codes_r\n\n\nif __name__ == ""__main__"":\n    test = TestKID()\n    test.setup()\n    test.test_polynomial_mmd()\n    test.test_polynomial_mmd_averages()\n    test.test_compute_mmd2()\n    test.teardown()\n'"
tests/nets/basemodel/test_basemodel.py,3,"b'import os\nimport shutil\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.basemodel.basemodel import BaseModel\n\n\nclass ExampleModel(BaseModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.linear = nn.Linear(1, 4)\n        nn.init.xavier_uniform_(self.linear.weight.data)\n\n    def forward(self, x):\n        return\n\n\nclass TestBaseModel:\n    def setup(self):\n        self.model = ExampleModel()\n        self.opt = optim.Adam(self.model.parameters(), 2e-4, betas=(0.0, 0.9))\n        self.global_step = 0\n\n        self.log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                    ""test_log"")\n\n    def test_save_and_restore_checkpoint(self):\n        ckpt_dir = os.path.join(self.log_dir, \'checkpoints/model\')\n        ckpt_file = os.path.join(ckpt_dir,\n                                 ""model_{}_steps.pth"".format(self.global_step))\n\n        self.model.save_checkpoint(directory=ckpt_dir,\n                                   optimizer=self.opt,\n                                   global_step=self.global_step)\n\n        restored_model = ExampleModel()\n        restored_opt = optim.Adam(self.model.parameters(),\n                                  2e-4,\n                                  betas=(0.0, 0.9))\n\n        restored_model.restore_checkpoint(ckpt_file=ckpt_file,\n                                          optimizer=self.opt)\n\n        # Check weights are preserved\n        assert all(\n            (restored_model.linear.weight == self.model.linear.weight) == 1)\n\n        # Check optimizers have same state dict\n        assert self.opt.state_dict() == restored_opt.state_dict()\n\n    def test_count_params(self):\n        num_total_params, num_trainable_params = self.model.count_params()\n\n        assert num_trainable_params == num_total_params == 8\n\n    def test_get_device(self):\n        assert type(self.model.device) == torch.device\n\n    def teardown(self):\n        if os.path.exists(self.log_dir):\n            shutil.rmtree(self.log_dir)\n\n        del self.model\n        del self.opt\n\n\nif __name__ == ""__main__"":\n    test = TestBaseModel()\n    test.setup()\n    test.test_save_and_restore_checkpoint()\n    test.test_count_params()\n    test.test_get_device()\n    test.teardown()\n'"
tests/nets/cgan_pd/test_cgan_pd_128.py,4,"b'""""""\nTest functions for cGAN-PD for image size 128.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.cgan_pd.cgan_pd_128 import CGANPDGenerator128, CGANPDDiscriminator128\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestCGANPD128:\n    def setup(self):\n        self.num_classes = 10\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (4, 3, 128, 128)\n\n        self.noise = torch.ones(self.N, self.nz)\n        self.images = torch.ones(self.N, self.C, self.H, self.W)\n        self.Y = torch.randint(low=0, high=self.num_classes, size=(self.N, ))\n\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = CGANPDGenerator128(num_classes=self.num_classes,\n                                       ngf=self.ngf)\n        self.netD = CGANPDDiscriminator128(num_classes=self.num_classes,\n                                           ndf=self.ndf)\n\n    def test_CGANPDGenerator128(self):\n        images = self.netG(self.noise, self.Y)\n\n        assert images.shape == (self.N, self.C, self.H, self.W)\n\n    def test_CGANPDDiscriminator128(self):\n        output = self.netD(self.images, self.Y)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.noise\n        del self.images\n        del self.Y\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestCGANPD128()\n    test.setup()\n    test.test_CGANPDGenerator128()\n    test.test_CGANPDDiscriminator128()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/cgan_pd/test_cgan_pd_32.py,4,"b'""""""\nTest functions for cGAN-PD for image size 32.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.cgan_pd.cgan_pd_32 import CGANPDGenerator32, CGANPDDiscriminator32\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestCGANPD32:\n    def setup(self):\n        self.num_classes = 10\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (4, 3, 32, 32)\n\n        self.noise = torch.ones(self.N, self.nz)\n        self.images = torch.ones(self.N, self.C, self.H, self.W)\n        self.Y = torch.randint(low=0, high=self.num_classes, size=(self.N, ))\n\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = CGANPDGenerator32(num_classes=self.num_classes,\n                                      ngf=self.ngf)\n        self.netD = CGANPDDiscriminator32(num_classes=self.num_classes,\n                                          ndf=self.ndf)\n\n    def test_CGANPDGenerator32(self):\n        images = self.netG(self.noise, self.Y)\n\n        assert images.shape == (self.N, self.C, self.H, self.W)\n\n    def test_CGANPDDiscriminator32(self):\n        output = self.netD(self.images, self.Y)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.noise\n        del self.images\n        del self.Y\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestCGANPD32()\n    test.setup()\n    test.test_CGANPDGenerator32()\n    test.test_CGANPDDiscriminator32()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/dcgan/test_dcgan_128.py,3,"b'""""""\nTest functions for DCGAN for image size 128.\n\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.dcgan.dcgan_128 import DCGANGenerator128, DCGANDiscriminator128\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestDCGAN128:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 128, 128)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = DCGANGenerator128(ngf=self.ngf)\n        self.netD = DCGANDiscriminator128(ndf=self.ndf)\n\n    def test_DCGANGenerator128(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_DCGANDiscriminator128(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestDCGAN128()\n    test.setup()\n    test.test_DCGANGenerator128()\n    test.test_DCGANDiscriminator128()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/dcgan/test_dcgan_32.py,3,"b'""""""\nTest functions for DCGAN for image size 32.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.dcgan.dcgan_32 import DCGANGenerator32, DCGANDiscriminator32\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestDCGAN32:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 32, 32)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = DCGANGenerator32(ngf=self.ngf)\n        self.netD = DCGANDiscriminator32(ndf=self.ndf)\n\n    def test_DCGANGenerator32(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_DCGANDiscriminator32(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestDCGAN32()\n    test.setup()\n    test.test_DCGANGenerator32()\n    test.test_DCGANDiscriminator32()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/dcgan/test_dcgan_48.py,3,"b'""""""\nTest functions for DCGAN for image size 48.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.dcgan.dcgan_48 import DCGANGenerator48, DCGANDiscriminator48\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestDCGAN48:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 48, 48)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = DCGANGenerator48(ngf=self.ngf)\n        self.netD = DCGANDiscriminator48(ndf=self.ndf)\n\n    def test_DCGANGenerator48(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_DCGANDiscriminator48(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestDCGAN48()\n    test.setup()\n    test.test_DCGANGenerator48()\n    test.test_DCGANDiscriminator48()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/dcgan/test_dcgan_64.py,3,"b'""""""\nTest functions for DCGAN for image size 64.\n\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.dcgan.dcgan_64 import DCGANGenerator64, DCGANDiscriminator64\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestDCGAN64:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 64, 64)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = DCGANGenerator64(ngf=self.ngf)\n        self.netD = DCGANDiscriminator64(ndf=self.ndf)\n\n    def test_DCGANGenerator64(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_DCGANDiscriminator64(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestDCGAN64()\n    test.setup()\n    test.test_DCGANGenerator64()\n    test.test_DCGANDiscriminator64()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/dcgan/test_dcgan_cifar.py,3,"b'""""""\nTest functions for DCGAN for image size 32.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.dcgan.dcgan_cifar import DCGANGeneratorCIFAR, DCGANDiscriminatorCIFAR\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestDCGAN32:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 32, 32)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = DCGANGeneratorCIFAR(ngf=self.ngf)\n        self.netD = DCGANDiscriminatorCIFAR(ndf=self.ndf)\n\n    def test_DCGANGeneratorCIFAR(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_DCGANDiscriminatorCIFAR(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestDCGAN32()\n    test.setup()\n    test.test_DCGANGeneratorCIFAR()\n    test.test_DCGANDiscriminatorCIFAR()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/gan/test_cgan.py,5,"b'import torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.gan.cgan import BaseConditionalGenerator, BaseConditionalDiscriminator\n\n\nclass ExampleGenerator(BaseConditionalGenerator):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.l1 = nn.Linear(1, 1)\n\n    def forward(self, x, y):\n        return torch.ones(x.shape[0], 3, 32, 32)\n\n\nclass ExampleDiscriminator(BaseConditionalDiscriminator):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.l1 = nn.Linear(1, 1)\n\n    def forward(self, x, y):\n        return torch.ones(x.shape[0])\n\n\nclass TestBaseGAN:\n    def setup(self):\n        self.N = 1\n        self.device = ""cpu""\n\n        self.nz = 16\n        self.ngf = 16\n        self.ndf = 16\n        self.bottom_width = 4\n        self.num_classes = 10\n        self.loss_type = \'gan\'\n\n        self.netG = ExampleGenerator(num_classes=self.num_classes,\n                                     ngf=self.ngf,\n                                     bottom_width=self.bottom_width,\n                                     nz=self.nz,\n                                     loss_type=self.loss_type)\n        self.netD = ExampleDiscriminator(num_classes=self.num_classes,\n                                         ndf=self.ndf,\n                                         loss_type=self.loss_type)\n        self.output_fake = torch.ones(self.N, 1)\n        self.output_real = torch.ones(self.N, 1)\n\n    def test_generate_images(self):\n        images = self.netG.generate_images(10)\n\n        assert images.shape == (10, 3, 32, 32)\n        assert images.device == self.netG.device\n\n    def test_generate_images_with_labels(self):\n        images, labels = self.netG.generate_images_with_labels(10)\n\n        assert images.shape == (10, 3, 32, 32)\n        assert images.device == self.netG.device\n        assert labels.shape == (10, )\n        assert labels.device == self.netG.device\n\n    def test_compute_GAN_loss(self):\n        losses = [\'gan\', \'ns\', \'hinge\', \'wasserstein\']\n\n        for loss_type in losses:\n            self.netG.loss_type = loss_type\n            self.netD.loss_type = loss_type\n\n            errG = self.netG.compute_gan_loss(output=self.output_fake)\n            errD = self.netD.compute_gan_loss(output_real=self.output_real,\n                                              output_fake=self.output_fake)\n\n            assert type(errG.item()) == float\n            assert type(errD.item()) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n        del self.output_real\n        del self.output_fake\n\n\nif __name__ == ""__main__"":\n    test = TestBaseGAN()\n    test.setup()\n    test.test_generate_images()\n    test.test_generate_images_with_labels()\n    test.test_compute_GAN_loss()\n    test.teardown()\n'"
tests/nets/gan/test_gan.py,5,"b'import torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.gan.gan import BaseGenerator, BaseDiscriminator\n\n\nclass ExampleGenerator(BaseGenerator):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.l1 = nn.Linear(1, 1)\n\n    def forward(self, x):\n        return torch.ones(x.shape[0], 3, 32, 32)\n\n\nclass ExampleDiscriminator(BaseDiscriminator):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.l1 = nn.Linear(1, 1)\n\n    def forward(self, x):\n        return torch.ones(x.shape[0])\n\n\nclass TestBaseGAN:\n    def setup(self):\n        self.N = 1\n        self.device = ""cpu""\n        self.real_label_val = 1.0\n        self.fake_label_val = 0.0\n\n        self.nz = 16\n        self.ngf = 16\n        self.ndf = 16\n        self.bottom_width = 4\n        self.loss_type = \'gan\'\n\n        self.netG = ExampleGenerator(ngf=self.ngf,\n                                     bottom_width=self.bottom_width,\n                                     nz=self.nz,\n                                     loss_type=self.loss_type)\n        self.netD = ExampleDiscriminator(ndf=self.ndf,\n                                         loss_type=self.loss_type)\n        self.output_fake = torch.ones(self.N, 1)\n        self.output_real = torch.ones(self.N, 1)\n\n    def test_generate_images(self):\n        images = self.netG.generate_images(10)\n\n        assert images.shape == (10, 3, 32, 32)\n        assert images.device == self.netG.device\n\n    def test_compute_GAN_loss(self):\n        losses = [\'gan\', \'ns\', \'hinge\', \'wasserstein\']\n\n        for loss_type in losses:\n            self.netG.loss_type = loss_type\n            self.netD.loss_type = loss_type\n\n            errG = self.netG.compute_gan_loss(output=self.output_fake)\n            errD = self.netD.compute_gan_loss(output_real=self.output_real,\n                                              output_fake=self.output_fake)\n\n            assert type(errG.item()) == float\n            assert type(errD.item()) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n        del self.output_real\n        del self.output_fake\n\n\nif __name__ == ""__main__"":\n    test = TestBaseGAN()\n    test.setup()\n    test.test_generate_images()\n    test.test_compute_GAN_loss()\n    test.teardown()\n'"
tests/nets/infomax_gan/test_infomax_gan_128.py,3,"b'""""""\nTest functions for InfoMaxGAN for image size 128.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.infomax_gan.infomax_gan_128 import InfoMaxGANGenerator128, InfoMaxGANDiscriminator128\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestInfoMaxGAN128:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 128, 128)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = InfoMaxGANGenerator128(ngf=self.ngf)\n        self.netD = InfoMaxGANDiscriminator128(ndf=self.ndf)\n\n    def test_InfoMaxGANGenerator128(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_InfoMaxGANDiscriminator128(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output, local_feat, global_feat = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n        assert local_feat.shape == (self.N, self.netD.ndf, self.H >> 5,\n                                    self.W >> 5)\n        assert global_feat.shape == (self.N, self.netD.ndf)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestInfoMaxGAN128()\n    test.setup()\n    test.test_InfoMaxGANGenerator128()\n    test.test_InfoMaxGANDiscriminator128()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/infomax_gan/test_infomax_gan_32.py,3,"b'""""""\nTest functions for InfoMaxGAN for image size 32.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.infomax_gan.infomax_gan_32 import InfoMaxGANGenerator32, InfoMaxGANDiscriminator32\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestInfoMaxGAN32:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 32, 32)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = InfoMaxGANGenerator32(ngf=self.ngf)\n        self.netD = InfoMaxGANDiscriminator32(ndf=self.ndf)\n\n    def test_InfoMaxGANGenerator32(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_InfoMaxGANDiscriminator32(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output, local_feat, global_feat = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n        assert local_feat.shape == (self.N, self.netD.ndf, self.H >> 2,\n                                    self.W >> 2)\n        assert global_feat.shape == (self.N, self.netD.ndf)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestInfoMaxGAN32()\n    test.setup()\n    test.test_InfoMaxGANGenerator32()\n    test.test_InfoMaxGANDiscriminator32()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/infomax_gan/test_infomax_gan_48.py,3,"b'""""""\nTest functions for InfoMaxGAN for image size 48.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.infomax_gan.infomax_gan_48 import InfoMaxGANGenerator48, InfoMaxGANDiscriminator48\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestInfoMaxGAN48:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 48, 48)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = InfoMaxGANGenerator48(ngf=self.ngf)\n        self.netD = InfoMaxGANDiscriminator48(ndf=self.ndf)\n\n    def test_InfoMaxGANGenerator48(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_InfoMaxGANDiscriminator48(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output, local_feat, global_feat = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n        assert local_feat.shape == (self.N, self.netD.ndf >> 1, self.H >> 4,\n                                    self.W >> 4)\n        assert global_feat.shape == (self.N, self.netD.ndf)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestInfoMaxGAN48()\n    test.setup()\n    test.test_InfoMaxGANGenerator48()\n    test.test_InfoMaxGANDiscriminator48()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/infomax_gan/test_infomax_gan_64.py,3,"b'""""""\nTest functions for InfoMaxGAN for image size 64.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.infomax_gan.infomax_gan_64 import InfoMaxGANGenerator64, InfoMaxGANDiscriminator64\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestInfoMaxGAN64:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 64, 64)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = InfoMaxGANGenerator64(ngf=self.ngf)\n        self.netD = InfoMaxGANDiscriminator64(ndf=self.ndf)\n\n    def test_InfoMaxGANGenerator64(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_InfoMaxGANDiscriminator64(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output, local_feat, global_feat = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n        assert local_feat.shape == (self.N, self.netD.ndf >> 1, self.H >> 4,\n                                    self.W >> 4)\n        assert global_feat.shape == (self.N, self.netD.ndf)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestInfoMaxGAN64()\n    test.setup()\n    test.test_InfoMaxGANGenerator64()\n    test.test_InfoMaxGANDiscriminator64()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/infomax_gan/test_infomax_gan_base.py,2,"b'""""""\nTest for SSGAN specific functions at the discriminator.\n""""""\n\nimport math\n\nimport torch\n\nfrom torch_mimicry.nets.infomax_gan.infomax_gan_base import BaseDiscriminator\n\n\nclass ExampleDiscriminator(BaseDiscriminator):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, x):\n        return\n\n\nclass TestInfoMaxGANBase:\n    def setup(self):\n        self.ndf = 16\n        self.nrkhs = 32\n        self.N = 4\n        self.netD = ExampleDiscriminator(ndf=self.ndf, nrkhs=self.nrkhs)\n\n    def test_infonce_loss(self):\n        l = torch.ones(self.N, self.nrkhs, 1)\n        m = torch.ones(self.N, self.nrkhs, 1)\n\n        loss = self.netD.infonce_loss(l=l, m=m)\n        prob = math.exp(-1 * loss.item())\n\n        assert type(loss.item()) == float\n\n        # 1/4 probability\n        assert abs(prob - 0.25) < 1e-2\n\n    def teardown(self):\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestInfoMaxGANBase()\n    test.setup()\n    test.test_infonce_loss()\n    test.teardown()\n'"
tests/nets/sagan/test_sagan_128.py,4,"b'""""""\nTest functions for SAGAN for image size 128.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.sagan.sagan_128 import SAGANGenerator128, SAGANDiscriminator128\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestSAGAN128:\n    def setup(self):\n        self.num_classes = 10\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (4, 3, 128, 128)\n\n        self.noise = torch.ones(self.N, self.nz)\n        self.images = torch.ones(self.N, self.C, self.H, self.W)\n        self.Y = torch.randint(low=0, high=self.num_classes, size=(self.N, ))\n\n        self.ngf = 32\n        self.ndf = 64\n\n        self.netG = SAGANGenerator128(num_classes=self.num_classes,\n                                      ngf=self.ngf)\n        self.netD = SAGANDiscriminator128(num_classes=self.num_classes,\n                                          ndf=self.ndf)\n\n    def test_SAGANGenerator128(self):\n        images = self.netG(self.noise, self.Y)\n\n        assert images.shape == (self.N, self.C, self.H, self.W)\n\n    def test_SAGANDiscriminator128(self):\n        output = self.netD(self.images, self.Y)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.noise\n        del self.images\n        del self.Y\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestSAGAN128()\n    test.setup()\n    test.test_SAGANGenerator128()\n    test.test_SAGANDiscriminator128()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/sagan/test_sagan_32.py,4,"b'""""""\nTest functions for SAGAN for image size 32.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.sagan.sagan_32 import SAGANGenerator32, SAGANDiscriminator32\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestSAGAN32:\n    def setup(self):\n        self.num_classes = 10\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (4, 3, 32, 32)\n\n        self.noise = torch.ones(self.N, self.nz)\n        self.images = torch.ones(self.N, self.C, self.H, self.W)\n        self.Y = torch.randint(low=0, high=self.num_classes, size=(self.N, ))\n\n        self.ngf = 32\n        self.ndf = 64\n\n        self.netG = SAGANGenerator32(num_classes=self.num_classes,\n                                      ngf=self.ngf)\n        self.netD = SAGANDiscriminator32(num_classes=self.num_classes,\n                                          ndf=self.ndf)\n\n    def test_SAGANGenerator32(self):\n        images = self.netG(self.noise, self.Y)\n\n        assert images.shape == (self.N, self.C, self.H, self.W)\n\n    def test_SAGANDiscriminator32(self):\n        output = self.netD(self.images, self.Y)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.noise\n        del self.images\n        del self.Y\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestSAGAN32()\n    test.setup()\n    test.test_SAGANGenerator32()\n    test.test_SAGANDiscriminator32()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/sngan/test_sngan_128.py,3,"b'""""""\nTest functions for SNGAN for image size 128.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.sngan.sngan_128 import SNGANGenerator128, SNGANDiscriminator128\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestSNGAN128:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 128, 128)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = SNGANGenerator128(ngf=self.ngf)\n        self.netD = SNGANDiscriminator128(ndf=self.ndf)\n\n    def test_SNGANGenerator128(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_SNGANDiscriminator128(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestSNGAN128()\n    test.setup()\n    test.test_SNGANGenerator128()\n    test.test_SNGANDiscriminator128()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/sngan/test_sngan_32.py,3,"b'""""""\nTest functions for SNGAN for image size 32.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.sngan.sngan_32 import SNGANGenerator32, SNGANDiscriminator32\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestSNGAN32:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 32, 32)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = SNGANGenerator32(ngf=self.ngf)\n        self.netD = SNGANDiscriminator32(ndf=self.ndf)\n\n    def test_SNGANGenerator32(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_SNGANDiscriminator32(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        # Get real and fake images\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestSNGAN32()\n    test.setup()\n    test.test_SNGANGenerator32()\n    test.test_SNGANDiscriminator32()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/sngan/test_sngan_48.py,3,"b'""""""\nTest functions for SNGAN for image size 48.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.sngan.sngan_48 import SNGANGenerator48, SNGANDiscriminator48\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestSNGAN48:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 48, 48)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = SNGANGenerator48(ngf=self.ngf)\n        self.netD = SNGANDiscriminator48(ndf=self.ndf)\n\n    def test_SNGANGenerator48(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_SNGANDiscriminator48(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestSNGAN48()\n    test.setup()\n    test.test_SNGANGenerator48()\n    test.test_SNGANDiscriminator48()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/sngan/test_sngan_64.py,3,"b'""""""\nTest functions for SNGAN for image size 64.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.sngan.sngan_64 import SNGANGenerator64, SNGANDiscriminator64\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestSNGAN64:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 64, 64)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = SNGANGenerator64(ngf=self.ngf)\n        self.netD = SNGANDiscriminator64(ndf=self.ndf)\n\n    def test_SNGANGenerator64(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_SNGANDiscriminator64(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestSNGAN64()\n    test.setup()\n    test.test_SNGANGenerator64()\n    test.test_SNGANDiscriminator64()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/ssgan/test_ssgan_128.py,3,"b'""""""\nTest functions for SSGAN for image size 128.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.ssgan.ssgan_128 import SSGANGenerator128, SSGANDiscriminator128\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestSSGAN128:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 128, 128)\n        self.ngf = 16\n        self.ndf = 16\n        self.device = \'cpu\'\n\n        self.netG = SSGANGenerator128(ngf=self.ngf)\n        self.netD = SSGANDiscriminator128(ndf=self.ndf)\n\n    def test_SSGANGenerator128(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_SSGANDiscriminator128(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output, labels = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n        assert labels.shape == (self.N, 4)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=self.device,\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=self.device)\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestSSGAN128()\n    test.setup()\n    test.test_SSGANGenerator128()\n    test.test_SSGANDiscriminator128()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/ssgan/test_ssgan_32.py,3,"b'""""""\nTest functions for SSGAN for image size 32.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.ssgan.ssgan_32 import SSGANGenerator32, SSGANDiscriminator32\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestSSGAN32:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 32, 32)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = SSGANGenerator32(ngf=self.ngf)\n        self.netD = SSGANDiscriminator32(ndf=self.ndf)\n\n    def test_SSGANGenerator32(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_SSGANDiscriminator32(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output, labels = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n        assert labels.shape == (self.N, 4)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestSSGAN32()\n    test.setup()\n    test.test_SSGANGenerator32()\n    test.test_SSGANDiscriminator32()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/ssgan/test_ssgan_48.py,3,"b'""""""\nTest functions for SSGAN for image size 48.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.ssgan.ssgan_48 import SSGANGenerator48, SSGANDiscriminator48\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestSSGAN48:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 48, 48)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = SSGANGenerator48(ngf=self.ngf)\n        self.netD = SSGANDiscriminator48(ndf=self.ndf)\n\n    def test_SSGANGenerator48(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_SSGANDiscriminator48(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output, labels = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n        assert labels.shape == (self.N, 4)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestSSGAN48()\n    test.setup()\n    test.test_SSGANGenerator48()\n    test.test_SSGANDiscriminator48()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/ssgan/test_ssgan_64.py,3,"b'""""""\nTest functions for SSGAN for image size 64.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.ssgan.ssgan_64 import SSGANGenerator64, SSGANDiscriminator64\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestSSGAN64:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 64, 64)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = SSGANGenerator64(ngf=self.ngf)\n        self.netD = SSGANDiscriminator64(ndf=self.ndf)\n\n    def test_SSGANGenerator64(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_SSGANDiscriminator64(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output, labels = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n        assert labels.shape == (self.N, 4)\n\n    def test_train_steps(self):\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestSSGAN64()\n    test.setup()\n    test.test_SSGANGenerator64()\n    test.test_SSGANDiscriminator64()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/ssgan/test_ssgan_base.py,3,"b'""""""\nTest for SSGAN specific functions at the discriminator.\n""""""\n\nimport torch\n\nfrom torch_mimicry.nets.ssgan.ssgan_base import SSGANBaseDiscriminator\nfrom torch_mimicry.utils import common\n\n\nclass ExampleDiscriminator(SSGANBaseDiscriminator):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, x):\n        return torch.ones(x.shape[0])\n\n\nclass TestSSGANBase:\n    def setup(self):\n        self.netD = ExampleDiscriminator(ndf=16)\n\n    def test_rot_tensor(self):\n        # Load image and model\n        image, _ = common.load_images(1, size=32)\n\n        # For any rotation, after performing the same action 4 times,\n        # you should return to the same pixel value\n        for deg in [0, 90, 180, 270]:\n            x = image.clone()\n            for _ in range(4):\n                x = self.netD._rot_tensor(x, deg)\n\n            assert torch.sum((x - image)**2) < 1e-5\n\n    def test_rotate_batch(self):\n        # Load image and model\n        images, _ = common.load_images(8, size=32)\n\n        check = images.clone()\n        check, labels = self.netD._rotate_batch(check)\n        degrees = [0, 90, 180, 270]\n\n        # Rotate 3 more times to get back to original.\n        for i in range(check.shape[0]):\n            for _ in range(3):\n                check[i] = self.netD._rot_tensor(check[i], degrees[labels[i]])\n\n        assert torch.sum((images - check)**2) < 1e-5\n\n    def teardown(self):\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestSSGANBase()\n    test.setup()\n    test.test_rot_tensor()\n    test.test_rotate_batch()\n    test.teardown()\n'"
tests/nets/wgan_gp/test_wgan_gp_128.py,3,"b'""""""\nTest functions for WGAN-GP for image size 128.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.wgan_gp.wgan_gp_128 import WGANGPGenerator128, WGANGPDiscriminator128\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestWGANGP128:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 128, 128)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = WGANGPGenerator128(ngf=self.ngf)\n        self.netD = WGANGPDiscriminator128(ndf=self.ndf)\n\n    def test_WGANGPGenerator128(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_WGANGPDiscriminator128(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        # Get real and fake images\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestWGANGP128()\n    test.setup()\n    test.test_WGANGPGenerator128()\n    test.test_WGANGPDiscriminator128()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/wgan_gp/test_wgan_gp_32.py,3,"b'""""""\nTest functions for WGAN-GP for image size 32.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.wgan_gp.wgan_gp_32 import WGANGPGenerator32, WGANGPDiscriminator32\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestWGANGP32:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 32, 32)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = WGANGPGenerator32(ngf=self.ngf)\n        self.netD = WGANGPDiscriminator32(ndf=self.ndf)\n\n    def test_WGANGPGenerator32(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_WGANGPDiscriminator32(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        # Get real and fake images\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestWGANGP32()\n    test.setup()\n    test.test_WGANGPGenerator32()\n    test.test_WGANGPDiscriminator32()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/wgan_gp/test_wgan_gp_48.py,3,"b'""""""\nTest functions for WGAN-GP for image size 48.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.wgan_gp.wgan_gp_48 import WGANGPGenerator48, WGANGPDiscriminator48\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestWGANGP48:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 48, 48)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = WGANGPGenerator48(ngf=self.ngf)\n        self.netD = WGANGPDiscriminator48(ndf=self.ndf)\n\n    def test_WGANGPGenerator48(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_WGANGPDiscriminator48(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        # Get real and fake images\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestWGANGP48()\n    test.setup()\n    test.test_WGANGPGenerator48()\n    test.test_WGANGPDiscriminator48()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/wgan_gp/test_wgan_gp_64.py,3,"b'""""""\nTest functions for WGAN-GP for image size 64.\n""""""\nimport torch\nimport torch.optim as optim\n\nfrom torch_mimicry.nets.wgan_gp.wgan_gp_64 import WGANGPGenerator64, WGANGPDiscriminator64\nfrom torch_mimicry.training import metric_log\nfrom torch_mimicry.utils import common\n\n\nclass TestWGANGP64:\n    def setup(self):\n        self.nz = 128\n        self.N, self.C, self.H, self.W = (8, 3, 64, 64)\n        self.ngf = 16\n        self.ndf = 16\n\n        self.netG = WGANGPGenerator64(ngf=self.ngf)\n        self.netD = WGANGPDiscriminator64(ndf=self.ndf)\n\n    def test_WGANGPGenerator64(self):\n        noise = torch.ones(self.N, self.nz)\n        output = self.netG(noise)\n\n        assert output.shape == (self.N, self.C, self.H, self.W)\n\n    def test_WGANGPDiscriminator64(self):\n        images = torch.ones(self.N, self.C, self.H, self.W)\n        output = self.netD(images)\n\n        assert output.shape == (self.N, 1)\n\n    def test_train_steps(self):\n        # Get real and fake images\n        real_batch = common.load_images(self.N, size=self.H)\n\n        # Setup optimizers\n        optD = optim.Adam(self.netD.parameters(), 2e-4, betas=(0.0, 0.9))\n        optG = optim.Adam(self.netG.parameters(), 2e-4, betas=(0.0, 0.9))\n\n        # Log statistics to check\n        log_data = metric_log.MetricLog()\n\n        # Test D train step\n        log_data = self.netD.train_step(real_batch=real_batch,\n                                        netG=self.netG,\n                                        optD=optD,\n                                        device=\'cpu\',\n                                        log_data=log_data)\n\n        log_data = self.netG.train_step(real_batch=real_batch,\n                                        netD=self.netD,\n                                        optG=optG,\n                                        log_data=log_data,\n                                        device=\'cpu\')\n\n        for name, metric_dict in log_data.items():\n            assert type(name) == str\n            assert type(metric_dict[\'value\']) == float\n\n    def teardown(self):\n        del self.netG\n        del self.netD\n\n\nif __name__ == ""__main__"":\n    test = TestWGANGP64()\n    test.setup()\n    test.test_WGANGPGenerator64()\n    test.test_WGANGPDiscriminator64()\n    test.test_train_steps()\n    test.teardown()\n'"
tests/nets/wgan_gp/test_wgan_gp_resblocks.py,2,"b'from itertools import product\n\nimport torch\n\nfrom torch_mimicry.nets.wgan_gp import wgan_gp_resblocks\n\n\nclass TestResBlocks:\n    def setup(self):\n        self.images = torch.ones(4, 3, 16, 16)\n\n    def test_GBlock(self):\n        # Arguments\n        num_classes_list = [0, 10]\n        spectral_norm_list = [True, False]\n        in_channels = 3\n        out_channels = 8\n        args_comb = product(num_classes_list, spectral_norm_list)\n\n        for args in args_comb:\n            num_classes = args[0]\n            spectral_norm = args[1]\n\n            if num_classes > 0:\n                y = torch.ones((4, ), dtype=torch.int64)\n            else:\n                y = None\n\n            gen_block_up = wgan_gp_resblocks.GBlock(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                upsample=True,\n                num_classes=num_classes,\n                spectral_norm=spectral_norm)\n\n            gen_block = wgan_gp_resblocks.GBlock(in_channels=in_channels,\n                                                 out_channels=out_channels,\n                                                 upsample=False,\n                                                 num_classes=num_classes,\n                                                 spectral_norm=spectral_norm)\n\n            assert gen_block_up(self.images, y).shape == (4, 8, 32, 32)\n            assert gen_block(self.images, y).shape == (4, 8, 16, 16)\n\n    def test_DBlocks(self):\n        in_channels = 3\n        out_channels = 8\n\n        for spectral_norm in [True, False]:\n            dis_block_down = wgan_gp_resblocks.DBlock(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                downsample=True,\n                spectral_norm=spectral_norm)\n\n            dis_block = wgan_gp_resblocks.DBlock(in_channels=in_channels,\n                                                 out_channels=out_channels,\n                                                 downsample=False,\n                                                 spectral_norm=spectral_norm)\n\n            dis_block_opt = wgan_gp_resblocks.DBlockOptimized(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                spectral_norm=spectral_norm)\n\n            assert dis_block(self.images).shape == (4, out_channels, 16, 16)\n            assert dis_block_down(self.images).shape == (4, out_channels, 8, 8)\n            assert dis_block_opt(self.images).shape == (4, out_channels, 8, 8)\n\n    def teardown(self):\n        del self.images\n\n\nif __name__ == ""__main__"":\n    test = TestResBlocks()\n    test.setup()\n    test.test_GBlock()\n    test.test_DBlocks()\n    test.teardown()\n'"
torch_mimicry/datasets/imagenet/__init__.py,0,b'from .imagenet import *\n'
torch_mimicry/datasets/imagenet/imagenet.py,2,"b'""""""\nModified version of imagenet torchvision dataset source code to allow customised token\nfor automatic download.\n""""""\nfrom __future__ import print_function\n\nimport os\nimport shutil\nimport tempfile\n\nimport torch\nfrom torchvision.datasets import ImageFolder\n\nfrom .imagenet_utils import check_integrity, download_and_extract_archive, extract_archive, \\\n    verify_str_arg\n\n\nclass ImageNet(ImageFolder):\n    """"""\n    `ImageNet <http://image-net.org/>`_ 2012 Classification Dataset.\n\n    Args:\n        - root (string): Root directory of the ImageNet Dataset.\n        - split (string, optional): The dataset split, supports ``train``, or ``val``.\n        - token (string): ImageNet token for download.\n        - download (bool, optional): If true, downloads the dataset from the internet and\n            puts it in root directory. If dataset is already downloaded, it is not\n            downloaded again.\n        - transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        - target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        - loader (callable, optional): A function to load an image given its path.\n\n     Attributes:\n        - classes (list): List of the class name tuples.\n        - class_to_idx (dict): Dict with items (class_name, class_index).\n        - wnids (list): List of the WordNet IDs.\n        - wnid_to_idx (dict): Dict with items (wordnet_id, class_index).\n        - imgs (list): List of (image path, class_index) tuples\n        - targets (list): The class_index value for each image in the dataset\n    """"""\n    def __init__(self,\n                 root,\n                 token=\'\',\n                 split=\'train\',\n                 download=False,\n                 **kwargs):\n        root = self.root = os.path.expanduser(root)\n        self.split = verify_str_arg(split, ""split"", (""train"", ""val""))\n\n        self.archive_dict = {\n            \'train\': {\n                \'url\':\n                \'http://www.image-net.org/challenges/LSVRC/2012/{}/ILSVRC2012_img_train.tar\'\n                .format(token),\n                \'md5\':\n                \'1d675b47d978889d74fa0da5fadfb00e\',\n            },\n            \'val\': {\n                \'url\':\n                \'http://www.image-net.org/challenges/LSVRC/2012/{}/ILSVRC2012_img_val.tar\'\n                .format(token),\n                \'md5\':\n                \'29b22e2961454d5413ddabcf34fc5622\',\n            },\n            \'devkit\': {\n                \'url\':\n                \'http://www.image-net.org/challenges/LSVRC/2012/{}/ILSVRC2012_devkit_t12.tar.gz\'\n                .format(token),\n                \'md5\':\n                \'fa75699e90414af021442c21a62c3abf\',\n            }\n        }\n\n        if download:\n            if len(token) == 0:\n                raise ValueError(\n                    ""ImageNet token is empty. Please obtain permission token from the official website.""\n                )\n\n            self.download()\n        wnid_to_classes = self._load_meta_file()[0]\n\n        super(ImageNet, self).__init__(self.split_folder, **kwargs)\n        self.root = root\n\n        self.wnids = self.classes\n        self.wnid_to_idx = self.class_to_idx\n        self.classes = [wnid_to_classes[wnid] for wnid in self.wnids]\n        self.class_to_idx = {\n            cls: idx\n            for idx, clss in enumerate(self.classes) for cls in clss\n        }\n\n    def download(self):\n        if not check_integrity(self.meta_file):\n            tmp_dir = tempfile.mkdtemp()\n\n            archive_dict = self.archive_dict[\'devkit\']\n            download_and_extract_archive(archive_dict[\'url\'],\n                                         self.root,\n                                         extract_root=tmp_dir,\n                                         md5=archive_dict[\'md5\'])\n            devkit_folder = _splitexts(os.path.basename(\n                archive_dict[\'url\']))[0]\n            meta = parse_devkit(os.path.join(tmp_dir, devkit_folder))\n            self._save_meta_file(*meta)\n\n            shutil.rmtree(tmp_dir)\n\n        if not os.path.isdir(self.split_folder):\n            archive_dict = self.archive_dict[self.split]\n            download_and_extract_archive(archive_dict[\'url\'],\n                                         self.root,\n                                         extract_root=self.split_folder,\n                                         md5=archive_dict[\'md5\'])\n\n            if self.split == \'train\':\n                prepare_train_folder(self.split_folder)\n            elif self.split == \'val\':\n                val_wnids = self._load_meta_file()[1]\n                prepare_val_folder(self.split_folder, val_wnids)\n        else:\n            msg = (\n                ""You set download=True, but a folder \'{}\' already exist in ""\n                ""the root directory. If you want to re-download or re-extract the ""\n                ""archive, delete the folder."")\n            print(msg.format(self.split))\n\n    @property\n    def meta_file(self):\n        return os.path.join(self.root, \'meta.bin\')\n\n    def _load_meta_file(self):\n        if check_integrity(self.meta_file):\n            return torch.load(self.meta_file)\n        else:\n            raise RuntimeError(""Meta file not found or corrupted."",\n                               ""You can use download=True to create it."")\n\n    def _save_meta_file(self, wnid_to_class, val_wnids):\n        torch.save((wnid_to_class, val_wnids), self.meta_file)\n\n    @property\n    def split_folder(self):\n        return os.path.join(self.root, self.split)\n\n    def extra_repr(self):\n        return ""Split: {split}"".format(**self.__dict__)\n\n\ndef parse_devkit(root):\n    idx_to_wnid, wnid_to_classes = parse_meta(root)\n    val_idcs = parse_val_groundtruth(root)\n    val_wnids = [idx_to_wnid[idx] for idx in val_idcs]\n    return wnid_to_classes, val_wnids\n\n\ndef parse_meta(devkit_root, path=\'data\', filename=\'meta.mat\'):\n    import scipy.io as sio\n\n    metafile = os.path.join(devkit_root, path, filename)\n    meta = sio.loadmat(metafile, squeeze_me=True)[\'synsets\']\n    nums_children = list(zip(*meta))[4]\n    meta = [\n        meta[idx] for idx, num_children in enumerate(nums_children)\n        if num_children == 0\n    ]\n    idcs, wnids, classes = list(zip(*meta))[:3]\n    classes = [tuple(clss.split(\', \')) for clss in classes]\n    idx_to_wnid = {idx: wnid for idx, wnid in zip(idcs, wnids)}\n    wnid_to_classes = {wnid: clss for wnid, clss in zip(wnids, classes)}\n    return idx_to_wnid, wnid_to_classes\n\n\ndef parse_val_groundtruth(devkit_root,\n                          path=\'data\',\n                          filename=\'ILSVRC2012_validation_ground_truth.txt\'):\n    with open(os.path.join(devkit_root, path, filename), \'r\') as txtfh:\n        val_idcs = txtfh.readlines()\n    return [int(val_idx) for val_idx in val_idcs]\n\n\ndef prepare_train_folder(folder):\n    for archive in [\n            os.path.join(folder, archive) for archive in os.listdir(folder)\n    ]:\n        extract_archive(archive,\n                        os.path.splitext(archive)[0],\n                        remove_finished=True)\n\n\ndef prepare_val_folder(folder, wnids):\n    img_files = sorted(\n        [os.path.join(folder, file) for file in os.listdir(folder)])\n\n    for wnid in set(wnids):\n        os.mkdir(os.path.join(folder, wnid))\n\n    for wnid, img_file in zip(wnids, img_files):\n        shutil.move(img_file,\n                    os.path.join(folder, wnid, os.path.basename(img_file)))\n\n\ndef _splitexts(root):\n    exts = []\n    ext = \'.\'\n    while ext:\n        root, ext = os.path.splitext(root)\n        exts.append(ext)\n    return root, \'\'.join(reversed(exts))\n'"
torch_mimicry/datasets/imagenet/imagenet_utils.py,3,"b'""""""\nOriginal file from TorchVision for loading imagenet.\n""""""\nimport errno\nimport gzip\nimport hashlib\nimport os\nimport os.path\nimport tarfile\nimport zipfile\n\nimport torch\nfrom torch._six import PY3\nfrom torch.utils.model_zoo import tqdm\n\n\ndef gen_bar_updater():\n    pbar = tqdm(total=None)\n\n    def bar_update(count, block_size, total_size):\n        if pbar.total is None and total_size:\n            pbar.total = total_size\n        progress_bytes = count * block_size\n        pbar.update(progress_bytes - pbar.n)\n\n    return bar_update\n\n\ndef calculate_md5(fpath, chunk_size=1024 * 1024):\n    md5 = hashlib.md5()\n    with open(fpath, \'rb\') as f:\n        for chunk in iter(lambda: f.read(chunk_size), b\'\'):\n            md5.update(chunk)\n    return md5.hexdigest()\n\n\ndef check_md5(fpath, md5, **kwargs):\n    return md5 == calculate_md5(fpath, **kwargs)\n\n\ndef check_integrity(fpath, md5=None):\n    if not os.path.isfile(fpath):\n        return False\n    if md5 is None:\n        return True\n    return check_md5(fpath, md5)\n\n\ndef makedir_exist_ok(dirpath):\n    """"""\n    Python2 support for os.makedirs(.., exist_ok=True)\n    """"""\n    try:\n        os.makedirs(dirpath)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            pass\n        else:\n            raise\n\n\ndef download_url(url, root, filename=None, md5=None):\n    """"""Download a file from a url and place it in root.\n\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    """"""\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # check if file is already present locally\n    if check_integrity(fpath, md5):\n        print(\'Using downloaded and verified file: \' + fpath)\n    else:  # download the file\n        try:\n            print(\'Downloading \' + url + \' to \' + fpath)\n            urllib.request.urlretrieve(url,\n                                       fpath,\n                                       reporthook=gen_bar_updater())\n        except (urllib.error.URLError, IOError) as e:\n            if url[:5] == \'https\':\n                url = url.replace(\'https:\', \'http:\')\n                print(\'Failed download. Trying https -> http instead.\'\n                      \' Downloading \' + url + \' to \' + fpath)\n                urllib.request.urlretrieve(url,\n                                           fpath,\n                                           reporthook=gen_bar_updater())\n            else:\n                raise e\n        # check integrity of downloaded file\n        if not check_integrity(fpath, md5):\n            raise RuntimeError(""File not found or corrupted."")\n\n\ndef list_dir(root, prefix=False):\n    """"""List all directories at a given root\n\n    Args:\n        root (str): Path to directory whose folders need to be listed\n        prefix (bool, optional): If true, prepends the path to each result, otherwise\n            only returns the name of the directories found\n    """"""\n    root = os.path.expanduser(root)\n    directories = list(\n        filter(lambda p: os.path.isdir(os.path.join(root, p)),\n               os.listdir(root)))\n\n    if prefix is True:\n        directories = [os.path.join(root, d) for d in directories]\n\n    return directories\n\n\ndef list_files(root, suffix, prefix=False):\n    """"""List all files ending with a suffix at a given root\n\n    Args:\n        root (str): Path to directory whose folders need to be listed\n        suffix (str or tuple): Suffix of the files to match, e.g. \'.png\' or (\'.jpg\', \'.png\').\n            It uses the Python ""str.endswith"" method and is passed directly\n        prefix (bool, optional): If true, prepends the path to each result, otherwise\n            only returns the name of the files found\n    """"""\n    root = os.path.expanduser(root)\n    files = list(\n        filter(\n            lambda p: os.path.isfile(os.path.join(root, p)) and p.endswith(\n                suffix), os.listdir(root)))\n\n    if prefix is True:\n        files = [os.path.join(root, d) for d in files]\n\n    return files\n\n\ndef download_file_from_google_drive(file_id, root, filename=None, md5=None):\n    """"""Download a Google Drive file from  and place it in root.\n\n    Args:\n        file_id (str): id of file to be downloaded\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the id of the file.\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    """"""\n    # Based on https://stackoverflow.com/questions/38511444/python-download-files-from-google-drive-using-url\n    import requests\n    url = ""https://docs.google.com/uc?export=download""\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = file_id\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print(\'Using downloaded and verified file: \' + fpath)\n    else:\n        session = requests.Session()\n\n        response = session.get(url, params={\'id\': file_id}, stream=True)\n        token = _get_confirm_token(response)\n\n        if token:\n            params = {\'id\': file_id, \'confirm\': token}\n            response = session.get(url, params=params, stream=True)\n\n        _save_response_content(response, fpath)\n\n\ndef _get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(\'download_warning\'):\n            return value\n\n    return None\n\n\ndef _save_response_content(response, destination, chunk_size=32768):\n    with open(destination, ""wb"") as f:\n        pbar = tqdm(total=None)\n        progress = 0\n        for chunk in response.iter_content(chunk_size):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n                progress += len(chunk)\n                pbar.update(progress - pbar.n)\n        pbar.close()\n\n\ndef _is_tarxz(filename):\n    return filename.endswith("".tar.xz"")\n\n\ndef _is_tar(filename):\n    return filename.endswith("".tar"")\n\n\ndef _is_targz(filename):\n    return filename.endswith("".tar.gz"")\n\n\ndef _is_gzip(filename):\n    return filename.endswith("".gz"") and not filename.endswith("".tar.gz"")\n\n\ndef _is_zip(filename):\n    return filename.endswith("".zip"")\n\n\ndef extract_archive(from_path, to_path=None, remove_finished=False):\n    if to_path is None:\n        to_path = os.path.dirname(from_path)\n\n    if _is_tar(from_path):\n        with tarfile.open(from_path, \'r\') as tar:\n            tar.extractall(path=to_path)\n    elif _is_targz(from_path):\n        with tarfile.open(from_path, \'r:gz\') as tar:\n            tar.extractall(path=to_path)\n    elif _is_tarxz(from_path) and PY3:\n        # .tar.xz archive only supported in Python 3.x\n        with tarfile.open(from_path, \'r:xz\') as tar:\n            tar.extractall(path=to_path)\n    elif _is_gzip(from_path):\n        to_path = os.path.join(\n            to_path,\n            os.path.splitext(os.path.basename(from_path))[0])\n        with open(to_path, ""wb"") as out_f, gzip.GzipFile(from_path) as zip_f:\n            out_f.write(zip_f.read())\n    elif _is_zip(from_path):\n        with zipfile.ZipFile(from_path, \'r\') as z:\n            z.extractall(to_path)\n    else:\n        raise ValueError(""Extraction of {} not supported"".format(from_path))\n\n    if remove_finished:\n        os.remove(from_path)\n\n\ndef download_and_extract_archive(url,\n                                 download_root,\n                                 extract_root=None,\n                                 filename=None,\n                                 md5=None,\n                                 remove_finished=False):\n    download_root = os.path.expanduser(download_root)\n    if extract_root is None:\n        extract_root = download_root\n    if not filename:\n        filename = os.path.basename(url)\n\n    download_url(url, download_root, filename, md5)\n\n    archive = os.path.join(download_root, filename)\n    print(""Extracting {} to {}"".format(archive, extract_root))\n    extract_archive(archive, extract_root, remove_finished)\n\n\ndef iterable_to_str(iterable):\n    return ""\'"" + ""\', \'"".join([str(item) for item in iterable]) + ""\'""\n\n\ndef verify_str_arg(value, arg=None, valid_values=None, custom_msg=None):\n    if not isinstance(value, torch._six.string_classes):\n        if arg is None:\n            msg = ""Expected type str, but got type {type}.""\n        else:\n            msg = ""Expected type str for argument {arg}, but got type {type}.""\n        msg = msg.format(type=type(value), arg=arg)\n        raise ValueError(msg)\n\n    if valid_values is None:\n        return value\n\n    if value not in valid_values:\n        if custom_msg is not None:\n            msg = custom_msg\n        else:\n            msg = (""Unknown value \'{value}\' for argument {arg}. ""\n                   ""Valid values are {{{valid_values}}}."")\n            msg = msg.format(value=value,\n                             arg=arg,\n                             valid_values=iterable_to_str(valid_values))\n        raise ValueError(msg)\n\n    return value\n'"
torch_mimicry/metrics/fid/__init__.py,0,b'from .fid_utils import *\n'
torch_mimicry/metrics/fid/fid_utils.py,0,"b'""""""\nHelper functions for calculating FID as adopted from the official FID code:\nhttps://github.com/kwotsin/dissertation/blob/master/eval/TTUR/fid.py\n""""""\nimport numpy as np\nfrom scipy import linalg\n\nfrom torch_mimicry.metrics.inception_model import inception_utils\n\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    """"""\n    Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n            \n    Stable version by Dougal J. Sutherland.\n\n    Args:\n        mu1 : Numpy array containing the activations of the pool_3 layer of the\n            inception net ( like returned by the function \'get_predictions\')\n            for generated samples.\n        mu2: The sample mean over activations of the pool_3 layer, precalcualted\n            on an representive data set.\n        sigma1 (ndarray): The covariance matrix over activations of the pool_3 layer for\n            generated samples.\n        sigma2: The covariance matrix over activations of the pool_3 layer,\n            precalcualted on an representive data set.\n\n    Returns:\n        np.float64: The Frechet Distance.\n    """"""\n    if mu1.shape != mu2.shape or sigma1.shape != sigma2.shape:\n        raise ValueError(\n            ""(mu1, sigma1) should have exactly the same shape as (mu2, sigma2).""\n        )\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    diff = mu1 - mu2\n\n    # Product might be almost singular\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        print(\n            ""WARNING: fid calculation produces singular product; adding {} to diagonal of cov estimates""\n            .format(eps))\n\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    # Numerical error might give slight imaginary component\n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError(""Imaginary component {}"".format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return diff.dot(diff) + np.trace(sigma1) + np.trace(\n        sigma2) - 2 * tr_covmean\n\n\ndef calculate_activation_statistics(images, sess, batch_size=50, verbose=True):\n    """"""\n    Calculation of the statistics used by the FID.\n\n    Args:\n        images (ndarray): Numpy array of shape (N, H, W, 3) and values in\n            the range [0, 255].\n        sess (Session): TensorFlow session object.\n        batch_size (int): Batch size for inference.\n        verbose (bool): If True, prints out logging information.\n\n    Returns:\n        ndarray: Mean of inception features from samples.\n        ndarray: Covariance of inception features from samples.\n    """"""\n    act = inception_utils.get_activations(images, sess, batch_size, verbose)\n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n\n    return mu, sigma\n'"
torch_mimicry/metrics/inception_model/__init__.py,0,b'from .inception_utils import *\n'
torch_mimicry/metrics/inception_model/inception_utils.py,0,"b'""""""\nCommon inception utils for computing metrics, as based on the FID helper code:\nhttps://github.com/kwotsin/dissertation/blob/master/eval/TTUR/fid.py\n""""""\nimport os\nimport pathlib\nimport tarfile\nimport time\nfrom urllib import request\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef _check_or_download_inception(inception_path):\n    """"""\n    Checks if the path to the inception file is valid, or downloads\n    the file if it is not present.\n\n    Args:\n        inception_path (str): Directory for storing the inception model.\n\n    Returns:\n        str: File path of the inception protobuf model.\n\n    """"""\n    # Build file path of model\n    inception_url = \'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\'\n    if inception_path is None:\n        inception_path = \'/tmp\'\n    inception_path = pathlib.Path(inception_path)\n    model_file = inception_path / \'classify_image_graph_def.pb\'\n\n    # Download model if required\n    if not model_file.exists():\n        print(""Downloading Inception model"")\n        fn, _ = request.urlretrieve(inception_url)\n\n        with tarfile.open(fn, mode=\'r\') as f:\n            f.extract(\'classify_image_graph_def.pb\', str(model_file.parent))\n\n    return str(model_file)\n\n\ndef _get_inception_layer(sess):\n    """"""\n    Prepares inception net for batched usage and returns pool_3 layer.\n\n    Args:\n        sess (Session): TensorFlow Session object.\n\n    Returns:\n        TensorFlow graph node representing inception model pool3 layer output.\n\n    """"""\n    # Get the output node\n    # layer_name = \'inception_model/pool_3:0\' # TODO: Remove when safe. TF2 syntax changes again.\n    layer_name = \'pool_3:0\'\n\n    pool3 = sess.graph.get_tensor_by_name(layer_name)\n\n    # Reshape to be batch size agnostic\n    ops = pool3.graph.get_operations()\n    for op_idx, op in enumerate(ops):\n        for o in op.outputs:\n            shape = o.get_shape()\n            if len(shape._dims) > 0:\n                try:\n                    shape = [s.value for s in shape]\n                except AttributeError:  # TF 2 uses None shape directly. No conversion needed.\n                    shape = shape\n\n                new_shape = []\n                for j, s in enumerate(shape):\n                    if s == 1 and j == 0:\n                        new_shape.append(None)\n                    else:\n                        new_shape.append(s)\n                o.__dict__[\'_shape_val\'] = tf.TensorShape(new_shape)\n\n    return pool3\n\n\ndef get_activations(images, sess, batch_size=50, verbose=True):\n    """"""\n    Calculates the activations of the pool_3 layer for all images.\n\n    Args:\n        images (ndarray): Numpy array of shape (N, C, H, W) with values ranging\n            in the range [0, 255].\n        sess (Session): TensorFlow Session object.\n        batch_size (int): The batch size to use for inference.\n        verbose (bool): If True, prints out logging data for batch inference.\n\n    Returns:\n        ndarray: Numpy array of shape (N, 2048) representing the pool3 features from the\n        inception model.\n\n    """"""\n    # Get output layer.\n    inception_layer = _get_inception_layer(sess)\n\n    # Inference variables\n    batch_size = min(batch_size, images.shape[0])\n    num_batches = images.shape[0] // batch_size\n\n    # Get features\n    pred_arr = np.empty((images.shape[0], 2048))\n    for i in range(num_batches):\n        start_time = time.time()\n\n        start = i * batch_size\n        end = start + batch_size\n        batch = images[start:end]\n        pred = sess.run(inception_layer, {\'ExpandDims:0\': batch})\n        # pred = sess.run(inception_layer,\n        #                 {\'inception_model/ExpandDims:0\': batch}) # TODO: Remove when safe. TF2 syntax changes again.\n        pred_arr[start:end] = pred.reshape(batch_size, -1)\n\n        if verbose:\n            print(""\\rINFO: Propagated batch %d/%d (%.4f sec/batch)"" \\\n                % (i+1, num_batches, time.time()-start_time), end="""", flush=True)\n\n    return pred_arr\n\n\ndef create_inception_graph(inception_path):\n    """"""\n    Creates a graph from saved GraphDef file.\n\n    Args:\n        inception_path (str): Directory for storing the inception model.\n\n    Returns:\n        None\n    """"""\n    if not os.path.exists(inception_path):\n        os.makedirs(inception_path)\n\n    # Get inception model file path\n    model_file = _check_or_download_inception(inception_path)\n\n    # Creates graph from saved graph_def.pb.\n    with tf.io.gfile.GFile(model_file, \'rb\') as f:\n        graph_def = tf.compat.v1.GraphDef()\n        graph_def.ParseFromString(f.read())\n        _ = tf.import_graph_def(graph_def, name=\'inception_model\')\n'"
torch_mimicry/metrics/inception_score/__init__.py,0,b'from .inception_score_utils import *\n'
torch_mimicry/metrics/inception_score/inception_score_utils.py,0,"b'""""""\nHelper functions for computing inception score, as based on:\nhttps://github.com/openai/improved-gan/tree/master/inception_score\n""""""\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom torch_mimicry.metrics.inception_model import inception_utils\n\n\ndef get_predictions(images, device=None, batch_size=50, print_every=20):\n    """"""\n    Get the output probabilities of images.\n\n    Args:\n        images (ndarray): Batch of images of shape (N, H, W, 3).\n        device (Device): Torch device object.\n        batch_size (int): Batch size for inference using inception model.\n        print_every (int): Prints logging variable every n batch inferences.\n\n    Returns:\n        ndarray: Batch of probabilities of equal size as number of images input.\n    """"""\n    if device and device.index is not None:\n        # Avoid unbounded memory usage\n        gpu_options = tf.GPUOptions(allow_growth=True,\n                                    per_process_gpu_memory_fraction=0.15,\n                                    visible_device_list=str(device.index))\n        config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n\n    else:\n        config = tf.compat.v1.ConfigProto(device_count={\'GPU\': 0})\n\n    # Inference variables\n    batch_size = min(batch_size, images.shape[0])\n    num_batches = images.shape[0] // batch_size\n\n    # Get predictions\n    preds = []\n    with tf.compat.v1.Session(config=config) as sess:\n        # Batch input preparation\n        inception_utils._get_inception_layer(sess)\n\n        # Define input/outputs of default graph.\n        pool3 = sess.graph.get_tensor_by_name(\'pool_3:0\')\n        w = sess.graph.get_operation_by_name(""softmax/logits/MatMul"").inputs[1]\n        # pool3 = sess.graph.get_tensor_by_name(\'inception_model/pool_3:0\') # TODO: Remove when safe. TF2 syntax changes again.\n        # w = sess.graph.get_operation_by_name(\n        #     ""inception_model/softmax/logits/MatMul"").inputs[1] # TODO: Remove when safe. TF2 syntax changes again.\n        logits = tf.matmul(tf.squeeze(pool3, [1, 2]), w)\n        softmax = tf.nn.softmax(logits)\n\n        # Predict images\n        start_time = time.time()\n        for i in range(num_batches):\n            batch = images[i * batch_size:(i + 1) * batch_size]\n\n            pred = sess.run(softmax, {\'ExpandDims:0\': batch})\n            # pred = sess.run(softmax, {\'inception_model/ExpandDims:0\': batch}) # TODO: Remove when safe. TF2 syntax changes again.\n            preds.append(pred)\n\n            if (i + 1) % min(print_every, num_batches) == 0:\n                end_time = time.time()\n                print(""INFO: Processed image {}/{}...({:.4f} sec/idx)"".format(\n                    (i + 1) * batch_size, images.shape[0],\n                    (end_time - start_time) / (print_every * batch_size)))\n                start_time = end_time\n\n    preds = np.concatenate(preds, 0)\n\n    return preds\n\n\ndef get_inception_score(images, splits=10, device=None):\n    """"""\n    Computes inception score according to official OpenAI implementation.\n\n    Args:\n        images (ndarray): Batch of images of shape (N, H, W, 3), which should have values\n            in the range [0, 255].\n        splits (int): Number of splits to use for computing IS.\n        device (Device): Torch device object to decide which GPU to use for TF session.\n\n    Returns:\n        tuple: Tuple of mean and standard deviation of the inception score computed.\n    """"""\n    if np.max(images[0] < 10) and np.max(images[0] < 0):\n        raise ValueError(""Images should have value ranging from 0 to 255."")\n\n    # Load graph and get probabilities\n    preds = get_predictions(images, device=device)\n\n    # Compute scores\n    N = preds.shape[0]\n    scores = []\n    for i in range(splits):\n        part = preds[(i * N // splits):((i + 1) * N // splits), :]\n        kl = part * (np.log(part) -\n                     np.log(np.expand_dims(np.mean(part, 0), 0)))\n        kl = np.mean(np.sum(kl, 1))\n        scores.append(np.exp(kl))\n\n    return float(np.mean(scores)), float(np.std(scores))\n'"
torch_mimicry/metrics/kid/__init__.py,0,b'from .kid_utils import *\n'
torch_mimicry/metrics/kid/kid_utils.py,0,"b'""""""\nHelper functions for computing FID, as based on:\nhttps://github.com/mbinkowski/MMD-GAN/blob/master/gan/compute_scores.py\n""""""\nimport numpy as np\nfrom sklearn.metrics.pairwise import polynomial_kernel\n\n\ndef polynomial_mmd(codes_g, codes_r, degree=3, gamma=None, coef0=1):\n    """"""\n    Compute MMD between two sets of features.\n\n    Polynomial kernel given by:\n    K(X, Y) = (gamma <X, Y> + coef0)^degree\n\n    Args:\n        codes_g (ndarray): Set of features from 1st distribution.\n        codes_r (ndarray): Set of features from 2nd distribution.\n        degree (int): Power of the kernel.\n        gamma (float): Scaling factor of dot product.\n        coeff0 (float): Constant factor of kernel.\n\n    Returns:\n        np.float64: Scalar MMD score between features of 2 distributions.\n    """"""\n    X = codes_g\n    Y = codes_r\n\n    K_XX = polynomial_kernel(X, degree=degree, gamma=gamma, coef0=coef0)\n    K_YY = polynomial_kernel(Y, degree=degree, gamma=gamma, coef0=coef0)\n    K_XY = polynomial_kernel(X, Y, degree=degree, gamma=gamma, coef0=coef0)\n\n    return _compute_mmd2(K_XX, K_XY, K_YY)\n\n\ndef polynomial_mmd_averages(codes_g,\n                            codes_r,\n                            n_subsets=50,\n                            subset_size=1000,\n                            **kernel_args):\n    """"""\n    Computes average MMD between two set of features using n_subsets,\n    each of which is of subset_size.\n\n    Args:\n        codes_g (ndarray): Set of features from 1st distribution.\n        codes_r (ndarray): Set of features from 2nd distribution.\n        n_subsets (int): Number of subsets to compute averages.\n        subset_size (int): Size of each subset of features to choose.\n\n    Returns:\n        list: List of n_subsets MMD scores.\n    """"""\n    m = min(codes_g.shape[0], codes_r.shape[0])\n    mmds = np.zeros(n_subsets)\n\n    # Account for inordinately small subset sizes\n    n_subsets = min(m, n_subsets)\n    subset_size = min(subset_size, m // n_subsets)\n\n    for i in range(n_subsets):\n        g = codes_g[np.random.choice(len(codes_g), subset_size, replace=False)]\n        r = codes_r[np.random.choice(len(codes_r), subset_size, replace=False)]\n        o = polynomial_mmd(g, r, **kernel_args)\n        mmds[i] = o\n\n    return mmds\n\n\ndef _sqn(arr):\n    flat = np.ravel(arr)\n    return flat.dot(flat)\n\n\ndef _compute_mmd2(K_XX, K_XY, K_YY, unit_diagonal=False, mmd_est=\'unbiased\'):\n    """"""\n    Based on https://github.com/dougalsutherland/opt-mmd/blob/master/two_sample/mmd.py\n    but changed to not compute the full kernel matrix at once.\n    """"""\n    if mmd_est not in [\'unbiased\', \'u-statistic\']:\n        raise ValueError(\n            ""mmd_est should be one of [unbiased\', \'u-statistic] but got {}."".\n            format(mmd_est))\n\n    m = K_XX.shape[0]\n    if K_XX.shape != (m, m):\n        raise ValueError(""K_XX shape should be {} but got {} instead."".format(\n            (m, m), K_XX.shape))\n\n    if K_XY.shape != (m, m):\n        raise ValueError(""K_XX shape should be {} but got {} instead."".format(\n            (m, m), K_XY.shape))\n\n    if K_YY.shape != (m, m):\n        raise ValueError(""K_XX shape should be {} but got {} instead."".format(\n            (m, m), K_YY.shape))\n\n    # Get the various sums of kernels that we\'ll use\n    # Kts drop the diagonal, but we don\'t need to compute them explicitly\n    if unit_diagonal:\n        diag_X = diag_Y = 1\n        sum_diag_X = sum_diag_Y = m\n        sum_diag2_X = sum_diag2_Y = m\n    else:\n        diag_X = np.diagonal(K_XX)\n        diag_Y = np.diagonal(K_YY)\n\n        sum_diag_X = diag_X.sum()\n        sum_diag_Y = diag_Y.sum()\n\n        sum_diag2_X = _sqn(diag_X)\n        sum_diag2_Y = _sqn(diag_Y)\n\n    Kt_XX_sums = K_XX.sum(axis=1) - diag_X\n    Kt_YY_sums = K_YY.sum(axis=1) - diag_Y\n    K_XY_sums_0 = K_XY.sum(axis=0)\n    K_XY_sums_1 = K_XY.sum(axis=1)\n\n    Kt_XX_sum = Kt_XX_sums.sum()\n    Kt_YY_sum = Kt_YY_sums.sum()\n    K_XY_sum = K_XY_sums_0.sum()\n\n    if mmd_est == \'biased\':\n        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m) + (Kt_YY_sum + sum_diag_Y) /\n                (m * m) - 2 * K_XY_sum / (m * m))\n    else:\n        mmd2 = (Kt_XX_sum + Kt_YY_sum) / (m * (m - 1))\n\n        if mmd_est == \'unbiased\':\n            mmd2 -= 2 * K_XY_sum / (m * m)\n        else:\n            mmd2 -= 2 * (K_XY_sum - np.trace(K_XY)) / (m * (m - 1))\n\n    return mmd2\n'"
torch_mimicry/nets/basemodel/__init__.py,0,b'from .basemodel import *\r\n'
torch_mimicry/nets/basemodel/basemodel.py,4,"b'""""""\nImplementation of BaseModel.\n""""""\nimport os\nfrom abc import ABC, abstractmethod\n\nimport torch\nimport torch.nn as nn\n\n\nclass BaseModel(nn.Module, ABC):\n    r""""""\n    BaseModel with basic functionalities for checkpointing and restoration.\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def forward(self, x):\n        pass\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def restore_checkpoint(self, ckpt_file, optimizer=None):\n        r""""""\n        Restores checkpoint from a pth file and restores optimizer state.\n\n        Args:\n            ckpt_file (str): A PyTorch pth file containing model weights.\n            optimizer (Optimizer): A vanilla optimizer to have its state restored from.\n\n        Returns:\n            int: Global step variable where the model was last checkpointed.\n        """"""\n        if not ckpt_file:\n            raise ValueError(""No checkpoint file to be restored."")\n\n        try:\n            ckpt_dict = torch.load(ckpt_file)\n        except RuntimeError:\n            ckpt_dict = torch.load(ckpt_file,\n                                   map_location=lambda storage, loc: storage)\n\n        # Restore model weights\n        self.load_state_dict(ckpt_dict[\'model_state_dict\'])\n\n        # Restore optimizer status if existing. Evaluation doesn\'t need this\n        if optimizer:\n            optimizer.load_state_dict(ckpt_dict[\'optimizer_state_dict\'])\n\n        # Return global step\n        return ckpt_dict[\'global_step\']\n\n    def save_checkpoint(self,\n                        directory,\n                        global_step,\n                        optimizer=None,\n                        name=None):\n        r""""""\n        Saves checkpoint at a certain global step during training. Optimizer state\n        is also saved together.\n\n        Args:\n            directory (str): Path to save checkpoint to.\n            global_step (int): The global step variable during training.\n            optimizer (Optimizer): Optimizer state to be saved concurrently.\n            name (str): The name to save the checkpoint file as.\n\n        Returns:\n            None\n        """"""\n        # Create directory to save to\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Build checkpoint dict to save.\n        ckpt_dict = {\n            \'model_state_dict\':\n            self.state_dict(),\n            \'optimizer_state_dict\':\n            optimizer.state_dict() if optimizer is not None else None,\n            \'global_step\':\n            global_step\n        }\n\n        # Save the file with specific name\n        if name is None:\n            name = ""{}_{}_steps.pth"".format(\n                os.path.basename(directory),  # netD or netG\n                global_step)\n\n        torch.save(ckpt_dict, os.path.join(directory, name))\n\n    def count_params(self):\n        r""""""\n        Computes the number of parameters in this model.\n\n        Args: None\n\n        Returns:\n            int: Total number of weight parameters for this model.\n            int: Total number of trainable parameters for this model.\n\n        """"""\n        num_total_params = sum(p.numel() for p in self.parameters())\n        num_trainable_params = sum(p.numel() for p in self.parameters()\n                                   if p.requires_grad)\n\n        return num_total_params, num_trainable_params\n'"
torch_mimicry/nets/cgan_pd/__init__.py,0,b'from .cgan_pd_32 import *\nfrom .cgan_pd_base import *'
torch_mimicry/nets/cgan_pd/cgan_pd_128.py,5,"b'""""""\nImplementation of cGAN-PD for image size 128.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.cgan_pd import cgan_pd_base\nfrom torch_mimicry.modules.layers import SNLinear, SNEmbedding\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass CGANPDGenerator128(cgan_pd_base.CGANPDBaseGenerator):\n    r""""""\n    ResNet backbone generator for cGAN-PD,\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.    \n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self,\n                 num_classes,\n                 nz=128,\n                 ngf=1024,\n                 bottom_width=4,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         num_classes=num_classes,\n                         **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf,\n                             self.ngf,\n                             upsample=True,\n                             num_classes=self.num_classes)\n        self.block3 = GBlock(self.ngf,\n                             self.ngf >> 1,\n                             upsample=True,\n                             num_classes=self.num_classes)\n        self.block4 = GBlock(self.ngf >> 1,\n                             self.ngf >> 2,\n                             upsample=True,\n                             num_classes=self.num_classes)\n        self.block5 = GBlock(self.ngf >> 2,\n                             self.ngf >> 3,\n                             upsample=True,\n                             num_classes=self.num_classes)\n        self.block6 = GBlock(self.ngf >> 3,\n                             self.ngf >> 4,\n                             upsample=True,\n                             num_classes=self.num_classes)\n        self.b7 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c7 = nn.Conv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c7.weight.data, 1.0)\n\n    def forward(self, x, y=None):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images, also\n        conditioning the batch norm with labels of the images to be produced.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n            y (Tensor): A batch of labels of shape (N,) for conditional batch norm.\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        if y is None:\n            y = torch.randint(low=0,\n                              high=self.num_classes,\n                              size=(x.shape[0], ),\n                              device=x.device)\n\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h, y)\n        h = self.block3(h, y)\n        h = self.block4(h, y)\n        h = self.block5(h, y)\n        h = self.block6(h, y)\n        h = self.b7(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c7(h))\n\n        return h\n\n\nclass CGANPDDiscriminator128(cgan_pd_base.CGANPDBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for cGAN-PD.\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.        \n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.                \n    """"""\n    def __init__(self, num_classes, ndf=128, **kwargs):\n        super().__init__(ndf=ndf, num_classes=num_classes, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4)\n        self.block2 = DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True)\n        self.block3 = DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True)\n        self.block4 = DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True)\n        self.block5 = DBlock(self.ndf >> 1, self.ndf, downsample=True)\n        self.block6 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.l7 = SNLinear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Produce label vector from trained embedding\n        self.l_y = SNEmbedding(num_embeddings=self.num_classes,\n                               embedding_dim=self.ndf)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l7.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.l_y.weight.data, 1.0)\n\n        self.activation = nn.ReLU(True)\n\n    def forward(self, x, y=None):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n        Further projects labels to condition on the output logit score.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n            y (Tensor): A batch of labels of shape (N,).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.block6(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l7(h)\n\n        # Add the projection loss\n        w_y = self.l_y(y)\n        output += torch.sum((w_y * h), dim=1, keepdim=True)\n\n        return output\n'"
torch_mimicry/nets/cgan_pd/cgan_pd_32.py,5,"b'""""""\nImplementation of cGAN-PD for image size 32.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.cgan_pd import cgan_pd_base\nfrom torch_mimicry.modules import SNLinear, SNEmbedding\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass CGANPDGenerator32(cgan_pd_base.CGANPDBaseGenerator):\n    r""""""\n    ResNet backbone generator for cGAN-PD,\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.    \n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, num_classes, bottom_width=4, nz=128, ngf=256, **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         num_classes=num_classes,\n                         **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf,\n                             self.ngf,\n                             upsample=True,\n                             num_classes=self.num_classes)\n        self.block3 = GBlock(self.ngf,\n                             self.ngf,\n                             upsample=True,\n                             num_classes=self.num_classes)\n        self.block4 = GBlock(self.ngf,\n                             self.ngf,\n                             upsample=True,\n                             num_classes=self.num_classes)\n        self.b5 = nn.BatchNorm2d(self.ngf)\n        self.c5 = nn.Conv2d(self.ngf, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x, y=None):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images, also\n        conditioning the batch norm with labels of the images to be produced.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n            y (Tensor): A batch of labels of shape (N,) for conditional batch norm.\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        if y is None:\n            y = torch.randint(low=0,\n                              high=self.num_classes,\n                              size=(x.shape[0], ),\n                              device=x.device)\n\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h, y)\n        h = self.block3(h, y)\n        h = self.block4(h, y)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n\nclass CGANPDDiscriminator32(cgan_pd_base.CGANPDBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for cGAN-PD.\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.        \n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.                \n    """"""\n    def __init__(self, num_classes, ndf=128, **kwargs):\n        super().__init__(ndf=ndf, num_classes=num_classes, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf)\n        self.block2 = DBlock(self.ndf, self.ndf, downsample=True)\n        self.block3 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.block4 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.l5 = SNLinear(self.ndf, 1)\n\n        # Produce label vector from trained embedding\n        self.l_y = SNEmbedding(num_embeddings=self.num_classes,\n                               embedding_dim=self.ndf)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l5.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.l_y.weight.data, 1.0)\n\n        self.activation = nn.ReLU(True)\n\n    def forward(self, x, y=None):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n        Further projects labels to condition on the output logit score.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n            y (Tensor): A batch of labels of shape (N,).\n\n        Returns:\n            output (Tensor): A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l5(h)\n\n        # Add the projection loss\n        w_y = self.l_y(y)\n        output += torch.sum((w_y * h), dim=1, keepdim=True)\n\n        return output\n'"
torch_mimicry/nets/cgan_pd/cgan_pd_base.py,0,"b'""""""\nBase class definition of cGAN-PD.\n""""""\n\nfrom torch_mimicry.nets.gan import cgan\n\n\nclass CGANPDBaseGenerator(cgan.BaseConditionalGenerator):\n    r""""""\n    ResNet backbone generator for cGAN-PD,\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.    \n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self,\n                 num_classes,\n                 bottom_width,\n                 nz,\n                 ngf,\n                 loss_type=\'hinge\',\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         num_classes=num_classes,\n                         **kwargs)\n\n\nclass CGANPDBaseDiscriminator(cgan.BaseConditionalDiscriminator):\n    r""""""\n    ResNet backbone discriminator for cGAN-PD.\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.        \n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.                \n    """"""\n    def __init__(self, num_classes, ndf, loss_type=\'hinge\', **kwargs):\n        super().__init__(ndf=ndf,\n                         loss_type=loss_type,\n                         num_classes=num_classes,\n                         **kwargs)\n'"
torch_mimicry/nets/dcgan/__init__.py,0,b'from .dcgan_128 import *\nfrom .dcgan_32 import *\nfrom .dcgan_48 import *\nfrom .dcgan_64 import *\nfrom .dcgan_base import *\nfrom .dcgan_cifar import *\n'
torch_mimicry/nets/dcgan/dcgan_128.py,3,"b'""""""\nImplementation of DCGAN for image size 128.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.dcgan import dcgan_base\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass DCGANGenerator128(dcgan_base.DCGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for ResNet DCGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, nz=128, ngf=1024, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block4 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block5 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.block6 = GBlock(self.ngf >> 3, self.ngf >> 4, upsample=True)\n        self.b7 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c7 = nn.Conv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c7.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.block6(h)\n        h = self.b7(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c7(h))\n\n        return h\n\n\nclass DCGANDiscriminator128(dcgan_base.DCGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for ResNet DCGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4, spectral_norm=False)\n        self.block2 = DBlock(self.ndf >> 4,\n                             self.ndf >> 3,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block3 = DBlock(self.ndf >> 3,\n                             self.ndf >> 2,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block4 = DBlock(self.ndf >> 2,\n                             self.ndf >> 1,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block5 = DBlock(self.ndf >> 1,\n                             self.ndf,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block6 = DBlock(self.ndf,\n                             self.ndf,\n                             downsample=False,\n                             spectral_norm=False)\n        self.l7 = nn.Linear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l7.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.block6(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l7(h)\n\n        return output\n'"
torch_mimicry/nets/dcgan/dcgan_32.py,3,"b'""""""\nImplementation of DCGAN for image size 32.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.dcgan import dcgan_base\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass DCGANGenerator32(dcgan_base.DCGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for ResNet DCGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.\n    """"""\n    def __init__(self, nz=128, ngf=256, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block4 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf)\n        self.c5 = nn.Conv2d(self.ngf, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))  # tanh\n\n        return h\n\n\nclass DCGANDiscriminator32(dcgan_base.DCGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for ResNet DCGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n    """"""\n    def __init__(self, ndf=128, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf, spectral_norm=False)\n        self.block2 = DBlock(self.ndf,\n                             self.ndf,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block3 = DBlock(self.ndf,\n                             self.ndf,\n                             downsample=False,\n                             spectral_norm=False)\n        self.block4 = DBlock(self.ndf,\n                             self.ndf,\n                             downsample=False,\n                             spectral_norm=False)\n        self.l5 = nn.Linear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.activation(h)\n\n        # Global average pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l5(h)\n\n        return output\n'"
torch_mimicry/nets/dcgan/dcgan_48.py,3,"b'""""""\nImplementation of DCGAN for image size 48.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.dcgan import dcgan_base\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass DCGANGenerator48(dcgan_base.DCGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for ResNet DCGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.\n    """"""\n    def __init__(self, nz=128, ngf=512, bottom_width=6, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block3 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block4 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf >> 3)\n        self.c5 = nn.Conv2d(self.ngf >> 3, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n\nclass DCGANDiscriminator48(dcgan_base.DCGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for ResNet DCGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4, spectral_norm=False)\n        self.block2 = DBlock(self.ndf >> 4,\n                             self.ndf >> 3,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block3 = DBlock(self.ndf >> 3,\n                             self.ndf >> 2,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block4 = DBlock(self.ndf >> 2,\n                             self.ndf >> 1,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block5 = DBlock(self.ndf >> 1,\n                             self.ndf,\n                             downsample=False,\n                             spectral_norm=False)\n        self.l5 = nn.Linear(self.ndf, 1)\n\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.activation(h)\n\n        # Global average pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l5(h)\n\n        return output\n'"
torch_mimicry/nets/dcgan/dcgan_64.py,3,"b'""""""\nImplementation of DCGAN for image size 64.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.dcgan import dcgan_base\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass DCGANGenerator64(dcgan_base.DCGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for ResNet DCGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, nz=128, ngf=1024, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block3 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block4 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.block5 = GBlock(self.ngf >> 3, self.ngf >> 4, upsample=True)\n        self.b6 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c6 = nn.Conv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c6.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.b6(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c6(h))\n\n        return h\n\n\nclass DCGANDiscriminator64(dcgan_base.DCGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for ResNet DCGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4, spectral_norm=False)\n        self.block2 = DBlock(self.ndf >> 4,\n                             self.ndf >> 3,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block3 = DBlock(self.ndf >> 3,\n                             self.ndf >> 2,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block4 = DBlock(self.ndf >> 2,\n                             self.ndf >> 1,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block5 = DBlock(self.ndf >> 1,\n                             self.ndf,\n                             downsample=True,\n                             spectral_norm=False)\n        self.l6 = nn.Linear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l6.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.activation(h)\n\n        # Global average pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l6(h)\n\n        return output\n'"
torch_mimicry/nets/dcgan/dcgan_base.py,0,"b'""""""\nBase class definition of DCGAN.\n""""""\nfrom torch_mimicry.nets.gan import gan\n\n\nclass DCGANBaseGenerator(gan.BaseGenerator):\n    r""""""\n    ResNet backbone generator for ResNet DCGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, nz, ngf, bottom_width, loss_type=\'ns\', **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         **kwargs)\n\n\nclass DCGANBaseDiscriminator(gan.BaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for ResNet DCGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, ndf, loss_type=\'ns\', **kwargs):\n        super().__init__(ndf=ndf, loss_type=loss_type, **kwargs)\n'"
torch_mimicry/nets/dcgan/dcgan_cifar.py,3,"b'""""""\nImplementation of DCGAN based on Kurach et al. specifically for CIFAR-10. \nThe main difference with dcgan_32 is in using sigmoid \nas the final activation for the generator instead of tanh.\n\nTo reproduce scores, CIFAR-10 images should not be normalized from -1 to 1, and should\ninstead have values from 0 to 1, which is the default when loading images as np arrays.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.dcgan import dcgan_base\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass DCGANGeneratorCIFAR(dcgan_base.DCGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for ResNet DCGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, nz=128, ngf=256, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block4 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf)\n        self.c5 = nn.Conv2d(self.ngf, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.sigmoid(self.c5(h))\n\n        return h\n\n\nclass DCGANDiscriminatorCIFAR(dcgan_base.DCGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for ResNet DCGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, ndf=128, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf, spectral_norm=False)\n        self.block2 = DBlock(self.ndf,\n                             self.ndf,\n                             downsample=True,\n                             spectral_norm=False)\n        self.block3 = DBlock(self.ndf,\n                             self.ndf,\n                             downsample=False,\n                             spectral_norm=False)\n        self.block4 = DBlock(self.ndf,\n                             self.ndf,\n                             downsample=False,\n                             spectral_norm=False)\n        self.l5 = nn.Linear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.activation(h)\n\n        # Global mean pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l5(h)\n\n        return output\n'"
torch_mimicry/nets/gan/__init__.py,0,b'from .cgan import *\nfrom .gan import *'
torch_mimicry/nets/gan/cgan.py,8,"b'""""""\nImplementation of Base GAN models for a generic conditional GAN.\n""""""\nimport torch\n\nfrom torch_mimicry.nets.gan import gan\n\n\nclass BaseConditionalGenerator(gan.BaseGenerator):\n    r""""""\n    Base class for a generic conditional generator model.\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.    \n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, num_classes, nz, ngf, bottom_width, loss_type,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         **kwargs)\n        self.num_classes = num_classes\n\n    def generate_images(self, num_images, c=None, device=None):\n        r""""""\n        Generate images with possibility for conditioning on a fixed class.\n\n        Args:\n            num_images (int): The number of images to generate.\n            c (int): The class of images to generate. If None, generates random images.\n            device (int): The device to send the generated images to.\n\n        Returns:\n            tuple: Batch of generated images and their corresponding labels.\n        """"""\n        if device is None:\n            device = self.device\n\n        if c is not None and c >= self.num_classes:\n            raise ValueError(\n                ""Input class to generate must be in the range [0, {})"".format(\n                    self.num_classes))\n\n        if c is None:\n            fake_class_labels = torch.randint(low=0,\n                                              high=self.num_classes,\n                                              size=(num_images, ),\n                                              device=device)\n\n        else:\n            fake_class_labels = torch.randint(low=c,\n                                              high=c + 1,\n                                              size=(num_images, ),\n                                              device=device)\n\n        noise = torch.randn((num_images, self.nz), device=device)\n        fake_images = self.forward(noise, fake_class_labels)\n\n        return fake_images\n\n    def generate_images_with_labels(self, num_images, c=None, device=None):\n        r""""""\n        Generate images with possibility for conditioning on a fixed class.\n        Additionally returns labels.\n\n        Args:\n            num_images (int): The number of images to generate.\n            c (int): The class of images to generate. If None, generates random images.\n            device (int): The device to send the generated images to.\n\n        Returns:\n            tuple: Batch of generated images and their corresponding labels.\n        """"""\n        if device is None:\n            device = self.device\n\n        if c is not None and c >= self.num_classes:\n            raise ValueError(\n                ""Input class to generate must be in the range [0, {})"".format(\n                    self.num_classes))\n\n        if c is None:\n            fake_class_labels = torch.randint(low=0,\n                                              high=self.num_classes,\n                                              size=(num_images, ),\n                                              device=device)\n\n        else:\n            fake_class_labels = torch.randint(low=c,\n                                              high=c + 1,\n                                              size=(num_images, ),\n                                              device=device)\n\n        noise = torch.randn((num_images, self.nz), device=device)\n        fake_images = self.forward(noise, fake_class_labels)\n\n        return fake_images, fake_class_labels\n\n    def train_step(self,\n                   real_batch,\n                   netD,\n                   optG,\n                   log_data,\n                   device=None,\n                   global_step=None,\n                   **kwargs):\n        r""""""\n        Takes one training step for G.\n\n        Args:\n            real_batch (Tensor): A batch of real images of shape (N, C, H, W).\n                Used for obtaining current batch size.\n            netD (nn.Module): Discriminator model for obtaining losses.\n            optG (Optimizer): Optimizer for updating generator\'s parameters.\n            log_data (MetricLog): A dict mapping name to values for logging uses.\n            device (torch.device): Device to use for running the model.\n            global_step (int): Variable to sync training, logging and checkpointing.\n                Useful for dynamic changes to model amidst training.\n\n        Returns:\n            MetricLog: Returns MetricLog object containing updated logging variables after 1 training step.\n\n        """"""\n        self.zero_grad()\n\n        # Get only batch size from real batch\n        batch_size = real_batch[0].shape[0]\n\n        # Produce fake images and labels\n        fake_images, fake_class_labels = self.generate_images_with_labels(\n            num_images=batch_size, device=device)\n\n        # Compute output logit of D thinking image real\n        output = netD(fake_images, fake_class_labels)\n\n        # Compute loss and backprop\n        errG = self.compute_gan_loss(output)\n\n        # Backprop and update gradients\n        errG.backward()\n        optG.step()\n\n        # Log statistics\n        log_data.add_metric(\'errG\', errG, group=\'loss\')\n\n        return log_data\n\n\nclass BaseConditionalDiscriminator(gan.BaseDiscriminator):\n    r""""""\n    Base class for a generic conditional discriminator model.\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.        \n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.                \n    """"""\n    def __init__(self, num_classes, ndf, loss_type, **kwargs):\n        super().__init__(ndf=ndf, loss_type=loss_type, **kwargs)\n        self.num_classes = num_classes\n\n    def train_step(self,\n                   real_batch,\n                   netG,\n                   optD,\n                   log_data,\n                   device=None,\n                   global_step=None,\n                   **kwargs):\n        r""""""\n        Takes one training step for D.\n\n        Args:\n            real_batch (Tensor): A batch of real images of shape (N, C, H, W).\n            loss_type (str): Name of loss to use for GAN loss.\n            netG (nn.Module): Generator model for obtaining fake images.\n            optD (Optimizer): Optimizer for updating discriminator\'s parameters.\n            device (torch.device): Device to use for running the model.\n            log_data (MetricLog): A dict mapping name to values for logging uses.\n            global_step (int): Variable to sync training, logging and checkpointing.\n                Useful for dynamic changes to model amidst training.\n\n        Returns:\n            MetricLog: Returns MetricLog object containing updated logging variables after 1 training step.\n        """"""\n        self.zero_grad()\n\n        real_images, real_class_labels = real_batch\n        batch_size = real_images.shape[0]  # Match batch sizes for last iter\n\n        # Produce logits for real images\n        output_real = self.forward(real_images, real_class_labels)\n\n        # Produce fake images and labels\n        fake_images, fake_class_labels = netG.generate_images_with_labels(\n            num_images=batch_size, device=device)\n        fake_images, fake_class_labels = fake_images.detach(\n        ), fake_class_labels.detach()\n\n        # Produce logits for fake images\n        output_fake = self.forward(fake_images, fake_class_labels)\n\n        # Compute loss for D\n        errD = self.compute_gan_loss(output_real=output_real,\n                                     output_fake=output_fake)\n\n        # Backprop and update gradients\n        errD.backward()\n        optD.step()\n\n        # Compute probabilities\n        D_x, D_Gz = self.compute_probs(output_real=output_real,\n                                       output_fake=output_fake)\n\n        # Log statistics for D once out of loop\n        log_data.add_metric(\'errD\', errD, group=\'loss\')\n        log_data.add_metric(\'D(x)\', D_x, group=\'prob\')\n        log_data.add_metric(\'D(G(z))\', D_Gz, group=\'prob\')\n\n        return log_data\n'"
torch_mimicry/nets/gan/gan.py,6,"b'""""""\nImplementation of Base GAN models.\n""""""\nimport torch\n\nfrom torch_mimicry.nets.basemodel import basemodel\nfrom torch_mimicry.modules import losses\n\n\nclass BaseGenerator(basemodel.BaseModel):\n    r""""""\n    Base class for a generic unconditional generator model.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.\n    """"""\n    def __init__(self, nz, ngf, bottom_width, loss_type, **kwargs):\n        super().__init__(**kwargs)\n        self.nz = nz\n        self.ngf = ngf\n        self.bottom_width = bottom_width\n        self.loss_type = loss_type\n\n    def generate_images(self, num_images, device=None):\n        r""""""\n        Generates num_images randomly.\n\n        Args:\n            num_images (int): Number of images to generate\n            device (torch.device): Device to send images to.\n\n        Returns:\n            Tensor: A batch of generated images.\n        """"""\n        if device is None:\n            device = self.device\n\n        noise = torch.randn((num_images, self.nz), device=device)\n        fake_images = self.forward(noise)\n\n        return fake_images\n\n    def compute_gan_loss(self, output):\n        r""""""\n        Computes GAN loss for generator.\n\n        Args:\n            output (Tensor): A batch of output logits from the discriminator of shape (N, 1).\n\n        Returns:\n            Tensor: A batch of GAN losses for the generator.\n        """"""\n        # Compute loss and backprop\n        if self.loss_type == ""gan"":\n            errG = losses.minimax_loss_gen(output)\n\n        elif self.loss_type == ""ns"":\n            errG = losses.ns_loss_gen(output)\n\n        elif self.loss_type == ""hinge"":\n            errG = losses.hinge_loss_gen(output)\n\n        elif self.loss_type == ""wasserstein"":\n            errG = losses.wasserstein_loss_gen(output)\n\n        else:\n            raise ValueError(""Invalid loss_type {} selected."".format(\n                self.loss_type))\n\n        return errG\n\n    def train_step(self,\n                   real_batch,\n                   netD,\n                   optG,\n                   log_data,\n                   device=None,\n                   global_step=None,\n                   **kwargs):\n        r""""""\n        Takes one training step for G.\n\n        Args:\n            real_batch (Tensor): A batch of real images of shape (N, C, H, W).\n                Used for obtaining current batch size.\n            netD (nn.Module): Discriminator model for obtaining losses.\n            optG (Optimizer): Optimizer for updating generator\'s parameters.\n            log_data (dict): A dict mapping name to values for logging uses.\n            device (torch.device): Device to use for running the model.\n            global_step (int): Variable to sync training, logging and checkpointing.\n                Useful for dynamic changes to model amidst training.\n\n        Returns:\n            Returns MetricLog object containing updated logging variables after 1 training step.\n\n        """"""\n        self.zero_grad()\n\n        # Get only batch size from real batch\n        batch_size = real_batch[0].shape[0]\n\n        # Produce fake images\n        fake_images = self.generate_images(num_images=batch_size,\n                                           device=device)\n\n        # Compute output logit of D thinking image real\n        output = netD(fake_images)\n\n        # Compute loss\n        errG = self.compute_gan_loss(output=output)\n\n        # Backprop and update gradients\n        errG.backward()\n        optG.step()\n\n        # Log statistics\n        log_data.add_metric(\'errG\', errG, group=\'loss\')\n\n        return log_data\n\n\nclass BaseDiscriminator(basemodel.BaseModel):\n    r""""""\n    Base class for a generic unconditional discriminator model.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n    """"""\n    def __init__(self, ndf, loss_type, **kwargs):\n        super().__init__(**kwargs)\n        self.ndf = ndf\n        self.loss_type = loss_type\n\n    def compute_gan_loss(self, output_real, output_fake):\n        r""""""\n        Computes GAN loss for discriminator.\n\n        Args:\n            output_real (Tensor): A batch of output logits of shape (N, 1) from real images.\n            output_fake (Tensor): A batch of output logits of shape (N, 1) from fake images.\n\n        Returns:\n            errD (Tensor): A batch of GAN losses for the discriminator.\n        """"""\n        # Compute loss for D\n        if self.loss_type == ""gan"" or self.loss_type == ""ns"":\n            errD = losses.minimax_loss_dis(output_fake=output_fake,\n                                           output_real=output_real)\n\n        elif self.loss_type == ""hinge"":\n            errD = losses.hinge_loss_dis(output_fake=output_fake,\n                                         output_real=output_real)\n\n        elif self.loss_type == ""wasserstein"":\n            errD = losses.wasserstein_loss_dis(output_fake=output_fake,\n                                               output_real=output_real)\n\n        else:\n            raise ValueError(""Invalid loss_type selected."")\n\n        return errD\n\n    def compute_probs(self, output_real, output_fake):\n        r""""""\n        Computes probabilities from real/fake images logits.\n\n        Args:\n            output_real (Tensor): A batch of output logits of shape (N, 1) from real images.\n            output_fake (Tensor): A batch of output logits of shape (N, 1) from fake images.\n\n        Returns:\n            tuple: Average probabilities of real/fake image considered as real for the batch.\n        """"""\n        D_x = torch.sigmoid(output_real).mean().item()\n        D_Gz = torch.sigmoid(output_fake).mean().item()\n\n        return D_x, D_Gz\n\n    def train_step(self,\n                   real_batch,\n                   netG,\n                   optD,\n                   log_data,\n                   device=None,\n                   global_step=None,\n                   **kwargs):\n        r""""""\n        Takes one training step for D.\n\n        Args:\n            real_batch (Tensor): A batch of real images of shape (N, C, H, W).\n            loss_type (str): Name of loss to use for GAN loss.\n            netG (nn.Module): Generator model for obtaining fake images.\n            optD (Optimizer): Optimizer for updating discriminator\'s parameters.\n            device (torch.device): Device to use for running the model.\n            log_data (dict): A dict mapping name to values for logging uses.\n            global_step (int): Variable to sync training, logging and checkpointing.\n                Useful for dynamic changes to model amidst training.\n\n        Returns:\n            MetricLog: Returns MetricLog object containing updated logging variables after 1 training step.\n        """"""\n        self.zero_grad()\n        real_images, real_labels = real_batch\n        batch_size = real_images.shape[0]  # Match batch sizes for last iter\n\n        # Produce logits for real images\n        output_real = self.forward(real_images)\n\n        # Produce fake images\n        fake_images = netG.generate_images(num_images=batch_size,\n                                           device=device).detach()\n\n        # Produce logits for fake images\n        output_fake = self.forward(fake_images)\n\n        # Compute loss for D\n        errD = self.compute_gan_loss(output_real=output_real,\n                                     output_fake=output_fake)\n\n        # Backprop and update gradients\n        errD.backward()\n        optD.step()\n\n        # Compute probabilities\n        D_x, D_Gz = self.compute_probs(output_real=output_real,\n                                       output_fake=output_fake)\n\n        # Log statistics for D once out of loop\n        log_data.add_metric(\'errD\', errD.item(), group=\'loss\')\n        log_data.add_metric(\'D(x)\', D_x, group=\'prob\')\n        log_data.add_metric(\'D(G(z))\', D_Gz, group=\'prob\')\n\n        return log_data\n'"
torch_mimicry/nets/infomax_gan/__init__.py,0,b'from .infomax_gan_128 import *\nfrom .infomax_gan_32 import *\nfrom .infomax_gan_48 import *\nfrom .infomax_gan_64 import *\nfrom .infomax_gan_base import *\n'
torch_mimicry/nets/infomax_gan/infomax_gan_128.py,3,"b'""""""\nImplementation of InfoMax-GAN for image size 128.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.infomax_gan import infomax_gan_base\nfrom torch_mimicry.modules.layers import SNConv2d, SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass InfoMaxGANGenerator128(infomax_gan_base.InfoMaxGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for InfoMax-GAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n        infomax_loss_scale (float): The alpha parameter used for scaling the generator infomax loss.\n    """"""\n    def __init__(self, nz=128, ngf=1024, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block4 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block5 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.block6 = GBlock(self.ngf >> 3, self.ngf >> 4, upsample=True)\n        self.b7 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c7 = nn.Conv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c7.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.block6(h)\n        h = self.b7(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c7(h))\n\n        return h\n\n\nclass InfoMaxGANDiscriminator128(infomax_gan_base.BaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SNGAN-InfoMax.\n\n    Attributes:\n        nrkhs (int): The RKHS dimension R to project the local and global features to.\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n        infomax_loss_scale (float): The beta parameter used for scaling the discriminator infomax loss.\n    """"""\n    def __init__(self, nrkhs=1024, ndf=1024, **kwargs):\n        super().__init__(nrkhs=nrkhs, ndf=ndf, **kwargs)\n\n        # Decide activation used\n        self.activation = nn.ReLU(True)\n\n        # ----------------\n        #   GAN Layers\n        # ----------------\n        self.local_feat_blocks = nn.Sequential(\n            DBlockOptimized(3, self.ndf >> 4),\n            DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True),\n            DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True),\n            DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True),\n            DBlock(self.ndf >> 1, self.ndf, downsample=True))\n\n        self.global_feat_blocks = nn.Sequential(\n            DBlock(self.ndf, self.ndf, downsample=False))\n\n        self.linear = SNLinear(self.ndf, 1)\n        nn.init.xavier_uniform_(self.linear.weight.data, 1.0)\n\n        # --------------------\n        #   InfoMax Layers\n        # --------------------\n        # Critic network layers for local features\n        self.local_nrkhs_a = SNConv2d(self.ndf, self.ndf, 1, 1, 0)\n        self.local_nrkhs_b = SNConv2d(self.ndf, self.nrkhs, 1, 1, 0)\n        self.local_nrkhs_sc = SNConv2d(self.ndf, self.nrkhs, 1, 1, 0)\n\n        nn.init.xavier_uniform_(self.local_nrkhs_a.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.local_nrkhs_b.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.local_nrkhs_sc.weight.data, 1.0)\n\n        # Critic network layers for global features\n        self.global_nrkhs_a = SNLinear(self.ndf, self.ndf)\n        self.global_nrkhs_b = SNLinear(self.ndf, self.nrkhs)\n        self.global_nrkhs_sc = SNLinear(self.ndf, self.nrkhs)\n\n        nn.init.xavier_uniform_(self.global_nrkhs_a.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.global_nrkhs_b.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.global_nrkhs_sc.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits,\n        local features of the images, and global features of the images.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n            Tensor: A batch of local features of shape (N, ndf, H>>2, W>>2).\n            Tensor: A batch of global features of shape (N, ndf)\n        """"""\n        h = x\n\n        # Get features\n        local_feat = self.local_feat_blocks(h)  # (N, C, H, W)\n        global_feat = self.global_feat_blocks(local_feat)\n        global_feat = self.activation(global_feat)\n        global_feat = torch.sum(global_feat, dim=(2, 3))\n\n        # GAN task output\n        output = self.linear(global_feat)\n\n        return output, local_feat, global_feat\n'"
torch_mimicry/nets/infomax_gan/infomax_gan_32.py,3,"b'""""""\nImplementation of InfoMax-GAN for image size 32.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.infomax_gan import infomax_gan_base\nfrom torch_mimicry.modules.layers import SNConv2d, SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass InfoMaxGANGenerator32(infomax_gan_base.InfoMaxGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for InfoMax-GAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n        infomax_loss_scale (float): The alpha parameter used for scaling the generator infomax loss.\n    """"""\n    def __init__(self, nz=128, ngf=256, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block4 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf)\n        self.c5 = nn.Conv2d(self.ngf, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n\nclass InfoMaxGANDiscriminator32(infomax_gan_base.BaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for InfoMax-GAN.\n\n    Attributes:\n        nrkhs (int): The RKHS dimension R to project the local and global features to.\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n        infomax_loss_scale (float): The beta parameter used for scaling the discriminator infomax loss.\n    """"""\n    def __init__(self, nrkhs=1024, ndf=128, **kwargs):\n        super().__init__(nrkhs=nrkhs, ndf=ndf, **kwargs)\n\n        # Define activation used\n        self.activation = nn.ReLU(True)\n\n        # ----------------\n        #   GAN Layers\n        # ----------------\n        self.local_feat_blocks = nn.Sequential(\n            DBlockOptimized(3, self.ndf),\n            DBlock(self.ndf, self.ndf, downsample=True),\n            DBlock(self.ndf, self.ndf, downsample=False))\n\n        self.global_feat_blocks = nn.Sequential(\n            DBlock(self.ndf, self.ndf, downsample=False))\n\n        self.linear = SNLinear(self.ndf, 1)\n        nn.init.xavier_uniform_(self.linear.weight.data, 1.0)\n\n        # --------------------\n        #   InfoMax Layers\n        # --------------------\n        # Critic network layers for local features\n        self.local_nrkhs_a = SNConv2d(self.ndf, self.ndf, 1, 1, 0)\n        self.local_nrkhs_b = SNConv2d(self.ndf, self.nrkhs, 1, 1, 0)\n        self.local_nrkhs_sc = SNConv2d(self.ndf, self.nrkhs, 1, 1, 0)\n\n        nn.init.xavier_uniform_(self.local_nrkhs_a.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.local_nrkhs_b.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.local_nrkhs_sc.weight.data, 1.0)\n\n        # Critic network layers for global features\n        self.global_nrkhs_a = SNLinear(self.ndf, self.ndf)\n        self.global_nrkhs_b = SNLinear(self.ndf, self.nrkhs)\n        self.global_nrkhs_sc = SNLinear(self.ndf, self.nrkhs)\n\n        nn.init.xavier_uniform_(self.global_nrkhs_a.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.global_nrkhs_b.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.global_nrkhs_sc.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits,\n        local features of the images, and global features of the images.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n            Tensor: A batch of local features of shape (N, ndf, H>>2, W>>2).\n            Tensor: A batch of global features of shape (N, ndf)\n        """"""\n        h = x\n\n        # Get features\n        local_feat = self.local_feat_blocks(h)  # (N, C, H, W)\n        global_feat = self.global_feat_blocks(local_feat)\n        global_feat = self.activation(global_feat)\n        global_feat = torch.sum(global_feat, dim=(2, 3))\n\n        # GAN task output\n        output = self.linear(global_feat)\n\n        return output, local_feat, global_feat\n'"
torch_mimicry/nets/infomax_gan/infomax_gan_48.py,3,"b'""""""\nImplementation of InfoMax-GAN for image size 48.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.infomax_gan import infomax_gan_base\nfrom torch_mimicry.modules.layers import SNConv2d, SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass InfoMaxGANGenerator48(infomax_gan_base.InfoMaxGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for InfoMax-GAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n        infomax_loss_scale (float): The alpha parameter used for scaling the generator infomax loss.\n    """"""\n    def __init__(self, nz=128, ngf=512, bottom_width=6, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block3 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block4 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf >> 3)\n        self.c5 = nn.Conv2d(self.ngf >> 3, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n\nclass InfoMaxGANDiscriminator48(infomax_gan_base.BaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SNGAN-InfoMax.\n\n    Attributes:\n        nrkhs (int): The RKHS dimension R to project the local and global features to.\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n        infomax_loss_scale (float): The beta parameter used for scaling the discriminator infomax loss.\n    """"""\n    def __init__(self, ndf=1024, nrkhs=1024, **kwargs):\n        super().__init__(nrkhs=nrkhs, ndf=ndf, **kwargs)\n\n        # Decide activation used\n        self.activation = nn.ReLU(True)\n\n        # ----------------\n        #   GAN Layers\n        # ----------------\n        self.local_feat_blocks = nn.Sequential(\n            DBlockOptimized(3, self.ndf >> 4),\n            DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True),\n            DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True),\n            DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True))\n\n        self.global_feat_blocks = nn.Sequential(\n            DBlock(self.ndf >> 1, self.ndf, downsample=False))\n\n        self.linear = SNLinear(self.ndf, 1)\n        nn.init.xavier_uniform_(self.linear.weight.data, 1.0)\n\n        # -------------------\n        #   InfoMax Layers\n        # -------------------\n        # Critic network layers for local features\n        self.local_nrkhs_a = SNConv2d(self.ndf >> 1, self.ndf >> 1, 1, 1, 0)\n        self.local_nrkhs_b = SNConv2d(self.ndf >> 1, self.nrkhs, 1, 1, 0)\n        self.local_nrkhs_sc = SNConv2d(self.ndf >> 1, self.nrkhs, 1, 1, 0)\n\n        nn.init.xavier_uniform_(self.local_nrkhs_a.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.local_nrkhs_b.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.local_nrkhs_sc.weight.data, 1.0)\n\n        # Critic network layers for global features\n        self.global_nrkhs_a = SNLinear(self.ndf, self.ndf)\n        self.global_nrkhs_b = SNLinear(self.ndf, self.nrkhs)\n        self.global_nrkhs_sc = SNLinear(self.ndf, self.nrkhs)\n\n        nn.init.xavier_uniform_(self.global_nrkhs_a.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.global_nrkhs_b.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.global_nrkhs_sc.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits,\n        local features of the images, and global features of the images.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n            Tensor: A batch of local features of shape (N, ndf, H>>2, W>>2).\n            Tensor: A batch of global features of shape (N, ndf)\n        """"""\n        h = x\n\n        # Get features\n        local_feat = self.local_feat_blocks(h)  # (N, C, H, W)\n        global_feat = self.global_feat_blocks(local_feat)\n        global_feat = self.activation(global_feat)\n        global_feat = torch.sum(global_feat, dim=(2, 3))\n\n        # GAN task output\n        output = self.linear(global_feat)\n\n        return output, local_feat, global_feat\n'"
torch_mimicry/nets/infomax_gan/infomax_gan_64.py,3,"b'""""""\nImplementation of InfoMax-GAN for image size 64.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.infomax_gan import infomax_gan_base\nfrom torch_mimicry.modules.layers import SNConv2d, SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass InfoMaxGANGenerator64(infomax_gan_base.InfoMaxGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for InfoMax-GAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n        infomax_loss_scale (float): The alpha parameter used for scaling the generator infomax loss.\n    """"""\n    def __init__(self, nz=128, ngf=1024, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block3 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block4 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.block5 = GBlock(self.ngf >> 3, self.ngf >> 4, upsample=True)\n        self.b6 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c6 = nn.Conv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c6.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.b6(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c6(h))\n\n        return h\n\n\nclass InfoMaxGANDiscriminator64(infomax_gan_base.BaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for InfoMax-GAN.\n\n    Attributes:\n        nrkhs (int): The RKHS dimension R to project the local and global features to.\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n        infomax_loss_scale (float): The beta parameter used for scaling the discriminator infomax loss.\n    """"""\n    def __init__(self, nrkhs=1024, ndf=1024, **kwargs):\n        super().__init__(nrkhs=nrkhs, ndf=ndf, **kwargs)\n\n        # Decide activation used\n        self.activation = nn.ReLU(True)\n\n        # ----------------\n        #   GAN Layers\n        # ----------------\n        self.local_feat_blocks = nn.Sequential(\n            DBlockOptimized(3, self.ndf >> 4),\n            DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True),\n            DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True),\n            DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True))\n\n        self.global_feat_blocks = nn.Sequential(\n            DBlock(self.ndf >> 1, self.ndf, downsample=True))\n\n        self.linear = SNLinear(self.ndf, 1)\n        nn.init.xavier_uniform_(self.linear.weight.data, 1.0)\n\n        # --------------------\n        #   InfoMax Layers\n        # --------------------\n        # Critic network layers for local features\n        self.local_nrkhs_a = SNConv2d(self.ndf >> 1, self.ndf >> 1, 1, 1, 0)\n        self.local_nrkhs_b = SNConv2d(self.ndf >> 1, self.nrkhs, 1, 1, 0)\n        self.local_nrkhs_sc = SNConv2d(self.ndf >> 1, self.nrkhs, 1, 1, 0)\n\n        nn.init.xavier_uniform_(self.local_nrkhs_a.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.local_nrkhs_b.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.local_nrkhs_sc.weight.data, 1.0)\n\n        # Critic network layers for global features\n        self.global_nrkhs_a = SNLinear(self.ndf, self.ndf)\n        self.global_nrkhs_b = SNLinear(self.ndf, self.nrkhs)\n        self.global_nrkhs_sc = SNLinear(self.ndf, self.nrkhs)\n\n        nn.init.xavier_uniform_(self.global_nrkhs_a.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.global_nrkhs_b.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.global_nrkhs_sc.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits,\n        local features of the images, and global features of the images.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n            Tensor: A batch of local features of shape (N, ndf, H>>2, W>>2).\n            Tensor: A batch of global features of shape (N, ndf)\n        """"""\n        h = x\n\n        # Get features\n        local_feat = self.local_feat_blocks(h)  # (N, C, H, W)\n        global_feat = self.global_feat_blocks(local_feat)\n        global_feat = self.activation(global_feat)\n        global_feat = torch.sum(global_feat, dim=(2, 3))\n\n        # GAN task output\n        output = self.linear(global_feat)\n\n        return output, local_feat, global_feat\n'"
torch_mimicry/nets/infomax_gan/infomax_gan_base.py,9,"b'""""""\nImplementation of InfoMax-GAN base model.\n""""""\nimport torch\nimport torch.nn.functional as F\n\nfrom torch_mimicry.nets.gan import gan\n\n\nclass InfoMaxGANBaseGenerator(gan.BaseGenerator):\n    r""""""\n    ResNet backbone generator for InfoMax-GAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n        infomax_loss_scale (float): The alpha parameter used for scaling the generator infomax loss.\n    """"""\n    def __init__(self,\n                 nz,\n                 ngf,\n                 bottom_width,\n                 loss_type=\'hinge\',\n                 infomax_loss_scale=0.2,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         **kwargs)\n        self.infomax_loss_scale = infomax_loss_scale\n\n    def train_step(self,\n                   real_batch,\n                   netD,\n                   optG,\n                   log_data,\n                   device=None,\n                   global_step=None,\n                   **kwargs):\n        r""""""\n        Takes one training step for G.\n\n        Args:\n            real_batch (Tensor): A batch of real images of shape (N, C, H, W).\n                Used for obtaining current batch size.\n            netD (nn.Module): Discriminator model for obtaining losses.\n            optG (Optimizer): Optimizer for updating generator\'s parameters.\n            log_data (MetricLog): An object to add custom metrics for visualisations.\n            device (torch.device): Device to use for running the model.\n            global_step (int): Variable to sync training, logging and checkpointing.\n                Useful for dynamic changes to model amidst training.\n\n        Returns:\n            MetricLog: Returns MetricLog object containing updated logging variables after 1 training step.\n\n        """"""\n        # Zero gradient every step.\n        self.zero_grad()\n\n        # Get only batch size from real batch\n        real_images, _ = real_batch\n        batch_size = real_images.shape[0]\n\n        # Produce fake images\n        fake_images = self.generate_images(num_images=batch_size,\n                                           device=device)\n\n        # Get logits and projected features\n        output_fake, local_feat_fake, global_feat_fake = netD(fake_images)\n\n        local_feat_fake, global_feat_fake = netD.project_features(\n            local_feat=local_feat_fake, global_feat=global_feat_fake)\n\n        # Compute losses\n        errG = self.compute_gan_loss(output_fake)\n\n        errG_IM = netD.compute_infomax_loss(local_feat=local_feat_fake,\n                                            global_feat=global_feat_fake,\n                                            scale=self.infomax_loss_scale)\n\n        # Backprop and update gradients\n        errG_total = errG + errG_IM\n\n        errG_total.backward()\n        optG.step()\n\n        # Log statistics\n        log_data.add_metric(\'errG\', errG, group=\'loss\')\n        log_data.add_metric(\'errG_IM\', errG_IM, group=\'loss_IM\')\n\n        return log_data\n\n\nclass BaseDiscriminator(gan.BaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SNGAN-Infomax.\n\n    Attributes:\n        nrkhs (int): The RKHS dimension R to project the local and global features to.\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n        infomax_loss_scale (float): The beta parameter used for scaling the discriminator infomax loss.\n    """"""\n    def __init__(self,\n                 nrkhs,\n                 ndf,\n                 loss_type=\'hinge\',\n                 infomax_loss_scale=0.2,\n                 **kwargs):\n        super().__init__(ndf=ndf, loss_type=loss_type, **kwargs)\n        self.nrkhs = nrkhs\n        self.infomax_loss_scale = infomax_loss_scale\n\n    def _project_local(self, local_feat):\n        r""""""\n        Helper function for projecting local features to RKHS.\n        """"""\n        local_feat_sc = self.local_nrkhs_sc(local_feat)\n\n        local_feat = self.local_nrkhs_a(local_feat)\n        local_feat = self.activation(local_feat)\n        local_feat = self.local_nrkhs_b(local_feat)\n        local_feat += local_feat_sc\n\n        return local_feat\n\n    def _project_global(self, global_feat):\n        r""""""\n        Helper function for projecting global features to RKHS.\n        """"""\n        global_feat_sc = self.global_nrkhs_sc(global_feat)\n\n        global_feat = self.global_nrkhs_a(global_feat)\n        global_feat = self.activation(global_feat)\n        global_feat = self.global_nrkhs_b(global_feat)\n        global_feat += global_feat_sc\n\n        return global_feat\n\n    def project_features(self, local_feat, global_feat):\n        r""""""\n        Projects local and global features.\n        """"""\n        local_feat = self._project_local(\n            local_feat)  # (N, C, H, W) --> (N, nrkhs, H, W)\n        global_feat = self._project_global(\n            global_feat)  # (N, C) --> (N, nrkhs)\n\n        return local_feat, global_feat\n\n    def infonce_loss(self, l, m):\n        r""""""\n        InfoNCE loss for local and global feature maps as used in DIM: \n        https://github.com/rdevon/DIM/blob/master/cortex_DIM/functions/dim_losses.py\n\n        Args:\n            l (Tensor): Local feature map of shape (N, ndf, H*W).\n            m (Tensor): Global feature vector of shape (N, ndf, 1).\n        Returns:\n            Tensor: Scalar loss Tensor.\n        """"""\n        N, units, n_locals = l.size()\n        _, _, n_multis = m.size()\n\n        # First we make the input tensors the right shape.\n        l_p = l.permute(0, 2, 1)\n        m_p = m.permute(0, 2, 1)\n\n        l_n = l_p.reshape(-1, units)\n        m_n = m_p.reshape(-1, units)\n\n        # Inner product for positive samples. Outer product for negative. We need to do it this way\n        # for the multiclass loss. For the outer product, we want a N x N x n_local x n_multi tensor.\n        u_p = torch.matmul(l_p, m).unsqueeze(2)\n        u_n = torch.mm(m_n, l_n.t())\n        u_n = u_n.reshape(N, n_multis, N, n_locals).permute(0, 2, 3, 1)\n\n        # We need to mask the diagonal part of the negative tensor.\n        mask = torch.eye(N)[:, :, None, None].to(l.device)\n        n_mask = 1 - mask\n\n        # Masking is done by shifting the diagonal before exp.\n        u_n = (n_mask * u_n) - (10. * (1 - n_mask))  # mask out ""self"" examples\n        u_n = u_n.reshape(N, N * n_locals, n_multis).unsqueeze(dim=1).expand(\n            -1, n_locals, -1, -1)\n\n        # Since this is multiclass, we concat the positive along the class dimension before performing log softmax.\n        pred_lgt = torch.cat(\n            [u_p, u_n], dim=2\n        )  # So the first of each ""row"" is positive, and we have N+1 elements\n        pred_log = F.log_softmax(pred_lgt, dim=2)\n\n        # The positive score is the first element of the log softmax.\n        loss = -pred_log[:, :, 0].mean()\n\n        return loss\n\n    def compute_infomax_loss(self, local_feat, global_feat, scale):\n        r""""""\n        Given local and global features of a real or fake image, produce the average\n        dot product score between each local and global features, which is then used\n        to obtain infoNCE loss.\n\n        Args\n            local_feat (Tensor): A batch of local features.\n            global_feat (Tensor): A batch of global features.\n            scale (float): The scaling hyperparameter for the infomax loss.\n\n        Returns:\n            Tensor: Scalar Tensor representing the scaled infomax loss.\n        """"""\n        if local_feat.shape[1] != self.nrkhs:\n            raise ValueError(\n                ""Features have not been projected. Expected {} dim but got {} instead""\n                .format(self.nrkhs, local_feat.shape[1]))\n\n        # Prepare shapes for local and global features.\n        local_feat = torch.flatten(local_feat, start_dim=2,\n                                   end_dim=3)  # (N, C, H, W) --> (N, C, H*W)\n        global_feat = torch.unsqueeze(global_feat, 2)  # (N, C) --> (N, C, 1)\n\n        # Compute infomax loss and scale\n        loss = self.infonce_loss(l=local_feat, m=global_feat)\n\n        loss = scale * loss\n\n        return loss\n\n    def train_step(self,\n                   real_batch,\n                   netG,\n                   optD,\n                   log_data,\n                   device=None,\n                   global_step=None,\n                   **kwargs):\n        r""""""\n        Takes one training step for D.\n\n        Args:\n            real_batch (Tensor): A batch of real images of shape (N, C, H, W).\n            netG (nn.Module): Generator model for obtaining fake images.\n            optD (Optimizer): Optimizer for updating discriminator\'s parameters.\n            device (torch.device): Device to use for running the model.\n            log_data (MetricLog): An object to add custom metrics for visualisations.\n            global_step (int): Variable to sync training, logging and checkpointing.\n                Useful for dynamic changes to model amidst training.\n\n        Returns:\n            MetricLog: Returns MetricLog object containing updated logging variables after 1 training step.\n\n        """"""\n        self.zero_grad()\n\n        # Produce real images\n        real_images, _ = real_batch\n        batch_size = real_images.shape[0]  # Match batch sizes for last iter\n\n        # Produce fake images\n        fake_images = netG.generate_images(num_images=batch_size,\n                                           device=device).detach()\n\n        # Compute real and fake logits for gan loss\n        output_real, local_feat_real, global_feat_real = self.forward(\n            real_images)\n        output_fake, _, _ = self.forward(fake_images)\n\n        # Project the features\n        local_feat_real, global_feat_real = self.project_features(\n            local_feat=local_feat_real, global_feat=global_feat_real)\n\n        # Compute losses\n        errD = self.compute_gan_loss(output_real=output_real,\n                                     output_fake=output_fake)\n\n        errD_IM = self.compute_infomax_loss(local_feat=local_feat_real,\n                                            global_feat=global_feat_real,\n                                            scale=self.infomax_loss_scale)\n\n        # Backprop and update gradients\n        errD_total = errD + errD_IM\n        errD_total.backward()\n        optD.step()\n\n        # Compute probabilities\n        D_x, D_Gz = self.compute_probs(output_real=output_real,\n                                       output_fake=output_fake)\n\n        # Log statistics for D\n        log_data.add_metric(\'errD\', errD, group=\'loss\')\n        log_data.add_metric(\'errD_IM\', errD_IM, group=\'loss_IM\')\n        log_data.add_metric(\'D(x)\', D_x, group=\'prob\')\n        log_data.add_metric(\'D(G(z))\', D_Gz, group=\'prob\')\n\n        return log_data\n'"
torch_mimicry/nets/sagan/sagan_128.py,5,"b'""""""\nImplementation of SAGAN for image size 128.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.sagan import sagan_base\nfrom torch_mimicry.modules.layers import SNLinear, SNConv2d, SNEmbedding, SelfAttention\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass SAGANGenerator128(sagan_base.SAGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for SAGAN,\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.    \n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self,\n                 num_classes,\n                 nz=128,\n                 ngf=1024,\n                 bottom_width=4,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         num_classes=num_classes,\n                         **kwargs)\n\n        # Build the layers\n        self.l1 = SNLinear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf,\n                             self.ngf,\n                             upsample=True,\n                             num_classes=self.num_classes,\n                             spectral_norm=True)\n        self.block3 = GBlock(self.ngf,\n                             self.ngf >> 1,\n                             upsample=True,\n                             num_classes=self.num_classes,\n                             spectral_norm=True)\n        self.block4 = GBlock(self.ngf >> 1,\n                             self.ngf >> 2,\n                             upsample=True,\n                             num_classes=self.num_classes,\n                             spectral_norm=True)\n        self.block5 = GBlock(self.ngf >> 2,\n                             self.ngf >> 3,\n                             upsample=True,\n                             num_classes=self.num_classes,\n                             spectral_norm=True)\n        self.block6 = GBlock(self.ngf >> 3,\n                             self.ngf >> 4,\n                             upsample=True,\n                             num_classes=self.num_classes,\n                             spectral_norm=True)\n        self.b7 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c7 = SNConv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # SA block\n        self.attn_block = SelfAttention(self.ngf >> 2, spectral_norm=True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c7.weight.data, 1.0)\n\n    def forward(self, x, y=None):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images, also\n        conditioning the batch norm with labels of the images to be produced.\n\n        Self attention is applied after 3rd residual block at G.\n        https://github.com/brain-research/self-attention-gan/blob/master/generator.py#L208\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n            y (Tensor): A batch of labels of shape (N,) for conditional batch norm.\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        if y is None:\n            y = torch.randint(low=0,\n                              high=self.num_classes,\n                              size=(x.shape[0], ),\n                              device=x.device)\n\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h, y)\n        h = self.block3(h, y)\n        h = self.block4(h, y)\n        h = self.attn_block(h)\n        h = self.block5(h, y)\n        h = self.block6(h, y)\n        h = self.b7(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c7(h))\n\n        return h\n\n\nclass SAGANDiscriminator128(sagan_base.SAGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SAGAN.\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.        \n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.                \n    """"""\n    def __init__(self, num_classes, ndf=128, **kwargs):\n        super().__init__(ndf=ndf, num_classes=num_classes, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4)\n        self.block2 = DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True)\n        self.block3 = DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True)\n        self.block4 = DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True)\n        self.block5 = DBlock(self.ndf >> 1, self.ndf, downsample=True)\n        self.block6 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.l7 = SNLinear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Produce label vector from trained embedding\n        self.l_y = SNEmbedding(num_embeddings=self.num_classes,\n                               embedding_dim=self.ndf)\n\n        # SA block\n        self.attn_block = SelfAttention(self.ndf >> 3, spectral_norm=True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l7.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.l_y.weight.data, 1.0)\n\n        self.activation = nn.ReLU(True)\n\n    def forward(self, x, y=None):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n        Further projects labels to condition on the output logit score.\n\n        Self-attention is applied after 2nd resblock in D:\n        https://github.com/brain-research/self-attention-gan/blob/master/discriminator.py#L191\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n            y (Tensor): A batch of labels of shape (N,).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.attn_block(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.block6(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l7(h)\n\n        # Add the projection loss\n        w_y = self.l_y(y)\n        output += torch.sum((w_y * h), dim=1, keepdim=True)\n\n        return output\n'"
torch_mimicry/nets/sagan/sagan_32.py,5,"b'""""""\nImplementation of SAGAN for image size 32.\n\nNote: The original SAGAN did not have architecture designs for datasets with\nresolutions other than 128x128. Thus, the current placement of the attention\nblock follows the version for 128x128 but may well not be optimal for this\nparticular resolution. Feel free to explore if it works for you.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.sagan import sagan_base\nfrom torch_mimicry.modules.layers import SNLinear, SNConv2d, SNEmbedding, SelfAttention\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass SAGANGenerator32(sagan_base.SAGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for SAGAN,\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.    \n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, num_classes, bottom_width=4, nz=128, ngf=256, **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         num_classes=num_classes,\n                         **kwargs)\n\n        # Build the layers\n        self.l1 = SNLinear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf,\n                             self.ngf,\n                             upsample=True,\n                             num_classes=self.num_classes,\n                             spectral_norm=True)\n        self.block3 = GBlock(self.ngf,\n                             self.ngf,\n                             upsample=True,\n                             num_classes=self.num_classes,\n                             spectral_norm=True)\n        self.block4 = GBlock(self.ngf,\n                             self.ngf,\n                             upsample=True,\n                             num_classes=self.num_classes,\n                             spectral_norm=True)\n        self.b5 = nn.BatchNorm2d(self.ngf)\n        self.c5 = SNConv2d(self.ngf, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # SA block\n        self.attn_block = SelfAttention(self.ngf, spectral_norm=True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x, y=None):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images, also\n        conditioning the batch norm with labels of the images to be produced.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n            y (Tensor): A batch of labels of shape (N,) for conditional batch norm.\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        if y is None:\n            y = torch.randint(low=0,\n                              high=self.num_classes,\n                              size=(x.shape[0], ),\n                              device=x.device)\n\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h, y)\n        h = self.block3(h, y)\n        h = self.attn_block(h)\n        h = self.block4(h, y)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n\nclass SAGANDiscriminator32(sagan_base.SAGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SAGAN.\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.        \n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.                \n    """"""\n    def __init__(self, num_classes, ndf=128, **kwargs):\n        super().__init__(ndf=ndf, num_classes=num_classes, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf)\n        self.block2 = DBlock(self.ndf, self.ndf, downsample=True)\n        self.block3 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.block4 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.l5 = SNLinear(self.ndf, 1)\n\n        # Produce label vector from trained embedding\n        self.l_y = SNEmbedding(num_embeddings=self.num_classes,\n                               embedding_dim=self.ndf)\n\n        # SA block\n        self.attn_block = SelfAttention(self.ndf, spectral_norm=True)        \n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l5.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.l_y.weight.data, 1.0)\n\n        self.activation = nn.ReLU(True)\n\n    def forward(self, x, y=None):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n        Further projects labels to condition on the output logit score.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n            y (Tensor): A batch of labels of shape (N,).\n\n        Returns:\n            output (Tensor): A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.attn_block(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l5(h)\n\n        # Add the projection loss\n        w_y = self.l_y(y)\n        output += torch.sum((w_y * h), dim=1, keepdim=True)\n\n        return output\n'"
torch_mimicry/nets/sagan/sagan_base.py,0,"b'""""""\nBase implementation of SAGAN.\n""""""\nfrom torch_mimicry.nets.gan import cgan\n\n\nclass SAGANBaseGenerator(cgan.BaseConditionalGenerator):\n    r""""""\n    ResNet backbone generator for cGAN-PD,\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.    \n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self,\n                 num_classes,\n                 bottom_width,\n                 nz,\n                 ngf,\n                 loss_type=\'hinge\',\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         num_classes=num_classes,\n                         **kwargs)\n\n\nclass SAGANBaseDiscriminator(cgan.BaseConditionalDiscriminator):\n    r""""""\n    ResNet backbone discriminator for cGAN-PD.\n\n    Attributes:\n        num_classes (int): Number of classes, more than 0 for conditional GANs.        \n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.                \n    """"""\n    def __init__(self, num_classes, ndf, loss_type=\'hinge\', **kwargs):\n        super().__init__(ndf=ndf,\n                         loss_type=loss_type,\n                         num_classes=num_classes,\n                         **kwargs)\n'"
torch_mimicry/nets/sngan/__init__.py,0,b'from .sngan_128 import *\nfrom .sngan_32 import *\nfrom .sngan_48 import *\nfrom .sngan_64 import *\nfrom .sngan_base import *\n'
torch_mimicry/nets/sngan/sngan_128.py,3,"b'""""""\nImplementation of SNGAN for image size 128.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.modules.layers import SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\nfrom torch_mimicry.nets.sngan import sngan_base\n\n\nclass SNGANGenerator128(sngan_base.SNGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for SNGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.\n    """"""\n    def __init__(self, nz=128, ngf=1024, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block4 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block5 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.block6 = GBlock(self.ngf >> 3, self.ngf >> 4, upsample=True)\n        self.b7 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c7 = nn.Conv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c7.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.block6(h)\n        h = self.b7(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c7(h))\n\n        return h\n\n\nclass SNGANDiscriminator128(sngan_base.SNGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SNGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4)\n        self.block2 = DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True)\n        self.block3 = DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True)\n        self.block4 = DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True)\n        self.block5 = DBlock(self.ndf >> 1, self.ndf, downsample=True)\n        self.block6 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.l7 = SNLinear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l7.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.block6(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l7(h)\n\n        return output\n'"
torch_mimicry/nets/sngan/sngan_32.py,3,"b'""""""\nImplementation of SNGAN for image size 32.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.modules.layers import SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\nfrom torch_mimicry.nets.sngan import sngan_base\n\n\nclass SNGANGenerator32(sngan_base.SNGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for SNGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.\n    """"""\n    def __init__(self, nz=128, ngf=256, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block4 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf)\n        self.c5 = nn.Conv2d(self.ngf, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n\nclass SNGANDiscriminator32(sngan_base.SNGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SNGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n    """"""\n    def __init__(self, ndf=128, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf)\n        self.block2 = DBlock(self.ndf, self.ndf, downsample=True)\n        self.block3 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.block4 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.l5 = SNLinear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l5(h)\n\n        return output\n'"
torch_mimicry/nets/sngan/sngan_48.py,3,"b'""""""\nImplementation of SNGAN for image size 48.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.modules.layers import SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\nfrom torch_mimicry.nets.sngan import sngan_base\n\n\nclass SNGANGenerator48(sngan_base.SNGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for SNGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, nz=128, ngf=512, bottom_width=6, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block3 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block4 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf >> 3)\n        self.c5 = nn.Conv2d(self.ngf >> 3, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n\nclass SNGANDiscriminator48(sngan_base.SNGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SNGAN.\n\n    Attribates:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4)\n        self.block2 = DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True)\n        self.block3 = DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True)\n        self.block4 = DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True)\n        self.block5 = DBlock(self.ndf >> 1, self.ndf, downsample=False)\n        self.l5 = SNLinear(self.ndf, 1)\n\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l5(h)\n\n        return output\n'"
torch_mimicry/nets/sngan/sngan_64.py,3,"b'""""""\nImplementation of SNGAN for image size 64.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.modules.layers import SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\nfrom torch_mimicry.nets.sngan import sngan_base\n\n\nclass SNGANGenerator64(sngan_base.SNGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for SNGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, nz=128, ngf=1024, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block3 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block4 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.block5 = GBlock(self.ngf >> 3, self.ngf >> 4, upsample=True)\n        self.b6 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c6 = nn.Conv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c6.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.b6(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c6(h))\n\n        return h\n\n\nclass SNGANDiscriminator64(sngan_base.SNGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SNGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4)\n        self.block2 = DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True)\n        self.block3 = DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True)\n        self.block4 = DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True)\n        self.block5 = DBlock(self.ndf >> 1, self.ndf, downsample=True)\n        self.l6 = SNLinear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l6.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l6(h)\n\n        return output\n'"
torch_mimicry/nets/sngan/sngan_base.py,0,"b'""""""\nBase implementation of SNGAN with default variables.\n""""""\nfrom torch_mimicry.nets.gan import gan\n\n\nclass SNGANBaseGenerator(gan.BaseGenerator):\n    r""""""\n    ResNet backbone generator for SNGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.\n    """"""\n    def __init__(self, nz, ngf, bottom_width, loss_type=\'hinge\', **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         **kwargs)\n\n\nclass SNGANBaseDiscriminator(gan.BaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SNGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n    """"""\n    def __init__(self, ndf, loss_type=\'hinge\', **kwargs):\n        super().__init__(ndf=ndf, loss_type=loss_type, **kwargs)\n'"
torch_mimicry/nets/ssgan/__init__.py,0,b'from .ssgan_128 import *\nfrom .ssgan_32 import *\nfrom .ssgan_48 import *\nfrom .ssgan_64 import *\nfrom .ssgan_base import *\n'
torch_mimicry/nets/ssgan/ssgan_128.py,3,"b'""""""\nImplementation of SSGAN for image size 128.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.modules.layers import SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\nfrom torch_mimicry.nets.ssgan import ssgan_base\n\n\nclass SSGANGenerator128(ssgan_base.SSGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for SSGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n        ss_loss_scale (float): Self-supervised loss scale for generator.\n    """"""\n    def __init__(self, nz=128, ngf=1024, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block4 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block5 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.block6 = GBlock(self.ngf >> 3, self.ngf >> 4, upsample=True)\n        self.b7 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c7 = nn.Conv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c7.weight.data, 1.0)\n\n    def forward(self, x):\n        """"""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.block6(h)\n        h = self.b7(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c7(h))\n\n        return h\n\n\nclass SSGANDiscriminator128(ssgan_base.SSGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SSGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.        \n        ss_loss_scale (float): Self-supervised loss scale for discriminator.        \n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4)\n        self.block2 = DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True)\n        self.block3 = DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True)\n        self.block4 = DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True)\n        self.block5 = DBlock(self.ndf >> 1, self.ndf, downsample=True)\n        self.block6 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.l7 = SNLinear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Produce label vector from trained embedding\n        self.l_y = SNLinear(self.ndf, self.num_classes)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l7.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.l_y.weight.data, 1.0)\n\n        self.activation = nn.ReLU(True)\n\n    def forward(self, x):\n        """"""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits,\n        and rotation classes.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n            Tensor: A batch of predicted classes of shape (N, num_classes).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.block6(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l7(h)\n\n        # Produce the class output logits\n        output_classes = self.l_y(h)\n\n        return output, output_classes\n'"
torch_mimicry/nets/ssgan/ssgan_32.py,3,"b'""""""\nImplementation of SSGAN for image size 32.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.modules import SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\nfrom torch_mimicry.nets.ssgan import ssgan_base\n\n\nclass SSGANGenerator32(ssgan_base.SSGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for SSGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.\n        ss_loss_scale (float): Self-supervised loss scale for generator.\n    """"""\n    def __init__(self, nz=128, ngf=256, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block4 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf)\n        self.c5 = nn.Conv2d(ngf, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n\nclass SSGANDiscriminator32(ssgan_base.SSGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SSGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n        ss_loss_scale (float): Self-supervised loss scale for discriminator.\n    """"""\n    def __init__(self, ndf=128, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf)\n        self.block2 = DBlock(self.ndf, self.ndf, downsample=True)\n        self.block3 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.block4 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.l5 = SNLinear(self.ndf, 1)\n\n        # Rotation class prediction layer\n        self.l_y = SNLinear(self.ndf, self.num_classes)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l5.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.l_y.weight.data, 1.0)\n\n        self.activation = nn.ReLU(True)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits,\n        and rotation classes.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n            Tensor: A batch of predicted classes of shape (N, num_classes).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l5(h)\n\n        # Produce the class output logits\n        output_classes = self.l_y(h)\n\n        return output, output_classes\n'"
torch_mimicry/nets/ssgan/ssgan_48.py,3,"b'""""""\nImplementation of SSGAN for image size 48.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.modules.layers import SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\nfrom torch_mimicry.nets.ssgan import ssgan_base\n\n\nclass SSGANGenerator48(ssgan_base.SSGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for SSGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n        ss_loss_scale (float): Self-supervised loss scale for generator.\n    """"""\n    def __init__(self, nz=128, ngf=512, bottom_width=6, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block3 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block4 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf >> 3)\n        self.c5 = nn.Conv2d(self.ngf >> 3, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n\nclass SSGANDiscriminator48(ssgan_base.SSGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SSGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.        \n        ss_loss_scale (float): Self-supervised loss scale for discriminator.        \n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4)\n        self.block2 = DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True)\n        self.block3 = DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True)\n        self.block4 = DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True)\n        self.block5 = DBlock(self.ndf >> 1, self.ndf, downsample=False)\n        self.l5 = SNLinear(self.ndf, 1)\n\n        # Produce label vector from trained embedding\n        self.l_y = SNLinear(self.ndf, self.num_classes)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l5.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.l_y.weight.data, 1.0)\n\n        self.activation = nn.ReLU(True)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits,\n        and rotation classes.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n            Tensor: A batch of predicted classes of shape (N, num_classes).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l5(h)\n\n        # Produce the class output logits\n        output_classes = self.l_y(h)\n\n        return output, output_classes\n'"
torch_mimicry/nets/ssgan/ssgan_64.py,3,"b'""""""\nImplementation of SSGAN for image size 64.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.modules.layers import SNLinear\nfrom torch_mimicry.modules.resblocks import DBlockOptimized, DBlock, GBlock\nfrom torch_mimicry.nets.ssgan import ssgan_base\n\n\nclass SSGANGenerator64(ssgan_base.SSGANBaseGenerator):\n    r""""""\n    ResNet backbone generator for SSGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n        ss_loss_scale (float): Self-supervised loss scale for generator.\n    """"""\n    def __init__(self, nz=128, ngf=1024, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block3 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block4 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.block5 = GBlock(self.ngf >> 3, self.ngf >> 4, upsample=True)\n        self.b6 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c6 = nn.Conv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.c6.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.b6(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c6(h))\n\n        return h\n\n\nclass SSGANDiscriminator64(ssgan_base.SSGANBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SSGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.        \n        ss_loss_scale (float): Self-supervised loss scale for discriminator.        \n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4)\n        self.block2 = DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True)\n        self.block3 = DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True)\n        self.block4 = DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True)\n        self.block5 = DBlock(self.ndf >> 1, self.ndf, downsample=True)\n        self.l6 = SNLinear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Produce label vector from trained embedding\n        self.l_y = SNLinear(self.ndf, self.num_classes)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l6.weight.data, 1.0)\n        nn.init.xavier_uniform_(self.l_y.weight.data, 1.0)\n\n        self.activation = nn.ReLU(True)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits,\n        and rotation classes.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n            Tensor: A batch of predicted classes of shape (N, num_classes).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.activation(h)\n\n        # Global sum pooling\n        h = torch.sum(h, dim=(2, 3))\n        output = self.l6(h)\n\n        # Produce the class output logits\n        output_classes = self.l_y(h)\n\n        return output, output_classes\n'"
torch_mimicry/nets/ssgan/ssgan_base.py,8,"b'""""""\nImplementation of Base SSGAN models.\n""""""\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom torch_mimicry.nets.gan import gan\n\n\nclass SSGANBaseGenerator(gan.BaseGenerator):\n    r""""""\n    ResNet backbone generator for SSGAN.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n        ss_loss_scale (float): Self-supervised loss scale for generator.\n    """"""\n    def __init__(self,\n                 nz,\n                 ngf,\n                 bottom_width,\n                 loss_type=\'hinge\',\n                 ss_loss_scale=0.2,\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         **kwargs)\n        self.ss_loss_scale = ss_loss_scale\n\n    def train_step(self,\n                   real_batch,\n                   netD,\n                   optG,\n                   log_data,\n                   device=None,\n                   global_step=None,\n                   **kwargs):\n        r""""""\n        Takes one training step for G.\n\n        Args:\n            real_batch (Tensor): A batch of real images of shape (N, C, H, W).\n                Used for obtaining current batch size.\n            netD (nn.Module): Discriminator model for obtaining losses.\n            optG (Optimizer): Optimizer for updating generator\'s parameters.\n            log_data (MetricLog): An object to add custom metrics for visualisations.\n            device (torch.device): Device to use for running the model.\n            global_step (int): Variable to sync training, logging and checkpointing.\n                Useful for dynamic changes to model amidst training.\n\n        Returns:\n            MetricLog: Returns MetricLog object containing updated logging variables after 1 training step.\n\n        """"""\n        self.zero_grad()\n\n        # Get only batch size from real batch\n        batch_size = real_batch[0].shape[0]\n\n        # Produce fake images and logits\n        fake_images = self.generate_images(num_images=batch_size,\n                                           device=device)\n        output, _ = netD(fake_images)\n\n        # Compute GAN loss, upright images only.\n        errG = self.compute_gan_loss(output)\n\n        # Compute SS loss, rotates the images.\n        errG_SS, _ = netD.compute_ss_loss(images=fake_images,\n                                          scale=self.ss_loss_scale)\n\n        # Backprop and update gradients\n        errG_total = errG + errG_SS\n        errG_total.backward()\n        optG.step()\n\n        # Log statistics\n        log_data.add_metric(\'errG\', errG, group=\'loss\')\n        log_data.add_metric(\'errG_SS\', errG_SS, group=\'loss_SS\')\n\n        return log_data\n\n\nclass SSGANBaseDiscriminator(gan.BaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for SSGAN.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.        \n        ss_loss_scale (float): Self-supervised loss scale for discriminator.        \n    """"""\n    def __init__(self, ndf, loss_type=\'hinge\', ss_loss_scale=1.0, **kwargs):\n        super().__init__(ndf=ndf, loss_type=loss_type, **kwargs)\n        self.num_classes = 4\n        self.ss_loss_scale = ss_loss_scale\n\n    def _rot_tensor(self, image, deg):\n        r""""""\n        Rotation for pytorch tensors using rotation matrix. Takes in a tensor of (C, H, W shape).\n        """"""\n        if deg == 90:\n            return image.transpose(1, 2).flip(1)\n\n        elif deg == 180:\n            return image.flip(1).flip(2)\n\n        elif deg == 270:\n            return image.transpose(1, 2).flip(2)\n\n        elif deg == 0:\n            return image\n\n        else:\n            raise NotImplementedError(\n                ""Function only supports 90,180,270,0 degree rotation."")\n\n    def _rotate_batch(self, images):\n        r""""""\n        Rotate a quarter batch of images in each of 4 directions.\n        """"""\n        N, C, H, W = images.shape\n        choices = [(i, i * 4 // N) for i in range(N)]\n\n        # Collect rotated images and labels\n        ret = []\n        ret_labels = []\n        degrees = [0, 90, 180, 270]\n        for i in range(N):\n            idx, rot_label = choices[i]\n\n            # Rotate images\n            image = self._rot_tensor(images[idx],\n                                     deg=degrees[rot_label])  # (C, H, W) shape\n            image = torch.unsqueeze(image, 0)  # (1, C, H, W) shape\n\n            # Get labels accordingly\n            label = torch.from_numpy(np.array(rot_label))  # Zero dimension\n            label = torch.unsqueeze(label, 0)\n\n            ret.append(image)\n            ret_labels.append(label)\n\n        # Concatenate images and labels to (N, C, H, W) and (N, ) shape respectively.\n        ret = torch.cat(ret, dim=0)\n        ret_labels = torch.cat(ret_labels, dim=0).to(ret.device)\n\n        return ret, ret_labels\n\n    def compute_ss_loss(self, images, scale):\n        r""""""\n        Function to compute SS loss.\n\n        Args:\n            images (Tensor): A batch of non-rotated, upright images.\n            scale (float): The parameter to scale SS loss by.\n\n        Returns:\n            Tensor: Scalar tensor representing the SS loss.\n        """"""\n        # Rotate images and produce labels here.\n        images_rot, class_labels = self._rotate_batch(images=images)\n\n        # Compute SS loss\n        _, output_classes = self.forward(images_rot)\n\n        err_SS = F.cross_entropy(input=output_classes, target=class_labels)\n\n        # Scale SS loss\n        err_SS = scale * err_SS\n\n        return err_SS, class_labels\n\n    def train_step(self,\n                   real_batch,\n                   netG,\n                   optD,\n                   log_data,\n                   device=None,\n                   global_step=None,\n                   **kwargs):\n        r""""""\n        Takes one training step for D.\n\n        Args:\n            real_batch (Tensor): A batch of real images of shape (N, C, H, W).\n            netG (nn.Module): Generator model for obtaining fake images.\n            optD (Optimizer): Optimizer for updating discriminator\'s parameters.\n            device (torch.device): Device to use for running the model.\n            log_data (MetricLog): An object to add custom metrics for visualisations.\n            global_step (int): Variable to sync training, logging and checkpointing.\n                Useful for dynamic changes to model amidst training.\n\n        Returns:\n            MetricLog: Returns MetricLog object containing updated logging variables after 1 training step.\n\n        """"""\n        self.zero_grad()\n\n        # Produce real images\n        real_images, _ = real_batch\n        batch_size = real_images.shape[0]  # Match batch sizes for last iter\n\n        # Produce fake images\n        fake_images = netG.generate_images(num_images=batch_size,\n                                           device=device).detach()\n\n        # Compute real and fake logits for gan loss\n        output_real, _ = self.forward(real_images)\n        output_fake, _ = self.forward(fake_images)\n\n        # Compute GAN loss, upright images only.\n        errD = self.compute_gan_loss(output_real=output_real,\n                                     output_fake=output_fake)\n\n        # Compute SS loss, rotates the images.\n        errD_SS, _ = self.compute_ss_loss(images=real_images,\n                                          scale=self.ss_loss_scale)\n\n        # Backprop and update gradients\n        errD_total = errD + errD_SS\n        errD_total.backward()\n        optD.step()\n\n        # Compute probabilities\n        D_x, D_Gz = self.compute_probs(output_real=output_real,\n                                       output_fake=output_fake)\n\n        # Log statistics for D once out of loop\n        log_data.add_metric(\'errD\', errD, group=\'loss\')\n        log_data.add_metric(\'errD_SS\', errD_SS, group=\'loss_SS\')\n        log_data.add_metric(\'D(x)\', D_x, group=\'prob\')\n        log_data.add_metric(\'D(G(z))\', D_Gz, group=\'prob\')\n\n        return log_data\n'"
torch_mimicry/nets/wgan_gp/__init__.py,0,b'from .wgan_gp_128 import *\nfrom .wgan_gp_32 import *\nfrom .wgan_gp_48 import *\nfrom .wgan_gp_64 import *\nfrom .wgan_gp_base import *\nfrom .wgan_gp_resblocks import *\n'
torch_mimicry/nets/wgan_gp/wgan_gp_128.py,3,"b'""""""\nImplementation of WGAN-GP for image size 128.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.wgan_gp import wgan_gp_base\nfrom torch_mimicry.nets.wgan_gp.wgan_gp_resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass WGANGPGenerator128(wgan_gp_base.WGANGPBaseGenerator):\n    r""""""\n    ResNet backbone generator for WGAN-GP.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, nz=128, ngf=1024, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block4 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block5 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.block6 = GBlock(self.ngf >> 3, self.ngf >> 4, upsample=True)\n        self.b7 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c7 = nn.Conv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.block6(h)\n        h = self.b7(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c7(h))\n\n        return h\n\n\nclass WGANGPDiscriminator128(wgan_gp_base.WGANGPBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for WGAN-GP.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n        gp_scale (float): Lamda parameter for gradient penalty.        \n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4)\n        self.block2 = DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True)\n        self.block3 = DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True)\n        self.block4 = DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True)\n        self.block5 = DBlock(self.ndf >> 1, self.ndf, downsample=True)\n        self.block6 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.l7 = nn.Linear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l7.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.block6(h)\n        h = self.activation(h)\n\n        # Global average pooling\n        h = torch.mean(h, dim=(2, 3))  # WGAN uses mean pooling\n        output = self.l7(h)\n\n        return output\n'"
torch_mimicry/nets/wgan_gp/wgan_gp_32.py,3,"b'""""""\nImplementation of WGAN-GP for image size 32.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.wgan_gp import wgan_gp_base\nfrom torch_mimicry.nets.wgan_gp.wgan_gp_resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass WGANGPGenerator32(wgan_gp_base.WGANGPBaseGenerator):\n    r""""""\n    ResNet backbone generator for WGAN-GP.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, nz=128, ngf=256, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block3 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.block4 = GBlock(self.ngf, self.ngf, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf)\n        self.c5 = nn.Conv2d(self.ngf, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n\nclass WGANGPDiscriminator32(wgan_gp_base.WGANGPBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for WGAN-GP.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n        gp_scale (float): Lamda parameter for gradient penalty.        \n    """"""\n    def __init__(self, ndf=128, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf)\n        self.block2 = DBlock(self.ndf, self.ndf, downsample=True)\n        self.block3 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.block4 = DBlock(self.ndf, self.ndf, downsample=False)\n        self.l5 = nn.Linear(self.ndf, 1)\n\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l5.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.activation(h)\n\n        # Global average pooling\n        h = torch.mean(h, dim=(2, 3))  # WGAN uses mean pooling\n        output = self.l5(h)\n\n        return output\n'"
torch_mimicry/nets/wgan_gp/wgan_gp_48.py,3,"b'""""""\nImplementation of WGAN-GP for image size 48.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.wgan_gp import wgan_gp_base\nfrom torch_mimicry.nets.wgan_gp.wgan_gp_resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass WGANGPGenerator48(wgan_gp_base.WGANGPBaseGenerator):\n    r""""""\n    ResNet backbone generator for WGAN-GP.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, nz=128, ngf=512, bottom_width=6, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block3 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block4 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.b5 = nn.BatchNorm2d(self.ngf >> 3)\n        self.c5 = nn.Conv2d(self.ngf >> 3, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.b5(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c5(h))\n\n        return h\n\n\nclass WGANGPDiscriminator48(wgan_gp_base.WGANGPBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for WGAN-GP.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n        gp_scale (float): Lamda parameter for gradient penalty.        \n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4)\n        self.block2 = DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True)\n        self.block3 = DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True)\n        self.block4 = DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True)\n        self.block5 = DBlock(self.ndf >> 1, self.ndf, downsample=False)\n        self.l6 = nn.Linear(self.ndf, 1)\n\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l6.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.activation(h)\n\n        # Global average pooling\n        h = torch.mean(h, dim=(2, 3))  # WGAN uses mean pooling\n        output = self.l6(h)\n\n        return output\n'"
torch_mimicry/nets/wgan_gp/wgan_gp_64.py,3,"b'""""""\nImplementation of WGAN-GP for image size 64.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom torch_mimicry.nets.wgan_gp import wgan_gp_base\nfrom torch_mimicry.nets.wgan_gp.wgan_gp_resblocks import DBlockOptimized, DBlock, GBlock\n\n\nclass WGANGPGenerator64(wgan_gp_base.WGANGPBaseGenerator):\n    r""""""\n    ResNet backbone generator for WGAN-GP.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self, nz=128, ngf=1024, bottom_width=4, **kwargs):\n        super().__init__(nz=nz, ngf=ngf, bottom_width=bottom_width, **kwargs)\n\n        # Build the layers\n        self.l1 = nn.Linear(self.nz, (self.bottom_width**2) * self.ngf)\n        self.block2 = GBlock(self.ngf, self.ngf >> 1, upsample=True)\n        self.block3 = GBlock(self.ngf >> 1, self.ngf >> 2, upsample=True)\n        self.block4 = GBlock(self.ngf >> 2, self.ngf >> 3, upsample=True)\n        self.block5 = GBlock(self.ngf >> 3, self.ngf >> 4, upsample=True)\n        self.b6 = nn.BatchNorm2d(self.ngf >> 4)\n        self.c6 = nn.Conv2d(self.ngf >> 4, 3, 3, 1, padding=1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l1.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of noise vectors into a batch of fake images.\n\n        Args:\n            x (Tensor): A batch of noise vectors of shape (N, nz).\n\n        Returns:\n            Tensor: A batch of fake images of shape (N, C, H, W).\n        """"""\n        h = self.l1(x)\n        h = h.view(x.shape[0], -1, self.bottom_width, self.bottom_width)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.b6(h)\n        h = self.activation(h)\n        h = torch.tanh(self.c6(h))\n\n        return h\n\n\nclass WGANGPDiscriminator64(wgan_gp_base.WGANGPBaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for WGAN-GP.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n        gp_scale (float): Lamda parameter for gradient penalty.        \n    """"""\n    def __init__(self, ndf=1024, **kwargs):\n        super().__init__(ndf=ndf, **kwargs)\n\n        # Build layers\n        self.block1 = DBlockOptimized(3, self.ndf >> 4)\n        self.block2 = DBlock(self.ndf >> 4, self.ndf >> 3, downsample=True)\n        self.block3 = DBlock(self.ndf >> 3, self.ndf >> 2, downsample=True)\n        self.block4 = DBlock(self.ndf >> 2, self.ndf >> 1, downsample=True)\n        self.block5 = DBlock(self.ndf >> 1, self.ndf, downsample=True)\n        self.l6 = nn.Linear(self.ndf, 1)\n        self.activation = nn.ReLU(True)\n\n        # Initialise the weights\n        nn.init.xavier_uniform_(self.l6.weight.data, 1.0)\n\n    def forward(self, x):\n        r""""""\n        Feedforwards a batch of real/fake images and produces a batch of GAN logits.\n\n        Args:\n            x (Tensor): A batch of images of shape (N, C, H, W).\n\n        Returns:\n            Tensor: A batch of GAN logits of shape (N, 1).\n        """"""\n        h = x\n        h = self.block1(h)\n        h = self.block2(h)\n        h = self.block3(h)\n        h = self.block4(h)\n        h = self.block5(h)\n        h = self.activation(h)\n\n        # Global average pooling\n        h = torch.mean(h, dim=(2, 3))  # WGAN uses mean pooling\n        output = self.l6(h)\n\n        return output\n'"
torch_mimicry/nets/wgan_gp/wgan_gp_base.py,4,"b'""""""\nBase class implementation of WGAN-GP.\n""""""\nimport torch\nfrom torch import autograd\n\nfrom torch_mimicry.nets.gan import gan\n\n\nclass WGANGPBaseGenerator(gan.BaseGenerator):\n    r""""""\n    ResNet backbone generator for WGAN-GP.\n\n    Attributes:\n        nz (int): Noise dimension for upsampling.\n        ngf (int): Variable controlling generator feature map sizes.\n        bottom_width (int): Starting width for upsampling generator output to an image.\n        loss_type (str): Name of loss to use for GAN loss.        \n    """"""\n    def __init__(self,\n                 nz,\n                 ngf,\n                 bottom_width,\n                 loss_type=\'wasserstein\',\n                 **kwargs):\n        super().__init__(nz=nz,\n                         ngf=ngf,\n                         bottom_width=bottom_width,\n                         loss_type=loss_type,\n                         **kwargs)\n\n    def train_step(self,\n                   real_batch,\n                   netD,\n                   optG,\n                   log_data,\n                   device=None,\n                   global_step=None,\n                   **kwargs):\n        r""""""\n        Takes one training step for G.\n\n        Args:\n            real_batch (Tensor): A batch of real images of shape (N, C, H, W).\n                Used for obtaining current batch size.\n            netD (nn.Module): Discriminator model for obtaining losses.\n            optG (Optimizer): Optimizer for updating generator\'s parameters.\n            log_data (MetricLog): An object to add custom metrics for visualisations.\n            device (torch.device): Device to use for running the model.\n            global_step (int): Variable to sync training, logging and checkpointing.\n                Useful for dynamic changes to model amidst training.\n\n        Returns:\n            MetricLog: Returns MetricLog object containing updated logging variables after 1 training step.\n\n        """"""\n        self.zero_grad()\n\n        # Get only batch size from real batch\n        batch_size = real_batch[0].shape[0]\n\n        # Produce fake images\n        fake_images = self.generate_images(num_images=batch_size,\n                                           device=device)\n\n        # Compute output logit of D thinking image real\n        output = netD(fake_images)\n\n        # Compute loss\n        errG = self.compute_gan_loss(output)\n\n        # Backprop and update gradients\n        errG.backward()\n        optG.step()\n\n        # Log statistics\n        log_data.add_metric(\'errG\', errG, group=\'loss\')\n\n        return log_data\n\n\nclass WGANGPBaseDiscriminator(gan.BaseDiscriminator):\n    r""""""\n    ResNet backbone discriminator for WGAN-GP.\n\n    Attributes:\n        ndf (int): Variable controlling discriminator feature map sizes.\n        loss_type (str): Name of loss to use for GAN loss.\n        gp_scale (float): Lamda parameter for gradient penalty.        \n    """"""\n    def __init__(self, ndf, loss_type=\'wasserstein\', gp_scale=10.0, **kwargs):\n        super().__init__(ndf=ndf, loss_type=loss_type, **kwargs)\n        self.gp_scale = gp_scale\n\n    def train_step(self,\n                   real_batch,\n                   netG,\n                   optD,\n                   log_data,\n                   device=None,\n                   global_step=None,\n                   **kwargs):\n        r""""""\n        Takes one training step for D.\n\n        Args:\n            real_batch (Tensor): A batch of real images of shape (N, C, H, W).\n            netG (nn.Module): Generator model for obtaining fake images.\n            optD (Optimizer): Optimizer for updating discriminator\'s parameters.\n            device (torch.device): Device to use for running the model.\n            log_data (MetricLog): An object to add custom metrics for visualisations.\n            global_step (int): Variable to sync training, logging and checkpointing.\n                Useful for dynamic changes to model amidst training.\n\n        Returns:\n            MetricLog: Returns MetricLog object containing updated logging variables after 1 training step.\n\n        """"""\n        self.zero_grad()\n\n        # Produce real images\n        real_images, _ = real_batch\n        batch_size = real_images.shape[0]  # Match batch sizes for last iter\n\n        # Produce fake images\n        fake_images = netG.generate_images(num_images=batch_size,\n                                           device=device).detach()\n\n        # Produce logits for real and fake images\n        output_real = self.forward(real_images)\n        output_fake = self.forward(fake_images)\n\n        # Compute losses\n        errD = self.compute_gan_loss(output_real=output_real,\n                                     output_fake=output_fake)\n\n        errD_GP = self.compute_gradient_penalty_loss(real_images=real_images,\n                                                     fake_images=fake_images,\n                                                     gp_scale=self.gp_scale)\n\n        # Backprop and update gradients\n        errD_total = errD + errD_GP\n        errD_total.backward()\n        optD.step()\n\n        # Compute probabilities\n        D_x, D_Gz = self.compute_probs(output_real=output_real,\n                                       output_fake=output_fake)\n\n        log_data.add_metric(\'errD\', errD, group=\'loss\')\n        log_data.add_metric(\'D(x)\', D_x, group=\'prob\')\n        log_data.add_metric(\'D(G(z))\', D_Gz, group=\'prob\')\n\n        return log_data\n\n    def compute_gradient_penalty_loss(self,\n                                      real_images,\n                                      fake_images,\n                                      gp_scale=10.0):\n        r""""""\n        Computes gradient penalty loss, as based on:\n        https://github.com/jalola/improved-wgan-pytorch/blob/master/gan_train.py\n        \n        Args:\n            real_images (Tensor): A batch of real images of shape (N, 3, H, W).\n            fake_images (Tensor): A batch of fake images of shape (N, 3, H, W).\n            gp_scale (float): Gradient penalty lamda parameter.\n\n        Returns:\n            Tensor: Scalar gradient penalty loss.\n        """"""\n        # Obtain parameters\n        N, _, H, W = real_images.shape\n        device = real_images.device\n\n        # Randomly sample some alpha between 0 and 1 for interpolation\n        # where alpha is of the same shape for elementwise multiplication.\n        alpha = torch.rand(N, 1)\n        alpha = alpha.expand(N, int(real_images.nelement() / N)).contiguous()\n        alpha = alpha.view(N, 3, H, W)\n        alpha = alpha.to(device)\n\n        # Obtain interpolates on line between real/fake images.\n        interpolates = alpha * real_images.detach() \\\n            + ((1 - alpha) * fake_images.detach())\n        interpolates = interpolates.to(device)\n        interpolates.requires_grad_(True)\n\n        # Get gradients of interpolates\n        disc_interpolates = self.forward(interpolates)\n        gradients = autograd.grad(outputs=disc_interpolates,\n                                  inputs=interpolates,\n                                  grad_outputs=torch.ones(\n                                      disc_interpolates.size()).to(device),\n                                  create_graph=True,\n                                  retain_graph=True,\n                                  only_inputs=True)[0]\n        gradients = gradients.view(gradients.size(0), -1)\n\n        # Compute GP loss\n        gradient_penalty = (\n            (gradients.norm(2, dim=1) - 1)**2).mean() * gp_scale\n\n        return gradient_penalty\n'"
torch_mimicry/nets/wgan_gp/wgan_gp_resblocks.py,2,"b'""""""\nResBlocks for WGAN-GP.\n""""""\nimport torch.nn as nn\nimport torch.functional as F\n\nfrom torch_mimicry.modules import resblocks\n\n\nclass GBlock(resblocks.GBlock):\n    r""""""\n    Residual block for generator. \n    Modifies original resblock definitions with small changes.\n\n    Uses bilinear (rather than nearest) interpolation, and align_corners\n    set to False. This is as per how torchvision does upsampling, as seen in:\n    https://github.com/pytorch/vision/blob/master/torchvision/models/segmentation/_utils.py\n\n    Attributes:\n        in_channels (int): The channel size of input feature map.\n        out_channels (int): The channel size of output feature map.\n        hidden_channels (int): The channel size of intermediate feature maps.\n        upsample (bool): If True, upsamples the input feature map.\n        num_classes (int): If more than 0, uses conditional batch norm instead.\n        spectral_norm (bool): If True, uses spectral norm for convolutional layers.\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 hidden_channels=None,\n                 upsample=False,\n                 num_classes=0,\n                 spectral_norm=False,\n                 **kwargs):\n        super().__init__(in_channels=in_channels,\n                         out_channels=out_channels,\n                         hidden_channels=hidden_channels,\n                         upsample=upsample,\n                         num_classes=num_classes,\n                         spectral_norm=spectral_norm,\n                         **kwargs)\n\n        # Redefine shortcut layer without act.\n        if self.learnable_sc:\n            self.c_sc = nn.Conv2d(self.in_channels,\n                                  self.out_channels,\n                                  1,\n                                  1,\n                                  padding=0)\n\n\nclass DBlock(resblocks.DBlock):\n    r""""""\n    Residual block for discriminator.\n\n    Modifies original resblock definition by including layer norm and removing\n    act for shortcut. Convs are LN-ReLU-Conv. See official TF code:\n    https://github.com/igul222/improved_wgan_training/blob/master/gan_cifar_resnet.py#L105\n\n    Attributes:\n        in_channels (int): The channel size of input feature map.\n        out_channels (int): The channel size of output feature map.\n        hidden_channels (int): The channel size of intermediate feature maps.\n        downsample (bool): If True, downsamples the input feature map.\n        spectral_norm (bool): If True, uses spectral norm for convolutional layers.        \n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 hidden_channels=None,\n                 downsample=False,\n                 spectral_norm=False,\n                 **kwargs):\n        super().__init__(in_channels=in_channels,\n                         out_channels=out_channels,\n                         hidden_channels=hidden_channels,\n                         downsample=downsample,\n                         spectral_norm=spectral_norm,\n                         **kwargs)\n\n        # Redefine shortcut layer without act.\n        # TODO: Maybe can encapsulate defining of learnable sc in a fn\n        # then override it later? Might be cleaner.\n        if self.learnable_sc:\n            self.c_sc = nn.Conv2d(self.in_channels, self.out_channels, 1, 1, 0)\n\n        self.norm1 = None\n        self.norm2 = None\n\n    def residual(self, x):\n        r""""""\n        Helper function for feedforwarding through main layers.\n        """"""\n        if self.norm1 is None:\n            self.norm1 = nn.LayerNorm(\n                [self.in_channels, x.shape[2], x.shape[3]])\n\n        h = x\n        h = self.norm1(h)\n        h = self.activation(h)\n        h = self.c1(h)\n\n        if self.norm2 is None:\n            self.norm2 = nn.LayerNorm(\n                [self.hidden_channels, h.shape[2], h.shape[3]])\n\n        h = self.norm2(h)\n        h = self.activation(h)\n        h = self.c2(h)\n        if self.downsample:\n            h = F.avg_pool2d(h, 2)\n\n        return h\n\n\nclass DBlockOptimized(resblocks.DBlockOptimized):\n    r""""""\n    Optimized residual block for discriminator.\n\n    Does not have any normalisation. See official TF Code:\n    https://github.com/igul222/improved_wgan_training/blob/master/gan_cifar_resnet.py#L139\n\n    Attributes:\n        in_channels (int): The channel size of input feature map.\n        out_channels (int): The channel size of output feature map.\n        spectral_norm (bool): If True, uses spectral norm for convolutional layers.        \n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 spectral_norm=False,\n                 **kwargs):\n        super().__init__(in_channels=in_channels,\n                         out_channels=out_channels,\n                         spectral_norm=spectral_norm,\n                         **kwargs)\n\n        # Redefine shortcut layer\n        self.c_sc = nn.Conv2d(self.in_channels, self.out_channels, 1, 1, 0)\n'"
