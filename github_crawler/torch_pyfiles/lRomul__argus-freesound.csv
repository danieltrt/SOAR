file_path,api_count,code
after_train_folds.py,1,"b'from argus import load_model\nfrom argus.callbacks import MonitorCheckpoint, LoggingToFile\n\nfrom torch.utils.data import DataLoader\n\nfrom src.datasets import FreesoundDataset, FreesoundNoisyDataset, RandomDataset\nfrom src.mixers import RandomMixer, AddMixer, SigmoidConcatMixer, UseMixerWithProb\nfrom src.transforms import get_transforms\nfrom src.argus_models import FreesoundModel\nfrom src.lr_scheduler import CosineAnnealing\nfrom src.utils import load_noisy_data, load_folds_data, get_best_model_path\nfrom src import config\n\n\nBASE_EXPERIMENT_NAME = \'noisy_mixup_001\'\nEXPERIMENT_NAME = \'noisy_mixup_001_after_001\'\n\nBATCH_SIZE = 128\nCROP_SIZE = 256\nDATASET_SIZE = 128 * 256\nNOISY_PROB = 0.33\nMIXER_PROB = 0.66\nWRAP_PAD_PROB = 0.5\nBASE_LR = 0.0003\nif config.kernel:\n    NUM_WORKERS = 2\nelse:\n    NUM_WORKERS = 8\nDEVICE = \'cuda\'\nBASE_DIR = config.experiments_dir / BASE_EXPERIMENT_NAME\nSAVE_DIR = config.experiments_dir / EXPERIMENT_NAME\n\n\ndef train_fold(base_model_path, save_dir, train_folds, val_folds,\n               folds_data, noisy_data):\n    train_transfrom = get_transforms(train=True,\n                                     size=CROP_SIZE,\n                                     wrap_pad_prob=WRAP_PAD_PROB)\n\n    mixer = RandomMixer([\n        SigmoidConcatMixer(sigmoid_range=(3, 12)),\n        AddMixer(alpha_dist=\'uniform\')\n    ], p=[0.6, 0.4])\n    mixer = UseMixerWithProb(mixer, prob=MIXER_PROB)\n\n    curated_dataset = FreesoundDataset(folds_data, train_folds,\n                                       transform=train_transfrom,\n                                       mixer=mixer)\n    noisy_dataset = FreesoundNoisyDataset(noisy_data,\n                                          transform=train_transfrom,\n                                          mixer=mixer)\n    train_dataset = RandomDataset([noisy_dataset, curated_dataset],\n                                  p=[NOISY_PROB, 1 - NOISY_PROB],\n                                  size=DATASET_SIZE)\n\n    val_dataset = FreesoundDataset(folds_data, val_folds,\n                                   get_transforms(False, CROP_SIZE))\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              shuffle=True, drop_last=True,\n                              num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2,\n                            shuffle=False, num_workers=NUM_WORKERS)\n\n    model = load_model(base_model_path, device=DEVICE)\n    model.set_lr(BASE_LR)\n\n    callbacks = [\n        MonitorCheckpoint(save_dir, monitor=\'val_lwlrap\', max_saves=3),\n        CosineAnnealing(T_0=10, T_mult=2, eta_min=0.00001),\n        LoggingToFile(save_dir / \'log.txt\'),\n    ]\n\n    model.fit(train_loader,\n              val_loader=val_loader,\n              max_epochs=150,\n              callbacks=callbacks,\n              metrics=[\'multi_accuracy\', \'lwlrap\'])\n\n\nif __name__ == ""__main__"":\n    if not SAVE_DIR.exists():\n        SAVE_DIR.mkdir(parents=True, exist_ok=True)\n    else:\n        print(f""Folder {SAVE_DIR} already exists."")\n\n    with open(SAVE_DIR / \'source.py\', \'w\') as outfile:\n        outfile.write(open(__file__).read())\n\n    folds_data = load_folds_data()\n    noisy_data = load_noisy_data()\n\n    for fold in config.folds:\n        val_folds = [fold]\n        train_folds = list(set(config.folds) - set(val_folds))\n        save_fold_dir = SAVE_DIR / f\'fold_{fold}\'\n        base_model_path = get_best_model_path(BASE_DIR / f\'fold_{fold}\')\n        print(f""Base model path: {base_model_path}"")\n        print(f""Val folds: {val_folds}, Train folds: {train_folds}"")\n        print(f""Fold save dir {save_fold_dir}"")\n        train_fold(base_model_path, save_fold_dir, train_folds, val_folds,\n                   folds_data, noisy_data)\n'"
blend_kernel_template.py,0,"b'import gzip\nimport base64\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nKERNEL_MODE = ""predict""\n\n# this is base64 encoded source code\nfile_data: Dict = {file_data}\n\n\nfor path, encoded in file_data.items():\n    print(path)\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_bytes(gzip.decompress(base64.b64decode(encoded)))\n\n\ndef run(command):\n    os.system(\'export PYTHONPATH=${PYTHONPATH}:/kaggle/working && \'\n              f\'export MODE={KERNEL_MODE} && \' + command)\n\n\nrun(\'python make_folds.py\')\nrun(\'python blend_predict.py\')\nrun(\'rm -rf argus src\')\n'"
blend_predict.py,0,"b'import numpy as np\nimport pandas as pd\n\nfrom src.predictor import Predictor\nfrom src.transforms import get_transforms\nfrom src.utils import get_best_model_path, gmean_preds_blend\nfrom src.datasets import get_test_data\nfrom src import config\n\n\nEXPERIMENTS = [\n    \'auxiliary_009\',\n    \'auxiliary_010\',\n    \'auxiliary_011\'\n]\n\nDEVICE = \'cuda\'\nCROP_SIZE = 256\nBATCH_SIZE = 16\n\n\ndef pred_test(predictor, test_data):\n    fname_lst, images_lst = test_data\n    pred_lst = []\n    for image in images_lst:\n        pred = predictor.predict(image)\n\n        pred = pred.mean(axis=0)\n        pred_lst.append(pred)\n\n    preds = np.stack(pred_lst, axis=0)\n    pred_df = pd.DataFrame(data=preds,\n                           index=fname_lst,\n                           columns=config.classes)\n    pred_df.index.name = \'fname\'\n\n    return pred_df\n\n\ndef experiment_pred(experiment_dir, test_data):\n    print(f""Start predict: {experiment_dir}"")\n    transforms = get_transforms(False, CROP_SIZE)\n\n    pred_df_lst = []\n    for fold in config.folds:\n        print(""Predict fold"", fold)\n        fold_dir = experiment_dir / f\'fold_{fold}\'\n        model_path = get_best_model_path(fold_dir)\n        print(""Model path"", model_path)\n        predictor = Predictor(model_path, transforms,\n                              BATCH_SIZE,\n                              (config.audio.n_mels, CROP_SIZE),\n                              (config.audio.n_mels, CROP_SIZE//4),\n                              device=DEVICE)\n\n        pred_df = pred_test(predictor, test_data)\n        pred_df_lst.append(pred_df)\n\n    pred_df = gmean_preds_blend(pred_df_lst)\n    return pred_df\n\n\nif __name__ == ""__main__"":\n    print(""Experiments"", EXPERIMENTS)\n    test_data = get_test_data()\n\n    exp_pred_df_lst = []\n    for experiment in EXPERIMENTS:\n        experiment_dir = config.experiments_dir / experiment\n        exp_pred_df = experiment_pred(experiment_dir, test_data)\n        exp_pred_df_lst.append(exp_pred_df)\n\n    blend_pred_df = gmean_preds_blend(exp_pred_df_lst)\n    blend_pred_df.to_csv(\'submission.csv\')\n'"
build_kernel.py,0,"b'#!/usr/bin/env python3\n# Kaggle script build system template: https://github.com/lopuhin/kaggle-script-template\nimport os\nimport base64\nimport gzip\nfrom pathlib import Path\n\n\nIGNORE_LIST = [""data"", ""build""]\n\nPACKAGES = [\n    (\'https://github.com/lRomul/argus.git\', \'v0.0.8\')\n]\n\n\ndef encode_file(path: Path) -> str:\n    compressed = gzip.compress(path.read_bytes(), compresslevel=9)\n    return base64.b64encode(compressed).decode(\'utf-8\')\n\n\ndef check_ignore(path: Path, ignore_list):\n    if not path.is_file():\n        return False\n    for ignore in ignore_list:\n        if str(path).startswith(ignore):\n            return False\n    return True\n\n\ndef clone_package(git_url, branch=""master""):\n    name = Path(git_url).stem\n    os.system(\'mkdir -p tmp\')\n    os.system(f\'rm -rf tmp/{name}\')\n    os.system(f\'cd tmp && git clone --depth 1 -b {branch} {git_url}\')\n    os.system(f\'cp -R tmp/{name}/{name} .\')\n    os.system(f\'rm -rf tmp/{name}\')\n\n\ndef build_script(ignore_list, packages, template_name=\'kernel_template.py\'):\n    to_encode = []\n\n    for path in Path(\'.\').glob(\'**/*.py\'):\n        if check_ignore(path, ignore_list + packages):\n            to_encode.append(path)\n\n    for package, branch in packages:\n        clone_package(package, branch)\n        package_name = Path(package).stem\n        for path in Path(package_name).glob(\'**/*\'):\n            if check_ignore(path, ignore_list):\n                to_encode.append(path)\n\n    file_data = {str(path): encode_file(path) for path in to_encode}\n    print(""Encoded python files:"")\n    for path in file_data:\n        print(path)\n    template = Path(template_name).read_text(\'utf8\')\n    (Path(\'kernel\') / template_name).write_text(\n        template.replace(\'{file_data}\', str(file_data)),\n        encoding=\'utf8\')\n\n\nif __name__ == \'__main__\':\n    os.system(\'rm -rf kernel && mkdir kernel\')\n    build_script(IGNORE_LIST, PACKAGES,\n                 template_name=\'kernel_template.py\')\n    build_script(IGNORE_LIST, PACKAGES,\n                 template_name=\'blend_kernel_template.py\')\n    build_script(IGNORE_LIST, PACKAGES,\n                 template_name=\'stacking_kernel_template.py\')\n'"
kernel_template.py,0,"b'import gzip\nimport base64\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nEXPERIMENT_NAME = \'corr_noisy_007\'\nKERNEL_MODE = ""predict""  # ""train"" or ""predict""\n\n# this is base64 encoded source code\nfile_data: Dict = {file_data}\n\n\nfor path, encoded in file_data.items():\n    print(path)\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_bytes(gzip.decompress(base64.b64decode(encoded)))\n\n\ndef run(command):\n    os.system(\'export PYTHONPATH=${PYTHONPATH}:/kaggle/working && \'\n              f\'export MODE={KERNEL_MODE} && \' + command)\n\n\nrun(\'python make_folds.py\')\nif KERNEL_MODE == ""train"":\n    run(f\'python train_folds.py --experiment {EXPERIMENT_NAME}\')\nelse:\n    run(f\'python predict_folds.py --experiment {EXPERIMENT_NAME}\')\nrun(\'rm -rf argus src\')\n'"
make_folds.py,0,"b'import random\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold\n\nfrom src import config\n\n\nif __name__ == \'__main__\':\n    random_state = 42\n\n    random.seed(random_state)\n    np.random.seed(random_state)\n\n    train_curated_df = pd.read_csv(config.train_curated_csv_path)\n    train_curated_df[\'fold\'] = -1\n    file_paths = train_curated_df.fname.apply(lambda x: config.train_curated_dir / x)\n    train_curated_df[\'file_path\'] = file_paths\n\n    kf = KFold(n_splits=config.n_folds, random_state=random_state, shuffle=True)\n\n    for fold, (_, val_index) in enumerate(kf.split(train_curated_df)):\n        train_curated_df.iloc[val_index, 2] = fold\n\n    train_curated_df.to_csv(config.train_folds_path, index=False)\n    print(f""Train folds saved to \'{config.train_folds_path}\'"")\n'"
predict_folds.py,0,"b'import json\nimport argparse\nimport numpy as np\nimport pandas as pd\n\nfrom src.predictor import Predictor\nfrom src.audio import read_as_melspectrogram\nfrom src.transforms import get_transforms\nfrom src.metrics import LwlrapBase\nfrom src.utils import get_best_model_path, gmean_preds_blend\nfrom src.datasets import get_test_data\nfrom src import config\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--experiment\', required=True, type=str)\nargs = parser.parse_args()\n\n\nEXPERIMENT_DIR = config.experiments_dir / args.experiment\nPREDICTION_DIR = config.predictions_dir / args.experiment\nDEVICE = \'cuda\'\nCROP_SIZE = 256\nBATCH_SIZE = 16\n\n\ndef pred_val_fold(predictor, fold):\n    fold_prediction_dir = PREDICTION_DIR / f\'fold_{fold}\' / \'val\'\n    fold_prediction_dir.mkdir(parents=True, exist_ok=True)\n\n    train_folds_df = pd.read_csv(config.train_folds_path)\n    train_folds_df = train_folds_df[train_folds_df.fold == fold]\n\n    fname_lst = []\n    pred_lst = []\n    for i, row in train_folds_df.iterrows():\n        image = read_as_melspectrogram(row.file_path)\n        pred = predictor.predict(image)\n\n        pred_path = fold_prediction_dir / f\'{row.fname}.npy\'\n        np.save(pred_path, pred)\n\n        pred = pred.mean(axis=0)\n        pred_lst.append(pred)\n        fname_lst.append(row.fname)\n\n    preds = np.stack(pred_lst, axis=0)\n    probs_df = pd.DataFrame(data=preds,\n                            index=fname_lst,\n                            columns=config.classes)\n    probs_df.index.name = \'fname\'\n    probs_df.to_csv(fold_prediction_dir / \'probs.csv\')\n\n\ndef pred_test_fold(predictor, fold, test_data):\n    fold_prediction_dir = PREDICTION_DIR / f\'fold_{fold}\' / \'test\'\n    fold_prediction_dir.mkdir(parents=True, exist_ok=True)\n\n    fname_lst, images_lst = test_data\n    pred_lst = []\n    for fname, image in zip(fname_lst, images_lst):\n        pred = predictor.predict(image)\n\n        pred_path = fold_prediction_dir / f\'{fname}.npy\'\n        np.save(pred_path, pred)\n\n        pred = pred.mean(axis=0)\n        pred_lst.append(pred)\n\n    preds = np.stack(pred_lst, axis=0)\n    subm_df = pd.DataFrame(data=preds,\n                           index=fname_lst,\n                           columns=config.classes)\n    subm_df.index.name = \'fname\'\n    subm_df.to_csv(fold_prediction_dir / \'probs.csv\')\n\n\ndef blend_test_predictions():\n    probs_df_lst = []\n    for fold in config.folds:\n        fold_probs_path = PREDICTION_DIR / f\'fold_{fold}\' / \'test\' / \'probs.csv\'\n        probs_df = pd.read_csv(fold_probs_path)\n        probs_df.set_index(\'fname\', inplace=True)\n        probs_df_lst.append(probs_df)\n\n    blend_df = gmean_preds_blend(probs_df_lst)\n\n    if config.kernel:\n        blend_df.to_csv(\'submission.csv\')\n    else:\n        blend_df.to_csv(PREDICTION_DIR / \'probs.csv\')\n\n\ndef calc_lwlrap_on_val():\n    probs_df_lst = []\n    for fold in config.folds:\n        fold_probs_path = PREDICTION_DIR / f\'fold_{fold}\' / \'val\' / \'probs.csv\'\n        probs_df = pd.read_csv(fold_probs_path)\n        probs_df.set_index(\'fname\', inplace=True)\n        probs_df_lst.append(probs_df)\n\n    probs_df = pd.concat(probs_df_lst, axis=0)\n    train_curated_df = pd.read_csv(config.train_curated_csv_path)\n\n    lwlrap = LwlrapBase(config.classes)\n    for i, row in train_curated_df.iterrows():\n        target = np.zeros(len(config.classes))\n        for label in row.labels.split(\',\'):\n            target[config.class2index[label]] = 1.\n\n        pred = probs_df.loc[row.fname].values\n        lwlrap.accumulate(target[np.newaxis], pred[np.newaxis])\n\n    result = {\n        \'overall_lwlrap\': lwlrap.overall_lwlrap(),\n        \'per_class_lwlrap\': {cls: lwl for cls, lwl in zip(config.classes,\n                                                          lwlrap.per_class_lwlrap())}\n    }\n    print(result)\n    with open(PREDICTION_DIR / \'val_lwlrap.json\', \'w\') as file:\n        json.dump(result, file, indent=2)\n\n\nif __name__ == ""__main__"":\n    transforms = get_transforms(False, CROP_SIZE)\n    test_data = get_test_data()\n\n    for fold in config.folds:\n        print(""Predict fold"", fold)\n        fold_dir = EXPERIMENT_DIR / f\'fold_{fold}\'\n        model_path = get_best_model_path(fold_dir)\n        print(""Model path"", model_path)\n        predictor = Predictor(model_path, transforms,\n                              BATCH_SIZE,\n                              (config.audio.n_mels, CROP_SIZE),\n                              (config.audio.n_mels, CROP_SIZE//4),\n                              device=DEVICE)\n\n        if not config.kernel:\n            print(""Val predict"")\n            pred_val_fold(predictor, fold)\n\n        print(""Test predict"")\n        pred_test_fold(predictor, fold, test_data)\n\n    print(""Blend folds predictions"")\n    blend_test_predictions()\n\n    if not config.kernel:\n        print(""Calculate lwlrap metric on cv"")\n        calc_lwlrap_on_val()\n'"
random_search.py,2,"b'import torch\nimport numpy as np\nimport random\nimport json\nimport time\nfrom pprint import pprint\n\nfrom argus.callbacks import MonitorCheckpoint, \\\n    EarlyStopping, LoggingToFile, ReduceLROnPlateau\n\nfrom torch.utils.data import DataLoader\n\nfrom src.datasets import FreesoundDataset, CombinedDataset, FreesoundNoisyDataset\nfrom src.transforms import get_transforms\nfrom src.argus_models import FreesoundModel\nfrom src.utils import load_folds_data, load_noisy_data\nfrom src import config\n\n\nEXPERIMENT_NAME = \'noisy_lsoft_rs_002\'\nVAL_FOLDS = [0]\nTRAIN_FOLDS = [1, 2, 3, 4]\nBATCH_SIZE = 128\nCROP_SIZE = 128\nDATASET_SIZE = 128 * 256\nif config.kernel:\n    NUM_WORKERS = 2\nelse:\n    NUM_WORKERS = 8\nSAVE_DIR = config.experiments_dir / EXPERIMENT_NAME\nSTART_FROM = 0\n\n\ndef train_experiment(folds_data, noisy_data, num):\n    experiment_dir = SAVE_DIR / f\'{num:04}\'\n    np.random.seed(num)\n    random.seed(num)\n\n    random_params = {\n        \'p_dropout\': float(np.random.uniform(0.1, 0.3)),\n        \'batch_size\': int(np.random.choice([128])),\n        \'lr\': float(np.random.choice([0.001, 0.0006, 0.0003])),\n        \'add_prob\': float(np.random.uniform(0.0, 1.0)),\n        \'noisy_prob\': float(np.random.uniform(0.0, 1.0)),\n        \'lsoft_beta\': float(np.random.uniform(0.2, 0.8)),\n        \'noisy_weight\': float(np.random.uniform(0.3, 0.7)),\n        \'patience\': int(np.random.randint(2, 10)),\n        \'factor\': float(np.random.uniform(0.5, 0.8))\n    }\n    pprint(random_params)\n\n    params = {\n        \'nn_module\': (\'SimpleKaggle\', {\n            \'num_classes\': len(config.classes),\n            \'dropout\': random_params[\'p_dropout\'],\n            \'base_size\': 64\n        }),\n        \'loss\': (\'OnlyNoisyLSoftLoss\', {\n            \'beta\': random_params[\'lsoft_beta\'],\n            \'noisy_weight\': random_params[\'noisy_weight\'],\n            \'curated_weight\': 1 - random_params[\'noisy_weight\']\n        }),\n        \'optimizer\': (\'Adam\', {\'lr\': random_params[\'lr\']}),\n        \'device\': \'cuda\',\n        \'amp\': {\n            \'opt_level\': \'O2\',\n            \'keep_batchnorm_fp32\': True,\n            \'loss_scale\': ""dynamic""\n        }\n    }\n    pprint(params)\n    try:\n        train_transfrom = get_transforms(True, CROP_SIZE)\n        curated_dataset = FreesoundDataset(folds_data, TRAIN_FOLDS,\n                                           transform=train_transfrom,\n                                           add_prob=random_params[\'add_prob\'])\n        noisy_dataset = FreesoundNoisyDataset(noisy_data,\n                                              transform=train_transfrom)\n        train_dataset = CombinedDataset(noisy_dataset, curated_dataset,\n                                        noisy_prob=random_params[\'noisy_prob\'],\n                                        size=DATASET_SIZE)\n\n        val_dataset = FreesoundDataset(folds_data, VAL_FOLDS,\n                                       get_transforms(False, CROP_SIZE))\n        train_loader = DataLoader(train_dataset, batch_size=random_params[\'batch_size\'],\n                                  shuffle=True, drop_last=True,\n                                  num_workers=NUM_WORKERS)\n        val_loader = DataLoader(val_dataset, batch_size=random_params[\'batch_size\'] * 2,\n                                shuffle=False, num_workers=NUM_WORKERS)\n\n        model = FreesoundModel(params)\n\n        callbacks = [\n            MonitorCheckpoint(experiment_dir, monitor=\'val_lwlrap\', max_saves=1),\n            ReduceLROnPlateau(monitor=\'val_lwlrap\',\n                              patience=random_params[\'patience\'],\n                              factor=random_params[\'factor\'],\n                              min_lr=1e-8),\n            EarlyStopping(monitor=\'val_lwlrap\', patience=20),\n            LoggingToFile(experiment_dir / \'log.txt\'),\n        ]\n\n        with open(experiment_dir / \'random_params.json\', \'w\') as outfile:\n            json.dump(random_params, outfile)\n\n        model.fit(train_loader,\n                  val_loader=val_loader,\n                  max_epochs=100,\n                  callbacks=callbacks,\n                  metrics=[\'multi_accuracy\', \'lwlrap\'])\n    except KeyboardInterrupt as e:\n        raise e\n    except BaseException as e:\n        print(f""Exception \'{e}\' with random params \'{random_params}\'"")\n\n\nif __name__ == ""__main__"":\n    print(""Start load train data"")\n    noisy_data = load_noisy_data()\n    folds_data = load_folds_data()\n\n    for i in range(START_FROM, 10000):\n        train_experiment(folds_data, noisy_data, i)\n        time.sleep(5.0)\n        torch.cuda.empty_cache()\n        time.sleep(5.0)\n'"
stacking_kernel_template.py,0,"b'import gzip\nimport base64\nimport os\nfrom pathlib import Path\nfrom typing import Dict\n\nKERNEL_MODE = ""predict""\n\n# this is base64 encoded source code\nfile_data: Dict = {file_data}\n\n\nfor path, encoded in file_data.items():\n    print(path)\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_bytes(gzip.decompress(base64.b64decode(encoded)))\n\n\ndef run(command):\n    os.system(\'export PYTHONPATH=${PYTHONPATH}:/kaggle/working && \'\n              f\'export MODE={KERNEL_MODE} && \' + command)\n\n\nrun(\'python stacking_predict.py\')\nrun(\'rm -rf argus src\')\n'"
stacking_predict.py,0,"b'import numpy as np\nimport pandas as pd\nfrom scipy.stats.mstats import gmean\n\nfrom src.predictor import Predictor\nfrom src.transforms import get_transforms\nfrom src.utils import get_best_model_path\nfrom src.datasets import get_test_data\nfrom src import config\n\nfrom src.stacking.predictor import StackPredictor\n\n\nNAME = ""stacking_008""\n\nEXPERIMENTS = [\n    \'auxiliary_016\',\n    \'auxiliary_019\',\n    \'corr_noisy_003\',\n    \'corr_noisy_004\',\n    \'corr_noisy_007\',\n    \'corrections_002\',\n    \'corrections_003\'\n]\n\nSTACKING_EXPERIMENTS = [\n    \'stacking_008_fcnet_43040\',\n    \'stacking_008_fcnet_45041\',\n    \'stacking_008_fcnet_50013\'\n]\n\nDEVICE = \'cuda\'\nCROP_SIZE = 256\nBATCH_SIZE = 16\nSTACK_BATCH_SIZE = 256\nTILE_STEP = 2\n\n\ndef pred_test(predictor, images_lst):\n    pred_lst = []\n    for image in images_lst:\n        pred = predictor.predict(image)\n\n        pred = pred.mean(axis=0)\n        pred_lst.append(pred)\n\n    preds = np.stack(pred_lst, axis=0)\n    return preds\n\n\ndef experiment_pred(experiment_dir, images_lst):\n    print(f""Start predict: {experiment_dir}"")\n    transforms = get_transforms(False, CROP_SIZE)\n\n    pred_lst = []\n    for fold in config.folds:\n        print(""Predict fold"", fold)\n        fold_dir = experiment_dir / f\'fold_{fold}\'\n        model_path = get_best_model_path(fold_dir)\n        print(""Model path"", model_path)\n        predictor = Predictor(model_path, transforms,\n                              BATCH_SIZE,\n                              (config.audio.n_mels, CROP_SIZE),\n                              (config.audio.n_mels, CROP_SIZE//TILE_STEP),\n                              device=DEVICE)\n\n        pred = pred_test(predictor, images_lst)\n        pred_lst.append(pred)\n\n    preds = gmean(pred_lst, axis=0)\n    return preds\n\n\ndef stacking_pred(experiment_dir, stack_probs):\n    print(f""Start predict: {experiment_dir}"")\n\n    pred_lst = []\n    for fold in config.folds:\n        print(""Predict fold"", fold)\n        fold_dir = experiment_dir / f\'fold_{fold}\'\n        model_path = get_best_model_path(fold_dir)\n        print(""Model path"", model_path)\n        predictor = StackPredictor(model_path, STACK_BATCH_SIZE,\n                                   device=DEVICE)\n        pred = predictor.predict(stack_probs)\n        pred_lst.append(pred)\n\n    preds = gmean(pred_lst, axis=0)\n    return preds\n\n\nif __name__ == ""__main__"":\n    print(""Name"", NAME)\n    print(""Experiments"", EXPERIMENTS)\n    print(""Stacking experiments"", STACKING_EXPERIMENTS)\n    print(""Device"", DEVICE)\n    print(""Crop size"", CROP_SIZE)\n    print(""Batch size"", BATCH_SIZE)\n    print(""Stacking batch size"", STACK_BATCH_SIZE)\n    print(""Tile step"", TILE_STEP)\n\n    fname_lst, images_lst = get_test_data()\n\n    exp_pred_lst = []\n    for experiment in EXPERIMENTS:\n        experiment_dir = config.experiments_dir / experiment\n        exp_pred = experiment_pred(experiment_dir, images_lst)\n        exp_pred_lst.append(exp_pred)\n\n    stack_probs = np.concatenate(exp_pred_lst, axis=1)\n\n    stack_pred_lst = []\n    for experiment in STACKING_EXPERIMENTS:\n        experiment_dir = config.experiments_dir / experiment\n        stack_pred = stacking_pred(experiment_dir, stack_probs)\n        stack_pred_lst.append(stack_pred)\n\n    stack_pred = gmean(exp_pred_lst + stack_pred_lst, axis=0)\n\n    stack_pred_df = pd.DataFrame(data=stack_pred,\n                                 index=fname_lst,\n                                 columns=config.classes)\n    stack_pred_df.index.name = \'fname\'\n    stack_pred_df.to_csv(\'submission.csv\')\n'"
stacking_random_search.py,2,"b'import json\nimport time\nimport torch\nimport random\nimport numpy as np\nfrom pprint import pprint\n\nfrom argus.callbacks import MonitorCheckpoint, \\\n    EarlyStopping, LoggingToFile, ReduceLROnPlateau\n\nfrom torch.utils.data import DataLoader\n\nfrom src.stacking.datasets import get_out_of_folds_data, StackingDataset\nfrom src.stacking.transforms import get_transforms\nfrom src.stacking.argus_models import StackingModel\nfrom src import config\n\nEXPERIMENT_NAME = \'fcnet_stacking_rs_004\'\nSTART_FROM = 0\nEXPERIMENTS = [\n    \'auxiliary_007\',\n    \'auxiliary_010\',\n    \'auxiliary_012\',\n    \'auxiliary_014\'\n]\nDATASET_SIZE = 128 * 256\nCORRECTIONS = True\nif config.kernel:\n    NUM_WORKERS = 2\nelse:\n    NUM_WORKERS = 4\nSAVE_DIR = config.experiments_dir / EXPERIMENT_NAME\n\n\ndef train_folds(save_dir, folds_data):\n    random_params = {\n        \'base_size\': int(np.random.choice([64, 128, 256, 512])),\n        \'reduction_scale\': int(np.random.choice([2, 4, 8, 16])),\n        \'p_dropout\': float(np.random.uniform(0.0, 0.5)),\n        \'lr\': float(np.random.uniform(0.0001, 0.00001)),\n        \'patience\': int(np.random.randint(3, 12)),\n        \'factor\': float(np.random.uniform(0.5, 0.8)),\n        \'batch_size\': int(np.random.choice([32, 64, 128])),\n    }\n    pprint(random_params)\n\n    save_dir.mkdir(parents=True, exist_ok=True)\n    with open(save_dir / \'random_params.json\', \'w\') as outfile:\n        json.dump(random_params, outfile)\n\n    params = {\n        \'nn_module\': (\'FCNet\', {\n            \'in_channels\': len(config.classes) * len(EXPERIMENTS),\n            \'num_classes\': len(config.classes),\n            \'base_size\': random_params[\'base_size\'],\n            \'reduction_scale\': random_params[\'reduction_scale\'],\n            \'p_dropout\': random_params[\'p_dropout\']\n        }),\n        \'loss\': \'BCEWithLogitsLoss\',\n        \'optimizer\': (\'Adam\', {\'lr\': random_params[\'lr\']}),\n        \'device\': \'cuda\',\n    }\n\n    for fold in config.folds:\n        val_folds = [fold]\n        train_folds = list(set(config.folds) - set(val_folds))\n        save_fold_dir = save_dir / f\'fold_{fold}\'\n        print(f""Val folds: {val_folds}, Train folds: {train_folds}"")\n        print(f""Fold save dir {save_fold_dir}"")\n\n        train_dataset = StackingDataset(folds_data, train_folds,\n                                        get_transforms(True),\n                                        DATASET_SIZE)\n        val_dataset = StackingDataset(folds_data, val_folds,\n                                      get_transforms(False))\n\n        train_loader = DataLoader(train_dataset,\n                                  batch_size=random_params[\'batch_size\'],\n                                  shuffle=True, drop_last=True,\n                                  num_workers=NUM_WORKERS)\n        val_loader = DataLoader(val_dataset,\n                                batch_size=random_params[\'batch_size\'] * 2,\n                                shuffle=False, num_workers=NUM_WORKERS)\n\n        model = StackingModel(params)\n\n        callbacks = [\n            MonitorCheckpoint(save_fold_dir, monitor=\'val_lwlrap\', max_saves=1),\n            ReduceLROnPlateau(monitor=\'val_lwlrap\',\n                              patience=random_params[\'patience\'],\n                              factor=random_params[\'factor\'],\n                              min_lr=1e-8),\n            EarlyStopping(monitor=\'val_lwlrap\', patience=20),\n            LoggingToFile(save_fold_dir / \'log.txt\'),\n        ]\n\n        model.fit(train_loader,\n                  val_loader=val_loader,\n                  max_epochs=300,\n                  callbacks=callbacks,\n                  metrics=[\'multi_accuracy\', \'lwlrap\'])\n\n\nif __name__ == ""__main__"":\n    SAVE_DIR.mkdir(parents=True, exist_ok=True)\n    with open(SAVE_DIR / \'source.py\', \'w\') as outfile:\n        outfile.write(open(__file__).read())\n\n    if CORRECTIONS:\n        with open(config.corrections_json_path) as file:\n            corrections = json.load(file)\n        print(""Corrections:"", corrections)\n    else:\n        corrections = None\n\n    folds_data = get_out_of_folds_data(EXPERIMENTS, corrections)\n\n    for num in range(START_FROM, 10000):\n        np.random.seed(num)\n        random.seed(num)\n\n        save_dir = SAVE_DIR / f\'{num:04}\'\n        train_folds(save_dir, folds_data)\n        time.sleep(5.0)\n        torch.cuda.empty_cache()\n        time.sleep(5.0)\n'"
stacking_val_predict.py,0,"b'import json\nimport numpy as np\nimport pandas as pd\n\nfrom src.stacking.datasets import load_fname_probs\nfrom src.stacking.predictor import StackPredictor\nfrom src.metrics import LwlrapBase\nfrom src.utils import get_best_model_path\nfrom src import config\n\n\nSTACKING_EXPERIMENT = ""stacking_008_fcnet_50013""\n\nEXPERIMENTS = [\n    \'auxiliary_016\',\n    \'auxiliary_019\',\n    \'corr_noisy_003\',\n    \'corr_noisy_004\',\n    \'corr_noisy_007\',\n    \'corrections_002\',\n    \'corrections_003\'\n]\n\nEXPERIMENT_DIR = config.experiments_dir / STACKING_EXPERIMENT\nPREDICTION_DIR = config.predictions_dir / STACKING_EXPERIMENT\nDEVICE = \'cuda\'\nBATCH_SIZE = 256\n\n\ndef pred_val_fold(predictor, fold):\n    fold_prediction_dir = PREDICTION_DIR / f\'fold_{fold}\' / \'val\'\n    fold_prediction_dir.mkdir(parents=True, exist_ok=True)\n\n    train_folds_df = pd.read_csv(config.train_folds_path)\n    train_folds_df = train_folds_df[train_folds_df.fold == fold]\n\n    fname_lst = []\n    probs_lst = []\n    for i, row in train_folds_df.iterrows():\n        probs = load_fname_probs(EXPERIMENTS, fold, row.fname)\n\n        probs_lst.append(probs.mean(axis=0))\n        fname_lst.append(row.fname)\n\n    stack_probs = np.stack(probs_lst, axis=0)\n    preds = predictor.predict(stack_probs)\n\n    probs_df = pd.DataFrame(data=list(preds),\n                            index=fname_lst,\n                            columns=config.classes)\n    probs_df.index.name = \'fname\'\n    probs_df.to_csv(fold_prediction_dir / \'probs.csv\')\n\n\ndef calc_lwlrap_on_val():\n    probs_df_lst = []\n    for fold in config.folds:\n        fold_probs_path = PREDICTION_DIR / f\'fold_{fold}\' / \'val\' / \'probs.csv\'\n        probs_df = pd.read_csv(fold_probs_path)\n        probs_df.set_index(\'fname\', inplace=True)\n        probs_df_lst.append(probs_df)\n\n    probs_df = pd.concat(probs_df_lst, axis=0)\n    train_curated_df = pd.read_csv(config.train_curated_csv_path)\n\n    lwlrap = LwlrapBase(config.classes)\n    for i, row in train_curated_df.iterrows():\n        target = np.zeros(len(config.classes))\n        for label in row.labels.split(\',\'):\n            target[config.class2index[label]] = 1.\n\n        pred = probs_df.loc[row.fname].values\n        lwlrap.accumulate(target[np.newaxis], pred[np.newaxis])\n\n    result = {\n        \'overall_lwlrap\': lwlrap.overall_lwlrap(),\n        \'per_class_lwlrap\': {cls: lwl for cls, lwl in zip(config.classes,\n                                                          lwlrap.per_class_lwlrap())}\n    }\n    print(result)\n    with open(PREDICTION_DIR / \'val_lwlrap.json\', \'w\') as file:\n        json.dump(result, file, indent=2)\n\n\nif __name__ == ""__main__"":\n    for fold in config.folds:\n        print(""Predict fold"", fold)\n        fold_dir = EXPERIMENT_DIR / f\'fold_{fold}\'\n        model_path = get_best_model_path(fold_dir)\n        print(""Model path"", model_path)\n        predictor = StackPredictor(model_path,\n                                   BATCH_SIZE,\n                                   device=DEVICE)\n\n        print(""Val predict"")\n        pred_val_fold(predictor, fold)\n\n    print(""Calculate lwlrap metric on cv"")\n    calc_lwlrap_on_val()\n'"
train_folds.py,1,"b'import json\nimport argparse\n\nfrom argus.callbacks import MonitorCheckpoint, \\\n    EarlyStopping, LoggingToFile, ReduceLROnPlateau\n\nfrom torch.utils.data import DataLoader\n\nfrom src.datasets import FreesoundDataset, FreesoundNoisyDataset, RandomDataset\nfrom src.datasets import get_corrected_noisy_data, FreesoundCorrectedNoisyDataset\nfrom src.mixers import RandomMixer, AddMixer, SigmoidConcatMixer, UseMixerWithProb\nfrom src.transforms import get_transforms\nfrom src.argus_models import FreesoundModel\nfrom src.utils import load_noisy_data, load_folds_data\nfrom src import config\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--experiment\', required=True, type=str)\nargs = parser.parse_args()\n\nBATCH_SIZE = 128\nCROP_SIZE = 256\nDATASET_SIZE = 128 * 256\nNOISY_PROB = 0.01\nCORR_NOISY_PROB = 0.42\nMIXER_PROB = 0.8\nWRAP_PAD_PROB = 0.5\nCORRECTIONS = True\nif config.kernel:\n    NUM_WORKERS = 2\nelse:\n    NUM_WORKERS = 8\nSAVE_DIR = config.experiments_dir / args.experiment\nPARAMS = {\n    \'nn_module\': (\'AuxSkipAttention\', {\n        \'num_classes\': len(config.classes),\n        \'base_size\': 64,\n        \'dropout\': 0.4,\n        \'ratio\': 16,\n        \'kernel_size\': 7,\n        \'last_filters\': 8,\n        \'last_fc\': 4\n    }),\n    \'loss\': (\'OnlyNoisyLSoftLoss\', {\n        \'beta\': 0.7,\n        \'noisy_weight\': 0.5,\n        \'curated_weight\': 0.5\n    }),\n    \'optimizer\': (\'Adam\', {\'lr\': 0.0009}),\n    \'device\': \'cuda\',\n    \'aux\': {\n        \'weights\': [1.0, 0.4, 0.2, 0.1]\n    },\n    \'amp\': {\n        \'opt_level\': \'O2\',\n        \'keep_batchnorm_fp32\': True,\n        \'loss_scale\': ""dynamic""\n    }\n}\n\n\ndef train_fold(save_dir, train_folds, val_folds,\n               folds_data, noisy_data, corrected_noisy_data):\n    train_transfrom = get_transforms(train=True,\n                                     size=CROP_SIZE,\n                                     wrap_pad_prob=WRAP_PAD_PROB,\n                                     resize_scale=(0.8, 1.0),\n                                     resize_ratio=(1.7, 2.3),\n                                     resize_prob=0.33,\n                                     spec_num_mask=2,\n                                     spec_freq_masking=0.15,\n                                     spec_time_masking=0.20,\n                                     spec_prob=0.5)\n\n    mixer = RandomMixer([\n        SigmoidConcatMixer(sigmoid_range=(3, 12)),\n        AddMixer(alpha_dist=\'uniform\')\n    ], p=[0.6, 0.4])\n    mixer = UseMixerWithProb(mixer, prob=MIXER_PROB)\n\n    curated_dataset = FreesoundDataset(folds_data, train_folds,\n                                       transform=train_transfrom,\n                                       mixer=mixer)\n    noisy_dataset = FreesoundNoisyDataset(noisy_data,\n                                          transform=train_transfrom,\n                                          mixer=mixer)\n    corr_noisy_dataset = FreesoundCorrectedNoisyDataset(corrected_noisy_data,\n                                                        transform=train_transfrom,\n                                                        mixer=mixer)\n    dataset_probs = [NOISY_PROB, CORR_NOISY_PROB, 1 - NOISY_PROB - CORR_NOISY_PROB]\n    print(""Dataset probs"", dataset_probs)\n    print(""Dataset lens"", len(noisy_dataset), len(corr_noisy_dataset), len(curated_dataset))\n    train_dataset = RandomDataset([noisy_dataset, corr_noisy_dataset, curated_dataset],\n                                  p=dataset_probs,\n                                  size=DATASET_SIZE)\n\n    val_dataset = FreesoundDataset(folds_data, val_folds,\n                                   get_transforms(False, CROP_SIZE))\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              shuffle=True, drop_last=True,\n                              num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2,\n                            shuffle=False, num_workers=NUM_WORKERS)\n\n    model = FreesoundModel(PARAMS)\n\n    callbacks = [\n        MonitorCheckpoint(save_dir, monitor=\'val_lwlrap\', max_saves=1),\n        ReduceLROnPlateau(monitor=\'val_lwlrap\', patience=6, factor=0.6, min_lr=1e-8),\n        EarlyStopping(monitor=\'val_lwlrap\', patience=18),\n        LoggingToFile(save_dir / \'log.txt\'),\n    ]\n\n    model.fit(train_loader,\n              val_loader=val_loader,\n              max_epochs=700,\n              callbacks=callbacks,\n              metrics=[\'multi_accuracy\', \'lwlrap\'])\n\n\nif __name__ == ""__main__"":\n    if not SAVE_DIR.exists():\n        SAVE_DIR.mkdir(parents=True, exist_ok=True)\n    else:\n        print(f""Folder {SAVE_DIR} already exists."")\n\n    with open(SAVE_DIR / \'source.py\', \'w\') as outfile:\n        outfile.write(open(__file__).read())\n\n    print(""Model params"", PARAMS)\n    with open(SAVE_DIR / \'params.json\', \'w\') as outfile:\n        json.dump(PARAMS, outfile)\n\n    folds_data = load_folds_data(use_corrections=CORRECTIONS)\n    noisy_data = load_noisy_data()\n    corrected_noisy_data = get_corrected_noisy_data()\n\n    for fold in config.folds:\n        val_folds = [fold]\n        train_folds = list(set(config.folds) - set(val_folds))\n        save_fold_dir = SAVE_DIR / f\'fold_{fold}\'\n        print(f""Val folds: {val_folds}, Train folds: {train_folds}"")\n        print(f""Fold save dir {save_fold_dir}"")\n        train_fold(save_fold_dir, train_folds, val_folds,\n                   folds_data, noisy_data, corrected_noisy_data)\n'"
train_stacking.py,1,"b'import json\n\nfrom argus.callbacks import MonitorCheckpoint, \\\n    EarlyStopping, LoggingToFile, ReduceLROnPlateau\n\nfrom torch.utils.data import DataLoader\n\nfrom src.stacking.datasets import get_out_of_folds_data, StackingDataset\nfrom src.stacking.transforms import get_transforms\nfrom src.stacking.argus_models import StackingModel\nfrom src import config\n\n\nSTACKING_EXPERIMENT = ""stacking_008_fcnet_50013""\n\nEXPERIMENTS = [\n    \'auxiliary_016\',\n    \'auxiliary_019\',\n    \'corr_noisy_003\',\n    \'corr_noisy_004\',\n    \'corr_noisy_007\',\n    \'corrections_002\',\n    \'corrections_003\'\n]\nRS_PARAMS = {""base_size"": 512, ""reduction_scale"": 1, ""p_dropout"": 0.1662788540244386, ""lr"": 2.5814932060476834e-05,\n             ""patience"": 7, ""factor"": 0.5537460438294733, ""batch_size"": 128}\nBATCH_SIZE = RS_PARAMS[\'batch_size\']\nDATASET_SIZE = 128 * 256\nCORRECTIONS = True\nif config.kernel:\n    NUM_WORKERS = 2\nelse:\n    NUM_WORKERS = 8\nSAVE_DIR = config.experiments_dir / STACKING_EXPERIMENT\nPARAMS = {\n    \'nn_module\': (\'FCNet\', {\n        \'in_channels\': len(config.classes) * len(EXPERIMENTS),\n        \'num_classes\': len(config.classes),\n        \'base_size\': RS_PARAMS[\'base_size\'],\n        \'reduction_scale\': RS_PARAMS[\'reduction_scale\'],\n        \'p_dropout\': RS_PARAMS[\'p_dropout\']\n    }),\n    \'loss\': \'BCEWithLogitsLoss\',\n    \'optimizer\': (\'Adam\', {\'lr\': RS_PARAMS[\'lr\']}),\n    \'device\': \'cuda\',\n}\n\n\ndef train_fold(save_dir, train_folds, val_folds, folds_data):\n    train_dataset = StackingDataset(folds_data, train_folds,\n                                    get_transforms(True),\n                                    DATASET_SIZE)\n    val_dataset = StackingDataset(folds_data, val_folds,\n                                  get_transforms(False))\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              shuffle=True, drop_last=True,\n                              num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2,\n                            shuffle=False, num_workers=NUM_WORKERS)\n\n    model = StackingModel(PARAMS)\n\n    callbacks = [\n        MonitorCheckpoint(save_dir, monitor=\'val_lwlrap\', max_saves=1),\n        ReduceLROnPlateau(monitor=\'val_lwlrap\',\n                          patience=RS_PARAMS[\'patience\'],\n                          factor=RS_PARAMS[\'factor\'],\n                          min_lr=1e-8),\n        EarlyStopping(monitor=\'val_lwlrap\', patience=30),\n        LoggingToFile(save_dir / \'log.txt\'),\n    ]\n\n    model.fit(train_loader,\n              val_loader=val_loader,\n              max_epochs=700,\n              callbacks=callbacks,\n              metrics=[\'multi_accuracy\', \'lwlrap\'])\n\n\nif __name__ == ""__main__"":\n    if not SAVE_DIR.exists():\n        SAVE_DIR.mkdir(parents=True, exist_ok=True)\n    else:\n        print(f""Folder {SAVE_DIR} already exists."")\n\n    with open(SAVE_DIR / \'source.py\', \'w\') as outfile:\n        outfile.write(open(__file__).read())\n\n    print(""Model params"", PARAMS)\n    with open(SAVE_DIR / \'params.json\', \'w\') as outfile:\n        json.dump(PARAMS, outfile)\n\n    if CORRECTIONS:\n        with open(config.corrections_json_path) as file:\n            corrections = json.load(file)\n        print(""Corrections:"", corrections)\n    else:\n        corrections = None\n\n    folds_data = get_out_of_folds_data(EXPERIMENTS, corrections)\n\n    for fold in config.folds:\n        val_folds = [fold]\n        train_folds = list(set(config.folds) - set(val_folds))\n        save_fold_dir = SAVE_DIR / f\'fold_{fold}\'\n        print(f""Val folds: {val_folds}, Train folds: {train_folds}"")\n        print(f""Fold save dir {save_fold_dir}"")\n        train_fold(save_fold_dir, train_folds, val_folds, folds_data)\n'"
src/__init__.py,0,b'import src.argus_models\nimport src.metrics\n'
src/argus_models.py,3,"b""import torch\n\nfrom argus import Model\nfrom argus.utils import deep_detach, deep_to\n\nfrom src.models import resnet\nfrom src.models import senet\nfrom src.models.feature_extractor import FeatureExtractor\nfrom src.models.simple_kaggle import SimpleKaggle\nfrom src.models.simple_attention import SimpleAttention\nfrom src.models.skip_attention import SkipAttention\nfrom src.models.aux_skip_attention import AuxSkipAttention\nfrom src.models.rnn_aux_skip_attention import RnnAuxSkipAttention\nfrom src.losses import OnlyNoisyLqLoss, OnlyNoisyLSoftLoss, BCEMaxOutlierLoss\nfrom src import config\n\n\nclass FreesoundModel(Model):\n    nn_module = {\n        'resnet18': resnet.resnet18,\n        'resnet34': resnet.resnet34,\n        'FeatureExtractor': FeatureExtractor,\n        'SimpleKaggle': SimpleKaggle,\n        'se_resnext50_32x4d': senet.se_resnext50_32x4d,\n        'SimpleAttention': SimpleAttention,\n        'SkipAttention': SkipAttention,\n        'AuxSkipAttention': AuxSkipAttention,\n        'RnnAuxSkipAttention': RnnAuxSkipAttention\n    }\n    loss = {\n        'OnlyNoisyLqLoss': OnlyNoisyLqLoss,\n        'OnlyNoisyLSoftLoss': OnlyNoisyLSoftLoss,\n        'BCEMaxOutlierLoss': BCEMaxOutlierLoss\n    }\n    prediction_transform = torch.nn.Sigmoid\n\n    def __init__(self, params):\n        super().__init__(params)\n\n        if 'aux' in params:\n            self.aux_weights = params['aux']['weights']\n        else:\n            self.aux_weights = None\n\n        self.use_amp = not config.kernel and 'amp' in params\n        if self.use_amp:\n            from apex import amp\n            self.amp = amp\n            self.nn_module, self.optimizer = self.amp.initialize(\n                self.nn_module, self.optimizer,\n                opt_level=params['amp']['opt_level'],\n                keep_batchnorm_fp32=params['amp']['keep_batchnorm_fp32'],\n                loss_scale=params['amp']['loss_scale']\n            )\n\n    def prepare_batch(self, batch, device):\n        input, target, noisy = batch\n        input = deep_to(input, device, non_blocking=True)\n        target = deep_to(target, device, non_blocking=True)\n        noisy = deep_to(noisy, device, non_blocking=True)\n        return input, target, noisy\n\n    def train_step(self, batch)-> dict:\n        if not self.nn_module.training:\n            self.nn_module.train()\n        self.optimizer.zero_grad()\n        input, target, noisy = self.prepare_batch(batch, self.device)\n        prediction = self.nn_module(input)\n        if self.aux_weights is not None:\n            loss = 0\n            for pred, weight in zip(prediction, self.aux_weights):\n                loss += self.loss(pred, target, noisy) * weight\n        else:\n            loss = self.loss(prediction, target, noisy)\n        if self.use_amp:\n            with self.amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n        self.optimizer.step()\n\n        prediction = deep_detach(prediction)\n        target = deep_detach(target)\n        return {\n            'prediction': self.prediction_transform(prediction[0]),\n            'target': target,\n            'loss': loss.item(),\n            'noisy': noisy\n        }\n\n    def val_step(self, batch) -> dict:\n        if self.nn_module.training:\n            self.nn_module.eval()\n        with torch.no_grad():\n            input, target, noisy = self.prepare_batch(batch, self.device)\n            prediction = self.nn_module(input)\n            if self.aux_weights is not None:\n                loss = 0\n                for pred, weight in zip(prediction, self.aux_weights):\n                    loss += self.loss(pred, target, noisy) * weight\n            else:\n                loss = self.loss(prediction, target, noisy)\n            return {\n                'prediction': self.prediction_transform(prediction[0]),\n                'target': target,\n                'loss': loss.item(),\n                'noisy': noisy\n            }\n\n    def predict(self, input):\n        assert self.predict_ready()\n        with torch.no_grad():\n            if self.nn_module.training:\n                self.nn_module.eval()\n            input = deep_to(input, self.device)\n            prediction = self.nn_module(input)\n            if self.aux_weights is not None:\n                prediction = prediction[0]\n            prediction = self.prediction_transform(prediction)\n            return prediction\n"""
src/audio.py,0,"b'# Source: https://www.kaggle.com/daisukelab/creating-fat2019-preprocessed-data\nimport numpy as np\n\nimport librosa\nimport librosa.display\n\nfrom src.config import audio as config\n\n\ndef get_audio_config():\n    return config.get_config_dict()\n\n\ndef read_audio(file_path):\n    min_samples = int(config.min_seconds * config.sampling_rate)\n    try:\n        y, sr = librosa.load(file_path, sr=config.sampling_rate)\n        trim_y, trim_idx = librosa.effects.trim(y)  # trim, top_db=default(60)\n\n        if len(trim_y) < min_samples:\n            center = (trim_idx[1] - trim_idx[0]) // 2\n            left_idx = max(0, center - min_samples // 2)\n            right_idx = min(len(y), center + min_samples // 2)\n            trim_y = y[left_idx:right_idx]\n\n            if len(trim_y) < min_samples:\n                padding = min_samples - len(trim_y)\n                offset = padding // 2\n                trim_y = np.pad(trim_y, (offset, padding - offset), \'constant\')\n        return trim_y\n    except BaseException as e:\n        print(f""Exception while reading file {e}"")\n        return np.zeros(min_samples, dtype=np.float32)\n\n\ndef audio_to_melspectrogram(audio):\n    spectrogram = librosa.feature.melspectrogram(audio,\n                                                 sr=config.sampling_rate,\n                                                 n_mels=config.n_mels,\n                                                 hop_length=config.hop_length,\n                                                 n_fft=config.n_fft,\n                                                 fmin=config.fmin,\n                                                 fmax=config.fmax)\n    spectrogram = librosa.power_to_db(spectrogram)\n    spectrogram = spectrogram.astype(np.float32)\n    return spectrogram\n\n\ndef show_melspectrogram(mels, title=\'Log-frequency power spectrogram\'):\n    import matplotlib.pyplot as plt\n\n    librosa.display.specshow(mels, x_axis=\'time\', y_axis=\'mel\',\n                             sr=config.sampling_rate, hop_length=config.hop_length,\n                             fmin=config.fmin, fmax=config.fmax)\n    plt.colorbar(format=\'%+2.0f dB\')\n    plt.title(title)\n    plt.show()\n\n\ndef read_as_melspectrogram(file_path, time_stretch=1.0, pitch_shift=0.0,\n                           debug_display=False):\n    x = read_audio(file_path)\n    if time_stretch != 1.0:\n        x = librosa.effects.time_stretch(x, time_stretch)\n\n    if pitch_shift != 0.0:\n        librosa.effects.pitch_shift(x, config.sampling_rate, n_steps=pitch_shift)\n\n    mels = audio_to_melspectrogram(x)\n    if debug_display:\n        import IPython\n        IPython.display.display(IPython.display.Audio(x, rate=config.sampling_rate))\n        show_melspectrogram(mels)\n    return mels\n\n\nif __name__ == ""__main__"":\n    x = read_as_melspectrogram(config.train_curated_dir / \'0b9906f7.wav\')\n    print(x.shape)\n'"
src/config.py,0,"b'import os\nimport json\nfrom pathlib import Path\nfrom hashlib import sha1\n\n\nkernel = False\nkernel_mode = """"\nif \'MODE\' in os.environ:\n    kernel = True\n    kernel_mode = os.environ[\'MODE\']\n    assert kernel_mode in [""train"", ""predict""]\n\nif kernel:\n    if kernel_mode == ""train"":\n        input_data_dir = Path(\'/kaggle/input/\')\n    else:\n        input_data_dir = Path(\'/kaggle/input/freesound-audio-tagging-2019/\')\n    save_data_dir = Path(\'/kaggle/working/\')\nelse:\n    input_data_dir = Path(\'/workdir/data/\')\n    save_data_dir = Path(\'/workdir/data/\')\n\ntrain_curated_dir = input_data_dir / \'train_curated\'\ntrain_noisy_dir = input_data_dir / \'train_noisy\'\ntrain_curated_csv_path = input_data_dir / \'train_curated.csv\'\ntrain_noisy_csv_path = input_data_dir / \'train_noisy.csv\'\ntest_dir = input_data_dir / \'test\'\nsample_submission = input_data_dir / \'sample_submission.csv\'\n\ntrain_folds_path = save_data_dir / \'train_folds.csv\'\npredictions_dir = save_data_dir / \'predictions\'\nif kernel and kernel_mode == ""predict"":\n    def find_kernel_data_dir():\n        kaggle_input = Path(\'/kaggle/input/\')\n        train_kernel_name = \'freesound-train\'\n        default = kaggle_input / train_kernel_name\n        if default.exists():\n            return default\n        else:\n            for path in kaggle_input.glob(\'*\'):\n                if path.is_dir():\n                    if path.name.startswith(train_kernel_name):\n                        return path\n        return default\n    experiments_dir = find_kernel_data_dir() / \'experiments\'\nelse:\n    experiments_dir = save_data_dir / \'experiments\'\n\nfolds_data_pkl_dir = save_data_dir / \'folds_data\'\naugment_folds_data_pkl_dir = save_data_dir / \'augment_folds_data\'\nnoisy_data_pkl_dir = save_data_dir / \'noisy_data\'\ncorrections_json_path = Path(\'/workdir/corrections.json\')\nnoisy_corrections_json_path = Path(\'/workdir/noisy_corrections.json\')\n\nn_folds = 5\nfolds = list(range(n_folds))\n\n\nclass audio:\n    sampling_rate = 44100\n    hop_length = 345 * 2\n    fmin = 20\n    fmax = sampling_rate // 2\n    n_mels = 128\n    n_fft = n_mels * 20\n    min_seconds = 0.5\n\n    @classmethod\n    def get_config_dict(cls):\n        config_dict = dict()\n        for key, value in cls.__dict__.items():\n            if key[:1] != \'_\' and \\\n                    key not in [\'get_config_dict\', \'get_hash\']:\n                config_dict[key] = value\n        return config_dict\n\n    @classmethod\n    def get_hash(cls, **kwargs):\n        config_dict = cls.get_config_dict()\n        config_dict = {**config_dict, **kwargs}\n        hash_str = json.dumps(config_dict,\n                              sort_keys=True,\n                              ensure_ascii=False,\n                              separators=None)\n        hash_str = hash_str.encode(\'utf-8\')\n        return sha1(hash_str).hexdigest()[:7]\n\n\nclasses = [\n    \'Accelerating_and_revving_and_vroom\',\n    \'Accordion\',\n    \'Acoustic_guitar\',\n    \'Applause\',\n    \'Bark\',\n    \'Bass_drum\',\n    \'Bass_guitar\',\n    \'Bathtub_(filling_or_washing)\',\n    \'Bicycle_bell\',\n    \'Burping_and_eructation\',\n    \'Bus\',\n    \'Buzz\',\n    \'Car_passing_by\',\n    \'Cheering\',\n    \'Chewing_and_mastication\',\n    \'Child_speech_and_kid_speaking\',\n    \'Chink_and_clink\',\n    \'Chirp_and_tweet\',\n    \'Church_bell\',\n    \'Clapping\',\n    \'Computer_keyboard\',\n    \'Crackle\',\n    \'Cricket\',\n    \'Crowd\',\n    \'Cupboard_open_or_close\',\n    \'Cutlery_and_silverware\',\n    \'Dishes_and_pots_and_pans\',\n    \'Drawer_open_or_close\',\n    \'Drip\',\n    \'Electric_guitar\',\n    \'Fart\',\n    \'Female_singing\',\n    \'Female_speech_and_woman_speaking\',\n    \'Fill_(with_liquid)\',\n    \'Finger_snapping\',\n    \'Frying_(food)\',\n    \'Gasp\',\n    \'Glockenspiel\',\n    \'Gong\',\n    \'Gurgling\',\n    \'Harmonica\',\n    \'Hi-hat\',\n    \'Hiss\',\n    \'Keys_jangling\',\n    \'Knock\',\n    \'Male_singing\',\n    \'Male_speech_and_man_speaking\',\n    \'Marimba_and_xylophone\',\n    \'Mechanical_fan\',\n    \'Meow\',\n    \'Microwave_oven\',\n    \'Motorcycle\',\n    \'Printer\',\n    \'Purr\',\n    \'Race_car_and_auto_racing\',\n    \'Raindrop\',\n    \'Run\',\n    \'Scissors\',\n    \'Screaming\',\n    \'Shatter\',\n    \'Sigh\',\n    \'Sink_(filling_or_washing)\',\n    \'Skateboard\',\n    \'Slam\',\n    \'Sneeze\',\n    \'Squeak\',\n    \'Stream\',\n    \'Strum\',\n    \'Tap\',\n    \'Tick-tock\',\n    \'Toilet_flush\',\n    \'Traffic_noise_and_roadway_noise\',\n    \'Trickle_and_dribble\',\n    \'Walk_and_footsteps\',\n    \'Water_tap_and_faucet\',\n    \'Waves_and_surf\',\n    \'Whispering\',\n    \'Writing\',\n    \'Yell\',\n    \'Zipper_(clothing)\'\n]\n\nclass2index = {cls: idx for idx, cls in enumerate(classes)}\n'"
src/datasets.py,8,"b'import json\nimport time\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom functools import partial\nimport multiprocessing as mp\nfrom torch.utils.data import Dataset\n\nfrom src.audio import read_as_melspectrogram, get_audio_config\nfrom src import config\n\n\nN_WORKERS = mp.cpu_count()\n\n\ndef get_test_data():\n    print(""Start load test data"")\n    fname_lst = []\n    wav_path_lst = []\n    for wav_path in sorted(config.test_dir.glob(\'*.wav\')):\n        wav_path_lst.append(wav_path)\n        fname_lst.append(wav_path.name)\n\n    with mp.Pool(N_WORKERS) as pool:\n        images_lst = pool.map(read_as_melspectrogram, wav_path_lst)\n\n    return fname_lst, images_lst\n\n\ndef get_folds_data(corrections=None):\n    print(""Start generate folds data"")\n    print(""Audio config"", get_audio_config())\n    train_folds_df = pd.read_csv(config.train_folds_path)\n\n    audio_paths_lst = []\n    targets_lst = []\n    folds_lst = []\n    for i, row in train_folds_df.iterrows():\n        labels = row.labels\n\n        if corrections is not None:\n            if row.fname in corrections:\n                action = corrections[row.fname]\n                if action == \'remove\':\n                    print(f""Skip {row.fname}"")\n                    continue\n                else:\n                    print(f""Replace labels {row.fname} from {labels} to {action}"")\n                    labels = action\n\n        folds_lst.append(row.fold)\n        audio_paths_lst.append(row.file_path)\n        target = torch.zeros(len(config.classes))\n        for label in labels.split(\',\'):\n            target[config.class2index[label]] = 1.\n        targets_lst.append(target)\n\n    with mp.Pool(N_WORKERS) as pool:\n        images_lst = pool.map(read_as_melspectrogram, audio_paths_lst)\n\n    return images_lst, targets_lst, folds_lst\n\n\ndef get_augment_folds_data_generator(time_stretch_lst, pitch_shift_lst):\n    print(""Start generate augment folds data"")\n    print(""Audio config"", get_audio_config())\n    print(""time_stretch_lst:"", time_stretch_lst)\n    print(""pitch_shift_lst:"", pitch_shift_lst)\n    train_folds_df = pd.read_csv(config.train_folds_path)\n\n    audio_paths_lst = []\n    targets_lst = []\n    folds_lst = []\n    for i, row in train_folds_df.iterrows():\n        folds_lst.append(row.fold)\n        audio_paths_lst.append(row.file_path)\n        target = torch.zeros(len(config.classes))\n        for label in row.labels.split(\',\'):\n            target[config.class2index[label]] = 1.\n        targets_lst.append(target)\n\n    with mp.Pool(N_WORKERS) as pool:\n        images_lst = pool.map(read_as_melspectrogram, audio_paths_lst)\n\n    yield images_lst, targets_lst, folds_lst\n    images_lst = []\n\n    for pitch_shift in pitch_shift_lst:\n        pitch_shift_read = partial(read_as_melspectrogram, pitch_shift=pitch_shift)\n        with mp.Pool(N_WORKERS) as pool:\n            images_lst = pool.map(pitch_shift_read, audio_paths_lst)\n\n        yield images_lst, targets_lst, folds_lst\n        images_lst = []\n\n    for time_stretch in time_stretch_lst:\n        time_stretch_read = partial(read_as_melspectrogram, time_stretch=time_stretch)\n        with mp.Pool(N_WORKERS) as pool:\n            images_lst = pool.map(time_stretch_read, audio_paths_lst)\n\n        yield images_lst, targets_lst, folds_lst\n        images_lst = []\n\n\nclass FreesoundDataset(Dataset):\n    def __init__(self, folds_data, folds,\n                 transform=None,\n                 mixer=None):\n        super().__init__()\n        self.folds = folds\n        self.transform = transform\n        self.mixer = mixer\n\n        self.images_lst = []\n        self.targets_lst = []\n        for img, trg, fold in zip(*folds_data):\n            if fold in folds:\n                self.images_lst.append(img)\n                self.targets_lst.append(trg)\n\n    def __len__(self):\n        return len(self.images_lst)\n\n    def __getitem__(self, idx):\n        image = self.images_lst[idx].copy()\n        target = self.targets_lst[idx].clone()\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        if self.mixer is not None:\n            image, target = self.mixer(self, image, target)\n\n        noisy = torch.tensor(0, dtype=torch.uint8)\n        return image, target, noisy\n\n\ndef get_noisy_data_generator():\n    print(""Start generate noisy data"")\n    print(""Audio config"", get_audio_config())\n    train_noisy_df = pd.read_csv(config.train_noisy_csv_path)\n\n    with open(config.noisy_corrections_json_path) as file:\n        corrections = json.load(file)\n\n    audio_paths_lst = []\n    targets_lst = []\n    for i, row in train_noisy_df.iterrows():\n        labels = row.labels\n\n        if row.fname in corrections:\n            action = corrections[row.fname]\n            if action == \'remove\':\n                continue\n            else:\n                labels = action\n\n        audio_paths_lst.append(config.train_noisy_dir / row.fname)\n        target = torch.zeros(len(config.classes))\n        for label in labels.split(\',\'):\n            target[config.class2index[label]] = 1.\n        targets_lst.append(target)\n\n        if len(audio_paths_lst) >= 5000:\n            with mp.Pool(N_WORKERS) as pool:\n                images_lst = pool.map(read_as_melspectrogram, audio_paths_lst)\n\n            yield images_lst, targets_lst\n\n            audio_paths_lst = []\n            images_lst = []\n            targets_lst = []\n\n    with mp.Pool(N_WORKERS) as pool:\n        images_lst = pool.map(read_as_melspectrogram, audio_paths_lst)\n\n    yield images_lst, targets_lst\n\n\nclass FreesoundNoisyDataset(Dataset):\n    def __init__(self, noisy_data, transform=None,\n                 mixer=None):\n        super().__init__()\n        self.transform = transform\n        self.mixer = mixer\n\n        self.images_lst = []\n        self.targets_lst = []\n        for img, trg in zip(*noisy_data):\n            self.images_lst.append(img)\n            self.targets_lst.append(trg)\n\n    def __len__(self):\n        return len(self.images_lst)\n\n    def __getitem__(self, idx):\n        image = self.images_lst[idx].copy()\n        target = self.targets_lst[idx].clone()\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        if self.mixer is not None:\n            image, target = self.mixer(self, image, target)\n\n        noisy = torch.tensor(1, dtype=torch.uint8)\n        return image, target, noisy\n\n\nclass RandomDataset(Dataset):\n    def __init__(self, datasets, p=None, size=4096):\n        self.datasets = datasets\n        self.p = p\n        self.size = size\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        seed = int(time.time() * 1000.0) + idx\n        random.seed(seed)\n        np.random.seed(seed % (2**31))\n\n        dataset_idx = np.random.choice(\n            range(len(self.datasets)), p=self.p)\n        dataset = self.datasets[dataset_idx]\n        idx = random.randint(0, len(dataset) - 1)\n        return dataset[idx]\n\n\ndef get_corrected_noisy_data():\n    print(""Start generate corrected noisy data"")\n    print(""Audio config"", get_audio_config())\n    train_noisy_df = pd.read_csv(config.train_noisy_csv_path)\n\n    with open(config.noisy_corrections_json_path) as file:\n        corrections = json.load(file)\n\n    audio_paths_lst = []\n    targets_lst = []\n    for i, row in train_noisy_df.iterrows():\n        labels = row.labels\n\n        if row.fname in corrections:\n            action = corrections[row.fname]\n            if action == \'remove\':\n                continue\n            else:\n                labels = action\n        else:\n            continue\n\n        audio_paths_lst.append(config.train_noisy_dir / row.fname)\n        target = torch.zeros(len(config.classes))\n\n        for label in labels.split(\',\'):\n            target[config.class2index[label]] = 1.\n        targets_lst.append(target)\n\n    with mp.Pool(N_WORKERS) as pool:\n        images_lst = pool.map(read_as_melspectrogram, audio_paths_lst)\n\n    return images_lst, targets_lst\n\n\nclass FreesoundCorrectedNoisyDataset(Dataset):\n    def __init__(self, noisy_data, transform=None,\n                 mixer=None):\n        super().__init__()\n        self.transform = transform\n        self.mixer = mixer\n\n        self.images_lst = []\n        self.targets_lst = []\n        for img, trg in zip(*noisy_data):\n            self.images_lst.append(img)\n            self.targets_lst.append(trg)\n\n    def __len__(self):\n        return len(self.images_lst)\n\n    def __getitem__(self, idx):\n        image = self.images_lst[idx].copy()\n        target = self.targets_lst[idx].clone()\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        if self.mixer is not None:\n            image, target = self.mixer(self, image, target)\n\n        noisy = torch.tensor(0, dtype=torch.uint8)\n        return image, target, noisy\n'"
src/losses.py,7,"b""import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\ndef lq_loss(y_pred, y_true, q):\n    eps = 1e-7\n    loss = y_pred * y_true\n    # loss, _ = torch.max(loss, dim=1)\n    loss = (1 - (loss + eps) ** q) / q\n    return loss.mean()\n\n\nclass LqLoss(nn.Module):\n    def __init__(self, q=0.5):\n        super().__init__()\n        self.q = q\n\n    def forward(self, output, target):\n        output = torch.sigmoid(output)\n        return lq_loss(output, target, self.q)\n\n\ndef l_soft(y_pred, y_true, beta):\n    eps = 1e-7\n\n    y_pred = torch.clamp(y_pred, eps, 1.0)\n\n    # (1) dynamically update the targets based on the current state of the model:\n    # bootstrapped target tensor\n    # use predicted class proba directly to generate regression targets\n    with torch.no_grad():\n        y_true_update = beta * y_true + (1 - beta) * y_pred\n\n    # (2) compute loss as always\n    loss = F.binary_cross_entropy(y_pred, y_true_update)\n    return loss\n\n\nclass LSoftLoss(nn.Module):\n    def __init__(self, beta=0.5):\n        super().__init__()\n        self.beta = beta\n\n    def forward(self, output, target):\n        output = torch.sigmoid(output)\n        return l_soft(output, target, self.beta)\n\n\nclass NoisyCuratedLoss(nn.Module):\n    def __init__(self, noisy_loss, curated_loss,\n                 noisy_weight=0.5, curated_weight=0.5):\n        super().__init__()\n        self.noisy_loss = noisy_loss\n        self.curated_loss = curated_loss\n        self.noisy_weight = noisy_weight\n        self.curated_weight = curated_weight\n\n    def forward(self, output, target, noisy):\n        batch_size = target.shape[0]\n\n        noisy_indexes = noisy.nonzero().squeeze(1)\n        curated_indexes = (noisy == 0).nonzero().squeeze(1)\n\n        noisy_len = noisy_indexes.shape[0]\n        if noisy_len > 0:\n            noisy_target = target[noisy_indexes]\n            noisy_output = output[noisy_indexes]\n            noisy_loss = self.noisy_loss(noisy_output, noisy_target)\n            noisy_loss = noisy_loss * (noisy_len / batch_size)\n        else:\n            noisy_loss = 0\n\n        curated_len = curated_indexes.shape[0]\n        if curated_len > 0:\n            curated_target = target[curated_indexes]\n            curated_output = output[curated_indexes]\n            curated_loss = self.curated_loss(curated_output, curated_target)\n            curated_loss = curated_loss * (curated_len / batch_size)\n        else:\n            curated_loss = 0\n\n        loss = noisy_loss * self.noisy_weight\n        loss += curated_loss * self.curated_weight\n        return loss\n\n\nclass OnlyNoisyLqLoss(nn.Module):\n    def __init__(self, q=0.5,\n                 noisy_weight=0.5,\n                 curated_weight=0.5):\n        super().__init__()\n        lq = LqLoss(q=q)\n        bce = nn.BCEWithLogitsLoss()\n        self.loss = NoisyCuratedLoss(lq, bce,\n                                     noisy_weight,\n                                     curated_weight)\n\n    def forward(self, output, target, noisy):\n        return self.loss(output, target, noisy)\n\n\nclass OnlyNoisyLSoftLoss(nn.Module):\n    def __init__(self, beta,\n                 noisy_weight=0.5,\n                 curated_weight=0.5):\n        super().__init__()\n        soft = LSoftLoss(beta)\n        bce = nn.BCEWithLogitsLoss()\n        self.loss = NoisyCuratedLoss(soft, bce,\n                                     noisy_weight,\n                                     curated_weight)\n\n    def forward(self, output, target, noisy):\n        return self.loss(output, target, noisy)\n\n\nclass BCEMaxOutlierLoss(nn.Module):\n    def __init__(self, alpha=0.8):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, output, target, noisy):\n        loss = F.binary_cross_entropy_with_logits(output, target,\n                                                  reduction='none')\n        loss = loss.mean(dim=1)\n\n        with torch.no_grad():\n            outlier_mask = loss > self.alpha * loss.max()\n            outlier_mask = outlier_mask * noisy\n            outlier_idx = (outlier_mask == 0).nonzero().squeeze(1)\n\n        loss = loss[outlier_idx].mean()\n        return loss\n"""
src/lr_scheduler.py,1,"b'import math\nfrom torch.optim.lr_scheduler import _LRScheduler\n\nfrom argus.callbacks.lr_schedulers import LRScheduler\n\n\nclass CosineAnnealingWarmRestarts(_LRScheduler):\n    r""""""Set the learning rate of each parameter group using a cosine annealing\n    schedule, where :math:`\\eta_{max}` is set to the initial lr, :math:`T_{cur}`\n    is the number of epochs since the last restart and :math:`T_{i}` is the number\n    of epochs between two warm restarts in SGDR:\n    .. math::\n        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n        \\cos(\\frac{T_{cur}}{T_{i}}\\pi))\n    When :math:`T_{cur}=T_{i}`, set :math:`\\eta_t = \\eta_{min}`.\n    When :math:`T_{cur}=0`(after restart), set :math:`\\eta_t=\\eta_{max}`.\n    It has been proposed in\n    `SGDR: Stochastic Gradient Descent with Warm Restarts`_.\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        T_0 (int): Number of iterations for the first restart.\n        T_mult (int, optional): A factor increases :math:`T_{i}` after a restart. Default: 1.\n        eta_min (float, optional): Minimum learning rate. Default: 0.\n        last_epoch (int, optional): The index of last epoch. Default: -1.\n    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n        https://arxiv.org/abs/1608.03983\n    """"""\n\n    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1):\n        if T_0 <= 0 or not isinstance(T_0, int):\n            raise ValueError(""Expected positive integer T_0, but got {}"".format(T_0))\n        if T_mult < 1 or not isinstance(T_mult, int):\n            raise ValueError(""Expected integer T_mult >= 1, but got {}"".format(T_mult))\n        self.T_0 = T_0\n        self.T_i = T_0\n        self.T_mult = T_mult\n        self.eta_min = eta_min\n        super(CosineAnnealingWarmRestarts, self).__init__(optimizer, last_epoch)\n        self.T_cur = last_epoch\n\n    def get_lr(self):\n        return [self.eta_min + (base_lr - self.eta_min) * (1 + math.cos(math.pi * self.T_cur / self.T_i)) / 2\n                for base_lr in self.base_lrs]\n\n    def step(self, epoch=None):\n        """"""Step could be called after every update, i.e. if one epoch has 10 iterations\n        (number_of_train_examples / batch_size), we should call SGDR.step(0.1), SGDR.step(0.2), etc.\n        This function can be called in an interleaved way.\n        Example:\n            >>> scheduler = SGDR(optimizer, T_0, T_mult)\n            >>> for epoch in range(20):\n            >>>     scheduler.step()\n            >>> scheduler.step(26)\n            >>> scheduler.step() # scheduler.step(27), instead of scheduler(20)\n        """"""\n        if epoch is None:\n            epoch = self.last_epoch + 1\n            self.T_cur = self.T_cur + 1\n            if self.T_cur >= self.T_i:\n                self.T_cur = self.T_cur - self.T_i\n                self.T_i = self.T_i * self.T_mult\n        else:\n            if epoch >= self.T_0:\n                if self.T_mult == 1:\n                    self.T_cur = epoch % self.T_0\n                else:\n                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n                    self.T_i = self.T_0 * self.T_mult ** (n)\n            else:\n                self.T_i = self.T_0\n                self.T_cur = epoch\n        self.last_epoch = math.floor(epoch)\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n\n\nclass CosineAnnealing(LRScheduler):\n    def __init__(self, T_0, T_mult=1, eta_min=0):\n        super().__init__(lambda opt: CosineAnnealingWarmRestarts(opt,\n                                                                 T_0,\n                                                                 T_mult=T_mult,\n                                                                 eta_min=eta_min))\n'"
src/metrics.py,3,"b'import torch\nimport numpy as np\n\nfrom argus.metrics.metric import Metric\n\nfrom src import config\n\n\nclass MultiCategoricalAccuracy(Metric):\n    name = \'multi_accuracy\'\n    better = \'max\'\n\n    def __init__(self, threshold=0.5):\n        self.threshold = threshold\n\n    def reset(self):\n        self.correct = 0\n        self.count = 0\n\n    def update(self, step_output: dict):\n        pred = step_output[\'prediction\']\n        trg = step_output[\'target\']\n        pred = (pred > self.threshold).to(torch.float32)\n        correct = torch.eq(pred, trg).all(dim=1).view(-1)\n        self.correct += torch.sum(correct).item()\n        self.count += correct.shape[0]\n\n    def compute(self):\n        if self.count == 0:\n            raise Exception(\'Must be at least one example for computation\')\n        return self.correct / self.count\n\n\n# Source: https://github.com/DCASE-REPO/dcase2019_task2_baseline/blob/master/evaluation.py\nclass LwlrapBase:\n    """"""Computes label-weighted label-ranked average precision (lwlrap).""""""\n\n    def __init__(self, class_map):\n        self.num_classes = 0\n        self.total_num_samples = 0\n        self._class_map = class_map\n\n    def accumulate(self, batch_truth, batch_scores):\n        """"""Accumulate a new batch of samples into the metric.\n        Args:\n          truth: np.array of (num_samples, num_classes) giving boolean\n            ground-truth of presence of that class in that sample for this batch.\n          scores: np.array of (num_samples, num_classes) giving the\n            classifier-under-test\'s real-valued score for each class for each\n            sample.\n        """"""\n        assert batch_scores.shape == batch_truth.shape\n        num_samples, num_classes = batch_truth.shape\n        if not self.num_classes:\n            self.num_classes = num_classes\n            self._per_class_cumulative_precision = np.zeros(self.num_classes)\n            self._per_class_cumulative_count = np.zeros(self.num_classes,\n                                                        dtype=np.int)\n        assert num_classes == self.num_classes\n        for truth, scores in zip(batch_truth, batch_scores):\n            pos_class_indices, precision_at_hits = (\n                self._one_sample_positive_class_precisions(scores, truth))\n            self._per_class_cumulative_precision[pos_class_indices] += (\n                precision_at_hits)\n            self._per_class_cumulative_count[pos_class_indices] += 1\n        self.total_num_samples += num_samples\n\n    def _one_sample_positive_class_precisions(self, scores, truth):\n        """"""Calculate precisions for each true class for a single sample.\n        Args:\n          scores: np.array of (num_classes,) giving the individual classifier scores.\n          truth: np.array of (num_classes,) bools indicating which classes are true.\n        Returns:\n          pos_class_indices: np.array of indices of the true classes for this sample.\n          pos_class_precisions: np.array of precisions corresponding to each of those\n            classes.\n        """"""\n        num_classes = scores.shape[0]\n        pos_class_indices = np.flatnonzero(truth > 0)\n        # Only calculate precisions if there are some true classes.\n        if not len(pos_class_indices):\n            return pos_class_indices, np.zeros(0)\n        # Retrieval list of classes for this sample.\n        retrieved_classes = np.argsort(scores)[::-1]\n        # class_rankings[top_scoring_class_index] == 0 etc.\n        class_rankings = np.zeros(num_classes, dtype=np.int)\n        class_rankings[retrieved_classes] = range(num_classes)\n        # Which of these is a true label?\n        retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n        retrieved_class_true[class_rankings[pos_class_indices]] = True\n        # Num hits for every truncated retrieval list.\n        retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n        # Precision of retrieval list truncated at each hit, in order of pos_labels.\n        precision_at_hits = (\n                retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n                (1 + class_rankings[pos_class_indices].astype(np.float)))\n        return pos_class_indices, precision_at_hits\n\n    def per_class_lwlrap(self):\n        """"""Return a vector of the per-class lwlraps for the accumulated samples.""""""\n        return (self._per_class_cumulative_precision /\n                np.maximum(1, self._per_class_cumulative_count))\n\n    def per_class_weight(self):\n        """"""Return a normalized weight vector for the contributions of each class.""""""\n        return (self._per_class_cumulative_count /\n                float(np.sum(self._per_class_cumulative_count)))\n\n    def overall_lwlrap(self):\n        """"""Return the scalar overall lwlrap for cumulated samples.""""""\n        return np.sum(self.per_class_lwlrap() * self.per_class_weight())\n\n    def __str__(self):\n        per_class_lwlrap = self.per_class_lwlrap()\n        # List classes in descending order of lwlrap.\n        s = ([\'Lwlrap(%s) = %.6f\' % (name, lwlrap) for (lwlrap, name) in\n              sorted([(per_class_lwlrap[i], self._class_map[i]) for i in range(self.num_classes)],\n                     reverse=True)])\n        s.append(\'Overall lwlrap = %.6f\' % (self.overall_lwlrap()))\n        return \'\\n\'.join(s)\n\n\nclass Lwlrap(Metric):\n    name = \'lwlrap\'\n    better = \'max\'\n\n    def __init__(self, classes=None):\n        self.classes = classes\n        if self.classes is None:\n            self.classes = config.classes\n\n        self.lwlrap = LwlrapBase(self.classes)\n\n    def reset(self):\n        self.lwlrap.num_classes = 0\n        self.lwlrap.total_num_samples = 0\n\n    def update(self, step_output: dict):\n        pred = step_output[\'prediction\'].cpu().numpy()\n        trg = step_output[\'target\'].cpu().numpy()\n        self.lwlrap.accumulate(trg, pred)\n\n    def compute(self):\n        return self.lwlrap.overall_lwlrap()\n'"
src/mixers.py,2,"b""import torch\nimport random\nimport numpy as np\n\n\ndef get_random_sample(dataset):\n    rnd_idx = random.randint(0, len(dataset) - 1)\n    rnd_image = dataset.images_lst[rnd_idx].copy()\n    rnd_target = dataset.targets_lst[rnd_idx].clone()\n    rnd_image = dataset.transform(rnd_image)\n    return rnd_image, rnd_target\n\n\nclass AddMixer:\n    def __init__(self, alpha_dist='uniform'):\n        assert alpha_dist in ['uniform', 'beta']\n        self.alpha_dist = alpha_dist\n\n    def sample_alpha(self):\n        if self.alpha_dist == 'uniform':\n            return random.uniform(0, 0.5)\n        elif self.alpha_dist == 'beta':\n            return np.random.beta(0.4, 0.4)\n\n    def __call__(self, dataset, image, target):\n        rnd_image, rnd_target = get_random_sample(dataset)\n\n        alpha = self.sample_alpha()\n        image = (1 - alpha) * image + alpha * rnd_image\n        target = (1 - alpha) * target + alpha * rnd_target\n        return image, target\n\n\nclass SigmoidConcatMixer:\n    def __init__(self, sigmoid_range=(3, 12)):\n        self.sigmoid_range = sigmoid_range\n\n    def sample_mask(self, size):\n        x_radius = random.randint(*self.sigmoid_range)\n\n        step = (x_radius * 2) / size[1]\n        x = np.arange(-x_radius, x_radius, step=step)\n        y = torch.sigmoid(torch.from_numpy(x)).numpy()\n        mix_mask = np.tile(y, (size[0], 1))\n        return torch.from_numpy(mix_mask.astype(np.float32))\n\n    def __call__(self, dataset, image, target):\n        rnd_image, rnd_target = get_random_sample(dataset)\n\n        mix_mask = self.sample_mask(image.shape[-2:])\n        rnd_mix_mask = 1 - mix_mask\n\n        image = mix_mask * image + rnd_mix_mask * rnd_image\n        target = target + rnd_target\n        target = np.clip(target, 0.0, 1.0)\n        return image, target\n\n\nclass RandomMixer:\n    def __init__(self, mixers, p=None):\n        self.mixers = mixers\n        self.p = p\n\n    def __call__(self, dataset, image, target):\n        mixer = np.random.choice(self.mixers, p=self.p)\n        image, target = mixer(dataset, image, target)\n        return image, target\n\n\nclass UseMixerWithProb:\n    def __init__(self, mixer, prob=.5):\n        self.mixer = mixer\n        self.prob = prob\n\n    def __call__(self, dataset, image, target):\n        if random.random() < self.prob:\n            return self.mixer(dataset, image, target)\n        return image, target\n"""
src/predictor.py,3,"b""import torch\nfrom torch.utils.data import DataLoader\n\nfrom argus import load_model\n\nfrom src.tiles import ImageSlicer\n\n\n@torch.no_grad()\ndef tile_prediction(model, image, transforms,\n                    tile_size, tile_step, batch_size):\n    tiler = ImageSlicer(image.shape,\n                        tile_size=tile_size,\n                        tile_step=tile_step)\n\n    tiles = tiler.split(image, value=float(image.min()))\n    tiles = [transforms(tile) for tile in tiles]\n\n    loader = DataLoader(tiles, batch_size=batch_size)\n\n    preds_lst = []\n\n    for tiles_batch in loader:\n        pred_batch = model.predict(tiles_batch)\n        preds_lst.append(pred_batch)\n\n    pred = torch.cat(preds_lst, dim=0)\n\n    return pred.cpu().numpy()\n\n\nclass Predictor:\n    def __init__(self, model_path, transforms,\n                 batch_size, tile_size, tile_step,\n                 device='cuda'):\n        self.model = load_model(model_path, device=device)\n        self.transforms = transforms\n        self.tile_size = tile_size\n        self.tile_step = tile_step\n        self.batch_size = batch_size\n\n    def predict(self, image):\n        pred = tile_prediction(self.model, image, self.transforms,\n                               self.tile_size,\n                               self.tile_step,\n                               self.batch_size)\n        return pred\n"""
src/random_resized_crop.py,0,"b'import math\nimport random\nimport numpy as np\nfrom PIL import Image\n\n\ndef resize(img, size, interpolation=Image.BILINEAR):\n    r""""""Resize the input PIL Image to the given size.\n    Args:\n        img (PIL Image): Image to be resized.\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), the output size will be matched to this. If size is an int,\n            the smaller edge of the image will be matched to this number maintaing\n            the aspect ratio. i.e, if height > width, then image will be rescaled to\n            :math:`\\left(\\text{size} \\times \\frac{\\text{height}}{\\text{width}}, \\text{size}\\right)`\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    Returns:\n        PIL Image: Resized image.\n    """"""\n    if isinstance(size, int):\n        w, h = img.size\n        if (w <= h and w == size) or (h <= w and h == size):\n            return img\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n            return img.resize((ow, oh), interpolation)\n        else:\n            oh = size\n            ow = int(size * w / h)\n            return img.resize((ow, oh), interpolation)\n    else:\n        return img.resize(size[::-1], interpolation)\n\n\ndef crop(img, i, j, h, w):\n    """"""Crop the given PIL Image.\n    Args:\n        img (PIL Image): Image to be cropped.\n        i (int): i in (i,j) i.e coordinates of the upper left corner.\n        j (int): j in (i,j) i.e coordinates of the upper left corner.\n        h (int): Height of the cropped image.\n        w (int): Width of the cropped image.\n    Returns:\n        PIL Image: Cropped image.\n    """"""\n    return img.crop((j, i, j + w, i + h))\n\n\ndef resized_crop(img, i, j, h, w, size, interpolation=Image.BILINEAR):\n    """"""Crop the given PIL Image and resize it to desired size.\n    Notably used in :class:`~torchvision.transforms.RandomResizedCrop`.\n    Args:\n        img (PIL Image): Image to be cropped.\n        i (int): i in (i,j) i.e coordinates of the upper left corner\n        j (int): j in (i,j) i.e coordinates of the upper left corner\n        h (int): Height of the cropped image.\n        w (int): Width of the cropped image.\n        size (sequence or int): Desired output size. Same semantics as ``resize``.\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``.\n    Returns:\n        PIL Image: Cropped image.\n    """"""\n    img = crop(img, i, j, h, w)\n    img = resize(img, size, interpolation)\n    return img\n\n\nclass RandomResizedCrop(object):\n    """"""Crop the given PIL Image to random size and aspect ratio.\n    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n    is finally resized to given size.\n    This is popularly used to train the Inception networks.\n    Args:\n        size: expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped\n        interpolation: Default: PIL.Image.BILINEAR\n    """"""\n\n    def __init__(self, size=None, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.), interpolation=Image.BILINEAR):\n        if isinstance(size, tuple) or size is None:\n            self.size = size\n        else:\n            self.size = (size, size)\n        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n            warnings.warn(""range should be of kind (min, max)"")\n\n        self.interpolation = interpolation\n        self.scale = scale\n        self.ratio = ratio\n\n    @staticmethod\n    def get_params(img, scale, ratio):\n        """"""Get parameters for ``crop`` for a random sized crop.\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        """"""\n        area = img.size[0] * img.size[1]\n\n        for attempt in range(10):\n            target_area = random.uniform(*scale) * area\n            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w <= img.size[0] and h <= img.size[1]:\n                i = random.randint(0, img.size[1] - h)\n                j = random.randint(0, img.size[0] - w)\n                return i, j, h, w\n\n        # Fallback to central crop\n        in_ratio = img.size[0] / img.size[1]\n        if (in_ratio < min(ratio)):\n            w = img.size[0]\n            h = w / min(ratio)\n        elif (in_ratio > max(ratio)):\n            h = img.size[1]\n            w = h * max(ratio)\n        else:  # whole image\n            w = img.size[0]\n            h = img.size[1]\n        i = (img.size[1] - h) // 2\n        j = (img.size[0] - w) // 2\n        return i, j, h, w\n\n    def __call__(self, np_image):\n        """"""\n        Args:\n            img (PIL Image): Image to be cropped and resized.\n        Returns:\n            PIL Image: Randomly cropped and resized image.\n        """"""\n\n        if self.size is None:\n            size = np_image.shape\n        else:\n            size = self.size\n\n        image = Image.fromarray(np_image)\n        i, j, h, w = self.get_params(image, self.scale, self.ratio)\n        image = resized_crop(image, i, j, h, w, size, self.interpolation)\n        np_image = np.array(image)\n        return np_image\n\n    def __repr__(self):\n        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        format_string = self.__class__.__name__ + \'(size={0}\'.format(self.size)\n        format_string += \', scale={0}\'.format(tuple(round(s, 4) for s in self.scale))\n        format_string += \', ratio={0}\'.format(tuple(round(r, 4) for r in self.ratio))\n        format_string += \', interpolation={0})\'.format(interpolate_str)\n        return format_string\n'"
src/tiles.py,5,"b'""""""Implementation of tile-based inference allowing to predict huge images that does not fit into GPU memory entirely\nin a sliding-window fashion and merging prediction mask back to full-resolution.\nSource: https://github.com/BloodAxe/pytorch-toolbelt/blob/develop/pytorch_toolbelt/inference/tiles.py\n""""""\nfrom typing import List\n\nimport numpy as np\nimport cv2\nimport math\nimport torch\n\n\ndef compute_pyramid_patch_weight_loss(width, height) -> np.ndarray:\n    """"""Compute a weight matrix that assigns bigger weight on pixels in center and\n    less weight to pixels on image boundary.\n    This weight matrix then used for merging individual tile predictions and helps dealing\n    with prediction artifacts on tile boundaries.\n\n    :param width: Tile width\n    :param height: Tile height\n    :return: Since-channel image [Width x Height]\n    """"""\n    xc = width * 0.5\n    yc = height * 0.5\n    xl = 0\n    xr = width\n    yb = 0\n    yt = height\n    Dc = np.zeros((width, height))\n    De = np.zeros((width, height))\n\n    for i in range(width):\n        for j in range(height):\n            Dc[i, j] = np.sqrt(np.square(i - xc + 0.5) + np.square(j - yc + 0.5))\n            De_l = np.sqrt(np.square(i - xl + 0.5) + np.square(j - j + 0.5))\n            De_r = np.sqrt(np.square(i - xr + 0.5) + np.square(j - j + 0.5))\n            De_b = np.sqrt(np.square(i - i + 0.5) + np.square(j - yb + 0.5))\n            De_t = np.sqrt(np.square(i - i + 0.5) + np.square(j - yt + 0.5))\n            De[i, j] = np.min([De_l, De_r, De_b, De_t])\n\n    alpha = (width * height) / np.sum(np.divide(De, np.add(Dc, De)))\n    W = alpha * np.divide(De, np.add(Dc, De))\n    return W, Dc, De\n\n\nclass ImageSlicer:\n    """"""\n    Helper class to slice image into tiles and merge them back\n    """"""\n\n    def __init__(self, image_shape, tile_size, tile_step=0, image_margin=0, weight=\'mean\'):\n        """"""\n\n        :param image_shape: Shape of the source image (H, W)\n        :param tile_size: Tile size (Scalar or tuple (H, W)\n        :param tile_step: Step in pixels between tiles (Scalar or tuple (H, W))\n        :param image_margin:\n        :param weight: Fusion algorithm. \'mean\' - avergaing\n        """"""\n        self.image_height = image_shape[0]\n        self.image_width = image_shape[1]\n\n        if isinstance(tile_size, (tuple, list)):\n            assert len(tile_size) == 2\n            self.tile_size = int(tile_size[0]), int(tile_size[1])\n        else:\n            self.tile_size = int(tile_size), int(tile_size)\n\n        if isinstance(tile_step, (tuple, list)):\n            assert len(tile_step) == 2\n            self.tile_step = int(tile_step[0]), int(tile_step[1])\n        else:\n            self.tile_step = int(tile_step), int(tile_step)\n\n        weights = {\n            \'mean\': self._mean,\n            \'pyramid\': self._pyramid\n        }\n\n        self.weight = weight if isinstance(weight, np.ndarray) else weights[weight](self.tile_size)\n\n        if self.tile_step[0] < 1 or self.tile_step[0] > self.tile_size[0]:\n            raise ValueError()\n        if self.tile_step[1] < 1 or self.tile_step[1] > self.tile_size[1]:\n            raise ValueError()\n\n        overlap = [\n            self.tile_size[0] - self.tile_step[0],\n            self.tile_size[1] - self.tile_step[1],\n        ]\n\n        self.margin_left = 0\n        self.margin_right = 0\n        self.margin_top = 0\n        self.margin_bottom = 0\n\n        if image_margin == 0:\n            # In case margin is not set, we compute it manually\n\n            nw = max(1, math.ceil((self.image_width - overlap[1]) / self.tile_step[1]))\n            nh = max(1, math.ceil((self.image_height - overlap[0]) / self.tile_step[0]))\n\n            extra_w = self.tile_step[1] * nw - (self.image_width - overlap[1])\n            extra_h = self.tile_step[0] * nh - (self.image_height - overlap[0])\n\n            self.margin_left = extra_w // 2\n            self.margin_right = extra_w - self.margin_left\n            self.margin_top = extra_h // 2\n            self.margin_bottom = extra_h - self.margin_top\n\n        else:\n            if (self.image_width - overlap[1] + 2 * image_margin) % self.tile_step[1] != 0:\n                raise ValueError()\n\n            if (self.image_height - overlap[0] + 2 * image_margin) % self.tile_step[0] != 0:\n                raise ValueError()\n\n            self.margin_left = image_margin\n            self.margin_right = image_margin\n            self.margin_top = image_margin\n            self.margin_bottom = image_margin\n\n        crops = []\n        bbox_crops = []\n\n        for y in range(0, self.image_height + self.margin_top + self.margin_bottom - self.tile_size[0] + 1, self.tile_step[0]):\n            for x in range(0, self.image_width + self.margin_left + self.margin_right - self.tile_size[1] + 1, self.tile_step[1]):\n                crops.append((x, y, self.tile_size[1], self.tile_size[0]))\n                bbox_crops.append((x - self.margin_left, y - self.margin_top, self.tile_size[1], self.tile_size[0]))\n\n        self.crops = np.array(crops)\n        self.bbox_crops = np.array(bbox_crops)\n\n    def split(self, image, border_type=cv2.BORDER_CONSTANT, value=0):\n        assert image.shape[0] == self.image_height\n        assert image.shape[1] == self.image_width\n\n        orig_shape_len = len(image.shape)\n        image = cv2.copyMakeBorder(image, self.margin_top, self.margin_bottom, self.margin_left, self.margin_right, borderType=border_type, value=value)\n\n        # This check recovers possible lack of last dummy dimension for single-channel images\n        if len(image.shape) != orig_shape_len:\n            image = np.expand_dims(image, axis=-1)\n\n        tiles = []\n        for x, y, tile_width, tile_height in self.crops:\n            tile = image[y:y + tile_height, x:x + tile_width].copy()\n            assert tile.shape[0] == self.tile_size[0]\n            assert tile.shape[1] == self.tile_size[1]\n\n            tiles.append(tile)\n\n        return tiles\n\n    def cut_patch(self, image: np.ndarray, slice_index, border_type=cv2.BORDER_CONSTANT, value=0):\n        assert image.shape[0] == self.image_height\n        assert image.shape[1] == self.image_width\n\n        orig_shape_len = len(image.shape)\n        image = cv2.copyMakeBorder(image, self.margin_top, self.margin_bottom, self.margin_left, self.margin_right, borderType=border_type, value=value)\n\n        # This check recovers possible lack of last dummy dimension for single-channel images\n        if len(image.shape) != orig_shape_len:\n            image = np.expand_dims(image, axis=-1)\n\n        x, y, tile_width, tile_height = self.crops[slice_index]\n\n        tile = image[y:y + tile_height, x:x + tile_width].copy()\n        assert tile.shape[0] == self.tile_size[0]\n        assert tile.shape[1] == self.tile_size[1]\n        return tile\n\n    @property\n    def target_shape(self):\n        target_shape = self.image_height + self.margin_bottom + self.margin_top, self.image_width + self.margin_right + self.margin_left\n        return target_shape\n\n    def merge(self, tiles: List[np.ndarray], dtype=np.float32):\n        if len(tiles) != len(self.crops):\n            raise ValueError\n\n        channels = 1 if len(tiles[0].shape) == 2 else tiles[0].shape[2]\n        target_shape = self.image_height + self.margin_bottom + self.margin_top, self.image_width + self.margin_right + self.margin_left, channels\n\n        image = np.zeros(target_shape, dtype=np.float64)\n        norm_mask = np.zeros(target_shape, dtype=np.float64)\n\n        w = np.dstack([self.weight] * channels)\n\n        for tile, (x, y, tile_width, tile_height) in zip(tiles, self.crops):\n            # print(x, y, tile_width, tile_height, image.shape)\n            image[y:y + tile_height, x:x + tile_width] += tile * w\n            norm_mask[y:y + tile_height, x:x + tile_width] += w\n\n        # print(norm_mask.min(), norm_mask.max())\n        norm_mask = np.clip(norm_mask, a_min=np.finfo(norm_mask.dtype).eps, a_max=None)\n        normalized = np.divide(image, norm_mask).astype(dtype)\n        crop = self.crop_to_orignal_size(normalized)\n        return crop\n\n    def crop_to_orignal_size(self, image):\n        assert image.shape[0] == self.target_shape[0]\n        assert image.shape[1] == self.target_shape[1]\n        crop = image[self.margin_top:self.image_height + self.margin_top, self.margin_left:self.image_width + self.margin_left]\n        assert crop.shape[0] == self.image_height\n        assert crop.shape[1] == self.image_width\n        return crop\n\n    def _mean(self, tile_size):\n        return np.ones((tile_size[0], tile_size[1]), dtype=np.float32)\n\n    def _pyramid(self, tile_size):\n        w, _, _ = compute_pyramid_patch_weight_loss(tile_size[0], tile_size[1])\n        return w\n\n\nclass CudaTileMerger:\n    """"""\n    Helper class to merge final image on GPU. This generally faster than moving individual tiles to CPU.\n    """"""\n\n    def __init__(self, image_shape, channels, weight):\n        """"""\n\n        :param image_shape: Shape of the source image\n        :param image_margin:\n        :param weight: Weighting matrix\n        """"""\n        self.image_height = image_shape[0]\n        self.image_width = image_shape[1]\n\n        self.weight = torch.from_numpy(np.expand_dims(weight, axis=0)).float().cuda()\n        self.channels = channels\n        self.image = torch.zeros((channels, self.image_height, self.image_width)).cuda()\n        self.norm_mask = torch.zeros((1, self.image_height, self.image_width)).cuda()\n\n    def integrate_batch(self, batch: torch.Tensor, crop_coords):\n        """"""\n        Accumulates batch of tile predictions\n        :param batch: Predicted tiles\n        :param crop_coords: Corresponding tile crops w.r.t to original image\n        """"""\n        if len(batch) != len(crop_coords):\n            raise ValueError(""Number of images in batch does not correspond to number of coordinates"")\n\n        for tile, (x, y, tile_width, tile_height) in zip(batch, crop_coords):\n            self.image[:, y:y + tile_height, x:x + tile_width] += tile * self.weight\n            self.norm_mask[:, y:y + tile_height, x:x + tile_width] += self.weight\n\n    def merge(self) -> torch.Tensor:\n        return self.image / self.norm_mask\n'"
src/transforms.py,1,"b'import cv2\nimport torch\nimport random\nimport librosa\nimport numpy as np\n\nfrom src.random_resized_crop import RandomResizedCrop\n\ncv2.setNumThreads(0)\n\n\ndef image_crop(image, bbox):\n    return image[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n\n\ndef gauss_noise(image, sigma_sq):\n    h, w = image.shape\n    gauss = np.random.normal(0, sigma_sq, (h, w))\n    gauss = gauss.reshape(h, w)\n    image = image + gauss\n    return image\n\n\n# Source: https://www.kaggle.com/davids1992/specaugment-quick-implementation\ndef spec_augment(spec: np.ndarray,\n                 num_mask=2,\n                 freq_masking=0.15,\n                 time_masking=0.20,\n                 value=0):\n    spec = spec.copy()\n    num_mask = random.randint(1, num_mask)\n    for i in range(num_mask):\n        all_freqs_num, all_frames_num  = spec.shape\n        freq_percentage = random.uniform(0.0, freq_masking)\n\n        num_freqs_to_mask = int(freq_percentage * all_freqs_num)\n        f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)\n        f0 = int(f0)\n        spec[f0:f0 + num_freqs_to_mask, :] = value\n\n        time_percentage = random.uniform(0.0, time_masking)\n\n        num_frames_to_mask = int(time_percentage * all_frames_num)\n        t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)\n        t0 = int(t0)\n        spec[:, t0:t0 + num_frames_to_mask] = value\n    return spec\n\n\nclass SpecAugment:\n    def __init__(self,\n                 num_mask=2,\n                 freq_masking=0.15,\n                 time_masking=0.20):\n        self.num_mask = num_mask\n        self.freq_masking = freq_masking\n        self.time_masking = time_masking\n\n    def __call__(self, image):\n        return spec_augment(image,\n                            self.num_mask,\n                            self.freq_masking,\n                            self.time_masking,\n                            image.min())\n\n\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, trg=None):\n        if trg is None:\n            for t in self.transforms:\n                image = t(image)\n            return image\n        else:\n            for t in self.transforms:\n                image, trg = t(image, trg)\n            return image, trg\n\n\nclass UseWithProb:\n    def __init__(self, transform, prob=.5):\n        self.transform = transform\n        self.prob = prob\n\n    def __call__(self, image, trg=None):\n        if trg is None:\n            if random.random() < self.prob:\n                image = self.transform(image)\n            return image\n        else:\n            if random.random() < self.prob:\n                image, trg = self.transform(image, trg)\n            return image, trg\n\n\nclass OneOf:\n    def __init__(self, transforms, p=None):\n        self.transforms = transforms\n        self.p = p\n\n    def __call__(self, image, trg=None):\n        transform = np.random.choice(self.transforms, p=self.p)\n        if trg is None:\n            image = transform(image)\n            return image\n        else:\n            image, trg = transform(image, trg)\n            return image, trg\n\n\nclass Flip:\n    def __init__(self, flip_code):\n        assert flip_code == 0 or flip_code == 1\n        self.flip_code = flip_code\n\n    def __call__(self, image):\n        image = cv2.flip(image, self.flip_code)\n        return image\n\n\nclass HorizontalFlip(Flip):\n    def __init__(self):\n        super().__init__(1)\n\n\nclass VerticalFlip(Flip):\n    def __init__(self):\n        super().__init__(0)\n\n\nclass GaussNoise:\n    def __init__(self, sigma_sq):\n        self.sigma_sq = sigma_sq\n\n    def __call__(self, image):\n        if self.sigma_sq > 0.0:\n            image = gauss_noise(image,\n                                np.random.uniform(0, self.sigma_sq))\n        return image\n\n\nclass RandomGaussianBlur:\n    \'\'\'Apply Gaussian blur with random kernel size\n    Args:\n        max_ksize (int): maximal size of a kernel to apply, should be odd\n        sigma_x (int): Standard deviation\n    \'\'\'\n    def __init__(self, max_ksize=5, sigma_x=20):\n        assert max_ksize % 2 == 1, ""max_ksize should be odd""\n        self.max_ksize = max_ksize // 2 + 1\n        self.sigma_x = sigma_x\n\n    def __call__(self, image):\n        kernel_size = tuple(2 * np.random.randint(0, self.max_ksize, 2) + 1)\n        blured_image = cv2.GaussianBlur(image, kernel_size, self.sigma_x)\n        return blured_image\n\n\nclass ImageToTensor:\n    def __call__(self, image):\n        delta = librosa.feature.delta(image)\n        accelerate = librosa.feature.delta(image, order=2)\n        image = np.stack([image, delta, accelerate], axis=0)\n        image = image.astype(np.float32) / 100\n        image = torch.from_numpy(image)\n        return image\n\n\nclass RandomCrop:\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, signal):\n        start = random.randint(0, signal.shape[1] - self.size)\n        return signal[:, start: start + self.size]\n\n\nclass CenterCrop:\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, signal):\n\n        if signal.shape[1] > self.size:\n            start = (signal.shape[1] - self.size) // 2\n            return signal[:, start: start + self.size]\n        else:\n            return signal\n\n\nclass PadToSize:\n    def __init__(self, size, mode=\'constant\'):\n        assert mode in [\'constant\', \'wrap\']\n        self.size = size\n        self.mode = mode\n\n    def __call__(self, signal):\n        if signal.shape[1] < self.size:\n            padding = self.size - signal.shape[1]\n            offset = padding // 2\n            pad_width = ((0, 0), (offset, padding - offset))\n            if self.mode == \'constant\':\n                signal = np.pad(signal, pad_width,\n                                \'constant\', constant_values=signal.min())\n            else:\n                signal = np.pad(signal, pad_width, \'wrap\')\n        return signal\n\n\ndef get_transforms(train, size,\n                   wrap_pad_prob=0.5,\n                   resize_scale=(0.8, 1.0),\n                   resize_ratio=(1.7, 2.3),\n                   resize_prob=0.33,\n                   spec_num_mask=2,\n                   spec_freq_masking=0.15,\n                   spec_time_masking=0.20,\n                   spec_prob=0.5):\n    if train:\n        transforms = Compose([\n            OneOf([\n                PadToSize(size, mode=\'wrap\'),\n                PadToSize(size, mode=\'constant\'),\n            ], p=[wrap_pad_prob, 1 - wrap_pad_prob]),\n            RandomCrop(size),\n            UseWithProb(\n                RandomResizedCrop(scale=resize_scale, ratio=resize_ratio),\n                prob=resize_prob\n            ),\n            UseWithProb(SpecAugment(num_mask=spec_num_mask,\n                                    freq_masking=spec_freq_masking,\n                                    time_masking=spec_time_masking), spec_prob),\n            ImageToTensor()\n        ])\n    else:\n        transforms = Compose([\n            PadToSize(size),\n            CenterCrop(size),\n            ImageToTensor()\n        ])\n    return transforms\n'"
src/utils.py,0,"b'import re\nimport json\nimport pickle\nimport numpy as np\nfrom pathlib import Path\nfrom scipy.stats.mstats import gmean\n\nfrom src.datasets import get_noisy_data_generator, get_folds_data, get_augment_folds_data_generator\nfrom src import config\n\n\ndef gmean_preds_blend(probs_df_lst):\n    blend_df = probs_df_lst[0]\n    blend_values = np.stack([df.loc[blend_df.index.values].values\n                             for df in probs_df_lst], axis=0)\n    blend_values = gmean(blend_values, axis=0)\n\n    blend_df.values[:] = blend_values\n    return blend_df\n\n\ndef get_best_model_path(dir_path: Path, return_score=False):\n    model_scores = []\n    for model_path in dir_path.glob(\'*.pth\'):\n        score = re.search(r\'-(\\d+(?:\\.\\d+)?).pth\', str(model_path))\n        if score is not None:\n            score = float(score.group(0)[1:-4])\n            model_scores.append((model_path, score))\n    model_score = sorted(model_scores, key=lambda x: x[1])\n    best_model_path = model_score[-1][0]\n    if return_score:\n        best_score = model_score[-1][1]\n        return best_model_path, best_score\n    else:\n        return best_model_path\n\n\ndef pickle_save(obj, filename):\n    print(f""Pickle save to: {filename}"")\n    with open(filename, \'wb\') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\n\ndef pickle_load(filename):\n    print(f""Pickle load from: {filename}"")\n    with open(filename, \'rb\') as f:\n        return pickle.load(f)\n    \n\ndef load_folds_data(use_corrections=True):\n    if use_corrections:\n        with open(config.corrections_json_path) as file:\n            corrections = json.load(file)\n        print(""Corrections:"", corrections)\n        pkl_name = f\'{config.audio.get_hash(corrections=corrections)}.pkl\'\n    else:\n        corrections = None\n        pkl_name = f\'{config.audio.get_hash()}.pkl\'\n\n    folds_data_pkl_path = config.folds_data_pkl_dir / pkl_name\n\n    if folds_data_pkl_path.exists():\n        folds_data = pickle_load(folds_data_pkl_path)\n    else:\n        folds_data = get_folds_data(corrections)\n        if not config.folds_data_pkl_dir.exists():\n            config.folds_data_pkl_dir.mkdir(parents=True, exist_ok=True)\n        pickle_save(folds_data, folds_data_pkl_path)\n    return folds_data\n\n\ndef load_noisy_data():\n    with open(config.noisy_corrections_json_path) as file:\n        corrections = json.load(file)\n\n    pkl_name_glob = f\'{config.audio.get_hash(corrections=corrections)}_*.pkl\'\n    pkl_paths = sorted(config.noisy_data_pkl_dir.glob(pkl_name_glob))\n\n    images_lst, targets_lst = [], []\n\n    if pkl_paths:\n        for pkl_path in pkl_paths:\n            data_batch = pickle_load(pkl_path)\n            images_lst += data_batch[0]\n            targets_lst += data_batch[1]\n    else:\n        if not config.noisy_data_pkl_dir.exists():\n            config.noisy_data_pkl_dir.mkdir(parents=True, exist_ok=True)\n\n        for i, data_batch in enumerate(get_noisy_data_generator()):\n            pkl_name = f\'{config.audio.get_hash(corrections=corrections)}_{i:02}.pkl\'\n            noisy_data_pkl_path = config.noisy_data_pkl_dir / pkl_name\n            pickle_save(data_batch, noisy_data_pkl_path)\n\n            images_lst += data_batch[0]\n            targets_lst += data_batch[1]\n\n    return images_lst, targets_lst\n\n\ndef load_augment_folds_data(time_stretch_lst, pitch_shift_lst):\n    config_hash = config.audio.get_hash(time_stretch_lst=time_stretch_lst,\n                                        pitch_shift_lst=pitch_shift_lst)\n    pkl_name_glob = f\'{config_hash}_*.pkl\'\n    pkl_paths = sorted(config.augment_folds_data_pkl_dir.glob(pkl_name_glob))\n\n    images_lst, targets_lst, folds_lst = [], [], []\n\n    if pkl_paths:\n        for pkl_path in pkl_paths:\n            data_batch = pickle_load(pkl_path)\n            images_lst += data_batch[0]\n            targets_lst += data_batch[1]\n            folds_lst += data_batch[2]\n    else:\n        if not config.augment_folds_data_pkl_dir.exists():\n            config.augment_folds_data_pkl_dir.mkdir(parents=True, exist_ok=True)\n\n        generator = get_augment_folds_data_generator(time_stretch_lst, pitch_shift_lst)\n        for i, data_batch in enumerate(generator):\n            pkl_name = f\'{config_hash}_{i:02}.pkl\'\n            augment_data_pkl_path = config.augment_folds_data_pkl_dir / pkl_name\n            pickle_save(data_batch, augment_data_pkl_path)\n\n            images_lst += data_batch[0]\n            targets_lst += data_batch[1]\n            folds_lst += data_batch[2]\n\n    return images_lst, targets_lst, folds_lst\n'"
src/models/__init__.py,0,b''
src/models/aux_skip_attention.py,5,"b""import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\n# Source: https://github.com/luuuyi/CBAM.PyTorch\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\n\n# Source: https://github.com/luuuyi/CBAM.PyTorch\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\n\nclass ConvolutionalBlockAttentionModule(nn.Module):\n    def __init__(self, in_planes, ratio=16, kernel_size=7):\n        super(ConvolutionalBlockAttentionModule, self).__init__()\n        self.ca = ChannelAttention(in_planes, ratio)\n        self.sa = SpatialAttention(kernel_size)\n\n    def forward(self, input):\n        out = self.ca(input) * input\n        out = self.sa(out) * out\n        return out\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.avg_pool2d(x, 2)\n        return x\n\n\nclass SkipBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, scale_factor=2):\n        super(SkipBlock, self).__init__()\n\n        self.conv = nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        if self.scale_factor >= 2:\n            x = F.avg_pool2d(x, self.scale_factor)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass AuxBlock(nn.Module):\n    def __init__(self, last_fc, num_classes, base_size, dropout):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(base_size*8, base_size*last_fc),\n            nn.PReLU(),\n            nn.BatchNorm1d(base_size*last_fc),\n            nn.Dropout(dropout/2),\n            nn.Linear(base_size*last_fc, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n\nclass AuxSkipAttention(nn.Module):\n    def __init__(self, num_classes, base_size=64,\n                 dropout=0.2, ratio=16, kernel_size=7,\n                 last_filters=8, last_fc=2):\n        super().__init__()\n\n        self.conv1 = ConvBlock(in_channels=3, out_channels=base_size)\n        self.skip1 = SkipBlock(in_channels=base_size, out_channels=base_size*8,\n                               scale_factor=8)\n\n        self.conv2 = ConvBlock(in_channels=base_size, out_channels=base_size*2)\n        self.skip2 = SkipBlock(in_channels=base_size * 2, out_channels=base_size*8,\n                               scale_factor=4)\n\n        self.conv3 = ConvBlock(in_channels=base_size*2, out_channels=base_size*4)\n        self.skip3 = SkipBlock(in_channels=base_size*4, out_channels=base_size*8,\n                               scale_factor=2)\n\n        self.conv4 = ConvBlock(in_channels=base_size*4, out_channels=base_size*8)\n\n        self.attention = ConvolutionalBlockAttentionModule(base_size*8*4,\n                                                           ratio=ratio,\n                                                           kernel_size=kernel_size)\n        self.merge = SkipBlock(base_size*8*4, base_size*last_filters, 1)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(base_size*last_filters, base_size*last_fc),\n            nn.PReLU(),\n            nn.BatchNorm1d(base_size*last_fc),\n            nn.Dropout(dropout/2),\n            nn.Linear(base_size*last_fc, num_classes),\n        )\n\n        self.aux1 = AuxBlock(last_fc, num_classes, base_size, dropout)\n        self.aux2 = AuxBlock(last_fc, num_classes, base_size, dropout)\n        self.aux3 = AuxBlock(last_fc, num_classes, base_size, dropout)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        skip1 = self.skip1(x)\n        aux1 = self.aux1(skip1)\n\n        x = self.conv2(x)\n        skip2 = self.skip2(x)\n        aux2 = self.aux2(skip2)\n\n        x = self.conv3(x)\n        skip3 = self.skip3(x)\n        aux3 = self.aux3(skip3)\n\n        x = self.conv4(x)\n\n        x = torch.cat([x, skip1, skip2, skip3], dim=1)\n\n        x = self.attention(x)\n        x = self.merge(x)\n\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x, aux3, aux2, aux1\n"""
src/models/feature_extractor.py,1,"b'import torch.nn as nn\n\nnonlinearity = nn.ReLU\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_planes, out_planes,\n            kernel_size=kernel_size, stride=stride, padding=padding, bias=False, dilation=dilation)\n        self.bn = nn.BatchNorm2d(out_planes)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, num_classes, input_channels=3, base_size=32, dropout=0.25):\n        super(FeatureExtractor, self).__init__()\n        self.input_channels = input_channels\n        self.base_size = base_size\n        s = base_size\n        self.dropout = dropout\n        self.input_conv = BasicConv2d(input_channels, s, 1)\n        self.conv_1 = BasicConv2d(s * 1, s * 1, 3, padding=1)\n        self.conv_2 = BasicConv2d(s * 1, s * 1, 3, padding=1)\n        self.conv_3 = BasicConv2d(s * 1, s * 2, 3, padding=1)\n        self.conv_4 = BasicConv2d(s * 2, s * 2, 3, padding=1)\n        self.conv_5 = BasicConv2d(s * 2, s * 4, 3, padding=1)\n        self.conv_6 = BasicConv2d(s * 4, s * 4, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout2d = nn.Dropout2d(p=dropout)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(s * 4, num_classes)\n\n    def forward(self, x):\n        x = self.input_conv(x)\n\n        x = self.conv_1(x)\n        x = self.pool(x)\n        x = self.conv_2(x)\n        x = self.pool(x)\n        x = self.dropout2d(x)\n\n        x = self.conv_3(x)\n        x = self.pool(x)\n        x = self.conv_4(x)\n        x = self.pool(x)\n        x = self.dropout2d(x)\n\n        x = self.conv_5(x)\n        x = self.pool(x)\n        x = self.conv_6(x)\n        x = self.pool(x)\n        x = self.dropout2d(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n'"
src/models/resnet.py,7,"b'import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\', \'resnext50_32x4d\', \'resnext101_32x8d\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, groups=groups, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\'BasicBlock only supports groups=1 and base_width=64\')\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, norm_layer=None):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n\n        self.inplanes = 64\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, norm_layer=None):\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\n\ndef resnext50_32x4d(pretrained=False, **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 6, 3], groups=32, width_per_group=4, **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls[\'resnext50_32x4d\']))\n    return model\n\n\ndef resnext101_32x8d(pretrained=False, **kwargs):\n    model = ResNet(Bottleneck, [3, 4, 23, 3], groups=32, width_per_group=8, **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls[\'resnext101_32x8d\']))\n    return model\n'"
src/models/rnn_aux_skip_attention.py,7,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), \'kernel size must be 3 or 7\'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\n\nclass ConvolutionalBlockAttentionModule(nn.Module):\n    def __init__(self, in_planes, ratio=16, kernel_size=7):\n        super(ConvolutionalBlockAttentionModule, self).__init__()\n        self.ca = ChannelAttention(in_planes, ratio)\n        self.sa = SpatialAttention(kernel_size)\n\n    def forward(self, input):\n        out = self.ca(input) * input\n        out = self.sa(out) * out\n        return out\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.avg_pool2d(x, 2)\n        return x\n\n\nclass SkipBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, scale_factor=2):\n        super(SkipBlock, self).__init__()\n\n        self.conv = nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        if self.scale_factor >= 2:\n            x = F.avg_pool2d(x, self.scale_factor)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass AuxBlock(nn.Module):\n    def __init__(self, last_fc, num_classes, base_size, dropout):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(base_size*8, base_size*last_fc),\n            nn.PReLU(),\n            nn.BatchNorm1d(base_size*last_fc),\n            nn.Dropout(dropout/2),\n            nn.Linear(base_size*last_fc, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n\nclass BidirectionalLSTM(nn.Module):\n    def __init__(self, in_channels, hidden, out_channels):\n        super(BidirectionalLSTM, self).__init__()\n\n        self.rnn = nn.LSTM(in_channels, hidden, bidirectional=True)\n        self.embedding = nn.Linear(hidden * 2, out_channels)\n\n    def forward(self, input):\n        recurrent, _ = self.rnn(input)\n        T, b, h = recurrent.size()\n        t_rec = recurrent.view(T * b, h)\n\n        output = self.embedding(t_rec)\n        output = output.view(T, b, -1)\n\n        return output\n\n\nclass RnnAuxSkipAttention(nn.Module):\n    def __init__(self, num_classes, base_size=64,\n                 dropout=0.2, ratio=16, kernel_size=7,\n                 last_filters=8, last_fc=2):\n        super().__init__()\n\n        self.conv1 = ConvBlock(in_channels=3, out_channels=base_size)\n        self.skip1 = SkipBlock(in_channels=base_size, out_channels=base_size*8,\n                               scale_factor=8)\n\n        self.conv2 = ConvBlock(in_channels=base_size, out_channels=base_size*2)\n        self.skip2 = SkipBlock(in_channels=base_size * 2, out_channels=base_size*8,\n                               scale_factor=4)\n\n        self.conv3 = ConvBlock(in_channels=base_size*2, out_channels=base_size*4)\n        self.skip3 = SkipBlock(in_channels=base_size*4, out_channels=base_size*8,\n                               scale_factor=2)\n\n        self.conv4 = ConvBlock(in_channels=base_size*4, out_channels=base_size*8)\n\n        self.attention = ConvolutionalBlockAttentionModule(base_size*8*4,\n                                                           ratio=ratio,\n                                                           kernel_size=kernel_size)\n        self.merge = SkipBlock(base_size*8*4, base_size*last_filters, 1)\n\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(base_size*last_filters, base_size*last_fc),\n            nn.PReLU(),\n            nn.BatchNorm1d(base_size*last_fc),\n            nn.Dropout(dropout/2),\n            nn.Linear(base_size*last_fc, num_classes),\n        )\n\n        self.aux1 = AuxBlock(last_fc, num_classes, base_size, dropout)\n        self.aux2 = AuxBlock(last_fc, num_classes, base_size, dropout)\n        self.aux3 = AuxBlock(last_fc, num_classes, base_size, dropout)\n\n        self.rnn = BidirectionalLSTM(base_size*last_filters,\n                                     base_size*last_filters,\n                                     base_size*last_filters)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        skip1 = self.skip1(x)\n        aux1 = self.aux1(skip1)\n\n        x = self.conv2(x)\n        skip2 = self.skip2(x)\n        aux2 = self.aux2(skip2)\n\n        x = self.conv3(x)\n        skip3 = self.skip3(x)\n        aux3 = self.aux3(skip3)\n\n        x = self.conv4(x)\n\n        x = torch.cat([x, skip1, skip2, skip3], dim=1)\n\n        x = self.attention(x)\n        x = self.merge(x)\n\n        x = torch.mean(x, dim=2, keepdim=True)\n        b, c, h, w = x.size()\n        assert h == 1, f""the height of conv must be 1, got {h}""\n        x = x.squeeze(2)\n        x = x.permute(2, 0, 1)\n        x = self.rnn(x)\n        x = x.permute(1, 0, 2)\n        x = torch.mean(x, dim=1)\n\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x, aux3, aux2, aux1\n'"
src/models/senet.py,2,"b'""""""\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n""""""\nfrom __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = [\'SENet\', \'senet154\', \'se_resnet50\', \'se_resnet101\', \'se_resnet152\',\n           \'se_resnext50_32x4d\', \'se_resnext101_32x4d\']\n\npretrained_settings = {\n    \'senet154\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet50\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet101\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet152\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext50_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Base class for bottlenecks that implements `forward()` method.\n    """"""\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    """"""\n    Bottleneck for SENet154.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    """"""\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    """"""\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        """"""\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        """"""\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                (\'bn1\', nn.BatchNorm2d(64)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n                (\'conv2\', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn2\', nn.BatchNorm2d(64)),\n                (\'relu2\', nn.ReLU(inplace=True)),\n                (\'conv3\', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn3\', nn.BatchNorm2d(inplanes)),\n                (\'relu3\', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                (\'bn1\', nn.BatchNorm2d(inplanes)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        # layer0_modules.append((\'pool\', nn.MaxPool2d(3, stride=2,\n        #                                             ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings[\'num_classes\'], \\\n        \'num_classes should be {}, but is {}\'.format(\n            settings[\'num_classes\'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n\n\ndef senet154(num_classes=1000, pretrained=None, dropout_p=0.2):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=dropout_p, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'senet154\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained=None, dropout_p=None):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=dropout_p, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet50\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained=None, dropout_p=None):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=dropout_p, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet101\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained=None, dropout_p=None):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=dropout_p, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet152\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained=None, dropout_p=None):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=dropout_p, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext50_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained=None, dropout_p=None):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=dropout_p, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext101_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n'"
src/models/simple_attention.py,4,"b""import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\n\nclass ConvolutionalBlockAttentionModule(nn.Module):\n    def __init__(self, in_planes, ratio=16, kernel_size=7):\n        super(ConvolutionalBlockAttentionModule, self).__init__()\n        self.ca = ChannelAttention(in_planes, ratio)\n        self.sa = SpatialAttention(kernel_size)\n\n    def forward(self, input):\n        out = self.ca(input) * input\n        out = self.sa(out) * out\n        return out\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.avg_pool2d(x, 2)\n        return x\n\n\nclass SimpleAttention(nn.Module):\n    def __init__(self, num_classes, base_size=64, dropout=0.2,\n                 ratio=16, kernel_size=7):\n        super().__init__()\n\n        self.conv = nn.Sequential(\n            ConvBlock(in_channels=3, out_channels=base_size),\n            ConvBlock(in_channels=base_size, out_channels=base_size*2),\n            ConvBlock(in_channels=base_size*2, out_channels=base_size*4),\n            ConvBlock(in_channels=base_size*4, out_channels=base_size*8),\n        )\n        self.attention = ConvolutionalBlockAttentionModule(base_size*8,\n                                                           ratio=ratio,\n                                                           kernel_size=kernel_size)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(base_size*8, base_size*2),\n            nn.PReLU(),\n            nn.BatchNorm1d(base_size*2),\n            nn.Dropout(dropout/2),\n            nn.Linear(base_size*2, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.attention(x)\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
src/models/simple_kaggle.py,3,"b'# Source: https://www.kaggle.com/mhiro2/simple-2d-cnn-classifier-with-pytorch\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.avg_pool2d(x, 2)\n        return x\n\n\nclass SimpleKaggle(nn.Module):\n    def __init__(self, num_classes, base_size=64, dropout=0.2):\n        super().__init__()\n\n        self.conv = nn.Sequential(\n            ConvBlock(in_channels=3, out_channels=base_size),\n            ConvBlock(in_channels=base_size, out_channels=base_size*2),\n            ConvBlock(in_channels=base_size*2, out_channels=base_size*4),\n            ConvBlock(in_channels=base_size*4, out_channels=base_size*8),\n        )\n\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(base_size*8, base_size*2),\n            nn.PReLU(),\n            nn.BatchNorm1d(base_size*2),\n            nn.Dropout(dropout/2),\n            nn.Linear(base_size*2, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.mean(x, dim=3)\n        x, _ = torch.max(x, dim=2)\n        x = self.fc(x)\n        return x\n'"
src/models/skip_attention.py,5,"b""import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\n\nclass ConvolutionalBlockAttentionModule(nn.Module):\n    def __init__(self, in_planes, ratio=16, kernel_size=7):\n        super(ConvolutionalBlockAttentionModule, self).__init__()\n        self.ca = ChannelAttention(in_planes, ratio)\n        self.sa = SpatialAttention(kernel_size)\n\n    def forward(self, input):\n        out = self.ca(input) * input\n        out = self.sa(out) * out\n        return out\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.avg_pool2d(x, 2)\n        return x\n\n\nclass SkipBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, scale_factor=2):\n        super(SkipBlock, self).__init__()\n\n        self.conv = nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.scale_factor = scale_factor\n\n    def forward(self, x):\n        if self.scale_factor >= 2:\n            x = F.avg_pool2d(x, self.scale_factor)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass SkipAttention(nn.Module):\n    def __init__(self, num_classes, base_size=64,\n                 dropout=0.2, ratio=16, kernel_size=7,\n                 last_filters=8, last_fc=2):\n        super().__init__()\n\n        self.conv1 = ConvBlock(in_channels=3, out_channels=base_size)\n        self.skip1 = SkipBlock(in_channels=base_size, out_channels=base_size*8,\n                               scale_factor=8)\n\n        self.conv2 = ConvBlock(in_channels=base_size, out_channels=base_size*2)\n        self.skip2 = SkipBlock(in_channels=base_size * 2, out_channels=base_size*8,\n                               scale_factor=4)\n\n        self.conv3 = ConvBlock(in_channels=base_size*2, out_channels=base_size*4)\n        self.skip3 = SkipBlock(in_channels=base_size*4, out_channels=base_size*8,\n                               scale_factor=2)\n\n        self.conv4 = ConvBlock(in_channels=base_size*4, out_channels=base_size*8)\n\n        self.attention = ConvolutionalBlockAttentionModule(base_size*8*4,\n                                                           ratio=ratio,\n                                                           kernel_size=kernel_size)\n        self.merge = SkipBlock(base_size*8*4, base_size*last_filters, 1)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(base_size*last_filters, base_size*last_fc),\n            nn.PReLU(),\n            nn.BatchNorm1d(base_size*last_fc),\n            nn.Dropout(dropout/2),\n            nn.Linear(base_size*last_fc, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        skip1 = self.skip1(x)\n\n        x = self.conv2(x)\n        skip2 = self.skip2(x)\n\n        x = self.conv3(x)\n        skip3 = self.skip3(x)\n\n        x = self.conv4(x)\n\n        x = torch.cat([x, skip1, skip2, skip3], dim=1)\n\n        x = self.attention(x)\n        x = self.merge(x)\n\n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n"""
src/stacking/__init__.py,0,b'import src.stacking.argus_models\n'
src/stacking/argus_models.py,1,"b""import torch\nfrom argus import Model\n\nfrom src.stacking.models import FCNet\n\n\nclass StackingModel(Model):\n    nn_module = {\n        'FCNet': FCNet\n    }\n    prediction_transform = torch.nn.Sigmoid\n"""
src/stacking/datasets.py,2,"b'import time\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom torch.utils.data import Dataset\n\nfrom src import config\n\n\ndef load_fname_probs(experiments, fold, fname):\n    prob_lst = []\n    for experiment in experiments:\n        npy_path = config.predictions_dir / experiment / f\'fold_{fold}\' / \'val\' / (fname + \'.npy\')\n        prob = np.load(npy_path)\n        prob_lst.append(prob)\n    probs = np.concatenate(prob_lst, axis=1)\n    return probs\n\n\ndef get_out_of_folds_data(experiments, corrections=None):\n    train_folds_df = pd.read_csv(config.train_folds_path)\n\n    probs_lst = []\n    targets_lst = []\n    folds_lst = []\n    fname_lst = []\n    for i, row in train_folds_df.iterrows():\n        labels = row.labels\n\n        if corrections is not None:\n            if row.fname in corrections:\n                action = corrections[row.fname]\n                if action == \'remove\':\n                    print(f""Skip {row.fname}"")\n                    continue\n                else:\n                    print(f""Replace labels {row.fname} from {labels} to {action}"")\n                    labels = action\n\n        folds_lst.append(row.fold)\n        probs = load_fname_probs(experiments, row.fold, row.fname)\n        probs_lst.append(probs)\n        target = torch.zeros(len(config.classes))\n        for label in labels.split(\',\'):\n            target[config.class2index[label]] = 1.\n        targets_lst.append(target)\n        fname_lst.append(row.fname)\n\n    return probs_lst, targets_lst, folds_lst\n\n\nclass StackingDataset(Dataset):\n    def __init__(self, folds_data, folds,\n                 transform=None, size=None):\n        super().__init__()\n        self.folds = folds\n        self.transform = transform\n        self.size = size\n\n        self.probs_lst = []\n        self.targets_lst = []\n        for prob, trg, fold in zip(*folds_data):\n            if fold in folds:\n                self.probs_lst.append(prob)\n                self.targets_lst.append(trg)\n\n    def __len__(self):\n        if self.size is None:\n            return len(self.probs_lst)\n        else:\n            return self.size\n\n    def __getitem__(self, idx):\n        if self.size is not None:\n            seed = int(time.time() * 1000.0) + idx\n            np.random.seed(seed % (2 ** 31))\n            idx = np.random.randint(len(self.probs_lst))\n\n        probs = self.probs_lst[idx].copy()\n        target = self.targets_lst[idx].clone()\n\n        if self.transform is not None:\n            probs = self.transform(probs)\n\n        return probs, target\n'"
src/stacking/models.py,2,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass SEScale(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super().__init__()\n        channel = in_channels\n        self.fc1 = nn.Linear(channel, reduction)\n        self.fc2 = nn.Linear(reduction, channel)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x, inplace=True)\n        x = self.fc2(x)\n        x = torch.sigmoid(x)\n        return x\n\n\nclass FCNet(nn.Module):\n    def __init__(self, in_channels, num_classes,\n                 base_size=64, reduction_scale=8,\n                 p_dropout=0.2):\n        super().__init__()\n        self.p_dropout = p_dropout\n\n        self.scale = SEScale(in_channels, in_channels // reduction_scale)\n        self.linear1 = nn.Linear(in_channels, base_size*2)\n        self.relu1 = nn.PReLU()\n        self.linear2 = nn.Linear(base_size*2, base_size)\n        self.relu2 = nn.PReLU()\n        self.fc = nn.Linear(base_size, num_classes)\n\n    def forward(self, x):\n        x = self.scale(x) * x\n\n        x = self.linear1(x)\n        x = self.relu1(x)\n        if self.p_dropout is not None:\n            x = F.dropout(x, p=self.p_dropout, training=self.training)\n        x = self.linear2(x)\n        x = self.relu2(x)\n        if self.p_dropout is not None:\n            x = F.dropout(x, p=self.p_dropout, training=self.training)\n        x = self.fc(x)\n        return x\n'"
src/stacking/predictor.py,3,"b""import torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\nfrom argus import load_model\n\n\nclass StackPredictor:\n    def __init__(self, model_path,\n                 batch_size, device='cuda'):\n        self.model = load_model(model_path, device=device)\n        self.batch_size = batch_size\n\n    def predict(self, probs):\n        probs = probs.copy()\n        stack_tensors = [torch.from_numpy(prob.astype(np.float32))\n                         for prob in probs]\n\n        loader = DataLoader(stack_tensors, batch_size=self.batch_size)\n\n        preds_lst = []\n        for probs_batch in loader:\n            pred_batch = self.model.predict(probs_batch)\n            preds_lst.append(pred_batch)\n\n        pred = torch.cat(preds_lst, dim=0)\n\n        return pred.cpu().numpy()\n"""
src/stacking/transforms.py,1,"b""import torch\nimport random\nimport numpy as np\n\nfrom src.transforms import OneOf, UseWithProb, Compose\n\n\nclass PadToSize:\n    def __init__(self, size, mode='constant'):\n        assert mode in ['constant', 'wrap']\n        self.size = size\n        self.mode = mode\n\n    def __call__(self, signal):\n        if signal.shape[0] < self.size:\n            padding = self.size - signal.shape[0]\n            pad_width = ((0, padding), (0, 0))\n            if self.mode == 'constant':\n                signal = np.pad(signal, pad_width, 'constant')\n            else:\n                signal = np.pad(signal, pad_width, 'wrap')\n        return signal\n\n\nclass CenterCrop:\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, signal):\n\n        if signal.shape[0] > self.size:\n            start = (signal.shape[0] - self.size) // 2\n            return signal[start: start + self.size]\n        else:\n            return signal\n\n\nclass RandomCrop:\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, signal):\n        start = random.randint(0, signal.shape[0] - self.size)\n        return signal[start: start + self.size]\n\n\nclass RandomSizedCrop:\n    def __call__(self, signal):\n        size = random.randint(1, signal.shape[0])\n        start = random.randint(0, signal.shape[0] - size)\n        return signal[start: start + size]\n\n\nclass MeanOverTime:\n    def __call__(self, probs):\n        return probs.mean(axis=0)\n\n\nclass Flatten:\n    def __call__(self, probs):\n        return probs.flatten()\n\n\nclass ToTensor:\n    def __call__(self, probs):\n        probs = torch.from_numpy(probs)\n        return probs\n\n\nclass RandomStrideCrop:\n    def __call__(self, signal):\n        size = random.randint(1, signal.shape[0])\n        indexes = np.random.choice(np.arange(0, signal.shape[0]),\n                                   size=size, replace=False)\n        return signal[indexes]\n\n\ndef get_transforms(train):\n    if train:\n        transforms = Compose([\n            UseWithProb(RandomSizedCrop(), prob=0.5),\n            UseWithProb(RandomStrideCrop(), prob=0.5),\n            MeanOverTime(),\n            ToTensor()\n        ])\n    else:\n        transforms = Compose([\n            MeanOverTime(),\n            ToTensor()\n        ])\n    return transforms\n"""
