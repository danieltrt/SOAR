file_path,api_count,code
setup.py,0,"b'import setuptools\n\nwith open(""readme.rst"", ""r"") as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name=\'labml\',\n    version=\'0.4.14\',\n    author=""Varuna Jayasiri, Nipun Wijerathne"",\n    author_email=""vpjayasiri@gmail.com, hnipun@gmail.com"",\n    description=""Organize Machine Learning Experiments"",\n    long_description=long_description,\n    long_description_content_type=""text/x-rst"",\n    url=""https://github.com/lab-ml/labml"",\n    project_urls={\n        \'Documentation\': \'https://lab-ml.com/\'\n    },\n    packages=setuptools.find_packages(exclude=(\'test\',\n                                               \'test.*\')),\n    install_requires=[\'gitpython\',\n                      \'pyyaml\',\n                      \'numpy\'],\n    entry_points={\n        \'console_scripts\': [\'labml=labml.cli:main\'],\n    },\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Science/Research\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n    keywords=\'machine learning\',\n)\n'"
labml/__init__.py,0,b''
labml/cli.py,0,"b'import argparse\n\nfrom labml import logger\nfrom labml.logger import Text\n\n\ndef _open_dashboard():\n    try:\n        import labml_dashboard\n    except (ImportError, ModuleNotFoundError):\n        logger.log(""Cannot import "", (\'labml_dashboard\', Text.highlight), \'.\')\n        logger.log(\'Install with \',\n                   (\'pip install labml_dashboard\', Text.value))\n        return\n\n    labml_dashboard.start_server()\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'LabML CLI\')\n    parser.add_argument(\'command\', choices=[\'dashboard\'])\n\n    args = parser.parse_args()\n\n    if args.command == \'dashboard\':\n        _open_dashboard()\n    else:\n        assert False\n'"
labml/configs.py,0,"b'from typing import List, Callable, overload, Union, Tuple\n\nfrom labml.internal.configs.base import Configs as _Configs\nfrom labml.internal.configs.config_item import ConfigItem\nfrom labml.utils.errors import ConfigsError\n\n\nclass BaseConfigs(_Configs):\n    r""""""\n    You should sub-class this class to create your own configurations\n    """"""\n    pass\n\n\ndef _get_config_class(name: any):\n    if isinstance(name, ConfigItem):\n        return name.configs_class\n    elif isinstance(name, list) and len(name) > 0 and isinstance(name[0], ConfigItem):\n        return name[0].configs_class\n    else:\n        return None\n\n\n@overload\ndef option(name: Union[any, List[any]]):\n    ...\n\n\n@overload\ndef option(name: Union[any, List[any]], option_name: str):\n    ...\n\n\n@overload\ndef option(name: Union[any, List[any]], pass_params: List[any]):\n    ...\n\n\n@overload\ndef option(name: Union[any, List[any]], option_name: str, pass_params: List[any]):\n    ...\n\n\ndef option(name: Union[any, List[any]], *args: any):\n    r""""""\n    Use this as a decorator to register configuration options.\n\n    This has multiple overloads\n\n    .. function:: option(config_item: Union[any, List[any]])\n        :noindex:\n\n    .. function:: option(config_item: Union[any, List[any]], option_name: str)\n        :noindex:\n\n    .. function:: option(config_item: Union[any, List[any]], pass_params: List[any])\n        :noindex:\n\n    .. function:: option(config_item: Union[any, List[any]], option_name: str, pass_params: List[any])\n        :noindex:\n\n    Arguments:\n        name: the configuration item or a list of items.\n            If it is a list of items the function should return\n            tuple.\n\n        option_name (str, optional): name of the option.\n            If not provided it will be derived from the\n            function name.\n\n        pass_params (list, optional): list of params to be passed.\n            If not provided the configs object is passed.\n            If provided the corresponding calculated configuration items\n            will be passed to the function\n    """"""\n    config_class = _get_config_class(name)\n    if config_class is None:\n        raise ConfigsError(\'You need to pass config items to option\')\n\n    option_name = None\n    pass_params = None\n    for arg in args:\n        if isinstance(arg, str):\n            option_name = arg\n        elif isinstance(arg, list):\n            pass_params = arg\n\n    return config_class.calc(name, option_name, pass_params)\n\n\n@overload\ndef calculate(name: Union[any, List[any]], func: Callable):\n    ...\n\n\n@overload\ndef calculate(name: Union[any, List[any]], option_name: str, func: Callable):\n    ...\n\n\n@overload\ndef calculate(name: Union[any, List[any]],\n              pass_params: List[any], func: Callable):\n    ...\n\n\n@overload\ndef calculate(name: Union[any, List[any]], option_name: str,\n              pass_params: List[any], func: Callable):\n    ...\n\n\ndef calculate(name: any, *args: any):\n    r""""""\n    Use this to register configuration options.\n\n    This has multiple overloads\n\n    .. function:: calculate(name: Union[any, List[any]], func: Callable)\n        :noindex:\n\n    .. function:: calculate(name: Union[any, List[any]], option_name: str, func: Callable)\n        :noindex:\n\n    .. function:: calculate(name: Union[any, List[any]], pass_params: List[any], func: Callable)\n        :noindex:\n\n    .. function:: calculate(name: Union[any, List[any]], option_name: str, pass_params: List[any], func: Callable)\n        :noindex:\n\n    Arguments:\n        name: the configuration item or a list of items.\n            If it is a list of items the function should return\n            tuple.\n\n        func: the function to calculate the configuration\n\n        option_name (str, optional): name of the option.\n            If not provided it will be derived from the\n            function name.\n\n        pass_params (list, optional): list of params to be passed.\n            If not provided the configs object is passed.\n            If provided the corresponding calculated configuration items\n            will be passed to the function\n    """"""\n    config_class = _get_config_class(name)\n    if config_class is None:\n        raise ConfigsError(\'You need to pass config items to calculate\')\n\n    option_name = None\n    pass_params = None\n    func = None\n    for arg in args:\n        if isinstance(arg, str):\n            option_name = arg\n        elif isinstance(arg, list):\n            pass_params = arg\n        elif type(arg) == type:\n            func = arg\n        else:\n            func = arg\n\n    if func is None:\n        raise ConfigsError(\'You need to pass the function that calculates the configs\')\n\n    return config_class.calc_wrap(func, name, option_name, pass_params)\n\n\ndef hyperparams(*args: any, is_hyperparam=True):\n    r""""""\n    Identifies configuration as (or not) hyper-parameters\n\n    Arguments:\n        *args: list of configurations\n        is_hyperparam (bool, optional): whether the provided configuration\n            items are hyper-parameters. Defaults to ``True``.\n    """"""\n\n    for arg in args:\n        config_class = _get_config_class(arg)\n        if config_class is None:\n            raise ConfigsError(\'You need to pass config items to set hyperparams\')\n        config_class.set_hyperparams(arg, is_hyperparam=is_hyperparam)\n\n\ndef aggregate(name: any, option_name: str, *args: Tuple[any, any]):\n    r""""""\n    Aggregate configs\n\n    Arguments:\n        name: name of the aggregate\n        option_name: aggregate option name\n        *args: list of configs to be aggregated\n    """"""\n\n    config_class = _get_config_class(name)\n    if config_class is None:\n        raise ConfigsError(\'You need to pass config item to aggregate\')\n\n    config_class.aggregate(name, option_name, *args)\n'"
labml/experiment.py,2,"b'from pathlib import Path\nfrom typing import Optional, Set, Dict, List, Union, TYPE_CHECKING, overload\n\nimport numpy as np\n\nfrom labml.configs import BaseConfigs\nfrom labml.internal.experiment import \\\n    create_experiment as _create_experiment, \\\n    experiment_singleton as _experiment_singleton\n\nif TYPE_CHECKING:\n    import torch\n\n\ndef save_checkpoint():\n    r""""""\n    Saves model checkpoints\n    """"""\n    _experiment_singleton().save_checkpoint()\n\n\ndef get_uuid():\n    r""""""\n    Returns the UUID of the current experiment run\n    """"""\n\n    return _experiment_singleton().run.uuid\n\n\ndef create(*,\n           name: Optional[str] = None,\n           python_file: Optional[str] = None,\n           comment: Optional[str] = None,\n           writers: Set[str] = None,\n           ignore_callers: Set[str] = None,\n           tags: Optional[Set[str]] = None):\n    r""""""\n    Create an experiment\n\n    Keyword Arguments:\n        name (str, optional): name of the experiment\n        python_file (str, optional): path of the Python file that\n            created the experiment\n        comment (str, optional): a short description of the experiment\n        writers (Set[str], optional): list of writers to write stat to.\n            Defaults to ``{\'tensorboard\', \'sqlite\'}``.\n        ignore_callers: (Set[str], optional): list of files to ignore when\n            automatically determining ``python_file``\n        tags (Set[str], optional): Set of tags for experiment\n    """"""\n\n    if writers is None:\n        writers = {\'sqlite\', \'tensorboard\'}\n\n    if ignore_callers is None:\n        ignore_callers = {}\n\n    _create_experiment(name=name,\n                       python_file=python_file,\n                       comment=comment,\n                       writers=writers,\n                       ignore_callers=ignore_callers,\n                       tags=tags)\n\n\ndef add_pytorch_models(models: Dict[str, \'torch.nn.Module\']):\n    """"""\n    Set variables for saving and loading\n\n    Arguments:\n        models (Dict[str, torch.nn.Module]): a dictionary of torch modules\n            used in the experiment.\n            These will be saved with :func:`labml.experiment.save_checkpoint`\n            and loaded with :func:`labml.experiment.load`.\n    """"""\n\n    from labml.internal.experiment.pytorch import add_models as _add_pytorch_models\n    _add_pytorch_models(models)\n\n\ndef add_sklearn_models(models: Dict[str, any]):\n    """"""\n    .. warning::\n        This is still experimental.\n\n    Set variables for saving and loading\n\n    Arguments:\n        models (Dict[str, any]): a dictionary of SKLearn models\n            These will be saved with :func:`labml.experiment.save_checkpoint`\n            and loaded with :func:`labml.experiment.load`.\n    """"""\n    from labml.internal.experiment.sklearn import add_models as _add_sklearn_models\n    _add_sklearn_models(models)\n\n\n@overload\ndef calculate_configs(configs_dict: Dict[str, any]):\n    ...\n\n\n@overload\ndef calculate_configs(configs_dict: Dict[str, any], configs_override: Dict[str, any]):\n    ...\n\n\n@overload\ndef calculate_configs(configs: BaseConfigs):\n    ...\n\n\n@overload\ndef calculate_configs(configs: BaseConfigs, run_order: List[Union[List[str], str]]):\n    ...\n\n\n@overload\ndef calculate_configs(configs: BaseConfigs, *run_order: str):\n    ...\n\n\n@overload\ndef calculate_configs(configs: BaseConfigs, configs_override: Dict[str, any]):\n    ...\n\n\n@overload\ndef calculate_configs(configs: BaseConfigs, configs_override: Dict[str, any],\n                      run_order: List[Union[List[str], str]]):\n    ...\n\n\n@overload\ndef calculate_configs(configs: BaseConfigs, configs_override: Dict[str, any],\n                      *run_order: str):\n    ...\n\n\ndef calculate_configs(*args):\n    r""""""\n    Calculate configurations\n\n    This has multiple overloads\n\n    .. function:: calculate_configs(configs_dict: Dict[str, any])\n        :noindex:\n\n    .. function:: calculate_configs(configs_dict: Dict[str, any], configs_override: Dict[str, any])\n        :noindex:\n\n    .. function:: calculate_configs(configs: BaseConfigs)\n        :noindex:\n\n    .. function:: calculate_configs(configs: BaseConfigs, run_order: List[Union[List[str], str]])\n        :noindex:\n\n    .. function:: calculate_configs(configs: BaseConfigs, *run_order: str)\n        :noindex:\n\n    .. function:: calculate_configs(configs: BaseConfigs, configs_override: Dict[str, any])\n        :noindex:\n\n    .. function:: calculate_configs(configs: BaseConfigs, configs_override: Dict[str, any], run_order: List[Union[List[str], str]])\n        :noindex:\n\n    .. function:: calculate_configs(configs: BaseConfigs, configs_override: Dict[str, any], *run_order: str)\n        :noindex:\n\n    Arguments:\n        configs (BaseConfigs, optional): configurations object\n        configs_dict (Dict[str, any], optional): a dictionary of configs\n        configs_override (Dict[str, any], optional): a dictionary of\n            configs to be overridden\n        run_order (List[Union[str, List[str]]], optional): list of\n            configs to be calculated and the order in which they should be\n            calculated. If not provided all configs will be calculated.\n    """"""\n    configs_override: Optional[Dict[str, any]] = None\n    run_order: Optional[List[Union[List[str], str]]] = None\n    idx = 1\n\n    if isinstance(args[0], BaseConfigs):\n        if idx < len(args) and isinstance(args[idx], dict):\n            configs_override = args[idx]\n            idx += 1\n\n        if idx < len(args) and isinstance(args[idx], list):\n            run_order = args[idx]\n            if len(args) != idx + 1:\n                raise RuntimeError(""Invalid call to calculate configs"")\n            _experiment_singleton().calc_configs(args[0], configs_override, run_order)\n        else:\n            if idx == len(args):\n                _experiment_singleton().calc_configs(args[0], configs_override, run_order)\n            else:\n                run_order = list(args[idx:])\n                for key in run_order:\n                    if not isinstance(key, str):\n                        raise RuntimeError(""Invalid call to calculate configs"")\n                _experiment_singleton().calc_configs(args[0], configs_override, run_order)\n    elif isinstance(args[0], dict):\n        if idx < len(args) and isinstance(args[idx], dict):\n            configs_override = args[idx]\n            idx += 1\n\n        if idx != len(args):\n            raise RuntimeError(""Invalid call to calculate configs"")\n\n        _experiment_singleton().calc_configs_dict(args[0], configs_override)\n    else:\n        raise RuntimeError(""Invalid call to calculate configs"")\n\n\ndef start():\n    r""""""\n    Starts the experiment.\n    """"""\n    _experiment_singleton().start()\n\n\ndef load(run_uuid: str,\n         checkpoint: Optional[int] = None):\n    r""""""\n    Loads and starts the experiment from a previous checkpoint.\n\n    Keyword Arguments:\n        run_uuid (str): if provided the experiment will start from\n            a saved state in the run with UUID ``run_uuid``\n        checkpoint (str, optional): if provided the experiment will start from\n            given checkpoint. Otherwise it will start from the last checkpoint.\n    """"""\n    _experiment_singleton().start(run_uuid=run_uuid, checkpoint=checkpoint)\n\n\ndef save_numpy(name: str, array: np.ndarray):\n    r""""""\n    Saves a single numpy array. This is used to save processed data.\n    """"""\n    numpy_path = Path(_experiment_singleton().run.numpy_path)\n\n    if not numpy_path.exists():\n        numpy_path.mkdir(parents=True)\n    file_name = name + "".npy""\n    np.save(str(numpy_path / file_name), array)\n'"
labml/lab.py,0,b'from pathlib import PurePath\n\nfrom labml.internal.lab import lab_singleton as _internal\n\n\ndef get_data_path() -> PurePath:\n    return _internal().data_path\n\n\ndef get_experiments_path() -> PurePath:\n    return _internal().experiments\n'
labml/logger.py,0,"b'from typing import Union, List, Tuple, overload, Dict\n\nfrom labml.internal.util.colors import StyleCode\n\n\nclass Style(StyleCode):\n    r""""""\n    Output styles\n    """"""\n\n    none = None\n    normal = \'normal\'\n    bold = \'bold\'\n    underline = \'underline\'\n    light = \'light\'\n\n\nclass Color(StyleCode):\n    r""""""\n    Output colors\n    """"""\n\n    none = None\n    black = \'black\'\n    red = \'red\'\n    green = \'green\'\n    orange = \'orange\'\n    blue = \'blue\'\n    purple = \'purple\'\n    cyan = \'cyan\'\n    white = \'white\'\n\n\nclass Text(StyleCode):\n    r""""""\n    Standard styles we use in labml\n    """"""\n\n    none = None\n    danger = Color.red.value\n    success = Color.green.value\n    warning = Color.orange.value\n    meta = Color.blue.value\n    key = Color.cyan.value\n    meta2 = Color.purple.value\n    title = [Style.bold.value, Style.underline.value]\n    heading = Style.underline.value\n    value = Style.bold.value\n    highlight = [Style.bold.value, Color.orange.value]\n    subtle = [Style.light.value, Color.white.value]\n\n\n@overload\ndef log():\n    ...\n\n\n@overload\ndef log(message: str, *, is_new_line: bool = True):\n    ...\n\n\n@overload\ndef log(message: str, color: StyleCode,\n        *,\n        is_new_line: bool = True):\n    ...\n\n\n@overload\ndef log(message: str, colors: List[StyleCode],\n        *,\n        is_new_line: bool = True):\n    ...\n\n\n@overload\ndef log(messages: List[Union[str, Tuple[str, StyleCode]]],\n        *,\n        is_new_line: bool = True):\n    ...\n\n\n@overload\ndef log(*args: Union[str, Tuple[str, StyleCode]],\n        is_new_line: bool = True):\n    ...\n\n\ndef log(*args, is_new_line: bool = True):\n    r""""""\n    This has multiple overloads\n\n    .. function:: log(message: str, *, is_new_line=True)\n        :noindex:\n\n    .. function:: log(message: str, color: StyleCode, *, is_new_line=True)\n        :noindex:\n\n    .. function:: log(message: str, colors: List[StyleCode], *, is_new_line=True)\n        :noindex:\n\n    .. function:: log(messages: List[Union[str, Tuple[str, StyleCode]]], *, is_new_line=True)\n        :noindex:\n\n    .. function:: log(*args: Union[str, Tuple[str, StyleCode]], is_new_line: bool = True)\n        :noindex:\n\n    Arguments:\n        message (str): string to be printed\n        color (StyleCode): color/style of the message\n        colors (List[StyleCode]): list of colors/styles for the message\n        args (Union[str, Tuple[str, StyleCode]]): list of messages\n            Each element should be either a string or a tuple of string and styles.\n        messages (List[Union[str, Tuple[str, StyleCode]]]): a list of messages.\n            Each element should be either a string or a tuple of string and styles.\n\n    Keyword Arguments:\n        is_new_line (bool): whether to print a new line at the end\n\n    Example::\n        >>> logger.log(""test"")\n    """"""\n    from labml.internal.logger import logger_singleton as _internal\n\n    if len(args) == 0:\n        assert is_new_line == True\n        _internal().log([], is_new_line=True)\n    elif len(args) == 1:\n        message = args[0]\n        if isinstance(message, str):\n            _internal().log([(message, None)], is_new_line=is_new_line)\n        elif type(message) == list:\n            _internal().log(message, is_new_line=is_new_line)\n        else:\n            assert False\n    elif len(args) == 2 and isinstance(args[0], str) and isinstance(args[1], StyleCode):\n        _internal().log([(args[0], args[1])], is_new_line=is_new_line)\n    else:\n        _internal().log(args, is_new_line=is_new_line)\n\n\n@overload\ndef inspect(items: Dict):\n    ...\n\n\n@overload\ndef inspect(items: List):\n    ...\n\n\n@overload\ndef inspect(*items: any):\n    ...\n\n\n@overload\ndef inspect(**items: any):\n    ...\n\n\ndef inspect(*args, **kwargs):\n    """"""\n    Pretty prints a set of values.\n\n    This has multiple overloads\n\n    .. function:: inspect(items: Dict)\n        :noindex:\n\n    .. function:: inspect(items: List)\n        :noindex:\n\n    .. function:: inspect(*items: any)\n        :noindex:\n    """"""\n\n    from labml.internal.logger import logger_singleton as _internal\n\n    _internal().info(*args, **kwargs)\n'"
labml/monit.py,0,"b'from typing import Iterable, Sized\nfrom typing import Union, Optional, overload\n\nfrom labml.internal.logger import logger_singleton as _internal\n\n\ndef iterate(name, iterable: Union[Iterable, Sized, int],\n            total_steps: Optional[int] = None, *,\n            is_silent: bool = False,\n            is_timed: bool = True):\n    return _internal().iterate(name, iterable, total_steps,\n                               is_silent=is_silent,\n                               is_timed=is_timed)\n\n\ndef enum(name, iterable: Sized, *,\n         is_silent: bool = False,\n         is_timed: bool = True):\n    return _internal().enum(name, iterable, is_silent=is_silent, is_timed=is_timed)\n\n\ndef section(name, *,\n            is_silent: bool = False,\n            is_timed: bool = True,\n            is_partial: bool = False,\n            is_new_line: bool = True,\n            total_steps: float = 1.0):\n    return _internal().section(name, is_silent=is_silent,\n                               is_timed=is_timed,\n                               is_partial=is_partial,\n                               total_steps=total_steps,\n                               is_new_line=is_new_line)\n\n\ndef progress(steps: float):\n    _internal().progress(steps)\n\n\ndef fail():\n    _internal().set_successful(False)\n\n\n@overload\ndef loop(iterator_: int, *,\n         is_print_iteration_time: bool = True):\n    ...\n\n\n@overload\ndef loop(iterator_: range, *,\n         is_print_iteration_time: bool = True):\n    ...\n\n\ndef loop(iterator_: Union[range, int], *,\n         is_print_iteration_time: bool = True):\n    """"""\n        This has multiple overloads\n\n        .. function:: loop(iterator_: range, *,is_print_iteration_time=True)\n            :noindex:\n\n        .. function:: loop(iterator_: int, *,is_print_iteration_time=True)\n            :noindex:\n        """"""\n\n    if type(iterator_) == int:\n        return _internal().loop(range(iterator_), is_print_iteration_time=is_print_iteration_time)\n    else:\n        return _internal().loop(iterator_, is_print_iteration_time=is_print_iteration_time)\n\n\ndef finish_loop():\n    _internal().finish_loop()\n'"
labml/tracker.py,0,"b'from typing import Dict, overload, Optional\n\nfrom labml.internal.logger import logger_singleton as _internal\n\n\ndef set_global_step(global_step: Optional[int]):\n    _internal().set_global_step(global_step)\n\n\ndef add_global_step(increment_global_step: int = 1):\n    _internal().add_global_step(int(increment_global_step))\n\n\ndef get_global_step() -> int:\n    return _internal().global_step\n\n\ndef set_queue(name: str, queue_size: int = 10, is_print: bool = False):\n    from labml.internal.logger.store.indicators import Queue\n    _internal().add_indicator(Queue(name, queue_size, is_print))\n\n\ndef set_histogram(name: str, is_print: bool = False):\n    from labml.internal.logger.store.indicators import Histogram\n    _internal().add_indicator(Histogram(name, is_print))\n\n\ndef set_scalar(name: str, is_print: bool = False):\n    from labml.internal.logger.store.indicators import Scalar\n    _internal().add_indicator(Scalar(name, is_print))\n\n\ndef set_indexed_scalar(name: str):\n    from labml.internal.logger.store.indicators import IndexedScalar\n    _internal().add_indicator(IndexedScalar(name))\n\n\ndef set_image(name: str, is_print: bool = False):\n    from labml.internal.logger.store.artifacts import Image\n    _internal().add_artifact(Image(name, is_print))\n\n\ndef set_text(name: str, is_print: bool = False):\n    from labml.internal.logger.store.artifacts import Text\n    _internal().add_artifact(Text(name, is_print))\n\n\ndef set_tensor(name: str, is_once: bool = False):\n    from labml.internal.logger.store.artifacts import Tensor\n    _internal().add_artifact(Tensor(name, is_once=is_once))\n\n\ndef set_indexed_text(name: str, title: Optional[str] = None, is_print: bool = False):\n    from labml.internal.logger.store.artifacts import IndexedText\n    _internal().add_artifact(IndexedText(name, title, is_print))\n\n\ndef _add_dict(values: Dict[str, any]):\n    for k, v in values.items():\n        _internal().store(k, v)\n\n\n@overload\ndef add(values: Dict[str, any]):\n    ...\n\n\n@overload\ndef add(name: str, value: any):\n    ...\n\n\n@overload\ndef add(**kwargs: any):\n    ...\n\n\ndef add(*args, **kwargs):\n    """"""\n    This has multiple overloads\n\n    .. function:: add(values: Dict[str, any])\n        :noindex:\n\n    .. function:: add(name: str, value: any)\n        :noindex:\n\n    .. function:: add(**kwargs: any)\n        :noindex:\n    """"""\n    assert len(args) <= 2\n\n    if len(args) == 0:\n        _add_dict(kwargs)\n    elif len(args) == 1:\n        assert not kwargs\n        assert isinstance(args[0], dict)\n        _add_dict(args[0])\n    elif len(args) == 2:\n        assert not kwargs\n        assert isinstance(args[0], str)\n        _internal().store(args[0], args[1])\n\n\n@overload\ndef save():\n    ...\n\n\n@overload\ndef save(global_step: int):\n    ...\n\n\n@overload\ndef save(values: Dict[str, any]):\n    ...\n\n\n@overload\ndef save(name: str, value: any):\n    ...\n\n\n@overload\ndef save(**kwargs: any):\n    ...\n\n\n@overload\ndef save(global_step: int, values: Dict[str, any]):\n    ...\n\n\n@overload\ndef save(global_step: int, name: str, value: any):\n    ...\n\n\n@overload\ndef save(global_step: int, **kwargs: any):\n    ...\n\n\ndef save(*args, **kwargs):\n    r""""""\n    This has multiple overloads\n\n    .. function:: save()\n        :noindex:\n\n    .. function:: save(global_step: int)\n        :noindex:\n\n    .. function:: save(values: Dict[str, any])\n        :noindex:\n\n    .. function:: save(name: str, value: any)\n        :noindex:\n\n    .. function:: save(**kwargs: any)\n        :noindex:\n\n    .. function:: save(global_step: int, values: Dict[str, any])\n        :noindex:\n\n    .. function:: save(global_step: int, name: str, value: any)\n        :noindex:\n\n    .. function:: save(global_step: int, **kwargs: any)\n        :noindex:\n    """"""\n    if len(args) > 0 and type(args[0]) == int:\n        _internal().set_global_step(args[0])\n        args = args[1:]\n\n    if len(args) > 0 or len(kwargs.keys()) > 0:\n        add(*args, **kwargs)\n\n    _internal().write()\n\n\ndef namespace(name: str):\n    r""""""\n    Set a namespace for tracking\n    """"""\n    return _internal().store_namespace(name)\n\n\ndef reset():\n    r""""""\n    Reset indicators, for a new experiment\n    """"""\n    _internal().reset_store()\n'"
test/__init__.py,0,"b'r""""""\nThis is for experimental features\n""""""'"
test/configs.py,0,"b""from labml.internal.configs.base import Configs\nfrom labml.internal.configs.processor import ConfigProcessor\n\n\nclass SampleModel:\n    def __init__(self, c: 'Sample'):\n        self.w = c.workers_count\n\n\nclass Sample(Configs):\n    total_global_steps: int = 10\n    workers_count: int = 10\n    # empty: str\n\n    x = 'string'\n\n    input_model: int\n    model: int\n\n    # get from type annotations\n    model_obj: SampleModel\n\n\nclass SampleChild(Sample):\n    def __init__(self, *, test: int):\n        pass\n\n    new_attr = 2\n\n\n@Sample.calc()\ndef input_model(c: Sample):\n    return c.workers_count * 2\n\n\n@Sample.calc(Sample.input_model)\ndef input_model2(c: Sample):\n    return c.workers_count * 20\n\n\n@Sample.calc('model')\ndef simple_model(c: Sample):\n    return c.total_global_steps * 3\n\n\nconfigs = Sample()\n\nprocessor = ConfigProcessor(configs)\nprocessor()\nprocessor.print()\n\nprint(configs.__dict__)\n"""
test/configs_dict.py,0,"b""from labml.internal.configs.processor_dict import ConfigProcessorDict\n\nprocessor = ConfigProcessorDict({'cnn_size': 10, 'batch_size': 12}, {'batch_size': 2})\nprocessor()\nprocessor.print()\n"""
test/configs_new_api.py,0,"b'from labml.configs import BaseConfigs, option, calculate\nfrom labml.internal.configs.processor import ConfigProcessor\n\n\nclass Sample(BaseConfigs):\n    total_global_steps: int = 10\n    workers_count: int = 12\n    input_model: int\n    model: int\n\n\n@option(Sample.input_model)\ndef input_model(c: Sample):\n    return c.total_global_steps * 2\n\n\ncalculate(Sample.model, [Sample.workers_count], lambda x: x * 5)\n\nconfigs = Sample()\n\nprocessor = ConfigProcessor(configs)\nprocessor()\nprocessor.print()\n\nprint(configs.__dict__)\n'"
test/tracker_perf.py,0,"b'import time\n\nfrom labml import experiment, monit, tracker, logger\nfrom labml.logger import Text\n\n\ndef setup_and_add():\n    for t in range(10):\n        tracker.set_scalar(f""loss1.{t}"", is_print=t == 0)\n\n    experiment.start()\n\n    for i in monit.loop(1000):\n        for t in range(10):\n            tracker.add({f\'loss1.{t}\': i})\n            tracker.save()\n\n\ndef add():\n    experiment.start()\n\n    for i in monit.loop(1000):\n        for t in range(10):\n            if i == 0:\n                tracker.set_scalar(f""loss1.{t}"", is_print=t == 0)\n        for t in range(10):\n            tracker.add({f\'loss1.{t}\': i})\n            tracker.save()\n\n\ndef main():\n    experiment.create(writers={\'sqlite\'})\n\n    start = time.time()\n    setup_and_add()\n    logger.log(\'Time taken: \', (f\'{time.time() - start}\', Text.value))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
labml/analytics/__init__.py,0,"b'from typing import Tuple, Optional, List, overload, Union\n\nimport numpy as np\nfrom labml.internal.analytics import cache as _cache\nfrom labml.internal.analytics.altair import density as _density\nfrom labml.internal.analytics.altair import scatter as _scatter\nfrom labml.internal.analytics.altair import binned_heatmap as _binned_heatmap\nfrom labml.internal.analytics.indicators import IndicatorCollection as _IndicatorCollection\n\n\ndef _remove_names_prefix(names: List[Union[str, List[str]]]) -> List[str]:\n    if len(names) == 0:\n        return []\n\n    if isinstance(names[0], list):\n        common = names[0]\n    else:\n        common = None\n\n    for n in names:\n        if common is None:\n            break\n        if not isinstance(n, list):\n            common = None\n        merge = []\n        for x, y in zip(common, n):\n            if x != y:\n                merge.append(None)\n            else:\n                merge.append(x)\n        common = merge\n\n    res = []\n    for n in names:\n        if isinstance(n, list):\n            if common is not None:\n                n = [p for i, p in enumerate(n) if i > len(common) or p != common[i]]\n            n = \'-\'.join(n)\n\n        res.append(n)\n\n    return res\n\n\nclass IndicatorCollection(_IndicatorCollection):\n    r""""""\n    You can get a indicator collection with :func:`runs`.\n\n    >>> from labml import analytics\n    >>> indicators = analytics.runs(\'1d3f855874d811eabb9359457a24edc8\')\n\n    You can reference individual indicators as attributes.\n\n    >>> train_loss = indicators.train_loss\n\n    You can add multiple indicator collections\n\n    >>> losses = indicators.train_loss + indicators.validation_loss\n    """"""\n    pass\n\n\ndef runs(*uuids: str):\n    r""""""\n    This is used to analyze runs.\n    It fetches all the log indicators.\n\n    Arguments:\n        uuids (str): UUIDs of the runs. You can\n            get this from `dashboard <https://github.com/lab-ml/lab_dashboard>`_\n\n    Example:\n        >>> from labml import analytics\n        >>> indicators = analytics.runs(\'1d3f855874d811eabb9359457a24edc8\')\n    """"""\n\n    indicators = None\n    for r in uuids:\n        run = _cache.get_run(r)\n        indicators = indicators + run.indicators\n\n    return indicators\n\n\ndef get_run(uuid: str):\n    r""""""\n    Returns ``Run`` object\n    """"""\n    return _cache.get_run(uuid)\n\n\ndef set_preferred_db(db: str):\n    assert db in [\'tensorboard\', \'sqlite\']\n\n\n@overload\ndef distribution(indicators: IndicatorCollection, *,\n                 names: Optional[List[str]] = None,\n                 levels: int = 5, alpha: int = 0.6,\n                 height: int = 400, width: int = 800, height_minimap: int = 100):\n    ...\n\n\n@overload\ndef distribution(series: List[np.ndarray], *,\n                 names: Optional[List[str]] = None,\n                 levels: int = 5, alpha: int = 0.6,\n                 height: int = 400, width: int = 800, height_minimap: int = 100):\n    ...\n\n\n@overload\ndef distribution(series: List[np.ndarray],\n                 step: np.ndarray, *,\n                 names: Optional[List[str]] = None,\n                 levels: int = 5, alpha: int = 0.6,\n                 height: int = 400, width: int = 800, height_minimap: int = 100):\n    ...\n\n\ndef distribution(*args: any,\n                 names: Optional[List[str]] = None,\n                 levels: int = 5, alpha: int = 0.6,\n                 height: int = 400, width: int = 800, height_minimap: int = 100):\n    r""""""\n    Creates a distribution plot distribution with Altair\n\n    This has multiple overloads\n\n    .. function:: distribution(indicators: IndicatorCollection, *, names: Optional[List[str]] = None, levels: int = 5, alpha: int = 0.6, height: int = 400, width: int = 800, height_minimap: int = 100)\n        :noindex:\n\n    .. function:: distribution(series: List[np.ndarray], *, names: Optional[List[str]] = None, levels: int = 5, alpha: int = 0.6, height: int = 400, width: int = 800, height_minimap: int = 100)\n        :noindex:\n\n    .. function:: distribution(series: List[np.ndarray], step: np.ndarray, *, names: Optional[List[str]] = None, levels: int = 5, alpha: int = 0.6, height: int = 400, width: int = 800, height_minimap: int = 100)\n        :noindex:\n\n    Arguments:\n        indicators(IndicatorCollection): Set of indicators to be plotted\n        series(List[np.ndarray]): List of series of data\n        step(np.ndarray): Steps\n\n    Keyword Arguments:\n        names(List[str]): List of names of series\n        levels: how many levels of the distribution to be plotted\n        alpha: opacity of the distribution\n        height: height of the visualization\n        width: width of the visualization\n        height_minimap: height of the view finder\n\n    Return:\n        The Altair visualization\n\n    Example:\n        >>> from labml import analytics\n        >>> indicators = analytics.runs(\'1d3f855874d811eabb9359457a24edc8\')\n        >>> analytics.distribution(indicators)\n    """"""\n\n    series = None\n    step = None\n\n    if len(args) == 1:\n        if isinstance(args[0], _IndicatorCollection):\n            series, names_ = _cache.get_indicators_data(args[0])\n            if not series:\n                raise ValueError(""No series found"")\n            if names is None:\n                names = names_\n        elif isinstance(args[0], list):\n            series = args[0]\n    elif len(args) == 2:\n        series = args[0]\n        step = args[1]\n\n    if names is None:\n        names = [f\'{i + 1}\' for i in range(len(series))]\n\n    if series is None:\n        raise ValueError(""distribution should be called with an indicator collection""\n                         "" or a series. Check documentation for details."")\n\n    tables = [_density.data_to_table(s, step) for s in series]\n    names = _remove_names_prefix(names)\n\n    return _density.render(\n        tables,\n        names=names,\n        levels=levels,\n        alpha=alpha,\n        width=width,\n        height=height,\n        height_minimap=height_minimap)\n\n\n@overload\ndef scatter(indicators: IndicatorCollection, x_indicators: IndicatorCollection, *,\n            names: Optional[List[str]] = None, x_name: Optional[str] = None,\n            noise: Optional[Tuple[float, float]] = None,\n            circle_size: int = 20,\n            height: int = 400, width: int = 800, height_minimap: int = 100):\n    ...\n\n\n@overload\ndef scatter(series: List[np.ndarray],\n            x_series: np.ndarray, *,\n            names: Optional[List[str]] = None, x_name: Optional[str] = None,\n            noise: Optional[Tuple[float, float]] = None,\n            circle_size: int = 20,\n            height: int = 400, width: int = 800, height_minimap: int = 100):\n    ...\n\n\ndef scatter(*args: any,\n            names: Optional[List[str]] = None, x_name: Optional[str] = None,\n            noise: Optional[Tuple[float, float]] = None,\n            circle_size: int = 20,\n            height: int = 400, width: int = 800, height_minimap: int = 100):\n    r""""""\n    Creates a scatter plot with Altair\n\n    This has multiple overloads\n\n    .. function:: scatter(indicators: IndicatorCollection, x_indicators: IndicatorCollection, *, names: Optional[List[str]] = None, x_name: Optional[str] = None, noise: Optional[Tuple[float, float]] = None, circle_size: int = 20, height: int = 400, width: int = 800, height_minimap: int = 100)\n        :noindex:\n\n    .. function:: scatter(series: List[np.ndarray], x_series: np.ndarray, *, names: Optional[List[str]] = None, x_name: Optional[str] = None, noise: Optional[Tuple[float, float]] = None, circle_size: int = 20, height: int = 400, width: int = 800, height_minimap: int = 100)\n        :noindex:\n\n    Arguments:\n        indicators(IndicatorCollection): Set of indicators to be plotted\n        x_indicators(IndicatorCollection): Indicator for x-axis\n        series(List[np.ndarray]): List of series of data\n        x_series(np.ndarray): X series of data\n\n    Keyword Arguments:\n        names(List[str]): List of names of series\n        name(str): Name of X series\n        noise: Noise to be added to spread out the scatter plot\n        circle_size: size of circles in the plot\n        height: height of the visualization\n        width: width of the visualization\n        height_minimap: height of the view finder\n\n    Return:\n        The Altair visualization\n\n    :Example:\n        >>> from labml import analytics\n        >>> indicators = analytics.runs(\'1d3f855874d811eabb9359457a24edc8\')\n        >>> analytics.scatter(indicators.validation_loss, indicators.train_loss)\n    """"""\n\n    series = None\n    x_series = None\n\n    if len(args) == 2:\n        if isinstance(args[0], _IndicatorCollection) and isinstance(args[1], _IndicatorCollection):\n            series, names_ = _cache.get_indicators_data(args[0])\n            x_series, x_name_ = _cache.get_indicators_data(args[1])\n\n            if len(x_series) != 1:\n                raise ValueError(""There should be exactly one series for x-axis"")\n            if not series:\n                raise ValueError(""No series found"")\n            x_series = x_series[0]\n            if x_name is None:\n                x_name = x_name_[0]\n            if names is None:\n                names = names_\n        elif isinstance(args[0], list):\n            series = args[0]\n            x_series = args[1]\n\n    if series is None:\n        raise ValueError(""scatter should be called with an indicator collection""\n                         "" or a series. Check documentation for details."")\n\n    if x_name is None:\n        x_name = \'x\'\n    if names is None:\n        names = [f\'{i + 1}\' for i in range(len(series))]\n\n    tables = [_scatter.data_to_table(s, x_series, noise) for s in series]\n    names = _remove_names_prefix(names)\n\n    return _scatter.render(\n        tables,\n        names=names,\n        x_name=x_name,\n        width=width,\n        height=height,\n        height_minimap=height_minimap,\n        circle_size=circle_size)\n\n\n@overload\ndef binned_heatmap(indicators: IndicatorCollection, x_indicators: IndicatorCollection, *,\n                   names: Optional[List[str]] = None, x_name: Optional[str] = None,\n                   height: int = 400, width: int = 800, height_minimap: int = 100):\n    ...\n\n\n@overload\ndef binned_heatmap(series: List[np.ndarray],\n                   x_series: np.ndarray, *,\n                   names: Optional[List[str]] = None, x_name: Optional[str] = None,\n                   height: int = 400, width: int = 800, height_minimap: int = 100):\n    ...\n\n\ndef binned_heatmap(*args: any,\n                   names: Optional[List[str]] = None, x_name: Optional[str] = None,\n                   height: int = 400, width: int = 800, height_minimap: int = 100):\n    r""""""\n    Creates a scatter plot with Altair\n\n    This has multiple overloads\n\n    .. function:: binned_heatmap(indicators: IndicatorCollection, x_indicators: IndicatorCollection, *, names: Optional[List[str]] = None, x_name: Optional[str] = None, height: int = 400, width: int = 800, height_minimap: int = 100)\n        :noindex:\n\n    .. function:: binned_heatmap(series: List[np.ndarray], x_series: np.ndarray, *, names: Optional[List[str]] = None, x_name: Optional[str] = None, height: int = 400, width: int = 800, height_minimap: int = 100)\n        :noindex:\n\n    Arguments:\n        indicators(IndicatorCollection): Set of indicators to be plotted\n        x_indicators(IndicatorCollection): Indicator for x-axis\n        series(List[np.ndarray]): List of series of data\n        x_series(np.ndarray): X series of data\n\n    Keyword Arguments:\n        names(List[str]): List of names of series\n        name(str): Name of X series\n        noise: Noise to be added to spread out the scatter plot\n        circle_size: size of circles in the plot\n        height: height of the visualization\n        width: width of the visualization\n        height_minimap: height of the view finder\n\n    Return:\n        The Altair visualization\n\n    Example:\n        >>> from labml import analytics\n        >>> indicators = analytics.runs(\'1d3f855874d811eabb9359457a24edc8\')\n        >>> analytics.scatter(indicators.validation_loss, indicators.train_loss)\n    """"""\n\n    series = None\n    x_series = None\n\n    if len(args) == 2:\n        if isinstance(args[0], _IndicatorCollection) and isinstance(args[1], _IndicatorCollection):\n            series, names_ = _cache.get_indicators_data(args[0])\n            x_series, x_name_ = _cache.get_indicators_data(args[1])\n\n            if len(x_series) != 1:\n                raise ValueError(""There should be exactly one series for x-axis"")\n            if not series:\n                raise ValueError(""No series found"")\n            x_series = x_series[0]\n            if x_name is None:\n                x_name = x_name_[0]\n            if names is None:\n                names = names_\n        elif isinstance(args[0], list):\n            series = args[0]\n            x_series = args[1]\n\n    if series is None:\n        raise ValueError(""scatter should be called with an indicator collection""\n                         "" or a series. Check documentation for details."")\n\n    if x_name is None:\n        x_name = \'x\'\n    if names is None:\n        names = [f\'{i + 1}\' for i in range(len(series))]\n\n    tables = [_binned_heatmap.data_to_table(s, x_series) for s in series]\n    names = _remove_names_prefix(names)\n\n    return _binned_heatmap.render(\n        tables,\n        names=names,\n        x_name=x_name,\n        width=width,\n        height=height,\n        height_minimap=height_minimap)\n\n\ndef indicator_data(indicators: IndicatorCollection) -> Tuple[List[np.ndarray], List[str]]:\n    r""""""\n    Returns a tuple of a list of series and a list of names of series.\n    Each series, `S` is a timeseries of histograms of shape `[T, 10]`,\n    where `T` is the number of timesteps.\n    `S[:, 0]` is the `global_step`.\n    `S[:, 1:10]` represents the distribution at basis points\n    `0, 6.68, 15.87, 30.85, 50.00, 69.15, 84.13, 93.32, 100.00`.\n\n    Example:\n        >>> from labml import analytics\n        >>> indicators = analytics.runs(\'1d3f855874d811eabb9359457a24edc8\')\n        >>> analytics.indicator_data(indicators)\n    """"""\n\n    series, names = _cache.get_indicators_data(indicators)\n\n    if not series:\n        raise ValueError(""No series found"")\n\n    return series, names\n\n\ndef artifact_data(indicators: IndicatorCollection) -> Tuple[List[any], List[str]]:\n    r""""""\n    Returns a tuple of a list of series and a list of names of series.\n    Each series, ``S`` is a timeseries of histograms of shape ``[T, 10]``,\n    where ``T`` is the number of timesteps.\n    ``S[:, 0]`` is the `global_step`.\n    ``S[:, 1:10]`` represents the distribution at basis points:\n    ``0, 6.68, 15.87, 30.85, 50.00, 69.15, 84.13, 93.32, 100.00``.\n\n    Example:\n        >>> from labml import analytics\n        >>> indicators = analytics.runs(\'1d3f855874d811eabb9359457a24edc8\')\n        >>> analytics.artifact_data(indicators)\n    """"""\n\n    series, names = _cache.get_artifacts_data(indicators)\n\n    if not series:\n        raise ValueError(""No series found"")\n\n    return series, names\n'"
labml/analytics/matplotlib.py,0,"b'import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom labml.internal.analytics import cache as _cache\nfrom labml.internal.analytics.indicators import IndicatorCollection\nfrom labml.internal.analytics.matplotlib import MatPlotLibAnalytics as _MatPlotLibAnalytics\n\nMATPLOTLIB = _MatPlotLibAnalytics()\n\n\ndef render_matplotlib(indicators: IndicatorCollection):\n    r""""""\n    Creates a distribution plot distribution with MatPlotLib\n    """"""\n    _, ax = plt.subplots(figsize=(18, 9))\n    for i, ind in enumerate(indicators):\n        data = _cache.get_indicator_data(ind)\n        MATPLOTLIB.render_density(ax, data, sns.color_palette()[i], ind.key)\n    plt.show()\n'"
labml/helpers/__init__.py,0,b''
labml/helpers/training_loop.py,0,"b'import signal\nfrom typing import Optional, Tuple, Any\n\nfrom labml import tracker, logger, experiment, monit\nfrom labml.configs import BaseConfigs\nfrom labml.internal.logger import Loop\nfrom labml.logger import Text\n\n\nclass TrainingLoop:\n    __loop: Loop\n    __signal_received: Optional[Tuple[Any, Any]]\n\n    def __init__(self, *,\n                 loop_count: int,\n                 loop_step: int,\n                 is_save_models: bool,\n                 log_new_line_interval: int,\n                 log_write_interval: int,\n                 save_models_interval: int,\n                 is_loop_on_interrupt: bool):\n        self.__loop_count = loop_count\n        self.__loop_step = loop_step\n        self.__is_save_models = is_save_models\n        self.__log_new_line_interval = log_new_line_interval\n        self.__log_write_interval = log_write_interval\n        self.__save_models_interval = save_models_interval\n        self.__signal_received = None\n        self.__is_loop_on_interrupt = is_loop_on_interrupt\n\n    def __iter__(self):\n        self.__loop = monit.loop(range(tracker.get_global_step(),\n                                       self.__loop_count,\n                                       self.__loop_step))\n        iter(self.__loop)\n        try:\n            self.old_handler = signal.signal(signal.SIGINT, self.__handler)\n        except ValueError:\n            pass\n        return self\n\n    def __finish(self):\n        try:\n            signal.signal(signal.SIGINT, self.old_handler)\n        except ValueError:\n            pass\n        tracker.save()\n        logger.log()\n        if self.__is_save_models:\n            logger.log(""Saving model..."")\n            experiment.save_checkpoint()\n\n    def is_interval(self, interval: int, global_step: Optional[int] = None):\n        if global_step is None:\n            global_step = tracker.get_global_step()\n\n        if global_step - self.__loop_step < 0:\n            return False\n\n        if global_step // interval > (global_step - self.__loop_step) // interval:\n            return True\n        else:\n            return False\n\n    def __next__(self):\n        if self.__signal_received is not None:\n            logger.log(\'\\nKilling Loop.\', Text.danger)\n            monit.finish_loop()\n            self.__finish()\n            raise StopIteration(""SIGINT"")\n\n        try:\n            global_step = next(self.__loop)\n        except StopIteration as e:\n            self.__finish()\n            raise e\n\n        tracker.set_global_step(global_step)\n\n        if self.is_interval(self.__log_write_interval, global_step):\n            tracker.save()\n        if self.is_interval(self.__log_new_line_interval, global_step):\n            logger.log()\n\n        if (self.__is_save_models and\n                self.is_interval(self.__save_models_interval, global_step)):\n            experiment.save_checkpoint()\n\n        return global_step\n\n    def __handler(self, sig, frame):\n        # Pass second interrupt without delaying\n        if self.__signal_received is not None:\n            logger.log(\'\\nSIGINT received twice. Stopping...\', Text.danger)\n            self.old_handler(*self.__signal_received)\n            return\n\n        if self.__is_loop_on_interrupt:\n            # Store the interrupt signal for later\n            self.__signal_received = (sig, frame)\n            logger.log(\'\\nSIGINT received. Delaying KeyboardInterrupt.\', Text.danger)\n        else:\n            self.__finish()\n            logger.log(\'Killing loop...\', Text.danger)\n            self.old_handler(sig, frame)\n\n    def __str__(self):\n        return ""LabTrainingLoop""\n\n\nclass TrainingLoopConfigs(BaseConfigs):\n    loop_count: int = 10\n    loop_step: int = 1\n    is_save_models: bool = False\n    log_new_line_interval: int = 1\n    log_write_interval: int = 1\n    save_models_interval: int = 1\n    is_loop_on_interrupt: bool = True\n\n    training_loop: TrainingLoop\n\n\n@TrainingLoopConfigs.calc(TrainingLoopConfigs.training_loop)\ndef _loop_configs(c: TrainingLoopConfigs):\n    return TrainingLoop(loop_count=c.loop_count,\n                        loop_step=c.loop_step,\n                        is_save_models=c.is_save_models,\n                        log_new_line_interval=c.log_new_line_interval,\n                        log_write_interval=c.log_write_interval,\n                        save_models_interval=c.save_models_interval,\n                        is_loop_on_interrupt=c.is_loop_on_interrupt)\n'"
labml/internal/__init__.py,0,b''
labml/internal/lab.py,0,"b'from pathlib import PurePath, Path\nfrom typing import List, Optional\n\nfrom labml.internal import util\nfrom labml.utils import get_caller_file\n\n_CONFIG_FILE_NAME = \'.labml.yaml\'\n\n\nclass Lab:\n    """"""\n    ### Lab\n\n    Lab contains the labml specific properties.\n    """"""\n\n    def __init__(self):\n        self.path = None\n        self.check_repo_dirty = None\n        self.data_path = None\n        self.experiments = None\n\n        python_file = get_caller_file()\n        self.set_path(python_file)\n\n    def set_path(self, path: str):\n        configs = self.__get_config_files(path)\n\n        if len(configs) == 0:\n            raise RuntimeError(""No \'.labml.yaml\' config file found."")\n\n        config = self.__get_config(configs)\n\n        self.path = PurePath(config[\'path\'])\n        self.check_repo_dirty = config[\'check_repo_dirty\']\n        self.data_path = self.path / config[\'data_path\']\n        self.experiments = self.path / config[\'experiments_path\']\n\n    def __str__(self):\n        return f""<Lab path={self.path}>""\n\n    def __repr__(self):\n        return str(self)\n\n    @staticmethod\n    def __get_config(configs):\n        config = dict(\n            path=None,\n            check_repo_dirty=False,\n            is_log_python_file=True,\n            config_file_path=None,\n            data_path=\'data\',\n            experiments_path=\'logs\',\n            analytics_path=\'analytics\',\n            analytics_templates={}\n        )\n\n        for i, c in enumerate(reversed(configs)):\n            if config[\'path\'] is None:\n                config[\'path\'] = c[\'config_file_path\']\n\n            assert \'path\' not in c\n            assert i == 0 or \'experiments_path\' not in c\n            assert i == 0 or \'analytics_path\' not in c\n\n            for k, v in c.items():\n                if k not in config:\n                    raise RuntimeError(f""Unknown config parameter #{k} in file ""\n                                       f""{c[\'config_file_path\'] / _CONFIG_FILE_NAME}"")\n                else:\n                    config[k] = v\n\n        return config\n\n    @staticmethod\n    def __get_config_files(path: str):\n        path = Path(path).resolve()\n        configs = []\n\n        while path.exists():\n            if path.is_dir():\n                config_file = path / _CONFIG_FILE_NAME\n                if config_file.is_file():\n                    with open(str(config_file)) as f:\n                        config = util.yaml_load(f.read())\n                        if config is None:\n                            config = {}\n                        config[\'config_file_path\'] = path\n                        configs.append(config)\n\n            if str(path) == path.root:\n                break\n\n            path = path.parent\n\n        return configs\n\n    def get_experiments(self) -> List[Path]:\n        """"""\n        Get list of experiments\n        """"""\n        experiments_path = Path(self.experiments)\n        return [child for child in experiments_path.iterdir()]\n\n\n_internal: Optional[Lab] = None\n\n\ndef lab_singleton() -> Lab:\n    global _internal\n    if _internal is None:\n        _internal = Lab()\n\n    return _internal\n'"
labml/utils/__init__.py,0,"b""import os\nimport pathlib\nfrom typing import Set, List\n\n\ndef get_caller_file(ignore_callers: Set[str] = None):\n    if ignore_callers is None:\n        ignore_callers = {}\n\n    import inspect\n\n    frames: List[inspect.FrameInfo] = inspect.stack()\n    lab_src = pathlib.PurePath(__file__).parent.parent\n\n    for f in frames:\n        module_path = pathlib.PurePath(f.filename)\n        if str(module_path).startswith(str(lab_src)):\n            continue\n        if str(module_path) in ignore_callers:\n            continue\n        if str(module_path).startswith('<ipython'):\n            break\n        return str(module_path)\n\n    return os.path.abspath('')"""
labml/utils/delayed_keyboard_interrupt.py,0,"b'import signal\n\nfrom labml import logger\nfrom labml.logger import Text\n\n\nclass DelayedKeyboardInterrupt:\n    """"""\n    ### Capture `KeyboardInterrupt` and fire it later\n    """"""\n\n    def __init__(self):\n        self.signal_received = None\n\n    def __enter__(self):\n        self.signal_received = None\n        # Start capturing\n        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n\n    def handler(self, sig, frame):\n        # Pass second interrupt without delaying\n        if self.signal_received is not None:\n            self.old_handler(*self.signal_received)\n            return\n\n        # Store the interrupt signal for later\n        self.signal_received = (sig, frame)\n        logger.log([(\'\\nSIGINT received. Delaying KeyboardInterrupt.\',\n                     Text.danger)])\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Reset handler\n        signal.signal(signal.SIGINT, self.old_handler)\n\n        # Pass on any captured interrupt signals\n        if self.signal_received is not None:\n            self.old_handler(*self.signal_received)\n'"
labml/utils/errors.py,0,b'class ConfigsError(RuntimeError):\n    pass\n'
labml/utils/pytorch.py,4,"b'import torch\n\nfrom labml import tracker\nfrom labml.configs import BaseConfigs\n\n\ndef add_model_indicators(model: torch.nn.Module, model_name: str = ""model""):\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            tracker.set_histogram(f""{model_name}.{name}"")\n            tracker.set_histogram(f""{model_name}.{name}.grad"")\n\n\ndef store_model_indicators(model: torch.nn.Module, model_name: str = ""model""):\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            tracker.add(f""{model_name}.{name}"", param)\n            tracker.add(f""{model_name}.{name}.grad"", param.grad)\n\n\ndef get_modules(configs: BaseConfigs):\n    keys = dir(configs)\n\n    modules = {}\n    for k in keys:\n        value = getattr(configs, k)\n        if isinstance(value, torch.nn.Module):\n            modules[k] = value\n\n    return modules\n\n\ndef get_device(module: torch.nn.Module):\n    params = module.parameters()\n    try:\n        sample_param = next(params)\n        return sample_param.device\n    except StopIteration:\n        raise RuntimeError(f""Unable to determine""\n                           f"" device of {module.__class__.__name__}"") from None\n'"
labml/helpers/pytorch/__init__.py,0,b''
labml/helpers/pytorch/device.py,8,"b'import torch\n\nfrom labml.configs import BaseConfigs\n\n\nclass DeviceInfo:\n    def __init__(self, *,\n                 use_cuda: bool,\n                 cuda_device: int):\n        self.use_cuda = use_cuda\n        self.cuda_device = cuda_device\n        self.cuda_count = torch.cuda.device_count()\n\n        self.is_cuda = self.use_cuda and torch.cuda.is_available()\n        if not self.is_cuda:\n            self.device = torch.device(\'cpu\')\n        else:\n            if self.cuda_device < self.cuda_count:\n                self.device = torch.device(\'cuda\', self.cuda_device)\n            else:\n                self.device = torch.device(\'cuda\', self.cuda_count - 1)\n\n    def __str__(self):\n        if not self.is_cuda:\n            return ""CPU""\n\n        if self.cuda_device < self.cuda_count:\n            return f""GPU:{self.cuda_device} - {torch.cuda.get_device_name(self.cuda_device)}""\n        else:\n            return (f""GPU:{self.cuda_count - 1}({self.cuda_device}) ""\n                    f""- {torch.cuda.get_device_name(self.cuda_count - 1)}"")\n\n\nclass DeviceConfigs(BaseConfigs):\n    cuda_device: int = 0\n    use_cuda: bool = True\n\n    device_info: DeviceInfo\n\n    device: torch.device\n\n\n@DeviceConfigs.calc(DeviceConfigs.device)\ndef _device(c: DeviceConfigs):\n    return c.device_info.device\n\n\nDeviceConfigs.set_hyperparams(DeviceConfigs.cuda_device, DeviceConfigs.use_cuda,\n                              is_hyperparam=False)\n\n\n@DeviceConfigs.calc(DeviceConfigs.device_info)\ndef _device_info(c: DeviceConfigs):\n    return DeviceInfo(use_cuda=c.use_cuda,\n                      cuda_device=c.cuda_device)\n'"
labml/helpers/pytorch/module.py,3,"b'import torch.nn\n\n\nclass Module(torch.nn.Module):\n    r""""""\n    Wraps ``torch.nn.Module`` to overload ``__call__`` instead of\n    ``forward`` for better type checking.\n    """"""\n\n    def __init_subclass__(cls, **kwargs):\n        if cls.__dict__.get(\'__call__\', None) is None:\n            return\n\n        setattr(cls, \'forward\', cls.__dict__[\'__call__\'])\n        delattr(cls, \'__call__\')\n\n    @property\n    def device(self):\n        params = self.parameters()\n        try:\n            sample_param = next(params)\n            return sample_param.device\n        except StopIteration:\n            raise RuntimeError(f""Unable to determine""\n                               f"" device of {self.__class__.__name__}"") from None\n\n\nif __name__ == \'__main__\':\n    m = Module()\n    print(m.device)\n'"
labml/helpers/pytorch/seed.py,1,"b""import torch\nimport numpy as np\n\nfrom labml.configs import BaseConfigs\n\n\nclass SeedConfigs(BaseConfigs):\n    seed: int = 5\n\n    set_seed = 'set_seed'\n\n\n@SeedConfigs.calc(SeedConfigs.set_seed)\ndef set_seed(c: SeedConfigs):\n    torch.manual_seed(c.seed)\n    np.random.seed(c.seed)\n"""
labml/helpers/pytorch/train_valid.py,8,"b'from typing import Optional, Callable\n\nimport labml.utils.pytorch as pytorch_utils\nimport numpy as np\nimport torch.optim\nimport torch.utils.data\nfrom labml import tracker, monit\nfrom labml.configs import option\nfrom labml.helpers.training_loop import TrainingLoopConfigs\nfrom labml.utils.pytorch import get_device\nfrom torch import nn\n\n\nclass BatchStep:\n    def prepare_for_iteration(self):\n        raise NotImplementedError()\n\n    def init_stats(self):\n        return {}\n\n    def update_stats(self, stats: any, update: any):\n        for k, v in update.items():\n            if k not in stats:\n                stats[k] = []\n            stats[k].append(v)\n\n    def log_stats(self, stats: any):\n        raise NotImplementedError()\n\n    def process(self, batch: any):\n        raise NotImplementedError()\n\n\nclass SimpleBatchStep(BatchStep):\n    def __init__(self, *,\n                 model: nn.Module,\n                 optimizer: Optional[torch.optim.Adam],\n                 loss_func: Callable,\n                 accuracy_func: Callable):\n        self.accuracy_func = accuracy_func\n        self.loss_func = loss_func\n        self.optimizer = optimizer\n        self.model = model\n\n        tracker.set_queue("".loss"", 20, True)\n        if self.accuracy_func is not None:\n            tracker.set_scalar("".accuracy"", True)\n\n    def log_stats(self, stats: any):\n        if self.accuracy_func is not None:\n            tracker.add("".accuracy"", np.sum(stats[\'correct\']) / np.sum(stats[\'samples\']))\n\n    def prepare_for_iteration(self):\n        if self.optimizer is None:\n            self.model.eval()\n            return True\n        else:\n            self.model.train()\n            return False\n\n    def process(self, batch: any):\n        device = get_device(self.model)\n        data, target = batch\n        data, target = data.to(device), target.to(device)\n        stats = {\n            \'samples\': len(data)\n        }\n\n        if self.optimizer is not None:\n            self.optimizer.zero_grad()\n\n        output = self.model(data)\n        loss = self.loss_func(output, target)\n        if self.accuracy_func is not None:\n            stats[\'correct\'] = self.accuracy_func(output, target)\n\n        tracker.add("".loss"", loss)\n\n        if self.optimizer is not None:\n            loss.backward()\n            self.optimizer.step()\n\n        return stats\n\n\nclass Trainer:\n    def __init__(self, *,\n                 name: str,\n                 batch_step: BatchStep,\n                 data_loader: torch.utils.data.DataLoader,\n                 is_increment_global_step: bool,\n                 log_interval: Optional[int]):\n        self.batch_step = batch_step\n        self.log_interval = log_interval\n        self.is_increment_global_step = is_increment_global_step\n        self.data_loader = data_loader\n        self.name = name\n\n    def __call__(self):\n        if self.batch_step.prepare_for_iteration():\n            with torch.no_grad():\n                self.iterate()\n        else:\n            self.iterate()\n\n    def iterate(self):\n        stats = self.batch_step.init_stats()\n\n        for i, batch in monit.enum(self.name, self.data_loader):\n            update = self.batch_step.process(batch)\n            self.batch_step.update_stats(stats, update)\n\n            if self.is_increment_global_step:\n                tracker.add_global_step(update[\'samples\'])\n\n            if self.log_interval is not None and (i + 1) % self.log_interval == 0:\n                tracker.save()\n\n        self.batch_step.log_stats(stats)\n\n\nclass TrainValidConfigs(TrainingLoopConfigs):\n    epochs: int = 10\n\n    loss_func: Callable\n    accuracy_func: Callable\n    optimizer: torch.optim.Adam\n    model: nn.Module\n    train_batch_step: BatchStep = \'simple_train_batch_step\'\n    valid_batch_step: BatchStep = \'simple_valid_batch_step\'\n\n    trainer: Trainer\n    validator: Trainer\n\n    train_log_interval: int = 10\n\n    loop_count = \'data_loop_count\'\n    loop_step = \'data_loop_step\'\n\n    train_loader: torch.utils.data.DataLoader\n    valid_loader: torch.utils.data.DataLoader\n\n    is_log_parameters: bool = True\n\n    def run(self):\n        if self.is_log_parameters:\n            pytorch_utils.add_model_indicators(self.model)\n\n        for _ in self.training_loop:\n            with tracker.namespace(\'train\'):\n                self.trainer()\n            with tracker.namespace(\'valid\'):\n                self.validator()\n            if self.is_log_parameters:\n                pytorch_utils.store_model_indicators(self.model)\n\n\n@option(TrainValidConfigs.train_batch_step)\ndef simple_train_batch_step(c: TrainValidConfigs):\n    return SimpleBatchStep(model=c.model,\n                           optimizer=c.optimizer,\n                           loss_func=c.loss_func,\n                           accuracy_func=c.accuracy_func)\n\n\n@option(TrainValidConfigs.valid_batch_step)\ndef simple_valid_batch_step(c: TrainValidConfigs):\n    return SimpleBatchStep(model=c.model,\n                           optimizer=None,\n                           loss_func=c.loss_func,\n                           accuracy_func=c.accuracy_func)\n\n\n@option(TrainValidConfigs.trainer)\ndef trainer(c: TrainValidConfigs):\n    return Trainer(name=\'Train\',\n                   batch_step=c.train_batch_step,\n                   data_loader=c.train_loader,\n                   is_increment_global_step=True,\n                   log_interval=c.train_log_interval)\n\n\n@option(TrainValidConfigs.validator)\ndef validator(c: TrainValidConfigs):\n    return Trainer(name=\'Valid\',\n                   batch_step=c.valid_batch_step,\n                   data_loader=c.valid_loader,\n                   is_increment_global_step=False,\n                   log_interval=None)\n\n\n@option(TrainValidConfigs.loop_count)\ndef data_loop_count(c: TrainValidConfigs):\n    return c.epochs * len(c.train_loader.dataset)\n\n\n@option(TrainValidConfigs.loop_step)\ndef data_loop_step(c: TrainValidConfigs):\n    return len(c.train_loader.dataset)\n'"
labml/internal/analytics/__init__.py,0,b''
labml/internal/analytics/analytics.py,0,"b'from typing import List, Optional, NamedTuple\n\nimport numpy as np\n\nBASIS_POINTS = [\n    0,\n    6.68,\n    15.87,\n    30.85,\n    50.00,\n    69.15,\n    84.13,\n    93.32,\n    100.00\n]\n\n\nclass Event(NamedTuple):\n    step: int\n    tensor: np.ndarray\n\n\nclass Analytics:\n    def summarize(self, events):\n        raise NotImplementedError()\n\n    def summarize_scalars(self, events: List[any], points: Optional[int] = 100):\n        # Shrink to 100 histograms\n        if points is None:\n            interval = 1\n        else:\n            interval = max(1, len(events) // points)\n\n        merged = []\n        results = []\n        for i, e in enumerate(events):\n            if i > 0 and (i + 1) % interval == 0:\n                results.append(self.summarize(merged))\n                merged = []\n            merged.append(e)\n        if len(merged) > 0:\n            results.append(self.summarize(merged))\n\n        return np.array(results)\n'"
labml/internal/analytics/cache.py,0,"b'from pathlib import PurePath\nfrom typing import List, Tuple\n\nimport numpy as np\n\nfrom labml.internal.analytics.indicators import IndicatorClass, Indicator, Run, IndicatorCollection, \\\n    StepSelect\nfrom labml.internal.analytics.sqlite import SQLiteAnalytics\nfrom labml.internal.analytics.tensorboard import TensorBoardAnalytics\n\n_RUNS = {}\n\n_TENSORBOARD = {}\n\n_SQLITE = {}\n\n_NUMPY_ARRAYS = {}\n\n\ndef get_run(uuid: str) -> Run:\n    if uuid not in _RUNS:\n        _RUNS[uuid] = Run(uuid)\n\n    return _RUNS[uuid]\n\n\ndef get_name(ind: Indicator) -> List[str]:\n    run = get_run(ind.uuid)\n    return [run.name, ind.uuid[-5:], run.run_info.comment, ind.key]\n\n\ndef get_tensorboard_data(indicator: Indicator):\n    run = get_run(indicator.uuid)\n\n    if indicator.uuid not in _TENSORBOARD:\n        _TENSORBOARD[indicator.uuid] = TensorBoardAnalytics(run.run_info.tensorboard_log_path)\n\n    tb: TensorBoardAnalytics = _TENSORBOARD[indicator.uuid]\n    try:\n        tb.load()\n    except FileNotFoundError:\n        return None\n\n    try:\n        tensor = tb.tensor(indicator.key, indicator.select.start, indicator.select.end)\n    except KeyError:\n        return None\n    if indicator.class_ in [IndicatorClass.histogram, IndicatorClass.queue]:\n        data = tb.summarize_compressed_histogram(tensor)\n    else:\n        data = tb.summarize_scalars(tensor)\n\n    return data\n\n\ndef _get_sqlite_scalar_data(sqlite: SQLiteAnalytics, key: str, select: StepSelect):\n    data = sqlite.scalar(key, select.start, select.end)\n    if not data:\n        return None\n\n    data = sqlite.summarize_scalars(data)\n\n    return data\n\n\ndef get_sqlite_data(indicator: Indicator):\n    run = get_run(indicator.uuid)\n\n    if indicator.uuid not in _SQLITE:\n        _SQLITE[indicator.uuid] = SQLiteAnalytics(run.run_info.sqlite_path)\n\n    sqlite: SQLiteAnalytics = _SQLITE[indicator.uuid]\n\n    if indicator.class_ in [IndicatorClass.histogram, IndicatorClass.queue]:\n        return _get_sqlite_scalar_data(sqlite, f""{indicator.key}.mean"", indicator.select)\n    elif indicator.class_ == IndicatorClass.scalar:\n        return _get_sqlite_scalar_data(sqlite, indicator.key, indicator.select)\n    else:\n        return None\n\n\n_PREFERRED_DB = \'tensorboard\'\n\n\ndef set_preferred_db(db: str):\n    global _PREFERRED_DB\n    _PREFERRED_DB = db\n\n\ndef get_indicator_data(indicator: Indicator):\n    if _PREFERRED_DB == \'tensorboard\':\n        data = get_tensorboard_data(indicator)\n        if data is None:\n            data = get_sqlite_data(indicator)\n    else:\n        data = get_sqlite_data(indicator)\n        if data is None:\n            data = get_tensorboard_data(indicator)\n\n    return data\n\n\ndef get_indicators_data(indicators: IndicatorCollection):\n    series = []\n    names = []\n    for i, ind in enumerate(indicators):\n        d = get_indicator_data(ind)\n        if d is not None:\n            series.append(d)\n            names.append(get_name(ind))\n\n    return series, names\n\n\ndef _get_numpy_array(path: PurePath):\n    path = str(path)\n    if path not in _NUMPY_ARRAYS:\n        _NUMPY_ARRAYS[path] = np.load(path)\n\n    return _NUMPY_ARRAYS[path]\n\n\ndef get_artifact_files(indicator: Indicator):\n    run = get_run(indicator.uuid)\n\n    if indicator.uuid not in _SQLITE:\n        _SQLITE[indicator.uuid] = SQLiteAnalytics(run.run_info.sqlite_path)\n\n    sqlite: SQLiteAnalytics = _SQLITE[indicator.uuid]\n\n    if indicator.class_ != IndicatorClass.tensor:\n        return None\n\n    data = sqlite.tensor(indicator.key, indicator.select.start, indicator.select.end)\n\n    if not data:\n        return None\n\n    return data\n\n\ndef _get_condensed_steps(series: List[List[Tuple[int, any]]], limit: int):\n    steps = {}\n    step_lookups = []\n\n    series = [s for s in series if len(s) != 1 or s[0][0] != -1]\n\n    for s in series:\n        lookup = {}\n        for i, c in enumerate(s):\n            steps[c[0]] = steps.get(c[0], 0) + 1\n            lookup[c[0]] = i\n        step_lookups.append(lookup)\n\n    steps = [k for k, v in steps.items() if v == len(series)]\n    if len(steps) == 0:\n        return [], step_lookups\n\n    steps = sorted(steps)\n    last = steps[-1]\n    interval = max(1, last // (limit - 1))\n\n    condensed = [steps[0]]\n    for s in steps[1:]:\n        if s - condensed[-1] >= interval:\n            condensed.append(s)\n    if condensed[-1] != steps[-1]:\n        condensed.append(steps[-1])\n    return condensed, step_lookups\n\n\ndef get_artifacts_data(indicators: IndicatorCollection, limit: int = 100):\n    series = []\n    names = []\n    series_inds = []\n    for i, ind in enumerate(indicators):\n        d = get_artifact_files(ind)\n        if d is not None:\n            series.append(d)\n            series_inds.append(ind)\n            names.append(get_name(ind))\n\n    steps, step_lookups = _get_condensed_steps(series, limit)\n\n    data_series = []\n    for si, s in enumerate(series):\n        ind: Indicator = series_inds[si]\n        run: Run = get_run(ind.uuid)\n        if ind.props.get(\'is_once\', False):\n            assert s[0][0] == -1\n            filename = s[0][1]\n            data = _get_numpy_array(run.run_info.artifacts_folder / filename)\n        else:\n            data = []\n            for step in steps:\n                filename = s[step_lookups[si][step]][1]\n                data.append((step, _get_numpy_array(run.run_info.artifacts_folder / filename)))\n        data_series.append(data)\n\n    return data_series, names\n'"
labml/internal/analytics/indicators.py,0,"b'from enum import Enum\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple, NamedTuple, Dict\n\nfrom labml import lab\nfrom labml.internal import util\nfrom labml.internal.experiment.experiment_run import RunInfo\n\n\nclass RunsSet:\n    _runs: Dict[str, Tuple[Path, str]]\n\n    def __init__(self):\n        experiment_path = Path(lab.get_experiments_path())\n        runs = {}\n        for exp_path in experiment_path.iterdir():\n            for run_path in exp_path.iterdir():\n                runs[run_path.name] = (run_path, experiment_path.name)\n\n        self._runs = runs\n\n    def get(self, uuid: str) -> Tuple[RunInfo, str]:\n        run_path = self._runs[uuid][0]\n        run_info_path = run_path / \'run.yaml\'\n\n        with open(str(run_info_path), \'r\') as f:\n            data = util.yaml_load(f.read())\n            run = RunInfo.from_dict(run_path.parent, data)\n\n        return run, self._runs[uuid][1]\n\n\nclass IndicatorClass(Enum):\n    scalar = \'scalar\'\n    histogram = \'histogram\'\n    queue = \'queue\'\n    tensor = \'tensor\'\n\n\nclass StepSelect(NamedTuple):\n    start: Optional[int]\n    end: Optional[int]\n\n\nclass Indicator:\n    def __init__(self, key: str, class_: IndicatorClass, uuid: str,\n                 props: Dict[str, any],\n                 select: Optional[StepSelect]):\n        self.uuid = uuid\n        self.class_ = class_\n        self.key = key\n        self.props = props\n        if class_ == IndicatorClass.tensor and props.get(\'is_once\', False):\n            select = None\n        if select is None:\n            select = StepSelect(None, None)\n        self.select = select\n\n    def hash_str(self):\n        return f""{self.uuid}#{self.key}""\n\n\nclass IndicatorCollection:\n    _indicators: List[Indicator]\n\n    def __init__(self, indicators: List[Indicator]):\n        has = set()\n        self._indicators = []\n        for ind in indicators:\n            h = ind.hash_str()\n            if h in has:\n                continue\n            has.add(h)\n            self._indicators.append(ind)\n\n        self._indicator_keys = {ind.key.replace(\'.\', \'_\'): ind.key for ind in self._indicators}\n        self._indicators_list = [k for k in self._indicator_keys.keys()]\n\n    def __dir__(self):\n        return self._indicators_list\n\n    def __getattr__(self, k: str):\n        key = self._indicator_keys[k]\n        inds = []\n        for ind in self._indicators:\n            if ind.key == key:\n                inds.append(ind)\n\n        return IndicatorCollection(inds)\n\n    def __add__(self, other: \'IndicatorCollection\'):\n        return IndicatorCollection(self._indicators + other._indicators)\n\n    def __radd__(self, other: Optional[\'IndicatorCollection\']):\n        if other is None:\n            return IndicatorCollection(self._indicators)\n        else:\n            return IndicatorCollection(self._indicators + other._indicators)\n\n    def __iter__(self):\n        return iter(self._indicators)\n\n    def __len__(self):\n        return len(self._indicators)\n\n    def __getitem__(self, item: slice):\n        select = StepSelect(item.start, item.stop)\n        inds = [Indicator(ind.key, ind.class_, ind.uuid, ind.props, select)\n                for ind in self._indicators]\n        return IndicatorCollection(inds)\n\n\nclass Run:\n    indicators: IndicatorCollection\n    name: str\n    run_info: RunInfo\n\n    def __init__(self, uuid: str):\n        runs = RunsSet()\n        self.run_info, self.name = runs.get(uuid)\n\n        with open(str(self.run_info.indicators_path), \'r\') as f:\n            indicators = util.yaml_load(f.read())\n\n        inds = []\n        for k, v in indicators.items():\n            cn = v[\'class_name\']\n            class_ = None\n            if cn == \'Histogram\':\n                class_ = IndicatorClass.histogram\n            elif cn == \'Queue\':\n                class_ = IndicatorClass.queue\n            elif cn == \'IndexedScalar\':\n                class_ = IndicatorClass.scalar\n            elif cn == \'Scalar\':\n                class_ = IndicatorClass.scalar\n\n            if class_ is None:\n                continue\n            inds.append(Indicator(k, class_, self.run_info.uuid, v, None))\n\n        with open(str(self.run_info.artifacts_path), \'r\') as f:\n            artifacts = util.yaml_load(f.read())\n\n        for k, v in artifacts.items():\n            cn = v[\'class_name\']\n            class_ = None\n            if cn == \'Tensor\':\n                class_ = IndicatorClass.tensor\n\n            if class_ is None:\n                continue\n            inds.append(Indicator(k, class_, self.run_info.uuid, v, None))\n\n        self.indicators = IndicatorCollection(inds)\n'"
labml/internal/analytics/sqlite.py,0,"b'import sqlite3\nfrom typing import Optional\n\nimport numpy as np\n\nfrom .analytics import Analytics, BASIS_POINTS\n\n\ndef _filter_steps(start_step: Optional[int], end_step: Optional[int]):\n    sql = \'\'\n    if start_step is not None:\n        sql += f\' AND step >= {start_step}\'\n    if end_step is not None:\n        sql += f\' AND step < {end_step}\'\n\n    return sql\n\n\nclass SQLiteAnalytics(Analytics):\n    def __init__(self, sqlite_path):\n        self.conn = sqlite3.connect(str(sqlite_path))\n\n    def get_key(self, name):\n        return name\n\n    def scalar(self, name: str, start_step: Optional[int], end_step: Optional[int]):\n        key = self.get_key(name)\n        sql = f\'SELECT step, value from scalars WHERE indicator = ""{key}""\'\n        sql += _filter_steps(start_step, end_step)\n        cur = self.conn.execute(sql)\n        return [c for c in cur]\n\n    def summarize(self, events):\n        step = np.mean([e[0] for e in events])\n        values = np.sort([e[1] for e in events])\n        basis_points = np.percentile(values, BASIS_POINTS)\n\n        return np.concatenate(([step], basis_points))\n\n    def tensor(self, name: str, start_step: Optional[int], end_step: Optional[int]):\n        key = self.get_key(name)\n        sql = f\'SELECT step, filename from tensors WHERE indicator = ""{key}""\'\n        sql += _filter_steps(start_step, end_step)\n        cur = self.conn.execute(sql)\n        return [c for c in cur]\n'"
labml/internal/analytics/tensorboard.py,0,"b""from typing import List, Optional\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorboard.backend.event_processing.directory_watcher import DirectoryDeletedError\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator\nfrom tensorboard.plugins.distribution import compressor\n\nfrom .analytics import Analytics, BASIS_POINTS, Event\n\n\ndef _is_filtered(event, start_step: Optional[int], end_step: Optional[int]):\n    if start_step is not None and event.step < start_step:\n        return False\n    elif end_step is not None and event.step >= end_step:\n        return False\n    else:\n        return True\n\n\nclass TensorBoardAnalytics(Analytics):\n    def __init__(self, log_path):\n        self.event_acc = EventAccumulator(str(log_path), size_guidance={'tensors': 1000})\n\n    def load(self):\n        try:\n            self.event_acc.Reload()\n        except DirectoryDeletedError:\n            raise FileNotFoundError()\n\n    def tensor(self, name: str, start_step: Optional[int], end_step: Optional[int]) -> List[Event]:\n        name = name.replace('.', '/')\n        events = self.event_acc.Tensors(name)\n        return [Event(e.step, tf.make_ndarray(e.tensor_proto))\n                for e in events if _is_filtered(e, start_step, end_step)]\n\n    def summarize(self, events: List[Event]):\n        step = np.mean([e.step for e in events])\n        values = np.sort([e.tensor for e in events])\n        basis_points = np.percentile(values, BASIS_POINTS)\n\n        return np.concatenate(([step], basis_points))\n\n    def summarize_compressed_histogram(self, events: List[Event]):\n        basis_points = [int(b) for b in np.multiply(BASIS_POINTS, 100)]\n        results = []\n        for e in events:\n            buckets = compressor.compress_histogram(e.tensor)\n            assert (len(buckets) == len(basis_points))\n            for i, c in enumerate(buckets):\n                assert (c.basis_point == basis_points[i])\n            results.append([e.step] + [c.value for c in buckets])\n\n        return np.asarray(results)\n"""
labml/internal/configs/__init__.py,0,b''
labml/internal/configs/base.py,0,"b""from typing import Dict, List, Callable, Union, Tuple, Optional\n\nfrom labml.internal.configs.eval_function import EvalFunction\nfrom .config_function import ConfigFunction\nfrom .config_item import ConfigItem\nfrom .parser import Parser, PropertyKeys\n\n\ndef _is_class_method(func: Callable):\n    if not callable(func):\n        return False\n\n    import inspect\n\n    spec: inspect.Signature = inspect.signature(func)\n    params: List[inspect.Parameter] = list(spec.parameters.values())\n    if len(params) == 0:\n        raise RuntimeError('Can only have methods in a config class', func)\n    p = params[0]\n    if p.kind != p.POSITIONAL_OR_KEYWORD:\n        raise RuntimeError('Can only have methods in a config class', func)\n    if p.name != 'self':\n        raise RuntimeError('Can only have methods in a config class', func)\n\n    return True\n\n\nclass Configs:\n    _calculators: Dict[str, List[ConfigFunction]] = {}\n    _evaluators: Dict[str, List[EvalFunction]] = {}\n\n    def __init_subclass__(cls, **kwargs):\n        configs = {}\n\n        for k, v in cls.__annotations__.items():\n            if not Parser.is_valid(k):\n                continue\n\n            configs[k] = ConfigItem(key=k,\n                                    configs_class=cls,\n                                    has_annotation=True, annotation=v,\n                                    has_value=k in cls.__dict__,\n                                    value=cls.__dict__.get(k, None))\n\n        evals = []\n        for k, v in cls.__dict__.items():\n            if not Parser.is_valid(k):\n                continue\n\n            if _is_class_method(v):\n                evals.append((k, v))\n                continue\n\n            configs[k] = ConfigItem(key=k,\n                                    configs_class=cls,\n                                    has_annotation=k in cls.__annotations__,\n                                    annotation=cls.__annotations__.get(k, None),\n                                    has_value=True, value=v)\n\n        for e in evals:\n            cls._add_eval_function(e[1], e[0])\n\n        for k, v in configs.items():\n            setattr(cls, k, v)\n\n    @classmethod\n    def _add_config_function(cls,\n                             func: Callable,\n                             name: Union[ConfigItem, List[ConfigItem]],\n                             option: Optional[str],\n                             pass_params: Optional[List[ConfigItem]]):\n        if PropertyKeys.calculators not in cls.__dict__:\n            cls._calculators = {}\n\n        calc = ConfigFunction(func,\n                              config_names=name,\n                              option_name=option,\n                              pass_params=pass_params)\n        if type(calc.config_names) == str:\n            config_names = [calc.config_names]\n        else:\n            config_names = calc.config_names\n\n        for n in config_names:\n            if n not in cls._calculators:\n                cls._calculators[n] = []\n            cls._calculators[n].append(calc)\n\n    @classmethod\n    def _add_eval_function(cls,\n                           func: Callable,\n                           name: str):\n        if PropertyKeys.evaluators not in cls.__dict__:\n            cls._evaluators = {}\n\n        calc = EvalFunction(func,\n                            config_name=name)\n\n        if name not in cls._evaluators:\n            cls._evaluators[name] = []\n        cls._evaluators[name].append(calc)\n\n    @classmethod\n    def _calc(cls,\n              name: Union[ConfigItem, List[ConfigItem]] = None,\n              option: Optional[str] = None,\n              pass_params: Optional[List[ConfigItem]] = None):\n\n        def wrapper(func: Callable):\n            cls._add_config_function(func, name, option, pass_params)\n\n            return func\n\n        return wrapper\n\n    @classmethod\n    def calc_wrap(cls, func: Callable,\n                  name: Union[ConfigItem, List[ConfigItem]],\n                  option_name: Optional[str] = None,\n                  pass_params: Optional[List[ConfigItem]] = None):\n        cls._add_config_function(func, name, option_name, pass_params)\n\n        return func\n\n    @classmethod\n    def calc(cls,\n             name: Union[ConfigItem, List[ConfigItem]] = None,\n             option_name: Optional[str] = None,\n             pass_params: Optional[List[ConfigItem]] = None):\n        return cls._calc(name, option_name, pass_params)\n\n    @classmethod\n    def set_hyperparams(cls, *args: ConfigItem, is_hyperparam=True):\n        if PropertyKeys.hyperparams not in cls.__dict__:\n            cls._hyperparams = {}\n\n        for h in args:\n            cls._hyperparams[h.key] = is_hyperparam\n\n    @classmethod\n    def aggregate(cls, name: ConfigItem, option: str,\n                  *args: Tuple[ConfigItem, any]):\n        assert args\n\n        if PropertyKeys.aggregates not in cls.__dict__:\n            cls._aggregates = {}\n\n        if name.key not in cls._aggregates:\n            cls._aggregates[name.key] = {}\n\n        pairs = {p[0].key: p[1] for p in args}\n        cls._aggregates[name.key][option] = pairs\n"""
labml/internal/configs/calculator.py,0,"b'from typing import List, Dict, Type, Set, Optional, \\\n    OrderedDict as OrderedDictType, Union, Any, Tuple\nfrom typing import TYPE_CHECKING\n\nfrom labml.internal.configs.eval_function import EvalFunction\n\nfrom .config_function import ConfigFunction\nfrom ... import logger\nfrom ... import monit\n\nif TYPE_CHECKING:\n    from .base import Configs\n\n\nclass Calculator:\n    evals: Dict[str, OrderedDictType[str, EvalFunction]]\n    options: Dict[str, OrderedDictType[str, ConfigFunction]]\n    types: Dict[str, Type]\n    values: Dict[str, any]\n\n    configs: \'Configs\'\n\n    dependencies: Dict[str, Set[str]]\n    topological_order: List[str]\n    stack: List[str]\n    visited: Set[str]\n    is_computed: Set[str]\n    is_top_sorted: Set[str]\n\n    def __init__(self, *,\n                 configs: \'Configs\',\n                 options: Dict[str, OrderedDictType[str, ConfigFunction]],\n                 evals: Dict[str, OrderedDictType[str, EvalFunction]],\n                 types: Dict[str, Type],\n                 values: Dict[str, any],\n                 aggregate_parent: Dict[str, str]):\n        self.aggregate_parent = aggregate_parent\n        self.evals = evals\n        self.configs = configs\n        self.options = options\n        self.types = types\n        self.values = values\n\n        self.visited = set()\n        self.stack = []\n        self.is_top_sorted = set()\n        self.topological_order = []\n        self.is_computed = set()\n\n    def __get_property(self, key) -> Tuple[Any, Union[None, ConfigFunction, List[ConfigFunction]]]:\n        if key in self.options:\n            value = self.values[key]\n            if value not in self.options[key]:\n                return value, None\n            return None, self.options[key][value]\n\n        return self.values[key], None\n\n    def __get_dependencies(self, key) -> Set[str]:\n        if key in self.options:\n            value = self.values[key]\n            if value not in self.options[key]:\n                return set()\n            return self.options[key][value].dependencies\n\n        if key in self.evals:\n            value = self.values.get(key, None)\n            if not value:\n                value = \'default\'\n            if value not in self.evals[key]:\n                return set()\n            return self.evals[key][value].dependencies\n\n        assert key in self.values, f""Cannot compute {key}""\n        # assert self.values[key] is not None, f""Cannot compute {key}""\n\n        return set()\n\n    def __create_graph(self):\n        self.dependencies = {}\n        for k in self.types:\n            self.dependencies[k] = self.__get_dependencies(k)\n        for k in self.evals:\n            self.dependencies[k] = self.__get_dependencies(k)\n\n    def __add_to_topological_order(self, key):\n        assert self.stack.pop() == key\n        self.is_top_sorted.add(key)\n        self.topological_order.append(key)\n\n    def __traverse(self, key):\n        for d in self.dependencies[key]:\n            if d not in self.is_top_sorted:\n                self.__add_to_stack(d)\n                return\n\n        self.__add_to_topological_order(key)\n\n    def __add_to_stack(self, key):\n        if key in self.is_top_sorted:\n            return\n\n        assert key not in self.visited, f""Cyclic dependency: {key}""\n\n        self.visited.add(key)\n        self.stack.append(key)\n\n    def __dfs(self):\n        while len(self.stack) > 0:\n            key = self.stack[-1]\n            self.__traverse(key)\n\n    def __topological_sort(self, keys: List[str]):\n        for k in keys:\n            assert k not in self.is_top_sorted\n\n        for k in keys:\n            self.__add_to_stack(k)\n            self.__dfs()\n\n    def __set_configs(self, key, value):\n        assert key not in self.is_computed\n        self.is_computed.add(key)\n        self.configs.__setattr__(key, value)\n\n    def __compute(self, key):\n        if key in self.is_computed:\n            return\n\n        if key in self.evals:\n            return\n\n        value, funcs = self.__get_property(key)\n        if funcs is None:\n            self.__set_configs(key, value)\n        elif type(funcs) == list:\n            self.__set_configs(key, [f(self.configs) for f in funcs])\n        else:\n            s = monit.section(f\'Prepare {key}\', is_new_line=False)\n            with s:\n                value = funcs(self.configs)\n            if s.get_estimated_time() >= 0.01:\n                logger.log()\n            else:\n                logger.log(\' \' * 100, is_new_line=False)\n\n            if type(funcs.config_names) == str:\n                self.__set_configs(key, value)\n            else:\n                for i, k in enumerate(funcs.config_names):\n                    self.__set_configs(k, value[i])\n\n    def __compute_values(self):\n        for k in self.topological_order:\n            if k not in self.is_computed:\n                self.__compute(k)\n\n        parents = set()\n        for k in self.topological_order:\n            if k in self.aggregate_parent:\n                parent = self.aggregate_parent[k]\n                if parent not in self.is_computed:\n                    parents.add(parent)\n\n        self.topological_order = list(parents) + self.topological_order\n\n    def __call__(self, run_order: Optional[List[Union[List[str], str]]]):\n        if run_order is None:\n            run_order = [list(self.types.keys())]\n        run_order: List[Union[List[str], str]]\n\n        for i in range(len(run_order)):\n            keys = run_order[i]\n            if type(keys) == str:\n                run_order[i] = [keys]\n\n        s = monit.section(\'Calculate config dependencies\', is_new_line=False)\n        with s:\n            self.__create_graph()\n        if s.get_estimated_time() >= 0.01:\n            logger.log()\n\n        self.visited = set()\n        self.stack = []\n        self.is_top_sorted = set()\n        self.topological_order = []\n        self.is_computed = set()\n\n        for keys in run_order:\n            self.__topological_sort(keys)\n            self.__compute_values()\n'"
labml/internal/configs/config_function.py,0,"b'import inspect\nimport warnings\nfrom enum import Enum\nfrom typing import List, Callable, cast, Set, Union, Optional\nfrom typing import TYPE_CHECKING\n\nfrom labml.internal.configs.dependency_parser import DependencyParser\nfrom .config_item import ConfigItem\nfrom ...utils.errors import ConfigsError\n\nif TYPE_CHECKING:\n    from .base import Configs\n\n\nclass FunctionKind(Enum):\n    pass_configs = \'pass_configs\'\n    pass_kwargs = \'pass_kwargs\'\n    pass_nothing = \'pass_nothing\'\n    pass_params = \'pass_params\'\n\n\nclass ConfigFunction:\n    func: Callable\n    kind: FunctionKind\n    dependencies: Set[str]\n    config_names: Union[str, List[str]]\n    option_name: str\n    params: List[inspect.Parameter]\n    pass_params: Optional[List[ConfigItem]]\n\n    def __get_type(self):\n        key, pos = 0, 0\n\n        for p in self.params:\n            if p.kind == p.POSITIONAL_OR_KEYWORD:\n                pos += 1\n            elif p.kind == p.KEYWORD_ONLY:\n                key += 1\n            else:\n                assert False, ""Only positional or keyword only arguments should be accepted""\n\n        if self.pass_params is not None:\n            if key > 0:\n                raise ConfigsError(\'No keyword arguments are supported when passing configs\')\n            if pos != len(self.pass_params):\n                raise ConfigsError(\'Number of argumnents to function should match the number of \'\n                                   \'configs to be passed\')\n            return FunctionKind.pass_params\n\n        if pos == 1:\n            assert key == 0\n            return FunctionKind.pass_configs\n        elif pos == 0 and key == 0:\n            return FunctionKind.pass_nothing\n        else:\n            warnings.warn(""Use configs object, because it\'s easier to refactor, find usage etc"",\n                          FutureWarning, stacklevel=4)\n            assert pos == 0\n            return FunctionKind.pass_kwargs\n\n    def __get_dependencies(self):\n        if self.kind == FunctionKind.pass_configs:\n            parser = DependencyParser(self.func)\n            assert not parser.is_referenced, \\\n                f""{self.func.__name__} should only use attributes of configs""\n            return parser.required\n        elif self.kind == FunctionKind.pass_kwargs:\n            return {p.name for p in self.params}\n        elif self.kind == FunctionKind.pass_params:\n            return {p.key for p in self.pass_params}\n        elif self.kind == FunctionKind.pass_nothing:\n            return set()\n        else:\n            assert False\n\n    def __get_option_name(self, option_name: str):\n        if option_name is not None:\n            return option_name\n        else:\n            return self.func.__name__\n\n    def __get_config_names(self, config_names: Union[str, ConfigItem, List[ConfigItem], List[str]]):\n        if config_names is None:\n            warnings.warn(""Use @Config.[name]"", FutureWarning, 4)\n            return self.func.__name__\n        elif type(config_names) == str:\n            if self.check_string_names:\n                warnings.warn(""Use @Config.[name] instead of \'[name]\'"", FutureWarning, 4)\n            return config_names\n        elif type(config_names) == ConfigItem:\n            return config_names.key\n        else:\n            assert type(config_names) == list\n            assert len(config_names) > 0\n            if type(config_names[0]) == str:\n                warnings.warn(""Use @Config.[name] instead of \'[name]\'"", FutureWarning, 4)\n                return config_names\n            else:\n                assert type(config_names[0]) == ConfigItem\n                return [c.key for c in config_names]\n\n    def __get_params(self):\n        func_type = type(self.func)\n\n        if func_type == type:\n            init_func = cast(object, self.func).__init__\n            spec: inspect.Signature = inspect.signature(init_func)\n            params: List[inspect.Parameter] = list(spec.parameters.values())\n            assert len(params) > 0\n            assert params[0].kind == inspect.Parameter.POSITIONAL_OR_KEYWORD, self.config_names\n            assert params[0].name == \'self\'\n            return params[1:]\n        else:\n            spec: inspect.Signature = inspect.signature(self.func)\n            params: List[inspect.Parameter] = list(spec.parameters.values())\n            return params\n\n    def __init__(self, func, *,\n                 config_names: Union[str, ConfigItem, List[ConfigItem], List[str]],\n                 option_name: str,\n                 pass_params: Optional[List[ConfigItem]] = None,\n                 check_string_names: bool = True):\n        self.func = func\n        self.check_string_names = check_string_names\n        self.pass_params = pass_params\n        self.config_names = self.__get_config_names(config_names)\n        self.option_name = self.__get_option_name(option_name)\n\n        self.params = self.__get_params()\n\n        self.kind = self.__get_type()\n        self.dependencies = self.__get_dependencies()\n\n    def __call__(self, configs: \'Configs\'):\n        if self.kind == FunctionKind.pass_configs:\n            if len(self.params) == 1:\n                return self.func(configs)\n            else:\n                return self.func()\n        elif self.kind == FunctionKind.pass_kwargs:\n            kwargs = {p.name: configs.__getattribute__(p.name) for p in self.params}\n            return self.func(**kwargs)\n        elif self.kind == FunctionKind.pass_params:\n            args = [configs.__getattribute__(p.key) for p in self.pass_params]\n            return self.func(*args)\n        elif self.kind == FunctionKind.pass_nothing:\n            return self.func()\n        else:\n            assert False\n'"
labml/internal/configs/config_item.py,0,"b""from typing import TYPE_CHECKING, Type, Optional, Callable\n\nif TYPE_CHECKING:\n    from .base import Configs\n\n\nclass ConfigItem:\n    def __init__(self, *,\n                 key: str,\n                 configs_class: Type['Configs'],\n                 has_annotation: bool, annotation: any,\n                 has_value: bool, value: any):\n        self.key = key\n        if annotation is None:\n            annotation = type(value)\n        self.annotation = annotation\n        self.value = value\n        self.has_annotation = has_annotation\n        self.has_value = has_value\n        self.configs_class = configs_class\n\n    def update(self, k: 'ConfigItem'):\n        if k.has_annotation:\n            self.has_annotation = True\n            self.annotation = k.annotation\n\n        if k.has_value:\n            self.has_value = True\n            self.value = k.value\n\n    # def calc(self, option: Optional[str] = None):\n    #     return self.configs_class.calc(self, option)\n    #\n    # def __call__(self, func: Callable):\n    #     return self.configs_class.calc_wrap(func, self)\n"""
labml/internal/configs/dependency_parser.py,0,"b'import ast\nimport inspect\nimport textwrap\nfrom typing import Callable, cast\n\n\nclass DependencyParser(ast.NodeVisitor):\n    def __init__(self, func: Callable):\n        if type(func) == type:\n            func = cast(object, func).__init__\n            spec: inspect.Signature = inspect.signature(func)\n            params = spec.parameters\n            assert len(params) == 2\n            param: inspect.Parameter = params[list(params.keys())[1]]\n            source = textwrap.dedent(inspect.getsource(func))\n\n        else:\n            spec: inspect.Signature = inspect.signature(func)\n            params = spec.parameters\n            assert len(params) >= 1\n            param: inspect.Parameter = params[list(params.keys())[0]]\n            source = inspect.getsource(func)\n\n        assert (param.kind == param.POSITIONAL_ONLY or\n                param.kind == param.POSITIONAL_OR_KEYWORD)\n\n        self.arg_name = param.name\n\n        self.required = set()\n        self.is_referenced = False\n\n        source = textwrap.dedent(source)\n        parsed = ast.parse(source)\n        self.visit(parsed)\n\n    def visit_Attribute(self, node: ast.Attribute):\n        if isinstance(node.value, ast.Name):\n            if node.value.id == self.arg_name:\n                self.required.add(node.attr)\n        else:\n            for child in ast.iter_child_nodes(node):\n                self.visit(child)\n\n    def visit_Name(self, node: ast.Name):\n        if node.id == self.arg_name:\n            self.is_referenced = True\n            print(f""Referenced {node.id} in {node.lineno}:{node.col_offset}"")'"
labml/internal/configs/eval_function.py,0,"b'import inspect\nfrom typing import List, Callable, Set\n\nfrom labml.internal.configs.dependency_parser import DependencyParser\n\n\nclass EvalFunction:\n    func: Callable\n    dependencies: Set[str]\n    config_name: str\n\n    def __check_type(self):\n        key, pos = 0, 0\n        spec: inspect.Signature = inspect.signature(self.func)\n        params: List[inspect.Parameter] = list(spec.parameters.values())\n\n        for p in params:\n            if p.kind == p.POSITIONAL_OR_KEYWORD:\n                pos += 1\n            elif p.kind == p.KEYWORD_ONLY:\n                key += 1\n            else:\n                assert False, ""Only positional or keyword only arguments should be accepted""\n\n        assert pos >= 1\n\n    def __get_dependencies(self):\n        parser = DependencyParser(self.func)\n        if parser.is_referenced:\n            raise RuntimeError(f""{self.func.__name__} should only use attributes of configs"")\n        return parser.required\n\n    def __init__(self, func, *,\n                 config_name: str):\n        self.func = func\n        self.config_name = config_name\n\n        self.__check_type()\n        self.dependencies = self.__get_dependencies()\n'"
labml/internal/configs/parser.py,0,"b'import warnings\nfrom collections import OrderedDict\nfrom typing import List, Dict, Type, OrderedDict as OrderedDictType, Set\nfrom typing import TYPE_CHECKING\n\nfrom labml.internal.configs.eval_function import EvalFunction\n\nfrom .config_function import ConfigFunction\nfrom .config_item import ConfigItem\n\nif TYPE_CHECKING:\n    from .base import Configs\n\nRESERVED = {\'calc\', \'list\', \'set_hyperparams\', \'aggregate\', \'calc_wrap\'}\n_STANDARD_TYPES = {int, str, bool, Dict, List}\n\n\nclass PropertyKeys:\n    calculators = \'_calculators\'\n    evaluators = \'_evaluators\'\n    hyperparams = \'_hyperparams\'\n    aggregates = \'_aggregates\'\n\n\ndef _get_base_classes(class_: Type[\'Configs\']) -> List[Type[\'Configs\']]:\n    classes = [class_]\n    level = [class_]\n    next_level = []\n\n    while len(level) > 0:\n        for c in level:\n            for b in c.__bases__:\n                if b == object:\n                    continue\n                next_level.append(b)\n        classes += next_level\n        level = next_level\n        next_level = []\n\n    classes.reverse()\n\n    unique_classes = []\n    hashes: Set[int] = set()\n    for c in classes:\n        if hash(c) not in hashes:\n            unique_classes.append(c)\n        hashes.add(hash(c))\n\n    return unique_classes\n\n\nclass Parser:\n    config_items: Dict[str, ConfigItem]\n    options: Dict[str, OrderedDictType[str, ConfigFunction]]\n    evals: Dict[str, OrderedDictType[str, EvalFunction]]\n    types: Dict[str, Type]\n    values: Dict[str, any]\n    explicitly_specified: Set[str]\n    hyperparams: Dict[str, bool]\n    aggregates: Dict[str, Dict[str, Dict[str, str]]]\n    aggregate_parent: Dict[str, str]\n\n    def __init__(self, configs: \'Configs\', values: Dict[str, any] = None):\n        classes = _get_base_classes(type(configs))\n\n        self.values = {}\n        self.types = {}\n        self.options = {}\n        self.evals = {}\n        self.config_items = {}\n        self.configs = configs\n        self.explicitly_specified = set()\n        self.hyperparams = {}\n        self.aggregates = {}\n        self.aggregate_parent = {}\n\n        for c in classes:\n            # for k, v in c.__annotations__.items():\n            #     self.__collect_annotation(k, v)\n            #\n            for k, v in c.__dict__.items():\n                if (PropertyKeys.evaluators in c.__dict__ and\n                        k in c.__dict__[PropertyKeys.evaluators]):\n                    continue\n                self.__collect_config_item(k, v)\n\n        for c in classes:\n            if PropertyKeys.calculators in c.__dict__:\n                for k, calculators in c.__dict__[PropertyKeys.calculators].items():\n                    assert k in self.types, \\\n                        f""{k} calculator is present but the config declaration is missing""\n                    for v in calculators:\n                        self.__collect_calculator(k, v)\n\n        for c in classes:\n            if PropertyKeys.evaluators in c.__dict__:\n                for k, evaluators in c.__dict__[PropertyKeys.evaluators].items():\n                    for v in evaluators:\n                        self.__collect_evaluator(k, v)\n\n        for c in classes:\n            if PropertyKeys.hyperparams in c.__dict__:\n                for k, is_hyperparam in c.__dict__[PropertyKeys.hyperparams].items():\n                    self.hyperparams[k] = is_hyperparam\n\n        for c in classes:\n            if PropertyKeys.aggregates in c.__dict__:\n                for k, aggregates in c.__dict__[PropertyKeys.aggregates].items():\n                    self.aggregates[k] = aggregates\n\n        for k, v in configs.__dict__.items():\n            assert k in self.types\n            self.__collect_value(k, v)\n\n        if values is not None:\n            for k, v in values.items():\n                assert k in self.types\n                self.__collect_value(k, v)\n\n        self.__calculate_aggregates()\n        self.__calculate_missing_values()\n\n    @staticmethod\n    def is_valid(key):\n        if key.startswith(\'_\'):\n            return False\n\n        if key in RESERVED:\n            return False\n\n        return True\n\n    def __collect_config_item(self, k, v: ConfigItem):\n        if not self.is_valid(k):\n            return\n\n        if v.has_value:\n            self.values[k] = v.value\n\n        if k in self.config_items:\n            self.config_items[k].update(v)\n        else:\n            self.config_items[k] = v\n\n        if k not in self.types:\n            self.types[k] = v.annotation\n\n    def __collect_value(self, k, v):\n        if not self.is_valid(k):\n            return\n\n        self.explicitly_specified.add(k)\n\n        self.values[k] = v\n        if k not in self.types:\n            self.types[k] = type(v)\n\n    def __collect_annotation(self, k, v):\n        if not self.is_valid(k):\n            return\n\n        self.types[k] = v\n\n    def __collect_calculator(self, k, v: ConfigFunction):\n        if k not in self.options:\n            self.options[k] = OrderedDict()\n        if v.option_name in self.options[k]:\n            if v != self.options[k][v.option_name]:\n                warnings.warn(f""Overriding option for {k}: {v.option_name}"", Warning,\n                              stacklevel=5)\n\n        self.options[k][v.option_name] = v\n\n    def __collect_evaluator(self, k, v: EvalFunction):\n        if k not in self.evals:\n            self.evals[k] = OrderedDict()\n\n        self.evals[k][\'default\'] = v\n\n    def __calculate_missing_values(self):\n        for k in self.types:\n            if k in self.values:\n                continue\n\n            if k in self.options:\n                self.values[k] = next(iter(self.options[k].keys()))\n                continue\n\n            if type(self.types[k]) == type:\n                if self.types[k] in _STANDARD_TYPES:\n                    continue\n\n                self.options[k] = OrderedDict()\n                self.options[k][k] = ConfigFunction(self.types[k],\n                                                    config_names=self.config_items[k],\n                                                    option_name=k)\n                self.values[k] = k\n                continue\n\n            assert k in self.values, f""Cannot compute {k}""\n\n    def __calculate_aggregates(self):\n        queue = []\n        for k in self.aggregates:\n            if k not in self.values or self.values[k] not in self.aggregates[k]:\n                continue\n\n            queue.append(k)\n\n        while queue:\n            k = queue.pop()\n            assert k in self.values\n            option = self.values[k]\n            assert option in self.aggregates[k]\n            pairs = self.aggregates[k][option]\n\n            for name, opt in pairs.items():\n                if name in self.values and self.values[name] != \'__none__\':\n                    continue\n\n                self.aggregate_parent[name] = k\n                self.values[name] = opt\n\n                if name in self.aggregates and opt in self.aggregates[name]:\n                    queue.append(name)\n'"
labml/internal/configs/processor.py,0,"b'from pathlib import PurePath\nfrom typing import Dict, Optional, List, Union\n\nfrom labml import logger\nfrom labml.internal import util\nfrom labml.internal.configs.base import Configs\nfrom labml.internal.configs.utils import Value\nfrom labml.logger import Text\n\nfrom .calculator import Calculator\nfrom .parser import Parser\n\n_CONFIG_PRINT_LEN = 40\n\n\nclass ConfigProcessor:\n    def __init__(self, configs: Configs, values: Dict[str, any] = None):\n        self.parser = Parser(configs, values)\n        self.calculator = Calculator(configs=configs,\n                                     options=self.parser.options,\n                                     evals=self.parser.evals,\n                                     types=self.parser.types,\n                                     values=self.parser.values,\n                                     aggregate_parent=self.parser.aggregate_parent)\n\n    def __call__(self, run_order: Optional[List[Union[List[str], str]]] = None):\n        self.calculator(run_order)\n\n    def save(self, configs_path: PurePath):\n        orders = {k: i for i, k in enumerate(self.calculator.topological_order)}\n        configs = {}\n        for k, v in self.parser.types.items():\n            configs[k] = {\n                \'name\': k,\n                \'type\': str(v),\n                \'value\': Value.to_yaml(self.parser.values.get(k, None)),\n                \'order\': orders.get(k, -1),\n                \'options\': list(self.parser.options.get(k, {}).keys()),\n                \'computed\': Value.to_yaml(getattr(self.calculator.configs, k, None)),\n                \'is_hyperparam\': self.parser.hyperparams.get(k, None),\n                \'is_explicitly_specified\': (k in self.parser.explicitly_specified)\n            }\n\n        with open(str(configs_path), ""w"") as file:\n            file.write(util.yaml_dump(configs))\n\n    def get_hyperparams(self):\n        order = self.calculator.topological_order.copy()\n\n        hyperparams = {}\n        for key in order:\n            if (self.parser.hyperparams.get(key, False) or\n                    key in self.parser.explicitly_specified):\n                value = getattr(self.calculator.configs, key, None)\n                if key in self.parser.options:\n                    value = self.parser.values[key]\n\n                if type(value) not in {int, float, str}:\n                    value = Value.to_str(value)\n\n                hyperparams[key] = value\n\n        return hyperparams\n\n    def __print_config(self, key, *, value=None, option=None,\n                       other_options=None, is_ignored=False, is_list=False):\n        parts = [\'\\t\']\n\n        if is_ignored:\n            parts.append((key, Text.subtle))\n            return parts\n\n        is_hyperparam = self.parser.hyperparams.get(key, None)\n        if is_hyperparam is None:\n            is_hyperparam = key in self.parser.explicitly_specified\n        if is_hyperparam:\n            parts.append((key, [Text.key, Text.highlight]))\n        else:\n            parts.append((key, Text.key))\n\n        if is_list:\n            parts.append((\'[]\', Text.subtle))\n\n        parts.append((\' = \', Text.subtle))\n\n        if other_options is None:\n            other_options = []\n\n        if value is not None:\n            value_str = Value.to_str(value)\n\n            value_str = value_str.replace(\'\\n\', \'\')\n            if len(value_str) < _CONFIG_PRINT_LEN:\n                parts.append((f""{value_str}"", Text.value))\n            else:\n                parts.append((f""{value_str[:_CONFIG_PRINT_LEN]}..."", Text.value))\n            parts.append(\'\\t\')\n\n        if option is not None:\n            if len(other_options) == 0:\n                parts.append((option, Text.subtle))\n            else:\n                parts.append((option, Text.none))\n\n        if value is None and option is None:\n            parts.append((""None"", Text.value))\n            parts.append(\'\\t\')\n\n        if len(other_options) > 0:\n            parts.append((\'\\t[\', Text.subtle))\n            for i, opt in enumerate(other_options):\n                if i > 0:\n                    parts.append((\', \', Text.subtle))\n                parts.append(opt)\n            parts.append((\']\', Text.subtle))\n\n        return parts\n\n    def print(self):\n        order = self.calculator.topological_order.copy()\n        order.sort()\n        added = set(order)\n        ignored = set()\n\n        for k in self.parser.types:\n            if k not in added:\n                added.add(k)\n                order.append(k)\n                ignored.add(k)\n\n        logger.log(""Configs:"", Text.heading)\n\n        for k in order:\n            computed = getattr(self.calculator.configs, k, None)\n\n            if k in ignored:\n                parts = self.__print_config(k, is_ignored=True)\n            elif k in self.parser.options:\n                v = self.parser.values[k]\n                opts = self.parser.options[k]\n                lst = list(opts.keys())\n                if v in opts:\n                    lst.remove(v)\n                else:\n                    v = None\n\n                parts = self.__print_config(k,\n                                            value=computed,\n                                            option=v,\n                                            other_options=lst)\n            else:\n                parts = self.__print_config(k, value=computed)\n\n            logger.log(parts)\n\n        logger.log()\n'"
labml/internal/configs/processor_dict.py,0,"b'from pathlib import PurePath\nfrom typing import Dict, Optional, List, Union\n\nfrom labml import logger\nfrom labml.internal import util\nfrom labml.internal.configs.base import Configs\nfrom labml.internal.configs.utils import Value\nfrom labml.logger import Text\n\nfrom .calculator import Calculator\nfrom .parser import Parser\n\n_CONFIG_PRINT_LEN = 40\n\n\nclass ConfigProcessorDict:\n    def __init__(self, configs: Dict[str, any], values: Dict[str, any] = None):\n        self.configs = configs\n        if values is None:\n            values = {}\n        self.values = values\n\n    def __call__(self, run_order: Optional[List[Union[List[str], str]]] = None):\n        self.configs.update(self.values)\n\n    def save(self, configs_path: PurePath):\n        orders = {k: i for i, k in enumerate(self.configs.keys())}\n        configs = {}\n        for k, v in self.configs.items():\n            configs[k] = {\n                \'name\': k,\n                \'type\': str(type(v)),\n                \'value\': Value.to_yaml(v),\n                \'order\': orders.get(k, -1),\n                \'options\': [],\n                \'computed\': Value.to_yaml(v),\n                \'is_hyperparam\': False,\n                \'is_explicitly_specified\': (k in self.values)\n            }\n\n        with open(str(configs_path), ""w"") as file:\n            file.write(util.yaml_dump(configs))\n\n    def get_hyperparams(self):\n        return self.values.copy()\n\n    def __print_config(self, key, *, value=None):\n        parts = [\'\\t\']\n\n        is_hyperparam = key in self.values\n\n        if is_hyperparam:\n            parts.append((key, [Text.key, Text.highlight]))\n        else:\n            parts.append((key, Text.key))\n\n        parts.append((\' = \', Text.subtle))\n\n        value_str = Value.to_str(value)\n\n        value_str = value_str.replace(\'\\n\', \'\')\n        if len(value_str) < _CONFIG_PRINT_LEN:\n            parts.append((f""{value_str}"", Text.value))\n        else:\n            parts.append((f""{value_str[:_CONFIG_PRINT_LEN]}..."", Text.value))\n        parts.append(\'\\t\')\n\n        return parts\n\n    def print(self):\n        order = list(self.configs.keys())\n        order.sort()\n\n        logger.log(""Configs:"", Text.heading)\n\n        for k in order:\n            parts = self.__print_config(k, value=self.configs[k])\n\n            logger.log(parts)\n\n        logger.log()\n'"
labml/internal/configs/utils.py,0,"b'class Value:\n    @staticmethod\n    def is_primitive(value):\n        if value is None:\n            return True\n\n        if type(value) == str:\n            return True\n\n        if type(value) == int:\n            return True\n\n        if type(value) == bool:\n            return True\n\n        if type(value) == list and all([Value.is_primitive(v) for v in value]):\n            return True\n\n        if type(value) == dict and all([Value.is_primitive(v) for v in value.values()]):\n            return True\n\n        return False\n\n    @staticmethod\n    def to_yaml(value):\n        if Value.is_primitive(value):\n            return value\n        else:\n            return Value.to_str(value)\n\n    @staticmethod\n    def to_str(value):\n        if str(value) == Value.default_repr(value):\n            if value.__class__.__module__ == \'__main__\':\n                return value.__class__.__name__\n            else:\n                return f""{value.__class__.__module__}.{value.__class__.__name__}""\n        else:\n            return str(value)\n\n    @staticmethod\n    def default_repr(value):\n        return \'<%s.%s object at %s>\' % (\n            value.__class__.__module__,\n            value.__class__.__name__,\n            hex(id(value))\n        )\n'"
labml/internal/experiment/__init__.py,0,"b'import json\nimport os\nimport pathlib\nimport time\nfrom typing import Optional, List, Set, Dict, Union\n\nimport git\n\nfrom labml import logger, monit\nfrom labml.internal.configs.base import Configs\nfrom labml.internal.configs.processor import ConfigProcessor\nfrom labml.internal.configs.processor_dict import ConfigProcessorDict\nfrom labml.internal.experiment.experiment_run import Run\nfrom labml.internal.lab import lab_singleton\nfrom labml.internal.logger import logger_singleton as logger_internal\nfrom labml.internal.util import is_ipynb\nfrom labml.logger import Text\nfrom labml.utils import get_caller_file\n\n\nclass CheckpointSaver:\n    def save(self, global_step):\n        raise NotImplementedError()\n\n    def load(self, checkpoint_path):\n        raise NotImplementedError()\n\n\nclass Checkpoint(CheckpointSaver):\n    _models: Dict[str, any]\n\n    def __init__(self, path: pathlib.PurePath):\n        self.path = path\n        self._models = {}\n\n    def add_models(self, models: Dict[str, any]):\n        """"""\n        ## Set variable for saving and loading\n        """"""\n        self._models.update(models)\n\n    def save_model(self,\n                   name: str,\n                   model: any,\n                   checkpoint_path: pathlib.Path) -> any:\n        raise NotImplementedError()\n\n    def save(self, global_step):\n        """"""\n        ## Save model as a set of numpy arrays\n        """"""\n\n        checkpoints_path = pathlib.Path(self.path)\n        if not checkpoints_path.exists():\n            checkpoints_path.mkdir()\n\n        checkpoint_path = checkpoints_path / str(global_step)\n        assert not checkpoint_path.exists()\n\n        checkpoint_path.mkdir()\n\n        files = {}\n        for name, model in self._models.items():\n            files[name] = self.save_model(name, model, checkpoint_path)\n\n        # Save header\n        with open(str(checkpoint_path / ""info.json""), ""w"") as f:\n            f.write(json.dumps(files))\n\n    def load_model(self,\n                   name: str,\n                   model: any,\n                   checkpoint_path: pathlib.Path,\n                   info: any):\n        raise NotImplementedError()\n\n    def load(self, checkpoint_path):\n        """"""\n        ## Load model as a set of numpy arrays\n        """"""\n\n        with open(str(checkpoint_path / ""info.json""), ""r"") as f:\n            files = json.loads(f.readline())\n\n        # Load each model\n        for name, model in self._models.items():\n            self.load_model(name, model, checkpoint_path, files[name])\n\n        return True\n\n\nclass Experiment:\n    r""""""\n    Each experiment has different configurations or algorithms.\n    An experiment can have multiple runs.\n\n    Keyword Arguments:\n        name (str, optional): name of the experiment\n        python_file (str, optional): path of the Python file that\n            created the experiment\n        comment (str, optional): a short description of the experiment\n        writers (Set[str], optional): list of writers to write stat to\n        ignore_callers: (Set[str], optional): list of files to ignore when\n            automatically determining ``python_file``\n        tags (Set[str], optional): Set of tags for experiment\n    """"""\n\n    run: Run\n    configs_processor: Optional[ConfigProcessor]\n\n    # whether not to start the experiment if there are uncommitted changes.\n    check_repo_dirty: bool\n    checkpoint_saver: CheckpointSaver\n\n    def __init__(self, *,\n                 name: Optional[str],\n                 python_file: Optional[str],\n                 comment: Optional[str],\n                 writers: Set[str],\n                 ignore_callers: Set[str],\n                 tags: Optional[Set[str]]):\n        if python_file is None:\n            python_file = get_caller_file(ignore_callers)\n\n        if python_file.startswith(\'<ipython\'):\n            assert is_ipynb()\n            if name is None:\n                raise ValueError(""You must specify python_file or experiment name""\n                                 "" when creating an experiment from a python notebook."")\n\n            lab_singleton().set_path(os.getcwd())\n            python_file = \'notebook.ipynb\'\n        else:\n            lab_singleton().set_path(python_file)\n\n            if name is None:\n                file_path = pathlib.PurePath(python_file)\n                name = file_path.stem\n\n        if comment is None:\n            comment = \'\'\n\n        self.name = name\n        self.experiment_path = lab_singleton().experiments / name\n\n        self.check_repo_dirty = lab_singleton().check_repo_dirty\n\n        self.configs_processor = None\n\n        experiment_path = pathlib.Path(self.experiment_path)\n        if not experiment_path.exists():\n            experiment_path.mkdir(parents=True)\n\n        if tags is None:\n            tags = set(name.split(\'_\'))\n\n        self.run = Run.create(\n            experiment_path=self.experiment_path,\n            python_file=python_file,\n            trial_time=time.localtime(),\n            comment=comment,\n            tags=list(tags))\n\n        repo = git.Repo(lab_singleton().path)\n\n        self.run.commit = repo.head.commit.hexsha\n        self.run.commit_message = repo.head.commit.message.strip()\n        self.run.is_dirty = repo.is_dirty()\n        self.run.diff = repo.git.diff()\n\n        logger_internal().reset_writers()\n\n        if \'sqlite\' in writers:\n            from labml.internal.logger.writers import sqlite\n            artifacts_folder = pathlib.Path(self.run.artifacts_folder)\n            if not artifacts_folder.exists():\n                artifacts_folder.mkdir(parents=True)\n            logger_internal().add_writer(\n                sqlite.Writer(self.run.sqlite_path, self.run.artifacts_folder))\n        if \'tensorboard\' in writers:\n            from labml.internal.logger.writers import tensorboard\n            logger_internal().add_writer(tensorboard.Writer(self.run.tensorboard_log_path))\n\n        self.checkpoint_saver = None\n\n    def __print_info_and_check_repo(self):\n        """"""\n        \xf0\x9f\x96\xa8 Print the experiment info and check git repo status\n        """"""\n\n        logger.log()\n        logger.log([\n            (self.name, Text.title),\n            \': \',\n            (str(self.run.uuid), Text.meta)\n        ])\n\n        if self.run.comment != \'\':\n            logger.log([\'\\t\', (self.run.comment, Text.highlight)])\n\n        logger.log([\n            ""\\t""\n            ""[dirty]"" if self.run.is_dirty else ""[clean]"",\n            "": "",\n            (f""\\""{self.run.commit_message.strip()}\\"""", Text.highlight)\n        ])\n\n        if self.run.load_run is not None:\n            logger.log([\n                ""\\t""\n                ""loaded from"",\n                "": "",\n                (f""{self.run.load_run}"", Text.meta2),\n            ])\n\n        # Exit if git repository is dirty\n        if self.check_repo_dirty and self.run.is_dirty:\n            logger.log([(""[FAIL]"", Text.danger),\n                        "" Cannot trial an experiment with uncommitted changes.""])\n            exit(1)\n\n    def _load_checkpoint(self, checkpoint_path: pathlib.PurePath):\n        if self.checkpoint_saver is not None:\n            self.checkpoint_saver.load(checkpoint_path)\n\n    def save_checkpoint(self):\n        if self.checkpoint_saver is not None:\n            self.checkpoint_saver.save(logger_internal().global_step)\n\n    def calc_configs(self,\n                     configs: Configs,\n                     configs_override: Optional[Dict[str, any]],\n                     run_order: Optional[List[Union[List[str], str]]]):\n        self.configs_processor = ConfigProcessor(configs, configs_override)\n        self.configs_processor(run_order)\n\n        logger.log()\n\n    def calc_configs_dict(self,\n                          configs: Dict[str, any],\n                          configs_override: Optional[Dict[str, any]]):\n        self.configs_processor = ConfigProcessorDict(configs, configs_override)\n        self.configs_processor()\n\n        logger.log()\n\n    def __start_from_checkpoint(self, run_uuid: str, checkpoint: Optional[int]):\n        checkpoint_path, global_step = experiment_run.get_last_run_checkpoint(\n            self.experiment_path,\n            run_uuid,\n            checkpoint)\n\n        if global_step is None:\n            return 0\n        else:\n            with monit.section(""Loading checkpoint""):\n                self._load_checkpoint(checkpoint_path)\n            self.run.load_run = run_uuid\n\n        return global_step\n\n    def start(self, *,\n              run_uuid: Optional[str] = None,\n              checkpoint: Optional[int] = None):\n        if run_uuid is not None:\n            if checkpoint is None:\n                checkpoint = -1\n            global_step = self.__start_from_checkpoint(run_uuid, checkpoint)\n        else:\n            global_step = 0\n\n        self.run.start_step = global_step\n        logger_internal().set_start_global_step(global_step)\n\n        self.__print_info_and_check_repo()\n        if self.configs_processor is not None:\n            self.configs_processor.print()\n\n        self.run.save_info()\n\n        if self.configs_processor is not None:\n            self.configs_processor.save(self.run.configs_path)\n\n        logger_internal().save_indicators(self.run.indicators_path)\n        logger_internal().save_artifacts(self.run.artifacts_path)\n        if self.configs_processor:\n            logger_internal().write_h_parameters(self.configs_processor.get_hyperparams())\n\n\n_internal: Optional[Experiment] = None\n\n\ndef experiment_singleton() -> Experiment:\n    global _internal\n\n    assert _internal is not None\n\n    return _internal\n\n\ndef create_experiment(*,\n                      name: Optional[str],\n                      python_file: Optional[str],\n                      comment: Optional[str],\n                      writers: Set[str],\n                      ignore_callers: Set[str],\n                      tags: Optional[Set[str]]):\n    global _internal\n\n    _internal = Experiment(name=name,\n                           python_file=python_file,\n                           comment=comment,\n                           writers=writers,\n                           ignore_callers=ignore_callers,\n                           tags=tags)\n'"
labml/internal/experiment/experiment_run.py,0,"b'import time\nfrom pathlib import Path, PurePath\nfrom typing import List, Dict, Optional, Set\n\nimport numpy as np\n\nfrom labml import logger\nfrom .. import util\nfrom ...logger import Text\n\n\ndef _struct_time_to_time(t: time.struct_time):\n    return f""{t.tm_hour :02}:{t.tm_min :02}:{t.tm_sec :02}""\n\n\ndef _struct_time_to_date(t: time.struct_time):\n    return f""{t.tm_year :04}-{t.tm_mon :02}-{t.tm_mday :02}""\n\n\n_GLOBAL_STEP = \'global_step\'\n\n\ndef _generate_uuid() -> str:\n    from uuid import uuid1\n    return uuid1().hex\n\n\nclass RunInfo:\n    def __init__(self, *,\n                 uuid: str,\n                 python_file: str,\n                 trial_date: str,\n                 trial_time: str,\n                 comment: str,\n                 commit: Optional[str] = None,\n                 commit_message: Optional[str] = None,\n                 is_dirty: bool = True,\n                 experiment_path: PurePath,\n                 start_step: int = 0,\n                 notes: str = \'\',\n                 load_run: Optional[str] = None,\n                 tags: List[str]):\n        self.uuid = uuid\n        self.commit = commit\n        self.is_dirty = is_dirty\n        self.python_file = python_file\n        self.trial_date = trial_date\n        self.trial_time = trial_time\n        self.comment = comment\n        self.commit_message = commit_message\n        self.start_step = start_step\n\n        self.load_run = load_run\n\n        self.experiment_path = experiment_path\n        self.run_path = experiment_path / str(uuid)\n        self.checkpoint_path = self.run_path / ""checkpoints""\n        self.numpy_path = self.run_path / ""numpy""\n\n        self.diff_path = self.run_path / ""source.diff""\n\n        self.sqlite_path = self.run_path / ""sqlite.db""\n        self.artifacts_folder = self.run_path / ""artifacts""\n        self.tensorboard_log_path = self.run_path / ""tensorboard""\n\n        self.info_path = self.run_path / ""run.yaml""\n        self.indicators_path = self.run_path / ""indicators.yaml""\n        self.artifacts_path = self.run_path / ""artifacts.yaml""\n        self.configs_path = self.run_path / ""configs.yaml""\n        self.notes = notes\n        self.tags = tags\n\n    @classmethod\n    def from_dict(cls, experiment_path: PurePath, data: Dict[str, any]):\n        """"""\n        ## Create a new trial from a dictionary\n        """"""\n        params = dict(experiment_path=experiment_path)\n        params.update(data)\n        return cls(**params)\n\n    def to_dict(self):\n        """"""\n        ## Convert trial to a dictionary for saving\n        """"""\n        return dict(\n            uuid=self.uuid,\n            python_file=self.python_file,\n            trial_date=self.trial_date,\n            trial_time=self.trial_time,\n            comment=self.comment,\n            commit=self.commit,\n            commit_message=self.commit_message,\n            is_dirty=self.is_dirty,\n            start_step=self.start_step,\n            notes=self.notes,\n            tags=self.tags,\n            load_run=self.load_run\n        )\n\n    def pretty_print(self) -> List[str]:\n        """"""\n        ## \xf0\x9f\x8e\xa8 Pretty print trial for the python file header\n        """"""\n\n        # Trial information\n        commit_status = ""[dirty]"" if self.is_dirty else ""[clean]""\n        res = [\n            f""{self.trial_date} {self.trial_time}"",\n            self.comment,\n            f""[{commit_status}]: {self.commit_message}"",\n            f""start_step: {self.start_step}""\n        ]\n        return res\n\n    def __str__(self):\n        return f""{self.__class__.__name__}(comment=\\""{self.comment}\\"","" \\\n               f"" commit=\\""{self.commit_message}\\"","" \\\n               f"" date={self.trial_date}, time={self.trial_time})""\n\n    def __repr__(self):\n        return self.__str__()\n\n    def is_after(self, run: \'Run\'):\n        if run.trial_date < self.trial_date:\n            return True\n        elif run.trial_date > self.trial_date:\n            return False\n        elif run.trial_time < self.trial_time:\n            return True\n        else:\n            return False\n\n\nclass Run(RunInfo):\n    """"""\n    # Trial \xf0\x9f\x8f\x83\xe2\x80\x8d\n\n    Every trial in an experiment has same configs.\n    It\'s just multiple runs.\n\n    A new trial will replace checkpoints and TensorBoard summaries\n    or previous trials, you should make a copy if needed.\n    The performance log in `trials.yaml` is not replaced.\n\n    You should run new trials after bug fixes or to see performance is\n     consistent.\n\n    If you want to try different configs, create multiple experiments.\n    """"""\n\n    diff: Optional[str]\n\n    def __init__(self, *,\n                 uuid: str,\n                 python_file: str,\n                 trial_date: str,\n                 trial_time: str,\n                 comment: str,\n                 commit: Optional[str] = None,\n                 commit_message: Optional[str] = None,\n                 is_dirty: bool = True,\n                 experiment_path: PurePath,\n                 start_step: int = 0,\n                 notes: str = \'\',\n                 tags: List[str]):\n        super().__init__(python_file=python_file, trial_date=trial_date, trial_time=trial_time,\n                         comment=comment, uuid=uuid, experiment_path=experiment_path,\n                         commit=commit, commit_message=commit_message, is_dirty=is_dirty,\n                         start_step=start_step, notes=notes, tags=tags)\n\n    @classmethod\n    def create(cls, *,\n               experiment_path: PurePath,\n               python_file: str,\n               trial_time: time.struct_time,\n               comment: str,\n               tags: List[str]):\n        """"""\n        ## Create a new trial\n        """"""\n        return cls(python_file=python_file,\n                   trial_date=_struct_time_to_date(trial_time),\n                   trial_time=_struct_time_to_time(trial_time),\n                   uuid=_generate_uuid(),\n                   experiment_path=experiment_path,\n                   comment=comment,\n                   tags=tags)\n\n    def save_info(self):\n        run_path = Path(self.run_path)\n        if not run_path.exists():\n            run_path.mkdir(parents=True)\n\n        with open(str(self.info_path), ""w"") as file:\n            file.write(util.yaml_dump(self.to_dict()))\n\n        if self.diff is not None:\n            with open(str(self.diff_path), ""w"") as f:\n                f.write(self.diff)\n\n\ndef get_checkpoints(experiment_path: PurePath, run_uuid: str):\n    run_path = experiment_path / run_uuid\n    checkpoint_path = Path(run_path / ""checkpoints"")\n    if not checkpoint_path.exists():\n        return {}\n\n    return {int(child.name) for child in Path(checkpoint_path).iterdir()}\n\n\ndef get_runs(experiment_path: PurePath):\n    return {child.name for child in Path(experiment_path).iterdir()}\n\n\ndef get_last_run(experiment_path: PurePath, runs: Set[str]) -> Run:\n    last: Optional[Run] = None\n    for run_uuid in runs:\n        run_path = experiment_path / run_uuid\n        info_path = run_path / ""run.yaml""\n        with open(str(info_path), ""r"") as file:\n            run = Run.from_dict(experiment_path, util.yaml_load(file.read()))\n            if last is None:\n                last = run\n            elif run.is_after(last):\n                last = run\n\n    return last\n\n\ndef get_run_checkpoint(experiment_path: PurePath,\n                       run_uuid: str, checkpoint: int = -1):\n    checkpoints = get_checkpoints(experiment_path, run_uuid)\n    if len(checkpoints) == 0:\n        return None\n\n    if checkpoint < 0:\n        required_ci = np.max(list(checkpoints)) + checkpoint + 1\n    else:\n        required_ci = checkpoint\n\n    for ci in range(required_ci, -1, -1):\n        if ci not in checkpoints:\n            continue\n\n        return ci\n\n\ndef get_last_run_checkpoint(experiment_path: PurePath,\n                            run_uuid: str,\n                            checkpoint: int = -1):\n    checkpoint = get_run_checkpoint(experiment_path, run_uuid,\n                                    checkpoint)\n\n    if checkpoint is None:\n        logger.log(""Couldn\'t find a previous run/checkpoint"")\n        return None, None\n\n    logger.log([""Selected "",\n                (""run"", Text.key),\n                "" = "",\n                (run_uuid, Text.value),\n                "" "",\n                (""checkpoint"", Text.key),\n                "" = "",\n                (checkpoint, Text.value)])\n\n    run_path = experiment_path / str(run_uuid)\n    checkpoint_path = run_path / ""checkpoints""\n    return checkpoint_path / str(checkpoint), checkpoint\n'"
labml/internal/experiment/pytorch.py,11,"b'import pathlib\nfrom typing import Dict\n\nimport numpy as np\nimport torch.nn\n\nfrom . import Checkpoint, experiment_singleton\n\n\nclass NumpyCheckpoint(Checkpoint):\n    """"""\n    Deprecated: left for backward compatibility only\n    Remove around August 2020\n    """"""\n\n    def save_model(self,\n                   name: str,\n                   model: torch.nn.Module,\n                   checkpoint_path: pathlib.Path) -> any:\n        state: Dict[str, torch.Tensor] = model.state_dict()\n        files = {}\n        for key, tensor in state.items():\n            if key == ""_metadata"":\n                continue\n\n            file_name = f""{name}_{key}.npy""\n            files[key] = file_name\n\n            np.save(str(checkpoint_path / file_name), tensor.cpu().numpy())\n\n        return files\n\n    def load_model(self,\n                   name: str,\n                   model: torch.nn.Module,\n                   checkpoint_path: pathlib.Path,\n                   info: any):\n        state: Dict[str, torch.Tensor] = model.state_dict()\n        for key, tensor in state.items():\n            file_name = info[key]\n            saved = np.load(str(checkpoint_path / file_name))\n            saved = torch.from_numpy(saved).to(tensor.device)\n            state[key] = saved\n\n        model.load_state_dict(state)\n\n\nclass PyTorchCheckpoint(Checkpoint):\n    def save_model(self,\n                   name: str,\n                   model: torch.nn.Module,\n                   checkpoint_path: pathlib.Path) -> any:\n        state = model.state_dict()\n        file_name = f""{name}.pth""\n        torch.save(state, str(checkpoint_path / file_name))\n        return file_name\n\n    def load_model(self,\n                   name: str,\n                   model: torch.nn.Module,\n                   checkpoint_path: pathlib.Path,\n                   info: any):\n        file_name: str = info\n        state = torch.load(str(checkpoint_path / file_name))\n\n        model.load_state_dict(state)\n\n\ndef add_models(models: Dict[str, torch.nn.Module]):\n    exp = experiment_singleton()\n    if exp.checkpoint_saver is None:\n        exp.checkpoint_saver = PyTorchCheckpoint(exp.run.checkpoint_path)\n\n    exp.checkpoint_saver.add_models(models)\n'"
labml/internal/experiment/sklearn.py,0,"b'import pathlib\nfrom typing import Dict\nfrom joblib import dump, load\n\nfrom . import Checkpoint, experiment_singleton\n\n\nclass SkLearnCheckpoint(Checkpoint):\n    def save_model(self,\n                   name: str,\n                   model: any,\n                   checkpoint_path: pathlib.Path) -> any:\n        file_name = f""{name}.pth""\n        dump(model, str(checkpoint_path / file_name))\n        return file_name\n\n    def load_model(self,\n                   name: str,\n                   model: any,\n                   checkpoint_path: pathlib.Path,\n                   info: any):\n        file_name: str = info\n\n        return load(str(checkpoint_path / file_name))\n\n\ndef add_models(models: Dict[str, any]):\n    exp = experiment_singleton()\n    if exp.checkpoint_saver is None:\n        exp.checkpoint_saver = SkLearnCheckpoint(exp.run.checkpoint_path)\n\n    exp.checkpoint_saver.add_models(models)\n'"
labml/internal/logger/__init__.py,0,"b'import typing\nfrom pathlib import PurePath\nfrom typing import Optional, List, Union, Tuple, Dict\n\nfrom labml.internal.logger.store.artifacts import Artifact\nfrom labml.internal.logger.store.indicators import Indicator\nfrom labml.internal.util.colors import StyleCode\nfrom .destinations.factory import create_destination\nfrom .inspect import Inspect\nfrom .iterator import Iterator\nfrom .loop import Loop\nfrom .sections import Section, OuterSection\nfrom .store import Store\nfrom .writers import Writer, ScreenWriter\nfrom ...logger import Text\n\n\nclass Logger:\n    """"""\n    This handles the interactions among sections, loop and store\n    """"""\n\n    def __init__(self):\n        self.__store = Store()\n        self.__writers: List[Writer] = []\n\n        self.__loop: Optional[Loop] = None\n        self.__sections: List[Section] = []\n\n        self.__indicators_print = []\n\n        self.__screen_writer = ScreenWriter()\n        self.__writers.append(self.__screen_writer)\n\n        self.__start_global_step: Optional[int] = None\n        self.__global_step: Optional[int] = None\n        self.__last_global_step: Optional[int] = None\n\n        self.__destination = create_destination()\n        self.__inspect = Inspect(self)\n\n    @property\n    def global_step(self) -> int:\n        if self.__global_step is not None:\n            return self.__global_step\n\n        global_step = 0\n        if self.__start_global_step is not None:\n            global_step = self.__start_global_step\n\n        if self.__is_looping():\n            return global_step + self.__loop.counter\n\n        if self.__last_global_step is not None:\n            return self.__last_global_step\n\n        return global_step\n\n    def add_writer(self, writer: Writer):\n        self.__writers.append(writer)\n\n    def reset_writers(self):\n        self.__writers = []\n        self.__writers.append(self.__screen_writer)\n\n    def log(self, parts: List[Union[str, Tuple[str, Optional[StyleCode]]]], *,\n            is_new_line=True):\n        self.__destination.log(parts, is_new_line=is_new_line)\n\n    def reset_store(self):\n        self.__store = Store()\n\n    def add_indicator(self, indicator: Indicator):\n        self.__store.add_indicator(indicator)\n\n    def add_artifact(self, artifact: Artifact):\n        self.__store.add_artifact(artifact)\n\n    def save_indicators(self, file: PurePath):\n        self.__store.save_indicators(file)\n\n    def save_artifacts(self, file: PurePath):\n        self.__store.save_artifacts(file)\n\n    def store(self, key: str, value: any):\n        self.__store.store(key, value)\n\n    def store_namespace(self, name: str):\n        return self.__store.create_namespace(name)\n\n    def set_global_step(self, global_step: Optional[int]):\n        self.__global_step = global_step\n\n    def set_start_global_step(self, global_step: Optional[int]):\n        self.__start_global_step = global_step\n\n    def add_global_step(self, increment_global_step: int = 1):\n        if self.__global_step is None:\n            if self.__start_global_step is not None:\n                self.__global_step = self.__start_global_step\n            else:\n                self.__global_step = 0\n\n        self.__global_step += increment_global_step\n\n    def __is_looping(self):\n        if self.__loop is not None and self.__loop.is_started:\n            return True\n        else:\n            return False\n\n    def write_h_parameters(self, hparams: Dict[str, any]):\n        for w in self.__writers:\n            w.write_h_parameters(hparams)\n\n    def write(self):\n        global_step = self.global_step\n\n        for w in self.__writers:\n            if w != self.__screen_writer:\n                self.__store.write(w, global_step)\n        self.__indicators_print = self.__store.write(self.__screen_writer, global_step)\n        self.__store.clear()\n\n        parts = [(f""{self.global_step :8,}:  "", Text.highlight)]\n        if self.__is_looping():\n            self.__log_looping_line()\n        else:\n            parts += self.__indicators_print\n            self.log(parts, is_new_line=False)\n\n    def iterate(self, name, iterable: Union[typing.Iterable, typing.Sized, int],\n                total_steps: Optional[int], *,\n                is_silent: bool,\n                is_timed: bool):\n        return Iterator(logger=self,\n                        name=name,\n                        iterable=iterable,\n                        is_silent=is_silent,\n                        is_timed=is_timed,\n                        total_steps=total_steps,\n                        is_enumerate=False)\n\n    def enum(self, name, iterable: typing.Sized, *,\n             is_silent: bool,\n             is_timed: bool):\n        return Iterator(logger=self,\n                        name=name,\n                        iterable=iterable,\n                        is_silent=is_silent,\n                        is_timed=is_timed,\n                        total_steps=None,\n                        is_enumerate=True)\n\n    def section(self, name, *,\n                is_silent: bool,\n                is_timed: bool,\n                is_partial: bool,\n                is_new_line: bool,\n                total_steps: float):\n\n        if self.__is_looping():\n            if len(self.__sections) != 0:\n                raise RuntimeError(""No nested sections within loop"")\n\n            section = self.__loop.get_section(name=name,\n                                              is_silent=is_silent,\n                                              is_timed=is_timed,\n                                              is_partial=is_partial,\n                                              total_steps=total_steps)\n            self.__sections.append(section)\n        else:\n            self.__sections.append(OuterSection(logger=self,\n                                                name=name,\n                                                is_silent=is_silent,\n                                                is_timed=is_timed,\n                                                is_partial=is_partial,\n                                                is_new_line=is_new_line,\n                                                total_steps=total_steps,\n                                                level=len(self.__sections)))\n\n        return self.__sections[-1]\n\n    def progress(self, steps: float):\n        if len(self.__sections) == 0:\n            raise RuntimeError(""You must be within a section to report progress"")\n\n        if self.__sections[-1].progress(steps):\n            self.__log_line()\n\n    def set_successful(self, is_successful=True):\n        if len(self.__sections) == 0:\n            raise RuntimeError(""You must be within a section to report success"")\n\n        self.__sections[-1].is_successful = is_successful\n        self.__log_line()\n\n    def loop(self, iterator_: range, *,\n             is_print_iteration_time: bool):\n        if len(self.__sections) != 0:\n            raise RuntimeError(""Cannot start a loop within a section"")\n\n        self.__loop = Loop(iterator=iterator_, logger=self,\n                           is_print_iteration_time=is_print_iteration_time)\n        return self.__loop\n\n    def finish_loop(self):\n        if len(self.__sections) != 0:\n            raise RuntimeError(""Cannot be within a section when finishing the loop"")\n        self.__last_global_step = self.global_step\n        self.__loop = None\n        for w in self.__writers:\n            w.flush()\n\n    def section_enter(self, section):\n        if len(self.__sections) == 0:\n            raise RuntimeError(""Entering a section without creating a section.\\n""\n                               ""Always use logger.section to create a section"")\n\n        if section is not self.__sections[-1]:\n            raise RuntimeError(""Entering a section other than the one last_created\\n""\n                               ""Always user with logger.section(...):"")\n\n        if len(self.__sections) > 1 and not self.__sections[-2].is_parented:\n            self.__sections[-2].make_parent()\n            if not self.__sections[-1].is_silent:\n                self.log([])\n\n        self.__log_line()\n\n    def __log_looping_line(self):\n        parts = [(f""{self.global_step :8,}:  "", Text.highlight)]\n        parts += self.__loop.log_sections()\n        parts += self.__indicators_print\n        parts += self.__loop.log_progress()\n\n        self.log(parts, is_new_line=False)\n\n    def __log_line(self):\n        if self.__is_looping():\n            self.__log_looping_line()\n            return\n\n        if len(self.__sections) == 0:\n            return\n\n        parts = self.__sections[-1].log()\n        if parts is None:\n            return\n\n        self.log(parts, is_new_line=False)\n\n    def section_exit(self, section):\n        if len(self.__sections) == 0:\n            raise RuntimeError(""Impossible"")\n\n        if section is not self.__sections[-1]:\n            raise RuntimeError(""Impossible"")\n\n        self.__log_line()\n        self.__sections.pop(-1)\n\n    def info(self, *args, **kwargs):\n        self.__inspect.info(*args, **kwargs)\n\n\n_internal: Optional[Logger] = None\n\n\ndef logger_singleton() -> Logger:\n    global _internal\n    if _internal is None:\n        _internal = Logger()\n\n    return _internal\n'"
labml/internal/logger/inspect.py,0,"b'from typing import TYPE_CHECKING, List, Tuple\n\nfrom ...logger import Text\n\nif TYPE_CHECKING:\n    from . import Logger\n\n\nclass Inspect:\n    def __init__(self, logger: \'Logger\'):\n        self.__logger = logger\n\n    def _log_key_value(self, items: List[Tuple[any, any]], is_show_count=True):\n        max_key_len = 0\n        for k, v in items:\n            max_key_len = max(max_key_len, len(str(k)))\n\n        count = 0\n        for k, v in items:\n            count += 1\n            spaces = "" "" * (max_key_len - len(str(k)))\n            s = str(v)\n            if len(s) > 80:\n                s = f""{s[:80]} ...""\n            self.__logger.log([(f""{spaces}{k}: "", Text.key),\n                               (s, Text.value)])\n\n        if is_show_count:\n            self.__logger.log([\n                ""Total "",\n                (str(count), Text.meta),\n                "" item(s)""])\n\n    def info(self, *args, **kwargs):\n        if len(args) == 0:\n            self._log_key_value([(k, v) for k, v in kwargs.items()], False)\n        elif len(args) == 1:\n            assert len(kwargs.keys()) == 0\n            arg = args[0]\n            if type(arg) == list:\n                self._log_key_value([(i, v) for i, v in enumerate(arg)])\n            elif type(arg) == dict:\n                self._log_key_value([(k, v) for k, v in arg.items()])\n            else:\n                self.__logger.log([str(arg)])\n        else:\n            assert len(kwargs.keys()) == 0\n            self._log_key_value([(i, v) for i, v in enumerate(args)], False)\n'"
labml/internal/logger/iterator.py,0,"b'import typing\nfrom typing import Optional, Iterable, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from labml.internal.logger import Logger\nfrom labml.internal.logger.sections import Section\n\n\nclass Iterator:\n    r""""""\n        Note:\n            You should use :meth:`labml.logger.iterate`\n            or :meth:`labml.monit.enum` to create iterators\n            and enumerators.\n    """"""\n\n    _section: Optional[Section]\n\n    def __init__(self, *,\n                 logger: \'Logger\',\n                 name: str,\n                 iterable: typing.Union[Iterable, typing.Sized, int],\n                 is_silent: bool,\n                 is_timed: bool,\n                 total_steps: Optional[int],\n                 is_enumerate: bool):\n        if is_enumerate:\n            total_steps = len(iterable)\n            iterable = enumerate(iterable)\n        if type(iterable) is int:\n            total_steps = iterable\n            iterable = range(total_steps)\n        if total_steps is None:\n            sized: typing.Sized = iterable\n            total_steps = len(sized)\n\n        self._logger = logger\n        self._name = name\n        self._iterable: Iterable = iterable\n        self._iterator = Optional[typing.Iterator]\n        self._total_steps = total_steps\n        self._section = None\n        self._is_silent = is_silent\n        self._is_timed = is_timed\n        self._counter = -1\n\n    def get_estimated_time(self) -> float:\n        if self._section:\n            return self._section.get_estimated_time()\n        else:\n            return 0\n\n    def __iter__(self):\n        self._section = self._logger.section(\n            self._name,\n            is_silent=self._is_silent,\n            is_timed=self._is_timed,\n            is_partial=False,\n            total_steps=self._total_steps,\n            is_new_line=True)\n        self._iterator = iter(self._iterable)\n        self._section.__enter__()\n\n        return self\n\n    def __next__(self):\n        try:\n            self._counter += 1\n            self._logger.progress(self._counter)\n            next_value = next(self._iterator)\n        except StopIteration as e:\n            self._section.__exit__(None, None, None)\n            raise e\n\n        return next_value\n'"
labml/internal/logger/loop.py,0,"b'import math\nimport time\nfrom typing import Optional, Dict, TYPE_CHECKING\n\nfrom labml.internal.logger.sections import LoopingSection\nfrom ...logger import Text\n\nif TYPE_CHECKING:\n    from . import Logger\n\n\nclass Loop:\n    def __init__(self, iterator: range, *,\n                 logger: \'Logger\',\n                 is_print_iteration_time: bool):\n        """"""\n        Creates an iterator with a range `iterator`.\n\n        See example for usage.\n        """"""\n        self.iterator = iterator\n        self._start_time = 0.\n        self._iter_start_time = 0.\n        self._init_time = 0.\n        self._iter_time = 0.\n        self._beta_pow = 1.\n        self._beta = 0.9\n        self.steps = len(iterator)\n        self.counter = 0\n        self.logger = logger\n        self.__global_step: Optional[int] = None\n        self.__looping_sections: Dict[str, LoopingSection] = {}\n        self._is_print_iteration_time = is_print_iteration_time\n        self.is_started = False\n\n    def __iter__(self):\n        self.is_started = True\n        self.iterator_iter = iter(self.iterator)\n        self._start_time = time.time()\n        self._init_time = 0.\n        self._iter_time = 0.\n        self.counter = 0\n        return self\n\n    def __next__(self):\n        try:\n            next_value = next(self.iterator_iter)\n        except StopIteration as e:\n            self.logger.finish_loop()\n            raise e\n\n        now = time.time()\n        if self.counter == 0:\n            self.__init_time = now - self._start_time\n        else:\n            self._beta_pow *= self._beta\n            self._iter_time *= self._beta\n            self._iter_time += (1 - self._beta) * (now - self._iter_start_time)\n\n        self._iter_start_time = now\n\n        self.counter = next_value\n\n        return next_value\n\n    def log_progress(self):\n        """"""\n        Show progress\n        """"""\n        now = time.time()\n        spent = now - self._start_time\n\n        if not math.isclose(self._iter_time, 0.):\n            estimate = self._iter_time / (1 - self._beta_pow)\n        else:\n            estimate = sum([s.get_estimated_time()\n                            for s in self.__looping_sections.values()])\n\n        total_time = estimate * self.steps + self._init_time\n        total_time = max(total_time, spent)\n        remain = total_time - spent\n\n        remain /= 60\n        spent /= 60\n        estimate *= 1000\n\n        spent_h = int(spent // 60)\n        spent_m = int(spent % 60)\n        remain_h = int(remain // 60)\n        remain_m = int(remain % 60)\n\n        to_print = [(""  "", None)]\n        if self._is_print_iteration_time:\n            to_print.append((f""{estimate:,.0f}ms"", Text.meta))\n        to_print.append((f""{spent_h:3d}:{spent_m:02d}m/{remain_h:3d}:{remain_m:02d}m  "",\n                         Text.meta2))\n\n        return to_print\n\n    def get_section(self, *, name: str,\n                    is_silent: bool,\n                    is_timed: bool,\n                    is_partial: bool,\n                    total_steps: float):\n        if name not in self.__looping_sections:\n            self.__looping_sections[name] = LoopingSection(logger=self.logger,\n                                                           name=name,\n                                                           is_silent=is_silent,\n                                                           is_timed=is_timed,\n                                                           is_partial=is_partial,\n                                                           total_steps=total_steps)\n        return self.__looping_sections[name]\n\n    def log_sections(self):\n        parts = []\n        for name, section in self.__looping_sections.items():\n            parts += section.log()\n\n        return parts\n'"
labml/internal/logger/sections.py,0,"b'import math\nimport time\nfrom typing import TYPE_CHECKING\n\nfrom ...logger import Text\n\nif TYPE_CHECKING:\n    from . import Logger\n\n\nclass Section:\n    r""""""\n        Note:\n            You should use :meth:`labml.logger.section` to create sections\n    """"""\n\n    def __init__(self, *,\n                 logger: \'Logger\',\n                 name: str,\n                 is_silent: bool,\n                 is_timed: bool,\n                 is_partial: bool,\n                 total_steps: float):\n        self._logger = logger\n        self._name = name\n        self.is_silent = is_silent\n        self._is_timed = is_timed\n        self._is_partial = is_partial\n        self._total_steps = total_steps\n\n        self._state = \'none\'\n        self._has_entered_ever = False\n\n        self._start_time = 0\n        self._end_time = -1\n        self._progress = 0.\n        self._start_progress = 0\n        self._end_progress = 0\n        self._is_parented = False\n\n        self.is_successful = True\n\n    def get_estimated_time(self) -> float:\n        raise NotImplementedError()\n\n    def __enter__(self):\n        self._state = \'entered\'\n        self._has_entered_ever = True\n        self.is_successful = True\n\n        if not self._is_partial:\n            self._progress = 0\n\n        self._start_progress = self._progress\n\n        if self._is_timed:\n            self._start_time = time.time()\n\n        self._logger.section_enter(self)\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self._state = \'exited\'\n        if self._is_timed:\n            self._end_time = time.time()\n\n        if not self._is_partial:\n            self._progress = 1.\n\n        self._end_progress = self._progress\n\n        self._logger.section_exit(self)\n\n    def log(self):\n        raise NotImplementedError()\n\n    def progress(self, steps):\n        old_progress = self._progress\n        self._progress = steps / self._total_steps\n\n        if self.is_silent:\n            return False\n\n        if math.floor(self._progress * 100) != math.floor(old_progress * 100):\n            return True\n        else:\n            return False\n\n    @property\n    def is_parented(self):\n        return self._is_parented\n\n    def make_parent(self):\n        self._is_parented = True\n\n\nclass OuterSection(Section):\n    def __init__(self, *,\n                 logger: \'Logger\',\n                 name: str,\n                 is_silent: bool,\n                 is_timed: bool,\n                 is_partial: bool,\n                 is_new_line: bool,\n                 total_steps: float,\n                 level: int):\n        if is_partial:\n            raise RuntimeError(""Only sections within the loop can be partial."")\n\n        super().__init__(logger=logger,\n                         name=name,\n                         is_silent=is_silent,\n                         is_timed=is_timed,\n                         is_partial=is_partial,\n                         total_steps=total_steps)\n\n        self._level = level\n        self._is_new_line = is_new_line\n\n    def get_estimated_time(self):\n        if self._state is \'entered\':\n            if self._progress == 0.:\n                return time.time() - self._start_time\n            else:\n                return (time.time() - self._start_time) / self._progress\n        else:\n            return self._end_time - self._start_time\n\n    def log(self):\n        if self.is_silent:\n            return\n\n        if self._state is \'none\':\n            return\n\n        parts = [(""  "" * self._level + f""{self._name}"", None)]\n\n        if self._state == \'entered\':\n            if self._progress == 0.:\n                parts.append(""..."")\n            else:\n                parts.append((f"" {math.floor(self._progress * 100) :4.0f}%"", Text.meta2))\n        else:\n            if self.is_successful:\n                parts.append((""...[DONE]"", Text.success))\n            else:\n                parts.append((""...[FAIL]"", Text.danger))\n\n        if self._is_timed and self._progress > 0.:\n            duration_ms = 1000 * self.get_estimated_time()\n            parts.append((f""\\t{duration_ms :,.2f}ms"",\n                          Text.meta))\n\n        if self._state != \'entered\' and self._is_new_line:\n            parts.append((""\\n"", None))\n\n        return parts\n\n\nclass LoopingSection(Section):\n    def __init__(self, *,\n                 logger: \'Logger\',\n                 name: str,\n                 is_silent: bool,\n                 is_timed: bool,\n                 is_partial: bool,\n                 total_steps: float):\n        super().__init__(logger=logger,\n                         name=name,\n                         is_silent=is_silent,\n                         is_timed=is_timed,\n                         is_partial=is_partial,\n                         total_steps=total_steps)\n        self._beta_pow = 1.\n        self._beta = 0.9\n        self._estimated_time = 0.\n        self._time_length = 7\n        self._last_end_time = -1.\n        self._last_start_time = -1.\n        self._last_step_time = 0.\n\n    def get_estimated_time(self):\n        et = self._estimated_time * self._beta\n        et += (1 - self._beta) * self._last_step_time\n        return et / (1 - self._beta_pow * self._beta)\n\n    def _calc_estimated_time(self):\n        if self._state != \'entered\':\n            if self._last_end_time == self._end_time:\n                return self.get_estimated_time()\n            end_time = self._end_time\n            end_progress = self._end_progress\n            self._last_end_time = self._end_time\n        else:\n            end_time = time.time()\n            end_progress = self._progress\n\n        if end_progress - self._start_progress < 1e-6:\n            return self.get_estimated_time()\n\n        current_estimate = ((end_time - self._start_time) /\n                            (end_progress - self._start_progress))\n\n        if self._last_start_time == self._start_time:\n            # print(current_estimate)\n            self._last_step_time = current_estimate\n        else:\n            if self._last_step_time >= 0.:\n                self._beta_pow *= self._beta\n                self._estimated_time *= self._beta\n                self._estimated_time += (1 - self._beta) * self._last_step_time\n            # print(self._last_step_time, current_estimate)\n            self._last_step_time = current_estimate\n            self._last_start_time = self._start_time\n\n        return self.get_estimated_time()\n\n    def log(self):\n        if self.is_silent:\n            return []\n\n        if self._state == \'none\':\n            return []\n\n        parts = [(f""{self._name}:"", None)]\n        color = None\n\n        if not self.is_successful:\n            color = Text.danger\n\n        if self._progress == 0.:\n            parts.append((""  ..."", Text.subtle))\n        else:\n            parts.append((f""{math.floor(self._progress * 100) :4.0f}%"",\n                          color or Text.subtle))\n\n        if self._is_timed:\n            duration_ms = 1000 * self._calc_estimated_time()\n            s = f"" {duration_ms:,.0f}ms  ""\n            tl = len(s)\n            if tl > self._time_length:\n                self._time_length = tl\n            else:\n                s = ("" "" * (self._time_length - tl)) + s\n\n            parts.append((s, color or Text.meta))\n\n        return parts\n'"
labml/internal/util/__init__.py,0,"b""import pathlib\nimport random\nimport string\nfrom typing import Dict, Callable\n\nimport yaml\n\nget_ipython: Callable\n\n\ndef yaml_load(s: str):\n    return yaml.load(s, Loader=yaml.FullLoader)\n\n\ndef yaml_dump(obj: any):\n    return yaml.dump(obj, default_flow_style=False)\n\n\ndef rm_tree(path_to_remove: pathlib.Path):\n    if path_to_remove.is_dir():\n        for f in path_to_remove.iterdir():\n            if f.is_dir():\n                rm_tree(f)\n            else:\n                f.unlink()\n        path_to_remove.rmdir()\n    else:\n        path_to_remove.unlink()\n\n\ndef random_string(length=10):\n    letters = string.ascii_lowercase\n    return ''.join(random.choice(letters) for _ in range(length))\n\n\ndef is_ipynb():\n    try:\n        cfg = get_ipython().config\n        if cfg['IPKernelApp'] is None:\n            return False\n\n        app: Dict = cfg['IPKernelApp']\n        if len(app.keys()) > 0:\n            return True\n        else:\n            return False\n    except NameError:\n        return False\n\n\nif __name__ == '__main__':\n    print(is_ipynb())\n"""
labml/internal/util/colors.py,0,"b'""""""\nConsole colors\n""""""\nfrom enum import Enum\n\n_ANSI_CODES = dict(\n    normal=0,\n    bold=1,\n    light=2,  # - PyCharm/Jupyter\n\n    italic=3,  # - PyCharm/Jupyter\n    underline=4,\n\n    highlight=7,  # Changes background in PyCharm/Terminal\n\n    # Colors\n    black=30,\n    red=31,\n    green=32,\n    orange=33,\n    blue=34,\n    purple=35,\n    cyan=36,\n    white=37,\n\n    # Background [Not used anymore]\n    bg_black=40,\n    bg_red=41,\n    bg_green=42,\n    bg_orange=43,\n    bg_blue=44,\n    bg_purple=45,\n    bg_cyan=46,\n    bg_white=47,\n\n    # Bright Colors [Not used anymore]\n    bright_black=90,\n    bright_red=91,\n    bright_green=92,\n    bright_orange=93,\n    bright_blue=94,\n    bright_purple=95,\n    bright_cyan=96,\n    bright_white=97,\n\n    # Bright Background Colors [Not used anymore]\n    bg_bright_black=100,\n    bg_bright_red=101,\n    bg_bright_green=102,\n    bg_bright_orange=103,\n    bg_bright_blue=104,\n    bg_bright_purple=105,\n    bg_bright_cyan=106,\n    bg_bright_white=107\n)\n\nANSI_RESET = ""\\33[0m""\n\n_HTML_STYLES = dict(\n    normal=(\'\', \'\'),\n    bold=(\'<strong>\', \'</strong>\'),\n    underline=(\'<span style=""text-decoration: underline"">\', \'</span>\'),\n    light=(\'\', \'\'),\n\n    # Colors\n    black=(\'<span style=""color: #3E424D"">\', \'</span>\'),\n    red=(\'<span style=""color: #E75C58"">\', \'</span>\'),\n    green=(\'<span style=""color: #00A250"">\', \'</span>\'),\n    orange=(\'<span style=""color: #DDB62B"">\', \'</span>\'),\n    blue=(\'<span style=""color: #208FFB"">\', \'</span>\'),\n    purple=(\'<span style=""color: #D160C4"">\', \'</span>\'),\n    cyan=(\'<span style=""color: #60C6C8"">\', \'</span>\'),\n    white=(\'<span style=""color: #C5C1B4"">\', \'</span>\')\n)\n\n\nclass StyleCode(Enum):\n    r""""""\n    This is the base class for different style enumerations\n    """"""\n\n    def ansi(self):\n        if self.value is None:\n            return f""\\33[{_ANSI_CODES[\'normal\']}m""\n        elif type(self.value) == str:\n            return f""\\33[{_ANSI_CODES[self.value]}m""\n        elif type(self.value) == list:\n            return \'\'.join([f""\\33[{_ANSI_CODES[v]}m"" for v in self.value])\n        else:\n            assert False\n\n    def html_open(self):\n        if self.value is None:\n            return """"\n        elif type(self.value) == str:\n            return _HTML_STYLES[self.value][0]\n        elif type(self.value) == list:\n            return \'\'.join([_HTML_STYLES[v][0] for v in self.value])\n        else:\n            assert False\n\n    def html_close(self):\n        if self.value is None:\n            return """"\n        elif type(self.value) == str:\n            return _HTML_STYLES[self.value][1]\n        elif type(self.value) == list:\n            return \'\'.join([_HTML_STYLES[v][1] for v in reversed(self.value)])\n        else:\n            assert False\n\n\ndef _test():\n    for i in [0, 38, 48]:\n        for j in [5]:\n            for k in range(16):\n                print(""\\33[{};{};{}m{:02d},{},{:03d}\\33[0m\\t"".format(i, j, k, i, j, k),\n                      end=\'\')\n                if (k + 1) % 6 == 0:\n                    print("""")\n            print("""")\n\n    for i in range(0, 128):\n        print(f""\\33[{i}m{i :03d}\\33[0m "", end=\'\')\n        if (i + 1) % 10 == 0:\n            print("""")\n\n    print("""")\n\n\nif __name__ == ""__main__"":\n    _test()\n'"
labml/utils/data/__init__.py,0,b''
labml/utils/data/pytorch.py,3,"b""import pandas as pd\nimport torch\n\nfrom typing import Callable, List\n\nfrom torch.utils.data import TensorDataset\n\n\nclass BaseDataset(TensorDataset):\n    data: pd\n    y_cols: List\n    x_cols: List\n    transform: Callable\n    test_fraction: float = 0.0\n    train: bool\n\n\nclass CsvDataset(BaseDataset):\n    def __init__(self, file_path: str, y_cols: List, x_cols: List, train: bool = True,\n                 transform: Callable = lambda: None, test_fraction: float = 0.0, nrows: int = None):\n        self.__dict__.update(**vars())\n\n        self.data = pd.read_csv(**{'filepath_or_buffer': file_path, 'nrows': nrows})\n\n        data_length = len(self.data)\n\n        self.test_size = int(data_length * self.test_fraction)\n        self.train_size = data_length - self.test_size\n\n        self.train_data = self.data.iloc[0:self.train_size]\n        self.test_data = self.data.iloc[self.train_size:]\n\n        if train:\n            x, y = torch.tensor(self.train_data[self.x_cols].values), torch.tensor(self.train_data[self.y_cols].values)\n        else:\n            x, y = torch.tensor(self.test_data[self.x_cols].values), torch.tensor(self.test_data[self.y_cols].values)\n\n        super(CsvDataset, self).__init__(x, y)\n"""
labml/helpers/pytorch/datasets/__init__.py,0,b''
labml/helpers/pytorch/datasets/cifar10.py,1,"b""from torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nfrom labml import lab\nfrom labml.configs import BaseConfigs\n\n\ndef _dataset(is_train, transform):\n    return datasets.CIFAR10(str(lab.get_data_path()),\n                            train=is_train,\n                            download=True,\n                            transform=transform)\n\n\nclass CIFAR10Configs(BaseConfigs):\n    dataset_name: str = 'CIFAR10'\n    dataset_transforms: transforms.Compose\n    train_dataset: datasets.CIFAR10\n    valid_dataset: datasets.CIFAR10\n\n    train_loader: DataLoader\n    valid_loader: DataLoader\n\n    train_batch_size: int = 64\n    valid_batch_size: int = 1024\n\n    train_loader_shuffle: bool = True\n    valid_loader_shuffle: bool = False\n\n\n@CIFAR10Configs.calc(CIFAR10Configs.dataset_transforms)\ndef cifar10_transforms():\n    return transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n\n@CIFAR10Configs.calc(CIFAR10Configs.train_dataset)\ndef cifar10_train_dataset(c: CIFAR10Configs):\n    return _dataset(True, c.dataset_transforms)\n\n\n@CIFAR10Configs.calc(CIFAR10Configs.valid_dataset)\ndef cifar10_valid_dataset(c: CIFAR10Configs):\n    return _dataset(False, c.dataset_transforms)\n\n\n@CIFAR10Configs.calc(CIFAR10Configs.train_loader)\ndef cifar10_train_loader(c: CIFAR10Configs):\n    return DataLoader(c.train_dataset,\n                      batch_size=c.train_batch_size,\n                      shuffle=c.train_loader_shuffle)\n\n\n@CIFAR10Configs.calc(CIFAR10Configs.valid_loader)\ndef cifar10_valid_loader(c: CIFAR10Configs):\n    return DataLoader(c.valid_dataset,\n                      batch_size=c.valid_batch_size,\n                      shuffle=c.valid_loader_shuffle)\n\n\nCIFAR10Configs.aggregate(CIFAR10Configs.dataset_name, 'CIFAR10',\n                       (CIFAR10Configs.dataset_transforms, 'cifar10_transforms'),\n                       (CIFAR10Configs.train_dataset, 'cifar10_train_dataset'),\n                       (CIFAR10Configs.valid_dataset, 'cifar10_valid_dataset'),\n                       (CIFAR10Configs.train_loader, 'cifar10_train_loader'),\n                       (CIFAR10Configs.valid_loader, 'cifar10_valid_loader'))\n"""
labml/helpers/pytorch/datasets/mnist.py,1,"b""from torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nfrom labml import lab\nfrom labml.configs import BaseConfigs\n\n\ndef _dataset(is_train, transform):\n    return datasets.MNIST(str(lab.get_data_path()),\n                          train=is_train,\n                          download=True,\n                          transform=transform)\n\n\nclass MNISTConfigs(BaseConfigs):\n    dataset_name: str = 'MNIST'\n    dataset_transforms: transforms.Compose\n    train_dataset: datasets.MNIST\n    valid_dataset: datasets.MNIST\n\n    train_loader: DataLoader\n    valid_loader: DataLoader\n\n    train_batch_size: int = 64\n    valid_batch_size: int = 1024\n\n    train_loader_shuffle: bool = True\n    valid_loader_shuffle: bool = False\n\n\n@MNISTConfigs.calc(MNISTConfigs.dataset_transforms)\ndef mnist_transforms():\n    return transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n\n@MNISTConfigs.calc(MNISTConfigs.train_dataset)\ndef mnist_train_dataset(c: MNISTConfigs):\n    return _dataset(True, c.dataset_transforms)\n\n\n@MNISTConfigs.calc(MNISTConfigs.valid_dataset)\ndef mnist_valid_dataset(c: MNISTConfigs):\n    return _dataset(False, c.dataset_transforms)\n\n\n@MNISTConfigs.calc(MNISTConfigs.train_loader)\ndef mnist_train_loader(c: MNISTConfigs):\n    return DataLoader(c.train_dataset,\n                      batch_size=c.train_batch_size,\n                      shuffle=c.train_loader_shuffle)\n\n\n@MNISTConfigs.calc(MNISTConfigs.valid_loader)\ndef mnist_valid_loader(c: MNISTConfigs):\n    return DataLoader(c.valid_dataset,\n                      batch_size=c.valid_batch_size,\n                      shuffle=c.valid_loader_shuffle)\n\n\nMNISTConfigs.aggregate(MNISTConfigs.dataset_name, 'MNIST',\n                       (MNISTConfigs.dataset_transforms, 'mnist_transforms'),\n                       (MNISTConfigs.train_dataset, 'mnist_train_dataset'),\n                       (MNISTConfigs.valid_dataset, 'mnist_valid_dataset'),\n                       (MNISTConfigs.train_loader, 'mnist_train_loader'),\n                       (MNISTConfigs.valid_loader, 'mnist_valid_loader'))\n"""
labml/internal/analytics/altair/__init__.py,0,b''
labml/internal/analytics/altair/binned_heatmap.py,0,"b'from typing import List\n\nimport altair as alt\nimport numpy as np\nfrom labml.internal.analytics.altair.utils import TABLEAU_10\n\n\ndef data_to_table(data: np.ndarray, x_data: np.ndarray):\n    table = []\n\n    for i in range(data.shape[0]):\n        if len(data.shape) == 2:  # Distribution\n            row = {\'x\': x_data[i, 5],\n                   \'y\': data[i, 5]}\n        else:\n            row = {\'x\': x_data[i],\n                   \'y\': data[i]}\n        table.append(row)\n\n    return alt.Data(values=table)\n\n\ndef _heatmap(table: alt.Data, *,\n             is_ticks: bool,\n             name: str,\n             x_name: str,\n             range_color: str,\n             height: int = None,\n             width: int = None,\n             selection: alt.Selection = None,\n             x_scale: alt.Scale = alt.Undefined,\n             y_scale: alt.Scale = alt.Undefined,\n             brush: alt.Selection = alt.Undefined) -> alt.Chart:\n    base = alt.Chart(table)\n    if selection is not None:\n        base = base.add_selection(selection)\n\n    if not is_ticks:\n        scat_x_title = x_name\n        scat_y_title = name\n    else:\n        scat_x_title = \'\'\n        scat_y_title = \'\'\n\n    scat = (base\n            .mark_rect()\n            .encode(x=alt.X(\'x:Q\', scale=x_scale, title=scat_x_title,\n                            bin=alt.Bin(maxbins=50, extent=brush)),\n                    y=alt.Y(\'y:Q\', scale=y_scale, title=scat_y_title,\n                            bin=alt.Bin(maxbins=50, extent=brush)),\n                    color=alt.Color(\'count(IMDB_Rating):Q\', scale=alt.Scale(scheme=\'greenblue\'))\n                    ))\n\n    if is_ticks:\n        tick_axis = alt.Axis(labels=False, domain=False, ticks=False)\n\n        x_ticks = base.mark_tick().encode(\n            x=alt.X(\'x:Q\', axis=tick_axis, scale=x_scale, title=x_name),\n            color=alt.value(range_color)\n        )\n\n        y_ticks = alt.Chart(table).mark_tick().encode(\n            y=alt.X(\'y:Q\', axis=tick_axis, scale=y_scale, title=name),\n            color=alt.value(range_color)\n        )\n\n        scat = scat.properties(width=width, height=height)\n        x_ticks = x_ticks.properties(width=width)\n        y_ticks = y_ticks.properties(height=height)\n        scat = y_ticks | (scat & x_ticks)\n\n    return scat\n\n\ndef render(tables: List[alt.Data], *,\n           names: List[str],\n           x_name: str,\n           height: int,\n           width: int,\n           height_minimap: int):\n    zoom = alt.selection_interval(encodings=[""x"", ""y""])\n\n    minimaps = None\n    for i, t in enumerate(tables):\n        z = zoom if i == 0 else None\n        minimap = _heatmap(t,\n                           is_ticks=False,\n                           range_color=TABLEAU_10[i % 10],\n                           name=\'\',\n                           x_name=\'\',\n                           selection=z)\n        if minimaps is None:\n            minimaps = minimap\n        else:\n            minimaps += minimap\n\n    details = None\n    for i, t in enumerate(tables):\n        detail = _heatmap(t,\n                          is_ticks=len(tables) == 1,\n                          name=names[i],\n                          x_name=x_name,\n                          range_color=TABLEAU_10[i % 10],\n                          height=height,\n                          width=width,\n                          x_scale=alt.Scale(domain={\'selection\': zoom.name,\n                                                    ""encoding"": ""x""}),\n                          y_scale=alt.Scale(domain={\'selection\': zoom.name,\n                                                    ""encoding"": ""y""}),\n                          brush=zoom)\n        if details is None:\n            details = detail\n        else:\n            details += detail\n\n    minimaps = minimaps.properties(width=width * height_minimap / height, height=height_minimap)\n\n    return details & minimaps\n'"
labml/internal/analytics/altair/density.py,0,"b'from typing import List\n\nimport altair as alt\nfrom labml.internal.analytics.altair.utils import TABLEAU_10\n\n\ndef data_to_table(data, step):\n    table = []\n\n    for i in range(data.shape[0]):\n        if len(data.shape) == 2:\n            if data.shape[1] == 10:\n                row = {\'step\': data[i, 0]}\n                for j in range(1, 10):\n                    row[f""v{j}""] = data[i, j]\n            else:\n                row = {\'step\': data[i, 0],\n                       \'v5\': data[i, 1]}\n        elif step is not None:\n            row = {\'step\': step[i],\n                   \'v5\': data[i]}\n        else:\n            row = {\'step\': i,\n                   \'v5\': data[i]}\n        table.append(row)\n\n    return alt.Data(values=table)\n\n\ndef _render_density(table: alt.Data, *,\n                    name: str,\n                    x_name: str,\n                    line_color: str,\n                    range_color: str,\n                    levels: int,\n                    alpha: float,\n                    selection: alt.Selection = None,\n                    x_scale: alt.Scale = alt.Undefined,\n                    y_scale: alt.Scale = alt.Undefined) -> alt.Chart:\n    areas: List[alt.Chart] = []\n    for i in range(1, levels):\n        y = f""v{5 - i}:Q""\n        y2 = f""v{5 + i}:Q""\n\n        areas.append(\n            alt.Chart(table)\n                .mark_area(opacity=alpha ** i)\n                .encode(x=alt.X(\'step:Q\', scale=x_scale),\n                        y=alt.Y(y, scale=y_scale),\n                        y2=alt.Y2(y2),\n                        color=alt.value(range_color)\n                        )\n        )\n\n    line: alt.Chart = (\n        alt.Chart(table)\n            .mark_line()\n            .encode(x=alt.X(\'step:Q\', scale=x_scale, title=x_name),\n                    y=alt.Y(""v5:Q"", scale=y_scale, title=name),\n                    color=alt.value(line_color)\n                    )\n    )\n    if selection is not None:\n        line = line.add_selection(selection)\n\n    areas_sum = None\n    for a in areas:\n        if areas_sum is None:\n            areas_sum = a\n        else:\n            areas_sum += a\n\n    if areas_sum is None:\n        return line\n    else:\n        return areas_sum + line\n\n\ndef render(tables: List[alt.Data], *,\n           names: List[str],\n           levels=5,\n           alpha=0.6,\n           height: int,\n           width: int,\n           height_minimap: int):\n    zoom = alt.selection_interval(encodings=[""x"", ""y""])\n\n    minimaps = None\n    for i, t in enumerate(tables):\n        z = zoom if i == 0 else None\n        minimap = _render_density(t,\n                                  name=\'\',\n                                  x_name=\'\',\n                                  line_color=TABLEAU_10[i % 10],\n                                  range_color=TABLEAU_10[i % 10],\n                                  levels=levels,\n                                  alpha=alpha,\n                                  selection=z)\n        if minimaps is None:\n            minimaps = minimap\n        else:\n            minimaps += minimap\n\n    details = None\n    for i, t in enumerate(tables):\n        detail = _render_density(t,\n                                 name=names[i] if len(names) == 1 else \'\',\n                                 x_name=\'Step\',\n                                 line_color=TABLEAU_10[i % 10],\n                                 range_color=TABLEAU_10[i % 10],\n                                 levels=levels,\n                                 alpha=alpha,\n                                 x_scale=alt.Scale(domain={\'selection\': zoom.name,\n                                                           ""encoding"": ""x""}),\n                                 y_scale=alt.Scale(domain={\'selection\': zoom.name,\n                                                           ""encoding"": ""y""}))\n        if details is None:\n            details = detail\n        else:\n            details += detail\n\n    colors = alt.Data(values=[{\'idx\': i, \'name\': name} for i, name in enumerate(names)])\n    legend = alt.Chart(colors).mark_rect().encode(\n        alt.Y(\'name:N\',\n              sort=alt.EncodingSortField(field=\'i\', order=\'ascending\'),\n              axis=alt.Axis(\n                  orient=\'right\',\n                  titleX=7,\n                  titleY=-2,\n                  titleAlign=\'left\',\n                  titleAngle=0\n              ),\n              title=\'\'),\n        alt.Color(\'idx:O\',\n                  scale=alt.Scale(domain=list(range(len(tables))),\n                                  range=TABLEAU_10),\n                  legend=None))\n\n    minimaps = minimaps.properties(width=width, height=height_minimap)\n    details = details.properties(width=width, height=height)\n\n    if len(names) == 1:\n        return details & minimaps\n    else:\n        return (details & minimaps) | legend\n'"
labml/internal/analytics/altair/scatter.py,0,"b'from typing import List, Optional, Tuple\n\nimport altair as alt\nimport numpy as np\nfrom labml.internal.analytics.altair.utils import TABLEAU_10\n\n\ndef data_to_table(data: np.ndarray, x_data: np.ndarray,\n                  noise: Optional[Tuple[float, float]]):\n    table = []\n\n    if noise:\n        nx = np.random.rand(data.shape[0]) * noise[1]\n        ny = np.random.rand(data.shape[0]) * noise[0]\n    else:\n        nx = np.zeros(data.shape[0])\n        ny = np.zeros(data.shape[0])\n\n    for i in range(data.shape[0]):\n        if len(data.shape) == 2:  # Distribution\n            row = {\'step\': x_data[i, 0],\n                   \'x\': x_data[i, 5] + nx[i],\n                   \'y\': data[i, 5] + ny[i]}\n        else:\n            row = {\'x\': x_data[i] + nx[i],\n                   \'y\': data[i] + ny[i]}\n        table.append(row)\n\n    return alt.Data(values=table)\n\n\ndef _scatter_chart(table: alt.Data, *,\n                   circle_size: int,\n                   is_ticks: bool,\n                   name: str,\n                   x_name: str,\n                   range_color: str,\n                   height: int = None,\n                   width: int = None,\n                   selection: alt.Selection = None,\n                   x_scale: alt.Scale = alt.Undefined,\n                   y_scale: alt.Scale = alt.Undefined) -> alt.Chart:\n    base = alt.Chart(table)\n    if selection is not None:\n        base = base.add_selection(selection)\n\n    if not is_ticks:\n        scat_x_title = x_name\n        scat_y_title = name\n    else:\n        scat_x_title = \'\'\n        scat_y_title = \'\'\n\n    encode_kwargs = dict(\n        color=alt.value(range_color)\n    )\n    if len(table.values) > 0 and \'step\' in table.values[0]:\n        encode_kwargs[\'opacity\'] = \'step:Q\'\n\n    scat = (base\n            .mark_circle(size=circle_size)\n            .encode(x=alt.X(\'x:Q\', scale=x_scale, title=scat_x_title),\n                    y=alt.Y(\'y:Q\', scale=y_scale, title=scat_y_title),\n                    **encode_kwargs))\n\n    if is_ticks:\n        tick_axis = alt.Axis(labels=False, domain=False, ticks=False)\n\n        x_ticks = base.mark_tick().encode(\n            x=alt.X(\'x:Q\', axis=tick_axis, scale=x_scale, title=x_name),\n            **encode_kwargs)\n\n        y_ticks = alt.Chart(table).mark_tick().encode(\n            y=alt.X(\'y:Q\', axis=tick_axis, scale=y_scale, title=name),\n            **encode_kwargs)\n\n        scat = scat.properties(width=width, height=height)\n        x_ticks = x_ticks.properties(width=width)\n        y_ticks = y_ticks.properties(height=height)\n        scat = y_ticks | (scat & x_ticks)\n\n    return scat\n\n\ndef render(tables: List[alt.Data], *,\n           names: List[str],\n           circle_size: int,\n           x_name: str,\n           height: int,\n           width: int,\n           height_minimap: int):\n    zoom = alt.selection_interval(encodings=[""x"", ""y""])\n\n    minimaps = None\n    for i, t in enumerate(tables):\n        z = zoom if i == 0 else None\n        minimap = _scatter_chart(t,\n                                 is_ticks=False,\n                                 range_color=TABLEAU_10[i % 10],\n                                 name=\'\',\n                                 x_name=\'\',\n                                 selection=z,\n                                 circle_size=circle_size)\n        if minimaps is None:\n            minimaps = minimap\n        else:\n            minimaps += minimap\n\n    details = None\n    for i, t in enumerate(tables):\n        detail = _scatter_chart(t,\n                                is_ticks=len(tables) == 1,\n                                name=names[i],\n                                x_name=x_name,\n                                range_color=TABLEAU_10[i % 10],\n                                height=height,\n                                width=width,\n                                x_scale=alt.Scale(domain={\'selection\': zoom.name,\n                                                          ""encoding"": ""x""}),\n                                y_scale=alt.Scale(domain={\'selection\': zoom.name,\n                                                          ""encoding"": ""y""}),\n                                circle_size=circle_size)\n\n        if details is None:\n            details = detail\n        else:\n            details += detail\n\n    minimaps = minimaps.properties(width=width * height_minimap / height, height=height_minimap)\n\n    return details & minimaps\n'"
labml/internal/analytics/altair/utils.py,0,"b""TABLEAU_10 = ['#4E79A7',\n              '#F28E2C',\n              '#E15759',\n              '#76B7B2',\n              '#59A14F',\n              '#EDC949',\n              '#AF7AA1',\n              '#FF9DA7',\n              '#9C755F',\n              '#BAB0AB']\n"""
labml/internal/analytics/matplotlib/__init__.py,0,"b'from matplotlib.axes import Axes\n\n\nclass MatPlotLibAnalytics:\n    def render_density(self, ax: Axes, data, color, name, *,\n                       levels=5,\n                       line_width=1,\n                       alpha=0.6):\n        ln = ax.plot(data[:, 0], data[:, 5],\n                     lw=line_width,\n                     color=color,\n                     alpha=1,\n                     label=name)\n\n        for i in range(1, levels):\n            ax.fill_between(\n                data[:, 0],\n                data[:, 5 - i],\n                data[:, 5 + i],\n                color=color,\n                lw=0,\n                alpha=alpha ** i)\n\n        return ln\n'"
labml/internal/analytics/matplotlib/tb.py,0,"b'from typing import List\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass MatPlotLibTensorBoardAnalytics:\n    @staticmethod\n    def render_matrix(matrix, ax, color):\n        from matplotlib.patches import Rectangle\n        from matplotlib.collections import PatchCollection\n\n        rows, cols = matrix.shape\n        x_ticks = [matrix[0, i] for i in range(1, cols)]\n        y_ticks = [matrix[i, 0] for i in range(1, rows)]\n\n        max_density = 0\n        for y in range(rows - 2):\n            for x in range(cols - 2):\n                area = (x_ticks[x + 1] - x_ticks[x]) * (y_ticks[y + 1] - y_ticks[y])\n                density = matrix[y + 1, x + 1] / area\n                if max_density < density:\n                    max_density = density\n\n        boxes = []\n        for y in range(rows - 2):\n            for x in range(cols - 2):\n                width = x_ticks[x + 1] - x_ticks[x]\n                height = y_ticks[y + 1] - y_ticks[y]\n                area = width * height\n                density = matrix[y + 1, x + 1] / area\n                density = max(density, 1)\n                # alpha = np.log(density) / np.log(max_density)\n                alpha = density / max_density\n                rect = Rectangle((x_ticks[x], y_ticks[y]), width, height,\n                                 alpha=alpha, color=color)\n                boxes.append(rect)\n\n        pc = PatchCollection(boxes, match_original=True)\n\n        ax.add_collection(pc)\n\n        # Plot something\n        _ = ax.errorbar([], [], xerr=[], yerr=[],\n                        fmt=\'None\', ecolor=\'k\')\n\n        return x_ticks[0], x_ticks[-1], y_ticks[0], y_ticks[-1]\n\n    def render_tensors(self, tensors: List[any], axes: np.ndarray, color):\n        assert len(axes.shape) == 2\n        assert axes.shape[0] * axes.shape[1] == len(tensors)\n        x_min = x_max = y_min = y_max = 0\n\n        for i in range(axes.shape[0]):\n            for j in range(axes.shape[1]):\n                idx = i * axes.shape[1] + j\n                axes[i, j].set_title(f""{tensors[idx].step :,}"")\n                matrix = tf.make_ndarray(tensors[idx].tensor_proto)\n                x1, x2, y1, y2 = self.render_matrix(matrix,\n                                                    axes[i, j],\n                                                    color)\n                x_min = min(x_min, x1)\n                x_max = max(x_max, x2)\n                y_min = min(y_min, y1)\n                y_max = max(y_max, y2)\n\n        for i in range(axes.shape[0]):\n            for j in range(axes.shape[1]):\n                axes[i, j].set_xlim(x_min, x_max)\n                axes[i, j].set_ylim(y_min, y_max)\n'"
labml/internal/logger/destinations/__init__.py,0,"b'from typing import List, Union, Tuple, Optional\n\nfrom labml.internal.util.colors import StyleCode\n\n\nclass Destination:\n    def log(self, parts: List[Union[str, Tuple[str, Optional[StyleCode]]]], *,\n            is_new_line=True):\n        raise NotImplementedError()\n'"
labml/internal/logger/destinations/console.py,0,"b'from typing import List, Union, Tuple, Optional\n\nfrom labml.internal.util.colors import StyleCode, ANSI_RESET\nfrom labml.internal.logger.destinations import Destination\n\n\nclass ConsoleDestination(Destination):\n    @staticmethod\n    def __ansi_code(text: str, color: List[StyleCode] or StyleCode or None):\n        """"""\n        ### Add ansi color codes\n        """"""\n        if color is None:\n            return text\n        elif type(color) is list:\n            return """".join([c.ansi() for c in color]) + f""{text}{ANSI_RESET}""\n        else:\n            return f""{color.ansi()}{text}{ANSI_RESET}""\n\n    def log(self, parts: List[Union[str, Tuple[str, Optional[StyleCode]]]], *,\n            is_new_line=True):\n        tuple_parts = []\n        for p in parts:\n            if type(p) == str:\n                tuple_parts.append((p, None))\n            else:\n                tuple_parts.append(p)\n        coded = [self.__ansi_code(text, color) for text, color in tuple_parts]\n\n        if is_new_line:\n            end_char = \'\\n\'\n        else:\n            end_char = \'\'\n\n        text = """".join(coded)\n\n        print(""\\r"" + text, end=end_char, flush=True)\n'"
labml/internal/logger/destinations/factory.py,0,b'from labml.internal.logger.destinations import Destination\nfrom labml.internal.util import is_ipynb\n\n\ndef create_destination() -> Destination:\n    if is_ipynb():\n        from labml.internal.logger.destinations.ipynb import IpynbDestination\n        return IpynbDestination()\n    else:\n        from labml.internal.logger.destinations.console import ConsoleDestination\n        return ConsoleDestination()\n'
labml/internal/logger/destinations/ipynb.py,0,"b'from typing import List, Union, Tuple, Callable, Optional\n\nfrom IPython.core.display import display, HTML\n\nfrom labml.internal.logger import StyleCode\nfrom labml.internal.logger.destinations import Destination\n\nget_ipython: Callable\n\n\nclass IpynbDestination(Destination):\n    def __init__(self):\n        self.__last_handle = None\n        self.__last_id = 1\n        self.__cell_lines = []\n        self.__cell_count = 0\n\n    def is_same_cell(self):\n        cells = get_ipython().ev(\'len(In)\')\n        if cells == self.__cell_count:\n            return True\n\n        self.__cell_count = cells\n        self.__cell_lines = []\n        self.__last_handle = None\n\n        return False\n\n    @staticmethod\n    def __html_code(text: str, color: List[StyleCode] or StyleCode or None):\n        """"""\n        ### Add ansi color codes\n        """"""\n        if text == \'\\n\':\n            assert color is None\n            return text\n\n        if color is None:\n            return text\n        elif type(color) is list:\n            open_tags = \'\'.join([c.html_open() for c in color])\n            close_tags = \'\'.join([c.html_close() for c in reversed(color)])\n        else:\n            open_tags = color.html_open()\n            close_tags = color.html_close()\n\n        return open_tags + text + close_tags\n\n    def log(self, parts: List[Union[str, Tuple[str, Optional[StyleCode]]]], *,\n            is_new_line=True):\n        tuple_parts = []\n        for p in parts:\n            if type(p) == str:\n                text = p\n                style = None\n            else:\n                text, style = p\n            lines = text.split(\'\\n\')\n            for line in lines[:-1]:\n                tuple_parts.append((line, style))\n                tuple_parts.append((\'\\n\', None))\n            tuple_parts.append((lines[-1], style))\n\n        coded = [self.__html_code(text, color) for text, color in tuple_parts]\n\n        text = """".join(coded)\n        lines = text.split(\'\\n\')\n        if self.is_same_cell():\n            self.__cell_lines.pop()\n            self.__cell_lines += lines\n            text = \'\\n\'.join(self.__cell_lines)\n            html = HTML(f""<pre>{text}</pre>"")\n            self.__last_handle.update(html)\n        else:\n            self.__cell_lines = lines\n            text = \'\\n\'.join(self.__cell_lines)\n            html = HTML(f""<pre>{text}</pre>"")\n            self.__last_handle = display(html, display_id=self.__last_id)\n            self.__last_id += 1\n\n        # print(len(self.__cell_lines), self.__cell_lines[-1], is_new_line)\n        if is_new_line:\n            self.__cell_lines.append(\'\')\n'"
labml/internal/logger/store/__init__.py,0,"b'from pathlib import PurePath\nfrom typing import Dict, List, Any\n\nfrom .artifacts import Artifact\nfrom .indicators import Indicator, Scalar\nfrom .namespace import Namespace\nfrom ..writers import Writer\nfrom ... import util\n\n\nclass Store:\n    dot_artifacts: Dict[str, Artifact]\n    dot_indicators: Dict[str, Indicator]\n    namespaces: List[Namespace]\n    artifacts: Dict[str, Artifact]\n    indicators: Dict[str, Indicator]\n\n    def __init__(self):\n        self.indicators = {}\n        self.artifacts = {}\n        self.dot_indicators = {}\n        self.dot_artifacts = {}\n        self.__indicators_file = None\n        self.__artifacts_file = None\n        self.namespaces = []\n\n    def save_indicators(self, file: PurePath):\n        self.__indicators_file = file\n\n        data = {k: ind.to_dict() for k, ind in self.indicators.items() }\n        with open(str(file), ""w"") as file:\n            file.write(util.yaml_dump(data))\n\n    def save_artifacts(self, file: PurePath):\n        self.__artifacts_file = file\n\n        data = {k: art.to_dict() for k, art in self.artifacts.items()}\n        with open(str(file), ""w"") as file:\n            file.write(util.yaml_dump(data))\n\n    def __assert_name(self, name: str, value: any):\n        if name.startswith("".""):\n            if name in self.dot_indicators:\n                assert self.dot_indicators[name].equals(value)\n            if name in self.dot_artifacts:\n                assert self.dot_artifacts[name].equals(value)\n\n        assert name not in self.indicators, f""{name} already used""\n        assert name not in self.artifacts, f""{name} already used""\n\n    def namespace_enter(self, ns: Namespace):\n        self.namespaces.append(ns)\n\n    def namespace_exit(self, ns: Namespace):\n        if len(self.namespaces) == 0:\n            raise RuntimeError(""Impossible"")\n\n        if ns is not self.namespaces[-1]:\n            raise RuntimeError(""Impossible"")\n\n        self.namespaces.pop(-1)\n\n    def add_indicator(self, indicator: Indicator):\n        self.__assert_name(indicator.name, indicator)\n        if indicator.name.startswith(\'.\'):\n            self.dot_indicators[indicator.name] = indicator\n            return\n\n        self.indicators[indicator.name] = indicator\n        indicator.clear()\n\n        if self.__indicators_file is not None:\n            self.save_indicators(self.__indicators_file)\n\n    def add_artifact(self, artifact: Artifact):\n        self.__assert_name(artifact.name, artifact)\n        if artifact.name.startswith(\'.\'):\n            self.dot_artifacts[artifact.name] = artifact\n            return\n\n        self.artifacts[artifact.name] = artifact\n        artifact.clear()\n\n        if self.__artifacts_file is not None:\n            self.save_artifacts(self.__artifacts_file)\n\n    def store(self, key: str, value: any):\n        suffix = key\n        if key.startswith(\'.\'):\n            key = \'.\'.join([ns.name for ns in self.namespaces] + [key[1:]])\n\n        if key in self.artifacts:\n            self.artifacts[key].collect_value(None, value)\n        elif key in self.indicators:\n            self.indicators[key].collect_value(value)\n        elif suffix in self.dot_indicators:\n            self.add_indicator(self.dot_indicators[suffix].copy(key))\n            self.indicators[key].collect_value(value)\n        elif suffix in self.dot_artifacts:\n            self.add_artifact(self.dot_artifacts[suffix].copy(key))\n            self.artifacts[key].collect_value(None, value)\n        else:\n            self.add_indicator(Scalar(key, True))\n            self.indicators[key].collect_value(value)\n\n    def clear(self):\n        for k, v in self.indicators.items():\n            v.clear()\n        for k, v in self.artifacts.items():\n            v.clear()\n\n    def write(self, writer: Writer, global_step):\n        return writer.write(global_step=global_step,\n                            indicators=self.indicators,\n                            artifacts=self.artifacts)\n\n    def create_namespace(self, name: str):\n        return Namespace(store=self,\n                         name=name)\n'"
labml/internal/logger/store/artifacts.py,2,"b'from abc import ABC\nfrom collections import OrderedDict\nfrom typing import Dict, Optional, Any, OrderedDict as OrderedDictType\nfrom uuid import uuid1\n\ntry:\n    import matplotlib.pyplot as plt\nexcept (ImportError, ModuleNotFoundError):\n    plt = None\n\nimport numpy as np\n\nfrom labml import logger\nfrom labml.logger import Text as TextStyle\n\ntry:\n    import torch\nexcept ImportError:\n    torch = None\n\n\ndef _to_numpy(value):\n    type_ = type(value)\n\n    if type_ == float or type_ == int:\n        return np.array(value)\n\n    if type_ == list:\n        return np.array(value)\n\n    if type_ == np.ndarray:\n        return value\n\n    if torch is not None:\n        if type_ == torch.nn.parameter.Parameter:\n            return value.data.cpu().numpy()\n        if type_ == torch.Tensor:\n            return value.data.cpu().numpy()\n\n    assert False, f""Unknown type {type_}""\n\n\nclass Artifact(ABC):\n    def __init__(self, *, name: str, is_print: bool):\n        self.is_print = is_print\n        self.name = name\n\n    def clear(self):\n        pass\n\n    def is_empty(self) -> bool:\n        raise NotImplementedError()\n\n    def to_dict(self) -> Dict:\n        return dict(class_name=self.__class__.__name__,\n                    name=self.name,\n                    is_print=self.is_print)\n\n    def _collect_value(self, key: str, value):\n        raise NotImplementedError()\n\n    def get_print_length(self) -> Optional[int]:\n        raise NotImplementedError()\n\n    @property\n    def is_indexed(self) -> bool:\n        return False\n\n    def get_string(self, key: str, others: Dict[str, \'Artifact\']) -> Optional[str]:\n        return None\n\n    def print_all(self, others: Dict[str, \'Artifact\']):\n        pass\n\n    def keys(self):\n        raise NotImplementedError()\n\n    def get_value(self, key: str):\n        return None\n\n    def collect_value(self, key: Optional[str], value):\n        if key is None:\n            if type(value) == tuple and len(value) == 2:\n                key = value[0]\n                value = value[1]\n            else:\n                key = uuid1().hex\n\n        self._collect_value(key, value)\n\n    def copy(self, key: str):\n        raise NotImplementedError()\n\n    def equals(self, value: any) -> bool:\n        if type(value) != type(self):\n            return False\n        return value.name == self.name and value.is_print == self.is_print\n\n\nclass _Collection(Artifact, ABC):\n    _values: OrderedDictType[str, Any]\n\n    def __init__(self, name: str, is_print=False):\n        super().__init__(name=name, is_print=is_print)\n        self._values = OrderedDict()\n\n    def _collect_value(self, key: str, value):\n        self._values[key] = value\n\n    def clear(self):\n        self._values = OrderedDict()\n\n    def is_empty(self) -> bool:\n        return len(self._values) == 0\n\n    def keys(self):\n        return self._values.keys()\n\n    def get_value(self, key: str):\n        return self._values[key]\n\n\nclass Tensor(_Collection):\n    def __init__(self, name: str, is_once: bool = False):\n        super().__init__(name=name, is_print=False)\n        self.is_once = is_once\n\n    def to_dict(self) -> Dict:\n        return dict(class_name=self.__class__.__name__,\n                    name=self.name,\n                    is_print=self.is_print,\n                    is_once=self.is_once)\n\n    def get_print_length(self) -> Optional[int]:\n        return None\n\n    def copy(self, key: str):\n        return Tensor(key, is_once=self.is_once)\n\n    def _collect_value(self, key: str, value):\n        self._values[key] = _to_numpy(value)\n\n    def equals(self, value: any) -> bool:\n        if not isinstance(value, Tensor):\n            return False\n        return value.name == self.name and value.is_once == self.is_once\n\n\nclass Image(_Collection):\n    def get_print_length(self) -> Optional[int]:\n        return None\n\n    def print_all(self, others: Dict[str, Artifact]):\n        if plt is None:\n            logger.log((\'matplotlib\', logger.Text.highlight),\n                       \' not found. So cannot display impages\')\n        images = [_to_numpy(v) for v in self._values.values()]\n        cols = 3\n        fig: plt.Figure\n        fig, axs = plt.subplots((len(images) + cols - 1) // cols, cols,\n                                sharex=\'all\', sharey=\'all\',\n                                figsize=(8, 10))\n        fig.suptitle(self.name)\n        for i, img in enumerate(images):\n            ax: plt.Axes = axs[i // cols, i % cols]\n            ax.imshow(img)\n        plt.show()\n\n    def copy(self, key: str):\n        return Image(key, is_print=self.is_print)\n\n\nclass Text(_Collection):\n    def get_print_length(self) -> Optional[int]:\n        return None\n\n    def print_all(self, others: Dict[str, Artifact]):\n        logger.log(self.name, TextStyle.heading)\n        for t in self._values.values():\n            logger.log(t, TextStyle.value)\n\n    def copy(self, key: str):\n        return Text(key, is_print=self.is_print)\n\n    def equals(self, value: any) -> bool:\n        if not isinstance(value, Text):\n            return False\n        return value.name == self.name and value.is_print == self.is_print\n\n\nclass IndexedText(_Collection):\n    def __init__(self, name: str, title: Optional[str] = None, is_print=False):\n        super().__init__(name=name, is_print=is_print)\n        self.title = title\n\n    @_Collection.is_indexed.getter\n    def is_indexed(self) -> bool:\n        return True\n\n    def get_print_length(self) -> Optional[int]:\n        return max((len(v) for v in self._values.values()))\n\n    def get_string(self, key: str, others: Dict[str, Artifact]) -> Optional[str]:\n        return self._values[key]\n\n    def copy(self, key: str):\n        return IndexedText(key, title=self.title, is_print=self.is_print)\n\n    def equals(self, value: any) -> bool:\n        if not super().equals(value):\n            return False\n        return value.title == self.title\n'"
labml/internal/logger/store/indicators.py,2,"b'from abc import ABC\nfrom collections import deque\nfrom typing import Dict, Optional\n\nimport numpy as np\n\ntry:\n    import torch\nexcept ImportError:\n    torch = None\n\n\ndef _to_numpy(value):\n    type_ = type(value)\n\n    if type_ == float or type_ == int:\n        return np.array(value).ravel()\n\n    if isinstance(value, np.number):\n        return np.array(value.item()).ravel()\n\n    if type_ == list:\n        return np.array(value).ravel()\n\n    if type_ == np.ndarray:\n        return value.ravel()\n\n    if torch is not None:\n        if type_ == torch.nn.parameter.Parameter:\n            return value.data.cpu().numpy().ravel()\n        if type_ == torch.Tensor:\n            return value.data.cpu().numpy().ravel()\n\n    assert False, f""Unknown type {type_}""\n\n\nclass Indicator:\n    def __init__(self, *, name: str, is_print: bool):\n        self.is_print = is_print\n        self.name = name\n\n    def clear(self):\n        pass\n\n    def is_empty(self) -> bool:\n        raise NotImplementedError()\n\n    def to_dict(self) -> Dict:\n        return dict(class_name=self.__class__.__name__,\n                    name=self.name,\n                    is_print=self.is_print)\n\n    def collect_value(self, value):\n        raise NotImplementedError()\n\n    def get_mean(self) -> Optional[float]:\n        return None\n\n    def get_histogram(self):\n        return None\n\n    @property\n    def mean_key(self):\n        return f\'{self.name}\'\n\n    def get_index_mean(self):\n        return None, None\n\n    def copy(self, key: str):\n        raise NotImplementedError()\n\n    def equals(self, value: any) -> bool:\n        if type(value) != type(self):\n            return False\n        return value.name == self.name and value.is_print == self.is_print\n\n\nclass Queue(Indicator):\n    def __init__(self, name: str, queue_size=10, is_print=False):\n        super().__init__(name=name, is_print=is_print)\n        self.queue_size = queue_size\n        self._values = deque(maxlen=queue_size)\n\n    def collect_value(self, value):\n        self._values.append(_to_numpy(value))\n\n    def to_dict(self) -> Dict:\n        res = super().to_dict().copy()\n        res.update({\'queue_size\': self._values.maxlen})\n        return res\n\n    def is_empty(self) -> bool:\n        return len(self._values) == 0\n\n    def get_mean(self) -> float:\n        return float(np.mean(self._values))\n\n    def get_histogram(self):\n        return self._values\n\n    @property\n    def mean_key(self):\n        return f\'{self.name}.mean\'\n\n    def copy(self, key: str):\n        return Queue(key, queue_size=self.queue_size, is_print=self.is_print)\n\n    def equals(self, value: any) -> bool:\n        if not super().equals(value):\n            return False\n        return value.queue_size == self.queue_size\n\n\nclass _Collection(Indicator, ABC):\n    def __init__(self, name: str, is_print: bool):\n        super().__init__(name=name, is_print=is_print)\n        self._values = []\n\n    def _merge(self):\n        if len(self._values) == 0:\n            return []\n        elif len(self._values) == 1:\n            return self._values[0]\n        else:\n            merged = np.concatenate(self._values, axis=0)\n            self._values = [merged]\n\n            return merged\n\n    def collect_value(self, value):\n        self._values.append(_to_numpy(value))\n\n    def clear(self):\n        self._values = []\n\n    def is_empty(self) -> bool:\n        return len(self._values) == 0\n\n    def get_mean(self) -> float:\n        return float(np.mean(self._merge()))\n\n    def get_histogram(self):\n        return self._merge()\n\n\nclass Histogram(_Collection):\n    @property\n    def mean_key(self):\n        return f\'{self.name}.mean\'\n\n    def copy(self, key: str):\n        return Histogram(key, is_print=self.is_print)\n\n\nclass Scalar(_Collection):\n    def get_histogram(self):\n        return None\n\n    def copy(self, key: str):\n        return Scalar(key, is_print=self.is_print)\n\n\nclass _IndexedCollection(Indicator, ABC):\n    def __init__(self, name: str):\n        super().__init__(name=name, is_print=False)\n        self._values = []\n        self._indexes = []\n\n    def clear(self):\n        self._values = []\n        self._indexes = []\n\n    def collect_value(self, value):\n        if type(value) == tuple:\n            assert len(value) == 2\n            if type(value[0]) == int:\n                self._indexes.append(value[0])\n                self._values.append(value[1])\n            else:\n                assert type(value[0]) == list\n                assert len(value[0]) == len(value[1])\n                self._indexes += value[0]\n                self._values += value[1]\n        else:\n            assert type(value) == list\n            self._indexes += [v[0] for v in value]\n            self._values += [v[1] for v in value]\n\n    def is_empty(self) -> bool:\n        return len(self._values) == 0\n\n    def get_mean(self) -> float:\n        return float(np.mean(self._values))\n\n    def get_index_mean(self):\n        summary = {}\n        for ind, values in zip(self._indexes, self._values):\n            if ind not in summary:\n                summary[ind] = []\n            summary[ind].append(values)\n\n        indexes = []\n        means = []\n        for ind, values in summary.items():\n            indexes.append(ind)\n            means.append(float(np.mean(values)))\n\n        return indexes, means\n\n\nclass IndexedScalar(_IndexedCollection):\n    def get_histogram(self):\n        return None\n\n    def copy(self, key: str):\n        return IndexedScalar(key)\n'"
labml/internal/logger/store/namespace.py,0,"b""from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from . import Store\n\n\nclass Namespace:\n    def __init__(self, *,\n                 store: 'Store',\n                 name: str):\n        self._store = store\n        self.name = name\n\n    def __enter__(self):\n        self._store.namespace_enter(self)\n\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self._store.namespace_exit(self)\n"""
labml/internal/logger/util/__init__.py,0,b''
labml/internal/logger/util/pytorch.py,0,b''
labml/internal/logger/writers/__init__.py,0,"b'from typing import Dict\n\nimport numpy as np\n\nfrom labml import logger\nfrom labml.logger import Text\nfrom labml.internal.logger.store.artifacts import Artifact\nfrom labml.internal.logger.store.indicators import Indicator\n\n\nclass Writer:\n    def write(self, *,\n              global_step: int,\n              indicators: Dict[str, Indicator],\n              artifacts: Dict[str, Artifact]):\n        raise NotImplementedError()\n\n    def flush(self):\n        pass\n\n    def write_h_parameters(self, hparams: Dict[str, any]):\n        pass\n\n\nclass ScreenWriter(Writer):\n    def __init__(self):\n        super().__init__()\n\n        self._estimates = {}\n        self._beta = 0.9\n        self._beta_pow = {}\n        self._last_printed_value = {}\n\n    def update_estimate(self, k, v):\n        if k not in self._estimates:\n            self._estimates[k] = 0\n            self._beta_pow[k] = 1.\n\n        self._estimates[k] *= self._beta\n        self._estimates[k] += (1 - self._beta) * v\n        self._beta_pow[k] *= self._beta\n\n    def get_empty_string(self, length, decimals):\n        return \' \' * (length - 2 - decimals) + \'-.\' + \'-\' * decimals\n\n    def get_value_string(self, k, v):\n        if k not in self._estimates:\n            assert v is None\n            return self.get_empty_string(8, 2)\n\n        estimate = self._estimates[k] / (1 - self._beta_pow[k])\n        if abs(estimate) < 1e-9:\n            lg = 0\n        else:\n            lg = int(np.ceil(np.log10(abs(estimate)))) + 1\n\n        decimals = 7 - lg\n        decimals = max(1, decimals)\n        decimals = min(6, decimals)\n\n        fmt = ""{v:8,."" + str(decimals) + ""f}""\n        if v is None:\n            return self.get_empty_string(8, decimals)\n        else:\n            return fmt.format(v=v)\n\n    @staticmethod\n    def __format_artifact(length: int, value: str):\n        fmt = ""{v:>"" + str(length + 1) + ""}""\n        return fmt.format(v=value)\n\n    def _get_indicator_string(self, indicators: Dict[str, Indicator]):\n        parts = []\n\n        for ind in indicators.values():\n            if not ind.is_print:\n                continue\n\n            parts.append((f"" {ind.name}: "", None))\n\n            if not ind.is_empty():\n                v = ind.get_mean()\n                self.update_estimate(ind.name, v)\n                value = self.get_value_string(ind.name, v)\n                self._last_printed_value[ind.name] = value\n                parts.append((value, Text.value))\n            elif ind.name in self._last_printed_value:\n                value = self._last_printed_value[ind.name]\n                parts.append((value, Text.subtle))\n            else:\n                value = self.get_value_string(ind.name, None)\n                parts.append((value, Text.subtle))\n\n        return parts\n\n    def _print_artifacts_list(self, table: Dict[str, int], artifacts: Dict[str, Artifact]):\n        order = list(table.keys())\n        if not len(order):\n            return\n\n        keys = {k for name in order for k in artifacts[name].keys()}\n        for k in keys:\n            for name in order:\n                value = artifacts[name].get_string(k, artifacts)\n                logger.log([(name, Text.key),\n                            "": "",\n                            (value, Text.value)])\n\n    def _print_artifacts_table(self, table: Dict[str, int], artifacts: Dict[str, Artifact]):\n        order = list(table.keys())\n        if not len(order):\n            return\n\n        keys = []\n        keys_set = set()\n\n        for name in order:\n            for k in artifacts[name].keys():\n                if k not in keys_set:\n                    keys_set.add(k)\n                    keys.append(k)\n\n        parts = [self.__format_artifact(table[name], name) for name in order]\n        logger.log(\'|\'.join(parts), Text.heading)\n\n        for k in keys:\n            parts = []\n            for name in order:\n                value = artifacts[name].get_string(k, artifacts)\n                parts.append(self.__format_artifact(table[name], value))\n            logger.log(\'|\'.join(parts), Text.value)\n\n    def _print_artifacts(self, artifacts: Dict[str, Artifact]):\n        table = {}\n        for art in artifacts.values():\n            if not art.is_print:\n                continue\n            if art.is_empty():\n                continue\n            if not art.is_indexed:\n                art.print_all(artifacts)\n                continue\n\n            table[art.name] = art.get_print_length()\n\n        if sum(table.values()) > 100:\n            self._print_artifacts_list(table, artifacts)\n        else:\n            self._print_artifacts_table(table, artifacts)\n\n    def write(self, *,\n              global_step: int,\n              indicators: Dict[str, Indicator],\n              artifacts: Dict[str, Artifact]):\n\n        self._print_artifacts(artifacts)\n\n        return self._get_indicator_string(indicators)\n'"
labml/internal/logger/writers/sqlite.py,0,"b'import sqlite3\nimport time\nfrom pathlib import PurePath\nfrom typing import Dict, Optional\n\nimport numpy as np\n\nfrom . import Writer as WriteBase\nfrom ..store.artifacts import Artifact, Tensor\nfrom ..store.indicators import Indicator\n\n\nclass Writer(WriteBase):\n    conn: Optional[sqlite3.Connection]\n\n    def __init__(self, sqlite_path: PurePath, artifacts_path: PurePath):\n        super().__init__()\n\n        self.sqlite_path = sqlite_path\n        self.artifacts_path = artifacts_path\n        self.conn = None\n        self.scalars_cache = []\n        self.indexed_scalars_cache = []\n        self.last_committed = time.time()\n\n    def __connect(self):\n        if self.conn is not None:\n            return\n\n        self.conn = sqlite3.connect(str(self.sqlite_path))\n\n        try:\n            self.conn.execute(f""CREATE TABLE scalars ""\n                              f""(indicator text, step integer, value real)"")\n            self.conn.execute(f""CREATE TABLE indexed_scalars ""\n                              f""(indicator text, step integer, idx integer, value real)"")\n            self.conn.execute(f""CREATE TABLE tensors ""\n                              f""(indicator text, step integer, filename text)"")\n\n        except sqlite3.OperationalError:\n            print(\'Scalar table exists\')\n\n    @staticmethod\n    def _parse_key(key: str):\n        return key\n        # if we name tables\n        # return key.replace(\'.\', \'_\')\n\n    def write(self, *,\n              global_step: int,\n              indicators: Dict[str, Indicator],\n              artifacts: Dict[str, Artifact]):\n        self.__connect()\n\n        for ind in indicators.values():\n            if ind.is_empty():\n                continue\n\n            value = ind.get_mean()\n            if value is not None:\n                key = self._parse_key(ind.mean_key)\n                self.conn.execute(\n                    f""INSERT INTO scalars VALUES (?, ?, ?)"",\n                    (key, global_step, value))\n\n            idx, value = ind.get_index_mean()\n            if idx is not None:\n                key = self._parse_key(ind.mean_key)\n                data = [(key, global_step, i, v) for i, v in zip(idx, value)]\n                self.conn.executemany(\n                    f""INSERT INTO indexed_scalars VALUES (?, ?, ?, ?)"",\n                    data)\n\n        for art in artifacts.values():\n            if art.is_empty():\n                continue\n            key = self._parse_key(art.name)\n            if isinstance(art, Tensor):\n                for k in art.keys():\n                    tensor = art.get_value(k)\n                    if not art.is_once:\n                        filename = f\'{key}_{global_step}_{k}.npy\'\n                        self.conn.execute(\n                            f""INSERT INTO tensors VALUES (?, ?, ?)"",\n                            (key, global_step, filename))\n                    else:\n                        filename = f\'{key}_{k}.npy\'\n                    self.conn.execute(\n                        f""INSERT INTO tensors VALUES (?, ?, ?)"",\n                        (key, -1, filename))\n                    np.save(str(self.artifacts_path / filename), tensor)\n\n        t = time.time()\n        if t - self.last_committed > 0.1:\n            self.last_committed = t\n            self.flush()\n\n    def flush(self):\n        if self.conn is not None:\n            self.conn.commit()\n'"
labml/internal/logger/writers/tensorboard.py,0,"b'from pathlib import PurePath\nfrom typing import Dict\n\nimport tensorflow as tf\nfrom tensorboard.plugins.hparams import api as hp\n\nfrom . import Writer as WriteBase\nfrom labml.internal.logger.store.artifacts import Artifact, Image\nfrom labml.internal.logger.store.indicators import Indicator\n\ntf.config.experimental.set_visible_devices([], ""GPU"")\n\n\n# os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n# os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\n\n\nclass Writer(WriteBase):\n    def __init__(self, log_path: PurePath):\n        super().__init__()\n\n        self.__log_path = log_path\n        self.__writer = None\n\n    def __connect(self):\n        if self.__writer is not None:\n            return\n\n        self.__writer = tf.summary.create_file_writer(str(self.__log_path))\n\n    @staticmethod\n    def _parse_key(key: str):\n        return key.replace(\'.\', \'/\')\n\n    def write_h_parameters(self, hparams: Dict[str, any]):\n        self.__connect()\n\n        with self.__writer.as_default():\n            hp.hparams(hparams)\n\n    def write(self, *,\n              global_step: int,\n              indicators: Dict[str, Indicator],\n              artifacts: Dict[str, Artifact]):\n        self.__connect()\n\n        with self.__writer.as_default():\n            for ind in indicators.values():\n                if ind.is_empty():\n                    continue\n\n                hist_data = ind.get_histogram()\n                if hist_data is not None:\n                    tf.summary.histogram(self._parse_key(ind.name), hist_data, step=global_step)\n\n                mean_value = ind.get_mean()\n                if mean_value is not None:\n                    tf.summary.scalar(self._parse_key(ind.mean_key), mean_value,\n                                      step=global_step)\n\n            for art in artifacts.values():\n                if art.is_empty():\n                    continue\n\n                if type(art) == Image:\n                    # Expecting NxCxHxW images in pytorch tensors normalized in [0, 1]\n                    for key in art.keys():\n                        img = art.get_value(key)\n                        img = img.cpu().detach().permute(0, 2, 3, 1).numpy()\n                        tf.summary.image(self._parse_key(art.name),\n                                         img,\n                                         step=global_step)\n'"
